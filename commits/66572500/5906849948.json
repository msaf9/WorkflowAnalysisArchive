[{"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 22, "deletions": 0, "changes": 22, "file_content_changes": "@@ -510,6 +510,28 @@ def TT_MakeRangeOp : TT_Op<\"make_range\", [Pure]> {\n     let assemblyFormat = \"attr-dict `:` type($result)\";\n }\n \n+//\n+// ElementwiseInlineAsm Op\n+//\n+def TT_ElementwiseInlineAsmOp : TT_Op<\"elementwise_inline_asm\", [Elementwise,\n+                                                                 SameOperandsAndResultEncoding,\n+                                                                 DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {\n+  let summary = \"inline assembly applying elementwise operation to a group of packed element.\";\n+  let description = [{\n+   This will apply the given in inline assembly to `packed_element` number of\n+   elements of the inputs. The elements packed together is unknown and will\n+   depend on the backend implementation.\n+  }];\n+\n+  let arguments = (ins StrAttr:$asm_string, StrAttr:$constraints, BoolAttr:$pure, I32Attr:$packed_element, Variadic<AnyTypeOf<[TT_Type]>>:$args);\n+  let results = (outs TT_Tensor:$result);\n+\n+\n+  let assemblyFormat = [{\n+    $asm_string attr-dict ($args^ `:` type($args))? `->` type($result)\n+  }];\n+}\n+\n //\n // Print Op\n //"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 81, "deletions": 0, "changes": 81, "file_content_changes": "@@ -811,6 +811,86 @@ struct ExternElementwiseOpConversion\n   }\n };\n \n+struct ElementwiseInlineAsmOpConversion\n+    : public ElementwiseOpConversionBase<ElementwiseInlineAsmOp,\n+                                         ElementwiseInlineAsmOpConversion> {\n+  using Base = ElementwiseOpConversionBase<ElementwiseInlineAsmOp,\n+                                           ElementwiseInlineAsmOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+  typedef typename Base::OpAdaptor OpAdaptor;\n+\n+  // If operand size is smaller than 32bits pack by groups of 32bits.\n+  // Otherwise have separate inputs.\n+  SmallVector<Value> packOperands(ElementwiseInlineAsmOp op,\n+                                  MultipleOperandsRange operands,\n+                                  ConversionPatternRewriter &rewriter,\n+                                  Location loc) const {\n+    SmallVector<Value> packedOperands;\n+    unsigned numPackedElements = op.getPackedElement();\n+    for (int i = 0, e = op.getNumOperands(); i < e; i++) {\n+      unsigned bitWidth =\n+          getElementType(op.getOperand(i)).getIntOrFloatBitWidth();\n+      unsigned numElementPerReg = bitWidth < 32 ? 32 / bitWidth : 1;\n+      numElementPerReg = std::min(numElementPerReg, numPackedElements);\n+      for (int j = 0; j < numPackedElements; j += numElementPerReg) {\n+        if (numElementPerReg == 1) {\n+          packedOperands.push_back(operands[j][i]);\n+          continue;\n+        }\n+        Type t = vec_ty(\n+            getTypeConverter()->convertType(getElementType(op.getOperand(i))),\n+            numElementPerReg);\n+        Value packed = undef(t);\n+        for (int k = 0; k < numElementPerReg; k++) {\n+          packed = insert_element(packed, operands[j + k][i], i32_val(k));\n+        }\n+        packedOperands.push_back(packed);\n+      }\n+    }\n+    return packedOperands;\n+  }\n+\n+  SmallVector<Value> createDestOps(ElementwiseInlineAsmOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n+    int numPackedElements = op.getPackedElement();\n+    if (operands.size() % numPackedElements != 0)\n+      llvm::report_fatal_error(\"Inline asm op has more packed elements than \"\n+                               \"number of elements per thread.\");\n+    SmallVector<Value> packedOperands =\n+        packOperands(op, operands, rewriter, loc);\n+    Type dstType =\n+        getTypeConverter()->convertType(getElementType(op.getResult()));\n+    Type retType = dstType;\n+    if (numPackedElements > 1)\n+      retType = vec_ty(retType, numPackedElements);\n+    Value result = rewriter\n+                       .create<LLVM::InlineAsmOp>(\n+                           loc, retType,\n+                           packedOperands,      // operands\n+                           op.getAsmString(),   // asm_string\n+                           op.getConstraints(), // constraints\n+                           !op.getPure(),       // has_side_effects\n+                           false,               // is_align_stack\n+                           LLVM::AsmDialectAttr::get(\n+                               rewriter.getContext(),\n+                               LLVM::AsmDialect::AD_ATT), // asm_dialect\n+                           ArrayAttr()                    // operand_attrs\n+                           )\n+                       ->getResult(0);\n+    SmallVector<Value> results;\n+    if (numPackedElements > 1) {\n+      for (int i = 0; i < numPackedElements; i++)\n+        results.push_back(extract_element(result, i32_val(i)));\n+    } else {\n+      results = {result};\n+    }\n+    return results;\n+  }\n+};\n+\n struct FDivOpConversion\n     : ElementwiseOpConversionBase<mlir::arith::DivFOp, FDivOpConversion> {\n   using Base =\n@@ -1213,6 +1293,7 @@ void populateElementwiseOpToLLVMPatterns(\n   patterns\n       .add<ExternElementwiseOpConversion<triton::ImpureExternElementwiseOp>>(\n           typeConverter, benefit);\n+  patterns.add<ElementwiseInlineAsmOpConversion>(typeConverter, benefit);\n   // ExpOpConversionApprox will try using ex2.approx if the input type is\n   // FP32. For other input types, ExpOpConversionApprox will return failure and\n   // ElementwiseOpConversion<math::ExpOp, math::ExpOp> defined below will call"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 20, "deletions": 20, "changes": 40, "file_content_changes": "@@ -695,26 +695,26 @@ class TritonReturnOpPattern : public OpConversionPattern<ReturnOp> {\n void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n                             RewritePatternSet &patterns, unsigned numCTAs) {\n   MLIRContext *context = patterns.getContext();\n-  patterns\n-      .insert< // TODO: view should have custom pattern that views the layout\n-          TritonGenericPattern<triton::AdvanceOp>,\n-          TritonGenericPattern<triton::MakeTensorPtrOp>,\n-          TritonGenericPattern<triton::ViewOp>,\n-          TritonGenericPattern<triton::BitcastOp>,\n-          TritonGenericPattern<triton::FpToFpOp>,\n-          TritonGenericPattern<triton::IntToPtrOp>,\n-          TritonGenericPattern<triton::PtrToIntOp>,\n-          TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n-          TritonGenericPattern<triton::AddPtrOp>, TritonCatPattern,\n-          TritonReducePattern, TritonReduceReturnPattern, TritonScanPattern,\n-          TritonScanReturnPattern, TritonTransPattern, TritonExpandDimsPattern,\n-          TritonMakeRangePattern, TritonDotPattern, TritonLoadPattern,\n-          TritonStorePattern,\n-          TritonExternElementwisePattern<triton::PureExternElementwiseOp>,\n-          TritonExternElementwisePattern<triton::ImpureExternElementwiseOp>,\n-          TritonPrintPattern, TritonAssertPattern, TritonAtomicRMWPattern,\n-          TritonFuncOpPattern, TritonReturnOpPattern, TritonCallOpPattern>(\n-          typeConverter, context);\n+  patterns.insert< // TODO: view should have custom pattern that views the\n+                   // layout\n+      TritonGenericPattern<triton::AdvanceOp>,\n+      TritonGenericPattern<triton::MakeTensorPtrOp>,\n+      TritonGenericPattern<triton::ViewOp>,\n+      TritonGenericPattern<triton::BitcastOp>,\n+      TritonGenericPattern<triton::FpToFpOp>,\n+      TritonGenericPattern<triton::IntToPtrOp>,\n+      TritonGenericPattern<triton::PtrToIntOp>,\n+      TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n+      TritonGenericPattern<triton::AddPtrOp>, TritonCatPattern,\n+      TritonGenericPattern<triton::ElementwiseInlineAsmOp>, TritonReducePattern,\n+      TritonReduceReturnPattern, TritonScanPattern, TritonScanReturnPattern,\n+      TritonTransPattern, TritonExpandDimsPattern, TritonMakeRangePattern,\n+      TritonDotPattern, TritonLoadPattern, TritonStorePattern,\n+      TritonExternElementwisePattern<triton::PureExternElementwiseOp>,\n+      TritonExternElementwisePattern<triton::ImpureExternElementwiseOp>,\n+      TritonPrintPattern, TritonAssertPattern, TritonAtomicRMWPattern,\n+      TritonFuncOpPattern, TritonReturnOpPattern, TritonCallOpPattern>(\n+      typeConverter, context);\n }\n \n //"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -870,5 +870,17 @@ LogicalResult triton::ReturnOp::verify() {\n   return success();\n }\n \n+// -- ElementwiseInlineAsmOp --\n+void ElementwiseInlineAsmOp::getEffects(\n+    SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>>\n+        &effects) {\n+  if (getPure())\n+    return;\n+  effects.emplace_back(MemoryEffects::Write::get(),\n+                       SideEffects::DefaultResource::get());\n+  effects.emplace_back(MemoryEffects::Read::get(),\n+                       SideEffects::DefaultResource::get());\n+}\n+\n } // namespace triton\n } // namespace mlir"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -1524,6 +1524,14 @@ void init_triton_ir(py::module &&m) {\n              return self.create<mlir::arith::SelectOp>(condition, trueValue,\n                                                        falseValue);\n            })\n+      .def(\"create_inline_asm\",\n+           [](TritonOpBuilder &self, const std::string &inlineAsm,\n+              const std::string &constraints,\n+              const std::vector<mlir::Value> &values, mlir::Type &type,\n+              bool isPure, int pack) -> mlir::Value {\n+             return self.create<mlir::triton::ElementwiseInlineAsmOp>(\n+                 type, inlineAsm, constraints, isPure, pack, values);\n+           })\n       .def(\"create_print\",\n            [](TritonOpBuilder &self, const std::string &prefix,\n               const std::vector<mlir::Value> &values) -> void {"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 54, "deletions": 0, "changes": 54, "file_content_changes": "@@ -3034,6 +3034,60 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n     # compare\n     np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n \n+\n+# -----------------------\n+# test inline asm\n+# -----------------------\n+\n+@pytest.mark.parametrize(\"num_ctas\", num_ctas_list)\n+def test_inline_asm(num_ctas, device):\n+    check_cuda_only(device)\n+\n+    @triton.jit\n+    def kernel(X, Y, Z, n: tl.constexpr, BLOCK: tl.constexpr):\n+        x = tl.load(X + tl.arange(0, BLOCK))\n+        y = tl.load(Y + tl.arange(0, BLOCK))\n+        s = tl.full([BLOCK], n, tl.int32)\n+        z = tl.inline_asm_elementwise(\"shf.l.wrap.b32 $0, $1, $2, $3;\", \"=r,r, r, r\", [x, y, s], dtype=tl.int32, is_pure=True, pack=1)\n+        tl.store(Z + tl.arange(0, BLOCK), z)\n+\n+    shape = (128, )\n+    rs = RandomState(17)\n+    x = numpy_random(shape, dtype_str='uint32', rs=rs)\n+    y = numpy_random(shape, dtype_str='uint32', rs=rs)\n+    x_tri = to_triton(x, device=device)\n+    y_tri = to_triton(y, device=device)\n+    n = 17\n+    z_tri = to_triton(numpy_random(shape, dtype_str='uint32', rs=rs), device=device)\n+    kernel[(1,)](x_tri, y_tri, z_tri, n, BLOCK=shape[0], num_ctas=num_ctas)\n+    y_ref = (y << n) | (x >> (32 - n))\n+    # compare\n+    np.testing.assert_equal(y_ref, to_numpy(z_tri))\n+\n+\n+@pytest.mark.parametrize(\"num_ctas\", num_ctas_list)\n+def test_inline_asm_packed(num_ctas, device):\n+    check_cuda_only(device)\n+    \n+    @triton.jit\n+    def kernel(X, Y, BLOCK: tl.constexpr):\n+        x = tl.load(X + tl.arange(0, BLOCK))\n+        # shift 4x8bits values together.\n+        y = tl.inline_asm_elementwise(\"and.b32 $0, $1, 0x1F1F1F1F; \\\n+                                       shl.b32 $0, $0, 3;\",\n+                                      \"=r,r\", [x,], dtype=tl.int8, is_pure=True, pack=4)\n+        tl.store(Y + tl.arange(0, BLOCK), y)\n+\n+    shape = (512, )\n+    rs = RandomState(17)\n+    x = numpy_random(shape, dtype_str='uint8', rs=rs)\n+    x_tri = to_triton(x, device=device)\n+    y_tri = to_triton(numpy_random(shape, dtype_str='uint8', rs=rs), device=device)\n+    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], num_ctas=num_ctas)\n+    y_ref = x << 3\n+    # compare\n+    np.testing.assert_equal(y_ref, to_numpy(y_tri))\n+\n # -----------------------\n # test control flow\n # -----------------------"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -54,6 +54,7 @@\n     float8e4,\n     float8e5,\n     function_type,\n+    inline_asm_elementwise,\n     int1,\n     int16,\n     int32,\n@@ -154,6 +155,7 @@\n     \"float8e5\",\n     \"full\",\n     \"function_type\",\n+    \"inline_asm_elementwise\",\n     \"int1\",\n     \"int16\",\n     \"int32\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 40, "deletions": 0, "changes": 40, "file_content_changes": "@@ -1788,6 +1788,46 @@ def device_assert(cond, msg=\"\", _builder=None):\n     return semantic.device_assert(_to_tensor(cond, _builder), msg, file_name, func_name, lineno, _builder)\n \n \n+@builtin\n+def inline_asm_elementwise(asm: str, constraints: str, args: list, dtype, is_pure: bool, pack: int, _builder=None):\n+    '''\n+        Execute the inline assembly to a packed of elements of the tensor\n+        :param asm: assembly to be inlined, it has to match the target assembly format\n+        :param constraints: string representing the mapping of operands to register\n+        :param args: the arguments of the operation\n+        :param dtype: the element type of the returned variable\n+        :param is_pure: whether the operation is pure\n+        :param pack: the number of elements to be processed by one instance of inline assembly\n+        :param _builder: the builder\n+        :return: the return value of the function\n+    '''\n+    dispatch_args = args.copy()\n+    asm = _constexpr_to_value(asm)\n+    constraints = _constexpr_to_value(constraints)\n+    pack = _constexpr_to_value(pack)\n+    is_pure = _constexpr_to_value(is_pure)\n+    ret_shape = None\n+    arg_types = []\n+    for i in range(len(dispatch_args)):\n+        dispatch_args[i] = _to_tensor(dispatch_args[i], _builder)\n+        arg_types.append(dispatch_args[i].dtype)\n+    if len(arg_types) > 0:\n+        arg_types = tuple(arg_types)\n+        broadcast_arg = dispatch_args[0]\n+        # Get the broadcast shape over all the arguments\n+        for i, item in enumerate(dispatch_args):\n+            _, broadcast_arg = semantic.binary_op_type_checking_impl(\n+                item, broadcast_arg, _builder, arithmetic_check=False)\n+        # Change the shape of each argument based on the broadcast shape\n+        for i in range(len(dispatch_args)):\n+            dispatch_args[i], _ = semantic.binary_op_type_checking_impl(\n+                dispatch_args[i], broadcast_arg, _builder, arithmetic_check=False)\n+    ret_shape = broadcast_arg.shape\n+    res_ty = block_type(dtype, ret_shape).to_ir(_builder)\n+    call = _builder.create_inline_asm(asm, constraints, [t.handle for t in args], res_ty, is_pure, pack)\n+    return tensor(call, block_type(dtype, ret_shape))\n+\n+\n # -----------------------\n # Iterators\n # -----------------------"}, {"filename": "test/Conversion/triton_ops.mlir", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -201,5 +201,12 @@ tt.func @scan_op(%ptr: tensor<1x2x4x!tt.ptr<f32>>, %v : tensor<1x2x4xf32>) {\n   }) : (tensor<1x2x4xf32>) -> tensor<1x2x4xf32>\n   tt.store %ptr, %a : tensor<1x2x4xf32>\n   tt.return\n+}\n \n+// CHECK-LABEL: inline_asm\n+// CHECK: tt.elementwise_inline_asm \"shl.b32 $0, $0, 3;\"\n+tt.func @inline_asm(%0: tensor<512xi8>) {\n+  %1 = tt.elementwise_inline_asm \"shl.b32 $0, $0, 3;\"\n+    {constraints = \"=r,r\", packed_element = 4 : i32, pure = true} %0 : tensor<512xi8> -> tensor<512xi8>\n+  tt.return\n }"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 39, "deletions": 0, "changes": 39, "file_content_changes": "@@ -1396,3 +1396,42 @@ module attributes {\"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 2 :\n     tt.return\n   }\n }\n+\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+module attributes {\"triton_gpu.compute-capability\" = 80 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  // CHECK-LABEL: inline_asm\n+  tt.func public @inline_asm(%arg0: !tt.ptr<i8, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8, 1> {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n+    %0 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked>\n+    %1 = tt.splat %arg0 : (!tt.ptr<i8, 1>) -> tensor<512x!tt.ptr<i8, 1>, #blocked>\n+    %2 = tt.addptr %1, %0 : tensor<512x!tt.ptr<i8, 1>, #blocked>, tensor<512xi32, #blocked>\n+    %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512xi8, #blocked>\n+// CHECK: %{{.*}} = llvm.inline_asm asm_dialect = att \"shl.b32 $0, $0, 3;\", \"=r,r\" %{{.*}} : (vector<4xi8>) -> vector<4xi8>\n+    %4 = tt.elementwise_inline_asm \"shl.b32 $0, $0, 3;\" {constraints = \"=r,r\", packed_element = 4 : i32, pure = true} %3 : tensor<512xi8, #blocked> -> tensor<512xi8, #blocked>\n+    %5 = tt.splat %arg1 : (!tt.ptr<i8, 1>) -> tensor<512x!tt.ptr<i8, 1>, #blocked>\n+    %6 = tt.addptr %5, %0 : tensor<512x!tt.ptr<i8, 1>, #blocked>, tensor<512xi32, #blocked>\n+    tt.store %6, %4 {cache = 1 : i32, evict = 1 : i32} : tensor<512xi8, #blocked>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+module attributes {\"triton_gpu.compute-capability\" = 80 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  // CHECK-LABEL: inline_asm_pack_16bit\n+  tt.func public @inline_asm_pack_16bit(%arg0: !tt.ptr<i8, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8, 1> {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n+    %0 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked>\n+    %1 = tt.splat %arg0 : (!tt.ptr<i8, 1>) -> tensor<512x!tt.ptr<i8, 1>, #blocked>\n+    %2 = tt.addptr %1, %0 : tensor<512x!tt.ptr<i8, 1>, #blocked>, tensor<512xi32, #blocked>\n+    %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512xi8, #blocked>\n+// CHECK: %{{.*}} = llvm.inline_asm asm_dialect = att \"shl.b16 $0, $0, 3;\", \"=h,h\" %{{.*}} : (vector<2xi8>) -> vector<2xi8>\n+    %4 = tt.elementwise_inline_asm \"shl.b16 $0, $0, 3;\" {constraints = \"=h,h\", packed_element = 2 : i32, pure = true} %3 : tensor<512xi8, #blocked> -> tensor<512xi8, #blocked>\n+    %5 = tt.splat %arg1 : (!tt.ptr<i8, 1>) -> tensor<512x!tt.ptr<i8, 1>, #blocked>\n+    %6 = tt.addptr %5, %0 : tensor<512x!tt.ptr<i8, 1>, #blocked>, tensor<512xi32, #blocked>\n+    tt.store %6, %4 {cache = 1 : i32, evict = 1 : i32} : tensor<512xi8, #blocked>\n+    tt.return\n+  }\n+}"}]