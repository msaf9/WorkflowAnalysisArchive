[{"filename": "docs/meetups/08-22-2023.md", "status": "modified", "additions": 28, "deletions": 0, "changes": 28, "file_content_changes": "@@ -11,3 +11,31 @@\n 5. Intel working on the CPU backend for Triton.\n 6. AMD updates\n 7. Open discussion\n+\n+##### Minutes:\n+Recording link [here](https://drive.google.com/file/d/19Nnc0i7zUyn-ni2RSFHbPHHiPkYU96Mz/view)\n+\n+1. H100 updates:\n+   - Preliminary support is merged, disabled by default, can be enabled with env variables\n+   - Supports latest tensor cores, FP8s. Support for Flash Attention on the main branch coming soon.\n+   - Performance is very good on Matmuls, 80-90% of cublas on large Matmuls right now, will eventually reach parity with cublas. Above 600 teraflops on fp16 on xxm card, cublas is 670 on random input data. FP8 is twice that, around 1.2 petaflops.\n+   - Hopper support includes the full FP8 support for compute.\n+2. Triton release plan update\n+   - No specific dates for now, plan is to release before end of 2023.\n+   - Will move to 3.0 release due to minor backward compatibility breaking changes. For eg. Will move compiler options in the indexing operators as hardcoded operators in the kernel, will bump the major version.\n+   - Functionally the main goal will be to have 3rd party plugins for Intel and AMD gpus.\n+   - May synchronise with a PyTorch release so that PyTorch can benefit from the latest features, however continuous integration workflow is the default release cadence expected.\n+   - Will switch the default behavior to optimized mode for the release, needs more discussion with Nvidia.\n+   - Will expose flags for a user to enable kernel selection themselves.\n+   - Open question: Pytorch hasn\u2019t rebased to latest triton, it is close to PyTorch code freeze \u2013 will PyTorch still sync with Triton 2.0? Will we have another release to support triton 2.0?\n+   - Community can start with the latest stable branch and rebase 3rd party plugin on top of that. OAI has no resources to commit to, but community can contribute.\n+3. Linalg updates\n+   - Discussion on Github for Linalg as a middle layer between the language and target hardware. Includes support for block pointers and modulo operators.\n+   - Please join the conversation [here](https://github.com/openai/triton/discussions/1842)\n+   - Branch pushed is behind the tip, will work on getting it caught up on the tip.\n+4. Intel GPU Backend status update.\n+   - Please refer to slides [here](https://github.com/openai/triton/blob/main/docs/meetups/Intel%20XPU%20Backend%20for%20Triton%20-%20Update%20-%200823.pptx)\n+5. Intel working on the CPU backend for Triton.\n+   - Please refer to slides [here](https://github.com/openai/triton/blob/main/docs/meetups/Intel%20XPU%20Backend%20for%20Triton%20-%20Update%20-%200823.pptx)\n+6. AMD updates\n+   - Please refer to slides [here](https://github.com/openai/triton/blob/main/docs/meetups/Triton_AMD_update_0823.pdf)."}]