[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -27,7 +27,7 @@ jobs:\n         run: |\n           if [ x\"${{ github.repository }}\" == x\"openai/triton\" ]; then\n             echo '::set-output name=matrix-required::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"H100\"]]'\n-            echo '::set-output name=matrix-optional::[]'\n+            echo '::set-output name=matrix-optional::[[\"self-hosted\", \"gfx908\"], [\"self-hosted\", \"arc770\"]]'\n           else\n             echo '::set-output name=matrix-required::[\"ubuntu-latest\"]'\n             echo '::set-output name=matrix-optional::[\"ubuntu-latest\"]'\n@@ -209,10 +209,12 @@ jobs:\n       - name: Install Triton on ROCM\n         if: ${{ env.BACKEND == 'ROCM'}}\n         run: |\n+          git submodule update --init --recursive\n           cd python\n           python3 -m pip install --upgrade pip\n           python3 -m pip install cmake==3.24\n           python3 -m pip install torch==1.13.1 --index-url https://download.pytorch.org/whl/rocm5.2\n+          export TRITON_CODEGEN_AMD_HIP_BACKEND=1\n           python3 -m pip install --no-build-isolation -vvv '.[tests]'\n \n       - name: Install Triton on XPU\n@@ -234,7 +236,7 @@ jobs:\n         if: ${{ env.BACKEND == 'ROCM'}}\n         run: |\n           cd python/test/unit/language\n-          python3 -m pytest --capture=tee-sys -rfs --verbose \"test_core.py::test_empty_kernel\"\n+          python3 -m pytest --capture=tee-sys -rfs --verbose \"test_core.py\"\n \n       - name: Run python tests on XPU\n         if: ${{ env.BACKEND == 'XPU'}}"}, {"filename": ".gitmodules", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -1,3 +1,7 @@\n [submodule \"third_party/intel_xpu_backend\"]\n \tpath = third_party/intel_xpu_backend\n \turl = http://github.com/intel/intel-xpu-backend-for-triton\n+[submodule \"third_party/amd_hip_backend\"]\n+\tpath = third_party/amd_hip_backend\n+\turl = https://github.com/ROCmSoftwarePlatform/triton\n+\tbranch = third_party_backend_2"}, {"filename": "CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -212,7 +212,6 @@ if(TRITON_BUILD_PYTHON_MODULE)\n     TritonNvidiaGPUTransforms\n     TritonLLVMIR\n     TritonPTX\n-    TritonHSACO\n     ${dialect_libs}\n     ${conversion_libs}\n "}, {"filename": "bin/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -53,7 +53,6 @@ llvm_update_compile_flags(triton-translate)\n          TritonNvidiaGPUTransforms\n          TritonLLVMIR\n          TritonPTX\n-         TritonHSACO\n          ${dialect_libs}\n          ${conversion_libs}\n          # tests"}, {"filename": "bin/triton-translate.cpp", "status": "modified", "additions": 2, "deletions": 8, "changes": 10, "file_content_changes": "@@ -15,7 +15,6 @@\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n-#include \"triton/Target/HSACO/HSACOTranslation.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n #include \"triton/Target/PTX/PTXTranslation.h\"\n #include \"llvm/IR/LLVMContext.h\"\n@@ -131,16 +130,11 @@ LogicalResult tritonTranslateMain(int argc, char **argv,\n     llvm::errs() << \"Translate to LLVM IR failed\";\n   }\n \n-  if (targetKind == \"llvmir\")\n+  if (targetKind == \"llvmir\") {\n     llvm::outs() << *llvmir << '\\n';\n-  else if (targetKind == \"ptx\")\n+  } else if (targetKind == \"ptx\") {\n     llvm::outs() << ::triton::translateLLVMIRToPTX(*llvmir, SMArch.getValue(),\n                                                    ptxVersion.getValue());\n-  else if (targetKind == \"hsaco\") {\n-    auto [module, hsaco] = ::triton::translateLLVMIRToHSACO(\n-        *llvmir, GCNArch.getValue(), GCNTriple.getValue(),\n-        GCNFeatures.getValue());\n-    llvm::outs() << hsaco;\n   } else {\n     llvm::errs() << \"Error: Unknown target specified: \" << targetKind << \"\\n\";\n     return failure();"}, {"filename": "include/triton/Target/AMDGCN/AMDGCNTranslation.h", "status": "removed", "additions": 0, "deletions": 19, "changes": 19, "file_content_changes": "@@ -1,19 +0,0 @@\n-#ifndef TRITON_TARGET_AMDGCNTRANSLATION_H\n-#define TRITON_TARGET_AMDGCNTRANSLATION_H\n-\n-#include <string>\n-#include <tuple>\n-\n-namespace llvm {\n-class Module;\n-} // namespace llvm\n-\n-namespace triton {\n-\n-// Translate LLVM IR to AMDGCN code.\n-std::tuple<std::string, std::string>\n-translateLLVMIRToAMDGCN(llvm::Module &module, std::string cc);\n-\n-} // namespace triton\n-\n-#endif"}, {"filename": "include/triton/Target/HSACO/HSACOTranslation.h", "status": "removed", "additions": 0, "deletions": 21, "changes": 21, "file_content_changes": "@@ -1,21 +0,0 @@\n-#ifndef TRITON_TARGET_HSACOTRANSLATION_H\n-#define TRITON_TARGET_HSACOTRANSLATION_H\n-\n-#include <memory>\n-#include <string>\n-#include <tuple>\n-\n-namespace llvm {\n-class Module;\n-} // namespace llvm\n-\n-namespace triton {\n-\n-// Translate TritonGPU IR to HSACO code.\n-std::tuple<std::string, std::string>\n-translateLLVMIRToHSACO(llvm::Module &module, std::string gfx_arch,\n-                       std::string gfx_triple, std::string gfx_features);\n-\n-} // namespace triton\n-\n-#endif"}, {"filename": "include/triton/Tools/Sys/GetEnv.hpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -46,7 +46,7 @@ inline std::string getenv(const char *name) {\n \n inline bool getBoolEnv(const std::string &env) {\n   std::string msg = \"Environment variable \" + env + \" is not recognized\";\n-  assert(triton::ENV_VARS.find(env.c_str()) != triton::ENV_VARS.end() &&\n+  assert(::triton::ENV_VARS.find(env.c_str()) != ::triton::ENV_VARS.end() &&\n          msg.c_str());\n   const char *s = std::getenv(env.c_str());\n   std::string str(s ? s : \"\");"}, {"filename": "lib/Target/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,3 +1,2 @@\n add_subdirectory(LLVMIR)\n add_subdirectory(PTX)\n-add_subdirectory(HSACO)"}, {"filename": "lib/Target/HSACO/CMakeLists.txt", "status": "removed", "additions": 0, "deletions": 9, "changes": 9, "file_content_changes": "@@ -1,9 +0,0 @@\n-add_mlir_translation_library(TritonHSACO\n-        HSACOTranslation.cpp\n-\n-        LINK_COMPONENTS\n-        Core\n-\n-        LINK_LIBS PUBLIC\n-        TritonLLVMIR\n-        )"}, {"filename": "lib/Target/HSACO/HSACOTranslation.cpp", "status": "removed", "additions": 0, "deletions": 182, "changes": 182, "file_content_changes": "@@ -1,182 +0,0 @@\n-#include \"triton/Target/HSACO/HSACOTranslation.h\"\n-#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n-#include \"mlir/ExecutionEngine/ExecutionEngine.h\"\n-#include \"mlir/ExecutionEngine/OptUtils.h\"\n-#include \"mlir/IR/BuiltinOps.h\"\n-#include \"mlir/IR/Dialect.h\"\n-#include \"mlir/Pass/Pass.h\"\n-#include \"mlir/Pass/PassManager.h\"\n-#include \"mlir/Support/LogicalResult.h\"\n-#include \"mlir/Target/LLVMIR/Dialect/LLVMIR/LLVMToLLVMIRTranslation.h\"\n-#include \"mlir/Target/LLVMIR/Export.h\"\n-#include \"mlir/Target/LLVMIR/LLVMTranslationInterface.h\"\n-#include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n-#include \"triton/Tools/Sys/GetEnv.hpp\"\n-\n-#include \"llvm/ExecutionEngine/ExecutionEngine.h\"\n-#include \"llvm/ExecutionEngine/SectionMemoryManager.h\"\n-#include \"llvm/IR/IRBuilder.h\"\n-#include \"llvm/IR/IRPrintingPasses.h\"\n-#include \"llvm/IR/LegacyPassManager.h\"\n-#include \"llvm/IR/Module.h\"\n-#include \"llvm/IR/Verifier.h\"\n-#include \"llvm/MC/TargetRegistry.h\"\n-#include \"llvm/Support/CodeGen.h\"\n-#include \"llvm/Support/CommandLine.h\"\n-#include \"llvm/Support/SourceMgr.h\"\n-#include \"llvm/Support/TargetSelect.h\"\n-#include \"llvm/Support/raw_ostream.h\"\n-#include \"llvm/Target/TargetMachine.h\"\n-#include \"llvm/Target/TargetOptions.h\"\n-#include \"llvm/Transforms/Scalar.h\"\n-#include \"llvm/Transforms/Utils/Cloning.h\"\n-#include <filesystem>\n-#include <iostream>\n-#include <memory>\n-#include <random>\n-\n-namespace {\n-\n-void init_llvm() {\n-  LLVMInitializeAMDGPUTarget();\n-  LLVMInitializeAMDGPUTargetInfo();\n-  LLVMInitializeAMDGPUTargetMC();\n-  LLVMInitializeAMDGPUAsmParser();\n-  LLVMInitializeAMDGPUAsmPrinter();\n-}\n-\n-std::unique_ptr<llvm::TargetMachine>\n-initialize_module(llvm::Module *module, const std::string &triple,\n-                  const std::string &proc, const std::string &features) {\n-  // verify and store llvm\n-  llvm::legacy::PassManager pm;\n-  pm.add(llvm::createVerifierPass());\n-  pm.run(*module);\n-\n-  module->setTargetTriple(triple);\n-\n-  std::string error;\n-  auto target =\n-      llvm::TargetRegistry::lookupTarget(module->getTargetTriple(), error);\n-  llvm::TargetOptions opt;\n-  opt.AllowFPOpFusion = llvm::FPOpFusion::Fast;\n-  opt.UnsafeFPMath = false;\n-  opt.NoInfsFPMath = false;\n-  opt.NoNaNsFPMath = true;\n-  llvm::TargetMachine *machine = target->createTargetMachine(\n-      module->getTargetTriple(), proc, features, opt, llvm::Reloc::PIC_,\n-      std::nullopt, llvm::CodeGenOpt::Aggressive);\n-\n-  module->setDataLayout(machine->createDataLayout());\n-\n-  for (llvm::Function &f : module->functions())\n-    f.addFnAttr(llvm::Attribute::AlwaysInline);\n-\n-  return std::unique_ptr<llvm::TargetMachine>(machine);\n-}\n-\n-std::string generate_amdgcn_assembly(llvm::Module *module,\n-                                     const std::string &triple,\n-                                     const std::string &proc,\n-                                     const std::string &features) {\n-  auto machine = initialize_module(module, triple, proc, features);\n-  llvm::SmallVector<char, 0> buffer;\n-  llvm::legacy::PassManager pass;\n-  llvm::raw_svector_ostream stream(buffer);\n-\n-  // emit\n-  machine->addPassesToEmitFile(pass, stream, nullptr,\n-                               llvm::CodeGenFileType::CGFT_AssemblyFile);\n-  pass.run(*module);\n-\n-  std::string amdgcn(buffer.begin(), buffer.end());\n-  if (::triton::tools::getBoolEnv(\"AMDGCN_ENABLE_DUMP\")) {\n-    std::cout << \"// -----// AMDGCN Dump //----- //\\n\" << amdgcn << std::endl;\n-  }\n-\n-  return amdgcn;\n-}\n-\n-std::string generate_hsaco(llvm::Module *module, const std::string &triple,\n-                           const std::string &proc,\n-                           const std::string &features) {\n-  auto machine = initialize_module(module, triple, proc, features);\n-\n-  // create unique dir for kernel's binary and hsaco\n-  std::error_code ec;\n-  std::string kernel_name_base = \"amd_triton_kernel\";\n-  std::filesystem::path tmp = std::filesystem::temp_directory_path();\n-  std::filesystem::path kernel_dir_base(kernel_name_base);\n-  llvm::SmallString<256> unique_dir;\n-  ec = llvm::sys::fs::createUniqueDirectory((tmp / kernel_dir_base).string(),\n-                                            unique_dir);\n-  if (ec) {\n-    std::cerr << \"Directory for \" << kernel_name_base\n-              << \" was not created. error code: \" << ec << std::endl;\n-  }\n-  std::filesystem::path kernel_dir(unique_dir.data());\n-  std::string kernel_name = kernel_dir.stem();\n-\n-  // Save GCN ISA binary.\n-  std::filesystem::path isa_binary(kernel_name + \".o\");\n-  std::string isabin_path = (kernel_dir / isa_binary).string();\n-  std::unique_ptr<llvm::raw_fd_ostream> isabin_fs(\n-      new llvm::raw_fd_ostream(isabin_path, ec, llvm::sys::fs::OF_Text));\n-  if (ec) {\n-    std::cerr << isabin_path << \" was not created. error code: \" << ec\n-              << std::endl;\n-  }\n-\n-  // emit\n-  llvm::legacy::PassManager pass;\n-  machine->addPassesToEmitFile(pass, *isabin_fs, nullptr,\n-                               llvm::CGFT_ObjectFile);\n-  pass.run(*module);\n-\n-  // generate HASCO file\n-  std::filesystem::path hsaco(kernel_name + \".hsaco\");\n-  std::string hsaco_path = (kernel_dir / hsaco).string();\n-  std::string error_message;\n-  std::string lld_path = \"/opt/rocm/llvm/bin/ld.lld\";\n-  int lld_result = llvm::sys::ExecuteAndWait(\n-      lld_path,\n-      {lld_path, \"-flavor\", \"gnu\", \"-shared\", \"-o\", hsaco_path, isabin_path},\n-      std::nullopt, {}, 0, 0, &error_message);\n-  if (lld_result) {\n-    std::cout << \"ld.lld execute fail: \" << std::endl;\n-    std::cout << error_message << std::endl;\n-    std::cout << lld_result << std::endl;\n-  }\n-\n-  return hsaco_path;\n-}\n-\n-std::tuple<std::string, std::string>\n-llir_to_amdgcn_and_hsaco(llvm::Module *module, std::string gfx_arch,\n-                         std::string gfx_triple, std::string gfx_features) {\n-\n-  init_llvm();\n-\n-  // verify and store llvm\n-  auto module_obj = llvm::CloneModule(*module);\n-  auto amdgcn =\n-      generate_amdgcn_assembly(module, gfx_triple, gfx_arch, gfx_features);\n-  auto hsaco_path =\n-      generate_hsaco(module_obj.get(), gfx_triple, gfx_arch, gfx_features);\n-\n-  return std::make_tuple(amdgcn, hsaco_path);\n-}\n-\n-} // namespace\n-\n-namespace triton {\n-\n-std::tuple<std::string, std::string>\n-translateLLVMIRToHSACO(llvm::Module &module, std::string gfx_arch,\n-                       std::string gfx_triple, std::string gfx_features) {\n-  auto hsacoCode =\n-      llir_to_amdgcn_and_hsaco(&module, gfx_arch, gfx_triple, gfx_features);\n-  return hsacoCode;\n-}\n-\n-} // namespace triton"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 1, "deletions": 19, "changes": 20, "file_content_changes": "@@ -33,7 +33,6 @@\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n-#include \"triton/Target/HSACO/HSACOTranslation.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n #include \"triton/Target/PTX/PTXTranslation.h\"\n #include \"triton/Target/PTX/TmaMetadata.h\"\n@@ -257,6 +256,7 @@ void init_triton_ir(py::module &&m) {\n         // we load LLVM because the frontend uses LLVM.undef for\n         // some placeholders\n         self.getOrLoadDialect<mlir::LLVM::LLVMDialect>();\n+        self.getOrLoadDialect<mlir::tensor::TensorDialect>();\n       });\n   // .def(py::init([](){\n   //   mlir::MLIRContext context;\n@@ -1958,24 +1958,6 @@ void init_triton_translation(py::module &m) {\n            const std::vector<std::string> &paths) {\n           ::mlir::triton::addExternalLibs(op, names, paths);\n         });\n-\n-  m.def(\n-      \"translate_llvmir_to_hsaco\",\n-      [](const std::string llvmIR, std::string gfx_arch, std::string gfx_triple,\n-         std::string gfx_features) -> std::tuple<std::string, std::string> {\n-        // create LLVM module from C++\n-        llvm::LLVMContext context;\n-        std::unique_ptr<llvm::MemoryBuffer> buffer =\n-            llvm::MemoryBuffer::getMemBuffer(llvmIR.c_str());\n-        llvm::SMDiagnostic error;\n-        std::unique_ptr<llvm::Module> module =\n-            llvm::parseIR(buffer->getMemBufferRef(), error, context);\n-        // translate module to HSACO\n-        auto hsacoCode = triton::translateLLVMIRToHSACO(\n-            *module, gfx_arch, gfx_triple, gfx_features);\n-        return hsacoCode;\n-      },\n-      ret::take_ownership);\n }\n \n void init_triton(py::module &m) {"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 178, "deletions": 54, "changes": 232, "file_content_changes": "@@ -12,6 +12,7 @@\n import triton\n import triton._C.libtriton.triton as _triton\n import triton.language as tl\n+from triton.common.build import is_hip\n from triton.runtime.jit import JITFunction, TensorWrapper, reinterpret\n \n int_dtypes = ['int8', 'int16', 'int32', 'int64']\n@@ -25,6 +26,13 @@\n # num_ctas_list = [1, 4] if torch.cuda.get_device_capability()[0] == 9 else [1]\n num_ctas_list = [1]\n \n+if is_hip():\n+    GPU_DIALECT = \"triton_gpu_rocm\"\n+    THREADS_PER_WARP = 64\n+else:\n+    GPU_DIALECT = \"triton_gpu\"\n+    THREADS_PER_WARP = 32\n+\n \n def _bitwidth(dtype: str) -> int:\n     # ex.: \"int64\" -> 64\n@@ -137,7 +145,7 @@ def __init__(self, version, warps_per_cta, ctas_per_cga, cta_split_num, cta_orde\n         self.instr_shape = str(instr_shape)\n \n     def __str__(self):\n-        return f\"#triton_gpu.mma<{{versionMajor={self.version[0]}, versionMinor={self.version[1]}, warpsPerCTA={self.warps_per_cta}, CTAsPerCGA={self.ctas_per_cga}, CTASplitNum={self.cta_split_num}, CTAOrder={self.cta_order}, instrShape={self.instr_shape}}}>\"\n+        return f\"#{GPU_DIALECT}.mma<{{versionMajor={self.version[0]}, versionMinor={self.version[1]}, warpsPerCTA={self.warps_per_cta}, CTAsPerCGA={self.ctas_per_cga}, CTASplitNum={self.cta_split_num}, CTAOrder={self.cta_order}, instrShape={self.instr_shape}}}>\"\n \n \n class BlockedLayout:\n@@ -151,7 +159,7 @@ def __init__(self, size_per_thread, threads_per_warp, warps_per_cta, order, ctas\n         self.cta_order = str(cta_order)\n \n     def __str__(self):\n-        return f\"#triton_gpu.blocked<{{sizePerThread={self.sz_per_thread}, threadsPerWarp={self.threads_per_warp}, warpsPerCTA={self.warps_per_cta}, order={self.order}, CTAsPerCGA={self.ctas_per_cga}, CTASplitNum={self.cta_split_num}, CTAOrder={self.cta_order}}}>\"\n+        return f\"#{GPU_DIALECT}.blocked<{{sizePerThread={self.sz_per_thread}, threadsPerWarp={self.threads_per_warp}, warpsPerCTA={self.warps_per_cta}, order={self.order}, CTAsPerCGA={self.ctas_per_cga}, CTASplitNum={self.cta_split_num}, CTAOrder={self.cta_order}}}>\"\n \n \n class SharedLayout:\n@@ -165,7 +173,7 @@ def __init__(self, vec, per_phase, max_phase, order, ctas_per_cga, cta_split_num\n         self.cta_order = str(cta_order)\n \n     def __str__(self):\n-        return f\"#triton_gpu.shared<{{vec={self.vec}, perPhase={self.per_phase}, maxPhase={self.max_phase}, order={self.order}, CTAsPerCGA={self.ctas_per_cga}, CTASplitNum={self.cta_split_num}, CTAOrder={self.cta_order}}}>\"\n+        return f\"#{GPU_DIALECT}.shared<{{vec={self.vec}, perPhase={self.per_phase}, maxPhase={self.max_phase}, order={self.order}, CTAsPerCGA={self.ctas_per_cga}, CTASplitNum={self.cta_split_num}, CTAOrder={self.cta_order}}}>\"\n \n \n @pytest.mark.parametrize(\"dtype_x\", list(dtypes) + [\"bfloat16\"])\n@@ -851,6 +859,8 @@ def test_abs(dtype_x, device):\n \n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4nv, tl.float8e5])\n def test_abs_fp8(in_dtype, device):\n+    if is_hip():\n+        pytest.skip('test_abs_fp8 not supported on HIP.')\n \n     @triton.jit\n     def abs_kernel(X, Z, SIZE: tl.constexpr):\n@@ -1056,6 +1066,9 @@ def noinline_multi_values_fn(x, y, Z):\n \n @pytest.mark.parametrize(\"mode\", [\"simple\", \"call_graph\", \"shared\", \"dynamic\", \"multi_values\"])\n def test_noinline(mode, device):\n+    if is_hip() and mode == \"shared\":\n+        pytest.skip('test_noinline[\"shared\"] not supported on HIP.')\n+\n     @triton.jit\n     def kernel(X, Y, Z):\n         x = tl.load(X)\n@@ -1141,6 +1154,9 @@ def kernel(X, Z):\n     else:\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n     sem_str = \"acq_rel\" if sem is None else sem\n+    if is_hip():\n+        return\n+\n     assert f\"atom.global.gpu.{sem_str}\" in h.asm[\"ptx\"]\n \n \n@@ -1232,6 +1248,8 @@ def serialized_add(data, Lock, SEM: tl.constexpr):\n     h = serialized_add[(64,)](data, Lock, SEM=sem, num_ctas=num_ctas)\n     sem_str = \"acq_rel\" if sem is None else sem\n     np.testing.assert_allclose(to_numpy(data), to_numpy(ref))\n+    if is_hip():\n+        return\n     assert f\"atom.global.{sem_str}\" in h.asm[\"ptx\"]\n \n \n@@ -1261,6 +1279,9 @@ def test_cast(dtype_x, dtype_z, bitcast, num_ctas, device):\n     check_type_supported(dtype_x, device)\n     check_type_supported(dtype_z, device)\n \n+    if is_hip() and (dtype_z == \"bfloat16\"):\n+        pytest.skip(f'test_cast{(dtype_x, dtype_z)} cast to bfloat16 not supported on HIP.')\n+\n     size = 1024\n     # This is tricky because numpy doesn't have bfloat, and torch doesn't have uints.\n     if dtype_x.startswith('bfloat'):\n@@ -1358,7 +1379,10 @@ def kernel(in_out_ptr):\n \n     for _ in range(1000):\n         x = torch.ones((65536,), device=device, dtype=torch.float32)\n-        kernel[(65536,)](x, num_warps=32)\n+        if is_hip():\n+            kernel[(65536,)](x, num_warps=16)  # threads per Warp for ROCM is 64\n+        else:\n+            kernel[(65536,)](x, num_warps=32)\n         assert torch.all(x == 2)\n \n \n@@ -1452,6 +1476,8 @@ def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n     \"\"\"\n     check_type_supported(in_dtype, device)\n     check_type_supported(out_dtype, device)\n+    if is_hip():\n+        pytest.skip('test_abs_fp8 not supported on HIP.')\n \n     @triton.jit\n     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n@@ -1507,6 +1533,9 @@ def get_reduced_dtype(dtype_str, op):\n def test_reduce1d(op, dtype_str, shape, num_ctas, device):\n     check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n \n+    if is_hip():\n+        pytest.skip(f\"test_reduce1d not supported on HIP\")\n+\n     # triton kernel\n     @triton.jit\n     def kernel(X, Z, BLOCK: tl.constexpr):\n@@ -1597,7 +1626,10 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n def test_reduce2d(op, dtype_str, shape, axis, num_ctas, device):\n     check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n \n+    if is_hip():\n+        pytest.skip(f\"test_reduce2d not supported on HIP\")\n     # triton kernel\n+\n     @triton.jit\n     def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexpr):\n         range_m = tl.arange(0, BLOCK_M)\n@@ -1667,6 +1699,8 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n \n @pytest.mark.parametrize(\"op, dtype_str, shape, axis, num_warps\", scan_configs)\n def test_scan2d(op, dtype_str, shape, axis, num_warps, device):\n+    if is_hip():\n+        pytest.skip(\"test_scan2d is not supported in HIP\")\n     check_type_supported(dtype_str, device)\n \n     # triton kernel\n@@ -1720,6 +1754,9 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n @pytest.mark.parametrize(\"src_layout\", scan_layouts)\n @pytest.mark.parametrize(\"axis\", [0, 1])\n def test_scan_layouts(M, N, src_layout, axis, device):\n+    if is_hip():\n+        pytest.skip(\"test_scan_layouts is not supported in HIP\")\n+\n     ir = f\"\"\"\n     #blocked = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32}} {{\n@@ -1783,6 +1820,9 @@ def test_scan_layouts(M, N, src_layout, axis, device):\n @pytest.mark.parametrize(\"src_layout\", layouts)\n @pytest.mark.parametrize(\"axis\", [0, 1])\n def test_reduce_layouts(M, N, src_layout, axis, device):\n+    if is_hip():\n+        pytest.skip(\"test_reduce_layouts is not supported in HIP\")\n+\n     rdims_2d = f\"1x{N}\" if axis == 0 else f\"{M}x1\"\n     rdims_1d = f\"{N}\" if axis == 0 else f\"{M}\"\n     store_range = \"%7\" if axis == 0 else \"%1\"\n@@ -1792,28 +1832,28 @@ def test_reduce_layouts(M, N, src_layout, axis, device):\n     #src = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32}} {{\n     tt.func public @kernel_0d1d2c3d4c(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: i32 {{tt.divisibility = 16 : i32}}, %arg2: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n-        %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>\n-        %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>) -> tensor<{M}x1xi32, #blocked>\n+        %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #blocked}}>>\n+        %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #blocked}}>>) -> tensor<{M}x1xi32, #blocked>\n         %2 = tt.splat %arg1 : (i32) -> tensor<{M}x1xi32, #blocked>\n         %3 = arith.muli %1, %2 : tensor<{M}x1xi32, #blocked>\n         %4 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x1x!tt.ptr<i32>, #blocked>\n         %5 = tt.addptr %4, %3 : tensor<{M}x1x!tt.ptr<i32>, #blocked>, tensor<{M}x1xi32, #blocked>\n-        %6 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>\n-        %7 = tt.expand_dims %6 {{axis = 0 : i32}} : (tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>) -> tensor<1x{N}xi32, #blocked>\n+        %6 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #{GPU_DIALECT}.slice<{{dim = 0, parent = #blocked}}>>\n+        %7 = tt.expand_dims %6 {{axis = 0 : i32}} : (tensor<{N}xi32, #{GPU_DIALECT}.slice<{{dim = 0, parent = #blocked}}>>) -> tensor<1x{N}xi32, #blocked>\n         %8 = tt.broadcast %5 : (tensor<{M}x1x!tt.ptr<i32>, #blocked>) -> tensor<{M}x{N}x!tt.ptr<i32>, #blocked>\n         %9 = tt.broadcast %7 : (tensor<1x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #blocked>\n         %10 = tt.addptr %8, %9 : tensor<{M}x{N}x!tt.ptr<i32>, #blocked>, tensor<{M}x{N}xi32, #blocked>\n         %11 = tt.splat %arg2 : (!tt.ptr<i32>) -> tensor<{rdims_2d}x!tt.ptr<i32>, #blocked>\n         %12 = tt.addptr %11, {store_range} : tensor<{rdims_2d}x!tt.ptr<i32>, #blocked>, tensor<{rdims_2d}xi32, #blocked>\n         %13 = tt.load %10 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xi32, #blocked>\n-        %14 = triton_gpu.convert_layout %13 : (tensor<{M}x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #src>\n+        %14 = {GPU_DIALECT}.convert_layout %13 : (tensor<{M}x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #src>\n         %15 = \"tt.reduce\"(%14) ({{\n         ^bb0(%arg3: i32, %arg4: i32):\n           %17 = arith.addi %arg3, %arg4 : i32\n           tt.reduce.return %17 : i32\n-        }}) {{axis = {axis} : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>\n-        %18 = triton_gpu.convert_layout %15 : (tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>) -> tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>\n-        %19 = tt.expand_dims %18 {{axis = {axis} : i32}} : (tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>) -> tensor<{rdims_2d}xi32, #blocked>\n+        }}) {{axis = {axis} : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{rdims_1d}xi32, #{GPU_DIALECT}.slice<{{dim = {axis}, parent = #src}}>>\n+        %18 = {GPU_DIALECT}.convert_layout %15 : (tensor<{rdims_1d}xi32, #{GPU_DIALECT}.slice<{{dim = {axis}, parent = #src}}>>) -> tensor<{rdims_1d}xi32, #{GPU_DIALECT}.slice<{{dim = {axis}, parent = #blocked}}>>\n+        %19 = tt.expand_dims %18 {{axis = {axis} : i32}} : (tensor<{rdims_1d}xi32, #{GPU_DIALECT}.slice<{{dim = {axis}, parent = #blocked}}>>) -> tensor<{rdims_2d}xi32, #blocked>\n         tt.store %12, %19 {{cache = 1 : i32, evict = 1 : i32}} : tensor<{rdims_2d}xi32, #blocked>\n         tt.return\n     }}\n@@ -1854,17 +1894,20 @@ def test_reduce_layouts(M, N, src_layout, axis, device):\n @pytest.mark.parametrize(\"M\", [32, 64, 128, 256])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n def test_store_op(M, src_layout, device):\n+    if is_hip():\n+        pytest.skip(\"test_convert1d is not supported yet in HIP\")\n+\n     ir = f\"\"\"\n     #src = {src_layout}\n-    module attributes {{\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32}} {{\n+    module attributes {{\"{GPU_DIALECT}.num-warps\" = 4 : i32, \"{GPU_DIALECT}.num-ctas\" = 1 : i32, \"{GPU_DIALECT}.threads-per-warp\" = {THREADS_PER_WARP} : i32}} {{\n         tt.func public @kernel(%arg0: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}) {{\n-            %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n-            %1 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<{M}x!tt.ptr<f32>, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n-            %2 = tt.addptr %1, %0 : tensor<{M}x!tt.ptr<f32>, #triton_gpu.slice<{{dim = 1, parent = #src}}>>, tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n-            %3 = tt.load %2 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}xf32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n-            %4 = tt.expand_dims %3 {{axis = 1 : i32}} : (tensor<{M}xf32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xf32, #src>\n-            %5 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n-            %6 = tt.expand_dims %5 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xi32, #src>\n+            %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>\n+            %1 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<{M}x!tt.ptr<f32>, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>\n+            %2 = tt.addptr %1, %0 : tensor<{M}x!tt.ptr<f32>, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>, tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>\n+            %3 = tt.load %2 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}xf32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>\n+            %4 = tt.expand_dims %3 {{axis = 1 : i32}} : (tensor<{M}xf32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xf32, #src>\n+            %5 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>\n+            %6 = tt.expand_dims %5 {{axis = 1 : i32}} : (tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xi32, #src>\n             %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<{M}x1x!tt.ptr<f32>, #src>\n             %8 = tt.addptr %7, %6 : tensor<{M}x1x!tt.ptr<f32>, #src>, tensor<{M}x1xi32, #src>\n             tt.store %8, %4 : tensor<{M}x1xf32, #src>\n@@ -1903,20 +1946,23 @@ def test_store_op(M, src_layout, device):\n @pytest.mark.parametrize(\"src_dim\", [0, 1])\n @pytest.mark.parametrize(\"dst_dim\", [0, 1])\n def test_convert1d(M, src_layout, dst_layout, src_dim, dst_dim, device):\n+    if is_hip():\n+        pytest.skip(\"test_convert1d is not supported in HIP\")\n+\n     ir = f\"\"\"\n     #dst = {dst_layout}\n     #src = {src_layout}\n-    module attributes {{\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32}} {{\n+    module attributes {{\"{GPU_DIALECT}.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.threads-per-warp\" = {THREADS_PER_WARP} : i32}} {{\n         tt.func public @kernel(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n-            %0 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x!tt.ptr<i32>, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>\n-            %1 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>\n-            %2 = tt.addptr %0, %1 : tensor<{M}x!tt.ptr<i32>, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>, tensor<{M}xi32, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>\n-            %3 = tt.load %2 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>\n-            %4 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<{M}x!tt.ptr<i32>, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n-            %5 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n-            %6 = tt.addptr %4, %5 : tensor<{M}x!tt.ptr<i32>, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>, tensor<{M}xi32, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n-            %7 = triton_gpu.convert_layout %3 : (tensor<{M}xi32, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>) -> tensor<{M}xi32, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n-            tt.store %6, %7 : tensor<{M}xi32, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            %0 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x!tt.ptr<i32>, #{GPU_DIALECT}.slice<{{dim = {src_dim}, parent = #src}}>>\n+            %1 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {src_dim}, parent = #src}}>>\n+            %2 = tt.addptr %0, %1 : tensor<{M}x!tt.ptr<i32>, #{GPU_DIALECT}.slice<{{dim = {src_dim}, parent = #src}}>>, tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {src_dim}, parent = #src}}>>\n+            %3 = tt.load %2 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {src_dim}, parent = #src}}>>\n+            %4 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<{M}x!tt.ptr<i32>, #{GPU_DIALECT}.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            %5 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            %6 = tt.addptr %4, %5 : tensor<{M}x!tt.ptr<i32>, #{GPU_DIALECT}.slice<{{dim = {dst_dim}, parent = #dst}}>>, tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            %7 = {GPU_DIALECT}.convert_layout %3 : (tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {src_dim}, parent = #src}}>>) -> tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            tt.store %6, %7 : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {dst_dim}, parent = #dst}}>>\n             tt.return\n         }}\n     }}\n@@ -1962,26 +2008,29 @@ def _welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n @pytest.mark.parametrize(\"op\", [\"sum\", \"max\"])\n @pytest.mark.parametrize(\"first_axis\", [0, 1])\n def test_chain_reduce(M, N, src_layout, op, device, first_axis):\n+    if is_hip():\n+        pytest.skip(\"test_chain_reduce is not supported in HIP\")\n+\n     op_str = \"\"\n     if op == \"sum\":\n         op_str = f\"\"\"\n         %13 = arith.addi %arg2, %arg3 : i32\n         tt.reduce.return %13 : i32\"\"\"\n     elif op == \"max\":\n         op_str = f\"\"\"\n-        %13 = \"triton_gpu.cmpi\"(%arg2, %arg3) <{{predicate = 4 : i64}}> : (i32, i32) -> i1\n+        %13 = \"{GPU_DIALECT}.cmpi\"(%arg2, %arg3) <{{predicate = 4 : i64}}> : (i32, i32) -> i1\n         %14 = arith.select %13, %arg2, %arg3 : i32\n         tt.reduce.return %14 : i32\"\"\"\n     ir = f\"\"\"\n     #src = {src_layout}\n-    module attributes {{\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32}} {{\n+    module attributes {{\"{GPU_DIALECT}.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.threads-per-warp\" = {THREADS_PER_WARP} : i32}} {{\n     tt.func public @sum_kernel_0d1d(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n         %cst = arith.constant dense<{N}> : tensor<{M}x1xi32, #src>\n-        %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n-        %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xi32, #src>\n+        %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>\n+        %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xi32, #src>\n         %2 = arith.muli %1, %cst : tensor<{M}x1xi32, #src>\n-        %3 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #src}}>>\n-        %4 = tt.expand_dims %3 {{axis = 0 : i32}} : (tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #src}}>>) -> tensor<1x{N}xi32, #src>\n+        %3 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #{GPU_DIALECT}.slice<{{dim = 0, parent = #src}}>>\n+        %4 = tt.expand_dims %3 {{axis = 0 : i32}} : (tensor<{N}xi32, #{GPU_DIALECT}.slice<{{dim = 0, parent = #src}}>>) -> tensor<1x{N}xi32, #src>\n         %5 = tt.broadcast %2 : (tensor<{M}x1xi32, #src>) -> tensor<{M}x{N}xi32, #src>\n         %6 = tt.broadcast %4 : (tensor<1x{N}xi32, #src>) -> tensor<{M}x{N}xi32, #src>\n         %7 = arith.addi %5, %6 : tensor<{M}x{N}xi32, #src>\n@@ -1991,11 +2040,11 @@ def test_chain_reduce(M, N, src_layout, op, device, first_axis):\n         %11 = \"tt.reduce\"(%10) ({{\n         ^bb0(%arg2: i32, %arg3: i32):\n         {op_str}\n-        }}) {{axis = {first_axis} : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{M if first_axis == 1 else N}xi32, #triton_gpu.slice<{{dim = {first_axis}, parent = #src}}>>\n+        }}) {{axis = {first_axis} : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{M if first_axis == 1 else N}xi32, #{GPU_DIALECT}.slice<{{dim = {first_axis}, parent = #src}}>>\n         %12 = \"tt.reduce\"(%11) ({{\n         ^bb0(%arg2: i32, %arg3: i32):\n         {op_str}\n-        }}) {{axis = 0 : i32}} : (tensor<{M if first_axis == 1 else N}xi32, #triton_gpu.slice<{{dim = {first_axis}, parent = #src}}>>) -> i32\n+        }}) {{axis = 0 : i32}} : (tensor<{M if first_axis == 1 else N}xi32, #{GPU_DIALECT}.slice<{{dim = {first_axis}, parent = #src}}>>) -> i32\n         tt.store %arg1, %12 {{cache = 1 : i32, evict = 1 : i32}} : i32\n         tt.return\n     }}\n@@ -2063,6 +2112,8 @@ def var_mean_kernel(X, out_mean, out_var, BLOCK: tl.constexpr):\n @pytest.mark.parametrize(\"num_ctas\", num_ctas_list)\n def test_permute(dtype_str, shape, perm, num_ctas, device):\n     check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n+    if is_hip():\n+        pytest.skip(f\"test_permute is not supported in HIP\")\n \n     # triton kernel\n     @triton.jit\n@@ -2099,6 +2150,10 @@ def kernel(X, stride_xm, stride_xn,\n     # compare\n     np.testing.assert_allclose(to_numpy(z_tri), z_ref)\n     np.testing.assert_allclose(to_numpy(z_tri_contiguous), z_ref)\n+\n+    if is_hip():\n+        return\n+\n     # parse ptx to make sure ld/st are vectorized\n     ptx = pgm.asm['ptx']\n     assert 'ld.global.v4' in ptx\n@@ -2115,7 +2170,7 @@ def kernel(X, stride_xm, stride_xn,\n \n @pytest.mark.parametrize(\"M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype\",\n                          [(*shape, 4, False, False, epilogue, allow_tf32, in_dtype, out_dtype)\n-                          for shape in [(64, 64, 64), (16, 16, 16)]\n+                          for shape in [(64, 64, 64), (32, 32, 32), (16, 16, 16)]\n                           for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols', 'softmax', 'chain-dot']\n                           for allow_tf32 in [True, False]\n                           for in_dtype, out_dtype in [('float16', 'float16'),\n@@ -2146,6 +2201,17 @@ def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, o\n     check_cuda_only(device)\n \n     capability = torch.cuda.get_device_capability()\n+\n+    if is_hip():\n+        # set capability to large number to jump over check below\n+        # check are not relevant to amd gpu, left them for smaller diff between test_core.py and test_core_amd.py tests\n+        capability = (100, 100)\n+        if out_dtype is None:\n+            if in_dtype in float_dtypes:\n+                out_dtype = \"float32\"\n+            else:\n+                out_dtype = \"int32\"\n+\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n     if capability[0] < 8:\n@@ -2160,6 +2226,16 @@ def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, o\n             # TODO: support out_dtype=float16 for tl.dot on V100\n             pytest.skip(\"Only test out_dtype=float16 on devices with sm >=80\")\n \n+    if is_hip():\n+        if (M, N, K) in [(64, 128, 128)]:\n+            pytest.skip(f\"test_dot{(M, N, K)} not supported on HIP: memory out of resource.\")\n+        if (M, N, K, num_warps) in [(128, 256, 32, 8), (128, 128, 64, 4)]:\n+            pytest.skip(f\"test_dot{(M, N, K)} not supported on HIP. Reduce Warp to work\")\n+        if M == 16 or N == 16 or K == 16:\n+            pytest.skip(f\"test_dot{(M, N, K)} segfaults on HIP\")\n+        if epilogue == \"softmax\":\n+            pytest.skip(f\"test_dot{epilogue} segfaults on HIP\")\n+\n     torch.backends.cuda.matmul.allow_tf32 = allow_tf32\n \n     if num_ctas > 1 and in_dtype == 'int8':\n@@ -2247,6 +2323,7 @@ def kernel(X, stride_xm, stride_xk,\n         out_dtype = tl.float16\n     else:\n         out_dtype = tl.float32\n+\n     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n                          y_tri, y_tri.stride(0), y_tri.stride(1),\n                          w_tri, w_tri.stride(0), w_tri.stride(1),\n@@ -2261,20 +2338,24 @@ def kernel(X, stride_xm, stride_xk,\n                          ALLOW_TF32=allow_tf32,\n                          num_warps=num_warps, num_ctas=num_ctas,\n                          out_dtype=out_dtype)\n+\n     if epilogue == 'softmax' and (in_dtype != 'float32' or allow_tf32):\n-        ptx = pgm.asm[\"ptx\"]\n-        start = ptx.find(\"shfl.sync\")\n-        end = ptx.find(\"cvt.rn.f16.f32\")\n-        red_code = ptx[start:end]\n-        assert len(red_code) > 0\n-        import os\n-        enable_mmav3 = os.environ.get('ENABLE_MMA_V3', 'not found').lower()\n-        enable_tma = os.environ.get('ENABLE_TMA', 'not found').lower()\n-        # skip this check on hopper because there are some functions whose name contain \"shared\" in ptx.\n-        # TODO: we should eliminate these unused functions in ptx code.\n-        if not (enable_mmav3 in [\"on\", \"true\", \"1\"] and enable_tma in [\"on\", \"true\", \"1\"]):\n-            assert \"shared\" not in red_code\n-        assert \"bar.sync\" not in red_code\n+        if is_hip():\n+            pass\n+        else:\n+            ptx = pgm.asm[\"ptx\"]\n+            start = ptx.find(\"shfl.sync\")\n+            end = ptx.find(\"cvt.rn.f16.f32\")\n+            red_code = ptx[start:end]\n+            assert len(red_code) > 0\n+            import os\n+            enable_mmav3 = os.environ.get('ENABLE_MMA_V3', 'not found').lower()\n+            enable_tma = os.environ.get('ENABLE_TMA', 'not found').lower()\n+            # skip this check on hopper because there are some functions whose name contain \"shared\" in ptx.\n+            # TODO: we should eliminate these unused functions in ptx code.\n+            if not (enable_mmav3 in [\"on\", \"true\", \"1\"] and enable_tma in [\"on\", \"true\", \"1\"]):\n+                assert \"shared\" not in red_code\n+            assert \"bar.sync\" not in red_code\n     # torch result\n     if in_dtype == 'int8':\n         z_ref = np.matmul(x.astype(np.float32),\n@@ -2300,9 +2381,12 @@ def kernel(X, stride_xm, stride_xk,\n         # XXX: Somehow there's a larger difference when we use float32\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)\n     elif out_dtype == tl.float16:\n-        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)\n+        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-2)\n     else:\n-        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+        # added atol, to loose precision for float16xfloat16->float32 case\n+        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)\n+    if is_hip():\n+        return\n     # make sure ld/st are vectorized\n     ptx = pgm.asm['ptx']\n     if (K > 16 or N > 16 or M > 16) and (M * N // (num_warps * 32) >= 4):\n@@ -2366,6 +2450,9 @@ def kernel(Z, X, Y,\n     h = kernel[grid](z_tri, x_tri, y_tri, M, N, K, BM, BN, BK)\n     z_ref = np.matmul(x, y)\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), atol=0.01)\n+\n+    if is_hip():\n+        return\n     assert \"tt.dot\" in h.asm['ttir']\n     # with option ENABLE_MMA_V3 on, we will not pipeline the load op for Y\n     # as the loaded value is in rowmajor. But MMAv3 requires it's second\n@@ -2432,6 +2519,9 @@ def test_dot_without_load(dtype_str, device):\n     capability = torch.cuda.get_device_capability()\n     allow_tf32 = capability[0] > 7\n \n+    if is_hip() and dtype_str == \"float16\":\n+        pytest.skip(\"test_dot_without_load[float16] not supported in HIP\")\n+\n     @triton.jit\n     def _kernel(out, ALLOW_TF32: tl.constexpr):\n         a = GENERATE_TEST_HERE\n@@ -2512,6 +2602,9 @@ def _kernel(in_ptr, out_ptr, in_size: tl.constexpr, out_size: tl.constexpr):\n # FIXME: Shape too small for ldmatrix when num_ctas=4\n @pytest.mark.parametrize(\"dtype\", [torch.bfloat16, torch.float16, torch.float32])\n def test_masked_load_shared_memory(dtype, device):\n+    if is_hip():\n+        pytest.skip(\"test_masked_load_shared_memory is not supported in HIP\")\n+\n     check_type_supported(dtype, device)  # bfloat16 on cc < 80 will not be tested\n \n     M = 32\n@@ -2571,6 +2664,9 @@ def _kernel(dst, src, CACHE: tl.constexpr):\n         tl.store(dst + offsets, x)\n \n     pgm = _kernel[(1,)](dst, src, CACHE=cache)\n+    if is_hip():\n+        return\n+\n     ptx = pgm.asm['ptx']\n     if cache == '':\n         assert 'ld.global.ca' not in ptx\n@@ -2597,6 +2693,10 @@ def _kernel(dst, src, N, BLOCK_SIZE: tl.constexpr):\n         tl.store(dst + offsets, x, mask=offsets < N)\n     pgm = _kernel[(1,)](\n         dst, src, N=N, BLOCK_SIZE=block_size)\n+\n+    if is_hip():\n+        return\n+\n     ptx = pgm.asm[\"ptx\"]\n     if N % 16 == 0:\n         assert \"ld.global.v4.b32\" in ptx\n@@ -2620,6 +2720,9 @@ def _kernel(dst, src, off, N, BLOCK_SIZE: tl.constexpr, HINT: tl.constexpr):\n         x = tl.load(src + offsets, mask=offsets < N)\n         tl.store(dst + offsets, x, mask=offsets < N)\n     pgm = _kernel[(1,)](dst, src, off, N=1024, BLOCK_SIZE=src.shape[0], HINT=has_hints)\n+    if is_hip():\n+        return\n+\n     ptx = pgm.asm[\"ptx\"]\n     if has_hints:\n         assert \"ld.global.v4.b32\" in ptx\n@@ -2642,6 +2745,8 @@ def _kernel(dst, src, CACHE: tl.constexpr):\n         x = tl.load(src + offsets)\n         tl.store(dst + offsets, x, cache_modifier=CACHE)\n \n+    if is_hip():\n+        return\n     pgm = _kernel[(1,)](dst, src, CACHE=cache)\n     ptx = pgm.asm['ptx']\n     if cache == '':\n@@ -2793,6 +2898,9 @@ def kernel(VALUE, X):\n @pytest.mark.parametrize(\"is_lhs_constexpr\", [False, True])\n @pytest.mark.parametrize(\"is_rhs_constexpr\", [True, False])\n def test_bin_op_constexpr(op, is_lhs_constexpr, is_rhs_constexpr, device):\n+    if is_hip():\n+        if (is_rhs_constexpr, is_lhs_constexpr, op) in [(False, False, \"<<\"), (False, False, \">>\"), (False, True, \"<<\")]:\n+            pytest.skip(f\"test_bin_op_constexpr[{is_lhs_constexpr}-{is_rhs_constexpr}-{op}] is not supported in HIP\")\n \n     @triton.jit\n     def kernel(Z, X, Y):\n@@ -2968,6 +3076,9 @@ def _kernel(dst):\n @pytest.mark.parametrize(\"num_ctas\", num_ctas_list)\n def test_math_tensor(dtype_str, expr, lib_path, num_ctas, device):\n \n+    if is_hip() and expr == \"math.scalbn\":\n+        pytest.skip(\"test_math_tensor[math.scalbn] is not supported in HIP\")\n+\n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n         x = tl.load(X + tl.arange(0, BLOCK))\n@@ -3063,6 +3174,9 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n def test_inline_asm(num_ctas, device):\n     check_cuda_only(device)\n \n+    if is_hip():\n+        pytest.skip(\"test_inline_asm is not supported in HIP\")\n+\n     @triton.jit\n     def kernel(X, Y, Z, n: tl.constexpr, BLOCK: tl.constexpr):\n         x = tl.load(X + tl.arange(0, BLOCK))\n@@ -3089,6 +3203,9 @@ def kernel(X, Y, Z, n: tl.constexpr, BLOCK: tl.constexpr):\n def test_inline_asm_packed(num_ctas, device):\n     check_cuda_only(device)\n \n+    if is_hip():\n+        pytest.skip(\"test_inline_asm is not supported in HIP\")\n+\n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n         x = tl.load(X + tl.arange(0, BLOCK))\n@@ -3392,6 +3509,8 @@ def nested_while(data, countPtr):\n \n \n def test_globaltimer(device):\n+    if is_hip():\n+        pytest.skip(\"test_globaltimer is not supported in HIP\")\n     check_cuda_only(device)\n \n     @triton.jit\n@@ -3411,6 +3530,8 @@ def kernel(Out1, Out2):\n \n \n def test_smid(device):\n+    if is_hip():\n+        pytest.skip(\"test_smid is not supported in HIP\")\n     check_cuda_only(device)\n \n     @triton.jit\n@@ -3456,6 +3577,9 @@ def kernel(Out):\n @pytest.mark.parametrize(\"interm_layout\", intermediate_layouts)\n @pytest.mark.parametrize(\"dst_layout\", layouts)\n def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device):\n+    if is_hip():\n+        pytest.skip(\"test_convert2d is not supported in HIP\")\n+\n     if str(src_layout) == str(dst_layout):\n         pytest.skip()\n     if 'mma' in str(src_layout) and 'mma' in str(dst_layout):"}, {"filename": "python/triton/common/backend.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -5,6 +5,7 @@\n import os\n import re\n import subprocess\n+import traceback\n from typing import Dict\n \n from ..runtime.driver import DriverBase\n@@ -94,7 +95,7 @@ def get_backend(device_type: str):\n             try:\n                 importlib.import_module(device_backend_package_name, package=__spec__.name)\n             except Exception:\n-                return None\n+                traceback.print_exc()\n         else:\n             return None\n     return _backends[device_type] if device_type in _backends else None"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 45, "deletions": 117, "changes": 162, "file_content_changes": "@@ -5,18 +5,18 @@\n import json\n import os\n import re\n-import subprocess\n import tempfile\n from collections import namedtuple\n from pathlib import Path\n-from typing import Any, Tuple\n+from typing import Any\n \n from .._C.libtriton.triton import (ClusterInfo, TMAInfos, add_external_libs,\n                                    compile_ptx_to_cubin, get_env_vars, get_num_warps,\n                                    get_shared_memory_size, ir, runtime,\n-                                   translate_llvmir_to_hsaco, translate_llvmir_to_ptx,\n+                                   translate_llvmir_to_ptx,\n                                    translate_triton_gpu_to_llvmir)\n from ..common.backend import get_backend, path_to_ptxas\n+from ..common.build import is_hip\n # from ..runtime import driver, jit, JITFunction\n # TODO: runtime.errors\n from ..runtime.autotuner import OutOfResources\n@@ -188,72 +188,6 @@ def ptx_to_cubin(ptx: str, arch: int):\n     return compile_ptx_to_cubin(ptx, ptxas, arch)\n \n \n-# AMDGCN translation\n-\n-def get_amdgcn_bitcode_paths(arch):\n-    gpu_arch_agnostic_bitcode_libraries = [\"opencl.bc\",\n-                                           \"ocml.bc\",\n-                                           \"ockl.bc\",\n-                                           \"oclc_finite_only_off.bc\",\n-                                           \"oclc_daz_opt_off.bc\",\n-                                           \"oclc_correctly_rounded_sqrt_on.bc\",\n-                                           \"oclc_unsafe_math_off.bc\",\n-                                           \"oclc_wavefrontsize64_on.bc\"]\n-\n-    gfx_arch = arch[1]\n-    gfx_arch_id = re.search('gfx(\\\\w+)', gfx_arch).group(1).strip()\n-\n-    gpu_arch_specific_bitcode_library = 'oclc_isa_version_' + gfx_arch_id + \".bc\"\n-    bitcode_path_dir = os.path.join(Path(__file__).parent.resolve(), \"third_party/rocm/lib/bitcode/\")\n-\n-    amdgcn_bitcode_paths = {}\n-    i = 1\n-    for bc_lib in gpu_arch_agnostic_bitcode_libraries:\n-        bc_path = bitcode_path_dir + bc_lib\n-        if os.path.exists(bc_path):\n-            amdgcn_bitcode_paths['library_' + str(i)] = bc_path\n-            i += 1\n-    bc_gfx_path = bitcode_path_dir + gpu_arch_specific_bitcode_library\n-    if os.path.exists(bc_gfx_path):\n-        amdgcn_bitcode_paths['library_' + str(i)] = bc_gfx_path\n-\n-    return amdgcn_bitcode_paths\n-\n-\n-def get_amdgpu_arch_fulldetails():\n-    \"\"\"\n-    get the amdgpu fulll ISA details for compiling:\n-    i.e., arch_triple: amdgcn-amd-amdhsa; arch_name: gfx906; arch_features: sramecc+:xnack-\n-    \"\"\"\n-    try:\n-        # TODO: package rocm.cc with Triton\n-        rocm_path_dir = os.getenv(\"ROCM_PATH\", default=\"/opt/rocm\")\n-        rocminfo = subprocess.check_output(rocm_path_dir + '/bin/rocminfo').decode()\n-        gfx_arch_details = re.search('amd.*', rocminfo).group(0).strip().split('--')\n-        arch_triple = gfx_arch_details[0]\n-        arch_name_features = gfx_arch_details[1].split(':')\n-        arch_name = arch_name_features[0]\n-        arch_features = \"\"\n-\n-        if (len(arch_name_features) == 3):\n-            arch_features = \"+\" + re.search('\\\\w+', arch_name_features[1]).group(0) + \",\"\\\n-                            \"-\" + re.search('\\\\w+', arch_name_features[2]).group(0)\n-        return [arch_triple, arch_name, arch_features]\n-    except BaseException:\n-        return None\n-\n-\n-def llir_to_amdgcn_and_hsaco(mod: Any, gfx_arch: str, gfx_triple: str, gfx_features: str) -> Tuple[str, str]:\n-    '''\n-    Translate TritonGPU module to HSACO code based on full details of gpu architecture.\n-    :param mod: a TritonGPU dialect module\n-    :return:\n-        - AMDGCN code\n-        - Path to HSACO object\n-    '''\n-    return translate_llvmir_to_hsaco(mod, gfx_arch, gfx_triple, gfx_features)\n-\n-\n # ------------------------------------------------------------------------------\n # compiler\n # ------------------------------------------------------------------------------\n@@ -320,8 +254,10 @@ def make_hash(fn, arch, env_vars, **kwargs):\n     \"ttgir\": mlir_arg_type_pattern,\n     \"ptx\": ptx_arg_type_pattern,\n }\n-\n-ttgir_num_warps_pattern = r'\"triton_gpu.num-warps\"\\s?=\\s?(\\d+)\\s?:'\n+if is_hip():\n+    ttgir_num_warps_pattern = r'\"triton_gpu_rocm.num-warps\"\\s?=\\s?(\\d+)\\s?:'\n+else:\n+    ttgir_num_warps_pattern = r'\"triton_gpu.num-warps\"\\s?=\\s?(\\d+)\\s?:'\n \n \n def _get_jsonable_constants(constants):\n@@ -354,17 +290,10 @@ def _is_cuda(arch):\n \n \n def get_architecture_descriptor(capability):\n-    try:\n-        import torch\n-    except ImportError:\n-        raise ImportError(\"Triton requires PyTorch to be installed\")\n     if capability is None:\n-        if torch.version.hip is None:\n-            device = get_current_device()\n-            capability = get_device_capability(device)\n-            capability = capability[0] * 10 + capability[1]\n-        else:\n-            capability = get_amdgpu_arch_fulldetails()\n+        device = get_current_device()\n+        capability = get_device_capability(device)\n+        capability = capability[0] * 10 + capability[1]\n     return capability\n \n \n@@ -394,23 +323,6 @@ def get_arch_default_num_stages(device_type, capability=None):\n     return num_stages\n \n \n-def add_rocm_stages(arch, extern_libs, stages):\n-    extern_libs.update(get_amdgcn_bitcode_paths(arch))\n-\n-    for key in list(extern_libs):\n-        if extern_libs[key] == '' or extern_libs[key] is None:\n-            extern_libs.pop(key)\n-\n-    gfx_arch_full_details = arch\n-    gfx_arch = os.environ.get('MI_GPU_ARCH', gfx_arch_full_details[1])\n-    if gfx_arch is None:\n-        raise RuntimeError('gfx_arch is None (not specified)')\n-    stages[\"amdgcn\"] = (lambda path: Path(path).read_text(),\n-                        lambda src: llir_to_amdgcn_and_hsaco(src, gfx_arch,\n-                                                             gfx_arch_full_details[0],\n-                                                             gfx_arch_full_details[2]))\n-\n-\n def add_cuda_stages(arch, extern_libs, stages):\n \n     stages[\"ptx\"] = (lambda path: Path(path).read_text(),\n@@ -422,18 +334,22 @@ def add_cuda_stages(arch, extern_libs, stages):\n def compile(fn, **kwargs):\n     # Get device type to decide which backend should be used\n     device_type = kwargs.get(\"device_type\", \"cuda\")\n-    _device_backend = get_backend(device_type)\n     capability = kwargs.get(\"cc\", None)\n \n-    if device_type in [\"cuda\", \"hip\"]:\n+    if is_hip():\n+        device_type = \"hip\"\n+\n+    if device_type == \"cuda\":\n+        _device_backend = get_backend(device_type)\n         arch = get_architecture_descriptor(capability)\n     else:\n         _device_backend = get_backend(device_type)\n         assert _device_backend\n         arch = _device_backend.get_architecture_descriptor(**kwargs)\n \n     is_cuda = device_type == \"cuda\" and _is_cuda(arch)\n-    is_hip = device_type in [\"cuda\", \"hip\"] and not is_cuda\n+    if is_hip():\n+        is_cuda = False\n     context = ir.context()\n     constants = kwargs.get(\"constants\", dict())\n     num_warps = kwargs.get(\"num_warps\", get_arch_default_num_warps(device_type))\n@@ -464,14 +380,20 @@ def compile(fn, **kwargs):\n     stages[\"ast\"] = (lambda path: fn, None)\n     stages[\"ttir\"] = (lambda path: parse_mlir_module(path, context),\n                       lambda src: optimize_ttir(ast_to_ttir(src, signature, configs[0], constants, debug=debug, arch=arch), arch))\n-    stages[\"ttgir\"] = (lambda path: parse_mlir_module(path, context),\n-                       lambda src: optimize_ttgir(ttir_to_ttgir(src, num_warps, num_ctas, arch), num_stages, num_warps, num_ctas, arch, cluster_info, enable_warp_specialization, enable_persistent, optimize_epilogue))\n-    stages[\"llir\"] = (lambda path: Path(path).read_text(),\n-                      lambda src: ttgir_to_llir(src, extern_libs, arch, tma_infos))\n     if is_cuda:\n+        stages[\"ttgir\"] = (lambda path: parse_mlir_module(path, context),\n+                           lambda src: optimize_ttgir(ttir_to_ttgir(src, num_warps, num_ctas, arch), num_stages, num_warps, num_ctas, arch, cluster_info, enable_warp_specialization, enable_persistent, optimize_epilogue))\n+        stages[\"llir\"] = (lambda path: Path(path).read_text(),\n+                          lambda src: ttgir_to_llir(src, extern_libs, arch, tma_infos))\n         add_cuda_stages(arch, extern_libs, stages)\n-    elif is_hip:\n-        add_rocm_stages(arch, extern_libs, stages)\n+    elif device_type == \"hip\":\n+        _device_backend.add_stages(arch, extern_libs, stages, num_warps=num_warps, num_stages=num_stages)\n+    elif device_type == \"xpu\":\n+        stages[\"ttgir\"] = (lambda path: parse_mlir_module(path, context),\n+                           lambda src: optimize_ttgir(ttir_to_ttgir(src, num_warps, num_ctas, arch), num_stages, num_warps, num_ctas, arch, cluster_info, enable_warp_specialization, enable_persistent, optimize_epilogue))\n+        stages[\"llir\"] = (lambda path: Path(path).read_text(),\n+                          lambda src: ttgir_to_llir(src, extern_libs, arch, tma_infos))\n+        _device_backend.add_stages(arch, extern_libs, stages)\n     else:\n         # pass the user's configuration to the backend device.\n         arch[\"num_warps\"] = num_warps\n@@ -564,7 +486,7 @@ def compile(fn, **kwargs):\n             path = metadata_group.get(ir_filename)\n             if path is None:\n                 next_module = compile_kernel(module)\n-                if ir == \"amdgcn\":\n+                if ir_name == \"amdgcn\":\n                     extra_file_name = f\"{name}.hsaco_path\"\n                     metadata_group[ir_filename] = fn_cache_manager.put(next_module[0], ir_filename)\n                     metadata_group[extra_file_name] = fn_cache_manager.put(next_module[1], extra_file_name)\n@@ -587,17 +509,23 @@ def compile(fn, **kwargs):\n         else:\n             asm[ir_name] = str(next_module)\n         if ir_name == \"llir\" and \"shared\" not in metadata:\n-            metadata[\"shared\"] = get_shared_memory_size(module)\n+            if is_hip():\n+                metadata[\"shared\"] = _device_backend.get_shared_memory_size(module)\n+            else:\n+                metadata[\"shared\"] = get_shared_memory_size(module)\n         if ir_name == \"ttgir\":\n-            metadata[\"enable_warp_specialization\"] = ir.is_ws_supported(next_module)\n-            if metadata[\"enable_warp_specialization\"]:\n-                metadata[\"num_warps\"] = get_num_warps(next_module)\n+            if is_hip():\n+                metadata[\"num_warps\"] = _device_backend.get_num_warps(next_module)\n+            else:\n+                metadata[\"enable_warp_specialization\"] = ir.is_ws_supported(next_module)\n+                if metadata[\"enable_warp_specialization\"]:\n+                    metadata[\"num_warps\"] = get_num_warps(next_module)\n         if ir_name == \"ptx\":\n             metadata[\"name\"] = get_kernel_name(next_module, pattern='// .globl')\n         if ir_name == \"amdgcn\":\n             metadata[\"name\"] = get_kernel_name(next_module[0], pattern='.globl')\n             asm[\"hsaco_path\"] = next_module[1]\n-        if not is_cuda and not is_hip:\n+        if not is_cuda and not is_hip():\n             _device_backend.add_meta_info(ir_name, module, next_module, metadata, asm)\n         module = next_module\n \n@@ -622,7 +550,7 @@ def compile(fn, **kwargs):\n     ids_of_const_exprs = tuple(fn.constexprs) if isinstance(fn, JITFunction) else ()\n     ids = {\"ids_of_tensormaps\": ids_of_tensormaps, \"ids_of_folded_args\": ids_of_folded_args, \"ids_of_const_exprs\": ids_of_const_exprs}\n     # cache manager\n-    if is_cuda or is_hip:\n+    if is_cuda:\n         so_path = make_stub(name, signature, constants, ids, enable_warp_specialization=enable_warp_specialization)\n     else:\n         so_path = _device_backend.make_launcher_stub(name, signature, constants, ids)\n@@ -660,7 +588,7 @@ def __init__(self, fn, so_path, metadata, asm):\n             self.tensormaps_info = metadata[\"tensormaps_info\"]\n         self.constants = metadata[\"constants\"]\n         self.device_type = metadata[\"device_type\"]\n-        self.device_backend = get_backend(self.device_type) if self.device_type not in [\"cuda\", \"hip\"] else None\n+        self.device_backend = get_backend(self.device_type) if self.device_type not in [\"cuda\"] else None\n         # initialize asm dict\n         self.asm = asm\n         # binaries are lazily initialized\n@@ -674,7 +602,7 @@ def _init_handles(self):\n         if self.cu_module is not None:\n             return\n \n-        if self.device_type in [\"cuda\", \"hip\"]:\n+        if self.device_type in [\"cuda\"]:\n             device = get_current_device()\n             bin_path = {\n                 driver.HIP: \"hsaco_path\",\n@@ -720,7 +648,7 @@ def __getitem__(self, grid):\n         def runner(*args, stream=None):\n             args_expand = self.assemble_tensormap_to_arg(args)\n             if stream is None:\n-                if self.device_type in [\"cuda\", \"hip\"]:\n+                if self.device_type in [\"cuda\"]:\n                     stream = get_cuda_stream()\n                 else:\n                     stream = get_backend(self.device_type).get_stream(None)"}, {"filename": "python/triton/compiler/make_launcher.py", "status": "modified", "additions": 4, "deletions": 151, "changes": 155, "file_content_changes": "@@ -3,16 +3,11 @@\n import tempfile\n \n from ..common import _build\n+from ..common.build import is_hip\n from ..runtime.cache import get_cache_manager\n from ..runtime.jit import version_key\n from .utils import generate_cu_signature\n \n-\n-def is_hip():\n-    import torch\n-    return torch.version.hip is not None\n-\n-\n # ----- stub --------\n \n \n@@ -103,151 +98,9 @@ def format_of(ty):\n     format = \"iiiiiiiiiKKOOO\" + ''.join([format_of(_extracted_type(ty)) for ty in signature.values()])\n \n     # generate glue code\n-    if is_hip():\n-        src = f\"\"\"\n-    #define __HIP_PLATFORM_AMD__\n-    #include <hip/hip_runtime.h>\n-    #include <Python.h>\n-    #include <stdio.h>\n-\n-    static inline void gpuAssert(hipError_t code, const char *file, int line)\n-    {{\n-      if (code != HIP_SUCCESS)\n-      {{\n-         const char* prefix = \"Triton Error [HIP]: \";\n-         const char* str = hipGetErrorString(code);\n-         char err[1024] = {{0}};\n-         snprintf(err, 1024, \"%s Code: %d, Messsage: %s\", prefix, code, str );\n-         PyErr_SetString(PyExc_RuntimeError, err);\n-      }}\n-    }}\n-\n-    #define HIP_CHECK(ans) {{ gpuAssert((ans), __FILE__, __LINE__); }}\n-\n-    static void _launch(int gridX, int gridY, int gridZ, int num_warps, int shared_memory, hipStream_t stream, hipFunction_t function, {arg_decls}) {{\n-      void *params[] = {{ {', '.join(f\"&arg{i}\" for i in signature.keys() if i not in constants)} }};\n-      if (gridX*gridY*gridZ > 0) {{\n-          HIP_CHECK(hipModuleLaunchKernel(function, gridX, gridY, gridZ, 64*num_warps, 1, 1, shared_memory, stream, params, 0));\n-      }}\n-    }}\n-\n-    typedef struct _DevicePtrInfo {{\n-      hipDeviceptr_t dev_ptr;\n-      bool valid;\n-    }} DevicePtrInfo;\n-\n-    static inline DevicePtrInfo getPointer(PyObject *obj, int idx) {{\n-      DevicePtrInfo ptr_info;\n-      ptr_info.dev_ptr = 0;\n-      ptr_info.valid = true;\n-\n-      if (PyLong_Check(obj)) {{\n-        ptr_info.dev_ptr = (hipDeviceptr_t)PyLong_AsUnsignedLongLong(obj);\n-        return ptr_info;\n-      }}\n-\n-      if (obj == Py_None) {{\n-        // valid nullptr\n-        return ptr_info;\n-      }}\n-\n-      PyObject *ptr = PyObject_GetAttrString(obj, \"data_ptr\");\n-\n-      if (ptr) {{\n-        PyObject *empty_tuple = PyTuple_New(0);\n-        PyObject *ret = PyObject_Call(ptr, empty_tuple, NULL);\n-        Py_DECREF(empty_tuple);\n-        Py_DECREF(ptr);\n-\n-        if (!PyLong_Check(ret)) {{\n-          PyErr_SetString(PyExc_TypeError, \"data_ptr method of Pointer object must return 64-bit int\");\n-          ptr_info.valid = false;\n-          return ptr_info;\n-        }}\n-\n-        ptr_info.dev_ptr = (hipDeviceptr_t)PyLong_AsUnsignedLongLong(ret);\n-\n-        if (!ptr_info.dev_ptr)\n-          return ptr_info;\n-\n-        uint64_t dev_ptr;\n-        hipError_t status = hipPointerGetAttribute(&dev_ptr, HIP_POINTER_ATTRIBUTE_DEVICE_POINTER, ptr_info.dev_ptr);\n-        if (status == hipErrorInvalidValue) {{\n-            PyErr_Format(PyExc_ValueError,\n-                         \"Pointer argument (at %d) cannot be accessed from Triton (cpu tensor?)\", idx);\n-            ptr_info.valid = false;\n-        }}\n-\n-        ptr_info.dev_ptr = (hipDeviceptr_t)dev_ptr;\n-        return ptr_info;\n-      }}\n-\n-      PyErr_SetString(PyExc_TypeError, \"Pointer argument must be either uint64 or have data_ptr method\");\n-      ptr_info.valid = false;\n-      return ptr_info;\n-    }}\n-\n-    static PyObject* launch(PyObject* self, PyObject* args) {{\n-\n-      int gridX, gridY, gridZ;\n-      uint64_t _stream;\n-      uint64_t _function;\n-      int num_warps;\n-      int shared_memory;\n-      PyObject *launch_enter_hook = NULL;\n-      PyObject *launch_exit_hook = NULL;\n-      PyObject *compiled_kernel = NULL;\n-\n-      {' '.join([f\"{_extracted_type(ty)} _arg{i}; \" for i, ty in signature.items()])}\n-      if (!PyArg_ParseTuple(args, \\\"{format}\\\", &gridX, &gridY, &gridZ, &num_warps, &shared_memory, &_stream, &_function, &launch_enter_hook, &launch_exit_hook, &compiled_kernel{', ' + ', '.join(f\"&_arg{i}\" for i, ty in signature.items()) if len(signature) > 0 else ''})) {{\n-        return NULL;\n-      }}\n-\n-      if (launch_enter_hook != Py_None && !PyObject_CallObject(launch_enter_hook, args)) {{\n-        return NULL;\n-      }}\n-\n-      // raise exception asap\n-      {\"; \".join([f\"DevicePtrInfo ptr_info{i} = getPointer(_arg{i}, {i}); if (!ptr_info{i}.valid) return NULL;\" if ty[0] == \"*\" else \"\" for i, ty in signature.items()])};\n-      Py_BEGIN_ALLOW_THREADS;\n-      _launch(gridX, gridY, gridZ, num_warps, shared_memory, (hipStream_t)_stream, (hipFunction_t)_function{', ' + ', '.join(f\"ptr_info{i}.dev_ptr\" if ty[0]==\"*\" else f\"_arg{i}\" for i, ty in signature.items()) if len(signature) > 0 else ''});\n-      Py_END_ALLOW_THREADS;\n-\n-      if (launch_exit_hook != Py_None && !PyObject_CallObject(launch_exit_hook, args)) {{\n-        return NULL;\n-      }}\n-\n-      // return None\n-      Py_INCREF(Py_None);\n-      return Py_None;\n-    }}\n-\n-    static PyMethodDef ModuleMethods[] = {{\n-      {{\"launch\", launch, METH_VARARGS, \"Entry point for all kernels with this signature\"}},\n-      {{NULL, NULL, 0, NULL}} // sentinel\n-    }};\n-\n-    static struct PyModuleDef ModuleDef = {{\n-      PyModuleDef_HEAD_INIT,\n-      \\\"__triton_launcher\\\",\n-      NULL, //documentation\n-      -1, //size\n-      ModuleMethods\n-    }};\n-\n-    PyMODINIT_FUNC PyInit___triton_launcher(void) {{\n-      PyObject *m = PyModule_Create(&ModuleDef);\n-      if(m == NULL) {{\n-        return NULL;\n-      }}\n-      PyModule_AddFunctions(m, ModuleMethods);\n-      return m;\n-    }}\n-    \"\"\"\n-    else:\n-        folded_without_constexprs = [c for c in ids['ids_of_folded_args'] if c not in ids['ids_of_const_exprs']]\n-        params = [i for i in signature.keys() if i >= start_desc or (i not in constants and i not in folded_without_constexprs)]\n-        src = f\"\"\"\n+    folded_without_constexprs = [c for c in ids['ids_of_folded_args'] if c not in ids['ids_of_const_exprs']]\n+    params = [i for i in signature.keys() if i >= start_desc or (i not in constants and i not in folded_without_constexprs)]\n+    src = f\"\"\"\n #include \\\"cuda.h\\\"\n #include <stdbool.h>\n #include <Python.h>"}, {"filename": "python/triton/language/math.py", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -1,17 +1,18 @@\n import functools\n import os\n \n+from ..common.build import is_hip\n from . import core\n \n \n @functools.lru_cache()\n def libdevice_path():\n-    import torch\n     third_party_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"third_party\")\n-    if torch.version.hip is None:\n-        default = os.path.join(third_party_dir, \"cuda\", \"lib\", \"libdevice.10.bc\")\n+    if is_hip():\n+        default = os.path.join(third_party_dir, \"hip\", \"lib\", \"bitcode\", \"cuda2gcn.bc\")\n     else:\n-        default = ''\n+        default = os.path.join(third_party_dir, \"cuda\", \"lib\", \"libdevice.10.bc\")\n+\n     return os.getenv(\"TRITON_LIBDEVICE_PATH\", default)\n \n "}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 40, "deletions": 0, "changes": 40, "file_content_changes": "@@ -4,6 +4,7 @@\n from typing import List, Optional, Sequence, Tuple, TypeVar\n \n from .._C.libtriton.triton import ir\n+from ..common.build import is_hip\n from . import core as tl\n \n T = TypeVar('T')\n@@ -1239,6 +1240,19 @@ def atomic_xchg(ptr: tl.tensor,\n # ===----------------------------------------------------------------------===//\n \n \n+def gpu_has_mfma() -> bool:\n+    if not is_hip():\n+        return False\n+    return True  # mfma supported in ['gfx908', 'gfx90a']\n+\n+\n+def mfma_supported(M, N, K, allow_tf32, ret_scalar_ty) -> bool:\n+    if not gpu_has_mfma():\n+        return False\n+    # TODO: Add check for configurations and types.\n+    return True\n+\n+\n def dot(lhs: tl.tensor,\n         rhs: tl.tensor,\n         allow_tf32: bool,\n@@ -1292,6 +1306,32 @@ def assert_dtypes_valid(lhs_dtype, rhs_dtype, arch):\n \n     M = lhs.type.shape[0]\n     N = rhs.type.shape[1]\n+\n+    # Cast operands of types f16 and i8 for configurations where FMA only supported.\n+    if is_hip() and not mfma_supported(M, N, lhs.type.shape[1], allow_tf32, ret_scalar_ty):\n+        ret_cast_scalar_ty = tl.float32 if lhs.type.scalar.is_int() else ret_scalar_ty\n+        lhs = cast(lhs, ret_cast_scalar_ty, builder)\n+        rhs = cast(rhs, ret_cast_scalar_ty, builder)\n+        if ret_cast_scalar_ty == tl.float16:\n+            _0 = builder.create_splat(builder.get_fp16(0), [M, N])\n+        else:\n+            _0 = builder.create_splat(builder.get_fp32(0), [M, N])\n+        ret_ty = tl.block_type(ret_cast_scalar_ty, [M, N])\n+        ret = tl.tensor(builder.create_dot(lhs.handle, rhs.handle, _0, allow_tf32),\n+                        ret_ty)\n+        return cast(ret, ret_scalar_ty, builder)\n+    if is_hip() and mfma_supported(M, N, lhs.type.shape[1], allow_tf32, ret_scalar_ty) and ret_scalar_ty.primitive_bitwidth < 32:\n+        if lhs.type.scalar.is_int():\n+            ret_dot_scalar_ty = tl.int32\n+            _0 = builder.create_splat(builder.get_int32(0), [M, N])\n+        else:\n+            ret_dot_scalar_ty = tl.float32\n+            _0 = builder.create_splat(builder.get_fp32(0), [M, N])\n+        ret_ty = tl.block_type(ret_dot_scalar_ty, [M, N])\n+        ret = tl.tensor(builder.create_dot(lhs.handle, rhs.handle, _0, allow_tf32),\n+                        ret_ty)\n+        return cast(ret, ret_scalar_ty, builder)\n+\n     _0 = builder.create_splat(_0, [M, N])\n     ret_ty = tl.block_type(ret_scalar_ty, [M, N])\n     return tl.tensor(builder.create_dot(lhs.handle, rhs.handle, _0, allow_tf32),"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -383,20 +383,20 @@ def {self.fn.__name__}({args_signature}grid=None, num_warps=None, num_ctas=1, nu\n         device_type = self._conclude_device_type(device_types, {pinned_memory_flags})\n \n     device_backend = None\n-    if device_type not in ['cuda', 'hip']:\n+    if device_type not in ['cuda']:\n         device_backend = get_backend(device_type)\n         if device_backend is None:\n             raise ValueError('Cannot find backend for ' + device_type)\n \n     if device is None:\n-        if device_type in ['cuda', 'hip']:\n+        if device_type in ['cuda']:\n             device = get_current_device()\n             set_current_device(device)\n         else:\n             device = device_backend.get_current_device()\n             device_backend.set_current_device(device)\n     if stream is None and not warmup:\n-        if device_type in ['cuda', 'hip']:\n+        if device_type in ['cuda']:\n             stream = get_cuda_stream(device)\n         else:\n             stream = device_backend.get_stream()"}, {"filename": "third_party/amd_hip_backend", "status": "added", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -0,0 +1 @@\n+Subproject commit d0ad70d55df3ebe11cc80bbb364a91551e6b6248"}]