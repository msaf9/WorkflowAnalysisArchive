[{"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -361,7 +361,7 @@ bool supportMMA(triton::DotOp op, int version) {\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n     if (!(numWarps % 4 == 0 && retShapePerCTA[0] % 64 == 0 &&\n           retShapePerCTA[1] % 8 == 0 &&\n-          (aElemTy.isFloat8E5M2() || aElemTy.isFloat8E4M3FN() ||\n+          (aElemTy.isFloat8E5M2() || aElemTy.isFloat8E4M3FNUZ() ||\n            aElemTy.isInteger(8) || aElemTy.isF16() || aElemTy.isBF16() ||\n            aElemTy.isF32()))) {\n       return false;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/WGMMA.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -58,7 +58,7 @@ triton::nvgpu::WGMMAEltType getMmaOperandType(Value a, bool allowTF32) {\n     return triton::nvgpu::WGMMAEltType::s8;\n   } else if (aTy.isFloat8E5M2()) {\n     return triton::nvgpu::WGMMAEltType::e5m2;\n-  } else if (aTy.isFloat8E4M3FN()) {\n+  } else if (aTy.isFloat8E4M3FNUZ()) {\n     return triton::nvgpu::WGMMAEltType::e4m3;\n   } else {\n     llvm::report_fatal_error(\"Unsupported mma operand type found\");\n@@ -298,7 +298,7 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n \n   triton::nvgpu::WGMMAEltType eltTypeC = getMmaRetType(d);\n   triton::nvgpu::WGMMAEltType eltTypeA = getMmaOperandType(a, allowTF32);\n-  triton::nvgpu::WGMMAEltType eltTypeB = eltTypeA;\n+  triton::nvgpu::WGMMAEltType eltTypeB = getMmaOperandType(b, allowTF32);\n \n   triton::nvgpu::WGMMALayout layoutA = transA ? triton::nvgpu::WGMMALayout::col\n                                               : triton::nvgpu::WGMMALayout::row;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 87, "deletions": 129, "changes": 216, "file_content_changes": "@@ -154,96 +154,14 @@ const std::string Fp16_to_Fp8E4M3B15x4 =\n     \"lop3.b32 $0, $0, $2, 0xbf80bf80, 0xf8;  \\n\"\n     \"}\";\n \n-/* ----- FP8E4M3 ------ */\n-// Note: when handled by software, this format\n-// does not handle denormals and has\n-// more than a single NaN values.\n-\n-// Fp8E4M3 -> Fp16 (packed)\n-const std::string Fp8E4M3_to_Fp16 =\n-    \"{                                      \\n\"\n-    \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n-    \"prmt.b32 a0, 0, $2, 0x0504;            \\n\" // a0 = 0x00f300f4\n-    \"prmt.b32 a1, 0, $2, 0x0706;            \\n\" // a1 = 0x00f100f2\n-    \"and.b32  b0, a0, 0x00800080;           \\n\" // b0 = a0 & 0x00800080\n-    \"and.b32  b1, a1, 0x00800080;           \\n\" // (extract sign)\n-    \"add.u32  b0, b0, a0;                   \\n\" // b0 = b0 + a0\n-    \"add.u32  b1, b1, a1;                   \\n\" // (move sign to the left)\n-    \"mad.lo.u32 $0, b0, 128, 0x20002000;    \\n\" // out0 = (b0 << 7) + 0x20002000\n-    \"mad.lo.u32 $1, b1, 128, 0x20002000;    \\n\" // (shift into position and bias\n-                                                // exponent)\n-    \"}\";\n-\n-// Fp16 -> Fp8E4M3 (packed)\n-const std::string Fp16_to_Fp8E4M3 =\n-    \"{                                      \\n\"\n-    \".reg .b32 a<2>, b<2>;                  \\n\" // see Fp8E4M3x4ToFp16x4\n-    \"and.b32 a0, $1, 0x7fff7fff;            \\n\" // a0 = input0 & 0x7fff7fff\n-    \"and.b32 a1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n-    \"mad.lo.u32 a0, a0, 2, 0x40804080;      \\n\" // shift exponent (<< 1),\n-    \"mad.lo.u32 a1, a1, 2, 0x40804080;      \\n\" // correct bias (0x40004000),\n-                                                // and round to nearest\n-    \"lop3.b32 b0, $1, 0x80008000, a0, 0xe2; \\n\" // b0 = 0x80008000 ? in0 : a0\n-    \"lop3.b32 b1, $2, 0x80008000, a1, 0xe2; \\n\" // (restore sign)\n-    \"prmt.b32 $0, b0, b1, 0x7531;           \\n\" // output = b1b0\n-    \"}\";\n-\n-// WARN: subnormal (0bs0000xxx) are not handled\n-const std::string Fp8E4M3_to_Bf16 =\n-    \"{                                      \\n\"\n-    \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n-    \"prmt.b32 a0, 0, $2, 0x0504;            \\n\" // a0 = 0x00f300f4\n-    \"prmt.b32 a1, 0, $2, 0x0706;            \\n\" // a1 = 0x00f100f2\n-    \"and.b32  b0, a0, 0x00800080;           \\n\" // b0 = a0 & 0x00800080\n-    \"and.b32  b1, a1, 0x00800080;           \\n\" // (extract sign)\n-    \"mad.lo.u32 b0, b0, 15, a0;             \\n\" // b0 = b0 * 15 + a0\n-    \"mad.lo.u32 b1, b1, 15, a1;             \\n\" // (move sign to the left)\n-    \"mad.lo.u32 $0, b0, 16, 0x3c003c00;     \\n\" // out0 = (b0 << 4) + 0x3c003c00\n-    \"mad.lo.u32 $1, b1, 16, 0x3c003c00;     \\n\" // (shift into position and bias\n-                                                // exponent)\n-    \"}\";\n-\n-const std::string Bf16_to_Fp8E4M3 =\n-    \"{                                           \\n\" // bf16=fp8>>4 + 120<<7\n-    \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\" // fp8_min = 0b00000000\n-    \".reg .u32 fp8_min, fp8_max, rn_;            \\n\" // fp8_max = 0b11111111\n-    \"mov.u32 fp8_min, 0x3c003c00;                \\n\" // so bf16_min = 0x3c00\n-    \"mov.u32 fp8_max, 0x43f043f0;                \\n\" // so bf16_max = 0x43f0\n-    \"mov.u32 rn_, 0x80008;                       \\n\" // round to nearest\n-    \"and.b32 sign0, $1, 0x80008000;              \\n\" // sign0=in0&0x80008000\n-    \"and.b32 sign1, $2, 0x80008000;              \\n\" // (store sign)\n-    \"prmt.b32 sign, sign0, sign1, 0x7531;        \\n\"\n-    \"and.b32 nosign0, $1, 0x7fff7fff;            \\n\" // nosign0=in0&0x7fff7fff\n-    \"and.b32 nosign1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n-\n-    // nosign = clamp(nosign, min, max)\n-    \".reg .u32 nosign_0_<2>, nosign_1_<2>;       \\n\"\n-    \"and.b32 nosign_0_0, nosign0, 0xffff0000;    \\n\"\n-    \"max.u32 nosign_0_0, nosign_0_0, 0x3c000000; \\n\"\n-    \"min.u32 nosign_0_0, nosign_0_0, 0x43f00000; \\n\"\n-    \"and.b32 nosign_0_1, nosign0, 0x0000ffff;    \\n\"\n-    \"max.u32 nosign_0_1, nosign_0_1, 0x3c00;     \\n\"\n-    \"min.u32 nosign_0_1, nosign_0_1, 0x43f0;     \\n\"\n-    \"or.b32 nosign0, nosign_0_0, nosign_0_1;     \\n\"\n-    \"and.b32 nosign_1_0, nosign1, 0xffff0000;    \\n\"\n-    \"max.u32 nosign_1_0, nosign_1_0, 0x3c000000; \\n\"\n-    \"min.u32 nosign_1_0, nosign_1_0, 0x43f00000; \\n\"\n-    \"and.b32 nosign_1_1, nosign1, 0x0000ffff;    \\n\"\n-    \"max.u32 nosign_1_1, nosign_1_1, 0x3c00;     \\n\"\n-    \"min.u32 nosign_1_1, nosign_1_1, 0x43f0;     \\n\"\n-    \"or.b32 nosign1, nosign_1_0, nosign_1_1;     \\n\"\n-\n-    \"add.u32 nosign0, nosign0, rn_;              \\n\" // nosign0 += rn_\n-    \"add.u32 nosign1, nosign1, rn_;              \\n\" // (round to nearest)\n-    \"sub.u32 nosign0, nosign0, 0x3c003c00;       \\n\" // nosign0-=0x3c003c00\n-    \"sub.u32 nosign1, nosign1, 0x3c003c00;       \\n\" // (compensate offset)\n-    \"shr.u32 nosign0, nosign0, 4;                \\n\" // nosign0 >>= 4\n-    \"shr.u32 nosign1, nosign1, 4;                \\n\" // shift into to fp8e4\n-    \"prmt.b32 nosign, nosign0, nosign1, 0x6420;  \\n\" // nosign0 = 0x00f100f2\n-                                                     // nosign1 = 0x00f300f4\n-                                                     // nosign = 0xf3f4f1f2\n-    \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n-    \"}\";\n+// Fp8E4M3 (x2) -> Fp16 (x2) (packed)\n+const std::string Fp8E4M3Nv_to_Fp16 = \"{ \\n\"\n+                                      \"cvt.rn.f16x2.e4m3x2 $0, $1; \\n\"\n+                                      \"}\";\n+// Fp16 (x2) -> Fp8E4M3 (x2) (packed)\n+const std::string Fp16_to_Fp8E4M3Nv = \"{ \\n\"\n+                                      \"cvt.rn.satfinite.e4m3x2.f16x2 $0, $1; \\n\"\n+                                      \"}\";\n \n /* ----- Packed integer to BF16 ------ */\n const std::string S8_to_Bf16 =\n@@ -391,40 +309,49 @@ inline SmallVector<Value> packI32(const SmallVector<Value> &inValues,\n }\n \n typedef std::function<SmallVector<Value>(Location, ConversionPatternRewriter &,\n-                                         const Value &, const Value &,\n-                                         const Value &, const Value &)>\n+                                         const SmallVector<Value> &)>\n     ConverterT;\n \n static ConverterT makeConverterFromPtx(const std::string &ptxAsm, Type inType,\n-                                       Type outType) {\n+                                       Type outType,\n+                                       const int inVecWidthBits = 32,\n+                                       const int outVecWidthBits = 32) {\n+\n+  ConverterT converter =\n+      [ptxAsm, inType, outType, inVecWidthBits,\n+       outVecWidthBits](Location loc, ConversionPatternRewriter &rewriter,\n+                        const SmallVector<Value> &v) -> SmallVector<Value> {\n+    int numElements = v.size();\n+    assert(numElements == 4 || numElements == 2 && \"invalid vector size\");\n \n-  ConverterT converter = [ptxAsm, inType, outType](\n-                             Location loc, ConversionPatternRewriter &rewriter,\n-                             const Value &v0, const Value &v1, const Value &v2,\n-                             const Value &v3) -> SmallVector<Value> {\n-    SmallVector<Value> v = {v0, v1, v2, v3};\n     auto ctx = rewriter.getContext();\n     int inBitwidth = inType.getIntOrFloatBitWidth();\n     int outBitwidth = outType.getIntOrFloatBitWidth();\n     // first, we pack `v` into 32-bit ints\n-    int inVecWidth = 32 / inBitwidth;\n+    int inVecWidth = inVecWidthBits / inBitwidth;\n     auto inVecTy = vec_ty(inType, inVecWidth);\n-    SmallVector<Value> inPacked(4 / inVecWidth, undef(inVecTy));\n-    for (size_t i = 0; i < 4; i++)\n+    SmallVector<Value> inPacked(numElements / inVecWidth, undef(inVecTy));\n+    for (size_t i = 0; i < numElements; i++)\n       inPacked[i / inVecWidth] = insert_element(\n           inVecTy, inPacked[i / inVecWidth], v[i], i32_val(i % inVecWidth));\n     for (size_t i = 0; i < inPacked.size(); i++)\n-      inPacked[i] = bitcast(inPacked[i], i32_ty);\n+      inPacked[i] = bitcast(inPacked[i], int_ty(inVecWidthBits));\n \n     // then, we run the provided inline PTX\n-    int outVecWidth = 32 / outBitwidth;\n-    int outNums = 4 / outVecWidth;\n+    int outVecWidth = outVecWidthBits / outBitwidth;\n+    int outNums = numElements / outVecWidth;\n     PTXBuilder builder;\n     SmallVector<PTXBuilder::Operand *> operands;\n-    for (int i = 0; i < outNums; i++)\n-      operands.push_back(builder.newOperand(\"=r\"));\n-    for (Value inVal : inPacked)\n-      operands.push_back(builder.newOperand(inVal, \"r\"));\n+    auto outConstriant = outVecWidthBits == 16 ? \"=h\" : \"=r\";\n+    auto inConstraint = inVecWidthBits == 16 ? \"h\" : \"r\";\n+    for (int i = 0; i < outNums; i++) {\n+      operands.push_back(builder.newOperand(outConstriant));\n+    }\n+\n+    for (Value inVal : inPacked) {\n+      operands.push_back(builder.newOperand(inVal, inConstraint));\n+    }\n+\n     auto &ptxOp = *builder.create(ptxAsm);\n     ptxOp(operands, /*onlyAttachMLIRArgs=*/true);\n     auto outVecTy = vec_ty(outType, outVecWidth);\n@@ -439,7 +366,7 @@ static ConverterT makeConverterFromPtx(const std::string &ptxAsm, Type inType,\n     }\n     // unpack the output\n     SmallVector<Value> ret;\n-    for (size_t i = 0; i < 4; i++)\n+    for (size_t i = 0; i < numElements; i++)\n       ret.push_back(extract_element(outType, outPacked[i / outVecWidth],\n                                     i32_val(i % outVecWidth)));\n     return ret;\n@@ -526,6 +453,9 @@ class ElementwiseOpConversionBase\n \n     return success();\n   }\n+\n+private:\n+  int computeCapability;\n };\n \n template <typename SourceOp, typename DestOp>\n@@ -538,11 +468,6 @@ struct ElementwiseOpConversion\n   using Base::Base;\n   using OpAdaptor = typename Base::OpAdaptor;\n \n-  explicit ElementwiseOpConversion(LLVMTypeConverter &typeConverter,\n-                                   PatternBenefit benefit = 1)\n-      : ElementwiseOpConversionBase<SourceOp, ElementwiseOpConversion>(\n-            typeConverter, benefit) {}\n-\n   // An interface to support variant DestOp builder.\n   SmallVector<DestOp> createDestOps(SourceOp op, OpAdaptor adaptor,\n                                     ConversionPatternRewriter &rewriter,\n@@ -559,6 +484,11 @@ struct FpToFpOpConversion\n   using ElementwiseOpConversionBase<\n       triton::FpToFpOp, FpToFpOpConversion>::ElementwiseOpConversionBase;\n \n+  explicit FpToFpOpConversion(TritonGPUToLLVMTypeConverter &typeConverter,\n+                              int computeCapability, PatternBenefit benefit = 1)\n+      : ElementwiseOpConversionBase(typeConverter, benefit),\n+        computeCapability(computeCapability) {}\n+\n   static Value convertBf16ToFp32(Location loc,\n                                  ConversionPatternRewriter &rewriter,\n                                  const Value &v) {\n@@ -618,58 +548,83 @@ struct FpToFpOpConversion\n         // F8 -> F16\n         {{F8E4M3B15TyID, F16TyID}, Fp8E4M3B15_to_Fp16},\n         {{F8E4M3FNTyID, F16TyID}, Fp8E4M3B15x4_to_Fp16},\n-        {{F8E4M3TyID, F16TyID}, Fp8E4M3_to_Fp16},\n+        {{F8E4M3TyID, F16TyID}, Fp8E4M3Nv_to_Fp16},\n         {{F8E5M2TyID, F16TyID}, Fp8E5M2_to_Fp16},\n         // F16 -> F8\n         {{F16TyID, F8E4M3B15TyID}, Fp16_to_Fp8E4M3B15},\n         {{F16TyID, F8E4M3FNTyID}, Fp16_to_Fp8E4M3B15x4},\n-        {{F16TyID, F8E4M3TyID}, Fp16_to_Fp8E4M3},\n+        {{F16TyID, F8E4M3TyID}, Fp16_to_Fp8E4M3Nv},\n         {{F16TyID, F8E5M2TyID}, Fp16_to_Fp8E5M2},\n         // F8 -> BF16\n-        {{F8E4M3TyID, BF16TyID}, Fp8E4M3_to_Bf16},\n         {{F8E5M2TyID, BF16TyID}, Fp8E5M2_to_Bf16},\n         // BF16 -> F8\n-        {{BF16TyID, F8E4M3TyID}, Bf16_to_Fp8E4M3},\n         {{BF16TyID, F8E5M2TyID}, Bf16_to_Fp8E5M2},\n     };\n+    int inVecWidthBits = 32;\n+    int outVecWidthBits = 32;\n+    if (srcTy.isFloat8E4M3FNUZ()) {\n+      inVecWidthBits = 16;\n+      outVecWidthBits = 32;\n+    }\n+    if (dstTy.isFloat8E4M3FNUZ()) {\n+      inVecWidthBits = 32;\n+      outVecWidthBits = 16;\n+    }\n \n     std::pair<TypeID, TypeID> key = {srcTy.getTypeID(), dstTy.getTypeID()};\n     if (srcMap.count(key) == 0) {\n       llvm::errs() << \"Unsupported conversion from \" << srcTy << \" to \" << dstTy\n                    << \"\\n\";\n       llvm_unreachable(\"\");\n     }\n+    if (computeCapability < 90 &&\n+        (srcTy.isFloat8E4M3FNUZ() || dstTy.isFloat8E4M3FNUZ())) {\n+      llvm::errs() << \"Conversion from/to f8e4m3nv is only supported on \"\n+                      \"compute capability >= 90\"\n+                   << \"\\n\";\n+      llvm_unreachable(\"\");\n+    }\n     return makeConverterFromPtx(srcMap.lookup(key),\n                                 getTypeConverter()->convertType(srcTy),\n-                                getTypeConverter()->convertType(dstTy));\n+                                getTypeConverter()->convertType(dstTy),\n+                                inVecWidthBits, outVecWidthBits);\n   }\n \n   SmallVector<Value> createDestOps(triton::FpToFpOp op, OpAdaptor adaptor,\n                                    ConversionPatternRewriter &rewriter,\n                                    Type elemTy, MultipleOperandsRange operands,\n                                    Location loc) const {\n-    assert(operands.size() % 4 == 0 &&\n-           \"FP8 casting only support tensors with 4-aligned sizes\");\n     auto srcElementType = getElementType(op.getFrom());\n     auto dstElementType = getElementType(op.getResult());\n+    int numElements = 4;\n+    if (srcElementType.isFloat8E4M3FNUZ() ||\n+        dstElementType.isFloat8E4M3FNUZ()) {\n+      numElements = 2;\n+    }\n+    assert(operands.size() % numElements == 0 &&\n+           \"FP8 casting only support tensors with aligned sizes\");\n     bool isSrcFP32 = srcElementType.isF32();\n     bool isDstFP32 = dstElementType.isF32();\n     auto cvtFunc = getConversionFunc(isSrcFP32 ? f16_ty : srcElementType,\n                                      isDstFP32 ? f16_ty : dstElementType);\n-    SmallVector<Value> inVals = {operands[0][0], operands[1][0], operands[2][0],\n-                                 operands[3][0]};\n+    SmallVector<Value> inVals;\n+    for (unsigned i = 0; i < numElements; i++) {\n+      inVals.push_back(operands[i][0]);\n+    }\n     if (isSrcFP32)\n       for (Value &v : inVals)\n         v = convertFp32ToFp16(loc, rewriter, v);\n-    SmallVector<Value> outVals =\n-        cvtFunc(loc, rewriter, inVals[0], inVals[1], inVals[2], inVals[3]);\n+    SmallVector<Value> outVals = cvtFunc(loc, rewriter, inVals);\n     assert(outVals.size() == inVals.size());\n     if (isDstFP32)\n       for (Value &v : outVals)\n         v = convertFp16ToFp32(loc, rewriter, v);\n     // Pack values\n     return outVals;\n   }\n+\n+private:\n+  int computeCapability;\n };\n \n struct CmpIOpConversion\n@@ -1038,8 +993,9 @@ struct SIToFPOpConversion\n       auto cvtFunc = makeConverterFromPtx(\n           S8_to_Bf16, getTypeConverter()->convertType(inElemTy),\n           getTypeConverter()->convertType(outElemTy));\n-      auto outVals = cvtFunc(loc, rewriter, operands[0][0], operands[1][0],\n-                             operands[2][0], operands[3][0]);\n+      SmallVector<Value> inVals = {operands[0][0], operands[1][0],\n+                                   operands[2][0], operands[3][0]};\n+      auto outVals = cvtFunc(loc, rewriter, inVals);\n       assert(outVals.size() == 4);\n       return outVals;\n     } else if (outElemTy.isBF16()) {\n@@ -1224,7 +1180,9 @@ struct IndexCastOpLowering\n void populateElementwiseOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n-    ModuleAllocation &allocation, PatternBenefit benefit) {\n+    ModuleAllocation &allocation,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    int computeCapability, PatternBenefit benefit) {\n #define POPULATE_TERNARY_OP(SRC_OP, DST_OP)                                    \\\n   patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n   POPULATE_TERNARY_OP(triton::gpu::SelectOp, LLVM::SelectOp)\n@@ -1288,7 +1246,7 @@ void populateElementwiseOpToLLVMPatterns(\n   patterns.add<SIToFPOpConversion>(typeConverter, benefit);\n   patterns.add<IndexCastOpLowering>(typeConverter, benefit);\n \n-  patterns.add<FpToFpOpConversion>(typeConverter, benefit);\n+  patterns.add<FpToFpOpConversion>(typeConverter, computeCapability, benefit);\n \n   patterns.add<ExternElementwiseOpConversion>(typeConverter, benefit);\n   patterns.add<ElementwiseInlineAsmOpConversion>(typeConverter, benefit);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.h", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -9,7 +9,9 @@ using namespace mlir::triton;\n void populateElementwiseOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n-    ModuleAllocation &allocation, PatternBenefit benefit);\n+    ModuleAllocation &allocation,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    int computeCapability, PatternBenefit benefit);\n \n bool isLegalElementwiseOp(Operation *op);\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -528,7 +528,7 @@ struct ConvertTritonGPUToLLVM\n     populatePatterns1(populateTritonGPUToLLVMPatterns);\n     populatePatterns1(populateConvertLayoutOpToLLVMPatterns);\n     populatePatterns2(populateDotOpToLLVMPatterns);\n-    populatePatterns2(populateElementwiseOpToLLVMPatterns);\n+    populatePatterns4(populateElementwiseOpToLLVMPatterns);\n     populatePatterns3(populateLoadStoreOpToLLVMPatterns);\n     populatePatterns4(populateReduceOpToLLVMPatterns);\n     populatePatterns1(populateScanOpToLLVMPatterns);\n@@ -837,9 +837,9 @@ struct ConvertTritonGPUToLLVM\n                                       .dyn_cast<MmaEncodingAttr>();\n       if (mmaLayout) {\n         bool isNativeHopperFP8 =\n-            AElType.isFloat8E5M2() || AElType.isFloat8E4M3FN();\n+            AElType.isFloat8E5M2() || AElType.isFloat8E4M3FNUZ();\n         bool isFP8 = isNativeHopperFP8 || AElType.isFloat8E5M2FNUZ() ||\n-                     AElType.isFloat8E4M3FNUZ();\n+                     AElType.isFloat8E4M3FN();\n         if (!isFP8 || (isNativeHopperFP8 && mmaLayout.isHopper()))\n           return;\n         promoteType = builder.getF16Type();"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -401,8 +401,10 @@ mlir::LogicalResult mlir::triton::DotOp::inferReturnTypes(\n LogicalResult mlir::triton::DotOp::verify() {\n   auto aTy = getOperand(0).getType().cast<RankedTensorType>();\n   auto bTy = getOperand(1).getType().cast<RankedTensorType>();\n-  if (aTy.getElementType() != bTy.getElementType())\n-    return emitError(\"element types of operands A and B must match\");\n+  if (aTy.getElementType().getIntOrFloatBitWidth() !=\n+      bTy.getElementType().getIntOrFloatBitWidth())\n+    return emitError(\n+        \"element types of operands A and B must have same bit width\");\n   auto aEncoding = aTy.getEncoding();\n   auto bEncoding = bTy.getEncoding();\n   if (!aEncoding && !bEncoding)"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -26,8 +26,8 @@ SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n     SmallVector<unsigned> validN;\n \n     // MMAv3 with larger instruction shape is preferred.\n-    if (eltType.isFloat8E5M2() || eltType.isFloat8E4M3FN() || eltType.isF16() ||\n-        eltType.isBF16() || eltType.isF32()) {\n+    if (eltType.isFloat8E5M2() || eltType.isFloat8E4M3FNUZ() ||\n+        eltType.isF16() || eltType.isBF16() || eltType.isF32()) {\n       validN.assign({256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176,\n                      168, 160, 152, 144, 136, 128, 120, 112, 104, 96,  88,\n                      80,  72,  64,  56,  48,  40,  32,  24,  16,  8});"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -760,7 +760,7 @@ void init_triton_ir(py::module &&m) {\n            [](TritonOpBuilder &self) -> mlir::Type {\n              return self.getBuilder().getI64Type();\n            })\n-      .def(\"get_fp8e4_ty\",\n+      .def(\"get_fp8e4nv_ty\",\n            [](TritonOpBuilder &self) -> mlir::Type {\n              return self.getBuilder().getType<mlir::Float8E4M3FNUZType>();\n            })"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 7, "deletions": 4, "changes": 11, "file_content_changes": "@@ -123,6 +123,8 @@ def check_type_supported(dtype, device):\n         cc = torch.cuda.get_device_capability()\n         if cc[0] < 8 and (dtype is tl.bfloat16 or dtype == \"bfloat16\" or dtype is torch.bfloat16):\n             pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n+        if cc[0] < 9 and (dtype is tl.float8e4nv or dtype == \"float8e4\"):\n+            pytest.skip(\"float8e4 is only supported on NVGPU with cc >= 90\")\n \n \n class MmaLayout:\n@@ -847,7 +849,7 @@ def test_abs(dtype_x, device):\n     _test_unary(dtype_x, 'tl.abs(x)', 'np.abs(x) ', device=device)\n \n \n-@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4, tl.float8e5])\n+@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4nv, tl.float8e5])\n def test_abs_fp8(in_dtype, device):\n \n     @triton.jit\n@@ -1379,8 +1381,8 @@ def convert_float_to_float32(fp: torch.tensor, dtype=None):\n \n     extended_exp = ((1 << (tl.float32.primitive_bitwidth - tl.float32.fp_mantissa_width - 1)) - 1) << tl.float32.fp_mantissa_width\n     # special cases, exp is 0b11..1\n-    if dtype in [tl.float8e4, tl.float8e4b15]:\n-        # float8e4m3 does not have infinities\n+    if dtype in [tl.float8e4nv, tl.float8e4b15]:\n+        # float8e4m3nv does not have infinities\n         output[fp == 0b01111111] = torch.nan\n         output[fp == 0b11111111] = torch.nan\n     else:\n@@ -1439,7 +1441,7 @@ def deserialize_fp8(np_data, in_dtype):\n         return np_data\n \n \n-@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4b15x4, tl.float8e4, tl.float8e5])\n+@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4b15x4, tl.float8e4nv, tl.float8e5])\n @pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.float32])\n def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n     \"\"\"\n@@ -1448,6 +1450,7 @@ def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n         - conversion tri_fp8 = convert(input=tri_fp16, out=out_dtype) matches the original\n     this is only possible if both conversions are correct\n     \"\"\"\n+    check_type_supported(in_dtype, device)\n     check_type_supported(out_dtype, device)\n \n     @triton.jit"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 9, "deletions": 4, "changes": 13, "file_content_changes": "@@ -81,8 +81,11 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n                 (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n                 (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n-            ] for ADTYPE, BDTYPE in [(\"float8e4\", \"float8e5\"),\n-                                     (\"float8e4\", \"float16\"),\n+            ] for ADTYPE, BDTYPE in [(\"float8e4nv\", \"float8e5\"),\n+                                     (\"float8e4nv\", \"float8e4nv\"),\n+                                     (\"float8e5\", \"float8e4nv\"),\n+                                     (\"float8e5\", \"float8e5\"),\n+                                     (\"float8e4nv\", \"float16\"),\n                                      (\"float16\", \"float8e5\"),\n                                      (\"float16\", \"float32\"),\n                                      (\"float32\", \"float16\"),\n@@ -97,7 +100,7 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                                      (\"float8e4b15\", \"float16\"),\n                                      (\"float16\", \"float8e4b15\"),\n                                      (\"float8e5\", \"float8e5\"),\n-                                     (\"float8e4\", \"float8e4\"),\n+                                     (\"float8e4nv\", \"float8e4nv\"),\n                                      (\"int8\", \"int8\")]\n         ]\n     ),\n@@ -108,6 +111,8 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT,\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n     if capability[0] < 8 and (ADTYPE == \"bfloat16\" or BDTYPE == \"bfloat16\"):\n         pytest.skip(\"Only test bfloat16 on devices with sm >= 80\")\n+    if capability[0] < 9 and (ADTYPE == \"float8e4nv\" or BDTYPE == \"float8e4nv\"):\n+        pytest.skip(\"Only test float8e4nv on devices with sm >= 90\")\n     if (ADTYPE == \"bfloat16\" or BDTYPE == \"bfloat16\") and SPLIT_K != 1:\n         pytest.skip(\"bfloat16 matmuls don't allow split_k for now\")\n     torch.manual_seed(0)\n@@ -131,7 +136,7 @@ def maybe_upcast(x, dtype, is_float8):\n \n     def init_input(m, n, dtype):\n         if 'float8' in dtype:\n-            ewidth = {'float8e4b15': 4, 'float8e4': 4, 'float8e5': 5}[dtype]\n+            ewidth = {'float8e4b15': 4, 'float8e4nv': 4, 'float8e5': 5}[dtype]\n             sign = torch.randint(2, size=(m, n), device=\"cuda\", dtype=torch.int8) * 128\n             val = torch.randint(2**3 - 1, size=(m, n), device=\"cuda\", dtype=torch.int8) << 7 - ewidth\n             return sign | val"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1062,7 +1062,7 @@ def str_to_ty(name):\n         ty = str_to_ty(name[1:])\n         return language.pointer_type(ty)\n     tys = {\n-        \"fp8e4\": language.float8e4,\n+        \"fp8e4nv\": language.float8e4nv,\n         \"fp8e5\": language.float8e5,\n         \"fp8e4b15\": language.float8e4b15,\n         \"fp8e4b15x4\": language.float8e4b15x4,"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -51,7 +51,7 @@\n     float64,\n     float8e4b15,\n     float8e4b15x4,\n-    float8e4,\n+    float8e4nv,\n     float8e5,\n     function_type,\n     inline_asm_elementwise,\n@@ -151,7 +151,7 @@\n     \"float64\",\n     \"float8e4b15\",\n     \"float8e4b15x4\",\n-    \"float8e4\",\n+    \"float8e4nv\",\n     \"float8e5\",\n     \"full\",\n     \"function_type\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 10, "deletions": 7, "changes": 17, "file_content_changes": "@@ -76,7 +76,7 @@ def _to_tensor(x, builder):\n class dtype:\n     SINT_TYPES = ['int8', 'int16', 'int32', 'int64']\n     UINT_TYPES = ['int1', 'uint8', 'uint16', 'uint32', 'uint64']\n-    FP_TYPES = ['fp8e4b15', 'fp8e4b15x4', 'fp8e4', 'fp8e5', 'fp16', 'bf16', 'fp32', 'fp64']\n+    FP_TYPES = ['fp8e4b15', 'fp8e4b15x4', 'fp8e4nv', 'fp8e5', 'fp16', 'bf16', 'fp32', 'fp64']\n     STANDARD_FP_TYPES = ['fp16', 'bf16', 'fp32', 'fp64']\n     OTHER_TYPES = ['void']\n \n@@ -104,7 +104,7 @@ def __init__(self, name):\n                 self.fp_mantissa_width = 3\n                 self.primitive_bitwidth = 8\n                 self.exponent_bias = 15\n-            elif name == 'fp8e4':\n+            elif name == 'fp8e4nv':\n                 self.fp_mantissa_width = 3\n                 self.primitive_bitwidth = 8\n                 self.exponent_bias = 7\n@@ -136,15 +136,18 @@ def __init__(self, name):\n     def is_fp8(self):\n         return 'fp8' in self.name\n \n-    def is_fp8e4(self):\n-        return self.name == 'fp8e4'\n+    def is_fp8e4nv(self):\n+        return self.name == 'fp8e4nv'\n \n     def is_fp8e4b15(self):\n         return self.name == 'fp8e4b15'\n \n     def is_fp8e4b15x4(self):\n         return self.name == 'fp8e4b15x4'\n \n+    def is_fp8e5(self):\n+        return self.name == 'fp8e5'\n+\n     def is_fp16(self):\n         return self.name == 'fp16'\n \n@@ -244,8 +247,8 @@ def to_ir(self, builder: ir.builder) -> ir.type:\n             return builder.get_int64_ty()\n         elif self.name == 'fp8e5':\n             return builder.get_fp8e5_ty()\n-        elif self.name == 'fp8e4':\n-            return builder.get_fp8e4_ty()\n+        elif self.name == 'fp8e4nv':\n+            return builder.get_fp8e4nv_ty()\n         elif self.name == 'fp8e4b15':\n             return builder.get_fp8e4b15_ty()\n         elif self.name == 'fp8e4b15x4':\n@@ -382,7 +385,7 @@ def to_ir(self, builder: ir.builder):\n uint32 = dtype('uint32')\n uint64 = dtype('uint64')\n float8e5 = dtype('fp8e5')\n-float8e4 = dtype('fp8e4')\n+float8e4nv = dtype('fp8e4nv')\n float8e4b15 = dtype('fp8e4b15')\n float8e4b15x4 = dtype('fp8e4b15x4')\n float16 = dtype('fp16')"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 28, "deletions": 5, "changes": 33, "file_content_changes": "@@ -1,6 +1,5 @@\n from __future__ import annotations  # remove after python 3.11\n \n-import warnings\n from functools import wraps\n from typing import List, Optional, Sequence, Tuple, TypeVar\n \n@@ -685,9 +684,8 @@ def cast(input: tl.tensor,\n     dst_sca_ty = dst_ty.scalar\n \n     if _is_cuda(builder.arch) and builder.arch < 89 and \\\n-       (src_sca_ty.is_fp8e4() or dst_sca_ty.is_fp8e4()):\n-        warnings.warn(\"Standard tl.float8e4 format will be deprecated on SM < 89. \"\n-                      \"Please use tl.float8e4b15.\", DeprecationWarning)\n+       (src_sca_ty.is_fp8e4nv() or dst_sca_ty.is_fp8e4nv()):\n+        assert False, \"fp8e4nv data type is not supported on CUDA arch < 89\"\n \n     # Casting with customized floating types involved: fp8 <=> bf16, fp16, fp32, fp64\n     if (src_sca_ty.is_fp8() and dst_sca_ty.is_floating()) or \\\n@@ -1246,8 +1244,33 @@ def dot(lhs: tl.tensor,\n         allow_tf32: bool,\n         out_dtype: tl.dtype,\n         builder: ir.builder) -> tl.tensor:\n+    def assert_dtypes_valid(lhs_dtype, rhs_dtype, arch):\n+        # Checks for non-cuda archs\n+        if not _is_cuda(builder.arch):\n+            assert lhs_dtype == rhs_dtype, f\"First input ({lhs_dtype}) and second input ({rhs_dtype}) must have the same dtype!\"\n+            return\n+        # Checks for cuda arch\n+        if arch < 90:\n+            assert not lhs_dtype.is_fp8e4nv() and not rhs_dtype.is_fp8e4nv(), \"Dot op does not support fp8e4nv on CUDA arch < 90\"\n+            assert lhs_dtype == rhs_dtype, f\"First input ({lhs_dtype}) and second input ({rhs_dtype}) must have the same dtype!\"\n+        else:\n+            assert not lhs_dtype.is_fp8e4b15() and not rhs_dtype.is_fp8e4b15(), \"Dot op does not support fp8e4b15 on CUDA arch >= 90\"\n+            assert not lhs_dtype.is_fp8e4b15x4() and not rhs_dtype.is_fp8e4b15x4(), \"Dot op does not support fp8e4b15x4 on CUDA arch >= 90\"\n+            if lhs_dtype.is_int() or rhs_dtype.is_int():\n+                assert lhs_dtype == rhs_dtype, f\"Both operands must be same type. First operand ({lhs_dtype}) and second operand ({rhs_dtype})\"\n+                assert lhs_dtype.is_int8() or lhs_dtype.is_uint8(), f\"Both operands must be either int8 or uint8. Operand type ({lhs_dtype})\"\n+            elif lhs_dtype.is_fp8() or rhs_dtype.is_fp8():\n+                assert lhs_dtype.is_fp8e4nv() or lhs_dtype.is_fp8e5(), f\"Only supports fp8e4nv or fp8e5. First operand ({lhs_dtype})\"\n+                assert rhs_dtype.is_fp8e4nv() or rhs_dtype.is_fp8e5(), f\"Only supports fp8e4nv or fp8e5. Second operand ({rhs_dtype})\"\n+            else:\n+                assert lhs_dtype.is_fp16() or lhs_dtype.is_bf16() or lhs_dtype.is_fp32() or lhs_dtype.is_int1(), f\"Unsupported dtype {lhs_dtype}\"\n+                assert rhs_dtype.is_fp16() or rhs_dtype.is_bf16() or rhs_dtype.is_fp32() or rhs_dtype.is_int1(), f\"Unsupported dtype {rhs_dtype}\"\n+                assert lhs_dtype == rhs_dtype, f\"First input ({lhs_dtype}) and second input ({rhs_dtype}) must have the same dtype!\"\n+\n     assert lhs.type.is_block() and rhs.type.is_block()\n-    assert lhs.dtype == rhs.dtype, f\"First input ({lhs.dtype}) and second input ({rhs.dtype}) must have the same dtype!\"\n+\n+    assert_dtypes_valid(lhs.dtype, rhs.dtype, builder.arch)\n+\n     assert len(lhs.shape) == 2, f\"First input shape ({lhs.shape}) is not two dimensional!\"\n     assert len(rhs.shape) == 2, f\"Second input shape ({rhs.shape}) is not two dimensional!\"\n     assert lhs.shape[1].value == rhs.shape[0].value, f\"First input shape ({lhs.shape}) and second input shape {rhs.shape} are not compatible for matmul (second index of first shape ({lhs.shape[1].value}) must be equal to first index of second shape ({rhs.shape[0].value})\""}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 8, "deletions": 5, "changes": 13, "file_content_changes": "@@ -82,7 +82,7 @@ def _kernel(A, B, C, M, N, K,\n             stride_cm, stride_cn,\n             dot_out_dtype: tl.constexpr,\n             BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n-            GROUP_M: tl.constexpr, SPLIT_K: tl.constexpr, EVEN_K: tl.constexpr,\n+            GROUP_M: tl.constexpr, SPLIT_K: tl.constexpr, EVEN_K: tl.constexpr, AB_DTYPE: tl.constexpr\n             ):\n     # matrix multiplication\n     pid = tl.program_id(0)\n@@ -114,7 +114,7 @@ def _kernel(A, B, C, M, N, K,\n             _0 = tl.zeros((1, 1), dtype=C.dtype.element_ty)\n             a = tl.load(A, mask=rk[None, :] < k_remaining, other=_0)\n             b = tl.load(B, mask=rk[:, None] < k_remaining, other=_0)\n-        if a.dtype != b.dtype:\n+        if AB_DTYPE:\n             a = a.to(C.dtype.element_ty)\n             b = b.to(C.dtype.element_ty)\n         acc += tl.dot(a, b, out_dtype=dot_out_dtype)\n@@ -151,8 +151,8 @@ def _call(a, b, dot_out_dtype):\n         M, K = a.shape\n         _, N = b.shape\n         # allocates output\n-        if a.dtype in [tl.float8e4, tl.float8e4b15, tl.float8e5] or\\\n-           b.dtype in [tl.float8e4, tl.float8e4b15, tl.float8e5]:\n+        if a.dtype in [tl.float8e4nv, tl.float8e4b15, tl.float8e5] or\\\n+           b.dtype in [tl.float8e4nv, tl.float8e4b15, tl.float8e5]:\n             c_dtype = torch.float16\n         else:\n             c_dtype = get_higher_dtype(a.dtype, b.dtype)\n@@ -170,14 +170,17 @@ def _call(a, b, dot_out_dtype):\n                 dot_out_dtype = tl.float32\n             else:\n                 dot_out_dtype = tl.int32\n+        ab_dtype = True\n+        if a.dtype in [tl.float8e4nv, tl.float8e5] and b.dtype in [tl.float8e4nv, tl.float8e5]:\n+            ab_dtype = False\n         # launch kernel\n         grid = lambda META: (cdiv(M, META['BLOCK_M']) * cdiv(N, META['BLOCK_N']), META['SPLIT_K'])\n         _kernel[grid](a, b, c, M, N, K,\n                       a.stride(0), a.stride(1),\n                       b.stride(0), b.stride(1),\n                       c.stride(0), c.stride(1),\n                       dot_out_dtype=dot_out_dtype,\n-                      GROUP_M=8)\n+                      GROUP_M=8, AB_DTYPE=ab_dtype)\n         return c\n \n     @staticmethod"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -245,7 +245,7 @@ def _type_of(key):\n         dtype_str = str(key).split(\".\")[-1]\n         tys = {\n             \"bool\": \"i1\",\n-            \"float8e4\": \"fp8e4\",\n+            \"float8e4nv\": \"fp8e4nv\",\n             \"float8e5\": \"fp8e5\",\n             \"float8e4b15\": \"fp8e4b15\",\n             \"float8e4b15x4\": \"fp8e4b15x4\","}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -381,7 +381,7 @@ def get_max_tensorcore_tflops(dtype, backend=None, device=None, clock_rate=None)\n             ops_per_sub_core = 256\n         elif dtype in [torch.float16, torch.bfloat16, torch.int16]:\n             ops_per_sub_core = 512\n-        elif dtype in [torch.int8, tl.float8e4, tl.float8e4b15, tl.float8e5]:\n+        elif dtype in [torch.int8, tl.float8e4nv, tl.float8e4b15, tl.float8e5]:\n             ops_per_sub_core = 1024\n         else:\n             raise RuntimeError(\"dtype not supported\")"}, {"filename": "test/Conversion/invalid.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -5,7 +5,7 @@\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, kWidth=2}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   tt.func @convert_dot(%A: tensor<16x16xf32, #dot_operand_a>, %B: tensor<16x16xf16, #dot_operand_b>, %C: tensor<16x16xf32, #mma0>) {\n-    // expected-error@+1 {{element types of operands A and B must match}}\n+    // expected-error@+1 {{element types of operands A and B must have same bit width}}\n     %D = tt.dot %A, %B, %C {allowTF32 = true, transA = false, transB = false} :\n         tensor<16x16xf32, #dot_operand_a> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n     tt.return"}]