[{"filename": "include/triton/Dialect/NVGPU/IR/NVGPUOps.td", "status": "modified", "additions": 9, "deletions": 1, "changes": 10, "file_content_changes": "@@ -97,7 +97,15 @@ def WGMMADesc_ModeAttr : I32EnumAttr<\"WGMMADescMode\",\n }\n \n def NVGPU_WGMMADescCreateOp : NVGPU_Op<\"wgmma_desc_create\", []> {\n-  let arguments = (ins LLVM_AnyPointer:$buffer, I32:$height, WGMMADesc_ModeAttr:$mode);\n+  let arguments = (ins LLVM_AnyPointer:$buffer, I32:$height, WGMMADesc_ModeAttr:$mode, I64Attr:$swizzling);\n+  let builders = [\n+    OpBuilder<(ins \"Value\":$buffer,\n+                     \"Value\":$height,\n+                     \"WGMMADescMode\":$mode), [{\n+                      uint32_t mode_ = static_cast<uint32_t>(mode);\n+                      uint64_t swizzling = (mode_ == 1 ? 128 : mode_ == 2 ? 64 : 32);\n+                      build($_builder, $_state, $_builder.getIntegerType(64), buffer, height, WGMMADescModeAttr::get($_builder.getContext(), mode), $_builder.getI64IntegerAttr(swizzling));\n+                     }]>];\n   let results = (outs I64:$res);\n   let assemblyFormat = \"$buffer `,` $height attr-dict `:` functional-type(operands, results)\";\n }"}, {"filename": "lib/Conversion/NVGPUToLLVM/NVGPUToLLVMPass.cpp", "status": "modified", "additions": 779, "deletions": 867, "changes": 1646, "file_content_changes": "@@ -21,12 +21,245 @@ using ::mlir::LLVM::getSRegValue;\n \n namespace {\n \n+using OperandsAndConstraints = std::vector<std::pair<mlir::Value, std::string>>;\n+typedef std::vector<std::string> Constraints;\n+\n+const std::string Reg_Alloc_Op = \"setmaxnreg.inc.sync.aligned.u32 #regCount;\";\n+const std::string Wgmma_Fence_Op = \"wgmma.fence.sync.aligned;\";\n+const std::string Cga_Barrier_Sync_op = \"barrier.cluster.sync.aligned;\";\n+const std::string Wgmma_Commit_Group_Op = \"wgmma.commit_group.sync.aligned;\";\n+const std::string Wgmma_Wait_Group_Op =\n+    \"wgmma.wait_group.sync.aligned #pendings;\";\n+const std::string Cluster_Wait_Op = \"barrier.cluster.wait.aligned;\";\n+const std::string Fence_Mbarrier_Init_Op =\n+    \"fence.mbarrier_init.release.cluster;\";\n+const std::string Cga_Barrier_Arrive_Op = \"barrier.cluster.arrive;\";\n+const std::string Cga_Barrier_Wait_Op = \"barrier.cluster.wait;\";\n+const std::string Reg_Dealloc_Op = \"setmaxnreg.dec.sync.aligned.u32 #regCount;\";\n+const std::string Wgmma_Desc_Create_op =\n+    \"{\\n\"\n+    \".reg .u64 a<5>;                              \\n\"\n+    \"mov.u64 a0, #swizzling;\\n\"\n+    \"shl.b64 a1, a0, 3;\\n\"             // stride dimension\n+    \"shr.b64 a1, a1, 4;\\n\"             // stride dimension\n+    \"mul.lo.u64 a2, $2, #swizzling;\\n\" // leadingDimension\n+    \"shr.b64 a2, a2, 4;\\n\"             // leadingDimension\n+    \"shl.b64 a3, $1, 46; \\n\"           // startAddr\n+    \"shr.b64 a3, a3, 50; \\n\"           // startAddr\n+    \"mov.u64 a4, #mode; \\n\"            // mode\n+    \"shl.b64 a4, a4, 62; \\n\"\n+    \"shl.b64 a1, a1, 32; \\n\"\n+    \"or.b64 a1, a4, a1; \\n\"\n+    \"shl.b64 a2, a2, 16; \\n\"\n+    \"or.b64 a1, a1, a2; \\n\"\n+    \"or.b64 $0, a1, a3; \\n\"\n+    \"}\";\n+\n+const std::string Mbarrier_Init_Op =\n+    \"@$1 mbarrier.init.shared.b64 [$0], #count;\";\n+const std::string Mbarrier_Wait_Op =\n+    \"{                                                           \\n\"\n+    \".reg .pred P1;                                              \\n\"\n+    \"LAB_WAIT:                                                   \\n\"\n+    \"mbarrier.try_wait.parity.shared.b64 P1, [$0], $1, 0x989680; \\n\"\n+    \"@P1 bra.uni DONE;                                           \\n\"\n+    \"bra.uni LAB_WAIT;                                           \\n\"\n+    \"DONE:                                                       \\n\"\n+    \"}                                                           \\n\";\n+const std::string Named_Barrier_Arrive_Op = \"bar.arrive $0, $1;\";\n+const std::string Named_Barrier_Wait_Op = \"bar.sync $0, $1;\";\n+const std::string Sts64_Op = \"st.shared.v2.b32 [$0], {$1, $2};\";\n+const std::string Cluster_Cta_Id_Op = \"{\\n\"\n+                                      \".reg .u32 a<5>;              \\n\"\n+                                      \"mov.u32 a0, %cluster_ctaid.x;\\n\"  // x\n+                                      \"mov.u32 a1, %cluster_ctaid.y;\\n\"  // y\n+                                      \"mov.u32 a2, %cluster_ctaid.z;\\n\"  // z\n+                                      \"mov.u32 a3, %cluster_nctaid.x;\\n\" // nx\n+                                      \"mov.u32 a4, %cluster_nctaid.y;\\n\" // ny\n+                                      \"mad.lo.u32 a1, a2, a4, a1;     \\n\"\n+                                      \"mad.lo.u32 $0, a1, a3, a0;     \\n\"\n+                                      \"}\";\n+\n+bool isNumber(const std::string &s) {\n+  return !s.empty() && std::find_if(s.begin(), s.end(), [](unsigned char c) {\n+                         return !std::isdigit(c);\n+                       }) == s.end();\n+}\n+\n+Type getTypeFromConstraint(char constraint, mlir::PatternRewriter &rewriter) {\n+  Type ty;\n+  if (constraint == 'b')\n+    ty = IntegerType::get(rewriter.getContext(), 1);\n+  else if (constraint == 'h')\n+    ty = IntegerType::get(rewriter.getContext(), 16);\n+  else if (constraint == 'r')\n+    ty = IntegerType::get(rewriter.getContext(), 32);\n+  else if (constraint == 'l')\n+    ty = IntegerType::get(rewriter.getContext(), 64);\n+  else if (constraint == 'f')\n+    ty = FloatType::getF32(rewriter.getContext());\n+  else if (constraint == 'd')\n+    ty = FloatType::getF64(rewriter.getContext());\n+  else {\n+    assert(false && \"Unsupported constraint\");\n+  }\n+  return ty;\n+}\n+\n template <typename SourceOp, typename ConcreteT>\n class NVGPUOpPatternBase : public mlir::RewritePattern {\n public:\n   explicit NVGPUOpPatternBase(mlir::MLIRContext *context)\n       : mlir::RewritePattern(SourceOp::getOperationName(), 1, context) {}\n \n+  // Converts the given value to the type represented by the constraint\n+  // E.g. if val is of type llvmptr and constraint is 'r', then we convert\n+  // val to i32 using ptrtoint(i32_ty, val)\n+  mlir::Value convertToType(mlir::Value val, std::string constraint,\n+                            Location &loc,\n+                            mlir::PatternRewriter &rewriter) const {\n+    auto isConstraintNumber = isNumber(constraint);\n+    if (!isConstraintNumber) {\n+      auto ty = getTypeFromConstraint(constraint[0], rewriter);\n+      if (val.getType().isa<LLVM::LLVMPointerType>()) {\n+        return ptrtoint(ty, val);\n+      } else {\n+        assert(val.getType().getIntOrFloatBitWidth() <=\n+                   ty.getIntOrFloatBitWidth() &&\n+               \"Cannot convert to a smaller type\");\n+        return zext(ty, val);\n+      }\n+    }\n+    return val;\n+  }\n+\n+  SmallVector<PTXBuilder::Operand *>\n+  getPtxOutputs(std::vector<std::string> &outputConstraints,\n+                PTXBuilder &ptxBuilder) const {\n+    SmallVector<PTXBuilder::Operand *> ptxOutputs;\n+    for (unsigned i = 0; i < outputConstraints.size(); i++) {\n+      auto *ptxOutput = ptxBuilder.newOperand(outputConstraints[i]);\n+      ptxOutputs.push_back(ptxOutput);\n+    }\n+    return ptxOutputs;\n+  }\n+\n+  OperandsAndConstraints\n+  unpackOperands(OperandsAndConstraints &operandsAndConstraints,\n+                 PTXBuilder &ptxBuilder, Location &loc,\n+                 mlir::PatternRewriter &rewriter) const {\n+    OperandsAndConstraints unpackedOperands;\n+    for (auto &[operand, constraint] : operandsAndConstraints) {\n+      auto llvmStruct = llvm::dyn_cast<LLVM::LLVMStructType>(operand.getType());\n+      // if a constraint is a number, then we are doing input/output tying\n+      // if the operand is a struct, then we need to unpack it, and\n+      // add the constraint to each of the unpacked operands uses the constraint\n+      // as an offset\n+      auto isConstraintNumber = isNumber(constraint);\n+      if (llvmStruct) {\n+        for (unsigned i = 0; i < llvmStruct.getBody().size(); i++) {\n+          if (isConstraintNumber) {\n+            auto constraintInt = std::stoi(constraint) + i;\n+            unpackedOperands.push_back(\n+                {extract_val(llvmStruct.getBody()[i], operand, i),\n+                 std::to_string(constraintInt)});\n+          } else {\n+            unpackedOperands.push_back(\n+                {extract_val(llvmStruct.getBody()[i], operand, i), constraint});\n+          }\n+        }\n+      } else {\n+        unpackedOperands.push_back({operand, constraint});\n+      }\n+    }\n+    return unpackedOperands;\n+  }\n+\n+  SmallVector<PTXBuilder::Operand *>\n+  getPtxOperands(OperandsAndConstraints &operandsAndConstraints,\n+                 PTXBuilder &ptxBuilder, Location &loc,\n+                 mlir::PatternRewriter &rewriter) const {\n+    SmallVector<PTXBuilder::Operand *> ptxOperands;\n+    auto unpackedOperandsAndConstraints =\n+        unpackOperands(operandsAndConstraints, ptxBuilder, loc, rewriter);\n+    for (auto &[operand, constraint] : unpackedOperandsAndConstraints) {\n+      auto convertedOperand = convertToType(operand, constraint, loc, rewriter);\n+      auto *ptxOperand = ptxBuilder.newOperand(convertedOperand, constraint);\n+      ptxOperands.push_back(ptxOperand);\n+    }\n+    return ptxOperands;\n+  }\n+\n+  virtual std::vector<std::string> getOutputConstraints(SourceOp op) const {\n+    return {};\n+  }\n+\n+  virtual OperandsAndConstraints getOperandsAndConstraints(SourceOp op) const {\n+    return {};\n+  }\n+\n+  Type getReturnType(std::vector<std::string> outputConstraints,\n+                     mlir::PatternRewriter &rewriter) const {\n+    auto ctx = rewriter.getContext();\n+    Type resTy;\n+    if (outputConstraints.empty()) {\n+      resTy = void_ty(ctx);\n+    } else {\n+      SmallVector<Type> retTys;\n+      for (auto &outputConstraint : outputConstraints) {\n+        assert(outputConstraint[0] == '=' &&\n+               \"Constraint must be for an output\");\n+        Type retTy = getTypeFromConstraint(outputConstraint[1], rewriter);\n+        retTys.push_back(retTy);\n+      }\n+      if (retTys.size() == 1) {\n+        resTy = retTys[0];\n+      } else {\n+        resTy = struct_ty(retTys);\n+      }\n+    }\n+    return resTy;\n+  }\n+\n+  std::string patchPtxAsm(mlir::Operation *op, std::string ptxAsm) const {\n+    std::vector<std::pair<int, int>> patchLocations;\n+    std::vector<std::string> patchValues;\n+    auto start = ptxAsm.find(\"#\", 0);\n+    while (start != std::string::npos) {\n+      auto endIterator =\n+          std::find_if(ptxAsm.begin() + start + 1, ptxAsm.end(),\n+                       [](unsigned char c) { return !std::isalnum(c); });\n+\n+      assert(endIterator != ptxAsm.end() && \"unexpected asm format\");\n+\n+      auto end = std::distance(ptxAsm.begin(), endIterator);\n+      auto patchLocation = std::make_pair(start, end);\n+      patchLocations.push_back(patchLocation);\n+      auto patchValue = ptxAsm.substr(start + 1, end - start - 1);\n+      patchValues.push_back(patchValue);\n+      start = ptxAsm.find(\"#\", end);\n+    }\n+    assert(patchLocations.size() == patchValues.size() &&\n+           \"patchLocations and patchValues should have the same size\");\n+    if (patchLocations.size() == 0) {\n+      return ptxAsm;\n+    }\n+    std::string res = \"\";\n+    size_t prevStart = 0;\n+    unsigned i = 0;\n+    for (auto &[start, end] : patchLocations) {\n+      res += ptxAsm.substr(prevStart, start - prevStart);\n+      auto integerAttr = op->getAttrOfType<IntegerAttr>(patchValues[i]);\n+      auto attr = integerAttr.getInt();\n+      res += std::to_string(attr);\n+      prevStart = end;\n+      i++;\n+    }\n+    if (prevStart < ptxAsm.size())\n+      res += ptxAsm.substr(prevStart, ptxAsm.size() - prevStart);\n+    return res;\n+  }\n+\n   LogicalResult\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n@@ -35,30 +268,62 @@ class NVGPUOpPatternBase : public mlir::RewritePattern {\n     auto sourceOp = llvm::dyn_cast<SourceOp>(op);\n     if (!sourceOp)\n       return mlir::failure();\n-    auto ptxAsm = static_cast<const ConcreteT *>(this)->getPtxAsm(sourceOp);\n+    auto concrete = static_cast<const ConcreteT *>(this);\n+    auto ptxAsm = concrete->getPtxAsm(sourceOp);\n+    auto ptxAsmPatched = patchPtxAsm(sourceOp, ptxAsm);\n     auto hasSideEffects = !isMemoryEffectFree(sourceOp);\n+    auto operandsAndConstraints = concrete->getOperandsAndConstraints(sourceOp);\n+    auto outputConstraints = concrete->getOutputConstraints(sourceOp);\n+\n     PTXBuilder ptxBuilder;\n-    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n-    ptxInstr({}, /*onlyAttachMLIRArgs=*/true);\n-    auto asmReturnTy = void_ty(ctx);\n-    ptxBuilder.launch(rewriter, loc, asmReturnTy,\n-                      /*hasSideEffects*/ hasSideEffects);\n-    rewriter.eraseOp(op);\n+    auto ptxOutputs = getPtxOutputs(outputConstraints, ptxBuilder);\n+    auto ptxOperands =\n+        getPtxOperands(operandsAndConstraints, ptxBuilder, loc, rewriter);\n+    SmallVector<PTXBuilder::Operand *> outputsAndOperands = ptxOutputs;\n+    outputsAndOperands.append(ptxOperands.begin(), ptxOperands.end());\n+    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsmPatched);\n+    ptxInstr(outputsAndOperands, /*onlyAttachMLIRArgs=*/true);\n+    auto retTy = getReturnType(outputConstraints, rewriter);\n+    auto res = ptxBuilder.launch(rewriter, loc, retTy,\n+                                 /*hasSideEffects*/ hasSideEffects);\n+    if (op->getNumResults() == 0) {\n+      rewriter.eraseOp(op);\n+    } else {\n+      rewriter.replaceOp(op, res);\n+    }\n+\n     return mlir::success();\n   }\n };\n \n-class CGABarrierSyncOpPattern\n-    : public NVGPUOpPatternBase<ttn::CGABarrierSyncOp,\n-                                CGABarrierSyncOpPattern> {\n+template <typename SourceOp>\n+class NVGPUOpGenericPattern\n+    : public NVGPUOpPatternBase<SourceOp, NVGPUOpGenericPattern<SourceOp>> {\n public:\n-  using Base =\n-      NVGPUOpPatternBase<ttn::CGABarrierSyncOp, CGABarrierSyncOpPattern>;\n-  using Base::Base;\n-\n-  std::string getPtxAsm(ttn::CGABarrierSyncOp op) const {\n-    return \"barrier.cluster.sync.aligned;\";\n+  explicit NVGPUOpGenericPattern(mlir::MLIRContext *context, std::string ptxAsm,\n+                                 std::vector<std::string> outputConstraints,\n+                                 std::vector<std::string> inputConstraints)\n+      : NVGPUOpPatternBase<SourceOp, NVGPUOpGenericPattern<SourceOp>>(context),\n+        ptxAsm(ptxAsm), outputConstraints(outputConstraints),\n+        inputConstraints(inputConstraints) {}\n+\n+  std::vector<std::string> getOutputConstraints(SourceOp op) const {\n+    return outputConstraints;\n+  }\n+  OperandsAndConstraints getOperandsAndConstraints(SourceOp op) const {\n+    OperandsAndConstraints operandsAndConstraints;\n+    for (unsigned i = 0; i < inputConstraints.size(); i++) {\n+      operandsAndConstraints.push_back(\n+          {op->getOperand(i), inputConstraints[i]});\n+    }\n+    return operandsAndConstraints;\n   }\n+  std::string getPtxAsm(SourceOp op) const { return ptxAsm; }\n+\n+private:\n+  std::string ptxAsm;\n+  std::vector<std::string> outputConstraints;\n+  std::vector<std::string> inputConstraints;\n };\n \n class FenceAsyncSharedOpPattern\n@@ -78,437 +343,342 @@ class FenceAsyncSharedOpPattern\n   }\n };\n \n-class WGMMAFenceOpPattern\n-    : public NVGPUOpPatternBase<ttn::WGMMAFenceOp, WGMMAFenceOpPattern> {\n-public:\n-  using Base = NVGPUOpPatternBase<ttn::WGMMAFenceOp, WGMMAFenceOpPattern>;\n-  using Base::Base;\n-\n-  std::string getPtxAsm(ttn::WGMMAFenceOp op) const {\n-    return \"wgmma.fence.sync.aligned;\";\n-  }\n-};\n-\n-class WGMMACommitGroupOpPattern\n-    : public NVGPUOpPatternBase<ttn::WGMMACommitGroupOp,\n-                                WGMMACommitGroupOpPattern> {\n+class ClusterArriveOpPattern\n+    : public NVGPUOpPatternBase<ttn::ClusterArriveOp, ClusterArriveOpPattern> {\n public:\n-  using Base =\n-      NVGPUOpPatternBase<ttn::WGMMACommitGroupOp, WGMMACommitGroupOpPattern>;\n+  using Base = NVGPUOpPatternBase<ttn::ClusterArriveOp, ClusterArriveOpPattern>;\n   using Base::Base;\n \n-  std::string getPtxAsm(ttn::WGMMACommitGroupOp op) const {\n-    return \"wgmma.commit_group.sync.aligned;\";\n+  std::string getPtxAsm(ttn::ClusterArriveOp op) const {\n+    auto relaxed = op.getRelaxed();\n+    if (relaxed)\n+      return \"barrier.cluster.arrive.relaxed.aligned;\";\n+    else\n+      return \"barrier.cluster.arrive.aligned;\";\n   }\n };\n \n-class WGMMAWaitGroupOpPattern\n-    : public NVGPUOpPatternBase<ttn::WGMMAWaitGroupOp,\n-                                WGMMAWaitGroupOpPattern> {\n+class StoreMatrixOpPattern\n+    : public NVGPUOpPatternBase<ttn::StoreMatrixOp, StoreMatrixOpPattern> {\n public:\n-  using Base =\n-      NVGPUOpPatternBase<ttn::WGMMAWaitGroupOp, WGMMAWaitGroupOpPattern>;\n+  using Base = NVGPUOpPatternBase<ttn::StoreMatrixOp, StoreMatrixOpPattern>;\n   using Base::Base;\n \n-  std::string getPtxAsm(ttn::WGMMAWaitGroupOp op) const {\n-    auto pendings = op.getPendings();\n-    return \"wgmma.wait_group.sync.aligned \" + std::to_string(pendings) + \";\";\n+  OperandsAndConstraints\n+  getOperandsAndConstraints(ttn::StoreMatrixOp op) const {\n+    OperandsAndConstraints operandsAndTypes;\n+    auto addr = op.getAddr();\n+    auto datas = op.getDatas();\n+    operandsAndTypes.push_back({addr, \"r\"});\n+    for (unsigned i = 0; i < datas.size(); i++) {\n+      operandsAndTypes.push_back({datas[i], \"r\"});\n+    }\n+    return operandsAndTypes;\n   }\n-};\n-\n-class StoreMatrixOpPattern : public mlir::RewritePattern {\n-public:\n-  StoreMatrixOpPattern(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(ttn::StoreMatrixOp::getOperationName(), 1,\n-                             context) {}\n \n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto ctx = rewriter.getContext();\n-    auto storeMatrixOp = llvm::dyn_cast<ttn::StoreMatrixOp>(op);\n-    if (!storeMatrixOp)\n-      return mlir::failure();\n-    auto loc = op->getLoc();\n-    auto addr = storeMatrixOp.getAddr();\n-    auto datas = storeMatrixOp.getDatas();\n-\n-    assert(datas.size() == 1 || datas.size() == 2 ||\n-           datas.size() == 4 && \"Invalid size for StoreMatrixOp\");\n-    PTXBuilder ptxBuilder;\n-    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(\n-        \"stmatrix.sync.aligned.m8n8.x\" + std::to_string(datas.size()) +\n-        \".shared.b16\");\n-    auto *addrOpr = ptxBuilder.newAddrOperand(ptrtoint(i32_ty, addr), \"r\");\n-\n-    SmallVector<std::pair<Value, std::string>> args;\n-    for (unsigned i = 0; i < datas.size(); ++i) {\n-      args.push_back({datas[i], \"r\"});\n+  std::string getPtxAsm(ttn::StoreMatrixOp op) const {\n+    auto datas = op.getDatas();\n+    std::string ptxAsm;\n+    switch (datas.size()) {\n+    case 1:\n+      ptxAsm = \"stmatrix.sync.aligned.m8n8.x1.shared.b16 [$0], {$1};\";\n+      break;\n+    case 2:\n+      ptxAsm = \"stmatrix.sync.aligned.m8n8.x2.shared.b16 [$0], {$1, $2};\";\n+      break;\n+    case 4:\n+      ptxAsm =\n+          \"stmatrix.sync.aligned.m8n8.x4.shared.b16 [$0], {$1, $2, $3, $4};\";\n+      break;\n+    default:\n+      assert(false && \"Invalid size\");\n     }\n-    auto *operands = ptxBuilder.newListOperand(args);\n-\n-    ptxInstr(addrOpr, operands);\n-\n-    auto asmReturnTy = void_ty(ctx);\n-    ptxBuilder.launch(rewriter, loc, asmReturnTy);\n-    rewriter.eraseOp(op);\n-    return mlir::success();\n+    return ptxAsm;\n   }\n };\n \n-class MBarrierInitOpPattern : public mlir::RewritePattern {\n+class MBarrierArriveOpPattern\n+    : public NVGPUOpPatternBase<ttn::MBarrierArriveOp,\n+                                MBarrierArriveOpPattern> {\n public:\n-  MBarrierInitOpPattern(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(ttn::MBarrierInitOp::getOperationName(), 1,\n-                             context) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto ctx = rewriter.getContext();\n-    auto mBarrierInitOp = llvm::dyn_cast<ttn::MBarrierInitOp>(op);\n-    if (!mBarrierInitOp)\n-      return mlir::failure();\n-    auto loc = op->getLoc();\n-    Value mbarrier = mBarrierInitOp.getMbarrier();\n-    Value pred = mBarrierInitOp.getPred();\n-    uint32_t count = mBarrierInitOp.getCount();\n-    PTXBuilder ptxBuilder;\n-\n-    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(\"mbarrier.init.shared.b64\");\n-    auto *barOpr =\n-        ptxBuilder.newAddrOperand(ptrtoint(i32_ty, mbarrier), \"r\", 0);\n-    auto *expectedOpr = ptxBuilder.newConstantOperand(count);\n-\n-    ptxInstr(barOpr, expectedOpr).predicate(pred, \"b\");\n+  using Base =\n+      NVGPUOpPatternBase<ttn::MBarrierArriveOp, MBarrierArriveOpPattern>;\n+  using Base::Base;\n \n-    auto asmReturnTy = void_ty(ctx);\n-    ptxBuilder.launch(rewriter, loc, asmReturnTy);\n-    rewriter.eraseOp(op);\n-    return mlir::success();\n+  OperandsAndConstraints\n+  getOperandsAndConstraints(ttn::MBarrierArriveOp op) const {\n+    OperandsAndConstraints operandsAndTypes;\n+    Value mbarrier = op.getMbarrier();\n+    Value pred = op.getPred();\n+    Value ctaId = op.getCtaId();\n+    auto arriveType = op.getArriveType();\n+\n+    switch (arriveType) {\n+    case ttn::MBarriveType::normal:\n+    case ttn::MBarriveType::cp_async:\n+    case ttn::MBarriveType::expect_tx:\n+      operandsAndTypes.push_back({mbarrier, \"r\"});\n+      operandsAndTypes.push_back({pred, \"b\"});\n+      break;\n+    case ttn::MBarriveType::remote:\n+      operandsAndTypes.push_back({mbarrier, \"r\"});\n+      operandsAndTypes.push_back({ctaId, \"r\"});\n+      operandsAndTypes.push_back({pred, \"b\"});\n+      break;\n+    default:\n+      llvm::errs() << \"Unsupported mbarrier arrive type \" << arriveType << \"\\n\";\n+      llvm_unreachable(\"\");\n+      break;\n+    }\n+    return operandsAndTypes;\n   }\n-};\n-\n-class MBarrierArriveOpPattern : public mlir::RewritePattern {\n-public:\n-  MBarrierArriveOpPattern(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(ttn::MBarrierArriveOp::getOperationName(), 1,\n-                             context) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto ctx = rewriter.getContext();\n-    auto mbarrierArriveOp = llvm::dyn_cast<ttn::MBarrierArriveOp>(op);\n-    if (!mbarrierArriveOp)\n-      return mlir::failure();\n-    auto loc = op->getLoc();\n-    Value mbarrier = mbarrierArriveOp.getMbarrier();\n-    Value pred = mbarrierArriveOp.getPred();\n-    Value ctaId = mbarrierArriveOp.getCtaId();\n-    auto arriveType = mbarrierArriveOp.getArriveType();\n-    uint32_t txCount = mbarrierArriveOp.getTxCount();\n \n-    PTXBuilder ptxBuilder;\n-    if (arriveType == ttn::MBarriveType::normal) {\n-      auto &ptxInstr =\n-          *ptxBuilder.create<PTXInstr>(\"mbarrier.arrive.shared.b64 _,\");\n-      auto *barOpr =\n-          ptxBuilder.newAddrOperand(ptrtoint(i32_ty, mbarrier), \"r\", 0);\n-\n-      ptxInstr(barOpr).predicate(pred, \"b\");\n-    } else if (arriveType == ttn::MBarriveType::cp_async) {\n-      auto &ptxInstr = *ptxBuilder.create<PTXInstr>(\n-          \"cp.async.mbarrier.arrive.noinc.shared.b64\");\n-      auto *barOpr =\n-          ptxBuilder.newAddrOperand(ptrtoint(i32_ty, mbarrier), \"r\", 0);\n-\n-      ptxInstr(barOpr).predicate(pred, \"b\");\n-    } else if (arriveType == ttn::MBarriveType::expect_tx) {\n+  std::string getPtxAsm(ttn::MBarrierArriveOp op) const {\n+    Value ctaId = op.getCtaId();\n+    auto arriveType = op.getArriveType();\n+    uint32_t txCount = op.getTxCount();\n+    std::string ptxAsm;\n+    switch (arriveType) {\n+    case ttn::MBarriveType::normal:\n+      ptxAsm = \"@$1 mbarrier.arrive.shared.b64 _, [$0];\";\n+      break;\n+    case ttn::MBarriveType::cp_async:\n+      ptxAsm = \"@$1 cp.async.mbarrier.arrive.noinc.shared.b64 [$0];\";\n+      break;\n+    case ttn::MBarriveType::expect_tx:\n       assert(txCount > 0 && \"txCount should be valid\");\n-      auto &ptxInstr = *ptxBuilder.create<PTXInstr>(\n-          \"mbarrier.arrive.expect_tx.shared.b64 _,\");\n-      auto *barOpr =\n-          ptxBuilder.newAddrOperand(ptrtoint(i32_ty, mbarrier), \"r\", 0);\n-      auto *expectedOpr = ptxBuilder.newConstantOperand(txCount);\n-\n-      ptxInstr(barOpr, expectedOpr).predicate(pred, \"b\");\n-    } else if (arriveType == ttn::MBarriveType::remote) {\n+      ptxAsm = \"@$1 mbarrier.arrive.expect_tx.shared.b64 _, [$0], \" +\n+               std::to_string(txCount) + \";\";\n+      break;\n+    case ttn::MBarriveType::remote:\n       assert(ctaId && \"ctaId should have a valid value\");\n-      auto ptxAsm =\n+      ptxAsm =\n           \" { .reg .b32 remAddr32;                                       \\n\"\n           \"  @$2 mapa.shared::cluster.u32  remAddr32, $0, $1;            \\n\"\n           \"  @$2 mbarrier.arrive.shared::cluster.b64  _, [remAddr32]; }  \\n\";\n-      auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n-      auto *barOpr =\n-          ptxBuilder.newAddrOperand(ptrtoint(i32_ty, mbarrier), \"r\", 0);\n-      auto *ctaIdOpr = ptxBuilder.newOperand(ctaId, \"r\");\n-      auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n-\n-      ptxInstr({barOpr, ctaIdOpr, predOpr}, /*onlyAttachMLIRArgs=*/true);\n-    } else {\n-      assert(false &&\n-             \"Unsupported mbarrier arrive type\"); // TODO: is this the right way\n-                                                  // to assert in LLVM pass ?\n+      break;\n+    default:\n+      llvm::errs() << \"Unsupported mbarrier arrive type \" << arriveType << \"\\n\";\n+      llvm_unreachable(\"\");\n+      break;\n     }\n-    auto asmReturnTy = void_ty(ctx);\n-    ptxBuilder.launch(rewriter, loc, asmReturnTy);\n-    rewriter.eraseOp(op);\n-    return mlir::success();\n-  }\n-};\n-class MBarrierWaitOpPattern : public mlir::RewritePattern {\n-public:\n-  MBarrierWaitOpPattern(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(ttn::MBarrierWaitOp::getOperationName(), 1,\n-                             context) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto ctx = rewriter.getContext();\n-    auto mBarrierWaitOp = llvm::dyn_cast<ttn::MBarrierWaitOp>(op);\n-    if (!mBarrierWaitOp)\n-      return mlir::failure();\n-    auto loc = op->getLoc();\n-    Value mbarrier = mBarrierWaitOp.getMbarrier();\n-    Value phase = mBarrierWaitOp.getPhase();\n-    PTXBuilder ptxBuilder;\n-\n-    auto ptxAsm =\n-        \"{\\n\"\n-        \".reg .pred                P1; \\n\"\n-        \"LAB_WAIT: \\n\"\n-        \"mbarrier.try_wait.parity.shared.b64 P1, [$0], $1, 0x989680; \\n\"\n-        \"@P1                       bra.uni DONE; \\n\"\n-        \"bra.uni                   LAB_WAIT; \\n\"\n-        \"DONE: \\n\"\n-        \"}\";\n-    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n-    auto *barOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, mbarrier), \"r\");\n-    auto *phaseOpr = ptxBuilder.newOperand(zext(i32_ty, phase), \"r\");\n-\n-    ptxInstr({barOpr, phaseOpr},\n-             /*onlyAttachMLIRArgs=*/true);\n-\n-    auto asmReturnTy = void_ty(ctx);\n-    ptxBuilder.launch(rewriter, loc, asmReturnTy);\n-    rewriter.eraseOp(op);\n-    return mlir::success();\n+    return ptxAsm;\n   }\n };\n \n-class ClusterArriveOpPattern\n-    : public NVGPUOpPatternBase<ttn::ClusterArriveOp, ClusterArriveOpPattern> {\n+class TMALoadTiledOpPattern\n+    : public NVGPUOpPatternBase<ttn::TMALoadTiledOp, TMALoadTiledOpPattern> {\n public:\n-  using Base = NVGPUOpPatternBase<ttn::ClusterArriveOp, ClusterArriveOpPattern>;\n+  using Base = NVGPUOpPatternBase<ttn::TMALoadTiledOp, TMALoadTiledOpPattern>;\n   using Base::Base;\n \n-  std::string getPtxAsm(ttn::ClusterArriveOp op) const {\n-    auto relaxed = op.getRelaxed();\n-    if (relaxed)\n-      return \"barrier.cluster.arrive.relaxed.aligned;\";\n-    else\n-      return \"barrier.cluster.arrive.aligned;\";\n-  }\n-};\n+  OperandsAndConstraints\n+  getOperandsAndConstraints(ttn::TMALoadTiledOp op) const {\n+    OperandsAndConstraints operandsAndTypes;\n+    auto dst = op.getDst();\n+    auto mbarrier = op.getMbarrier();\n+    auto tmaDesc = op.getTmaDesc();\n+    auto l2Desc = op.getL2Desc();\n+    auto pred = op.getPred();\n+    auto coords = op.getCoords();\n+    auto mcastMask = op.getMcastMask();\n \n-class ClusterWaitOpPattern\n-    : public NVGPUOpPatternBase<ttn::ClusterWaitOp, ClusterWaitOpPattern> {\n-public:\n-  using Base = NVGPUOpPatternBase<ttn::ClusterWaitOp, ClusterWaitOpPattern>;\n-  using Base::Base;\n-  std::string getPtxAsm(ttn::ClusterWaitOp op) const {\n-    return \"barrier.cluster.wait.aligned;\";\n-  }\n-};\n+    auto dimSize = coords.size();\n+    assert(dimSize == 2 || (dimSize == 4 && mcastMask == nullptr) &&\n+                               \"Does not support TMA configuration\");\n \n-class TMALoadTiledOpPattern : public mlir::RewritePattern {\n-public:\n-  TMALoadTiledOpPattern(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(ttn::TMALoadTiledOp::getOperationName(), 1,\n-                             context) {}\n+    operandsAndTypes.push_back({dst, \"r\"});\n+    operandsAndTypes.push_back({tmaDesc, \"l\"});\n+    for (unsigned i = 0; i < coords.size(); i++) {\n+      operandsAndTypes.push_back({coords[i], \"r\"});\n+    }\n+    operandsAndTypes.push_back({mbarrier, \"l\"});\n+    if (mcastMask) {\n+      operandsAndTypes.push_back({mcastMask, \"h\"});\n+    }\n+    operandsAndTypes.push_back({l2Desc, \"l\"});\n+    operandsAndTypes.push_back({pred, \"b\"});\n \n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto ctx = rewriter.getContext();\n-    auto tmaLoadTiledOp = llvm::dyn_cast<ttn::TMALoadTiledOp>(op);\n-    if (!tmaLoadTiledOp)\n-      return mlir::failure();\n-    auto loc = op->getLoc();\n-    auto dst = tmaLoadTiledOp.getDst();\n-    auto mbarrier = tmaLoadTiledOp.getMbarrier();\n-    auto tmaDesc = tmaLoadTiledOp.getTmaDesc();\n-    auto l2Desc = tmaLoadTiledOp.getL2Desc();\n-    auto pred = tmaLoadTiledOp.getPred();\n-    auto coords = tmaLoadTiledOp.getCoords();\n-    auto mcastMask = tmaLoadTiledOp.getMcastMask();\n+    return operandsAndTypes;\n+  }\n \n+  std::string getPtxAsm(ttn::TMALoadTiledOp op) const {\n+    auto coords = op.getCoords();\n+    auto mcastMask = op.getMcastMask();\n     auto dimSize = coords.size();\n-\n-    PTXBuilder ptxBuilder;\n+    std::string ptxAsm;\n     if (dimSize == 2) {\n       if (mcastMask == nullptr) {\n-        auto ptxAsm =\n-            \"@$6 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier:\"\n-            \":complete_tx\"\n-            \"::bytes.L2::cache_hint [$0], [$1, {$2, $3}], [$4], $5;\";\n-        auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n-        auto *dstOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, dst), \"r\");\n-        auto *descOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, tmaDesc), \"l\");\n-        auto *c0Opr = ptxBuilder.newOperand(coords[0], \"r\");\n-        auto *c1Opr = ptxBuilder.newOperand(coords[1], \"r\");\n-        auto *barOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, mbarrier), \"r\");\n-        auto *l2DescOpr = ptxBuilder.newOperand(l2Desc, \"l\");\n-        auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n-\n-        ptxInstr({dstOpr, descOpr, c0Opr, c1Opr, barOpr, l2DescOpr, predOpr},\n-                 /*onlyAttachMLIRArgs=*/true);\n+        ptxAsm = \"@$6 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier:\"\n+                 \":complete_tx\"\n+                 \"::bytes.L2::cache_hint [$0], [$1, {$2, $3}], [$4], $5;\";\n       } else {\n-        auto ptxAsm =\n-            \"@$7 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::\"\n-            \"complete_tx::bytes.multicast::cluster.L2::cache_hint\"\n-            \" [$0], [$1, {$2, $3}], [$4], $5, $6;\";\n-        auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n-        auto *dstOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, dst), \"r\");\n-        auto *descOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, tmaDesc), \"l\");\n-        auto *c0Opr = ptxBuilder.newOperand(coords[0], \"r\");\n-        auto *c1Opr = ptxBuilder.newOperand(coords[1], \"r\");\n-        auto *barOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, mbarrier), \"r\");\n-        auto *maskOpr = ptxBuilder.newOperand(mcastMask, \"h\");\n-        auto *l2DescOpr = ptxBuilder.newOperand(l2Desc, \"l\");\n-        auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n-        ptxInstr({dstOpr, descOpr, c0Opr, c1Opr, barOpr, maskOpr, l2DescOpr,\n-                  predOpr},\n-                 /*onlyAttachMLIRArgs=*/true);\n+        ptxAsm = \"@$7 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::\"\n+                 \"complete_tx::bytes.multicast::cluster.L2::cache_hint\"\n+                 \" [$0], [$1, {$2, $3}], [$4], $5, $6;\";\n       }\n     } else if (dimSize == 4) {\n       assert(mcastMask == nullptr && \"Does not support multicast\");\n-      auto ptxAsm = \"@$8 \"\n-                    \"cp.async.bulk.tensor.4d.shared::cluster.global.mbarrier:\"\n-                    \":complete_tx\"\n-                    \"::bytes.L2::cache_hint [$0], [$1, {$2, $3, $4, $5}], \"\n-                    \"[$6], $7;\";\n-      auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n-      auto *dstOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, dst), \"r\");\n-      auto *descOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, tmaDesc), \"l\");\n-      auto *c0Opr = ptxBuilder.newOperand(coords[0], \"r\");\n-      auto *c1Opr = ptxBuilder.newOperand(coords[1], \"r\");\n-      auto *c2Opr = ptxBuilder.newOperand(coords[2], \"r\");\n-      auto *c3Opr = ptxBuilder.newOperand(coords[3], \"r\");\n-      auto *barOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, mbarrier), \"r\");\n-      auto *l2DescOpr = ptxBuilder.newOperand(l2Desc, \"l\");\n-      auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n-      ptxInstr({dstOpr, descOpr, c0Opr, c1Opr, c2Opr, c3Opr, barOpr, l2DescOpr,\n-                predOpr},\n-               /*onlyAttachMLIRArgs=*/true);\n+      ptxAsm = \"@$8 \"\n+               \"cp.async.bulk.tensor.4d.shared::cluster.global.mbarrier:\"\n+               \":complete_tx\"\n+               \"::bytes.L2::cache_hint [$0], [$1, {$2, $3, $4, $5}], \"\n+               \"[$6], $7;\";\n     } else {\n-      assert(false && \"invalid dim size\");\n+      llvm::errs() << \"Unsupported dimSize \" << dimSize << \"\\n\";\n+      llvm_unreachable(\"\");\n     }\n-\n-    auto asmReturnTy = void_ty(ctx);\n-    ptxBuilder.launch(rewriter, loc, asmReturnTy, /*hasSideEffect*/ true);\n-    rewriter.eraseOp(op);\n-    return mlir::success();\n+    return ptxAsm;\n   }\n };\n \n-class TMAStoreTiledOpPattern : public mlir::RewritePattern {\n+class TMAStoreTiledOpPattern\n+    : public NVGPUOpPatternBase<ttn::TMAStoreTiledOp, TMAStoreTiledOpPattern> {\n public:\n-  TMAStoreTiledOpPattern(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(ttn::TMAStoreTiledOp::getOperationName(), 1,\n-                             context) {}\n+  using Base = NVGPUOpPatternBase<ttn::TMAStoreTiledOp, TMAStoreTiledOpPattern>;\n+  using Base::Base;\n \n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto ctx = rewriter.getContext();\n-    auto tmaStoreTiledOp = llvm::dyn_cast<ttn::TMAStoreTiledOp>(op);\n-    if (!tmaStoreTiledOp)\n-      return mlir::failure();\n-    auto loc = op->getLoc();\n-    auto src = tmaStoreTiledOp.getSrc();\n-    auto tmaDesc = tmaStoreTiledOp.getTmaDesc();\n-    auto pred = tmaStoreTiledOp.getPred();\n-    auto coords = tmaStoreTiledOp.getCoords();\n+  OperandsAndConstraints\n+  getOperandsAndConstraints(ttn::TMAStoreTiledOp op) const {\n+    OperandsAndConstraints operandsAndTypes;\n+    auto src = op.getSrc();\n+    auto tmaDesc = op.getTmaDesc();\n+    auto pred = op.getPred();\n+    auto coords = op.getCoords();\n \n     auto dimSize = coords.size();\n+    if (dimSize != 2 && dimSize != 3 && dimSize != 4) {\n+      llvm::errs() << \"Unsupported dimSize \" << dimSize << \"\\n\";\n+      llvm_unreachable(\"\");\n+    }\n+    operandsAndTypes.push_back({tmaDesc, \"l\"});\n+    operandsAndTypes.push_back({src, \"r\"});\n+    for (unsigned i = 0; i < dimSize; i++) {\n+      operandsAndTypes.push_back({coords[i], \"r\"});\n+    }\n+    operandsAndTypes.push_back({pred, \"b\"});\n \n-    PTXBuilder ptxBuilder;\n+    return operandsAndTypes;\n+  }\n+\n+  std::string getPtxAsm(ttn::TMAStoreTiledOp op) const {\n+    auto coords = op.getCoords();\n+    auto dimSize = coords.size();\n+    std::string ptxAsm;\n     if (dimSize == 2) {\n-      auto ptxAsm = \"cp.async.bulk.tensor.2d.global.shared::cta.bulk_group\"\n-                    \"[$0, {$2, $3}], [$1];\";\n-      auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n-\n-      auto *descOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, tmaDesc), \"l\");\n-      auto *srcOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, src), \"r\");\n-      auto *c0Opr = ptxBuilder.newOperand(coords[0], \"r\");\n-      auto *c1Opr = ptxBuilder.newOperand(coords[1], \"r\");\n-      auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n-      ptxInstr({descOpr, srcOpr, c0Opr, c1Opr, predOpr},\n-               /*onlyAttachMLIRArgs=*/true);\n+      ptxAsm = \"cp.async.bulk.tensor.2d.global.shared::cta.bulk_group\"\n+               \"[$0, {$2, $3}], [$1];\";\n     } else if (dimSize == 3) {\n-      auto ptxAsm = \"@$5 cp.async.bulk.tensor.3d.global.shared::cta.bulk_group\"\n-                    \"[$0, {$2, $3, $4}], [$1];\";\n-      auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n-\n-      auto *descOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, tmaDesc), \"l\");\n-      auto *srcOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, src), \"r\");\n-      auto *c0Opr = ptxBuilder.newOperand(coords[0], \"r\");\n-      auto *c1Opr = ptxBuilder.newOperand(coords[1], \"r\");\n-      auto *c2Opr = ptxBuilder.newOperand(coords[2], \"r\");\n-      auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n-      ptxInstr({descOpr, srcOpr, c0Opr, c1Opr, c2Opr, predOpr},\n-               /*onlyAttachMLIRArgs=*/true);\n+      ptxAsm = \"@$5 cp.async.bulk.tensor.3d.global.shared::cta.bulk_group\"\n+               \"[$0, {$2, $3, $4}], [$1];\";\n     } else if (dimSize == 4) {\n-      auto ptxAsm = \"@$6 cp.async.bulk.tensor.4d.global.shared::cta.bulk_group\"\n-                    \"[$0, {$2, $3, $4, $5}], [$1];\";\n-      auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n-      auto *descOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, tmaDesc), \"l\");\n-      auto *srcOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, src), \"r\");\n-      auto *c0Opr = ptxBuilder.newOperand(coords[0], \"r\");\n-      auto *c1Opr = ptxBuilder.newOperand(coords[1], \"r\");\n-      auto *c2Opr = ptxBuilder.newOperand(coords[2], \"r\");\n-      auto *c3Opr = ptxBuilder.newOperand(coords[3], \"r\");\n-      auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n-      ptxInstr({descOpr, srcOpr, c0Opr, c1Opr, c2Opr, c3Opr, predOpr},\n-               /*onlyAttachMLIRArgs=*/true);\n+      ptxAsm = \"@$6 cp.async.bulk.tensor.4d.global.shared::cta.bulk_group\"\n+               \"[$0, {$2, $3, $4, $5}], [$1];\";\n     } else {\n-      assert(false && \"invalid dim size\");\n+      llvm::errs() << \"Unsupported dimSize \" << dimSize << \"\\n\";\n+      llvm_unreachable(\"\");\n     }\n+    return ptxAsm;\n+  }\n+};\n \n-    auto asmReturnTy = void_ty(ctx);\n-    ptxBuilder.launch(rewriter, loc, asmReturnTy, /*hasSideEffect*/ true);\n-    rewriter.eraseOp(op);\n-    return mlir::success();\n+class StoreDSmemOpPattern\n+    : public NVGPUOpPatternBase<ttn::StoreDSmemOp, StoreDSmemOpPattern> {\n+public:\n+  using Base = NVGPUOpPatternBase<ttn::StoreDSmemOp, StoreDSmemOpPattern>;\n+  using Base::Base;\n+\n+  OperandsAndConstraints getOperandsAndConstraints(ttn::StoreDSmemOp op) const {\n+    OperandsAndConstraints operandsAndTypes;\n+    auto addr = op.getAddr();\n+    auto ctaId = op.getCtaId();\n+    auto values = op.getValues();\n+    auto pred = op.getPred();\n+    auto bitwidth = op.getBitwidth();\n+    operandsAndTypes.push_back({addr, \"r\"});\n+    operandsAndTypes.push_back({ctaId, \"r\"});\n+    operandsAndTypes.push_back({pred, \"b\"});\n+    std::string c = bitwidth == 16 ? \"h\" : (bitwidth == 32 ? \"r\" : \"l\");\n+    for (unsigned i = 0; i < values.size(); i++) {\n+      operandsAndTypes.push_back({values[i], c});\n+    }\n+    return operandsAndTypes;\n+  }\n+\n+  std::string getPtxAsm(ttn::StoreDSmemOp op) const {\n+    auto bitwidth = op.getBitwidth();\n+    auto vec = op.getVec();\n+    auto values = op.getValues();\n+    assert(\n+        (bitwidth == 8 || bitwidth == 16 || bitwidth == 32 || bitwidth == 64) &&\n+        \"invalid bitwidth\");\n+    assert((vec == 1 || vec == 2 || vec == 4) && vec == values.size() &&\n+           \"invalid vec size\");\n+    std::string ptxAsm;\n+    if (vec == 1) {\n+      ptxAsm = \"{                                           \\n\"\n+               \".reg .u32 remoteAddr;                       \\n\"\n+               \"mapa.shared::cluster.u32 remoteAddr, $0, $1;\\n\"\n+               \".reg .pred p;                               \\n\"\n+               \"mov.pred p, $2;                             \\n\"\n+               \"@p st.shared::cluster.u#bitwidth [remoteAddr], $3; \\n\"\n+               \"}\\n\";\n+    }\n+    if (vec == 2) {\n+      ptxAsm = \"{                                           \\n\"\n+               \".reg .u32 remoteAddr;                       \\n\"\n+               \"mapa.shared::cluster.u32 remoteAddr, $0, $1;\\n\"\n+               \".reg .pred p;                               \\n\"\n+               \"mov.pred p, $2;                             \\n\"\n+               \"@p st.shared::cluster.v.u#bitwidth [remoteAddr], {$3, $4}; \\n\"\n+               \"}\\n\";\n+    }\n+    if (vec == 4) {\n+      ptxAsm = \"{                                           \\n\"\n+               \".reg .u32 remoteAddr;                       \\n\"\n+               \"mapa.shared::cluster.u32 remoteAddr, $0, $1;\\n\"\n+               \".reg .pred p;                               \\n\"\n+               \"mov.pred p, $2;                             \\n\"\n+               \"@p st.shared::cluster.v.u#bitwidth [remoteAddr], {$3, $4, $5, \"\n+               \"$6}; \\n\"\n+               \"}\\n\";\n+    }\n+    return ptxAsm;\n   }\n };\n \n-class LoadDSmemOpPattern : public mlir::RewritePattern {\n+class LoadDSmemOpPattern\n+    : public NVGPUOpPatternBase<ttn::LoadDSmemOp, LoadDSmemOpPattern> {\n public:\n-  LoadDSmemOpPattern(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(ttn::LoadDSmemOp::getOperationName(), 1, context) {\n+  using Base = NVGPUOpPatternBase<ttn::LoadDSmemOp, LoadDSmemOpPattern>;\n+  using Base::Base;\n+\n+  std::vector<std::string> getOutputConstraints(ttn::LoadDSmemOp op) const {\n+    auto bitwidth = op.getBitwidth();\n+    std::string c = bitwidth == 16 ? \"=h\" : (bitwidth == 32 ? \"=r\" : \"=l\");\n+    auto vec = op.getVec();\n+    return std::vector<std::string>(vec, c);\n+  }\n+  OperandsAndConstraints getOperandsAndConstraints(ttn::LoadDSmemOp op) const {\n+    OperandsAndConstraints operandsAndTypes;\n+    auto addr = op.getAddr();\n+    auto ctaId = op.getCtaId();\n+\n+    operandsAndTypes.push_back({addr, \"r\"});\n+    operandsAndTypes.push_back({ctaId, \"r\"});\n+    return operandsAndTypes;\n   }\n \n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto ctx = rewriter.getContext();\n-    auto loadDSmemOp = llvm::dyn_cast<ttn::LoadDSmemOp>(op);\n-    if (!loadDSmemOp)\n-      return mlir::failure();\n-    auto loc = op->getLoc();\n-    auto addr = loadDSmemOp.getAddr();\n-    auto ctaId = loadDSmemOp.getCtaId();\n-    auto bitwidth = loadDSmemOp.getBitwidth();\n-    auto vec = loadDSmemOp.getVec();\n+  std::string getPtxAsm(ttn::LoadDSmemOp op) const {\n+    auto addr = op.getAddr();\n+    auto ctaId = op.getCtaId();\n+    auto bitwidth = op.getBitwidth();\n+    auto vec = op.getVec();\n \n     assert(\n         (bitwidth == 8 || bitwidth == 16 || bitwidth == 32 || bitwidth == 64) &&\n         \"invalid bitwidth\");\n     assert((vec == 1 || vec == 2 || vec == 4) && \"invalid vec size\");\n-    PTXBuilder ptxBuilder;\n \n     std::string o1 = vec > 1 ? \".v.u\" : \".u\";\n     std::string vecStr = vec == 1   ? \"$0\"\n@@ -524,58 +694,64 @@ class LoadDSmemOpPattern : public mlir::RewritePattern {\n                   o1 + std::to_string(bitwidth) + \" \" + vecStr +\n                   \", [remoteAddr];\\n\"\n                   \"}\\n\";\n+    return ptxAsm;\n+  }\n+};\n \n-    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n-    std::string c = bitwidth == 16 ? \"=h\" : (bitwidth == 32 ? \"=r\" : \"=l\");\n-    SmallVector<PTXBuilder::Operand *> oprs;\n-    for (unsigned i = 0; i < vec; ++i) {\n-      auto *ret = ptxBuilder.newOperand(c);\n-      oprs.push_back(ret);\n-    }\n-    auto *addrOpr = ptxBuilder.newOperand(addr, \"r\");\n-    auto *ctaIdOpr = ptxBuilder.newOperand(ctaId, \"r\");\n-    oprs.push_back(addrOpr);\n-    oprs.push_back(ctaIdOpr);\n-\n-    Type retTy = IntegerType::get(rewriter.getContext(), bitwidth);\n-    SmallVector<Type> retTys(vec, retTy);\n-    if (vec > 1)\n-      retTy = struct_ty(retTys);\n+class WGMMAOpPattern : public NVGPUOpPatternBase<ttn::WGMMAOp, WGMMAOpPattern> {\n+public:\n+  using Base = NVGPUOpPatternBase<ttn::WGMMAOp, WGMMAOpPattern>;\n+  using Base::Base;\n \n-    ptxInstr(oprs,\n-             /*onlyAttachMLIRArgs=*/true);\n+  std::vector<std::string> getOutputConstraints(ttn::WGMMAOp op) const {\n+    // TODO (zahi): Return type must always be a struct for wgmma, currently\n+    // we rely on the size of output constraints vector to determine whether\n+    // the output is a struct or not. We should find a way to pass this info\n+    auto opC = op.getOpC();\n+    auto typeC = opC.getType();\n \n-    auto res = ptxBuilder.launch(rewriter, loc, retTy);\n-    rewriter.replaceOp(op, {res});\n-    return mlir::success();\n+    auto structTypeC = typeC.dyn_cast<LLVM::LLVMStructType>();\n+    uint32_t numCRegs = structTypeC.getBody().size();\n+    std::string c = structTypeC.getBody().front().isF32() ? \"=f\" : \"=r\";\n+    return std::vector<std::string>(numCRegs, c);\n   }\n-};\n \n-class WGMMAOpPattern : public mlir::RewritePattern {\n-public:\n-  WGMMAOpPattern(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(ttn::WGMMAOp::getOperationName(), 1, context) {}\n+  OperandsAndConstraints getOperandsAndConstraints(ttn::WGMMAOp op) const {\n+    OperandsAndConstraints operandsAndConstraints;\n+    auto opA = op.getOpA();\n+    auto opB = op.getOpB();\n+    auto opC = op.getOpC();\n+    auto typeA = opA.getType();\n \n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n+    auto structTypeA = typeA.dyn_cast<LLVM::LLVMStructType>();\n+\n+    // TODO (zahi): is this the best way to tie inputs/outputs ?\n+    operandsAndConstraints.push_back({opC, \"0\"});\n+\n+    if (structTypeA) {\n+      operandsAndConstraints.push_back({opA, \"f\"});\n+    } else {\n+      operandsAndConstraints.push_back({opA, \"l\"});\n+    }\n+\n+    // Operand B (must be `desc`)\n+    operandsAndConstraints.push_back({opB, \"l\"});\n+    return operandsAndConstraints;\n+  }\n+\n+  std::string getPtxAsm(ttn::WGMMAOp op) const {\n     using namespace ttn;\n-    auto ctx = rewriter.getContext();\n-    auto wgmmaOp = llvm::dyn_cast<ttn::WGMMAOp>(op);\n-    if (!wgmmaOp)\n-      return mlir::failure();\n-    auto loc = op->getLoc();\n-    auto opA = wgmmaOp.getOpA();\n-    auto opB = wgmmaOp.getOpB();\n-    auto opC = wgmmaOp.getOpC();\n-    auto m = wgmmaOp.getM();\n-    auto n = wgmmaOp.getN();\n-    auto k = wgmmaOp.getK();\n-    auto eltTypeC = wgmmaOp.getEltTypeC();\n-    auto eltTypeA = wgmmaOp.getEltTypeA();\n-    auto eltTypeB = wgmmaOp.getEltTypeB();\n-    auto layoutA = wgmmaOp.getLayoutA();\n-    auto layoutB = wgmmaOp.getLayoutB();\n+    auto opA = op.getOpA();\n+    auto opB = op.getOpB();\n+    auto opC = op.getOpC();\n+    auto m = op.getM();\n+    auto n = op.getN();\n+    auto k = op.getK();\n+    auto eltTypeC = op.getEltTypeC();\n+    auto eltTypeA = op.getEltTypeA();\n+    auto eltTypeB = op.getEltTypeB();\n+    auto layoutA = op.getLayoutA();\n+    auto layoutB = op.getLayoutB();\n \n     // Register checks\n     auto typeA = opA.getType();\n@@ -624,8 +800,6 @@ class WGMMAOpPattern : public mlir::RewritePattern {\n                    (m == 64 && 8 <= n && n <= 224 && k == 32);\n     }\n     assert(supported && \"WGMMA type or shape is not supported\");\n-    PTXBuilder ptxBuilder;\n-    SmallVector<PTXBuilder::Operand *> oprs;\n \n     // Operands\n     uint32_t asmOpIdx = 0;\n@@ -637,25 +811,9 @@ class WGMMAOpPattern : public mlir::RewritePattern {\n     args += \"{\";\n     for (uint32_t i = 0; i < numCRegs; ++i) {\n       args += \"$\" + std::to_string(asmOpIdx++) + (i == numCRegs - 1 ? \"\" : \",\");\n-      // LLVM does not support `+` semantic, we must repeat the arguments for\n-      // both input and outputs\n-      PTXBuilder::Operand *opr;\n-      if (structTypeC.getBody().front().isF32())\n-        opr = ptxBuilder.newOperand(\n-            extract_val(structTypeC.getBody()[i], opC, i), \"=f\");\n-      else\n-        opr = ptxBuilder.newOperand(\n-            extract_val(structTypeC.getBody()[i], opC, i), \"=r\");\n-      oprs.push_back(opr);\n     }\n     args += \"}, \";\n \n-    for (uint32_t i = asmOpIdx - numCRegs; i < asmOpIdx; ++i) {\n-      auto *opr = ptxBuilder.newOperand(i);\n-      oprs.push_back(opr);\n-    }\n-\n-    // Note that LLVM will not skip the indexed repeating placeholders\n     asmOpIdx += numCRegs;\n     // Operand A\n     if (structTypeA) {\n@@ -665,21 +823,14 @@ class WGMMAOpPattern : public mlir::RewritePattern {\n       for (uint32_t i = 0; i < numARegs; ++i) {\n         args +=\n             \"$\" + std::to_string(asmOpIdx++) + (i == numARegs - 1 ? \"\" : \",\");\n-        auto *opr = ptxBuilder.newOperand(\n-            extract_val(structTypeA.getBody()[i], opA, i), \"f\");\n-        oprs.push_back(opr);\n       }\n       args += \"}, \";\n     } else {\n       args += \"$\" + std::to_string(asmOpIdx++) + \", \";\n-      auto *opr = ptxBuilder.newOperand(opA, \"l\");\n-      oprs.push_back(opr);\n     }\n \n     // Operand B (must be `desc`)\n     args += \"$\" + std::to_string(asmOpIdx++) + \", \";\n-    auto *opr = ptxBuilder.newOperand(opB, \"l\");\n-    oprs.push_back(opr);\n \n     // `scale-d` is 1 by default\n     args += \"1\";\n@@ -699,338 +850,37 @@ class WGMMAOpPattern : public mlir::RewritePattern {\n                   std::to_string(k) + \".\" + stringifyEnum(eltTypeC).str() +\n                   \".\" + stringifyEnum(eltTypeA).str() + \".\" +\n                   stringifyEnum(eltTypeB).str() + \" \" + args + \";\";\n-\n-    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n-    ptxInstr(oprs,\n-             /*onlyAttachMLIRArgs=*/true);\n-\n-    auto res =\n-        ptxBuilder.launch(rewriter, loc, structTypeC, /*hasSideEffect*/ true);\n-    rewriter.replaceOp(op, {res});\n-    return mlir::success();\n-  }\n-};\n-\n-class FenceMBarrierInitOpPattern\n-    : public NVGPUOpPatternBase<ttn::FenceMBarrierInitOp,\n-                                FenceMBarrierInitOpPattern> {\n-public:\n-  using Base =\n-      NVGPUOpPatternBase<ttn::FenceMBarrierInitOp, FenceMBarrierInitOpPattern>;\n-  using Base::Base;\n-\n-  std::string getPtxAsm(ttn::FenceMBarrierInitOp op) const {\n-    return \"fence.mbarrier_init.release.cluster;\";\n-  }\n-};\n-\n-class NamedBarrierArriveOpPattern : public mlir::RewritePattern {\n-public:\n-  NamedBarrierArriveOpPattern(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(ttn::NamedBarrierArriveOp::getOperationName(), 1,\n-                             context) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto ctx = rewriter.getContext();\n-    auto namedBarrierArriveOp = llvm::dyn_cast<ttn::NamedBarrierArriveOp>(op);\n-    if (!namedBarrierArriveOp)\n-      return mlir::failure();\n-    auto loc = op->getLoc();\n-    auto bar = namedBarrierArriveOp.getBar();\n-    auto numThreads = namedBarrierArriveOp.getNumThreads();\n-    PTXBuilder ptxBuilder;\n-\n-    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(\"bar.arrive $0, $1;\");\n-    auto *barOpr = ptxBuilder.newOperand(bar, \"r\");\n-    auto *numThreadsOpr = ptxBuilder.newOperand(numThreads, \"r\");\n-    ptxInstr({barOpr, numThreadsOpr}, /*onlyAttachMLIRArgs=*/true);\n-\n-    auto asmReturnTy = void_ty(ctx);\n-    ptxBuilder.launch(rewriter, loc, asmReturnTy);\n-    rewriter.eraseOp(op);\n-    return mlir::success();\n-  }\n-};\n-\n-class NamedBarrierWaitOpPattern : public mlir::RewritePattern {\n-public:\n-  NamedBarrierWaitOpPattern(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(ttn::NamedBarrierWaitOp::getOperationName(), 1,\n-                             context) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto ctx = rewriter.getContext();\n-    auto namedBarrierWaitOp = llvm::dyn_cast<ttn::NamedBarrierWaitOp>(op);\n-    if (!namedBarrierWaitOp)\n-      return mlir::failure();\n-    auto loc = op->getLoc();\n-    auto bar = namedBarrierWaitOp.getBar();\n-    auto numThreads = namedBarrierWaitOp.getNumThreads();\n-    PTXBuilder ptxBuilder;\n-\n-    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(\"bar.sync $0, $1;\");\n-    auto *barOpr = ptxBuilder.newOperand(bar, \"r\");\n-    auto *numThreadsOpr = ptxBuilder.newOperand(numThreads, \"r\");\n-    ptxInstr({barOpr, numThreadsOpr}, /*onlyAttachMLIRArgs=*/true);\n-\n-    auto asmReturnTy = void_ty(ctx);\n-    ptxBuilder.launch(rewriter, loc, asmReturnTy);\n-    rewriter.eraseOp(op);\n-    return mlir::success();\n-  }\n-};\n-\n-class CGABarrierArriveOpPattern\n-    : public NVGPUOpPatternBase<ttn::CGABarrierArriveOp,\n-                                CGABarrierArriveOpPattern> {\n-public:\n-  using Base =\n-      NVGPUOpPatternBase<ttn::CGABarrierArriveOp, CGABarrierArriveOpPattern>;\n-  using Base::Base;\n-  std::string getPtxAsm(ttn::CGABarrierArriveOp op) const {\n-    return \"barrier.cluster.arrive;\";\n-  }\n-};\n-\n-class CGABarrierWaitOpPattern\n-    : public NVGPUOpPatternBase<ttn::CGABarrierWaitOp,\n-                                CGABarrierWaitOpPattern> {\n-public:\n-  using Base =\n-      NVGPUOpPatternBase<ttn::CGABarrierWaitOp, CGABarrierWaitOpPattern>;\n-  using Base::Base;\n-  std::string getPtxAsm(ttn::CGABarrierWaitOp op) const {\n-    return \"barrier.cluster.wait;\";\n-  }\n-};\n-\n-class StoreDSmemOpPattern : public mlir::RewritePattern {\n-public:\n-  StoreDSmemOpPattern(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(ttn::StoreDSmemOp::getOperationName(), 1,\n-                             context) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto ctx = rewriter.getContext();\n-    auto storeDSmemOp = llvm::dyn_cast<ttn::StoreDSmemOp>(op);\n-    if (!storeDSmemOp)\n-      return mlir::failure();\n-    auto loc = op->getLoc();\n-    auto addr = storeDSmemOp.getAddr();\n-    auto ctaId = storeDSmemOp.getCtaId();\n-    auto values = storeDSmemOp.getValues();\n-    auto pred = storeDSmemOp.getPred();\n-\n-    auto bitwidth = storeDSmemOp.getBitwidth();\n-    auto vec = storeDSmemOp.getVec();\n-    assert(\n-        (bitwidth == 8 || bitwidth == 16 || bitwidth == 32 || bitwidth == 64) &&\n-        \"invalid bitwidth\");\n-    assert((vec == 1 || vec == 2 || vec == 4) && vec == values.size() &&\n-           \"invalid vec size\");\n-\n-    PTXBuilder ptxBuilder;\n-\n-    std::string ptxAsm = \"{\\n\\t\"\n-                         \".reg .u32 remoteAddr;\\n\\t\"\n-                         \"mapa.shared::cluster.u32 remoteAddr, $0, $1;\\n\\t\"\n-                         \".reg .pred p;\\n\\t\"\n-                         \"mov.pred p, $2;\\n\\t\"\n-                         \"@p st.shared::cluster\";\n-    if (vec > 1)\n-      ptxAsm += \".v\" + std::to_string(vec);\n-    ptxAsm += \".u\" + std::to_string(bitwidth) + \" [remoteAddr], \";\n-    if (vec == 1)\n-      ptxAsm += \"$3\";\n-    else if (vec == 2)\n-      ptxAsm += \"{$3, $4}\";\n-    else if (vec == 4)\n-      ptxAsm += \"{$3, $4, $5, $6}\";\n-    ptxAsm += \";\\n\\t\";\n-    ptxAsm += \"}\\n\";\n-    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n-\n-    std::string c = bitwidth == 16 ? \"h\" : (bitwidth == 32 ? \"r\" : \"l\");\n-    SmallVector<PTXBuilder::Operand *> oprs;\n-    auto *addrOpr = ptxBuilder.newOperand(addr, \"r\");\n-    oprs.push_back(addrOpr);\n-    auto *ctaIdOpr = ptxBuilder.newOperand(ctaId, \"r\");\n-    oprs.push_back(ctaIdOpr);\n-    auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n-    oprs.push_back(predOpr);\n-    for (unsigned i = 0; i < values.size(); i++) {\n-      auto *valueOpr = ptxBuilder.newOperand(values[i], c);\n-      oprs.push_back(valueOpr);\n-    }\n-    ptxInstr(oprs,\n-             /*onlyAttachMLIRArgs=*/true);\n-\n-    auto asmReturnTy = void_ty(ctx);\n-    ptxBuilder.launch(rewriter, loc, asmReturnTy, /*hasSideEffect*/ true);\n-    rewriter.eraseOp(op);\n-    return mlir::success();\n-  }\n-};\n-\n-class Sts64OpPattern : public mlir::RewritePattern {\n-public:\n-  Sts64OpPattern(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(ttn::Sts64Op::getOperationName(), 1, context) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto ctx = rewriter.getContext();\n-    auto sts64Op = llvm::dyn_cast<ttn::Sts64Op>(op);\n-    if (!sts64Op)\n-      return mlir::failure();\n-    auto loc = op->getLoc();\n-    auto offset = sts64Op.getOffset();\n-    auto d0 = sts64Op.getD0();\n-    auto d1 = sts64Op.getD1();\n-\n-    PTXBuilder ptxBuilder;\n-\n-    std::string ptxAsm = \"st.shared.v2.b32 [$0], {$1, $2}\";\n-    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n-\n-    SmallVector<PTXBuilder::Operand *> oprs;\n-    auto *addrOpr = ptxBuilder.newOperand(offset, \"r\");\n-    auto *d0Opr = ptxBuilder.newOperand(d0, \"r\");\n-    auto *d1Opr = ptxBuilder.newOperand(d1, \"r\");\n-\n-    ptxInstr({addrOpr, d0Opr, d1Opr},\n-             /*onlyAttachMLIRArgs=*/true);\n-\n-    auto asmReturnTy = void_ty(ctx);\n-    ptxBuilder.launch(rewriter, loc, asmReturnTy);\n-    rewriter.eraseOp(op);\n-    return mlir::success();\n+    return ptxAsm;\n   }\n };\n \n-class RegAllocOpPattern\n-    : public NVGPUOpPatternBase<ttn::RegAllocOp, RegAllocOpPattern> {\n+class OffsetOfSts64OpPattern\n+    : public NVGPUOpPatternBase<ttn::OffsetOfSts64Op, OffsetOfSts64OpPattern> {\n public:\n-  using Base = NVGPUOpPatternBase<ttn::RegAllocOp, RegAllocOpPattern>;\n+  using Base = NVGPUOpPatternBase<ttn::OffsetOfSts64Op, OffsetOfSts64OpPattern>;\n   using Base::Base;\n \n-  std::string getPtxAsm(ttn::RegAllocOp op) const {\n-    auto regCount = op.getRegCount();\n-    return \"setmaxnreg.inc.sync.aligned.u32 \" + std::to_string(regCount) + \";\";\n+  std::vector<std::string> getOutputConstraints(ttn::OffsetOfSts64Op op) const {\n+    return {\"=r\"};\n   }\n-};\n-\n-class RegDeallocOpPattern\n-    : public NVGPUOpPatternBase<ttn::RegDeallocOp, RegDeallocOpPattern> {\n-public:\n-  using Base = NVGPUOpPatternBase<ttn::RegDeallocOp, RegDeallocOpPattern>;\n-  using Base::Base;\n-\n-  std::string getPtxAsm(ttn::RegDeallocOp op) const {\n-    auto regCount = op.getRegCount();\n-    return \"setmaxnreg.dec.sync.aligned.u32 \" + std::to_string(regCount) + \";\";\n-  }\n-};\n-\n-class ClusterCTAIdOpPattern : public mlir::RewritePattern {\n-public:\n-  ClusterCTAIdOpPattern(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(ttn::ClusterCTAIdOp::getOperationName(), 1,\n-                             context) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto ctx = rewriter.getContext();\n-    auto clusterCTAIdOp = llvm::dyn_cast<ttn::ClusterCTAIdOp>(op);\n-    if (!clusterCTAIdOp)\n-      return mlir::failure();\n-    auto loc = op->getLoc();\n \n-    auto x = getSRegValue(rewriter, loc, \"%cluster_ctaid.x\");\n-    auto y = getSRegValue(rewriter, loc, \"%cluster_ctaid.y\");\n-    auto z = getSRegValue(rewriter, loc, \"%cluster_ctaid.z\");\n-    auto nx = getSRegValue(rewriter, loc, \"%cluster_nctaid.x\");\n-    auto ny = getSRegValue(rewriter, loc, \"%cluster_nctaid.y\");\n-    auto res = add(x, mul(add(y, mul(z, ny)), nx));\n-    rewriter.replaceOp(op, {res});\n-    return mlir::success();\n-  }\n-};\n+  OperandsAndConstraints\n+  getOperandsAndConstraints(ttn::OffsetOfSts64Op op) const {\n+    OperandsAndConstraints operandsAndConstraints;\n+    auto threadId = op.getThreadId();\n+    auto rowOfWarp = op.getRowOfWarp();\n+    auto elemIdx = op.getElemIdx();\n \n-class WGMMADescCreateOpPattern : public mlir::RewritePattern {\n-public:\n-  WGMMADescCreateOpPattern(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(ttn::WGMMADescCreateOp::getOperationName(), 1,\n-                             context) {}\n+    operandsAndConstraints.push_back({threadId, \"r\"});\n+    operandsAndConstraints.push_back({elemIdx, \"r\"});\n+    operandsAndConstraints.push_back({rowOfWarp, \"r\"});\n \n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto ctx = rewriter.getContext();\n-    auto wgmmaDescCreateOp = llvm::dyn_cast<ttn::WGMMADescCreateOp>(op);\n-    if (!wgmmaDescCreateOp)\n-      return mlir::failure();\n-    auto loc = op->getLoc();\n-    auto buffer = wgmmaDescCreateOp.getBuffer();\n-    auto height = wgmmaDescCreateOp.getHeight();\n-    uint32_t mode = static_cast<uint32_t>(wgmmaDescCreateOp.getMode());\n-\n-    auto smem_nvvm_pointer = ptrtoint(i64_ty, buffer);\n-\n-    Value desc = int_val(64, 0);\n-    uint64_t swizzling = (mode == 1 ? 128 : mode == 2 ? 64 : 32);\n-    Value swizzling_ = int_val(64, swizzling);\n-    Value smem_address_bit = smem_nvvm_pointer;\n-\n-    Value strideDimension =\n-        lshr(shl(swizzling_, int_val(64, 3)), int_val(64, 4));\n-    Value height64 = zext(i64_ty, height);\n-    Value leadingDimension = lshr(mul(height64, swizzling_), int_val(64, 4));\n-\n-    // Value baseOffset = int_val(64, 0);\n-    Value startAddr =\n-        lshr(shl(smem_address_bit, int_val(64, 46)), int_val(64, 50));\n-\n-    Value mode_ = int_val(64, mode);\n-    desc = or_(desc, shl(mode_, int_val(64, 62)));\n-    desc = or_(desc, shl(strideDimension, int_val(64, 32)));\n-    desc = or_(desc, shl(leadingDimension, int_val(64, 16)));\n-    // desc = or_(desc, shl(baseOffset, int_val(64, 49)));\n-    desc = or_(desc, startAddr);\n-\n-    rewriter.replaceOp(op, {desc});\n-    return mlir::success();\n+    return operandsAndConstraints;\n   }\n-};\n \n-class OffsetOfSts64OpPattern : public mlir::RewritePattern {\n-public:\n-  OffsetOfSts64OpPattern(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(ttn::OffsetOfSts64Op::getOperationName(), 1,\n-                             context) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto ctx = rewriter.getContext();\n-    auto offsetOfSts64Op = llvm::dyn_cast<ttn::OffsetOfSts64Op>(op);\n-    if (!offsetOfSts64Op)\n-      return mlir::failure();\n-    auto loc = op->getLoc();\n-    auto threadId = offsetOfSts64Op.getThreadId();\n-    auto rowOfWarp = offsetOfSts64Op.getRowOfWarp();\n-    auto elemIdx = offsetOfSts64Op.getElemIdx();\n-    auto leadingDimOffset = offsetOfSts64Op.getLeadingDimOffset();\n-    auto rowStride = offsetOfSts64Op.getRowStride();\n-    auto swizzleEnabled = offsetOfSts64Op.getSwizzleEnabled();\n+  std::string getPtxAsm(ttn::OffsetOfSts64Op op) const {\n+    auto rowStride = op.getRowStride();\n+    auto swizzleEnabled = op.getSwizzleEnabled();\n \n     if (swizzleEnabled) {\n       assert((rowStride == 32 || rowStride == 64 || rowStride == 128) &&\n@@ -1048,51 +898,77 @@ class OffsetOfSts64OpPattern : public mlir::RewritePattern {\n     } else if (rowStride == 32) {\n       perPhase = 4;\n       maxPhase = 2;\n+    } else {\n+      assert(false && \"Unsupported rowStride\");\n     }\n \n-    auto laneId = and_(threadId, i32_val(0x1f));\n-    auto myRow =\n-        add(mul(and_(lshr(elemIdx, i32_val(1)), i32_val(0x1)), i32_val(8)),\n-            udiv(laneId, i32_val(4)));\n-    auto myCol = add(mul(udiv(elemIdx, i32_val(4)), i32_val(8)),\n-                     mul(urem(laneId, i32_val(4)), i32_val(2)));\n-    myRow = add(myRow, rowOfWarp);\n-    auto phase = urem(udiv(myRow, i32_val(perPhase)), i32_val(maxPhase));\n-    auto lineOffset =\n-        add(mul(urem(myRow, i32_val(perPhase)), i32_val(rowStride)),\n-            mul(myCol, i32_val(4)));\n-    auto colOffset =\n-        add(mul(xor_(udiv(lineOffset, i32_val(16)), phase), i32_val(16)),\n-            urem(lineOffset, i32_val(16)));\n-    auto offset =\n-        add(mul(udiv(myRow, i32_val(perPhase)), i32_val(128)), colOffset);\n-\n-    rewriter.replaceOp(op, {offset});\n-    return mlir::success();\n+    auto ptxAsm = \"{\\n\"\n+                  \".reg .u32 a<9>;      \\n\"\n+                  \"and.b32 a0, $1, 0x1f;\\n\" // laneid\n+                  \"shr.b32 a1, $2, 4; \\n\"\n+                  \"and.b32 a1, a1, 0x1; \\n\"\n+                  \"div.u32 a2, a0, 4; \\n\"\n+                  \"mad.lo.u32 a2, a1, 8, a2; \\n\" // myRow\n+                  \"div.u32 a3, $2, 4; \\n\"\n+                  \"rem.u32 a4, a0, 4; \\n\"\n+                  \"mul.lo.u32 a4, a4, 2; \\n\"\n+                  \"mad.lo.u32 a4, a3, 8, a4; \\n\" // myCol\n+                  \"add.u32 a2, a2, $3; \\n\"       // myRow = myRow + rowOfWarp\n+                  \"div.u32 a3, a2, \" +\n+                  std::to_string(perPhase) +\n+                  \"; \\n\"\n+                  \"rem.u32 a3, a3, \" +\n+                  std::to_string(maxPhase) +\n+                  \"; \\n\" // phase\n+                  \"rem.u32 a5, a2, \" +\n+                  std::to_string(perPhase) +\n+                  \"; \\n\" // lineOffset\n+                  \"mul.lo.u32 a5, a5, #rowStride; \\n\"\n+                  \"mad.lo.u32 a5, a4, 4, a5; \\n\" // lineOffset\n+                  \"div.u32 a6, a5, 16; \\n\"\n+                  \"xor.b32 a6, a6, a3; \\n\" // colOffset\n+                  \"rem.u32 a7, a5, 16; \\n\"\n+                  \"mad.lo.u32 a7, a6, 16, a7; \\n\" // colOffset\n+                  \"div.u32 a8, a2, #perPhase; \\n\"\n+                  \"mad.lo.u32 $0, a8, 128, a7; \\n\" // offset\n+                  \"}\";\n+    return ptxAsm;\n   }\n };\n \n-class OffsetOfStmatrixV4OpPattern : public mlir::RewritePattern {\n+class OffsetOfStmatrixV4OpPattern\n+    : public NVGPUOpPatternBase<ttn::OffsetOfStmatrixV4Op,\n+                                OffsetOfStmatrixV4OpPattern> {\n public:\n-  OffsetOfStmatrixV4OpPattern(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(ttn::OffsetOfStmatrixV4Op::getOperationName(), 1,\n-                             context) {}\n+  using Base = NVGPUOpPatternBase<ttn::OffsetOfStmatrixV4Op,\n+                                  OffsetOfStmatrixV4OpPattern>;\n+  using Base::Base;\n \n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto ctx = rewriter.getContext();\n-    auto offsetOfStmatrixV4Op = llvm::dyn_cast<ttn::OffsetOfStmatrixV4Op>(op);\n-    if (!offsetOfStmatrixV4Op)\n-      return mlir::failure();\n-    auto loc = op->getLoc();\n-    auto threadId = offsetOfStmatrixV4Op.getThreadId();\n-    auto rowOfWarp = offsetOfStmatrixV4Op.getRowOfWarp();\n-    auto elemIdx = offsetOfStmatrixV4Op.getElemIdx();\n-    auto leadingDimOffset = offsetOfStmatrixV4Op.getLeadingDimOffset();\n-    auto rowStride = offsetOfStmatrixV4Op.getRowStride();\n-    auto swizzleEnabled = offsetOfStmatrixV4Op.getSwizzleEnabled();\n+  std::vector<std::string>\n+  getOutputConstraints(ttn::OffsetOfStmatrixV4Op op) const {\n+    return {\"=r\"};\n+  }\n \n+  OperandsAndConstraints\n+  getOperandsAndConstraints(ttn::OffsetOfStmatrixV4Op op) const {\n+    OperandsAndConstraints operandsAndConstraints;\n+    auto threadId = op.getThreadId();\n+    auto rowOfWarp = op.getRowOfWarp();\n+    auto elemIdx = op.getElemIdx();\n+\n+    operandsAndConstraints.push_back({threadId, \"r\"});\n+    operandsAndConstraints.push_back({elemIdx, \"r\"});\n+    operandsAndConstraints.push_back({rowOfWarp, \"r\"});\n+\n+    return operandsAndConstraints;\n+  }\n+\n+  std::string getPtxAsm(ttn::OffsetOfStmatrixV4Op op) const {\n+    auto leadingDimOffset = op.getLeadingDimOffset();\n+    auto rowStride = op.getRowStride();\n+    auto swizzleEnabled = op.getSwizzleEnabled();\n+\n+    std::string ptxAsm;\n     if (swizzleEnabled) {\n       uint32_t perPhase = 0;\n       uint32_t maxPhase = 0;\n@@ -1105,43 +981,71 @@ class OffsetOfStmatrixV4OpPattern : public mlir::RewritePattern {\n       } else if (rowStride == 16) {\n         perPhase = 4;\n         maxPhase = 2;\n+      } else {\n+        assert(false && \"Unsupported rowStride\");\n       }\n \n-      Value iterOfCol = udiv(elemIdx, i32_val(8));\n-      Value myRow = add(rowOfWarp, and_(threadId, i32_val(0xf)));\n-      Value myCol =\n-          mul(and_(lshr(threadId, i32_val(4)), i32_val(0x1)), i32_val(8));\n-      myCol = add(myCol, mul(iterOfCol, i32_val(16)));\n-\n-      Value offset0 =\n-          mul(udiv(myCol, i32_val(rowStride)), i32_val(leadingDimOffset));\n-      myCol = urem(myCol, i32_val(rowStride));\n-\n-      Value phase = urem(udiv(myRow, i32_val(perPhase)), i32_val(maxPhase));\n-\n-      Value lineOffset =\n-          add(mul(urem(myRow, i32_val(perPhase)), i32_val(rowStride)), myCol);\n-      Value colOffset =\n-          add(mul(xor_(udiv(lineOffset, i32_val(8)), phase), i32_val(8)),\n-              urem(lineOffset, i32_val(8)));\n-      Value offset1 =\n-          add(mul(udiv(myRow, i32_val(perPhase)), i32_val(64)), colOffset);\n-\n-      Value res = add(offset1, offset0);\n-\n-      rewriter.replaceOp(op, {res});\n+      ptxAsm =\n+          \"{\\n\"\n+          \".reg .u32 a<10>;      \\n\"\n+          \"div.u32 a0, $2, 8; \\n\"    // iterOfCol = udiv(elemIdx, i32_val(8))\n+          \"and.b32 a1, $1, 0xf; \\n\"  // myRow = and_(threadId, i32_val(0xf))\n+          \"add.u32 a1, a1, $3; \\n\"   // myRow = myRow + rowOfWarp\n+          \"shr.b32 a2, $1, 4; \\n\"    // myCol = lshr(threadId, i32_val(4))\n+          \"and.b32 a2, a2, 0x1; \\n\"  // myCol = and_(myCol, i32_val(0x1))\n+          \"mul.lo.u32 a2, a2, 8; \\n\" // myCol = mul(myCol, i32_val(8))\n+          \"mad.lo.u32 a2, a0, 16, a2; \\n\"  // myCol = add(myCol,\n+                                           // mul(iterOfCol, i32_val(16)))\n+          \"div.u32 a3, a2, #rowStride; \\n\" // offset0 = udiv(myCol,\n+                                           // i32_val(rowStride))\n+          \"mul.lo.u32 a3, a3, #leadingDimOffset; \\n\" // offset0 = mul(offset0,\n+                                                     // i32_val(leadingDimOffset))\n+          \"rem.u32 a2, a2, #rowStride; \\n\" // myCol = myCol % rowStride\n+          \"div.u32 a4, a1, \" +\n+          std::to_string(perPhase) +\n+          \"; \\n\" // phase =  myrow // perPhase\n+          \"rem.u32 a4, a4, \" +\n+          std::to_string(maxPhase) +\n+          \"; \\n\" // phase = phase % maxPhase\n+          \"rem.u32 a5, a1, \" +\n+          std::to_string(perPhase) +\n+          \"; \\n\" // lineOffset = urem(myRow, i32_val(perPhase))\n+          \"mad.lo.u32 a5, a5, #rowStride, a2; \\n\" // lineOffset =\n+                                                  // add(mul(lineOffset,\n+                                                  // rowStride), myCol)\n+          \"div.u32 a6, a5, 8; \\n\"  // colOffset = udiv(lineOffset, i32_val(8)\n+          \"xor.b32 a6, a6, a4; \\n\" // colOffset = xor_(colOffset, phase)\n+          \"rem.u32 a7, a5, 8; \\n\"  // temp = urem(lineOffset, i32_val(8)\n+          \"mad.lo.u32 a7, a6, 8, a7; \\n\" // colOffset = add(mul(colOffset,\n+                                         // i32_val(8)), temp)\n+          \"div.u32 a8, a1, \" +\n+          std::to_string(perPhase) +\n+          \"; \\n\" // offset1 = udiv(myRow, i32_val(perPhase))\n+          \"mad.lo.u32 a9, a8, 64, a7; \\n\" // offset1 = add(mul(offset1,\n+                                          // i32_val(64)), colOffset)\n+          \"add.u32 $0, a9, a3; \\n\"        // result = add(offset1, offset0)\n+          \"}\";\n     } else {\n-      Value iterOfCol = udiv(elemIdx, i32_val(4));\n-      Value myRow = add(rowOfWarp, and_(threadId, i32_val(0xf)));\n-      Value myCol =\n-          mul(and_(lshr(threadId, i32_val(4)), i32_val(0x1)), i32_val(8));\n-      myCol = add(myCol, mul(iterOfCol, i32_val(16)));\n-\n-      Value offset =\n-          add(mul(myRow, i32_val(rowStride)), mul(myCol, i32_val(2)));\n-      rewriter.replaceOp(op, {offset});\n+      ptxAsm = \"{\\n\"\n+               \".reg .u64 a<5>;      \\n\"\n+               \"div.u32 a0, $2, 4; \\n\"          // iterOfCol = udiv(elemIdx,\n+                                                // i32_val(4))\n+               \"and.b32 a1, $1, 0xf; \\n\"        // myRow = and_(threadId,\n+                                                // i32_val(0xf))\n+               \"add.u32 a1, a1, $3; \\n\"         // myRow = myRow + rowOfWarp\n+               \"shr.b32 a2, $1, 4; \\n\"          // myCol = lshr(threadId,\n+                                                // i32_val(4))\n+               \"and.b32 a2, a2, 0x1; \\n\"        // myCol = and_(myCol,\n+                                                // i32_val(0x1))\n+               \"mul.lo.u32 a2, a2, 8; \\n\"       // myCol = mul(myCol,\n+                                                // i32_val(8))\n+               \"mul.u32 a3, a1, #rowStride; \\n\" // offset = myRow * rowStride\n+               \"mad.lo.u32 $0, a2, 2, a3; \\n\"   // result = add(mul(myCol,\n+                                                // i32_val(2)), offset)\n+               \"}\\n\";\n     }\n-    return mlir::success();\n+\n+    return ptxAsm;\n   }\n };\n \n@@ -1155,35 +1059,43 @@ class ConvertNVGPUToLLVM : public ConvertNVGPUToLLVMBase<ConvertNVGPUToLLVM> {\n     ModuleOp mod = getOperation();\n     RewritePatternSet patterns(context);\n \n-    patterns.add<CGABarrierSyncOpPattern>(context);\n-    patterns.add<FenceAsyncSharedOpPattern>(context);\n-    patterns.add<WGMMAFenceOpPattern>(context);\n-    patterns.add<WGMMACommitGroupOpPattern>(context);\n-    patterns.add<WGMMAWaitGroupOpPattern>(context);\n-    patterns.add<StoreMatrixOpPattern>(context);\n-    patterns.add<OffsetOfStmatrixV4OpPattern>(context);\n-    patterns.add<WGMMADescCreateOpPattern>(context);\n-    patterns.add<MBarrierInitOpPattern>(context);\n-    patterns.add<MBarrierArriveOpPattern>(context);\n-    patterns.add<MBarrierWaitOpPattern>(context);\n-    patterns.add<ClusterArriveOpPattern>(context);\n-    patterns.add<ClusterWaitOpPattern>(context);\n-    patterns.add<TMALoadTiledOpPattern>(context);\n-    patterns.add<TMAStoreTiledOpPattern>(context);\n-    patterns.add<LoadDSmemOpPattern>(context);\n-    patterns.add<ClusterCTAIdOpPattern>(context);\n-    patterns.add<RegAllocOpPattern>(context);\n-    patterns.add<RegDeallocOpPattern>(context);\n-    patterns.add<WGMMAOpPattern>(context);\n-    patterns.add<NamedBarrierWaitOpPattern>(context);\n-    patterns.add<NamedBarrierArriveOpPattern>(context);\n-\n-    patterns.add<FenceMBarrierInitOpPattern>(context);\n-    patterns.add<StoreDSmemOpPattern>(context);\n-    patterns.add<Sts64OpPattern>(context);\n-    patterns.add<OffsetOfSts64OpPattern>(context);\n-    patterns.add<CGABarrierWaitOpPattern>(context);\n-    patterns.add<CGABarrierArriveOpPattern>(context);\n+#define POPULATE_NVGPU_OP(SRC_OP, ASM)                                         \\\n+  patterns.add<NVGPUOpGenericPattern<SRC_OP>>(context, ASM, Constraints(),     \\\n+                                              Constraints());\n+    POPULATE_NVGPU_OP(ttn::RegAllocOp, Reg_Alloc_Op)\n+    POPULATE_NVGPU_OP(ttn::WGMMAFenceOp, Wgmma_Fence_Op)\n+    POPULATE_NVGPU_OP(ttn::CGABarrierSyncOp, Cga_Barrier_Sync_op)\n+    POPULATE_NVGPU_OP(ttn::WGMMACommitGroupOp, Wgmma_Commit_Group_Op)\n+    POPULATE_NVGPU_OP(ttn::WGMMAWaitGroupOp, Wgmma_Wait_Group_Op)\n+    POPULATE_NVGPU_OP(ttn::ClusterWaitOp, Cluster_Wait_Op)\n+    POPULATE_NVGPU_OP(ttn::FenceMBarrierInitOp, Fence_Mbarrier_Init_Op)\n+    POPULATE_NVGPU_OP(ttn::CGABarrierArriveOp, Cga_Barrier_Arrive_Op)\n+    POPULATE_NVGPU_OP(ttn::CGABarrierWaitOp, Cga_Barrier_Wait_Op)\n+    POPULATE_NVGPU_OP(ttn::RegDeallocOp, Reg_Dealloc_Op)\n+#undef POPULATE_NVGPU_OP\n+    patterns.add<NVGPUOpGenericPattern<ttn::MBarrierInitOp>>(\n+        context, Mbarrier_Init_Op, Constraints(), Constraints({\"r\", \"b\"}));\n+    patterns.add<NVGPUOpGenericPattern<ttn::MBarrierWaitOp>>(\n+        context, Mbarrier_Wait_Op, Constraints(), Constraints({\"r\", \"r\"}));\n+    patterns.add<NVGPUOpGenericPattern<ttn::NamedBarrierArriveOp>>(\n+        context, Named_Barrier_Arrive_Op, Constraints(),\n+        Constraints({\"r\", \"r\"}));\n+    patterns.add<NVGPUOpGenericPattern<ttn::NamedBarrierWaitOp>>(\n+        context, Named_Barrier_Wait_Op, Constraints(), Constraints({\"r\", \"r\"}));\n+    patterns.add<NVGPUOpGenericPattern<ttn::Sts64Op>>(\n+        context, Sts64_Op, Constraints(), Constraints({\"r\", \"r\", \"r\"}));\n+    patterns.add<NVGPUOpGenericPattern<ttn::ClusterCTAIdOp>>(\n+        context, Cluster_Cta_Id_Op, Constraints({\"=r\"}), Constraints());\n+    patterns.add<NVGPUOpGenericPattern<ttn::WGMMADescCreateOp>>(\n+        context, Wgmma_Desc_Create_op, Constraints({\"=l\"}),\n+        Constraints({\"l\", \"l\"}));\n+\n+    patterns.add<FenceAsyncSharedOpPattern, StoreMatrixOpPattern,\n+                 OffsetOfStmatrixV4OpPattern, MBarrierArriveOpPattern,\n+                 ClusterArriveOpPattern, TMALoadTiledOpPattern,\n+                 TMAStoreTiledOpPattern, LoadDSmemOpPattern, WGMMAOpPattern,\n+                 StoreDSmemOpPattern, OffsetOfSts64OpPattern>(context);\n+\n     if (applyPatternsAndFoldGreedily(mod, std::move(patterns)).failed())\n       signalPassFailure();\n   }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/WGMMA.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -115,7 +115,7 @@ class DotOpMmaV3SmemLoader {\n     mode = getModeFromLayout(sharedLayout, widthInByte);\n \n     baseDesc = rewriter.create<triton::nvgpu::WGMMADescCreateOp>(\n-        loc, i64_ty, base, i32_val(shape[ord[1]]), mode);\n+        loc, base, i32_val(shape[ord[1]]), mode);\n   }\n \n   Value smemLoad(int a, int b) {"}, {"filename": "test/NVGPU/test_cga.mlir", "status": "modified", "additions": 0, "deletions": 8, "changes": 8, "file_content_changes": "@@ -17,14 +17,6 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 2 :\n     %ptr = llvm.mlir.null : !llvm.ptr<i32, 3>\n \n     // CHECK: llvm.inline_asm\n-    // CHECK: llvm.inline_asm\n-    // CHECK: llvm.inline_asm\n-    // CHECK: llvm.inline_asm\n-    // CHECK: llvm.inline_asm\n-    // CHECK: llvm.mul\n-    // CHECK: llvm.add\n-    // CHECK: llvm.mul\n-    // CHECK: llvm.add\n     %v = nvgpu.cluster_id\n     llvm.store %v, %ptr : !llvm.ptr<i32, 3>\n "}, {"filename": "test/NVGPU/test_wgmma.mlir", "status": "modified", "additions": 4, "deletions": 30, "changes": 34, "file_content_changes": "@@ -5,37 +5,11 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32,  \"triton_gpu.num-ctas\" = 2\n     %buffer = llvm.mlir.null : !llvm.ptr<i64, 3>\n     %height = arith.constant 16 : i32\n     // CHECK: llvm.ptrtoint\n-    // CHECK: llvm.shl\n-    // CHECK: llvm.lshr\n-    // CHECK: llvm.zext\n-    // CHECK: llvm.mul\n-    // CHECK: llvm.lshr\n-    // CHECK: llvm.shl\n-    // CHECK: llvm.lshr\n-    // CHECK: llvm.shl\n-    // CHECK: llvm.or\n-    // CHECK: llvm.shl\n-    // CHECK: llvm.or\n-    // CHECK: llvm.shl\n-    // CHECK: llvm.or\n-    // CHECK: llvm.or\n-    %descA = nvgpu.wgmma_desc_create %buffer, %height {mode = 2 : i32}: (!llvm.ptr<i64, 3>, i32) -> (i64)\n+    // CHECK: llvm.inline_asm\n+    %descA = nvgpu.wgmma_desc_create %buffer, %height {mode = 2 : i32, swizzling = 64 : i64}: (!llvm.ptr<i64, 3>, i32) -> (i64)\n     // CHECK: llvm.ptrtoint\n-    // CHECK: llvm.shl\n-    // CHECK: llvm.lshr\n-    // CHECK: llvm.zext\n-    // CHECK: llvm.mul\n-    // CHECK: llvm.lshr\n-    // CHECK: llvm.shl\n-    // CHECK: llvm.lshr\n-    // CHECK: llvm.shl\n-    // CHECK: llvm.or\n-    // CHECK: llvm.shl\n-    // CHECK: llvm.or\n-    // CHECK: llvm.shl\n-    // CHECK: llvm.or\n-    // CHECK: llvm.or\n-    %descB = nvgpu.wgmma_desc_create %buffer, %height {mode = 2 : i32}: (!llvm.ptr<i64, 3>, i32) -> (i64)\n+    // CHECK: llvm.inline_asm\n+    %descB = nvgpu.wgmma_desc_create %buffer, %height {mode = 2 : i32, swizzling = 64 : i64}: (!llvm.ptr<i64, 3>, i32) -> (i64)\n \n     // CHECK-COUNT-32: llvm.extractvalue\n     // CHECK: llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 {$0,$1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,$18,$19,$20,$21,$22,$23,$24,$25,$26,$27,$28,$29,$30,$31}, $64, $65, 1, 1, 1, 0, 1;\""}]