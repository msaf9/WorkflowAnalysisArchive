[{"filename": "python/test/unit/hopper/test_gemm.py", "status": "modified", "additions": 87, "deletions": 76, "changes": 163, "file_content_changes": "@@ -211,102 +211,113 @@ def matmul_kernel(\n \n \n @pytest.mark.parametrize('BLOCK_M,BLOCK_N,BLOCK_K,NUM_WARPS,NUM_CTAS,M,N,K,TRANS_A,TRANS_B,TRANS_OUTPUT,epilogue,out_dtype,USE_TMA_STORE,NUM_STAGES,ENABLE_WS',\n-                         [(128, 128, 64, 4, 1, *shape_w_c, 'none', out_dtype, use_tma_store, 3, enable_ws)\n-                          for shape_w_c in [\n-                             # badcase from cublas-important-layers\n-                             [4096, 1, 1024, False, False, True],\n-                             [2048, 204, 1000, True, False, True],\n-                             [4096, 1, 1024, False, False, False],\n-                             [2048, 204, 1000, True, False, False],\n-                         ]\n+                         [\n+                             # corner shapes\n+                             (128, 128, 64, 4, 1, *shape_w_c, 'none', out_dtype, use_tma_store, 3, enable_ws)\n+                             for shape_w_c in [\n+                                 [4096, 1, 1024, False, False, True],\n+                                 [2048, 204, 1000, True, False, True],\n+                                 [4096, 1, 1024, False, False, False],\n+                                 [2048, 204, 1000, True, False, False],\n+                             ]\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n                              for enable_ws in [False, True]\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n-                              # softmax works for one CTA\n-                              for shape_w_c in [\n-                             [64, 64, 16, 4, 1, 64, 64, 64],\n-                             [128, 128, 64, 4, 1, None, None, None],\n-                             [16, 16, 64, 4, 1, 16, 16, 64],\n-                             [64, 64, 32, 8, 1, 64, 64, 64],\n-                             [128, 128, 64, 4, 1, 128, 128, 128],\n-                         ]\n+                         ] + [\n+                             # softmax epilogue\n+                             (*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for shape_w_c in [\n+                                 [64, 64, 16, 4, 1, 64, 64, 64],\n+                                 [128, 128, 64, 4, 1, None, None, None],\n+                                 [16, 16, 64, 4, 1, 16, 16, 64],\n+                                 [64, 64, 32, 8, 1, 64, 64, 64],\n+                                 [128, 128, 64, 4, 1, 128, 128, 128],\n+                             ]\n                              for epilogue in ['softmax']\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n-                             for trans_a in [False, True]\n-                             for trans_b in [False, True]\n-                             for trans_output in [False, True]\n+                             for trans_a in [False,]\n+                             for trans_b in [True,]\n+                             for trans_output in [False,]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n-                              for shape_w_c in [\n-                             [64, 64, 16, 4, 1, 128, 128, 64],\n-                             *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4, 8] for num_ctas in [1, 2, 4]],\n-                             # for chain-dot\n-                             [128, 128, 64, 4, 1, None, None, None],\n-                             [64, 64, 16, 4, 1, None, None, None],\n-                             # small BLOCK_M and BLOCK_K\n-                             [16, 16, 64, 4, 1, 128, 128, 64],\n-                             *[[16, 32, 64, num_warps, num_ctas, 256, 256, 256] for num_warps in [4, 8] for num_ctas in [1, 2]],\n-                             # repeat\n-                             [64, 64, 32, 8, 1, 128, 256, 64],\n-                             [64, 64, 16, 8, 2, 128, 128, 64],\n-                             # irregular shape\n-                             [128, 128, 64, 4, 1, 500, 200, 128],\n-                             [128, 128, 64, 4, 2, 513, 193, 192],\n-                         ]\n+                         ] + [\n+                             # loop over epilogues besides of softmax\n+                             (*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for shape_w_c in [\n+                                 [64, 64, 16, 4, 1, 128, 128, 64],\n+                                 *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4, 8] for num_ctas in [1, 2, 4]],\n+                                 # for chain-dot\n+                                 [128, 128, 64, 4, 1, None, None, None],\n+                                 [64, 64, 16, 4, 1, None, None, None],\n+                                 # small BLOCK_M and BLOCK_K\n+                                 [16, 16, 64, 4, 1, 128, 128, 64],\n+                                 *[[16, 32, 64, num_warps, num_ctas, 256, 256, 256] for num_warps in [4, 8] for num_ctas in [1, 2]],\n+                                 # repeat\n+                                 [64, 64, 32, 8, 1, 128, 256, 64],\n+                                 [64, 64, 16, 8, 2, 128, 128, 64],\n+                                 # irregular shape\n+                                 [128, 128, 64, 4, 1, 500, 200, 128],\n+                                 [128, 128, 64, 4, 2, 513, 193, 192],\n+                             ]\n                              for epilogue in ['none', 'add-matrix', 'add-rows', 'add-cols', 'chain-dot']\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n-                             for trans_a in [False, True]\n-                             for trans_b in [False, True]\n-                             for trans_output in [False, True]\n+                             for trans_a in [False,]\n+                             for trans_b in [True,]\n+                             for trans_output in [False,]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n                              if not (epilogue == 'chain-dot' and (shape_w_c[6] is not None or shape_w_c[1] != shape_w_c[6]))\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n-                              for shape_w_c in [\n-                             [64, 64, 32, 4, 1, 128, 256, 64],\n-                             [128, 128, 16, 4, 4, 512, 256, 64],\n-                             [128, 256, 32, 4, 8, 256, 256, 192],\n-                             [512, 256, 32, 4, 8, 1024, 256, 192],\n-                             # BLOCK_K >= 128\n-                             [64, 128, 128, 4, 1, 512, 256, 256],\n-                             [128, 128, 128, 4, 1, 256, 256, 192],\n-                             [128, 128, 128, 4, 2, 256, 256, 192],\n-                             # small BLOCK_M and BLOCK_K\n-                             [16, 32, 32, 4, 1, 128, 256, 64],\n-                             [32, 32, 16, 4, 1, 256, 256, 192],\n-                             [16, 32, 64, 4, 4, 512, 256, 64],\n-                         ]\n-                             for out_dtype in ['float16', 'float32']\n-                             for use_tma_store in [False, True]\n+                         ] + [\n+                             # loop over tile shapes and transpose combinations\n+                             (*shape_w_c, trans_a, trans_b, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for shape_w_c in [\n+                                 [64, 64, 32, 4, 1, 128, 256, 64],\n+                                 [128, 128, 16, 4, 4, 512, 256, 64],\n+                                 [128, 256, 32, 4, 8, 256, 256, 192],\n+                                 [512, 256, 32, 4, 8, 1024, 256, 192],\n+                                 # BLOCK_K >= 128\n+                                 [64, 128, 128, 4, 1, 512, 256, 256],\n+                                 [128, 128, 128, 4, 1, 256, 256, 192],\n+                                 [128, 128, 128, 4, 2, 256, 256, 192],\n+                                 # small BLOCK_M and BLOCK_K\n+                                 [16, 32, 32, 4, 1, 128, 256, 64],\n+                                 [32, 32, 16, 4, 1, 256, 256, 192],\n+                                 [16, 32, 64, 4, 4, 512, 256, 64],\n+                             ]\n+                             for out_dtype in ['float32',]\n+                             for use_tma_store in [False,]\n                              for trans_a in [False, True]\n                              for trans_b in [False, True]\n                              for trans_output in [False, True]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n-                         ] + [(64, n, 16, 4, 1, 512, 256, 256, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n-                              # loop over instr shapes\n-                              for n in [16, 32, 64, 128, 256]\n-                              for trans_output in [False, True]\n-                              for out_dtype in ['float16', 'float32']\n-                              for use_tma_store in [False, True]\n-                              for num_stages in [2, 4, 5, 7]\n-                              for enable_ws in [False, True]\n-                              ] + [(*shape_w_c, *shape, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n-                                   # irregular shapes\n-                                   for shape_w_c in [\n-                                       [128, 128, 64, 4, 1],\n-                                       [256, 128, 64, 4, 2],\n-                                       [128, 128, 128, 4, 2],\n-                              ]\n-                             for shape in list(itertools.product([*range(512, 4096, 360)], [*range(512, 4096, 360)], [512, 1024]))\n-                             for trans_output in [False, True]\n-                             for out_dtype in ['float16', 'float32']\n+                         ] + [\n+                             # loop over instr shapes & pipeline stages\n+                             (64, n, 16, 4, 1, 512, 256, 256, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for n in [16, 32, 64, 128, 256]\n+                             for trans_output in [False,]\n+                             for out_dtype in ['float32',]\n+                             for use_tma_store in [False,]\n+                             for num_stages in [2, 4, 5, 7]\n+                             for enable_ws in [False, True]\n+                         ] + [\n+                             # irregular shapes\n+                             (*shape_w_c, *shape, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for shape_w_c in [\n+                                 [128, 128, 64, 4, 1],\n+                                 [256, 128, 64, 4, 2],\n+                                 [128, 128, 128, 4, 2],\n+                             ]\n+                             for shape in [\n+                                 [512, 360, 1024],\n+                                 [360, 4096, 512],\n+                             ]\n+                             for trans_output in [False,]\n+                             for out_dtype in ['float32',]\n                              for use_tma_store in [False, True]\n-                             for num_stages in [2, 3, 4]\n+                             for num_stages in [3, 4]\n                              for enable_ws in [False, True]\n                          ])\n @pytest.mark.skipif(torch.cuda.get_device_capability()"}, {"filename": "python/test/unit/hopper/test_persistent_warp_specialized_gemm.py", "status": "modified", "additions": 39, "deletions": 32, "changes": 71, "file_content_changes": "@@ -696,9 +696,9 @@ def full_static_persistent_matmul_kernel(\n \n @pytest.mark.parametrize('BLOCK_M,BLOCK_N,BLOCK_K,NUM_WARPS,NUM_CTAS,M,N,K,TRANS_A,TRANS_B,epilogue,out_dtype,USE_TMA_STORE,NUM_STAGES,ENABLE_WS',\n                          [\n+                             # corner shapes\n                              (128, 128, 64, 4, 1, *shape_w_c, 'none', out_dtype, use_tma_store, 3, enable_ws)\n                              for shape_w_c in [\n-                                 # bad from cublas-important-layers\n                                  [4096, 1, 1024, False, False],\n                                  [2048, 204, 1000, True, False],\n                                  [16, 524288, 32, False, True],\n@@ -707,6 +707,7 @@ def full_static_persistent_matmul_kernel(\n                              for use_tma_store in [False, True]\n                              for enable_ws in [True]\n                          ] + [\n+                             # softmax epilogue\n                              (*shape_w_c, trans_a, trans_b, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n                              # softmax works for one CTA\n                              for shape_w_c in [\n@@ -720,11 +721,12 @@ def full_static_persistent_matmul_kernel(\n                              for epilogue in ['softmax']\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n-                             for trans_a in [False, True]\n-                             for trans_b in [False, True]\n+                             for trans_a in [False,]\n+                             for trans_b in [True,]\n                              for num_stages in [3]\n                              for enable_ws in [True]\n                          ] + [\n+                             # loop over tile shapes and transpose combinations\n                              (*shape_w_c, trans_a, trans_b, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                              for shape_w_c in [\n                                  [64, 64, 32, 4, 1, 128, 256, 64],\n@@ -740,58 +742,63 @@ def full_static_persistent_matmul_kernel(\n                                  [32, 32, 16, 4, 1, 256, 256, 192],\n                                  [16, 32, 64, 4, 4, 512, 256, 64],\n                              ]\n-                             for out_dtype in ['float16', 'float32']\n-                             for use_tma_store in [False, True]\n+                             for out_dtype in ['float32',]\n+                             for use_tma_store in [False,]\n                              for trans_a in [False, True]\n                              for trans_b in [False, True]\n                              for num_stages in [3]\n                              for enable_ws in [True]\n-                         ] + [(*shape_w_c, trans_a, trans_b, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n-                              for shape_w_c in [\n-                             [64, 64, 16, 4, 1, 128, 128, 64],\n-                             *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4] for num_ctas in [1, 2, 4]],\n-                             # for chain-dot\n-                             [128, 128, 64, 4, 1, None, None, None],\n-                             [64, 64, 16, 4, 1, None, None, None],\n-                             # small BLOCK_M and BLOCK_K\n-                             [16, 16, 64, 4, 1, 128, 128, 64],\n-                             *[[16, 32, 64, num_warps, num_ctas, 256, 256, 256] for num_warps in [4] for num_ctas in [1, 2]],\n-                             #  # TODO: enable when num_warps != 4 is supported.\n-                             #  # repeat\n-                             #  # [64, 64, 32, 8, 1, 128, 256, 64],\n-                             #  # [64, 64, 16, 8, 2, 128, 128, 64],\n-                             # irregular shape\n-                             [128, 128, 64, 4, 1, 500, 200, 128],\n-                             [128, 128, 64, 4, 1, 513, 193, 192],\n-                         ]\n+                         ] + [\n+                             # loop over epilogues besides of softmax\n+                             (*shape_w_c, trans_a, trans_b, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for shape_w_c in [\n+                                 [64, 64, 16, 4, 1, 128, 128, 64],\n+                                 *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4] for num_ctas in [1, 2, 4]],\n+                                 # for chain-dot\n+                                 [128, 128, 64, 4, 1, None, None, None],\n+                                 [64, 64, 16, 4, 1, None, None, None],\n+                                 # small BLOCK_M and BLOCK_K\n+                                 [16, 16, 64, 4, 1, 128, 128, 64],\n+                                 *[[16, 32, 64, num_warps, num_ctas, 256, 256, 256] for num_warps in [4] for num_ctas in [1, 2]],\n+                                 #  # TODO: enable when num_warps != 4 is supported.\n+                                 #  # repeat\n+                                 #  # [64, 64, 32, 8, 1, 128, 256, 64],\n+                                 #  # [64, 64, 16, 8, 2, 128, 128, 64],\n+                                 # irregular shape\n+                                 [128, 128, 64, 4, 1, 500, 200, 128],\n+                                 [128, 128, 64, 4, 1, 513, 193, 192],\n+                             ]\n                              for epilogue in ['none', 'add-matrix', 'add-rows', 'add-cols', 'chain-dot']\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n-                             for trans_a in [False, True]\n-                             for trans_b in [False, True]\n+                             for trans_a in [False,]\n+                             for trans_b in [True,]\n                              for num_stages in [3]\n                              for enable_ws in [True]\n                              if not (epilogue == 'chain-dot' and (shape_w_c[5] is not None or shape_w_c[0] != shape_w_c[1]))\n                          ] + [\n+                             # loop over instr shapes & pipeline stages\n                              (64, n, 16, 4, 1, 512, 256, 256, False, True, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n-                             # loop over instr shapes\n                              for n in [16, 32, 64, 128, 256]\n-                             for out_dtype in ['float16', 'float32']\n-                             for use_tma_store in [False, True]\n+                             for out_dtype in ['float32']\n+                             for use_tma_store in [False,]\n                              for num_stages in [2, 4, 5, 7]\n                              for enable_ws in [True]\n                          ] + [\n-                             (*shape_w_c, *shape, False, True, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                              # irregular shapes\n+                             (*shape_w_c, *shape, False, True, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                              for shape_w_c in [\n                                  [128, 128, 64, 4, 1],\n                                  [256, 128, 64, 4, 2],\n                                  [128, 128, 128, 4, 2]\n                              ]\n-                             for shape in list(itertools.product([*range(512, 4096, 360)], [*range(512, 4096, 360)], [512, 1024]))\n-                             for out_dtype in ['float16', 'float32']\n+                             for shape in [\n+                                 [512, 360, 1024],\n+                                 [360, 4096, 512],\n+                             ]\n+                             for out_dtype in ['float32']\n                              for use_tma_store in [False, True]\n-                             for num_stages in [2, 3, 4]\n+                             for num_stages in [3, 4]\n                              for enable_ws in [True]\n                          ]\n                          )"}]