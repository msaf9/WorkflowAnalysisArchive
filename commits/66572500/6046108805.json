[{"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -346,7 +346,7 @@ struct ConvertLayoutOpConversion\n     SmallVector<unsigned> numCTAsEachRep(rank, 1);\n     SmallVector<unsigned> shapePerCTATile = getShapePerCTATile(layout, shape);\n     SmallVector<int64_t> shapePerCTA = getShapePerCTA(layout, shape);\n-    auto elemTy = type.getElementType();\n+    auto elemTy = getTypeConverter()->convertType(type.getElementType());\n \n     int ctaId = 0;\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -247,7 +247,8 @@ class TritonGPUOptimizeDotOperandsPass\n \n     mlir::RewritePatternSet patterns(context);\n     patterns.add<ConvertTransConvert>(context);\n-    patterns.add<MoveOpAfterLayoutConversion>(context);\n+    if (triton::gpu::TritonGPUDialect::getComputeCapability(m) >= 80)\n+      patterns.add<MoveOpAfterLayoutConversion>(context);\n     patterns.add<FuseTransHopper>(context);\n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n       signalPassFailure();"}, {"filename": "test/TritonGPU/dot-operands.mlir", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -15,7 +15,7 @@\n #BLR = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #BLC = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [0, 1]}>\n \n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.compute-capability\" = 80} {\n \n // CHECK: tt.func @push_elementwise\n // CHECK: %[[ALOAD:.*]] = tt.load %arg0\n@@ -69,7 +69,7 @@ tt.func @succeeds_if_arg_is_not_convert_layout(\n #blockedA = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n #blockedB = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n #mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}>\n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.compute-capability\" = 80} {\n \n // CHECK: #[[BA:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0], CTAsPerCGA = [], CTASplitNum = [], CTAOrder = []}>\n // CHECK: #[[BB:.*]] = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1], CTAsPerCGA = [], CTASplitNum = [], CTAOrder = []}>\n@@ -104,7 +104,7 @@ tt.func @push_convert_both_operands(\n #blockedA = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n #blockedB = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n #mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}>\n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.compute-capability\" = 80} {\n \n // CHECK: #[[BA:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0], CTAsPerCGA = [], CTASplitNum = [], CTAOrder = []}>\n // CHECK: #[[BB:.*]] = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1], CTAsPerCGA = [], CTASplitNum = [], CTAOrder = []}>"}]