[{"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "modified", "additions": 49, "deletions": 10, "changes": 59, "file_content_changes": "@@ -21,11 +21,25 @@ template <class T> SmallVector<unsigned, 4> argSort(const T &arr) {\n   return ret;\n }\n \n+unsigned getElementBitWidth(const Value &val) {\n+  auto valType = val.getType();\n+  if (valType.isa<PointerType>())\n+    valType = valType.cast<PointerType>().getPointeeType();\n+  auto tensorType = valType.cast<RankedTensorType>();\n+\n+  auto typeForMem =\n+      tensorType.getElementType().isa<PointerType>()\n+          ? tensorType.getElementType().cast<PointerType>().getPointeeType()\n+          : tensorType.getElementType();\n+  return typeForMem.getIntOrFloatBitWidth();\n+}\n+\n typedef DenseMap<Value, std::function<Type(Type)>> LayoutMap;\n \n struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n   Attribute getCoalescedEncoding(ModuleAxisInfoAnalysis &axisInfoAnalysis,\n-                                 Value ptr, int numWarps, int threadsPerWarp) {\n+                                 Value ptr, Operation *op, int numWarps,\n+                                 int threadsPerWarp) {\n     auto refType = ptr.getType();\n     if (refType.isa<PointerType>())\n       refType = refType.cast<PointerType>().getPointeeType();\n@@ -74,6 +88,18 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n       order = argSort(queryAxisInfo(ptr).getContiguity());\n     }\n \n+    auto matchesOrder = [&refTensorType](const Value &val) {\n+      if (val.getType() == refTensorType) {\n+        return true;\n+      }\n+\n+      auto rttType = val.getType().dyn_cast<RankedTensorType>();\n+      if (!rttType) {\n+        return false;\n+      }\n+      return rttType.getShape() == refTensorType.getShape();\n+    };\n+\n     // The desired divisibility is the maximum divisibility\n     // among all dependent pointers who have the same order as\n     // `ptr`.\n@@ -83,7 +109,7 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     if (refType.isa<RankedTensorType>() && ptr.getDefiningOp()) {\n       for (Operation *op : mlir::multiRootGetSlice(ptr.getDefiningOp())) {\n         for (Value val : op->getResults()) {\n-          if (val.getType() != refTensorType)\n+          if (!matchesOrder(val))\n             continue;\n           auto currOrder =\n               argSort(axisInfoAnalysis.getAxisInfo(val)->getContiguity());\n@@ -109,11 +135,11 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n \n     // Thread tile size depends on memory alignment\n     SmallVector<unsigned, 4> sizePerThread(refTensorType.getRank(), 1);\n-    unsigned elemNumBits = typeForMem.getIntOrFloatBitWidth();\n-    unsigned elemNumBytes = std::max(elemNumBits / 8, 1u);\n     unsigned perThread = 1;\n     for (Value val : withSameOrder) {\n       auto valInfo = queryAxisInfo(val);\n+      unsigned elemNumBits = getElementBitWidth(val);\n+      unsigned elemNumBytes = std::max(elemNumBits / 8, 1u);\n       unsigned maxMultipleBytes = valInfo.getDivisibility(order[0]);\n       unsigned maxMultiple = std::max(maxMultipleBytes / elemNumBytes, 1u);\n       unsigned maxContig =\n@@ -122,7 +148,20 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n       unsigned currPerThread = std::min(alignment, 128 / elemNumBits);\n       perThread = std::max(perThread, currPerThread);\n     }\n-    sizePerThread[order[0]] = std::min<int>(perThread, numElemsPerThread);\n+\n+    perThread = std::min<int>(perThread, numElemsPerThread);\n+\n+    if (!dyn_cast<triton::LoadOp>(op)) {\n+      // For ops that can result in a global memory write, we should enforce\n+      // that each thread handles at most 128 bits, which is the widest\n+      // available vectorized store op; otherwise, the store will have \"gaps\"\n+      // in the memory write at the warp level, resulting in worse performance.\n+      // For loads, we can expect that the gaps won't matter due to the L1\n+      // cache.\n+      unsigned elemNumBits = getElementBitWidth(ptr);\n+      perThread = std::min<int>(perThread, 128 / elemNumBits);\n+    }\n+    sizePerThread[order[0]] = perThread;\n \n     auto CTALayout = triton::gpu::getCTALayout(refTensorType.getEncoding());\n     return triton::gpu::BlockedEncodingAttr::get(\n@@ -132,9 +171,9 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n \n   std::function<Type(Type)>\n   getTypeConverter(ModuleAxisInfoAnalysis &axisInfoAnalysis, Value ptr,\n-                   int numWarps, int threadsPerWarp) {\n-    Attribute encoding =\n-        getCoalescedEncoding(axisInfoAnalysis, ptr, numWarps, threadsPerWarp);\n+                   Operation *op, int numWarps, int threadsPerWarp) {\n+    Attribute encoding = getCoalescedEncoding(axisInfoAnalysis, ptr, op,\n+                                              numWarps, threadsPerWarp);\n     return [encoding](Type type) {\n       RankedTensorType tensorType = type.cast<RankedTensorType>();\n       return RankedTensorType::get(tensorType.getShape(),\n@@ -240,8 +279,8 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n       int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n       int threadsPerWarp =\n           triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n-      auto convertType =\n-          getTypeConverter(axisInfoAnalysis, ptr, numWarps, threadsPerWarp);\n+      auto convertType = getTypeConverter(axisInfoAnalysis, ptr, curr, numWarps,\n+                                          threadsPerWarp);\n       layoutMap[ptr] = convertType;\n     });\n "}, {"filename": "test/TritonGPU/coalesce.mlir", "status": "modified", "additions": 68, "deletions": 0, "changes": 68, "file_content_changes": "@@ -69,3 +69,71 @@ tt.func @load_tensor(%arg0: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32}, %arg1:\n }\n \n }\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+module attributes {\"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+\n+\n+// CHECK: [[NARROW_LAYOUT:#.*]] = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+// CHECK: [[WIDE_LAYOUT:#.*]] = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+tt.func public @load_tensors_two_types(%arg0: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32}, %arg3: i32) attributes {noinline = false} {\n+    %c1024_i32 = arith.constant 1024 : i32\n+    %0 = tt.get_program_id x : i32\n+    %1 = arith.muli %0, %c1024_i32 : i32\n+    %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #blocked>\n+    %3 = tt.splat %1 : (i32) -> tensor<1024xi32, #blocked>\n+    %4 = arith.addi %3, %2 : tensor<1024xi32, #blocked>\n+    %5 = tt.splat %arg3 : (i32) -> tensor<1024xi32, #blocked>\n+    %6 = \"triton_gpu.cmpi\"(%4, %5) <{predicate = 2 : i64}> : (tensor<1024xi32, #blocked>, tensor<1024xi32, #blocked>) -> tensor<1024xi1, #blocked>\n+    %7 = tt.splat %arg0 : (!tt.ptr<f32, 1>) -> tensor<1024x!tt.ptr<f32, 1>, #blocked>\n+    %8 = tt.addptr %7, %4 : tensor<1024x!tt.ptr<f32, 1>, #blocked>, tensor<1024xi32, #blocked>\n+    %9 = tt.load %8, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked>\n+    %10 = tt.splat %arg1 : (!tt.ptr<f16, 1>) -> tensor<1024x!tt.ptr<f16, 1>, #blocked>\n+    %11 = tt.addptr %10, %4 : tensor<1024x!tt.ptr<f16, 1>, #blocked>, tensor<1024xi32, #blocked>\n+    %12 = tt.load %11, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf16, #blocked>\n+    %13 = arith.extf %12 : tensor<1024xf16, #blocked> to tensor<1024xf32, #blocked>\n+    %14 = arith.addf %9, %13 : tensor<1024xf32, #blocked>\n+    %15 = tt.splat %arg2 : (!tt.ptr<f32, 1>) -> tensor<1024x!tt.ptr<f32, 1>, #blocked>\n+    %16 = tt.addptr %15, %4 : tensor<1024x!tt.ptr<f32, 1>, #blocked>, tensor<1024xi32, #blocked>\n+    // CHECK: tt.store {{.*}} : tensor<1024xf32, [[WIDE_LAYOUT]]>\n+    tt.store %16, %14, %6 {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf32, #blocked>\n+    tt.return\n+}\n+\n+}\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+module attributes {\"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+\n+// CHECK-NOT: sizePerThread = [4]\n+// CHECK: #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+// CHECK-NOT: sizePerThread = [4]\n+tt.func public @load_tensors_two_types(%arg0: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg3: i32) attributes {noinline = false} {\n+    %c1024_i32 = arith.constant 1024 : i32\n+    %0 = tt.get_program_id x : i32\n+    %1 = arith.muli %0, %c1024_i32 : i32\n+    %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #blocked>\n+    %3 = tt.splat %1 : (i32) -> tensor<1024xi32, #blocked>\n+    %4 = arith.addi %3, %2 : tensor<1024xi32, #blocked>\n+    %5 = tt.splat %arg3 : (i32) -> tensor<1024xi32, #blocked>\n+    %6 = \"triton_gpu.cmpi\"(%4, %5) <{predicate = 2 : i64}> : (tensor<1024xi32, #blocked>, tensor<1024xi32, #blocked>) -> tensor<1024xi1, #blocked>\n+    %7 = tt.splat %arg0 : (!tt.ptr<f32, 1>) -> tensor<1024x!tt.ptr<f32, 1>, #blocked>\n+    %8 = tt.addptr %7, %4 : tensor<1024x!tt.ptr<f32, 1>, #blocked>, tensor<1024xi32, #blocked>\n+    %9 = tt.load %8, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked>\n+    %10 = tt.splat %arg1 : (!tt.ptr<f16, 1>) -> tensor<1024x!tt.ptr<f16, 1>, #blocked>\n+    %11 = tt.addptr %10, %4 : tensor<1024x!tt.ptr<f16, 1>, #blocked>, tensor<1024xi32, #blocked>\n+    %12 = tt.load %11, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf16, #blocked>\n+    %13 = arith.extf %12 : tensor<1024xf16, #blocked> to tensor<1024xf32, #blocked>\n+    %14 = arith.addf %9, %13 : tensor<1024xf32, #blocked>\n+    %15 = tt.splat %arg2 : (!tt.ptr<f16, 1>) -> tensor<1024x!tt.ptr<f16, 1>, #blocked>\n+    %16 = tt.addptr %15, %4 : tensor<1024x!tt.ptr<f16, 1>, #blocked>, tensor<1024xi32, #blocked>\n+    %17 = arith.truncf %14 : tensor<1024xf32, #blocked> to tensor<1024xf16, #blocked>\n+    tt.store %16, %17, %6 {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf16, #blocked>\n+    tt.return\n+}\n+\n+}"}]