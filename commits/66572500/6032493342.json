[{"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 0, "deletions": 8, "changes": 8, "file_content_changes": "@@ -511,14 +511,6 @@ std::function<void(int, int)> getLoadMatrixFn(\n   const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n   auto order = sharedLayout.getOrder();\n \n-  if (tensor.getType()\n-          .cast<RankedTensorType>()\n-          .getElementType()\n-          .isa<mlir::Float8E4M3B11FNUZType, mlir::Float8E4M3FNType>()) {\n-    bool noTrans = (isA ^ (order[0] == 0));\n-    assert(noTrans && \"float8e4b15 must have row-col layout\");\n-  }\n-\n   if (kWidth != (4 / elemBytes))\n     assert(vecPhase == 1 || vecPhase == 4 * kWidth);\n "}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 1, "deletions": 11, "changes": 12, "file_content_changes": "@@ -85,6 +85,7 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                                      (\"float8e4nv\", \"float8e4nv\"),\n                                      (\"float8e5\", \"float8e4nv\"),\n                                      (\"float8e5\", \"float8e5\"),\n+                                     (\"float8e4b15\", \"float8e4b15\"),\n                                      (\"float8e4nv\", \"float16\"),\n                                      (\"float16\", \"float8e5\"),\n                                      (\"float16\", \"float32\"),\n@@ -105,17 +106,6 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                                      (\"bfloat16\", \"float32\"),\n                                      (\"float32\", \"bfloat16\")] for AT in [False, True] for BT in [False, True]\n         ],\n-        *[\n-            # float8e4b15 only supports row-col layout\n-            [\n-                (128, 128, 32, 1, 4, 2, None, None, None, False, True, ADTYPE, BDTYPE, True),\n-            ] for ADTYPE, BDTYPE in [(\"float8e4b15\", \"float8e5\"),\n-                                     (\"float8e4b15\", \"float16\"),\n-                                     (\"float16\", \"float8e4b15\"),\n-                                     (\"float8e5\", \"float8e5\"),\n-                                     (\"float8e4nv\", \"float8e4nv\"),\n-                                     (\"int8\", \"int8\")]\n-        ]\n     ),\n )\n def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, ADTYPE, BDTYPE, ALLOW_TF32):"}]