[{"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 55, "deletions": 30, "changes": 85, "file_content_changes": "@@ -748,6 +748,25 @@ static void rewriteSlice(SetVector<Value> &slice,\n   rewriteSlice(slice, layout, convertOp, mapping);\n }\n \n+static LogicalResult getRematerializableSlice(\n+    Value root, Attribute rootEncoding, SetVector<Value> &slice,\n+    DenseMap<Value, Attribute> &layout,\n+    std::function<bool(Operation *)> stopPropagation = nullptr) {\n+  LogicalResult result = getConvertBackwardSlice(root, slice, rootEncoding,\n+                                                 layout, stopPropagation);\n+  if (result.failed() || slice.empty())\n+    return failure();\n+\n+  // Check if all the operations in the slice can be rematerialized.\n+  for (Value v : slice) {\n+    if (Operation *op = v.getDefiningOp()) {\n+      if (!canBeRemat(op))\n+        return failure();\n+    }\n+  }\n+  return success();\n+}\n+\n static void backwardRematerialization(ConvertLayoutOp convertOp) {\n   // we don't want to rematerialize any conversion to/from shared\n   if (triton::gpu::isSharedEncoding(convertOp.getResult()) ||\n@@ -759,22 +778,16 @@ static void backwardRematerialization(ConvertLayoutOp convertOp) {\n   if (targetType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n     return;\n \n-  // 1. Take a backward slice of all the tensor dependencies.\n+  // 1. Take a backward slice of all the tensor dependencies that can be\n+  // rematerialized.\n   SetVector<Value> slice;\n   DenseMap<Value, Attribute> layout;\n-  LogicalResult result = getConvertBackwardSlice(\n-      convertOp.getOperand(), slice, targetType.getEncoding(), layout);\n-  if (result.failed() || slice.empty())\n+  LogicalResult result = getRematerializableSlice(\n+      convertOp.getOperand(), targetType.getEncoding(), slice, layout);\n+  if (result.failed())\n     return;\n \n-  // 2. Check if all the operations in the slice can be rematerialized.\n-  for (Value v : slice) {\n-    if (Operation *op = v.getDefiningOp()) {\n-      if (!canBeRemat(op))\n-        return;\n-    }\n-  }\n-  // 3. Rewrite the slice.\n+  // 2. Rewrite the slice.\n   rewriteSlice(slice, layout, convertOp);\n }\n \n@@ -791,32 +804,44 @@ static void hoistConvertOnTopOfExt(ConvertLayoutOp convertOp) {\n   if (targetType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n     return;\n \n-  // 1. Take a backward slice of all the tensor dependencies.\n-  SetVector<Value> slice;\n-  DenseMap<Value, Attribute> layout;\n   auto isExtOp = [](Operation *op) {\n     return isa<arith::ExtSIOp, arith::ExtUIOp, arith::ExtFOp>(op);\n   };\n-  // Get a backward slice but don't go past ext ops\n-  LogicalResult result = getConvertBackwardSlice(\n-      convertOp.getOperand(), slice, targetType.getEncoding(), layout, isExtOp);\n-  if (result.failed() || slice.empty())\n+  // 1. Take a backward slice of all the tensor dependencies.\n+  SetVector<Value> slice;\n+  DenseMap<Value, Attribute> layout;\n+  LogicalResult result = getRematerializableSlice(\n+      convertOp.getOperand(), targetType.getEncoding(), slice, layout, isExtOp);\n+  if (result.failed())\n     return;\n+\n   Operation *extOp = nullptr;\n-  // 2. Check if all the operations in the slice can be rematerialized.\n-  for (Value v : slice) {\n-    if (Operation *op = v.getDefiningOp()) {\n-      if (!canBeRemat(op))\n-        return;\n-      if (isExtOp(op)) {\n-        // Only apply it if there is a single ext op otherwise we would have to\n-        // duplicate the convert.\n-        if (extOp != nullptr)\n-          return;\n-        extOp = op;\n+  unsigned sliceSize = slice.size();\n+  for (unsigned i = 0; i < sliceSize; i++) {\n+    Value v = slice[i];\n+    Operation *op = v.getDefiningOp();\n+    if (!op)\n+      continue;\n+    if (isExtOp(op)) {\n+      SetVector<Value> tempSlice;\n+      DenseMap<Value, Attribute> tempLayout;\n+      LogicalResult result = getRematerializableSlice(\n+          op->getOperand(0), layout[v], tempSlice, tempLayout);\n+      // If we can rematerialize the rest of the ext slice we can ignore this\n+      // ext as it won't need a convert.\n+      if (result.succeeded()) {\n+        slice.insert(tempSlice.begin(), tempSlice.end());\n+        layout.insert(tempLayout.begin(), tempLayout.end());\n+        continue;\n       }\n+      // Only apply it if there is a single ext op otherwise we would have to\n+      // duplicate the convert.\n+      if (extOp != nullptr)\n+        return;\n+      extOp = op;\n     }\n   }\n+\n   if (extOp == nullptr)\n     return;\n   // Move the convert before the ext op and rewrite the slice."}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 19, "deletions": 3, "changes": 22, "file_content_changes": "@@ -85,12 +85,28 @@ tt.func @hoist_above_ext(%arg0: tensor<1024xf16, #layout0>, %arg1: f32) -> tenso\n // CHECK-NOT: triton_gpu.convert_layout\n // CHECK: tt.return\n   %0 = arith.extf %arg0 : tensor<1024xf16, #layout0> to tensor<1024xf32, #layout0>\n-  %1 = tt.splat %arg1 : (f32) -> tensor<1024xf32, #layout1>\n-  %2 = triton_gpu.convert_layout %0 : (tensor<1024xf32, #layout0>) -> tensor<1024xf32, #layout1>\n-  %3 = arith.addf %1, %2 : tensor<1024xf32, #layout1>\n+  %1 = tt.splat %arg1 : (f32) -> tensor<1024xf32, #layout0>\n+  %2 = arith.addf %0, %1 : tensor<1024xf32, #layout0>\n+  %3 = triton_gpu.convert_layout %2 : (tensor<1024xf32, #layout0>) -> tensor<1024xf32, #layout1>\n   tt.return %3 : tensor<1024xf32, #layout1>\n }\n \n+// CHECK-LABEL: hoist_above_ext2\n+tt.func @hoist_above_ext2(%arg0: tensor<1024xf16, #layout0>, %arg1: f16) -> tensor<1024xf32, #layout1> {\n+// CHECK: %[[CVT:.+]] = triton_gpu.convert_layout\n+// CHECK: arith.extf %[[CVT]]\n+// CHECK-NOT: triton_gpu.convert_layout\n+// CHECK: tt.return\n+  %0 = arith.extf %arg0 : tensor<1024xf16, #layout0> to tensor<1024xf32, #layout0>\n+  %1 = tt.splat %arg1 : (f16) -> tensor<1024xf16, #layout0>\n+  %2 = arith.extf %1 : tensor<1024xf16, #layout0> to tensor<1024xf32, #layout0>\n+  %3 = arith.addf %0, %2 : tensor<1024xf32, #layout0>\n+  %4 = triton_gpu.convert_layout %3 : (tensor<1024xf32, #layout0>) -> tensor<1024xf32, #layout1>\n+  tt.return %4 : tensor<1024xf32, #layout1>\n+}\n+\n+\n+\n // CHECK-LABEL: if\n tt.func @if(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout"}]