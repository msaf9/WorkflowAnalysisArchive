[{"filename": "bin/triton-translate.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -125,7 +125,7 @@ LogicalResult tritonTranslateMain(int argc, char **argv,\n   llvm::LLVMContext llvmContext;\n   mlir::triton::gpu::TMAMetadataTy tmaInfos;\n   auto llvmir = translateTritonGPUToLLVMIR(\n-      &llvmContext, *module, SMArch.getValue(), tmaInfos, false /*isRocm*/);\n+      &llvmContext, *module, SMArch.getValue(), tmaInfos, Target::Default);\n \n   if (!llvmir) {\n     llvm::errs() << \"Translate to LLVM IR failed\";"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/Passes.td", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "file_content_changes": "@@ -30,9 +30,13 @@ def ConvertTritonGPUToLLVM : Pass<\"convert-triton-gpu-to-llvm\", \"mlir::ModuleOp\"\n         Option<\"tmaMetadata\", \"tma-metadata\",\n                \"mlir::triton::gpu::TMAMetadataTy*\", /*default*/\"nullptr\",\n                \"tma metadata to the runtime\">,\n-        Option<\"isROCM\", \"is-rocm\",\n-               \"bool\", /*default*/\"false\",\n-               \"compile for ROCM-compatible LLVM\">,\n+        Option<\"target\", \"target\", \"enum Target\", \"mlir::triton::Target::Default\",\n+               \"compile for target compatible LLVM\",\n+               \"llvm::cl::values(\"\n+               \"clEnumValN(mlir::triton::Target::NVVM, \\\"nvvm\\\", \\\"compile for \"\n+               \"NVVM-compatible LLVM\\\"), \"\n+               \"clEnumValN(mlir::triton::Target::ROCDL, \\\"rocdl\\\", \\\"compile for \"\n+               \"ROCDL-compatible LLVM\\\"))\">,\n     ];\n }\n "}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -14,6 +14,8 @@ template <typename T> class OperationPass;\n \n namespace triton {\n \n+enum Target { NVVM, ROCDL, Default = NVVM };\n+\n #define GEN_PASS_DECL\n #include \"triton/Conversion/TritonGPUToLLVM/Passes.h.inc\"\n "}, {"filename": "include/triton/Target/LLVMIR/LLVMIRTranslation.h", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -1,5 +1,6 @@\n #ifndef TRITON_TARGET_LLVM_IR_LLVM_IR_TRANSLATION_H\n #define TRITON_TARGET_LLVM_IR_LLVM_IR_TRANSLATION_H\n+#include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n #include \"triton/Target/PTX/TmaMetadata.h\"\n #include \"llvm/ADT/StringRef.h\"\n #include <memory>\n@@ -28,15 +29,15 @@ std::unique_ptr<llvm::Module>\n translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n                            mlir::ModuleOp module, int computeCapability,\n                            mlir::triton::gpu::TMAMetadataTy &tmaInfos,\n-                           bool isROCM);\n+                           Target target);\n \n // Translate mlir LLVM dialect to LLVMIR, return null if failed.\n std::unique_ptr<llvm::Module>\n translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module,\n-                      bool isROCM);\n+                      Target target);\n \n bool linkExternLib(llvm::Module &module, llvm::StringRef name,\n-                   llvm::StringRef path, bool isROCM);\n+                   llvm::StringRef path, Target target);\n \n } // namespace triton\n } // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 24, "deletions": 15, "changes": 39, "file_content_changes": "@@ -63,14 +63,17 @@ static void addWSNamedAttrs(Operation *op,\n \n class TritonLLVMFunctionConversionTarget : public ConversionTarget {\n public:\n-  explicit TritonLLVMFunctionConversionTarget(MLIRContext &ctx, bool isROCM)\n+  explicit TritonLLVMFunctionConversionTarget(MLIRContext &ctx, Target target)\n       : ConversionTarget(ctx) {\n     addLegalDialect<index::IndexDialect>();\n     addLegalDialect<LLVM::LLVMDialect>();\n-    if (isROCM) {\n-      addLegalDialect<ROCDL::ROCDLDialect>();\n-    } else {\n+    switch (target) {\n+    case Target::NVVM:\n       addLegalDialect<NVVM::NVVMDialect>();\n+      break;\n+    case Target::ROCDL:\n+      addLegalDialect<ROCDL::ROCDLDialect>();\n+      break;\n     }\n     addLegalOp<mlir::UnrealizedConversionCastOp>();\n   }\n@@ -359,13 +362,16 @@ struct CallOpConversion : public ConvertOpToLLVMPattern<triton::CallOp> {\n \n class TritonLLVMConversionTarget : public ConversionTarget {\n public:\n-  explicit TritonLLVMConversionTarget(MLIRContext &ctx, bool isROCM)\n+  explicit TritonLLVMConversionTarget(MLIRContext &ctx, Target target)\n       : ConversionTarget(ctx) {\n     addLegalDialect<LLVM::LLVMDialect>();\n-    if (isROCM) {\n-      addLegalDialect<ROCDL::ROCDLDialect>();\n-    } else {\n+    switch (target) {\n+    case Target::NVVM:\n       addLegalDialect<NVVM::NVVMDialect>();\n+      break;\n+    case Target::ROCDL:\n+      addLegalDialect<ROCDL::ROCDLDialect>();\n+      break;\n     }\n     addLegalDialect<mlir::triton::nvgpu::NVGPUDialect>();\n     addIllegalDialect<triton::TritonDialect>();\n@@ -387,7 +393,7 @@ struct ConvertTritonGPUToLLVM\n     mlir::LowerToLLVMOptions option(context);\n     option.overrideIndexBitwidth(32);\n     TritonGPUToLLVMTypeConverter typeConverter(context, option);\n-    TritonLLVMConversionTarget target(*context, isROCM);\n+    TritonLLVMConversionTarget convTarget(*context, target);\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n     int numCTAs = triton::gpu::TritonGPUDialect::getNumCTAs(mod);\n     int threadsPerWarp = triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n@@ -441,7 +447,7 @@ struct ConvertTritonGPUToLLVM\n     {\n       mlir::LowerToLLVMOptions option(context);\n       TritonGPUToLLVMTypeConverter typeConverter(context, option);\n-      TritonLLVMFunctionConversionTarget funcTarget(*context, isROCM);\n+      TritonLLVMFunctionConversionTarget funcTarget(*context, target);\n       RewritePatternSet funcPatterns(context);\n       funcPatterns.add<FuncOpConversion>(typeConverter, numWarps, allocation,\n                                          /*benefit=*/1);\n@@ -461,7 +467,7 @@ struct ConvertTritonGPUToLLVM\n     {\n       mlir::LowerToLLVMOptions option(context);\n       TritonGPUToLLVMTypeConverter typeConverter(context, option);\n-      TritonLLVMFunctionConversionTarget funcTarget(*context, isROCM);\n+      TritonLLVMFunctionConversionTarget funcTarget(*context, target);\n       RewritePatternSet funcPatterns(context);\n       funcPatterns.add<CallOpConversion>(typeConverter, numWarps, allocation,\n                                          /*benefit=*/1);\n@@ -539,16 +545,19 @@ struct ConvertTritonGPUToLLVM\n     mlir::populateMathToLLVMConversionPatterns(typeConverter, patterns);\n \n     // Native lowering patterns\n-    if (isROCM) {\n+    switch (target) {\n+    case Target::NVVM:\n+      mlir::populateGpuToNVVMConversionPatterns(typeConverter, patterns);\n+      break;\n+    case Target::ROCDL:\n       mlir::populateGpuToROCDLConversionPatterns(typeConverter, patterns,\n                                                  mlir::gpu::amd::HIP);\n-    } else {\n-      mlir::populateGpuToNVVMConversionPatterns(typeConverter, patterns);\n+      break;\n     }\n \n     mlir::cf::populateControlFlowToLLVMConversionPatterns(typeConverter,\n                                                           patterns);\n-    if (failed(applyPartialConversion(mod, target, std::move(patterns))))\n+    if (failed(applyPartialConversion(mod, convTarget, std::move(patterns))))\n       return signalPassFailure();\n \n     // Fold CTAId when there is only 1 CTA."}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 16, "deletions": 14, "changes": 30, "file_content_changes": "@@ -55,7 +55,7 @@ struct NVVMMetadata {\n \n // Add the nvvm related metadata to LLVM IR.\n static void amendLLVMFunc(llvm::Function *func, const NVVMMetadata &metadata,\n-                          bool isROCM) {\n+                          Target target) {\n   auto *module = func->getParent();\n   auto &ctx = func->getContext();\n \n@@ -85,16 +85,19 @@ static void amendLLVMFunc(llvm::Function *func, const NVVMMetadata &metadata,\n   }\n \n   if (metadata.isKernel) {\n-    if (isROCM) {\n-      func->setCallingConv(llvm::CallingConv::AMDGPU_KERNEL);\n-      func->addFnAttr(\"amdgpu-flat-work-group-size\", \"1, 1024\");\n-    } else {\n+    switch (target) {\n+    case Target::NVVM: {\n       llvm::Metadata *mdArgs[] = {\n           llvm::ValueAsMetadata::get(func), llvm::MDString::get(ctx, \"kernel\"),\n           llvm::ValueAsMetadata::get(\n               llvm::ConstantInt::get(llvm::Type::getInt32Ty(ctx), 1))};\n       module->getOrInsertNamedMetadata(\"nvvm.annotations\")\n           ->addOperand(llvm::MDNode::get(ctx, mdArgs));\n+    } break;\n+    case Target::ROCDL: {\n+      func->setCallingConv(llvm::CallingConv::AMDGPU_KERNEL);\n+      func->addFnAttr(\"amdgpu-flat-work-group-size\", \"1, 1024\");\n+    } break;\n     }\n   }\n }\n@@ -240,7 +243,7 @@ static void linkLibdevice(llvm::Module &module) {\n }\n \n bool linkExternLib(llvm::Module &module, llvm::StringRef name,\n-                   llvm::StringRef path, bool isROCM) {\n+                   llvm::StringRef path, Target target) {\n   llvm::SMDiagnostic err;\n   auto &ctx = module.getContext();\n \n@@ -259,8 +262,7 @@ bool linkExternLib(llvm::Module &module, llvm::StringRef name,\n     return true;\n   }\n \n-  // check if ROCM\n-  if (!isROCM) {\n+  if (target == Target::NVVM) {\n     if (name == \"libdevice\") {\n       linkLibdevice(module);\n     }\n@@ -274,7 +276,7 @@ bool linkExternLib(llvm::Module &module, llvm::StringRef name,\n \n std::unique_ptr<llvm::Module>\n translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module,\n-                      bool isROCM) {\n+                      Target target) {\n   DialectRegistry registry;\n   mlir::registerBuiltinDialectTranslation(registry);\n   mlir::registerLLVMDialectTranslation(registry);\n@@ -302,7 +304,7 @@ translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module,\n   // dead code.\n   auto externLibs = getExternLibs(module);\n   for (auto &lib : externLibs) {\n-    if (linkExternLib(*llvmModule, lib.first, lib.second, isROCM))\n+    if (linkExternLib(*llvmModule, lib.first, lib.second, target))\n       return nullptr;\n   }\n \n@@ -318,7 +320,7 @@ translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module,\n   for (auto &func : llvmModule->functions()) {\n     auto it = nvvmMetadata.find(func.getName());\n     if (it != nvvmMetadata.end())\n-      amendLLVMFunc(&func, it->second, isROCM);\n+      amendLLVMFunc(&func, it->second, target);\n   }\n \n   return llvmModule;\n@@ -328,7 +330,7 @@ std::unique_ptr<llvm::Module>\n translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n                            mlir::ModuleOp module, int computeCapability,\n                            mlir::triton::gpu::TMAMetadataTy &tmaInfos,\n-                           bool isROCM) {\n+                           Target target) {\n   mlir::PassManager pm(module->getContext());\n   mlir::registerPassManagerCLOptions();\n   if (failed(applyPassManagerCLOptions(pm))) {\n@@ -351,7 +353,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n   pm.addPass(mlir::createConvertSCFToCFPass());\n   pm.addPass(mlir::createConvertIndexToLLVMPass());\n   pm.addPass(\n-      createConvertTritonGPUToLLVMPass({computeCapability, &tmaInfos, isROCM}));\n+      createConvertTritonGPUToLLVMPass({computeCapability, &tmaInfos, target}));\n   pm.addPass(createConvertNVGPUToLLVMPass());\n   pm.addPass(mlir::createArithToLLVMConversionPass());\n   pm.addPass(mlir::createCanonicalizerPass());\n@@ -366,7 +368,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n     return nullptr;\n   }\n \n-  auto llvmIR = translateLLVMToLLVMIR(llvmContext, module, isROCM);\n+  auto llvmIR = translateLLVMToLLVMIR(llvmContext, module, target);\n   if (!llvmIR) {\n     llvm::errs() << \"Translate to LLVM IR failed\";\n     return nullptr;"}, {"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -56,7 +56,7 @@ static void linkExternal(llvm::Module &module) {\n   //     std::filesystem::path(__BUILD_DIR__) / \"lib\" / \"Hopper\" /\n   //     \"libhopper_helpers.bc\";\n   if (mlir::triton::linkExternLib(module, \"libhopper_helpers\", path.string(),\n-                                  /*isROCM*/ false))\n+                                  mlir::triton::Target::NVVM))\n     llvm::errs() << \"Link failed for: libhopper_helpers.bc\";\n }\n "}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "file_content_changes": "@@ -81,6 +81,11 @@ void init_triton_runtime(py::module &&m) {\n       .value(\"CUDA\", CUDA)\n       .value(\"ROCM\", ROCM)\n       .export_values();\n+\n+  py::enum_<mlir::triton::Target>(m, \"TARGET\")\n+      .value(\"NVVM\", mlir::triton::NVVM)\n+      .value(\"ROCDL\", mlir::triton::ROCDL)\n+      .export_values();\n }\n \n // A custom op builder that keeps track of the last location\n@@ -1804,11 +1809,12 @@ void init_triton_translation(py::module &m) {\n   m.def(\n       \"translate_triton_gpu_to_llvmir\",\n       [](mlir::ModuleOp op, int computeCapability,\n-         mlir::triton::gpu::TMAMetadataTy &tmaInfos, bool isROCM) {\n+         mlir::triton::gpu::TMAMetadataTy &tmaInfos,\n+         mlir::triton::Target target) {\n         py::gil_scoped_release allow_threads;\n         llvm::LLVMContext llvmContext;\n         auto llvmModule = ::mlir::triton::translateTritonGPUToLLVMIR(\n-            &llvmContext, op, computeCapability, tmaInfos, isROCM);\n+            &llvmContext, op, computeCapability, tmaInfos, target);\n         if (!llvmModule)\n           llvm::report_fatal_error(\"Failed to translate TritonGPU to LLVM IR.\");\n "}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -13,7 +13,7 @@\n \n from .._C.libtriton.triton import (ClusterInfo, TMAInfos, add_external_libs,\n                                    compile_ptx_to_cubin, get_env_vars, get_num_warps,\n-                                   get_shared_memory_size, ir,\n+                                   get_shared_memory_size, ir, runtime,\n                                    translate_llvmir_to_hsaco, translate_llvmir_to_ptx,\n                                    translate_triton_gpu_to_llvmir)\n from ..common.backend import get_backend, path_to_ptxas\n@@ -142,9 +142,9 @@ def ttgir_to_llir(mod, extern_libs, arch, tma_infos):\n         _add_external_libs(mod, extern_libs)\n     # TODO: separate tritongpu_to_llvmir for different backends\n     if _is_cuda(arch):\n-        return translate_triton_gpu_to_llvmir(mod, arch, tma_infos, False)\n+        return translate_triton_gpu_to_llvmir(mod, arch, tma_infos, runtime.TARGET.NVVM)\n     else:\n-        return translate_triton_gpu_to_llvmir(mod, 0, True)\n+        return translate_triton_gpu_to_llvmir(mod, 0, TMAInfos(), runtime.TARGET.ROCDL)\n \n \n # PTX translation"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -split-input-file --convert-triton-gpu-to-llvm | FileCheck %s\n+// RUN: triton-opt %s -split-input-file --convert-triton-gpu-to-llvm=\"target=nvvm\" | FileCheck %s\n \n module attributes {\"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK: llvm.func @test_empty_kernel(%arg0: i64, %arg1: !llvm.ptr<f16, 1>)"}]