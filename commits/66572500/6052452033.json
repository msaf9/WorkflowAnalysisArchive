[{"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 26, "deletions": 6, "changes": 32, "file_content_changes": "@@ -914,6 +914,18 @@ struct StoreAsyncOpConversion\n   const TensorPtrMapT *tensorPtrMap;\n };\n \n+namespace {\n+void createBarrier(ConversionPatternRewriter &rewriter, Location loc,\n+                   int numCTAs) {\n+  if (numCTAs == 1) {\n+    barrier();\n+  } else {\n+    rewriter.create<triton::nvidia_gpu::ClusterArriveOp>(loc, false);\n+    rewriter.create<triton::nvidia_gpu::ClusterWaitOp>(loc);\n+  }\n+}\n+} // namespace\n+\n struct AtomicCASOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::AtomicCASOp>,\n       public LoadStoreConversionBase {\n@@ -934,6 +946,10 @@ struct AtomicCASOpConversion\n     auto loc = op.getLoc();\n     MLIRContext *ctx = rewriter.getContext();\n \n+    auto moduleOp = op->getParentOfType<ModuleOp>();\n+    assert(moduleOp && \"Parent ModuleOp not found for AtomicCASOp\");\n+    int numCTAs = triton::gpu::TritonGPUDialect::getNumCTAs(moduleOp);\n+\n     Value llPtr = adaptor.getPtr();\n     Value llCmp = adaptor.getCmp();\n     Value llVal = adaptor.getVal();\n@@ -971,7 +987,7 @@ struct AtomicCASOpConversion\n     atom.global().o(semStr).o(\"cas\").o(\"b32\");\n     atom(dstOpr, ptrOpr, cmpOpr, valOpr).predicate(mask);\n     auto old = ptxBuilderAtomicCAS.launch(rewriter, loc, valueElemTy);\n-    barrier();\n+    createBarrier(rewriter, loc, numCTAs);\n \n     PTXBuilder ptxBuilderStore;\n     auto *dstOprStore = ptxBuilderStore.newAddrOperand(atomPtr, \"r\");\n@@ -981,9 +997,9 @@ struct AtomicCASOpConversion\n     st(dstOprStore, valOprStore).predicate(mask);\n     auto ASMReturnTy = void_ty(ctx);\n     ptxBuilderStore.launch(rewriter, loc, ASMReturnTy);\n-    barrier();\n+    createBarrier(rewriter, loc, numCTAs);\n     Value ret = load(atomPtr);\n-    barrier();\n+    createBarrier(rewriter, loc, numCTAs);\n     rewriter.replaceOp(op, {ret});\n     return success();\n   }\n@@ -1008,7 +1024,11 @@ struct AtomicRMWOpConversion\n                   ConversionPatternRewriter &rewriter) const override {\n     auto loc = op.getLoc();\n     MLIRContext *ctx = rewriter.getContext();\n-    //\n+\n+    auto moduleOp = op->getParentOfType<ModuleOp>();\n+    assert(moduleOp && \"Parent ModuleOp not found for AtomicRMWOp\");\n+    int numCTAs = triton::gpu::TritonGPUDialect::getNumCTAs(moduleOp);\n+\n     auto atomicRmwAttr = op.getAtomicRmwOp();\n \n     Value val = op.getVal();\n@@ -1139,9 +1159,9 @@ struct AtomicRMWOpConversion\n         auto *valOpr = ptxBuilderStore.newOperand(old, tyId);\n         storeShared(ptrOpr, valOpr).predicate(rmwMask);\n         ptxBuilderStore.launch(rewriter, loc, void_ty(ctx));\n-        barrier();\n+        createBarrier(rewriter, loc, numCTAs);\n         Value ret = load(atomPtr);\n-        barrier();\n+        createBarrier(rewriter, loc, numCTAs);\n         rewriter.replaceOp(op, {ret});\n       }\n     }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -11,6 +11,7 @@ using ::mlir::LLVM::delinearize;\n using ::mlir::LLVM::linearize;\n using ::mlir::LLVM::shflSync;\n using ::mlir::LLVM::storeShared;\n+using ::mlir::triton::gpu::getCTASplitNum;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n \n@@ -28,6 +29,12 @@ struct ReduceOpConversion\n   LogicalResult\n   matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n+    // When cross-CTA reduction is implemented in the future, this assertion can\n+    // be removed\n+    assert(isReduceWithinCTA(op) &&\n+           \"Layout optimization passes such as PlanCTAPass and \"\n+           \"RemoveLayoutConversionPass should avoid cross-CTA reduction\");\n+\n     if (ReduceOpHelper(op).isFastReduction())\n       return matchAndRewriteFast(op, adaptor, rewriter);\n     return matchAndRewriteBasic(op, adaptor, rewriter);\n@@ -36,6 +43,15 @@ struct ReduceOpConversion\n private:\n   int computeCapability;\n \n+  bool isReduceWithinCTA(triton::ReduceOp op) const {\n+    auto axis = op.getAxis();\n+    ReduceOpHelper helper(op);\n+    auto srcLayout = helper.getSrcLayout();\n+    auto CTASplitNum = getCTASplitNum(srcLayout);\n+    assert(axis < CTASplitNum.size());\n+    return CTASplitNum[axis] == 1;\n+  }\n+\n   void accumulate(ConversionPatternRewriter &rewriter, Region &combineOp,\n                   SmallVector<Value> &acc, ValueRange cur, bool isFirst) const {\n     if (isFirst) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 15, "deletions": 6, "changes": 21, "file_content_changes": "@@ -541,6 +541,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto tensorTy = valueTy.dyn_cast<RankedTensorType>();\n     Value mask = int_val(1, 1);\n     auto tid = tid_val();\n+    auto clusterCTAId = getClusterCTAId(rewriter, loc);\n     if (tensorTy) {\n       auto layout = tensorTy.getEncoding();\n       auto shape = tensorTy.getShape();\n@@ -576,7 +577,6 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n         auto CTASplitNum = triton::gpu::getCTASplitNum(layout);\n         auto CTAOrder = triton::gpu::getCTAOrder(layout);\n \n-        auto clusterCTAId = getClusterCTAId(rewriter, loc);\n         auto multiDimClusterCTAId =\n             delinearize(rewriter, loc, clusterCTAId, CTAsPerCGA, CTAOrder);\n \n@@ -586,14 +586,23 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n             continue;\n           // This wrapping rule must be consistent with emitCTAOffsetForLayout\n           unsigned splitNum = std::min<unsigned>(shape[dim], CTASplitNum[dim]);\n-          multiDimClusterCTAId[dim] =\n-              urem(multiDimClusterCTAId[dim], i32_val(splitNum));\n-          mask = and_(mask, icmp_eq(multiDimClusterCTAId[dim], _0));\n+          Value repId = udiv(multiDimClusterCTAId[dim], i32_val(splitNum));\n+          // Consider the example where CTAsPerCGA = [4] and CTASplitNum = [2]:\n+          //     CTA0 and CTA2 holds data of block0,\n+          //     CTA1 and CTA3 holds data of block1.\n+          // Only CTA0 and CTA1 are expected to write while CTA2 and CTA3 should\n+          // be masked. We add the following mask:\n+          //     multiDimClusterCTAId[dim] / splitNum == 0\n+          // Actually in all existing cases of multicast, splitNum is always 1.\n+          // The mask is equivalent to:\n+          //     multiDimClusterCTAId[dim] == 0\n+          mask = and_(mask, icmp_eq(repId, _0));\n         }\n       }\n     } else {\n-      // If the tensor is not ranked, then it is a scalar and only thread 0 can\n-      // write\n+      // If the tensor is not ranked, then it is a scalar and only thread 0 of\n+      // CTA0 can write\n+      mask = and_(mask, icmp_eq(clusterCTAId, i32_val(0)));\n       mask = and_(mask, icmp_eq(tid, i32_val(0)));\n     }\n     return mask;"}]