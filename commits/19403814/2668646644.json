[{"filename": "master/.buildinfo", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n # Sphinx build info version 1\n # This file hashes the configuration used when building these files. When it is not found, a full rebuild will be done.\n-config: 255e5b7e27427f0ab6fda308ee6aef63\n+config: 8d52c5eda79abb41e578ed40b306519c\n tags: 645f666f9bcd5a90fca523b33c5a78b7"}, {"filename": "master/.doctrees/environment.pickle", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/getting-started/installation.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/getting-started/tutorials/01-vector-add.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/getting-started/tutorials/02-fused-softmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/getting-started/tutorials/03-matrix-multiplication.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/getting-started/tutorials/04-low-memory-dropout.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/getting-started/tutorials/05-layer-norm.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/getting-started/tutorials/06-fused-attention.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/getting-started/tutorials/07-libdevice-function.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/getting-started/tutorials/index.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/getting-started/tutorials/sg_execution_times.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/index.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/programming-guide/chapter-1/introduction.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/programming-guide/chapter-2/related-work.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.Config.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.autotune.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.heuristics.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.jit.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.arange.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.atomic_add.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.atomic_and.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.atomic_cas.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.atomic_max.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.atomic_min.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.atomic_or.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.atomic_xchg.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.atomic_xor.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.broadcast_to.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.cos.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.dot.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.exp.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.load.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.log.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.max.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.maximum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.min.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.minimum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.multiple_of.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.num_programs.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.program_id.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.rand.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.randint.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.randint4x.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.randn.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.ravel.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.reshape.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.sigmoid.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.sin.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.softmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.sqrt.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.store.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.sum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.where.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.zeros.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.testing.Benchmark.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.testing.do_bench.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.testing.perf_report.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/triton.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/triton.language.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/triton.testing.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/_downloads/1bc2e471d2fb0ec017c4d1d0890db4e2/07-libdevice-function.ipynb", "status": "added", "additions": 97, "deletions": 0, "changes": 97, "file_content_changes": "@@ -0,0 +1,97 @@\n+{\n+  \"cells\": [\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"%matplotlib inline\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"\\n# Libdevice function\\nTriton can invoke a custom function from an external library.\\nIn this example, we will use the `libdevice` library to apply `asin` on a tensor.\\nPlease refer to https://docs.nvidia.com/cuda/libdevice-users-guide/index.html regarding the semantics of all available libdevice functions.\\n\\nIn `trition/language/libdevice.py`, we try to aggregate functions with the same computation but different data types together.\\nFor example, both `__nv_asin` and `__nvasinf` calculate the principal value of the arc sine of the input, but `__nv_asin` operates on `double` and `__nv_asinf` operates on `float`.\\nUsing triton, you can simply call `tl.libdevice.asinf`.\\ntriton automatically selects the correct underlying device function to invoke based on input and output types.\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## asin Kernel\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.jit\\ndef asin_kernel(\\n    x_ptr,\\n    y_ptr,\\n    n_elements,\\n    BLOCK_SIZE: tl.constexpr,\\n):\\n    pid = tl.program_id(axis=0)\\n    block_start = pid * BLOCK_SIZE\\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\\n    mask = offsets < n_elements\\n    x = tl.load(x_ptr + offsets, mask=mask)\\n    x = tl.libdevice.asin(x)\\n    tl.store(y_ptr + offsets, x, mask=mask)\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Using the default libdevice library path\\nWe can use the default libdevice library path encoded in `triton/language/libdevice.py`\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"torch.manual_seed(0)\\nsize = 98432\\nx = torch.rand(size, device='cuda')\\noutput_triton = torch.zeros(size, device='cuda')\\noutput_torch = torch.asin(x)\\nassert x.is_cuda and output_triton.is_cuda\\nn_elements = output_torch.numel()\\ngrid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\\nasin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024)\\nprint(output_torch)\\nprint(output_triton)\\nprint(\\n    f'The maximum difference between torch and triton is '\\n    f'{torch.max(torch.abs(output_torch - output_triton))}'\\n)\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Customize the libdevice library path\\nWe can also customize the libdevice library path by passing the path to the `libdevice` library to the `asin` kernel.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"output_triton = torch.empty_like(x)\\nasin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024,\\n                  extern_libs={'libdevice': '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'})\\nprint(output_torch)\\nprint(output_triton)\\nprint(\\n    f'The maximum difference between torch and triton is '\\n    f'{torch.max(torch.abs(output_torch - output_triton))}'\\n)\"\n+      ]\n+    }\n+  ],\n+  \"metadata\": {\n+    \"kernelspec\": {\n+      \"display_name\": \"Python 3\",\n+      \"language\": \"python\",\n+      \"name\": \"python3\"\n+    },\n+    \"language_info\": {\n+      \"codemirror_mode\": {\n+        \"name\": \"ipython\",\n+        \"version\": 3\n+      },\n+      \"file_extension\": \".py\",\n+      \"mimetype\": \"text/x-python\",\n+      \"name\": \"python\",\n+      \"nbconvert_exporter\": \"python\",\n+      \"pygments_lexer\": \"ipython3\",\n+      \"version\": \"3.8.10\"\n+    }\n+  },\n+  \"nbformat\": 4,\n+  \"nbformat_minor\": 0\n+}\n\\ No newline at end of file"}, {"filename": "master/_downloads/3176accb6c7288b0e45f41d94eebacb9/06-fused-attention.ipynb", "status": "added", "additions": 54, "deletions": 0, "changes": 54, "file_content_changes": "@@ -0,0 +1,54 @@\n+{\n+  \"cells\": [\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"%matplotlib inline\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"\\n# Fused Attention\\nThis is a Triton implementation of the Flash Attention algorithm \\n(see: Dao et al., https://arxiv.org/pdf/2205.14135v2.pdf; Rabe and Staats https://arxiv.org/pdf/2112.05682v2.pdf)\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import pytest\\nimport torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.jit\\ndef _fwd_kernel(\\n    Q, K, V, sm_scale,\\n    TMP, L, M,  # NOTE: TMP is a scratchpad buffer to workaround a compiler bug\\n    Out,\\n    stride_qz, stride_qh, stride_qm, stride_qk,\\n    stride_kz, stride_kh, stride_kn, stride_kk,\\n    stride_vz, stride_vh, stride_vk, stride_vn,\\n    stride_oz, stride_oh, stride_om, stride_on,\\n    Z, H, N_CTX,\\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\\n    BLOCK_N: tl.constexpr,\\n):\\n    start_m = tl.program_id(0)\\n    off_hz = tl.program_id(1)\\n    # initialize offsets\\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\\n    offs_n = tl.arange(0, BLOCK_N)\\n    offs_d = tl.arange(0, BLOCK_DMODEL)\\n    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\\n    off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :] * stride_kk\\n    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\\n    # Initialize pointers to Q, K, V\\n    q_ptrs = Q + off_q\\n    k_ptrs = K + off_k\\n    v_ptrs = V + off_v\\n    # initialize pointer to m and l\\n    t_ptrs = TMP + off_hz * N_CTX + offs_m\\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\\\"inf\\\")\\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n    # load q: it will stay in SRAM throughout\\n    q = tl.load(q_ptrs)\\n    # loop over k, v and update accumulator\\n    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\\n        start_n = tl.multiple_of(start_n, BLOCK_N)\\n        # -- compute qk ----\\n        k = tl.load(k_ptrs + start_n * stride_kn)\\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\\n        qk += tl.dot(q, k, trans_b=True)\\n        qk *= sm_scale\\n        qk += tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), 0, float(\\\"-inf\\\"))\\n        # -- compute m_ij, p, l_ij\\n        m_ij = tl.max(qk, 1)\\n        p = tl.exp(qk - m_ij[:, None])\\n        l_ij = tl.sum(p, 1)\\n        # -- update m_i and l_i\\n        m_i_new = tl.maximum(m_i, m_ij)\\n        alpha = tl.exp(m_i - m_i_new)\\n        beta = tl.exp(m_ij - m_i_new)\\n        l_i_new = alpha * l_i + beta * l_ij\\n        # -- update output accumulator --\\n        # scale p\\n        p_scale = beta / l_i_new\\n        p = p * p_scale[:, None]\\n        # scale acc\\n        acc_scale = l_i / l_i_new * alpha\\n        tl.store(t_ptrs, acc_scale)\\n        acc_scale = tl.load(t_ptrs)  # BUG: have to store and immediately load\\n        acc = acc * acc_scale[:, None]\\n        # update acc\\n        v = tl.load(v_ptrs + start_n * stride_vk)\\n        p = p.to(tl.float16)\\n        acc += tl.dot(p, v)\\n        # update m_i and l_i\\n        l_i = l_i_new\\n        m_i = m_i_new\\n    # rematerialize offsets to save registers\\n    start_m = tl.program_id(0)\\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\\n    # write back l and m\\n    l_ptrs = L + off_hz * N_CTX + offs_m\\n    m_ptrs = M + off_hz * N_CTX + offs_m\\n    tl.store(l_ptrs, l_i)\\n    tl.store(m_ptrs, m_i)\\n    # initialize pointers to output\\n    offs_n = tl.arange(0, BLOCK_DMODEL)\\n    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\\n    out_ptrs = Out + off_o\\n    tl.store(out_ptrs, acc)\\n\\n\\n@triton.jit\\ndef _bwd_preprocess(\\n    Out, DO, L,\\n    NewDO, Delta,\\n    BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\\n):\\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\\n    off_n = tl.arange(0, D_HEAD)\\n    # load\\n    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\\n    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\\n    denom = tl.load(L + off_m).to(tl.float32)\\n    # compute\\n    do = do / denom[:, None]\\n    delta = tl.sum(o * do, axis=1)\\n    # write-back\\n    tl.store(NewDO + off_m[:, None] * D_HEAD + off_n[None, :], do)\\n    tl.store(Delta + off_m, delta)\\n\\n\\n@triton.jit\\ndef _bwd_kernel(\\n    Q, K, V, sm_scale, Out, DO,\\n    DQ, DK, DV,\\n    L, M,\\n    D,\\n    stride_qz, stride_qh, stride_qm, stride_qk,\\n    stride_kz, stride_kh, stride_kn, stride_kk,\\n    stride_vz, stride_vh, stride_vk, stride_vn,\\n    Z, H, N_CTX,\\n    num_block,\\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\\n    BLOCK_N: tl.constexpr,\\n):\\n    off_hz = tl.program_id(0)\\n    off_z = off_hz // H\\n    off_h = off_hz % H\\n    # offset pointers for batch/head\\n    Q += off_z * stride_qz + off_h * stride_qh\\n    K += off_z * stride_qz + off_h * stride_qh\\n    V += off_z * stride_qz + off_h * stride_qh\\n    DO += off_z * stride_qz + off_h * stride_qh\\n    DQ += off_z * stride_qz + off_h * stride_qh\\n    DK += off_z * stride_qz + off_h * stride_qh\\n    DV += off_z * stride_qz + off_h * stride_qh\\n    for start_n in range(0, num_block):\\n        lo = start_n * BLOCK_M\\n        # initialize row/col offsets\\n        offs_qm = lo + tl.arange(0, BLOCK_M)\\n        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\\n        offs_m = tl.arange(0, BLOCK_N)\\n        offs_k = tl.arange(0, BLOCK_DMODEL)\\n        # initialize pointers to value-like data\\n        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\\n        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        # pointer to row-wise quantities in value-like data\\n        D_ptrs = D + off_hz * N_CTX\\n        m_ptrs = M + off_hz * N_CTX\\n        # initialize dv amd dk\\n        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n        # k and v stay in SRAM throughout\\n        k = tl.load(k_ptrs)\\n        v = tl.load(v_ptrs)\\n        # loop over rows\\n        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\\n            offs_m_curr = start_m + offs_m\\n            # load q, k, v, do on-chip\\n            q = tl.load(q_ptrs)\\n            # recompute p = softmax(qk, dim=-1).T\\n            # NOTE: `do` is pre-divided by `l`; no normalization here\\n            qk = tl.dot(q, k, trans_b=True)\\n            qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\\\"-inf\\\"))\\n            m = tl.load(m_ptrs + offs_m_curr)\\n            p = tl.exp(qk * sm_scale - m[:, None])\\n            # compute dv\\n            do = tl.load(do_ptrs)\\n            dv += tl.dot(p.to(tl.float16), do, trans_a=True)\\n            # compute dp = dot(v, do)\\n            Di = tl.load(D_ptrs + offs_m_curr)\\n            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\\n            dp += tl.dot(do, v, trans_b=True)\\n            # compute ds = p * (dp - delta[:, None])\\n            ds = p * dp * sm_scale\\n            # compute dk = dot(ds.T, q)\\n            dk += tl.dot(ds.to(tl.float16), q, trans_a=True)\\n            # # compute dq\\n            dq = tl.load(dq_ptrs, eviction_policy=\\\"evict_last\\\")\\n            dq += tl.dot(ds.to(tl.float16), k)\\n            tl.store(dq_ptrs, dq, eviction_policy=\\\"evict_last\\\")\\n            # # increment pointers\\n            dq_ptrs += BLOCK_M * stride_qm\\n            q_ptrs += BLOCK_M * stride_qm\\n            do_ptrs += BLOCK_M * stride_qm\\n        # write-back\\n        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\\n        tl.store(dv_ptrs, dv)\\n        tl.store(dk_ptrs, dk)\\n\\n\\nclass _attention(torch.autograd.Function):\\n\\n    @staticmethod\\n    def forward(ctx, q, k, v, sm_scale):\\n        BLOCK = 128\\n        # shape constraints\\n        Lq, Lk = q.shape[-1], k.shape[-1]\\n        assert Lq == Lk\\n        o = torch.empty_like(q)\\n        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1])\\n        tmp = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\\n        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\\n        m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\\n        _fwd_kernel[grid](\\n            q, k, v, sm_scale,\\n            tmp, L, m,\\n            o,\\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\\n            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\\n            q.shape[0], q.shape[1], q.shape[2],\\n            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\\n            BLOCK_DMODEL=64, num_warps=4,\\n            num_stages=1,\\n        )\\n        ctx.save_for_backward(q, k, v, o, L, m)\\n        ctx.BLOCK = BLOCK\\n        ctx.grid = grid\\n        ctx.sm_scale = sm_scale\\n        ctx.BLOCK_DMODEL = 64\\n        return o\\n\\n    @staticmethod\\n    def backward(ctx, do):\\n        q, k, v, o, l, m = ctx.saved_tensors\\n        do = do.contiguous()\\n        dq = torch.zeros_like(q, dtype=torch.float32)\\n        dk = torch.empty_like(k)\\n        dv = torch.empty_like(v)\\n        do_scaled = torch.empty_like(do)\\n        delta = torch.empty_like(l)\\n        _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\\n            o, do, l,\\n            do_scaled, delta,\\n            BLOCK_M=ctx.BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\\n        )\\n        _bwd_kernel[(ctx.grid[1],)](\\n            q, k, v, ctx.sm_scale,\\n            o, do_scaled,\\n            dq, dk, dv,\\n            l, m,\\n            delta,\\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\\n            q.shape[0], q.shape[1], q.shape[2],\\n            ctx.grid[0],\\n            BLOCK_M=ctx.BLOCK, BLOCK_N=ctx.BLOCK,\\n            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\\n            num_stages=1,\\n        )\\n        return dq, dk, dv, None\\n\\n\\nattention = _attention.apply\\n\\n\\n@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(3, 2, 2048, 64)])\\ndef test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\\n    torch.manual_seed(20)\\n    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0, std=.5).requires_grad_()\\n    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0, std=.5).requires_grad_()\\n    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0, std=.5).requires_grad_()\\n    sm_scale = 0.3\\n    dout = torch.randn_like(q)\\n    # reference implementation\\n    M = torch.tril(torch.ones((N_CTX, N_CTX), device=\\\"cuda\\\"))\\n    p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\\n    for z in range(Z):\\n        for h in range(H):\\n            p[:, :, M == 0] = float(\\\"-inf\\\")\\n    p = torch.softmax(p.float(), dim=-1).half()\\n    ref_out = torch.matmul(p, v)\\n    ref_out.backward(dout)\\n    ref_dv, v.grad = v.grad.clone(), None\\n    ref_dk, k.grad = k.grad.clone(), None\\n    ref_dq, q.grad = q.grad.clone(), None\\n    # triton implementation\\n    tri_out = attention(q, k, v, sm_scale)\\n    tri_out.backward(dout)\\n    tri_dv, v.grad = v.grad.clone(), None\\n    tri_dk, k.grad = k.grad.clone(), None\\n    tri_dq, q.grad = q.grad.clone(), None\\n    # compare\\n    triton.testing.assert_almost_equal(ref_out, tri_out)\\n    triton.testing.assert_almost_equal(ref_dv, tri_dv)\\n    triton.testing.assert_almost_equal(ref_dk, tri_dk)\\n    triton.testing.assert_almost_equal(ref_dq, tri_dq)\\n\\n\\ntry:\\n    from flash_attn.flash_attn_interface import flash_attn_func\\n    HAS_FLASH = True\\nexcept BaseException:\\n    HAS_FLASH = False\\n\\nBATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\\n# vary seq length for fixed head and batch=4\\nconfigs = [triton.testing.Benchmark(\\n    x_names=['N_CTX'],\\n    x_vals=[2**i for i in range(10, 16)],\\n    line_arg='provider',\\n    line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\\n    line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\\n    styles=[('red', '-'), ('blue', '-')],\\n    ylabel='ms',\\n    plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\\n    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode}\\n) for mode in ['bwd']]\\n\\n\\n@triton.testing.perf_report(configs)\\ndef bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.float16, device=\\\"cuda\\\"):\\n    assert mode in ['fwd', 'bwd']\\n    warmup = 25\\n    rep = 100\\n    if provider == \\\"triton\\\":\\n        q = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        sm_scale = 1.3\\n        fn = lambda: attention(q, k, v, sm_scale)\\n        if mode == 'bwd':\\n            o = fn()\\n            do = torch.randn_like(o)\\n            fn = lambda: o.backward(do, retain_graph=True)\\n        ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\\n        return ms\\n    if provider == \\\"flash\\\":\\n        lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\\n        cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\\n        cu_seqlens[1:] = lengths.cumsum(0)\\n        qkv = torch.randn((BATCH * N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\\n        fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=True)\\n        if mode == 'bwd':\\n            o = fn()\\n            do = torch.randn_like(o)\\n            fn = lambda: o.backward(do, retain_graph=True)\\n        ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\\n        return ms\\n\\n# only works on A100 at the moment\\n# bench_flash_attention.run(save_path='.', print_data=True)\"\n+      ]\n+    }\n+  ],\n+  \"metadata\": {\n+    \"kernelspec\": {\n+      \"display_name\": \"Python 3\",\n+      \"language\": \"python\",\n+      \"name\": \"python3\"\n+    },\n+    \"language_info\": {\n+      \"codemirror_mode\": {\n+        \"name\": \"ipython\",\n+        \"version\": 3\n+      },\n+      \"file_extension\": \".py\",\n+      \"mimetype\": \"text/x-python\",\n+      \"name\": \"python\",\n+      \"nbconvert_exporter\": \"python\",\n+      \"pygments_lexer\": \"ipython3\",\n+      \"version\": \"3.8.10\"\n+    }\n+  },\n+  \"nbformat\": 4,\n+  \"nbformat_minor\": 0\n+}\n\\ No newline at end of file"}, {"filename": "master/_downloads/3ff29f967ace7985da24aab10352fc76/07-libdevice-function.py", "status": "added", "additions": 74, "deletions": 0, "changes": 74, "file_content_changes": "@@ -0,0 +1,74 @@\n+\"\"\"\n+Libdevice function\n+===============\n+Triton can invoke a custom function from an external library.\n+In this example, we will use the `libdevice` library to apply `asin` on a tensor.\n+Please refer to https://docs.nvidia.com/cuda/libdevice-users-guide/index.html regarding the semantics of all available libdevice functions.\n+\n+In `trition/language/libdevice.py`, we try to aggregate functions with the same computation but different data types together.\n+For example, both `__nv_asin` and `__nvasinf` calculate the principal value of the arc sine of the input, but `__nv_asin` operates on `double` and `__nv_asinf` operates on `float`.\n+Using triton, you can simply call `tl.libdevice.asinf`.\n+triton automatically selects the correct underlying device function to invoke based on input and output types.\n+\"\"\"\n+\n+# %%\n+#  asin Kernel\n+# --------------------------\n+\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def asin_kernel(\n+    x_ptr,\n+    y_ptr,\n+    n_elements,\n+    BLOCK_SIZE: tl.constexpr,\n+):\n+    pid = tl.program_id(axis=0)\n+    block_start = pid * BLOCK_SIZE\n+    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+    mask = offsets < n_elements\n+    x = tl.load(x_ptr + offsets, mask=mask)\n+    x = tl.libdevice.asin(x)\n+    tl.store(y_ptr + offsets, x, mask=mask)\n+\n+# %%\n+#  Using the default libdevice library path\n+# --------------------------\n+# We can use the default libdevice library path encoded in `triton/language/libdevice.py`\n+\n+\n+torch.manual_seed(0)\n+size = 98432\n+x = torch.rand(size, device='cuda')\n+output_triton = torch.zeros(size, device='cuda')\n+output_torch = torch.asin(x)\n+assert x.is_cuda and output_triton.is_cuda\n+n_elements = output_torch.numel()\n+grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+asin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024)\n+print(output_torch)\n+print(output_triton)\n+print(\n+    f'The maximum difference between torch and triton is '\n+    f'{torch.max(torch.abs(output_torch - output_triton))}'\n+)\n+\n+# %%\n+#  Customize the libdevice library path\n+# --------------------------\n+# We can also customize the libdevice library path by passing the path to the `libdevice` library to the `asin` kernel.\n+\n+output_triton = torch.empty_like(x)\n+asin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024,\n+                  extern_libs={'libdevice': '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'})\n+print(output_torch)\n+print(output_triton)\n+print(\n+    f'The maximum difference between torch and triton is '\n+    f'{torch.max(torch.abs(output_torch - output_triton))}'\n+)"}, {"filename": "master/_downloads/54a35f6ec55f9746935b9566fb6bb1df/06-fused-attention.py", "status": "added", "additions": 354, "deletions": 0, "changes": 354, "file_content_changes": "@@ -0,0 +1,354 @@\n+\"\"\"\n+Fused Attention\n+===============\n+This is a Triton implementation of the Flash Attention algorithm \n+(see: Dao et al., https://arxiv.org/pdf/2205.14135v2.pdf; Rabe and Staats https://arxiv.org/pdf/2112.05682v2.pdf)\n+\"\"\"\n+\n+import pytest\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def _fwd_kernel(\n+    Q, K, V, sm_scale,\n+    TMP, L, M,  # NOTE: TMP is a scratchpad buffer to workaround a compiler bug\n+    Out,\n+    stride_qz, stride_qh, stride_qm, stride_qk,\n+    stride_kz, stride_kh, stride_kn, stride_kk,\n+    stride_vz, stride_vh, stride_vk, stride_vn,\n+    stride_oz, stride_oh, stride_om, stride_on,\n+    Z, H, N_CTX,\n+    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n+    BLOCK_N: tl.constexpr,\n+):\n+    start_m = tl.program_id(0)\n+    off_hz = tl.program_id(1)\n+    # initialize offsets\n+    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+    offs_n = tl.arange(0, BLOCK_N)\n+    offs_d = tl.arange(0, BLOCK_DMODEL)\n+    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n+    off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :] * stride_kk\n+    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n+    # Initialize pointers to Q, K, V\n+    q_ptrs = Q + off_q\n+    k_ptrs = K + off_k\n+    v_ptrs = V + off_v\n+    # initialize pointer to m and l\n+    t_ptrs = TMP + off_hz * N_CTX + offs_m\n+    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n+    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+    # load q: it will stay in SRAM throughout\n+    q = tl.load(q_ptrs)\n+    # loop over k, v and update accumulator\n+    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n+        start_n = tl.multiple_of(start_n, BLOCK_N)\n+        # -- compute qk ----\n+        k = tl.load(k_ptrs + start_n * stride_kn)\n+        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n+        qk += tl.dot(q, k, trans_b=True)\n+        qk *= sm_scale\n+        qk += tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), 0, float(\"-inf\"))\n+        # -- compute m_ij, p, l_ij\n+        m_ij = tl.max(qk, 1)\n+        p = tl.exp(qk - m_ij[:, None])\n+        l_ij = tl.sum(p, 1)\n+        # -- update m_i and l_i\n+        m_i_new = tl.maximum(m_i, m_ij)\n+        alpha = tl.exp(m_i - m_i_new)\n+        beta = tl.exp(m_ij - m_i_new)\n+        l_i_new = alpha * l_i + beta * l_ij\n+        # -- update output accumulator --\n+        # scale p\n+        p_scale = beta / l_i_new\n+        p = p * p_scale[:, None]\n+        # scale acc\n+        acc_scale = l_i / l_i_new * alpha\n+        tl.store(t_ptrs, acc_scale)\n+        acc_scale = tl.load(t_ptrs)  # BUG: have to store and immediately load\n+        acc = acc * acc_scale[:, None]\n+        # update acc\n+        v = tl.load(v_ptrs + start_n * stride_vk)\n+        p = p.to(tl.float16)\n+        acc += tl.dot(p, v)\n+        # update m_i and l_i\n+        l_i = l_i_new\n+        m_i = m_i_new\n+    # rematerialize offsets to save registers\n+    start_m = tl.program_id(0)\n+    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+    # write back l and m\n+    l_ptrs = L + off_hz * N_CTX + offs_m\n+    m_ptrs = M + off_hz * N_CTX + offs_m\n+    tl.store(l_ptrs, l_i)\n+    tl.store(m_ptrs, m_i)\n+    # initialize pointers to output\n+    offs_n = tl.arange(0, BLOCK_DMODEL)\n+    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n+    out_ptrs = Out + off_o\n+    tl.store(out_ptrs, acc)\n+\n+\n+@triton.jit\n+def _bwd_preprocess(\n+    Out, DO, L,\n+    NewDO, Delta,\n+    BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\n+):\n+    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n+    off_n = tl.arange(0, D_HEAD)\n+    # load\n+    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n+    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n+    denom = tl.load(L + off_m).to(tl.float32)\n+    # compute\n+    do = do / denom[:, None]\n+    delta = tl.sum(o * do, axis=1)\n+    # write-back\n+    tl.store(NewDO + off_m[:, None] * D_HEAD + off_n[None, :], do)\n+    tl.store(Delta + off_m, delta)\n+\n+\n+@triton.jit\n+def _bwd_kernel(\n+    Q, K, V, sm_scale, Out, DO,\n+    DQ, DK, DV,\n+    L, M,\n+    D,\n+    stride_qz, stride_qh, stride_qm, stride_qk,\n+    stride_kz, stride_kh, stride_kn, stride_kk,\n+    stride_vz, stride_vh, stride_vk, stride_vn,\n+    Z, H, N_CTX,\n+    num_block,\n+    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n+    BLOCK_N: tl.constexpr,\n+):\n+    off_hz = tl.program_id(0)\n+    off_z = off_hz // H\n+    off_h = off_hz % H\n+    # offset pointers for batch/head\n+    Q += off_z * stride_qz + off_h * stride_qh\n+    K += off_z * stride_qz + off_h * stride_qh\n+    V += off_z * stride_qz + off_h * stride_qh\n+    DO += off_z * stride_qz + off_h * stride_qh\n+    DQ += off_z * stride_qz + off_h * stride_qh\n+    DK += off_z * stride_qz + off_h * stride_qh\n+    DV += off_z * stride_qz + off_h * stride_qh\n+    for start_n in range(0, num_block):\n+        lo = start_n * BLOCK_M\n+        # initialize row/col offsets\n+        offs_qm = lo + tl.arange(0, BLOCK_M)\n+        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n+        offs_m = tl.arange(0, BLOCK_N)\n+        offs_k = tl.arange(0, BLOCK_DMODEL)\n+        # initialize pointers to value-like data\n+        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n+        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        # pointer to row-wise quantities in value-like data\n+        D_ptrs = D + off_hz * N_CTX\n+        m_ptrs = M + off_hz * N_CTX\n+        # initialize dv amd dk\n+        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+        # k and v stay in SRAM throughout\n+        k = tl.load(k_ptrs)\n+        v = tl.load(v_ptrs)\n+        # loop over rows\n+        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n+            offs_m_curr = start_m + offs_m\n+            # load q, k, v, do on-chip\n+            q = tl.load(q_ptrs)\n+            # recompute p = softmax(qk, dim=-1).T\n+            # NOTE: `do` is pre-divided by `l`; no normalization here\n+            qk = tl.dot(q, k, trans_b=True)\n+            qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\"-inf\"))\n+            m = tl.load(m_ptrs + offs_m_curr)\n+            p = tl.exp(qk * sm_scale - m[:, None])\n+            # compute dv\n+            do = tl.load(do_ptrs)\n+            dv += tl.dot(p.to(tl.float16), do, trans_a=True)\n+            # compute dp = dot(v, do)\n+            Di = tl.load(D_ptrs + offs_m_curr)\n+            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n+            dp += tl.dot(do, v, trans_b=True)\n+            # compute ds = p * (dp - delta[:, None])\n+            ds = p * dp * sm_scale\n+            # compute dk = dot(ds.T, q)\n+            dk += tl.dot(ds.to(tl.float16), q, trans_a=True)\n+            # # compute dq\n+            dq = tl.load(dq_ptrs, eviction_policy=\"evict_last\")\n+            dq += tl.dot(ds.to(tl.float16), k)\n+            tl.store(dq_ptrs, dq, eviction_policy=\"evict_last\")\n+            # # increment pointers\n+            dq_ptrs += BLOCK_M * stride_qm\n+            q_ptrs += BLOCK_M * stride_qm\n+            do_ptrs += BLOCK_M * stride_qm\n+        # write-back\n+        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n+        tl.store(dv_ptrs, dv)\n+        tl.store(dk_ptrs, dk)\n+\n+\n+class _attention(torch.autograd.Function):\n+\n+    @staticmethod\n+    def forward(ctx, q, k, v, sm_scale):\n+        BLOCK = 128\n+        # shape constraints\n+        Lq, Lk = q.shape[-1], k.shape[-1]\n+        assert Lq == Lk\n+        o = torch.empty_like(q)\n+        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1])\n+        tmp = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        _fwd_kernel[grid](\n+            q, k, v, sm_scale,\n+            tmp, L, m,\n+            o,\n+            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n+            q.shape[0], q.shape[1], q.shape[2],\n+            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n+            BLOCK_DMODEL=64, num_warps=4,\n+            num_stages=1,\n+        )\n+        ctx.save_for_backward(q, k, v, o, L, m)\n+        ctx.BLOCK = BLOCK\n+        ctx.grid = grid\n+        ctx.sm_scale = sm_scale\n+        ctx.BLOCK_DMODEL = 64\n+        return o\n+\n+    @staticmethod\n+    def backward(ctx, do):\n+        q, k, v, o, l, m = ctx.saved_tensors\n+        do = do.contiguous()\n+        dq = torch.zeros_like(q, dtype=torch.float32)\n+        dk = torch.empty_like(k)\n+        dv = torch.empty_like(v)\n+        do_scaled = torch.empty_like(do)\n+        delta = torch.empty_like(l)\n+        _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n+            o, do, l,\n+            do_scaled, delta,\n+            BLOCK_M=ctx.BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n+        )\n+        _bwd_kernel[(ctx.grid[1],)](\n+            q, k, v, ctx.sm_scale,\n+            o, do_scaled,\n+            dq, dk, dv,\n+            l, m,\n+            delta,\n+            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+            q.shape[0], q.shape[1], q.shape[2],\n+            ctx.grid[0],\n+            BLOCK_M=ctx.BLOCK, BLOCK_N=ctx.BLOCK,\n+            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n+            num_stages=1,\n+        )\n+        return dq, dk, dv, None\n+\n+\n+attention = _attention.apply\n+\n+\n+@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(3, 2, 2048, 64)])\n+def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n+    torch.manual_seed(20)\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0, std=.5).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0, std=.5).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0, std=.5).requires_grad_()\n+    sm_scale = 0.3\n+    dout = torch.randn_like(q)\n+    # reference implementation\n+    M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n+    p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n+    for z in range(Z):\n+        for h in range(H):\n+            p[:, :, M == 0] = float(\"-inf\")\n+    p = torch.softmax(p.float(), dim=-1).half()\n+    ref_out = torch.matmul(p, v)\n+    ref_out.backward(dout)\n+    ref_dv, v.grad = v.grad.clone(), None\n+    ref_dk, k.grad = k.grad.clone(), None\n+    ref_dq, q.grad = q.grad.clone(), None\n+    # triton implementation\n+    tri_out = attention(q, k, v, sm_scale)\n+    tri_out.backward(dout)\n+    tri_dv, v.grad = v.grad.clone(), None\n+    tri_dk, k.grad = k.grad.clone(), None\n+    tri_dq, q.grad = q.grad.clone(), None\n+    # compare\n+    triton.testing.assert_almost_equal(ref_out, tri_out)\n+    triton.testing.assert_almost_equal(ref_dv, tri_dv)\n+    triton.testing.assert_almost_equal(ref_dk, tri_dk)\n+    triton.testing.assert_almost_equal(ref_dq, tri_dq)\n+\n+\n+try:\n+    from flash_attn.flash_attn_interface import flash_attn_func\n+    HAS_FLASH = True\n+except BaseException:\n+    HAS_FLASH = False\n+\n+BATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\n+# vary seq length for fixed head and batch=4\n+configs = [triton.testing.Benchmark(\n+    x_names=['N_CTX'],\n+    x_vals=[2**i for i in range(10, 16)],\n+    line_arg='provider',\n+    line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n+    line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\n+    styles=[('red', '-'), ('blue', '-')],\n+    ylabel='ms',\n+    plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n+    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode}\n+) for mode in ['bwd']]\n+\n+\n+@triton.testing.perf_report(configs)\n+def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.float16, device=\"cuda\"):\n+    assert mode in ['fwd', 'bwd']\n+    warmup = 25\n+    rep = 100\n+    if provider == \"triton\":\n+        q = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+        k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+        v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+        sm_scale = 1.3\n+        fn = lambda: attention(q, k, v, sm_scale)\n+        if mode == 'bwd':\n+            o = fn()\n+            do = torch.randn_like(o)\n+            fn = lambda: o.backward(do, retain_graph=True)\n+        ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n+        return ms\n+    if provider == \"flash\":\n+        lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n+        cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n+        cu_seqlens[1:] = lengths.cumsum(0)\n+        qkv = torch.randn((BATCH * N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\n+        fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=True)\n+        if mode == 'bwd':\n+            o = fn()\n+            do = torch.randn_like(o)\n+            fn = lambda: o.backward(do, retain_graph=True)\n+        ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n+        return ms\n+\n+# only works on A100 at the moment\n+# bench_flash_attention.run(save_path='.', print_data=True)"}, {"filename": "master/_downloads/662999063954282841dc90b8945f85ce/tutorials_jupyter.zip", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/_downloads/763344228ae6bc253ed1a6cf586aa30d/tutorials_python.zip", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/_downloads/935c0dd0fbeb4b2e69588471cbb2d4b2/05-layer-norm.py", "status": "modified", "additions": 26, "deletions": 22, "changes": 48, "file_content_changes": "@@ -128,17 +128,19 @@ def _layer_norm_bwd_dwdb(\n     cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n     dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n     db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-    for i in range(0, M, BLOCK_SIZE_M):\n-        rows = i + tl.arange(0, BLOCK_SIZE_M)\n-        mask = (rows[:, None] < M) & (cols[None, :] < N)\n-        offs = rows[:, None] * N + cols[None, :]\n-        a = tl.load(A + offs, mask=mask, other=0.).to(tl.float32)\n-        dout = tl.load(DOut + offs, mask=mask, other=0.).to(tl.float32)\n-        mean = tl.load(Mean + rows, mask=rows < M, other=0.)\n-        rstd = tl.load(Var + rows, mask=rows < M, other=0.)\n-        a_hat = (a - mean[:, None]) * rstd[:, None]\n-        dw += dout * a_hat\n-        db += dout\n+    UNROLL: tl.constexpr = 4\n+    for i in range(0, M, BLOCK_SIZE_M * UNROLL):\n+        for j in range(UNROLL):\n+            rows = i + j * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+            mask = (rows[:, None] < M) & (cols[None, :] < N)\n+            offs = rows[:, None] * N + cols[None, :]\n+            a = tl.load(A + offs, mask=mask, other=0.).to(tl.float32)\n+            dout = tl.load(DOut + offs, mask=mask, other=0.).to(tl.float32)\n+            mean = tl.load(Mean + rows, mask=rows < M, other=0.)\n+            rstd = tl.load(Var + rows, mask=rows < M, other=0.)\n+            a_hat = (a - mean[:, None]) * rstd[:, None]\n+            dw += dout * a_hat\n+            db += dout\n     sum_dw = tl.sum(dw, axis=0)\n     sum_db = tl.sum(db, axis=0)\n     tl.store(DW + cols, sum_dw, mask=cols < N)\n@@ -211,7 +213,15 @@ def backward(ctx, dout):\n             BLOCK_SIZE_N=ctx.BLOCK_SIZE,\n             num_warps=ctx.num_warps,\n         )\n-        # accumulate partial sums in separate kernel\n+        if N > 10240:\n+            BLOCK_SIZE_N = 128\n+            BLOCK_SIZE_M = 32\n+            num_warps = 4\n+        else:\n+            # maximize occupancy for small N\n+            BLOCK_SIZE_N = 16\n+            BLOCK_SIZE_M = 16\n+            num_warps = 8\n         grid = lambda meta: [triton.cdiv(N, meta[\"BLOCK_SIZE_N\"])]\n         _layer_norm_bwd_dwdb[grid](\n             a, dout,\n@@ -220,17 +230,11 @@ def backward(ctx, dout):\n             dbias,\n             M,\n             N,\n-            BLOCK_SIZE_M=32,\n-            BLOCK_SIZE_N=128,\n+            BLOCK_SIZE_M=BLOCK_SIZE_M,\n+            BLOCK_SIZE_N=BLOCK_SIZE_N,\n+            num_warps=num_warps\n         )\n-        return (da, None, dweight, dbias, None, None,\n-                None, None, None, None,\n-                None,\n-                None, None, None,\n-                None,\n-                None, None, None,\n-                None, None, None,\n-                None, None, None)\n+        return (da, None, dweight, dbias, None)\n \n \n def layer_norm(a, normalized_shape, weight, bias, eps):"}, {"filename": "master/_downloads/ae7fff29e1b574187bc930ed94bcc353/05-layer-norm.ipynb", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -26,7 +26,7 @@\n       },\n       \"outputs\": [],\n       \"source\": [\n-        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\ntry:\\n    # This is https://github.com/NVIDIA/apex, NOT the apex on PyPi, so it\\n    # should not be added to extras_require in setup.py.\\n    import apex\\n    HAS_APEX = True\\nexcept ModuleNotFoundError:\\n    HAS_APEX = False\\n\\n\\n@triton.jit\\ndef _layer_norm_fwd_fused(\\n    Out,\\n    A,\\n    Weight,\\n    Bias,\\n    Mean, Rstd,\\n    stride, N, eps,\\n    BLOCK_SIZE: tl.constexpr,\\n):\\n    # position of elements processed by this program\\n    row = tl.program_id(0)\\n    Out += row * stride\\n    A += row * stride\\n    # compute mean\\n    mean = 0\\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\\n    for off in range(0, N, BLOCK_SIZE):\\n        cols = off + tl.arange(0, BLOCK_SIZE)\\n        a = tl.load(A + cols, mask=cols < N, other=0., eviction_policy=\\\"evict_last\\\").to(tl.float32)\\n        _mean += a\\n    mean = tl.sum(_mean, axis=0) / N\\n    # compute variance\\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\\n    for off in range(0, N, BLOCK_SIZE):\\n        cols = off + tl.arange(0, BLOCK_SIZE)\\n        a = tl.load(A + cols, mask=cols < N, other=0., eviction_policy=\\\"evict_last\\\").to(tl.float32)\\n        a = tl.where(cols < N, a - mean, 0.)\\n        _var += a * a\\n    var = tl.sum(_var, axis=0) / N\\n    rstd = 1 / tl.sqrt(var + eps)\\n    # write-back mean/rstd\\n    tl.store(Mean + row, mean)\\n    tl.store(Rstd + row, rstd)\\n    # multiply by weight and add bias\\n    for off in range(0, N, BLOCK_SIZE):\\n        cols = off + tl.arange(0, BLOCK_SIZE)\\n        mask = cols < N\\n        weight = tl.load(Weight + cols, mask=mask)\\n        bias = tl.load(Bias + cols, mask=mask)\\n        a = tl.load(A + cols, mask=mask, other=0., eviction_policy=\\\"evict_first\\\").to(tl.float32)\\n        a_hat = (a - mean) * rstd\\n        out = a_hat * weight + bias\\n        # # write-back\\n        tl.store(Out + cols, out, mask=mask)\\n\\n# Backward pass (DA + partial DW + partial DB)\\n\\n\\n@triton.jit\\ndef _layer_norm_bwd_dx_fused(\\n    _DA,\\n    _DOut,\\n    _A,\\n    Weight,\\n    Mean, Rstd,\\n    stride, NumRows, NumCols, eps,\\n    BLOCK_SIZE_N: tl.constexpr,\\n):\\n    # position of elements processed by this program\\n    pid = tl.program_id(0)\\n    row = pid\\n    A = _A + row * stride\\n    DOut = _DOut + row * stride\\n    DA = _DA + row * stride\\n    mean = tl.load(Mean + row)\\n    rstd = tl.load(Rstd + row)\\n    # load data to SRAM\\n    _mean1 = tl.zeros([BLOCK_SIZE_N], dtype=tl.float32)\\n    _mean2 = tl.zeros([BLOCK_SIZE_N], dtype=tl.float32)\\n    for off in range(0, NumCols, BLOCK_SIZE_N):\\n        cols = off + tl.arange(0, BLOCK_SIZE_N)\\n        mask = cols < NumCols\\n        a = tl.load(A + cols, mask=mask, other=0).to(tl.float32)\\n        dout = tl.load(DOut + cols, mask=mask, other=0).to(tl.float32)\\n        weight = tl.load(Weight + cols, mask=mask, other=0).to(tl.float32)\\n        a_hat = (a - mean) * rstd\\n        wdout = weight * dout\\n        _mean1 += a_hat * wdout\\n        _mean2 += wdout\\n    mean1 = tl.sum(_mean1, axis=0) / NumCols\\n    mean2 = 0.\\n    mean2 = tl.sum(_mean2, axis=0) / NumCols\\n    for off in range(0, NumCols, BLOCK_SIZE_N):\\n        cols = off + tl.arange(0, BLOCK_SIZE_N)\\n        mask = cols < NumCols\\n        a = tl.load(A + cols, mask=mask, other=0).to(tl.float32)\\n        dout = tl.load(DOut + cols, mask=mask, other=0).to(tl.float32)\\n        weight = tl.load(Weight + cols, mask=mask, other=0).to(tl.float32)\\n        a_hat = (a - mean) * rstd\\n        wdout = weight * dout\\n        da = (wdout - (a_hat * mean1 + mean2)) * rstd\\n        # write-back dx\\n        tl.store(DA + cols, da, mask=mask)\\n\\n\\n# Backward pass (total DW + total DB)\\n@triton.jit\\ndef _layer_norm_bwd_dwdb(\\n    A, DOut,\\n    Mean, Var,\\n    DW,\\n    DB,\\n    M, N,\\n    BLOCK_SIZE_M: tl.constexpr,\\n    BLOCK_SIZE_N: tl.constexpr,\\n):\\n    pid = tl.program_id(0)\\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\\n    db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\\n    for i in range(0, M, BLOCK_SIZE_M):\\n        rows = i + tl.arange(0, BLOCK_SIZE_M)\\n        mask = (rows[:, None] < M) & (cols[None, :] < N)\\n        offs = rows[:, None] * N + cols[None, :]\\n        a = tl.load(A + offs, mask=mask, other=0.).to(tl.float32)\\n        dout = tl.load(DOut + offs, mask=mask, other=0.).to(tl.float32)\\n        mean = tl.load(Mean + rows, mask=rows < M, other=0.)\\n        rstd = tl.load(Var + rows, mask=rows < M, other=0.)\\n        a_hat = (a - mean[:, None]) * rstd[:, None]\\n        dw += dout * a_hat\\n        db += dout\\n    sum_dw = tl.sum(dw, axis=0)\\n    sum_db = tl.sum(db, axis=0)\\n    tl.store(DW + cols, sum_dw, mask=cols < N)\\n    tl.store(DB + cols, sum_db, mask=cols < N)\\n\\n\\nclass LayerNorm(torch.autograd.Function):\\n    @staticmethod\\n    def forward(ctx, a, normalized_shape, weight, bias, eps):\\n        # allocate output\\n        out = torch.empty_like(a)\\n        # reshape input data into 2D tensor\\n        a_arg = a.reshape(-1, a.shape[-1])\\n        M, N = a_arg.shape\\n        mean = torch.empty((M,), dtype=torch.float32, device=\\\"cuda\\\")\\n        rstd = torch.empty((M,), dtype=torch.float32, device=\\\"cuda\\\")\\n        # Less than 64KB per feature: enqueue fused kernel\\n        MAX_FUSED_SIZE = 65536 // a.element_size()\\n        BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\\n        BLOCK_SIZE = max(BLOCK_SIZE, 128)\\n        BLOCK_SIZE = min(BLOCK_SIZE, 4096)\\n        # heuristics for number of warps\\n        num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\\n        _layer_norm_fwd_fused[(M,)](\\n            out,\\n            a_arg,\\n            weight,\\n            bias,\\n            mean, rstd,\\n            a_arg.stride(0), N, eps,\\n            BLOCK_SIZE=BLOCK_SIZE,\\n            num_warps=num_warps,\\n        )\\n        ctx.save_for_backward(\\n            a, weight, bias, mean, rstd,\\n        )\\n        ctx.BLOCK_SIZE = BLOCK_SIZE\\n        ctx.num_warps = num_warps\\n        ctx.eps = eps\\n        if hasattr(bias, \\\"config\\\"):\\n            assert bias.config.grad_scale_name == weight.config.grad_scale_name\\n            grad_scale_name = bias.config.grad_scale_name\\n        else:\\n            grad_scale_name = None\\n        ctx.grad_scale_gain_bias_name = grad_scale_name\\n        return out\\n\\n    @staticmethod\\n    def backward(ctx, dout):\\n        assert dout.is_contiguous()\\n        a, weight, bias, mean, var = ctx.saved_tensors\\n        # heuristics for amount of parallel reduction stream for DG/DB\\n        N = weight.shape[0]\\n        # allocate output\\n        da = torch.empty_like(dout)\\n        # enqueue kernel using forward pass heuristics\\n        # also compute partial sums for DW and DB\\n        x_arg = a.reshape(-1, a.shape[-1])\\n        M, N = x_arg.shape\\n        dweight = torch.empty((weight.shape[0],), dtype=weight.dtype, device=weight.device)\\n        dbias = torch.empty((weight.shape[0],), dtype=weight.dtype, device=weight.device)\\n        _layer_norm_bwd_dx_fused[(M,)](\\n            da,\\n            dout,\\n            a,\\n            weight,\\n            mean, var,\\n            x_arg.stride(0), M, N,\\n            ctx.eps,\\n            BLOCK_SIZE_N=ctx.BLOCK_SIZE,\\n            num_warps=ctx.num_warps,\\n        )\\n        # accumulate partial sums in separate kernel\\n        grid = lambda meta: [triton.cdiv(N, meta[\\\"BLOCK_SIZE_N\\\"])]\\n        _layer_norm_bwd_dwdb[grid](\\n            a, dout,\\n            mean, var,\\n            dweight,\\n            dbias,\\n            M,\\n            N,\\n            BLOCK_SIZE_M=32,\\n            BLOCK_SIZE_N=128,\\n        )\\n        return (da, None, dweight, dbias, None, None,\\n                None, None, None, None,\\n                None,\\n                None, None, None,\\n                None,\\n                None, None, None,\\n                None, None, None,\\n                None, None, None)\\n\\n\\ndef layer_norm(a, normalized_shape, weight, bias, eps):\\n    return LayerNorm.apply(a, normalized_shape, weight, bias, eps)\\n\\n\\ndef test_layer_norm(M, N, dtype, eps=1e-5, device='cuda'):\\n    torch.manual_seed(0)\\n    # create data\\n    x_shape = (M, N)\\n    w_shape = (x_shape[-1], )\\n    weight = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\\n    bias = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\\n    x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\\n    dy = .1 * torch.randn_like(x)\\n    x.requires_grad_(True)\\n    # forward pass\\n    y_tri = layer_norm(x, w_shape, weight, bias, eps)\\n    y_ref = torch.nn.functional.layer_norm(x, w_shape, weight, bias, eps).to(dtype)\\n    # backward pass (triton)\\n    y_tri.backward(dy, retain_graph=True)\\n    dx_tri, dw_tri, db_tri = [_.grad.clone() for _ in [x, weight, bias]]\\n    x.grad, weight.grad, bias.grad = None, None, None\\n    # backward pass (torch)\\n    y_ref.backward(dy, retain_graph=True)\\n    dx_ref, dw_ref, db_ref = [_.grad.clone() for _ in [x, weight, bias]]\\n    # compare\\n    triton.testing.assert_almost_equal(y_tri, y_ref)\\n    triton.testing.assert_almost_equal(dx_tri, dx_ref)\\n    triton.testing.assert_almost_equal(db_tri, db_ref, decimal=1)\\n    triton.testing.assert_almost_equal(dw_tri, dw_ref, decimal=1)\\n\\n\\n@triton.testing.perf_report(\\n    triton.testing.Benchmark(\\n        x_names=['N'],\\n        x_vals=[512 * i for i in range(2, 32)],\\n        line_arg='provider',\\n        line_vals=['triton', 'torch'] + (['apex'] if HAS_APEX else []),\\n        line_names=['Triton', 'Torch'] + (['Apex'] if HAS_APEX else []),\\n        styles=[('blue', '-'), ('green', '-'), ('orange', '-')],\\n        ylabel='GB/s',\\n        plot_name='layer-norm',\\n        args={'M': 4096, 'dtype': torch.float16, 'mode': 'forward'}\\n    )\\n)\\ndef bench_layer_norm(M, N, dtype, provider, mode, eps=1e-5, device='cuda'):\\n    # create data\\n    x_shape = (M, N)\\n    w_shape = (x_shape[-1], )\\n    weight = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\\n    bias = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\\n    x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\\n    dy = .1 * torch.randn_like(x)\\n    x.requires_grad_(True)\\n    # utility functions\\n    if provider == 'triton':\\n        y_fwd = lambda: layer_norm(x, w_shape, weight, bias, eps)\\n    if provider == 'torch':\\n        y_fwd = lambda: torch.nn.functional.layer_norm(x, w_shape, weight, bias, eps)\\n    if provider == 'apex':\\n        apex_layer_norm = apex.normalization.FusedLayerNorm(w_shape).to(x.device).to(x.dtype)\\n        y_fwd = lambda: apex_layer_norm(x)\\n    # forward pass\\n    if mode == 'forward':\\n        gbps = lambda ms: 2 * x.numel() * x.element_size() / ms * 1e-6\\n        ms, min_ms, max_ms = triton.testing.do_bench(y_fwd, rep=500)\\n    # backward pass\\n    if mode == 'backward':\\n        gbps = lambda ms: 3 * x.numel() * x.element_size() / ms * 1e-6\\n        y = y_fwd()\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: y.backward(dy, retain_graph=True),\\n                                                     grad_to_none=[x], rep=500)\\n    return gbps(ms), gbps(max_ms), gbps(min_ms)\\n\\n\\n# test_layer_norm(1151, 8192, torch.float16)\\nbench_layer_norm.run(save_path='.', print_data=True)\"\n+        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\ntry:\\n    # This is https://github.com/NVIDIA/apex, NOT the apex on PyPi, so it\\n    # should not be added to extras_require in setup.py.\\n    import apex\\n    HAS_APEX = True\\nexcept ModuleNotFoundError:\\n    HAS_APEX = False\\n\\n\\n@triton.jit\\ndef _layer_norm_fwd_fused(\\n    Out,\\n    A,\\n    Weight,\\n    Bias,\\n    Mean, Rstd,\\n    stride, N, eps,\\n    BLOCK_SIZE: tl.constexpr,\\n):\\n    # position of elements processed by this program\\n    row = tl.program_id(0)\\n    Out += row * stride\\n    A += row * stride\\n    # compute mean\\n    mean = 0\\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\\n    for off in range(0, N, BLOCK_SIZE):\\n        cols = off + tl.arange(0, BLOCK_SIZE)\\n        a = tl.load(A + cols, mask=cols < N, other=0., eviction_policy=\\\"evict_last\\\").to(tl.float32)\\n        _mean += a\\n    mean = tl.sum(_mean, axis=0) / N\\n    # compute variance\\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\\n    for off in range(0, N, BLOCK_SIZE):\\n        cols = off + tl.arange(0, BLOCK_SIZE)\\n        a = tl.load(A + cols, mask=cols < N, other=0., eviction_policy=\\\"evict_last\\\").to(tl.float32)\\n        a = tl.where(cols < N, a - mean, 0.)\\n        _var += a * a\\n    var = tl.sum(_var, axis=0) / N\\n    rstd = 1 / tl.sqrt(var + eps)\\n    # write-back mean/rstd\\n    tl.store(Mean + row, mean)\\n    tl.store(Rstd + row, rstd)\\n    # multiply by weight and add bias\\n    for off in range(0, N, BLOCK_SIZE):\\n        cols = off + tl.arange(0, BLOCK_SIZE)\\n        mask = cols < N\\n        weight = tl.load(Weight + cols, mask=mask)\\n        bias = tl.load(Bias + cols, mask=mask)\\n        a = tl.load(A + cols, mask=mask, other=0., eviction_policy=\\\"evict_first\\\").to(tl.float32)\\n        a_hat = (a - mean) * rstd\\n        out = a_hat * weight + bias\\n        # # write-back\\n        tl.store(Out + cols, out, mask=mask)\\n\\n# Backward pass (DA + partial DW + partial DB)\\n\\n\\n@triton.jit\\ndef _layer_norm_bwd_dx_fused(\\n    _DA,\\n    _DOut,\\n    _A,\\n    Weight,\\n    Mean, Rstd,\\n    stride, NumRows, NumCols, eps,\\n    BLOCK_SIZE_N: tl.constexpr,\\n):\\n    # position of elements processed by this program\\n    pid = tl.program_id(0)\\n    row = pid\\n    A = _A + row * stride\\n    DOut = _DOut + row * stride\\n    DA = _DA + row * stride\\n    mean = tl.load(Mean + row)\\n    rstd = tl.load(Rstd + row)\\n    # load data to SRAM\\n    _mean1 = tl.zeros([BLOCK_SIZE_N], dtype=tl.float32)\\n    _mean2 = tl.zeros([BLOCK_SIZE_N], dtype=tl.float32)\\n    for off in range(0, NumCols, BLOCK_SIZE_N):\\n        cols = off + tl.arange(0, BLOCK_SIZE_N)\\n        mask = cols < NumCols\\n        a = tl.load(A + cols, mask=mask, other=0).to(tl.float32)\\n        dout = tl.load(DOut + cols, mask=mask, other=0).to(tl.float32)\\n        weight = tl.load(Weight + cols, mask=mask, other=0).to(tl.float32)\\n        a_hat = (a - mean) * rstd\\n        wdout = weight * dout\\n        _mean1 += a_hat * wdout\\n        _mean2 += wdout\\n    mean1 = tl.sum(_mean1, axis=0) / NumCols\\n    mean2 = 0.\\n    mean2 = tl.sum(_mean2, axis=0) / NumCols\\n    for off in range(0, NumCols, BLOCK_SIZE_N):\\n        cols = off + tl.arange(0, BLOCK_SIZE_N)\\n        mask = cols < NumCols\\n        a = tl.load(A + cols, mask=mask, other=0).to(tl.float32)\\n        dout = tl.load(DOut + cols, mask=mask, other=0).to(tl.float32)\\n        weight = tl.load(Weight + cols, mask=mask, other=0).to(tl.float32)\\n        a_hat = (a - mean) * rstd\\n        wdout = weight * dout\\n        da = (wdout - (a_hat * mean1 + mean2)) * rstd\\n        # write-back dx\\n        tl.store(DA + cols, da, mask=mask)\\n\\n\\n# Backward pass (total DW + total DB)\\n@triton.jit\\ndef _layer_norm_bwd_dwdb(\\n    A, DOut,\\n    Mean, Var,\\n    DW,\\n    DB,\\n    M, N,\\n    BLOCK_SIZE_M: tl.constexpr,\\n    BLOCK_SIZE_N: tl.constexpr,\\n):\\n    pid = tl.program_id(0)\\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\\n    db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\\n    UNROLL: tl.constexpr = 4\\n    for i in range(0, M, BLOCK_SIZE_M * UNROLL):\\n        for j in range(UNROLL):\\n            rows = i + j * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\\n            mask = (rows[:, None] < M) & (cols[None, :] < N)\\n            offs = rows[:, None] * N + cols[None, :]\\n            a = tl.load(A + offs, mask=mask, other=0.).to(tl.float32)\\n            dout = tl.load(DOut + offs, mask=mask, other=0.).to(tl.float32)\\n            mean = tl.load(Mean + rows, mask=rows < M, other=0.)\\n            rstd = tl.load(Var + rows, mask=rows < M, other=0.)\\n            a_hat = (a - mean[:, None]) * rstd[:, None]\\n            dw += dout * a_hat\\n            db += dout\\n    sum_dw = tl.sum(dw, axis=0)\\n    sum_db = tl.sum(db, axis=0)\\n    tl.store(DW + cols, sum_dw, mask=cols < N)\\n    tl.store(DB + cols, sum_db, mask=cols < N)\\n\\n\\nclass LayerNorm(torch.autograd.Function):\\n    @staticmethod\\n    def forward(ctx, a, normalized_shape, weight, bias, eps):\\n        # allocate output\\n        out = torch.empty_like(a)\\n        # reshape input data into 2D tensor\\n        a_arg = a.reshape(-1, a.shape[-1])\\n        M, N = a_arg.shape\\n        mean = torch.empty((M,), dtype=torch.float32, device=\\\"cuda\\\")\\n        rstd = torch.empty((M,), dtype=torch.float32, device=\\\"cuda\\\")\\n        # Less than 64KB per feature: enqueue fused kernel\\n        MAX_FUSED_SIZE = 65536 // a.element_size()\\n        BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\\n        BLOCK_SIZE = max(BLOCK_SIZE, 128)\\n        BLOCK_SIZE = min(BLOCK_SIZE, 4096)\\n        # heuristics for number of warps\\n        num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\\n        _layer_norm_fwd_fused[(M,)](\\n            out,\\n            a_arg,\\n            weight,\\n            bias,\\n            mean, rstd,\\n            a_arg.stride(0), N, eps,\\n            BLOCK_SIZE=BLOCK_SIZE,\\n            num_warps=num_warps,\\n        )\\n        ctx.save_for_backward(\\n            a, weight, bias, mean, rstd,\\n        )\\n        ctx.BLOCK_SIZE = BLOCK_SIZE\\n        ctx.num_warps = num_warps\\n        ctx.eps = eps\\n        if hasattr(bias, \\\"config\\\"):\\n            assert bias.config.grad_scale_name == weight.config.grad_scale_name\\n            grad_scale_name = bias.config.grad_scale_name\\n        else:\\n            grad_scale_name = None\\n        ctx.grad_scale_gain_bias_name = grad_scale_name\\n        return out\\n\\n    @staticmethod\\n    def backward(ctx, dout):\\n        assert dout.is_contiguous()\\n        a, weight, bias, mean, var = ctx.saved_tensors\\n        # heuristics for amount of parallel reduction stream for DG/DB\\n        N = weight.shape[0]\\n        # allocate output\\n        da = torch.empty_like(dout)\\n        # enqueue kernel using forward pass heuristics\\n        # also compute partial sums for DW and DB\\n        x_arg = a.reshape(-1, a.shape[-1])\\n        M, N = x_arg.shape\\n        dweight = torch.empty((weight.shape[0],), dtype=weight.dtype, device=weight.device)\\n        dbias = torch.empty((weight.shape[0],), dtype=weight.dtype, device=weight.device)\\n        _layer_norm_bwd_dx_fused[(M,)](\\n            da,\\n            dout,\\n            a,\\n            weight,\\n            mean, var,\\n            x_arg.stride(0), M, N,\\n            ctx.eps,\\n            BLOCK_SIZE_N=ctx.BLOCK_SIZE,\\n            num_warps=ctx.num_warps,\\n        )\\n        if N > 10240:\\n            BLOCK_SIZE_N = 128\\n            BLOCK_SIZE_M = 32\\n            num_warps = 4\\n        else:\\n            # maximize occupancy for small N\\n            BLOCK_SIZE_N = 16\\n            BLOCK_SIZE_M = 16\\n            num_warps = 8\\n        grid = lambda meta: [triton.cdiv(N, meta[\\\"BLOCK_SIZE_N\\\"])]\\n        _layer_norm_bwd_dwdb[grid](\\n            a, dout,\\n            mean, var,\\n            dweight,\\n            dbias,\\n            M,\\n            N,\\n            BLOCK_SIZE_M=BLOCK_SIZE_M,\\n            BLOCK_SIZE_N=BLOCK_SIZE_N,\\n            num_warps=num_warps\\n        )\\n        return (da, None, dweight, dbias, None)\\n\\n\\ndef layer_norm(a, normalized_shape, weight, bias, eps):\\n    return LayerNorm.apply(a, normalized_shape, weight, bias, eps)\\n\\n\\ndef test_layer_norm(M, N, dtype, eps=1e-5, device='cuda'):\\n    torch.manual_seed(0)\\n    # create data\\n    x_shape = (M, N)\\n    w_shape = (x_shape[-1], )\\n    weight = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\\n    bias = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\\n    x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\\n    dy = .1 * torch.randn_like(x)\\n    x.requires_grad_(True)\\n    # forward pass\\n    y_tri = layer_norm(x, w_shape, weight, bias, eps)\\n    y_ref = torch.nn.functional.layer_norm(x, w_shape, weight, bias, eps).to(dtype)\\n    # backward pass (triton)\\n    y_tri.backward(dy, retain_graph=True)\\n    dx_tri, dw_tri, db_tri = [_.grad.clone() for _ in [x, weight, bias]]\\n    x.grad, weight.grad, bias.grad = None, None, None\\n    # backward pass (torch)\\n    y_ref.backward(dy, retain_graph=True)\\n    dx_ref, dw_ref, db_ref = [_.grad.clone() for _ in [x, weight, bias]]\\n    # compare\\n    triton.testing.assert_almost_equal(y_tri, y_ref)\\n    triton.testing.assert_almost_equal(dx_tri, dx_ref)\\n    triton.testing.assert_almost_equal(db_tri, db_ref, decimal=1)\\n    triton.testing.assert_almost_equal(dw_tri, dw_ref, decimal=1)\\n\\n\\n@triton.testing.perf_report(\\n    triton.testing.Benchmark(\\n        x_names=['N'],\\n        x_vals=[512 * i for i in range(2, 32)],\\n        line_arg='provider',\\n        line_vals=['triton', 'torch'] + (['apex'] if HAS_APEX else []),\\n        line_names=['Triton', 'Torch'] + (['Apex'] if HAS_APEX else []),\\n        styles=[('blue', '-'), ('green', '-'), ('orange', '-')],\\n        ylabel='GB/s',\\n        plot_name='layer-norm',\\n        args={'M': 4096, 'dtype': torch.float16, 'mode': 'forward'}\\n    )\\n)\\ndef bench_layer_norm(M, N, dtype, provider, mode, eps=1e-5, device='cuda'):\\n    # create data\\n    x_shape = (M, N)\\n    w_shape = (x_shape[-1], )\\n    weight = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\\n    bias = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\\n    x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\\n    dy = .1 * torch.randn_like(x)\\n    x.requires_grad_(True)\\n    # utility functions\\n    if provider == 'triton':\\n        y_fwd = lambda: layer_norm(x, w_shape, weight, bias, eps)\\n    if provider == 'torch':\\n        y_fwd = lambda: torch.nn.functional.layer_norm(x, w_shape, weight, bias, eps)\\n    if provider == 'apex':\\n        apex_layer_norm = apex.normalization.FusedLayerNorm(w_shape).to(x.device).to(x.dtype)\\n        y_fwd = lambda: apex_layer_norm(x)\\n    # forward pass\\n    if mode == 'forward':\\n        gbps = lambda ms: 2 * x.numel() * x.element_size() / ms * 1e-6\\n        ms, min_ms, max_ms = triton.testing.do_bench(y_fwd, rep=500)\\n    # backward pass\\n    if mode == 'backward':\\n        gbps = lambda ms: 3 * x.numel() * x.element_size() / ms * 1e-6\\n        y = y_fwd()\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: y.backward(dy, retain_graph=True),\\n                                                     grad_to_none=[x], rep=500)\\n    return gbps(ms), gbps(max_ms), gbps(min_ms)\\n\\n\\n# test_layer_norm(1151, 8192, torch.float16)\\nbench_layer_norm.run(save_path='.', print_data=True)\"\n       ]\n     }\n   ],"}, {"filename": "master/_images/sphx_glr_01-vector-add_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/_images/sphx_glr_01-vector-add_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/_images/sphx_glr_02-fused-softmax_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/_images/sphx_glr_02-fused-softmax_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/_images/sphx_glr_03-matrix-multiplication_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/_images/sphx_glr_03-matrix-multiplication_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/_images/sphx_glr_05-layer-norm_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/_images/sphx_glr_05-layer-norm_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/_images/sphx_glr_06-fused-attention_thumb.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/_images/sphx_glr_07-libdevice-function_thumb.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/_sources/getting-started/tutorials/01-vector-add.rst.txt", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -238,7 +238,7 @@ We can now run the decorated function above. Pass `print_data=True` to see the p\n     3       32768.0   76.800002   76.800002\n     4       65536.0  127.999995  127.999995\n     5      131072.0  219.428568  219.428568\n-    6      262144.0  341.333321  384.000001\n+    6      262144.0  341.333321  341.333321\n     7      524288.0  472.615390  472.615390\n     8     1048576.0  614.400016  614.400016\n     9     2097152.0  722.823517  722.823517\n@@ -255,7 +255,7 @@ We can now run the decorated function above. Pass `print_data=True` to see the p\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 1 minutes  34.829 seconds)\n+   **Total running time of the script:** ( 1 minutes  50.020 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_01-vector-add.py:"}, {"filename": "master/_sources/getting-started/tutorials/02-fused-softmax.rst.txt", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "@@ -278,17 +278,17 @@ We will then compare its performance against (1) :code:`torch.softmax` and (2) t\n \n     softmax-performance:\n               N      Triton  Torch (native)  Torch (jit)\n-    0     256.0  546.133347      512.000001   188.321838\n-    1     384.0  614.400016      585.142862   153.600004\n-    2     512.0  655.360017      585.142849   154.566038\n+    0     256.0  546.133347      546.133347   186.181817\n+    1     384.0  614.400016      585.142862   151.703707\n+    2     512.0  655.360017      606.814814   154.566038\n     3     640.0  706.206879      640.000002   160.000000\n-    4     768.0  722.823517      664.216187   162.754967\n+    4     768.0  722.823517      664.216187   163.839992\n     ..      ...         ...             ...          ...\n     93  12160.0  812.359066      406.179533   198.936606\n-    94  12288.0  812.429770      415.222812   199.298541\n-    95  12416.0  812.498981      412.149375   198.954424\n-    96  12544.0  812.566838      412.758863   199.209928\n-    97  12672.0  811.007961      412.097543   199.264875\n+    94  12288.0  812.429770      415.222812   199.096718\n+    95  12416.0  812.498981      412.149375   198.854847\n+    96  12544.0  810.925276      412.971190   199.012395\n+    97  12672.0  811.007961      412.097543   199.167004\n \n     [98 rows x 4 columns]\n \n@@ -306,7 +306,7 @@ In the above plot, we can see that:\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 3 minutes  18.076 seconds)\n+   **Total running time of the script:** ( 3 minutes  32.089 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:"}, {"filename": "master/_sources/getting-started/tutorials/03-matrix-multiplication.rst.txt", "status": "modified", "additions": 26, "deletions": 26, "changes": 52, "file_content_changes": "@@ -459,37 +459,37 @@ We can now compare the performance of our kernel against that of cuBLAS. Here we\n \n     matmul-performance:\n              M     cuBLAS  ...     Triton  Triton (+ LeakyReLU)\n-    0    256.0   2.730667  ...   2.978909              2.978909\n+    0    256.0   2.730667  ...   3.276800              2.978909\n     1    384.0   7.372800  ...   7.899428              7.899428\n-    2    512.0  14.563555  ...  15.420235             15.420235\n+    2    512.0  14.563555  ...  16.384000             15.420235\n     3    640.0  22.260869  ...  24.380953             24.380953\n     4    768.0  32.768000  ...  35.389441             34.028308\n-    5    896.0  37.971025  ...  40.140799             39.025776\n+    5    896.0  39.025776  ...  40.140799             39.025776\n     6   1024.0  49.932191  ...  53.773130             52.428801\n-    7   1152.0  45.242181  ...  48.161033             47.396572\n+    7   1152.0  45.242181  ...  47.396572             47.396572\n     8   1280.0  51.200001  ...  57.690139             57.690139\n-    9   1408.0  64.138541  ...  68.147202             65.684049\n-    10  1536.0  79.526831  ...  81.355034             78.643199\n-    11  1664.0  63.372618  ...  63.372618             62.492442\n+    9   1408.0  64.138541  ...  68.147202             66.485074\n+    10  1536.0  80.430545  ...  80.430545             78.643199\n+    11  1664.0  62.929456  ...  63.372618             62.492442\n     12  1792.0  72.983276  ...  72.983276             59.154861\n-    13  1920.0  68.776119  ...  71.626943             70.892307\n-    14  2048.0  73.584279  ...  78.033565             76.959706\n-    15  2176.0  83.155572  ...  87.494120             86.367588\n-    16  2304.0  68.446623  ...  78.064941             77.057651\n-    17  2432.0  71.305746  ...  86.179335             85.393507\n-    18  2560.0  77.833728  ...  82.956960             81.715711\n-    19  2688.0  83.737433  ...  91.185232             89.464755\n-    20  2816.0  82.446516  ...  84.523664             83.712490\n-    21  2944.0  81.967162  ...  83.758038             82.373605\n-    22  3072.0  82.420822  ...  88.750943             86.579673\n-    23  3200.0  81.528664  ...  91.233074             95.665176\n-    24  3328.0  83.516586  ...  85.908470             83.323259\n-    25  3456.0  81.435930  ...  92.138932             90.180725\n-    26  3584.0  83.954614  ...  91.189190             95.858629\n-    27  3712.0  85.822459  ...  83.806497             87.783251\n-    28  3840.0  80.901241  ...  89.259080             89.548180\n-    29  3968.0  87.913500  ...  92.829164             84.096442\n-    30  4096.0  93.825748  ...  89.299883             90.139506\n+    13  1920.0  69.120002  ...  71.257735             71.257735\n+    14  2048.0  73.584279  ...  78.398206             77.314362\n+    15  2176.0  83.155572  ...  87.494120             85.998493\n+    16  2304.0  68.446623  ...  78.320893             77.558029\n+    17  2432.0  71.305746  ...  86.711310             75.421383\n+    18  2560.0  77.833728  ...  82.747477             81.715711\n+    19  2688.0  83.552988  ...  90.532356             89.464755\n+    20  2816.0  84.197315  ...  84.035084             84.035084\n+    21  2944.0  82.784108  ...  83.969728             83.060049\n+    22  3072.0  81.825298  ...  89.593522             88.473602\n+    23  3200.0  84.768213  ...  96.096095             95.808380\n+    24  3328.0  83.226931  ...  85.908470             84.596116\n+    25  3456.0  81.766291  ...  91.824110             91.097818\n+    26  3584.0  87.466332  ...  91.194972             94.847460\n+    27  3712.0  85.822459  ...  87.246590             87.860458\n+    28  3840.0  81.859361  ...  87.011801             90.168771\n+    29  3968.0  89.921841  ...  91.954739             85.271796\n+    30  4096.0  93.596744  ...  88.243079             90.382307\n \n     [31 rows x 5 columns]\n \n@@ -499,7 +499,7 @@ We can now compare the performance of our kernel against that of cuBLAS. Here we\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 5 minutes  52.578 seconds)\n+   **Total running time of the script:** ( 7 minutes  13.827 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_03-matrix-multiplication.py:"}, {"filename": "master/_sources/getting-started/tutorials/04-low-memory-dropout.rst.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -240,7 +240,7 @@ References\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  0.476 seconds)\n+   **Total running time of the script:** ( 0 minutes  0.279 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_04-low-memory-dropout.py:"}, {"filename": "master/_sources/getting-started/tutorials/05-layer-norm.rst.txt", "status": "modified", "additions": 55, "deletions": 51, "changes": 106, "file_content_changes": "@@ -21,7 +21,7 @@\n Layer Normalization\n ====================\n \n-.. GENERATED FROM PYTHON SOURCE LINES 5-312\n+.. GENERATED FROM PYTHON SOURCE LINES 5-316\n \n \n \n@@ -40,34 +40,34 @@ Layer Normalization\n               N      Triton       Torch        Apex\n     0    1024.0  585.142849  277.694907  468.114273\n     1    1536.0  630.153868  323.368435  511.999982\n-    2    2048.0  682.666643  334.367358  520.126988\n-    3    2560.0  694.237267  365.714281  518.481028\n-    4    3072.0  712.347810  378.092307  501.551037\n-    5    3584.0  725.873439  384.859062  458.751978\n-    6    4096.0  728.177767  381.023256  458.293714\n-    7    4608.0  670.254540  396.387087  426.173427\n-    8    5120.0  694.237267  397.669909  426.666652\n-    9    5632.0  704.000002  396.969169  413.357796\n-    10   6144.0  702.171410  402.885254  411.313806\n+    2    2048.0  668.734716  337.814445  528.516136\n+    3    2560.0  694.237267  362.477870  512.000013\n+    4    3072.0  712.347810  375.206126  501.551037\n+    5    3584.0  725.873439  384.859062  451.527536\n+    6    4096.0  728.177767  381.023256  455.111095\n+    7    4608.0  670.254540  396.387087  421.302872\n+    8    5120.0  688.403381  395.748783  422.268057\n+    9    5632.0  698.542675  396.969169  409.599997\n+    10   6144.0  702.171410  402.885254  409.600010\n     11   6656.0  700.631610  400.360920  400.360920\n-    12   7168.0  695.078767  396.844306  388.772874\n-    13   7680.0  682.666656  393.846167  387.634072\n-    14   8192.0  642.509816  393.609605  372.363633\n-    15   8704.0  627.315309  389.005597  380.502740\n-    16   9216.0  606.814809  407.337026  383.999986\n-    17   9728.0  589.575753  409.599987  383.369452\n-    18  10240.0  566.920437  408.578556  382.803739\n-    19  10752.0  549.623009  411.559798  381.445676\n-    20  11264.0  536.380957  406.826188  373.134567\n-    21  11776.0  523.377770  410.492372  377.587162\n-    22  12288.0  517.389457  414.784810  383.251457\n-    23  12800.0  505.679014  410.420828  376.470582\n-    24  13312.0  494.180982  405.699062  376.976995\n-    25  13824.0  482.934503  411.888257  379.389355\n-    26  14336.0  471.967074  406.695045  374.185964\n-    27  14848.0  461.297068  408.192434  375.304904\n-    28  15360.0  454.269882  406.214870  378.092307\n-    29  15872.0  447.887117  407.627589  376.225175\n+    12   7168.0  678.627194  386.154893  384.859062\n+    13   7680.0  682.666656  391.337574  386.415087\n+    14   8192.0  645.674867  390.095241  376.643677\n+    15   8704.0  624.502255  390.095225  379.465939\n+    16   9216.0  604.327881  405.098894  383.002605\n+    17   9728.0  585.142883  409.599987  382.427505\n+    18  10240.0  564.965524  409.600010  382.803739\n+    19  10752.0  546.133312  410.577576  380.601764\n+    20  11264.0  531.634232  395.228063  370.069806\n+    21  11776.0  520.486200  409.599991  376.831982\n+    22  12288.0  516.031509  413.911572  383.251457\n+    23  12800.0  504.433489  410.420828  375.779805\n+    24  13312.0  494.180982  405.699062  376.310952\n+    25  13824.0  481.882350  411.888257  378.739711\n+    26  14336.0  471.967074  401.709294  372.969090\n+    27  14848.0  461.297068  407.492270  375.898745\n+    28  15360.0  453.431739  406.887417  378.092307\n+    29  15872.0  447.098578  406.323209  376.225175\n \n \n \n@@ -204,17 +204,19 @@ Layer Normalization\n         cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n         dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n         db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-        for i in range(0, M, BLOCK_SIZE_M):\n-            rows = i + tl.arange(0, BLOCK_SIZE_M)\n-            mask = (rows[:, None] < M) & (cols[None, :] < N)\n-            offs = rows[:, None] * N + cols[None, :]\n-            a = tl.load(A + offs, mask=mask, other=0.).to(tl.float32)\n-            dout = tl.load(DOut + offs, mask=mask, other=0.).to(tl.float32)\n-            mean = tl.load(Mean + rows, mask=rows < M, other=0.)\n-            rstd = tl.load(Var + rows, mask=rows < M, other=0.)\n-            a_hat = (a - mean[:, None]) * rstd[:, None]\n-            dw += dout * a_hat\n-            db += dout\n+        UNROLL: tl.constexpr = 4\n+        for i in range(0, M, BLOCK_SIZE_M * UNROLL):\n+            for j in range(UNROLL):\n+                rows = i + j * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+                mask = (rows[:, None] < M) & (cols[None, :] < N)\n+                offs = rows[:, None] * N + cols[None, :]\n+                a = tl.load(A + offs, mask=mask, other=0.).to(tl.float32)\n+                dout = tl.load(DOut + offs, mask=mask, other=0.).to(tl.float32)\n+                mean = tl.load(Mean + rows, mask=rows < M, other=0.)\n+                rstd = tl.load(Var + rows, mask=rows < M, other=0.)\n+                a_hat = (a - mean[:, None]) * rstd[:, None]\n+                dw += dout * a_hat\n+                db += dout\n         sum_dw = tl.sum(dw, axis=0)\n         sum_db = tl.sum(db, axis=0)\n         tl.store(DW + cols, sum_dw, mask=cols < N)\n@@ -287,7 +289,15 @@ Layer Normalization\n                 BLOCK_SIZE_N=ctx.BLOCK_SIZE,\n                 num_warps=ctx.num_warps,\n             )\n-            # accumulate partial sums in separate kernel\n+            if N > 10240:\n+                BLOCK_SIZE_N = 128\n+                BLOCK_SIZE_M = 32\n+                num_warps = 4\n+            else:\n+                # maximize occupancy for small N\n+                BLOCK_SIZE_N = 16\n+                BLOCK_SIZE_M = 16\n+                num_warps = 8\n             grid = lambda meta: [triton.cdiv(N, meta[\"BLOCK_SIZE_N\"])]\n             _layer_norm_bwd_dwdb[grid](\n                 a, dout,\n@@ -296,17 +306,11 @@ Layer Normalization\n                 dbias,\n                 M,\n                 N,\n-                BLOCK_SIZE_M=32,\n-                BLOCK_SIZE_N=128,\n+                BLOCK_SIZE_M=BLOCK_SIZE_M,\n+                BLOCK_SIZE_N=BLOCK_SIZE_N,\n+                num_warps=num_warps\n             )\n-            return (da, None, dweight, dbias, None, None,\n-                    None, None, None, None,\n-                    None,\n-                    None, None, None,\n-                    None,\n-                    None, None, None,\n-                    None, None, None,\n-                    None, None, None)\n+            return (da, None, dweight, dbias, None)\n \n \n     def layer_norm(a, normalized_shape, weight, bias, eps):\n@@ -389,7 +393,7 @@ Layer Normalization\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 5 minutes  24.641 seconds)\n+   **Total running time of the script:** ( 5 minutes  32.552 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_05-layer-norm.py:"}, {"filename": "master/_sources/getting-started/tutorials/06-fused-attention.rst.txt", "status": "added", "additions": 416, "deletions": 0, "changes": 416, "file_content_changes": "@@ -0,0 +1,416 @@\n+\n+.. DO NOT EDIT.\n+.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.\n+.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:\n+.. \"getting-started/tutorials/06-fused-attention.py\"\n+.. LINE NUMBERS ARE GIVEN BELOW.\n+\n+.. only:: html\n+\n+    .. note::\n+        :class: sphx-glr-download-link-note\n+\n+        Click :ref:`here <sphx_glr_download_getting-started_tutorials_06-fused-attention.py>`\n+        to download the full example code\n+\n+.. rst-class:: sphx-glr-example-title\n+\n+.. _sphx_glr_getting-started_tutorials_06-fused-attention.py:\n+\n+\n+Fused Attention\n+===============\n+This is a Triton implementation of the Flash Attention algorithm \n+(see: Dao et al., https://arxiv.org/pdf/2205.14135v2.pdf; Rabe and Staats https://arxiv.org/pdf/2112.05682v2.pdf)\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 7-355\n+\n+\n+\n+\n+\n+\n+\n+.. code-block:: default\n+\n+\n+    import pytest\n+    import torch\n+\n+    import triton\n+    import triton.language as tl\n+\n+\n+    @triton.jit\n+    def _fwd_kernel(\n+        Q, K, V, sm_scale,\n+        TMP, L, M,  # NOTE: TMP is a scratchpad buffer to workaround a compiler bug\n+        Out,\n+        stride_qz, stride_qh, stride_qm, stride_qk,\n+        stride_kz, stride_kh, stride_kn, stride_kk,\n+        stride_vz, stride_vh, stride_vk, stride_vn,\n+        stride_oz, stride_oh, stride_om, stride_on,\n+        Z, H, N_CTX,\n+        BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n+        BLOCK_N: tl.constexpr,\n+    ):\n+        start_m = tl.program_id(0)\n+        off_hz = tl.program_id(1)\n+        # initialize offsets\n+        offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+        offs_n = tl.arange(0, BLOCK_N)\n+        offs_d = tl.arange(0, BLOCK_DMODEL)\n+        off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n+        off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :] * stride_kk\n+        off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n+        # Initialize pointers to Q, K, V\n+        q_ptrs = Q + off_q\n+        k_ptrs = K + off_k\n+        v_ptrs = V + off_v\n+        # initialize pointer to m and l\n+        t_ptrs = TMP + off_hz * N_CTX + offs_m\n+        m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+        l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n+        acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+        # load q: it will stay in SRAM throughout\n+        q = tl.load(q_ptrs)\n+        # loop over k, v and update accumulator\n+        for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n+            start_n = tl.multiple_of(start_n, BLOCK_N)\n+            # -- compute qk ----\n+            k = tl.load(k_ptrs + start_n * stride_kn)\n+            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n+            qk += tl.dot(q, k, trans_b=True)\n+            qk *= sm_scale\n+            qk += tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), 0, float(\"-inf\"))\n+            # -- compute m_ij, p, l_ij\n+            m_ij = tl.max(qk, 1)\n+            p = tl.exp(qk - m_ij[:, None])\n+            l_ij = tl.sum(p, 1)\n+            # -- update m_i and l_i\n+            m_i_new = tl.maximum(m_i, m_ij)\n+            alpha = tl.exp(m_i - m_i_new)\n+            beta = tl.exp(m_ij - m_i_new)\n+            l_i_new = alpha * l_i + beta * l_ij\n+            # -- update output accumulator --\n+            # scale p\n+            p_scale = beta / l_i_new\n+            p = p * p_scale[:, None]\n+            # scale acc\n+            acc_scale = l_i / l_i_new * alpha\n+            tl.store(t_ptrs, acc_scale)\n+            acc_scale = tl.load(t_ptrs)  # BUG: have to store and immediately load\n+            acc = acc * acc_scale[:, None]\n+            # update acc\n+            v = tl.load(v_ptrs + start_n * stride_vk)\n+            p = p.to(tl.float16)\n+            acc += tl.dot(p, v)\n+            # update m_i and l_i\n+            l_i = l_i_new\n+            m_i = m_i_new\n+        # rematerialize offsets to save registers\n+        start_m = tl.program_id(0)\n+        offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+        # write back l and m\n+        l_ptrs = L + off_hz * N_CTX + offs_m\n+        m_ptrs = M + off_hz * N_CTX + offs_m\n+        tl.store(l_ptrs, l_i)\n+        tl.store(m_ptrs, m_i)\n+        # initialize pointers to output\n+        offs_n = tl.arange(0, BLOCK_DMODEL)\n+        off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n+        out_ptrs = Out + off_o\n+        tl.store(out_ptrs, acc)\n+\n+\n+    @triton.jit\n+    def _bwd_preprocess(\n+        Out, DO, L,\n+        NewDO, Delta,\n+        BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\n+    ):\n+        off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n+        off_n = tl.arange(0, D_HEAD)\n+        # load\n+        o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n+        do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n+        denom = tl.load(L + off_m).to(tl.float32)\n+        # compute\n+        do = do / denom[:, None]\n+        delta = tl.sum(o * do, axis=1)\n+        # write-back\n+        tl.store(NewDO + off_m[:, None] * D_HEAD + off_n[None, :], do)\n+        tl.store(Delta + off_m, delta)\n+\n+\n+    @triton.jit\n+    def _bwd_kernel(\n+        Q, K, V, sm_scale, Out, DO,\n+        DQ, DK, DV,\n+        L, M,\n+        D,\n+        stride_qz, stride_qh, stride_qm, stride_qk,\n+        stride_kz, stride_kh, stride_kn, stride_kk,\n+        stride_vz, stride_vh, stride_vk, stride_vn,\n+        Z, H, N_CTX,\n+        num_block,\n+        BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n+        BLOCK_N: tl.constexpr,\n+    ):\n+        off_hz = tl.program_id(0)\n+        off_z = off_hz // H\n+        off_h = off_hz % H\n+        # offset pointers for batch/head\n+        Q += off_z * stride_qz + off_h * stride_qh\n+        K += off_z * stride_qz + off_h * stride_qh\n+        V += off_z * stride_qz + off_h * stride_qh\n+        DO += off_z * stride_qz + off_h * stride_qh\n+        DQ += off_z * stride_qz + off_h * stride_qh\n+        DK += off_z * stride_qz + off_h * stride_qh\n+        DV += off_z * stride_qz + off_h * stride_qh\n+        for start_n in range(0, num_block):\n+            lo = start_n * BLOCK_M\n+            # initialize row/col offsets\n+            offs_qm = lo + tl.arange(0, BLOCK_M)\n+            offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n+            offs_m = tl.arange(0, BLOCK_N)\n+            offs_k = tl.arange(0, BLOCK_DMODEL)\n+            # initialize pointers to value-like data\n+            q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+            k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n+            v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+            do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+            dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+            # pointer to row-wise quantities in value-like data\n+            D_ptrs = D + off_hz * N_CTX\n+            m_ptrs = M + off_hz * N_CTX\n+            # initialize dv amd dk\n+            dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+            dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+            # k and v stay in SRAM throughout\n+            k = tl.load(k_ptrs)\n+            v = tl.load(v_ptrs)\n+            # loop over rows\n+            for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n+                offs_m_curr = start_m + offs_m\n+                # load q, k, v, do on-chip\n+                q = tl.load(q_ptrs)\n+                # recompute p = softmax(qk, dim=-1).T\n+                # NOTE: `do` is pre-divided by `l`; no normalization here\n+                qk = tl.dot(q, k, trans_b=True)\n+                qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\"-inf\"))\n+                m = tl.load(m_ptrs + offs_m_curr)\n+                p = tl.exp(qk * sm_scale - m[:, None])\n+                # compute dv\n+                do = tl.load(do_ptrs)\n+                dv += tl.dot(p.to(tl.float16), do, trans_a=True)\n+                # compute dp = dot(v, do)\n+                Di = tl.load(D_ptrs + offs_m_curr)\n+                dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n+                dp += tl.dot(do, v, trans_b=True)\n+                # compute ds = p * (dp - delta[:, None])\n+                ds = p * dp * sm_scale\n+                # compute dk = dot(ds.T, q)\n+                dk += tl.dot(ds.to(tl.float16), q, trans_a=True)\n+                # # compute dq\n+                dq = tl.load(dq_ptrs, eviction_policy=\"evict_last\")\n+                dq += tl.dot(ds.to(tl.float16), k)\n+                tl.store(dq_ptrs, dq, eviction_policy=\"evict_last\")\n+                # # increment pointers\n+                dq_ptrs += BLOCK_M * stride_qm\n+                q_ptrs += BLOCK_M * stride_qm\n+                do_ptrs += BLOCK_M * stride_qm\n+            # write-back\n+            dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+            dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n+            tl.store(dv_ptrs, dv)\n+            tl.store(dk_ptrs, dk)\n+\n+\n+    class _attention(torch.autograd.Function):\n+\n+        @staticmethod\n+        def forward(ctx, q, k, v, sm_scale):\n+            BLOCK = 128\n+            # shape constraints\n+            Lq, Lk = q.shape[-1], k.shape[-1]\n+            assert Lq == Lk\n+            o = torch.empty_like(q)\n+            grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1])\n+            tmp = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+            L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+            m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+            _fwd_kernel[grid](\n+                q, k, v, sm_scale,\n+                tmp, L, m,\n+                o,\n+                q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+                k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+                v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+                o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n+                q.shape[0], q.shape[1], q.shape[2],\n+                BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n+                BLOCK_DMODEL=64, num_warps=4,\n+                num_stages=1,\n+            )\n+            ctx.save_for_backward(q, k, v, o, L, m)\n+            ctx.BLOCK = BLOCK\n+            ctx.grid = grid\n+            ctx.sm_scale = sm_scale\n+            ctx.BLOCK_DMODEL = 64\n+            return o\n+\n+        @staticmethod\n+        def backward(ctx, do):\n+            q, k, v, o, l, m = ctx.saved_tensors\n+            do = do.contiguous()\n+            dq = torch.zeros_like(q, dtype=torch.float32)\n+            dk = torch.empty_like(k)\n+            dv = torch.empty_like(v)\n+            do_scaled = torch.empty_like(do)\n+            delta = torch.empty_like(l)\n+            _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n+                o, do, l,\n+                do_scaled, delta,\n+                BLOCK_M=ctx.BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n+            )\n+            _bwd_kernel[(ctx.grid[1],)](\n+                q, k, v, ctx.sm_scale,\n+                o, do_scaled,\n+                dq, dk, dv,\n+                l, m,\n+                delta,\n+                q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+                k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+                v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+                q.shape[0], q.shape[1], q.shape[2],\n+                ctx.grid[0],\n+                BLOCK_M=ctx.BLOCK, BLOCK_N=ctx.BLOCK,\n+                BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n+                num_stages=1,\n+            )\n+            return dq, dk, dv, None\n+\n+\n+    attention = _attention.apply\n+\n+\n+    @pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(3, 2, 2048, 64)])\n+    def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n+        torch.manual_seed(20)\n+        q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0, std=.5).requires_grad_()\n+        k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0, std=.5).requires_grad_()\n+        v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0, std=.5).requires_grad_()\n+        sm_scale = 0.3\n+        dout = torch.randn_like(q)\n+        # reference implementation\n+        M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n+        p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n+        for z in range(Z):\n+            for h in range(H):\n+                p[:, :, M == 0] = float(\"-inf\")\n+        p = torch.softmax(p.float(), dim=-1).half()\n+        ref_out = torch.matmul(p, v)\n+        ref_out.backward(dout)\n+        ref_dv, v.grad = v.grad.clone(), None\n+        ref_dk, k.grad = k.grad.clone(), None\n+        ref_dq, q.grad = q.grad.clone(), None\n+        # triton implementation\n+        tri_out = attention(q, k, v, sm_scale)\n+        tri_out.backward(dout)\n+        tri_dv, v.grad = v.grad.clone(), None\n+        tri_dk, k.grad = k.grad.clone(), None\n+        tri_dq, q.grad = q.grad.clone(), None\n+        # compare\n+        triton.testing.assert_almost_equal(ref_out, tri_out)\n+        triton.testing.assert_almost_equal(ref_dv, tri_dv)\n+        triton.testing.assert_almost_equal(ref_dk, tri_dk)\n+        triton.testing.assert_almost_equal(ref_dq, tri_dq)\n+\n+\n+    try:\n+        from flash_attn.flash_attn_interface import flash_attn_func\n+        HAS_FLASH = True\n+    except BaseException:\n+        HAS_FLASH = False\n+\n+    BATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\n+    # vary seq length for fixed head and batch=4\n+    configs = [triton.testing.Benchmark(\n+        x_names=['N_CTX'],\n+        x_vals=[2**i for i in range(10, 16)],\n+        line_arg='provider',\n+        line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n+        line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\n+        styles=[('red', '-'), ('blue', '-')],\n+        ylabel='ms',\n+        plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n+        args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode}\n+    ) for mode in ['bwd']]\n+\n+\n+    @triton.testing.perf_report(configs)\n+    def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.float16, device=\"cuda\"):\n+        assert mode in ['fwd', 'bwd']\n+        warmup = 25\n+        rep = 100\n+        if provider == \"triton\":\n+            q = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+            k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+            v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+            sm_scale = 1.3\n+            fn = lambda: attention(q, k, v, sm_scale)\n+            if mode == 'bwd':\n+                o = fn()\n+                do = torch.randn_like(o)\n+                fn = lambda: o.backward(do, retain_graph=True)\n+            ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n+            return ms\n+        if provider == \"flash\":\n+            lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n+            cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n+            cu_seqlens[1:] = lengths.cumsum(0)\n+            qkv = torch.randn((BATCH * N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\n+            fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=True)\n+            if mode == 'bwd':\n+                o = fn()\n+                do = torch.randn_like(o)\n+                fn = lambda: o.backward(do, retain_graph=True)\n+            ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n+            return ms\n+\n+    # only works on A100 at the moment\n+    # bench_flash_attention.run(save_path='.', print_data=True)\n+\n+\n+.. rst-class:: sphx-glr-timing\n+\n+   **Total running time of the script:** ( 0 minutes  0.072 seconds)\n+\n+\n+.. _sphx_glr_download_getting-started_tutorials_06-fused-attention.py:\n+\n+\n+.. only :: html\n+\n+ .. container:: sphx-glr-footer\n+    :class: sphx-glr-footer-example\n+\n+\n+\n+  .. container:: sphx-glr-download sphx-glr-download-python\n+\n+     :download:`Download Python source code: 06-fused-attention.py <06-fused-attention.py>`\n+\n+\n+\n+  .. container:: sphx-glr-download sphx-glr-download-jupyter\n+\n+     :download:`Download Jupyter notebook: 06-fused-attention.ipynb <06-fused-attention.ipynb>`\n+\n+\n+.. only:: html\n+\n+ .. rst-class:: sphx-glr-signature\n+\n+    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"}, {"filename": "master/_sources/getting-started/tutorials/07-libdevice-function.rst.txt", "status": "added", "additions": 183, "deletions": 0, "changes": 183, "file_content_changes": "@@ -0,0 +1,183 @@\n+\n+.. DO NOT EDIT.\n+.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.\n+.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:\n+.. \"getting-started/tutorials/07-libdevice-function.py\"\n+.. LINE NUMBERS ARE GIVEN BELOW.\n+\n+.. only:: html\n+\n+    .. note::\n+        :class: sphx-glr-download-link-note\n+\n+        Click :ref:`here <sphx_glr_download_getting-started_tutorials_07-libdevice-function.py>`\n+        to download the full example code\n+\n+.. rst-class:: sphx-glr-example-title\n+\n+.. _sphx_glr_getting-started_tutorials_07-libdevice-function.py:\n+\n+\n+Libdevice function\n+===============\n+Triton can invoke a custom function from an external library.\n+In this example, we will use the `libdevice` library to apply `asin` on a tensor.\n+Please refer to https://docs.nvidia.com/cuda/libdevice-users-guide/index.html regarding the semantics of all available libdevice functions.\n+\n+In `trition/language/libdevice.py`, we try to aggregate functions with the same computation but different data types together.\n+For example, both `__nv_asin` and `__nvasinf` calculate the principal value of the arc sine of the input, but `__nv_asin` operates on `double` and `__nv_asinf` operates on `float`.\n+Using triton, you can simply call `tl.libdevice.asinf`.\n+triton automatically selects the correct underlying device function to invoke based on input and output types.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 15-17\n+\n+asin Kernel\n+--------------------------\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 17-39\n+\n+.. code-block:: default\n+\n+\n+    import torch\n+\n+    import triton\n+    import triton.language as tl\n+\n+\n+    @triton.jit\n+    def asin_kernel(\n+        x_ptr,\n+        y_ptr,\n+        n_elements,\n+        BLOCK_SIZE: tl.constexpr,\n+    ):\n+        pid = tl.program_id(axis=0)\n+        block_start = pid * BLOCK_SIZE\n+        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+        mask = offsets < n_elements\n+        x = tl.load(x_ptr + offsets, mask=mask)\n+        x = tl.libdevice.asin(x)\n+        tl.store(y_ptr + offsets, x, mask=mask)\n+\n+\n+\n+\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 40-43\n+\n+Using the default libdevice library path\n+--------------------------\n+We can use the default libdevice library path encoded in `triton/language/libdevice.py`\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 43-61\n+\n+.. code-block:: default\n+\n+\n+\n+    torch.manual_seed(0)\n+    size = 98432\n+    x = torch.rand(size, device='cuda')\n+    output_triton = torch.zeros(size, device='cuda')\n+    output_torch = torch.asin(x)\n+    assert x.is_cuda and output_triton.is_cuda\n+    n_elements = output_torch.numel()\n+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+    asin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024)\n+    print(output_torch)\n+    print(output_triton)\n+    print(\n+        f'The maximum difference between torch and triton is '\n+        f'{torch.max(torch.abs(output_torch - output_triton))}'\n+    )\n+\n+\n+\n+\n+\n+.. rst-class:: sphx-glr-script-out\n+\n+ Out:\n+\n+ .. code-block:: none\n+\n+    tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device='cuda:0')\n+    tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device='cuda:0')\n+    The maximum difference between torch and triton is 2.384185791015625e-07\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 62-65\n+\n+Customize the libdevice library path\n+--------------------------\n+We can also customize the libdevice library path by passing the path to the `libdevice` library to the `asin` kernel.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 65-75\n+\n+.. code-block:: default\n+\n+\n+    output_triton = torch.empty_like(x)\n+    asin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024,\n+                      extern_libs={'libdevice': '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'})\n+    print(output_torch)\n+    print(output_triton)\n+    print(\n+        f'The maximum difference between torch and triton is '\n+        f'{torch.max(torch.abs(output_torch - output_triton))}'\n+    )\n+\n+\n+\n+\n+.. rst-class:: sphx-glr-script-out\n+\n+ Out:\n+\n+ .. code-block:: none\n+\n+    tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device='cuda:0')\n+    tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device='cuda:0')\n+    The maximum difference between torch and triton is 2.384185791015625e-07\n+\n+\n+\n+\n+\n+.. rst-class:: sphx-glr-timing\n+\n+   **Total running time of the script:** ( 0 minutes  0.501 seconds)\n+\n+\n+.. _sphx_glr_download_getting-started_tutorials_07-libdevice-function.py:\n+\n+\n+.. only :: html\n+\n+ .. container:: sphx-glr-footer\n+    :class: sphx-glr-footer-example\n+\n+\n+\n+  .. container:: sphx-glr-download sphx-glr-download-python\n+\n+     :download:`Download Python source code: 07-libdevice-function.py <07-libdevice-function.py>`\n+\n+\n+\n+  .. container:: sphx-glr-download sphx-glr-download-jupyter\n+\n+     :download:`Download Jupyter notebook: 07-libdevice-function.ipynb <07-libdevice-function.ipynb>`\n+\n+\n+.. only:: html\n+\n+ .. rst-class:: sphx-glr-signature\n+\n+    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"}, {"filename": "master/_sources/getting-started/tutorials/index.rst.txt", "status": "modified", "additions": 42, "deletions": 0, "changes": 42, "file_content_changes": "@@ -122,6 +122,48 @@ To install the dependencies for the tutorials:\n    :hidden:\n \n    /getting-started/tutorials/05-layer-norm\n+\n+.. raw:: html\n+\n+    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"Fused Attention\">\n+\n+.. only:: html\n+\n+ .. figure:: /getting-started/tutorials/images/thumb/sphx_glr_06-fused-attention_thumb.png\n+     :alt: Fused Attention\n+\n+     :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py`\n+\n+.. raw:: html\n+\n+    </div>\n+\n+\n+.. toctree::\n+   :hidden:\n+\n+   /getting-started/tutorials/06-fused-attention\n+\n+.. raw:: html\n+\n+    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"In trition/language/libdevice.py, we try to aggregate functions with the same computation but d...\">\n+\n+.. only:: html\n+\n+ .. figure:: /getting-started/tutorials/images/thumb/sphx_glr_07-libdevice-function_thumb.png\n+     :alt: Libdevice function\n+\n+     :ref:`sphx_glr_getting-started_tutorials_07-libdevice-function.py`\n+\n+.. raw:: html\n+\n+    </div>\n+\n+\n+.. toctree::\n+   :hidden:\n+\n+   /getting-started/tutorials/07-libdevice-function\n .. raw:: html\n \n     <div class=\"sphx-glr-clear\"></div>"}, {"filename": "master/_sources/getting-started/tutorials/sg_execution_times.rst.txt", "status": "modified", "additions": 10, "deletions": 6, "changes": 16, "file_content_changes": "@@ -5,16 +5,20 @@\n \n Computation times\n =================\n-**16:10.599** total execution time for **getting-started_tutorials** files:\n+**18:09.339** total execution time for **getting-started_tutorials** files:\n \n +---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``) | 05:52.578 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``) | 07:13.827 | 0.0 MB |\n +---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                       | 05:24.641 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                       | 05:32.552 | 0.0 MB |\n +---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                 | 03:18.076 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                 | 03:32.089 | 0.0 MB |\n +---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                       | 01:34.829 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                       | 01:50.020 | 0.0 MB |\n +---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)       | 00:00.476 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_07-libdevice-function.py` (``07-libdevice-function.py``)       | 00:00.501 | 0.0 MB |\n++---------------------------------------------------------------------------------------------------------+-----------+--------+\n+| :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)       | 00:00.279 | 0.0 MB |\n++---------------------------------------------------------------------------------------------------------+-----------+--------+\n+| :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py` (``06-fused-attention.py``)             | 00:00.072 | 0.0 MB |\n +---------------------------------------------------------------------------------------------------------+-----------+--------+"}, {"filename": "master/getting-started/tutorials/01-vector-add.html", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -105,6 +105,8 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-libdevice-function.html\">Libdevice function</a></li>\n </ul>\n </li>\n </ul>\n@@ -328,7 +330,7 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n 3       32768.0   76.800002   76.800002\n 4       65536.0  127.999995  127.999995\n 5      131072.0  219.428568  219.428568\n-6      262144.0  341.333321  384.000001\n+6      262144.0  341.333321  341.333321\n 7      524288.0  472.615390  472.615390\n 8     1048576.0  614.400016  614.400016\n 9     2097152.0  722.823517  722.823517\n@@ -340,7 +342,7 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n 15  134217728.0  849.737435  850.656574\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  34.829 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  50.020 seconds)</p>\n <div class=\"sphx-glr-footer class sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-01-vector-add-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/62d97d49a32414049819dd8bb8378080/01-vector-add.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">01-vector-add.py</span></code></a></p>"}, {"filename": "master/getting-started/tutorials/02-fused-softmax.html", "status": "modified", "additions": 11, "deletions": 9, "changes": 20, "file_content_changes": "@@ -108,6 +108,8 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-libdevice-function.html\">Libdevice function</a></li>\n </ul>\n </li>\n </ul>\n@@ -369,17 +371,17 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n <p class=\"sphx-glr-script-out\">Out:</p>\n <div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>softmax-performance:\n           N      Triton  Torch (native)  Torch (jit)\n-0     256.0  546.133347      512.000001   188.321838\n-1     384.0  614.400016      585.142862   153.600004\n-2     512.0  655.360017      585.142849   154.566038\n+0     256.0  546.133347      546.133347   186.181817\n+1     384.0  614.400016      585.142862   151.703707\n+2     512.0  655.360017      606.814814   154.566038\n 3     640.0  706.206879      640.000002   160.000000\n-4     768.0  722.823517      664.216187   162.754967\n+4     768.0  722.823517      664.216187   163.839992\n ..      ...         ...             ...          ...\n 93  12160.0  812.359066      406.179533   198.936606\n-94  12288.0  812.429770      415.222812   199.298541\n-95  12416.0  812.498981      412.149375   198.954424\n-96  12544.0  812.566838      412.758863   199.209928\n-97  12672.0  811.007961      412.097543   199.264875\n+94  12288.0  812.429770      415.222812   199.096718\n+95  12416.0  812.498981      412.149375   198.854847\n+96  12544.0  810.925276      412.971190   199.012395\n+97  12672.0  811.007961      412.097543   199.167004\n \n [98 rows x 4 columns]\n </pre></div>\n@@ -392,7 +394,7 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n Note however that the PyTorch <cite>softmax</cite> operation is more general and will works on tensors of any shape.</p></li>\n </ul>\n </div></blockquote>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 3 minutes  18.076 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 3 minutes  32.089 seconds)</p>\n <div class=\"sphx-glr-footer class sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-02-fused-softmax-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d91442ac2982c4e0cc3ab0f43534afbc/02-fused-softmax.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">02-fused-softmax.py</span></code></a></p>"}, {"filename": "master/getting-started/tutorials/03-matrix-multiplication.html", "status": "modified", "additions": 28, "deletions": 26, "changes": 54, "file_content_changes": "@@ -115,6 +115,8 @@\n </li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-libdevice-function.html\">Libdevice function</a></li>\n </ul>\n </li>\n </ul>\n@@ -565,42 +567,42 @@ <h3>Square Matrix Performance<a class=\"headerlink\" href=\"#square-matrix-performa\n <p class=\"sphx-glr-script-out\">Out:</p>\n <div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>matmul-performance:\n          M     cuBLAS  ...     Triton  Triton (+ LeakyReLU)\n-0    256.0   2.730667  ...   2.978909              2.978909\n+0    256.0   2.730667  ...   3.276800              2.978909\n 1    384.0   7.372800  ...   7.899428              7.899428\n-2    512.0  14.563555  ...  15.420235             15.420235\n+2    512.0  14.563555  ...  16.384000             15.420235\n 3    640.0  22.260869  ...  24.380953             24.380953\n 4    768.0  32.768000  ...  35.389441             34.028308\n-5    896.0  37.971025  ...  40.140799             39.025776\n+5    896.0  39.025776  ...  40.140799             39.025776\n 6   1024.0  49.932191  ...  53.773130             52.428801\n-7   1152.0  45.242181  ...  48.161033             47.396572\n+7   1152.0  45.242181  ...  47.396572             47.396572\n 8   1280.0  51.200001  ...  57.690139             57.690139\n-9   1408.0  64.138541  ...  68.147202             65.684049\n-10  1536.0  79.526831  ...  81.355034             78.643199\n-11  1664.0  63.372618  ...  63.372618             62.492442\n+9   1408.0  64.138541  ...  68.147202             66.485074\n+10  1536.0  80.430545  ...  80.430545             78.643199\n+11  1664.0  62.929456  ...  63.372618             62.492442\n 12  1792.0  72.983276  ...  72.983276             59.154861\n-13  1920.0  68.776119  ...  71.626943             70.892307\n-14  2048.0  73.584279  ...  78.033565             76.959706\n-15  2176.0  83.155572  ...  87.494120             86.367588\n-16  2304.0  68.446623  ...  78.064941             77.057651\n-17  2432.0  71.305746  ...  86.179335             85.393507\n-18  2560.0  77.833728  ...  82.956960             81.715711\n-19  2688.0  83.737433  ...  91.185232             89.464755\n-20  2816.0  82.446516  ...  84.523664             83.712490\n-21  2944.0  81.967162  ...  83.758038             82.373605\n-22  3072.0  82.420822  ...  88.750943             86.579673\n-23  3200.0  81.528664  ...  91.233074             95.665176\n-24  3328.0  83.516586  ...  85.908470             83.323259\n-25  3456.0  81.435930  ...  92.138932             90.180725\n-26  3584.0  83.954614  ...  91.189190             95.858629\n-27  3712.0  85.822459  ...  83.806497             87.783251\n-28  3840.0  80.901241  ...  89.259080             89.548180\n-29  3968.0  87.913500  ...  92.829164             84.096442\n-30  4096.0  93.825748  ...  89.299883             90.139506\n+13  1920.0  69.120002  ...  71.257735             71.257735\n+14  2048.0  73.584279  ...  78.398206             77.314362\n+15  2176.0  83.155572  ...  87.494120             85.998493\n+16  2304.0  68.446623  ...  78.320893             77.558029\n+17  2432.0  71.305746  ...  86.711310             75.421383\n+18  2560.0  77.833728  ...  82.747477             81.715711\n+19  2688.0  83.552988  ...  90.532356             89.464755\n+20  2816.0  84.197315  ...  84.035084             84.035084\n+21  2944.0  82.784108  ...  83.969728             83.060049\n+22  3072.0  81.825298  ...  89.593522             88.473602\n+23  3200.0  84.768213  ...  96.096095             95.808380\n+24  3328.0  83.226931  ...  85.908470             84.596116\n+25  3456.0  81.766291  ...  91.824110             91.097818\n+26  3584.0  87.466332  ...  91.194972             94.847460\n+27  3712.0  85.822459  ...  87.246590             87.860458\n+28  3840.0  81.859361  ...  87.011801             90.168771\n+29  3968.0  89.921841  ...  91.954739             85.271796\n+30  4096.0  93.596744  ...  88.243079             90.382307\n \n [31 rows x 5 columns]\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 5 minutes  52.578 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 7 minutes  13.827 seconds)</p>\n <div class=\"sphx-glr-footer class sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-03-matrix-multiplication-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d5fee5b55a64e47f1b5724ec39adf171/03-matrix-multiplication.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">03-matrix-multiplication.py</span></code></a></p>"}, {"filename": "master/getting-started/tutorials/04-low-memory-dropout.html", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -108,6 +108,8 @@\n </ul>\n </li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-libdevice-function.html\">Libdevice function</a></li>\n </ul>\n </li>\n </ul>\n@@ -372,7 +374,7 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n <dd><p>Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \u201cDropout: A Simple Way to Prevent Neural Networks from Overfitting\u201d, JMLR 2014</p>\n </dd>\n </dl>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.476 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.279 seconds)</p>\n <div class=\"sphx-glr-footer class sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-04-low-memory-dropout-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/c9aed78977a4c05741d675a38dde3d7d/04-low-memory-dropout.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">04-low-memory-dropout.py</span></code></a></p>"}, {"filename": "master/getting-started/tutorials/05-layer-norm.html", "status": "modified", "additions": 58, "deletions": 52, "changes": 110, "file_content_changes": "@@ -46,7 +46,7 @@\n     \n     <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n     <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton\" href=\"../../python-api/triton.html\" />\n+    <link rel=\"next\" title=\"Fused Attention\" href=\"06-fused-attention.html\" />\n     <link rel=\"prev\" title=\"Low-Memory Dropout\" href=\"04-low-memory-dropout.html\" /> \n </head>\n \n@@ -101,6 +101,8 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n <li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-libdevice-function.html\">Libdevice function</a></li>\n </ul>\n </li>\n </ul>\n@@ -196,34 +198,34 @@\n           N      Triton       Torch        Apex\n 0    1024.0  585.142849  277.694907  468.114273\n 1    1536.0  630.153868  323.368435  511.999982\n-2    2048.0  682.666643  334.367358  520.126988\n-3    2560.0  694.237267  365.714281  518.481028\n-4    3072.0  712.347810  378.092307  501.551037\n-5    3584.0  725.873439  384.859062  458.751978\n-6    4096.0  728.177767  381.023256  458.293714\n-7    4608.0  670.254540  396.387087  426.173427\n-8    5120.0  694.237267  397.669909  426.666652\n-9    5632.0  704.000002  396.969169  413.357796\n-10   6144.0  702.171410  402.885254  411.313806\n+2    2048.0  668.734716  337.814445  528.516136\n+3    2560.0  694.237267  362.477870  512.000013\n+4    3072.0  712.347810  375.206126  501.551037\n+5    3584.0  725.873439  384.859062  451.527536\n+6    4096.0  728.177767  381.023256  455.111095\n+7    4608.0  670.254540  396.387087  421.302872\n+8    5120.0  688.403381  395.748783  422.268057\n+9    5632.0  698.542675  396.969169  409.599997\n+10   6144.0  702.171410  402.885254  409.600010\n 11   6656.0  700.631610  400.360920  400.360920\n-12   7168.0  695.078767  396.844306  388.772874\n-13   7680.0  682.666656  393.846167  387.634072\n-14   8192.0  642.509816  393.609605  372.363633\n-15   8704.0  627.315309  389.005597  380.502740\n-16   9216.0  606.814809  407.337026  383.999986\n-17   9728.0  589.575753  409.599987  383.369452\n-18  10240.0  566.920437  408.578556  382.803739\n-19  10752.0  549.623009  411.559798  381.445676\n-20  11264.0  536.380957  406.826188  373.134567\n-21  11776.0  523.377770  410.492372  377.587162\n-22  12288.0  517.389457  414.784810  383.251457\n-23  12800.0  505.679014  410.420828  376.470582\n-24  13312.0  494.180982  405.699062  376.976995\n-25  13824.0  482.934503  411.888257  379.389355\n-26  14336.0  471.967074  406.695045  374.185964\n-27  14848.0  461.297068  408.192434  375.304904\n-28  15360.0  454.269882  406.214870  378.092307\n-29  15872.0  447.887117  407.627589  376.225175\n+12   7168.0  678.627194  386.154893  384.859062\n+13   7680.0  682.666656  391.337574  386.415087\n+14   8192.0  645.674867  390.095241  376.643677\n+15   8704.0  624.502255  390.095225  379.465939\n+16   9216.0  604.327881  405.098894  383.002605\n+17   9728.0  585.142883  409.599987  382.427505\n+18  10240.0  564.965524  409.600010  382.803739\n+19  10752.0  546.133312  410.577576  380.601764\n+20  11264.0  531.634232  395.228063  370.069806\n+21  11776.0  520.486200  409.599991  376.831982\n+22  12288.0  516.031509  413.911572  383.251457\n+23  12800.0  504.433489  410.420828  375.779805\n+24  13312.0  494.180982  405.699062  376.310952\n+25  13824.0  481.882350  411.888257  378.739711\n+26  14336.0  471.967074  401.709294  372.969090\n+27  14848.0  461.297068  407.492270  375.898745\n+28  15360.0  453.431739  406.887417  378.092307\n+29  15872.0  447.098578  406.323209  376.225175\n </pre></div>\n </div>\n <div class=\"line-block\">\n@@ -354,17 +356,19 @@\n     <span class=\"n\">cols</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_N</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)</span>\n     <span class=\"n\">dw</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n     <span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n-    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">):</span>\n-        <span class=\"n\">rows</span> <span class=\"o\">=</span> <span class=\"n\">i</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">)</span>\n-        <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">rows</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&lt;</span> <span class=\"n\">M</span><span class=\"p\">)</span> <span class=\"o\">&amp;</span> <span class=\"p\">(</span><span class=\"n\">cols</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n-        <span class=\"n\">offs</span> <span class=\"o\">=</span> <span class=\"n\">rows</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">N</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n-        <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">A</span> <span class=\"o\">+</span> <span class=\"n\">offs</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n-        <span class=\"n\">dout</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">DOut</span> <span class=\"o\">+</span> <span class=\"n\">offs</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n-        <span class=\"n\">mean</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">Mean</span> <span class=\"o\">+</span> <span class=\"n\">rows</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">rows</span> <span class=\"o\">&lt;</span> <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">)</span>\n-        <span class=\"n\">rstd</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">Var</span> <span class=\"o\">+</span> <span class=\"n\">rows</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">rows</span> <span class=\"o\">&lt;</span> <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">)</span>\n-        <span class=\"n\">a_hat</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">a</span> <span class=\"o\">-</span> <span class=\"n\">mean</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"n\">rstd</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n-        <span class=\"n\">dw</span> <span class=\"o\">+=</span> <span class=\"n\">dout</span> <span class=\"o\">*</span> <span class=\"n\">a_hat</span>\n-        <span class=\"n\">db</span> <span class=\"o\">+=</span> <span class=\"n\">dout</span>\n+    <span class=\"n\">UNROLL</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span> <span class=\"o\">=</span> <span class=\"mi\">4</span>\n+    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span> <span class=\"o\">*</span> <span class=\"n\">UNROLL</span><span class=\"p\">):</span>\n+        <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">UNROLL</span><span class=\"p\">):</span>\n+            <span class=\"n\">rows</span> <span class=\"o\">=</span> <span class=\"n\">i</span> <span class=\"o\">+</span> <span class=\"n\">j</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">)</span>\n+            <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">rows</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&lt;</span> <span class=\"n\">M</span><span class=\"p\">)</span> <span class=\"o\">&amp;</span> <span class=\"p\">(</span><span class=\"n\">cols</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n+            <span class=\"n\">offs</span> <span class=\"o\">=</span> <span class=\"n\">rows</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">N</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n+            <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">A</span> <span class=\"o\">+</span> <span class=\"n\">offs</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+            <span class=\"n\">dout</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">DOut</span> <span class=\"o\">+</span> <span class=\"n\">offs</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+            <span class=\"n\">mean</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">Mean</span> <span class=\"o\">+</span> <span class=\"n\">rows</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">rows</span> <span class=\"o\">&lt;</span> <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">)</span>\n+            <span class=\"n\">rstd</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">Var</span> <span class=\"o\">+</span> <span class=\"n\">rows</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">rows</span> <span class=\"o\">&lt;</span> <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">)</span>\n+            <span class=\"n\">a_hat</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">a</span> <span class=\"o\">-</span> <span class=\"n\">mean</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"n\">rstd</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n+            <span class=\"n\">dw</span> <span class=\"o\">+=</span> <span class=\"n\">dout</span> <span class=\"o\">*</span> <span class=\"n\">a_hat</span>\n+            <span class=\"n\">db</span> <span class=\"o\">+=</span> <span class=\"n\">dout</span>\n     <span class=\"n\">sum_dw</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">dw</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n     <span class=\"n\">sum_db</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">db</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n     <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">DW</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">sum_dw</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">cols</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n@@ -437,7 +441,15 @@\n             <span class=\"n\">BLOCK_SIZE_N</span><span class=\"o\">=</span><span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">BLOCK_SIZE</span><span class=\"p\">,</span>\n             <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">num_warps</span><span class=\"p\">,</span>\n         <span class=\"p\">)</span>\n-        <span class=\"c1\"># accumulate partial sums in separate kernel</span>\n+        <span class=\"k\">if</span> <span class=\"n\">N</span> <span class=\"o\">&gt;</span> <span class=\"mi\">10240</span><span class=\"p\">:</span>\n+            <span class=\"n\">BLOCK_SIZE_N</span> <span class=\"o\">=</span> <span class=\"mi\">128</span>\n+            <span class=\"n\">BLOCK_SIZE_M</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>\n+            <span class=\"n\">num_warps</span> <span class=\"o\">=</span> <span class=\"mi\">4</span>\n+        <span class=\"k\">else</span><span class=\"p\">:</span>\n+            <span class=\"c1\"># maximize occupancy for small N</span>\n+            <span class=\"n\">BLOCK_SIZE_N</span> <span class=\"o\">=</span> <span class=\"mi\">16</span>\n+            <span class=\"n\">BLOCK_SIZE_M</span> <span class=\"o\">=</span> <span class=\"mi\">16</span>\n+            <span class=\"n\">num_warps</span> <span class=\"o\">=</span> <span class=\"mi\">8</span>\n         <span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">meta</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">meta</span><span class=\"p\">[</span><span class=\"s2\">&quot;BLOCK_SIZE_N&quot;</span><span class=\"p\">])]</span>\n         <span class=\"n\">_layer_norm_bwd_dwdb</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span>\n             <span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">dout</span><span class=\"p\">,</span>\n@@ -446,17 +458,11 @@\n             <span class=\"n\">dbias</span><span class=\"p\">,</span>\n             <span class=\"n\">M</span><span class=\"p\">,</span>\n             <span class=\"n\">N</span><span class=\"p\">,</span>\n-            <span class=\"n\">BLOCK_SIZE_M</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">,</span>\n-            <span class=\"n\">BLOCK_SIZE_N</span><span class=\"o\">=</span><span class=\"mi\">128</span><span class=\"p\">,</span>\n+            <span class=\"n\">BLOCK_SIZE_M</span><span class=\"o\">=</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span>\n+            <span class=\"n\">BLOCK_SIZE_N</span><span class=\"o\">=</span><span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">,</span>\n+            <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"n\">num_warps</span>\n         <span class=\"p\">)</span>\n-        <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">da</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">dweight</span><span class=\"p\">,</span> <span class=\"n\">dbias</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n-                <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n-                <span class=\"kc\">None</span><span class=\"p\">,</span>\n-                <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n-                <span class=\"kc\">None</span><span class=\"p\">,</span>\n-                <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n-                <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n-                <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>\n+        <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">da</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">dweight</span><span class=\"p\">,</span> <span class=\"n\">dbias</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>\n \n \n <span class=\"k\">def</span> <span class=\"nf\">layer_norm</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">normalized_shape</span><span class=\"p\">,</span> <span class=\"n\">weight</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"p\">):</span>\n@@ -537,7 +543,7 @@\n <span class=\"n\">bench_layer_norm</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;.&#39;</span><span class=\"p\">,</span> <span class=\"n\">print_data</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 5 minutes  24.641 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 5 minutes  32.552 seconds)</p>\n <div class=\"sphx-glr-footer class sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-05-layer-norm-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/935c0dd0fbeb4b2e69588471cbb2d4b2/05-layer-norm.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">05-layer-norm.py</span></code></a></p>\n@@ -555,7 +561,7 @@\n           </div>\n           <footer>\n     <div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"footer navigation\">\n-        <a href=\"../../python-api/triton.html\" class=\"btn btn-neutral float-right\" title=\"triton\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+        <a href=\"06-fused-attention.html\" class=\"btn btn-neutral float-right\" title=\"Fused Attention\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n         <a href=\"04-low-memory-dropout.html\" class=\"btn btn-neutral float-left\" title=\"Low-Memory Dropout\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n     </div>\n "}, {"filename": "master/getting-started/tutorials/06-fused-attention.html", "status": "added", "additions": 623, "deletions": 0, "changes": 623, "file_content_changes": "@@ -0,0 +1,623 @@\n+\n+\n+<!DOCTYPE html>\n+<html class=\"writer-html5\" lang=\"en\" >\n+<head>\n+  <meta charset=\"utf-8\" />\n+  \n+  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n+  \n+  <title>Fused Attention &mdash; Triton  documentation</title>\n+  \n+\n+  \n+  <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n+  <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n+  <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n+  <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n+  <link rel=\"stylesheet\" href=\"../../_static/gallery.css\" type=\"text/css\" />\n+  <link rel=\"stylesheet\" href=\"../../_static/gallery-binder.css\" type=\"text/css\" />\n+  <link rel=\"stylesheet\" href=\"../../_static/gallery-dataframe.css\" type=\"text/css\" />\n+  <link rel=\"stylesheet\" href=\"../../_static/gallery-rendered-html.css\" type=\"text/css\" />\n+  <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n+\n+  \n+  \n+\n+  \n+  \n+\n+  \n+\n+  \n+  <!--[if lt IE 9]>\n+    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n+  <![endif]-->\n+  \n+    \n+      <script type=\"text/javascript\" id=\"documentation_options\" data-url_root=\"../../\" src=\"../../_static/documentation_options.js\"></script>\n+        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n+        <script src=\"../../_static/jquery.js\"></script>\n+        <script src=\"../../_static/underscore.js\"></script>\n+        <script src=\"../../_static/doctools.js\"></script>\n+    \n+    <script type=\"text/javascript\" src=\"../../_static/js/theme.js\"></script>\n+\n+    \n+    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n+    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n+    <link rel=\"next\" title=\"Libdevice function\" href=\"07-libdevice-function.html\" />\n+    <link rel=\"prev\" title=\"Layer Normalization\" href=\"05-layer-norm.html\" /> \n+</head>\n+\n+<body class=\"wy-body-for-nav\">\n+\n+   \n+  <div class=\"wy-grid-for-nav\">\n+    \n+    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n+      <div class=\"wy-side-scroll\">\n+        <div class=\"wy-side-nav-search\" >\n+          \n+\n+          \n+            <a href=\"../../index.html\" class=\"icon icon-home\"> Triton\n+          \n+\n+          \n+          </a>\n+\n+          \n+            \n+            \n+          \n+\n+          \n+<div role=\"search\">\n+  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n+    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" />\n+    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n+    <input type=\"hidden\" name=\"area\" value=\"default\" />\n+  </form>\n+</div>\n+\n+          \n+        </div>\n+\n+        \n+        <div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"main navigation\">\n+          \n+            \n+            \n+              \n+            \n+            \n+              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n+<ul class=\"current\">\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../installation.html\">Installation</a></li>\n+<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"index.html\">Tutorials</a><ul class=\"current\">\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"01-vector-add.html\">Vector Addition</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"02-fused-softmax.html\">Fused Softmax</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Fused Attention</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-libdevice-function.html\">Libdevice function</a></li>\n+</ul>\n+</li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n+</ul>\n+\n+            \n+          \n+        </div>\n+        \n+      </div>\n+    </nav>\n+\n+    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\">\n+\n+      \n+      <nav class=\"wy-nav-top\" aria-label=\"top navigation\">\n+        \n+          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n+          <a href=\"../../index.html\">Triton</a>\n+        \n+      </nav>\n+\n+\n+      <div class=\"wy-nav-content\">\n+        \n+        <div class=\"rst-content\">\n+        \n+          \n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+<div role=\"navigation\" aria-label=\"breadcrumbs navigation\">\n+\n+  <ul class=\"wy-breadcrumbs\">\n+    \n+      <li><a href=\"../../index.html\" class=\"icon icon-home\"></a> &raquo;</li>\n+        \n+          <li><a href=\"index.html\">Tutorials</a> &raquo;</li>\n+        \n+      <li>Fused Attention</li>\n+    \n+    \n+      <li class=\"wy-breadcrumbs-aside\">\n+        \n+          \n+            <a href=\"../../_sources/getting-started/tutorials/06-fused-attention.rst.txt\" rel=\"nofollow\"> View page source</a>\n+          \n+        \n+      </li>\n+    \n+  </ul>\n+\n+  \n+  <hr/>\n+</div>\n+          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n+           <div itemprop=\"articleBody\">\n+            \n+  <div class=\"sphx-glr-download-link-note admonition note\">\n+<p class=\"admonition-title\">Note</p>\n+<p>Click <a class=\"reference internal\" href=\"#sphx-glr-download-getting-started-tutorials-06-fused-attention-py\"><span class=\"std std-ref\">here</span></a>\n+to download the full example code</p>\n+</div>\n+<div class=\"sphx-glr-example-title section\" id=\"fused-attention\">\n+<span id=\"sphx-glr-getting-started-tutorials-06-fused-attention-py\"></span><h1>Fused Attention<a class=\"headerlink\" href=\"#fused-attention\" title=\"Permalink to this headline\">\u00b6</a></h1>\n+<p>This is a Triton implementation of the Flash Attention algorithm\n+(see: Dao et al., <a class=\"reference external\" href=\"https://arxiv.org/pdf/2205.14135v2.pdf\">https://arxiv.org/pdf/2205.14135v2.pdf</a>; Rabe and Staats <a class=\"reference external\" href=\"https://arxiv.org/pdf/2112.05682v2.pdf\">https://arxiv.org/pdf/2112.05682v2.pdf</a>)</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">pytest</span>\n+<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n+\n+<span class=\"kn\">import</span> <span class=\"nn\">triton</span>\n+<span class=\"kn\">import</span> <span class=\"nn\">triton.language</span> <span class=\"k\">as</span> <span class=\"nn\">tl</span>\n+\n+\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n+<span class=\"k\">def</span> <span class=\"nf\">_fwd_kernel</span><span class=\"p\">(</span>\n+    <span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"n\">sm_scale</span><span class=\"p\">,</span>\n+    <span class=\"n\">TMP</span><span class=\"p\">,</span> <span class=\"n\">L</span><span class=\"p\">,</span> <span class=\"n\">M</span><span class=\"p\">,</span>  <span class=\"c1\"># NOTE: TMP is a scratchpad buffer to workaround a compiler bug</span>\n+    <span class=\"n\">Out</span><span class=\"p\">,</span>\n+    <span class=\"n\">stride_qz</span><span class=\"p\">,</span> <span class=\"n\">stride_qh</span><span class=\"p\">,</span> <span class=\"n\">stride_qm</span><span class=\"p\">,</span> <span class=\"n\">stride_qk</span><span class=\"p\">,</span>\n+    <span class=\"n\">stride_kz</span><span class=\"p\">,</span> <span class=\"n\">stride_kh</span><span class=\"p\">,</span> <span class=\"n\">stride_kn</span><span class=\"p\">,</span> <span class=\"n\">stride_kk</span><span class=\"p\">,</span>\n+    <span class=\"n\">stride_vz</span><span class=\"p\">,</span> <span class=\"n\">stride_vh</span><span class=\"p\">,</span> <span class=\"n\">stride_vk</span><span class=\"p\">,</span> <span class=\"n\">stride_vn</span><span class=\"p\">,</span>\n+    <span class=\"n\">stride_oz</span><span class=\"p\">,</span> <span class=\"n\">stride_oh</span><span class=\"p\">,</span> <span class=\"n\">stride_om</span><span class=\"p\">,</span> <span class=\"n\">stride_on</span><span class=\"p\">,</span>\n+    <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span>\n+    <span class=\"n\">BLOCK_M</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+    <span class=\"n\">BLOCK_N</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+<span class=\"p\">):</span>\n+    <span class=\"n\">start_m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"n\">off_hz</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># initialize offsets</span>\n+    <span class=\"n\">offs_m</span> <span class=\"o\">=</span> <span class=\"n\">start_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">)</span>\n+    <span class=\"n\">offs_n</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"p\">)</span>\n+    <span class=\"n\">offs_d</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">)</span>\n+    <span class=\"n\">off_q</span> <span class=\"o\">=</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span> <span class=\"o\">+</span> <span class=\"n\">offs_m</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qm</span> <span class=\"o\">+</span> <span class=\"n\">offs_d</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qk</span>\n+    <span class=\"n\">off_k</span> <span class=\"o\">=</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span> <span class=\"o\">+</span> <span class=\"n\">offs_n</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_kn</span> <span class=\"o\">+</span> <span class=\"n\">offs_d</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_kk</span>\n+    <span class=\"n\">off_v</span> <span class=\"o\">=</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span> <span class=\"o\">+</span> <span class=\"n\">offs_n</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qm</span> <span class=\"o\">+</span> <span class=\"n\">offs_d</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qk</span>\n+    <span class=\"c1\"># Initialize pointers to Q, K, V</span>\n+    <span class=\"n\">q_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">Q</span> <span class=\"o\">+</span> <span class=\"n\">off_q</span>\n+    <span class=\"n\">k_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">K</span> <span class=\"o\">+</span> <span class=\"n\">off_k</span>\n+    <span class=\"n\">v_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">V</span> <span class=\"o\">+</span> <span class=\"n\">off_v</span>\n+    <span class=\"c1\"># initialize pointer to m and l</span>\n+    <span class=\"n\">t_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">TMP</span> <span class=\"o\">+</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">N_CTX</span> <span class=\"o\">+</span> <span class=\"n\">offs_m</span>\n+    <span class=\"n\">m_i</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_M</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s2\">&quot;inf&quot;</span><span class=\"p\">)</span>\n+    <span class=\"n\">l_i</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_M</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"n\">acc</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># load q: it will stay in SRAM throughout</span>\n+    <span class=\"n\">q</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">q_ptrs</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># loop over k, v and update accumulator</span>\n+    <span class=\"k\">for</span> <span class=\"n\">start_n</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">start_m</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"p\">):</span>\n+        <span class=\"n\">start_n</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">multiple_of</span><span class=\"p\">(</span><span class=\"n\">start_n</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># -- compute qk ----</span>\n+        <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">k_ptrs</span> <span class=\"o\">+</span> <span class=\"n\">start_n</span> <span class=\"o\">*</span> <span class=\"n\">stride_kn</span><span class=\"p\">)</span>\n+        <span class=\"n\">qk</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+        <span class=\"n\">qk</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">trans_b</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+        <span class=\"n\">qk</span> <span class=\"o\">*=</span> <span class=\"n\">sm_scale</span>\n+        <span class=\"n\">qk</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">offs_m</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&gt;=</span> <span class=\"p\">(</span><span class=\"n\">start_n</span> <span class=\"o\">+</span> <span class=\"n\">offs_n</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]),</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s2\">&quot;-inf&quot;</span><span class=\"p\">))</span>\n+        <span class=\"c1\"># -- compute m_ij, p, l_ij</span>\n+        <span class=\"n\">m_ij</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">qk</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n+        <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">qk</span> <span class=\"o\">-</span> <span class=\"n\">m_ij</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">])</span>\n+        <span class=\"n\">l_ij</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># -- update m_i and l_i</span>\n+        <span class=\"n\">m_i_new</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">maximum</span><span class=\"p\">(</span><span class=\"n\">m_i</span><span class=\"p\">,</span> <span class=\"n\">m_ij</span><span class=\"p\">)</span>\n+        <span class=\"n\">alpha</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">m_i</span> <span class=\"o\">-</span> <span class=\"n\">m_i_new</span><span class=\"p\">)</span>\n+        <span class=\"n\">beta</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">m_ij</span> <span class=\"o\">-</span> <span class=\"n\">m_i_new</span><span class=\"p\">)</span>\n+        <span class=\"n\">l_i_new</span> <span class=\"o\">=</span> <span class=\"n\">alpha</span> <span class=\"o\">*</span> <span class=\"n\">l_i</span> <span class=\"o\">+</span> <span class=\"n\">beta</span> <span class=\"o\">*</span> <span class=\"n\">l_ij</span>\n+        <span class=\"c1\"># -- update output accumulator --</span>\n+        <span class=\"c1\"># scale p</span>\n+        <span class=\"n\">p_scale</span> <span class=\"o\">=</span> <span class=\"n\">beta</span> <span class=\"o\">/</span> <span class=\"n\">l_i_new</span>\n+        <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">p</span> <span class=\"o\">*</span> <span class=\"n\">p_scale</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n+        <span class=\"c1\"># scale acc</span>\n+        <span class=\"n\">acc_scale</span> <span class=\"o\">=</span> <span class=\"n\">l_i</span> <span class=\"o\">/</span> <span class=\"n\">l_i_new</span> <span class=\"o\">*</span> <span class=\"n\">alpha</span>\n+        <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">t_ptrs</span><span class=\"p\">,</span> <span class=\"n\">acc_scale</span><span class=\"p\">)</span>\n+        <span class=\"n\">acc_scale</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">t_ptrs</span><span class=\"p\">)</span>  <span class=\"c1\"># BUG: have to store and immediately load</span>\n+        <span class=\"n\">acc</span> <span class=\"o\">=</span> <span class=\"n\">acc</span> <span class=\"o\">*</span> <span class=\"n\">acc_scale</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n+        <span class=\"c1\"># update acc</span>\n+        <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">v_ptrs</span> <span class=\"o\">+</span> <span class=\"n\">start_n</span> <span class=\"o\">*</span> <span class=\"n\">stride_vk</span><span class=\"p\">)</span>\n+        <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n+        <span class=\"n\">acc</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># update m_i and l_i</span>\n+        <span class=\"n\">l_i</span> <span class=\"o\">=</span> <span class=\"n\">l_i_new</span>\n+        <span class=\"n\">m_i</span> <span class=\"o\">=</span> <span class=\"n\">m_i_new</span>\n+    <span class=\"c1\"># rematerialize offsets to save registers</span>\n+    <span class=\"n\">start_m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"n\">offs_m</span> <span class=\"o\">=</span> <span class=\"n\">start_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># write back l and m</span>\n+    <span class=\"n\">l_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">L</span> <span class=\"o\">+</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">N_CTX</span> <span class=\"o\">+</span> <span class=\"n\">offs_m</span>\n+    <span class=\"n\">m_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">M</span> <span class=\"o\">+</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">N_CTX</span> <span class=\"o\">+</span> <span class=\"n\">offs_m</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">l_ptrs</span><span class=\"p\">,</span> <span class=\"n\">l_i</span><span class=\"p\">)</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">m_ptrs</span><span class=\"p\">,</span> <span class=\"n\">m_i</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># initialize pointers to output</span>\n+    <span class=\"n\">offs_n</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">)</span>\n+    <span class=\"n\">off_o</span> <span class=\"o\">=</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">stride_oh</span> <span class=\"o\">+</span> <span class=\"n\">offs_m</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_om</span> <span class=\"o\">+</span> <span class=\"n\">offs_n</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_on</span>\n+    <span class=\"n\">out_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">Out</span> <span class=\"o\">+</span> <span class=\"n\">off_o</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">out_ptrs</span><span class=\"p\">,</span> <span class=\"n\">acc</span><span class=\"p\">)</span>\n+\n+\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n+<span class=\"k\">def</span> <span class=\"nf\">_bwd_preprocess</span><span class=\"p\">(</span>\n+    <span class=\"n\">Out</span><span class=\"p\">,</span> <span class=\"n\">DO</span><span class=\"p\">,</span> <span class=\"n\">L</span><span class=\"p\">,</span>\n+    <span class=\"n\">NewDO</span><span class=\"p\">,</span> <span class=\"n\">Delta</span><span class=\"p\">,</span>\n+    <span class=\"n\">BLOCK_M</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+<span class=\"p\">):</span>\n+    <span class=\"n\">off_m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">)</span>\n+    <span class=\"n\">off_n</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># load</span>\n+    <span class=\"n\">o</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">Out</span> <span class=\"o\">+</span> <span class=\"n\">off_m</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">D_HEAD</span> <span class=\"o\">+</span> <span class=\"n\">off_n</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:])</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"n\">do</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">DO</span> <span class=\"o\">+</span> <span class=\"n\">off_m</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">D_HEAD</span> <span class=\"o\">+</span> <span class=\"n\">off_n</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:])</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"n\">denom</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">L</span> <span class=\"o\">+</span> <span class=\"n\">off_m</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># compute</span>\n+    <span class=\"n\">do</span> <span class=\"o\">=</span> <span class=\"n\">do</span> <span class=\"o\">/</span> <span class=\"n\">denom</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n+    <span class=\"n\">delta</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">o</span> <span class=\"o\">*</span> <span class=\"n\">do</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># write-back</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">NewDO</span> <span class=\"o\">+</span> <span class=\"n\">off_m</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">D_HEAD</span> <span class=\"o\">+</span> <span class=\"n\">off_n</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:],</span> <span class=\"n\">do</span><span class=\"p\">)</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">Delta</span> <span class=\"o\">+</span> <span class=\"n\">off_m</span><span class=\"p\">,</span> <span class=\"n\">delta</span><span class=\"p\">)</span>\n+\n+\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n+<span class=\"k\">def</span> <span class=\"nf\">_bwd_kernel</span><span class=\"p\">(</span>\n+    <span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"n\">sm_scale</span><span class=\"p\">,</span> <span class=\"n\">Out</span><span class=\"p\">,</span> <span class=\"n\">DO</span><span class=\"p\">,</span>\n+    <span class=\"n\">DQ</span><span class=\"p\">,</span> <span class=\"n\">DK</span><span class=\"p\">,</span> <span class=\"n\">DV</span><span class=\"p\">,</span>\n+    <span class=\"n\">L</span><span class=\"p\">,</span> <span class=\"n\">M</span><span class=\"p\">,</span>\n+    <span class=\"n\">D</span><span class=\"p\">,</span>\n+    <span class=\"n\">stride_qz</span><span class=\"p\">,</span> <span class=\"n\">stride_qh</span><span class=\"p\">,</span> <span class=\"n\">stride_qm</span><span class=\"p\">,</span> <span class=\"n\">stride_qk</span><span class=\"p\">,</span>\n+    <span class=\"n\">stride_kz</span><span class=\"p\">,</span> <span class=\"n\">stride_kh</span><span class=\"p\">,</span> <span class=\"n\">stride_kn</span><span class=\"p\">,</span> <span class=\"n\">stride_kk</span><span class=\"p\">,</span>\n+    <span class=\"n\">stride_vz</span><span class=\"p\">,</span> <span class=\"n\">stride_vh</span><span class=\"p\">,</span> <span class=\"n\">stride_vk</span><span class=\"p\">,</span> <span class=\"n\">stride_vn</span><span class=\"p\">,</span>\n+    <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span>\n+    <span class=\"n\">num_block</span><span class=\"p\">,</span>\n+    <span class=\"n\">BLOCK_M</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+    <span class=\"n\">BLOCK_N</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+<span class=\"p\">):</span>\n+    <span class=\"n\">off_hz</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"n\">off_z</span> <span class=\"o\">=</span> <span class=\"n\">off_hz</span> <span class=\"o\">//</span> <span class=\"n\">H</span>\n+    <span class=\"n\">off_h</span> <span class=\"o\">=</span> <span class=\"n\">off_hz</span> <span class=\"o\">%</span> <span class=\"n\">H</span>\n+    <span class=\"c1\"># offset pointers for batch/head</span>\n+    <span class=\"n\">Q</span> <span class=\"o\">+=</span> <span class=\"n\">off_z</span> <span class=\"o\">*</span> <span class=\"n\">stride_qz</span> <span class=\"o\">+</span> <span class=\"n\">off_h</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span>\n+    <span class=\"n\">K</span> <span class=\"o\">+=</span> <span class=\"n\">off_z</span> <span class=\"o\">*</span> <span class=\"n\">stride_qz</span> <span class=\"o\">+</span> <span class=\"n\">off_h</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span>\n+    <span class=\"n\">V</span> <span class=\"o\">+=</span> <span class=\"n\">off_z</span> <span class=\"o\">*</span> <span class=\"n\">stride_qz</span> <span class=\"o\">+</span> <span class=\"n\">off_h</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span>\n+    <span class=\"n\">DO</span> <span class=\"o\">+=</span> <span class=\"n\">off_z</span> <span class=\"o\">*</span> <span class=\"n\">stride_qz</span> <span class=\"o\">+</span> <span class=\"n\">off_h</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span>\n+    <span class=\"n\">DQ</span> <span class=\"o\">+=</span> <span class=\"n\">off_z</span> <span class=\"o\">*</span> <span class=\"n\">stride_qz</span> <span class=\"o\">+</span> <span class=\"n\">off_h</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span>\n+    <span class=\"n\">DK</span> <span class=\"o\">+=</span> <span class=\"n\">off_z</span> <span class=\"o\">*</span> <span class=\"n\">stride_qz</span> <span class=\"o\">+</span> <span class=\"n\">off_h</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span>\n+    <span class=\"n\">DV</span> <span class=\"o\">+=</span> <span class=\"n\">off_z</span> <span class=\"o\">*</span> <span class=\"n\">stride_qz</span> <span class=\"o\">+</span> <span class=\"n\">off_h</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span>\n+    <span class=\"k\">for</span> <span class=\"n\">start_n</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">num_block</span><span class=\"p\">):</span>\n+        <span class=\"n\">lo</span> <span class=\"o\">=</span> <span class=\"n\">start_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span>\n+        <span class=\"c1\"># initialize row/col offsets</span>\n+        <span class=\"n\">offs_qm</span> <span class=\"o\">=</span> <span class=\"n\">lo</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">)</span>\n+        <span class=\"n\">offs_n</span> <span class=\"o\">=</span> <span class=\"n\">start_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">)</span>\n+        <span class=\"n\">offs_m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"p\">)</span>\n+        <span class=\"n\">offs_k</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># initialize pointers to value-like data</span>\n+        <span class=\"n\">q_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">Q</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_qm</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qm</span> <span class=\"o\">+</span> <span class=\"n\">offs_k</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qk</span><span class=\"p\">)</span>\n+        <span class=\"n\">k_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">K</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_n</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_kn</span> <span class=\"o\">+</span> <span class=\"n\">offs_k</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_kk</span><span class=\"p\">)</span>\n+        <span class=\"n\">v_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">V</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_n</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qm</span> <span class=\"o\">+</span> <span class=\"n\">offs_k</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qk</span><span class=\"p\">)</span>\n+        <span class=\"n\">do_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">DO</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_qm</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qm</span> <span class=\"o\">+</span> <span class=\"n\">offs_k</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qk</span><span class=\"p\">)</span>\n+        <span class=\"n\">dq_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">DQ</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_qm</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qm</span> <span class=\"o\">+</span> <span class=\"n\">offs_k</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qk</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># pointer to row-wise quantities in value-like data</span>\n+        <span class=\"n\">D_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">D</span> <span class=\"o\">+</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">N_CTX</span>\n+        <span class=\"n\">m_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">M</span> <span class=\"o\">+</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">N_CTX</span>\n+        <span class=\"c1\"># initialize dv amd dk</span>\n+        <span class=\"n\">dv</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+        <span class=\"n\">dk</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># k and v stay in SRAM throughout</span>\n+        <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">k_ptrs</span><span class=\"p\">)</span>\n+        <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">v_ptrs</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># loop over rows</span>\n+        <span class=\"k\">for</span> <span class=\"n\">start_m</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">lo</span><span class=\"p\">,</span> <span class=\"n\">num_block</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">):</span>\n+            <span class=\"n\">offs_m_curr</span> <span class=\"o\">=</span> <span class=\"n\">start_m</span> <span class=\"o\">+</span> <span class=\"n\">offs_m</span>\n+            <span class=\"c1\"># load q, k, v, do on-chip</span>\n+            <span class=\"n\">q</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">q_ptrs</span><span class=\"p\">)</span>\n+            <span class=\"c1\"># recompute p = softmax(qk, dim=-1).T</span>\n+            <span class=\"c1\"># NOTE: `do` is pre-divided by `l`; no normalization here</span>\n+            <span class=\"n\">qk</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">trans_b</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+            <span class=\"n\">qk</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">offs_m_curr</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&gt;=</span> <span class=\"p\">(</span><span class=\"n\">offs_n</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]),</span> <span class=\"n\">qk</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s2\">&quot;-inf&quot;</span><span class=\"p\">))</span>\n+            <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">m_ptrs</span> <span class=\"o\">+</span> <span class=\"n\">offs_m_curr</span><span class=\"p\">)</span>\n+            <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">qk</span> <span class=\"o\">*</span> <span class=\"n\">sm_scale</span> <span class=\"o\">-</span> <span class=\"n\">m</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">])</span>\n+            <span class=\"c1\"># compute dv</span>\n+            <span class=\"n\">do</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">do_ptrs</span><span class=\"p\">)</span>\n+            <span class=\"n\">dv</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">),</span> <span class=\"n\">do</span><span class=\"p\">,</span> <span class=\"n\">trans_a</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+            <span class=\"c1\"># compute dp = dot(v, do)</span>\n+            <span class=\"n\">Di</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">D_ptrs</span> <span class=\"o\">+</span> <span class=\"n\">offs_m_curr</span><span class=\"p\">)</span>\n+            <span class=\"n\">dp</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">Di</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n+            <span class=\"n\">dp</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">do</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">trans_b</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+            <span class=\"c1\"># compute ds = p * (dp - delta[:, None])</span>\n+            <span class=\"n\">ds</span> <span class=\"o\">=</span> <span class=\"n\">p</span> <span class=\"o\">*</span> <span class=\"n\">dp</span> <span class=\"o\">*</span> <span class=\"n\">sm_scale</span>\n+            <span class=\"c1\"># compute dk = dot(ds.T, q)</span>\n+            <span class=\"n\">dk</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">trans_a</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+            <span class=\"c1\"># # compute dq</span>\n+            <span class=\"n\">dq</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">dq_ptrs</span><span class=\"p\">,</span> <span class=\"n\">eviction_policy</span><span class=\"o\">=</span><span class=\"s2\">&quot;evict_last&quot;</span><span class=\"p\">)</span>\n+            <span class=\"n\">dq</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">),</span> <span class=\"n\">k</span><span class=\"p\">)</span>\n+            <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">dq_ptrs</span><span class=\"p\">,</span> <span class=\"n\">dq</span><span class=\"p\">,</span> <span class=\"n\">eviction_policy</span><span class=\"o\">=</span><span class=\"s2\">&quot;evict_last&quot;</span><span class=\"p\">)</span>\n+            <span class=\"c1\"># # increment pointers</span>\n+            <span class=\"n\">dq_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_M</span> <span class=\"o\">*</span> <span class=\"n\">stride_qm</span>\n+            <span class=\"n\">q_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_M</span> <span class=\"o\">*</span> <span class=\"n\">stride_qm</span>\n+            <span class=\"n\">do_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_M</span> <span class=\"o\">*</span> <span class=\"n\">stride_qm</span>\n+        <span class=\"c1\"># write-back</span>\n+        <span class=\"n\">dv_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">DV</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_n</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qm</span> <span class=\"o\">+</span> <span class=\"n\">offs_k</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qk</span><span class=\"p\">)</span>\n+        <span class=\"n\">dk_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">DK</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_n</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_kn</span> <span class=\"o\">+</span> <span class=\"n\">offs_k</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_kk</span><span class=\"p\">)</span>\n+        <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">dv_ptrs</span><span class=\"p\">,</span> <span class=\"n\">dv</span><span class=\"p\">)</span>\n+        <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">dk_ptrs</span><span class=\"p\">,</span> <span class=\"n\">dk</span><span class=\"p\">)</span>\n+\n+\n+<span class=\"k\">class</span> <span class=\"nc\">_attention</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">autograd</span><span class=\"o\">.</span><span class=\"n\">Function</span><span class=\"p\">):</span>\n+\n+    <span class=\"nd\">@staticmethod</span>\n+    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">,</span> <span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">sm_scale</span><span class=\"p\">):</span>\n+        <span class=\"n\">BLOCK</span> <span class=\"o\">=</span> <span class=\"mi\">128</span>\n+        <span class=\"c1\"># shape constraints</span>\n+        <span class=\"n\">Lq</span><span class=\"p\">,</span> <span class=\"n\">Lk</span> <span class=\"o\">=</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n+        <span class=\"k\">assert</span> <span class=\"n\">Lq</span> <span class=\"o\">==</span> <span class=\"n\">Lk</span>\n+        <span class=\"n\">o</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">)</span>\n+        <span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"n\">BLOCK</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n+        <span class=\"n\">tmp</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+        <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+        <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+        <span class=\"n\">_fwd_kernel</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span>\n+            <span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">sm_scale</span><span class=\"p\">,</span>\n+            <span class=\"n\">tmp</span><span class=\"p\">,</span> <span class=\"n\">L</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">,</span>\n+            <span class=\"n\">o</span><span class=\"p\">,</span>\n+            <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n+            <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n+            <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n+            <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n+            <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">],</span>\n+            <span class=\"n\">BLOCK_M</span><span class=\"o\">=</span><span class=\"n\">BLOCK</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"o\">=</span><span class=\"n\">BLOCK</span><span class=\"p\">,</span>\n+            <span class=\"n\">BLOCK_DMODEL</span><span class=\"o\">=</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span>\n+            <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n+        <span class=\"p\">)</span>\n+        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">save_for_backward</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">o</span><span class=\"p\">,</span> <span class=\"n\">L</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">)</span>\n+        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">BLOCK</span> <span class=\"o\">=</span> <span class=\"n\">BLOCK</span>\n+        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"n\">grid</span>\n+        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">sm_scale</span> <span class=\"o\">=</span> <span class=\"n\">sm_scale</span>\n+        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">BLOCK_DMODEL</span> <span class=\"o\">=</span> <span class=\"mi\">64</span>\n+        <span class=\"k\">return</span> <span class=\"n\">o</span>\n+\n+    <span class=\"nd\">@staticmethod</span>\n+    <span class=\"k\">def</span> <span class=\"nf\">backward</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">,</span> <span class=\"n\">do</span><span class=\"p\">):</span>\n+        <span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">o</span><span class=\"p\">,</span> <span class=\"n\">l</span><span class=\"p\">,</span> <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">saved_tensors</span>\n+        <span class=\"n\">do</span> <span class=\"o\">=</span> <span class=\"n\">do</span><span class=\"o\">.</span><span class=\"n\">contiguous</span><span class=\"p\">()</span>\n+        <span class=\"n\">dq</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros_like</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+        <span class=\"n\">dk</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">)</span>\n+        <span class=\"n\">dv</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">)</span>\n+        <span class=\"n\">do_scaled</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">do</span><span class=\"p\">)</span>\n+        <span class=\"n\">delta</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)</span>\n+        <span class=\"n\">_bwd_preprocess</span><span class=\"p\">[(</span><span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">)](</span>\n+            <span class=\"n\">o</span><span class=\"p\">,</span> <span class=\"n\">do</span><span class=\"p\">,</span> <span class=\"n\">l</span><span class=\"p\">,</span>\n+            <span class=\"n\">do_scaled</span><span class=\"p\">,</span> <span class=\"n\">delta</span><span class=\"p\">,</span>\n+            <span class=\"n\">BLOCK_M</span><span class=\"o\">=</span><span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">BLOCK</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"o\">=</span><span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">,</span>\n+        <span class=\"p\">)</span>\n+        <span class=\"n\">_bwd_kernel</span><span class=\"p\">[(</span><span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],)](</span>\n+            <span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">sm_scale</span><span class=\"p\">,</span>\n+            <span class=\"n\">o</span><span class=\"p\">,</span> <span class=\"n\">do_scaled</span><span class=\"p\">,</span>\n+            <span class=\"n\">dq</span><span class=\"p\">,</span> <span class=\"n\">dk</span><span class=\"p\">,</span> <span class=\"n\">dv</span><span class=\"p\">,</span>\n+            <span class=\"n\">l</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">,</span>\n+            <span class=\"n\">delta</span><span class=\"p\">,</span>\n+            <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n+            <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n+            <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n+            <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">],</span>\n+            <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span>\n+            <span class=\"n\">BLOCK_M</span><span class=\"o\">=</span><span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">BLOCK</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"o\">=</span><span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">BLOCK</span><span class=\"p\">,</span>\n+            <span class=\"n\">BLOCK_DMODEL</span><span class=\"o\">=</span><span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">,</span>\n+            <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n+        <span class=\"p\">)</span>\n+        <span class=\"k\">return</span> <span class=\"n\">dq</span><span class=\"p\">,</span> <span class=\"n\">dk</span><span class=\"p\">,</span> <span class=\"n\">dv</span><span class=\"p\">,</span> <span class=\"kc\">None</span>\n+\n+\n+<span class=\"n\">attention</span> <span class=\"o\">=</span> <span class=\"n\">_attention</span><span class=\"o\">.</span><span class=\"n\">apply</span>\n+\n+\n+<span class=\"nd\">@pytest</span><span class=\"o\">.</span><span class=\"n\">mark</span><span class=\"o\">.</span><span class=\"n\">parametrize</span><span class=\"p\">(</span><span class=\"s1\">&#39;Z, H, N_CTX, D_HEAD&#39;</span><span class=\"p\">,</span> <span class=\"p\">[(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2048</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">)])</span>\n+<span class=\"k\">def</span> <span class=\"nf\">test_op</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">):</span>\n+    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">)</span>\n+    <span class=\"n\">q</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">normal_</span><span class=\"p\">(</span><span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">std</span><span class=\"o\">=</span><span class=\"mf\">.5</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">requires_grad_</span><span class=\"p\">()</span>\n+    <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">normal_</span><span class=\"p\">(</span><span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">std</span><span class=\"o\">=</span><span class=\"mf\">.5</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">requires_grad_</span><span class=\"p\">()</span>\n+    <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">normal_</span><span class=\"p\">(</span><span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">std</span><span class=\"o\">=</span><span class=\"mf\">.5</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">requires_grad_</span><span class=\"p\">()</span>\n+    <span class=\"n\">sm_scale</span> <span class=\"o\">=</span> <span class=\"mf\">0.3</span>\n+    <span class=\"n\">dout</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn_like</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># reference implementation</span>\n+    <span class=\"n\">M</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tril</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">))</span>\n+    <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">))</span> <span class=\"o\">*</span> <span class=\"n\">sm_scale</span>\n+    <span class=\"k\">for</span> <span class=\"n\">z</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"p\">):</span>\n+        <span class=\"k\">for</span> <span class=\"n\">h</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">H</span><span class=\"p\">):</span>\n+            <span class=\"n\">p</span><span class=\"p\">[:,</span> <span class=\"p\">:,</span> <span class=\"n\">M</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s2\">&quot;-inf&quot;</span><span class=\"p\">)</span>\n+    <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">float</span><span class=\"p\">(),</span> <span class=\"n\">dim</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">half</span><span class=\"p\">()</span>\n+    <span class=\"n\">ref_out</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">)</span>\n+    <span class=\"n\">ref_out</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">(</span><span class=\"n\">dout</span><span class=\"p\">)</span>\n+    <span class=\"n\">ref_dv</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">clone</span><span class=\"p\">(),</span> <span class=\"kc\">None</span>\n+    <span class=\"n\">ref_dk</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">clone</span><span class=\"p\">(),</span> <span class=\"kc\">None</span>\n+    <span class=\"n\">ref_dq</span><span class=\"p\">,</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">clone</span><span class=\"p\">(),</span> <span class=\"kc\">None</span>\n+    <span class=\"c1\"># triton implementation</span>\n+    <span class=\"n\">tri_out</span> <span class=\"o\">=</span> <span class=\"n\">attention</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">sm_scale</span><span class=\"p\">)</span>\n+    <span class=\"n\">tri_out</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">(</span><span class=\"n\">dout</span><span class=\"p\">)</span>\n+    <span class=\"n\">tri_dv</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">clone</span><span class=\"p\">(),</span> <span class=\"kc\">None</span>\n+    <span class=\"n\">tri_dk</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">clone</span><span class=\"p\">(),</span> <span class=\"kc\">None</span>\n+    <span class=\"n\">tri_dq</span><span class=\"p\">,</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">clone</span><span class=\"p\">(),</span> <span class=\"kc\">None</span>\n+    <span class=\"c1\"># compare</span>\n+    <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">assert_almost_equal</span><span class=\"p\">(</span><span class=\"n\">ref_out</span><span class=\"p\">,</span> <span class=\"n\">tri_out</span><span class=\"p\">)</span>\n+    <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">assert_almost_equal</span><span class=\"p\">(</span><span class=\"n\">ref_dv</span><span class=\"p\">,</span> <span class=\"n\">tri_dv</span><span class=\"p\">)</span>\n+    <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">assert_almost_equal</span><span class=\"p\">(</span><span class=\"n\">ref_dk</span><span class=\"p\">,</span> <span class=\"n\">tri_dk</span><span class=\"p\">)</span>\n+    <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">assert_almost_equal</span><span class=\"p\">(</span><span class=\"n\">ref_dq</span><span class=\"p\">,</span> <span class=\"n\">tri_dq</span><span class=\"p\">)</span>\n+\n+\n+<span class=\"k\">try</span><span class=\"p\">:</span>\n+    <span class=\"kn\">from</span> <span class=\"nn\">flash_attn.flash_attn_interface</span> <span class=\"kn\">import</span> <span class=\"n\">flash_attn_func</span>\n+    <span class=\"n\">HAS_FLASH</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>\n+<span class=\"k\">except</span> <span class=\"ne\">BaseException</span><span class=\"p\">:</span>\n+    <span class=\"n\">HAS_FLASH</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n+\n+<span class=\"n\">BATCH</span><span class=\"p\">,</span> <span class=\"n\">N_HEADS</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span> <span class=\"o\">=</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">48</span><span class=\"p\">,</span> <span class=\"mi\">4096</span><span class=\"p\">,</span> <span class=\"mi\">64</span>\n+<span class=\"c1\"># vary seq length for fixed head and batch=4</span>\n+<span class=\"n\">configs</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">Benchmark</span><span class=\"p\">(</span>\n+    <span class=\"n\">x_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;N_CTX&#39;</span><span class=\"p\">],</span>\n+    <span class=\"n\">x_vals</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"o\">**</span><span class=\"n\">i</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">16</span><span class=\"p\">)],</span>\n+    <span class=\"n\">line_arg</span><span class=\"o\">=</span><span class=\"s1\">&#39;provider&#39;</span><span class=\"p\">,</span>\n+    <span class=\"n\">line_vals</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"p\">([</span><span class=\"s1\">&#39;flash&#39;</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">HAS_FLASH</span> <span class=\"k\">else</span> <span class=\"p\">[]),</span>\n+    <span class=\"n\">line_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;Triton&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"p\">([</span><span class=\"s1\">&#39;Flash&#39;</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">HAS_FLASH</span> <span class=\"k\">else</span> <span class=\"p\">[]),</span>\n+    <span class=\"n\">styles</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s1\">&#39;red&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;blue&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">)],</span>\n+    <span class=\"n\">ylabel</span><span class=\"o\">=</span><span class=\"s1\">&#39;ms&#39;</span><span class=\"p\">,</span>\n+    <span class=\"n\">plot_name</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"s1\">&#39;fused-attention-batch</span><span class=\"si\">{</span><span class=\"n\">BATCH</span><span class=\"si\">}</span><span class=\"s1\">-head</span><span class=\"si\">{</span><span class=\"n\">N_HEADS</span><span class=\"si\">}</span><span class=\"s1\">-d</span><span class=\"si\">{</span><span class=\"n\">D_HEAD</span><span class=\"si\">}</span><span class=\"s1\">-</span><span class=\"si\">{</span><span class=\"n\">mode</span><span class=\"si\">}</span><span class=\"s1\">&#39;</span><span class=\"p\">,</span>\n+    <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;H&#39;</span><span class=\"p\">:</span> <span class=\"n\">N_HEADS</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BATCH&#39;</span><span class=\"p\">:</span> <span class=\"n\">BATCH</span><span class=\"p\">,</span> <span class=\"s1\">&#39;D_HEAD&#39;</span><span class=\"p\">:</span> <span class=\"n\">D_HEAD</span><span class=\"p\">,</span> <span class=\"s1\">&#39;dtype&#39;</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">,</span> <span class=\"s1\">&#39;mode&#39;</span><span class=\"p\">:</span> <span class=\"n\">mode</span><span class=\"p\">}</span>\n+<span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">mode</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"s1\">&#39;bwd&#39;</span><span class=\"p\">]]</span>\n+\n+\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">perf_report</span><span class=\"p\">(</span><span class=\"n\">configs</span><span class=\"p\">)</span>\n+<span class=\"k\">def</span> <span class=\"nf\">bench_flash_attention</span><span class=\"p\">(</span><span class=\"n\">BATCH</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">,</span> <span class=\"n\">mode</span><span class=\"p\">,</span> <span class=\"n\">provider</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">):</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">mode</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"s1\">&#39;fwd&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;bwd&#39;</span><span class=\"p\">]</span>\n+    <span class=\"n\">warmup</span> <span class=\"o\">=</span> <span class=\"mi\">25</span>\n+    <span class=\"n\">rep</span> <span class=\"o\">=</span> <span class=\"mi\">100</span>\n+    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;triton&quot;</span><span class=\"p\">:</span>\n+        <span class=\"n\">q</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"n\">BATCH</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+        <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"n\">BATCH</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+        <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"n\">BATCH</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+        <span class=\"n\">sm_scale</span> <span class=\"o\">=</span> <span class=\"mf\">1.3</span>\n+        <span class=\"n\">fn</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">attention</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">sm_scale</span><span class=\"p\">)</span>\n+        <span class=\"k\">if</span> <span class=\"n\">mode</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;bwd&#39;</span><span class=\"p\">:</span>\n+            <span class=\"n\">o</span> <span class=\"o\">=</span> <span class=\"n\">fn</span><span class=\"p\">()</span>\n+            <span class=\"n\">do</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn_like</span><span class=\"p\">(</span><span class=\"n\">o</span><span class=\"p\">)</span>\n+            <span class=\"n\">fn</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">(</span><span class=\"n\">do</span><span class=\"p\">,</span> <span class=\"n\">retain_graph</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+        <span class=\"n\">ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"n\">fn</span><span class=\"p\">,</span> <span class=\"n\">percentiles</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">warmup</span><span class=\"o\">=</span><span class=\"n\">warmup</span><span class=\"p\">,</span> <span class=\"n\">rep</span><span class=\"o\">=</span><span class=\"n\">rep</span><span class=\"p\">)</span>\n+        <span class=\"k\">return</span> <span class=\"n\">ms</span>\n+    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;flash&quot;</span><span class=\"p\">:</span>\n+        <span class=\"n\">lengths</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"n\">BATCH</span><span class=\"p\">,),</span> <span class=\"n\">fill_value</span><span class=\"o\">=</span><span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">)</span>\n+        <span class=\"n\">cu_seqlens</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">BATCH</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">,),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">)</span>\n+        <span class=\"n\">cu_seqlens</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:]</span> <span class=\"o\">=</span> <span class=\"n\">lengths</span><span class=\"o\">.</span><span class=\"n\">cumsum</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+        <span class=\"n\">qkv</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"n\">BATCH</span> <span class=\"o\">*</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+        <span class=\"n\">fn</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">flash_attn_func</span><span class=\"p\">(</span><span class=\"n\">qkv</span><span class=\"p\">,</span> <span class=\"n\">cu_seqlens</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">causal</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+        <span class=\"k\">if</span> <span class=\"n\">mode</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;bwd&#39;</span><span class=\"p\">:</span>\n+            <span class=\"n\">o</span> <span class=\"o\">=</span> <span class=\"n\">fn</span><span class=\"p\">()</span>\n+            <span class=\"n\">do</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn_like</span><span class=\"p\">(</span><span class=\"n\">o</span><span class=\"p\">)</span>\n+            <span class=\"n\">fn</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">(</span><span class=\"n\">do</span><span class=\"p\">,</span> <span class=\"n\">retain_graph</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+        <span class=\"n\">ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"n\">fn</span><span class=\"p\">,</span> <span class=\"n\">percentiles</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">warmup</span><span class=\"o\">=</span><span class=\"n\">warmup</span><span class=\"p\">,</span> <span class=\"n\">rep</span><span class=\"o\">=</span><span class=\"n\">rep</span><span class=\"p\">)</span>\n+        <span class=\"k\">return</span> <span class=\"n\">ms</span>\n+\n+<span class=\"c1\"># only works on A100 at the moment</span>\n+<span class=\"c1\"># bench_flash_attention.run(save_path=&#39;.&#39;, print_data=True)</span>\n+</pre></div>\n+</div>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.072 seconds)</p>\n+<div class=\"sphx-glr-footer class sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-06-fused-attention-py\">\n+<div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/54a35f6ec55f9746935b9566fb6bb1df/06-fused-attention.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">06-fused-attention.py</span></code></a></p>\n+</div>\n+<div class=\"sphx-glr-download sphx-glr-download-jupyter docutils container\">\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/3176accb6c7288b0e45f41d94eebacb9/06-fused-attention.ipynb\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Jupyter</span> <span class=\"pre\">notebook:</span> <span class=\"pre\">06-fused-attention.ipynb</span></code></a></p>\n+</div>\n+</div>\n+<p class=\"sphx-glr-signature\"><a class=\"reference external\" href=\"https://sphinx-gallery.github.io\">Gallery generated by Sphinx-Gallery</a></p>\n+</div>\n+\n+\n+           </div>\n+           \n+          </div>\n+          <footer>\n+    <div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"footer navigation\">\n+        <a href=\"07-libdevice-function.html\" class=\"btn btn-neutral float-right\" title=\"Libdevice function\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+        <a href=\"05-layer-norm.html\" class=\"btn btn-neutral float-left\" title=\"Layer Normalization\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+    </div>\n+\n+  <hr/>\n+\n+  <div role=\"contentinfo\">\n+    <p>\n+        &#169; Copyright 2020, Philippe Tillet.\n+\n+    </p>\n+  </div>\n+    \n+    \n+    \n+    Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n+    \n+    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n+    \n+    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>. \n+\n+</footer>\n+        </div>\n+      </div>\n+\n+    </section>\n+\n+  </div>\n+  \n+<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n+    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n+        <span class=\"fa fa-book\"> Other Versions</span>\n+        v: master\n+        <span class=\"fa fa-caret-down\"></span>\n+    </span>\n+    <div class=\"rst-other-versions\">\n+        <dl>\n+            <dt>Tags</dt>\n+            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n+        </dl>\n+        <dl>\n+            <dt>Branches</dt>\n+            <dd><a href=\"06-fused-attention.html\">master</a></dd>\n+        </dl>\n+    </div>\n+</div>\n+\n+  <script type=\"text/javascript\">\n+      jQuery(function () {\n+          SphinxRtdTheme.Navigation.enable(true);\n+      });\n+  </script>\n+\n+  \n+  \n+    \n+   \n+\n+</body>\n+</html>\n\\ No newline at end of file"}, {"filename": "master/getting-started/tutorials/07-libdevice-function.html", "status": "added", "additions": 357, "deletions": 0, "changes": 357, "file_content_changes": "@@ -0,0 +1,357 @@\n+\n+\n+<!DOCTYPE html>\n+<html class=\"writer-html5\" lang=\"en\" >\n+<head>\n+  <meta charset=\"utf-8\" />\n+  \n+  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n+  \n+  <title>Libdevice function &mdash; Triton  documentation</title>\n+  \n+\n+  \n+  <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n+  <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n+  <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n+  <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n+  <link rel=\"stylesheet\" href=\"../../_static/gallery.css\" type=\"text/css\" />\n+  <link rel=\"stylesheet\" href=\"../../_static/gallery-binder.css\" type=\"text/css\" />\n+  <link rel=\"stylesheet\" href=\"../../_static/gallery-dataframe.css\" type=\"text/css\" />\n+  <link rel=\"stylesheet\" href=\"../../_static/gallery-rendered-html.css\" type=\"text/css\" />\n+  <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n+\n+  \n+  \n+\n+  \n+  \n+\n+  \n+\n+  \n+  <!--[if lt IE 9]>\n+    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n+  <![endif]-->\n+  \n+    \n+      <script type=\"text/javascript\" id=\"documentation_options\" data-url_root=\"../../\" src=\"../../_static/documentation_options.js\"></script>\n+        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n+        <script src=\"../../_static/jquery.js\"></script>\n+        <script src=\"../../_static/underscore.js\"></script>\n+        <script src=\"../../_static/doctools.js\"></script>\n+    \n+    <script type=\"text/javascript\" src=\"../../_static/js/theme.js\"></script>\n+\n+    \n+    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n+    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n+    <link rel=\"next\" title=\"triton\" href=\"../../python-api/triton.html\" />\n+    <link rel=\"prev\" title=\"Fused Attention\" href=\"06-fused-attention.html\" /> \n+</head>\n+\n+<body class=\"wy-body-for-nav\">\n+\n+   \n+  <div class=\"wy-grid-for-nav\">\n+    \n+    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n+      <div class=\"wy-side-scroll\">\n+        <div class=\"wy-side-nav-search\" >\n+          \n+\n+          \n+            <a href=\"../../index.html\" class=\"icon icon-home\"> Triton\n+          \n+\n+          \n+          </a>\n+\n+          \n+            \n+            \n+          \n+\n+          \n+<div role=\"search\">\n+  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n+    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" />\n+    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n+    <input type=\"hidden\" name=\"area\" value=\"default\" />\n+  </form>\n+</div>\n+\n+          \n+        </div>\n+\n+        \n+        <div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"main navigation\">\n+          \n+            \n+            \n+              \n+            \n+            \n+              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n+<ul class=\"current\">\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../installation.html\">Installation</a></li>\n+<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"index.html\">Tutorials</a><ul class=\"current\">\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"01-vector-add.html\">Vector Addition</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"02-fused-softmax.html\">Fused Softmax</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n+<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Libdevice function</a><ul>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#asin-kernel\">asin Kernel</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#using-the-default-libdevice-library-path\">Using the default libdevice library path</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#customize-the-libdevice-library-path\">Customize the libdevice library path</a></li>\n+</ul>\n+</li>\n+</ul>\n+</li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n+</ul>\n+\n+            \n+          \n+        </div>\n+        \n+      </div>\n+    </nav>\n+\n+    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\">\n+\n+      \n+      <nav class=\"wy-nav-top\" aria-label=\"top navigation\">\n+        \n+          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n+          <a href=\"../../index.html\">Triton</a>\n+        \n+      </nav>\n+\n+\n+      <div class=\"wy-nav-content\">\n+        \n+        <div class=\"rst-content\">\n+        \n+          \n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+<div role=\"navigation\" aria-label=\"breadcrumbs navigation\">\n+\n+  <ul class=\"wy-breadcrumbs\">\n+    \n+      <li><a href=\"../../index.html\" class=\"icon icon-home\"></a> &raquo;</li>\n+        \n+          <li><a href=\"index.html\">Tutorials</a> &raquo;</li>\n+        \n+      <li>Libdevice function</li>\n+    \n+    \n+      <li class=\"wy-breadcrumbs-aside\">\n+        \n+          \n+            <a href=\"../../_sources/getting-started/tutorials/07-libdevice-function.rst.txt\" rel=\"nofollow\"> View page source</a>\n+          \n+        \n+      </li>\n+    \n+  </ul>\n+\n+  \n+  <hr/>\n+</div>\n+          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n+           <div itemprop=\"articleBody\">\n+            \n+  <div class=\"sphx-glr-download-link-note admonition note\">\n+<p class=\"admonition-title\">Note</p>\n+<p>Click <a class=\"reference internal\" href=\"#sphx-glr-download-getting-started-tutorials-07-libdevice-function-py\"><span class=\"std std-ref\">here</span></a>\n+to download the full example code</p>\n+</div>\n+<div class=\"sphx-glr-example-title section\" id=\"libdevice-function\">\n+<span id=\"sphx-glr-getting-started-tutorials-07-libdevice-function-py\"></span><h1>Libdevice function<a class=\"headerlink\" href=\"#libdevice-function\" title=\"Permalink to this headline\">\u00b6</a></h1>\n+<p>Triton can invoke a custom function from an external library.\n+In this example, we will use the <cite>libdevice</cite> library to apply <cite>asin</cite> on a tensor.\n+Please refer to <a class=\"reference external\" href=\"https://docs.nvidia.com/cuda/libdevice-users-guide/index.html\">https://docs.nvidia.com/cuda/libdevice-users-guide/index.html</a> regarding the semantics of all available libdevice functions.</p>\n+<p>In <cite>trition/language/libdevice.py</cite>, we try to aggregate functions with the same computation but different data types together.\n+For example, both <cite>__nv_asin</cite> and <cite>__nvasinf</cite> calculate the principal value of the arc sine of the input, but <cite>__nv_asin</cite> operates on <cite>double</cite> and <cite>__nv_asinf</cite> operates on <cite>float</cite>.\n+Using triton, you can simply call <cite>tl.libdevice.asinf</cite>.\n+triton automatically selects the correct underlying device function to invoke based on input and output types.</p>\n+<div class=\"section\" id=\"asin-kernel\">\n+<h2>asin Kernel<a class=\"headerlink\" href=\"#asin-kernel\" title=\"Permalink to this headline\">\u00b6</a></h2>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n+\n+<span class=\"kn\">import</span> <span class=\"nn\">triton</span>\n+<span class=\"kn\">import</span> <span class=\"nn\">triton.language</span> <span class=\"k\">as</span> <span class=\"nn\">tl</span>\n+\n+\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n+<span class=\"k\">def</span> <span class=\"nf\">asin_kernel</span><span class=\"p\">(</span>\n+    <span class=\"n\">x_ptr</span><span class=\"p\">,</span>\n+    <span class=\"n\">y_ptr</span><span class=\"p\">,</span>\n+    <span class=\"n\">n_elements</span><span class=\"p\">,</span>\n+    <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+<span class=\"p\">):</span>\n+    <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"n\">block_start</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE</span>\n+    <span class=\"n\">offsets</span> <span class=\"o\">=</span> <span class=\"n\">block_start</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">)</span>\n+    <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">offsets</span> <span class=\"o\">&lt;</span> <span class=\"n\">n_elements</span>\n+    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">x_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n+    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">libdevice</span><span class=\"o\">.</span><span class=\"n\">asin</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">y_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+</div>\n+<div class=\"section\" id=\"using-the-default-libdevice-library-path\">\n+<h2>Using the default libdevice library path<a class=\"headerlink\" href=\"#using-the-default-libdevice-library-path\" title=\"Permalink to this headline\">\u00b6</a></h2>\n+<p>We can use the default libdevice library path encoded in <cite>triton/language/libdevice.py</cite></p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+<span class=\"n\">size</span> <span class=\"o\">=</span> <span class=\"mi\">98432</span>\n+<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n+<span class=\"n\">output_triton</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n+<span class=\"n\">output_torch</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">asin</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n+<span class=\"k\">assert</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">is_cuda</span> <span class=\"ow\">and</span> <span class=\"n\">output_triton</span><span class=\"o\">.</span><span class=\"n\">is_cuda</span>\n+<span class=\"n\">n_elements</span> <span class=\"o\">=</span> <span class=\"n\">output_torch</span><span class=\"o\">.</span><span class=\"n\">numel</span><span class=\"p\">()</span>\n+<span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">meta</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">meta</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">]),)</span>\n+<span class=\"n\">asin_kernel</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">output_triton</span><span class=\"p\">,</span> <span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"o\">=</span><span class=\"mi\">1024</span><span class=\"p\">)</span>\n+<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">output_torch</span><span class=\"p\">)</span>\n+<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">output_triton</span><span class=\"p\">)</span>\n+<span class=\"nb\">print</span><span class=\"p\">(</span>\n+    <span class=\"sa\">f</span><span class=\"s1\">&#39;The maximum difference between torch and triton is &#39;</span>\n+    <span class=\"sa\">f</span><span class=\"s1\">&#39;</span><span class=\"si\">{</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">abs</span><span class=\"p\">(</span><span class=\"n\">output_torch</span> <span class=\"o\">-</span> <span class=\"n\">output_triton</span><span class=\"p\">))</span><span class=\"si\">}</span><span class=\"s1\">&#39;</span>\n+<span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+<p class=\"sphx-glr-script-out\">Out:</p>\n+<div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device=&#39;cuda:0&#39;)\n+tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device=&#39;cuda:0&#39;)\n+The maximum difference between torch and triton is 2.384185791015625e-07\n+</pre></div>\n+</div>\n+</div>\n+<div class=\"section\" id=\"customize-the-libdevice-library-path\">\n+<h2>Customize the libdevice library path<a class=\"headerlink\" href=\"#customize-the-libdevice-library-path\" title=\"Permalink to this headline\">\u00b6</a></h2>\n+<p>We can also customize the libdevice library path by passing the path to the <cite>libdevice</cite> library to the <cite>asin</cite> kernel.</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">output_triton</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n+<span class=\"n\">asin_kernel</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">output_triton</span><span class=\"p\">,</span> <span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"o\">=</span><span class=\"mi\">1024</span><span class=\"p\">,</span>\n+                  <span class=\"n\">extern_libs</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;libdevice&#39;</span><span class=\"p\">:</span> <span class=\"s1\">&#39;/usr/local/cuda/nvvm/libdevice/libdevice.10.bc&#39;</span><span class=\"p\">})</span>\n+<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">output_torch</span><span class=\"p\">)</span>\n+<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">output_triton</span><span class=\"p\">)</span>\n+<span class=\"nb\">print</span><span class=\"p\">(</span>\n+    <span class=\"sa\">f</span><span class=\"s1\">&#39;The maximum difference between torch and triton is &#39;</span>\n+    <span class=\"sa\">f</span><span class=\"s1\">&#39;</span><span class=\"si\">{</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">abs</span><span class=\"p\">(</span><span class=\"n\">output_torch</span> <span class=\"o\">-</span> <span class=\"n\">output_triton</span><span class=\"p\">))</span><span class=\"si\">}</span><span class=\"s1\">&#39;</span>\n+<span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+<p class=\"sphx-glr-script-out\">Out:</p>\n+<div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device=&#39;cuda:0&#39;)\n+tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device=&#39;cuda:0&#39;)\n+The maximum difference between torch and triton is 2.384185791015625e-07\n+</pre></div>\n+</div>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.501 seconds)</p>\n+<div class=\"sphx-glr-footer class sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-07-libdevice-function-py\">\n+<div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/3ff29f967ace7985da24aab10352fc76/07-libdevice-function.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">07-libdevice-function.py</span></code></a></p>\n+</div>\n+<div class=\"sphx-glr-download sphx-glr-download-jupyter docutils container\">\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/1bc2e471d2fb0ec017c4d1d0890db4e2/07-libdevice-function.ipynb\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Jupyter</span> <span class=\"pre\">notebook:</span> <span class=\"pre\">07-libdevice-function.ipynb</span></code></a></p>\n+</div>\n+</div>\n+<p class=\"sphx-glr-signature\"><a class=\"reference external\" href=\"https://sphinx-gallery.github.io\">Gallery generated by Sphinx-Gallery</a></p>\n+</div>\n+</div>\n+\n+\n+           </div>\n+           \n+          </div>\n+          <footer>\n+    <div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"footer navigation\">\n+        <a href=\"../../python-api/triton.html\" class=\"btn btn-neutral float-right\" title=\"triton\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+        <a href=\"06-fused-attention.html\" class=\"btn btn-neutral float-left\" title=\"Fused Attention\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+    </div>\n+\n+  <hr/>\n+\n+  <div role=\"contentinfo\">\n+    <p>\n+        &#169; Copyright 2020, Philippe Tillet.\n+\n+    </p>\n+  </div>\n+    \n+    \n+    \n+    Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n+    \n+    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n+    \n+    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>. \n+\n+</footer>\n+        </div>\n+      </div>\n+\n+    </section>\n+\n+  </div>\n+  \n+<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n+    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n+        <span class=\"fa fa-book\"> Other Versions</span>\n+        v: master\n+        <span class=\"fa fa-caret-down\"></span>\n+    </span>\n+    <div class=\"rst-other-versions\">\n+        <dl>\n+            <dt>Tags</dt>\n+            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n+        </dl>\n+        <dl>\n+            <dt>Branches</dt>\n+            <dd><a href=\"07-libdevice-function.html\">master</a></dd>\n+        </dl>\n+    </div>\n+</div>\n+\n+  <script type=\"text/javascript\">\n+      jQuery(function () {\n+          SphinxRtdTheme.Navigation.enable(true);\n+      });\n+  </script>\n+\n+  \n+  \n+    \n+   \n+\n+</body>\n+</html>\n\\ No newline at end of file"}, {"filename": "master/getting-started/tutorials/index.html", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -101,6 +101,8 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-libdevice-function.html\">Libdevice function</a></li>\n </ul>\n </li>\n </ul>\n@@ -219,6 +221,18 @@\n </div>\n </div><div class=\"toctree-wrapper compound\">\n </div>\n+<div class=\"sphx-glr-thumbcontainer\" tooltip=\"Fused Attention\"><div class=\"figure align-default\" id=\"id6\">\n+<img alt=\"Fused Attention\" src=\"../../_images/sphx_glr_06-fused-attention_thumb.png\" />\n+<p class=\"caption\"><span class=\"caption-text\"><a class=\"reference internal\" href=\"06-fused-attention.html#sphx-glr-getting-started-tutorials-06-fused-attention-py\"><span class=\"std std-ref\">Fused Attention</span></a></span><a class=\"headerlink\" href=\"#id6\" title=\"Permalink to this image\">\u00b6</a></p>\n+</div>\n+</div><div class=\"toctree-wrapper compound\">\n+</div>\n+<div class=\"sphx-glr-thumbcontainer\" tooltip=\"In trition/language/libdevice.py, we try to aggregate functions with the same computation but d...\"><div class=\"figure align-default\" id=\"id7\">\n+<img alt=\"Libdevice function\" src=\"../../_images/sphx_glr_07-libdevice-function_thumb.png\" />\n+<p class=\"caption\"><span class=\"caption-text\"><a class=\"reference internal\" href=\"07-libdevice-function.html#sphx-glr-getting-started-tutorials-07-libdevice-function-py\"><span class=\"std std-ref\">Libdevice function</span></a></span><a class=\"headerlink\" href=\"#id7\" title=\"Permalink to this image\">\u00b6</a></p>\n+</div>\n+</div><div class=\"toctree-wrapper compound\">\n+</div>\n <div class=\"sphx-glr-clear\"></div><div class=\"sphx-glr-footer class sphx-glr-footer-gallery docutils container\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/763344228ae6bc253ed1a6cf586aa30d/tutorials_python.zip\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">all</span> <span class=\"pre\">examples</span> <span class=\"pre\">in</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">tutorials_python.zip</span></code></a></p>"}, {"filename": "master/getting-started/tutorials/sg_execution_times.html", "status": "modified", "additions": 15, "deletions": 7, "changes": 22, "file_content_changes": "@@ -174,7 +174,7 @@\n             \n   <div class=\"section\" id=\"computation-times\">\n <span id=\"sphx-glr-getting-started-tutorials-sg-execution-times\"></span><h1>Computation times<a class=\"headerlink\" href=\"#computation-times\" title=\"Permalink to this headline\">\u00b6</a></h1>\n-<p><strong>16:10.599</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n+<p><strong>18:09.339</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n <table class=\"docutils align-default\">\n <colgroup>\n <col style=\"width: 85%\" />\n@@ -183,23 +183,31 @@\n </colgroup>\n <tbody>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"03-matrix-multiplication.html#sphx-glr-getting-started-tutorials-03-matrix-multiplication-py\"><span class=\"std std-ref\">Matrix Multiplication</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">03-matrix-multiplication.py</span></code>)</p></td>\n-<td><p>05:52.578</p></td>\n+<td><p>07:13.827</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"05-layer-norm.html#sphx-glr-getting-started-tutorials-05-layer-norm-py\"><span class=\"std std-ref\">Layer Normalization</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">05-layer-norm.py</span></code>)</p></td>\n-<td><p>05:24.641</p></td>\n+<td><p>05:32.552</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py\"><span class=\"std std-ref\">Fused Softmax</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">02-fused-softmax.py</span></code>)</p></td>\n-<td><p>03:18.076</p></td>\n+<td><p>03:32.089</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py\"><span class=\"std std-ref\">Vector Addition</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">01-vector-add.py</span></code>)</p></td>\n-<td><p>01:34.829</p></td>\n+<td><p>01:50.020</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n-<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"04-low-memory-dropout.html#sphx-glr-getting-started-tutorials-04-low-memory-dropout-py\"><span class=\"std std-ref\">Low-Memory Dropout</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">04-low-memory-dropout.py</span></code>)</p></td>\n-<td><p>00:00.476</p></td>\n+<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"07-libdevice-function.html#sphx-glr-getting-started-tutorials-07-libdevice-function-py\"><span class=\"std std-ref\">Libdevice function</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">07-libdevice-function.py</span></code>)</p></td>\n+<td><p>00:00.501</p></td>\n+<td><p>0.0 MB</p></td>\n+</tr>\n+<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"04-low-memory-dropout.html#sphx-glr-getting-started-tutorials-04-low-memory-dropout-py\"><span class=\"std std-ref\">Low-Memory Dropout</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">04-low-memory-dropout.py</span></code>)</p></td>\n+<td><p>00:00.279</p></td>\n+<td><p>0.0 MB</p></td>\n+</tr>\n+<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"06-fused-attention.html#sphx-glr-getting-started-tutorials-06-fused-attention-py\"><span class=\"std std-ref\">Fused Attention</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">06-fused-attention.py</span></code>)</p></td>\n+<td><p>00:00.072</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n </tbody>"}, {"filename": "master/objects.inv", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/python-api/generated/triton.language.dot.html", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -197,7 +197,7 @@\n <h1>triton.language.dot<a class=\"headerlink\" href=\"#triton-language-dot\" title=\"Permalink to this headline\">\u00b6</a></h1>\n <dl class=\"py function\">\n <dt class=\"sig sig-object py\" id=\"triton.language.dot\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">dot</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">input</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">other</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">allow_tf32</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.dot\" title=\"Permalink to this definition\">\u00b6</a></dt>\n+<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">dot</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">input</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">other</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">trans_a</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">trans_b</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">allow_tf32</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.dot\" title=\"Permalink to this definition\">\u00b6</a></dt>\n <dd><p>Returns the matrix product of two blocks.</p>\n <p>The two blocks must be two dimensionals and have compatible inner dimensions.</p>\n <dl class=\"field-list simple\">"}, {"filename": "master/python-api/generated/triton.language.store.html", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -200,7 +200,7 @@\n <h1>triton.language.store<a class=\"headerlink\" href=\"#triton-language-store\" title=\"Permalink to this headline\">\u00b6</a></h1>\n <dl class=\"py function\">\n <dt class=\"sig sig-object py\" id=\"triton.language.store\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">store</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">pointer</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">value</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">mask</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.store\" title=\"Permalink to this definition\">\u00b6</a></dt>\n+<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">store</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">pointer</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">value</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">mask</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">eviction_policy</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">''</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.store\" title=\"Permalink to this definition\">\u00b6</a></dt>\n <dd><p>Stores <code class=\"code docutils literal notranslate\"><span class=\"pre\">value</span></code> tensor of elements in memory, element-wise, at the memory locations specified by <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer</span></code>.</p>\n <p><code class=\"code docutils literal notranslate\"><span class=\"pre\">value</span></code> is implicitly broadcast to <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer.shape</span></code> and typecast to <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer.dtype.element_ty</span></code>.</p>\n <dl class=\"field-list simple\">"}, {"filename": "master/python-api/triton.html", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -47,7 +47,7 @@\n     <link rel=\"index\" title=\"Index\" href=\"../genindex.html\" />\n     <link rel=\"search\" title=\"Search\" href=\"../search.html\" />\n     <link rel=\"next\" title=\"triton.jit\" href=\"generated/triton.jit.html\" />\n-    <link rel=\"prev\" title=\"Layer Normalization\" href=\"../getting-started/tutorials/05-layer-norm.html\" /> \n+    <link rel=\"prev\" title=\"Libdevice function\" href=\"../getting-started/tutorials/07-libdevice-function.html\" /> \n </head>\n \n <body class=\"wy-body-for-nav\">\n@@ -211,7 +211,7 @@ <h1>triton<a class=\"headerlink\" href=\"#triton\" title=\"Permalink to this headline\n           <footer>\n     <div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"footer navigation\">\n         <a href=\"generated/triton.jit.html\" class=\"btn btn-neutral float-right\" title=\"triton.jit\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-        <a href=\"../getting-started/tutorials/05-layer-norm.html\" class=\"btn btn-neutral float-left\" title=\"Layer Normalization\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"../getting-started/tutorials/07-libdevice-function.html\" class=\"btn btn-neutral float-left\" title=\"Libdevice function\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n     </div>\n \n   <hr/>"}, {"filename": "master/searchindex.js", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1 +1 @@\n-Search.setIndex({docnames:[\"getting-started/installation\",\"getting-started/tutorials/01-vector-add\",\"getting-started/tutorials/02-fused-softmax\",\"getting-started/tutorials/03-matrix-multiplication\",\"getting-started/tutorials/04-low-memory-dropout\",\"getting-started/tutorials/05-layer-norm\",\"getting-started/tutorials/index\",\"getting-started/tutorials/sg_execution_times\",\"index\",\"programming-guide/chapter-1/introduction\",\"programming-guide/chapter-2/related-work\",\"python-api/generated/triton.Config\",\"python-api/generated/triton.autotune\",\"python-api/generated/triton.heuristics\",\"python-api/generated/triton.jit\",\"python-api/generated/triton.language.arange\",\"python-api/generated/triton.language.atomic_add\",\"python-api/generated/triton.language.atomic_and\",\"python-api/generated/triton.language.atomic_cas\",\"python-api/generated/triton.language.atomic_max\",\"python-api/generated/triton.language.atomic_min\",\"python-api/generated/triton.language.atomic_or\",\"python-api/generated/triton.language.atomic_xchg\",\"python-api/generated/triton.language.atomic_xor\",\"python-api/generated/triton.language.broadcast_to\",\"python-api/generated/triton.language.cos\",\"python-api/generated/triton.language.dot\",\"python-api/generated/triton.language.exp\",\"python-api/generated/triton.language.load\",\"python-api/generated/triton.language.log\",\"python-api/generated/triton.language.max\",\"python-api/generated/triton.language.maximum\",\"python-api/generated/triton.language.min\",\"python-api/generated/triton.language.minimum\",\"python-api/generated/triton.language.multiple_of\",\"python-api/generated/triton.language.num_programs\",\"python-api/generated/triton.language.program_id\",\"python-api/generated/triton.language.rand\",\"python-api/generated/triton.language.randint\",\"python-api/generated/triton.language.randint4x\",\"python-api/generated/triton.language.randn\",\"python-api/generated/triton.language.ravel\",\"python-api/generated/triton.language.reshape\",\"python-api/generated/triton.language.sigmoid\",\"python-api/generated/triton.language.sin\",\"python-api/generated/triton.language.softmax\",\"python-api/generated/triton.language.sqrt\",\"python-api/generated/triton.language.store\",\"python-api/generated/triton.language.sum\",\"python-api/generated/triton.language.where\",\"python-api/generated/triton.language.zeros\",\"python-api/generated/triton.testing.Benchmark\",\"python-api/generated/triton.testing.do_bench\",\"python-api/generated/triton.testing.perf_report\",\"python-api/triton\",\"python-api/triton.language\",\"python-api/triton.testing\"],envversion:{\"sphinx.domains.c\":2,\"sphinx.domains.changeset\":1,\"sphinx.domains.citation\":1,\"sphinx.domains.cpp\":4,\"sphinx.domains.index\":1,\"sphinx.domains.javascript\":2,\"sphinx.domains.math\":2,\"sphinx.domains.python\":3,\"sphinx.domains.rst\":2,\"sphinx.domains.std\":2,\"sphinx.ext.intersphinx\":1,sphinx:56},filenames:[\"getting-started/installation.rst\",\"getting-started/tutorials/01-vector-add.rst\",\"getting-started/tutorials/02-fused-softmax.rst\",\"getting-started/tutorials/03-matrix-multiplication.rst\",\"getting-started/tutorials/04-low-memory-dropout.rst\",\"getting-started/tutorials/05-layer-norm.rst\",\"getting-started/tutorials/index.rst\",\"getting-started/tutorials/sg_execution_times.rst\",\"index.rst\",\"programming-guide/chapter-1/introduction.rst\",\"programming-guide/chapter-2/related-work.rst\",\"python-api/generated/triton.Config.rst\",\"python-api/generated/triton.autotune.rst\",\"python-api/generated/triton.heuristics.rst\",\"python-api/generated/triton.jit.rst\",\"python-api/generated/triton.language.arange.rst\",\"python-api/generated/triton.language.atomic_add.rst\",\"python-api/generated/triton.language.atomic_and.rst\",\"python-api/generated/triton.language.atomic_cas.rst\",\"python-api/generated/triton.language.atomic_max.rst\",\"python-api/generated/triton.language.atomic_min.rst\",\"python-api/generated/triton.language.atomic_or.rst\",\"python-api/generated/triton.language.atomic_xchg.rst\",\"python-api/generated/triton.language.atomic_xor.rst\",\"python-api/generated/triton.language.broadcast_to.rst\",\"python-api/generated/triton.language.cos.rst\",\"python-api/generated/triton.language.dot.rst\",\"python-api/generated/triton.language.exp.rst\",\"python-api/generated/triton.language.load.rst\",\"python-api/generated/triton.language.log.rst\",\"python-api/generated/triton.language.max.rst\",\"python-api/generated/triton.language.maximum.rst\",\"python-api/generated/triton.language.min.rst\",\"python-api/generated/triton.language.minimum.rst\",\"python-api/generated/triton.language.multiple_of.rst\",\"python-api/generated/triton.language.num_programs.rst\",\"python-api/generated/triton.language.program_id.rst\",\"python-api/generated/triton.language.rand.rst\",\"python-api/generated/triton.language.randint.rst\",\"python-api/generated/triton.language.randint4x.rst\",\"python-api/generated/triton.language.randn.rst\",\"python-api/generated/triton.language.ravel.rst\",\"python-api/generated/triton.language.reshape.rst\",\"python-api/generated/triton.language.sigmoid.rst\",\"python-api/generated/triton.language.sin.rst\",\"python-api/generated/triton.language.softmax.rst\",\"python-api/generated/triton.language.sqrt.rst\",\"python-api/generated/triton.language.store.rst\",\"python-api/generated/triton.language.sum.rst\",\"python-api/generated/triton.language.where.rst\",\"python-api/generated/triton.language.zeros.rst\",\"python-api/generated/triton.testing.Benchmark.rst\",\"python-api/generated/triton.testing.do_bench.rst\",\"python-api/generated/triton.testing.perf_report.rst\",\"python-api/triton.rst\",\"python-api/triton.language.rst\",\"python-api/triton.testing.rst\"],objects:{\"triton.Config\":{__init__:[11,1,1,\"\"]},\"triton.language\":{arange:[15,2,1,\"\"],atomic_add:[16,2,1,\"\"],atomic_and:[17,2,1,\"\"],atomic_cas:[18,2,1,\"\"],atomic_max:[19,2,1,\"\"],atomic_min:[20,2,1,\"\"],atomic_or:[21,2,1,\"\"],atomic_xchg:[22,2,1,\"\"],atomic_xor:[23,2,1,\"\"],broadcast_to:[24,2,1,\"\"],cos:[25,2,1,\"\"],dot:[26,2,1,\"\"],exp:[27,2,1,\"\"],load:[28,2,1,\"\"],log:[29,2,1,\"\"],max:[30,2,1,\"\"],maximum:[31,2,1,\"\"],min:[32,2,1,\"\"],minimum:[33,2,1,\"\"],multiple_of:[34,2,1,\"\"],num_programs:[35,2,1,\"\"],program_id:[36,2,1,\"\"],rand:[37,2,1,\"\"],randint4x:[39,2,1,\"\"],randint:[38,2,1,\"\"],randn:[40,2,1,\"\"],ravel:[41,2,1,\"\"],reshape:[42,2,1,\"\"],sigmoid:[43,2,1,\"\"],sin:[44,2,1,\"\"],softmax:[45,2,1,\"\"],sqrt:[46,2,1,\"\"],store:[47,2,1,\"\"],sum:[48,2,1,\"\"],where:[49,2,1,\"\"],zeros:[50,2,1,\"\"]},\"triton.testing\":{Benchmark:[51,0,1,\"\"],do_bench:[52,2,1,\"\"],perf_report:[53,2,1,\"\"]},\"triton.testing.Benchmark\":{__init__:[51,1,1,\"\"]},triton:{Config:[11,0,1,\"\"],autotune:[12,2,1,\"\"],heuristics:[13,2,1,\"\"],jit:[14,2,1,\"\"]}},objnames:{\"0\":[\"py\",\"class\",\"Python class\"],\"1\":[\"py\",\"method\",\"Python method\"],\"2\":[\"py\",\"function\",\"Python function\"]},objtypes:{\"0\":\"py:class\",\"1\":\"py:method\",\"2\":\"py:function\"},terms:{\"0\":[1,2,3,4,5,7,9,10,35,36,37,40,50,52],\"00\":7,\"0000\":3,\"000000\":2,\"000001\":[1,2],\"000002\":[2,5],\"004273\":1,\"005597\":5,\"007961\":2,\"01\":[1,3,7],\"02\":[2,7],\"023256\":5,\"025776\":3,\"028308\":3,\"03\":[3,7],\"033565\":3,\"04\":[4,7],\"05\":[5,7],\"057651\":3,\"0625\":3,\"064941\":3,\"076\":[2,7],\"078767\":5,\"08199\":4,\"08452\":4,\"084721\":1,\"092307\":5,\"0938\":3,\"096442\":3,\"097543\":2,\"0f\":10,\"0s\":4,\"1\":[1,2,3,4,5,8,10,13,35,36,37,40],\"10\":[1,3,4,5,7,37,38,39,40],\"100\":[2,52],\"1024\":[1,3,4,5,12],\"10240\":5,\"1045\":3,\"1048576\":1,\"106434\":4,\"10752\":5,\"11\":[0,1,3,5],\"11264\":5,\"114273\":5,\"1151\":5,\"1152\":3,\"11776\":5,\"12\":[1,3,5],\"12160\":2,\"12288\":[2,5],\"123\":4,\"12416\":2,\"12544\":2,\"12672\":2,\"126988\":5,\"127\":1,\"128\":[1,2,3,5,12],\"1280\":3,\"12800\":5,\"13\":[1,3,5],\"131072\":1,\"1328\":3,\"13312\":5,\"133347\":2,\"134217728\":1,\"134567\":5,\"13686\":4,\"13824\":5,\"138541\":3,\"138932\":3,\"139506\":3,\"14\":[1,3,5],\"140799\":3,\"1408\":3,\"142849\":[2,5],\"142862\":2,\"14336\":5,\"147202\":3,\"14848\":5,\"149375\":2,\"149397\":4,\"15\":[1,3,5],\"153\":2,\"1536\":[3,5],\"15360\":5,\"153868\":5,\"154\":2,\"154861\":3,\"155572\":3,\"15872\":5,\"16\":[2,3,5,7,10,50],\"160\":2,\"161033\":3,\"162\":2,\"16384\":1,\"1664\":3,\"16777216\":1,\"17\":[3,5],\"171410\":5,\"173427\":5,\"177767\":5,\"17879\":4,\"1792\":3,\"179335\":3,\"179533\":2,\"18\":[2,3,5,7],\"180725\":3,\"180982\":5,\"1823\":2,\"185232\":3,\"185964\":5,\"188\":2,\"189190\":3,\"19\":[1,3,5],\"190482\":1,\"192\":1,\"1920\":3,\"192434\":5,\"198\":2,\"1982\":10,\"1983\":9,\"1984\":10,\"1989\":10,\"199\":2,\"1991\":[9,10],\"1999\":10,\"1d\":[1,2,3],\"1e\":[1,2,3,5],\"1s\":4,\"2\":[1,2,3,4,5,8,10,11,13,35,36,52],\"20\":[3,5,52],\"200000\":1,\"200001\":3,\"2004\":10,\"2006\":10,\"2011\":4,\"2012\":10,\"2013\":9,\"2014\":[4,9],\"2016\":[9,10],\"2017\":9,\"2018\":[9,10],\"2019\":10,\"2021\":[9,10],\"2048\":[2,3,5],\"206879\":2,\"2097152\":1,\"209928\":2,\"21\":[3,5],\"212868\":4,\"2141\":1,\"214186\":4,\"214870\":5,\"216187\":2,\"2176\":3,\"219\":1,\"22\":[3,5],\"220\":3,\"222812\":2,\"225175\":5,\"23\":[3,5],\"2304\":3,\"233074\":3,\"237267\":5,\"24\":[3,5,7],\"242181\":3,\"2432\":3,\"245\":3,\"25\":[3,5,52],\"251457\":5,\"254540\":5,\"256\":[1,2,3,5,11],\"2560\":[3,5],\"259080\":3,\"26\":[3,5],\"260869\":3,\"262144\":1,\"264875\":2,\"2656\":3,\"2688\":3,\"269882\":5,\"27\":[3,5],\"277\":5,\"28\":[1,3,5],\"2812\":3,\"2816\":3,\"2891\":3,\"29\":[3,5],\"293429\":4,\"293714\":5,\"2944\":3,\"297068\":5,\"298541\":2,\"299883\":3,\"2d\":[3,5,26],\"2m\":2,\"2mn\":2,\"3\":[0,1,2,3,4,5,10],\"30\":3,\"304904\":5,\"305746\":3,\"3072\":[3,5],\"3076\":1,\"31\":3,\"3125\":3,\"313806\":5,\"315309\":5,\"32\":[3,5,11],\"3200\":3,\"321838\":2,\"323\":5,\"323259\":3,\"32768\":1,\"3281\":3,\"33\":3,\"3328\":3,\"333321\":1,\"334\":5,\"33554432\":1,\"337026\":5,\"34\":[1,3,7],\"341\":1,\"34172\":4,\"3438\":3,\"3456\":3,\"3477\":3,\"347810\":5,\"35\":3,\"3516\":3,\"355034\":3,\"3555\":3,\"357796\":5,\"3584\":[3,5],\"359066\":2,\"36\":3,\"360017\":2,\"360920\":5,\"362445\":1,\"363633\":5,\"365\":5,\"367358\":5,\"367588\":3,\"368435\":5,\"369452\":5,\"37\":3,\"3712\":3,\"3713\":1,\"371721\":4,\"372\":5,\"372618\":3,\"372800\":3,\"373\":5,\"373605\":3,\"374\":5,\"375\":5,\"376\":5,\"377\":5,\"377770\":5,\"378\":5,\"379\":5,\"38\":1,\"380\":5,\"380953\":3,\"380957\":5,\"381\":5,\"382\":5,\"383\":5,\"384\":[1,2,3,5],\"3840\":3,\"387\":5,\"387087\":5,\"388\":5,\"389\":5,\"389355\":5,\"389441\":3,\"389457\":5,\"39\":3,\"3906\":3,\"393\":5,\"393507\":3,\"396\":5,\"396572\":3,\"3968\":3,\"397\":5,\"3984\":3,\"3986\":4,\"3d\":[35,36],\"3mn\":2,\"4\":[1,2,3,5,10,11,12,38],\"40\":3,\"400\":5,\"400001\":1,\"400016\":[1,2],\"402\":5,\"4023\":3,\"403344\":4,\"403347\":4,\"405\":5,\"406\":[2,5],\"4062\":3,\"407\":5,\"408\":5,\"408716\":4,\"409\":5,\"4096\":[1,2,3,5],\"410\":5,\"411\":5,\"412\":2,\"413\":5,\"414\":5,\"415\":2,\"41576\":4,\"4194304\":1,\"420235\":3,\"420822\":3,\"420828\":5,\"42142\":4,\"426\":5,\"428372\":4,\"428568\":1,\"428801\":3,\"429770\":[1,2],\"431969\":4,\"435930\":3,\"445676\":5,\"446516\":3,\"446623\":3,\"447\":5,\"448255\":1,\"4492\":3,\"45\":3,\"4531\":3,\"454\":5,\"458\":5,\"4608\":5,\"4609\":3,\"461\":5,\"464755\":3,\"468\":5,\"4688\":3,\"47\":3,\"470582\":5,\"471\":5,\"472\":1,\"476\":[4,7],\"48\":3,\"481028\":5,\"482\":5,\"49\":3,\"492372\":5,\"492442\":3,\"494\":5,\"4940\":1,\"494120\":3,\"498981\":2,\"4m\":2,\"4x\":2,\"5\":[1,3,4,5,10,52],\"500\":5,\"5000\":3,\"501\":5,\"502740\":5,\"505\":5,\"509816\":5,\"51\":3,\"511\":5,\"512\":[2,3,4,5],\"5120\":5,\"516586\":3,\"517\":5,\"518\":5,\"52\":[3,7],\"520\":5,\"523\":5,\"523664\":3,\"524288\":1,\"526831\":3,\"528664\":3,\"53\":3,\"5312\":3,\"536\":5,\"54\":3,\"541\":4,\"546\":2,\"548180\":3,\"549\":5,\"551037\":5,\"559798\":5,\"5632\":5,\"563555\":3,\"566\":5,\"566038\":2,\"566838\":2,\"568431\":4,\"57\":3,\"575753\":5,\"578\":[3,7],\"578556\":5,\"579673\":3,\"584279\":3,\"585\":[2,5],\"5859\":3,\"586858\":4,\"587162\":5,\"589\":5,\"5898\":3,\"59\":3,\"599\":7,\"599987\":5,\"5mn\":2,\"6\":[0,1,3,5],\"600000\":1,\"600004\":2,\"606\":5,\"6094\":3,\"609605\":5,\"614\":[1,2],\"6144\":5,\"615390\":1,\"62\":3,\"623009\":5,\"626943\":3,\"627\":5,\"627589\":5,\"63\":3,\"630\":5,\"631610\":5,\"634072\":5,\"64\":[1,3],\"640\":[2,3],\"641\":[5,7],\"642\":5,\"643199\":3,\"64kb\":5,\"65\":3,\"655\":2,\"65536\":[1,5],\"656574\":1,\"664\":2,\"665176\":3,\"6656\":5,\"666643\":5,\"666652\":5,\"666656\":5,\"669909\":5,\"670\":5,\"67086\":4,\"67108864\":1,\"6724\":1,\"679014\":5,\"68\":3,\"682\":5,\"684049\":3,\"690139\":3,\"694\":5,\"694907\":5,\"695\":5,\"695045\":5,\"6953\":3,\"699062\":5,\"7\":[0,1,3,5,10],\"70\":3,\"700\":5,\"702\":5,\"7031\":3,\"704\":5,\"706\":2,\"7070\":3,\"707878\":4,\"71\":3,\"712\":5,\"712490\":3,\"714281\":5,\"715711\":3,\"7168\":5,\"719258\":4,\"72\":3,\"722\":[1,2],\"725\":5,\"728\":5,\"73\":3,\"730667\":3,\"737433\":3,\"737435\":1,\"743443\":4,\"7500\":3,\"750943\":3,\"751978\":5,\"754967\":2,\"758038\":3,\"758863\":2,\"76\":[1,3],\"768\":[2,3],\"7680\":5,\"768000\":3,\"77\":3,\"772874\":5,\"773130\":3,\"776119\":3,\"78\":3,\"780\":1,\"781\":2,\"783251\":3,\"784810\":5,\"79\":3,\"79719\":4,\"8\":[1,2,3,5,10,11,12,50,52],\"80\":[3,52],\"800002\":1,\"803739\":5,\"806497\":3,\"806694\":4,\"81\":3,\"811\":2,\"812\":[1,2],\"814809\":5,\"817432\":4,\"8192\":[1,5],\"82\":3,\"822459\":3,\"823517\":[1,2],\"825748\":3,\"826188\":5,\"829\":[1,7],\"829164\":3,\"83\":3,\"833\":1,\"833728\":3,\"838026\":4,\"8388608\":1,\"84\":3,\"842\":1,\"84284\":4,\"844306\":5,\"846167\":5,\"847\":1,\"848\":1,\"849\":1,\"85\":3,\"850\":1,\"858629\":3,\"859062\":5,\"86\":3,\"87\":3,\"8704\":5,\"873439\":5,\"88\":3,\"8828\":3,\"885254\":5,\"8867\":3,\"887117\":5,\"888257\":5,\"89\":3,\"8906\":3,\"892307\":3,\"8945\":3,\"896\":3,\"899428\":3,\"8mn\":2,\"9\":[0,1,2,3,4,5],\"90\":3,\"901241\":3,\"908470\":3,\"91\":3,\"913500\":3,\"92\":3,\"920437\":5,\"9216\":5,\"9219\":3,\"93\":[2,3],\"932191\":3,\"934503\":5,\"936606\":2,\"9375\":3,\"94\":2,\"9492\":3,\"95\":[2,3],\"952835\":4,\"9531\":3,\"954424\":2,\"954614\":3,\"956960\":3,\"959706\":3,\"96\":2,\"967074\":5,\"967162\":3,\"9688\":3,\"969169\":5,\"97\":2,\"971025\":3,\"9728\":5,\"9733\":1,\"976995\":5,\"978909\":3,\"98\":2,\"9805\":3,\"983276\":3,\"98432\":1,\"9844\":3,\"999982\":5,\"999986\":5,\"999995\":1,\"abstract\":[9,10],\"break\":10,\"byte\":2,\"case\":[1,2,9,10,13,18],\"class\":[2,5,9,10,11,51],\"default\":52,\"do\":[2,3,9,10,12,16,17,19,20,21,22,23,28,47],\"float\":[2,9,10,52],\"function\":[1,2,3,4,5,10,11,12,13,14,51,52,53],\"import\":[1,2,3,4,5,9,10],\"int\":[1,9,10,13,15,24,35,36,42,50,52],\"new\":[24,42,50],\"return\":[1,2,3,4,5,12,15,16,17,18,19,20,21,22,23,26,28,30,32,35,36,37,38,39,40,41,48,49,50,52,53],\"static\":[0,9,10],\"super\":3,\"switch\":3,\"true\":[1,2,3,5,26,49],\"try\":[3,5,11],\"var\":[5,10],\"voil\\u00e0\":4,\"while\":[3,9],A:[3,4,5,9,10],And:[0,3],As:[2,3,4,9,10],At:[4,10],But:4,By:52,For:[3,9,10,11],If:[4,10,16,17,19,20,21,22,23,38,47,49,51],In:[1,2,3,4,10],It:[1,3,4,6,8,10,12,14],NOT:5,Of:9,On:10,One:3,The:[1,2,3,4,9,10,16,17,18,19,20,21,22,23,24,26,35,36,37,38,39,40,42,47,49,53],There:1,These:10,To:[1,4,6,9,10,12],_:5,__expf:2,__init__:[11,51],_a:5,_da:5,_dout:5,_dropout:4,_layer_norm_bwd_dwdb:5,_layer_norm_bwd_dx_fus:5,_layer_norm_fwd_fus:5,_matmul:3,_mean1:5,_mean2:5,_mean:5,_seeded_dropout:4,_var:5,a100:[3,10],a_arg:5,a_hat:5,a_ptr:3,ab:1,abl:10,about:[1,2,3,4,8],abov:[1,2,3,4,10,12],academ:9,acc:[3,9,10],acceler:9,access:[1,3,9,10,14],accomod:3,accordingli:10,account:10,accumul:[3,5,10],accuraci:[3,9],achiev:[3,9,10],across:[2,4,9,10],activ:3,actual:[3,9,10],ad:5,add:[1,4,5,7,16],add_kernel:1,addit:[2,6,7,9,52],addition:10,address:[9,28],adopt:10,advanc:[2,3,9],advoc:10,affect:3,affin:10,after:3,against:[0,1,2,3,8],aggress:[9,10],agnost:[9,10],ahead:10,aim:[2,8],al:[9,10],alex:4,algebra:10,algorithm:[3,4,9,10],alia:10,all:[2,3,4,6,9,10,12,30,32,34,48,51],allclos:[2,3],allen1984:10,allen:10,alloc:[1,2,3,5,9],allow:[1,2,9,10],allow_tf32:26,along:[1,3,30,32,35,36,48,52],also:[1,2,3,4,5,9,10],altern:4,alwai:[10,49],amd:9,amen:10,amount:[5,9],ampl:10,an:[1,2,3,4,9,10,11,16,17,18,19,20,21,22,23,37,38,39,40],analog:1,analysi:[9,10],analyz:10,ancourt1991:10,ancourt:10,ani:[1,2,3,10,12,13,51],anoth:[2,10],anytim:12,apart:10,apex:5,apex_layer_norm:5,api:51,appear:51,appli:[3,4,5,9,10,16,17,19,20,21,22,23],applic:[4,10,13],approach:[9,10],appropri:1,approxim:2,ar:[0,1,2,3,4,9,10,11,12,14,28,34,47,49,51],arang:[1,2,3,4,5],arbitrari:3,architectur:[3,9],area:10,arg:[1,2,3,5,11,13,14,51],argument:[1,2,3,11,12,13,14,49,51],arrai:[10,50],arrang:3,art:[9,10],artifici:4,arxiv:[9,10],ask:2,aspect:10,asplo:9,assert:[1,2,3,4,5],assert_almost_equ:5,assum:[2,51],asynchron:[1,9],atom:[16,17,18,19,20,21,22,23],auguin1983:9,auguin:9,auto:[2,3,10,11,12,13],autograd:5,autom:9,automat:[2,3,9,10,11],autotun:[3,10],avail:[0,4,9,10],avoid:[2,12,49],awar:9,awkward:4,axi:[1,2,3,4,5,30,32,35,36,48,51],b:[3,9,10],b_ptr:3,back:[1,2,3,4,5],backpropag:4,backward:5,bad:4,baghdadi2021:[9,10],baghdadi:[9,10],balanc:10,bandwidth:2,base:[4,8,9,10],basic:[1,6,10],becom:9,been:[1,9,10],befor:[3,11,12,16,17,18,19,20,21,22,23],begin:10,behavior:[10,12],being:[2,4],believ:10,below:[4,6,10],bench:[0,12],bench_layer_norm:5,benchmark:[0,5,52,53],benefit:[2,9,10],best:[1,9],between:[1,9],bfloat16:26,bia:5,bit:4,block:[1,2,3,4,9,10,16,17,18,19,20,21,22,23,24,25,26,27,28,29,31,33,37,38,39,40,41,43,44,45,46,47,49],block_siz:[1,2,4,5,10,12,13],block_size_k:3,block_size_m:[3,5],block_size_n:[3,5],block_start:[1,4],blue:[1,2,3,5],boil:10,bool:[49,51],both:[10,49],bound:[1,2,3,10],branch:10,broad:9,broadcast:[24,28,47,49],build:[0,3],built:[1,10],c:[3,9,10],c_mask:3,c_ptr:3,cach:[9,10,28],cache_modifi:28,call:[1,3,10,11,14,38],callabl:[1,13,14,52],can:[0,1,2,3,4,9,10,12,53],cannot:[3,9,10],capabl:[8,9],cd:[0,6],cdiv:[1,3,4,5],ceil:13,certain:13,cgo:[9,10],challeng:4,chang:[3,4,12,28],chapter:8,characterist:10,cheap:9,check:[3,8],checkpoint:4,chen2018:9,chen:9,chip:2,choic:8,click:[1,2,3,4,5],clone:[0,5],close:10,cmake:0,cmp:18,coalesc:9,code:[1,2,3,4,5,6,9,10],col:[3,5,10],col_offset:2,color:51,column:[2,3],com:[0,5],combin:9,come:[2,3,10],command:0,common:10,commonli:10,compar:[2,3,4,5,8,10,18],compat:26,compil:[2,3,8,9,11,14,34],complet:10,complex:10,compos:[4,9],composit:10,comprehens:[9,10],comput:[4,5,8,9,10,13,25,27,29,31,33,43,44,45,46],computation:[9,10],concern:10,concis:[1,51],condit:[10,49],config:[3,5,12],configur:[3,11,12,53],confirm:2,connectom:9,consecut:10,consequ:9,consid:2,consist:4,constexpr:[1,2,3,4,5,37,38,39,40,45],constraint:[3,10],construct:9,constructor:51,consum:3,contain:[10,18,51],contextu:10,contigu:[3,15,41],control:[9,10],conveni:3,convert:[1,3,14],convolut:9,cooper:11,copi:[4,9,18],core:[9,10,37,38,39,40,45],correct:1,correspond:[1,2,3,51],cosin:25,cost:10,could:[2,10],cours:9,cpython:0,creat:[1,2,3,5,9],crucial:4,csv:1,ctx:5,cubla:[3,9],cuda:[1,2,3,4,5,9],cudnn:9,current:36,custom:[1,2,3,8],cut:3,cvpr:9,d:[2,4,12,14],da:5,dart:10,darte1999:10,data:[1,3,4,5,9,10,16,17,18,19,20,21,22,23,28,49,50],data_ptr:14,dataflow:10,david:4,db:5,db_ref:5,db_tri:5,dbia:5,deal:4,decad:9,decim:5,declar:1,decompos:10,decor:[1,3,12,13,14],decreas:4,dedic:3,deep:[3,4,9,10],def:[1,2,3,4,5,12,13],defin:[1,2,3,10,28],definit:10,denomin:2,denot:1,dens:10,depend:[0,6,10,49],deploi:9,describ:[4,10],design:10,desir:[24,42],detail:[3,10],detect:9,develop:[9,10],devic:[1,2,3,5],dg:5,dialect:10,dict:[12,13],dictionari:[11,13],diesel:10,differ:[1,2,3,4,9,10,12,51],difficult:10,difficulti:[3,9],dijkstra82:10,dijkstra:10,dim:[2,10],dimens:[3,26,30,32,48],dimension:[3,10,26],dir:0,direct:3,disjoint:10,disk:1,dissert:10,distribut:[2,4,10],divis:3,dnn:[8,9,10],do_bench:[1,2,3,5],doc:4,doe:[1,2,3,10],doesn:10,domain:[9,10],don:[1,2,3],done:[3,9,30,32,48],dot:3,doubli:3,doubt:10,dout:5,down:[3,10],download:[0,1,2,3,4,5,6],dram:[1,2],dropout:[6,7],dror:4,dsl:[8,9,10],dtype:[1,2,3,5,16,17,18,19,20,21,22,23,28,47,50],dw:5,dw_ref:5,dw_tri:5,dweight:5,dx:5,dx_ref:5,dx_tri:5,dy:5,e:[0,2,3,4,6,9,10,50],each:[1,2,3,4,9,10,11,13],earli:12,early_config_prun:12,eas:10,easi:[3,4],easier:[1,2,9],easili:3,ed:[1,3],education:2,effect:10,effici:[3,4,9,39],effort:10,eg:12,either:[1,35,36,49],elango2018:10,elango:10,element:[1,2,3,4,5,25,27,29,30,31,32,33,43,44,45,46,47,48,49,51],element_s:[2,5],element_ti:[16,17,18,19,20,21,22,23,28,47],elementwis:[2,28],els:[3,5],emerg:9,empti:[3,5],empty_lik:[1,2,4,5],enabl:10,encod:10,encourag:4,end:[9,10,15],enforc:10,engin:10,enqueu:[1,2,5],ensur:10,entir:10,entri:39,environ:8,ep:5,equal:10,error:3,especi:9,et:[4,9,10],euromicro:9,evalu:[3,4,12,49],even:[4,10],evict_first:5,evict_last:5,eviction_polici:[5,28],evidenc:9,evolv:9,exampl:[1,2,3,4,5,6,9,10,11],except:5,exchang:22,execut:[7,9,10,11,53],exist:[9,10],exp:2,expect:[2,18],expens:[9,10,13],explor:[4,9],exponenti:[2,27],express:[9,10],extend:[3,4],extra:1,extras_requir:5,extrem:10,f:[1,2,3,10],facilit:[9,10],fact:10,fairli:3,fals:[5,16,17,19,20,21,22,23,28,45,47,49,51,52],far:2,fast:[2,9,10],faster:[2,38],fastest:10,featur:5,feel:3,fetch:9,few:10,field:[9,12],figur:10,file:[1,2,3,7],fill:50,fine:4,first:[1,3,4,8,10,26,31,33],first_pid_m:3,firstli:4,fit:2,fix:51,flag:2,flatten:41,flexibl:9,float16:[3,5,26,50],float32:[1,2,3,4,5,26,37,40],flow:[9,10],fly:4,fn:[14,52],focu:[3,10],folder:4,follow:[0,2,3,8,9,10],footprint:4,forc:4,forget:1,formal:10,format:10,forward:5,found:18,foundat:10,four:39,fp16:3,fp32:3,frac:4,framework:[9,10],free:3,from:[1,2,3,4,9,10,28,49],full:[1,2,3,4,5],fulli:10,func:10,fundament:10,further:[4,10],fuse:[3,5,6,7],fusedlayernorm:5,fusion:[2,10],g:[3,4,9,10,50],galleri:[1,2,3,4,5,6],gb:[1,2,5],gbp:[1,2,5],gener:[1,2,3,4,5,6,9,10,37,38,39,40,51],geoffrei:4,geq:10,get:[1,2,3,4,7],girbal2006:10,girbal:10,git:0,github:[0,5],give:9,given:[2,3,4,24,35,36,37,38,39,40,42,50],global:10,go:[1,3,10],good:[1,10],gpgpu:9,gpu:[1,2,4,8,9,10,11,14],grad:5,grad_scale_gain_bias_nam:5,grad_scale_nam:5,grad_to_non:[5,52],gradient:52,grammat:10,graphic:9,greater:2,green:[1,2,3,5],grid:[1,2,3,4,5,35,36],grid_m:3,grid_n:3,grosser2012:10,grosser:10,group:3,group_id:3,group_m:3,group_size_m:3,grow:10,guard:[1,2],guid:9,ha:[1,3,4,9,10,35,36],had:1,halid:[9,10],hand:10,handl:[1,2,4,10],handwritten:9,hard:3,harder:10,hardwar:[3,8,10],has_apex:5,hasattr:5,hasn:1,have:[2,4,9,10,14,26,49,51],heavi:9,helper:[1,2],henc:3,here:[1,2,3,4,5],heurist:[2,5],hierarch:9,hierarchi:10,high:[3,9,10],higher:3,highli:9,highlight:10,hint:10,hinton:4,hit:3,how:[1,2,3,8,9,13],howev:[2,10],html:4,http:[0,4,5],i:[1,2,3,4,5,9,10],id:[3,36],idea:9,ideal:2,ident:2,identifi:1,idx:[16,17,19,20,21,22,23,28,47],ieee_round:45,ilya:4,imag:[9,10],implement:[1,2,3,4,9,10],implicitli:[1,14,28,47],importantli:10,impos:10,improv:[3,4],incompat:[3,10],incorrect:3,increas:[1,2,3,4],incred:9,increment:10,inde:10,independ:[2,10],index:1,indic:[10,49],induc:10,industri:9,inequ:10,inf:2,inform:10,infrastructur:10,initi:[1,3],inner:[3,26],inplac:3,input:[1,2,3,4,5,10,12,13,24,25,26,27,29,30,31,32,33,34,41,42,43,44,45,46,48],input_ptr:2,input_row_strid:2,instal:[6,8],instanc:[1,2,3,4,9,11,35,36],instanti:4,instead:[2,49],instruct:[8,9],int1:[16,17,19,20,21,22,23,28,47],int32:[4,38,39],integ:10,interchang:10,interest:[9,10],intermedi:10,intern:[2,10],interv:15,intrins:10,introduc:4,introduct:8,invari:[2,10],invoc:4,ipynb:[1,2,3,4,5],ir:10,irregular:[2,10],is_contigu:[3,4,5],is_cuda:1,isn:3,issu:[9,10],iter:[3,9,10],its:[1,2,3,10,12],j:[3,9,10],jit:[1,2,3,4,5,12,13],jmlr:4,john:4,johnson:4,journal:10,jrk2013:9,jupyt:[1,2,3,4,5,6],just:[3,10,13],k:[3,4,9,10],kb:9,keep:4,kei:[3,9,12],kellei:9,kernel:[4,5,8,9,11,12,13],keyword:[1,11],ki:10,kind:2,know:34,known:10,krizhevski:4,kwarg:[11,14],label:[1,2,3,51],lam1991:9,lam:9,lambda:[1,2,3,4,5,13],languag:[1,2,3,4,5,8,9,14],larg:[9,10],last:3,later:[2,10],latest:0,lattner2004:10,lattner2019:10,lattner:10,launch:[1,2,3,35,36],law:10,layer:[6,7,9,10],layer_norm:5,layernorm:5,lead:[4,9,10],leaky_relu:3,leakyrelu:3,learn:[1,2,3,4,8,9,10],least:10,lee2017:9,lee:9,left:10,legal:10,length:1,less:[4,5,9,10],let:[1,2,4,34],letter:10,level:[3,9,10],li:9,librari:[0,3,9,10],lifelong:10,like:[1,4,9,10,38],limit:[2,4],lindenstrauss:4,line:[1,2,3,4,10,51],line_arg:[1,2,3,5,51],line_nam:[1,2,3,5,51],line_v:[1,2,3,5,51],linear:[9,10],link:0,list:[1,3,12,13,51,52,53],litteratur:10,ll:4,llvm11:0,llvm:[0,10],load:[1,2,3,4,5,10,49],local:[9,10],locat:[3,16,17,18,19,20,21,22,23,28,47],log2:13,log:51,logarithm:[1,29],logic:[17,21,23],look:[4,8,9],loop:[3,10,11],low:[6,7,10],m:[0,2,3,5,9],machin:[9,10],machineri:[9,10],made:9,mai:[2,10,13],main:[3,9,10],maintain:[2,10],major:[3,10],make:[1,2,9,10],manag:[4,9],mani:[9,10],manual:[2,10],manual_se:[1,2,3,5],map:3,mapl:10,mark:[4,53],markedli:9,mask:[1,2,3,4,5,16,17,19,20,21,22,23,28,47,49],match:[3,18],math:13,mathbb:10,mathbf:10,mathcal:[10,40],mathemat:10,matmul:[3,10],matmul_kernel:3,matric:[2,3],matrix:[2,4,6,7,9,10,11,26],matrix_s:10,matter:[3,9,10],max:[1,2,5,19],max_fused_s:5,max_m:[1,2,3,5],maxim:[8,10,39],maximum:[1,2,30],mb:[7,9],mean1:5,mean2:5,mean:[3,5,10,12],mechan:[2,10],median:52,memori:[1,2,3,6,7,9,10,16,17,18,19,20,21,22,23,28,47,49],mention:3,meta:[1,2,3,4,5,11,12,13],metaparamet:1,method:[10,11,14,51,53],methodolog:10,micro:9,min:[3,5,20],min_m:[1,2,3,5],minimum:32,minut:[1,2,3,4,5],miss:10,mitig:10,ml:9,mlir:10,mn:2,mode:5,model:[1,9,10,12],modern:[3,8,9,10],modular:10,modulenotfounderror:5,moor:10,mora:4,more:[2,3,4,8,9,10,51],most:[3,10],mostli:11,move:3,movement:4,ms:[1,2,3,5,52],much:[2,3],mullapudi2016:10,mullapudi:10,multi:[3,9,10],multipl:[1,4,6,7,9,10,11,12,34,38],multipli:[3,4,5,10,26],must:[2,3,15,26,49],n:[2,3,5,9,40],n_col:2,n_element:[1,4],n_round:[37,38,39,40],n_row:2,naiv:[2,4],naive_softmax:2,name:[1,2,3,12,13,51],nativ:[1,2,3],natur:[2,9,29],nb:9,necessari:2,need:[1,2,3,4,38],nelement:2,nest:[3,10],net:10,network:[4,9,10],neural:[4,9,10],neurosci:9,never:4,next:[2,3],next_power_of_2:[2,5],nightli:0,nip:9,nitish:4,nn:[3,5],non:9,none:[2,3,5,11,12,16,17,19,20,21,22,23,28,47,51,52],nonzero:49,norm:[4,5,7],normal:[2,6,7],normalized_shap:5,note:[0,1,2,3,4,10,12,14,49],notebook:[1,2,3,4,5,6],notic:[2,10],notori:[3,9],novel:9,now:[1,3],num_pid_in_group:3,num_pid_m:3,num_pid_n:3,num_stag:[3,11,12],num_warp:[2,3,5,11,12],number:[1,2,3,4,5,10,11,12,35,37,38,39,40],numcol:5,numel:[1,4,5],numer:[2,9],numrow:5,nvidia:[5,9,28],o:[2,4],object:[1,3,9,11,12,14,16,17,18,19,20,21,22,23],obtain:1,obvious:2,occur:10,off:5,offer:9,offici:0,offs_am:3,offs_bn:3,offs_cm:3,offs_cn:3,offs_k:3,offset:[1,4,37,38,39,40],often:3,omega:10,onc:[2,9,10],one:[2,3,4,6,9,10,51],onli:[2,3,4,9,10,14],op:[1,2],open:15,openai:0,opencl:9,oper:[1,2,3,4,6,9,16,17,18,19,20,21,22,23,49],opportun:9,opsila:9,optim:[9,10],option:[3,12,16,17,19,20,21,22,23,28,47,51,52],orang:5,order:[2,3,6,10],org:4,origin:10,osdi:9,other:[2,3,4,5,8,10,14,26,28,31,33],otherwis:[4,49],our:[1,2,3,9],out:[1,2,3,4,5,8,10],outlin:10,output2:4,output3:4,output:[1,2,3,4,5],output_ptr:[1,2,4],output_row_start_ptr:2,output_row_strid:2,output_torch:1,output_triton:1,over:[2,4,9,10],overfit:4,overflow:2,own:3,p:[4,10],pa:3,packag:14,pact:10,pad:2,par:3,paradigm:[9,10],paragraph:4,parallel:[1,2,3,4,5,8,9,10,11],paralleliz:9,param:13,paramet:[1,3,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53],parametr:9,part:[3,4,10],partial:5,particular:[2,3],particularli:[9,10],partit:9,pass:[1,5,10,11],past:[9,10],path:1,pattern:9,pb:3,peak:10,per:[2,4,5],percentil:52,perf:3,perf_model:12,perf_report:[1,2,3,5,51],perform:[1,2,4,9,10,12,16,17,18,19,20,21,22,23,52],persist:4,person:10,perspect:10,phase:10,philosophi:10,philox:[4,39],pid:[1,3,4,5],pid_m:3,pid_n:3,pip:[0,6],pipelin:[9,10,11],platform:[8,10],pldi:9,plot:[0,1,2,3,51],plot_nam:[1,2,3,5,51],pmatrix:10,point:[1,10,39],pointer:[1,2,4,14,16,17,18,19,20,21,22,23,28,47],pointerdtyp:[16,17,18,19,20,21,22,23,28,47],polli:10,polyhedr:9,polyhedra:10,popular:10,portabl:[9,10],pose:9,posit:[5,13],possibl:[1,2,3,10,11],power:[2,4,10,13,15],ppopp:10,practic:[1,2,3,9],pragma:9,pre:[0,9],pre_hook:11,prealloc:1,predic:12,predict:10,prefer:2,premis:9,present:0,preserv:10,preserve_rng_st:4,prevent:[4,10],primer:10,primit:[9,14],principl:10,print:[1,3,4],print_data:[1,2,3,5],prng:4,probabl:[4,10],problem:1,problemat:10,procedur:10,process:[1,5,9,10],processor:9,produc:[3,4],product:[8,10,26],program:[1,2,3,4,5,8,9,35,36],program_id:[1,2,3,4,5],programm:[9,10],prohibitev:13,project:[4,9],promot:[3,10],properli:2,properti:10,propos:9,proprietari:3,provid:[1,2,3,4,5,8,10,12,30,32,48,52],prune:[4,12],prune_configs_bi:12,pseudo:[3,4,39],pseudorandom:4,ptr:3,ptx:28,purpos:[9,10],push:10,put:4,py:[0,1,2,3,4,5,7],pypi:[0,5],pytest:0,python:[1,2,3,4,5,6,14],pytorch:[1,2,4],qquad:10,r:2,ragan:9,rand:[1,4,5],randint4x:38,randn:[2,3,4,5],randn_lik:5,random:[4,37,38,39,40],randomli:4,rang:[1,2,3,5,9,10],rapidli:[9,10],rate:3,rather:9,raw:1,rdom:10,re:[1,3],read:[2,3,6],reader:10,real:9,reason:10,recent:9,recommend:6,recomput:[4,9],record_clock:52,rectifi:9,redmon2016:9,redmon:9,reduct:[2,5,30,32,48],refer:1,regardless:[4,49],regim:4,regrett:9,regular:[4,10],rel:[1,10],relat:8,releas:[0,9],reli:10,relu:3,remain:[9,51],rememb:3,reorder:10,rep:[5,52],repetit:52,repres:[2,3,10,11],requir:[2,4,10],requires_grad:5,requires_grad_:5,research:[9,10],reset:[12,52],reset_to_zero:12,reshap:5,resolut:10,resourc:9,resp:10,respect:10,restrict:10,result:[0,1,2,9,10],ret:2,retain_graph:5,retriev:10,reus:3,revisit:9,right:10,rise:10,role:10,ron:4,root:46,roughli:3,row:[2,3,4,5],row_idx:2,row_minus_max:2,row_start_ptr:2,rstd:5,run:[0,1,2,3,4,5,8,10,12,14,53],runtim:[10,52],ruslan:4,rvar:10,s:[1,2,4,5,10,39],said:10,salakhutdinov:4,salmon2011:4,salmon:4,same:[4,9,51],sato2019:10,sato:10,save:[1,2,3],save_for_backward:5,save_path:[1,5],saved_tensor:5,sc:10,scalabl:10,scalar:[4,9,26,37,38,39,40,50],scale:51,scan:10,schedul:9,scienc:10,scientif:10,scop:10,scope:10,script:[0,1,2,3,4,5],second:[1,2,3,4,5,10,26,31,33],secondli:4,section:[3,10],see:[1,2,3,4,10],seed:[37,38,39,40],seeded_dropout:4,seem:[1,10],select:[9,10,49],self:[11,51],semant:10,semi:10,sens:[1,9,10],separ:[5,10],sequenc:9,set:[1,4,10],setup:[0,5],sever:[9,10],shall:10,shape:[1,2,3,4,5,10,24,28,42,47,49,50],share:9,shaw:4,shift:2,should:[1,3,5,9,10,11,30,32,48,51],show_plot:[1,2,3],shown:10,side:10,sight:10,signal:9,significantli:2,sigplan:10,simd:9,simpl:[1,2,3,4],simplest:6,simpli:10,simplic:3,simplifi:4,sinc:[1,2,3],sine:44,singl:[2,4,9,38],size:[1,2,4,10],slower:[9,10],slowest:10,sm80:11,sm:10,smaller:[3,4],smallest:[2,13],snemi3d:9,so:[1,2,3,4,5,10],softmax:[4,6,7],softmax_kernel:2,softmax_output:2,softwar:11,solid:10,solut:3,solv:10,some:3,sometim:10,sourc:[1,2,3,4,5,6,10],space:[9,10],spars:[4,9,10],spatial:10,speak:3,special:9,specif:[3,9],specifi:[10,13,16,17,18,19,20,21,22,23,47],speed:2,sphinx:[1,2,3,4,5,6],split:10,spmd:[1,9,10],sqrt:5,squar:46,sram:[2,3,5],srivastava2014:4,srivastava:4,stabil:2,stabl:0,stage:11,standard:10,start:[6,15],started_tutori:7,state:[4,9,10],statement:10,staticmethod:5,step:10,still:[1,2,3,10],stop:15,store:[1,2,3,4,5,16,17,18,19,20,21,22,23,49],str:[12,13,28,51],straightforward:3,strategi:[4,10],stream:[5,38],strength:9,stride:[2,3,4,5],stride_ak:3,stride_am:3,stride_bk:3,stride_bn:3,stride_cm:3,stride_cn:3,stride_xi:3,stride_xj:3,structur:[9,10],style:[1,2,3,5,51],subscript:10,substanti:9,substract:2,subtract:2,successfulli:10,suffer:10,suit:9,sum:[1,2,5],sum_db:5,sum_dw:5,superhuman:9,support:[4,10],sure:2,surprisingli:9,surround:10,suspicion:2,sutskev:[4,9],sutskever2014:9,swap:18,swizzl:9,synchron:[1,9],system:[0,3,9,10],t:[1,2,3,10],t_:10,tabul:4,taco:10,take:[3,4,8,12,13],taken:10,target:9,techniqu:[9,10],temperatur:4,tempor:10,tend:10,tension:9,tensor:[1,2,3,4,5,9,10,12,14,24,26,28,30,31,32,33,41,42,47,48,49,50,52],tensorrt:9,test:[0,1,5,8],test_layer_norm:5,text:10,tflop:3,th:52,than:[2,3,5,9,10,38,51],thei:[3,9,10],them:1,themselv:3,theoret:2,therebi:10,therefor:3,theta:10,theta_:10,thi:[1,2,3,4,5,9,10,11,12,13,14,39,51],thing:[1,4],think:2,those:2,though:[9,10],thought:10,thread:[2,9,11],through:[6,10],throughout:[10,51],throughput:8,tile:10,time:[0,1,2,3,4,5,9,10,12,38,52],tiramisu:[9,10],tl:[1,2,3,4,5,50],tmp:0,tog:10,togeth:4,tolist:4,top_k:12,topic:10,torch:[1,2,3,4,5,14,52],torch_output:3,torch_relu:3,total:[1,2,3,4,5,7],tradit:[4,9,10],transform:[4,10],travers:10,trend:9,tri:[24,42],trick:2,tricki:4,trigger:[3,12],triton:[0,1,2,3,4,5,6,9,10],triton_output:3,trivial:9,tune:[2,3,10,12,13],tuner:11,tupl:[1,24,42,50],tutori:[1,2,3,4,8],tutorials_jupyt:6,tutorials_python:6,tvm:[9,10],two:[1,2,3,10,12,13,15,26],type:[13,26,28,49,50],typecast:[28,47],typic:10,u:[0,37],un:10,uncommon:10,underneath:10,understand:2,undesir:12,unfortun:[3,10],unifi:9,uniformli:4,unint:49,unit:[0,9],univers:10,unrol:10,up:2,updat:[3,10,12],us:[1,2,3,4,5,9,10,11,12,13,14,38,49,51,53],util:[1,5],v100:10,val:[16,17,18,19,20,21,22,23],valid:1,valu:[1,2,3,4,12,13,15,16,17,18,19,20,21,22,23,25,27,28,29,30,32,34,43,44,45,46,47,48,49,50,51,53],valuabl:2,variabl:[3,11],varianc:5,variant:9,variou:6,vasilach:[9,10],vasilache2018:[9,10],vast:10,vec:10,vector:[4,6,7,9,10],vendor:3,veri:[2,4,10],verif:10,verifi:[2,10],via:10,view:41,visibl:10,vision:9,volatil:28,vs:0,w:10,w_shape:5,wa:4,wai:[2,3,4],want:[2,4,49],warmup:52,warp:[2,5,11],wast:2,wdout:5,we:[1,2,3,4,9,10],weight:5,well:[4,9,10],whatev:12,wheel:0,when:[2,3,4,9,10,11,12,14,49],where:[1,3,4,5,10,13,47],whether:[9,51],which:[1,2,3,4,9,10,12,30,32,48,51],whose:[1,2,3,4,10,12,28],wide:10,wise:[1,2,25,27,29,31,33,43,44,45,46,47],wish:[3,10],within:[3,14,15],without:10,wolf:10,wolfe1989:10,won:2,word:10,work:[2,4,8,9],workload:[3,11],wors:[3,9,10],would:[1,2,4],wouldn:10,wrapper:3,write:[1,2,3,4,5,6,8,10],wrote:2,x:[1,2,3,4,5,10,25,27,29,31,33,41,43,44,45,46,49,51],x_arg:5,x_keep:4,x_keep_ptr:4,x_log:[1,51],x_max:2,x_name:[1,2,3,5,51],x_ptr:[1,4,12,13],x_shape:5,x_size:[12,13],x_val:[1,2,3,5,51],xi:10,xii:10,xlabel:51,xo:10,xor:23,y:[1,2,3,5,10,31,33,49,51],y_fwd:5,y_log:51,y_name:[1,2],y_ptr:1,y_ref:5,y_torch:2,y_tri:5,y_triton:2,year:10,yet:[9,10],yi:10,yield:49,yii:10,ylabel:[1,2,3,5,51],yo:10,you:[0,1,2,3,4,6,9,12,38,49],your:[0,1,8],yourself:[2,3],z:[1,2,10],zero:[3,4,5,12],zip:6},titles:[\"Installation\",\"Vector Addition\",\"Fused Softmax\",\"Matrix Multiplication\",\"Low-Memory Dropout\",\"Layer Normalization\",\"Tutorials\",\"Computation times\",\"Welcome to Triton\\u2019s documentation!\",\"Introduction\",\"Related Work\",\"triton.Config\",\"triton.autotune\",\"triton.heuristics\",\"triton.jit\",\"triton.language.arange\",\"triton.language.atomic_add\",\"triton.language.atomic_and\",\"triton.language.atomic_cas\",\"triton.language.atomic_max\",\"triton.language.atomic_min\",\"triton.language.atomic_or\",\"triton.language.atomic_xchg\",\"triton.language.atomic_xor\",\"triton.language.broadcast_to\",\"triton.language.cos\",\"triton.language.dot\",\"triton.language.exp\",\"triton.language.load\",\"triton.language.log\",\"triton.language.max\",\"triton.language.maximum\",\"triton.language.min\",\"triton.language.minimum\",\"triton.language.multiple_of\",\"triton.language.num_programs\",\"triton.language.program_id\",\"triton.language.rand\",\"triton.language.randint\",\"triton.language.randint4x\",\"triton.language.randn\",\"triton.language.ravel\",\"triton.language.reshape\",\"triton.language.sigmoid\",\"triton.language.sin\",\"triton.language.softmax\",\"triton.language.sqrt\",\"triton.language.store\",\"triton.language.sum\",\"triton.language.where\",\"triton.language.zeros\",\"triton.testing.Benchmark\",\"triton.testing.do_bench\",\"triton.testing.perf_report\",\"triton\",\"triton.language\",\"triton.testing\"],titleterms:{\"final\":3,addit:1,advantag:10,algebra:55,api:8,arang:15,arithmet:3,atom:55,atomic_add:16,atomic_and:17,atomic_ca:18,atomic_max:19,atomic_min:20,atomic_or:21,atomic_xchg:22,atomic_xor:23,autotun:12,baselin:4,benchmark:[1,2,3,51],binari:0,broadcast_to:24,cach:3,challeng:9,co:25,comparison:55,compil:[10,55],comput:[1,2,3,7],config:11,creation:55,distribut:0,do_bench:52,document:8,dot:26,dropout:4,exercis:4,exp:27,from:0,further:8,fuse:2,gener:55,get:8,go:8,heurist:13,hint:55,index:55,instal:0,introduct:9,jit:14,kernel:[1,2,3],l2:3,languag:[10,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,55],layer:5,limit:10,linear:55,load:28,log:29,low:4,manipul:55,math:55,matrix:3,max:30,maximum:31,memori:[4,55],min:32,minimum:33,model:55,motiv:[2,3,9],multipl:3,multiple_of:34,normal:5,num_program:35,number:55,op:55,optim:3,packag:0,perf_report:53,perform:3,pointer:3,polyhedr:10,program:[10,55],program_id:36,python:[0,8],rand:37,randint4x:39,randint:38,randn:40,random:55,ravel:41,reduct:55,refer:[4,9,10],relat:10,represent:10,reshap:42,result:3,s:8,schedul:10,seed:4,shape:55,sigmoid:43,sin:44,softmax:[2,45],sourc:0,sqrt:46,squar:3,start:8,store:47,sum:48,test:[2,3,51,52,53,56],time:7,triton:[8,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56],tutori:6,unit:[2,3],vector:1,welcom:8,where:49,work:10,zero:50}})\n\\ No newline at end of file\n+Search.setIndex({docnames:[\"getting-started/installation\",\"getting-started/tutorials/01-vector-add\",\"getting-started/tutorials/02-fused-softmax\",\"getting-started/tutorials/03-matrix-multiplication\",\"getting-started/tutorials/04-low-memory-dropout\",\"getting-started/tutorials/05-layer-norm\",\"getting-started/tutorials/06-fused-attention\",\"getting-started/tutorials/07-libdevice-function\",\"getting-started/tutorials/index\",\"getting-started/tutorials/sg_execution_times\",\"index\",\"programming-guide/chapter-1/introduction\",\"programming-guide/chapter-2/related-work\",\"python-api/generated/triton.Config\",\"python-api/generated/triton.autotune\",\"python-api/generated/triton.heuristics\",\"python-api/generated/triton.jit\",\"python-api/generated/triton.language.arange\",\"python-api/generated/triton.language.atomic_add\",\"python-api/generated/triton.language.atomic_and\",\"python-api/generated/triton.language.atomic_cas\",\"python-api/generated/triton.language.atomic_max\",\"python-api/generated/triton.language.atomic_min\",\"python-api/generated/triton.language.atomic_or\",\"python-api/generated/triton.language.atomic_xchg\",\"python-api/generated/triton.language.atomic_xor\",\"python-api/generated/triton.language.broadcast_to\",\"python-api/generated/triton.language.cos\",\"python-api/generated/triton.language.dot\",\"python-api/generated/triton.language.exp\",\"python-api/generated/triton.language.load\",\"python-api/generated/triton.language.log\",\"python-api/generated/triton.language.max\",\"python-api/generated/triton.language.maximum\",\"python-api/generated/triton.language.min\",\"python-api/generated/triton.language.minimum\",\"python-api/generated/triton.language.multiple_of\",\"python-api/generated/triton.language.num_programs\",\"python-api/generated/triton.language.program_id\",\"python-api/generated/triton.language.rand\",\"python-api/generated/triton.language.randint\",\"python-api/generated/triton.language.randint4x\",\"python-api/generated/triton.language.randn\",\"python-api/generated/triton.language.ravel\",\"python-api/generated/triton.language.reshape\",\"python-api/generated/triton.language.sigmoid\",\"python-api/generated/triton.language.sin\",\"python-api/generated/triton.language.softmax\",\"python-api/generated/triton.language.sqrt\",\"python-api/generated/triton.language.store\",\"python-api/generated/triton.language.sum\",\"python-api/generated/triton.language.where\",\"python-api/generated/triton.language.zeros\",\"python-api/generated/triton.testing.Benchmark\",\"python-api/generated/triton.testing.do_bench\",\"python-api/generated/triton.testing.perf_report\",\"python-api/triton\",\"python-api/triton.language\",\"python-api/triton.testing\"],envversion:{\"sphinx.domains.c\":2,\"sphinx.domains.changeset\":1,\"sphinx.domains.citation\":1,\"sphinx.domains.cpp\":4,\"sphinx.domains.index\":1,\"sphinx.domains.javascript\":2,\"sphinx.domains.math\":2,\"sphinx.domains.python\":3,\"sphinx.domains.rst\":2,\"sphinx.domains.std\":2,\"sphinx.ext.intersphinx\":1,sphinx:56},filenames:[\"getting-started/installation.rst\",\"getting-started/tutorials/01-vector-add.rst\",\"getting-started/tutorials/02-fused-softmax.rst\",\"getting-started/tutorials/03-matrix-multiplication.rst\",\"getting-started/tutorials/04-low-memory-dropout.rst\",\"getting-started/tutorials/05-layer-norm.rst\",\"getting-started/tutorials/06-fused-attention.rst\",\"getting-started/tutorials/07-libdevice-function.rst\",\"getting-started/tutorials/index.rst\",\"getting-started/tutorials/sg_execution_times.rst\",\"index.rst\",\"programming-guide/chapter-1/introduction.rst\",\"programming-guide/chapter-2/related-work.rst\",\"python-api/generated/triton.Config.rst\",\"python-api/generated/triton.autotune.rst\",\"python-api/generated/triton.heuristics.rst\",\"python-api/generated/triton.jit.rst\",\"python-api/generated/triton.language.arange.rst\",\"python-api/generated/triton.language.atomic_add.rst\",\"python-api/generated/triton.language.atomic_and.rst\",\"python-api/generated/triton.language.atomic_cas.rst\",\"python-api/generated/triton.language.atomic_max.rst\",\"python-api/generated/triton.language.atomic_min.rst\",\"python-api/generated/triton.language.atomic_or.rst\",\"python-api/generated/triton.language.atomic_xchg.rst\",\"python-api/generated/triton.language.atomic_xor.rst\",\"python-api/generated/triton.language.broadcast_to.rst\",\"python-api/generated/triton.language.cos.rst\",\"python-api/generated/triton.language.dot.rst\",\"python-api/generated/triton.language.exp.rst\",\"python-api/generated/triton.language.load.rst\",\"python-api/generated/triton.language.log.rst\",\"python-api/generated/triton.language.max.rst\",\"python-api/generated/triton.language.maximum.rst\",\"python-api/generated/triton.language.min.rst\",\"python-api/generated/triton.language.minimum.rst\",\"python-api/generated/triton.language.multiple_of.rst\",\"python-api/generated/triton.language.num_programs.rst\",\"python-api/generated/triton.language.program_id.rst\",\"python-api/generated/triton.language.rand.rst\",\"python-api/generated/triton.language.randint.rst\",\"python-api/generated/triton.language.randint4x.rst\",\"python-api/generated/triton.language.randn.rst\",\"python-api/generated/triton.language.ravel.rst\",\"python-api/generated/triton.language.reshape.rst\",\"python-api/generated/triton.language.sigmoid.rst\",\"python-api/generated/triton.language.sin.rst\",\"python-api/generated/triton.language.softmax.rst\",\"python-api/generated/triton.language.sqrt.rst\",\"python-api/generated/triton.language.store.rst\",\"python-api/generated/triton.language.sum.rst\",\"python-api/generated/triton.language.where.rst\",\"python-api/generated/triton.language.zeros.rst\",\"python-api/generated/triton.testing.Benchmark.rst\",\"python-api/generated/triton.testing.do_bench.rst\",\"python-api/generated/triton.testing.perf_report.rst\",\"python-api/triton.rst\",\"python-api/triton.language.rst\",\"python-api/triton.testing.rst\"],objects:{\"triton.Config\":{__init__:[13,1,1,\"\"]},\"triton.language\":{arange:[17,2,1,\"\"],atomic_add:[18,2,1,\"\"],atomic_and:[19,2,1,\"\"],atomic_cas:[20,2,1,\"\"],atomic_max:[21,2,1,\"\"],atomic_min:[22,2,1,\"\"],atomic_or:[23,2,1,\"\"],atomic_xchg:[24,2,1,\"\"],atomic_xor:[25,2,1,\"\"],broadcast_to:[26,2,1,\"\"],cos:[27,2,1,\"\"],dot:[28,2,1,\"\"],exp:[29,2,1,\"\"],load:[30,2,1,\"\"],log:[31,2,1,\"\"],max:[32,2,1,\"\"],maximum:[33,2,1,\"\"],min:[34,2,1,\"\"],minimum:[35,2,1,\"\"],multiple_of:[36,2,1,\"\"],num_programs:[37,2,1,\"\"],program_id:[38,2,1,\"\"],rand:[39,2,1,\"\"],randint4x:[41,2,1,\"\"],randint:[40,2,1,\"\"],randn:[42,2,1,\"\"],ravel:[43,2,1,\"\"],reshape:[44,2,1,\"\"],sigmoid:[45,2,1,\"\"],sin:[46,2,1,\"\"],softmax:[47,2,1,\"\"],sqrt:[48,2,1,\"\"],store:[49,2,1,\"\"],sum:[50,2,1,\"\"],where:[51,2,1,\"\"],zeros:[52,2,1,\"\"]},\"triton.testing\":{Benchmark:[53,0,1,\"\"],do_bench:[54,2,1,\"\"],perf_report:[55,2,1,\"\"]},\"triton.testing.Benchmark\":{__init__:[53,1,1,\"\"]},triton:{Config:[13,0,1,\"\"],autotune:[14,2,1,\"\"],heuristics:[15,2,1,\"\"],jit:[16,2,1,\"\"]}},objnames:{\"0\":[\"py\",\"class\",\"Python class\"],\"1\":[\"py\",\"method\",\"Python method\"],\"2\":[\"py\",\"function\",\"Python function\"]},objtypes:{\"0\":\"py:class\",\"1\":\"py:method\",\"2\":\"py:function\"},terms:{\"0\":[1,2,3,4,5,6,7,9,11,12,37,38,39,42,52,54],\"00\":9,\"0000\":3,\"000000\":2,\"000002\":2,\"000013\":5,\"002605\":5,\"004273\":1,\"007961\":2,\"01\":[1,3,9],\"011801\":3,\"012395\":2,\"02\":[2,9],\"020\":[1,9],\"023256\":5,\"0249\":7,\"025776\":3,\"028308\":3,\"03\":[3,9],\"031509\":5,\"035084\":3,\"04\":[4,9],\"0424\":7,\"05\":[5,9],\"05682v2\":6,\"06\":[6,9],\"060049\":3,\"0625\":3,\"069806\":5,\"07\":[7,9],\"072\":[6,9],\"08199\":4,\"08452\":4,\"084721\":1,\"089\":[2,9],\"09\":9,\"092307\":5,\"0938\":3,\"095225\":5,\"095241\":5,\"096095\":3,\"096718\":2,\"097543\":2,\"097818\":3,\"098578\":5,\"098894\":5,\"0f\":12,\"0s\":4,\"1\":[1,2,3,4,5,6,10,12,15,37,38,39,42],\"10\":[1,3,4,5,6,7,39,40,41,42],\"100\":[2,6,54],\"1024\":[1,3,4,5,7,14],\"10240\":5,\"1045\":3,\"1048576\":1,\"106434\":4,\"10752\":5,\"11\":[0,1,3,5],\"111095\":5,\"11264\":5,\"114273\":5,\"1151\":5,\"1152\":3,\"11776\":5,\"12\":[1,3,5],\"120002\":3,\"12160\":2,\"12288\":[2,5],\"123\":4,\"12416\":2,\"12544\":2,\"12672\":2,\"127\":1,\"128\":[1,2,3,5,6,14],\"1280\":3,\"12800\":5,\"13\":[1,3,5,9],\"131072\":1,\"1328\":3,\"13312\":5,\"133312\":5,\"133347\":2,\"134217728\":1,\"13686\":4,\"13824\":5,\"138541\":3,\"14\":[1,3,5],\"140799\":3,\"1408\":3,\"14135v2\":6,\"142849\":5,\"142862\":2,\"142883\":5,\"14336\":5,\"147202\":3,\"14848\":5,\"149375\":2,\"149397\":4,\"15\":[1,3,5],\"151\":2,\"1536\":[3,5],\"15360\":5,\"153868\":5,\"154\":2,\"154861\":3,\"154893\":5,\"155572\":3,\"15872\":5,\"16\":[2,3,5,6,12,52],\"160\":2,\"163\":2,\"16384\":1,\"1664\":3,\"167004\":2,\"16777216\":1,\"168771\":3,\"17\":[3,5],\"171410\":5,\"177767\":5,\"17879\":4,\"1792\":3,\"179533\":2,\"18\":[3,5,9],\"180982\":5,\"181817\":2,\"1823\":2,\"186\":2,\"19\":[1,3,5],\"190482\":1,\"192\":1,\"1920\":3,\"194972\":3,\"197315\":3,\"198\":2,\"1982\":12,\"1983\":11,\"1984\":12,\"1989\":12,\"199\":2,\"1991\":[11,12],\"1999\":12,\"1d\":[1,2,3],\"1e\":[1,2,3,5],\"1s\":4,\"2\":[1,2,3,4,5,6,7,10,12,13,15,37,38,54],\"20\":[3,5,6,54],\"200000\":1,\"200001\":3,\"2004\":12,\"2006\":12,\"2011\":4,\"2012\":12,\"2013\":11,\"2014\":[4,11],\"2016\":[11,12],\"2017\":11,\"2018\":[11,12],\"2019\":12,\"2021\":[11,12],\"2048\":[2,3,5,6],\"206126\":5,\"206879\":2,\"2097152\":1,\"21\":[3,5],\"2112\":6,\"212868\":4,\"2141\":1,\"214186\":4,\"216187\":2,\"2176\":3,\"219\":1,\"22\":[3,5],\"220\":3,\"2205\":6,\"222812\":2,\"225175\":5,\"226931\":3,\"228063\":5,\"23\":[3,5],\"2304\":3,\"237267\":5,\"24\":[3,5],\"242181\":3,\"243079\":3,\"2432\":3,\"245\":3,\"246590\":3,\"25\":[3,5,6,54],\"251457\":5,\"254540\":5,\"256\":[1,2,3,5,13],\"2560\":[3,5],\"257735\":3,\"26\":[3,5],\"260869\":3,\"262144\":1,\"2656\":3,\"268057\":5,\"2688\":3,\"27\":[3,5],\"271796\":3,\"276800\":3,\"277\":5,\"279\":[4,9],\"28\":[1,3,5],\"2812\":3,\"2816\":3,\"2891\":3,\"29\":[3,5],\"293429\":4,\"2944\":3,\"297068\":5,\"2d\":[3,5,28],\"2m\":2,\"2mn\":2,\"3\":[0,1,2,3,4,5,6,12],\"30\":3,\"302872\":5,\"305746\":3,\"3072\":[3,5],\"3076\":1,\"31\":3,\"310952\":5,\"3125\":3,\"314362\":3,\"32\":[2,3,5,9,13],\"3200\":3,\"320893\":3,\"323\":5,\"323209\":5,\"32768\":1,\"327881\":5,\"3281\":3,\"33\":3,\"3328\":3,\"333321\":1,\"33554432\":1,\"337\":5,\"337574\":5,\"339\":9,\"34\":3,\"341\":1,\"34172\":4,\"3438\":3,\"3456\":3,\"3477\":3,\"347810\":5,\"35\":3,\"3516\":3,\"3555\":3,\"3584\":[3,5],\"359066\":2,\"36\":3,\"360017\":2,\"360920\":5,\"362\":5,\"362445\":1,\"368435\":5,\"370\":5,\"3712\":3,\"3713\":1,\"371721\":4,\"372\":5,\"372618\":3,\"372800\":3,\"375\":5,\"376\":5,\"378\":5,\"379\":5,\"38\":1,\"380\":5,\"380953\":3,\"381\":5,\"382\":5,\"382307\":3,\"383\":5,\"384\":[2,3,5],\"3840\":3,\"384000\":3,\"384185791015625e\":7,\"386\":5,\"387087\":5,\"389441\":3,\"39\":3,\"390\":5,\"3906\":3,\"391\":5,\"395\":5,\"396\":5,\"396572\":3,\"3968\":3,\"398206\":3,\"3984\":3,\"3986\":4,\"3d\":[37,38],\"3mn\":2,\"4\":[1,2,3,5,6,12,13,14,40],\"40\":3,\"400\":5,\"400001\":1,\"400016\":[1,2],\"401\":5,\"402\":5,\"4023\":3,\"403344\":4,\"403347\":4,\"403381\":5,\"405\":5,\"406\":[2,5],\"4062\":3,\"407\":5,\"408716\":4,\"409\":5,\"4096\":[1,2,3,5,6],\"410\":5,\"4105\":7,\"411\":5,\"412\":2,\"413\":5,\"415\":2,\"415087\":5,\"41576\":4,\"4194304\":1,\"420235\":3,\"420828\":5,\"421\":5,\"421383\":3,\"42142\":4,\"422\":5,\"427505\":5,\"428372\":4,\"428568\":1,\"428801\":3,\"429770\":[1,2],\"430545\":3,\"431739\":5,\"431969\":4,\"433489\":5,\"446623\":3,\"447\":5,\"448255\":1,\"4492\":3,\"45\":3,\"451\":5,\"453\":5,\"4531\":3,\"455\":5,\"4608\":5,\"4609\":3,\"461\":5,\"464755\":3,\"465939\":5,\"466332\":3,\"468\":5,\"4688\":3,\"47\":3,\"471\":5,\"472\":1,\"473602\":3,\"477870\":5,\"48\":6,\"481\":5,\"485074\":3,\"486200\":5,\"49\":3,\"492270\":5,\"492442\":3,\"494\":5,\"4940\":1,\"494120\":3,\"498981\":2,\"4m\":2,\"4x\":2,\"5\":[1,3,4,5,6,12,54],\"50\":[1,9],\"500\":5,\"5000\":3,\"501\":[5,7,9],\"502255\":5,\"504\":5,\"51\":3,\"511\":5,\"512\":[2,3,4,5],\"5120\":5,\"516\":5,\"516136\":5,\"52\":3,\"520\":5,\"524288\":1,\"527536\":5,\"528\":5,\"53\":3,\"531\":5,\"5312\":3,\"532356\":3,\"5351\":7,\"54\":3,\"541\":4,\"542675\":5,\"5430\":7,\"546\":[2,5],\"551037\":5,\"552\":[5,9],\"552988\":3,\"558029\":3,\"5632\":5,\"563555\":3,\"564\":5,\"566038\":2,\"568431\":4,\"57\":3,\"577576\":5,\"584279\":3,\"585\":[2,5],\"5859\":3,\"586858\":4,\"5898\":3,\"59\":3,\"593522\":3,\"596116\":3,\"596744\":3,\"599987\":5,\"599991\":5,\"599997\":5,\"5mn\":2,\"6\":[0,1,3,5],\"600000\":1,\"600010\":5,\"601764\":5,\"604\":5,\"606\":2,\"6094\":3,\"614\":[1,2],\"6144\":5,\"615390\":1,\"62\":3,\"624\":5,\"627194\":5,\"63\":3,\"630\":5,\"631610\":5,\"634232\":5,\"64\":[1,3,6],\"640\":[2,3],\"643199\":3,\"643677\":5,\"645\":5,\"64kb\":5,\"655\":2,\"65536\":[1,5],\"656574\":1,\"66\":3,\"664\":2,\"6656\":5,\"666656\":5,\"668\":5,\"670\":5,\"67086\":4,\"67108864\":1,\"6724\":1,\"674867\":5,\"678\":5,\"68\":3,\"682\":5,\"688\":5,\"69\":3,\"690139\":3,\"694\":5,\"694907\":5,\"6953\":3,\"698\":5,\"699062\":5,\"7\":[0,1,3,5,12],\"700\":5,\"702\":5,\"7031\":3,\"703707\":2,\"706\":2,\"7070\":3,\"707878\":4,\"709294\":5,\"71\":3,\"711310\":3,\"712\":5,\"715711\":3,\"7168\":5,\"719258\":4,\"72\":3,\"722\":[1,2],\"725\":5,\"728\":5,\"73\":3,\"730667\":3,\"734716\":5,\"737435\":1,\"739711\":5,\"743443\":4,\"747477\":3,\"748783\":5,\"75\":3,\"7500\":3,\"76\":1,\"766291\":3,\"768\":[2,3],\"7680\":5,\"768000\":3,\"768213\":3,\"77\":3,\"773130\":3,\"779805\":5,\"78\":3,\"780\":1,\"781\":2,\"784108\":3,\"79719\":4,\"8\":[1,2,3,5,6,12,13,14,52,54],\"80\":[3,54],\"800002\":1,\"803739\":5,\"806694\":4,\"808380\":3,\"81\":3,\"810\":2,\"811\":2,\"812\":[1,2],\"814445\":5,\"814814\":2,\"8149\":7,\"817432\":4,\"8192\":[1,5],\"82\":3,\"822459\":3,\"823517\":[1,2],\"824110\":3,\"825298\":3,\"827\":[3,9],\"83\":3,\"831982\":5,\"833\":1,\"833728\":3,\"838026\":4,\"8388608\":1,\"839992\":2,\"84\":3,\"842\":1,\"84284\":4,\"847\":1,\"847460\":3,\"848\":1,\"849\":1,\"85\":3,\"850\":1,\"854847\":2,\"859062\":5,\"859361\":3,\"86\":3,\"860458\":3,\"87\":3,\"8704\":5,\"873439\":5,\"88\":3,\"882350\":5,\"8828\":3,\"885254\":5,\"8867\":3,\"887417\":5,\"888257\":5,\"89\":3,\"8906\":3,\"8945\":3,\"896\":3,\"898745\":5,\"899428\":3,\"8mn\":2,\"9\":[0,1,2,3,4,5],\"90\":3,\"908470\":3,\"91\":3,\"911572\":5,\"9216\":5,\"921841\":3,\"9219\":3,\"925276\":2,\"929456\":3,\"93\":[2,3],\"932191\":3,\"936606\":2,\"9375\":3,\"94\":[2,3],\"9492\":3,\"95\":[2,3],\"952835\":4,\"9531\":3,\"954739\":3,\"96\":[2,3],\"965524\":5,\"967074\":5,\"9688\":3,\"969090\":5,\"969169\":5,\"969728\":3,\"97\":2,\"971190\":2,\"9728\":5,\"9733\":1,\"978909\":3,\"98\":2,\"9805\":3,\"983276\":3,\"98432\":[1,7],\"9844\":3,\"998493\":3,\"999982\":5,\"999995\":1,\"abstract\":[11,12],\"break\":12,\"byte\":2,\"case\":[1,2,11,12,15,20],\"class\":[2,5,6,11,12,13,53],\"default\":54,\"do\":[2,3,6,11,12,14,18,19,21,22,23,24,25,30,49],\"float\":[2,6,7,11,12,54],\"function\":[1,2,3,4,5,6,8,9,12,13,14,15,16,53,54,55],\"import\":[1,2,3,4,5,6,7,11,12],\"int\":[1,11,12,15,17,26,37,38,44,52,54],\"new\":[26,44,52],\"return\":[1,2,3,4,5,6,14,17,18,19,20,21,22,23,24,25,28,30,32,34,37,38,39,40,41,42,43,50,51,52,54,55],\"static\":[0,11,12],\"super\":3,\"switch\":3,\"true\":[1,2,3,5,6,28,51],\"try\":[3,5,6,7,13],\"var\":[5,12],\"voil\\u00e0\":4,\"while\":[3,11],A:[3,4,5,11,12],And:[0,3],As:[2,3,4,11,12],At:[4,12],But:4,By:54,For:[3,7,11,12,13],If:[4,12,18,19,21,22,23,24,25,40,49,51,53],In:[1,2,3,4,7,12],It:[1,3,4,8,10,12,14,16],NOT:5,Of:11,On:12,One:3,The:[1,2,3,4,7,11,12,18,19,20,21,22,23,24,25,26,28,37,38,39,40,41,42,44,49,51,55],There:1,These:12,To:[1,4,8,11,12,14],_:5,__expf:2,__init__:[13,53],__nv_asin:7,__nv_asinf:7,__nvasinf:7,_a:5,_attent:6,_bwd_kernel:6,_bwd_preprocess:6,_da:5,_dout:5,_dropout:4,_fwd_kernel:6,_layer_norm_bwd_dwdb:5,_layer_norm_bwd_dx_fus:5,_layer_norm_fwd_fus:5,_matmul:3,_mean1:5,_mean2:5,_mean:5,_seeded_dropout:4,_var:5,a100:[3,6,12],a_arg:5,a_hat:5,a_ptr:3,ab:[1,7],abl:12,about:[1,2,3,4,10],abov:[1,2,3,4,12,14],academ:11,acc:[3,6,11,12],acc_scal:6,acceler:11,access:[1,3,11,12,16],accomod:3,accordingli:12,account:12,accumul:[3,6,12],accuraci:[3,11],achiev:[3,11,12],across:[2,4,11,12],activ:3,actual:[3,11,12],ad:5,add:[1,4,5,9,18],add_kernel:1,addit:[2,8,9,11,54],addition:12,address:[11,30],adopt:12,advanc:[2,3,11],advoc:12,affect:3,affin:12,after:3,against:[0,1,2,3,10],aggreg:7,aggress:[11,12],agnost:[11,12],ahead:12,aim:[2,10],al:[6,11,12],alex:4,algebra:12,algorithm:[3,4,6,11,12],alia:12,all:[2,3,4,7,8,11,12,14,32,34,36,50,53],allclos:[2,3],allen1984:12,allen:12,alloc:[1,2,3,5,11],allow:[1,2,11,12],allow_tf32:28,along:[1,3,32,34,37,38,50,54],alpha:6,also:[1,2,3,4,5,7,11,12],altern:4,alwai:[12,51],amd:[6,11],amen:12,amount:[5,11],ampl:12,an:[1,2,3,4,7,11,12,13,18,19,20,21,22,23,24,25,39,40,41,42],analog:1,analysi:[11,12],analyz:12,ancourt1991:12,ancourt:12,ani:[1,2,3,12,14,15,53],anoth:[2,12],anytim:14,apart:12,apex:5,apex_layer_norm:5,api:53,appear:53,appli:[3,4,5,6,7,11,12,18,19,21,22,23,24,25],applic:[4,12,15],approach:[11,12],appropri:1,approxim:2,ar:[0,1,2,3,4,11,12,13,14,16,30,36,49,51,53],arang:[1,2,3,4,5,6,7],arbitrari:3,arc:7,architectur:[3,11],area:12,arg:[1,2,3,5,6,13,15,16,53],argument:[1,2,3,13,14,15,16,51,53],arrai:[12,52],arrang:3,art:[11,12],artifici:4,arxiv:[6,11,12],asin_kernel:7,asinf:7,ask:2,aspect:12,asplo:11,assert:[1,2,3,4,5,6,7],assert_almost_equ:[5,6],assum:[2,53],asynchron:[1,11],atom:[18,19,20,21,22,23,24,25],attent:[8,9],auguin1983:11,auguin:11,auto:[2,3,12,13,14,15],autograd:[5,6],autom:11,automat:[2,3,7,11,12,13],autotun:[3,12],avail:[0,4,7,11,12],avoid:[2,14,51],awar:11,awkward:4,axi:[1,2,3,4,5,6,7,32,34,37,38,50,53],b:[3,11,12],b_ptr:3,back:[1,2,3,4,5,6],backpropag:4,backward:[5,6],bad:4,baghdadi2021:[11,12],baghdadi:[11,12],balanc:12,bandwidth:2,base:[4,7,10,11,12],baseexcept:6,basic:[1,8,12],batch:6,bc:7,becom:11,been:[1,11,12],befor:[3,13,14,18,19,20,21,22,23,24,25],begin:12,behavior:[12,14],being:[2,4],believ:12,below:[4,8,12],bench:[0,14],bench_flash_attent:6,bench_layer_norm:5,benchmark:[0,5,6,54,55],benefit:[2,11,12],best:[1,11],beta:6,between:[1,7,11],bfloat16:28,bia:5,bit:4,block:[1,2,3,4,6,11,12,18,19,20,21,22,23,24,25,26,27,28,29,30,31,33,35,39,40,41,42,43,45,46,47,48,49,51],block_dmodel:6,block_m:6,block_n:6,block_siz:[1,2,4,5,7,12,14,15],block_size_k:3,block_size_m:[3,5],block_size_n:[3,5],block_start:[1,4,7],blue:[1,2,3,5,6],boil:12,bool:[51,53],both:[7,12,51],bound:[1,2,3,12],branch:12,broad:11,broadcast:[26,30,49,51],buffer:6,bug:6,build:[0,3],built:[1,12],bwd:6,c:[3,11,12],c_mask:3,c_ptr:3,cach:[11,12,30],cache_modifi:30,calcul:7,call:[1,3,7,12,13,16,40],callabl:[1,15,16,54],can:[0,1,2,3,4,7,11,12,14,55],cannot:[3,11,12],capabl:[10,11],causal:6,cd:[0,8],cdiv:[1,3,4,5,6,7],ceil:15,certain:15,cgo:[11,12],challeng:4,chang:[3,4,14,30],chapter:10,characterist:12,cheap:11,check:[3,10],checkpoint:4,chen2018:11,chen:11,chip:[2,6],choic:10,click:[1,2,3,4,5,6,7],clone:[0,5,6],close:12,cmake:0,cmp:20,coalesc:11,code:[1,2,3,4,5,6,7,8,11,12],col:[3,5,6,12],col_offset:2,color:53,column:[2,3],com:[0,5,7],combin:11,come:[2,3,12],command:0,common:12,commonli:12,compar:[2,3,4,5,6,10,12,20],compat:28,compil:[2,3,6,10,11,13,16,36],complet:12,complex:12,compos:[4,11],composit:12,comprehens:[11,12],comput:[4,5,6,7,10,11,12,15,27,29,31,33,35,45,46,47,48],computation:[11,12],concern:12,concis:[1,53],condit:[12,51],config:[3,5,6,14],configur:[3,13,14,55],confirm:2,connectom:11,consecut:12,consequ:11,consid:2,consist:4,constexpr:[1,2,3,4,5,6,7,39,40,41,42,47],constraint:[3,6,12],construct:11,constructor:53,consum:3,contain:[12,20,53],contextu:12,contigu:[3,6,17,43],control:[11,12],conveni:3,convert:[1,3,16],convolut:11,cooper:13,copi:[4,11,20],core:[11,12,39,40,41,42,47],correct:[1,7],correspond:[1,2,3,53],cosin:27,cost:12,could:[2,12],cours:11,cpython:0,creat:[1,2,3,5,11],crucial:4,csv:1,ctx:[5,6],cu_seqlen:6,cubla:[3,11],cuda:[1,2,3,4,5,6,7,11],cudnn:11,cumsum:6,current:38,custom:[1,2,3,10],cut:3,cvpr:11,d:[2,4,6,14,16],d_head:6,d_ptr:6,da:5,dao:6,dart:12,darte1999:12,data:[1,3,4,5,6,7,11,12,18,19,20,21,22,23,24,25,30,51,52],data_ptr:16,dataflow:12,david:4,db:5,db_ref:5,db_tri:5,dbia:5,deal:4,decad:11,decim:5,declar:1,decompos:12,decor:[1,3,14,15,16],decreas:4,dedic:3,deep:[3,4,11,12],def:[1,2,3,4,5,6,7,14,15],defin:[1,2,3,12,30],definit:12,delta:6,denom:6,denomin:2,denot:1,dens:12,depend:[0,8,12,51],deploi:11,describ:[4,12],design:12,desir:[26,44],detail:[3,12],detect:11,develop:[11,12],devic:[1,2,3,5,6,7],dg:5,di:6,dialect:12,dict:[14,15],dictionari:[13,15],diesel:12,differ:[1,2,3,4,7,11,12,14,53],difficult:12,difficulti:[3,11],dijkstra82:12,dijkstra:12,dim:[2,6,12],dimens:[3,28,32,34,50],dimension:[3,12,28],dir:0,direct:3,disjoint:12,disk:1,dissert:12,distribut:[2,4,12],divid:6,divis:3,dk:6,dk_ptr:6,dnn:[10,11,12],do_bench:[1,2,3,5,6],do_ptr:6,do_scal:6,doc:[4,7],doe:[1,2,3,12],doesn:12,domain:[11,12],don:[1,2,3],done:[3,11,32,34,50],dot:[3,6],doubl:7,doubli:3,doubt:12,dout:[5,6],down:[3,12],download:[0,1,2,3,4,5,6,7,8],dp:6,dq:6,dq_ptr:6,dram:[1,2],dropout:[8,9],dror:4,ds:6,dsl:[10,11,12],dtype:[1,2,3,5,6,18,19,20,21,22,23,24,25,30,49,52],dv:6,dv_ptr:6,dw:5,dw_ref:5,dw_tri:5,dweight:5,dx:5,dx_ref:5,dx_tri:5,dy:5,e:[0,2,3,4,8,11,12,52],each:[1,2,3,4,11,12,13,15],earli:14,early_config_prun:14,eas:12,easi:[3,4],easier:[1,2,11],easili:3,ed:[1,3],education:2,effect:12,effici:[3,4,11,41],effort:12,eg:14,either:[1,37,38,51],elango2018:12,elango:12,element:[1,2,3,4,5,27,29,31,32,33,34,35,45,46,47,48,49,50,51,53],element_s:[2,5],element_ti:[18,19,20,21,22,23,24,25,30,49],elementwis:[2,30],els:[3,5,6],emerg:11,empti:[3,5,6],empty_lik:[1,2,4,5,6,7],enabl:12,encod:[7,12],encourag:4,end:[11,12,17],enforc:12,engin:12,enqueu:[1,2,5],ensur:12,entir:12,entri:41,environ:10,ep:5,equal:12,error:3,especi:11,et:[4,6,11,12],euromicro:11,evalu:[3,4,14,51],even:[4,12],evict_first:5,evict_last:[5,6],eviction_polici:[5,6,30,49],evidenc:11,evolv:11,exampl:[1,2,3,4,5,6,7,8,11,12,13],except:[5,6],exchang:24,execut:[9,11,12,13,55],exist:[11,12],exp:[2,6],expect:[2,20],expens:[11,12,15],explor:[4,11],exponenti:[2,29],express:[11,12],extend:[3,4],extern:7,extern_lib:7,extra:1,extras_requir:5,extrem:12,f:[1,2,3,6,7,12],facilit:[11,12],fact:12,fairli:3,fals:[5,6,18,19,21,22,23,24,25,28,30,47,49,51,53,54],far:2,fast:[2,11,12],faster:[2,40],fastest:12,featur:5,feel:3,fetch:11,few:12,field:[11,14],figur:12,file:[1,2,3,9],fill:52,fill_valu:6,fine:4,first:[1,3,4,10,12,28,33,35],first_pid_m:3,firstli:4,fit:2,fix:[6,53],flag:2,flash:6,flash_attn:6,flash_attn_func:6,flash_attn_interfac:6,flatten:43,flexibl:11,float16:[3,5,6,28,52],float32:[1,2,3,4,5,6,28,39,42],flow:[11,12],fly:4,fn:[6,16,54],focu:[3,12],folder:4,follow:[0,2,3,10,11,12],footprint:4,forc:4,forget:1,formal:12,format:12,forward:[5,6],found:20,foundat:12,four:41,fp16:3,fp32:3,frac:4,framework:[11,12],free:3,from:[1,2,3,4,6,7,11,12,30,51],full:[1,2,3,4,5,6,7],fulli:12,func:12,fundament:12,further:[4,12],fuse:[3,5,8,9],fusedlayernorm:5,fusion:[2,12],fwd:6,g:[3,4,11,12,52],galleri:[1,2,3,4,5,6,7,8],gb:[1,2,5],gbp:[1,2,5],gener:[1,2,3,4,5,6,7,8,11,12,39,40,41,42,53],geoffrei:4,geq:12,get:[1,2,3,4,9],girbal2006:12,girbal:12,git:0,github:[0,5],give:11,given:[2,3,4,26,37,38,39,40,41,42,44,52],global:12,go:[1,3,12],good:[1,12],gpgpu:11,gpu:[1,2,4,10,11,12,13,16],grad:[5,6],grad_scale_gain_bias_nam:5,grad_scale_nam:5,grad_to_non:[5,54],gradient:54,grammat:12,graphic:11,greater:2,green:[1,2,3,5],grid:[1,2,3,4,5,6,7,37,38],grid_m:3,grid_n:3,grosser2012:12,grosser:12,group:3,group_id:3,group_m:3,group_size_m:3,grow:12,guard:[1,2],guid:[7,11],h:6,ha:[1,3,4,11,12,37,38],had:1,half:6,halid:[11,12],hand:12,handl:[1,2,4,12],handwritten:11,hard:3,harder:12,hardwar:[3,10,12],has_apex:5,has_flash:6,hasattr:5,hasn:1,have:[2,4,6,11,12,16,28,51,53],head:6,heavi:11,helper:[1,2],henc:3,here:[1,2,3,4,5,6,7],heurist:[2,5],hierarch:11,hierarchi:12,high:[3,11,12],higher:3,highli:11,highlight:12,hint:12,hinton:4,hit:3,how:[1,2,3,10,11,15],howev:[2,12],html:[4,7],http:[0,4,5,6,7],i:[1,2,3,4,5,6,11,12],id:[3,38],idea:11,ideal:2,ident:2,identifi:1,idx:[18,19,21,22,23,24,25,30,49],ieee_round:47,ilya:4,imag:[11,12],immedi:6,implement:[1,2,3,4,6,11,12],implicitli:[1,16,30,49],importantli:12,impos:12,improv:[3,4],incompat:[3,12],incorrect:3,increas:[1,2,3,4],incred:11,increment:[6,12],inde:12,independ:[2,12],index:[1,7],indic:[12,51],induc:12,industri:11,inequ:12,inf:[2,6],inform:12,infrastructur:12,initi:[1,3,6],inner:[3,28],inplac:3,input:[1,2,3,4,5,7,12,14,15,26,27,28,29,31,32,33,34,35,36,43,44,45,46,47,48,50],input_ptr:2,input_row_strid:2,instal:[8,10],instanc:[1,2,3,4,11,13,37,38],instanti:4,instead:[2,51],instruct:[10,11],int1:[18,19,21,22,23,24,25,30,49],int32:[4,6,40,41],integ:12,interchang:12,interest:[11,12],intermedi:12,intern:[2,12],interv:17,intrins:12,introduc:4,introduct:10,invari:[2,12],invoc:4,invok:7,ipynb:[1,2,3,4,5,6,7],ir:12,irregular:[2,12],is_contigu:[3,4,5],is_cuda:[1,7],isn:3,issu:[11,12],iter:[3,11,12],its:[1,2,3,12,14],j:[3,5,11,12],jit:[1,2,3,4,5,6,7,14,15],jmlr:4,john:4,johnson:4,journal:12,jrk2013:11,jupyt:[1,2,3,4,5,6,7,8],just:[3,12,15],k:[3,4,6,11,12],k_ptr:6,kb:11,keep:4,kei:[3,11,14],kellei:11,kernel:[4,5,10,11,13,14,15],keyword:[1,13],ki:12,kind:2,know:36,known:12,krizhevski:4,kwarg:[13,16],l:6,l_i:6,l_i_new:6,l_ij:6,l_ptr:6,label:[1,2,3,53],lam1991:11,lam:11,lambda:[1,2,3,4,5,6,7,15],languag:[1,2,3,4,5,6,7,10,11,16],larg:[11,12],last:3,later:[2,12],latest:0,lattner2004:12,lattner2019:12,lattner:12,launch:[1,2,3,37,38],law:12,layer:[8,9,11,12],layer_norm:5,layernorm:5,lead:[4,11,12],leaky_relu:3,leakyrelu:3,learn:[1,2,3,4,10,11,12],least:12,lee2017:11,lee:11,left:12,legal:12,length:[1,6],less:[4,5,11,12],let:[1,2,4,36],letter:12,level:[3,11,12],li:11,libdevic:[8,9],librari:[0,3,11,12],lifelong:12,like:[1,4,6,11,12,40],limit:[2,4],lindenstrauss:4,line:[1,2,3,4,12,53],line_arg:[1,2,3,5,6,53],line_nam:[1,2,3,5,6,53],line_v:[1,2,3,5,6,53],linear:[11,12],link:0,list:[1,3,14,15,53,54,55],litteratur:12,lk:6,ll:4,llvm11:0,llvm:[0,12],lo:6,load:[1,2,3,4,5,6,7,12,51],local:[7,11,12],locat:[3,18,19,20,21,22,23,24,25,30,49],log2:15,log:53,logarithm:[1,31],logic:[19,23,25],look:[4,10,11],loop:[3,6,12,13],low:[8,9,12],lq:6,m:[0,2,3,5,6,11],m_i:6,m_i_new:6,m_ij:6,m_ptr:6,machin:[11,12],machineri:[11,12],made:11,mai:[2,12,15],main:[3,11,12],maintain:[2,12],major:[3,12],make:[1,2,11,12],manag:[4,11],mani:[11,12],manual:[2,12],manual_se:[1,2,3,5,6,7],map:3,mapl:12,mark:[4,6,55],markedli:11,mask:[1,2,3,4,5,7,18,19,21,22,23,24,25,30,49,51],match:[3,20],math:15,mathbb:12,mathbf:12,mathcal:[12,42],mathemat:12,matmul:[3,6,12],matmul_kernel:3,matric:[2,3],matrix:[2,4,8,9,11,12,13,28],matrix_s:12,matter:[3,11,12],max:[1,2,5,6,7,21],max_fused_s:5,max_m:[1,2,3,5],maxim:[5,10,12,41],maximum:[1,2,6,7,32],mb:[9,11],mean1:5,mean2:5,mean:[3,5,6,12,14],mechan:[2,12],median:54,memori:[1,2,3,8,9,11,12,18,19,20,21,22,23,24,25,30,49,51],mention:3,meta:[1,2,3,4,5,7,13,14,15],metaparamet:1,method:[12,13,16,53,55],methodolog:12,micro:11,min:[3,5,22],min_m:[1,2,3,5],minimum:34,minut:[1,2,3,4,5,6,7],miss:12,mitig:12,ml:11,mlir:12,mn:2,mode:[5,6],model:[1,11,12,14],modern:[3,10,11,12],modular:12,modulenotfounderror:5,moment:6,moor:12,mora:4,more:[2,3,4,10,11,12,53],most:[3,12],mostli:13,move:3,movement:4,ms:[1,2,3,5,6,54],much:[2,3],mullapudi2016:12,mullapudi:12,multi:[3,11,12],multipl:[1,4,8,9,11,12,13,14,36,40],multiple_of:6,multipli:[3,4,5,12,28],must:[2,3,17,28,51],n:[2,3,5,11,42],n_col:2,n_ctx:6,n_element:[1,4,7],n_head:6,n_round:[39,40,41,42],n_row:2,naiv:[2,4],naive_softmax:2,name:[1,2,3,14,15,53],nativ:[1,2,3],natur:[2,11,31],nb:11,necessari:2,need:[1,2,3,4,40],nelement:2,nest:[3,12],net:12,network:[4,11,12],neural:[4,11,12],neurosci:11,never:4,newdo:6,next:[2,3],next_power_of_2:[2,5],nightli:0,nip:11,nitish:4,nn:[3,5],non:11,none:[2,3,5,6,13,14,18,19,21,22,23,24,25,30,49,53,54],nonzero:51,norm:[4,5,9],normal:[2,6,8,9],normal_:6,normalized_shap:5,note:[0,1,2,3,4,6,12,14,16,51],notebook:[1,2,3,4,5,6,7,8],notic:[2,12],notori:[3,11],novel:11,now:[1,3],num_block:6,num_pid_in_group:3,num_pid_m:3,num_pid_n:3,num_stag:[3,6,13,14],num_warp:[2,3,5,6,13,14],number:[1,2,3,4,5,12,13,14,37,39,40,41,42],numcol:5,numel:[1,4,5,7],numer:[2,11],numrow:5,nvidia:[5,7,11,30],nvvm:7,o:[2,4,6],object:[1,3,11,13,14,16,18,19,20,21,22,23,24,25],obtain:1,obvious:2,occup:5,occur:12,off:5,off_h:6,off_hz:6,off_k:6,off_m:6,off_n:6,off_o:6,off_q:6,off_v:6,off_z:6,offer:11,offici:0,offs_am:3,offs_bn:3,offs_cm:3,offs_cn:3,offs_d:6,offs_k:[3,6],offs_m:6,offs_m_curr:6,offs_n:6,offs_qm:6,offset:[1,4,6,7,39,40,41,42],often:3,omega:12,onc:[2,11,12],one:[2,3,4,8,11,12,53],ones:6,onli:[2,3,4,6,11,12,16],op:[1,2],open:17,openai:0,opencl:11,oper:[1,2,3,4,7,8,11,18,19,20,21,22,23,24,25,51],opportun:11,opsila:11,optim:[11,12],option:[3,14,18,19,21,22,23,24,25,30,49,53,54],orang:5,order:[2,3,8,12],org:[4,6],origin:12,osdi:11,other:[2,3,4,5,10,12,16,28,30,33,35],otherwis:[4,51],our:[1,2,3,11],out:[1,2,3,4,5,6,7,10,12],out_ptr:6,outlin:12,output2:4,output3:4,output:[1,2,3,4,5,6,7],output_ptr:[1,2,4],output_row_start_ptr:2,output_row_strid:2,output_torch:[1,7],output_triton:[1,7],over:[2,4,6,11,12],overfit:4,overflow:2,own:3,p:[4,6,12],p_scale:6,pa:3,packag:16,pact:12,pad:2,par:3,paradigm:[11,12],paragraph:4,parallel:[1,2,3,4,5,10,11,12,13],paralleliz:11,param:15,paramet:[1,3,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55],parametr:[6,11],part:[3,4,12],partial:5,particular:[2,3],particularli:[11,12],partit:11,pass:[1,5,7,12,13],past:[11,12],path:1,pattern:11,pb:3,pdf:6,peak:12,per:[2,4,5],percentil:[6,54],perf:3,perf_model:14,perf_report:[1,2,3,5,6,53],perform:[1,2,4,11,12,14,18,19,20,21,22,23,24,25,54],persist:4,person:12,perspect:12,phase:12,philosophi:12,philox:[4,41],pid:[1,3,4,5,7],pid_m:3,pid_n:3,pip:[0,8],pipelin:[11,12,13],platform:[10,12],pldi:11,pleas:7,plot:[0,1,2,3,53],plot_nam:[1,2,3,5,6,53],pmatrix:12,point:[1,12,41],pointer:[1,2,4,6,16,18,19,20,21,22,23,24,25,30,49],pointerdtyp:[18,19,20,21,22,23,24,25,30,49],polli:12,polyhedr:11,polyhedra:12,popular:12,portabl:[11,12],pose:11,posit:[5,15],possibl:[1,2,3,12,13],power:[2,4,12,15,17],ppopp:12,practic:[1,2,3,11],pragma:11,pre:[0,6,11],pre_hook:13,prealloc:1,predic:14,predict:12,prefer:2,premis:11,present:0,preserv:12,preserve_rng_st:4,prevent:[4,12],primer:12,primit:[11,16],princip:7,principl:12,print:[1,3,4,7],print_data:[1,2,3,5,6],prng:4,probabl:[4,12],problem:1,problemat:12,procedur:12,process:[1,5,11,12],processor:11,produc:[3,4],product:[10,12,28],program:[1,2,3,4,5,10,11,37,38],program_id:[1,2,3,4,5,6,7],programm:[11,12],prohibitev:15,project:[4,11],promot:[3,12],properli:2,properti:12,propos:11,proprietari:3,provid:[1,2,3,4,5,6,10,12,14,32,34,50,54],prune:[4,14],prune_configs_bi:14,pseudo:[3,4,41],pseudorandom:4,ptr:3,ptx:30,purpos:[11,12],push:12,put:4,py:[0,1,2,3,4,5,6,7,9],pypi:[0,5],pytest:[0,6],python:[1,2,3,4,5,6,7,8,16],pytorch:[1,2,4],q:6,q_ptr:6,qk:6,qkv:6,qquad:12,quantiti:6,r:2,rabe:6,ragan:11,rand:[1,4,5,7],randint4x:40,randn:[2,3,4,5,6],randn_lik:[5,6],random:[4,39,40,41,42],randomli:4,rang:[1,2,3,5,6,11,12],rapidli:[11,12],rate:3,rather:11,raw:1,rdom:12,re:[1,3],read:[2,3,8],reader:12,real:11,reason:12,recent:11,recommend:8,recomput:[4,6,11],record_clock:54,rectifi:11,red:6,redmon2016:11,redmon:11,reduct:[2,5,32,34,50],ref_dk:6,ref_dq:6,ref_dv:6,ref_out:6,refer:[1,6,7],regard:7,regardless:[4,51],regim:4,regist:6,regrett:11,regular:[4,12],rel:[1,12],relat:10,releas:[0,11],reli:12,relu:3,remain:[11,53],remateri:6,rememb:3,reorder:12,rep:[5,6,54],repetit:54,repres:[2,3,12,13],requir:[2,4,12],requires_grad:[5,6],requires_grad_:[5,6],research:[11,12],reset:[14,54],reset_to_zero:14,reshap:5,resolut:12,resourc:11,resp:12,respect:12,restrict:12,result:[0,1,2,11,12],ret:2,retain_graph:[5,6],retriev:12,reus:3,revisit:11,right:12,rise:12,role:12,ron:4,root:48,roughli:3,row:[2,3,4,5,6],row_idx:2,row_minus_max:2,row_start_ptr:2,rstd:5,run:[0,1,2,3,4,5,6,7,10,12,14,16,55],runtim:[12,54],ruslan:4,rvar:12,s:[1,2,4,5,12,41],said:12,salakhutdinov:4,salmon2011:4,salmon:4,same:[4,7,11,53],sato2019:12,sato:12,save:[1,2,3,6],save_for_backward:[5,6],save_path:[1,5,6],saved_tensor:[5,6],sc:12,scalabl:12,scalar:[4,11,28,39,40,41,42,52],scale:[6,53],scan:12,schedul:11,scienc:12,scientif:12,scop:12,scope:12,scratchpad:6,script:[0,1,2,3,4,5,6,7],second:[1,2,3,4,5,6,7,12,28,33,35],secondli:4,section:[3,12],see:[1,2,3,4,6,12],seed:[39,40,41,42],seeded_dropout:4,seem:[1,12],select:[7,11,12,51],self:[13,53],semant:[7,12],semi:12,sens:[1,11,12],separ:12,seq:6,sequenc:11,set:[1,4,12],setup:[0,5],sever:[11,12],shall:12,shape:[1,2,3,4,5,6,12,26,30,44,49,51,52],share:11,shaw:4,shift:2,should:[1,3,5,11,12,13,32,34,50,53],show_plot:[1,2,3],shown:12,side:12,sight:12,signal:11,significantli:2,sigplan:12,simd:11,simpl:[1,2,3,4],simplest:8,simpli:[7,12],simplic:3,simplifi:4,sinc:[1,2,3],sine:[7,46],singl:[2,4,11,40],size:[1,2,4,7,12],slower:[11,12],slowest:12,sm80:13,sm:12,sm_scale:6,small:5,smaller:[3,4],smallest:[2,15],snemi3d:11,so:[1,2,3,4,5,12],softmax:[4,6,8,9],softmax_kernel:2,softmax_output:2,softwar:13,solid:12,solut:3,solv:12,some:3,sometim:12,sourc:[1,2,3,4,5,6,7,8,12],space:[11,12],spars:[4,11,12],spatial:12,speak:3,special:11,specif:[3,11],specifi:[12,15,18,19,20,21,22,23,24,25,49],speed:2,sphinx:[1,2,3,4,5,6,7,8],split:12,spmd:[1,11,12],sqrt:5,squar:48,sram:[2,3,5,6],srivastava2014:4,srivastava:4,staat:6,stabil:2,stabl:0,stage:13,stai:6,standard:12,start:[8,17],start_m:6,start_n:6,started_tutori:9,state:[4,11,12],statement:12,staticmethod:[5,6],std:6,step:12,still:[1,2,3,12],stop:17,store:[1,2,3,4,5,6,7,18,19,20,21,22,23,24,25,51],str:[14,15,30,53],straightforward:3,strategi:[4,12],stream:[5,40],strength:11,stride:[2,3,4,5,6],stride_ak:3,stride_am:3,stride_bk:3,stride_bn:3,stride_cm:3,stride_cn:3,stride_kh:6,stride_kk:6,stride_kn:6,stride_kz:6,stride_oh:6,stride_om:6,stride_on:6,stride_oz:6,stride_qh:6,stride_qk:6,stride_qm:6,stride_qz:6,stride_vh:6,stride_vk:6,stride_vn:6,stride_vz:6,stride_xi:3,stride_xj:3,structur:[11,12],style:[1,2,3,5,6,53],subscript:12,substanti:11,substract:2,subtract:2,successfulli:12,suffer:12,suit:11,sum:[1,2,5,6],sum_db:5,sum_dw:5,superhuman:11,support:[4,12],sure:2,surprisingli:11,surround:12,suspicion:2,sutskev:[4,11],sutskever2014:11,swap:20,swizzl:11,synchron:[1,11],system:[0,3,11,12],t:[1,2,3,6,12],t_:12,t_ptr:6,tabul:4,taco:12,take:[3,4,10,14,15],taken:12,target:11,techniqu:[11,12],temperatur:4,tempor:12,tend:12,tension:11,tensor:[1,2,3,4,5,7,11,12,14,16,26,28,30,32,33,34,35,43,44,49,50,51,52,54],tensorrt:11,test:[0,1,5,6,10],test_layer_norm:5,test_op:6,text:12,tflop:3,th:54,than:[2,3,5,11,12,40,53],thei:[3,11,12],them:1,themselv:3,theoret:2,therebi:12,therefor:3,theta:12,theta_:12,thi:[1,2,3,4,5,6,7,11,12,13,14,15,16,41,53],thing:[1,4],think:2,those:2,though:[11,12],thought:12,thread:[2,11,13],through:[8,12],throughout:[6,12,53],throughput:10,tile:12,time:[0,1,2,3,4,5,6,7,11,12,14,40,54],tiramisu:[11,12],tl:[1,2,3,4,5,6,7,52],tmp:[0,6],tog:12,togeth:[4,7],tolist:4,top_k:14,topic:12,torch:[1,2,3,4,5,6,7,16,54],torch_output:3,torch_relu:3,total:[1,2,3,4,5,6,7,9],tradit:[4,11,12],trans_a:[6,28],trans_b:[6,28],transform:[4,12],transpos:6,travers:12,trend:11,tri:[26,44],tri_dk:6,tri_dq:6,tri_dv:6,tri_out:6,trick:2,tricki:4,trigger:[3,14],tril:6,trition:7,triton:[0,1,2,3,4,5,6,7,8,11,12],triton_output:3,trivial:11,tune:[2,3,12,14,15],tuner:13,tupl:[1,26,44,52],tutori:[1,2,3,4,10],tutorials_jupyt:8,tutorials_python:8,tvm:[11,12],two:[1,2,3,12,14,15,17,28],type:[7,15,28,30,51,52],typecast:[30,49],typic:12,u:[0,39],un:12,uncommon:12,underli:7,underneath:12,understand:2,undesir:14,unfortun:[3,12],unifi:11,uniformli:4,unint:51,unit:[0,11],univers:12,unrol:[5,12],up:2,updat:[3,6,12,14],us:[1,2,3,4,5,11,12,13,14,15,16,40,51,53,55],user:7,usr:7,util:[1,5],v100:12,v:6,v_ptr:6,val:[18,19,20,21,22,23,24,25],valid:1,valu:[1,2,3,4,6,7,14,15,17,18,19,20,21,22,23,24,25,27,29,30,31,32,34,36,45,46,47,48,49,50,51,52,53,55],valuabl:2,vari:6,variabl:[3,13],varianc:5,variant:11,variou:8,vasilach:[11,12],vasilache2018:[11,12],vast:12,vec:12,vector:[4,8,9,11,12],vendor:3,veri:[2,4,12],verif:12,verifi:[2,12],via:12,view:43,visibl:12,vision:11,volatil:30,vs:0,w:12,w_shape:5,wa:4,wai:[2,3,4],want:[2,4,51],warmup:[6,54],warp:[2,5,13],wast:2,wdout:5,we:[1,2,3,4,7,11,12],weight:5,well:[4,11,12],whatev:14,wheel:0,when:[2,3,4,11,12,13,14,16,51],where:[1,3,4,5,6,12,15,49],whether:[11,53],which:[1,2,3,4,11,12,14,32,34,50,53],whose:[1,2,3,4,12,14,30],wide:12,wise:[1,2,6,27,29,31,33,35,45,46,47,48,49],wish:[3,12],within:[3,16,17],without:12,wolf:12,wolfe1989:12,won:2,word:12,work:[2,4,6,10,11],workaround:6,workload:[3,13],wors:[3,11,12],would:[1,2,4],wouldn:12,wrapper:3,write:[1,2,3,4,5,6,8,10,12],wrote:2,x:[1,2,3,4,5,7,12,27,29,31,33,35,43,45,46,47,48,51,53],x_arg:5,x_keep:4,x_keep_ptr:4,x_log:[1,53],x_max:2,x_name:[1,2,3,5,6,53],x_ptr:[1,4,7,14,15],x_shape:5,x_size:[14,15],x_val:[1,2,3,5,6,53],xi:12,xii:12,xlabel:53,xo:12,xor:25,y:[1,2,3,5,12,33,35,51,53],y_fwd:5,y_log:53,y_name:[1,2],y_ptr:[1,7],y_ref:5,y_torch:2,y_tri:5,y_triton:2,year:12,yet:[11,12],yi:12,yield:51,yii:12,ylabel:[1,2,3,5,6,53],yo:12,you:[0,1,2,3,4,7,8,11,14,40,51],your:[0,1,10],yourself:[2,3],z:[1,2,6,12],zero:[3,4,5,6,7,14],zeros_lik:6,zip:8},titles:[\"Installation\",\"Vector Addition\",\"Fused Softmax\",\"Matrix Multiplication\",\"Low-Memory Dropout\",\"Layer Normalization\",\"Fused Attention\",\"Libdevice function\",\"Tutorials\",\"Computation times\",\"Welcome to Triton\\u2019s documentation!\",\"Introduction\",\"Related Work\",\"triton.Config\",\"triton.autotune\",\"triton.heuristics\",\"triton.jit\",\"triton.language.arange\",\"triton.language.atomic_add\",\"triton.language.atomic_and\",\"triton.language.atomic_cas\",\"triton.language.atomic_max\",\"triton.language.atomic_min\",\"triton.language.atomic_or\",\"triton.language.atomic_xchg\",\"triton.language.atomic_xor\",\"triton.language.broadcast_to\",\"triton.language.cos\",\"triton.language.dot\",\"triton.language.exp\",\"triton.language.load\",\"triton.language.log\",\"triton.language.max\",\"triton.language.maximum\",\"triton.language.min\",\"triton.language.minimum\",\"triton.language.multiple_of\",\"triton.language.num_programs\",\"triton.language.program_id\",\"triton.language.rand\",\"triton.language.randint\",\"triton.language.randint4x\",\"triton.language.randn\",\"triton.language.ravel\",\"triton.language.reshape\",\"triton.language.sigmoid\",\"triton.language.sin\",\"triton.language.softmax\",\"triton.language.sqrt\",\"triton.language.store\",\"triton.language.sum\",\"triton.language.where\",\"triton.language.zeros\",\"triton.testing.Benchmark\",\"triton.testing.do_bench\",\"triton.testing.perf_report\",\"triton\",\"triton.language\",\"triton.testing\"],titleterms:{\"default\":7,\"final\":3,\"function\":7,addit:1,advantag:12,algebra:57,api:10,arang:17,arithmet:3,asin:7,atom:57,atomic_add:18,atomic_and:19,atomic_ca:20,atomic_max:21,atomic_min:22,atomic_or:23,atomic_xchg:24,atomic_xor:25,attent:6,autotun:14,baselin:4,benchmark:[1,2,3,53],binari:0,broadcast_to:26,cach:3,challeng:11,co:27,comparison:57,compil:[12,57],comput:[1,2,3,9],config:13,creation:57,custom:7,distribut:0,do_bench:54,document:10,dot:28,dropout:4,exercis:4,exp:29,from:0,further:10,fuse:[2,6],gener:57,get:10,go:10,heurist:15,hint:57,index:57,instal:0,introduct:11,jit:16,kernel:[1,2,3,7],l2:3,languag:[12,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,57],layer:5,libdevic:7,librari:7,limit:12,linear:57,load:30,log:31,low:4,manipul:57,math:57,matrix:3,max:32,maximum:33,memori:[4,57],min:34,minimum:35,model:57,motiv:[2,3,11],multipl:3,multiple_of:36,normal:5,num_program:37,number:57,op:57,optim:3,packag:0,path:7,perf_report:55,perform:3,pointer:3,polyhedr:12,program:[12,57],program_id:38,python:[0,10],rand:39,randint4x:41,randint:40,randn:42,random:57,ravel:43,reduct:57,refer:[4,11,12],relat:12,represent:12,reshap:44,result:3,s:10,schedul:12,seed:4,shape:57,sigmoid:45,sin:46,softmax:[2,47],sourc:0,sqrt:48,squar:3,start:10,store:49,sum:50,test:[2,3,53,54,55,58],time:9,triton:[10,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58],tutori:8,unit:[2,3],us:7,vector:1,welcom:10,where:51,work:12,zero:52}})\n\\ No newline at end of file"}, {"filename": "v1.1.2/.buildinfo", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n # Sphinx build info version 1\n # This file hashes the configuration used when building these files. When it is not found, a full rebuild will be done.\n-config: e38b42b0e79f6250810f1562672b009a\n+config: 0170c1c0118ef1d594643552136fcc46\n tags: 645f666f9bcd5a90fca523b33c5a78b7"}, {"filename": "v1.1.2/.doctrees/environment.pickle", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/getting-started/installation.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/getting-started/tutorials/01-vector-add.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/getting-started/tutorials/02-fused-softmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/getting-started/tutorials/03-matrix-multiplication.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/getting-started/tutorials/04-low-memory-dropout.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/getting-started/tutorials/05-layer-norm.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/getting-started/tutorials/index.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/getting-started/tutorials/sg_execution_times.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/index.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/programming-guide/chapter-1/introduction.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/programming-guide/chapter-2/related-work.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.Config.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.autotune.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.heuristics.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.jit.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.arange.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.atomic_add.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.atomic_cas.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.atomic_max.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.atomic_min.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.atomic_xchg.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.broadcast_to.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.cos.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.dot.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.exp.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.load.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.log.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.max.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.maximum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.min.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.minimum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.multiple_of.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.num_programs.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.program_id.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.rand.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.randint.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.randint4x.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.randn.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.ravel.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.reshape.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.sigmoid.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.sin.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.softmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.sqrt.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.store.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.sum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.where.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.language.zeros.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.testing.Benchmark.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.testing.do_bench.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/generated/triton.testing.perf_report.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/triton.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/triton.language.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/.doctrees/python-api/triton.testing.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/_downloads/662999063954282841dc90b8945f85ce/tutorials_jupyter.zip", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/_downloads/763344228ae6bc253ed1a6cf586aa30d/tutorials_python.zip", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/_images/sphx_glr_01-vector-add_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/_images/sphx_glr_01-vector-add_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/_images/sphx_glr_02-fused-softmax_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/_images/sphx_glr_02-fused-softmax_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/_images/sphx_glr_03-matrix-multiplication_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/_images/sphx_glr_03-matrix-multiplication_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/_images/sphx_glr_05-layer-norm_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/_images/sphx_glr_05-layer-norm_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "v1.1.2/_sources/getting-started/tutorials/01-vector-add.rst.txt", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -240,11 +240,11 @@ We can now run the decorated function above. Pass `print_data=True` to see the p\n     6      262144.0  341.333321  384.000001\n     7      524288.0  472.615390  472.615390\n     8     1048576.0  614.400016  614.400016\n-    9     2097152.0  722.823517  722.823517\n+    9     2097152.0  722.823517  702.171410\n     10    4194304.0  780.190482  780.190482\n     11    8388608.0  812.429770  812.429770\n     12   16777216.0  833.084721  833.084721\n-    13   33554432.0  842.004273  843.811163\n+    13   33554432.0  842.004273  842.004273\n     14   67108864.0  847.448255  848.362445\n     15  134217728.0  849.737435  850.656574\n \n@@ -254,7 +254,7 @@ We can now run the decorated function above. Pass `print_data=True` to see the p\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 1 minutes  42.833 seconds)\n+   **Total running time of the script:** ( 1 minutes  41.456 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_01-vector-add.py:"}, {"filename": "v1.1.2/_sources/getting-started/tutorials/02-fused-softmax.rst.txt", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -286,17 +286,17 @@ We will then compare its performance against (1) :code:`torch.softmax` and (2) t\n \n     softmax-performance:\n               N      Triton  Torch (native)  Torch (jit)\n-    0     256.0  512.000001      512.000001   188.321838\n-    1     384.0  585.142862      585.142862   153.600004\n-    2     512.0  655.360017      606.814814   154.566038\n+    0     256.0  512.000001      546.133347   186.181817\n+    1     384.0  585.142862      585.142862   151.703707\n+    2     512.0  655.360017      606.814814   156.038096\n     3     640.0  682.666684      640.000002   160.000000\n-    4     768.0  722.823517      664.216187   162.754967\n+    4     768.0  722.823517      664.216187   163.839992\n     ..      ...         ...             ...          ...\n-    93  12160.0  814.058574      406.179533   198.530610\n-    94  12288.0  814.111783      415.661740   198.895304\n-    95  12416.0  812.498981      411.935714   198.556711\n-    96  12544.0  812.566838      412.546756   198.716830\n-    97  12672.0  812.633240      412.097543   198.776477\n+    93  12160.0  814.058574      405.755985   198.834951\n+    94  12288.0  814.111783      415.222812   199.197579\n+    95  12416.0  812.498981      411.722274   198.755369\n+    96  12544.0  812.566838      412.758863   198.963085\n+    97  12672.0  812.633240      412.097543   199.069228\n \n     [98 rows x 4 columns]\n \n@@ -314,7 +314,7 @@ In the above plot, we can see that:\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 3 minutes  22.357 seconds)\n+   **Total running time of the script:** ( 3 minutes  20.765 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:"}, {"filename": "v1.1.2/_sources/getting-started/tutorials/03-matrix-multiplication.rst.txt", "status": "modified", "additions": 24, "deletions": 24, "changes": 48, "file_content_changes": "@@ -462,37 +462,37 @@ We can now compare the performance of our kernel against that of cuBLAS. Here we\n \n     matmul-performance:\n              M     cuBLAS  ...     Triton  Triton (+ LeakyReLU)\n-    0    256.0   2.730667  ...   2.978909              2.978909\n-    1    384.0   7.372800  ...   8.507077              7.899428\n-    2    512.0  14.563555  ...  15.420235             15.420235\n+    0    256.0   2.730667  ...   3.276800              3.276800\n+    1    384.0   7.372800  ...   7.899428              7.899428\n+    2    512.0  14.563555  ...  16.384000             16.384000\n     3    640.0  22.260869  ...  24.380953             24.380953\n     4    768.0  32.768000  ...  34.028308             34.028308\n     5    896.0  39.025776  ...  40.140799             39.025776\n-    6   1024.0  51.150050  ...  53.773130             52.428801\n+    6   1024.0  51.150050  ...  52.428801             52.428801\n     7   1152.0  45.242181  ...  46.656000             46.656000\n     8   1280.0  51.200001  ...  56.888887             56.109587\n     9   1408.0  64.138541  ...  67.305878             66.485074\n-    10  1536.0  80.430545  ...  79.526831             78.643199\n-    11  1664.0  63.372618  ...  62.492442             62.061463\n+    10  1536.0  80.430545  ...  79.526831             79.526831\n+    11  1664.0  62.929456  ...  62.492442             62.061463\n     12  1792.0  72.983276  ...  72.047592             71.588687\n-    13  1920.0  69.120002  ...  70.172588             70.172588\n-    14  2048.0  73.584279  ...  76.959706             76.608294\n+    13  1920.0  69.467336  ...  70.172588             70.172588\n+    14  2048.0  73.908442  ...  76.959706             76.608294\n     15  2176.0  83.155572  ...  85.998493             85.269692\n-    16  2304.0  68.251065  ...  76.809875             76.563695\n-    17  2432.0  71.305746  ...  85.134737             84.621881\n-    18  2560.0  78.019048  ...  81.310171             80.709358\n-    19  2688.0  83.552988  ...  88.836198             89.254248\n-    20  2816.0  83.233226  ...  83.712490             82.368662\n-    21  2944.0  82.784108  ...  82.373605             82.237674\n-    22  3072.0  82.782312  ...  88.473602             89.310890\n-    23  3200.0  84.768213  ...  95.522391             94.955488\n-    24  3328.0  83.226931  ...  83.226931             84.795401\n-    25  3456.0  82.604067  ...  86.783176             88.595129\n-    26  3584.0  88.152348  ...  94.747514             97.628001\n-    27  3712.0  85.896254  ...  85.748791             86.566152\n-    28  3840.0  83.465663  ...  87.771425             91.777595\n-    29  3968.0  87.976885  ...  87.535103             88.680186\n-    30  4096.0  94.088836  ...  93.336389             87.267706\n+    16  2304.0  68.446623  ...  76.809875             76.563695\n+    17  2432.0  71.305746  ...  85.134737             84.877538\n+    18  2560.0  77.833728  ...  81.310171             80.908642\n+    19  2688.0  83.552988  ...  89.676257             89.888756\n+    20  2816.0  81.521884  ...  82.916747             82.759409\n+    21  2944.0  82.509987  ...  81.698415             81.832567\n+    22  3072.0  79.526831  ...  88.612060             89.310890\n+    23  3200.0  84.880639  ...  95.380032             95.096582\n+    24  3328.0  83.130825  ...  84.995628             84.895397\n+    25  3456.0  81.766291  ...  90.484366             90.841203\n+    26  3584.0  87.211821  ...  91.656871             97.734120\n+    27  3712.0  85.748791  ...  86.191546             86.867254\n+    28  3840.0  83.528704  ...  88.900318             91.625518\n+    29  3968.0  87.035620  ...  91.609561             88.680186\n+    30  4096.0  93.990003  ...  92.948562             88.330190\n \n     [31 rows x 5 columns]\n \n@@ -502,7 +502,7 @@ We can now compare the performance of our kernel against that of cuBLAS. Here we\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 5 minutes  16.080 seconds)\n+   **Total running time of the script:** ( 5 minutes  19.908 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_03-matrix-multiplication.py:"}, {"filename": "v1.1.2/_sources/getting-started/tutorials/05-layer-norm.rst.txt", "status": "modified", "additions": 31, "deletions": 31, "changes": 62, "file_content_changes": "@@ -38,36 +38,36 @@ Layer Normalization\n \n     layer-norm-backward:\n               N      Triton       Torch        Apex\n-    0    1024.0  307.200008   99.497980  307.200008\n-    1    1536.0  351.085717  133.083026  338.201833\n-    2    2048.0  423.724127  162.217818  327.679984\n-    3    2560.0  461.954908  182.857144  325.079368\n-    4    3072.0  511.999982  191.501303  319.168834\n-    5    3584.0  551.384634  208.271186  309.410081\n-    6    4096.0  568.231237  220.412561  298.796351\n-    7    4608.0  495.928261  231.849059  286.507772\n-    8    5120.0  525.128191  242.845844  283.787523\n-    9    5632.0  538.517949  243.545956  291.310338\n-    10   6144.0  542.117638  248.661056  285.767458\n-    11   6656.0  527.207907  256.000009  286.793541\n-    12   7168.0  505.976473  262.243907  288.160801\n-    13   7680.0  482.513091  260.707203  277.172933\n-    14   8192.0  460.440290  269.326017  286.600589\n-    15   8704.0  416.958106  267.815384  284.987724\n-    16   9216.0  428.651187  272.729961  289.507855\n-    17   9728.0  438.857162  280.615388  288.950501\n-    18  10240.0  447.650282  286.433562  290.153487\n-    19  10752.0  429.364408  246.464170  290.267711\n-    20  11264.0  429.104745  245.313973  285.767446\n-    21  11776.0  421.198220  249.667843  288.686414\n-    22  12288.0  420.102570  254.453844  294.911986\n-    23  12800.0  415.135142  253.256381  288.180121\n-    24  13312.0  412.242569  252.559690  290.179836\n-    25  13824.0  404.604870  257.190689  292.571423\n-    26  14336.0  397.761846  254.673567  286.481278\n-    27  14848.0  383.586664  257.293872  289.246765\n-    28  15360.0  374.253788  257.610071  286.656296\n-    29  15872.0  366.982663  262.708969  291.229369\n+    0    1024.0  311.088617   99.497980  311.088617\n+    1    1536.0  354.461542  133.565214  338.201833\n+    2    2048.0  420.102553  162.754967  325.509933\n+    3    2560.0  461.954908  182.857144  330.322572\n+    4    3072.0  511.999982  191.501303  312.406770\n+    5    3584.0  547.872604  208.271186  309.410081\n+    6    4096.0  568.231237  220.412561  300.623865\n+    7    4608.0  500.416301  231.364016  289.507855\n+    8    5120.0  522.893618  242.845844  286.433562\n+    9    5632.0  542.843364  242.671458  287.591490\n+    10   6144.0  542.117638  250.349744  286.879370\n+    11   6656.0  534.260858  255.182111  284.242007\n+    12   7168.0  513.528374  255.619613  279.726817\n+    13   7680.0  481.253256  263.314295  277.590365\n+    14   8192.0  463.698115  266.406514  277.694924\n+    15   8704.0  417.791980  263.757583  281.530996\n+    16   9216.0  429.483477  272.394084  289.507855\n+    17   9728.0  438.033784  281.630872  289.667485\n+    18  10240.0  446.836366  285.435547  287.775181\n+    19  10752.0  431.518385  246.464170  290.922209\n+    20  11264.0  429.786952  245.760001  287.897767\n+    21  11776.0  421.826879  249.447482  288.686414\n+    22  12288.0  419.504980  254.673582  294.617366\n+    23  12800.0  415.696898  253.465340  288.721817\n+    24  13312.0  410.125805  250.775503  288.867982\n+    25  13824.0  404.604870  257.190689  292.056329\n+    26  14336.0  399.146178  255.429842  288.160801\n+    27  14848.0  381.942121  256.184041  287.496569\n+    28  15360.0  373.495460  259.605636  288.902809\n+    29  15872.0  368.046389  262.527914  291.006885\n \n \n \n@@ -329,7 +329,7 @@ Layer Normalization\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 2 minutes  10.499 seconds)\n+   **Total running time of the script:** ( 2 minutes  12.763 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_05-layer-norm.py:"}, {"filename": "v1.1.2/_sources/getting-started/tutorials/sg_execution_times.rst.txt", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -5,16 +5,16 @@\n \n Computation times\n =================\n-**12:31.780** total execution time for **getting-started_tutorials** files:\n+**12:34.902** total execution time for **getting-started_tutorials** files:\n \n +---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``) | 05:16.080 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``) | 05:19.908 | 0.0 MB |\n +---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                 | 03:22.357 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                 | 03:20.765 | 0.0 MB |\n +---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                       | 02:10.499 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                       | 02:12.763 | 0.0 MB |\n +---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                       | 01:42.833 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                       | 01:41.456 | 0.0 MB |\n +---------------------------------------------------------------------------------------------------------+-----------+--------+\n | :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)       | 00:00.011 | 0.0 MB |\n +---------------------------------------------------------------------------------------------------------+-----------+--------+"}, {"filename": "v1.1.2/getting-started/tutorials/01-vector-add.html", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -330,16 +330,16 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n 6      262144.0  341.333321  384.000001\n 7      524288.0  472.615390  472.615390\n 8     1048576.0  614.400016  614.400016\n-9     2097152.0  722.823517  722.823517\n+9     2097152.0  722.823517  702.171410\n 10    4194304.0  780.190482  780.190482\n 11    8388608.0  812.429770  812.429770\n 12   16777216.0  833.084721  833.084721\n-13   33554432.0  842.004273  843.811163\n+13   33554432.0  842.004273  842.004273\n 14   67108864.0  847.448255  848.362445\n 15  134217728.0  849.737435  850.656574\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  42.833 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  41.456 seconds)</p>\n <div class=\"sphx-glr-footer class sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-01-vector-add-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/62d97d49a32414049819dd8bb8378080/01-vector-add.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">01-vector-add.py</span></code></a></p>"}, {"filename": "v1.1.2/getting-started/tutorials/02-fused-softmax.html", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -374,17 +374,17 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n <p class=\"sphx-glr-script-out\">Out:</p>\n <div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>softmax-performance:\n           N      Triton  Torch (native)  Torch (jit)\n-0     256.0  512.000001      512.000001   188.321838\n-1     384.0  585.142862      585.142862   153.600004\n-2     512.0  655.360017      606.814814   154.566038\n+0     256.0  512.000001      546.133347   186.181817\n+1     384.0  585.142862      585.142862   151.703707\n+2     512.0  655.360017      606.814814   156.038096\n 3     640.0  682.666684      640.000002   160.000000\n-4     768.0  722.823517      664.216187   162.754967\n+4     768.0  722.823517      664.216187   163.839992\n ..      ...         ...             ...          ...\n-93  12160.0  814.058574      406.179533   198.530610\n-94  12288.0  814.111783      415.661740   198.895304\n-95  12416.0  812.498981      411.935714   198.556711\n-96  12544.0  812.566838      412.546756   198.716830\n-97  12672.0  812.633240      412.097543   198.776477\n+93  12160.0  814.058574      405.755985   198.834951\n+94  12288.0  814.111783      415.222812   199.197579\n+95  12416.0  812.498981      411.722274   198.755369\n+96  12544.0  812.566838      412.758863   198.963085\n+97  12672.0  812.633240      412.097543   199.069228\n \n [98 rows x 4 columns]\n </pre></div>\n@@ -397,7 +397,7 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n Note however that the PyTorch <cite>softmax</cite> operation is more general and will works on tensors of any shape.</p></li>\n </ul>\n </div></blockquote>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 3 minutes  22.357 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 3 minutes  20.765 seconds)</p>\n <div class=\"sphx-glr-footer class sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-02-fused-softmax-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d91442ac2982c4e0cc3ab0f43534afbc/02-fused-softmax.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">02-fused-softmax.py</span></code></a></p>"}, {"filename": "v1.1.2/getting-started/tutorials/03-matrix-multiplication.html", "status": "modified", "additions": 24, "deletions": 24, "changes": 48, "file_content_changes": "@@ -568,42 +568,42 @@ <h3>Square Matrix Performance<a class=\"headerlink\" href=\"#square-matrix-performa\n <p class=\"sphx-glr-script-out\">Out:</p>\n <div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>matmul-performance:\n          M     cuBLAS  ...     Triton  Triton (+ LeakyReLU)\n-0    256.0   2.730667  ...   2.978909              2.978909\n-1    384.0   7.372800  ...   8.507077              7.899428\n-2    512.0  14.563555  ...  15.420235             15.420235\n+0    256.0   2.730667  ...   3.276800              3.276800\n+1    384.0   7.372800  ...   7.899428              7.899428\n+2    512.0  14.563555  ...  16.384000             16.384000\n 3    640.0  22.260869  ...  24.380953             24.380953\n 4    768.0  32.768000  ...  34.028308             34.028308\n 5    896.0  39.025776  ...  40.140799             39.025776\n-6   1024.0  51.150050  ...  53.773130             52.428801\n+6   1024.0  51.150050  ...  52.428801             52.428801\n 7   1152.0  45.242181  ...  46.656000             46.656000\n 8   1280.0  51.200001  ...  56.888887             56.109587\n 9   1408.0  64.138541  ...  67.305878             66.485074\n-10  1536.0  80.430545  ...  79.526831             78.643199\n-11  1664.0  63.372618  ...  62.492442             62.061463\n+10  1536.0  80.430545  ...  79.526831             79.526831\n+11  1664.0  62.929456  ...  62.492442             62.061463\n 12  1792.0  72.983276  ...  72.047592             71.588687\n-13  1920.0  69.120002  ...  70.172588             70.172588\n-14  2048.0  73.584279  ...  76.959706             76.608294\n+13  1920.0  69.467336  ...  70.172588             70.172588\n+14  2048.0  73.908442  ...  76.959706             76.608294\n 15  2176.0  83.155572  ...  85.998493             85.269692\n-16  2304.0  68.251065  ...  76.809875             76.563695\n-17  2432.0  71.305746  ...  85.134737             84.621881\n-18  2560.0  78.019048  ...  81.310171             80.709358\n-19  2688.0  83.552988  ...  88.836198             89.254248\n-20  2816.0  83.233226  ...  83.712490             82.368662\n-21  2944.0  82.784108  ...  82.373605             82.237674\n-22  3072.0  82.782312  ...  88.473602             89.310890\n-23  3200.0  84.768213  ...  95.522391             94.955488\n-24  3328.0  83.226931  ...  83.226931             84.795401\n-25  3456.0  82.604067  ...  86.783176             88.595129\n-26  3584.0  88.152348  ...  94.747514             97.628001\n-27  3712.0  85.896254  ...  85.748791             86.566152\n-28  3840.0  83.465663  ...  87.771425             91.777595\n-29  3968.0  87.976885  ...  87.535103             88.680186\n-30  4096.0  94.088836  ...  93.336389             87.267706\n+16  2304.0  68.446623  ...  76.809875             76.563695\n+17  2432.0  71.305746  ...  85.134737             84.877538\n+18  2560.0  77.833728  ...  81.310171             80.908642\n+19  2688.0  83.552988  ...  89.676257             89.888756\n+20  2816.0  81.521884  ...  82.916747             82.759409\n+21  2944.0  82.509987  ...  81.698415             81.832567\n+22  3072.0  79.526831  ...  88.612060             89.310890\n+23  3200.0  84.880639  ...  95.380032             95.096582\n+24  3328.0  83.130825  ...  84.995628             84.895397\n+25  3456.0  81.766291  ...  90.484366             90.841203\n+26  3584.0  87.211821  ...  91.656871             97.734120\n+27  3712.0  85.748791  ...  86.191546             86.867254\n+28  3840.0  83.528704  ...  88.900318             91.625518\n+29  3968.0  87.035620  ...  91.609561             88.680186\n+30  4096.0  93.990003  ...  92.948562             88.330190\n \n [31 rows x 5 columns]\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 5 minutes  16.080 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 5 minutes  19.908 seconds)</p>\n <div class=\"sphx-glr-footer class sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-03-matrix-multiplication-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d5fee5b55a64e47f1b5724ec39adf171/03-matrix-multiplication.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">03-matrix-multiplication.py</span></code></a></p>"}, {"filename": "v1.1.2/getting-started/tutorials/05-layer-norm.html", "status": "modified", "additions": 31, "deletions": 31, "changes": 62, "file_content_changes": "@@ -194,36 +194,36 @@\n <p class=\"sphx-glr-script-out\">Out:</p>\n <div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>layer-norm-backward:\n           N      Triton       Torch        Apex\n-0    1024.0  307.200008   99.497980  307.200008\n-1    1536.0  351.085717  133.083026  338.201833\n-2    2048.0  423.724127  162.217818  327.679984\n-3    2560.0  461.954908  182.857144  325.079368\n-4    3072.0  511.999982  191.501303  319.168834\n-5    3584.0  551.384634  208.271186  309.410081\n-6    4096.0  568.231237  220.412561  298.796351\n-7    4608.0  495.928261  231.849059  286.507772\n-8    5120.0  525.128191  242.845844  283.787523\n-9    5632.0  538.517949  243.545956  291.310338\n-10   6144.0  542.117638  248.661056  285.767458\n-11   6656.0  527.207907  256.000009  286.793541\n-12   7168.0  505.976473  262.243907  288.160801\n-13   7680.0  482.513091  260.707203  277.172933\n-14   8192.0  460.440290  269.326017  286.600589\n-15   8704.0  416.958106  267.815384  284.987724\n-16   9216.0  428.651187  272.729961  289.507855\n-17   9728.0  438.857162  280.615388  288.950501\n-18  10240.0  447.650282  286.433562  290.153487\n-19  10752.0  429.364408  246.464170  290.267711\n-20  11264.0  429.104745  245.313973  285.767446\n-21  11776.0  421.198220  249.667843  288.686414\n-22  12288.0  420.102570  254.453844  294.911986\n-23  12800.0  415.135142  253.256381  288.180121\n-24  13312.0  412.242569  252.559690  290.179836\n-25  13824.0  404.604870  257.190689  292.571423\n-26  14336.0  397.761846  254.673567  286.481278\n-27  14848.0  383.586664  257.293872  289.246765\n-28  15360.0  374.253788  257.610071  286.656296\n-29  15872.0  366.982663  262.708969  291.229369\n+0    1024.0  311.088617   99.497980  311.088617\n+1    1536.0  354.461542  133.565214  338.201833\n+2    2048.0  420.102553  162.754967  325.509933\n+3    2560.0  461.954908  182.857144  330.322572\n+4    3072.0  511.999982  191.501303  312.406770\n+5    3584.0  547.872604  208.271186  309.410081\n+6    4096.0  568.231237  220.412561  300.623865\n+7    4608.0  500.416301  231.364016  289.507855\n+8    5120.0  522.893618  242.845844  286.433562\n+9    5632.0  542.843364  242.671458  287.591490\n+10   6144.0  542.117638  250.349744  286.879370\n+11   6656.0  534.260858  255.182111  284.242007\n+12   7168.0  513.528374  255.619613  279.726817\n+13   7680.0  481.253256  263.314295  277.590365\n+14   8192.0  463.698115  266.406514  277.694924\n+15   8704.0  417.791980  263.757583  281.530996\n+16   9216.0  429.483477  272.394084  289.507855\n+17   9728.0  438.033784  281.630872  289.667485\n+18  10240.0  446.836366  285.435547  287.775181\n+19  10752.0  431.518385  246.464170  290.922209\n+20  11264.0  429.786952  245.760001  287.897767\n+21  11776.0  421.826879  249.447482  288.686414\n+22  12288.0  419.504980  254.673582  294.617366\n+23  12800.0  415.696898  253.465340  288.721817\n+24  13312.0  410.125805  250.775503  288.867982\n+25  13824.0  404.604870  257.190689  292.056329\n+26  14336.0  399.146178  255.429842  288.160801\n+27  14848.0  381.942121  256.184041  287.496569\n+28  15360.0  373.495460  259.605636  288.902809\n+29  15872.0  368.046389  262.527914  291.006885\n </pre></div>\n </div>\n <div class=\"line-block\">\n@@ -477,7 +477,7 @@\n <span class=\"n\">bench_layer_norm</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;.&#39;</span><span class=\"p\">,</span> <span class=\"n\">print_data</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 2 minutes  10.499 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 2 minutes  12.763 seconds)</p>\n <div class=\"sphx-glr-footer class sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-05-layer-norm-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/935c0dd0fbeb4b2e69588471cbb2d4b2/05-layer-norm.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">05-layer-norm.py</span></code></a></p>"}, {"filename": "v1.1.2/getting-started/tutorials/sg_execution_times.html", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -174,7 +174,7 @@\n             \n   <div class=\"section\" id=\"computation-times\">\n <span id=\"sphx-glr-getting-started-tutorials-sg-execution-times\"></span><h1>Computation times<a class=\"headerlink\" href=\"#computation-times\" title=\"Permalink to this headline\">\u00b6</a></h1>\n-<p><strong>12:31.780</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n+<p><strong>12:34.902</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n <table class=\"docutils align-default\">\n <colgroup>\n <col style=\"width: 85%\" />\n@@ -183,19 +183,19 @@\n </colgroup>\n <tbody>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"03-matrix-multiplication.html#sphx-glr-getting-started-tutorials-03-matrix-multiplication-py\"><span class=\"std std-ref\">Matrix Multiplication</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">03-matrix-multiplication.py</span></code>)</p></td>\n-<td><p>05:16.080</p></td>\n+<td><p>05:19.908</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py\"><span class=\"std std-ref\">Fused Softmax</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">02-fused-softmax.py</span></code>)</p></td>\n-<td><p>03:22.357</p></td>\n+<td><p>03:20.765</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"05-layer-norm.html#sphx-glr-getting-started-tutorials-05-layer-norm-py\"><span class=\"std std-ref\">Layer Normalization</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">05-layer-norm.py</span></code>)</p></td>\n-<td><p>02:10.499</p></td>\n+<td><p>02:12.763</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py\"><span class=\"std std-ref\">Vector Addition</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">01-vector-add.py</span></code>)</p></td>\n-<td><p>01:42.833</p></td>\n+<td><p>01:41.456</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"04-low-memory-dropout.html#sphx-glr-getting-started-tutorials-04-low-memory-dropout-py\"><span class=\"std std-ref\">Low-Memory Dropout</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">04-low-memory-dropout.py</span></code>)</p></td>"}, {"filename": "v1.1.2/searchindex.js", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1 +1 @@\n-Search.setIndex({docnames:[\"getting-started/installation\",\"getting-started/tutorials/01-vector-add\",\"getting-started/tutorials/02-fused-softmax\",\"getting-started/tutorials/03-matrix-multiplication\",\"getting-started/tutorials/04-low-memory-dropout\",\"getting-started/tutorials/05-layer-norm\",\"getting-started/tutorials/index\",\"getting-started/tutorials/sg_execution_times\",\"index\",\"programming-guide/chapter-1/introduction\",\"programming-guide/chapter-2/related-work\",\"python-api/generated/triton.Config\",\"python-api/generated/triton.autotune\",\"python-api/generated/triton.heuristics\",\"python-api/generated/triton.jit\",\"python-api/generated/triton.language.arange\",\"python-api/generated/triton.language.atomic_add\",\"python-api/generated/triton.language.atomic_cas\",\"python-api/generated/triton.language.atomic_max\",\"python-api/generated/triton.language.atomic_min\",\"python-api/generated/triton.language.atomic_xchg\",\"python-api/generated/triton.language.broadcast_to\",\"python-api/generated/triton.language.cos\",\"python-api/generated/triton.language.dot\",\"python-api/generated/triton.language.exp\",\"python-api/generated/triton.language.load\",\"python-api/generated/triton.language.log\",\"python-api/generated/triton.language.max\",\"python-api/generated/triton.language.maximum\",\"python-api/generated/triton.language.min\",\"python-api/generated/triton.language.minimum\",\"python-api/generated/triton.language.multiple_of\",\"python-api/generated/triton.language.num_programs\",\"python-api/generated/triton.language.program_id\",\"python-api/generated/triton.language.rand\",\"python-api/generated/triton.language.randint\",\"python-api/generated/triton.language.randint4x\",\"python-api/generated/triton.language.randn\",\"python-api/generated/triton.language.ravel\",\"python-api/generated/triton.language.reshape\",\"python-api/generated/triton.language.sigmoid\",\"python-api/generated/triton.language.sin\",\"python-api/generated/triton.language.softmax\",\"python-api/generated/triton.language.sqrt\",\"python-api/generated/triton.language.store\",\"python-api/generated/triton.language.sum\",\"python-api/generated/triton.language.where\",\"python-api/generated/triton.language.zeros\",\"python-api/generated/triton.testing.Benchmark\",\"python-api/generated/triton.testing.do_bench\",\"python-api/generated/triton.testing.perf_report\",\"python-api/triton\",\"python-api/triton.language\",\"python-api/triton.testing\"],envversion:{\"sphinx.domains.c\":2,\"sphinx.domains.changeset\":1,\"sphinx.domains.citation\":1,\"sphinx.domains.cpp\":4,\"sphinx.domains.index\":1,\"sphinx.domains.javascript\":2,\"sphinx.domains.math\":2,\"sphinx.domains.python\":3,\"sphinx.domains.rst\":2,\"sphinx.domains.std\":2,\"sphinx.ext.intersphinx\":1,sphinx:56},filenames:[\"getting-started/installation.rst\",\"getting-started/tutorials/01-vector-add.rst\",\"getting-started/tutorials/02-fused-softmax.rst\",\"getting-started/tutorials/03-matrix-multiplication.rst\",\"getting-started/tutorials/04-low-memory-dropout.rst\",\"getting-started/tutorials/05-layer-norm.rst\",\"getting-started/tutorials/index.rst\",\"getting-started/tutorials/sg_execution_times.rst\",\"index.rst\",\"programming-guide/chapter-1/introduction.rst\",\"programming-guide/chapter-2/related-work.rst\",\"python-api/generated/triton.Config.rst\",\"python-api/generated/triton.autotune.rst\",\"python-api/generated/triton.heuristics.rst\",\"python-api/generated/triton.jit.rst\",\"python-api/generated/triton.language.arange.rst\",\"python-api/generated/triton.language.atomic_add.rst\",\"python-api/generated/triton.language.atomic_cas.rst\",\"python-api/generated/triton.language.atomic_max.rst\",\"python-api/generated/triton.language.atomic_min.rst\",\"python-api/generated/triton.language.atomic_xchg.rst\",\"python-api/generated/triton.language.broadcast_to.rst\",\"python-api/generated/triton.language.cos.rst\",\"python-api/generated/triton.language.dot.rst\",\"python-api/generated/triton.language.exp.rst\",\"python-api/generated/triton.language.load.rst\",\"python-api/generated/triton.language.log.rst\",\"python-api/generated/triton.language.max.rst\",\"python-api/generated/triton.language.maximum.rst\",\"python-api/generated/triton.language.min.rst\",\"python-api/generated/triton.language.minimum.rst\",\"python-api/generated/triton.language.multiple_of.rst\",\"python-api/generated/triton.language.num_programs.rst\",\"python-api/generated/triton.language.program_id.rst\",\"python-api/generated/triton.language.rand.rst\",\"python-api/generated/triton.language.randint.rst\",\"python-api/generated/triton.language.randint4x.rst\",\"python-api/generated/triton.language.randn.rst\",\"python-api/generated/triton.language.ravel.rst\",\"python-api/generated/triton.language.reshape.rst\",\"python-api/generated/triton.language.sigmoid.rst\",\"python-api/generated/triton.language.sin.rst\",\"python-api/generated/triton.language.softmax.rst\",\"python-api/generated/triton.language.sqrt.rst\",\"python-api/generated/triton.language.store.rst\",\"python-api/generated/triton.language.sum.rst\",\"python-api/generated/triton.language.where.rst\",\"python-api/generated/triton.language.zeros.rst\",\"python-api/generated/triton.testing.Benchmark.rst\",\"python-api/generated/triton.testing.do_bench.rst\",\"python-api/generated/triton.testing.perf_report.rst\",\"python-api/triton.rst\",\"python-api/triton.language.rst\",\"python-api/triton.testing.rst\"],objects:{\"triton.Config\":{__init__:[11,1,1,\"\"]},\"triton.language\":{arange:[15,2,1,\"\"],atomic_add:[16,2,1,\"\"],atomic_cas:[17,2,1,\"\"],atomic_max:[18,2,1,\"\"],atomic_min:[19,2,1,\"\"],atomic_xchg:[20,2,1,\"\"],broadcast_to:[21,2,1,\"\"],cos:[22,2,1,\"\"],dot:[23,2,1,\"\"],exp:[24,2,1,\"\"],load:[25,2,1,\"\"],log:[26,2,1,\"\"],max:[27,2,1,\"\"],maximum:[28,2,1,\"\"],min:[29,2,1,\"\"],minimum:[30,2,1,\"\"],multiple_of:[31,2,1,\"\"],num_programs:[32,2,1,\"\"],program_id:[33,2,1,\"\"],rand:[34,2,1,\"\"],randint4x:[36,2,1,\"\"],randint:[35,2,1,\"\"],randn:[37,2,1,\"\"],ravel:[38,2,1,\"\"],reshape:[39,2,1,\"\"],sigmoid:[40,2,1,\"\"],sin:[41,2,1,\"\"],softmax:[42,2,1,\"\"],sqrt:[43,2,1,\"\"],store:[44,2,1,\"\"],sum:[45,2,1,\"\"],where:[46,2,1,\"\"],zeros:[47,2,1,\"\"]},\"triton.testing\":{Benchmark:[48,0,1,\"\"],do_bench:[49,2,1,\"\"],perf_report:[50,2,1,\"\"]},\"triton.testing.Benchmark\":{__init__:[48,1,1,\"\"]},triton:{Config:[11,0,1,\"\"],autotune:[12,2,1,\"\"],heuristics:[13,2,1,\"\"],jit:[14,2,1,\"\"]}},objnames:{\"0\":[\"py\",\"class\",\"Python class\"],\"1\":[\"py\",\"method\",\"Python method\"],\"2\":[\"py\",\"function\",\"Python function\"]},objtypes:{\"0\":\"py:class\",\"1\":\"py:method\",\"2\":\"py:function\"},terms:{\"0\":[1,2,3,4,5,7,9,10,32,33,34,37,47,49],\"00\":7,\"0000\":3,\"000000\":2,\"000001\":[1,2],\"000002\":2,\"000009\":5,\"004273\":1,\"01\":[1,3,7],\"011\":[4,7],\"019048\":3,\"02\":[2,7],\"025776\":3,\"028308\":3,\"03\":[3,7],\"04\":[4,7],\"047592\":3,\"05\":[5,7],\"058574\":2,\"061463\":3,\"0625\":3,\"079368\":5,\"080\":[3,7],\"08199\":4,\"083026\":5,\"08452\":4,\"084721\":1,\"085717\":5,\"088836\":3,\"0938\":3,\"097543\":2,\"0f\":10,\"0s\":4,\"1\":[1,2,3,4,5,8,10,13,32,33,34,37],\"10\":[1,3,4,5,7],\"100\":[2,49],\"1024\":[1,3,4,5,12],\"10240\":5,\"102570\":5,\"1045\":3,\"104745\":5,\"1048576\":1,\"106434\":4,\"10752\":5,\"109587\":3,\"11\":[0,1,3,5],\"111783\":2,\"11264\":5,\"1152\":3,\"117638\":5,\"11776\":5,\"12\":[1,3,5,7],\"120002\":3,\"12160\":2,\"12288\":[2,5],\"123\":4,\"12416\":2,\"12544\":2,\"12672\":2,\"127\":1,\"128\":[1,2,3,5,12],\"1280\":3,\"12800\":5,\"128191\":5,\"13\":[1,3,5],\"131072\":1,\"1328\":3,\"133\":5,\"13312\":5,\"134217728\":1,\"134737\":3,\"135142\":5,\"13686\":4,\"13824\":5,\"138541\":3,\"14\":[1,3,5],\"140799\":3,\"1408\":3,\"142862\":2,\"14336\":5,\"14848\":5,\"149397\":4,\"15\":[1,3,5],\"150050\":3,\"152348\":3,\"153\":2,\"153487\":5,\"1536\":[3,5],\"15360\":5,\"154\":2,\"155572\":3,\"15872\":5,\"16\":[2,3,5,7,10,47],\"160\":2,\"160801\":5,\"162\":[2,5],\"16384\":1,\"1664\":3,\"16777216\":1,\"168834\":5,\"17\":[3,5],\"172588\":3,\"172933\":5,\"17879\":4,\"1792\":3,\"179533\":2,\"179836\":5,\"18\":[3,5],\"180121\":5,\"182\":5,\"1823\":2,\"188\":2,\"19\":[1,3,5],\"190482\":1,\"190689\":5,\"191\":5,\"192\":1,\"1920\":3,\"198\":2,\"1982\":10,\"198220\":5,\"1983\":9,\"1984\":10,\"1989\":10,\"1991\":[9,10],\"1999\":10,\"1d\":[1,2,3],\"1e\":[1,2,3,5],\"1s\":4,\"2\":[1,2,3,4,5,8,10,11,13,32,33,49],\"20\":[3,5,49],\"200000\":1,\"200001\":3,\"200008\":5,\"2004\":10,\"2006\":10,\"2011\":4,\"2012\":10,\"2013\":9,\"2014\":[4,9],\"2016\":[9,10],\"2017\":9,\"2018\":[9,10],\"201833\":5,\"2019\":10,\"2021\":[9,10],\"2048\":[2,3,5],\"207907\":5,\"208\":5,\"2097152\":1,\"21\":[3,5],\"212868\":4,\"2141\":1,\"214186\":4,\"216187\":2,\"2176\":3,\"217818\":5,\"219\":1,\"22\":[2,3,5,7],\"220\":[3,5],\"226931\":3,\"229369\":5,\"23\":[3,5],\"2304\":3,\"231\":5,\"231237\":5,\"233226\":3,\"237674\":3,\"24\":[3,5],\"242\":5,\"242181\":3,\"242569\":5,\"243\":5,\"2432\":3,\"243907\":5,\"245\":[3,5],\"246\":5,\"246765\":5,\"248\":5,\"249\":5,\"25\":[3,5,49],\"251065\":3,\"252\":5,\"253\":5,\"253788\":5,\"254\":5,\"254248\":3,\"256\":[1,2,3,5,11],\"2560\":[3,5],\"256381\":5,\"257\":5,\"26\":[3,5],\"260\":5,\"260869\":3,\"262\":5,\"262144\":1,\"2656\":3,\"267\":5,\"267706\":3,\"267711\":5,\"2688\":3,\"269\":5,\"269692\":3,\"27\":[3,5],\"271186\":5,\"272\":5,\"277\":5,\"28\":[1,3,5],\"280\":5,\"2812\":3,\"2816\":3,\"283\":5,\"284\":5,\"285\":5,\"286\":5,\"288\":5,\"289\":5,\"2891\":3,\"29\":[3,5],\"290\":5,\"291\":5,\"292\":5,\"293429\":4,\"293872\":5,\"294\":5,\"2944\":3,\"298\":5,\"2d\":[3,5,23],\"2m\":2,\"2mn\":2,\"3\":[0,1,2,3,4,5,10],\"30\":3,\"305746\":3,\"305878\":3,\"307\":5,\"3072\":[3,5],\"3076\":1,\"309\":5,\"31\":[3,7],\"310171\":3,\"310338\":5,\"310890\":3,\"3125\":3,\"313973\":5,\"319\":5,\"32\":[3,5,11],\"3200\":3,\"321838\":2,\"325\":5,\"326017\":5,\"327\":5,\"32768\":1,\"3281\":3,\"33\":3,\"3328\":3,\"333321\":1,\"33554432\":1,\"336389\":3,\"338\":5,\"34\":3,\"341\":1,\"34172\":4,\"3438\":3,\"3456\":3,\"3477\":3,\"351\":5,\"3516\":3,\"3555\":3,\"357\":[2,7],\"3584\":[3,5],\"36\":3,\"360017\":2,\"362445\":1,\"364408\":5,\"366\":5,\"368662\":3,\"3712\":3,\"3713\":1,\"371721\":4,\"372618\":3,\"372800\":3,\"373605\":3,\"374\":5,\"38\":1,\"380953\":3,\"383\":5,\"384\":[1,2,3],\"3840\":3,\"384634\":5,\"39\":3,\"3906\":3,\"3968\":3,\"397\":5,\"3984\":3,\"3986\":4,\"3d\":[32,33],\"3mn\":2,\"4\":[1,2,3,5,10,11,12,35],\"40\":3,\"400001\":1,\"400016\":1,\"4023\":3,\"403344\":4,\"403347\":4,\"404\":5,\"406\":2,\"4062\":3,\"408716\":4,\"4096\":[1,2,3,5],\"410081\":5,\"411\":2,\"412\":[2,5],\"412561\":5,\"415\":[2,5],\"41576\":4,\"416\":5,\"4194304\":1,\"42\":[1,7],\"420\":5,\"420235\":3,\"421\":5,\"42142\":4,\"423\":5,\"428\":5,\"428372\":4,\"428568\":1,\"428801\":3,\"429\":5,\"429770\":1,\"430545\":3,\"431969\":4,\"433562\":5,\"438\":5,\"440290\":5,\"447\":5,\"448255\":1,\"4492\":3,\"45\":3,\"4531\":3,\"453844\":5,\"46\":3,\"460\":5,\"4608\":5,\"4609\":3,\"461\":5,\"464170\":5,\"465663\":3,\"4688\":3,\"472\":1,\"473602\":3,\"481278\":5,\"482\":5,\"485074\":3,\"492442\":3,\"4940\":1,\"495\":5,\"497980\":5,\"498981\":2,\"499\":[5,7],\"4m\":2,\"4x\":2,\"5\":[1,3,4,5,10,49],\"500\":5,\"5000\":3,\"501303\":5,\"505\":5,\"507077\":3,\"507772\":5,\"507855\":5,\"51\":3,\"511\":5,\"512\":[2,3,4,5],\"5120\":5,\"513091\":5,\"517949\":5,\"52\":3,\"522391\":3,\"524288\":1,\"525\":5,\"526831\":3,\"527\":5,\"53\":3,\"530610\":2,\"5312\":3,\"535103\":3,\"538\":5,\"54\":3,\"541\":4,\"542\":5,\"545956\":5,\"546756\":2,\"551\":5,\"552988\":3,\"556711\":2,\"559690\":5,\"56\":3,\"5632\":5,\"563555\":3,\"563695\":3,\"566038\":2,\"566152\":3,\"566838\":2,\"568\":5,\"568431\":4,\"571423\":5,\"584279\":3,\"585\":2,\"5859\":3,\"586664\":5,\"586858\":4,\"588687\":3,\"5898\":3,\"595129\":3,\"5mn\":2,\"6\":[0,1,3,5],\"600000\":1,\"600004\":2,\"600589\":5,\"604067\":3,\"604870\":5,\"606\":2,\"608294\":3,\"6094\":3,\"610071\":5,\"614\":1,\"6144\":5,\"615388\":5,\"615390\":1,\"62\":3,\"621881\":3,\"628001\":3,\"63\":[1,3],\"633240\":2,\"64\":[1,3,5],\"640\":[2,3],\"643199\":3,\"64kb\":5,\"650282\":5,\"651187\":5,\"655\":2,\"65536\":[1,5],\"656000\":3,\"656296\":5,\"656574\":1,\"66\":3,\"661056\":5,\"661740\":2,\"664\":2,\"6656\":5,\"666684\":2,\"667843\":5,\"67\":3,\"67086\":4,\"67108864\":1,\"6724\":1,\"673567\":5,\"679984\":5,\"68\":3,\"680186\":3,\"682\":2,\"686414\":5,\"69\":3,\"6953\":3,\"7\":[0,1,3,5,10],\"70\":3,\"7031\":3,\"7070\":3,\"707203\":5,\"707878\":4,\"708969\":5,\"709358\":3,\"71\":3,\"712490\":3,\"7168\":5,\"716830\":2,\"719258\":4,\"72\":3,\"722\":[1,2],\"724127\":5,\"729961\":5,\"73\":3,\"730667\":3,\"737435\":1,\"743443\":4,\"747514\":3,\"748791\":3,\"7500\":3,\"754967\":2,\"76\":[1,3],\"761846\":5,\"767446\":5,\"767458\":5,\"768\":[2,3],\"7680\":5,\"768000\":3,\"768213\":3,\"771425\":3,\"773130\":3,\"776477\":2,\"777595\":3,\"78\":3,\"780\":[1,7],\"781\":2,\"782312\":3,\"783176\":3,\"784108\":3,\"787523\":5,\"79\":3,\"793541\":5,\"795401\":3,\"796351\":5,\"79719\":4,\"8\":[1,2,3,5,10,11,12,47,49],\"80\":[3,49],\"800002\":1,\"806694\":4,\"809875\":3,\"81\":3,\"811163\":1,\"812\":[1,2],\"814\":2,\"814814\":2,\"815384\":5,\"817432\":4,\"8192\":[1,5],\"82\":3,\"823517\":[1,2],\"83\":3,\"833\":[1,7],\"836198\":3,\"838026\":4,\"8388608\":1,\"84\":3,\"842\":1,\"84284\":4,\"843\":1,\"845844\":5,\"847\":1,\"848\":1,\"849\":1,\"849059\":5,\"85\":3,\"850\":1,\"857144\":5,\"857162\":5,\"86\":3,\"87\":3,\"8704\":5,\"88\":3,\"8828\":3,\"8867\":3,\"888887\":3,\"89\":3,\"8906\":3,\"8945\":3,\"895304\":2,\"896\":3,\"896254\":3,\"899428\":3,\"8mn\":2,\"9\":[0,1,2,3,4,5],\"90\":3,\"91\":3,\"911986\":5,\"9216\":5,\"9219\":3,\"928261\":5,\"93\":[2,3],\"935714\":2,\"9375\":3,\"94\":[2,3],\"9492\":3,\"95\":[2,3],\"950501\":5,\"952835\":4,\"9531\":3,\"954908\":5,\"955488\":3,\"958106\":5,\"959706\":3,\"96\":[2,5],\"9688\":3,\"97\":[2,3],\"9728\":5,\"9733\":1,\"976473\":5,\"976885\":3,\"978909\":3,\"98\":2,\"9805\":3,\"982663\":5,\"983276\":3,\"98432\":1,\"9844\":3,\"987724\":5,\"99\":5,\"998493\":3,\"999982\":5,\"999995\":1,\"999998\":1,\"abstract\":[9,10],\"break\":10,\"byte\":2,\"case\":[1,2,9,10,13,16,17,18,19,20],\"class\":[2,5,9,10,11,48],\"default\":49,\"do\":[2,3,9,10,25,44],\"float\":[2,9,10,49],\"function\":[1,2,3,4,5,10,12,13,14,48,49,50],\"import\":[1,2,3,4,5,9,10],\"int\":[1,9,10,13,15,21,32,33,39,47,49],\"new\":[21,39,47],\"return\":[1,2,3,4,5,15,16,17,18,19,20,23,25,27,29,32,33,34,35,36,37,38,45,46,47,49,50],\"static\":[0,9,10],\"super\":3,\"switch\":3,\"true\":[1,2,3,5,46],\"try\":[3,11],\"var\":[5,10],\"voil\\u00e0\":4,\"while\":[3,5,9],A:[3,4,9,10],And:[0,3],As:[2,3,4,9,10],At:[4,10],But:4,By:49,For:[3,9,10,11],If:[4,10,35,44,46,48],In:[1,2,3,4,10],It:[1,3,4,6,8,10,14],Of:9,On:10,One:3,The:[1,2,3,4,9,10,16,17,18,19,20,21,23,32,33,34,35,36,37,39,44,46,50],There:1,These:10,To:[1,4,9,10,12],_:5,__expf:2,__init__:[11,48],_db:5,_dropout:4,_dw:5,_layer_norm_bwd_dwdb:5,_layer_norm_bwd_dx_fus:5,_layer_norm_fwd_fus:5,_matmul:3,_seeded_dropout:4,a100:[3,10],a_ptr:3,ab:1,abl:10,about:[1,2,3,4,8],abov:[1,2,3,4,10,12],academ:9,acc:[3,9,10],acceler:9,access:[1,3,9,10,14],accomod:3,accordingli:10,account:10,accumul:[3,5,10],accuraci:[3,9],achiev:[3,9,10],across:[2,4,9,10],activ:3,actual:[3,9,10],add:[1,4,5,7,16],add_kernel:1,addit:[2,6,7,9,49],addition:10,address:[9,25],adopt:10,advanc:[2,3,9],advoc:10,affect:3,affin:10,after:3,against:[0,1,2,3,8],aggress:[9,10],agnost:[9,10],ahead:10,aim:[2,8],al:[9,10],alex:4,algebra:10,algorithm:[3,4,9,10],alia:10,all:[2,3,4,6,9,10,12,27,29,31,45,48],allclos:[2,3],allen1984:10,allen:10,alloc:[1,2,3,5,9],allow:[1,2,5,9,10],along:[1,3,27,29,32,33,45,49],also:[1,2,3,4,5,9,10],altern:4,alwai:[10,46],amd:9,amen:10,amount:[5,9],ampl:10,an:[1,2,3,4,9,10,11,16,17,18,19,20,34,35,36,37],analog:1,analysi:[9,10],analyz:10,ancourt1991:10,ancourt:10,ani:[1,2,3,10,12,13,48],anoth:[2,10],anytim:12,apart:10,apex:5,apex_layer_norm:5,api:48,appear:48,appli:[3,4,5,9,10],applic:[4,10,13],approach:[9,10],appropri:1,approxim:2,ar:[0,1,2,3,4,9,10,12,14,25,31,44,46,48],arang:[1,2,3,4,5],arbitrari:3,architectur:[3,9],area:10,arg:[1,2,3,5,13,14,48],argument:[1,2,3,11,12,13,14,46,48],arrai:[10,47],arrang:3,art:[9,10],artifici:4,arxiv:[9,10],ask:2,aspect:10,asplo:9,assert:[1,3,4],assert_almost_equ:5,assum:[2,48],asynchron:[1,9],atom:[16,17,18,19,20],atomic_ca:5,atomic_xchg:5,auguin1983:9,auguin:9,auto:[2,3,10,11,12,13],autograd:5,autom:9,automat:[2,3,9,10,11],autotun:[3,10],avail:[0,4,9,10],avoid:[2,12,46],awar:9,awkward:4,axi:[1,2,3,4,5,27,29,32,33,45,48],b:[3,5,9,10],b_ptr:3,back:[1,2,3,4,5],backpropag:4,backward:5,bad:4,baghdadi2021:[9,10],baghdadi:[9,10],balanc:10,bandwidth:2,base:[4,8,9,10],basic:[1,6,10],becom:9,been:[1,9,10],befor:[3,12,16,17,18,19,20],begin:10,behavior:[10,12],being:[2,4],believ:10,below:[4,6,10],bench:0,bench_layer_norm:5,benchmark:[0,5,49,50],benefit:[2,9,10],best:[1,9],between:[1,9],bia:5,bit:4,block:[1,2,3,4,9,10,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,34,35,36,37,38,39,40,41,42,43,44,45,46,47],block_siz:[1,2,4,5,10,12,13],block_size_k:3,block_size_m:[3,5],block_size_n:[3,5],block_start:[1,4],blue:[1,2,3,5],boil:10,bool:[46,48],both:[10,46],bound:[1,2,3,10],branch:10,broad:9,broadcast:[21,25,44,46],buffer:5,build:[0,3],built:[1,10],c:[3,9,10],c_mask:3,c_ptr:3,cach:[9,10,25],cache_modifi:25,call:[1,3,10,14,35],callabl:[1,13,14,49],can:[0,1,2,3,4,9,10,12,50],cannot:[3,9,10],capabl:[8,9],cast:5,cd:0,cdiv:[1,3,4,5],ceil:13,certain:13,cgo:[9,10],challeng:4,chang:[3,4,12,25],chapter:8,characterist:10,cheap:9,check:[3,8],checkpoint:4,chen2018:9,chen:9,chip:2,choic:8,click:[1,2,3,4,5],clone:[0,5],close:10,cmake:0,cmp:[16,17,18,19,20],coalesc:9,code:[1,2,3,4,5,6,9,10],col:[3,5,10],col_offset:2,color:48,column:[2,3],com:0,combin:9,come:[2,3,10],command:0,common:10,commonli:10,compar:[2,3,4,5,8,10,16,17,18,19,20],compat:23,compil:[2,3,8,9,11,14,31],complet:10,complex:10,compos:[4,9],composit:10,comprehens:[9,10],comput:[4,5,8,9,10,13,22,24,26,28,30,40,41,42,43],computation:[9,10],concern:10,concis:[1,48],condit:[10,46],config:[3,12],configur:[3,11,12,50],confirm:2,connectom:9,consecut:10,consequ:9,consid:2,consist:4,constraint:[3,10],construct:9,constructor:48,consum:3,contain:[10,16,17,18,19,20,48],contextu:10,contigu:[3,15,38],control:[9,10],conveni:3,convert:[1,3,14],convolut:9,cooper:11,copi:[4,9,16,17,18,19,20],core:[9,10],correct:1,correspond:[1,2,3,48],cosin:22,cost:10,could:[2,10],count:5,cours:9,cpython:0,creat:[1,2,3,5,9],crucial:4,csv:1,ctx:5,cubla:[3,9],cuda:[1,2,3,4,5,9],cudnn:9,current:33,custom:[1,2,3,8],cut:3,cvpr:9,d:[2,4,12,14],dart:10,darte1999:10,data:[1,3,4,5,9,10,16,17,18,19,20,25,46,47],data_ptr:14,dataflow:10,david:4,db:5,db_ref:5,db_tri:5,deal:4,decad:9,decim:5,declar:1,decompos:10,decor:[1,3,12,13,14],decreas:4,dedic:3,deep:[3,4,9,10],def:[1,2,3,4,5,12,13],defin:[1,2,3,10,25],definit:10,denomin:2,denot:1,dens:10,depend:[0,10,46],deploi:9,describ:[4,10],design:10,desir:[21,39],detail:[3,10],detect:9,develop:[9,10],devic:[1,2,3,5],dg:5,dialect:10,dict:13,dictionari:[11,13],diesel:10,differ:[1,2,3,4,9,10,48],difficult:10,difficulti:[3,9],dijkstra82:10,dijkstra:10,dim:[2,5,10],dimens:[3,23,27,29,45],dimension:[3,10,23],dir:0,direct:3,disjoint:10,disk:1,dissert:10,distribut:[2,4,10],divis:3,dnn:[8,9,10],do_bench:[1,2,3,5],doc:4,doe:[1,2,3,10],doesn:[5,10],domain:[9,10],don:[1,2,3],done:[3,9,27,29,45],dot:3,doubli:3,doubt:10,down:[3,10],download:[0,1,2,3,4,5,6],dram:[1,2],dropout:[6,7],dror:4,dsl:[8,9,10],dtype:[1,2,3,5,16,17,18,19,20,25,44,47],dw:5,dw_ref:5,dw_tri:5,dx:5,dx_ref:5,dx_tri:5,dy:5,e:[0,2,3,4,9,10,47],each:[1,2,3,4,5,9,10,11,13],eas:10,easi:[3,4],easier:[1,2,9],easili:3,ed:[1,3],education:2,effect:10,effici:[3,4,9,36],effort:10,either:[1,32,33,46],elango2018:10,elango:10,element:[1,2,3,4,5,22,24,26,27,28,29,30,40,41,42,43,44,45,46,48],element_s:[2,5],element_ti:[16,17,18,19,20,25,44],elementwis:[2,25],els:[3,5],emerg:9,empti:[3,5],empty_lik:[1,2,4,5],enabl:10,encod:10,encourag:4,end:[9,10,15],enforc:10,engin:10,enqueu:[1,2,5],ensur:10,entir:10,entri:36,environ:8,ep:5,equal:10,error:3,especi:9,et:[4,9,10],euromicro:9,evalu:[3,4,12,46],even:[4,10],evidenc:9,evolv:9,exampl:[1,2,3,4,5,6,9,10,11],exchang:20,execut:[7,9,10,11,50],exist:[9,10],exp:2,expect:[2,16,17,18,19,20],expens:[9,10,13],explor:[4,9],exponenti:[2,24],express:[9,10],extar:1,extend:[3,4],extract:3,extrem:10,f:[1,2,3,10],facilit:[9,10],fact:10,fairli:3,fals:[25,44,46,48,49],far:2,fast:[2,5,9,10],faster:[2,35],fastest:10,featur:5,feel:3,fetch:9,few:10,field:9,figur:10,file:[1,2,3,7],fill:47,final_db:5,final_dw:5,fine:4,first:[1,3,4,5,8,10,23,28,30],first_pid_m:3,firstli:4,fit:2,fix:48,flag:2,flatten:38,flexibl:9,float16:[3,5,23,47],float32:[1,2,3,4,5,23,34,37],flow:[9,10],fly:4,fn:[14,49],focu:[3,10],folder:4,follow:[0,2,3,8,9,10],footprint:4,forc:4,forget:1,formal:10,format:10,forward:5,found:[16,17,18,19,20],foundat:10,four:36,fp16:3,fp32:3,frac:4,framework:[9,10],free:3,from:[1,2,3,4,9,10,25,46],full:[1,2,3,4,5],fulli:10,func:10,fundament:10,further:[4,10],fuse:[3,5,6,7],fusedlayernorm:5,fusion:[2,10],g:[3,4,9,10,47],galleri:[1,2,3,4,5,6],gb:[1,2,5],gbp:[1,2,5],gener:[1,2,3,4,5,6,9,10,34,35,36,37,48],geoffrei:4,geq:10,get:[1,2,3,4,7],girbal2006:10,girbal:10,git:0,github:0,give:9,given:[2,3,4,21,32,33,34,35,36,37,39,47],global:10,go:[1,3,10],good:[1,10],gpgpu:9,gpu:[1,2,4,8,9,10,11,14],grad:5,grad_to_non:[5,49],gradient:[5,49],grammat:10,graphic:9,greater:2,green:[1,2,3,5],grid:[1,2,3,4,5,32,33],grid_m:3,grid_n:3,grosser2012:10,grosser:10,group:3,group_id:3,group_m:3,group_size_m:[3,5],grow:10,guard:[1,2],guid:9,ha:[1,3,4,9,10,32,33],had:1,halid:[9,10],hand:10,handl:[1,2,4,10],handwritten:9,hard:3,harder:10,hardwar:[3,8,10],hasn:1,have:[2,4,9,10,14,23,46,48],heavi:9,helper:[1,2],henc:3,here:[1,2,3,4,5],heurist:[2,5],hierarch:9,hierarchi:10,high:[3,9,10],higher:3,highli:9,highlight:10,hint:10,hinton:4,hit:3,how:[1,2,3,8,9,13],howev:[2,10],html:4,http:[0,4],i:[1,2,3,4,5,9,10],id:[3,33],idea:9,ideal:2,ident:2,identifi:1,idx:[25,44],ilya:4,imag:[9,10],implement:[1,2,3,4,9,10],implicitli:[1,14,25,44],importantli:10,impos:10,improv:[3,4],incompat:[3,10],incorrect:3,increas:[1,2,3,4],incred:9,increment:10,inde:10,independ:[2,5,10],index:1,indic:[10,46],induc:10,industri:9,inequ:10,inf:2,inform:10,infrastructur:10,initi:[1,3],inner:[3,23],inplac:3,input:[1,2,3,4,5,10,13,21,22,23,24,26,27,28,29,30,31,38,39,40,41,42,43,45],input_ptr:2,input_row_strid:2,instal:8,instanc:[1,2,3,4,5,9,11,32,33],instanti:4,instead:[2,46],instruct:[8,9],int1:[25,44],int32:[4,5,35,36],integ:10,interchang:10,interest:[5,9,10],intermedi:10,intern:[2,10],interv:15,intrins:10,introduc:4,introduct:8,invari:[2,10],invoc:4,ipynb:[1,2,3,4,5],ir:10,irregular:[2,10],is_contigu:[3,4],is_cuda:1,isn:3,issu:[9,10],iter:[3,9,10],its:[1,2,3,10],j:[3,9,10],jit:[1,2,3,4,5,12,13],jmlr:4,john:4,johnson:4,journal:10,jrk2013:9,jupyt:[1,2,3,4,5,6],just:[3,10,13],k:[3,4,9,10],kb:9,keep:4,kei:[3,9,12],kellei:9,kernel:[4,5,8,9,11,12,13],keyword:[1,11],ki:10,kind:2,know:31,known:10,krizhevski:4,kwarg:14,l2:5,label:[1,2,3,48],lam1991:9,lam:9,lambda:[1,2,3,4,5,13],languag:[1,2,3,4,5,8,9,14],larg:[9,10],last:3,later:[2,10],latest:0,lattner2004:10,lattner2019:10,lattner:10,launch:[1,2,3,32,33],law:10,layer:[6,7,9,10],layer_norm:5,layernorm:5,lead:[4,9,10],leaky_relu:3,leakyrelu:3,learn:[1,2,3,4,8,9,10],least:10,lee2017:9,lee:9,left:10,legal:10,length:1,less:[4,5,9,10],let:[1,2,4,31],letter:10,level:[3,9,10],li:9,librari:[0,3,9,10],lifelong:10,like:[1,4,9,10,35],limit:[2,4],lindenstrauss:4,line:[1,2,3,4,10,48],line_arg:[1,2,3,5,48],line_nam:[1,2,3,5,48],line_v:[1,2,3,5,48],linear:[9,10],link:0,list:[1,3,12,13,48,49,50],litteratur:10,ll:4,llvm11:0,llvm:[0,10],load:[1,2,3,4,5,10,46],local:[9,10],locat:[3,16,17,18,19,20,25,44],lock:5,lock_id:5,log2:13,log:48,logarithm:[1,26],look:[4,8,9],loop:[3,10,11],low:[6,7,10],m:[0,2,3,5,9],machin:[9,10],machineri:[9,10],made:9,mai:[2,10,13],main:[3,9,10],maintain:[2,10],major:[3,10],make:[1,2,9,10],manag:[4,9],mani:[1,9,10],manual:[2,10],manual_se:[1,2,3],map:3,mapl:10,mark:[4,50],markedli:9,mask:[1,2,3,4,5,16,18,19,20,25,44,46],match:[3,16,17,18,19,20],math:13,mathbb:10,mathbf:10,mathcal:[10,37],mathemat:10,matmul:[3,10],matmul_kernel:3,matric:[2,3],matrix:[2,4,6,7,9,10,11,23],matrix_s:10,matter:[3,9,10],max:[1,2,5,18],max_fused_s:5,max_m:[1,2,3,5],maxim:[8,10,36],maximum:[1,2,27],mb:[7,9],mean1:5,mean2:5,mean:[3,5,10,12],mechan:[2,10],median:49,memori:[1,2,3,6,7,9,10,16,17,18,19,20,25,44,46],mention:3,meta:[1,2,3,4,5,11,12,13],metaparamet:1,method:[10,11,14,48,50],methodolog:10,micro:9,min:[3,5,19],min_m:[1,2,3,5],minimum:29,minut:[1,2,3,4,5],miss:10,mitig:10,ml:9,mlir:10,mn:2,mode:5,model:[1,9,10],modern:[3,8,9,10],modular:10,moor:10,mora:4,more:[2,3,4,8,9,10,48],most:[3,10],mostli:11,move:3,movement:4,ms:[1,2,3,5,49],much:[2,3],mullapudi2016:10,mullapudi:10,multi:[3,9,10],multipl:[1,4,6,7,9,10,11,12,31,35],multipli:[3,4,5,10,23],must:[2,3,15,23,46],n:[2,3,5,9,37],n_col:2,n_element:[1,4],n_row:2,naiv:[2,4],naive_softmax:2,name:[1,2,3,12,13,48],nativ:[1,2,3],natur:[2,9,26],nb:9,necessari:2,need:[1,2,3,4,35],nelement:2,nest:[3,10],net:10,network:[4,9,10],neural:[4,9,10],neurosci:9,never:4,next:[2,3],next_power_of_2:[2,5],nightli:0,nip:9,nitish:4,nn:[3,5],non:9,none:[2,3,5,12,16,18,19,20,25,44,48,49],nonzero:46,norm:[4,5,7],normal:[2,3,6,7],normalized_shap:5,note:[0,1,2,3,4,10,12,14,46],notebook:[1,2,3,4,5,6],notic:[2,10],notori:[3,9],novel:9,now:[1,3],num_pid_in_group:3,num_pid_m:3,num_pid_n:3,num_stag:[3,11],num_warp:[2,3,5,11,12],number:[1,2,3,4,5,10,11,32,34,35,36,37],numel:[1,4,5],numer:[2,9],nvidia:[9,25],o:[2,4],object:[1,3,9,11,12,14,16,17,18,19,20],obtain:1,obvious:2,occur:10,off:5,offer:9,offici:0,offs_am:3,offs_bn:3,offs_cm:3,offs_cn:3,offs_k:3,offset:[1,4,5,34,35,36,37],often:3,omega:10,onc:[2,9,10],one:[2,3,4,5,6,9,10,48],onli:[2,3,4,9,10,14],op:[1,2],open:15,openai:0,opencl:9,oper:[1,2,3,4,6,9,16,17,18,19,20,46],opportun:9,opsila:9,optim:[9,10],option:[1,3,25,44,48,49],orang:5,order:[2,3,6,10],org:4,origin:10,osdi:9,other:[2,3,4,5,8,10,14,23,25,28,30],otherwis:[4,46],our:[1,2,3,9],out:[1,2,3,4,5,8,10],outlin:10,output2:4,output3:4,output:[1,2,3,4,5],output_ptr:[1,2,4],output_row_start_ptr:2,output_row_strid:2,output_torch:1,output_triton:1,over:[2,4,9,10],overfit:4,overflow:2,own:3,p:[4,10],pa:3,packag:14,pact:10,pad:2,par:3,paradigm:[9,10],paragraph:4,parallel:[1,2,3,4,5,8,9,10,11],paralleliz:9,param:13,paramet:[1,3,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50],parametr:9,part:[3,4,10],partial:5,partial_db:5,partial_dw:5,particular:[2,3],particularli:[9,10],partit:9,pass:[1,5,10,11],past:[9,10],path:1,pattern:9,pb:3,peak:10,per:[2,4,5],percentil:49,perf:3,perf_report:[1,2,3,5,48],perform:[1,2,4,9,10,16,17,18,19,20,49],persist:4,person:10,perspect:10,pgm:1,phase:10,philosophi:10,philox:[4,36],pid:[1,3,4,5],pid_m:3,pid_n:3,pip:0,pipelin:[9,10,11],platform:[8,10],pldi:9,plot:[0,1,2,3,48],plot_nam:[1,2,3,5,48],pmatrix:10,point:[1,10,36],pointer:[1,2,4,5,14,16,17,18,19,20,25,44],pointerdtyp:[16,17,18,19,20,25,44],polli:10,polyhedr:9,polyhedra:10,popular:10,portabl:[9,10],pose:9,posit:[5,13],possibl:[1,2,3,10,11],power:[2,4,10,13,15],ppopp:10,practic:[1,2,3,9],pragma:9,pre:[0,9],prealloc:1,predict:10,prefer:2,premis:9,present:[0,3],preserv:10,preserve_rng_st:4,prevent:[4,10],primer:10,primit:[9,14],principl:10,print:[1,2,3,4],print_data:[1,2,3,5],prng:4,probabl:[4,10],problem:1,problemat:10,procedur:10,process:[1,5,9,10],processor:9,produc:[3,4],product:[8,10,23],program:[1,2,3,4,5,8,9,32,33],program_id:[1,2,3,4,5],programm:[9,10],prohibitev:13,project:[4,9],promot:[3,10],properli:2,properti:10,propos:9,proprietari:3,provid:[1,2,3,4,5,8,10,12,27,29,45,49],prune:4,pseudo:[3,4,36],pseudorandom:4,ptr:3,ptx:25,purpos:[9,10],push:10,put:4,py:[0,1,2,3,4,5,7],pypi:0,pytest:0,python:[1,2,3,4,5,6,14],pytorch:[1,2,4],qquad:10,r:[0,2],ragan:9,rais:5,rand:[1,4,5],randint4x:35,randn:[2,3,4,5],randn_lik:5,random:[4,34,35,36,37],randomli:4,rang:[1,2,3,5,9,10],rapidli:[9,10],rate:3,rather:9,raw:1,rdom:10,re:[1,3],read:[2,3,6],reader:10,real:9,reason:10,recent:9,recommend:6,recomput:[4,9],record_clock:49,rectifi:9,redmon2016:9,redmon:9,reduct:[2,5,27,29,45],refer:1,regardless:[4,46],regim:4,regrett:9,regular:[4,10],rel:[1,10],relat:8,releas:[0,5,9],reli:10,relu:3,remain:[9,48],rememb:3,reorder:10,rep:[5,49],repetit:49,repres:[2,3,10,11],requir:[0,2,4,10],requires_grad:5,requires_grad_:5,research:[9,10],reset:[12,49],reset_to_zero:12,reshap:5,resolut:10,resourc:9,resp:10,respect:10,restrict:10,result:[0,1,2,9,10],ret:2,retain_graph:5,retriev:10,reus:3,revisit:9,right:10,rise:10,role:10,ron:4,root:43,roughli:3,row:[2,3,4,5],row_idx:2,row_minus_max:2,row_start_ptr:2,rstd:5,run:[0,1,2,3,4,5,8,10,12,14,50],runtim:[10,49],runtimeerror:5,ruslan:4,rvar:10,s:[1,2,4,5,10,36],said:10,salakhutdinov:4,salmon2011:4,salmon:4,same:[4,9,48],sato2019:10,sato:10,save:[1,2,3],save_for_backward:5,save_path:[1,5],saved_tensor:5,sc:10,scalabl:10,scalar:[4,9,23,34,35,36,37,47],scale:48,scan:10,schedul:9,scienc:10,scientif:10,scop:10,scope:10,script:[0,1,2,3,4,5],second:[1,2,3,4,5,10,23,28,30],secondli:4,section:[3,10],see:[1,2,3,4,10],seed:[34,35,36,37],seeded_dropout:4,seem:[1,10],select:[9,10,46],self:[11,48],semant:10,semi:10,sens:[1,9,10],separ:[5,10],sequenc:9,set:[1,4,10],setup:0,sever:[9,10],shall:10,shape:[2,3,4,5,10,21,25,39,44,46,47],share:9,shaw:4,shift:2,should:[1,3,9,10,11,27,29,45,48],show_plot:[1,2,3],shown:10,side:10,sight:10,signal:9,significantli:2,sigplan:10,simd:9,simpl:[1,2,3,4],simplest:6,simpli:10,simplic:3,simplifi:4,sinc:[1,2,3],sine:41,singl:[2,4,9,35],size:[1,2,4,10],slower:[9,10],slowest:10,sm80:11,sm:10,smaller:[3,4],smallest:[2,13],snemi3d:9,so:[1,2,3,4,10],softmax:[4,6,7],softmax_kernel:2,softmax_output:2,softwar:11,solid:10,solut:3,solv:10,some:3,sometim:10,sourc:[1,2,3,4,5,6,10],space:[9,10],spars:[4,9,10],spatial:10,speak:3,special:9,specif:[3,9],specifi:[10,13,16,17,18,19,20,44],speed:2,sphinx:[1,2,3,4,5,6],split:10,spmd:[1,9,10],sqrt:5,squar:43,sram:[2,3,5],srivastava2014:4,srivastava:4,stabil:2,stabl:0,stage:11,stai:5,standard:10,start:[5,6,15],started_tutori:7,state:[4,9,10],statement:10,staticmethod:5,std:5,step:10,still:[1,2,3,10],stop:15,store:[1,2,3,4,5,16,17,18,19,20,46],str:[12,13,25,48],straightforward:3,strategi:[4,10],stream:[5,35],strength:9,stride:[2,3,4,5],stride_ak:3,stride_am:3,stride_bk:3,stride_bn:3,stride_cm:3,stride_cn:3,stride_xi:3,stride_xj:3,structur:[9,10],style:[1,2,3,5,48],subscript:10,substanti:9,substract:2,subtract:2,successfulli:10,suffer:10,suit:9,sum:[1,2,5],sum_db:5,sum_dw:5,superhuman:9,support:[4,5,10],sure:2,surprisingli:9,surround:10,suspicion:2,sutskev:[4,9],sutskever2014:9,swap:[16,17,18,19,20],swizzl:9,synchron:[1,9],system:[0,3,9,10],t:[1,2,3,5,10],t_:10,tabul:4,taco:10,take:[3,4,8,13],taken:10,target:9,techniqu:[3,9,10],temperatur:4,tempor:10,tend:10,tension:9,tensor:[1,2,3,4,5,9,10,12,14,49],tensorrt:9,test:[0,1,5,8],test_layer_norm:5,text:10,tflop:3,th:49,than:[2,3,5,9,10,35,48],thei:[3,9,10],them:1,themselv:3,theoret:2,therebi:10,therefor:3,theta:10,theta_:10,thi:[1,2,3,4,5,9,10,12,13,14,36,48],thing:[1,4],think:2,those:2,though:[9,10],thought:10,thread:[2,9,11],through:[6,10],throughout:[10,48],throughput:8,tile:10,time:[0,1,2,3,4,5,9,10,12,35,49],tiramisu:[9,10],tl:[1,2,3,4,5,47],tmp:0,tog:10,togeth:4,tolist:4,topic:10,torch:[1,2,3,4,5,14,49],torch_output:3,torch_relu:3,total:[1,2,3,4,5,7],tradit:[4,9,10],transform:[4,10],travers:10,trend:9,tri:[21,39],trick:2,tricki:4,trigger:[3,12],triton:[0,1,2,3,4,5,6,9,10],triton_output:3,trivial:9,tune:[2,3,10,12,13],tuner:11,tupl:[1,21,39,47],tutori:[1,2,3,4,8],tutorials_jupyt:6,tutorials_python:6,tvm:[9,10],two:[1,2,3,10,12,13,15,23],txt:0,type:[13,23,25,46,47],typecast:[25,44],typic:10,u:[0,34],un:10,uncommon:10,underneath:10,understand:2,undesir:12,unfortun:[3,10],unifi:9,uniformli:4,unint:46,unit:[0,9],univers:10,unrol:10,up:2,updat:[3,10,12],us:[1,2,3,4,5,9,10,11,12,13,14,35,46,48,50],util:[1,5],v100:10,v:5,val:[16,17,18,19,20],valid:1,valu:[1,2,3,4,12,13,15,16,17,18,19,20,22,24,25,26,27,29,31,40,41,42,43,44,45,46,47,48,50],valuabl:2,variabl:[3,11],variant:9,variou:6,vasilach:[9,10],vasilache2018:[9,10],vast:10,vec:10,vector:[4,6,7,9,10],vendor:3,veri:[2,4,10],verif:10,verifi:[2,10],via:10,view:38,visibl:10,vision:9,vs:0,w:[5,10],w_shape:5,wa:4,wai:[2,3,4],want:[2,4,46],warmup:49,warp:[2,5,11],wast:2,wdy:5,we:[1,2,3,4,9,10],weight:5,well:[4,9,10],whatev:12,wheel:0,when:[2,3,4,9,10,11,12,14,46],where:[1,3,4,5,10,13,44],whether:[9,48],which:[1,2,3,4,5,9,10,12,27,29,45,48],whose:[1,2,3,4,10,12,25],wide:10,wise:[1,2,22,24,26,28,30,40,41,42,43,44],wish:[3,10],within:[3,14,15],without:10,wolf:10,wolfe1989:10,won:2,word:10,work:[2,4,8,9],workload:[3,11],wors:[3,9,10],would:[1,2,4],wouldn:10,wrapper:3,write:[1,2,3,4,5,6,8,10],wrote:2,x:[1,2,3,4,5,10,22,24,26,28,30,38,40,41,42,43,46,48],x_arg:5,x_keep:4,x_keep_ptr:4,x_log:[1,48],x_max:2,x_name:[1,2,3,5,48],x_ptr:[1,4,12,13],x_shape:5,x_size:[12,13],x_val:[1,2,3,5,48],xhat:5,xi:10,xii:10,xlabel:48,xmean:5,xo:10,y:[1,2,3,5,10,28,30,46,48],y_fwd:5,y_log:48,y_name:[1,2],y_ptr:1,y_ref:5,y_torch:2,y_tri:5,y_triton:2,year:10,yet:[9,10],yi:10,yield:46,yii:10,ylabel:[1,2,3,5,48],yo:10,you:[0,1,2,3,4,6,9,12,35,46],your:[0,1,8],yourself:[2,3],z:[1,2,10],zero:[3,4,5,12],zip:6},titles:[\"Installation\",\"Vector Addition\",\"Fused Softmax\",\"Matrix Multiplication\",\"Low-Memory Dropout\",\"Layer Normalization\",\"Tutorials\",\"Computation times\",\"Welcome to Triton\\u2019s documentation!\",\"Introduction\",\"Related Work\",\"triton.Config\",\"triton.autotune\",\"triton.heuristics\",\"triton.jit\",\"triton.language.arange\",\"triton.language.atomic_add\",\"triton.language.atomic_cas\",\"triton.language.atomic_max\",\"triton.language.atomic_min\",\"triton.language.atomic_xchg\",\"triton.language.broadcast_to\",\"triton.language.cos\",\"triton.language.dot\",\"triton.language.exp\",\"triton.language.load\",\"triton.language.log\",\"triton.language.max\",\"triton.language.maximum\",\"triton.language.min\",\"triton.language.minimum\",\"triton.language.multiple_of\",\"triton.language.num_programs\",\"triton.language.program_id\",\"triton.language.rand\",\"triton.language.randint\",\"triton.language.randint4x\",\"triton.language.randn\",\"triton.language.ravel\",\"triton.language.reshape\",\"triton.language.sigmoid\",\"triton.language.sin\",\"triton.language.softmax\",\"triton.language.sqrt\",\"triton.language.store\",\"triton.language.sum\",\"triton.language.where\",\"triton.language.zeros\",\"triton.testing.Benchmark\",\"triton.testing.do_bench\",\"triton.testing.perf_report\",\"triton\",\"triton.language\",\"triton.testing\"],titleterms:{\"final\":3,addit:1,advantag:10,algebra:52,api:8,arang:15,arithmet:3,atom:52,atomic_add:16,atomic_ca:17,atomic_max:18,atomic_min:19,atomic_xchg:20,autotun:12,baselin:4,benchmark:[1,2,3,48],binari:0,broadcast_to:21,cach:3,challeng:9,co:22,comparison:52,compil:[10,52],comput:[1,2,3,7],config:11,creation:52,distribut:0,do_bench:49,document:8,dot:23,dropout:4,exercis:4,exp:24,from:0,further:8,fuse:2,gener:52,get:8,go:8,heurist:13,hint:52,index:52,instal:0,introduct:9,jit:14,kernel:[1,2,3],l2:3,languag:[10,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,52],layer:5,limit:10,linear:52,load:25,log:26,low:4,manipul:52,math:52,matrix:3,max:27,maximum:28,memori:[4,52],min:29,minimum:30,model:52,motiv:[2,3,9],multipl:3,multiple_of:31,normal:5,num_program:32,number:52,op:52,optim:3,packag:0,perf_report:50,perform:3,pointer:3,polyhedr:10,program:[10,52],program_id:33,python:[0,8],rand:34,randint4x:36,randint:35,randn:37,random:52,ravel:38,reduct:52,refer:[4,9,10],relat:10,represent:10,reshap:39,result:3,s:8,schedul:10,seed:4,shape:52,sigmoid:40,sin:41,softmax:[2,42],sourc:0,sqrt:43,squar:3,start:8,store:44,sum:45,test:[2,3,48,49,50,53],time:7,triton:[8,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53],tutori:6,unit:[2,3],vector:1,welcom:8,where:46,work:10,zero:47}})\n\\ No newline at end of file\n+Search.setIndex({docnames:[\"getting-started/installation\",\"getting-started/tutorials/01-vector-add\",\"getting-started/tutorials/02-fused-softmax\",\"getting-started/tutorials/03-matrix-multiplication\",\"getting-started/tutorials/04-low-memory-dropout\",\"getting-started/tutorials/05-layer-norm\",\"getting-started/tutorials/index\",\"getting-started/tutorials/sg_execution_times\",\"index\",\"programming-guide/chapter-1/introduction\",\"programming-guide/chapter-2/related-work\",\"python-api/generated/triton.Config\",\"python-api/generated/triton.autotune\",\"python-api/generated/triton.heuristics\",\"python-api/generated/triton.jit\",\"python-api/generated/triton.language.arange\",\"python-api/generated/triton.language.atomic_add\",\"python-api/generated/triton.language.atomic_cas\",\"python-api/generated/triton.language.atomic_max\",\"python-api/generated/triton.language.atomic_min\",\"python-api/generated/triton.language.atomic_xchg\",\"python-api/generated/triton.language.broadcast_to\",\"python-api/generated/triton.language.cos\",\"python-api/generated/triton.language.dot\",\"python-api/generated/triton.language.exp\",\"python-api/generated/triton.language.load\",\"python-api/generated/triton.language.log\",\"python-api/generated/triton.language.max\",\"python-api/generated/triton.language.maximum\",\"python-api/generated/triton.language.min\",\"python-api/generated/triton.language.minimum\",\"python-api/generated/triton.language.multiple_of\",\"python-api/generated/triton.language.num_programs\",\"python-api/generated/triton.language.program_id\",\"python-api/generated/triton.language.rand\",\"python-api/generated/triton.language.randint\",\"python-api/generated/triton.language.randint4x\",\"python-api/generated/triton.language.randn\",\"python-api/generated/triton.language.ravel\",\"python-api/generated/triton.language.reshape\",\"python-api/generated/triton.language.sigmoid\",\"python-api/generated/triton.language.sin\",\"python-api/generated/triton.language.softmax\",\"python-api/generated/triton.language.sqrt\",\"python-api/generated/triton.language.store\",\"python-api/generated/triton.language.sum\",\"python-api/generated/triton.language.where\",\"python-api/generated/triton.language.zeros\",\"python-api/generated/triton.testing.Benchmark\",\"python-api/generated/triton.testing.do_bench\",\"python-api/generated/triton.testing.perf_report\",\"python-api/triton\",\"python-api/triton.language\",\"python-api/triton.testing\"],envversion:{\"sphinx.domains.c\":2,\"sphinx.domains.changeset\":1,\"sphinx.domains.citation\":1,\"sphinx.domains.cpp\":4,\"sphinx.domains.index\":1,\"sphinx.domains.javascript\":2,\"sphinx.domains.math\":2,\"sphinx.domains.python\":3,\"sphinx.domains.rst\":2,\"sphinx.domains.std\":2,\"sphinx.ext.intersphinx\":1,sphinx:56},filenames:[\"getting-started/installation.rst\",\"getting-started/tutorials/01-vector-add.rst\",\"getting-started/tutorials/02-fused-softmax.rst\",\"getting-started/tutorials/03-matrix-multiplication.rst\",\"getting-started/tutorials/04-low-memory-dropout.rst\",\"getting-started/tutorials/05-layer-norm.rst\",\"getting-started/tutorials/index.rst\",\"getting-started/tutorials/sg_execution_times.rst\",\"index.rst\",\"programming-guide/chapter-1/introduction.rst\",\"programming-guide/chapter-2/related-work.rst\",\"python-api/generated/triton.Config.rst\",\"python-api/generated/triton.autotune.rst\",\"python-api/generated/triton.heuristics.rst\",\"python-api/generated/triton.jit.rst\",\"python-api/generated/triton.language.arange.rst\",\"python-api/generated/triton.language.atomic_add.rst\",\"python-api/generated/triton.language.atomic_cas.rst\",\"python-api/generated/triton.language.atomic_max.rst\",\"python-api/generated/triton.language.atomic_min.rst\",\"python-api/generated/triton.language.atomic_xchg.rst\",\"python-api/generated/triton.language.broadcast_to.rst\",\"python-api/generated/triton.language.cos.rst\",\"python-api/generated/triton.language.dot.rst\",\"python-api/generated/triton.language.exp.rst\",\"python-api/generated/triton.language.load.rst\",\"python-api/generated/triton.language.log.rst\",\"python-api/generated/triton.language.max.rst\",\"python-api/generated/triton.language.maximum.rst\",\"python-api/generated/triton.language.min.rst\",\"python-api/generated/triton.language.minimum.rst\",\"python-api/generated/triton.language.multiple_of.rst\",\"python-api/generated/triton.language.num_programs.rst\",\"python-api/generated/triton.language.program_id.rst\",\"python-api/generated/triton.language.rand.rst\",\"python-api/generated/triton.language.randint.rst\",\"python-api/generated/triton.language.randint4x.rst\",\"python-api/generated/triton.language.randn.rst\",\"python-api/generated/triton.language.ravel.rst\",\"python-api/generated/triton.language.reshape.rst\",\"python-api/generated/triton.language.sigmoid.rst\",\"python-api/generated/triton.language.sin.rst\",\"python-api/generated/triton.language.softmax.rst\",\"python-api/generated/triton.language.sqrt.rst\",\"python-api/generated/triton.language.store.rst\",\"python-api/generated/triton.language.sum.rst\",\"python-api/generated/triton.language.where.rst\",\"python-api/generated/triton.language.zeros.rst\",\"python-api/generated/triton.testing.Benchmark.rst\",\"python-api/generated/triton.testing.do_bench.rst\",\"python-api/generated/triton.testing.perf_report.rst\",\"python-api/triton.rst\",\"python-api/triton.language.rst\",\"python-api/triton.testing.rst\"],objects:{\"triton.Config\":{__init__:[11,1,1,\"\"]},\"triton.language\":{arange:[15,2,1,\"\"],atomic_add:[16,2,1,\"\"],atomic_cas:[17,2,1,\"\"],atomic_max:[18,2,1,\"\"],atomic_min:[19,2,1,\"\"],atomic_xchg:[20,2,1,\"\"],broadcast_to:[21,2,1,\"\"],cos:[22,2,1,\"\"],dot:[23,2,1,\"\"],exp:[24,2,1,\"\"],load:[25,2,1,\"\"],log:[26,2,1,\"\"],max:[27,2,1,\"\"],maximum:[28,2,1,\"\"],min:[29,2,1,\"\"],minimum:[30,2,1,\"\"],multiple_of:[31,2,1,\"\"],num_programs:[32,2,1,\"\"],program_id:[33,2,1,\"\"],rand:[34,2,1,\"\"],randint4x:[36,2,1,\"\"],randint:[35,2,1,\"\"],randn:[37,2,1,\"\"],ravel:[38,2,1,\"\"],reshape:[39,2,1,\"\"],sigmoid:[40,2,1,\"\"],sin:[41,2,1,\"\"],softmax:[42,2,1,\"\"],sqrt:[43,2,1,\"\"],store:[44,2,1,\"\"],sum:[45,2,1,\"\"],where:[46,2,1,\"\"],zeros:[47,2,1,\"\"]},\"triton.testing\":{Benchmark:[48,0,1,\"\"],do_bench:[49,2,1,\"\"],perf_report:[50,2,1,\"\"]},\"triton.testing.Benchmark\":{__init__:[48,1,1,\"\"]},triton:{Config:[11,0,1,\"\"],autotune:[12,2,1,\"\"],heuristics:[13,2,1,\"\"],jit:[14,2,1,\"\"]}},objnames:{\"0\":[\"py\",\"class\",\"Python class\"],\"1\":[\"py\",\"method\",\"Python method\"],\"2\":[\"py\",\"function\",\"Python function\"]},objtypes:{\"0\":\"py:class\",\"1\":\"py:method\",\"2\":\"py:function\"},terms:{\"0\":[1,2,3,4,5,7,9,10,32,33,34,37,47,49],\"00\":7,\"0000\":3,\"000000\":2,\"000001\":[1,2],\"000002\":2,\"004273\":1,\"006885\":5,\"01\":[1,3,7],\"011\":[4,7],\"02\":[2,7],\"025776\":3,\"028308\":3,\"03\":[3,7],\"033784\":5,\"035620\":3,\"038096\":2,\"04\":[4,7],\"046389\":5,\"047592\":3,\"05\":[5,7],\"056329\":5,\"058574\":2,\"061463\":3,\"0625\":3,\"069228\":2,\"08199\":4,\"08452\":4,\"084721\":1,\"088617\":5,\"0938\":3,\"096582\":3,\"097543\":2,\"0f\":10,\"0s\":4,\"1\":[1,2,3,4,5,8,10,13,32,33,34,37],\"10\":[1,3,4,5],\"100\":[2,49],\"1024\":[1,3,4,5,12],\"10240\":5,\"102553\":5,\"1045\":3,\"1048576\":1,\"106434\":4,\"10752\":5,\"109587\":3,\"11\":[0,1,3,5],\"111783\":2,\"11264\":5,\"1152\":3,\"117638\":5,\"11776\":5,\"12\":[1,3,5,7],\"12160\":2,\"12288\":[2,5],\"123\":4,\"12416\":2,\"12544\":2,\"125805\":5,\"12672\":2,\"127\":1,\"128\":[1,2,3,5,12],\"1280\":3,\"12800\":5,\"13\":[1,3,5],\"130825\":3,\"131072\":1,\"1328\":3,\"133\":5,\"13312\":5,\"133347\":2,\"134217728\":1,\"134737\":3,\"13686\":4,\"13824\":5,\"138541\":3,\"14\":[1,3,5],\"140799\":3,\"1408\":3,\"142862\":2,\"14336\":5,\"146178\":5,\"14848\":5,\"149397\":4,\"15\":[1,3,5],\"150050\":3,\"151\":2,\"1536\":[3,5],\"15360\":5,\"155572\":3,\"156\":2,\"15872\":5,\"16\":[2,3,5,10,47],\"160\":2,\"160801\":5,\"162\":5,\"163\":2,\"16384\":1,\"1664\":3,\"16777216\":1,\"17\":[3,5],\"171410\":1,\"172588\":3,\"17879\":4,\"1792\":3,\"18\":[3,5],\"181817\":2,\"182\":5,\"182111\":5,\"1823\":2,\"184041\":5,\"186\":2,\"19\":[1,3,5,7],\"190482\":1,\"190689\":5,\"191\":5,\"191546\":3,\"192\":1,\"1920\":3,\"197579\":2,\"198\":2,\"1982\":10,\"1983\":9,\"1984\":10,\"1989\":10,\"199\":2,\"1991\":[9,10],\"1999\":10,\"1d\":[1,2,3],\"1e\":[1,2,3,5],\"1s\":4,\"2\":[1,2,3,4,5,8,10,11,13,32,33,49],\"20\":[2,3,5,7,49],\"200000\":1,\"200001\":3,\"2004\":10,\"2006\":10,\"2011\":4,\"2012\":10,\"2013\":9,\"2014\":[4,9],\"2016\":[9,10],\"2017\":9,\"2018\":[9,10],\"201833\":5,\"2019\":10,\"2021\":[9,10],\"2048\":[2,3,5],\"208\":5,\"2097152\":1,\"21\":[3,5],\"211821\":3,\"212868\":4,\"2141\":1,\"214186\":4,\"216187\":2,\"2176\":3,\"219\":1,\"22\":[3,5],\"220\":[3,5],\"222812\":2,\"23\":[3,5],\"2304\":3,\"231\":5,\"231237\":5,\"24\":[3,5],\"242\":5,\"242007\":5,\"242181\":3,\"2432\":3,\"245\":[3,5],\"246\":5,\"249\":5,\"25\":[3,5,49],\"250\":5,\"253\":5,\"253256\":5,\"254\":5,\"255\":5,\"256\":[1,2,3,5,11],\"2560\":[3,5],\"257\":5,\"259\":5,\"26\":[3,5],\"260858\":5,\"260869\":3,\"262\":5,\"262144\":1,\"263\":5,\"2656\":3,\"266\":5,\"2688\":3,\"269692\":3,\"27\":[3,5],\"271186\":5,\"272\":5,\"276800\":3,\"277\":5,\"279\":5,\"28\":[1,3,5],\"281\":5,\"2812\":3,\"2816\":3,\"284\":5,\"285\":5,\"286\":5,\"287\":5,\"288\":5,\"289\":5,\"2891\":3,\"29\":[3,5],\"290\":5,\"291\":5,\"292\":5,\"293429\":4,\"294\":5,\"2944\":3,\"2d\":[3,5,23],\"2m\":2,\"2mn\":2,\"3\":[0,1,2,3,4,5,10],\"30\":3,\"300\":5,\"305746\":3,\"305878\":3,\"3072\":[3,5],\"3076\":1,\"309\":5,\"31\":3,\"310171\":3,\"310890\":3,\"311\":5,\"312\":5,\"3125\":3,\"314295\":5,\"32\":[3,5,11],\"3200\":3,\"322572\":5,\"325\":5,\"32768\":1,\"3281\":3,\"33\":3,\"330\":5,\"330190\":3,\"3328\":3,\"333321\":1,\"33554432\":1,\"338\":5,\"34\":[3,7],\"341\":1,\"34172\":4,\"3438\":3,\"3456\":3,\"3477\":3,\"349744\":5,\"3516\":3,\"354\":5,\"3555\":3,\"3584\":[3,5],\"36\":3,\"360017\":2,\"362445\":1,\"364016\":5,\"368\":5,\"3712\":3,\"3713\":1,\"371721\":4,\"372800\":3,\"373\":5,\"38\":1,\"380032\":3,\"380953\":3,\"381\":5,\"384\":[1,2,3],\"3840\":3,\"384000\":3,\"39\":3,\"3906\":3,\"394084\":5,\"3968\":3,\"3984\":3,\"3986\":4,\"399\":5,\"3d\":[32,33],\"3mn\":2,\"4\":[1,2,3,5,10,11,12,35],\"40\":3,\"400001\":1,\"400016\":1,\"4023\":3,\"403344\":4,\"403347\":4,\"404\":5,\"405\":2,\"4062\":3,\"406514\":5,\"406770\":5,\"408716\":4,\"4096\":[1,2,3,5],\"41\":[1,7],\"410\":5,\"410081\":5,\"411\":2,\"412\":2,\"412561\":5,\"415\":[2,5],\"41576\":4,\"416301\":5,\"417\":5,\"419\":5,\"4194304\":1,\"420\":5,\"421\":5,\"42142\":4,\"428372\":4,\"428568\":1,\"428801\":3,\"429\":5,\"429770\":1,\"429842\":5,\"430545\":3,\"431\":5,\"431969\":4,\"433562\":5,\"435547\":5,\"438\":5,\"446\":5,\"446623\":3,\"447482\":5,\"448255\":1,\"4492\":3,\"45\":3,\"4531\":3,\"456\":[1,7],\"46\":3,\"4608\":5,\"4609\":3,\"461\":5,\"461542\":5,\"463\":5,\"464170\":5,\"465340\":5,\"467336\":3,\"4688\":3,\"472\":1,\"481\":5,\"483477\":5,\"484366\":3,\"485074\":3,\"492442\":3,\"4940\":1,\"495460\":5,\"496569\":5,\"497980\":5,\"498981\":2,\"4m\":2,\"4x\":2,\"5\":[1,3,4,5,10,49],\"500\":5,\"5000\":3,\"501303\":5,\"504980\":5,\"507855\":5,\"509933\":5,\"509987\":3,\"51\":3,\"511\":5,\"512\":[2,3,4,5],\"5120\":5,\"513\":5,\"518385\":5,\"52\":3,\"521884\":3,\"522\":5,\"524288\":1,\"526831\":3,\"527914\":5,\"528374\":5,\"528704\":3,\"530996\":5,\"5312\":3,\"534\":5,\"54\":3,\"541\":4,\"542\":5,\"546\":2,\"547\":5,\"552988\":3,\"56\":3,\"5632\":5,\"563555\":3,\"563695\":3,\"565214\":5,\"566838\":2,\"568\":5,\"568431\":4,\"585\":2,\"5859\":3,\"586858\":4,\"588687\":3,\"5898\":3,\"590365\":5,\"591490\":5,\"5mn\":2,\"6\":[0,1,3,5],\"600000\":1,\"604870\":5,\"605636\":5,\"606\":2,\"608294\":3,\"6094\":3,\"609561\":3,\"612060\":3,\"614\":1,\"6144\":5,\"615390\":1,\"617366\":5,\"619613\":5,\"62\":3,\"623865\":5,\"625518\":3,\"63\":1,\"630872\":5,\"633240\":2,\"64\":[1,3,5],\"640\":[2,3],\"64kb\":5,\"655\":2,\"65536\":[1,5],\"656000\":3,\"656574\":1,\"656871\":3,\"66\":3,\"664\":2,\"6656\":5,\"666684\":2,\"667485\":5,\"67\":3,\"67086\":4,\"67108864\":1,\"671458\":5,\"6724\":1,\"673582\":5,\"676257\":3,\"68\":3,\"680186\":3,\"682\":2,\"686414\":5,\"69\":3,\"694924\":5,\"6953\":3,\"696898\":5,\"698115\":5,\"698415\":3,\"7\":[0,1,3,5,10],\"70\":3,\"702\":1,\"7031\":3,\"703707\":2,\"7070\":3,\"707878\":4,\"71\":3,\"7168\":5,\"719258\":4,\"72\":3,\"721817\":5,\"722\":[1,2],\"722274\":2,\"726817\":5,\"73\":3,\"730667\":3,\"734120\":3,\"737435\":1,\"743443\":4,\"748791\":3,\"7500\":3,\"754967\":5,\"755369\":2,\"755985\":2,\"757583\":5,\"758863\":2,\"759409\":3,\"76\":[1,3],\"760001\":5,\"763\":[5,7],\"765\":[2,7],\"766291\":3,\"768\":[2,3],\"7680\":5,\"768000\":3,\"77\":3,\"775181\":5,\"775503\":5,\"780\":1,\"781\":2,\"786952\":5,\"79\":3,\"791980\":5,\"79719\":4,\"8\":[1,2,3,5,10,11,12,47,49],\"80\":[3,49],\"800002\":1,\"806694\":4,\"809875\":3,\"81\":3,\"812\":[1,2],\"814\":2,\"814814\":2,\"817432\":4,\"8192\":[1,5],\"82\":3,\"823517\":[1,2],\"826879\":5,\"83\":3,\"832567\":3,\"833\":1,\"833728\":3,\"834951\":2,\"836366\":5,\"838026\":4,\"8388608\":1,\"839992\":2,\"84\":3,\"841203\":3,\"842\":1,\"84284\":4,\"843364\":5,\"845844\":5,\"847\":1,\"848\":1,\"849\":1,\"85\":3,\"850\":1,\"857144\":5,\"86\":3,\"867254\":3,\"867982\":5,\"87\":3,\"8704\":5,\"872604\":5,\"877538\":3,\"879370\":5,\"88\":3,\"880639\":3,\"8828\":3,\"8867\":3,\"888756\":3,\"888887\":3,\"89\":3,\"8906\":3,\"893618\":5,\"8945\":3,\"895397\":3,\"896\":3,\"897767\":5,\"899428\":3,\"8mn\":2,\"9\":[0,1,2,3,4,5],\"90\":3,\"900318\":3,\"902\":7,\"902809\":5,\"908\":[3,7],\"908442\":3,\"908642\":3,\"91\":3,\"916747\":3,\"92\":3,\"9216\":5,\"9219\":3,\"922209\":5,\"929456\":3,\"93\":[2,3],\"9375\":3,\"94\":2,\"942121\":5,\"948562\":3,\"9492\":3,\"95\":[2,3],\"952835\":4,\"9531\":3,\"954908\":5,\"959706\":3,\"96\":[2,5],\"963085\":2,\"9688\":3,\"97\":[2,3],\"9728\":5,\"9733\":1,\"98\":2,\"9805\":3,\"983276\":3,\"98432\":1,\"9844\":3,\"99\":5,\"990003\":3,\"995628\":3,\"998493\":3,\"999982\":5,\"999995\":1,\"999998\":1,\"abstract\":[9,10],\"break\":10,\"byte\":2,\"case\":[1,2,9,10,13,16,17,18,19,20],\"class\":[2,5,9,10,11,48],\"default\":49,\"do\":[2,3,9,10,25,44],\"float\":[2,9,10,49],\"function\":[1,2,3,4,5,10,12,13,14,48,49,50],\"import\":[1,2,3,4,5,9,10],\"int\":[1,9,10,13,15,21,32,33,39,47,49],\"new\":[21,39,47],\"return\":[1,2,3,4,5,15,16,17,18,19,20,23,25,27,29,32,33,34,35,36,37,38,45,46,47,49,50],\"static\":[0,9,10],\"super\":3,\"switch\":3,\"true\":[1,2,3,5,46],\"try\":[3,11],\"var\":[5,10],\"voil\\u00e0\":4,\"while\":[3,5,9],A:[3,4,9,10],And:[0,3],As:[2,3,4,9,10],At:[4,10],But:4,By:49,For:[3,9,10,11],If:[4,10,35,44,46,48],In:[1,2,3,4,10],It:[1,3,4,6,8,10,14],Of:9,On:10,One:3,The:[1,2,3,4,9,10,16,17,18,19,20,21,23,32,33,34,35,36,37,39,44,46,50],There:1,These:10,To:[1,4,9,10,12],_:5,__expf:2,__init__:[11,48],_db:5,_dropout:4,_dw:5,_layer_norm_bwd_dwdb:5,_layer_norm_bwd_dx_fus:5,_layer_norm_fwd_fus:5,_matmul:3,_seeded_dropout:4,a100:[3,10],a_ptr:3,ab:1,abl:10,about:[1,2,3,4,8],abov:[1,2,3,4,10,12],academ:9,acc:[3,9,10],acceler:9,access:[1,3,9,10,14],accomod:3,accordingli:10,account:10,accumul:[3,5,10],accuraci:[3,9],achiev:[3,9,10],across:[2,4,9,10],activ:3,actual:[3,9,10],add:[1,4,5,7,16],add_kernel:1,addit:[2,6,7,9,49],addition:10,address:[9,25],adopt:10,advanc:[2,3,9],advoc:10,affect:3,affin:10,after:3,against:[0,1,2,3,8],aggress:[9,10],agnost:[9,10],ahead:10,aim:[2,8],al:[9,10],alex:4,algebra:10,algorithm:[3,4,9,10],alia:10,all:[2,3,4,6,9,10,12,27,29,31,45,48],allclos:[2,3],allen1984:10,allen:10,alloc:[1,2,3,5,9],allow:[1,2,5,9,10],along:[1,3,27,29,32,33,45,49],also:[1,2,3,4,5,9,10],altern:4,alwai:[10,46],amd:9,amen:10,amount:[5,9],ampl:10,an:[1,2,3,4,9,10,11,16,17,18,19,20,34,35,36,37],analog:1,analysi:[9,10],analyz:10,ancourt1991:10,ancourt:10,ani:[1,2,3,10,12,13,48],anoth:[2,10],anytim:12,apart:10,apex:5,apex_layer_norm:5,api:48,appear:48,appli:[3,4,5,9,10],applic:[4,10,13],approach:[9,10],appropri:1,approxim:2,ar:[0,1,2,3,4,9,10,12,14,25,31,44,46,48],arang:[1,2,3,4,5],arbitrari:3,architectur:[3,9],area:10,arg:[1,2,3,5,13,14,48],argument:[1,2,3,11,12,13,14,46,48],arrai:[10,47],arrang:3,art:[9,10],artifici:4,arxiv:[9,10],ask:2,aspect:10,asplo:9,assert:[1,3,4],assert_almost_equ:5,assum:[2,48],asynchron:[1,9],atom:[16,17,18,19,20],atomic_ca:5,atomic_xchg:5,auguin1983:9,auguin:9,auto:[2,3,10,11,12,13],autograd:5,autom:9,automat:[2,3,9,10,11],autotun:[3,10],avail:[0,4,9,10],avoid:[2,12,46],awar:9,awkward:4,axi:[1,2,3,4,5,27,29,32,33,45,48],b:[3,5,9,10],b_ptr:3,back:[1,2,3,4,5],backpropag:4,backward:5,bad:4,baghdadi2021:[9,10],baghdadi:[9,10],balanc:10,bandwidth:2,base:[4,8,9,10],basic:[1,6,10],becom:9,been:[1,9,10],befor:[3,12,16,17,18,19,20],begin:10,behavior:[10,12],being:[2,4],believ:10,below:[4,6,10],bench:0,bench_layer_norm:5,benchmark:[0,5,49,50],benefit:[2,9,10],best:[1,9],between:[1,9],bia:5,bit:4,block:[1,2,3,4,9,10,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,34,35,36,37,38,39,40,41,42,43,44,45,46,47],block_siz:[1,2,4,5,10,12,13],block_size_k:3,block_size_m:[3,5],block_size_n:[3,5],block_start:[1,4],blue:[1,2,3,5],boil:10,bool:[46,48],both:[10,46],bound:[1,2,3,10],branch:10,broad:9,broadcast:[21,25,44,46],buffer:5,build:[0,3],built:[1,10],c:[3,9,10],c_mask:3,c_ptr:3,cach:[9,10,25],cache_modifi:25,call:[1,3,10,14,35],callabl:[1,13,14,49],can:[0,1,2,3,4,9,10,12,50],cannot:[3,9,10],capabl:[8,9],cast:5,cd:0,cdiv:[1,3,4,5],ceil:13,certain:13,cgo:[9,10],challeng:4,chang:[3,4,12,25],chapter:8,characterist:10,cheap:9,check:[3,8],checkpoint:4,chen2018:9,chen:9,chip:2,choic:8,click:[1,2,3,4,5],clone:[0,5],close:10,cmake:0,cmp:[16,17,18,19,20],coalesc:9,code:[1,2,3,4,5,6,9,10],col:[3,5,10],col_offset:2,color:48,column:[2,3],com:0,combin:9,come:[2,3,10],command:0,common:10,commonli:10,compar:[2,3,4,5,8,10,16,17,18,19,20],compat:23,compil:[2,3,8,9,11,14,31],complet:10,complex:10,compos:[4,9],composit:10,comprehens:[9,10],comput:[4,5,8,9,10,13,22,24,26,28,30,40,41,42,43],computation:[9,10],concern:10,concis:[1,48],condit:[10,46],config:[3,12],configur:[3,11,12,50],confirm:2,connectom:9,consecut:10,consequ:9,consid:2,consist:4,constraint:[3,10],construct:9,constructor:48,consum:3,contain:[10,16,17,18,19,20,48],contextu:10,contigu:[3,15,38],control:[9,10],conveni:3,convert:[1,3,14],convolut:9,cooper:11,copi:[4,9,16,17,18,19,20],core:[9,10],correct:1,correspond:[1,2,3,48],cosin:22,cost:10,could:[2,10],count:5,cours:9,cpython:0,creat:[1,2,3,5,9],crucial:4,csv:1,ctx:5,cubla:[3,9],cuda:[1,2,3,4,5,9],cudnn:9,current:33,custom:[1,2,3,8],cut:3,cvpr:9,d:[2,4,12,14],dart:10,darte1999:10,data:[1,3,4,5,9,10,16,17,18,19,20,25,46,47],data_ptr:14,dataflow:10,david:4,db:5,db_ref:5,db_tri:5,deal:4,decad:9,decim:5,declar:1,decompos:10,decor:[1,3,12,13,14],decreas:4,dedic:3,deep:[3,4,9,10],def:[1,2,3,4,5,12,13],defin:[1,2,3,10,25],definit:10,denomin:2,denot:1,dens:10,depend:[0,10,46],deploi:9,describ:[4,10],design:10,desir:[21,39],detail:[3,10],detect:9,develop:[9,10],devic:[1,2,3,5],dg:5,dialect:10,dict:13,dictionari:[11,13],diesel:10,differ:[1,2,3,4,9,10,48],difficult:10,difficulti:[3,9],dijkstra82:10,dijkstra:10,dim:[2,5,10],dimens:[3,23,27,29,45],dimension:[3,10,23],dir:0,direct:3,disjoint:10,disk:1,dissert:10,distribut:[2,4,10],divis:3,dnn:[8,9,10],do_bench:[1,2,3,5],doc:4,doe:[1,2,3,10],doesn:[5,10],domain:[9,10],don:[1,2,3],done:[3,9,27,29,45],dot:3,doubli:3,doubt:10,down:[3,10],download:[0,1,2,3,4,5,6],dram:[1,2],dropout:[6,7],dror:4,dsl:[8,9,10],dtype:[1,2,3,5,16,17,18,19,20,25,44,47],dw:5,dw_ref:5,dw_tri:5,dx:5,dx_ref:5,dx_tri:5,dy:5,e:[0,2,3,4,9,10,47],each:[1,2,3,4,5,9,10,11,13],eas:10,easi:[3,4],easier:[1,2,9],easili:3,ed:[1,3],education:2,effect:10,effici:[3,4,9,36],effort:10,either:[1,32,33,46],elango2018:10,elango:10,element:[1,2,3,4,5,22,24,26,27,28,29,30,40,41,42,43,44,45,46,48],element_s:[2,5],element_ti:[16,17,18,19,20,25,44],elementwis:[2,25],els:[3,5],emerg:9,empti:[3,5],empty_lik:[1,2,4,5],enabl:10,encod:10,encourag:4,end:[9,10,15],enforc:10,engin:10,enqueu:[1,2,5],ensur:10,entir:10,entri:36,environ:8,ep:5,equal:10,error:3,especi:9,et:[4,9,10],euromicro:9,evalu:[3,4,12,46],even:[4,10],evidenc:9,evolv:9,exampl:[1,2,3,4,5,6,9,10,11],exchang:20,execut:[7,9,10,11,50],exist:[9,10],exp:2,expect:[2,16,17,18,19,20],expens:[9,10,13],explor:[4,9],exponenti:[2,24],express:[9,10],extar:1,extend:[3,4],extract:3,extrem:10,f:[1,2,3,10],facilit:[9,10],fact:10,fairli:3,fals:[25,44,46,48,49],far:2,fast:[2,5,9,10],faster:[2,35],fastest:10,featur:5,feel:3,fetch:9,few:10,field:9,figur:10,file:[1,2,3,7],fill:47,final_db:5,final_dw:5,fine:4,first:[1,3,4,5,8,10,23,28,30],first_pid_m:3,firstli:4,fit:2,fix:48,flag:2,flatten:38,flexibl:9,float16:[3,5,23,47],float32:[1,2,3,4,5,23,34,37],flow:[9,10],fly:4,fn:[14,49],focu:[3,10],folder:4,follow:[0,2,3,8,9,10],footprint:4,forc:4,forget:1,formal:10,format:10,forward:5,found:[16,17,18,19,20],foundat:10,four:36,fp16:3,fp32:3,frac:4,framework:[9,10],free:3,from:[1,2,3,4,9,10,25,46],full:[1,2,3,4,5],fulli:10,func:10,fundament:10,further:[4,10],fuse:[3,5,6,7],fusedlayernorm:5,fusion:[2,10],g:[3,4,9,10,47],galleri:[1,2,3,4,5,6],gb:[1,2,5],gbp:[1,2,5],gener:[1,2,3,4,5,6,9,10,34,35,36,37,48],geoffrei:4,geq:10,get:[1,2,3,4,7],girbal2006:10,girbal:10,git:0,github:0,give:9,given:[2,3,4,21,32,33,34,35,36,37,39,47],global:10,go:[1,3,10],good:[1,10],gpgpu:9,gpu:[1,2,4,8,9,10,11,14],grad:5,grad_to_non:[5,49],gradient:[5,49],grammat:10,graphic:9,greater:2,green:[1,2,3,5],grid:[1,2,3,4,5,32,33],grid_m:3,grid_n:3,grosser2012:10,grosser:10,group:3,group_id:3,group_m:3,group_size_m:[3,5],grow:10,guard:[1,2],guid:9,ha:[1,3,4,9,10,32,33],had:1,halid:[9,10],hand:10,handl:[1,2,4,10],handwritten:9,hard:3,harder:10,hardwar:[3,8,10],hasn:1,have:[2,4,9,10,14,23,46,48],heavi:9,helper:[1,2],henc:3,here:[1,2,3,4,5],heurist:[2,5],hierarch:9,hierarchi:10,high:[3,9,10],higher:3,highli:9,highlight:10,hint:10,hinton:4,hit:3,how:[1,2,3,8,9,13],howev:[2,10],html:4,http:[0,4],i:[1,2,3,4,5,9,10],id:[3,33],idea:9,ideal:2,ident:2,identifi:1,idx:[25,44],ilya:4,imag:[9,10],implement:[1,2,3,4,9,10],implicitli:[1,14,25,44],importantli:10,impos:10,improv:[3,4],incompat:[3,10],incorrect:3,increas:[1,2,3,4],incred:9,increment:10,inde:10,independ:[2,5,10],index:1,indic:[10,46],induc:10,industri:9,inequ:10,inf:2,inform:10,infrastructur:10,initi:[1,3],inner:[3,23],inplac:3,input:[1,2,3,4,5,10,13,21,22,23,24,26,27,28,29,30,31,38,39,40,41,42,43,45],input_ptr:2,input_row_strid:2,instal:8,instanc:[1,2,3,4,5,9,11,32,33],instanti:4,instead:[2,46],instruct:[8,9],int1:[25,44],int32:[4,5,35,36],integ:10,interchang:10,interest:[5,9,10],intermedi:10,intern:[2,10],interv:15,intrins:10,introduc:4,introduct:8,invari:[2,10],invoc:4,ipynb:[1,2,3,4,5],ir:10,irregular:[2,10],is_contigu:[3,4],is_cuda:1,isn:3,issu:[9,10],iter:[3,9,10],its:[1,2,3,10],j:[3,9,10],jit:[1,2,3,4,5,12,13],jmlr:4,john:4,johnson:4,journal:10,jrk2013:9,jupyt:[1,2,3,4,5,6],just:[3,10,13],k:[3,4,9,10],kb:9,keep:4,kei:[3,9,12],kellei:9,kernel:[4,5,8,9,11,12,13],keyword:[1,11],ki:10,kind:2,know:31,known:10,krizhevski:4,kwarg:14,l2:5,label:[1,2,3,48],lam1991:9,lam:9,lambda:[1,2,3,4,5,13],languag:[1,2,3,4,5,8,9,14],larg:[9,10],last:3,later:[2,10],latest:0,lattner2004:10,lattner2019:10,lattner:10,launch:[1,2,3,32,33],law:10,layer:[6,7,9,10],layer_norm:5,layernorm:5,lead:[4,9,10],leaky_relu:3,leakyrelu:3,learn:[1,2,3,4,8,9,10],least:10,lee2017:9,lee:9,left:10,legal:10,length:1,less:[4,5,9,10],let:[1,2,4,31],letter:10,level:[3,9,10],li:9,librari:[0,3,9,10],lifelong:10,like:[1,4,9,10,35],limit:[2,4],lindenstrauss:4,line:[1,2,3,4,10,48],line_arg:[1,2,3,5,48],line_nam:[1,2,3,5,48],line_v:[1,2,3,5,48],linear:[9,10],link:0,list:[1,3,12,13,48,49,50],litteratur:10,ll:4,llvm11:0,llvm:[0,10],load:[1,2,3,4,5,10,46],local:[9,10],locat:[3,16,17,18,19,20,25,44],lock:5,lock_id:5,log2:13,log:48,logarithm:[1,26],look:[4,8,9],loop:[3,10,11],low:[6,7,10],m:[0,2,3,5,9],machin:[9,10],machineri:[9,10],made:9,mai:[2,10,13],main:[3,9,10],maintain:[2,10],major:[3,10],make:[1,2,9,10],manag:[4,9],mani:[1,9,10],manual:[2,10],manual_se:[1,2,3],map:3,mapl:10,mark:[4,50],markedli:9,mask:[1,2,3,4,5,16,18,19,20,25,44,46],match:[3,16,17,18,19,20],math:13,mathbb:10,mathbf:10,mathcal:[10,37],mathemat:10,matmul:[3,10],matmul_kernel:3,matric:[2,3],matrix:[2,4,6,7,9,10,11,23],matrix_s:10,matter:[3,9,10],max:[1,2,5,18],max_fused_s:5,max_m:[1,2,3,5],maxim:[8,10,36],maximum:[1,2,27],mb:[7,9],mean1:5,mean2:5,mean:[3,5,10,12],mechan:[2,10],median:49,memori:[1,2,3,6,7,9,10,16,17,18,19,20,25,44,46],mention:3,meta:[1,2,3,4,5,11,12,13],metaparamet:1,method:[10,11,14,48,50],methodolog:10,micro:9,min:[3,5,19],min_m:[1,2,3,5],minimum:29,minut:[1,2,3,4,5],miss:10,mitig:10,ml:9,mlir:10,mn:2,mode:5,model:[1,9,10],modern:[3,8,9,10],modular:10,moor:10,mora:4,more:[2,3,4,8,9,10,48],most:[3,10],mostli:11,move:3,movement:4,ms:[1,2,3,5,49],much:[2,3],mullapudi2016:10,mullapudi:10,multi:[3,9,10],multipl:[1,4,6,7,9,10,11,12,31,35],multipli:[3,4,5,10,23],must:[2,3,15,23,46],n:[2,3,5,9,37],n_col:2,n_element:[1,4],n_row:2,naiv:[2,4],naive_softmax:2,name:[1,2,3,12,13,48],nativ:[1,2,3],natur:[2,9,26],nb:9,necessari:2,need:[1,2,3,4,35],nelement:2,nest:[3,10],net:10,network:[4,9,10],neural:[4,9,10],neurosci:9,never:4,next:[2,3],next_power_of_2:[2,5],nightli:0,nip:9,nitish:4,nn:[3,5],non:9,none:[2,3,5,12,16,18,19,20,25,44,48,49],nonzero:46,norm:[4,5,7],normal:[2,3,6,7],normalized_shap:5,note:[0,1,2,3,4,10,12,14,46],notebook:[1,2,3,4,5,6],notic:[2,10],notori:[3,9],novel:9,now:[1,3],num_pid_in_group:3,num_pid_m:3,num_pid_n:3,num_stag:[3,11],num_warp:[2,3,5,11,12],number:[1,2,3,4,5,10,11,32,34,35,36,37],numel:[1,4,5],numer:[2,9],nvidia:[9,25],o:[2,4],object:[1,3,9,11,12,14,16,17,18,19,20],obtain:1,obvious:2,occur:10,off:5,offer:9,offici:0,offs_am:3,offs_bn:3,offs_cm:3,offs_cn:3,offs_k:3,offset:[1,4,5,34,35,36,37],often:3,omega:10,onc:[2,9,10],one:[2,3,4,5,6,9,10,48],onli:[2,3,4,9,10,14],op:[1,2],open:15,openai:0,opencl:9,oper:[1,2,3,4,6,9,16,17,18,19,20,46],opportun:9,opsila:9,optim:[9,10],option:[1,3,25,44,48,49],orang:5,order:[2,3,6,10],org:4,origin:10,osdi:9,other:[2,3,4,5,8,10,14,23,25,28,30],otherwis:[4,46],our:[1,2,3,9],out:[1,2,3,4,5,8,10],outlin:10,output2:4,output3:4,output:[1,2,3,4,5],output_ptr:[1,2,4],output_row_start_ptr:2,output_row_strid:2,output_torch:1,output_triton:1,over:[2,4,9,10],overfit:4,overflow:2,own:3,p:[4,10],pa:3,packag:14,pact:10,pad:2,par:3,paradigm:[9,10],paragraph:4,parallel:[1,2,3,4,5,8,9,10,11],paralleliz:9,param:13,paramet:[1,3,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50],parametr:9,part:[3,4,10],partial:5,partial_db:5,partial_dw:5,particular:[2,3],particularli:[9,10],partit:9,pass:[1,5,10,11],past:[9,10],path:1,pattern:9,pb:3,peak:10,per:[2,4,5],percentil:49,perf:3,perf_report:[1,2,3,5,48],perform:[1,2,4,9,10,16,17,18,19,20,49],persist:4,person:10,perspect:10,pgm:1,phase:10,philosophi:10,philox:[4,36],pid:[1,3,4,5],pid_m:3,pid_n:3,pip:0,pipelin:[9,10,11],platform:[8,10],pldi:9,plot:[0,1,2,3,48],plot_nam:[1,2,3,5,48],pmatrix:10,point:[1,10,36],pointer:[1,2,4,5,14,16,17,18,19,20,25,44],pointerdtyp:[16,17,18,19,20,25,44],polli:10,polyhedr:9,polyhedra:10,popular:10,portabl:[9,10],pose:9,posit:[5,13],possibl:[1,2,3,10,11],power:[2,4,10,13,15],ppopp:10,practic:[1,2,3,9],pragma:9,pre:[0,9],prealloc:1,predict:10,prefer:2,premis:9,present:[0,3],preserv:10,preserve_rng_st:4,prevent:[4,10],primer:10,primit:[9,14],principl:10,print:[1,2,3,4],print_data:[1,2,3,5],prng:4,probabl:[4,10],problem:1,problemat:10,procedur:10,process:[1,5,9,10],processor:9,produc:[3,4],product:[8,10,23],program:[1,2,3,4,5,8,9,32,33],program_id:[1,2,3,4,5],programm:[9,10],prohibitev:13,project:[4,9],promot:[3,10],properli:2,properti:10,propos:9,proprietari:3,provid:[1,2,3,4,5,8,10,12,27,29,45,49],prune:4,pseudo:[3,4,36],pseudorandom:4,ptr:3,ptx:25,purpos:[9,10],push:10,put:4,py:[0,1,2,3,4,5,7],pypi:0,pytest:0,python:[1,2,3,4,5,6,14],pytorch:[1,2,4],qquad:10,r:[0,2],ragan:9,rais:5,rand:[1,4,5],randint4x:35,randn:[2,3,4,5],randn_lik:5,random:[4,34,35,36,37],randomli:4,rang:[1,2,3,5,9,10],rapidli:[9,10],rate:3,rather:9,raw:1,rdom:10,re:[1,3],read:[2,3,6],reader:10,real:9,reason:10,recent:9,recommend:6,recomput:[4,9],record_clock:49,rectifi:9,redmon2016:9,redmon:9,reduct:[2,5,27,29,45],refer:1,regardless:[4,46],regim:4,regrett:9,regular:[4,10],rel:[1,10],relat:8,releas:[0,5,9],reli:10,relu:3,remain:[9,48],rememb:3,reorder:10,rep:[5,49],repetit:49,repres:[2,3,10,11],requir:[0,2,4,10],requires_grad:5,requires_grad_:5,research:[9,10],reset:[12,49],reset_to_zero:12,reshap:5,resolut:10,resourc:9,resp:10,respect:10,restrict:10,result:[0,1,2,9,10],ret:2,retain_graph:5,retriev:10,reus:3,revisit:9,right:10,rise:10,role:10,ron:4,root:43,roughli:3,row:[2,3,4,5],row_idx:2,row_minus_max:2,row_start_ptr:2,rstd:5,run:[0,1,2,3,4,5,8,10,12,14,50],runtim:[10,49],runtimeerror:5,ruslan:4,rvar:10,s:[1,2,4,5,10,36],said:10,salakhutdinov:4,salmon2011:4,salmon:4,same:[4,9,48],sato2019:10,sato:10,save:[1,2,3],save_for_backward:5,save_path:[1,5],saved_tensor:5,sc:10,scalabl:10,scalar:[4,9,23,34,35,36,37,47],scale:48,scan:10,schedul:9,scienc:10,scientif:10,scop:10,scope:10,script:[0,1,2,3,4,5],second:[1,2,3,4,5,10,23,28,30],secondli:4,section:[3,10],see:[1,2,3,4,10],seed:[34,35,36,37],seeded_dropout:4,seem:[1,10],select:[9,10,46],self:[11,48],semant:10,semi:10,sens:[1,9,10],separ:[5,10],sequenc:9,set:[1,4,10],setup:0,sever:[9,10],shall:10,shape:[2,3,4,5,10,21,25,39,44,46,47],share:9,shaw:4,shift:2,should:[1,3,9,10,11,27,29,45,48],show_plot:[1,2,3],shown:10,side:10,sight:10,signal:9,significantli:2,sigplan:10,simd:9,simpl:[1,2,3,4],simplest:6,simpli:10,simplic:3,simplifi:4,sinc:[1,2,3],sine:41,singl:[2,4,9,35],size:[1,2,4,10],slower:[9,10],slowest:10,sm80:11,sm:10,smaller:[3,4],smallest:[2,13],snemi3d:9,so:[1,2,3,4,10],softmax:[4,6,7],softmax_kernel:2,softmax_output:2,softwar:11,solid:10,solut:3,solv:10,some:3,sometim:10,sourc:[1,2,3,4,5,6,10],space:[9,10],spars:[4,9,10],spatial:10,speak:3,special:9,specif:[3,9],specifi:[10,13,16,17,18,19,20,44],speed:2,sphinx:[1,2,3,4,5,6],split:10,spmd:[1,9,10],sqrt:5,squar:43,sram:[2,3,5],srivastava2014:4,srivastava:4,stabil:2,stabl:0,stage:11,stai:5,standard:10,start:[5,6,15],started_tutori:7,state:[4,9,10],statement:10,staticmethod:5,std:5,step:10,still:[1,2,3,10],stop:15,store:[1,2,3,4,5,16,17,18,19,20,46],str:[12,13,25,48],straightforward:3,strategi:[4,10],stream:[5,35],strength:9,stride:[2,3,4,5],stride_ak:3,stride_am:3,stride_bk:3,stride_bn:3,stride_cm:3,stride_cn:3,stride_xi:3,stride_xj:3,structur:[9,10],style:[1,2,3,5,48],subscript:10,substanti:9,substract:2,subtract:2,successfulli:10,suffer:10,suit:9,sum:[1,2,5],sum_db:5,sum_dw:5,superhuman:9,support:[4,5,10],sure:2,surprisingli:9,surround:10,suspicion:2,sutskev:[4,9],sutskever2014:9,swap:[16,17,18,19,20],swizzl:9,synchron:[1,9],system:[0,3,9,10],t:[1,2,3,5,10],t_:10,tabul:4,taco:10,take:[3,4,8,13],taken:10,target:9,techniqu:[3,9,10],temperatur:4,tempor:10,tend:10,tension:9,tensor:[1,2,3,4,5,9,10,12,14,49],tensorrt:9,test:[0,1,5,8],test_layer_norm:5,text:10,tflop:3,th:49,than:[2,3,5,9,10,35,48],thei:[3,9,10],them:1,themselv:3,theoret:2,therebi:10,therefor:3,theta:10,theta_:10,thi:[1,2,3,4,5,9,10,12,13,14,36,48],thing:[1,4],think:2,those:2,though:[9,10],thought:10,thread:[2,9,11],through:[6,10],throughout:[10,48],throughput:8,tile:10,time:[0,1,2,3,4,5,9,10,12,35,49],tiramisu:[9,10],tl:[1,2,3,4,5,47],tmp:0,tog:10,togeth:4,tolist:4,topic:10,torch:[1,2,3,4,5,14,49],torch_output:3,torch_relu:3,total:[1,2,3,4,5,7],tradit:[4,9,10],transform:[4,10],travers:10,trend:9,tri:[21,39],trick:2,tricki:4,trigger:[3,12],triton:[0,1,2,3,4,5,6,9,10],triton_output:3,trivial:9,tune:[2,3,10,12,13],tuner:11,tupl:[1,21,39,47],tutori:[1,2,3,4,8],tutorials_jupyt:6,tutorials_python:6,tvm:[9,10],two:[1,2,3,10,12,13,15,23],txt:0,type:[13,23,25,46,47],typecast:[25,44],typic:10,u:[0,34],un:10,uncommon:10,underneath:10,understand:2,undesir:12,unfortun:[3,10],unifi:9,uniformli:4,unint:46,unit:[0,9],univers:10,unrol:10,up:2,updat:[3,10,12],us:[1,2,3,4,5,9,10,11,12,13,14,35,46,48,50],util:[1,5],v100:10,v:5,val:[16,17,18,19,20],valid:1,valu:[1,2,3,4,12,13,15,16,17,18,19,20,22,24,25,26,27,29,31,40,41,42,43,44,45,46,47,48,50],valuabl:2,variabl:[3,11],variant:9,variou:6,vasilach:[9,10],vasilache2018:[9,10],vast:10,vec:10,vector:[4,6,7,9,10],vendor:3,veri:[2,4,10],verif:10,verifi:[2,10],via:10,view:38,visibl:10,vision:9,vs:0,w:[5,10],w_shape:5,wa:4,wai:[2,3,4],want:[2,4,46],warmup:49,warp:[2,5,11],wast:2,wdy:5,we:[1,2,3,4,9,10],weight:5,well:[4,9,10],whatev:12,wheel:0,when:[2,3,4,9,10,11,12,14,46],where:[1,3,4,5,10,13,44],whether:[9,48],which:[1,2,3,4,5,9,10,12,27,29,45,48],whose:[1,2,3,4,10,12,25],wide:10,wise:[1,2,22,24,26,28,30,40,41,42,43,44],wish:[3,10],within:[3,14,15],without:10,wolf:10,wolfe1989:10,won:2,word:10,work:[2,4,8,9],workload:[3,11],wors:[3,9,10],would:[1,2,4],wouldn:10,wrapper:3,write:[1,2,3,4,5,6,8,10],wrote:2,x:[1,2,3,4,5,10,22,24,26,28,30,38,40,41,42,43,46,48],x_arg:5,x_keep:4,x_keep_ptr:4,x_log:[1,48],x_max:2,x_name:[1,2,3,5,48],x_ptr:[1,4,12,13],x_shape:5,x_size:[12,13],x_val:[1,2,3,5,48],xhat:5,xi:10,xii:10,xlabel:48,xmean:5,xo:10,y:[1,2,3,5,10,28,30,46,48],y_fwd:5,y_log:48,y_name:[1,2],y_ptr:1,y_ref:5,y_torch:2,y_tri:5,y_triton:2,year:10,yet:[9,10],yi:10,yield:46,yii:10,ylabel:[1,2,3,5,48],yo:10,you:[0,1,2,3,4,6,9,12,35,46],your:[0,1,8],yourself:[2,3],z:[1,2,10],zero:[3,4,5,12],zip:6},titles:[\"Installation\",\"Vector Addition\",\"Fused Softmax\",\"Matrix Multiplication\",\"Low-Memory Dropout\",\"Layer Normalization\",\"Tutorials\",\"Computation times\",\"Welcome to Triton\\u2019s documentation!\",\"Introduction\",\"Related Work\",\"triton.Config\",\"triton.autotune\",\"triton.heuristics\",\"triton.jit\",\"triton.language.arange\",\"triton.language.atomic_add\",\"triton.language.atomic_cas\",\"triton.language.atomic_max\",\"triton.language.atomic_min\",\"triton.language.atomic_xchg\",\"triton.language.broadcast_to\",\"triton.language.cos\",\"triton.language.dot\",\"triton.language.exp\",\"triton.language.load\",\"triton.language.log\",\"triton.language.max\",\"triton.language.maximum\",\"triton.language.min\",\"triton.language.minimum\",\"triton.language.multiple_of\",\"triton.language.num_programs\",\"triton.language.program_id\",\"triton.language.rand\",\"triton.language.randint\",\"triton.language.randint4x\",\"triton.language.randn\",\"triton.language.ravel\",\"triton.language.reshape\",\"triton.language.sigmoid\",\"triton.language.sin\",\"triton.language.softmax\",\"triton.language.sqrt\",\"triton.language.store\",\"triton.language.sum\",\"triton.language.where\",\"triton.language.zeros\",\"triton.testing.Benchmark\",\"triton.testing.do_bench\",\"triton.testing.perf_report\",\"triton\",\"triton.language\",\"triton.testing\"],titleterms:{\"final\":3,addit:1,advantag:10,algebra:52,api:8,arang:15,arithmet:3,atom:52,atomic_add:16,atomic_ca:17,atomic_max:18,atomic_min:19,atomic_xchg:20,autotun:12,baselin:4,benchmark:[1,2,3,48],binari:0,broadcast_to:21,cach:3,challeng:9,co:22,comparison:52,compil:[10,52],comput:[1,2,3,7],config:11,creation:52,distribut:0,do_bench:49,document:8,dot:23,dropout:4,exercis:4,exp:24,from:0,further:8,fuse:2,gener:52,get:8,go:8,heurist:13,hint:52,index:52,instal:0,introduct:9,jit:14,kernel:[1,2,3],l2:3,languag:[10,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,52],layer:5,limit:10,linear:52,load:25,log:26,low:4,manipul:52,math:52,matrix:3,max:27,maximum:28,memori:[4,52],min:29,minimum:30,model:52,motiv:[2,3,9],multipl:3,multiple_of:31,normal:5,num_program:32,number:52,op:52,optim:3,packag:0,perf_report:50,perform:3,pointer:3,polyhedr:10,program:[10,52],program_id:33,python:[0,8],rand:34,randint4x:36,randint:35,randn:37,random:52,ravel:38,reduct:52,refer:[4,9,10],relat:10,represent:10,reshap:39,result:3,s:8,schedul:10,seed:4,shape:52,sigmoid:40,sin:41,softmax:[2,42],sourc:0,sqrt:43,squar:3,start:8,store:44,sum:45,test:[2,3,48,49,50,53],time:7,triton:[8,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53],tutori:6,unit:[2,3],vector:1,welcom:8,where:46,work:10,zero:47}})\n\\ No newline at end of file"}]