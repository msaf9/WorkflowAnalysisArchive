[{"filename": "main/.buildinfo", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n # Sphinx build info version 1\n # This file hashes the configuration used when building these files. When it is not found, a full rebuild will be done.\n-config: b9066f9b0ff67de4ac6548163620a09a\n+config: f85e3a669e14696c0e21cf9b59030f27\n tags: 645f666f9bcd5a90fca523b33c5a78b7"}, {"filename": "main/.doctrees/environment.pickle", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/installation.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/01-vector-add.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/02-fused-softmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/03-matrix-multiplication.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/04-low-memory-dropout.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/05-layer-norm.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/06-fused-attention.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/07-math-functions.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/08-experimental-block-pointer.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/index.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/sg_execution_times.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/index.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/programming-guide/chapter-1/introduction.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/programming-guide/chapter-2/related-work.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.Config.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.autotune.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.heuristics.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.jit.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.abs.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.arange.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.argmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.argmin.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.associative_scan.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_add.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_cas.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_max.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_min.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_xchg.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.broadcast.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.broadcast_to.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cat.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cos.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cumprod.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cumsum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.debug_barrier.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.device_assert.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.device_print.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.dot.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.exp.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.expand_dims.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.fdiv.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.full.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.load.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.log.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.max.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.max_constancy.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.max_contiguous.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.maximum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.min.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.minimum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.multiple_of.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.num_programs.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.program_id.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.rand.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randint.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randint4x.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randn.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.ravel.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.reduce.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.reshape.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sigmoid.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sin.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.softmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sqrt.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.static_assert.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.static_print.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.static_range.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.store.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.trans.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.umulhi.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.view.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.where.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.xor_sum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.zeros.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.Benchmark.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.do_bench.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.perf_report.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.language.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.testing.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/3176accb6c7288b0e45f41d94eebacb9/06-fused-attention.ipynb", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -26,7 +26,7 @@\n       },\n       \"outputs\": [],\n       \"source\": [\n-        \"import pytest\\nimport torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.jit\\ndef max_fn(x, y):\\n    return tl.math.max(x, y)\\n\\n\\n@triton.jit\\ndef _fwd_kernel(\\n    Q, K, V, sm_scale,\\n    L,\\n    Out,\\n    stride_qz, stride_qh, stride_qm, stride_qk,\\n    stride_kz, stride_kh, stride_kn, stride_kk,\\n    stride_vz, stride_vh, stride_vk, stride_vn,\\n    stride_oz, stride_oh, stride_om, stride_on,\\n    Z, H, N_CTX,\\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\\n    BLOCK_N: tl.constexpr,\\n    IS_CAUSAL: tl.constexpr,\\n):\\n    start_m = tl.program_id(0)\\n    off_hz = tl.program_id(1)\\n    qvk_offset = off_hz * stride_qh\\n    Q_block_ptr = tl.make_block_ptr(\\n        base=Q + qvk_offset,\\n        shape=(N_CTX, BLOCK_DMODEL),\\n        strides=(stride_qm, stride_qk),\\n        offsets=(start_m * BLOCK_M, 0),\\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\\n        order=(1, 0)\\n    )\\n    K_block_ptr = tl.make_block_ptr(\\n        base=K + qvk_offset,\\n        shape=(BLOCK_DMODEL, N_CTX),\\n        strides=(stride_kk, stride_kn),\\n        offsets=(0, 0),\\n        block_shape=(BLOCK_DMODEL, BLOCK_N),\\n        order=(0, 1)\\n    )\\n    V_block_ptr = tl.make_block_ptr(\\n        base=V + qvk_offset,\\n        shape=(N_CTX, BLOCK_DMODEL),\\n        strides=(stride_vk, stride_vn),\\n        offsets=(0, 0),\\n        block_shape=(BLOCK_N, BLOCK_DMODEL),\\n        order=(1, 0)\\n    )\\n    # initialize offsets\\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\\n    offs_n = tl.arange(0, BLOCK_N)\\n    # initialize pointer to m and l\\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\\\"inf\\\")\\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n    # scale sm_scale by log_2(e) and use\\n    # 2^x instead of exp in the loop because CSE and LICM\\n    # don't work as expected with `exp` in the loop\\n    qk_scale = sm_scale * 1.44269504\\n    # load q: it will stay in SRAM throughout\\n    q = tl.load(Q_block_ptr)\\n    q = (q * qk_scale).to(tl.float16)\\n    # loop over k, v and update accumulator\\n    lo = 0\\n    hi = (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX\\n    for start_n in range(lo, hi, BLOCK_N):\\n        # -- load k, v --\\n        k = tl.load(K_block_ptr)\\n        v = tl.load(V_block_ptr)\\n        # -- compute qk ---\\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\\n        if IS_CAUSAL:\\n            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\\\"-inf\\\"))\\n        qk += tl.dot(q, k)\\n        # -- compute scaling constant ---\\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\\n        alpha = tl.math.exp2(m_i - m_i_new)\\n        p = tl.math.exp2(qk - m_i_new[:, None])\\n        # -- scale and update acc --\\n        acc_scale = l_i * 0 + alpha  # workaround some compiler bug\\n        acc *= acc_scale[:, None]\\n        acc += tl.dot(p.to(tl.float16), v)\\n        # -- update m_i and l_i --\\n        l_i = l_i * alpha + tl.sum(p, 1)\\n        m_i = m_i_new\\n        # update pointers\\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\\n    # write back l and m\\n    acc = acc / l_i[:, None]\\n    l_ptrs = L + off_hz * N_CTX + offs_m\\n    tl.store(l_ptrs, m_i + tl.math.log2(l_i))\\n    # write back O\\n    O_block_ptr = tl.make_block_ptr(\\n        base=Out + qvk_offset,\\n        shape=(N_CTX, BLOCK_DMODEL),\\n        strides=(stride_om, stride_on),\\n        offsets=(start_m * BLOCK_M, 0),\\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\\n        order=(1, 0)\\n    )\\n    tl.store(O_block_ptr, acc.to(tl.float16))\\n\\n\\n@triton.jit\\ndef _bwd_preprocess(\\n    Out, DO,\\n    Delta,\\n    BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\\n):\\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\\n    off_n = tl.arange(0, D_HEAD)\\n    # load\\n    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\\n    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\\n    # compute\\n    delta = tl.sum(o * do, axis=1)\\n    # write-back\\n    tl.store(Delta + off_m, delta)\\n\\n\\n@triton.jit\\ndef _bwd_kernel(\\n    Q, K, V, sm_scale, Out, DO,\\n    DQ, DK, DV,\\n    L,\\n    D,\\n    stride_qz, stride_qh, stride_qm, stride_qk,\\n    stride_kz, stride_kh, stride_kn, stride_kk,\\n    stride_vz, stride_vh, stride_vk, stride_vn,\\n    Z, H, N_CTX,\\n    num_block,\\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\\n    BLOCK_N: tl.constexpr,\\n    CAUSAL: tl.constexpr,\\n):\\n    off_hz = tl.program_id(0)\\n    off_z = off_hz // H\\n    off_h = off_hz % H\\n    qk_scale = sm_scale * 1.44269504\\n    # offset pointers for batch/head\\n    Q += off_z * stride_qz + off_h * stride_qh\\n    K += off_z * stride_qz + off_h * stride_qh\\n    V += off_z * stride_qz + off_h * stride_qh\\n    DO += off_z * stride_qz + off_h * stride_qh\\n    DQ += off_z * stride_qz + off_h * stride_qh\\n    DK += off_z * stride_qz + off_h * stride_qh\\n    DV += off_z * stride_qz + off_h * stride_qh\\n    for start_n in range(0, num_block):\\n        if CAUSAL:\\n            lo = start_n * BLOCK_M\\n        else:\\n            lo = 0\\n        # initialize row/col offsets\\n        offs_qm = lo + tl.arange(0, BLOCK_M)\\n        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\\n        offs_m = tl.arange(0, BLOCK_N)\\n        offs_k = tl.arange(0, BLOCK_DMODEL)\\n        # initialize pointers to value-like data\\n        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\\n        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        # pointer to row-wise quantities in value-like data\\n        D_ptrs = D + off_hz * N_CTX\\n        l_ptrs = L + off_hz * N_CTX\\n        # initialize dv amd dk\\n        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n        # k and v stay in SRAM throughout\\n        k = tl.load(k_ptrs)\\n        v = tl.load(v_ptrs)\\n        # loop over rows\\n        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\\n            offs_m_curr = start_m + offs_m\\n            # load q, k, v, do on-chip\\n            q = tl.load(q_ptrs)\\n            # recompute p = softmax(qk, dim=-1).T\\n            if CAUSAL:\\n                qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\\\"-inf\\\"))\\n            else:\\n                qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\\n            qk += tl.dot(q, tl.trans(k))\\n            qk *= qk_scale\\n            l_i = tl.load(l_ptrs + offs_m_curr)\\n            p = tl.math.exp2(qk - l_i[:, None])\\n            # compute dv\\n            do = tl.load(do_ptrs)\\n            dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\\n            # compute dp = dot(v, do)\\n            Di = tl.load(D_ptrs + offs_m_curr)\\n            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\\n            dp += tl.dot(do, tl.trans(v))\\n            # compute ds = p * (dp - delta[:, None])\\n            ds = p * dp * sm_scale\\n            # compute dk = dot(ds.T, q)\\n            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)\\n            # compute dq\\n            dq = tl.load(dq_ptrs)\\n            dq += tl.dot(ds.to(Q.dtype.element_ty), k)\\n            tl.store(dq_ptrs, dq)\\n            # increment pointers\\n            dq_ptrs += BLOCK_M * stride_qm\\n            q_ptrs += BLOCK_M * stride_qm\\n            do_ptrs += BLOCK_M * stride_qm\\n        # write-back\\n        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\\n        tl.store(dv_ptrs, dv)\\n        tl.store(dk_ptrs, dk)\\n\\n\\nempty = torch.empty(128, device=\\\"cuda\\\")\\n\\n\\nclass _attention(torch.autograd.Function):\\n\\n    @staticmethod\\n    def forward(ctx, q, k, v, causal, sm_scale):\\n        # shape constraints\\n        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\\n        assert Lq == Lk and Lk == Lv\\n        assert Lk in {16, 32, 64, 128}\\n        o = torch.empty_like(q)\\n        BLOCK_M = 128\\n        BLOCK_N = 64\\n        grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\\n        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\\n\\n        num_warps = 4 if Lk <= 64 else 8\\n        _fwd_kernel[grid](\\n            q, k, v, sm_scale,\\n            L,\\n            o,\\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\\n            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\\n            q.shape[0], q.shape[1], q.shape[2],\\n            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=Lk,\\n            IS_CAUSAL=causal,\\n            num_warps=num_warps,\\n            num_stages=4)\\n\\n        ctx.save_for_backward(q, k, v, o, L)\\n        ctx.grid = grid\\n        ctx.sm_scale = sm_scale\\n        ctx.BLOCK_DMODEL = Lk\\n        ctx.causal = causal\\n        return o\\n\\n    @staticmethod\\n    def backward(ctx, do):\\n        BLOCK = 128\\n        q, k, v, o, L = ctx.saved_tensors\\n        do = do.contiguous()\\n        dq = torch.zeros_like(q, dtype=torch.float32)\\n        dk = torch.empty_like(k)\\n        dv = torch.empty_like(v)\\n        delta = torch.empty_like(L)\\n        _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\\n            o, do,\\n            delta,\\n            BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\\n        )\\n        _bwd_kernel[(ctx.grid[1],)](\\n            q, k, v, ctx.sm_scale,\\n            o, do,\\n            dq, dk, dv,\\n            L, delta,\\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\\n            q.shape[0], q.shape[1], q.shape[2],\\n            ctx.grid[0],\\n            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\\n            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\\n            CAUSAL=ctx.causal,\\n            num_stages=1,\\n        )\\n        return dq, dk, dv, None, None\\n\\n\\nattention = _attention.apply\\n\\n\\n@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(6, 9, 1024, 64)])\\n@pytest.mark.parametrize('causal', [False, True])\\ndef test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\\n    torch.manual_seed(20)\\n    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0., std=0.5).requires_grad_()\\n    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0., std=0.5).requires_grad_()\\n    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0., std=0.5).requires_grad_()\\n    sm_scale = 0.5\\n    dout = torch.randn_like(q)\\n    # reference implementation\\n    M = torch.tril(torch.ones((N_CTX, N_CTX), device=\\\"cuda\\\"))\\n    p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\\n    if causal:\\n        p[:, :, M == 0] = float(\\\"-inf\\\")\\n    p = torch.softmax(p.float(), dim=-1).half()\\n    # p = torch.exp(p)\\n    ref_out = torch.matmul(p, v)\\n    ref_out.backward(dout)\\n    ref_dv, v.grad = v.grad.clone(), None\\n    ref_dk, k.grad = k.grad.clone(), None\\n    ref_dq, q.grad = q.grad.clone(), None\\n    # triton implementation\\n    tri_out = attention(q, k, v, causal, sm_scale).half()\\n    tri_out.backward(dout)\\n    tri_dv, v.grad = v.grad.clone(), None\\n    tri_dk, k.grad = k.grad.clone(), None\\n    tri_dq, q.grad = q.grad.clone(), None\\n    # compare\\n    assert torch.allclose(ref_out, tri_out, atol=1e-2, rtol=0)\\n    assert torch.allclose(ref_dv, tri_dv, atol=1e-2, rtol=0)\\n    assert torch.allclose(ref_dk, tri_dk, atol=1e-2, rtol=0)\\n    assert torch.allclose(ref_dq, tri_dq, atol=1e-2, rtol=0)\\n\\n\\ntry:\\n    from flash_attn.flash_attn_interface import flash_attn_func\\n    HAS_FLASH = True\\nexcept BaseException:\\n    HAS_FLASH = False\\n\\nBATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\\n# vary seq length for fixed head and batch=4\\nconfigs = [triton.testing.Benchmark(\\n    x_names=['N_CTX'],\\n    x_vals=[2**i for i in range(10, 15)],\\n    line_arg='provider',\\n    line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\\n    line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\\n    styles=[('red', '-'), ('blue', '-')],\\n    ylabel='ms',\\n    plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\\n    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode, 'causal': causal}\\n) for mode in ['fwd', 'bwd'] for causal in [False, True]]\\n\\n\\n@triton.testing.perf_report(configs)\\ndef bench_flash_attention(BATCH, H, N_CTX, D_HEAD, causal, mode, provider, dtype=torch.float16, device=\\\"cuda\\\"):\\n    assert mode in ['fwd', 'bwd']\\n    warmup = 25\\n    rep = 100\\n    if provider == \\\"triton\\\":\\n        q = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        sm_scale = 1.3\\n        fn = lambda: attention(q, k, v, causal, sm_scale)\\n        if mode == 'bwd':\\n            o = fn()\\n            do = torch.randn_like(o)\\n            fn = lambda: o.backward(do, retain_graph=True)\\n        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\\n    if provider == \\\"flash\\\":\\n        lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\\n        cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\\n        cu_seqlens[1:] = lengths.cumsum(0)\\n        qkv = torch.randn((BATCH * N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\\n        fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=causal)\\n        if mode == 'bwd':\\n            o = fn()\\n            do = torch.randn_like(o)\\n            fn = lambda: o.backward(do, retain_graph=True)\\n        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\\n    flops_per_matmul = 2. * BATCH * H * N_CTX * N_CTX * D_HEAD\\n    total_flops = 2 * flops_per_matmul\\n    if causal:\\n        total_flops *= 0.5\\n    if mode == 'bwd':\\n        total_flops *= 2.5  # 2.0(bwd) + 0.5(recompute)\\n    return total_flops / ms * 1e-9\\n\\n\\n# only works on post-Ampere GPUs right now\\nbench_flash_attention.run(save_path='.', print_data=True)\"\n+        \"import pytest\\nimport torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.jit\\ndef max_fn(x, y):\\n    return tl.math.max(x, y)\\n\\n\\n@triton.jit\\ndef _fwd_kernel(\\n    Q, K, V, sm_scale,\\n    L,\\n    Out,\\n    stride_qz, stride_qh, stride_qm, stride_qk,\\n    stride_kz, stride_kh, stride_kn, stride_kk,\\n    stride_vz, stride_vh, stride_vk, stride_vn,\\n    stride_oz, stride_oh, stride_om, stride_on,\\n    Z, H, N_CTX,\\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\\n    BLOCK_N: tl.constexpr,\\n    IS_CAUSAL: tl.constexpr,\\n):\\n    start_m = tl.program_id(0)\\n    off_hz = tl.program_id(1)\\n    qvk_offset = off_hz * stride_qh\\n    Q_block_ptr = tl.make_block_ptr(\\n        base=Q + qvk_offset,\\n        shape=(N_CTX, BLOCK_DMODEL),\\n        strides=(stride_qm, stride_qk),\\n        offsets=(start_m * BLOCK_M, 0),\\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\\n        order=(1, 0)\\n    )\\n    K_block_ptr = tl.make_block_ptr(\\n        base=K + qvk_offset,\\n        shape=(BLOCK_DMODEL, N_CTX),\\n        strides=(stride_kk, stride_kn),\\n        offsets=(0, 0),\\n        block_shape=(BLOCK_DMODEL, BLOCK_N),\\n        order=(0, 1)\\n    )\\n    V_block_ptr = tl.make_block_ptr(\\n        base=V + qvk_offset,\\n        shape=(N_CTX, BLOCK_DMODEL),\\n        strides=(stride_vk, stride_vn),\\n        offsets=(0, 0),\\n        block_shape=(BLOCK_N, BLOCK_DMODEL),\\n        order=(1, 0)\\n    )\\n    # initialize offsets\\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\\n    offs_n = tl.arange(0, BLOCK_N)\\n    # initialize pointer to m and l\\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\\\"inf\\\")\\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n    # scale sm_scale by log_2(e) and use\\n    # 2^x instead of exp in the loop because CSE and LICM\\n    # don't work as expected with `exp` in the loop\\n    qk_scale = sm_scale * 1.44269504\\n    # load q: it will stay in SRAM throughout\\n    q = tl.load(Q_block_ptr)\\n    q = (q * qk_scale).to(tl.float16)\\n    # loop over k, v and update accumulator\\n    lo = 0\\n    hi = (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX\\n    for start_n in range(lo, hi, BLOCK_N):\\n        # -- load k, v --\\n        k = tl.load(K_block_ptr)\\n        v = tl.load(V_block_ptr)\\n        # -- compute qk ---\\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\\n        if IS_CAUSAL:\\n            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\\\"-inf\\\"))\\n        qk += tl.dot(q, k)\\n        # -- compute scaling constant ---\\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\\n        alpha = tl.math.exp2(m_i - m_i_new)\\n        p = tl.math.exp2(qk - m_i_new[:, None])\\n        # -- scale and update acc --\\n        acc_scale = l_i * 0 + alpha  # workaround some compiler bug\\n        acc *= acc_scale[:, None]\\n        acc += tl.dot(p.to(tl.float16), v)\\n        # -- update m_i and l_i --\\n        l_i = l_i * alpha + tl.sum(p, 1)\\n        m_i = m_i_new\\n        # update pointers\\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\\n    # write back l and m\\n    acc = acc / l_i[:, None]\\n    l_ptrs = L + off_hz * N_CTX + offs_m\\n    tl.store(l_ptrs, m_i + tl.math.log2(l_i))\\n    # write back O\\n    O_block_ptr = tl.make_block_ptr(\\n        base=Out + qvk_offset,\\n        shape=(N_CTX, BLOCK_DMODEL),\\n        strides=(stride_om, stride_on),\\n        offsets=(start_m * BLOCK_M, 0),\\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\\n        order=(1, 0)\\n    )\\n    tl.store(O_block_ptr, acc.to(tl.float16))\\n\\n\\n@triton.jit\\ndef _bwd_preprocess(\\n    Out, DO,\\n    Delta,\\n    BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\\n):\\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\\n    off_n = tl.arange(0, D_HEAD)\\n    # load\\n    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\\n    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\\n    # compute\\n    delta = tl.sum(o * do, axis=1)\\n    # write-back\\n    tl.store(Delta + off_m, delta)\\n\\n\\n@triton.jit\\ndef _bwd_kernel(\\n    Q, K, V, sm_scale, Out, DO,\\n    DQ, DK, DV,\\n    L,\\n    D,\\n    stride_qz, stride_qh, stride_qm, stride_qk,\\n    stride_kz, stride_kh, stride_kn, stride_kk,\\n    stride_vz, stride_vh, stride_vk, stride_vn,\\n    Z, H, N_CTX,\\n    num_block,\\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\\n    BLOCK_N: tl.constexpr,\\n    CAUSAL: tl.constexpr,\\n):\\n    off_hz = tl.program_id(0)\\n    off_z = off_hz // H\\n    off_h = off_hz % H\\n    qk_scale = sm_scale * 1.44269504\\n    # offset pointers for batch/head\\n    Q += off_z * stride_qz + off_h * stride_qh\\n    K += off_z * stride_qz + off_h * stride_qh\\n    V += off_z * stride_qz + off_h * stride_qh\\n    DO += off_z * stride_qz + off_h * stride_qh\\n    DQ += off_z * stride_qz + off_h * stride_qh\\n    DK += off_z * stride_qz + off_h * stride_qh\\n    DV += off_z * stride_qz + off_h * stride_qh\\n    for start_n in range(0, num_block):\\n        if CAUSAL:\\n            lo = start_n * BLOCK_M\\n        else:\\n            lo = 0\\n        # initialize row/col offsets\\n        offs_qm = lo + tl.arange(0, BLOCK_M)\\n        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\\n        offs_m = tl.arange(0, BLOCK_N)\\n        offs_k = tl.arange(0, BLOCK_DMODEL)\\n        # initialize pointers to value-like data\\n        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\\n        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        # pointer to row-wise quantities in value-like data\\n        D_ptrs = D + off_hz * N_CTX\\n        l_ptrs = L + off_hz * N_CTX\\n        # initialize dv amd dk\\n        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n        # k and v stay in SRAM throughout\\n        k = tl.load(k_ptrs)\\n        v = tl.load(v_ptrs)\\n        # loop over rows\\n        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\\n            offs_m_curr = start_m + offs_m\\n            # load q, k, v, do on-chip\\n            q = tl.load(q_ptrs)\\n            # recompute p = softmax(qk, dim=-1).T\\n            if CAUSAL:\\n                qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\\\"-inf\\\"))\\n            else:\\n                qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\\n            qk += tl.dot(q, tl.trans(k))\\n            qk *= qk_scale\\n            l_i = tl.load(l_ptrs + offs_m_curr)\\n            p = tl.math.exp2(qk - l_i[:, None])\\n            # compute dv\\n            do = tl.load(do_ptrs)\\n            dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\\n            # compute dp = dot(v, do)\\n            Di = tl.load(D_ptrs + offs_m_curr)\\n            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\\n            dp += tl.dot(do, tl.trans(v))\\n            # compute ds = p * (dp - delta[:, None])\\n            ds = p * dp * sm_scale\\n            # compute dk = dot(ds.T, q)\\n            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)\\n            # compute dq\\n            dq = tl.load(dq_ptrs)\\n            dq += tl.dot(ds.to(Q.dtype.element_ty), k)\\n            tl.store(dq_ptrs, dq)\\n            # increment pointers\\n            dq_ptrs += BLOCK_M * stride_qm\\n            q_ptrs += BLOCK_M * stride_qm\\n            do_ptrs += BLOCK_M * stride_qm\\n        # write-back\\n        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\\n        tl.store(dv_ptrs, dv)\\n        tl.store(dk_ptrs, dk)\\n\\n\\nempty = torch.empty(128, device=\\\"cuda\\\")\\n\\n\\nclass _attention(torch.autograd.Function):\\n\\n    @staticmethod\\n    def forward(ctx, q, k, v, causal, sm_scale):\\n        # shape constraints\\n        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\\n        assert Lq == Lk and Lk == Lv\\n        assert Lk in {16, 32, 64, 128}\\n        o = torch.empty_like(q)\\n        BLOCK_M = 128\\n        BLOCK_N = 64\\n        grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\\n        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\\n\\n        num_warps = 4 if Lk <= 64 else 8\\n        _fwd_kernel[grid](\\n            q, k, v, sm_scale,\\n            L,\\n            o,\\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\\n            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\\n            q.shape[0], q.shape[1], q.shape[2],\\n            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=Lk,\\n            IS_CAUSAL=causal,\\n            num_warps=num_warps,\\n            num_stages=4)\\n\\n        ctx.save_for_backward(q, k, v, o, L)\\n        ctx.grid = grid\\n        ctx.sm_scale = sm_scale\\n        ctx.BLOCK_DMODEL = Lk\\n        ctx.causal = causal\\n        return o\\n\\n    @staticmethod\\n    def backward(ctx, do):\\n        BLOCK = 128\\n        q, k, v, o, L = ctx.saved_tensors\\n        do = do.contiguous()\\n        dq = torch.zeros_like(q, dtype=torch.float32)\\n        dk = torch.empty_like(k)\\n        dv = torch.empty_like(v)\\n        delta = torch.empty_like(L)\\n        _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\\n            o, do,\\n            delta,\\n            BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\\n        )\\n        _bwd_kernel[(ctx.grid[1],)](\\n            q, k, v, ctx.sm_scale,\\n            o, do,\\n            dq, dk, dv,\\n            L, delta,\\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\\n            q.shape[0], q.shape[1], q.shape[2],\\n            ctx.grid[0],\\n            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\\n            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\\n            CAUSAL=ctx.causal,\\n            num_stages=1,\\n        )\\n        return dq, dk, dv, None, None\\n\\n\\nattention = _attention.apply\\n\\n\\n@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(6, 9, 1024, 64)])\\n@pytest.mark.parametrize('causal', [False, True])\\ndef test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\\n    torch.manual_seed(20)\\n    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0., std=0.5).requires_grad_()\\n    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0., std=0.5).requires_grad_()\\n    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0., std=0.5).requires_grad_()\\n    sm_scale = 0.5\\n    dout = torch.randn_like(q)\\n    # reference implementation\\n    M = torch.tril(torch.ones((N_CTX, N_CTX), device=\\\"cuda\\\"))\\n    p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\\n    if causal:\\n        p[:, :, M == 0] = float(\\\"-inf\\\")\\n    p = torch.softmax(p.float(), dim=-1).half()\\n    # p = torch.exp(p)\\n    ref_out = torch.matmul(p, v)\\n    ref_out.backward(dout)\\n    ref_dv, v.grad = v.grad.clone(), None\\n    ref_dk, k.grad = k.grad.clone(), None\\n    ref_dq, q.grad = q.grad.clone(), None\\n    # triton implementation\\n    tri_out = attention(q, k, v, causal, sm_scale).half()\\n    tri_out.backward(dout)\\n    tri_dv, v.grad = v.grad.clone(), None\\n    tri_dk, k.grad = k.grad.clone(), None\\n    tri_dq, q.grad = q.grad.clone(), None\\n    # compare\\n    assert torch.allclose(ref_out, tri_out, atol=1e-2, rtol=0)\\n    assert torch.allclose(ref_dv, tri_dv, atol=1e-2, rtol=0)\\n    assert torch.allclose(ref_dk, tri_dk, atol=1e-2, rtol=0)\\n    assert torch.allclose(ref_dq, tri_dq, atol=1e-2, rtol=0)\\n\\n\\ntry:\\n    from flash_attn.flash_attn_interface import flash_attn_qkvpacked_func as flash_attn_func\\n    FLASH_VER = 2\\nexcept BaseException:\\n    try:\\n        from flash_attn.flash_attn_interface import flash_attn_func\\n        FLASH_VER = 1\\n    except BaseException:\\n        FLASH_VER = None\\nHAS_FLASH = FLASH_VER is not None\\n\\nBATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\\n# vary seq length for fixed head and batch=4\\nconfigs = [triton.testing.Benchmark(\\n    x_names=['N_CTX'],\\n    x_vals=[2**i for i in range(10, 15)],\\n    line_arg='provider',\\n    line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\\n    line_names=['Triton'] + ([f'Flash-{FLASH_VER}'] if HAS_FLASH else []),\\n    styles=[('red', '-'), ('blue', '-')],\\n    ylabel='ms',\\n    plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\\n    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode, 'causal': causal}\\n) for mode in ['fwd', 'bwd'] for causal in [False, True]]\\n\\n\\n@triton.testing.perf_report(configs)\\ndef bench_flash_attention(BATCH, H, N_CTX, D_HEAD, causal, mode, provider, dtype=torch.float16, device=\\\"cuda\\\"):\\n    assert mode in ['fwd', 'bwd']\\n    warmup = 25\\n    rep = 100\\n    if provider == \\\"triton\\\":\\n        q = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        sm_scale = 1.3\\n        fn = lambda: attention(q, k, v, causal, sm_scale)\\n        if mode == 'bwd':\\n            o = fn()\\n            do = torch.randn_like(o)\\n            fn = lambda: o.backward(do, retain_graph=True)\\n        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\\n    if provider == \\\"flash\\\":\\n        qkv = torch.randn((BATCH, N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\\n        if FLASH_VER == 1:\\n            lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\\n            cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\\n            cu_seqlens[1:] = lengths.cumsum(0)\\n            qkv = qkv.reshape(BATCH * N_CTX, 3, H, D_HEAD)\\n            fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=causal)\\n        elif FLASH_VER == 2:\\n            fn = lambda: flash_attn_func(qkv, causal=causal)\\n        else:\\n            raise ValueError(f'unknown {FLASH_VER = }')\\n        if mode == 'bwd':\\n            o = fn()\\n            do = torch.randn_like(o)\\n            fn = lambda: o.backward(do, retain_graph=True)\\n        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\\n    flops_per_matmul = 2. * BATCH * H * N_CTX * N_CTX * D_HEAD\\n    total_flops = 2 * flops_per_matmul\\n    if causal:\\n        total_flops *= 0.5\\n    if mode == 'bwd':\\n        total_flops *= 2.5  # 2.0(bwd) + 0.5(recompute)\\n    return total_flops / ms * 1e-9\\n\\n\\n# only works on post-Ampere GPUs right now\\nbench_flash_attention.run(save_path='.', print_data=True)\"\n       ]\n     }\n   ],"}, {"filename": "main/_downloads/54a35f6ec55f9746935b9566fb6bb1df/06-fused-attention.py", "status": "modified", "additions": 20, "deletions": 9, "changes": 29, "file_content_changes": "@@ -338,10 +338,15 @@ def test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\n \n \n try:\n-    from flash_attn.flash_attn_interface import flash_attn_func\n-    HAS_FLASH = True\n+    from flash_attn.flash_attn_interface import flash_attn_qkvpacked_func as flash_attn_func\n+    FLASH_VER = 2\n except BaseException:\n-    HAS_FLASH = False\n+    try:\n+        from flash_attn.flash_attn_interface import flash_attn_func\n+        FLASH_VER = 1\n+    except BaseException:\n+        FLASH_VER = None\n+HAS_FLASH = FLASH_VER is not None\n \n BATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\n # vary seq length for fixed head and batch=4\n@@ -350,7 +355,7 @@ def test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\n     x_vals=[2**i for i in range(10, 15)],\n     line_arg='provider',\n     line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n-    line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\n+    line_names=['Triton'] + ([f'Flash-{FLASH_VER}'] if HAS_FLASH else []),\n     styles=[('red', '-'), ('blue', '-')],\n     ylabel='ms',\n     plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n@@ -375,11 +380,17 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, causal, mode, provider, dtype\n             fn = lambda: o.backward(do, retain_graph=True)\n         ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n     if provider == \"flash\":\n-        lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n-        cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n-        cu_seqlens[1:] = lengths.cumsum(0)\n-        qkv = torch.randn((BATCH * N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\n-        fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=causal)\n+        qkv = torch.randn((BATCH, N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\n+        if FLASH_VER == 1:\n+            lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n+            cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n+            cu_seqlens[1:] = lengths.cumsum(0)\n+            qkv = qkv.reshape(BATCH * N_CTX, 3, H, D_HEAD)\n+            fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=causal)\n+        elif FLASH_VER == 2:\n+            fn = lambda: flash_attn_func(qkv, causal=causal)\n+        else:\n+            raise ValueError(f'unknown {FLASH_VER = }')\n         if mode == 'bwd':\n             o = fn()\n             do = torch.randn_like(o)"}, {"filename": "main/_downloads/662999063954282841dc90b8945f85ce/tutorials_jupyter.zip", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/763344228ae6bc253ed1a6cf586aa30d/tutorials_python.zip", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_01-vector-add_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_01-vector-add_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_02-fused-softmax_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_02-fused-softmax_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_03-matrix-multiplication_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_03-matrix-multiplication_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_05-layer-norm_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_05-layer-norm_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_002.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_003.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_004.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_sources/getting-started/tutorials/01-vector-add.rst.txt", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -237,20 +237,20 @@ We can now run the decorated function above. Pass `print_data=True` to see the p\n     vector-add-performance:\n                size       Triton        Torch\n     0        4096.0     8.000000     9.600000\n-    1        8192.0    15.999999    19.200000\n+    1        8192.0    15.999999    15.999999\n     2       16384.0    31.999999    31.999999\n     3       32768.0    63.999998    63.999998\n     4       65536.0   127.999995   127.999995\n     5      131072.0   219.428568   219.428568\n     6      262144.0   384.000001   384.000001\n     7      524288.0   614.400016   614.400016\n     8     1048576.0   819.200021   819.200021\n-    9     2097152.0  1068.521715  1023.999964\n-    10    4194304.0  1228.800031  1228.800031\n+    9     2097152.0  1023.999964  1023.999964\n+    10    4194304.0  1260.307736  1228.800031\n     11    8388608.0  1424.695621  1424.695621\n     12   16777216.0  1560.380965  1560.380965\n-    13   33554432.0  1631.601649  1624.859540\n-    14   67108864.0  1669.706983  1664.406388\n+    13   33554432.0  1624.859540  1624.859540\n+    14   67108864.0  1669.706983  1662.646960\n     15  134217728.0  1684.008546  1680.410210\n \n \n@@ -259,7 +259,7 @@ We can now run the decorated function above. Pass `print_data=True` to see the p\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  5.748 seconds)\n+   **Total running time of the script:** ( 0 minutes  5.818 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_01-vector-add.py:"}, {"filename": "main/_sources/getting-started/tutorials/02-fused-softmax.rst.txt", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -286,17 +286,17 @@ We will then compare its performance against (1) :code:`torch.softmax` and (2) t\n \n     softmax-performance:\n               N       Triton  Torch (native)  Torch (jit)\n-    0     256.0   682.666643      744.727267   273.066674\n+    0     256.0   682.666643      744.727267   264.258068\n     1     384.0   877.714274      819.200021   332.108094\n     2     512.0   910.222190      910.222190   372.363633\n-    3     640.0   975.238103      930.909084   409.600010\n-    4     768.0  1068.521715     1023.999964   438.857137\n+    3     640.0   975.238103      975.238103   409.600010\n+    4     768.0  1068.521715      983.040025   431.157886\n     ..      ...          ...             ...          ...\n     93  12160.0  1601.316858     1066.082150   592.267887\n-    94  12288.0  1604.963246     1013.443336   592.192778\n-    95  12416.0  1595.630495     1024.000037   590.359582\n-    96  12544.0  1599.235121     1013.656595   591.175267\n-    97  12672.0  1602.782573     1006.213368   591.113716\n+    94  12288.0  1604.963246     1013.443336   593.085987\n+    95  12416.0  1595.630495     1026.645975   589.483671\n+    96  12544.0  1599.235121     1011.103284   592.047213\n+    97  12672.0  1596.472358     1008.716405   591.113716\n \n     [98 rows x 4 columns]\n \n@@ -313,7 +313,7 @@ In the above plot, we can see that:\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  38.192 seconds)\n+   **Total running time of the script:** ( 0 minutes  39.168 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:"}, {"filename": "main/_sources/getting-started/tutorials/03-matrix-multiplication.rst.txt", "status": "modified", "additions": 20, "deletions": 20, "changes": 40, "file_content_changes": "@@ -449,7 +449,7 @@ but feel free to arrange this script as you wish to benchmark any other matrix s\n \n     matmul-performance:\n              M      cuBLAS      Triton\n-    0    256.0    4.681143    4.096000\n+    0    256.0    4.096000    4.096000\n     1    384.0   12.288000   12.288000\n     2    512.0   26.214401   23.831273\n     3    640.0   42.666665   42.666665\n@@ -462,32 +462,32 @@ but feel free to arrange this script as you wish to benchmark any other matrix s\n     10  1536.0  181.484314  157.286398\n     11  1664.0  183.651271  179.978245\n     12  1792.0  172.914215  208.137481\n-    13  1920.0  203.294114  168.585369\n-    14  2048.0  199.728763  188.508043\n-    15  2176.0  191.653792  211.827867\n-    16  2304.0  229.691080  227.503545\n-    17  2432.0  203.583068  202.118452\n-    18  2560.0  221.405396  217.006622\n-    19  2688.0  197.567993  198.602388\n-    20  2816.0  210.696652  208.680416\n-    21  2944.0  221.493479  221.493479\n-    22  3072.0  208.173173  207.410628\n-    23  3200.0  214.046818  213.333323\n-    24  3328.0  203.941342  206.278780\n-    25  3456.0  215.565692  217.896133\n-    26  3584.0  216.142772  207.656790\n-    27  3712.0  208.119472  216.228019\n-    28  3840.0  209.058596  205.944129\n-    29  3968.0  210.386099  216.738793\n-    30  4096.0  220.029067  211.366499\n+    13  1920.0  200.347822  166.554219\n+    14  2048.0  197.379013  190.650180\n+    15  2176.0  193.496618  211.827867\n+    16  2304.0  227.503545  229.691080\n+    17  2432.0  203.583068  200.674737\n+    18  2560.0  224.438347  214.169933\n+    19  2688.0  197.567993  197.567993\n+    20  2816.0  208.680416  210.696652\n+    21  2944.0  224.486628  223.479969\n+    22  3072.0  208.173173  206.653671\n+    23  3200.0  216.949149  217.687077\n+    24  3328.0  206.871539  204.520726\n+    25  3456.0  215.565692  215.565692\n+    26  3584.0  215.624440  205.756041\n+    27  3712.0  205.973906  216.697064\n+    28  3840.0  208.271176  211.053446\n+    29  3968.0  210.023986  219.073497\n+    30  4096.0  219.310012  208.412616\n \n \n \n \n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  40.850 seconds)\n+   **Total running time of the script:** ( 0 minutes  40.858 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_03-matrix-multiplication.py:"}, {"filename": "main/_sources/getting-started/tutorials/04-low-memory-dropout.rst.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -242,7 +242,7 @@ References\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  0.640 seconds)\n+   **Total running time of the script:** ( 0 minutes  0.645 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_04-low-memory-dropout.py:"}, {"filename": "main/_sources/getting-started/tutorials/05-layer-norm.rst.txt", "status": "modified", "additions": 25, "deletions": 25, "changes": 50, "file_content_changes": "@@ -431,36 +431,36 @@ Specifically, one can set :code:`'mode': 'backward'` to benchmark the backward p\n \n     layer-norm-backward:\n               N       Triton       Torch\n-    0    1024.0   178.086953  372.363633\n-    1    1536.0   267.130429  438.857146\n-    2    2048.0   364.088903  496.484863\n-    3    2560.0   391.337574  534.260858\n-    4    3072.0   494.818794  542.117638\n-    5    3584.0   618.820161  470.032796\n-    6    4096.0   682.666643  474.898540\n-    7    4608.0   718.129898  480.834772\n+    0    1024.0   184.781963  372.363633\n+    1    1536.0   281.404594  438.857146\n+    2    2048.0   375.206126  496.484863\n+    3    2560.0   476.279097  534.260858\n+    4    3072.0   580.535403  538.160602\n+    5    3584.0   646.736871  470.032796\n+    6    4096.0   733.611931  474.898540\n+    7    4608.0   718.129898  478.753251\n     8    5120.0   772.830175  483.779502\n-    9    5632.0   824.195135  489.739120\n-    10   6144.0   867.388239  494.818794\n+    9    5632.0   819.199976  489.739120\n+    10   6144.0   862.315754  494.818794\n     11   6656.0   907.636357  499.200013\n     12   7168.0   940.065592  477.866659\n-    13   7680.0   975.238103  481.253256\n+    13   7680.0   975.238103  479.999983\n     14   8192.0  1013.443336  489.074621\n-    15   8704.0   676.038845  489.217808\n-    16   9216.0   706.658154  491.520008\n-    17   9728.0   729.600018  495.694261\n-    18  10240.0   749.268305  495.483878\n-    19  10752.0   777.253021  487.803392\n-    20  11264.0   804.571435  488.853509\n-    21  11776.0   821.581395  494.097911\n+    15   8704.0   676.038845  488.074767\n+    16   9216.0   704.407633  491.520008\n+    17   9728.0   729.600018  496.748937\n+    18  10240.0   746.990876  496.484863\n+    19  10752.0   774.918911  487.803392\n+    20  11264.0   802.183964  488.853509\n+    21  11776.0   821.581395  493.235604\n     22  12288.0   845.020035  499.005061\n-    23  12800.0   850.969498  501.141916\n+    23  12800.0   848.618804  501.141916\n     24  13312.0   868.173894  501.551005\n-    25  13824.0   863.999969  501.172223\n-    26  14336.0   873.258878  492.223158\n-    27  14848.0   875.557709  495.621695\n-    28  15360.0   892.590796  501.551014\n-    29  15872.0   890.018693  503.873020\n+    25  13824.0   861.755862  500.416301\n+    26  14336.0   868.848510  492.223158\n+    27  14848.0   873.411781  495.621695\n+    28  15360.0   888.289183  501.551014\n+    29  15872.0   887.944041  503.207397\n \n \n \n@@ -475,7 +475,7 @@ References\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  29.314 seconds)\n+   **Total running time of the script:** ( 0 minutes  29.092 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_05-layer-norm.py:"}, {"filename": "main/_sources/getting-started/tutorials/06-fused-attention.rst.txt", "status": "modified", "additions": 42, "deletions": 31, "changes": 73, "file_content_changes": "@@ -28,7 +28,7 @@ Extra Credits:\n - Rabe and Staats (https://arxiv.org/pdf/2112.05682v2.pdf)\n - Adam P. Goucher for simplified vector math\n \n-.. GENERATED FROM PYTHON SOURCE LINES 13-399\n+.. GENERATED FROM PYTHON SOURCE LINES 13-410\n \n \n \n@@ -70,32 +70,32 @@ Extra Credits:\n \n     fused-attention-batch4-head48-d64-fwd:\n          N_CTX      Triton\n-    0   1024.0  157.860784\n-    1   2048.0  165.186092\n-    2   4096.0  172.228558\n-    3   8192.0  174.599752\n-    4  16384.0  175.629769\n+    0   1024.0  157.415728\n+    1   2048.0  165.463052\n+    2   4096.0  172.317023\n+    3   8192.0  174.601620\n+    4  16384.0  175.634550\n     fused-attention-batch4-head48-d64-fwd:\n          N_CTX      Triton\n-    0   1024.0  120.017170\n-    1   2048.0  138.834658\n-    2   4096.0  150.976069\n-    3   8192.0  158.414006\n-    4  16384.0  157.717652\n+    0   1024.0  119.564145\n+    1   2048.0  138.308878\n+    2   4096.0  151.987187\n+    3   8192.0  158.625495\n+    4  16384.0  156.451804\n     fused-attention-batch4-head48-d64-bwd:\n          N_CTX     Triton\n-    0   1024.0  77.793238\n-    1   2048.0  87.920955\n-    2   4096.0  92.952857\n-    3   8192.0  95.529767\n-    4  16384.0  97.845650\n+    0   1024.0  77.903830\n+    1   2048.0  87.986063\n+    2   4096.0  94.078948\n+    3   8192.0  95.667082\n+    4  16384.0  97.981358\n     fused-attention-batch4-head48-d64-bwd:\n          N_CTX     Triton\n-    0   1024.0  54.549761\n-    1   2048.0  68.524681\n-    2   4096.0  77.466501\n-    3   8192.0  82.812960\n-    4  16384.0  85.458609\n+    0   1024.0  54.569915\n+    1   2048.0  68.501546\n+    2   4096.0  77.525315\n+    3   8192.0  82.799340\n+    4  16384.0  85.959859\n \n \n \n@@ -434,10 +434,15 @@ Extra Credits:\n \n \n     try:\n-        from flash_attn.flash_attn_interface import flash_attn_func\n-        HAS_FLASH = True\n+        from flash_attn.flash_attn_interface import flash_attn_qkvpacked_func as flash_attn_func\n+        FLASH_VER = 2\n     except BaseException:\n-        HAS_FLASH = False\n+        try:\n+            from flash_attn.flash_attn_interface import flash_attn_func\n+            FLASH_VER = 1\n+        except BaseException:\n+            FLASH_VER = None\n+    HAS_FLASH = FLASH_VER is not None\n \n     BATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\n     # vary seq length for fixed head and batch=4\n@@ -446,7 +451,7 @@ Extra Credits:\n         x_vals=[2**i for i in range(10, 15)],\n         line_arg='provider',\n         line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n-        line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\n+        line_names=['Triton'] + ([f'Flash-{FLASH_VER}'] if HAS_FLASH else []),\n         styles=[('red', '-'), ('blue', '-')],\n         ylabel='ms',\n         plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n@@ -471,11 +476,17 @@ Extra Credits:\n                 fn = lambda: o.backward(do, retain_graph=True)\n             ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n         if provider == \"flash\":\n-            lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n-            cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n-            cu_seqlens[1:] = lengths.cumsum(0)\n-            qkv = torch.randn((BATCH * N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\n-            fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=causal)\n+            qkv = torch.randn((BATCH, N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\n+            if FLASH_VER == 1:\n+                lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n+                cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n+                cu_seqlens[1:] = lengths.cumsum(0)\n+                qkv = qkv.reshape(BATCH * N_CTX, 3, H, D_HEAD)\n+                fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=causal)\n+            elif FLASH_VER == 2:\n+                fn = lambda: flash_attn_func(qkv, causal=causal)\n+            else:\n+                raise ValueError(f'unknown {FLASH_VER = }')\n             if mode == 'bwd':\n                 o = fn()\n                 do = torch.randn_like(o)\n@@ -496,7 +507,7 @@ Extra Credits:\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  14.141 seconds)\n+   **Total running time of the script:** ( 0 minutes  14.274 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_06-fused-attention.py:"}, {"filename": "main/_sources/getting-started/tutorials/08-experimental-block-pointer.rst.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -298,7 +298,7 @@ Still we can test our matrix multiplication with block pointers against a native\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  6.457 seconds)\n+   **Total running time of the script:** ( 0 minutes  6.432 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_08-experimental-block-pointer.py:"}, {"filename": "main/_sources/getting-started/tutorials/sg_execution_times.rst.txt", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -6,22 +6,22 @@\n \n Computation times\n =================\n-**02:15.565** total execution time for **getting-started_tutorials** files:\n+**02:16.510** total execution time for **getting-started_tutorials** files:\n \n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``)           | 00:40.850 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``)           | 00:40.858 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                           | 00:38.192 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                           | 00:39.168 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                                 | 00:29.314 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                                 | 00:29.092 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py` (``06-fused-attention.py``)                       | 00:14.141 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py` (``06-fused-attention.py``)                       | 00:14.274 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_08-experimental-block-pointer.py` (``08-experimental-block-pointer.py``) | 00:06.457 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_08-experimental-block-pointer.py` (``08-experimental-block-pointer.py``) | 00:06.432 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                                 | 00:05.748 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                                 | 00:05.818 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)                 | 00:00.640 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)                 | 00:00.645 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n | :ref:`sphx_glr_getting-started_tutorials_07-math-functions.py` (``07-math-functions.py``)                         | 00:00.223 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+"}, {"filename": "main/getting-started/tutorials/01-vector-add.html", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -236,24 +236,24 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n <img src=\"../../_images/sphx_glr_01-vector-add_001.png\" srcset=\"../../_images/sphx_glr_01-vector-add_001.png\" alt=\"01 vector add\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>vector-add-performance:\n            size       Triton        Torch\n 0        4096.0     8.000000     9.600000\n-1        8192.0    15.999999    19.200000\n+1        8192.0    15.999999    15.999999\n 2       16384.0    31.999999    31.999999\n 3       32768.0    63.999998    63.999998\n 4       65536.0   127.999995   127.999995\n 5      131072.0   219.428568   219.428568\n 6      262144.0   384.000001   384.000001\n 7      524288.0   614.400016   614.400016\n 8     1048576.0   819.200021   819.200021\n-9     2097152.0  1068.521715  1023.999964\n-10    4194304.0  1228.800031  1228.800031\n+9     2097152.0  1023.999964  1023.999964\n+10    4194304.0  1260.307736  1228.800031\n 11    8388608.0  1424.695621  1424.695621\n 12   16777216.0  1560.380965  1560.380965\n-13   33554432.0  1631.601649  1624.859540\n-14   67108864.0  1669.706983  1664.406388\n+13   33554432.0  1624.859540  1624.859540\n+14   67108864.0  1669.706983  1662.646960\n 15  134217728.0  1684.008546  1680.410210\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  5.748 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  5.818 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-01-vector-add-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/62d97d49a32414049819dd8bb8378080/01-vector-add.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">01-vector-add.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/02-fused-softmax.html", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -282,17 +282,17 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n </div>\n <img src=\"../../_images/sphx_glr_02-fused-softmax_001.png\" srcset=\"../../_images/sphx_glr_02-fused-softmax_001.png\" alt=\"02 fused softmax\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>softmax-performance:\n           N       Triton  Torch (native)  Torch (jit)\n-0     256.0   682.666643      744.727267   273.066674\n+0     256.0   682.666643      744.727267   264.258068\n 1     384.0   877.714274      819.200021   332.108094\n 2     512.0   910.222190      910.222190   372.363633\n-3     640.0   975.238103      930.909084   409.600010\n-4     768.0  1068.521715     1023.999964   438.857137\n+3     640.0   975.238103      975.238103   409.600010\n+4     768.0  1068.521715      983.040025   431.157886\n ..      ...          ...             ...          ...\n 93  12160.0  1601.316858     1066.082150   592.267887\n-94  12288.0  1604.963246     1013.443336   592.192778\n-95  12416.0  1595.630495     1024.000037   590.359582\n-96  12544.0  1599.235121     1013.656595   591.175267\n-97  12672.0  1602.782573     1006.213368   591.113716\n+94  12288.0  1604.963246     1013.443336   593.085987\n+95  12416.0  1595.630495     1026.645975   589.483671\n+96  12544.0  1599.235121     1011.103284   592.047213\n+97  12672.0  1596.472358     1008.716405   591.113716\n \n [98 rows x 4 columns]\n </pre></div>\n@@ -305,7 +305,7 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n </ul>\n </dd>\n </dl>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  38.192 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  39.168 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-02-fused-softmax-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d91442ac2982c4e0cc3ab0f43534afbc/02-fused-softmax.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">02-fused-softmax.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/03-matrix-multiplication.html", "status": "modified", "additions": 20, "deletions": 20, "changes": 40, "file_content_changes": "@@ -463,7 +463,7 @@ <h3>Square Matrix Performance<a class=\"headerlink\" href=\"#square-matrix-performa\n </div>\n <img src=\"../../_images/sphx_glr_03-matrix-multiplication_001.png\" srcset=\"../../_images/sphx_glr_03-matrix-multiplication_001.png\" alt=\"03 matrix multiplication\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>matmul-performance:\n          M      cuBLAS      Triton\n-0    256.0    4.681143    4.096000\n+0    256.0    4.096000    4.096000\n 1    384.0   12.288000   12.288000\n 2    512.0   26.214401   23.831273\n 3    640.0   42.666665   42.666665\n@@ -476,27 +476,27 @@ <h3>Square Matrix Performance<a class=\"headerlink\" href=\"#square-matrix-performa\n 10  1536.0  181.484314  157.286398\n 11  1664.0  183.651271  179.978245\n 12  1792.0  172.914215  208.137481\n-13  1920.0  203.294114  168.585369\n-14  2048.0  199.728763  188.508043\n-15  2176.0  191.653792  211.827867\n-16  2304.0  229.691080  227.503545\n-17  2432.0  203.583068  202.118452\n-18  2560.0  221.405396  217.006622\n-19  2688.0  197.567993  198.602388\n-20  2816.0  210.696652  208.680416\n-21  2944.0  221.493479  221.493479\n-22  3072.0  208.173173  207.410628\n-23  3200.0  214.046818  213.333323\n-24  3328.0  203.941342  206.278780\n-25  3456.0  215.565692  217.896133\n-26  3584.0  216.142772  207.656790\n-27  3712.0  208.119472  216.228019\n-28  3840.0  209.058596  205.944129\n-29  3968.0  210.386099  216.738793\n-30  4096.0  220.029067  211.366499\n+13  1920.0  200.347822  166.554219\n+14  2048.0  197.379013  190.650180\n+15  2176.0  193.496618  211.827867\n+16  2304.0  227.503545  229.691080\n+17  2432.0  203.583068  200.674737\n+18  2560.0  224.438347  214.169933\n+19  2688.0  197.567993  197.567993\n+20  2816.0  208.680416  210.696652\n+21  2944.0  224.486628  223.479969\n+22  3072.0  208.173173  206.653671\n+23  3200.0  216.949149  217.687077\n+24  3328.0  206.871539  204.520726\n+25  3456.0  215.565692  215.565692\n+26  3584.0  215.624440  205.756041\n+27  3712.0  205.973906  216.697064\n+28  3840.0  208.271176  211.053446\n+29  3968.0  210.023986  219.073497\n+30  4096.0  219.310012  208.412616\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  40.850 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  40.858 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-03-matrix-multiplication-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d5fee5b55a64e47f1b5724ec39adf171/03-matrix-multiplication.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">03-matrix-multiplication.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/04-low-memory-dropout.html", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -287,7 +287,7 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n <p>Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \u201cDropout: A Simple Way to Prevent Neural Networks from Overfitting\u201d, JMLR 2014</p>\n </div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.640 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.645 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-04-low-memory-dropout-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/c9aed78977a4c05741d675a38dde3d7d/04-low-memory-dropout.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">04-low-memory-dropout.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/05-layer-norm.html", "status": "modified", "additions": 25, "deletions": 25, "changes": 50, "file_content_changes": "@@ -460,36 +460,36 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n </div>\n <img src=\"../../_images/sphx_glr_05-layer-norm_001.png\" srcset=\"../../_images/sphx_glr_05-layer-norm_001.png\" alt=\"05 layer norm\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>layer-norm-backward:\n           N       Triton       Torch\n-0    1024.0   178.086953  372.363633\n-1    1536.0   267.130429  438.857146\n-2    2048.0   364.088903  496.484863\n-3    2560.0   391.337574  534.260858\n-4    3072.0   494.818794  542.117638\n-5    3584.0   618.820161  470.032796\n-6    4096.0   682.666643  474.898540\n-7    4608.0   718.129898  480.834772\n+0    1024.0   184.781963  372.363633\n+1    1536.0   281.404594  438.857146\n+2    2048.0   375.206126  496.484863\n+3    2560.0   476.279097  534.260858\n+4    3072.0   580.535403  538.160602\n+5    3584.0   646.736871  470.032796\n+6    4096.0   733.611931  474.898540\n+7    4608.0   718.129898  478.753251\n 8    5120.0   772.830175  483.779502\n-9    5632.0   824.195135  489.739120\n-10   6144.0   867.388239  494.818794\n+9    5632.0   819.199976  489.739120\n+10   6144.0   862.315754  494.818794\n 11   6656.0   907.636357  499.200013\n 12   7168.0   940.065592  477.866659\n-13   7680.0   975.238103  481.253256\n+13   7680.0   975.238103  479.999983\n 14   8192.0  1013.443336  489.074621\n-15   8704.0   676.038845  489.217808\n-16   9216.0   706.658154  491.520008\n-17   9728.0   729.600018  495.694261\n-18  10240.0   749.268305  495.483878\n-19  10752.0   777.253021  487.803392\n-20  11264.0   804.571435  488.853509\n-21  11776.0   821.581395  494.097911\n+15   8704.0   676.038845  488.074767\n+16   9216.0   704.407633  491.520008\n+17   9728.0   729.600018  496.748937\n+18  10240.0   746.990876  496.484863\n+19  10752.0   774.918911  487.803392\n+20  11264.0   802.183964  488.853509\n+21  11776.0   821.581395  493.235604\n 22  12288.0   845.020035  499.005061\n-23  12800.0   850.969498  501.141916\n+23  12800.0   848.618804  501.141916\n 24  13312.0   868.173894  501.551005\n-25  13824.0   863.999969  501.172223\n-26  14336.0   873.258878  492.223158\n-27  14848.0   875.557709  495.621695\n-28  15360.0   892.590796  501.551014\n-29  15872.0   890.018693  503.873020\n+25  13824.0   861.755862  500.416301\n+26  14336.0   868.848510  492.223158\n+27  14848.0   873.411781  495.621695\n+28  15360.0   888.289183  501.551014\n+29  15872.0   887.944041  503.207397\n </pre></div>\n </div>\n </section>\n@@ -501,7 +501,7 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n <p>Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \u201cLayer Normalization\u201d, Arxiv 2016</p>\n </div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  29.314 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  29.092 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-05-layer-norm-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/935c0dd0fbeb4b2e69588471cbb2d4b2/05-layer-norm.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">05-layer-norm.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/06-fused-attention.html", "status": "modified", "additions": 41, "deletions": 30, "changes": 71, "file_content_changes": "@@ -117,32 +117,32 @@\n </ul>\n <div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>fused-attention-batch4-head48-d64-fwd:\n      N_CTX      Triton\n-0   1024.0  157.860784\n-1   2048.0  165.186092\n-2   4096.0  172.228558\n-3   8192.0  174.599752\n-4  16384.0  175.629769\n+0   1024.0  157.415728\n+1   2048.0  165.463052\n+2   4096.0  172.317023\n+3   8192.0  174.601620\n+4  16384.0  175.634550\n fused-attention-batch4-head48-d64-fwd:\n      N_CTX      Triton\n-0   1024.0  120.017170\n-1   2048.0  138.834658\n-2   4096.0  150.976069\n-3   8192.0  158.414006\n-4  16384.0  157.717652\n+0   1024.0  119.564145\n+1   2048.0  138.308878\n+2   4096.0  151.987187\n+3   8192.0  158.625495\n+4  16384.0  156.451804\n fused-attention-batch4-head48-d64-bwd:\n      N_CTX     Triton\n-0   1024.0  77.793238\n-1   2048.0  87.920955\n-2   4096.0  92.952857\n-3   8192.0  95.529767\n-4  16384.0  97.845650\n+0   1024.0  77.903830\n+1   2048.0  87.986063\n+2   4096.0  94.078948\n+3   8192.0  95.667082\n+4  16384.0  97.981358\n fused-attention-batch4-head48-d64-bwd:\n      N_CTX     Triton\n-0   1024.0  54.549761\n-1   2048.0  68.524681\n-2   4096.0  77.466501\n-3   8192.0  82.812960\n-4  16384.0  85.458609\n+0   1024.0  54.569915\n+1   2048.0  68.501546\n+2   4096.0  77.525315\n+3   8192.0  82.799340\n+4  16384.0  85.959859\n </pre></div>\n </div>\n <div class=\"line-block\">\n@@ -475,10 +475,15 @@\n \n \n <span class=\"k\">try</span><span class=\"p\">:</span>\n-    <span class=\"kn\">from</span> <span class=\"nn\">flash_attn.flash_attn_interface</span> <span class=\"kn\">import</span> <span class=\"n\">flash_attn_func</span>\n-    <span class=\"n\">HAS_FLASH</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>\n+    <span class=\"kn\">from</span> <span class=\"nn\">flash_attn.flash_attn_interface</span> <span class=\"kn\">import</span> <span class=\"n\">flash_attn_qkvpacked_func</span> <span class=\"k\">as</span> <span class=\"n\">flash_attn_func</span>\n+    <span class=\"n\">FLASH_VER</span> <span class=\"o\">=</span> <span class=\"mi\">2</span>\n <span class=\"k\">except</span> <span class=\"ne\">BaseException</span><span class=\"p\">:</span>\n-    <span class=\"n\">HAS_FLASH</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n+    <span class=\"k\">try</span><span class=\"p\">:</span>\n+        <span class=\"kn\">from</span> <span class=\"nn\">flash_attn.flash_attn_interface</span> <span class=\"kn\">import</span> <span class=\"n\">flash_attn_func</span>\n+        <span class=\"n\">FLASH_VER</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n+    <span class=\"k\">except</span> <span class=\"ne\">BaseException</span><span class=\"p\">:</span>\n+        <span class=\"n\">FLASH_VER</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n+<span class=\"n\">HAS_FLASH</span> <span class=\"o\">=</span> <span class=\"n\">FLASH_VER</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span>\n \n <span class=\"n\">BATCH</span><span class=\"p\">,</span> <span class=\"n\">N_HEADS</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span> <span class=\"o\">=</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">48</span><span class=\"p\">,</span> <span class=\"mi\">4096</span><span class=\"p\">,</span> <span class=\"mi\">64</span>\n <span class=\"c1\"># vary seq length for fixed head and batch=4</span>\n@@ -487,7 +492,7 @@\n     <span class=\"n\">x_vals</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"o\">**</span><span class=\"n\">i</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">15</span><span class=\"p\">)],</span>\n     <span class=\"n\">line_arg</span><span class=\"o\">=</span><span class=\"s1\">&#39;provider&#39;</span><span class=\"p\">,</span>\n     <span class=\"n\">line_vals</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"p\">([</span><span class=\"s1\">&#39;flash&#39;</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">HAS_FLASH</span> <span class=\"k\">else</span> <span class=\"p\">[]),</span>\n-    <span class=\"n\">line_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;Triton&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"p\">([</span><span class=\"s1\">&#39;Flash&#39;</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">HAS_FLASH</span> <span class=\"k\">else</span> <span class=\"p\">[]),</span>\n+    <span class=\"n\">line_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;Triton&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"p\">([</span><span class=\"sa\">f</span><span class=\"s1\">&#39;Flash-</span><span class=\"si\">{</span><span class=\"n\">FLASH_VER</span><span class=\"si\">}</span><span class=\"s1\">&#39;</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">HAS_FLASH</span> <span class=\"k\">else</span> <span class=\"p\">[]),</span>\n     <span class=\"n\">styles</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s1\">&#39;red&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;blue&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">)],</span>\n     <span class=\"n\">ylabel</span><span class=\"o\">=</span><span class=\"s1\">&#39;ms&#39;</span><span class=\"p\">,</span>\n     <span class=\"n\">plot_name</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"s1\">&#39;fused-attention-batch</span><span class=\"si\">{</span><span class=\"n\">BATCH</span><span class=\"si\">}</span><span class=\"s1\">-head</span><span class=\"si\">{</span><span class=\"n\">N_HEADS</span><span class=\"si\">}</span><span class=\"s1\">-d</span><span class=\"si\">{</span><span class=\"n\">D_HEAD</span><span class=\"si\">}</span><span class=\"s1\">-</span><span class=\"si\">{</span><span class=\"n\">mode</span><span class=\"si\">}</span><span class=\"s1\">&#39;</span><span class=\"p\">,</span>\n@@ -512,11 +517,17 @@\n             <span class=\"n\">fn</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">(</span><span class=\"n\">do</span><span class=\"p\">,</span> <span class=\"n\">retain_graph</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n         <span class=\"n\">ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"n\">fn</span><span class=\"p\">,</span> <span class=\"n\">warmup</span><span class=\"o\">=</span><span class=\"n\">warmup</span><span class=\"p\">,</span> <span class=\"n\">rep</span><span class=\"o\">=</span><span class=\"n\">rep</span><span class=\"p\">)</span>\n     <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;flash&quot;</span><span class=\"p\">:</span>\n-        <span class=\"n\">lengths</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"n\">BATCH</span><span class=\"p\">,),</span> <span class=\"n\">fill_value</span><span class=\"o\">=</span><span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">)</span>\n-        <span class=\"n\">cu_seqlens</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">BATCH</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">,),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">)</span>\n-        <span class=\"n\">cu_seqlens</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:]</span> <span class=\"o\">=</span> <span class=\"n\">lengths</span><span class=\"o\">.</span><span class=\"n\">cumsum</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-        <span class=\"n\">qkv</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"n\">BATCH</span> <span class=\"o\">*</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n-        <span class=\"n\">fn</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">flash_attn_func</span><span class=\"p\">(</span><span class=\"n\">qkv</span><span class=\"p\">,</span> <span class=\"n\">cu_seqlens</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">causal</span><span class=\"o\">=</span><span class=\"n\">causal</span><span class=\"p\">)</span>\n+        <span class=\"n\">qkv</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"n\">BATCH</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+        <span class=\"k\">if</span> <span class=\"n\">FLASH_VER</span> <span class=\"o\">==</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n+            <span class=\"n\">lengths</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"n\">BATCH</span><span class=\"p\">,),</span> <span class=\"n\">fill_value</span><span class=\"o\">=</span><span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">)</span>\n+            <span class=\"n\">cu_seqlens</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">BATCH</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">,),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">)</span>\n+            <span class=\"n\">cu_seqlens</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:]</span> <span class=\"o\">=</span> <span class=\"n\">lengths</span><span class=\"o\">.</span><span class=\"n\">cumsum</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+            <span class=\"n\">qkv</span> <span class=\"o\">=</span> <span class=\"n\">qkv</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">BATCH</span> <span class=\"o\">*</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">)</span>\n+            <span class=\"n\">fn</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">flash_attn_func</span><span class=\"p\">(</span><span class=\"n\">qkv</span><span class=\"p\">,</span> <span class=\"n\">cu_seqlens</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">causal</span><span class=\"o\">=</span><span class=\"n\">causal</span><span class=\"p\">)</span>\n+        <span class=\"k\">elif</span> <span class=\"n\">FLASH_VER</span> <span class=\"o\">==</span> <span class=\"mi\">2</span><span class=\"p\">:</span>\n+            <span class=\"n\">fn</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">flash_attn_func</span><span class=\"p\">(</span><span class=\"n\">qkv</span><span class=\"p\">,</span> <span class=\"n\">causal</span><span class=\"o\">=</span><span class=\"n\">causal</span><span class=\"p\">)</span>\n+        <span class=\"k\">else</span><span class=\"p\">:</span>\n+            <span class=\"k\">raise</span> <span class=\"ne\">ValueError</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s1\">&#39;unknown </span><span class=\"si\">{</span><span class=\"n\">FLASH_VER</span><span class=\"w\"> </span><span class=\"si\">= }</span><span class=\"s1\">&#39;</span><span class=\"p\">)</span>\n         <span class=\"k\">if</span> <span class=\"n\">mode</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;bwd&#39;</span><span class=\"p\">:</span>\n             <span class=\"n\">o</span> <span class=\"o\">=</span> <span class=\"n\">fn</span><span class=\"p\">()</span>\n             <span class=\"n\">do</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn_like</span><span class=\"p\">(</span><span class=\"n\">o</span><span class=\"p\">)</span>\n@@ -535,7 +546,7 @@\n <span class=\"n\">bench_flash_attention</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;.&#39;</span><span class=\"p\">,</span> <span class=\"n\">print_data</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  14.141 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  14.274 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-06-fused-attention-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/54a35f6ec55f9746935b9566fb6bb1df/06-fused-attention.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">06-fused-attention.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/08-experimental-block-pointer.html", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -335,7 +335,7 @@ <h2>Unit Test<a class=\"headerlink\" href=\"#unit-test\" title=\"Permalink to this he\n \u2705 Triton and Torch match\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  6.457 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  6.432 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-08-experimental-block-pointer-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/4d6052117d61c2ca779cd4b75567fee5/08-experimental-block-pointer.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">08-experimental-block-pointer.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/sg_execution_times.html", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -86,35 +86,35 @@\n              \n   <section id=\"computation-times\">\n <span id=\"sphx-glr-getting-started-tutorials-sg-execution-times\"></span><h1>Computation times<a class=\"headerlink\" href=\"#computation-times\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<p><strong>02:15.565</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n+<p><strong>02:16.510</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n <table class=\"docutils align-default\">\n <tbody>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"03-matrix-multiplication.html#sphx-glr-getting-started-tutorials-03-matrix-multiplication-py\"><span class=\"std std-ref\">Matrix Multiplication</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">03-matrix-multiplication.py</span></code>)</p></td>\n-<td><p>00:40.850</p></td>\n+<td><p>00:40.858</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py\"><span class=\"std std-ref\">Fused Softmax</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">02-fused-softmax.py</span></code>)</p></td>\n-<td><p>00:38.192</p></td>\n+<td><p>00:39.168</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"05-layer-norm.html#sphx-glr-getting-started-tutorials-05-layer-norm-py\"><span class=\"std std-ref\">Layer Normalization</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">05-layer-norm.py</span></code>)</p></td>\n-<td><p>00:29.314</p></td>\n+<td><p>00:29.092</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"06-fused-attention.html#sphx-glr-getting-started-tutorials-06-fused-attention-py\"><span class=\"std std-ref\">Fused Attention</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">06-fused-attention.py</span></code>)</p></td>\n-<td><p>00:14.141</p></td>\n+<td><p>00:14.274</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"08-experimental-block-pointer.html#sphx-glr-getting-started-tutorials-08-experimental-block-pointer-py\"><span class=\"std std-ref\">Block Pointer (Experimental)</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">08-experimental-block-pointer.py</span></code>)</p></td>\n-<td><p>00:06.457</p></td>\n+<td><p>00:06.432</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py\"><span class=\"std std-ref\">Vector Addition</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">01-vector-add.py</span></code>)</p></td>\n-<td><p>00:05.748</p></td>\n+<td><p>00:05.818</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"04-low-memory-dropout.html#sphx-glr-getting-started-tutorials-04-low-memory-dropout-py\"><span class=\"std std-ref\">Low-Memory Dropout</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">04-low-memory-dropout.py</span></code>)</p></td>\n-<td><p>00:00.640</p></td>\n+<td><p>00:00.645</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"07-math-functions.html#sphx-glr-getting-started-tutorials-07-math-functions-py\"><span class=\"std std-ref\">Libdevice (tl.math) function</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">07-math-functions.py</span></code>)</p></td>"}, {"filename": "main/searchindex.js", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "N/A"}]