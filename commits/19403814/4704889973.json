[{"filename": "main/.buildinfo", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n # Sphinx build info version 1\n # This file hashes the configuration used when building these files. When it is not found, a full rebuild will be done.\n-config: 2f3fc59ccdc3ec0b99873223c23d6538\n+config: c0246c2755c9369676fe2eee0eabd07d\n tags: 645f666f9bcd5a90fca523b33c5a78b7"}, {"filename": "main/.doctrees/environment.pickle", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/installation.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/01-vector-add.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/02-fused-softmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/03-matrix-multiplication.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/04-low-memory-dropout.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/05-layer-norm.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/06-fused-attention.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/07-math-functions.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/08-experimental-block-pointer.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/index.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/sg_execution_times.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/index.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/programming-guide/chapter-1/introduction.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/programming-guide/chapter-2/related-work.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.Config.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.autotune.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.heuristics.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.jit.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.abs.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.arange.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.argmax.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.argmin.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_add.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_cas.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_max.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_min.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_xchg.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.broadcast_to.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cos.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.dot.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.exp.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.load.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.log.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.max.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.maximum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.min.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.minimum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.multiple_of.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.num_programs.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.program_id.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.rand.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randint.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randint4x.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randn.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.ravel.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.reduce.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.reshape.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sigmoid.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sin.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.softmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sqrt.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.store.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.where.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.xor_sum.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.zeros.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.Benchmark.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.do_bench.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.perf_report.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.language.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.testing.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/034d953b6214fedce6ea03803c712b89/02-fused-softmax.ipynb", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -15,7 +15,7 @@\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"\\n# Fused Softmax\\n\\nIn this tutorial, you will write a fused softmax operation that is significantly faster\\nthan PyTorch's native op for a particular class of matrices: those whose rows can fit in\\nthe GPU's SRAM.\\n\\nIn doing so, you will learn about:\\n- The benefits of kernel fusion for bandwidth-bound operations.\\n- Reduction operators in Triton.\\n\"\n+        \"\\n# Fused Softmax\\n\\nIn this tutorial, you will write a fused softmax operation that is significantly faster\\nthan PyTorch's native op for a particular class of matrices: those whose rows can fit in\\nthe GPU's SRAM.\\n\\nIn doing so, you will learn about:\\n\\n* The benefits of kernel fusion for bandwidth-bound operations.\\n\\n* Reduction operators in Triton.\\n\"\n       ]\n     },\n     {"}, {"filename": "main/_downloads/4d6052117d61c2ca779cd4b75567fee5/08-experimental-block-pointer.py", "status": "added", "additions": 220, "deletions": 0, "changes": 220, "file_content_changes": "@@ -0,0 +1,220 @@\n+\"\"\"\n+Block Pointer (Experimental)\n+============================\n+This tutorial will guide you through writing a matrix multiplication algorithm that utilizes block pointer semantics.\n+These semantics are more friendly for Triton to optimize and can result in better performance on specific hardware.\n+Note that this feature is still experimental and may change in the future.\n+\n+\"\"\"\n+\n+# %%\n+# Motivations\n+# -----------\n+# In the previous matrix multiplication tutorial, we constructed blocks of values by de-referencing blocks of pointers,\n+# i.e., :code:`load(block<pointer_type<element_type>>) -> block<element_type>`, which involved loading blocks of\n+# elements from memory. This approach allowed for flexibility in using hardware-managed cache and implementing complex\n+# data structures, such as tensors of trees or unstructured look-up tables.\n+#\n+# However, the drawback of this approach is that it relies heavily on complex optimization passes by the compiler to\n+# optimize memory access patterns. This can result in brittle code that may suffer from performance degradation when the\n+# optimizer fails to perform adequately. Additionally, as memory controllers specialize to accommodate dense spatial\n+# data structures commonly used in machine learning workloads, this problem is likely to worsen.\n+#\n+# To address this issue, we will use block pointers :code:`pointer_type<block<element_type>>` and load them into\n+# :code:`block<element_type>`, in which way gives better friendliness for the compiler to optimize memory access\n+# patterns.\n+#\n+# Let's start with the previous matrix multiplication example and demonstrate how to rewrite it to utilize block pointer\n+# semantics.\n+\n+# %%\n+# Make a Block Pointer\n+# --------------------\n+# A block pointer pointers to a block in a parent tensor and is constructed by :code:`make_block_ptr` function,\n+# which takes the following information as arguments:\n+# - :code:`base`: the base pointer to the parent tensor;\n+# - :code:`shape`: the shape of the parent tensor;\n+# - :code:`strides`: the strides of the parent tensor, which means how much to increase the pointer by when moving by 1 element in a specific axis;\n+# - :code:`offsets`: the offsets of the block;\n+# - :code:`block_shape`: the shape of the block;\n+# - :code:`order`: the order of the block, which means how the block is laid out in memory.\n+#\n+# For example, to a block pointer to a :code:`BLOCK_SIZE_M`x:code:`BLOCK_SIZE_K` block in a row-major 2D matrix A by\n+# offsets :code:`(pid_m * BLOCK_SIZE_M, 0)` and strides :code:`(stride_am, stride_ak)`, we can use the following code\n+# (exactly the same as the previous matrix multiplication tutorial):\n+#\n+# .. code-block:: python\n+#\n+#     a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\n+#                                     offsets=(pid_m * BLOCK_SIZE_M, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K),\n+#                                     order=(1, 0))\n+#\n+# Note that the :code:`order` argument is set to :code:`(1, 0)`, which means the second axis is the inner dimension in\n+# terms of storage, and the first axis is the outer dimension. This information may sound redundant, but it is necessary\n+# for some hardware backends to optimize for better performance.\n+\n+# %%\n+# Load/Store a Block Pointer\n+# --------------------------\n+# To load/store a block pointer, we can use :code:`load/store` function, which takes a block pointer as an argument,\n+# de-references it, and loads/stores a block. You may mask some values in the block, here we have an extra argument\n+# :code:`boundary_check` to specify whether to check the boundary of each axis for the block pointer. With check on and\n+# out-of-bound values will be masked according to the :code:`padding_option` argument (load only), which can be\n+# :code:`zero` or :code:`nan`. Temporarily, we do not support other values due to some hardware limitations. In this\n+# mode of block pointer load/store does not support :code:`mask` or :code:`other` arguments in the legacy mode.\n+#\n+# So to load a block in A, we can simply write :code:`a = tl.load(a_block_ptr, boundary_check=(0, 1))`. Boundary check\n+# may cost extra performance, so if you can guarantee that the block pointer is always in-bound in some axis, you can\n+# turn off the check by not passing the index into the :code:`boundary_check` argument. For example, if we know that\n+# :code:`M` is a multiple of :code:`BLOCK_SIZE_M`, we can write :code:`a = tl.load(a_block_ptr, boundary_check=(1, ))`.\n+\n+# %%\n+# Advance a Block Pointer\n+# -----------------------\n+# To advance a block pointer, we can use :code:`advance` function, which takes a block pointer and the increment for\n+# each axis as arguments and returns a new block pointer with the same shape and strides as the original one,\n+# but with the offsets advanced by the specified amount.\n+#\n+# For example, to advance the block pointer by :code:`BLOCK_SIZE_K` in the second axis\n+# (no need to multiply with stride), we can write :code:`a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))`.\n+\n+# %%\n+# Final Result\n+# ------------\n+\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.autotune(\n+    configs=[\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n+        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n+    ],\n+    key=['M', 'N', 'K'],\n+)\n+@triton.jit\n+def matmul_kernel_with_block_pointers(\n+        # Pointers to matrices\n+        a_ptr, b_ptr, c_ptr,\n+        # Matrix dimensions\n+        M, N, K,\n+        # The stride variables represent how much to increase the ptr by when moving by 1\n+        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n+        # by to get the element one row down (A has M rows).\n+        stride_am, stride_ak,\n+        stride_bk, stride_bn,\n+        stride_cm, stride_cn,\n+        # Meta-parameters\n+        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+        GROUP_SIZE_M: tl.constexpr\n+):\n+    \"\"\"Kernel for computing the matmul C = A x B.\n+    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n+    \"\"\"\n+    # -----------------------------------------------------------\n+    # Map program ids `pid` to the block of C it should compute.\n+    # This is done in a grouped ordering to promote L2 data reuse.\n+    # See the matrix multiplication tutorial for details.\n+    pid = tl.program_id(axis=0)\n+    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n+    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n+    group_id = pid // num_pid_in_group\n+    first_pid_m = group_id * GROUP_SIZE_M\n+    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n+    pid_m = first_pid_m + (pid % group_size_m)\n+    pid_n = (pid % num_pid_in_group) // group_size_m\n+\n+    # ----------------------------------------------------------\n+    # Create block pointers for the first blocks of A and B.\n+    # We will advance this pointer as we move in the K direction and accumulate.\n+    # See above `Make a Block Pointer` section for details.\n+    a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\n+                                    offsets=(pid_m * BLOCK_SIZE_M, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K),\n+                                    order=(1, 0))\n+    b_block_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(stride_bk, stride_bn),\n+                                    offsets=(0, pid_n * BLOCK_SIZE_N), block_shape=(BLOCK_SIZE_K, BLOCK_SIZE_N),\n+                                    order=(1, 0))\n+\n+    # -----------------------------------------------------------\n+    # Iterate to compute a block of the C matrix.\n+    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block.\n+    # of fp32 values for higher accuracy.\n+    # `accumulator` will be converted back to fp16 after the loop.\n+    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+    for k in range(0, K, BLOCK_SIZE_K):\n+        # Load with boundary checks, no need to calculate the mask manually.\n+        # For better performance, you may remove some axis from the boundary\n+        # check, if you can guarantee that the access is always in-bound in\n+        # that axis.\n+        # See above `Load/Store a Block Pointer` section for details.\n+        a = tl.load(a_block_ptr, boundary_check=(0, 1))\n+        b = tl.load(b_block_ptr, boundary_check=(0, 1))\n+        # We accumulate along the K dimension.\n+        accumulator += tl.dot(a, b)\n+        # Advance the block pointer to the next K block.\n+        # See above `Advance a Block Pointer` section for details.\n+        a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))\n+        b_block_ptr = tl.advance(b_block_ptr, (BLOCK_SIZE_K, 0))\n+    c = accumulator.to(tl.float16)\n+\n+    # ----------------------------------------------------------------\n+    # Write back the block of the output matrix C with boundary checks.\n+    # See above `Load/Store a Block Pointer` section for details.\n+    c_block_ptr = tl.make_block_ptr(base=c_ptr, shape=(M, N), strides=(stride_cm, stride_cn),\n+                                    offsets=(pid_m * BLOCK_SIZE_M, pid_n * BLOCK_SIZE_N),\n+                                    block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_N), order=(1, 0))\n+    tl.store(c_block_ptr, c, boundary_check=(0, 1))\n+\n+\n+# We can now create a convenience wrapper function that only takes two input tensors,\n+# and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\n+def matmul(a, b):\n+    # Check constraints.\n+    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n+    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n+    assert b.is_contiguous(), \"Matrix B must be contiguous\"\n+    M, K = a.shape\n+    K, N = b.shape\n+    # Allocates output.\n+    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n+    # 1D launch kernel where each block gets its own program.\n+    grid = lambda META: (\n+        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n+    )\n+    matmul_kernel_with_block_pointers[grid](\n+        a, b, c,\n+        M, N, K,\n+        a.stride(0), a.stride(1),\n+        b.stride(0), b.stride(1),\n+        c.stride(0), c.stride(1),\n+    )\n+    return c\n+\n+\n+# %%\n+# Unit Test\n+# ---------\n+#\n+# Still we can test our matrix multiplication with block pointers against a native torch implementation (i.e., cuBLAS).\n+\n+torch.manual_seed(0)\n+a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n+b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n+triton_output = matmul(a, b)\n+torch_output = torch.matmul(a, b)\n+print(f\"triton_output={triton_output}\")\n+print(f\"torch_output={torch_output}\")\n+if torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\n+    print(\"\u2705 Triton and Torch match\")\n+else:\n+    print(\"\u274c Triton and Torch differ\")"}, {"filename": "main/_downloads/62d97d49a32414049819dd8bb8378080/01-vector-add.py", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "file_content_changes": "@@ -5,9 +5,13 @@\n In this tutorial, you will write a simple vector addition using Triton.\n \n In doing so, you will learn about:\n-- The basic programming model of Triton.\n-- The `triton.jit` decorator, which is used to define Triton kernels.\n-- The best practices for validating and benchmarking your custom ops against native reference implementations.\n+\n+* The basic programming model of Triton.\n+\n+* The `triton.jit` decorator, which is used to define Triton kernels.\n+\n+* The best practices for validating and benchmarking your custom ops against native reference implementations.\n+\n \"\"\"\n \n # %%"}, {"filename": "main/_downloads/662999063954282841dc90b8945f85ce/tutorials_jupyter.zip", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/763344228ae6bc253ed1a6cf586aa30d/tutorials_python.zip", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/77571465f7f4bd281d3a847dc2633146/07-math-functions.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -1,6 +1,6 @@\n \"\"\"\n Libdevice (`tl.math`) function\n-===============\n+==============================\n Triton can invoke a custom function from an external library.\n In this example, we will use the `libdevice` library (a.k.a `math` in triton) to apply `asin` on a tensor.\n Please refer to https://docs.nvidia.com/cuda/libdevice-users-guide/index.html regarding the semantics of all available libdevice functions.\n@@ -12,7 +12,7 @@\n \n # %%\n #  asin Kernel\n-# --------------------------\n+# ------------\n \n import torch\n \n@@ -37,7 +37,7 @@ def asin_kernel(\n \n # %%\n #  Using the default libdevice library path\n-# --------------------------\n+# -----------------------------------------\n # We can use the default libdevice library path encoded in `triton/language/math.py`\n \n \n@@ -59,7 +59,7 @@ def asin_kernel(\n \n # %%\n #  Customize the libdevice library path\n-# --------------------------\n+# -------------------------------------\n # We can also customize the libdevice library path by passing the path to the `libdevice` library to the `asin` kernel.\n \n output_triton = torch.empty_like(x)"}, {"filename": "main/_downloads/935c0dd0fbeb4b2e69588471cbb2d4b2/05-layer-norm.py", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -5,8 +5,11 @@\n kernel that runs faster than the PyTorch implementation.\n \n In doing so, you will learn about:\n-- Implementing backward pass in Triton\n-- Implementing parallel reduction in Triton\n+\n+* Implementing backward pass in Triton.\n+\n+* Implementing parallel reduction in Triton.\n+\n \"\"\"\n \n # %%"}, {"filename": "main/_downloads/9705af6f33c6e66c6fa78f2394331536/08-experimental-block-pointer.ipynb", "status": "added", "additions": 107, "deletions": 0, "changes": 107, "file_content_changes": "@@ -0,0 +1,107 @@\n+{\n+  \"cells\": [\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"%matplotlib inline\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"\\n# Block Pointer (Experimental)\\nThis tutorial will guide you through writing a matrix multiplication algorithm that utilizes block pointer semantics.\\nThese semantics are more friendly for Triton to optimize and can result in better performance on specific hardware.\\nNote that this feature is still experimental and may change in the future.\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Motivations\\nIn the previous matrix multiplication tutorial, we constructed blocks of values by de-referencing blocks of pointers,\\ni.e., :code:`load(block<pointer_type<element_type>>) -> block<element_type>`, which involved loading blocks of\\nelements from memory. This approach allowed for flexibility in using hardware-managed cache and implementing complex\\ndata structures, such as tensors of trees or unstructured look-up tables.\\n\\nHowever, the drawback of this approach is that it relies heavily on complex optimization passes by the compiler to\\noptimize memory access patterns. This can result in brittle code that may suffer from performance degradation when the\\noptimizer fails to perform adequately. Additionally, as memory controllers specialize to accommodate dense spatial\\ndata structures commonly used in machine learning workloads, this problem is likely to worsen.\\n\\nTo address this issue, we will use block pointers :code:`pointer_type<block<element_type>>` and load them into\\n:code:`block<element_type>`, in which way gives better friendliness for the compiler to optimize memory access\\npatterns.\\n\\nLet's start with the previous matrix multiplication example and demonstrate how to rewrite it to utilize block pointer\\nsemantics.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Make a Block Pointer\\nA block pointer pointers to a block in a parent tensor and is constructed by :code:`make_block_ptr` function,\\nwhich takes the following information as arguments:\\n- :code:`base`: the base pointer to the parent tensor;\\n- :code:`shape`: the shape of the parent tensor;\\n- :code:`strides`: the strides of the parent tensor, which means how much to increase the pointer by when moving by 1 element in a specific axis;\\n- :code:`offsets`: the offsets of the block;\\n- :code:`block_shape`: the shape of the block;\\n- :code:`order`: the order of the block, which means how the block is laid out in memory.\\n\\nFor example, to a block pointer to a :code:`BLOCK_SIZE_M`x:code:`BLOCK_SIZE_K` block in a row-major 2D matrix A by\\noffsets :code:`(pid_m * BLOCK_SIZE_M, 0)` and strides :code:`(stride_am, stride_ak)`, we can use the following code\\n(exactly the same as the previous matrix multiplication tutorial):\\n\\n```python\\na_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\\n                                offsets=(pid_m * BLOCK_SIZE_M, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K),\\n                                order=(1, 0))\\n```\\nNote that the :code:`order` argument is set to :code:`(1, 0)`, which means the second axis is the inner dimension in\\nterms of storage, and the first axis is the outer dimension. This information may sound redundant, but it is necessary\\nfor some hardware backends to optimize for better performance.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Load/Store a Block Pointer\\nTo load/store a block pointer, we can use :code:`load/store` function, which takes a block pointer as an argument,\\nde-references it, and loads/stores a block. You may mask some values in the block, here we have an extra argument\\n:code:`boundary_check` to specify whether to check the boundary of each axis for the block pointer. With check on and\\nout-of-bound values will be masked according to the :code:`padding_option` argument (load only), which can be\\n:code:`zero` or :code:`nan`. Temporarily, we do not support other values due to some hardware limitations. In this\\nmode of block pointer load/store does not support :code:`mask` or :code:`other` arguments in the legacy mode.\\n\\nSo to load a block in A, we can simply write :code:`a = tl.load(a_block_ptr, boundary_check=(0, 1))`. Boundary check\\nmay cost extra performance, so if you can guarantee that the block pointer is always in-bound in some axis, you can\\nturn off the check by not passing the index into the :code:`boundary_check` argument. For example, if we know that\\n:code:`M` is a multiple of :code:`BLOCK_SIZE_M`, we can write :code:`a = tl.load(a_block_ptr, boundary_check=(1, ))`.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Advance a Block Pointer\\nTo advance a block pointer, we can use :code:`advance` function, which takes a block pointer and the increment for\\neach axis as arguments and returns a new block pointer with the same shape and strides as the original one,\\nbut with the offsets advanced by the specified amount.\\n\\nFor example, to advance the block pointer by :code:`BLOCK_SIZE_K` in the second axis\\n(no need to multiply with stride), we can write :code:`a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))`.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Final Result\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.autotune(\\n    configs=[\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\\n    ],\\n    key=['M', 'N', 'K'],\\n)\\n@triton.jit\\ndef matmul_kernel_with_block_pointers(\\n        # Pointers to matrices\\n        a_ptr, b_ptr, c_ptr,\\n        # Matrix dimensions\\n        M, N, K,\\n        # The stride variables represent how much to increase the ptr by when moving by 1\\n        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\\n        # by to get the element one row down (A has M rows).\\n        stride_am, stride_ak,\\n        stride_bk, stride_bn,\\n        stride_cm, stride_cn,\\n        # Meta-parameters\\n        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\\n        GROUP_SIZE_M: tl.constexpr\\n):\\n    \\\"\\\"\\\"Kernel for computing the matmul C = A x B.\\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\\n    \\\"\\\"\\\"\\n    # -----------------------------------------------------------\\n    # Map program ids `pid` to the block of C it should compute.\\n    # This is done in a grouped ordering to promote L2 data reuse.\\n    # See the matrix multiplication tutorial for details.\\n    pid = tl.program_id(axis=0)\\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\\n    group_id = pid // num_pid_in_group\\n    first_pid_m = group_id * GROUP_SIZE_M\\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\\n    pid_m = first_pid_m + (pid % group_size_m)\\n    pid_n = (pid % num_pid_in_group) // group_size_m\\n\\n    # ----------------------------------------------------------\\n    # Create block pointers for the first blocks of A and B.\\n    # We will advance this pointer as we move in the K direction and accumulate.\\n    # See above `Make a Block Pointer` section for details.\\n    a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\\n                                    offsets=(pid_m * BLOCK_SIZE_M, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K),\\n                                    order=(1, 0))\\n    b_block_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(stride_bk, stride_bn),\\n                                    offsets=(0, pid_n * BLOCK_SIZE_N), block_shape=(BLOCK_SIZE_K, BLOCK_SIZE_N),\\n                                    order=(1, 0))\\n\\n    # -----------------------------------------------------------\\n    # Iterate to compute a block of the C matrix.\\n    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block.\\n    # of fp32 values for higher accuracy.\\n    # `accumulator` will be converted back to fp16 after the loop.\\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\\n    for k in range(0, K, BLOCK_SIZE_K):\\n        # Load with boundary checks, no need to calculate the mask manually.\\n        # For better performance, you may remove some axis from the boundary\\n        # check, if you can guarantee that the access is always in-bound in\\n        # that axis.\\n        # See above `Load/Store a Block Pointer` section for details.\\n        a = tl.load(a_block_ptr, boundary_check=(0, 1))\\n        b = tl.load(b_block_ptr, boundary_check=(0, 1))\\n        # We accumulate along the K dimension.\\n        accumulator += tl.dot(a, b)\\n        # Advance the block pointer to the next K block.\\n        # See above `Advance a Block Pointer` section for details.\\n        a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))\\n        b_block_ptr = tl.advance(b_block_ptr, (BLOCK_SIZE_K, 0))\\n    c = accumulator.to(tl.float16)\\n\\n    # ----------------------------------------------------------------\\n    # Write back the block of the output matrix C with boundary checks.\\n    # See above `Load/Store a Block Pointer` section for details.\\n    c_block_ptr = tl.make_block_ptr(base=c_ptr, shape=(M, N), strides=(stride_cm, stride_cn),\\n                                    offsets=(pid_m * BLOCK_SIZE_M, pid_n * BLOCK_SIZE_N),\\n                                    block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_N), order=(1, 0))\\n    tl.store(c_block_ptr, c, boundary_check=(0, 1))\\n\\n\\n# We can now create a convenience wrapper function that only takes two input tensors,\\n# and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\\ndef matmul(a, b):\\n    # Check constraints.\\n    assert a.shape[1] == b.shape[0], \\\"Incompatible dimensions\\\"\\n    assert a.is_contiguous(), \\\"Matrix A must be contiguous\\\"\\n    assert b.is_contiguous(), \\\"Matrix B must be contiguous\\\"\\n    M, K = a.shape\\n    K, N = b.shape\\n    # Allocates output.\\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\\n    # 1D launch kernel where each block gets its own program.\\n    grid = lambda META: (\\n        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\\n    )\\n    matmul_kernel_with_block_pointers[grid](\\n        a, b, c,\\n        M, N, K,\\n        a.stride(0), a.stride(1),\\n        b.stride(0), b.stride(1),\\n        c.stride(0), c.stride(1),\\n    )\\n    return c\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Unit Test\\n\\nStill we can test our matrix multiplication with block pointers against a native torch implementation (i.e., cuBLAS).\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"torch.manual_seed(0)\\na = torch.randn((512, 512), device='cuda', dtype=torch.float16)\\nb = torch.randn((512, 512), device='cuda', dtype=torch.float16)\\ntriton_output = matmul(a, b)\\ntorch_output = torch.matmul(a, b)\\nprint(f\\\"triton_output={triton_output}\\\")\\nprint(f\\\"torch_output={torch_output}\\\")\\nif torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\\n    print(\\\"\\u2705 Triton and Torch match\\\")\\nelse:\\n    print(\\\"\\u274c Triton and Torch differ\\\")\"\n+      ]\n+    }\n+  ],\n+  \"metadata\": {\n+    \"kernelspec\": {\n+      \"display_name\": \"Python 3\",\n+      \"language\": \"python\",\n+      \"name\": \"python3\"\n+    },\n+    \"language_info\": {\n+      \"codemirror_mode\": {\n+        \"name\": \"ipython\",\n+        \"version\": 3\n+      },\n+      \"file_extension\": \".py\",\n+      \"mimetype\": \"text/x-python\",\n+      \"name\": \"python\",\n+      \"nbconvert_exporter\": \"python\",\n+      \"pygments_lexer\": \"ipython3\",\n+      \"version\": \"3.8.10\"\n+    }\n+  },\n+  \"nbformat\": 4,\n+  \"nbformat_minor\": 0\n+}\n\\ No newline at end of file"}, {"filename": "main/_downloads/ae7fff29e1b574187bc930ed94bcc353/05-layer-norm.ipynb", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -15,7 +15,7 @@\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"\\n# Layer Normalization\\nIn this tutorial, you will write a high-performance layer normalization\\nkernel that runs faster than the PyTorch implementation.\\n\\nIn doing so, you will learn about:\\n- Implementing backward pass in Triton\\n- Implementing parallel reduction in Triton\\n\"\n+        \"\\n# Layer Normalization\\nIn this tutorial, you will write a high-performance layer normalization\\nkernel that runs faster than the PyTorch implementation.\\n\\nIn doing so, you will learn about:\\n\\n* Implementing backward pass in Triton.\\n\\n* Implementing parallel reduction in Triton.\\n\"\n       ]\n     },\n     {"}, {"filename": "main/_downloads/b51b68bc1c6b1a5e509f67800b6235af/03-matrix-multiplication.ipynb", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -15,21 +15,21 @@\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"\\n# Matrix Multiplication\\nIn this tutorial, you will write a 25-lines high-performance FP16 matrix multiplication\\nkernel that achieves performance on par with cuBLAS.\\n\\nIn doing so, you will learn about:\\n- Block-level matrix multiplications\\n- Multi-dimensional pointer arithmetic\\n- Program re-ordering for improved L2 cache hit rate\\n- Automatic performance tuning\\n\"\n+        \"\\n# Matrix Multiplication\\nIn this tutorial, you will write a very short high-performance FP16 matrix multiplication kernel that achieves\\nperformance on parallel with cuBLAS.\\n\\nYou will specifically learn about:\\n\\n* Block-level matrix multiplications.\\n\\n* Multi-dimensional pointer arithmetics.\\n\\n* Program re-ordering for improved L2 cache hit rate.\\n\\n* Automatic performance tuning.\\n\"\n       ]\n     },\n     {\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"## Motivations\\n\\nMatrix multiplications are a key building block of most modern high-performance computing systems.\\nThey are notoriously hard to optimize, hence their implementation is generally done by\\nhardware vendors themselves as part of so-called \\\"kernel libraries\\\" (e.g., cuBLAS).\\nUnfortunately, these libraries are often proprietary and cannot be easily customized\\nto accommodate the needs of modern deep learning workloads (e.g., fused activation functions).\\nIn this tutorial, you will learn how to implement efficient matrix multiplications by\\nyourself with Triton, in a way that is easy to customize and extend.\\n\\nRoughly speaking, the kernel that we will write will implement the following blocked\\nalgorithm to multiply a (M, K) by a (K, N) matrix:\\n\\n```python\\n# do in parallel\\nfor m in range(0, M, BLOCK_SIZE_M):\\n  # do in parallel\\n  for n in range(0, N, BLOCK_SIZE_N):\\n    acc = zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=float32)\\n    for k in range(0, K, BLOCK_SIZE_K):\\n      a = A[m : m+BLOCK_SIZE_M, k : k+BLOCK_SIZE_K]\\n      b = B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]\\n      acc += dot(a, b)\\n    C[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N] = acc;\\n```\\nwhere each iteration of the doubly-nested for-loop is performed by a dedicated Triton program instance.\\n\\n\"\n+        \"## Motivations\\n\\nMatrix multiplications are a key building block of most modern high-performance computing systems.\\nThey are notoriously hard to optimize, hence their implementation is generally done by\\nhardware vendors themselves as part of so-called \\\"kernel libraries\\\" (e.g., cuBLAS).\\nUnfortunately, these libraries are often proprietary and cannot be easily customized\\nto accommodate the needs of modern deep learning workloads (e.g., fused activation functions).\\nIn this tutorial, you will learn how to implement efficient matrix multiplications by\\nyourself with Triton, in a way that is easy to customize and extend.\\n\\nRoughly speaking, the kernel that we will write will implement the following blocked\\nalgorithm to multiply a (M, K) by a (K, N) matrix:\\n\\n```python\\n# Do in parallel\\nfor m in range(0, M, BLOCK_SIZE_M):\\n  # Do in parallel\\n  for n in range(0, N, BLOCK_SIZE_N):\\n    acc = zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=float32)\\n    for k in range(0, K, BLOCK_SIZE_K):\\n      a = A[m : m+BLOCK_SIZE_M, k : k+BLOCK_SIZE_K]\\n      b = B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]\\n      acc += dot(a, b)\\n    C[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N] = acc\\n```\\nwhere each iteration of the doubly-nested for-loop is performed by a dedicated Triton program instance.\\n\\n\"\n       ]\n     },\n     {\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"## Compute Kernel\\n\\nThe above algorithm is, actually, fairly straightforward to implement in Triton.\\nThe main difficulty comes from the computation of the memory locations at which blocks\\nof :code:`A` and :code:`B` must be read in the inner loop. For that, we need\\nmulti-dimensional pointer arithmetic.\\n\\n\\n### Pointer Arithmetic\\n\\nFor a row-major 2D tensor :code:`X`, the memory location of :code:`X[i, j]` is given b\\ny :code:`&X[i, j] = X + i*stride_xi + j*stride_xj`.\\nTherefore, blocks of pointers for :code:`A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K]` and\\n:code:`B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]` can be defined in pseudo-code as:\\n\\n```python\\n&A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K] =  a_ptr + (m : m+BLOCK_SIZE_M)[:, None]*A.stride(0) + (k : k+BLOCK_SIZE_K)[None, :]*A.stride(1);\\n&B[k : k+BLOCK_SIZE_K, n:n+BLOCK_SIZE_N] =  b_ptr + (k : k+BLOCK_SIZE_K)[:, None]*B.stride(0) + (n : n+BLOCK_SIZE_N)[None, :]*B.stride(1);\\n```\\nWhich means that pointers for blocks of A and B can be initialized (i.e., :code:`k=0`) in Triton as:\\n\\n```python\\noffs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\\noffs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\\noffs_k = tl.arange(0, BLOCK_SIZE_K)\\na_ptrs = a_ptr + (offs_am[:, None]*stride_am + offs_k [None, :]*stride_ak)\\nb_ptrs = b_ptr + (offs_k [:, None]*stride_bk + offs_bn[None, :]*stride_bn)\\n```\\nAnd then updated in the inner loop as follows:\\n\\n```python\\na_ptrs += BLOCK_SIZE_K * stride_ak;\\nb_ptrs += BLOCK_SIZE_K * stride_bk;\\n```\\n### L2 Cache Optimizations\\n\\nAs mentioned above, each program instance computes a :code:`[BLOCK_SIZE_M, BLOCK_SIZE_N]`\\nblock of :code:`C`.\\nIt is important to remember that the order in which these blocks are computed does\\nmatter, since it affects the L2 cache hit rate of our program. and unfortunately, a\\na simple row-major ordering\\n\\n```Python\\npid = triton.program_id(0);\\ngrid_m = (M + BLOCK_SIZE_M - 1) // BLOCK_SIZE_M;\\ngrid_n = (N + BLOCK_SIZE_N - 1) // BLOCK_SIZE_N;\\npid_m = pid / grid_n;\\npid_n = pid % grid_n;\\n```\\nis just not going to cut it.\\n\\nOne possible solution is to launch blocks in an order that promotes data reuse.\\nThis can be done by 'super-grouping' blocks in groups of :code:`GROUP_M` rows before\\nswitching to the next column:\\n\\n```python\\n# program ID\\npid = tl.program_id(axis=0)\\n# number of program ids along the M axis\\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\\n# number of programs ids along the N axis\\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\\n# number of programs in group\\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\\n# id of the group this program is in\\ngroup_id = pid // num_pid_in_group\\n# row-id of the first program in the group\\nfirst_pid_m = group_id * GROUP_SIZE_M\\n# if `num_pid_m` isn't divisible by `GROUP_SIZE_M`, the last group is smaller\\ngroup_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\\n# *within groups*, programs are ordered in a column-major order\\n# row-id of the program in the *launch grid*\\npid_m = first_pid_m + (pid % group_size_m)\\n# col-id of the program in the *launch grid*\\npid_n = (pid % num_pid_in_group) // group_size_m\\n```\\nFor example, in the following matmul where each matrix is 9 blocks by 9 blocks,\\nwe can see that if we compute the output in row-major ordering, we need to load 90\\nblocks into SRAM to compute the first 9 output blocks, but if we do it in grouped\\nordering, we only need to load 54 blocks.\\n  .. image:: grouped_vs_row_major_ordering.png\\n\\nIn practice, this can improve the performance of our matrix multiplication kernel by\\nmore than 10\\\\% on some hardware architecture (e.g., 220 to 245 TFLOPS on A100).\\n\\n\\n\"\n+        \"## Compute Kernel\\n\\nThe above algorithm is, actually, fairly straightforward to implement in Triton.\\nThe main difficulty comes from the computation of the memory locations at which blocks\\nof :code:`A` and :code:`B` must be read in the inner loop. For that, we need\\nmulti-dimensional pointer arithmetics.\\n\\n### Pointer Arithmetics\\n\\nFor a row-major 2D tensor :code:`X`, the memory location of :code:`X[i, j]` is given b\\ny :code:`&X[i, j] = X + i*stride_xi + j*stride_xj`.\\nTherefore, blocks of pointers for :code:`A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K]` and\\n:code:`B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]` can be defined in pseudo-code as:\\n\\n```python\\n&A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K] =  a_ptr + (m : m+BLOCK_SIZE_M)[:, None]*A.stride(0) + (k : k+BLOCK_SIZE_K)[None, :]*A.stride(1);\\n&B[k : k+BLOCK_SIZE_K, n:n+BLOCK_SIZE_N] =  b_ptr + (k : k+BLOCK_SIZE_K)[:, None]*B.stride(0) + (n : n+BLOCK_SIZE_N)[None, :]*B.stride(1);\\n```\\nWhich means that pointers for blocks of A and B can be initialized (i.e., :code:`k=0`) in Triton as the following\\ncode. Also note that we need an extra modulo to handle the case where :code:`M` is not a multiple of\\n:code:`BLOCK_SIZE_M` or :code:`N` is not a multiple of :code:`BLOCK_SIZE_N`, in which case we can pad the data with\\nsome useless values, which will not contribute to the results. For the :code:`K` dimension, we will handle that later\\nusing masking load semantics.\\n\\n```python\\noffs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\\noffs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\\noffs_k = tl.arange(0, BLOCK_SIZE_K)\\na_ptrs = a_ptr + (offs_am[:, None]*stride_am + offs_k [None, :]*stride_ak)\\nb_ptrs = b_ptr + (offs_k [:, None]*stride_bk + offs_bn[None, :]*stride_bn)\\n```\\nAnd then updated in the inner loop as follows:\\n\\n```python\\na_ptrs += BLOCK_SIZE_K * stride_ak;\\nb_ptrs += BLOCK_SIZE_K * stride_bk;\\n```\\n### L2 Cache Optimizations\\n\\nAs mentioned above, each program instance computes a :code:`[BLOCK_SIZE_M, BLOCK_SIZE_N]`\\nblock of :code:`C`.\\nIt is important to remember that the order in which these blocks are computed does\\nmatter, since it affects the L2 cache hit rate of our program. and unfortunately, a\\na simple row-major ordering\\n\\n```Python\\npid = triton.program_id(0);\\ngrid_m = (M + BLOCK_SIZE_M - 1) // BLOCK_SIZE_M;\\ngrid_n = (N + BLOCK_SIZE_N - 1) // BLOCK_SIZE_N;\\npid_m = pid / grid_n;\\npid_n = pid % grid_n;\\n```\\nis just not going to cut it.\\n\\nOne possible solution is to launch blocks in an order that promotes data reuse.\\nThis can be done by 'super-grouping' blocks in groups of :code:`GROUP_M` rows before\\nswitching to the next column:\\n\\n```python\\n# Program ID\\npid = tl.program_id(axis=0)\\n# Number of program ids along the M axis\\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\\n# Number of programs ids along the N axis\\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\\n# Number of programs in group\\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\\n# Id of the group this program is in\\ngroup_id = pid // num_pid_in_group\\n# Row-id of the first program in the group\\nfirst_pid_m = group_id * GROUP_SIZE_M\\n# If `num_pid_m` isn't divisible by `GROUP_SIZE_M`, the last group is smaller\\ngroup_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\\n# *Within groups*, programs are ordered in a column-major order\\n# Row-id of the program in the *launch grid*\\npid_m = first_pid_m + (pid % group_size_m)\\n# Col-id of the program in the *launch grid*\\npid_n = (pid % num_pid_in_group) // group_size_m\\n```\\nFor example, in the following matmul where each matrix is 9 blocks by 9 blocks,\\nwe can see that if we compute the output in row-major ordering, we need to load 90\\nblocks into SRAM to compute the first 9 output blocks, but if we do it in grouped\\nordering, we only need to load 54 blocks.\\n\\n  .. image:: grouped_vs_row_major_ordering.png\\n\\nIn practice, this can improve the performance of our matrix multiplication kernel by\\nmore than 10\\\\% on some hardware architecture (e.g., 220 to 245 TFLOPS on A100).\\n\\n\\n\"\n       ]\n     },\n     {\n@@ -47,14 +47,14 @@\n       },\n       \"outputs\": [],\n       \"source\": [\n-        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n# %\\n# :code:`triton.jit`'ed functions can be auto-tuned by using the `triton.autotune`\\n# decorator, which consumes:\\n#   - A list of :code:`triton.Config` objects that define different configurations of\\n#       meta-parameters (e.g., BLOCK_SIZE_M) and compilation options (e.g., num_warps) to try\\n#   - An autotuning *key* whose change in values will trigger evaluation of all the\\n#       provided configs\\n\\n\\n@triton.autotune(\\n    configs=[\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\\n    ],\\n    key=['M', 'N', 'K'],\\n)\\n@triton.jit\\ndef matmul_kernel(\\n    # Pointers to matrices\\n    a_ptr, b_ptr, c_ptr,\\n    # Matrix dimensions\\n    M, N, K,\\n    # The stride variables represent how much to increase the ptr by when moving by 1\\n    # element in a particular dimension. E.g. stride_am is how much to increase a_ptr\\n    # by to get the element one row down (A has M rows)\\n    stride_am, stride_ak,\\n    stride_bk, stride_bn,\\n    stride_cm, stride_cn,\\n    # Meta-parameters\\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\\n    GROUP_SIZE_M: tl.constexpr,\\n    ACTIVATION: tl.constexpr,\\n):\\n    \\\"\\\"\\\"Kernel for computing the matmul C = A x B.\\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\\n    \\\"\\\"\\\"\\n    # -----------------------------------------------------------\\n    # Map program ids `pid` to the block of C it should compute.\\n    # This is done in a grouped ordering to promote L2 data reuse\\n    # See above `L2 Cache Optimizations` section for details\\n    pid = tl.program_id(axis=0)\\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\\n    group_id = pid // num_pid_in_group\\n    first_pid_m = group_id * GROUP_SIZE_M\\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\\n    pid_m = first_pid_m + (pid % group_size_m)\\n    pid_n = (pid % num_pid_in_group) // group_size_m\\n\\n    # ----------------------------------------------------------\\n    # Create pointers for the first blocks of A and B.\\n    # We will advance this pointer as we move in the K direction\\n    # and accumulate\\n    # a_ptrs is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\\n    # b_ptrs is a block of [BLOCK_SIZE_K, BLOCK_SIZE_n] pointers\\n    # see above `Pointer Arithmetic` section for details\\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\\n\\n    # -----------------------------------------------------------\\n    # Iterate to compute a block of the C matrix\\n    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\\n    # of fp32 values for higher accuracy.\\n    # `accumulator` will be converted back to fp16 after the loop\\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\\n    for k in range(0, num_pid_k):\\n        # Note that for simplicity, we don't apply a mask here.\\n        # This means that if K is not a multiple of BLOCK_SIZE_K,\\n        # this will access out-of-bounds memory and produce an\\n        # error or (worse!) incorrect results.\\n        a = tl.load(a_ptrs)\\n        b = tl.load(b_ptrs)\\n        # We accumulate along the K dimension\\n        accumulator += tl.dot(a, b)\\n        # Advance the ptrs to the next K block\\n        a_ptrs += BLOCK_SIZE_K * stride_ak\\n        b_ptrs += BLOCK_SIZE_K * stride_bk\\n    # you can fuse arbitrary activation functions here\\n    # while the accumulator is still in FP32!\\n    if ACTIVATION:\\n        accumulator = ACTIVATION(accumulator)\\n    c = accumulator.to(tl.float16)\\n\\n    # -----------------------------------------------------------\\n    # Write back the block of the output matrix C\\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\\n    tl.store(c_ptrs, c, mask=c_mask)\\n\\n\\n# we can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`\\n@triton.jit\\ndef leaky_relu(x):\\n    return tl.where(x >= 0, x, 0.01 * x)\"\n+        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n# `triton.jit`'ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:\\n#   - A list of `triton.Config` objects that define different configurations of\\n#       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try\\n#   - An auto-tuning *key* whose change in values will trigger evaluation of all the\\n#       provided configs\\n@triton.autotune(\\n    configs=[\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\\n    ],\\n    key=['M', 'N', 'K'],\\n)\\n@triton.jit\\ndef matmul_kernel(\\n    # Pointers to matrices\\n    a_ptr, b_ptr, c_ptr,\\n    # Matrix dimensions\\n    M, N, K,\\n    # The stride variables represent how much to increase the ptr by when moving by 1\\n    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\\n    # by to get the element one row down (A has M rows).\\n    stride_am, stride_ak,\\n    stride_bk, stride_bn,\\n    stride_cm, stride_cn,\\n    # Meta-parameters\\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\\n    GROUP_SIZE_M: tl.constexpr,\\n    ACTIVATION: tl.constexpr,\\n):\\n    \\\"\\\"\\\"Kernel for computing the matmul C = A x B.\\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\\n    \\\"\\\"\\\"\\n    # -----------------------------------------------------------\\n    # Map program ids `pid` to the block of C it should compute.\\n    # This is done in a grouped ordering to promote L2 data reuse.\\n    # See above `L2 Cache Optimizations` section for details.\\n    pid = tl.program_id(axis=0)\\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\\n    group_id = pid // num_pid_in_group\\n    first_pid_m = group_id * GROUP_SIZE_M\\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\\n    pid_m = first_pid_m + (pid % group_size_m)\\n    pid_n = (pid % num_pid_in_group) // group_size_m\\n\\n    # ----------------------------------------------------------\\n    # Create pointers for the first blocks of A and B.\\n    # We will advance this pointer as we move in the K direction\\n    # and accumulate\\n    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\\n    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\\n    # See above `Pointer Arithmetics` section for details\\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\\n\\n    # -----------------------------------------------------------\\n    # Iterate to compute a block of the C matrix.\\n    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\\n    # of fp32 values for higher accuracy.\\n    # `accumulator` will be converted back to fp16 after the loop.\\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\\n        # Load the next block of A and B, generate a mask by checking the K dimension.\\n        # If it is out of bounds, set it to 0.\\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\\n        # We accumulate along the K dimension.\\n        accumulator += tl.dot(a, b)\\n        # Advance the ptrs to the next K block.\\n        a_ptrs += BLOCK_SIZE_K * stride_ak\\n        b_ptrs += BLOCK_SIZE_K * stride_bk\\n    # You can fuse arbitrary activation functions here\\n    # while the accumulator is still in FP32!\\n    if ACTIVATION == \\\"leaky_relu\\\":\\n        accumulator = leaky_relu(accumulator)\\n    c = accumulator.to(tl.float16)\\n\\n    # -----------------------------------------------------------\\n    # Write back the block of the output matrix C with masks.\\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\\n    tl.store(c_ptrs, c, mask=c_mask)\\n\\n\\n# We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.\\n@triton.jit\\ndef leaky_relu(x):\\n    x = x + 1\\n    return tl.where(x >= 0, x, 0.01 * x)\"\n       ]\n     },\n     {\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"We can now create a convenience wrapper function that only takes two input tensors\\nand (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel\\n\\n\"\n+        \"We can now create a convenience wrapper function that only takes two input tensors,\\nand (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\\n\\n\"\n       ]\n     },\n     {\n@@ -65,14 +65,14 @@\n       },\n       \"outputs\": [],\n       \"source\": [\n-        \"def matmul(a, b, activation=None):\\n    # checks constraints\\n    assert a.shape[1] == b.shape[0], \\\"incompatible dimensions\\\"\\n    assert a.is_contiguous(), \\\"matrix A must be contiguous\\\"\\n    assert b.is_contiguous(), \\\"matrix B must be contiguous\\\"\\n    M, K = a.shape\\n    K, N = b.shape\\n    assert (\\n        K % 32 == 0\\n    ), \\\"We don't check memory-out-of-bounds with K so K must be divisible by BLOCK_SIZE_K\\\"\\n    # allocates output\\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\\n    # 1D launch kernel where each block gets its own program.\\n    grid = lambda META: (\\n        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\\n    )\\n    matmul_kernel[grid](\\n        a, b, c,\\n        M, N, K,\\n        a.stride(0), a.stride(1),\\n        b.stride(0), b.stride(1),\\n        c.stride(0), c.stride(1),\\n        ACTIVATION=activation,\\n    )\\n    return c\"\n+        \"def matmul(a, b, activation=\\\"\\\"):\\n    # Check constraints.\\n    assert a.shape[1] == b.shape[0], \\\"Incompatible dimensions\\\"\\n    assert a.is_contiguous(), \\\"Matrix A must be contiguous\\\"\\n    assert b.is_contiguous(), \\\"Matrix B must be contiguous\\\"\\n    M, K = a.shape\\n    K, N = b.shape\\n    # Allocates output.\\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\\n    # 1D launch kernel where each block gets its own program.\\n    grid = lambda META: (\\n        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\\n    )\\n    matmul_kernel[grid](\\n        a, b, c,\\n        M, N, K,\\n        a.stride(0), a.stride(1),\\n        b.stride(0), b.stride(1),\\n        c.stride(0), c.stride(1),\\n        ACTIVATION=activation\\n    )\\n    return c\"\n       ]\n     },\n     {\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"## Unit Test\\n\\nWe can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS)\\n\\n\"\n+        \"## Unit Test\\n\\nWe can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS).\\n\\n\"\n       ]\n     },\n     {\n@@ -83,14 +83,14 @@\n       },\n       \"outputs\": [],\n       \"source\": [\n-        \"torch.manual_seed(0)\\na = torch.randn((512, 512), device='cuda', dtype=torch.float16)\\nb = torch.randn((512, 512), device='cuda', dtype=torch.float16)\\ntriton_output = matmul(a, b, activation=None)\\ntorch_output = torch.matmul(a, b)\\nprint(f\\\"triton_output={triton_output}\\\")\\nprint(f\\\"torch_output={torch_output}\\\")\\nif torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\\n    print(\\\"\\u2705 Triton and Torch match\\\")\\nelse:\\n    print(\\\"\\u274c Triton and Torch differ\\\")\"\n+        \"torch.manual_seed(0)\\na = torch.randn((512, 512), device='cuda', dtype=torch.float16)\\nb = torch.randn((512, 512), device='cuda', dtype=torch.float16)\\ntriton_output = matmul(a, b)\\ntorch_output = torch.matmul(a, b)\\nprint(f\\\"triton_output={triton_output}\\\")\\nprint(f\\\"torch_output={torch_output}\\\")\\nif torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\\n    print(\\\"\\u2705 Triton and Torch match\\\")\\nelse:\\n    print(\\\"\\u274c Triton and Torch differ\\\")\"\n       ]\n     },\n     {\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"## Benchmark\\n\\n### Square Matrix Performance\\n\\nWe can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices, but feel free to arrange this script as you wish to benchmark any other matrix shape.\\n\\n\"\n+        \"## Benchmark\\n\\n### Square Matrix Performance\\n\\nWe can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices,\\nbut feel free to arrange this script as you wish to benchmark any other matrix shape.\\n\\n\"\n       ]\n     },\n     {\n@@ -101,7 +101,7 @@\n       },\n       \"outputs\": [],\n       \"source\": [\n-        \"@triton.testing.perf_report(\\n    triton.testing.Benchmark(\\n        x_names=['M', 'N', 'K'],  # argument names to use as an x-axis for the plot\\n        x_vals=[\\n            128 * i for i in range(2, 33)\\n        ],  # different possible values for `x_name`\\n        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\\n        # possible values for `line_arg``\\n        line_vals=['cublas', 'triton'],\\n        # label name for the lines\\n        line_names=[\\\"cuBLAS\\\", \\\"Triton\\\"],\\n        # line styles\\n        styles=[('green', '-'), ('blue', '-')],\\n        ylabel=\\\"TFLOPS\\\",  # label name for the y-axis\\n        plot_name=\\\"matmul-performance\\\",  # name for the plot. Used also as a file name for saving the plot.\\n        args={},\\n    )\\n)\\ndef benchmark(M, N, K, provider):\\n    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\\n    b = torch.randn((K, N), device='cuda', dtype=torch.float16)\\n    quantiles = [0.5, 0.2, 0.8]\\n    if provider == 'cublas':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\\n    if provider == 'triton':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\\n    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\\n    return perf(ms), perf(max_ms), perf(min_ms)\\n\\n\\nbenchmark.run(show_plots=True, print_data=True)\"\n+        \"@triton.testing.perf_report(\\n    triton.testing.Benchmark(\\n        x_names=['M', 'N', 'K'],  # Argument names to use as an x-axis for the plot\\n        x_vals=[\\n            128 * i for i in range(2, 33)\\n        ],  # Different possible values for `x_name`\\n        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot\\n        # Possible values for `line_arg`\\n        line_vals=['cublas', 'triton'],\\n        # Label name for the lines\\n        line_names=[\\\"cuBLAS\\\", \\\"Triton\\\"],\\n        # Line styles\\n        styles=[('green', '-'), ('blue', '-')],\\n        ylabel=\\\"TFLOPS\\\",  # Label name for the y-axis\\n        plot_name=\\\"matmul-performance\\\",  # Name for the plot, used also as a file name for saving the plot.\\n        args={},\\n    )\\n)\\ndef benchmark(M, N, K, provider):\\n    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\\n    b = torch.randn((K, N), device='cuda', dtype=torch.float16)\\n    quantiles = [0.5, 0.2, 0.8]\\n    if provider == 'cublas':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\\n    if provider == 'triton':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\\n    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\\n    return perf(ms), perf(max_ms), perf(min_ms)\\n\\n\\nbenchmark.run(show_plots=True, print_data=True)\"\n       ]\n     }\n   ],"}, {"filename": "main/_downloads/bc847dec325798bdc436c4ef5ac8b78a/04-low-memory-dropout.ipynb", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -15,7 +15,7 @@\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"\\n# Low-Memory Dropout\\n\\nIn this tutorial, you will write a memory-efficient implementation of dropout whose state\\nwill be composed of a single int32 seed. This differs from more traditional implementations of dropout,\\nwhose state is generally composed of a bit mask tensor of the same shape as the input.\\n\\nIn doing so, you will learn about:\\n- The limitations of naive implementations of Dropout with PyTorch\\n- Parallel pseudo-random number generation in Triton\\n\"\n+        \"\\n# Low-Memory Dropout\\n\\nIn this tutorial, you will write a memory-efficient implementation of dropout whose state\\nwill be composed of a single int32 seed. This differs from more traditional implementations of dropout,\\nwhose state is generally composed of a bit mask tensor of the same shape as the input.\\n\\nIn doing so, you will learn about:\\n\\n* The limitations of naive implementations of Dropout with PyTorch.\\n\\n* Parallel pseudo-random number generation in Triton.\\n\"\n       ]\n     },\n     {"}, {"filename": "main/_downloads/c9aed78977a4c05741d675a38dde3d7d/04-low-memory-dropout.py", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -7,8 +7,11 @@\n whose state is generally composed of a bit mask tensor of the same shape as the input.\n \n In doing so, you will learn about:\n-- The limitations of naive implementations of Dropout with PyTorch\n-- Parallel pseudo-random number generation in Triton\n+\n+* The limitations of naive implementations of Dropout with PyTorch.\n+\n+* Parallel pseudo-random number generation in Triton.\n+\n \"\"\"\n \n # %%"}, {"filename": "main/_downloads/d5fee5b55a64e47f1b5724ec39adf171/03-matrix-multiplication.py", "status": "modified", "additions": 86, "deletions": 84, "changes": 170, "file_content_changes": "@@ -1,14 +1,19 @@\n \"\"\"\n Matrix Multiplication\n =====================\n-In this tutorial, you will write a 25-lines high-performance FP16 matrix multiplication\n-kernel that achieves performance on par with cuBLAS.\n+In this tutorial, you will write a very short high-performance FP16 matrix multiplication kernel that achieves\n+performance on parallel with cuBLAS.\n+\n+You will specifically learn about:\n+\n+* Block-level matrix multiplications.\n+\n+* Multi-dimensional pointer arithmetics.\n+\n+* Program re-ordering for improved L2 cache hit rate.\n+\n+* Automatic performance tuning.\n \n-In doing so, you will learn about:\n-- Block-level matrix multiplications\n-- Multi-dimensional pointer arithmetic\n-- Program re-ordering for improved L2 cache hit rate\n-- Automatic performance tuning\n \"\"\"\n \n # %%\n@@ -28,16 +33,16 @@\n #\n #  .. code-block:: python\n #\n-#    # do in parallel\n+#    # Do in parallel\n #    for m in range(0, M, BLOCK_SIZE_M):\n-#      # do in parallel\n+#      # Do in parallel\n #      for n in range(0, N, BLOCK_SIZE_N):\n #        acc = zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=float32)\n #        for k in range(0, K, BLOCK_SIZE_K):\n #          a = A[m : m+BLOCK_SIZE_M, k : k+BLOCK_SIZE_K]\n #          b = B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]\n #          acc += dot(a, b)\n-#        C[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N] = acc;\n+#        C[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N] = acc\n #\n # where each iteration of the doubly-nested for-loop is performed by a dedicated Triton program instance.\n \n@@ -48,11 +53,10 @@\n # The above algorithm is, actually, fairly straightforward to implement in Triton.\n # The main difficulty comes from the computation of the memory locations at which blocks\n # of :code:`A` and :code:`B` must be read in the inner loop. For that, we need\n-# multi-dimensional pointer arithmetic.\n+# multi-dimensional pointer arithmetics.\n #\n-#\n-# Pointer Arithmetic\n-# ~~~~~~~~~~~~~~~~~~\n+# Pointer Arithmetics\n+# ~~~~~~~~~~~~~~~~~~~\n #\n # For a row-major 2D tensor :code:`X`, the memory location of :code:`X[i, j]` is given b\n # y :code:`&X[i, j] = X + i*stride_xi + j*stride_xj`.\n@@ -64,12 +68,16 @@\n #    &A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K] =  a_ptr + (m : m+BLOCK_SIZE_M)[:, None]*A.stride(0) + (k : k+BLOCK_SIZE_K)[None, :]*A.stride(1);\n #    &B[k : k+BLOCK_SIZE_K, n:n+BLOCK_SIZE_N] =  b_ptr + (k : k+BLOCK_SIZE_K)[:, None]*B.stride(0) + (n : n+BLOCK_SIZE_N)[None, :]*B.stride(1);\n #\n-# Which means that pointers for blocks of A and B can be initialized (i.e., :code:`k=0`) in Triton as:\n+# Which means that pointers for blocks of A and B can be initialized (i.e., :code:`k=0`) in Triton as the following\n+# code. Also note that we need an extra modulo to handle the case where :code:`M` is not a multiple of\n+# :code:`BLOCK_SIZE_M` or :code:`N` is not a multiple of :code:`BLOCK_SIZE_N`, in which case we can pad the data with\n+# some useless values, which will not contribute to the results. For the :code:`K` dimension, we will handle that later\n+# using masking load semantics.\n #\n #  .. code-block:: python\n #\n-#    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n-#    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+#    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n+#    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n #    offs_k = tl.arange(0, BLOCK_SIZE_K)\n #    a_ptrs = a_ptr + (offs_am[:, None]*stride_am + offs_k [None, :]*stride_ak)\n #    b_ptrs = b_ptr + (offs_k [:, None]*stride_bk + offs_bn[None, :]*stride_bn)\n@@ -107,30 +115,31 @@\n #\n #  .. code-block:: python\n #\n-#    # program ID\n+#    # Program ID\n #    pid = tl.program_id(axis=0)\n-#    # number of program ids along the M axis\n+#    # Number of program ids along the M axis\n #    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n-#    # number of programs ids along the N axis\n+#    # Number of programs ids along the N axis\n #    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n-#    # number of programs in group\n+#    # Number of programs in group\n #    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n-#    # id of the group this program is in\n+#    # Id of the group this program is in\n #    group_id = pid // num_pid_in_group\n-#    # row-id of the first program in the group\n+#    # Row-id of the first program in the group\n #    first_pid_m = group_id * GROUP_SIZE_M\n-#    # if `num_pid_m` isn't divisible by `GROUP_SIZE_M`, the last group is smaller\n+#    # If `num_pid_m` isn't divisible by `GROUP_SIZE_M`, the last group is smaller\n #    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n-#    # *within groups*, programs are ordered in a column-major order\n-#    # row-id of the program in the *launch grid*\n+#    # *Within groups*, programs are ordered in a column-major order\n+#    # Row-id of the program in the *launch grid*\n #    pid_m = first_pid_m + (pid % group_size_m)\n-#    # col-id of the program in the *launch grid*\n+#    # Col-id of the program in the *launch grid*\n #    pid_n = (pid % num_pid_in_group) // group_size_m\n #\n # For example, in the following matmul where each matrix is 9 blocks by 9 blocks,\n # we can see that if we compute the output in row-major ordering, we need to load 90\n # blocks into SRAM to compute the first 9 output blocks, but if we do it in grouped\n # ordering, we only need to load 54 blocks.\n+#\n #   .. image:: grouped_vs_row_major_ordering.png\n #\n # In practice, this can improve the performance of our matrix multiplication kernel by\n@@ -146,15 +155,12 @@\n import triton\n import triton.language as tl\n \n-# %\n-# :code:`triton.jit`'ed functions can be auto-tuned by using the `triton.autotune`\n-# decorator, which consumes:\n-#   - A list of :code:`triton.Config` objects that define different configurations of\n-#       meta-parameters (e.g., BLOCK_SIZE_M) and compilation options (e.g., num_warps) to try\n-#   - An autotuning *key* whose change in values will trigger evaluation of all the\n-#       provided configs\n-\n \n+# `triton.jit`'ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:\n+#   - A list of `triton.Config` objects that define different configurations of\n+#       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try\n+#   - An auto-tuning *key* whose change in values will trigger evaluation of all the\n+#       provided configs\n @triton.autotune(\n     configs=[\n         triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n@@ -175,8 +181,8 @@ def matmul_kernel(\n     # Matrix dimensions\n     M, N, K,\n     # The stride variables represent how much to increase the ptr by when moving by 1\n-    # element in a particular dimension. E.g. stride_am is how much to increase a_ptr\n-    # by to get the element one row down (A has M rows)\n+    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n+    # by to get the element one row down (A has M rows).\n     stride_am, stride_ak,\n     stride_bk, stride_bn,\n     stride_cm, stride_cn,\n@@ -190,12 +196,11 @@ def matmul_kernel(\n     \"\"\"\n     # -----------------------------------------------------------\n     # Map program ids `pid` to the block of C it should compute.\n-    # This is done in a grouped ordering to promote L2 data reuse\n-    # See above `L2 Cache Optimizations` section for details\n+    # This is done in a grouped ordering to promote L2 data reuse.\n+    # See above `L2 Cache Optimizations` section for details.\n     pid = tl.program_id(axis=0)\n     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n-    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n     num_pid_in_group = GROUP_SIZE_M * num_pid_n\n     group_id = pid // num_pid_in_group\n     first_pid_m = group_id * GROUP_SIZE_M\n@@ -207,70 +212,66 @@ def matmul_kernel(\n     # Create pointers for the first blocks of A and B.\n     # We will advance this pointer as we move in the K direction\n     # and accumulate\n-    # a_ptrs is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n-    # b_ptrs is a block of [BLOCK_SIZE_K, BLOCK_SIZE_n] pointers\n-    # see above `Pointer Arithmetic` section for details\n-    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n-    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n+    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n+    # See above `Pointer Arithmetics` section for details\n+    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n+    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n     offs_k = tl.arange(0, BLOCK_SIZE_K)\n     a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n     b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n \n     # -----------------------------------------------------------\n-    # Iterate to compute a block of the C matrix\n+    # Iterate to compute a block of the C matrix.\n     # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n     # of fp32 values for higher accuracy.\n-    # `accumulator` will be converted back to fp16 after the loop\n+    # `accumulator` will be converted back to fp16 after the loop.\n     accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-    for k in range(0, num_pid_k):\n-        # Note that for simplicity, we don't apply a mask here.\n-        # This means that if K is not a multiple of BLOCK_SIZE_K,\n-        # this will access out-of-bounds memory and produce an\n-        # error or (worse!) incorrect results.\n-        a = tl.load(a_ptrs)\n-        b = tl.load(b_ptrs)\n-        # We accumulate along the K dimension\n+    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n+        # Load the next block of A and B, generate a mask by checking the K dimension.\n+        # If it is out of bounds, set it to 0.\n+        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n+        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n+        # We accumulate along the K dimension.\n         accumulator += tl.dot(a, b)\n-        # Advance the ptrs to the next K block\n+        # Advance the ptrs to the next K block.\n         a_ptrs += BLOCK_SIZE_K * stride_ak\n         b_ptrs += BLOCK_SIZE_K * stride_bk\n-    # you can fuse arbitrary activation functions here\n+    # You can fuse arbitrary activation functions here\n     # while the accumulator is still in FP32!\n-    if ACTIVATION:\n-        accumulator = ACTIVATION(accumulator)\n+    if ACTIVATION == \"leaky_relu\":\n+        accumulator = leaky_relu(accumulator)\n     c = accumulator.to(tl.float16)\n \n     # -----------------------------------------------------------\n-    # Write back the block of the output matrix C\n+    # Write back the block of the output matrix C with masks.\n     offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n     offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n     c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n     c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n     tl.store(c_ptrs, c, mask=c_mask)\n \n \n-# we can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`\n+# We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.\n @triton.jit\n def leaky_relu(x):\n+    x = x + 1\n     return tl.where(x >= 0, x, 0.01 * x)\n \n \n # %%\n-# We can now create a convenience wrapper function that only takes two input tensors\n-# and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel\n+# We can now create a convenience wrapper function that only takes two input tensors,\n+# and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\n \n \n-def matmul(a, b, activation=None):\n-    # checks constraints\n-    assert a.shape[1] == b.shape[0], \"incompatible dimensions\"\n-    assert a.is_contiguous(), \"matrix A must be contiguous\"\n-    assert b.is_contiguous(), \"matrix B must be contiguous\"\n+def matmul(a, b, activation=\"\"):\n+    # Check constraints.\n+    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n+    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n+    assert b.is_contiguous(), \"Matrix B must be contiguous\"\n     M, K = a.shape\n     K, N = b.shape\n-    assert (\n-        K % 32 == 0\n-    ), \"We don't check memory-out-of-bounds with K so K must be divisible by BLOCK_SIZE_K\"\n-    # allocates output\n+    # Allocates output.\n     c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n     # 1D launch kernel where each block gets its own program.\n     grid = lambda META: (\n@@ -282,7 +283,7 @@ def matmul(a, b, activation=None):\n         a.stride(0), a.stride(1),\n         b.stride(0), b.stride(1),\n         c.stride(0), c.stride(1),\n-        ACTIVATION=activation,\n+        ACTIVATION=activation\n     )\n     return c\n \n@@ -291,12 +292,12 @@ def matmul(a, b, activation=None):\n # Unit Test\n # ---------\n #\n-# We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS)\n+# We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS).\n \n torch.manual_seed(0)\n a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n-triton_output = matmul(a, b, activation=None)\n+triton_output = matmul(a, b)\n torch_output = torch.matmul(a, b)\n print(f\"triton_output={triton_output}\")\n print(f\"torch_output={torch_output}\")\n@@ -312,24 +313,25 @@ def matmul(a, b, activation=None):\n # Square Matrix Performance\n # ~~~~~~~~~~~~~~~~~~~~~~~~~~\n #\n-# We can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices, but feel free to arrange this script as you wish to benchmark any other matrix shape.\n+# We can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices,\n+# but feel free to arrange this script as you wish to benchmark any other matrix shape.\n \n \n @triton.testing.perf_report(\n     triton.testing.Benchmark(\n-        x_names=['M', 'N', 'K'],  # argument names to use as an x-axis for the plot\n+        x_names=['M', 'N', 'K'],  # Argument names to use as an x-axis for the plot\n         x_vals=[\n             128 * i for i in range(2, 33)\n-        ],  # different possible values for `x_name`\n-        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n-        # possible values for `line_arg``\n+        ],  # Different possible values for `x_name`\n+        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot\n+        # Possible values for `line_arg`\n         line_vals=['cublas', 'triton'],\n-        # label name for the lines\n+        # Label name for the lines\n         line_names=[\"cuBLAS\", \"Triton\"],\n-        # line styles\n+        # Line styles\n         styles=[('green', '-'), ('blue', '-')],\n-        ylabel=\"TFLOPS\",  # label name for the y-axis\n-        plot_name=\"matmul-performance\",  # name for the plot. Used also as a file name for saving the plot.\n+        ylabel=\"TFLOPS\",  # Label name for the y-axis\n+        plot_name=\"matmul-performance\",  # Name for the plot, used also as a file name for saving the plot.\n         args={},\n     )\n )"}, {"filename": "main/_downloads/d91442ac2982c4e0cc3ab0f43534afbc/02-fused-softmax.py", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -7,8 +7,11 @@\n the GPU's SRAM.\n \n In doing so, you will learn about:\n-- The benefits of kernel fusion for bandwidth-bound operations.\n-- Reduction operators in Triton.\n+\n+* The benefits of kernel fusion for bandwidth-bound operations.\n+\n+* Reduction operators in Triton.\n+\n \"\"\"\n \n # %%"}, {"filename": "main/_downloads/f191ee1e78dc52eb5f7cba88f71cef2f/01-vector-add.ipynb", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -15,7 +15,7 @@\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"\\n# Vector Addition\\n\\nIn this tutorial, you will write a simple vector addition using Triton.\\n\\nIn doing so, you will learn about:\\n- The basic programming model of Triton.\\n- The `triton.jit` decorator, which is used to define Triton kernels.\\n- The best practices for validating and benchmarking your custom ops against native reference implementations.\\n\"\n+        \"\\n# Vector Addition\\n\\nIn this tutorial, you will write a simple vector addition using Triton.\\n\\nIn doing so, you will learn about:\\n\\n* The basic programming model of Triton.\\n\\n* The `triton.jit` decorator, which is used to define Triton kernels.\\n\\n* The best practices for validating and benchmarking your custom ops against native reference implementations.\\n\"\n       ]\n     },\n     {"}, {"filename": "main/_images/sphx_glr_01-vector-add_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_01-vector-add_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_02-fused-softmax_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_02-fused-softmax_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_03-matrix-multiplication_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_03-matrix-multiplication_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_05-layer-norm_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_05-layer-norm_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_002.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_08-experimental-block-pointer_thumb.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_sources/getting-started/tutorials/01-vector-add.rst.txt", "status": "modified", "additions": 20, "deletions": 17, "changes": 37, "file_content_changes": "@@ -24,16 +24,19 @@ Vector Addition\n In this tutorial, you will write a simple vector addition using Triton.\n \n In doing so, you will learn about:\n-- The basic programming model of Triton.\n-- The `triton.jit` decorator, which is used to define Triton kernels.\n-- The best practices for validating and benchmarking your custom ops against native reference implementations.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 14-16\n+* The basic programming model of Triton.\n+\n+* The `triton.jit` decorator, which is used to define Triton kernels.\n+\n+* The best practices for validating and benchmarking your custom ops against native reference implementations.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 18-20\n \n Compute Kernel\n --------------\n \n-.. GENERATED FROM PYTHON SOURCE LINES 16-52\n+.. GENERATED FROM PYTHON SOURCE LINES 20-56\n \n .. code-block:: default\n \n@@ -80,12 +83,12 @@ Compute Kernel\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 53-55\n+.. GENERATED FROM PYTHON SOURCE LINES 57-59\n \n Let's also declare a helper function to (1) allocate the `z` tensor\n and (2) enqueue the above kernel with appropriate grid/block sizes:\n \n-.. GENERATED FROM PYTHON SOURCE LINES 55-76\n+.. GENERATED FROM PYTHON SOURCE LINES 59-80\n \n .. code-block:: default\n \n@@ -117,11 +120,11 @@ and (2) enqueue the above kernel with appropriate grid/block sizes:\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 77-78\n+.. GENERATED FROM PYTHON SOURCE LINES 81-82\n \n We can now use the above function to compute the element-wise sum of two `torch.tensor` objects and test its correctness:\n \n-.. GENERATED FROM PYTHON SOURCE LINES 78-92\n+.. GENERATED FROM PYTHON SOURCE LINES 82-96\n \n .. code-block:: default\n \n@@ -154,11 +157,11 @@ We can now use the above function to compute the element-wise sum of two `torch.\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 93-94\n+.. GENERATED FROM PYTHON SOURCE LINES 97-98\n \n Seems like we're good to go!\n \n-.. GENERATED FROM PYTHON SOURCE LINES 96-102\n+.. GENERATED FROM PYTHON SOURCE LINES 100-106\n \n Benchmark\n ---------\n@@ -167,7 +170,7 @@ We can now benchmark our custom op on vectors of increasing sizes to get a sense\n To make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of our custom ops.\n for different problem sizes.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 102-132\n+.. GENERATED FROM PYTHON SOURCE LINES 106-136\n \n .. code-block:: default\n \n@@ -208,12 +211,12 @@ for different problem sizes.\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 133-135\n+.. GENERATED FROM PYTHON SOURCE LINES 137-139\n \n We can now run the decorated function above. Pass `print_data=True` to see the performance number, `show_plots=True` to plot them, and/or\n `save_path='/path/to/results/' to save them to disk along with raw CSV data:\n \n-.. GENERATED FROM PYTHON SOURCE LINES 135-136\n+.. GENERATED FROM PYTHON SOURCE LINES 139-140\n \n .. code-block:: default\n \n@@ -244,9 +247,9 @@ We can now run the decorated function above. Pass `print_data=True` to see the p\n     8     1048576.0   819.200021   819.200021\n     9     2097152.0  1023.999964  1023.999964\n     10    4194304.0  1260.307736  1228.800031\n-    11    8388608.0  1424.695621  1424.695621\n+    11    8388608.0  1424.695621  1404.342820\n     12   16777216.0  1560.380965  1560.380965\n-    13   33554432.0  1624.859540  1624.859540\n+    13   33554432.0  1631.601649  1624.859540\n     14   67108864.0  1669.706983  1662.646960\n     15  134217728.0  1684.008546  1678.616907\n \n@@ -256,7 +259,7 @@ We can now run the decorated function above. Pass `print_data=True` to see the p\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  24.188 seconds)\n+   **Total running time of the script:** ( 0 minutes  24.232 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_01-vector-add.py:"}, {"filename": "main/_sources/getting-started/tutorials/02-fused-softmax.rst.txt", "status": "modified", "additions": 27, "deletions": 25, "changes": 52, "file_content_changes": "@@ -26,18 +26,20 @@ than PyTorch's native op for a particular class of matrices: those whose rows ca\n the GPU's SRAM.\n \n In doing so, you will learn about:\n-- The benefits of kernel fusion for bandwidth-bound operations.\n-- Reduction operators in Triton.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 15-20\n+* The benefits of kernel fusion for bandwidth-bound operations.\n+\n+* Reduction operators in Triton.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 18-23\n \n Motivations\n -----------\n \n Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.\n Let us consider instead the case of a simple (numerically stabilized) softmax operation:\n \n-.. GENERATED FROM PYTHON SOURCE LINES 20-48\n+.. GENERATED FROM PYTHON SOURCE LINES 23-51\n \n .. code-block:: default\n \n@@ -76,7 +78,7 @@ Let us consider instead the case of a simple (numerically stabilized) softmax op\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 49-57\n+.. GENERATED FROM PYTHON SOURCE LINES 52-60\n \n When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \\in R^{M \\times N}`\n requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.\n@@ -87,7 +89,7 @@ expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).\n The `torch.jit.script` flags aims to perform this kind of \"kernel fusion\" automatically\n but, as we will see later, it is still far from ideal.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 59-68\n+.. GENERATED FROM PYTHON SOURCE LINES 62-71\n \n Compute Kernel\n --------------\n@@ -99,7 +101,7 @@ Note that one important limitation of Triton is that each block must have a\n power-of-two number of elements, so we need to internally \"pad\" each row and guard the\n memory operations properly if we want to handle any possible input shapes:\n \n-.. GENERATED FROM PYTHON SOURCE LINES 68-97\n+.. GENERATED FROM PYTHON SOURCE LINES 71-100\n \n .. code-block:: default\n \n@@ -139,11 +141,11 @@ memory operations properly if we want to handle any possible input shapes:\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 98-99\n+.. GENERATED FROM PYTHON SOURCE LINES 101-102\n \n We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 99-130\n+.. GENERATED FROM PYTHON SOURCE LINES 102-133\n \n .. code-block:: default\n \n@@ -185,17 +187,17 @@ We can create a helper function that enqueues the kernel and its (meta-)argument\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 131-133\n+.. GENERATED FROM PYTHON SOURCE LINES 134-136\n \n Unit Test\n ---------\n \n-.. GENERATED FROM PYTHON SOURCE LINES 135-137\n+.. GENERATED FROM PYTHON SOURCE LINES 138-140\n \n We make sure that we test our kernel on a matrix with an irregular number of rows and columns.\n This will allow us to verify that our padding mechanism works.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 137-144\n+.. GENERATED FROM PYTHON SOURCE LINES 140-147\n \n .. code-block:: default\n \n@@ -220,19 +222,19 @@ This will allow us to verify that our padding mechanism works.\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 145-146\n+.. GENERATED FROM PYTHON SOURCE LINES 148-149\n \n As expected, the results are identical.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 148-153\n+.. GENERATED FROM PYTHON SOURCE LINES 151-156\n \n Benchmark\n ---------\n \n Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.\n We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 153-193\n+.. GENERATED FROM PYTHON SOURCE LINES 156-196\n \n .. code-block:: default\n \n@@ -305,24 +307,24 @@ We will then compare its performance against (1) :code:`torch.softmax` and (2) t\n     [16384] 0\n     softmax-performance:\n               N       Triton  Torch (native)  Torch (jit)\n-    0     256.0   682.666643      682.666643   264.258068\n-    1     384.0   877.714274      819.200021   332.108094\n+    0     256.0   682.666643      744.727267   273.066674\n+    1     384.0   877.714274      877.714274   332.108094\n     2     512.0   910.222190      910.222190   372.363633\n-    3     640.0   975.238103      975.238103   401.568635\n+    3     640.0   975.238103      930.909084   409.600010\n     4     768.0  1068.521715     1023.999964   431.157886\n     ..      ...          ...             ...          ...\n-    93  12160.0  1594.754129     1069.010969   590.918747\n-    94  12288.0  1598.438956     1016.061996   590.414408\n-    95  12416.0  1576.634933     1031.979242   587.739623\n-    96  12544.0  1580.346374     1013.656595   590.305885\n-    97  12672.0  1584.000004     1006.213368   589.395349\n+    93  12160.0  1588.244879     1069.010969   590.022730\n+    94  12288.0  1591.967682     1018.694301   590.414408\n+    95  12416.0  1582.916395     1031.979242   586.871514\n+    96  12544.0  1580.346374     1013.656595   588.574769\n+    97  12672.0  1584.000004     1006.213368   588.539906\n \n     [98 rows x 4 columns]\n \n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 194-198\n+.. GENERATED FROM PYTHON SOURCE LINES 197-201\n \n In the above plot, we can see that:\n  - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.\n@@ -332,7 +334,7 @@ In the above plot, we can see that:\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 1 minutes  18.799 seconds)\n+   **Total running time of the script:** ( 1 minutes  17.617 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:"}, {"filename": "main/_sources/getting-started/tutorials/03-matrix-multiplication.rst.txt", "status": "modified", "additions": 116, "deletions": 115, "changes": 231, "file_content_changes": "@@ -20,16 +20,20 @@\n \n Matrix Multiplication\n =====================\n-In this tutorial, you will write a 25-lines high-performance FP16 matrix multiplication\n-kernel that achieves performance on par with cuBLAS.\n+In this tutorial, you will write a very short high-performance FP16 matrix multiplication kernel that achieves\n+performance on parallel with cuBLAS.\n \n-In doing so, you will learn about:\n-- Block-level matrix multiplications\n-- Multi-dimensional pointer arithmetic\n-- Program re-ordering for improved L2 cache hit rate\n-- Automatic performance tuning\n+You will specifically learn about:\n \n-.. GENERATED FROM PYTHON SOURCE LINES 15-43\n+* Block-level matrix multiplications.\n+\n+* Multi-dimensional pointer arithmetics.\n+\n+* Program re-ordering for improved L2 cache hit rate.\n+\n+* Automatic performance tuning.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 20-48\n \n Motivations\n -----------\n@@ -47,32 +51,31 @@ algorithm to multiply a (M, K) by a (K, N) matrix:\n \n  .. code-block:: python\n \n-   # do in parallel\n+   # Do in parallel\n    for m in range(0, M, BLOCK_SIZE_M):\n-     # do in parallel\n+     # Do in parallel\n      for n in range(0, N, BLOCK_SIZE_N):\n        acc = zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=float32)\n        for k in range(0, K, BLOCK_SIZE_K):\n          a = A[m : m+BLOCK_SIZE_M, k : k+BLOCK_SIZE_K]\n          b = B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]\n          acc += dot(a, b)\n-       C[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N] = acc;\n+       C[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N] = acc\n \n where each iteration of the doubly-nested for-loop is performed by a dedicated Triton program instance.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 45-139\n+.. GENERATED FROM PYTHON SOURCE LINES 50-148\n \n Compute Kernel\n --------------\n \n The above algorithm is, actually, fairly straightforward to implement in Triton.\n The main difficulty comes from the computation of the memory locations at which blocks\n of :code:`A` and :code:`B` must be read in the inner loop. For that, we need\n-multi-dimensional pointer arithmetic.\n-\n+multi-dimensional pointer arithmetics.\n \n-Pointer Arithmetic\n-~~~~~~~~~~~~~~~~~~\n+Pointer Arithmetics\n+~~~~~~~~~~~~~~~~~~~\n \n For a row-major 2D tensor :code:`X`, the memory location of :code:`X[i, j]` is given b\n y :code:`&X[i, j] = X + i*stride_xi + j*stride_xj`.\n@@ -84,12 +87,16 @@ Therefore, blocks of pointers for :code:`A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K]\n    &A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K] =  a_ptr + (m : m+BLOCK_SIZE_M)[:, None]*A.stride(0) + (k : k+BLOCK_SIZE_K)[None, :]*A.stride(1);\n    &B[k : k+BLOCK_SIZE_K, n:n+BLOCK_SIZE_N] =  b_ptr + (k : k+BLOCK_SIZE_K)[:, None]*B.stride(0) + (n : n+BLOCK_SIZE_N)[None, :]*B.stride(1);\n \n-Which means that pointers for blocks of A and B can be initialized (i.e., :code:`k=0`) in Triton as:\n+Which means that pointers for blocks of A and B can be initialized (i.e., :code:`k=0`) in Triton as the following\n+code. Also note that we need an extra modulo to handle the case where :code:`M` is not a multiple of\n+:code:`BLOCK_SIZE_M` or :code:`N` is not a multiple of :code:`BLOCK_SIZE_N`, in which case we can pad the data with\n+some useless values, which will not contribute to the results. For the :code:`K` dimension, we will handle that later\n+using masking load semantics.\n \n  .. code-block:: python\n \n-   offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n-   offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+   offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n+   offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None]*stride_am + offs_k [None, :]*stride_ak)\n    b_ptrs = b_ptr + (offs_k [:, None]*stride_bk + offs_bn[None, :]*stride_bn)\n@@ -127,42 +134,43 @@ switching to the next column:\n \n  .. code-block:: python\n \n-   # program ID\n+   # Program ID\n    pid = tl.program_id(axis=0)\n-   # number of program ids along the M axis\n+   # Number of program ids along the M axis\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n-   # number of programs ids along the N axis\n+   # Number of programs ids along the N axis\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n-   # number of programs in group\n+   # Number of programs in group\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n-   # id of the group this program is in\n+   # Id of the group this program is in\n    group_id = pid // num_pid_in_group\n-   # row-id of the first program in the group\n+   # Row-id of the first program in the group\n    first_pid_m = group_id * GROUP_SIZE_M\n-   # if `num_pid_m` isn't divisible by `GROUP_SIZE_M`, the last group is smaller\n+   # If `num_pid_m` isn't divisible by `GROUP_SIZE_M`, the last group is smaller\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n-   # *within groups*, programs are ordered in a column-major order\n-   # row-id of the program in the *launch grid*\n+   # *Within groups*, programs are ordered in a column-major order\n+   # Row-id of the program in the *launch grid*\n    pid_m = first_pid_m + (pid % group_size_m)\n-   # col-id of the program in the *launch grid*\n+   # Col-id of the program in the *launch grid*\n    pid_n = (pid % num_pid_in_group) // group_size_m\n \n For example, in the following matmul where each matrix is 9 blocks by 9 blocks,\n we can see that if we compute the output in row-major ordering, we need to load 90\n blocks into SRAM to compute the first 9 output blocks, but if we do it in grouped\n ordering, we only need to load 54 blocks.\n+\n   .. image:: grouped_vs_row_major_ordering.png\n \n In practice, this can improve the performance of our matrix multiplication kernel by\n more than 10\\% on some hardware architecture (e.g., 220 to 245 TFLOPS on A100).\n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 141-143\n+.. GENERATED FROM PYTHON SOURCE LINES 150-152\n \n Final Result\n ------------\n \n-.. GENERATED FROM PYTHON SOURCE LINES 143-258\n+.. GENERATED FROM PYTHON SOURCE LINES 152-262\n \n .. code-block:: default\n \n@@ -172,15 +180,12 @@ Final Result\n     import triton\n     import triton.language as tl\n \n-    # %\n-    # :code:`triton.jit`'ed functions can be auto-tuned by using the `triton.autotune`\n-    # decorator, which consumes:\n-    #   - A list of :code:`triton.Config` objects that define different configurations of\n-    #       meta-parameters (e.g., BLOCK_SIZE_M) and compilation options (e.g., num_warps) to try\n-    #   - An autotuning *key* whose change in values will trigger evaluation of all the\n-    #       provided configs\n-\n \n+    # `triton.jit`'ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:\n+    #   - A list of `triton.Config` objects that define different configurations of\n+    #       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try\n+    #   - An auto-tuning *key* whose change in values will trigger evaluation of all the\n+    #       provided configs\n     @triton.autotune(\n         configs=[\n             triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n@@ -201,8 +206,8 @@ Final Result\n         # Matrix dimensions\n         M, N, K,\n         # The stride variables represent how much to increase the ptr by when moving by 1\n-        # element in a particular dimension. E.g. stride_am is how much to increase a_ptr\n-        # by to get the element one row down (A has M rows)\n+        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n+        # by to get the element one row down (A has M rows).\n         stride_am, stride_ak,\n         stride_bk, stride_bn,\n         stride_cm, stride_cn,\n@@ -216,12 +221,11 @@ Final Result\n         \"\"\"\n         # -----------------------------------------------------------\n         # Map program ids `pid` to the block of C it should compute.\n-        # This is done in a grouped ordering to promote L2 data reuse\n-        # See above `L2 Cache Optimizations` section for details\n+        # This is done in a grouped ordering to promote L2 data reuse.\n+        # See above `L2 Cache Optimizations` section for details.\n         pid = tl.program_id(axis=0)\n         num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n         num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n-        num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n         num_pid_in_group = GROUP_SIZE_M * num_pid_n\n         group_id = pid // num_pid_in_group\n         first_pid_m = group_id * GROUP_SIZE_M\n@@ -233,51 +237,50 @@ Final Result\n         # Create pointers for the first blocks of A and B.\n         # We will advance this pointer as we move in the K direction\n         # and accumulate\n-        # a_ptrs is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n-        # b_ptrs is a block of [BLOCK_SIZE_K, BLOCK_SIZE_n] pointers\n-        # see above `Pointer Arithmetic` section for details\n-        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n-        offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+        # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n+        # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n+        # See above `Pointer Arithmetics` section for details\n+        offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n+        offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n         offs_k = tl.arange(0, BLOCK_SIZE_K)\n         a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n         b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n \n         # -----------------------------------------------------------\n-        # Iterate to compute a block of the C matrix\n+        # Iterate to compute a block of the C matrix.\n         # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n         # of fp32 values for higher accuracy.\n-        # `accumulator` will be converted back to fp16 after the loop\n+        # `accumulator` will be converted back to fp16 after the loop.\n         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-        for k in range(0, num_pid_k):\n-            # Note that for simplicity, we don't apply a mask here.\n-            # This means that if K is not a multiple of BLOCK_SIZE_K,\n-            # this will access out-of-bounds memory and produce an\n-            # error or (worse!) incorrect results.\n-            a = tl.load(a_ptrs)\n-            b = tl.load(b_ptrs)\n-            # We accumulate along the K dimension\n+        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n+            # Load the next block of A and B, generate a mask by checking the K dimension.\n+            # If it is out of bounds, set it to 0.\n+            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n+            b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n+            # We accumulate along the K dimension.\n             accumulator += tl.dot(a, b)\n-            # Advance the ptrs to the next K block\n+            # Advance the ptrs to the next K block.\n             a_ptrs += BLOCK_SIZE_K * stride_ak\n             b_ptrs += BLOCK_SIZE_K * stride_bk\n-        # you can fuse arbitrary activation functions here\n+        # You can fuse arbitrary activation functions here\n         # while the accumulator is still in FP32!\n-        if ACTIVATION:\n-            accumulator = ACTIVATION(accumulator)\n+        if ACTIVATION == \"leaky_relu\":\n+            accumulator = leaky_relu(accumulator)\n         c = accumulator.to(tl.float16)\n \n         # -----------------------------------------------------------\n-        # Write back the block of the output matrix C\n+        # Write back the block of the output matrix C with masks.\n         offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n         offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n         c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n         tl.store(c_ptrs, c, mask=c_mask)\n \n \n-    # we can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`\n+    # We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.\n     @triton.jit\n     def leaky_relu(x):\n+        x = x + 1\n         return tl.where(x >= 0, x, 0.01 * x)\n \n \n@@ -288,28 +291,25 @@ Final Result\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 259-261\n+.. GENERATED FROM PYTHON SOURCE LINES 263-265\n \n-We can now create a convenience wrapper function that only takes two input tensors\n-and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel\n+We can now create a convenience wrapper function that only takes two input tensors,\n+and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 261-290\n+.. GENERATED FROM PYTHON SOURCE LINES 265-291\n \n .. code-block:: default\n \n \n \n-    def matmul(a, b, activation=None):\n-        # checks constraints\n-        assert a.shape[1] == b.shape[0], \"incompatible dimensions\"\n-        assert a.is_contiguous(), \"matrix A must be contiguous\"\n-        assert b.is_contiguous(), \"matrix B must be contiguous\"\n+    def matmul(a, b, activation=\"\"):\n+        # Check constraints.\n+        assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n+        assert a.is_contiguous(), \"Matrix A must be contiguous\"\n+        assert b.is_contiguous(), \"Matrix B must be contiguous\"\n         M, K = a.shape\n         K, N = b.shape\n-        assert (\n-            K % 32 == 0\n-        ), \"We don't check memory-out-of-bounds with K so K must be divisible by BLOCK_SIZE_K\"\n-        # allocates output\n+        # Allocates output.\n         c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n         # 1D launch kernel where each block gets its own program.\n         grid = lambda META: (\n@@ -321,7 +321,7 @@ and (1) checks any shape constraint; (2) allocates the output; (3) launches the\n             a.stride(0), a.stride(1),\n             b.stride(0), b.stride(1),\n             c.stride(0), c.stride(1),\n-            ACTIVATION=activation,\n+            ACTIVATION=activation\n         )\n         return c\n \n@@ -333,22 +333,22 @@ and (1) checks any shape constraint; (2) allocates the output; (3) launches the\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 291-295\n+.. GENERATED FROM PYTHON SOURCE LINES 292-296\n \n Unit Test\n ---------\n \n-We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS)\n+We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS).\n \n-.. GENERATED FROM PYTHON SOURCE LINES 295-308\n+.. GENERATED FROM PYTHON SOURCE LINES 296-309\n \n .. code-block:: default\n \n \n     torch.manual_seed(0)\n     a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n     b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n-    triton_output = matmul(a, b, activation=None)\n+    triton_output = matmul(a, b)\n     torch_output = torch.matmul(a, b)\n     print(f\"triton_output={triton_output}\")\n     print(f\"torch_output={torch_output}\")\n@@ -386,37 +386,38 @@ We can test our custom matrix multiplication operation against a native torch im\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 309-316\n+.. GENERATED FROM PYTHON SOURCE LINES 310-318\n \n Benchmark\n ---------\n \n Square Matrix Performance\n ~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n-We can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices, but feel free to arrange this script as you wish to benchmark any other matrix shape.\n+We can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices,\n+but feel free to arrange this script as you wish to benchmark any other matrix shape.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 316-349\n+.. GENERATED FROM PYTHON SOURCE LINES 318-351\n \n .. code-block:: default\n \n \n \n     @triton.testing.perf_report(\n         triton.testing.Benchmark(\n-            x_names=['M', 'N', 'K'],  # argument names to use as an x-axis for the plot\n+            x_names=['M', 'N', 'K'],  # Argument names to use as an x-axis for the plot\n             x_vals=[\n                 128 * i for i in range(2, 33)\n-            ],  # different possible values for `x_name`\n-            line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n-            # possible values for `line_arg``\n+            ],  # Different possible values for `x_name`\n+            line_arg='provider',  # Argument name whose value corresponds to a different line in the plot\n+            # Possible values for `line_arg`\n             line_vals=['cublas', 'triton'],\n-            # label name for the lines\n+            # Label name for the lines\n             line_names=[\"cuBLAS\", \"Triton\"],\n-            # line styles\n+            # Line styles\n             styles=[('green', '-'), ('blue', '-')],\n-            ylabel=\"TFLOPS\",  # label name for the y-axis\n-            plot_name=\"matmul-performance\",  # name for the plot. Used also as a file name for saving the plot.\n+            ylabel=\"TFLOPS\",  # Label name for the y-axis\n+            plot_name=\"matmul-performance\",  # Name for the plot, used also as a file name for saving the plot.\n             args={},\n         )\n     )\n@@ -448,45 +449,45 @@ We can now compare the performance of our kernel against that of cuBLAS. Here we\n \n     matmul-performance:\n              M      cuBLAS      Triton\n-    0    256.0    4.681143    4.096000\n+    0    256.0    4.096000    4.096000\n     1    384.0   12.288000   12.288000\n     2    512.0   26.214401   23.831273\n     3    640.0   42.666665   39.384616\n     4    768.0   63.195428   58.982401\n     5    896.0   78.051553   82.642822\n-    6   1024.0  110.376426  104.857603\n+    6   1024.0  110.376426   99.864382\n     7   1152.0  135.726544  129.825388\n-    8   1280.0  163.840004  163.840004\n-    9   1408.0  151.438217  132.970149\n+    8   1280.0  157.538463  163.840004\n+    9   1408.0  155.765024  132.970149\n     10  1536.0  181.484314  157.286398\n-    11  1664.0  183.651271  179.978245\n+    11  1664.0  179.978245  179.978245\n     12  1792.0  172.914215  208.137481\n     13  1920.0  200.347822  168.585369\n     14  2048.0  197.379013  190.650180\n-    15  2176.0  188.071477  209.621326\n+    15  2176.0  189.845737  209.621326\n     16  2304.0  225.357284  227.503545\n-    17  2432.0  202.118452  200.674737\n-    18  2560.0  219.919464  217.006622\n-    19  2688.0  196.544332  197.567993\n-    20  2816.0  209.683695  210.696652\n-    21  2944.0  210.278616  225.502413\n-    22  3072.0  205.156169  207.410628\n-    23  3200.0  210.526325  216.949149\n-    24  3328.0  203.941342  206.278780\n-    25  3456.0  215.565692  216.724640\n-    26  3584.0  215.108588  206.702053\n-    27  3712.0  209.428397  217.168134\n-    28  3840.0  205.944129  208.664143\n-    29  3968.0  209.663117  218.680889\n-    30  4096.0  218.240199  214.405318\n+    17  2432.0  200.674737  200.674737\n+    18  2560.0  222.911566  217.006622\n+    19  2688.0  196.544332  196.544332\n+    20  2816.0  208.680416  208.680416\n+    21  2944.0  215.740400  222.482283\n+    22  3072.0  207.410628  207.410628\n+    23  3200.0  215.488222  217.687077\n+    24  3328.0  202.792385  205.689424\n+    25  3456.0  216.143621  217.896133\n+    26  3584.0  217.186932  205.756041\n+    27  3712.0  208.553950  214.371984\n+    28  3840.0  205.944129  206.328356\n+    29  3968.0  207.171367  213.702171\n+    30  4096.0  219.310012  214.405318\n \n \n \n \n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 1 minutes  36.046 seconds)\n+   **Total running time of the script:** ( 1 minutes  36.499 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_03-matrix-multiplication.py:"}, {"filename": "main/_sources/getting-started/tutorials/04-low-memory-dropout.rst.txt", "status": "modified", "additions": 12, "deletions": 10, "changes": 22, "file_content_changes": "@@ -26,10 +26,12 @@ will be composed of a single int32 seed. This differs from more traditional impl\n whose state is generally composed of a bit mask tensor of the same shape as the input.\n \n In doing so, you will learn about:\n-- The limitations of naive implementations of Dropout with PyTorch\n-- Parallel pseudo-random number generation in Triton\n \n-.. GENERATED FROM PYTHON SOURCE LINES 15-31\n+* The limitations of naive implementations of Dropout with PyTorch.\n+\n+* Parallel pseudo-random number generation in Triton.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 18-34\n \n Baseline\n --------\n@@ -48,7 +50,7 @@ keeps the norm consistent regardless of the dropout probability.\n \n Let's first take a look at the baseline implementation.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 31-84\n+.. GENERATED FROM PYTHON SOURCE LINES 34-87\n \n .. code-block:: default\n \n@@ -122,7 +124,7 @@ Let's first take a look at the baseline implementation.\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 85-104\n+.. GENERATED FROM PYTHON SOURCE LINES 88-107\n \n Seeded dropout\n --------------\n@@ -144,7 +146,7 @@ other :ref:`random number generation strategies <Random Number Generation>`.\n \n Let's put it all together.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 104-152\n+.. GENERATED FROM PYTHON SOURCE LINES 107-155\n \n .. code-block:: default\n \n@@ -214,13 +216,13 @@ Let's put it all together.\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 153-156\n+.. GENERATED FROM PYTHON SOURCE LINES 156-159\n \n Et Voil\u00e0! We have a triton kernel that applies the same dropout mask provided the seed is the same!\n If you'd like explore further applications of pseudorandomness in GPU programming, we encourage you\n to explore the `triton/language/random` folder!\n \n-.. GENERATED FROM PYTHON SOURCE LINES 158-164\n+.. GENERATED FROM PYTHON SOURCE LINES 161-167\n \n Exercises\n ---------\n@@ -229,7 +231,7 @@ Exercises\n 2. Add support for striding.\n 3. (challenge) Implement a kernel for sparse Johnson-Lindenstrauss transform which generates the projection matrix one the fly each time using a seed.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 166-171\n+.. GENERATED FROM PYTHON SOURCE LINES 169-174\n \n References\n ----------\n@@ -240,7 +242,7 @@ References\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  1.048 seconds)\n+   **Total running time of the script:** ( 0 minutes  0.707 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_04-low-memory-dropout.py:"}, {"filename": "main/_sources/getting-started/tutorials/05-layer-norm.rst.txt", "status": "modified", "additions": 37, "deletions": 35, "changes": 72, "file_content_changes": "@@ -24,10 +24,12 @@ In this tutorial, you will write a high-performance layer normalization\n kernel that runs faster than the PyTorch implementation.\n \n In doing so, you will learn about:\n-- Implementing backward pass in Triton\n-- Implementing parallel reduction in Triton\n \n-.. GENERATED FROM PYTHON SOURCE LINES 13-28\n+* Implementing backward pass in Triton.\n+\n+* Implementing parallel reduction in Triton.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 16-31\n \n Motivations\n -----------\n@@ -45,7 +47,7 @@ The forward pass can be expressed as follows:\n where :math:`\\epsilon` is a small constant added to the denominator for numerical stability.\n Let\u2019s first take a look at the forward pass implementation.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 28-93\n+.. GENERATED FROM PYTHON SOURCE LINES 31-96\n \n .. code-block:: default\n \n@@ -121,7 +123,7 @@ Let\u2019s first take a look at the forward pass implementation.\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 94-125\n+.. GENERATED FROM PYTHON SOURCE LINES 97-128\n \n Backward pass\n -------------\n@@ -155,7 +157,7 @@ In Stage 1, the rows of X that have the same color share the same buffer and thu\n In Stage 2, the buffers are further reduced to compute the final :math:`\\nabla_{w}` and :math:`\\nabla_{b}`.\n In the following implementation, Stage 1 is implemented by the function :code:`_layer_norm_bwd_dx_fused` and Stage 2 is implemented by the function :code:`_layer_norm_bwd_dwdb`.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 125-221\n+.. GENERATED FROM PYTHON SOURCE LINES 128-224\n \n .. code-block:: default\n \n@@ -262,7 +264,7 @@ In the following implementation, Stage 1 is implemented by the function :code:`_\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 222-228\n+.. GENERATED FROM PYTHON SOURCE LINES 225-231\n \n Benchmark\n ---------\n@@ -271,7 +273,7 @@ We can now compare the performance of our kernel against that of PyTorch.\n Here we focus on inputs that have Less than 64KB per feature.\n Specifically, one can set :code:`'mode': 'backward'` to benchmark the backward pass.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 228-367\n+.. GENERATED FROM PYTHON SOURCE LINES 231-370\n \n .. code-block:: default\n \n@@ -440,41 +442,41 @@ Specifically, one can set :code:`'mode': 'backward'` to benchmark the backward p\n     [16384] 0\n     layer-norm-backward:\n               N      Triton       Torch\n-    0    1024.0  240.941181  361.411758\n-    1    1536.0  347.773587  405.098894\n+    0    1024.0  240.941181  356.173905\n+    1    1536.0  361.411771  400.695643\n     2    2048.0  450.935778  438.857137\n-    3    2560.0  538.947358  472.615383\n-    4    3072.0  624.813540  501.551024\n+    3    2560.0  538.947358  469.007657\n+    4    3072.0  619.563043  501.551024\n     5    3584.0  688.127967  445.678757\n-    6    4096.0  750.412251  436.906674\n-    7    4608.0  708.923101  438.857146\n-    8    5120.0  763.229797  445.217381\n-    9    5632.0  809.389194  459.755106\n-    10   6144.0  847.448272  465.160886\n-    11   6656.0  882.563556  472.615367\n-    12   7168.0  919.957230  452.715775\n-    13   7680.0  950.103127  443.076928\n-    14   8192.0  983.040025  465.895721\n-    15   8704.0  663.161879  458.105254\n+    6    4096.0  744.727267  436.906674\n+    7    4608.0  708.923101  437.122520\n+    8    5120.0  763.229797  443.610086\n+    9    5632.0  804.571435  459.755106\n+    10   6144.0  842.605744  465.160886\n+    11   6656.0  882.563556  471.221251\n+    12   7168.0  915.063803  451.527570\n+    13   7680.0  950.103127  442.014385\n+    14   8192.0  978.149241  464.794337\n+    15   8704.0  663.161879  457.102857\n     16   9216.0  686.906817  466.632911\n-    17   9728.0  713.981680  471.184672\n-    18  10240.0  731.428577  467.224344\n-    19  10752.0  761.203560  466.632895\n+    17   9728.0  711.804890  470.709684\n+    18  10240.0  731.428577  466.337764\n+    19  10752.0  758.964709  466.632895\n     20  11264.0  781.317950  469.333317\n     21  11776.0  802.909085  471.826361\n-    22  12288.0  826.084006  471.105436\n-    23  12800.0  825.806430  471.889394\n-    24  13312.0  842.976243  477.560558\n-    25  13824.0  846.367375  478.408086\n-    26  14336.0  851.643583  476.542919\n-    27  14848.0  852.516725  477.044187\n-    28  15360.0  861.308412  476.279061\n-    29  15872.0  867.717548  478.552759\n+    22  12288.0  821.481899  471.105436\n+    23  12800.0  823.592520  471.889394\n+    24  13312.0  840.757868  477.560558\n+    25  13824.0  844.213752  478.753261\n+    26  14336.0  849.540743  476.542919\n+    27  14848.0  852.516725  476.406423\n+    28  15360.0  863.325544  476.279061\n+    29  15872.0  865.745448  479.154717\n \n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 368-372\n+.. GENERATED FROM PYTHON SOURCE LINES 371-375\n \n References\n ----------\n@@ -484,7 +486,7 @@ References\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  39.016 seconds)\n+   **Total running time of the script:** ( 0 minutes  38.028 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_05-layer-norm.py:"}, {"filename": "main/_sources/getting-started/tutorials/06-fused-attention.rst.txt", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "@@ -54,17 +54,17 @@ This is a Triton implementation of the Flash Attention algorithm\n     [128, 128] 1\n     fused-attention-batch4-head48-d64-fwd:\n         N_CTX     Triton\n-    0  1024.0   0.323668\n-    1  2048.0   1.094414\n-    2  4096.0   3.942359\n-    3  8192.0  15.056554\n+    0  1024.0   0.323017\n+    1  2048.0   1.094384\n+    2  4096.0   3.962921\n+    3  8192.0  14.962859\n     [128, 64] 1\n     fused-attention-batch4-head48-d64-bwd:\n         N_CTX     Triton\n-    0  1024.0   1.185261\n-    1  2048.0   3.758828\n-    2  4096.0  13.213110\n-    3  8192.0  49.180672\n+    0  1024.0   1.186626\n+    1  2048.0   3.760393\n+    2  4096.0  13.224523\n+    3  8192.0  49.601536\n \n \n \n@@ -433,7 +433,7 @@ This is a Triton implementation of the Flash Attention algorithm\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  5.598 seconds)\n+   **Total running time of the script:** ( 0 minutes  5.120 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_06-fused-attention.py:"}, {"filename": "main/_sources/getting-started/tutorials/07-math-functions.rst.txt", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -19,7 +19,7 @@\n \n \n Libdevice (`tl.math`) function\n-===============\n+==============================\n Triton can invoke a custom function from an external library.\n In this example, we will use the `libdevice` library (a.k.a `math` in triton) to apply `asin` on a tensor.\n Please refer to https://docs.nvidia.com/cuda/libdevice-users-guide/index.html regarding the semantics of all available libdevice functions.\n@@ -31,7 +31,7 @@ Triton automatically selects the correct underlying device function to invoke ba\n .. GENERATED FROM PYTHON SOURCE LINES 14-16\n \n asin Kernel\n---------------------------\n+------------\n \n .. GENERATED FROM PYTHON SOURCE LINES 16-38\n \n@@ -69,7 +69,7 @@ asin Kernel\n .. GENERATED FROM PYTHON SOURCE LINES 39-42\n \n Using the default libdevice library path\n---------------------------\n+-----------------------------------------\n We can use the default libdevice library path encoded in `triton/language/math.py`\n \n .. GENERATED FROM PYTHON SOURCE LINES 42-60\n@@ -112,7 +112,7 @@ We can use the default libdevice library path encoded in `triton/language/math.p\n .. GENERATED FROM PYTHON SOURCE LINES 61-64\n \n Customize the libdevice library path\n---------------------------\n+-------------------------------------\n We can also customize the libdevice library path by passing the path to the `libdevice` library to the `asin` kernel.\n \n .. GENERATED FROM PYTHON SOURCE LINES 64-74\n@@ -147,7 +147,7 @@ We can also customize the libdevice library path by passing the path to the `lib\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  0.380 seconds)\n+   **Total running time of the script:** ( 0 minutes  0.264 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_07-math-functions.py:"}, {"filename": "main/_sources/getting-started/tutorials/08-experimental-block-pointer.rst.txt", "status": "added", "additions": 318, "deletions": 0, "changes": 318, "file_content_changes": "@@ -0,0 +1,318 @@\n+\n+.. DO NOT EDIT.\n+.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.\n+.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:\n+.. \"getting-started/tutorials/08-experimental-block-pointer.py\"\n+.. LINE NUMBERS ARE GIVEN BELOW.\n+\n+.. only:: html\n+\n+    .. note::\n+        :class: sphx-glr-download-link-note\n+\n+        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_08-experimental-block-pointer.py>`\n+        to download the full example code\n+\n+.. rst-class:: sphx-glr-example-title\n+\n+.. _sphx_glr_getting-started_tutorials_08-experimental-block-pointer.py:\n+\n+\n+Block Pointer (Experimental)\n+============================\n+This tutorial will guide you through writing a matrix multiplication algorithm that utilizes block pointer semantics.\n+These semantics are more friendly for Triton to optimize and can result in better performance on specific hardware.\n+Note that this feature is still experimental and may change in the future.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 11-29\n+\n+Motivations\n+-----------\n+In the previous matrix multiplication tutorial, we constructed blocks of values by de-referencing blocks of pointers,\n+i.e., :code:`load(block<pointer_type<element_type>>) -> block<element_type>`, which involved loading blocks of\n+elements from memory. This approach allowed for flexibility in using hardware-managed cache and implementing complex\n+data structures, such as tensors of trees or unstructured look-up tables.\n+\n+However, the drawback of this approach is that it relies heavily on complex optimization passes by the compiler to\n+optimize memory access patterns. This can result in brittle code that may suffer from performance degradation when the\n+optimizer fails to perform adequately. Additionally, as memory controllers specialize to accommodate dense spatial\n+data structures commonly used in machine learning workloads, this problem is likely to worsen.\n+\n+To address this issue, we will use block pointers :code:`pointer_type<block<element_type>>` and load them into\n+:code:`block<element_type>`, in which way gives better friendliness for the compiler to optimize memory access\n+patterns.\n+\n+Let's start with the previous matrix multiplication example and demonstrate how to rewrite it to utilize block pointer\n+semantics.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 31-55\n+\n+Make a Block Pointer\n+--------------------\n+A block pointer pointers to a block in a parent tensor and is constructed by :code:`make_block_ptr` function,\n+which takes the following information as arguments:\n+- :code:`base`: the base pointer to the parent tensor;\n+- :code:`shape`: the shape of the parent tensor;\n+- :code:`strides`: the strides of the parent tensor, which means how much to increase the pointer by when moving by 1 element in a specific axis;\n+- :code:`offsets`: the offsets of the block;\n+- :code:`block_shape`: the shape of the block;\n+- :code:`order`: the order of the block, which means how the block is laid out in memory.\n+\n+For example, to a block pointer to a :code:`BLOCK_SIZE_M`x:code:`BLOCK_SIZE_K` block in a row-major 2D matrix A by\n+offsets :code:`(pid_m * BLOCK_SIZE_M, 0)` and strides :code:`(stride_am, stride_ak)`, we can use the following code\n+(exactly the same as the previous matrix multiplication tutorial):\n+\n+.. code-block:: python\n+\n+    a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\n+                                    offsets=(pid_m * BLOCK_SIZE_M, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K),\n+                                    order=(1, 0))\n+\n+Note that the :code:`order` argument is set to :code:`(1, 0)`, which means the second axis is the inner dimension in\n+terms of storage, and the first axis is the outer dimension. This information may sound redundant, but it is necessary\n+for some hardware backends to optimize for better performance.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 57-70\n+\n+Load/Store a Block Pointer\n+--------------------------\n+To load/store a block pointer, we can use :code:`load/store` function, which takes a block pointer as an argument,\n+de-references it, and loads/stores a block. You may mask some values in the block, here we have an extra argument\n+:code:`boundary_check` to specify whether to check the boundary of each axis for the block pointer. With check on and\n+out-of-bound values will be masked according to the :code:`padding_option` argument (load only), which can be\n+:code:`zero` or :code:`nan`. Temporarily, we do not support other values due to some hardware limitations. In this\n+mode of block pointer load/store does not support :code:`mask` or :code:`other` arguments in the legacy mode.\n+\n+So to load a block in A, we can simply write :code:`a = tl.load(a_block_ptr, boundary_check=(0, 1))`. Boundary check\n+may cost extra performance, so if you can guarantee that the block pointer is always in-bound in some axis, you can\n+turn off the check by not passing the index into the :code:`boundary_check` argument. For example, if we know that\n+:code:`M` is a multiple of :code:`BLOCK_SIZE_M`, we can write :code:`a = tl.load(a_block_ptr, boundary_check=(1, ))`.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 72-80\n+\n+Advance a Block Pointer\n+-----------------------\n+To advance a block pointer, we can use :code:`advance` function, which takes a block pointer and the increment for\n+each axis as arguments and returns a new block pointer with the same shape and strides as the original one,\n+but with the offsets advanced by the specified amount.\n+\n+For example, to advance the block pointer by :code:`BLOCK_SIZE_K` in the second axis\n+(no need to multiply with stride), we can write :code:`a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))`.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 82-84\n+\n+Final Result\n+------------\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 84-204\n+\n+.. code-block:: default\n+\n+\n+    import torch\n+\n+    import triton\n+    import triton.language as tl\n+\n+\n+    @triton.autotune(\n+        configs=[\n+            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n+            triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n+        ],\n+        key=['M', 'N', 'K'],\n+    )\n+    @triton.jit\n+    def matmul_kernel_with_block_pointers(\n+            # Pointers to matrices\n+            a_ptr, b_ptr, c_ptr,\n+            # Matrix dimensions\n+            M, N, K,\n+            # The stride variables represent how much to increase the ptr by when moving by 1\n+            # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n+            # by to get the element one row down (A has M rows).\n+            stride_am, stride_ak,\n+            stride_bk, stride_bn,\n+            stride_cm, stride_cn,\n+            # Meta-parameters\n+            BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+            GROUP_SIZE_M: tl.constexpr\n+    ):\n+        \"\"\"Kernel for computing the matmul C = A x B.\n+        A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n+        \"\"\"\n+        # -----------------------------------------------------------\n+        # Map program ids `pid` to the block of C it should compute.\n+        # This is done in a grouped ordering to promote L2 data reuse.\n+        # See the matrix multiplication tutorial for details.\n+        pid = tl.program_id(axis=0)\n+        num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n+        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+        num_pid_in_group = GROUP_SIZE_M * num_pid_n\n+        group_id = pid // num_pid_in_group\n+        first_pid_m = group_id * GROUP_SIZE_M\n+        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n+        pid_m = first_pid_m + (pid % group_size_m)\n+        pid_n = (pid % num_pid_in_group) // group_size_m\n+\n+        # ----------------------------------------------------------\n+        # Create block pointers for the first blocks of A and B.\n+        # We will advance this pointer as we move in the K direction and accumulate.\n+        # See above `Make a Block Pointer` section for details.\n+        a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\n+                                        offsets=(pid_m * BLOCK_SIZE_M, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K),\n+                                        order=(1, 0))\n+        b_block_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(stride_bk, stride_bn),\n+                                        offsets=(0, pid_n * BLOCK_SIZE_N), block_shape=(BLOCK_SIZE_K, BLOCK_SIZE_N),\n+                                        order=(1, 0))\n+\n+        # -----------------------------------------------------------\n+        # Iterate to compute a block of the C matrix.\n+        # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block.\n+        # of fp32 values for higher accuracy.\n+        # `accumulator` will be converted back to fp16 after the loop.\n+        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+        for k in range(0, K, BLOCK_SIZE_K):\n+            # Load with boundary checks, no need to calculate the mask manually.\n+            # For better performance, you may remove some axis from the boundary\n+            # check, if you can guarantee that the access is always in-bound in\n+            # that axis.\n+            # See above `Load/Store a Block Pointer` section for details.\n+            a = tl.load(a_block_ptr, boundary_check=(0, 1))\n+            b = tl.load(b_block_ptr, boundary_check=(0, 1))\n+            # We accumulate along the K dimension.\n+            accumulator += tl.dot(a, b)\n+            # Advance the block pointer to the next K block.\n+            # See above `Advance a Block Pointer` section for details.\n+            a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))\n+            b_block_ptr = tl.advance(b_block_ptr, (BLOCK_SIZE_K, 0))\n+        c = accumulator.to(tl.float16)\n+\n+        # ----------------------------------------------------------------\n+        # Write back the block of the output matrix C with boundary checks.\n+        # See above `Load/Store a Block Pointer` section for details.\n+        c_block_ptr = tl.make_block_ptr(base=c_ptr, shape=(M, N), strides=(stride_cm, stride_cn),\n+                                        offsets=(pid_m * BLOCK_SIZE_M, pid_n * BLOCK_SIZE_N),\n+                                        block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_N), order=(1, 0))\n+        tl.store(c_block_ptr, c, boundary_check=(0, 1))\n+\n+\n+    # We can now create a convenience wrapper function that only takes two input tensors,\n+    # and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\n+    def matmul(a, b):\n+        # Check constraints.\n+        assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n+        assert a.is_contiguous(), \"Matrix A must be contiguous\"\n+        assert b.is_contiguous(), \"Matrix B must be contiguous\"\n+        M, K = a.shape\n+        K, N = b.shape\n+        # Allocates output.\n+        c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n+        # 1D launch kernel where each block gets its own program.\n+        grid = lambda META: (\n+            triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n+        )\n+        matmul_kernel_with_block_pointers[grid](\n+            a, b, c,\n+            M, N, K,\n+            a.stride(0), a.stride(1),\n+            b.stride(0), b.stride(1),\n+            c.stride(0), c.stride(1),\n+        )\n+        return c\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 205-209\n+\n+Unit Test\n+---------\n+\n+Still we can test our matrix multiplication with block pointers against a native torch implementation (i.e., cuBLAS).\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 209-221\n+\n+.. code-block:: default\n+\n+\n+    torch.manual_seed(0)\n+    a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n+    b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n+    triton_output = matmul(a, b)\n+    torch_output = torch.matmul(a, b)\n+    print(f\"triton_output={triton_output}\")\n+    print(f\"torch_output={torch_output}\")\n+    if torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\n+        print(\"\u2705 Triton and Torch match\")\n+    else:\n+        print(\"\u274c Triton and Torch differ\")\n+\n+\n+\n+\n+.. rst-class:: sphx-glr-script-out\n+\n+ .. code-block:: none\n+\n+    triton_output=tensor([[-10.9531,  -4.7109,  15.6953,  ..., -28.4062,   4.3320, -26.4219],\n+            [ 26.8438,  10.0469,  -5.4297,  ..., -11.2969,  -8.5312,  30.7500],\n+            [-13.2578,  15.8516,  18.0781,  ..., -21.7656,  -8.6406,  10.2031],\n+            ...,\n+            [ 40.2812,  18.6094, -25.6094,  ...,  -2.7598,  -3.2441,  41.0000],\n+            [ -6.1211, -16.8281,   4.4844,  ..., -21.0312,  24.7031,  15.0234],\n+            [-17.0938, -19.0000,  -0.3831,  ...,  21.5469, -30.2344, -13.2188]],\n+           device='cuda:0', dtype=torch.float16)\n+    torch_output=tensor([[-10.9531,  -4.7109,  15.6953,  ..., -28.4062,   4.3320, -26.4219],\n+            [ 26.8438,  10.0469,  -5.4297,  ..., -11.2969,  -8.5312,  30.7500],\n+            [-13.2578,  15.8516,  18.0781,  ..., -21.7656,  -8.6406,  10.2031],\n+            ...,\n+            [ 40.2812,  18.6094, -25.6094,  ...,  -2.7598,  -3.2441,  41.0000],\n+            [ -6.1211, -16.8281,   4.4844,  ..., -21.0312,  24.7031,  15.0234],\n+            [-17.0938, -19.0000,  -0.3831,  ...,  21.5469, -30.2344, -13.2188]],\n+           device='cuda:0', dtype=torch.float16)\n+    \u2705 Triton and Torch match\n+\n+\n+\n+\n+\n+.. rst-class:: sphx-glr-timing\n+\n+   **Total running time of the script:** ( 0 minutes  9.969 seconds)\n+\n+\n+.. _sphx_glr_download_getting-started_tutorials_08-experimental-block-pointer.py:\n+\n+.. only:: html\n+\n+  .. container:: sphx-glr-footer sphx-glr-footer-example\n+\n+\n+\n+\n+    .. container:: sphx-glr-download sphx-glr-download-python\n+\n+      :download:`Download Python source code: 08-experimental-block-pointer.py <08-experimental-block-pointer.py>`\n+\n+    .. container:: sphx-glr-download sphx-glr-download-jupyter\n+\n+      :download:`Download Jupyter notebook: 08-experimental-block-pointer.ipynb <08-experimental-block-pointer.ipynb>`\n+\n+\n+.. only:: html\n+\n+ .. rst-class:: sphx-glr-signature\n+\n+    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"}, {"filename": "main/_sources/getting-started/tutorials/index.rst.txt", "status": "modified", "additions": 20, "deletions": 2, "changes": 22, "file_content_changes": "@@ -55,7 +55,7 @@ To install the dependencies for the tutorials:\n \n .. raw:: html\n \n-    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"In doing so, you will learn about: - Block-level matrix multiplications - Multi-dimensional poi...\">\n+    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"You will specifically learn about:\">\n \n .. only:: html\n \n@@ -89,7 +89,7 @@ To install the dependencies for the tutorials:\n \n .. raw:: html\n \n-    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"In doing so, you will learn about: - Implementing backward pass in Triton - Implementing parall...\">\n+    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"In doing so, you will learn about:\">\n \n .. only:: html\n \n@@ -138,6 +138,23 @@ To install the dependencies for the tutorials:\n     </div>\n \n \n+.. raw:: html\n+\n+    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"Block Pointer (Experimental)\">\n+\n+.. only:: html\n+\n+  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_08-experimental-block-pointer_thumb.png\n+    :alt:\n+\n+  :ref:`sphx_glr_getting-started_tutorials_08-experimental-block-pointer.py`\n+\n+.. raw:: html\n+\n+      <div class=\"sphx-glr-thumbnail-title\">Block Pointer (Experimental)</div>\n+    </div>\n+\n+\n .. raw:: html\n \n     </div>\n@@ -153,6 +170,7 @@ To install the dependencies for the tutorials:\n    /getting-started/tutorials/05-layer-norm\n    /getting-started/tutorials/06-fused-attention\n    /getting-started/tutorials/07-math-functions\n+   /getting-started/tutorials/08-experimental-block-pointer\n \n \n .. only:: html"}, {"filename": "main/_sources/getting-started/tutorials/sg_execution_times.rst.txt", "status": "modified", "additions": 18, "deletions": 16, "changes": 34, "file_content_changes": "@@ -6,20 +6,22 @@\n \n Computation times\n =================\n-**04:05.074** total execution time for **getting-started_tutorials** files:\n+**04:12.435** total execution time for **getting-started_tutorials** files:\n \n-+---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``) | 01:36.046 | 0.0 MB |\n-+---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                 | 01:18.799 | 0.0 MB |\n-+---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                       | 00:39.016 | 0.0 MB |\n-+---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                       | 00:24.188 | 0.0 MB |\n-+---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py` (``06-fused-attention.py``)             | 00:05.598 | 0.0 MB |\n-+---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)       | 00:01.048 | 0.0 MB |\n-+---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_07-math-functions.py` (``07-math-functions.py``)               | 00:00.380 | 0.0 MB |\n-+---------------------------------------------------------------------------------------------------------+-----------+--------+\n++-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n+| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``)           | 01:36.499 | 0.0 MB |\n++-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n+| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                           | 01:17.617 | 0.0 MB |\n++-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n+| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                                 | 00:38.028 | 0.0 MB |\n++-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n+| :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                                 | 00:24.232 | 0.0 MB |\n++-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n+| :ref:`sphx_glr_getting-started_tutorials_08-experimental-block-pointer.py` (``08-experimental-block-pointer.py``) | 00:09.969 | 0.0 MB |\n++-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n+| :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py` (``06-fused-attention.py``)                       | 00:05.120 | 0.0 MB |\n++-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n+| :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)                 | 00:00.707 | 0.0 MB |\n++-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n+| :ref:`sphx_glr_getting-started_tutorials_07-math-functions.py` (``07-math-functions.py``)                         | 00:00.264 | 0.0 MB |\n++-------------------------------------------------------------------------------------------------------------------+-----------+--------+"}, {"filename": "main/_sources/python-api/generated/triton.language.argmax.rst.txt", "status": "added", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -0,0 +1,6 @@\n+\ufefftriton.language.argmax\n+======================\n+\n+.. currentmodule:: triton.language\n+\n+.. autofunction:: argmax\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.argmin.rst.txt", "status": "added", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -0,0 +1,6 @@\n+\ufefftriton.language.argmin\n+======================\n+\n+.. currentmodule:: triton.language\n+\n+.. autofunction:: argmin\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.reduce.rst.txt", "status": "added", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -0,0 +1,6 @@\n+\ufefftriton.language.reduce\n+======================\n+\n+.. currentmodule:: triton.language\n+\n+.. autofunction:: reduce\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.xor_sum.rst.txt", "status": "added", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -0,0 +1,6 @@\n+\ufefftriton.language.xor\\_sum\n+========================\n+\n+.. currentmodule:: triton.language\n+\n+.. autofunction:: xor_sum\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/triton.language.rst.txt", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -96,9 +96,13 @@ Reduction Ops\n     :toctree: generated\n     :nosignatures:\n \n+    argmax\n+    argmin\n     max\n     min\n+    reduce\n     sum\n+    xor_sum\n \n \n Atomic Ops"}, {"filename": "main/genindex.html", "status": "modified", "additions": 17, "deletions": 2, "changes": 19, "file_content_changes": "@@ -101,6 +101,7 @@ <h1 id=\"index\">Index</h1>\n  | <a href=\"#R\"><strong>R</strong></a>\n  | <a href=\"#S\"><strong>S</strong></a>\n  | <a href=\"#W\"><strong>W</strong></a>\n+ | <a href=\"#X\"><strong>X</strong></a>\n  | <a href=\"#Z\"><strong>Z</strong></a>\n  \n </div>\n@@ -123,12 +124,16 @@ <h2 id=\"A\">A</h2>\n </li>\n       <li><a href=\"python-api/generated/triton.language.arange.html#triton.language.arange\">arange() (in module triton.language)</a>\n </li>\n-      <li><a href=\"python-api/generated/triton.language.atomic_add.html#triton.language.atomic_add\">atomic_add() (in module triton.language)</a>\n+      <li><a href=\"python-api/generated/triton.language.argmax.html#triton.language.argmax\">argmax() (in module triton.language)</a>\n </li>\n-      <li><a href=\"python-api/generated/triton.language.atomic_cas.html#triton.language.atomic_cas\">atomic_cas() (in module triton.language)</a>\n+      <li><a href=\"python-api/generated/triton.language.argmin.html#triton.language.argmin\">argmin() (in module triton.language)</a>\n+</li>\n+      <li><a href=\"python-api/generated/triton.language.atomic_add.html#triton.language.atomic_add\">atomic_add() (in module triton.language)</a>\n </li>\n   </ul></td>\n   <td style=\"width: 33%; vertical-align: top;\"><ul>\n+      <li><a href=\"python-api/generated/triton.language.atomic_cas.html#triton.language.atomic_cas\">atomic_cas() (in module triton.language)</a>\n+</li>\n       <li><a href=\"python-api/generated/triton.language.atomic_max.html#triton.language.atomic_max\">atomic_max() (in module triton.language)</a>\n </li>\n       <li><a href=\"python-api/generated/triton.language.atomic_min.html#triton.language.atomic_min\">atomic_min() (in module triton.language)</a>\n@@ -264,6 +269,8 @@ <h2 id=\"R\">R</h2>\n       <li><a href=\"python-api/generated/triton.language.randn.html#triton.language.randn\">randn() (in module triton.language)</a>\n </li>\n       <li><a href=\"python-api/generated/triton.language.ravel.html#triton.language.ravel\">ravel() (in module triton.language)</a>\n+</li>\n+      <li><a href=\"python-api/generated/triton.language.reduce.html#triton.language.reduce\">reduce() (in module triton.language)</a>\n </li>\n       <li><a href=\"python-api/generated/triton.language.reshape.html#triton.language.reshape\">reshape() (in module triton.language)</a>\n </li>\n@@ -298,6 +305,14 @@ <h2 id=\"W\">W</h2>\n   </ul></td>\n </tr></table>\n \n+<h2 id=\"X\">X</h2>\n+<table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n+  <td style=\"width: 33%; vertical-align: top;\"><ul>\n+      <li><a href=\"python-api/generated/triton.language.xor_sum.html#triton.language.xor_sum\">xor_sum() (in module triton.language)</a>\n+</li>\n+  </ul></td>\n+</tr></table>\n+\n <h2 id=\"Z\">Z</h2>\n <table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n   <td style=\"width: 33%; vertical-align: top;\"><ul>"}, {"filename": "main/getting-started/tutorials/01-vector-add.html", "status": "modified", "additions": 10, "deletions": 7, "changes": 17, "file_content_changes": "@@ -60,6 +60,7 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"08-experimental-block-pointer.html\">Block Pointer (Experimental)</a></li>\n </ul>\n </li>\n </ul>\n@@ -108,10 +109,12 @@\n <section class=\"sphx-glr-example-title\" id=\"vector-addition\">\n <span id=\"sphx-glr-getting-started-tutorials-01-vector-add-py\"></span><h1>Vector Addition<a class=\"headerlink\" href=\"#vector-addition\" title=\"Permalink to this heading\">\u00b6</a></h1>\n <p>In this tutorial, you will write a simple vector addition using Triton.</p>\n-<p>In doing so, you will learn about:\n-- The basic programming model of Triton.\n-- The <cite>triton.jit</cite> decorator, which is used to define Triton kernels.\n-- The best practices for validating and benchmarking your custom ops against native reference implementations.</p>\n+<p>In doing so, you will learn about:</p>\n+<ul class=\"simple\">\n+<li><p>The basic programming model of Triton.</p></li>\n+<li><p>The <cite>triton.jit</cite> decorator, which is used to define Triton kernels.</p></li>\n+<li><p>The best practices for validating and benchmarking your custom ops against native reference implementations.</p></li>\n+</ul>\n <section id=\"compute-kernel\">\n <h2>Compute Kernel<a class=\"headerlink\" href=\"#compute-kernel\" title=\"Permalink to this heading\">\u00b6</a></h2>\n <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n@@ -243,14 +246,14 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n 8     1048576.0   819.200021   819.200021\n 9     2097152.0  1023.999964  1023.999964\n 10    4194304.0  1260.307736  1228.800031\n-11    8388608.0  1424.695621  1424.695621\n+11    8388608.0  1424.695621  1404.342820\n 12   16777216.0  1560.380965  1560.380965\n-13   33554432.0  1624.859540  1624.859540\n+13   33554432.0  1631.601649  1624.859540\n 14   67108864.0  1669.706983  1662.646960\n 15  134217728.0  1684.008546  1678.616907\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  24.188 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  24.232 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-01-vector-add-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/62d97d49a32414049819dd8bb8378080/01-vector-add.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">01-vector-add.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/02-fused-softmax.html", "status": "modified", "additions": 15, "deletions": 12, "changes": 27, "file_content_changes": "@@ -63,6 +63,7 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"08-experimental-block-pointer.html\">Block Pointer (Experimental)</a></li>\n </ul>\n </li>\n </ul>\n@@ -113,9 +114,11 @@\n <p>In this tutorial, you will write a fused softmax operation that is significantly faster\n than PyTorch\u2019s native op for a particular class of matrices: those whose rows can fit in\n the GPU\u2019s SRAM.</p>\n-<p>In doing so, you will learn about:\n-- The benefits of kernel fusion for bandwidth-bound operations.\n-- Reduction operators in Triton.</p>\n+<p>In doing so, you will learn about:</p>\n+<ul class=\"simple\">\n+<li><p>The benefits of kernel fusion for bandwidth-bound operations.</p></li>\n+<li><p>Reduction operators in Triton.</p></li>\n+</ul>\n <section id=\"motivations\">\n <h2>Motivations<a class=\"headerlink\" href=\"#motivations\" title=\"Permalink to this heading\">\u00b6</a></h2>\n <p>Custom GPU kernels for elementwise additions are educationally valuable but won\u2019t get you very far in practice.\n@@ -297,17 +300,17 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n [16384] 0\n softmax-performance:\n           N       Triton  Torch (native)  Torch (jit)\n-0     256.0   682.666643      682.666643   264.258068\n-1     384.0   877.714274      819.200021   332.108094\n+0     256.0   682.666643      744.727267   273.066674\n+1     384.0   877.714274      877.714274   332.108094\n 2     512.0   910.222190      910.222190   372.363633\n-3     640.0   975.238103      975.238103   401.568635\n+3     640.0   975.238103      930.909084   409.600010\n 4     768.0  1068.521715     1023.999964   431.157886\n ..      ...          ...             ...          ...\n-93  12160.0  1594.754129     1069.010969   590.918747\n-94  12288.0  1598.438956     1016.061996   590.414408\n-95  12416.0  1576.634933     1031.979242   587.739623\n-96  12544.0  1580.346374     1013.656595   590.305885\n-97  12672.0  1584.000004     1006.213368   589.395349\n+93  12160.0  1588.244879     1069.010969   590.022730\n+94  12288.0  1591.967682     1018.694301   590.414408\n+95  12416.0  1582.916395     1031.979242   586.871514\n+96  12544.0  1580.346374     1013.656595   588.574769\n+97  12672.0  1584.000004     1006.213368   588.539906\n \n [98 rows x 4 columns]\n </pre></div>\n@@ -320,7 +323,7 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n </ul>\n </dd>\n </dl>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  18.799 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  17.617 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-02-fused-softmax-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d91442ac2982c4e0cc3ab0f43534afbc/02-fused-softmax.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">02-fused-softmax.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/03-matrix-multiplication.html", "status": "modified", "additions": 105, "deletions": 105, "changes": 210, "file_content_changes": "@@ -54,7 +54,7 @@\n <li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Matrix Multiplication</a><ul>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#motivations\">Motivations</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#compute-kernel\">Compute Kernel</a><ul>\n-<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#pointer-arithmetic\">Pointer Arithmetic</a></li>\n+<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#pointer-arithmetics\">Pointer Arithmetics</a></li>\n <li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#l2-cache-optimizations\">L2 Cache Optimizations</a></li>\n </ul>\n </li>\n@@ -70,6 +70,7 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"08-experimental-block-pointer.html\">Block Pointer (Experimental)</a></li>\n </ul>\n </li>\n </ul>\n@@ -117,13 +118,15 @@\n </div>\n <section class=\"sphx-glr-example-title\" id=\"matrix-multiplication\">\n <span id=\"sphx-glr-getting-started-tutorials-03-matrix-multiplication-py\"></span><h1>Matrix Multiplication<a class=\"headerlink\" href=\"#matrix-multiplication\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<p>In this tutorial, you will write a 25-lines high-performance FP16 matrix multiplication\n-kernel that achieves performance on par with cuBLAS.</p>\n-<p>In doing so, you will learn about:\n-- Block-level matrix multiplications\n-- Multi-dimensional pointer arithmetic\n-- Program re-ordering for improved L2 cache hit rate\n-- Automatic performance tuning</p>\n+<p>In this tutorial, you will write a very short high-performance FP16 matrix multiplication kernel that achieves\n+performance on parallel with cuBLAS.</p>\n+<p>You will specifically learn about:</p>\n+<ul class=\"simple\">\n+<li><p>Block-level matrix multiplications.</p></li>\n+<li><p>Multi-dimensional pointer arithmetics.</p></li>\n+<li><p>Program re-ordering for improved L2 cache hit rate.</p></li>\n+<li><p>Automatic performance tuning.</p></li>\n+</ul>\n <section id=\"motivations\">\n <h2>Motivations<a class=\"headerlink\" href=\"#motivations\" title=\"Permalink to this heading\">\u00b6</a></h2>\n <p>Matrix multiplications are a key building block of most modern high-performance computing systems.\n@@ -136,16 +139,16 @@ <h2>Motivations<a class=\"headerlink\" href=\"#motivations\" title=\"Permalink to thi\n <p>Roughly speaking, the kernel that we will write will implement the following blocked\n algorithm to multiply a (M, K) by a (K, N) matrix:</p>\n <blockquote>\n-<div><div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"c1\"># do in parallel</span>\n+<div><div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"c1\"># Do in parallel</span>\n <span class=\"k\">for</span> <span class=\"n\">m</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">):</span>\n-  <span class=\"c1\"># do in parallel</span>\n+  <span class=\"c1\"># Do in parallel</span>\n   <span class=\"k\">for</span> <span class=\"n\">n</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">):</span>\n     <span class=\"n\">acc</span> <span class=\"o\">=</span> <span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n     <span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">):</span>\n       <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">m</span> <span class=\"p\">:</span> <span class=\"n\">m</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">k</span> <span class=\"p\">:</span> <span class=\"n\">k</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">]</span>\n       <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">B</span><span class=\"p\">[</span><span class=\"n\">k</span> <span class=\"p\">:</span> <span class=\"n\">k</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">,</span> <span class=\"n\">n</span> <span class=\"p\">:</span> <span class=\"n\">n</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">]</span>\n       <span class=\"n\">acc</span> <span class=\"o\">+=</span> <span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n-    <span class=\"n\">C</span><span class=\"p\">[</span><span class=\"n\">m</span> <span class=\"p\">:</span> <span class=\"n\">m</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">n</span> <span class=\"p\">:</span> <span class=\"n\">n</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">acc</span><span class=\"p\">;</span>\n+    <span class=\"n\">C</span><span class=\"p\">[</span><span class=\"n\">m</span> <span class=\"p\">:</span> <span class=\"n\">m</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">n</span> <span class=\"p\">:</span> <span class=\"n\">n</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">acc</span>\n </pre></div>\n </div>\n </div></blockquote>\n@@ -156,9 +159,9 @@ <h2>Compute Kernel<a class=\"headerlink\" href=\"#compute-kernel\" title=\"Permalink\n <p>The above algorithm is, actually, fairly straightforward to implement in Triton.\n The main difficulty comes from the computation of the memory locations at which blocks\n of <code class=\"code docutils literal notranslate\"><span class=\"pre\">A</span></code> and <code class=\"code docutils literal notranslate\"><span class=\"pre\">B</span></code> must be read in the inner loop. For that, we need\n-multi-dimensional pointer arithmetic.</p>\n-<section id=\"pointer-arithmetic\">\n-<h3>Pointer Arithmetic<a class=\"headerlink\" href=\"#pointer-arithmetic\" title=\"Permalink to this heading\">\u00b6</a></h3>\n+multi-dimensional pointer arithmetics.</p>\n+<section id=\"pointer-arithmetics\">\n+<h3>Pointer Arithmetics<a class=\"headerlink\" href=\"#pointer-arithmetics\" title=\"Permalink to this heading\">\u00b6</a></h3>\n <p>For a row-major 2D tensor <code class=\"code docutils literal notranslate\"><span class=\"pre\">X</span></code>, the memory location of <code class=\"code docutils literal notranslate\"><span class=\"pre\">X[i,</span> <span class=\"pre\">j]</span></code> is given b\n y <code class=\"code docutils literal notranslate\"><span class=\"pre\">&amp;X[i,</span> <span class=\"pre\">j]</span> <span class=\"pre\">=</span> <span class=\"pre\">X</span> <span class=\"pre\">+</span> <span class=\"pre\">i*stride_xi</span> <span class=\"pre\">+</span> <span class=\"pre\">j*stride_xj</span></code>.\n Therefore, blocks of pointers for <code class=\"code docutils literal notranslate\"><span class=\"pre\">A[m</span> <span class=\"pre\">:</span> <span class=\"pre\">m+BLOCK_SIZE_M,</span> <span class=\"pre\">k:k+BLOCK_SIZE_K]</span></code> and\n@@ -169,10 +172,14 @@ <h3>Pointer Arithmetic<a class=\"headerlink\" href=\"#pointer-arithmetic\" title=\"Pe\n </pre></div>\n </div>\n </div></blockquote>\n-<p>Which means that pointers for blocks of A and B can be initialized (i.e., <code class=\"code docutils literal notranslate\"><span class=\"pre\">k=0</span></code>) in Triton as:</p>\n+<p>Which means that pointers for blocks of A and B can be initialized (i.e., <code class=\"code docutils literal notranslate\"><span class=\"pre\">k=0</span></code>) in Triton as the following\n+code. Also note that we need an extra modulo to handle the case where <code class=\"code docutils literal notranslate\"><span class=\"pre\">M</span></code> is not a multiple of\n+<code class=\"code docutils literal notranslate\"><span class=\"pre\">BLOCK_SIZE_M</span></code> or <code class=\"code docutils literal notranslate\"><span class=\"pre\">N</span></code> is not a multiple of <code class=\"code docutils literal notranslate\"><span class=\"pre\">BLOCK_SIZE_N</span></code>, in which case we can pad the data with\n+some useless values, which will not contribute to the results. For the <code class=\"code docutils literal notranslate\"><span class=\"pre\">K</span></code> dimension, we will handle that later\n+using masking load semantics.</p>\n <blockquote>\n-<div><div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">offs_am</span> <span class=\"o\">=</span> <span class=\"n\">pid_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">)</span>\n-<span class=\"n\">offs_bn</span> <span class=\"o\">=</span> <span class=\"n\">pid_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_N</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)</span>\n+<div><div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">offs_am</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">pid_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">))</span> <span class=\"o\">%</span> <span class=\"n\">M</span>\n+<span class=\"n\">offs_bn</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">pid_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_N</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">))</span> <span class=\"o\">%</span> <span class=\"n\">N</span>\n <span class=\"n\">offs_k</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">)</span>\n <span class=\"n\">a_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">a_ptr</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_am</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"n\">stride_am</span> <span class=\"o\">+</span> <span class=\"n\">offs_k</span> <span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span><span class=\"o\">*</span><span class=\"n\">stride_ak</span><span class=\"p\">)</span>\n <span class=\"n\">b_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">b_ptr</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_k</span> <span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"n\">stride_bk</span> <span class=\"o\">+</span> <span class=\"n\">offs_bn</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span><span class=\"o\">*</span><span class=\"n\">stride_bn</span><span class=\"p\">)</span>\n@@ -208,24 +215,24 @@ <h3>L2 Cache Optimizations<a class=\"headerlink\" href=\"#l2-cache-optimizations\" t\n This can be done by \u2018super-grouping\u2019 blocks in groups of <code class=\"code docutils literal notranslate\"><span class=\"pre\">GROUP_M</span></code> rows before\n switching to the next column:</p>\n <blockquote>\n-<div><div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"c1\"># program ID</span>\n+<div><div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"c1\"># Program ID</span>\n <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-<span class=\"c1\"># number of program ids along the M axis</span>\n+<span class=\"c1\"># Number of program ids along the M axis</span>\n <span class=\"n\">num_pid_m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">)</span>\n-<span class=\"c1\"># number of programs ids along the N axis</span>\n+<span class=\"c1\"># Number of programs ids along the N axis</span>\n <span class=\"n\">num_pid_n</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)</span>\n-<span class=\"c1\"># number of programs in group</span>\n+<span class=\"c1\"># Number of programs in group</span>\n <span class=\"n\">num_pid_in_group</span> <span class=\"o\">=</span> <span class=\"n\">GROUP_SIZE_M</span> <span class=\"o\">*</span> <span class=\"n\">num_pid_n</span>\n-<span class=\"c1\"># id of the group this program is in</span>\n+<span class=\"c1\"># Id of the group this program is in</span>\n <span class=\"n\">group_id</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">//</span> <span class=\"n\">num_pid_in_group</span>\n-<span class=\"c1\"># row-id of the first program in the group</span>\n+<span class=\"c1\"># Row-id of the first program in the group</span>\n <span class=\"n\">first_pid_m</span> <span class=\"o\">=</span> <span class=\"n\">group_id</span> <span class=\"o\">*</span> <span class=\"n\">GROUP_SIZE_M</span>\n-<span class=\"c1\"># if `num_pid_m` isn&#39;t divisible by `GROUP_SIZE_M`, the last group is smaller</span>\n+<span class=\"c1\"># If `num_pid_m` isn&#39;t divisible by `GROUP_SIZE_M`, the last group is smaller</span>\n <span class=\"n\">group_size_m</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">num_pid_m</span> <span class=\"o\">-</span> <span class=\"n\">first_pid_m</span><span class=\"p\">,</span> <span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">)</span>\n-<span class=\"c1\"># *within groups*, programs are ordered in a column-major order</span>\n-<span class=\"c1\"># row-id of the program in the *launch grid*</span>\n+<span class=\"c1\"># *Within groups*, programs are ordered in a column-major order</span>\n+<span class=\"c1\"># Row-id of the program in the *launch grid*</span>\n <span class=\"n\">pid_m</span> <span class=\"o\">=</span> <span class=\"n\">first_pid_m</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">pid</span> <span class=\"o\">%</span> <span class=\"n\">group_size_m</span><span class=\"p\">)</span>\n-<span class=\"c1\"># col-id of the program in the *launch grid*</span>\n+<span class=\"c1\"># Col-id of the program in the *launch grid*</span>\n <span class=\"n\">pid_n</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">pid</span> <span class=\"o\">%</span> <span class=\"n\">num_pid_in_group</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"n\">group_size_m</span>\n </pre></div>\n </div>\n@@ -248,15 +255,12 @@ <h2>Final Result<a class=\"headerlink\" href=\"#final-result\" title=\"Permalink to t\n <span class=\"kn\">import</span> <span class=\"nn\">triton</span>\n <span class=\"kn\">import</span> <span class=\"nn\">triton.language</span> <span class=\"k\">as</span> <span class=\"nn\">tl</span>\n \n-<span class=\"c1\"># %</span>\n-<span class=\"c1\"># :code:`triton.jit`&#39;ed functions can be auto-tuned by using the `triton.autotune`</span>\n-<span class=\"c1\"># decorator, which consumes:</span>\n-<span class=\"c1\">#   - A list of :code:`triton.Config` objects that define different configurations of</span>\n-<span class=\"c1\">#       meta-parameters (e.g., BLOCK_SIZE_M) and compilation options (e.g., num_warps) to try</span>\n-<span class=\"c1\">#   - An autotuning *key* whose change in values will trigger evaluation of all the</span>\n-<span class=\"c1\">#       provided configs</span>\n-\n \n+<span class=\"c1\"># `triton.jit`&#39;ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:</span>\n+<span class=\"c1\">#   - A list of `triton.Config` objects that define different configurations of</span>\n+<span class=\"c1\">#       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try</span>\n+<span class=\"c1\">#   - An auto-tuning *key* whose change in values will trigger evaluation of all the</span>\n+<span class=\"c1\">#       provided configs</span>\n <span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">autotune</span><span class=\"p\">(</span>\n     <span class=\"n\">configs</span><span class=\"o\">=</span><span class=\"p\">[</span>\n         <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n@@ -277,8 +281,8 @@ <h2>Final Result<a class=\"headerlink\" href=\"#final-result\" title=\"Permalink to t\n     <span class=\"c1\"># Matrix dimensions</span>\n     <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span>\n     <span class=\"c1\"># The stride variables represent how much to increase the ptr by when moving by 1</span>\n-    <span class=\"c1\"># element in a particular dimension. E.g. stride_am is how much to increase a_ptr</span>\n-    <span class=\"c1\"># by to get the element one row down (A has M rows)</span>\n+    <span class=\"c1\"># element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`</span>\n+    <span class=\"c1\"># by to get the element one row down (A has M rows).</span>\n     <span class=\"n\">stride_am</span><span class=\"p\">,</span> <span class=\"n\">stride_ak</span><span class=\"p\">,</span>\n     <span class=\"n\">stride_bk</span><span class=\"p\">,</span> <span class=\"n\">stride_bn</span><span class=\"p\">,</span>\n     <span class=\"n\">stride_cm</span><span class=\"p\">,</span> <span class=\"n\">stride_cn</span><span class=\"p\">,</span>\n@@ -292,12 +296,11 @@ <h2>Final Result<a class=\"headerlink\" href=\"#final-result\" title=\"Permalink to t\n <span class=\"sd\">    &quot;&quot;&quot;</span>\n     <span class=\"c1\"># -----------------------------------------------------------</span>\n     <span class=\"c1\"># Map program ids `pid` to the block of C it should compute.</span>\n-    <span class=\"c1\"># This is done in a grouped ordering to promote L2 data reuse</span>\n-    <span class=\"c1\"># See above `L2 Cache Optimizations` section for details</span>\n+    <span class=\"c1\"># This is done in a grouped ordering to promote L2 data reuse.</span>\n+    <span class=\"c1\"># See above `L2 Cache Optimizations` section for details.</span>\n     <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n     <span class=\"n\">num_pid_m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">)</span>\n     <span class=\"n\">num_pid_n</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)</span>\n-    <span class=\"n\">num_pid_k</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">)</span>\n     <span class=\"n\">num_pid_in_group</span> <span class=\"o\">=</span> <span class=\"n\">GROUP_SIZE_M</span> <span class=\"o\">*</span> <span class=\"n\">num_pid_n</span>\n     <span class=\"n\">group_id</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">//</span> <span class=\"n\">num_pid_in_group</span>\n     <span class=\"n\">first_pid_m</span> <span class=\"o\">=</span> <span class=\"n\">group_id</span> <span class=\"o\">*</span> <span class=\"n\">GROUP_SIZE_M</span>\n@@ -309,67 +312,63 @@ <h2>Final Result<a class=\"headerlink\" href=\"#final-result\" title=\"Permalink to t\n     <span class=\"c1\"># Create pointers for the first blocks of A and B.</span>\n     <span class=\"c1\"># We will advance this pointer as we move in the K direction</span>\n     <span class=\"c1\"># and accumulate</span>\n-    <span class=\"c1\"># a_ptrs is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers</span>\n-    <span class=\"c1\"># b_ptrs is a block of [BLOCK_SIZE_K, BLOCK_SIZE_n] pointers</span>\n-    <span class=\"c1\"># see above `Pointer Arithmetic` section for details</span>\n-    <span class=\"n\">offs_am</span> <span class=\"o\">=</span> <span class=\"n\">pid_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">)</span>\n-    <span class=\"n\">offs_bn</span> <span class=\"o\">=</span> <span class=\"n\">pid_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_N</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers</span>\n+    <span class=\"c1\"># `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers</span>\n+    <span class=\"c1\"># See above `Pointer Arithmetics` section for details</span>\n+    <span class=\"n\">offs_am</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">pid_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">))</span> <span class=\"o\">%</span> <span class=\"n\">M</span>\n+    <span class=\"n\">offs_bn</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">pid_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_N</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">))</span> <span class=\"o\">%</span> <span class=\"n\">N</span>\n     <span class=\"n\">offs_k</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">)</span>\n     <span class=\"n\">a_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">a_ptr</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_am</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_am</span> <span class=\"o\">+</span> <span class=\"n\">offs_k</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_ak</span><span class=\"p\">)</span>\n     <span class=\"n\">b_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">b_ptr</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_k</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_bk</span> <span class=\"o\">+</span> <span class=\"n\">offs_bn</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_bn</span><span class=\"p\">)</span>\n \n     <span class=\"c1\"># -----------------------------------------------------------</span>\n-    <span class=\"c1\"># Iterate to compute a block of the C matrix</span>\n+    <span class=\"c1\"># Iterate to compute a block of the C matrix.</span>\n     <span class=\"c1\"># We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block</span>\n     <span class=\"c1\"># of fp32 values for higher accuracy.</span>\n-    <span class=\"c1\"># `accumulator` will be converted back to fp16 after the loop</span>\n+    <span class=\"c1\"># `accumulator` will be converted back to fp16 after the loop.</span>\n     <span class=\"n\">accumulator</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n-    <span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">num_pid_k</span><span class=\"p\">):</span>\n-        <span class=\"c1\"># Note that for simplicity, we don&#39;t apply a mask here.</span>\n-        <span class=\"c1\"># This means that if K is not a multiple of BLOCK_SIZE_K,</span>\n-        <span class=\"c1\"># this will access out-of-bounds memory and produce an</span>\n-        <span class=\"c1\"># error or (worse!) incorrect results.</span>\n-        <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">a_ptrs</span><span class=\"p\">)</span>\n-        <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">b_ptrs</span><span class=\"p\">)</span>\n-        <span class=\"c1\"># We accumulate along the K dimension</span>\n+    <span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">)):</span>\n+        <span class=\"c1\"># Load the next block of A and B, generate a mask by checking the K dimension.</span>\n+        <span class=\"c1\"># If it is out of bounds, set it to 0.</span>\n+        <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">a_ptrs</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">offs_k</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">&lt;</span> <span class=\"n\">K</span> <span class=\"o\">-</span> <span class=\"n\">k</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">)</span>\n+        <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">b_ptrs</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">offs_k</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&lt;</span> <span class=\"n\">K</span> <span class=\"o\">-</span> <span class=\"n\">k</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># We accumulate along the K dimension.</span>\n         <span class=\"n\">accumulator</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n-        <span class=\"c1\"># Advance the ptrs to the next K block</span>\n+        <span class=\"c1\"># Advance the ptrs to the next K block.</span>\n         <span class=\"n\">a_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_SIZE_K</span> <span class=\"o\">*</span> <span class=\"n\">stride_ak</span>\n         <span class=\"n\">b_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_SIZE_K</span> <span class=\"o\">*</span> <span class=\"n\">stride_bk</span>\n-    <span class=\"c1\"># you can fuse arbitrary activation functions here</span>\n+    <span class=\"c1\"># You can fuse arbitrary activation functions here</span>\n     <span class=\"c1\"># while the accumulator is still in FP32!</span>\n-    <span class=\"k\">if</span> <span class=\"n\">ACTIVATION</span><span class=\"p\">:</span>\n-        <span class=\"n\">accumulator</span> <span class=\"o\">=</span> <span class=\"n\">ACTIVATION</span><span class=\"p\">(</span><span class=\"n\">accumulator</span><span class=\"p\">)</span>\n+    <span class=\"k\">if</span> <span class=\"n\">ACTIVATION</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;leaky_relu&quot;</span><span class=\"p\">:</span>\n+        <span class=\"n\">accumulator</span> <span class=\"o\">=</span> <span class=\"n\">leaky_relu</span><span class=\"p\">(</span><span class=\"n\">accumulator</span><span class=\"p\">)</span>\n     <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">accumulator</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n \n     <span class=\"c1\"># -----------------------------------------------------------</span>\n-    <span class=\"c1\"># Write back the block of the output matrix C</span>\n+    <span class=\"c1\"># Write back the block of the output matrix C with masks.</span>\n     <span class=\"n\">offs_cm</span> <span class=\"o\">=</span> <span class=\"n\">pid_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">)</span>\n     <span class=\"n\">offs_cn</span> <span class=\"o\">=</span> <span class=\"n\">pid_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_N</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)</span>\n     <span class=\"n\">c_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">c_ptr</span> <span class=\"o\">+</span> <span class=\"n\">stride_cm</span> <span class=\"o\">*</span> <span class=\"n\">offs_cm</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">stride_cn</span> <span class=\"o\">*</span> <span class=\"n\">offs_cn</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n     <span class=\"n\">c_mask</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">offs_cm</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&lt;</span> <span class=\"n\">M</span><span class=\"p\">)</span> <span class=\"o\">&amp;</span> <span class=\"p\">(</span><span class=\"n\">offs_cn</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n     <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">c_ptrs</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">c_mask</span><span class=\"p\">)</span>\n \n \n-<span class=\"c1\"># we can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`</span>\n+<span class=\"c1\"># We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.</span>\n <span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n <span class=\"k\">def</span> <span class=\"nf\">leaky_relu</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n+    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n     <span class=\"k\">return</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">&gt;=</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"mf\">0.01</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"p\">)</span>\n </pre></div>\n </div>\n-<p>We can now create a convenience wrapper function that only takes two input tensors\n-and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel</p>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">):</span>\n-    <span class=\"c1\"># checks constraints</span>\n-    <span class=\"k\">assert</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"s2\">&quot;incompatible dimensions&quot;</span>\n-    <span class=\"k\">assert</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">is_contiguous</span><span class=\"p\">(),</span> <span class=\"s2\">&quot;matrix A must be contiguous&quot;</span>\n-    <span class=\"k\">assert</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">is_contiguous</span><span class=\"p\">(),</span> <span class=\"s2\">&quot;matrix B must be contiguous&quot;</span>\n+<p>We can now create a convenience wrapper function that only takes two input tensors,\n+and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s2\">&quot;&quot;</span><span class=\"p\">):</span>\n+    <span class=\"c1\"># Check constraints.</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"s2\">&quot;Incompatible dimensions&quot;</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">is_contiguous</span><span class=\"p\">(),</span> <span class=\"s2\">&quot;Matrix A must be contiguous&quot;</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">is_contiguous</span><span class=\"p\">(),</span> <span class=\"s2\">&quot;Matrix B must be contiguous&quot;</span>\n     <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">K</span> <span class=\"o\">=</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n     <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n-    <span class=\"k\">assert</span> <span class=\"p\">(</span>\n-        <span class=\"n\">K</span> <span class=\"o\">%</span> <span class=\"mi\">32</span> <span class=\"o\">==</span> <span class=\"mi\">0</span>\n-    <span class=\"p\">),</span> <span class=\"s2\">&quot;We don&#39;t check memory-out-of-bounds with K so K must be divisible by BLOCK_SIZE_K&quot;</span>\n-    <span class=\"c1\"># allocates output</span>\n+    <span class=\"c1\"># Allocates output.</span>\n     <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n     <span class=\"c1\"># 1D launch kernel where each block gets its own program.</span>\n     <span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">META</span><span class=\"p\">:</span> <span class=\"p\">(</span>\n@@ -381,19 +380,19 @@ <h2>Final Result<a class=\"headerlink\" href=\"#final-result\" title=\"Permalink to t\n         <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n         <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n         <span class=\"n\">c</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">c</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n-        <span class=\"n\">ACTIVATION</span><span class=\"o\">=</span><span class=\"n\">activation</span><span class=\"p\">,</span>\n+        <span class=\"n\">ACTIVATION</span><span class=\"o\">=</span><span class=\"n\">activation</span>\n     <span class=\"p\">)</span>\n     <span class=\"k\">return</span> <span class=\"n\">c</span>\n </pre></div>\n </div>\n </section>\n <section id=\"unit-test\">\n <h2>Unit Test<a class=\"headerlink\" href=\"#unit-test\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS)</p>\n+<p>We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS).</p>\n <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n-<span class=\"n\">triton_output</span> <span class=\"o\">=</span> <span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">)</span>\n+<span class=\"n\">triton_output</span> <span class=\"o\">=</span> <span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n <span class=\"n\">torch_output</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;triton_output=</span><span class=\"si\">{</span><span class=\"n\">triton_output</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;torch_output=</span><span class=\"si\">{</span><span class=\"n\">torch_output</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n@@ -427,22 +426,23 @@ <h2>Unit Test<a class=\"headerlink\" href=\"#unit-test\" title=\"Permalink to this he\n <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this heading\">\u00b6</a></h2>\n <section id=\"square-matrix-performance\">\n <h3>Square Matrix Performance<a class=\"headerlink\" href=\"#square-matrix-performance\" title=\"Permalink to this heading\">\u00b6</a></h3>\n-<p>We can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices, but feel free to arrange this script as you wish to benchmark any other matrix shape.</p>\n+<p>We can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices,\n+but feel free to arrange this script as you wish to benchmark any other matrix shape.</p>\n <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">perf_report</span><span class=\"p\">(</span>\n     <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">Benchmark</span><span class=\"p\">(</span>\n-        <span class=\"n\">x_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;M&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;K&#39;</span><span class=\"p\">],</span>  <span class=\"c1\"># argument names to use as an x-axis for the plot</span>\n+        <span class=\"n\">x_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;M&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;K&#39;</span><span class=\"p\">],</span>  <span class=\"c1\"># Argument names to use as an x-axis for the plot</span>\n         <span class=\"n\">x_vals</span><span class=\"o\">=</span><span class=\"p\">[</span>\n             <span class=\"mi\">128</span> <span class=\"o\">*</span> <span class=\"n\">i</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">33</span><span class=\"p\">)</span>\n-        <span class=\"p\">],</span>  <span class=\"c1\"># different possible values for `x_name`</span>\n-        <span class=\"n\">line_arg</span><span class=\"o\">=</span><span class=\"s1\">&#39;provider&#39;</span><span class=\"p\">,</span>  <span class=\"c1\"># argument name whose value corresponds to a different line in the plot</span>\n-        <span class=\"c1\"># possible values for `line_arg``</span>\n+        <span class=\"p\">],</span>  <span class=\"c1\"># Different possible values for `x_name`</span>\n+        <span class=\"n\">line_arg</span><span class=\"o\">=</span><span class=\"s1\">&#39;provider&#39;</span><span class=\"p\">,</span>  <span class=\"c1\"># Argument name whose value corresponds to a different line in the plot</span>\n+        <span class=\"c1\"># Possible values for `line_arg`</span>\n         <span class=\"n\">line_vals</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;cublas&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">],</span>\n-        <span class=\"c1\"># label name for the lines</span>\n+        <span class=\"c1\"># Label name for the lines</span>\n         <span class=\"n\">line_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">&quot;cuBLAS&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;Triton&quot;</span><span class=\"p\">],</span>\n-        <span class=\"c1\"># line styles</span>\n+        <span class=\"c1\"># Line styles</span>\n         <span class=\"n\">styles</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s1\">&#39;green&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;blue&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">)],</span>\n-        <span class=\"n\">ylabel</span><span class=\"o\">=</span><span class=\"s2\">&quot;TFLOPS&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># label name for the y-axis</span>\n-        <span class=\"n\">plot_name</span><span class=\"o\">=</span><span class=\"s2\">&quot;matmul-performance&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># name for the plot. Used also as a file name for saving the plot.</span>\n+        <span class=\"n\">ylabel</span><span class=\"o\">=</span><span class=\"s2\">&quot;TFLOPS&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># Label name for the y-axis</span>\n+        <span class=\"n\">plot_name</span><span class=\"o\">=</span><span class=\"s2\">&quot;matmul-performance&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># Name for the plot, used also as a file name for saving the plot.</span>\n         <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{},</span>\n     <span class=\"p\">)</span>\n <span class=\"p\">)</span>\n@@ -463,40 +463,40 @@ <h3>Square Matrix Performance<a class=\"headerlink\" href=\"#square-matrix-performa\n </div>\n <img src=\"../../_images/sphx_glr_03-matrix-multiplication_001.png\" srcset=\"../../_images/sphx_glr_03-matrix-multiplication_001.png\" alt=\"03 matrix multiplication\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>matmul-performance:\n          M      cuBLAS      Triton\n-0    256.0    4.681143    4.096000\n+0    256.0    4.096000    4.096000\n 1    384.0   12.288000   12.288000\n 2    512.0   26.214401   23.831273\n 3    640.0   42.666665   39.384616\n 4    768.0   63.195428   58.982401\n 5    896.0   78.051553   82.642822\n-6   1024.0  110.376426  104.857603\n+6   1024.0  110.376426   99.864382\n 7   1152.0  135.726544  129.825388\n-8   1280.0  163.840004  163.840004\n-9   1408.0  151.438217  132.970149\n+8   1280.0  157.538463  163.840004\n+9   1408.0  155.765024  132.970149\n 10  1536.0  181.484314  157.286398\n-11  1664.0  183.651271  179.978245\n+11  1664.0  179.978245  179.978245\n 12  1792.0  172.914215  208.137481\n 13  1920.0  200.347822  168.585369\n 14  2048.0  197.379013  190.650180\n-15  2176.0  188.071477  209.621326\n+15  2176.0  189.845737  209.621326\n 16  2304.0  225.357284  227.503545\n-17  2432.0  202.118452  200.674737\n-18  2560.0  219.919464  217.006622\n-19  2688.0  196.544332  197.567993\n-20  2816.0  209.683695  210.696652\n-21  2944.0  210.278616  225.502413\n-22  3072.0  205.156169  207.410628\n-23  3200.0  210.526325  216.949149\n-24  3328.0  203.941342  206.278780\n-25  3456.0  215.565692  216.724640\n-26  3584.0  215.108588  206.702053\n-27  3712.0  209.428397  217.168134\n-28  3840.0  205.944129  208.664143\n-29  3968.0  209.663117  218.680889\n-30  4096.0  218.240199  214.405318\n+17  2432.0  200.674737  200.674737\n+18  2560.0  222.911566  217.006622\n+19  2688.0  196.544332  196.544332\n+20  2816.0  208.680416  208.680416\n+21  2944.0  215.740400  222.482283\n+22  3072.0  207.410628  207.410628\n+23  3200.0  215.488222  217.687077\n+24  3328.0  202.792385  205.689424\n+25  3456.0  216.143621  217.896133\n+26  3584.0  217.186932  205.756041\n+27  3712.0  208.553950  214.371984\n+28  3840.0  205.944129  206.328356\n+29  3968.0  207.171367  213.702171\n+30  4096.0  219.310012  214.405318\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  36.046 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  36.499 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-03-matrix-multiplication-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d5fee5b55a64e47f1b5724ec39adf171/03-matrix-multiplication.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">03-matrix-multiplication.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/04-low-memory-dropout.html", "status": "modified", "additions": 7, "deletions": 4, "changes": 11, "file_content_changes": "@@ -63,6 +63,7 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"08-experimental-block-pointer.html\">Block Pointer (Experimental)</a></li>\n </ul>\n </li>\n </ul>\n@@ -113,9 +114,11 @@\n <p>In this tutorial, you will write a memory-efficient implementation of dropout whose state\n will be composed of a single int32 seed. This differs from more traditional implementations of dropout,\n whose state is generally composed of a bit mask tensor of the same shape as the input.</p>\n-<p>In doing so, you will learn about:\n-- The limitations of naive implementations of Dropout with PyTorch\n-- Parallel pseudo-random number generation in Triton</p>\n+<p>In doing so, you will learn about:</p>\n+<ul class=\"simple\">\n+<li><p>The limitations of naive implementations of Dropout with PyTorch.</p></li>\n+<li><p>Parallel pseudo-random number generation in Triton.</p></li>\n+</ul>\n <section id=\"baseline\">\n <h2>Baseline<a class=\"headerlink\" href=\"#baseline\" title=\"Permalink to this heading\">\u00b6</a></h2>\n <p>The <em>dropout</em> operator was first introduced in <a class=\"reference internal\" href=\"#srivastava2014\" id=\"id1\"><span>[SRIVASTAVA2014]</span></a> as a way to improve the performance\n@@ -284,7 +287,7 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n <p>Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \u201cDropout: A Simple Way to Prevent Neural Networks from Overfitting\u201d, JMLR 2014</p>\n </div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  1.048 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.707 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-04-low-memory-dropout-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/c9aed78977a4c05741d675a38dde3d7d/04-low-memory-dropout.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">04-low-memory-dropout.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/05-layer-norm.html", "status": "modified", "additions": 32, "deletions": 29, "changes": 61, "file_content_changes": "@@ -63,6 +63,7 @@\n </li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"08-experimental-block-pointer.html\">Block Pointer (Experimental)</a></li>\n </ul>\n </li>\n </ul>\n@@ -112,9 +113,11 @@\n <span id=\"sphx-glr-getting-started-tutorials-05-layer-norm-py\"></span><h1>Layer Normalization<a class=\"headerlink\" href=\"#layer-normalization\" title=\"Permalink to this heading\">\u00b6</a></h1>\n <p>In this tutorial, you will write a high-performance layer normalization\n kernel that runs faster than the PyTorch implementation.</p>\n-<p>In doing so, you will learn about:\n-- Implementing backward pass in Triton\n-- Implementing parallel reduction in Triton</p>\n+<p>In doing so, you will learn about:</p>\n+<ul class=\"simple\">\n+<li><p>Implementing backward pass in Triton.</p></li>\n+<li><p>Implementing parallel reduction in Triton.</p></li>\n+</ul>\n <section id=\"motivations\">\n <h2>Motivations<a class=\"headerlink\" href=\"#motivations\" title=\"Permalink to this heading\">\u00b6</a></h2>\n <p>The <em>LayerNorm</em> operator was first introduced in <a class=\"reference internal\" href=\"#ba2016\" id=\"id1\"><span>[BA2016]</span></a> as a way to improve the performance\n@@ -468,36 +471,36 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n [16384] 0\n layer-norm-backward:\n           N      Triton       Torch\n-0    1024.0  240.941181  361.411758\n-1    1536.0  347.773587  405.098894\n+0    1024.0  240.941181  356.173905\n+1    1536.0  361.411771  400.695643\n 2    2048.0  450.935778  438.857137\n-3    2560.0  538.947358  472.615383\n-4    3072.0  624.813540  501.551024\n+3    2560.0  538.947358  469.007657\n+4    3072.0  619.563043  501.551024\n 5    3584.0  688.127967  445.678757\n-6    4096.0  750.412251  436.906674\n-7    4608.0  708.923101  438.857146\n-8    5120.0  763.229797  445.217381\n-9    5632.0  809.389194  459.755106\n-10   6144.0  847.448272  465.160886\n-11   6656.0  882.563556  472.615367\n-12   7168.0  919.957230  452.715775\n-13   7680.0  950.103127  443.076928\n-14   8192.0  983.040025  465.895721\n-15   8704.0  663.161879  458.105254\n+6    4096.0  744.727267  436.906674\n+7    4608.0  708.923101  437.122520\n+8    5120.0  763.229797  443.610086\n+9    5632.0  804.571435  459.755106\n+10   6144.0  842.605744  465.160886\n+11   6656.0  882.563556  471.221251\n+12   7168.0  915.063803  451.527570\n+13   7680.0  950.103127  442.014385\n+14   8192.0  978.149241  464.794337\n+15   8704.0  663.161879  457.102857\n 16   9216.0  686.906817  466.632911\n-17   9728.0  713.981680  471.184672\n-18  10240.0  731.428577  467.224344\n-19  10752.0  761.203560  466.632895\n+17   9728.0  711.804890  470.709684\n+18  10240.0  731.428577  466.337764\n+19  10752.0  758.964709  466.632895\n 20  11264.0  781.317950  469.333317\n 21  11776.0  802.909085  471.826361\n-22  12288.0  826.084006  471.105436\n-23  12800.0  825.806430  471.889394\n-24  13312.0  842.976243  477.560558\n-25  13824.0  846.367375  478.408086\n-26  14336.0  851.643583  476.542919\n-27  14848.0  852.516725  477.044187\n-28  15360.0  861.308412  476.279061\n-29  15872.0  867.717548  478.552759\n+22  12288.0  821.481899  471.105436\n+23  12800.0  823.592520  471.889394\n+24  13312.0  840.757868  477.560558\n+25  13824.0  844.213752  478.753261\n+26  14336.0  849.540743  476.542919\n+27  14848.0  852.516725  476.406423\n+28  15360.0  863.325544  476.279061\n+29  15872.0  865.745448  479.154717\n </pre></div>\n </div>\n </section>\n@@ -509,7 +512,7 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n <p>Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \u201cLayer Normalization\u201d, Arxiv 2016</p>\n </div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  39.016 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  38.028 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-05-layer-norm-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/935c0dd0fbeb4b2e69588471cbb2d4b2/05-layer-norm.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">05-layer-norm.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/06-fused-attention.html", "status": "modified", "additions": 10, "deletions": 9, "changes": 19, "file_content_changes": "@@ -56,6 +56,7 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n <li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Fused Attention</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"08-experimental-block-pointer.html\">Block Pointer (Experimental)</a></li>\n </ul>\n </li>\n </ul>\n@@ -113,17 +114,17 @@\n [128, 128] 1\n fused-attention-batch4-head48-d64-fwd:\n     N_CTX     Triton\n-0  1024.0   0.323668\n-1  2048.0   1.094414\n-2  4096.0   3.942359\n-3  8192.0  15.056554\n+0  1024.0   0.323017\n+1  2048.0   1.094384\n+2  4096.0   3.962921\n+3  8192.0  14.962859\n [128, 64] 1\n fused-attention-batch4-head48-d64-bwd:\n     N_CTX     Triton\n-0  1024.0   1.185261\n-1  2048.0   3.758828\n-2  4096.0  13.213110\n-3  8192.0  49.180672\n+0  1024.0   1.186626\n+1  2048.0   3.760393\n+2  4096.0  13.224523\n+3  8192.0  49.601536\n </pre></div>\n </div>\n <div class=\"line-block\">\n@@ -484,7 +485,7 @@\n <span class=\"n\">bench_flash_attention</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;.&#39;</span><span class=\"p\">,</span> <span class=\"n\">print_data</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  5.598 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  5.120 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-06-fused-attention-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/54a35f6ec55f9746935b9566fb6bb1df/06-fused-attention.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">06-fused-attention.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/07-math-functions.html", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -22,7 +22,7 @@\n     <script src=\"../../_static/js/theme.js\"></script>\n     <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n     <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton\" href=\"../../python-api/triton.html\" />\n+    <link rel=\"next\" title=\"Block Pointer (Experimental)\" href=\"08-experimental-block-pointer.html\" />\n     <link rel=\"prev\" title=\"Fused Attention\" href=\"06-fused-attention.html\" /> \n </head>\n \n@@ -61,6 +61,7 @@\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#customize-the-libdevice-library-path\">Customize the libdevice library path</a></li>\n </ul>\n </li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"08-experimental-block-pointer.html\">Block Pointer (Experimental)</a></li>\n </ul>\n </li>\n </ul>\n@@ -185,7 +186,7 @@ <h2>Customize the libdevice library path<a class=\"headerlink\" href=\"#customize-t\n The maximum difference between torch and triton is 2.384185791015625e-07\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.380 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.264 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-07-math-functions-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/77571465f7f4bd281d3a847dc2633146/07-math-functions.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">07-math-functions.py</span></code></a></p>\n@@ -203,7 +204,7 @@ <h2>Customize the libdevice library path<a class=\"headerlink\" href=\"#customize-t\n           </div>\n           <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n         <a href=\"06-fused-attention.html\" class=\"btn btn-neutral float-left\" title=\"Fused Attention\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"../../python-api/triton.html\" class=\"btn btn-neutral float-right\" title=\"triton\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+        <a href=\"08-experimental-block-pointer.html\" class=\"btn btn-neutral float-right\" title=\"Block Pointer (Experimental)\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n     </div>\n \n   <hr/>"}, {"filename": "main/getting-started/tutorials/08-experimental-block-pointer.html", "status": "added", "additions": 391, "deletions": 0, "changes": 391, "file_content_changes": "@@ -0,0 +1,391 @@\n+<!DOCTYPE html>\n+<html class=\"writer-html5\" lang=\"en\" >\n+<head>\n+  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n+\n+  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n+  <title>Block Pointer (Experimental) &mdash; Triton  documentation</title>\n+      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n+  <!--[if lt IE 9]>\n+    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n+  <![endif]-->\n+  \n+        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n+        <script src=\"../../_static/doctools.js\"></script>\n+        <script src=\"../../_static/sphinx_highlight.js\"></script>\n+    <script src=\"../../_static/js/theme.js\"></script>\n+    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n+    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n+    <link rel=\"next\" title=\"triton\" href=\"../../python-api/triton.html\" />\n+    <link rel=\"prev\" title=\"Libdevice (tl.math) function\" href=\"07-math-functions.html\" /> \n+</head>\n+\n+<body class=\"wy-body-for-nav\"> \n+  <div class=\"wy-grid-for-nav\">\n+    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n+      <div class=\"wy-side-scroll\">\n+        <div class=\"wy-side-nav-search\" >\n+\n+          \n+          \n+          <a href=\"../../index.html\" class=\"icon icon-home\">\n+            Triton\n+          </a>\n+<div role=\"search\">\n+  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n+    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n+    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n+    <input type=\"hidden\" name=\"area\" value=\"default\" />\n+  </form>\n+</div>\n+        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n+              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n+<ul class=\"current\">\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../installation.html\">Installation</a></li>\n+<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"index.html\">Tutorials</a><ul class=\"current\">\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"01-vector-add.html\">Vector Addition</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"02-fused-softmax.html\">Fused Softmax</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n+<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Block Pointer (Experimental)</a><ul>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#motivations\">Motivations</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#make-a-block-pointer\">Make a Block Pointer</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#load-store-a-block-pointer\">Load/Store a Block Pointer</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#advance-a-block-pointer\">Advance a Block Pointer</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#final-result\">Final Result</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#unit-test\">Unit Test</a></li>\n+</ul>\n+</li>\n+</ul>\n+</li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n+</ul>\n+\n+        </div>\n+      </div>\n+    </nav>\n+\n+    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n+          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n+          <a href=\"../../index.html\">Triton</a>\n+      </nav>\n+\n+      <div class=\"wy-nav-content\">\n+        <div class=\"rst-content\">\n+          <div role=\"navigation\" aria-label=\"Page navigation\">\n+  <ul class=\"wy-breadcrumbs\">\n+      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n+          <li class=\"breadcrumb-item\"><a href=\"index.html\">Tutorials</a></li>\n+      <li class=\"breadcrumb-item active\">Block Pointer (Experimental)</li>\n+      <li class=\"wy-breadcrumbs-aside\">\n+            <a href=\"../../_sources/getting-started/tutorials/08-experimental-block-pointer.rst.txt\" rel=\"nofollow\"> View page source</a>\n+      </li>\n+  </ul>\n+  <hr/>\n+</div>\n+          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n+           <div itemprop=\"articleBody\">\n+             \n+  <div class=\"sphx-glr-download-link-note admonition note\">\n+<p class=\"admonition-title\">Note</p>\n+<p><a class=\"reference internal\" href=\"#sphx-glr-download-getting-started-tutorials-08-experimental-block-pointer-py\"><span class=\"std std-ref\">Go to the end</span></a>\n+to download the full example code</p>\n+</div>\n+<section class=\"sphx-glr-example-title\" id=\"block-pointer-experimental\">\n+<span id=\"sphx-glr-getting-started-tutorials-08-experimental-block-pointer-py\"></span><h1>Block Pointer (Experimental)<a class=\"headerlink\" href=\"#block-pointer-experimental\" title=\"Permalink to this heading\">\u00b6</a></h1>\n+<p>This tutorial will guide you through writing a matrix multiplication algorithm that utilizes block pointer semantics.\n+These semantics are more friendly for Triton to optimize and can result in better performance on specific hardware.\n+Note that this feature is still experimental and may change in the future.</p>\n+<section id=\"motivations\">\n+<h2>Motivations<a class=\"headerlink\" href=\"#motivations\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>In the previous matrix multiplication tutorial, we constructed blocks of values by de-referencing blocks of pointers,\n+i.e., <code class=\"code docutils literal notranslate\"><span class=\"pre\">load(block&lt;pointer_type&lt;element_type&gt;&gt;)</span> <span class=\"pre\">-&gt;</span> <span class=\"pre\">block&lt;element_type&gt;</span></code>, which involved loading blocks of\n+elements from memory. This approach allowed for flexibility in using hardware-managed cache and implementing complex\n+data structures, such as tensors of trees or unstructured look-up tables.</p>\n+<p>However, the drawback of this approach is that it relies heavily on complex optimization passes by the compiler to\n+optimize memory access patterns. This can result in brittle code that may suffer from performance degradation when the\n+optimizer fails to perform adequately. Additionally, as memory controllers specialize to accommodate dense spatial\n+data structures commonly used in machine learning workloads, this problem is likely to worsen.</p>\n+<p>To address this issue, we will use block pointers <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer_type&lt;block&lt;element_type&gt;&gt;</span></code> and load them into\n+<code class=\"code docutils literal notranslate\"><span class=\"pre\">block&lt;element_type&gt;</span></code>, in which way gives better friendliness for the compiler to optimize memory access\n+patterns.</p>\n+<p>Let\u2019s start with the previous matrix multiplication example and demonstrate how to rewrite it to utilize block pointer\n+semantics.</p>\n+</section>\n+<section id=\"make-a-block-pointer\">\n+<h2>Make a Block Pointer<a class=\"headerlink\" href=\"#make-a-block-pointer\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>A block pointer pointers to a block in a parent tensor and is constructed by <code class=\"code docutils literal notranslate\"><span class=\"pre\">make_block_ptr</span></code> function,\n+which takes the following information as arguments:\n+- <code class=\"code docutils literal notranslate\"><span class=\"pre\">base</span></code>: the base pointer to the parent tensor;\n+- <code class=\"code docutils literal notranslate\"><span class=\"pre\">shape</span></code>: the shape of the parent tensor;\n+- <code class=\"code docutils literal notranslate\"><span class=\"pre\">strides</span></code>: the strides of the parent tensor, which means how much to increase the pointer by when moving by 1 element in a specific axis;\n+- <code class=\"code docutils literal notranslate\"><span class=\"pre\">offsets</span></code>: the offsets of the block;\n+- <code class=\"code docutils literal notranslate\"><span class=\"pre\">block_shape</span></code>: the shape of the block;\n+- <code class=\"code docutils literal notranslate\"><span class=\"pre\">order</span></code>: the order of the block, which means how the block is laid out in memory.</p>\n+<p>For example, to a block pointer to a <code class=\"code docutils literal notranslate\"><span class=\"pre\">BLOCK_SIZE_M`x:code:`BLOCK_SIZE_K</span></code> block in a row-major 2D matrix A by\n+offsets <code class=\"code docutils literal notranslate\"><span class=\"pre\">(pid_m</span> <span class=\"pre\">*</span> <span class=\"pre\">BLOCK_SIZE_M,</span> <span class=\"pre\">0)</span></code> and strides <code class=\"code docutils literal notranslate\"><span class=\"pre\">(stride_am,</span> <span class=\"pre\">stride_ak)</span></code>, we can use the following code\n+(exactly the same as the previous matrix multiplication tutorial):</p>\n+<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">a_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span><span class=\"n\">base</span><span class=\"o\">=</span><span class=\"n\">a_ptr</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">),</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_am</span><span class=\"p\">,</span> <span class=\"n\">stride_ak</span><span class=\"p\">),</span>\n+                                <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">pid_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">),</span>\n+                                <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n+</pre></div>\n+</div>\n+<p>Note that the <code class=\"code docutils literal notranslate\"><span class=\"pre\">order</span></code> argument is set to <code class=\"code docutils literal notranslate\"><span class=\"pre\">(1,</span> <span class=\"pre\">0)</span></code>, which means the second axis is the inner dimension in\n+terms of storage, and the first axis is the outer dimension. This information may sound redundant, but it is necessary\n+for some hardware backends to optimize for better performance.</p>\n+</section>\n+<section id=\"load-store-a-block-pointer\">\n+<h2>Load/Store a Block Pointer<a class=\"headerlink\" href=\"#load-store-a-block-pointer\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>To load/store a block pointer, we can use <code class=\"code docutils literal notranslate\"><span class=\"pre\">load/store</span></code> function, which takes a block pointer as an argument,\n+de-references it, and loads/stores a block. You may mask some values in the block, here we have an extra argument\n+<code class=\"code docutils literal notranslate\"><span class=\"pre\">boundary_check</span></code> to specify whether to check the boundary of each axis for the block pointer. With check on and\n+out-of-bound values will be masked according to the <code class=\"code docutils literal notranslate\"><span class=\"pre\">padding_option</span></code> argument (load only), which can be\n+<code class=\"code docutils literal notranslate\"><span class=\"pre\">zero</span></code> or <code class=\"code docutils literal notranslate\"><span class=\"pre\">nan</span></code>. Temporarily, we do not support other values due to some hardware limitations. In this\n+mode of block pointer load/store does not support <code class=\"code docutils literal notranslate\"><span class=\"pre\">mask</span></code> or <code class=\"code docutils literal notranslate\"><span class=\"pre\">other</span></code> arguments in the legacy mode.</p>\n+<p>So to load a block in A, we can simply write <code class=\"code docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">=</span> <span class=\"pre\">tl.load(a_block_ptr,</span> <span class=\"pre\">boundary_check=(0,</span> <span class=\"pre\">1))</span></code>. Boundary check\n+may cost extra performance, so if you can guarantee that the block pointer is always in-bound in some axis, you can\n+turn off the check by not passing the index into the <code class=\"code docutils literal notranslate\"><span class=\"pre\">boundary_check</span></code> argument. For example, if we know that\n+<code class=\"code docutils literal notranslate\"><span class=\"pre\">M</span></code> is a multiple of <code class=\"code docutils literal notranslate\"><span class=\"pre\">BLOCK_SIZE_M</span></code>, we can write <code class=\"code docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">=</span> <span class=\"pre\">tl.load(a_block_ptr,</span> <span class=\"pre\">boundary_check=(1,</span> <span class=\"pre\">))</span></code>.</p>\n+</section>\n+<section id=\"advance-a-block-pointer\">\n+<h2>Advance a Block Pointer<a class=\"headerlink\" href=\"#advance-a-block-pointer\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>To advance a block pointer, we can use <code class=\"code docutils literal notranslate\"><span class=\"pre\">advance</span></code> function, which takes a block pointer and the increment for\n+each axis as arguments and returns a new block pointer with the same shape and strides as the original one,\n+but with the offsets advanced by the specified amount.</p>\n+<p>For example, to advance the block pointer by <code class=\"code docutils literal notranslate\"><span class=\"pre\">BLOCK_SIZE_K</span></code> in the second axis\n+(no need to multiply with stride), we can write <code class=\"code docutils literal notranslate\"><span class=\"pre\">a_block_ptr</span> <span class=\"pre\">=</span> <span class=\"pre\">tl.advance(a_block_ptr,</span> <span class=\"pre\">(0,</span> <span class=\"pre\">BLOCK_SIZE_K))</span></code>.</p>\n+</section>\n+<section id=\"final-result\">\n+<h2>Final Result<a class=\"headerlink\" href=\"#final-result\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n+\n+<span class=\"kn\">import</span> <span class=\"nn\">triton</span>\n+<span class=\"kn\">import</span> <span class=\"nn\">triton.language</span> <span class=\"k\">as</span> <span class=\"nn\">tl</span>\n+\n+\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">autotune</span><span class=\"p\">(</span>\n+    <span class=\"n\">configs</span><span class=\"o\">=</span><span class=\"p\">[</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n+    <span class=\"p\">],</span>\n+    <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;M&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;K&#39;</span><span class=\"p\">],</span>\n+<span class=\"p\">)</span>\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n+<span class=\"k\">def</span> <span class=\"nf\">matmul_kernel_with_block_pointers</span><span class=\"p\">(</span>\n+        <span class=\"c1\"># Pointers to matrices</span>\n+        <span class=\"n\">a_ptr</span><span class=\"p\">,</span> <span class=\"n\">b_ptr</span><span class=\"p\">,</span> <span class=\"n\">c_ptr</span><span class=\"p\">,</span>\n+        <span class=\"c1\"># Matrix dimensions</span>\n+        <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span>\n+        <span class=\"c1\"># The stride variables represent how much to increase the ptr by when moving by 1</span>\n+        <span class=\"c1\"># element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`</span>\n+        <span class=\"c1\"># by to get the element one row down (A has M rows).</span>\n+        <span class=\"n\">stride_am</span><span class=\"p\">,</span> <span class=\"n\">stride_ak</span><span class=\"p\">,</span>\n+        <span class=\"n\">stride_bk</span><span class=\"p\">,</span> <span class=\"n\">stride_bn</span><span class=\"p\">,</span>\n+        <span class=\"n\">stride_cm</span><span class=\"p\">,</span> <span class=\"n\">stride_cn</span><span class=\"p\">,</span>\n+        <span class=\"c1\"># Meta-parameters</span>\n+        <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+        <span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span>\n+<span class=\"p\">):</span>\n+<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Kernel for computing the matmul C = A x B.</span>\n+<span class=\"sd\">    A has shape (M, K), B has shape (K, N) and C has shape (M, N)</span>\n+<span class=\"sd\">    &quot;&quot;&quot;</span>\n+    <span class=\"c1\"># -----------------------------------------------------------</span>\n+    <span class=\"c1\"># Map program ids `pid` to the block of C it should compute.</span>\n+    <span class=\"c1\"># This is done in a grouped ordering to promote L2 data reuse.</span>\n+    <span class=\"c1\"># See the matrix multiplication tutorial for details.</span>\n+    <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"n\">num_pid_m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">)</span>\n+    <span class=\"n\">num_pid_n</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)</span>\n+    <span class=\"n\">num_pid_in_group</span> <span class=\"o\">=</span> <span class=\"n\">GROUP_SIZE_M</span> <span class=\"o\">*</span> <span class=\"n\">num_pid_n</span>\n+    <span class=\"n\">group_id</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">//</span> <span class=\"n\">num_pid_in_group</span>\n+    <span class=\"n\">first_pid_m</span> <span class=\"o\">=</span> <span class=\"n\">group_id</span> <span class=\"o\">*</span> <span class=\"n\">GROUP_SIZE_M</span>\n+    <span class=\"n\">group_size_m</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">num_pid_m</span> <span class=\"o\">-</span> <span class=\"n\">first_pid_m</span><span class=\"p\">,</span> <span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">)</span>\n+    <span class=\"n\">pid_m</span> <span class=\"o\">=</span> <span class=\"n\">first_pid_m</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">pid</span> <span class=\"o\">%</span> <span class=\"n\">group_size_m</span><span class=\"p\">)</span>\n+    <span class=\"n\">pid_n</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">pid</span> <span class=\"o\">%</span> <span class=\"n\">num_pid_in_group</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"n\">group_size_m</span>\n+\n+    <span class=\"c1\"># ----------------------------------------------------------</span>\n+    <span class=\"c1\"># Create block pointers for the first blocks of A and B.</span>\n+    <span class=\"c1\"># We will advance this pointer as we move in the K direction and accumulate.</span>\n+    <span class=\"c1\"># See above `Make a Block Pointer` section for details.</span>\n+    <span class=\"n\">a_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span><span class=\"n\">base</span><span class=\"o\">=</span><span class=\"n\">a_ptr</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">),</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_am</span><span class=\"p\">,</span> <span class=\"n\">stride_ak</span><span class=\"p\">),</span>\n+                                    <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">pid_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">),</span>\n+                                    <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n+    <span class=\"n\">b_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span><span class=\"n\">base</span><span class=\"o\">=</span><span class=\"n\">b_ptr</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">),</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_bk</span><span class=\"p\">,</span> <span class=\"n\">stride_bn</span><span class=\"p\">),</span>\n+                                    <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">pid_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span> <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span>\n+                                    <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n+\n+    <span class=\"c1\"># -----------------------------------------------------------</span>\n+    <span class=\"c1\"># Iterate to compute a block of the C matrix.</span>\n+    <span class=\"c1\"># We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block.</span>\n+    <span class=\"c1\"># of fp32 values for higher accuracy.</span>\n+    <span class=\"c1\"># `accumulator` will be converted back to fp16 after the loop.</span>\n+    <span class=\"n\">accumulator</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">):</span>\n+        <span class=\"c1\"># Load with boundary checks, no need to calculate the mask manually.</span>\n+        <span class=\"c1\"># For better performance, you may remove some axis from the boundary</span>\n+        <span class=\"c1\"># check, if you can guarantee that the access is always in-bound in</span>\n+        <span class=\"c1\"># that axis.</span>\n+        <span class=\"c1\"># See above `Load/Store a Block Pointer` section for details.</span>\n+        <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">a_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n+        <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">b_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n+        <span class=\"c1\"># We accumulate along the K dimension.</span>\n+        <span class=\"n\">accumulator</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># Advance the block pointer to the next K block.</span>\n+        <span class=\"c1\"># See above `Advance a Block Pointer` section for details.</span>\n+        <span class=\"n\">a_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">advance</span><span class=\"p\">(</span><span class=\"n\">a_block_ptr</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">))</span>\n+        <span class=\"n\">b_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">advance</span><span class=\"p\">(</span><span class=\"n\">b_block_ptr</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n+    <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">accumulator</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n+\n+    <span class=\"c1\"># ----------------------------------------------------------------</span>\n+    <span class=\"c1\"># Write back the block of the output matrix C with boundary checks.</span>\n+    <span class=\"c1\"># See above `Load/Store a Block Pointer` section for details.</span>\n+    <span class=\"n\">c_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span><span class=\"n\">base</span><span class=\"o\">=</span><span class=\"n\">c_ptr</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">),</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_cm</span><span class=\"p\">,</span> <span class=\"n\">stride_cn</span><span class=\"p\">),</span>\n+                                    <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">pid_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">pid_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span>\n+                                    <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span> <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">c_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n+\n+\n+<span class=\"c1\"># We can now create a convenience wrapper function that only takes two input tensors,</span>\n+<span class=\"c1\"># and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.</span>\n+<span class=\"k\">def</span> <span class=\"nf\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">):</span>\n+    <span class=\"c1\"># Check constraints.</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"s2\">&quot;Incompatible dimensions&quot;</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">is_contiguous</span><span class=\"p\">(),</span> <span class=\"s2\">&quot;Matrix A must be contiguous&quot;</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">is_contiguous</span><span class=\"p\">(),</span> <span class=\"s2\">&quot;Matrix B must be contiguous&quot;</span>\n+    <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">K</span> <span class=\"o\">=</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n+    <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n+    <span class=\"c1\"># Allocates output.</span>\n+    <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># 1D launch kernel where each block gets its own program.</span>\n+    <span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">META</span><span class=\"p\">:</span> <span class=\"p\">(</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">META</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">META</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">]),</span>\n+    <span class=\"p\">)</span>\n+    <span class=\"n\">matmul_kernel_with_block_pointers</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span>\n+        <span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">,</span>\n+        <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span>\n+        <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n+        <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n+        <span class=\"n\">c</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">c</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n+    <span class=\"p\">)</span>\n+    <span class=\"k\">return</span> <span class=\"n\">c</span>\n+</pre></div>\n+</div>\n+</section>\n+<section id=\"unit-test\">\n+<h2>Unit Test<a class=\"headerlink\" href=\"#unit-test\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>Still we can test our matrix multiplication with block pointers against a native torch implementation (i.e., cuBLAS).</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+<span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n+<span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n+<span class=\"n\">triton_output</span> <span class=\"o\">=</span> <span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n+<span class=\"n\">torch_output</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n+<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;triton_output=</span><span class=\"si\">{</span><span class=\"n\">triton_output</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n+<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;torch_output=</span><span class=\"si\">{</span><span class=\"n\">torch_output</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n+<span class=\"k\">if</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">triton_output</span><span class=\"p\">,</span> <span class=\"n\">torch_output</span><span class=\"p\">,</span> <span class=\"n\">atol</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">,</span> <span class=\"n\">rtol</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">):</span>\n+    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;\u2705 Triton and Torch match&quot;</span><span class=\"p\">)</span>\n+<span class=\"k\">else</span><span class=\"p\">:</span>\n+    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;\u274c Triton and Torch differ&quot;</span><span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+<div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>triton_output=tensor([[-10.9531,  -4.7109,  15.6953,  ..., -28.4062,   4.3320, -26.4219],\n+        [ 26.8438,  10.0469,  -5.4297,  ..., -11.2969,  -8.5312,  30.7500],\n+        [-13.2578,  15.8516,  18.0781,  ..., -21.7656,  -8.6406,  10.2031],\n+        ...,\n+        [ 40.2812,  18.6094, -25.6094,  ...,  -2.7598,  -3.2441,  41.0000],\n+        [ -6.1211, -16.8281,   4.4844,  ..., -21.0312,  24.7031,  15.0234],\n+        [-17.0938, -19.0000,  -0.3831,  ...,  21.5469, -30.2344, -13.2188]],\n+       device=&#39;cuda:0&#39;, dtype=torch.float16)\n+torch_output=tensor([[-10.9531,  -4.7109,  15.6953,  ..., -28.4062,   4.3320, -26.4219],\n+        [ 26.8438,  10.0469,  -5.4297,  ..., -11.2969,  -8.5312,  30.7500],\n+        [-13.2578,  15.8516,  18.0781,  ..., -21.7656,  -8.6406,  10.2031],\n+        ...,\n+        [ 40.2812,  18.6094, -25.6094,  ...,  -2.7598,  -3.2441,  41.0000],\n+        [ -6.1211, -16.8281,   4.4844,  ..., -21.0312,  24.7031,  15.0234],\n+        [-17.0938, -19.0000,  -0.3831,  ...,  21.5469, -30.2344, -13.2188]],\n+       device=&#39;cuda:0&#39;, dtype=torch.float16)\n+\u2705 Triton and Torch match\n+</pre></div>\n+</div>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  9.969 seconds)</p>\n+<div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-08-experimental-block-pointer-py\">\n+<div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/4d6052117d61c2ca779cd4b75567fee5/08-experimental-block-pointer.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">08-experimental-block-pointer.py</span></code></a></p>\n+</div>\n+<div class=\"sphx-glr-download sphx-glr-download-jupyter docutils container\">\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/9705af6f33c6e66c6fa78f2394331536/08-experimental-block-pointer.ipynb\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Jupyter</span> <span class=\"pre\">notebook:</span> <span class=\"pre\">08-experimental-block-pointer.ipynb</span></code></a></p>\n+</div>\n+</div>\n+<p class=\"sphx-glr-signature\"><a class=\"reference external\" href=\"https://sphinx-gallery.github.io\">Gallery generated by Sphinx-Gallery</a></p>\n+</section>\n+</section>\n+\n+\n+           </div>\n+          </div>\n+          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n+        <a href=\"07-math-functions.html\" class=\"btn btn-neutral float-left\" title=\"Libdevice (tl.math) function\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"../../python-api/triton.html\" class=\"btn btn-neutral float-right\" title=\"triton\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+    </div>\n+\n+  <hr/>\n+\n+  <div role=\"contentinfo\">\n+    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n+  </div>\n+\n+  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n+    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n+    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n+   \n+\n+</footer>\n+        </div>\n+      </div>\n+    </section>\n+  </div>\n+  \n+<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n+    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n+        <span class=\"fa fa-book\"> Other Versions</span>\n+        v: main\n+        <span class=\"fa fa-caret-down\"></span>\n+    </span>\n+    <div class=\"rst-other-versions\">\n+        <dl>\n+            <dt>Branches</dt>\n+            <dd><a href=\"08-experimental-block-pointer.html\">main</a></dd>\n+        </dl>\n+    </div>\n+</div><script>\n+      jQuery(function () {\n+          SphinxRtdTheme.Navigation.enable(true);\n+      });\n+  </script> \n+\n+</body>\n+</html>\n\\ No newline at end of file"}, {"filename": "main/getting-started/tutorials/index.html", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "file_content_changes": "@@ -56,6 +56,7 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"08-experimental-block-pointer.html\">Block Pointer (Experimental)</a></li>\n </ul>\n </li>\n </ul>\n@@ -109,13 +110,13 @@ <h1>Tutorials<a class=\"headerlink\" href=\"#tutorials\" title=\"Permalink to this he\n </div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"In this tutorial, you will write a fused softmax operation that is significantly faster than Py...\"><img alt=\"\" src=\"../../_images/sphx_glr_02-fused-softmax_thumb.png\" />\n <p><a class=\"reference internal\" href=\"02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py\"><span class=\"std std-ref\">Fused Softmax</span></a></p>\n   <div class=\"sphx-glr-thumbnail-title\">Fused Softmax</div>\n-</div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"In doing so, you will learn about: - Block-level matrix multiplications - Multi-dimensional poi...\"><img alt=\"\" src=\"../../_images/sphx_glr_03-matrix-multiplication_thumb.png\" />\n+</div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"You will specifically learn about:\"><img alt=\"\" src=\"../../_images/sphx_glr_03-matrix-multiplication_thumb.png\" />\n <p><a class=\"reference internal\" href=\"03-matrix-multiplication.html#sphx-glr-getting-started-tutorials-03-matrix-multiplication-py\"><span class=\"std std-ref\">Matrix Multiplication</span></a></p>\n   <div class=\"sphx-glr-thumbnail-title\">Matrix Multiplication</div>\n </div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"In this tutorial, you will write a memory-efficient implementation of dropout whose state will ...\"><img alt=\"\" src=\"../../_images/sphx_glr_04-low-memory-dropout_thumb.png\" />\n <p><a class=\"reference internal\" href=\"04-low-memory-dropout.html#sphx-glr-getting-started-tutorials-04-low-memory-dropout-py\"><span class=\"std std-ref\">Low-Memory Dropout</span></a></p>\n   <div class=\"sphx-glr-thumbnail-title\">Low-Memory Dropout</div>\n-</div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"In doing so, you will learn about: - Implementing backward pass in Triton - Implementing parall...\"><img alt=\"\" src=\"../../_images/sphx_glr_05-layer-norm_thumb.png\" />\n+</div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"In doing so, you will learn about:\"><img alt=\"\" src=\"../../_images/sphx_glr_05-layer-norm_thumb.png\" />\n <p><a class=\"reference internal\" href=\"05-layer-norm.html#sphx-glr-getting-started-tutorials-05-layer-norm-py\"><span class=\"std std-ref\">Layer Normalization</span></a></p>\n   <div class=\"sphx-glr-thumbnail-title\">Layer Normalization</div>\n </div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"This is a Triton implementation of the Flash Attention algorithm (see: Dao et al., https://arxi...\"><img alt=\"\" src=\"../../_images/sphx_glr_06-fused-attention_thumb.png\" />\n@@ -124,6 +125,9 @@ <h1>Tutorials<a class=\"headerlink\" href=\"#tutorials\" title=\"Permalink to this he\n </div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"Libdevice (`tl.math`) function\"><img alt=\"\" src=\"../../_images/sphx_glr_07-math-functions_thumb.png\" />\n <p><a class=\"reference internal\" href=\"07-math-functions.html#sphx-glr-getting-started-tutorials-07-math-functions-py\"><span class=\"std std-ref\">Libdevice (tl.math) function</span></a></p>\n   <div class=\"sphx-glr-thumbnail-title\">Libdevice (`tl.math`) function</div>\n+</div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"Block Pointer (Experimental)\"><img alt=\"\" src=\"../../_images/sphx_glr_08-experimental-block-pointer_thumb.png\" />\n+<p><a class=\"reference internal\" href=\"08-experimental-block-pointer.html#sphx-glr-getting-started-tutorials-08-experimental-block-pointer-py\"><span class=\"std std-ref\">Block Pointer (Experimental)</span></a></p>\n+  <div class=\"sphx-glr-thumbnail-title\">Block Pointer (Experimental)</div>\n </div></div><div class=\"toctree-wrapper compound\">\n </div>\n <div class=\"sphx-glr-footer sphx-glr-footer-gallery docutils container\">"}, {"filename": "main/getting-started/tutorials/sg_execution_times.html", "status": "modified", "additions": 15, "deletions": 11, "changes": 26, "file_content_changes": "@@ -86,35 +86,39 @@\n              \n   <section id=\"computation-times\">\n <span id=\"sphx-glr-getting-started-tutorials-sg-execution-times\"></span><h1>Computation times<a class=\"headerlink\" href=\"#computation-times\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<p><strong>04:05.074</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n+<p><strong>04:12.435</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n <table class=\"docutils align-default\">\n <tbody>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"03-matrix-multiplication.html#sphx-glr-getting-started-tutorials-03-matrix-multiplication-py\"><span class=\"std std-ref\">Matrix Multiplication</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">03-matrix-multiplication.py</span></code>)</p></td>\n-<td><p>01:36.046</p></td>\n+<td><p>01:36.499</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py\"><span class=\"std std-ref\">Fused Softmax</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">02-fused-softmax.py</span></code>)</p></td>\n-<td><p>01:18.799</p></td>\n+<td><p>01:17.617</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"05-layer-norm.html#sphx-glr-getting-started-tutorials-05-layer-norm-py\"><span class=\"std std-ref\">Layer Normalization</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">05-layer-norm.py</span></code>)</p></td>\n-<td><p>00:39.016</p></td>\n+<td><p>00:38.028</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py\"><span class=\"std std-ref\">Vector Addition</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">01-vector-add.py</span></code>)</p></td>\n-<td><p>00:24.188</p></td>\n+<td><p>00:24.232</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n-<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"06-fused-attention.html#sphx-glr-getting-started-tutorials-06-fused-attention-py\"><span class=\"std std-ref\">Fused Attention</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">06-fused-attention.py</span></code>)</p></td>\n-<td><p>00:05.598</p></td>\n+<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"08-experimental-block-pointer.html#sphx-glr-getting-started-tutorials-08-experimental-block-pointer-py\"><span class=\"std std-ref\">Block Pointer (Experimental)</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">08-experimental-block-pointer.py</span></code>)</p></td>\n+<td><p>00:09.969</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n-<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"04-low-memory-dropout.html#sphx-glr-getting-started-tutorials-04-low-memory-dropout-py\"><span class=\"std std-ref\">Low-Memory Dropout</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">04-low-memory-dropout.py</span></code>)</p></td>\n-<td><p>00:01.048</p></td>\n+<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"06-fused-attention.html#sphx-glr-getting-started-tutorials-06-fused-attention-py\"><span class=\"std std-ref\">Fused Attention</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">06-fused-attention.py</span></code>)</p></td>\n+<td><p>00:05.120</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n-<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"07-math-functions.html#sphx-glr-getting-started-tutorials-07-math-functions-py\"><span class=\"std std-ref\">Libdevice (tl.math) function</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">07-math-functions.py</span></code>)</p></td>\n-<td><p>00:00.380</p></td>\n+<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"04-low-memory-dropout.html#sphx-glr-getting-started-tutorials-04-low-memory-dropout-py\"><span class=\"std std-ref\">Low-Memory Dropout</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">04-low-memory-dropout.py</span></code>)</p></td>\n+<td><p>00:00.707</p></td>\n+<td><p>0.0 MB</p></td>\n+</tr>\n+<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"07-math-functions.html#sphx-glr-getting-started-tutorials-07-math-functions-py\"><span class=\"std std-ref\">Libdevice (tl.math) function</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">07-math-functions.py</span></code>)</p></td>\n+<td><p>00:00.264</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n </tbody>"}, {"filename": "main/objects.inv", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.argmax.html", "status": "added", "additions": 178, "deletions": 0, "changes": 178, "file_content_changes": "@@ -0,0 +1,178 @@\n+<!DOCTYPE html>\n+<html class=\"writer-html5\" lang=\"en\" >\n+<head>\n+  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n+\n+  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n+  <title>triton.language.argmax &mdash; Triton  documentation</title>\n+      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n+  <!--[if lt IE 9]>\n+    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n+  <![endif]-->\n+  \n+        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n+        <script src=\"../../_static/doctools.js\"></script>\n+        <script src=\"../../_static/sphinx_highlight.js\"></script>\n+    <script src=\"../../_static/js/theme.js\"></script>\n+    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n+    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n+    <link rel=\"next\" title=\"triton.language.argmin\" href=\"triton.language.argmin.html\" />\n+    <link rel=\"prev\" title=\"triton.language.softmax\" href=\"triton.language.softmax.html\" /> \n+</head>\n+\n+<body class=\"wy-body-for-nav\"> \n+  <div class=\"wy-grid-for-nav\">\n+    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n+      <div class=\"wy-side-scroll\">\n+        <div class=\"wy-side-nav-search\" >\n+\n+          \n+          \n+          <a href=\"../../index.html\" class=\"icon icon-home\">\n+            Triton\n+          </a>\n+<div role=\"search\">\n+  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n+    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n+    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n+    <input type=\"hidden\" name=\"area\" value=\"default\" />\n+  </form>\n+</div>\n+        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n+              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/installation.html\">Installation</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/tutorials/index.html\">Tutorials</a></li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n+<ul class=\"current\">\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.html\">triton</a></li>\n+<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"../triton.language.html\">triton.language</a><ul class=\"current\">\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#programming-model\">Programming Model</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#creation-ops\">Creation Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#shape-manipulation-ops\">Shape Manipulation Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#linear-algebra-ops\">Linear Algebra Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a></li>\n+<li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#reduction-ops\">Reduction Ops</a><ul class=\"current\">\n+<li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.argmax</a><ul>\n+<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#triton.language.argmax\"><code class=\"docutils literal notranslate\"><span class=\"pre\">argmax()</span></code></a></li>\n+</ul>\n+</li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.argmin.html\">triton.language.argmin</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.max.html\">triton.language.max</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.min.html\">triton.language.min</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.reduce.html\">triton.language.reduce</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.sum.html\">triton.language.sum</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.xor_sum.html\">triton.language.xor_sum</a></li>\n+</ul>\n+</li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#atomic-ops\">Atomic Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#comparison-ops\">Comparison ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#random-number-generation\">Random Number Generation</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#compiler-hint-ops\">Compiler Hint Ops</a></li>\n+</ul>\n+</li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.testing.html\">triton.testing</a></li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n+</ul>\n+\n+        </div>\n+      </div>\n+    </nav>\n+\n+    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n+          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n+          <a href=\"../../index.html\">Triton</a>\n+      </nav>\n+\n+      <div class=\"wy-nav-content\">\n+        <div class=\"rst-content\">\n+          <div role=\"navigation\" aria-label=\"Page navigation\">\n+  <ul class=\"wy-breadcrumbs\">\n+      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n+          <li class=\"breadcrumb-item\"><a href=\"../triton.language.html\">triton.language</a></li>\n+      <li class=\"breadcrumb-item active\">triton.language.argmax</li>\n+      <li class=\"wy-breadcrumbs-aside\">\n+            <a href=\"../../_sources/python-api/generated/triton.language.argmax.rst.txt\" rel=\"nofollow\"> View page source</a>\n+      </li>\n+  </ul>\n+  <hr/>\n+</div>\n+          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n+           <div itemprop=\"articleBody\">\n+             \n+  <section id=\"triton-language-argmax\">\n+<h1>triton.language.argmax<a class=\"headerlink\" href=\"#triton-language-argmax\" title=\"Permalink to this heading\">\u00b6</a></h1>\n+<dl class=\"py function\">\n+<dt class=\"sig sig-object py\" id=\"triton.language.argmax\">\n+<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">argmax</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">input</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">axis</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.argmax\" title=\"Permalink to this definition\">\u00b6</a></dt>\n+<dd><p>Returns the maximum index of all elements in the <code class=\"code docutils literal notranslate\"><span class=\"pre\">input</span></code> tensor along the provided <code class=\"code docutils literal notranslate\"><span class=\"pre\">axis</span></code></p>\n+<dl class=\"field-list simple\">\n+<dt class=\"field-odd\">Parameters<span class=\"colon\">:</span></dt>\n+<dd class=\"field-odd\"><ul class=\"simple\">\n+<li><p><strong>input</strong> \u2013 the input values</p></li>\n+<li><p><strong>axis</strong> \u2013 the dimension along which the reduction should be done</p></li>\n+</ul>\n+</dd>\n+</dl>\n+</dd></dl>\n+\n+</section>\n+\n+\n+           </div>\n+          </div>\n+          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n+        <a href=\"triton.language.softmax.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.softmax\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"triton.language.argmin.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.argmin\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+    </div>\n+\n+  <hr/>\n+\n+  <div role=\"contentinfo\">\n+    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n+  </div>\n+\n+  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n+    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n+    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n+   \n+\n+</footer>\n+        </div>\n+      </div>\n+    </section>\n+  </div>\n+  \n+<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n+    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n+        <span class=\"fa fa-book\"> Other Versions</span>\n+        v: main\n+        <span class=\"fa fa-caret-down\"></span>\n+    </span>\n+    <div class=\"rst-other-versions\">\n+        <dl>\n+            <dt>Branches</dt>\n+            <dd><a href=\"triton.language.argmax.html\">main</a></dd>\n+        </dl>\n+    </div>\n+</div><script>\n+      jQuery(function () {\n+          SphinxRtdTheme.Navigation.enable(true);\n+      });\n+  </script> \n+\n+</body>\n+</html>\n\\ No newline at end of file"}, {"filename": "main/python-api/generated/triton.language.argmin.html", "status": "added", "additions": 178, "deletions": 0, "changes": 178, "file_content_changes": "@@ -0,0 +1,178 @@\n+<!DOCTYPE html>\n+<html class=\"writer-html5\" lang=\"en\" >\n+<head>\n+  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n+\n+  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n+  <title>triton.language.argmin &mdash; Triton  documentation</title>\n+      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n+  <!--[if lt IE 9]>\n+    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n+  <![endif]-->\n+  \n+        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n+        <script src=\"../../_static/doctools.js\"></script>\n+        <script src=\"../../_static/sphinx_highlight.js\"></script>\n+    <script src=\"../../_static/js/theme.js\"></script>\n+    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n+    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n+    <link rel=\"next\" title=\"triton.language.max\" href=\"triton.language.max.html\" />\n+    <link rel=\"prev\" title=\"triton.language.argmax\" href=\"triton.language.argmax.html\" /> \n+</head>\n+\n+<body class=\"wy-body-for-nav\"> \n+  <div class=\"wy-grid-for-nav\">\n+    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n+      <div class=\"wy-side-scroll\">\n+        <div class=\"wy-side-nav-search\" >\n+\n+          \n+          \n+          <a href=\"../../index.html\" class=\"icon icon-home\">\n+            Triton\n+          </a>\n+<div role=\"search\">\n+  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n+    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n+    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n+    <input type=\"hidden\" name=\"area\" value=\"default\" />\n+  </form>\n+</div>\n+        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n+              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/installation.html\">Installation</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/tutorials/index.html\">Tutorials</a></li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n+<ul class=\"current\">\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.html\">triton</a></li>\n+<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"../triton.language.html\">triton.language</a><ul class=\"current\">\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#programming-model\">Programming Model</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#creation-ops\">Creation Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#shape-manipulation-ops\">Shape Manipulation Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#linear-algebra-ops\">Linear Algebra Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a></li>\n+<li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#reduction-ops\">Reduction Ops</a><ul class=\"current\">\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.argmax.html\">triton.language.argmax</a></li>\n+<li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.argmin</a><ul>\n+<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#triton.language.argmin\"><code class=\"docutils literal notranslate\"><span class=\"pre\">argmin()</span></code></a></li>\n+</ul>\n+</li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.max.html\">triton.language.max</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.min.html\">triton.language.min</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.reduce.html\">triton.language.reduce</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.sum.html\">triton.language.sum</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.xor_sum.html\">triton.language.xor_sum</a></li>\n+</ul>\n+</li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#atomic-ops\">Atomic Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#comparison-ops\">Comparison ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#random-number-generation\">Random Number Generation</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#compiler-hint-ops\">Compiler Hint Ops</a></li>\n+</ul>\n+</li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.testing.html\">triton.testing</a></li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n+</ul>\n+\n+        </div>\n+      </div>\n+    </nav>\n+\n+    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n+          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n+          <a href=\"../../index.html\">Triton</a>\n+      </nav>\n+\n+      <div class=\"wy-nav-content\">\n+        <div class=\"rst-content\">\n+          <div role=\"navigation\" aria-label=\"Page navigation\">\n+  <ul class=\"wy-breadcrumbs\">\n+      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n+          <li class=\"breadcrumb-item\"><a href=\"../triton.language.html\">triton.language</a></li>\n+      <li class=\"breadcrumb-item active\">triton.language.argmin</li>\n+      <li class=\"wy-breadcrumbs-aside\">\n+            <a href=\"../../_sources/python-api/generated/triton.language.argmin.rst.txt\" rel=\"nofollow\"> View page source</a>\n+      </li>\n+  </ul>\n+  <hr/>\n+</div>\n+          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n+           <div itemprop=\"articleBody\">\n+             \n+  <section id=\"triton-language-argmin\">\n+<h1>triton.language.argmin<a class=\"headerlink\" href=\"#triton-language-argmin\" title=\"Permalink to this heading\">\u00b6</a></h1>\n+<dl class=\"py function\">\n+<dt class=\"sig sig-object py\" id=\"triton.language.argmin\">\n+<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">argmin</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">input</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">axis</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.argmin\" title=\"Permalink to this definition\">\u00b6</a></dt>\n+<dd><p>Returns the minimum index of all elements in the <code class=\"code docutils literal notranslate\"><span class=\"pre\">input</span></code> tensor along the provided <code class=\"code docutils literal notranslate\"><span class=\"pre\">axis</span></code></p>\n+<dl class=\"field-list simple\">\n+<dt class=\"field-odd\">Parameters<span class=\"colon\">:</span></dt>\n+<dd class=\"field-odd\"><ul class=\"simple\">\n+<li><p><strong>input</strong> \u2013 the input values</p></li>\n+<li><p><strong>axis</strong> \u2013 the dimension along which the reduction should be done</p></li>\n+</ul>\n+</dd>\n+</dl>\n+</dd></dl>\n+\n+</section>\n+\n+\n+           </div>\n+          </div>\n+          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n+        <a href=\"triton.language.argmax.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.argmax\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"triton.language.max.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.max\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+    </div>\n+\n+  <hr/>\n+\n+  <div role=\"contentinfo\">\n+    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n+  </div>\n+\n+  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n+    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n+    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n+   \n+\n+</footer>\n+        </div>\n+      </div>\n+    </section>\n+  </div>\n+  \n+<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n+    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n+        <span class=\"fa fa-book\"> Other Versions</span>\n+        v: main\n+        <span class=\"fa fa-caret-down\"></span>\n+    </span>\n+    <div class=\"rst-other-versions\">\n+        <dl>\n+            <dt>Branches</dt>\n+            <dd><a href=\"triton.language.argmin.html\">main</a></dd>\n+        </dl>\n+    </div>\n+</div><script>\n+      jQuery(function () {\n+          SphinxRtdTheme.Navigation.enable(true);\n+      });\n+  </script> \n+\n+</body>\n+</html>\n\\ No newline at end of file"}, {"filename": "main/python-api/generated/triton.language.atomic_add.html", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -23,7 +23,7 @@\n     <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n     <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n     <link rel=\"next\" title=\"triton.language.atomic_max\" href=\"triton.language.atomic_max.html\" />\n-    <link rel=\"prev\" title=\"triton.language.sum\" href=\"triton.language.sum.html\" /> \n+    <link rel=\"prev\" title=\"triton.language.xor_sum\" href=\"triton.language.xor_sum.html\" /> \n </head>\n \n <body class=\"wy-body-for-nav\"> \n@@ -134,7 +134,7 @@ <h1>triton.language.atomic_add<a class=\"headerlink\" href=\"#triton-language-atomi\n            </div>\n           </div>\n           <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"triton.language.sum.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.sum\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"triton.language.xor_sum.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.xor_sum\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n         <a href=\"triton.language.atomic_max.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.atomic_max\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n     </div>\n "}, {"filename": "main/python-api/generated/triton.language.max.html", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "file_content_changes": "@@ -23,7 +23,7 @@\n     <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n     <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n     <link rel=\"next\" title=\"triton.language.min\" href=\"triton.language.min.html\" />\n-    <link rel=\"prev\" title=\"triton.language.softmax\" href=\"triton.language.softmax.html\" /> \n+    <link rel=\"prev\" title=\"triton.language.argmin\" href=\"triton.language.argmin.html\" /> \n </head>\n \n <body class=\"wy-body-for-nav\"> \n@@ -62,12 +62,16 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a></li>\n <li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#reduction-ops\">Reduction Ops</a><ul class=\"current\">\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.argmax.html\">triton.language.argmax</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.argmin.html\">triton.language.argmin</a></li>\n <li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.max</a><ul>\n <li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#triton.language.max\"><code class=\"docutils literal notranslate\"><span class=\"pre\">max()</span></code></a></li>\n </ul>\n </li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.min.html\">triton.language.min</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.reduce.html\">triton.language.reduce</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.sum.html\">triton.language.sum</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.xor_sum.html\">triton.language.xor_sum</a></li>\n </ul>\n </li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#atomic-ops\">Atomic Ops</a></li>\n@@ -131,7 +135,7 @@ <h1>triton.language.max<a class=\"headerlink\" href=\"#triton-language-max\" title=\"\n            </div>\n           </div>\n           <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"triton.language.softmax.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.softmax\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"triton.language.argmin.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.argmin\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n         <a href=\"triton.language.min.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.min\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n     </div>\n "}, {"filename": "main/python-api/generated/triton.language.min.html", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "file_content_changes": "@@ -22,7 +22,7 @@\n     <script src=\"../../_static/js/theme.js\"></script>\n     <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n     <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton.language.sum\" href=\"triton.language.sum.html\" />\n+    <link rel=\"next\" title=\"triton.language.reduce\" href=\"triton.language.reduce.html\" />\n     <link rel=\"prev\" title=\"triton.language.max\" href=\"triton.language.max.html\" /> \n </head>\n \n@@ -62,12 +62,16 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a></li>\n <li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#reduction-ops\">Reduction Ops</a><ul class=\"current\">\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.argmax.html\">triton.language.argmax</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.argmin.html\">triton.language.argmin</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.max.html\">triton.language.max</a></li>\n <li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.min</a><ul>\n <li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#triton.language.min\"><code class=\"docutils literal notranslate\"><span class=\"pre\">min()</span></code></a></li>\n </ul>\n </li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.reduce.html\">triton.language.reduce</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.sum.html\">triton.language.sum</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.xor_sum.html\">triton.language.xor_sum</a></li>\n </ul>\n </li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#atomic-ops\">Atomic Ops</a></li>\n@@ -132,7 +136,7 @@ <h1>triton.language.min<a class=\"headerlink\" href=\"#triton-language-min\" title=\"\n           </div>\n           <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n         <a href=\"triton.language.max.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.max\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"triton.language.sum.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.sum\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+        <a href=\"triton.language.reduce.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.reduce\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n     </div>\n \n   <hr/>"}, {"filename": "main/python-api/generated/triton.language.reduce.html", "status": "added", "additions": 179, "deletions": 0, "changes": 179, "file_content_changes": "@@ -0,0 +1,179 @@\n+<!DOCTYPE html>\n+<html class=\"writer-html5\" lang=\"en\" >\n+<head>\n+  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n+\n+  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n+  <title>triton.language.reduce &mdash; Triton  documentation</title>\n+      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n+  <!--[if lt IE 9]>\n+    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n+  <![endif]-->\n+  \n+        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n+        <script src=\"../../_static/doctools.js\"></script>\n+        <script src=\"../../_static/sphinx_highlight.js\"></script>\n+    <script src=\"../../_static/js/theme.js\"></script>\n+    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n+    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n+    <link rel=\"next\" title=\"triton.language.sum\" href=\"triton.language.sum.html\" />\n+    <link rel=\"prev\" title=\"triton.language.min\" href=\"triton.language.min.html\" /> \n+</head>\n+\n+<body class=\"wy-body-for-nav\"> \n+  <div class=\"wy-grid-for-nav\">\n+    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n+      <div class=\"wy-side-scroll\">\n+        <div class=\"wy-side-nav-search\" >\n+\n+          \n+          \n+          <a href=\"../../index.html\" class=\"icon icon-home\">\n+            Triton\n+          </a>\n+<div role=\"search\">\n+  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n+    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n+    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n+    <input type=\"hidden\" name=\"area\" value=\"default\" />\n+  </form>\n+</div>\n+        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n+              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/installation.html\">Installation</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/tutorials/index.html\">Tutorials</a></li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n+<ul class=\"current\">\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.html\">triton</a></li>\n+<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"../triton.language.html\">triton.language</a><ul class=\"current\">\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#programming-model\">Programming Model</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#creation-ops\">Creation Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#shape-manipulation-ops\">Shape Manipulation Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#linear-algebra-ops\">Linear Algebra Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a></li>\n+<li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#reduction-ops\">Reduction Ops</a><ul class=\"current\">\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.argmax.html\">triton.language.argmax</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.argmin.html\">triton.language.argmin</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.max.html\">triton.language.max</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.min.html\">triton.language.min</a></li>\n+<li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.reduce</a><ul>\n+<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#triton.language.reduce\"><code class=\"docutils literal notranslate\"><span class=\"pre\">reduce()</span></code></a></li>\n+</ul>\n+</li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.sum.html\">triton.language.sum</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.xor_sum.html\">triton.language.xor_sum</a></li>\n+</ul>\n+</li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#atomic-ops\">Atomic Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#comparison-ops\">Comparison ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#random-number-generation\">Random Number Generation</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#compiler-hint-ops\">Compiler Hint Ops</a></li>\n+</ul>\n+</li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.testing.html\">triton.testing</a></li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n+</ul>\n+\n+        </div>\n+      </div>\n+    </nav>\n+\n+    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n+          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n+          <a href=\"../../index.html\">Triton</a>\n+      </nav>\n+\n+      <div class=\"wy-nav-content\">\n+        <div class=\"rst-content\">\n+          <div role=\"navigation\" aria-label=\"Page navigation\">\n+  <ul class=\"wy-breadcrumbs\">\n+      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n+          <li class=\"breadcrumb-item\"><a href=\"../triton.language.html\">triton.language</a></li>\n+      <li class=\"breadcrumb-item active\">triton.language.reduce</li>\n+      <li class=\"wy-breadcrumbs-aside\">\n+            <a href=\"../../_sources/python-api/generated/triton.language.reduce.rst.txt\" rel=\"nofollow\"> View page source</a>\n+      </li>\n+  </ul>\n+  <hr/>\n+</div>\n+          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n+           <div itemprop=\"articleBody\">\n+             \n+  <section id=\"triton-language-reduce\">\n+<h1>triton.language.reduce<a class=\"headerlink\" href=\"#triton-language-reduce\" title=\"Permalink to this heading\">\u00b6</a></h1>\n+<dl class=\"py function\">\n+<dt class=\"sig sig-object py\" id=\"triton.language.reduce\">\n+<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">reduce</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">input</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">axis</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">combine_fn</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.reduce\" title=\"Permalink to this definition\">\u00b6</a></dt>\n+<dd><p>Applies the combine_fn to all elements in <code class=\"code docutils literal notranslate\"><span class=\"pre\">input</span></code> tensors along the provided <code class=\"code docutils literal notranslate\"><span class=\"pre\">axis</span></code></p>\n+<dl class=\"field-list simple\">\n+<dt class=\"field-odd\">Parameters<span class=\"colon\">:</span></dt>\n+<dd class=\"field-odd\"><ul class=\"simple\">\n+<li><p><strong>input</strong> \u2013 the input tensor, or tuple of tensors</p></li>\n+<li><p><strong>axis</strong> \u2013 the dimension along which the reduction should be done</p></li>\n+<li><p><strong>combine_fn</strong> \u2013 a function to combine two groups of scalar tensors (must be marked with &#64;triton.jit)</p></li>\n+</ul>\n+</dd>\n+</dl>\n+</dd></dl>\n+\n+</section>\n+\n+\n+           </div>\n+          </div>\n+          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n+        <a href=\"triton.language.min.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.min\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"triton.language.sum.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.sum\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+    </div>\n+\n+  <hr/>\n+\n+  <div role=\"contentinfo\">\n+    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n+  </div>\n+\n+  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n+    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n+    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n+   \n+\n+</footer>\n+        </div>\n+      </div>\n+    </section>\n+  </div>\n+  \n+<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n+    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n+        <span class=\"fa fa-book\"> Other Versions</span>\n+        v: main\n+        <span class=\"fa fa-caret-down\"></span>\n+    </span>\n+    <div class=\"rst-other-versions\">\n+        <dl>\n+            <dt>Branches</dt>\n+            <dd><a href=\"triton.language.reduce.html\">main</a></dd>\n+        </dl>\n+    </div>\n+</div><script>\n+      jQuery(function () {\n+          SphinxRtdTheme.Navigation.enable(true);\n+      });\n+  </script> \n+\n+</body>\n+</html>\n\\ No newline at end of file"}, {"filename": "main/python-api/generated/triton.language.softmax.html", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -22,7 +22,7 @@\n     <script src=\"../../_static/js/theme.js\"></script>\n     <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n     <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton.language.max\" href=\"triton.language.max.html\" />\n+    <link rel=\"next\" title=\"triton.language.argmax\" href=\"triton.language.argmax.html\" />\n     <link rel=\"prev\" title=\"triton.language.sigmoid\" href=\"triton.language.sigmoid.html\" /> \n </head>\n \n@@ -134,7 +134,7 @@ <h1>triton.language.softmax<a class=\"headerlink\" href=\"#triton-language-softmax\"\n           </div>\n           <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n         <a href=\"triton.language.sigmoid.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.sigmoid\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"triton.language.max.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.max\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+        <a href=\"triton.language.argmax.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.argmax\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n     </div>\n \n   <hr/>"}, {"filename": "main/python-api/generated/triton.language.sum.html", "status": "modified", "additions": 8, "deletions": 4, "changes": 12, "file_content_changes": "@@ -22,8 +22,8 @@\n     <script src=\"../../_static/js/theme.js\"></script>\n     <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n     <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton.language.atomic_add\" href=\"triton.language.atomic_add.html\" />\n-    <link rel=\"prev\" title=\"triton.language.min\" href=\"triton.language.min.html\" /> \n+    <link rel=\"next\" title=\"triton.language.xor_sum\" href=\"triton.language.xor_sum.html\" />\n+    <link rel=\"prev\" title=\"triton.language.reduce\" href=\"triton.language.reduce.html\" /> \n </head>\n \n <body class=\"wy-body-for-nav\"> \n@@ -62,12 +62,16 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a></li>\n <li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#reduction-ops\">Reduction Ops</a><ul class=\"current\">\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.argmax.html\">triton.language.argmax</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.argmin.html\">triton.language.argmin</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.max.html\">triton.language.max</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.min.html\">triton.language.min</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.reduce.html\">triton.language.reduce</a></li>\n <li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.sum</a><ul>\n <li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#triton.language.sum\"><code class=\"docutils literal notranslate\"><span class=\"pre\">sum()</span></code></a></li>\n </ul>\n </li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.xor_sum.html\">triton.language.xor_sum</a></li>\n </ul>\n </li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#atomic-ops\">Atomic Ops</a></li>\n@@ -131,8 +135,8 @@ <h1>triton.language.sum<a class=\"headerlink\" href=\"#triton-language-sum\" title=\"\n            </div>\n           </div>\n           <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"triton.language.min.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.min\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"triton.language.atomic_add.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.atomic_add\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+        <a href=\"triton.language.reduce.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.reduce\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"triton.language.xor_sum.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.xor_sum\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n     </div>\n \n   <hr/>"}, {"filename": "main/python-api/generated/triton.language.xor_sum.html", "status": "added", "additions": 178, "deletions": 0, "changes": 178, "file_content_changes": "@@ -0,0 +1,178 @@\n+<!DOCTYPE html>\n+<html class=\"writer-html5\" lang=\"en\" >\n+<head>\n+  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n+\n+  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n+  <title>triton.language.xor_sum &mdash; Triton  documentation</title>\n+      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n+  <!--[if lt IE 9]>\n+    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n+  <![endif]-->\n+  \n+        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n+        <script src=\"../../_static/doctools.js\"></script>\n+        <script src=\"../../_static/sphinx_highlight.js\"></script>\n+    <script src=\"../../_static/js/theme.js\"></script>\n+    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n+    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n+    <link rel=\"next\" title=\"triton.language.atomic_add\" href=\"triton.language.atomic_add.html\" />\n+    <link rel=\"prev\" title=\"triton.language.sum\" href=\"triton.language.sum.html\" /> \n+</head>\n+\n+<body class=\"wy-body-for-nav\"> \n+  <div class=\"wy-grid-for-nav\">\n+    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n+      <div class=\"wy-side-scroll\">\n+        <div class=\"wy-side-nav-search\" >\n+\n+          \n+          \n+          <a href=\"../../index.html\" class=\"icon icon-home\">\n+            Triton\n+          </a>\n+<div role=\"search\">\n+  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n+    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n+    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n+    <input type=\"hidden\" name=\"area\" value=\"default\" />\n+  </form>\n+</div>\n+        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n+              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/installation.html\">Installation</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/tutorials/index.html\">Tutorials</a></li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n+<ul class=\"current\">\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.html\">triton</a></li>\n+<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"../triton.language.html\">triton.language</a><ul class=\"current\">\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#programming-model\">Programming Model</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#creation-ops\">Creation Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#shape-manipulation-ops\">Shape Manipulation Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#linear-algebra-ops\">Linear Algebra Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a></li>\n+<li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#reduction-ops\">Reduction Ops</a><ul class=\"current\">\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.argmax.html\">triton.language.argmax</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.argmin.html\">triton.language.argmin</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.max.html\">triton.language.max</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.min.html\">triton.language.min</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.reduce.html\">triton.language.reduce</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.sum.html\">triton.language.sum</a></li>\n+<li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.xor_sum</a><ul>\n+<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#triton.language.xor_sum\"><code class=\"docutils literal notranslate\"><span class=\"pre\">xor_sum()</span></code></a></li>\n+</ul>\n+</li>\n+</ul>\n+</li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#atomic-ops\">Atomic Ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#comparison-ops\">Comparison ops</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#random-number-generation\">Random Number Generation</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#compiler-hint-ops\">Compiler Hint Ops</a></li>\n+</ul>\n+</li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.testing.html\">triton.testing</a></li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n+</ul>\n+\n+        </div>\n+      </div>\n+    </nav>\n+\n+    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n+          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n+          <a href=\"../../index.html\">Triton</a>\n+      </nav>\n+\n+      <div class=\"wy-nav-content\">\n+        <div class=\"rst-content\">\n+          <div role=\"navigation\" aria-label=\"Page navigation\">\n+  <ul class=\"wy-breadcrumbs\">\n+      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n+          <li class=\"breadcrumb-item\"><a href=\"../triton.language.html\">triton.language</a></li>\n+      <li class=\"breadcrumb-item active\">triton.language.xor_sum</li>\n+      <li class=\"wy-breadcrumbs-aside\">\n+            <a href=\"../../_sources/python-api/generated/triton.language.xor_sum.rst.txt\" rel=\"nofollow\"> View page source</a>\n+      </li>\n+  </ul>\n+  <hr/>\n+</div>\n+          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n+           <div itemprop=\"articleBody\">\n+             \n+  <section id=\"triton-language-xor-sum\">\n+<h1>triton.language.xor_sum<a class=\"headerlink\" href=\"#triton-language-xor-sum\" title=\"Permalink to this heading\">\u00b6</a></h1>\n+<dl class=\"py function\">\n+<dt class=\"sig sig-object py\" id=\"triton.language.xor_sum\">\n+<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">xor_sum</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">input</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">axis</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.xor_sum\" title=\"Permalink to this definition\">\u00b6</a></dt>\n+<dd><p>Returns the xor sum of all elements in the <code class=\"code docutils literal notranslate\"><span class=\"pre\">input</span></code> tensor along the provided <code class=\"code docutils literal notranslate\"><span class=\"pre\">axis</span></code></p>\n+<dl class=\"field-list simple\">\n+<dt class=\"field-odd\">Parameters<span class=\"colon\">:</span></dt>\n+<dd class=\"field-odd\"><ul class=\"simple\">\n+<li><p><strong>input</strong> \u2013 the input values</p></li>\n+<li><p><strong>axis</strong> \u2013 the dimension along which the reduction should be done</p></li>\n+</ul>\n+</dd>\n+</dl>\n+</dd></dl>\n+\n+</section>\n+\n+\n+           </div>\n+          </div>\n+          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n+        <a href=\"triton.language.sum.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.sum\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"triton.language.atomic_add.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.atomic_add\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+    </div>\n+\n+  <hr/>\n+\n+  <div role=\"contentinfo\">\n+    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n+  </div>\n+\n+  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n+    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n+    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n+   \n+\n+</footer>\n+        </div>\n+      </div>\n+    </section>\n+  </div>\n+  \n+<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n+    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n+        <span class=\"fa fa-book\"> Other Versions</span>\n+        v: main\n+        <span class=\"fa fa-caret-down\"></span>\n+    </span>\n+    <div class=\"rst-other-versions\">\n+        <dl>\n+            <dt>Branches</dt>\n+            <dd><a href=\"triton.language.xor_sum.html\">main</a></dd>\n+        </dl>\n+    </div>\n+</div><script>\n+      jQuery(function () {\n+          SphinxRtdTheme.Navigation.enable(true);\n+      });\n+  </script> \n+\n+</body>\n+</html>\n\\ No newline at end of file"}, {"filename": "main/python-api/triton.html", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -23,7 +23,7 @@\n     <link rel=\"index\" title=\"Index\" href=\"../genindex.html\" />\n     <link rel=\"search\" title=\"Search\" href=\"../search.html\" />\n     <link rel=\"next\" title=\"triton.jit\" href=\"generated/triton.jit.html\" />\n-    <link rel=\"prev\" title=\"Libdevice (tl.math) function\" href=\"../getting-started/tutorials/07-math-functions.html\" /> \n+    <link rel=\"prev\" title=\"Block Pointer (Experimental)\" href=\"../getting-started/tutorials/08-experimental-block-pointer.html\" /> \n </head>\n \n <body class=\"wy-body-for-nav\"> \n@@ -116,7 +116,7 @@ <h1>triton<a class=\"headerlink\" href=\"#triton\" title=\"Permalink to this heading\"\n            </div>\n           </div>\n           <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"../getting-started/tutorials/07-math-functions.html\" class=\"btn btn-neutral float-left\" title=\"Libdevice (tl.math) function\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"../getting-started/tutorials/08-experimental-block-pointer.html\" class=\"btn btn-neutral float-left\" title=\"Block Pointer (Experimental)\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n         <a href=\"generated/triton.jit.html\" class=\"btn btn-neutral float-right\" title=\"triton.jit\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n     </div>\n "}, {"filename": "main/python-api/triton.language.html", "status": "modified", "additions": 17, "deletions": 1, "changes": 18, "file_content_changes": "@@ -98,9 +98,13 @@\n </ul>\n </li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"#reduction-ops\">Reduction Ops</a><ul>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"generated/triton.language.argmax.html\">triton.language.argmax</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"generated/triton.language.argmin.html\">triton.language.argmin</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"generated/triton.language.max.html\">triton.language.max</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"generated/triton.language.min.html\">triton.language.min</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"generated/triton.language.reduce.html\">triton.language.reduce</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"generated/triton.language.sum.html\">triton.language.sum</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"generated/triton.language.xor_sum.html\">triton.language.xor_sum</a></li>\n </ul>\n </li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"#atomic-ops\">Atomic Ops</a><ul>\n@@ -278,15 +282,27 @@ <h2>Math Ops<a class=\"headerlink\" href=\"#math-ops\" title=\"Permalink to this head\n <h2>Reduction Ops<a class=\"headerlink\" href=\"#reduction-ops\" title=\"Permalink to this heading\">\u00b6</a></h2>\n <table class=\"autosummary longtable docutils align-default\">\n <tbody>\n+<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.argmax.html#triton.language.argmax\" title=\"triton.language.argmax\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">argmax</span></code></a></p></td>\n+<td><p>Returns the maximum index of all elements in the <code class=\"code docutils literal notranslate\"><span class=\"pre\">input</span></code> tensor along the provided <code class=\"code docutils literal notranslate\"><span class=\"pre\">axis</span></code></p></td>\n+</tr>\n+<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.argmin.html#triton.language.argmin\" title=\"triton.language.argmin\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">argmin</span></code></a></p></td>\n+<td><p>Returns the minimum index of all elements in the <code class=\"code docutils literal notranslate\"><span class=\"pre\">input</span></code> tensor along the provided <code class=\"code docutils literal notranslate\"><span class=\"pre\">axis</span></code></p></td>\n+</tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.max.html#triton.language.max\" title=\"triton.language.max\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">max</span></code></a></p></td>\n <td><p>Returns the maximum of all elements in the <code class=\"code docutils literal notranslate\"><span class=\"pre\">input</span></code> tensor along the provided <code class=\"code docutils literal notranslate\"><span class=\"pre\">axis</span></code></p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.min.html#triton.language.min\" title=\"triton.language.min\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">min</span></code></a></p></td>\n <td><p>Returns the minimum of all elements in the <code class=\"code docutils literal notranslate\"><span class=\"pre\">input</span></code> tensor along the provided <code class=\"code docutils literal notranslate\"><span class=\"pre\">axis</span></code></p></td>\n </tr>\n-<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.sum.html#triton.language.sum\" title=\"triton.language.sum\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">sum</span></code></a></p></td>\n+<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.reduce.html#triton.language.reduce\" title=\"triton.language.reduce\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">reduce</span></code></a></p></td>\n+<td><p>Applies the combine_fn to all elements in <code class=\"code docutils literal notranslate\"><span class=\"pre\">input</span></code> tensors along the provided <code class=\"code docutils literal notranslate\"><span class=\"pre\">axis</span></code></p></td>\n+</tr>\n+<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.sum.html#triton.language.sum\" title=\"triton.language.sum\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">sum</span></code></a></p></td>\n <td><p>Returns the sum of all elements in the <code class=\"code docutils literal notranslate\"><span class=\"pre\">input</span></code> tensor along the provided <code class=\"code docutils literal notranslate\"><span class=\"pre\">axis</span></code></p></td>\n </tr>\n+<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.xor_sum.html#triton.language.xor_sum\" title=\"triton.language.xor_sum\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">xor_sum</span></code></a></p></td>\n+<td><p>Returns the xor sum of all elements in the <code class=\"code docutils literal notranslate\"><span class=\"pre\">input</span></code> tensor along the provided <code class=\"code docutils literal notranslate\"><span class=\"pre\">axis</span></code></p></td>\n+</tr>\n </tbody>\n </table>\n </section>"}, {"filename": "main/searchindex.js", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "N/A"}]