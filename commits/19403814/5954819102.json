[{"filename": "main/.buildinfo", "status": "removed", "additions": 0, "deletions": 4, "changes": 4, "file_content_changes": "@@ -1,4 +0,0 @@\n-# Sphinx build info version 1\n-# This file hashes the configuration used when building these files. When it is not found, a full rebuild will be done.\n-config: 9144905ecbf46ec5dabb98b75778fd8a\n-tags: 645f666f9bcd5a90fca523b33c5a78b7"}, {"filename": "main/.doctrees/environment.pickle", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/installation.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/01-vector-add.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/02-fused-softmax.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/03-matrix-multiplication.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/04-low-memory-dropout.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/05-layer-norm.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/06-fused-attention.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/07-math-functions.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/08-experimental-block-pointer.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/index.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/sg_execution_times.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/index.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/programming-guide/chapter-1/introduction.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/programming-guide/chapter-2/related-work.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.Config.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.autotune.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.heuristics.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.jit.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.abs.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.arange.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.argmax.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.argmin.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.associative_scan.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_add.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_cas.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_max.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_min.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_xchg.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.broadcast.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.broadcast_to.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cat.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cos.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cumprod.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cumsum.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.debug_barrier.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.device_assert.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.device_print.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.dot.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.exp.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.expand_dims.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.fdiv.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.full.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.load.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.log.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.max.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.max_constancy.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.max_contiguous.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.maximum.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.min.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.minimum.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.multiple_of.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.num_programs.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.program_id.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.rand.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randint.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randint4x.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randn.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.ravel.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.reduce.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.reshape.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sigmoid.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sin.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.softmax.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sqrt.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.static_assert.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.static_print.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.static_range.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.store.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sum.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.trans.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.umulhi.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.view.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.where.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.xor_sum.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.zeros.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.Benchmark.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.do_bench.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.perf_report.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.language.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.testing.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/034d953b6214fedce6ea03803c712b89/02-fused-softmax.ipynb", "status": "removed", "additions": 0, "deletions": 161, "changes": 161, "file_content_changes": "@@ -1,161 +0,0 @@\n-{\n-  \"cells\": [\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"%matplotlib inline\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"\\n# Fused Softmax\\n\\nIn this tutorial, you will write a fused softmax operation that is significantly faster\\nthan PyTorch's native op for a particular class of matrices: those whose rows can fit in\\nthe GPU's SRAM.\\n\\nIn doing so, you will learn about:\\n\\n* The benefits of kernel fusion for bandwidth-bound operations.\\n\\n* Reduction operators in Triton.\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Motivations\\n\\nCustom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.\\nLet us consider instead the case of a simple (numerically stabilized) softmax operation:\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@torch.jit.script\\ndef naive_softmax(x):\\n    \\\"\\\"\\\"Compute row-wise softmax of X using native pytorch\\n\\n    We subtract the maximum element in order to avoid overflows. Softmax is invariant to\\n    this shift.\\n    \\\"\\\"\\\"\\n    # read  MN elements ; write M  elements\\n    x_max = x.max(dim=1)[0]\\n    # read MN + M elements ; write MN elements\\n    z = x - x_max[:, None]\\n    # read  MN elements ; write MN elements\\n    numerator = torch.exp(z)\\n    # read  MN elements ; write M  elements\\n    denominator = numerator.sum(dim=1)\\n    # read MN + M elements ; write MN elements\\n    ret = numerator / denominator[:, None]\\n    # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements\\n    return ret\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for $x \\\\in R^{M \\\\times N}$\\nrequires reading $5MN + 2M$ elements from DRAM and writing back $3MN + 2M$ elements.\\nThis is obviously wasteful; we'd prefer to have a custom \\\"fused\\\" kernel that only reads\\nX once and does all the necessary computations on-chip.\\nDoing so would require reading and writing back only $MN$ bytes, so we could\\nexpect a theoretical speed-up of ~4x (i.e., $(8MN + 4M) / 2MN$).\\nThe `torch.jit.script` flags aims to perform this kind of \\\"kernel fusion\\\" automatically\\nbut, as we will see later, it is still far from ideal.\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Compute Kernel\\n\\nOur softmax kernel works as follows: each program loads a row of the input matrix X,\\nnormalizes it and writes back the result to the output Y.\\n\\nNote that one important limitation of Triton is that each block must have a\\npower-of-two number of elements, so we need to internally \\\"pad\\\" each row and guard the\\nmemory operations properly if we want to handle any possible input shapes:\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"@triton.jit\\ndef softmax_kernel(\\n    output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols,\\n    BLOCK_SIZE: tl.constexpr\\n):\\n    # The rows of the softmax are independent, so we parallelize across those\\n    row_idx = tl.program_id(0)\\n    # The stride represents how much we need to increase the pointer to advance 1 row\\n    row_start_ptr = input_ptr + row_idx * input_row_stride\\n    # The block size is the next power of two greater than n_cols, so we can fit each\\n    # row in a single block\\n    col_offsets = tl.arange(0, BLOCK_SIZE)\\n    input_ptrs = row_start_ptr + col_offsets\\n    # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\\n    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\\n    # Subtract maximum for numerical stability\\n    row_minus_max = row - tl.max(row, axis=0)\\n    # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\\n    numerator = tl.exp(row_minus_max)\\n    denominator = tl.sum(numerator, axis=0)\\n    softmax_output = numerator / denominator\\n    # Write back output to DRAM\\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\\n    output_ptrs = output_row_start_ptr + col_offsets\\n    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"def softmax(x):\\n    n_rows, n_cols = x.shape\\n    # The block size is the smallest power of two greater than the number of columns in `x`\\n    BLOCK_SIZE = triton.next_power_of_2(n_cols)\\n    # Another trick we can use is to ask the compiler to use more threads per row by\\n    # increasing the number of warps (`num_warps`) over which each row is distributed.\\n    # You will see in the next tutorial how to auto-tune this value in a more natural\\n    # way so you don't have to come up with manual heuristics yourself.\\n    num_warps = 4\\n    if BLOCK_SIZE >= 2048:\\n        num_warps = 8\\n    if BLOCK_SIZE >= 4096:\\n        num_warps = 16\\n    # Allocate output\\n    y = torch.empty_like(x)\\n    # Enqueue kernel. The 1D launch grid is simple: we have one kernel instance per row o\\n    # f the input matrix\\n    softmax_kernel[(n_rows,)](\\n        y,\\n        x,\\n        x.stride(0),\\n        y.stride(0),\\n        n_cols,\\n        num_warps=num_warps,\\n        BLOCK_SIZE=BLOCK_SIZE,\\n    )\\n    return y\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Unit Test\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"We make sure that we test our kernel on a matrix with an irregular number of rows and columns.\\nThis will allow us to verify that our padding mechanism works.\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"torch.manual_seed(0)\\nx = torch.randn(1823, 781, device='cuda')\\ny_triton = softmax(x)\\ny_torch = torch.softmax(x, axis=1)\\nassert torch.allclose(y_triton, y_torch), (y_triton, y_torch)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"As expected, the results are identical.\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Benchmark\\n\\nHere we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.\\nWe will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"@triton.testing.perf_report(\\n    triton.testing.Benchmark(\\n        x_names=['N'],  # argument names to use as an x-axis for the plot\\n        x_vals=[\\n            128 * i for i in range(2, 100)\\n        ],  # different possible values for `x_name`\\n        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\\n        line_vals=[\\n            'triton',\\n            'torch-native',\\n            'torch-jit',\\n        ],  # possible values for `line_arg``\\n        line_names=[\\n            \\\"Triton\\\",\\n            \\\"Torch (native)\\\",\\n            \\\"Torch (jit)\\\",\\n        ],  # label name for the lines\\n        styles=[('blue', '-'), ('green', '-'), ('green', '--')],  # line styles\\n        ylabel=\\\"GB/s\\\",  # label name for the y-axis\\n        plot_name=\\\"softmax-performance\\\",  # name for the plot. Used also as a file name for saving the plot.\\n        args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\\n    )\\n)\\ndef benchmark(M, N, provider):\\n    x = torch.randn(M, N, device='cuda', dtype=torch.float32)\\n    quantiles = [0.5, 0.2, 0.8]\\n    if provider == 'torch-native':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1), quantiles=quantiles)\\n    if provider == 'triton':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: softmax(x), quantiles=quantiles)\\n    if provider == 'torch-jit':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_softmax(x), quantiles=quantiles)\\n    gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\\n    return gbps(ms), gbps(max_ms), gbps(min_ms)\\n\\n\\nbenchmark.run(show_plots=True, print_data=True)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"In the above plot, we can see that:\\n - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.\\n - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.\\n   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.\\n\\n\"\n-      ]\n-    }\n-  ],\n-  \"metadata\": {\n-    \"kernelspec\": {\n-      \"display_name\": \"Python 3\",\n-      \"language\": \"python\",\n-      \"name\": \"python3\"\n-    },\n-    \"language_info\": {\n-      \"codemirror_mode\": {\n-        \"name\": \"ipython\",\n-        \"version\": 3\n-      },\n-      \"file_extension\": \".py\",\n-      \"mimetype\": \"text/x-python\",\n-      \"name\": \"python\",\n-      \"nbconvert_exporter\": \"python\",\n-      \"pygments_lexer\": \"ipython3\",\n-      \"version\": \"3.8.10\"\n-    }\n-  },\n-  \"nbformat\": 4,\n-  \"nbformat_minor\": 0\n-}\n\\ No newline at end of file"}, {"filename": "main/_downloads/302f1761fdc89516b532aad33b963ca0/07-math-functions.ipynb", "status": "removed", "additions": 0, "deletions": 97, "changes": 97, "file_content_changes": "@@ -1,97 +0,0 @@\n-{\n-  \"cells\": [\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"%matplotlib inline\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"\\n# Libdevice (`tl.math`) function\\nTriton can invoke a custom function from an external library.\\nIn this example, we will use the `libdevice` library (a.k.a `math` in triton) to apply `asin` on a tensor.\\nPlease refer to https://docs.nvidia.com/cuda/libdevice-users-guide/index.html regarding the semantics of all available libdevice functions.\\nIn `triton/language/math.py`, we try to aggregate functions with the same computation but different data types together.\\nFor example, both `__nv_asin` and `__nvasinf` calculate the principal value of the arc sine of the input, but `__nv_asin` operates on `double` and `__nv_asinf` operates on `float`.\\nUsing triton, you can simply call `tl.math.asin`.\\nTriton automatically selects the correct underlying device function to invoke based on input and output types.\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## asin Kernel\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.jit\\ndef asin_kernel(\\n        x_ptr,\\n        y_ptr,\\n        n_elements,\\n        BLOCK_SIZE: tl.constexpr,\\n):\\n    pid = tl.program_id(axis=0)\\n    block_start = pid * BLOCK_SIZE\\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\\n    mask = offsets < n_elements\\n    x = tl.load(x_ptr + offsets, mask=mask)\\n    x = tl.math.asin(x)\\n    tl.store(y_ptr + offsets, x, mask=mask)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Using the default libdevice library path\\nWe can use the default libdevice library path encoded in `triton/language/math.py`\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"torch.manual_seed(0)\\nsize = 98432\\nx = torch.rand(size, device='cuda')\\noutput_triton = torch.zeros(size, device='cuda')\\noutput_torch = torch.asin(x)\\nassert x.is_cuda and output_triton.is_cuda\\nn_elements = output_torch.numel()\\ngrid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\\nasin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024)\\nprint(output_torch)\\nprint(output_triton)\\nprint(\\n    f'The maximum difference between torch and triton is '\\n    f'{torch.max(torch.abs(output_torch - output_triton))}'\\n)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Customize the libdevice library path\\nWe can also customize the libdevice library path by passing the path to the `libdevice` library to the `asin` kernel.\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"output_triton = torch.empty_like(x)\\nasin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024,\\n                  extern_libs={'libdevice': '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'})\\nprint(output_torch)\\nprint(output_triton)\\nprint(\\n    f'The maximum difference between torch and triton is '\\n    f'{torch.max(torch.abs(output_torch - output_triton))}'\\n)\"\n-      ]\n-    }\n-  ],\n-  \"metadata\": {\n-    \"kernelspec\": {\n-      \"display_name\": \"Python 3\",\n-      \"language\": \"python\",\n-      \"name\": \"python3\"\n-    },\n-    \"language_info\": {\n-      \"codemirror_mode\": {\n-        \"name\": \"ipython\",\n-        \"version\": 3\n-      },\n-      \"file_extension\": \".py\",\n-      \"mimetype\": \"text/x-python\",\n-      \"name\": \"python\",\n-      \"nbconvert_exporter\": \"python\",\n-      \"pygments_lexer\": \"ipython3\",\n-      \"version\": \"3.8.10\"\n-    }\n-  },\n-  \"nbformat\": 4,\n-  \"nbformat_minor\": 0\n-}\n\\ No newline at end of file"}, {"filename": "main/_downloads/3176accb6c7288b0e45f41d94eebacb9/06-fused-attention.ipynb", "status": "removed", "additions": 0, "deletions": 54, "changes": 54, "file_content_changes": "@@ -1,54 +0,0 @@\n-{\n-  \"cells\": [\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"%matplotlib inline\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"\\n# Fused Attention\\n\\nThis is a Triton implementation of the Flash Attention v2 algorithm from Tri Dao (https://tridao.me/publications/flash2/flash2.pdf)\\n\\nExtra Credits:\\n- Original flash attention paper (https://arxiv.org/abs/2205.14135)\\n- Rabe and Staats (https://arxiv.org/pdf/2112.05682v2.pdf)\\n- Adam P. Goucher for simplified vector math\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"import pytest\\nimport torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.jit\\ndef max_fn(x, y):\\n    return tl.math.max(x, y)\\n\\n\\n@triton.jit\\ndef _fwd_kernel(\\n    Q, K, V, sm_scale,\\n    L,\\n    Out,\\n    stride_qz, stride_qh, stride_qm, stride_qk,\\n    stride_kz, stride_kh, stride_kn, stride_kk,\\n    stride_vz, stride_vh, stride_vk, stride_vn,\\n    stride_oz, stride_oh, stride_om, stride_on,\\n    Z, H, N_CTX,\\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\\n    BLOCK_N: tl.constexpr,\\n    IS_CAUSAL: tl.constexpr,\\n):\\n    start_m = tl.program_id(0)\\n    off_hz = tl.program_id(1)\\n    qvk_offset = off_hz * stride_qh\\n    Q_block_ptr = tl.make_block_ptr(\\n        base=Q + qvk_offset,\\n        shape=(N_CTX, BLOCK_DMODEL),\\n        strides=(stride_qm, stride_qk),\\n        offsets=(start_m * BLOCK_M, 0),\\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\\n        order=(1, 0)\\n    )\\n    K_block_ptr = tl.make_block_ptr(\\n        base=K + qvk_offset,\\n        shape=(BLOCK_DMODEL, N_CTX),\\n        strides=(stride_kk, stride_kn),\\n        offsets=(0, 0),\\n        block_shape=(BLOCK_DMODEL, BLOCK_N),\\n        order=(0, 1)\\n    )\\n    V_block_ptr = tl.make_block_ptr(\\n        base=V + qvk_offset,\\n        shape=(N_CTX, BLOCK_DMODEL),\\n        strides=(stride_vk, stride_vn),\\n        offsets=(0, 0),\\n        block_shape=(BLOCK_N, BLOCK_DMODEL),\\n        order=(1, 0)\\n    )\\n    # initialize offsets\\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\\n    offs_n = tl.arange(0, BLOCK_N)\\n    # initialize pointer to m and l\\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\\\"inf\\\")\\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n    # scale sm_scale by log_2(e) and use\\n    # 2^x instead of exp in the loop because CSE and LICM\\n    # don't work as expected with `exp` in the loop\\n    qk_scale = sm_scale * 1.44269504\\n    # load q: it will stay in SRAM throughout\\n    q = tl.load(Q_block_ptr)\\n    q = (q * qk_scale).to(tl.float16)\\n    # loop over k, v and update accumulator\\n    lo = 0\\n    hi = (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX\\n    for start_n in range(lo, hi, BLOCK_N):\\n        # -- load k, v --\\n        k = tl.load(K_block_ptr)\\n        v = tl.load(V_block_ptr)\\n        # -- compute qk ---\\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\\n        if IS_CAUSAL:\\n            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\\\"-inf\\\"))\\n        qk += tl.dot(q, k)\\n        # -- compute scaling constant ---\\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\\n        alpha = tl.math.exp2(m_i - m_i_new)\\n        p = tl.math.exp2(qk - m_i_new[:, None])\\n        # -- scale and update acc --\\n        acc_scale = l_i * 0 + alpha  # workaround some compiler bug\\n        acc *= acc_scale[:, None]\\n        acc += tl.dot(p.to(tl.float16), v)\\n        # -- update m_i and l_i --\\n        l_i = l_i * alpha + tl.sum(p, 1)\\n        m_i = m_i_new\\n        # update pointers\\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\\n    # write back l and m\\n    acc = acc / l_i[:, None]\\n    l_ptrs = L + off_hz * N_CTX + offs_m\\n    tl.store(l_ptrs, m_i + tl.math.log2(l_i))\\n    # write back O\\n    O_block_ptr = tl.make_block_ptr(\\n        base=Out + qvk_offset,\\n        shape=(N_CTX, BLOCK_DMODEL),\\n        strides=(stride_om, stride_on),\\n        offsets=(start_m * BLOCK_M, 0),\\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\\n        order=(1, 0)\\n    )\\n    tl.store(O_block_ptr, acc.to(tl.float16))\\n\\n\\n@triton.jit\\ndef _bwd_preprocess(\\n    Out, DO,\\n    Delta,\\n    BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\\n):\\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\\n    off_n = tl.arange(0, D_HEAD)\\n    # load\\n    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\\n    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\\n    # compute\\n    delta = tl.sum(o * do, axis=1)\\n    # write-back\\n    tl.store(Delta + off_m, delta)\\n\\n\\n@triton.jit\\ndef _bwd_kernel(\\n    Q, K, V, sm_scale, Out, DO,\\n    DQ, DK, DV,\\n    L,\\n    D,\\n    stride_qz, stride_qh, stride_qm, stride_qk,\\n    stride_kz, stride_kh, stride_kn, stride_kk,\\n    stride_vz, stride_vh, stride_vk, stride_vn,\\n    Z, H, N_CTX,\\n    num_block,\\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\\n    BLOCK_N: tl.constexpr,\\n    CAUSAL: tl.constexpr,\\n):\\n    off_hz = tl.program_id(0)\\n    off_z = off_hz // H\\n    off_h = off_hz % H\\n    qk_scale = sm_scale * 1.44269504\\n    # offset pointers for batch/head\\n    Q += off_z * stride_qz + off_h * stride_qh\\n    K += off_z * stride_qz + off_h * stride_qh\\n    V += off_z * stride_qz + off_h * stride_qh\\n    DO += off_z * stride_qz + off_h * stride_qh\\n    DQ += off_z * stride_qz + off_h * stride_qh\\n    DK += off_z * stride_qz + off_h * stride_qh\\n    DV += off_z * stride_qz + off_h * stride_qh\\n    for start_n in range(0, num_block):\\n        if CAUSAL:\\n            lo = start_n * BLOCK_M\\n        else:\\n            lo = 0\\n        # initialize row/col offsets\\n        offs_qm = lo + tl.arange(0, BLOCK_M)\\n        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\\n        offs_m = tl.arange(0, BLOCK_N)\\n        offs_k = tl.arange(0, BLOCK_DMODEL)\\n        # initialize pointers to value-like data\\n        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\\n        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        # pointer to row-wise quantities in value-like data\\n        D_ptrs = D + off_hz * N_CTX\\n        l_ptrs = L + off_hz * N_CTX\\n        # initialize dv amd dk\\n        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n        # k and v stay in SRAM throughout\\n        k = tl.load(k_ptrs)\\n        v = tl.load(v_ptrs)\\n        # loop over rows\\n        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\\n            offs_m_curr = start_m + offs_m\\n            # load q, k, v, do on-chip\\n            q = tl.load(q_ptrs)\\n            # recompute p = softmax(qk, dim=-1).T\\n            if CAUSAL:\\n                qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\\\"-inf\\\"))\\n            else:\\n                qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\\n            qk += tl.dot(q, tl.trans(k))\\n            qk *= qk_scale\\n            l_i = tl.load(l_ptrs + offs_m_curr)\\n            p = tl.math.exp2(qk - l_i[:, None])\\n            # compute dv\\n            do = tl.load(do_ptrs)\\n            dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\\n            # compute dp = dot(v, do)\\n            Di = tl.load(D_ptrs + offs_m_curr)\\n            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\\n            dp += tl.dot(do, tl.trans(v))\\n            # compute ds = p * (dp - delta[:, None])\\n            ds = p * dp * sm_scale\\n            # compute dk = dot(ds.T, q)\\n            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)\\n            # compute dq\\n            dq = tl.load(dq_ptrs)\\n            dq += tl.dot(ds.to(Q.dtype.element_ty), k)\\n            tl.store(dq_ptrs, dq)\\n            # increment pointers\\n            dq_ptrs += BLOCK_M * stride_qm\\n            q_ptrs += BLOCK_M * stride_qm\\n            do_ptrs += BLOCK_M * stride_qm\\n        # write-back\\n        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\\n        tl.store(dv_ptrs, dv)\\n        tl.store(dk_ptrs, dk)\\n\\n\\nempty = torch.empty(128, device=\\\"cuda\\\")\\n\\n\\nclass _attention(torch.autograd.Function):\\n\\n    @staticmethod\\n    def forward(ctx, q, k, v, causal, sm_scale):\\n        # shape constraints\\n        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\\n        assert Lq == Lk and Lk == Lv\\n        assert Lk in {16, 32, 64, 128}\\n        o = torch.empty_like(q)\\n        BLOCK_M = 128\\n        BLOCK_N = 64\\n        grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\\n        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\\n\\n        num_warps = 4 if Lk <= 64 else 8\\n        _fwd_kernel[grid](\\n            q, k, v, sm_scale,\\n            L,\\n            o,\\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\\n            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\\n            q.shape[0], q.shape[1], q.shape[2],\\n            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=Lk,\\n            IS_CAUSAL=causal,\\n            num_warps=num_warps,\\n            num_stages=4)\\n\\n        ctx.save_for_backward(q, k, v, o, L)\\n        ctx.grid = grid\\n        ctx.sm_scale = sm_scale\\n        ctx.BLOCK_DMODEL = Lk\\n        ctx.causal = causal\\n        return o\\n\\n    @staticmethod\\n    def backward(ctx, do):\\n        BLOCK = 128\\n        q, k, v, o, L = ctx.saved_tensors\\n        do = do.contiguous()\\n        dq = torch.zeros_like(q, dtype=torch.float32)\\n        dk = torch.empty_like(k)\\n        dv = torch.empty_like(v)\\n        delta = torch.empty_like(L)\\n        _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\\n            o, do,\\n            delta,\\n            BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\\n        )\\n        _bwd_kernel[(ctx.grid[1],)](\\n            q, k, v, ctx.sm_scale,\\n            o, do,\\n            dq, dk, dv,\\n            L, delta,\\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\\n            q.shape[0], q.shape[1], q.shape[2],\\n            ctx.grid[0],\\n            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\\n            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\\n            CAUSAL=ctx.causal,\\n            num_stages=1,\\n        )\\n        return dq, dk, dv, None, None\\n\\n\\nattention = _attention.apply\\n\\n\\n@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(6, 9, 1024, 64)])\\n@pytest.mark.parametrize('causal', [False, True])\\ndef test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\\n    torch.manual_seed(20)\\n    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0., std=0.5).requires_grad_()\\n    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0., std=0.5).requires_grad_()\\n    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0., std=0.5).requires_grad_()\\n    sm_scale = 0.5\\n    dout = torch.randn_like(q)\\n    # reference implementation\\n    M = torch.tril(torch.ones((N_CTX, N_CTX), device=\\\"cuda\\\"))\\n    p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\\n    if causal:\\n        p[:, :, M == 0] = float(\\\"-inf\\\")\\n    p = torch.softmax(p.float(), dim=-1).half()\\n    # p = torch.exp(p)\\n    ref_out = torch.matmul(p, v)\\n    ref_out.backward(dout)\\n    ref_dv, v.grad = v.grad.clone(), None\\n    ref_dk, k.grad = k.grad.clone(), None\\n    ref_dq, q.grad = q.grad.clone(), None\\n    # triton implementation\\n    tri_out = attention(q, k, v, causal, sm_scale).half()\\n    tri_out.backward(dout)\\n    tri_dv, v.grad = v.grad.clone(), None\\n    tri_dk, k.grad = k.grad.clone(), None\\n    tri_dq, q.grad = q.grad.clone(), None\\n    # compare\\n    assert torch.allclose(ref_out, tri_out, atol=1e-2, rtol=0)\\n    assert torch.allclose(ref_dv, tri_dv, atol=1e-2, rtol=0)\\n    assert torch.allclose(ref_dk, tri_dk, atol=1e-2, rtol=0)\\n    assert torch.allclose(ref_dq, tri_dq, atol=1e-2, rtol=0)\\n\\n\\ntry:\\n    from flash_attn.flash_attn_interface import \\\\\\n        flash_attn_qkvpacked_func as flash_attn_func\\n    FLASH_VER = 2\\nexcept BaseException:\\n    try:\\n        from flash_attn.flash_attn_interface import flash_attn_func\\n        FLASH_VER = 1\\n    except BaseException:\\n        FLASH_VER = None\\nHAS_FLASH = FLASH_VER is not None\\n\\nBATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\\n# vary seq length for fixed head and batch=4\\nconfigs = [triton.testing.Benchmark(\\n    x_names=['N_CTX'],\\n    x_vals=[2**i for i in range(10, 15)],\\n    line_arg='provider',\\n    line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\\n    line_names=['Triton'] + ([f'Flash-{FLASH_VER}'] if HAS_FLASH else []),\\n    styles=[('red', '-'), ('blue', '-')],\\n    ylabel='ms',\\n    plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\\n    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode, 'causal': causal}\\n) for mode in ['fwd', 'bwd'] for causal in [False, True]]\\n\\n\\n@triton.testing.perf_report(configs)\\ndef bench_flash_attention(BATCH, H, N_CTX, D_HEAD, causal, mode, provider, dtype=torch.float16, device=\\\"cuda\\\"):\\n    assert mode in ['fwd', 'bwd']\\n    warmup = 25\\n    rep = 100\\n    if provider == \\\"triton\\\":\\n        q = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        sm_scale = 1.3\\n        fn = lambda: attention(q, k, v, causal, sm_scale)\\n        if mode == 'bwd':\\n            o = fn()\\n            do = torch.randn_like(o)\\n            fn = lambda: o.backward(do, retain_graph=True)\\n        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\\n    if provider == \\\"flash\\\":\\n        qkv = torch.randn((BATCH, N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\\n        if FLASH_VER == 1:\\n            lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\\n            cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\\n            cu_seqlens[1:] = lengths.cumsum(0)\\n            qkv = qkv.reshape(BATCH * N_CTX, 3, H, D_HEAD)\\n            fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=causal)\\n        elif FLASH_VER == 2:\\n            fn = lambda: flash_attn_func(qkv, causal=causal)\\n        else:\\n            raise ValueError(f'unknown {FLASH_VER = }')\\n        if mode == 'bwd':\\n            o = fn()\\n            do = torch.randn_like(o)\\n            fn = lambda: o.backward(do, retain_graph=True)\\n        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\\n    flops_per_matmul = 2. * BATCH * H * N_CTX * N_CTX * D_HEAD\\n    total_flops = 2 * flops_per_matmul\\n    if causal:\\n        total_flops *= 0.5\\n    if mode == 'bwd':\\n        total_flops *= 2.5  # 2.0(bwd) + 0.5(recompute)\\n    return total_flops / ms * 1e-9\\n\\n\\n# only works on post-Ampere GPUs right now\\nbench_flash_attention.run(save_path='.', print_data=True)\"\n-      ]\n-    }\n-  ],\n-  \"metadata\": {\n-    \"kernelspec\": {\n-      \"display_name\": \"Python 3\",\n-      \"language\": \"python\",\n-      \"name\": \"python3\"\n-    },\n-    \"language_info\": {\n-      \"codemirror_mode\": {\n-        \"name\": \"ipython\",\n-        \"version\": 3\n-      },\n-      \"file_extension\": \".py\",\n-      \"mimetype\": \"text/x-python\",\n-      \"name\": \"python\",\n-      \"nbconvert_exporter\": \"python\",\n-      \"pygments_lexer\": \"ipython3\",\n-      \"version\": \"3.8.10\"\n-    }\n-  },\n-  \"nbformat\": 4,\n-  \"nbformat_minor\": 0\n-}\n\\ No newline at end of file"}, {"filename": "main/_downloads/4d6052117d61c2ca779cd4b75567fee5/08-experimental-block-pointer.py", "status": "removed", "additions": 0, "deletions": 228, "changes": 228, "file_content_changes": "@@ -1,228 +0,0 @@\n-\"\"\"\n-Block Pointer (Experimental)\n-============================\n-This tutorial will guide you through writing a matrix multiplication algorithm that utilizes block pointer semantics.\n-These semantics are more friendly for Triton to optimize and can result in better performance on specific hardware.\n-Note that this feature is still experimental and may change in the future.\n-\n-\"\"\"\n-\n-# %%\n-# Motivations\n-# -----------\n-# In the previous matrix multiplication tutorial, we constructed blocks of values by de-referencing blocks of pointers,\n-# i.e., :code:`load(block<pointer_type<element_type>>) -> block<element_type>`, which involved loading blocks of\n-# elements from memory. This approach allowed for flexibility in using hardware-managed cache and implementing complex\n-# data structures, such as tensors of trees or unstructured look-up tables.\n-#\n-# However, the drawback of this approach is that it relies heavily on complex optimization passes by the compiler to\n-# optimize memory access patterns. This can result in brittle code that may suffer from performance degradation when the\n-# optimizer fails to perform adequately. Additionally, as memory controllers specialize to accommodate dense spatial\n-# data structures commonly used in machine learning workloads, this problem is likely to worsen.\n-#\n-# To address this issue, we will use block pointers :code:`pointer_type<block<element_type>>` and load them into\n-# :code:`block<element_type>`, in which way gives better friendliness for the compiler to optimize memory access\n-# patterns.\n-#\n-# Let's start with the previous matrix multiplication example and demonstrate how to rewrite it to utilize block pointer\n-# semantics.\n-\n-# %%\n-# Make a Block Pointer\n-# --------------------\n-# A block pointer pointers to a block in a parent tensor and is constructed by :code:`make_block_ptr` function,\n-# which takes the following information as arguments:\n-#\n-# * :code:`base`: the base pointer to the parent tensor;\n-#\n-# * :code:`shape`: the shape of the parent tensor;\n-#\n-# * :code:`strides`: the strides of the parent tensor, which means how much to increase the pointer by when moving by 1 element in a specific axis;\n-#\n-# * :code:`offsets`: the offsets of the block;\n-#\n-# * :code:`block_shape`: the shape of the block;\n-#\n-# * :code:`order`: the order of the block, which means how the block is laid out in memory.\n-#\n-# For example, to a block pointer to a :code:`BLOCK_SIZE_M * BLOCK_SIZE_K` block in a row-major 2D matrix A by\n-# offsets :code:`(pid_m * BLOCK_SIZE_M, 0)` and strides :code:`(stride_am, stride_ak)`, we can use the following code\n-# (exactly the same as the previous matrix multiplication tutorial):\n-#\n-# .. code-block:: python\n-#\n-#     a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\n-#                                     offsets=(pid_m * BLOCK_SIZE_M, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K),\n-#                                     order=(1, 0))\n-#\n-# Note that the :code:`order` argument is set to :code:`(1, 0)`, which means the second axis is the inner dimension in\n-# terms of storage, and the first axis is the outer dimension. This information may sound redundant, but it is necessary\n-# for some hardware backends to optimize for better performance.\n-\n-# %%\n-# Load/Store a Block Pointer\n-# --------------------------\n-# To load/store a block pointer, we can use :code:`load/store` function, which takes a block pointer as an argument,\n-# de-references it, and loads/stores a block. You may mask some values in the block, here we have an extra argument\n-# :code:`boundary_check` to specify whether to check the boundary of each axis for the block pointer. With check on,\n-# out-of-bound values will be masked according to the :code:`padding_option` argument (load only), which can be\n-# :code:`zero` or :code:`nan`. Temporarily, we do not support other values due to some hardware limitations. In this\n-# mode of block pointer load/store does not support :code:`mask` or :code:`other` arguments in the legacy mode.\n-#\n-# So to load the block pointer of A in the previous section, we can simply write\n-# :code:`a = tl.load(a_block_ptr, boundary_check=(0, 1))`. Boundary check may cost extra performance, so if you can\n-# guarantee that the block pointer is always in-bound in some axis, you can turn off the check by not passing the index\n-# into the :code:`boundary_check` argument. For example, if we know that :code:`M` is a multiple of\n-# :code:`BLOCK_SIZE_M`, we can replace with :code:`a = tl.load(a_block_ptr, boundary_check=(1, ))`, since axis 0 is\n-# always in bound.\n-\n-# %%\n-# Advance a Block Pointer\n-# -----------------------\n-# To advance a block pointer, we can use :code:`advance` function, which takes a block pointer and the increment for\n-# each axis as arguments and returns a new block pointer with the same shape and strides as the original one,\n-# but with the offsets advanced by the specified amount.\n-#\n-# For example, to advance the block pointer by :code:`BLOCK_SIZE_K` in the second axis\n-# (no need to multiply with strides), we can write :code:`a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))`.\n-\n-# %%\n-# Final Result\n-# ------------\n-\n-import torch\n-\n-import triton\n-import triton.language as tl\n-\n-\n-@triton.autotune(\n-    configs=[\n-        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n-        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n-    ],\n-    key=['M', 'N', 'K'],\n-)\n-@triton.jit\n-def matmul_kernel_with_block_pointers(\n-        # Pointers to matrices\n-        a_ptr, b_ptr, c_ptr,\n-        # Matrix dimensions\n-        M, N, K,\n-        # The stride variables represent how much to increase the ptr by when moving by 1\n-        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n-        # by to get the element one row down (A has M rows).\n-        stride_am, stride_ak,\n-        stride_bk, stride_bn,\n-        stride_cm, stride_cn,\n-        # Meta-parameters\n-        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n-        GROUP_SIZE_M: tl.constexpr\n-):\n-    \"\"\"Kernel for computing the matmul C = A x B.\n-    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n-    \"\"\"\n-    # -----------------------------------------------------------\n-    # Map program ids `pid` to the block of C it should compute.\n-    # This is done in a grouped ordering to promote L2 data reuse.\n-    # See the matrix multiplication tutorial for details.\n-    pid = tl.program_id(axis=0)\n-    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n-    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n-    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n-    group_id = pid // num_pid_in_group\n-    first_pid_m = group_id * GROUP_SIZE_M\n-    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n-    pid_m = first_pid_m + (pid % group_size_m)\n-    pid_n = (pid % num_pid_in_group) // group_size_m\n-\n-    # ----------------------------------------------------------\n-    # Create block pointers for the first blocks of A and B.\n-    # We will advance this pointer as we move in the K direction and accumulate.\n-    # See above `Make a Block Pointer` section for details.\n-    a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\n-                                    offsets=(pid_m * BLOCK_SIZE_M, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K),\n-                                    order=(1, 0))\n-    b_block_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(stride_bk, stride_bn),\n-                                    offsets=(0, pid_n * BLOCK_SIZE_N), block_shape=(BLOCK_SIZE_K, BLOCK_SIZE_N),\n-                                    order=(1, 0))\n-\n-    # -----------------------------------------------------------\n-    # Iterate to compute a block of the C matrix.\n-    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block.\n-    # of fp32 values for higher accuracy.\n-    # `accumulator` will be converted back to fp16 after the loop.\n-    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-    for k in range(0, K, BLOCK_SIZE_K):\n-        # Load with boundary checks, no need to calculate the mask manually.\n-        # For better performance, you may remove some axis from the boundary\n-        # check, if you can guarantee that the access is always in-bound in\n-        # that axis.\n-        # See above `Load/Store a Block Pointer` section for details.\n-        a = tl.load(a_block_ptr, boundary_check=(0, 1))\n-        b = tl.load(b_block_ptr, boundary_check=(0, 1))\n-        # We accumulate along the K dimension.\n-        accumulator += tl.dot(a, b)\n-        # Advance the block pointer to the next K block.\n-        # See above `Advance a Block Pointer` section for details.\n-        a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))\n-        b_block_ptr = tl.advance(b_block_ptr, (BLOCK_SIZE_K, 0))\n-    c = accumulator.to(tl.float16)\n-\n-    # ----------------------------------------------------------------\n-    # Write back the block of the output matrix C with boundary checks.\n-    # See above `Load/Store a Block Pointer` section for details.\n-    c_block_ptr = tl.make_block_ptr(base=c_ptr, shape=(M, N), strides=(stride_cm, stride_cn),\n-                                    offsets=(pid_m * BLOCK_SIZE_M, pid_n * BLOCK_SIZE_N),\n-                                    block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_N), order=(1, 0))\n-    tl.store(c_block_ptr, c, boundary_check=(0, 1))\n-\n-\n-# We can now create a convenience wrapper function that only takes two input tensors,\n-# and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\n-def matmul(a, b):\n-    # Check constraints.\n-    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n-    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n-    assert b.is_contiguous(), \"Matrix B must be contiguous\"\n-    M, K = a.shape\n-    K, N = b.shape\n-    # Allocates output.\n-    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n-    # 1D launch kernel where each block gets its own program.\n-    grid = lambda META: (\n-        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n-    )\n-    matmul_kernel_with_block_pointers[grid](\n-        a, b, c,\n-        M, N, K,\n-        a.stride(0), a.stride(1),\n-        b.stride(0), b.stride(1),\n-        c.stride(0), c.stride(1),\n-    )\n-    return c\n-\n-\n-# %%\n-# Unit Test\n-# ---------\n-#\n-# Still we can test our matrix multiplication with block pointers against a native torch implementation (i.e., cuBLAS).\n-\n-torch.manual_seed(0)\n-a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n-b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n-triton_output = matmul(a, b)\n-torch_output = torch.matmul(a, b)\n-print(f\"triton_output={triton_output}\")\n-print(f\"torch_output={torch_output}\")\n-if torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\n-    print(\"\u2705 Triton and Torch match\")\n-else:\n-    print(\"\u274c Triton and Torch differ\")"}, {"filename": "main/_downloads/54a35f6ec55f9746935b9566fb6bb1df/06-fused-attention.py", "status": "removed", "additions": 0, "deletions": 410, "changes": 410, "file_content_changes": "@@ -1,410 +0,0 @@\n-\"\"\"\n-Fused Attention\n-===============\n-\n-This is a Triton implementation of the Flash Attention v2 algorithm from Tri Dao (https://tridao.me/publications/flash2/flash2.pdf)\n-\n-Extra Credits:\n-- Original flash attention paper (https://arxiv.org/abs/2205.14135)\n-- Rabe and Staats (https://arxiv.org/pdf/2112.05682v2.pdf)\n-- Adam P. Goucher for simplified vector math\n-\n-\"\"\"\n-\n-import pytest\n-import torch\n-\n-import triton\n-import triton.language as tl\n-\n-\n-@triton.jit\n-def max_fn(x, y):\n-    return tl.math.max(x, y)\n-\n-\n-@triton.jit\n-def _fwd_kernel(\n-    Q, K, V, sm_scale,\n-    L,\n-    Out,\n-    stride_qz, stride_qh, stride_qm, stride_qk,\n-    stride_kz, stride_kh, stride_kn, stride_kk,\n-    stride_vz, stride_vh, stride_vk, stride_vn,\n-    stride_oz, stride_oh, stride_om, stride_on,\n-    Z, H, N_CTX,\n-    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n-    BLOCK_N: tl.constexpr,\n-    IS_CAUSAL: tl.constexpr,\n-):\n-    start_m = tl.program_id(0)\n-    off_hz = tl.program_id(1)\n-    qvk_offset = off_hz * stride_qh\n-    Q_block_ptr = tl.make_block_ptr(\n-        base=Q + qvk_offset,\n-        shape=(N_CTX, BLOCK_DMODEL),\n-        strides=(stride_qm, stride_qk),\n-        offsets=(start_m * BLOCK_M, 0),\n-        block_shape=(BLOCK_M, BLOCK_DMODEL),\n-        order=(1, 0)\n-    )\n-    K_block_ptr = tl.make_block_ptr(\n-        base=K + qvk_offset,\n-        shape=(BLOCK_DMODEL, N_CTX),\n-        strides=(stride_kk, stride_kn),\n-        offsets=(0, 0),\n-        block_shape=(BLOCK_DMODEL, BLOCK_N),\n-        order=(0, 1)\n-    )\n-    V_block_ptr = tl.make_block_ptr(\n-        base=V + qvk_offset,\n-        shape=(N_CTX, BLOCK_DMODEL),\n-        strides=(stride_vk, stride_vn),\n-        offsets=(0, 0),\n-        block_shape=(BLOCK_N, BLOCK_DMODEL),\n-        order=(1, 0)\n-    )\n-    # initialize offsets\n-    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n-    offs_n = tl.arange(0, BLOCK_N)\n-    # initialize pointer to m and l\n-    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n-    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n-    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n-    # scale sm_scale by log_2(e) and use\n-    # 2^x instead of exp in the loop because CSE and LICM\n-    # don't work as expected with `exp` in the loop\n-    qk_scale = sm_scale * 1.44269504\n-    # load q: it will stay in SRAM throughout\n-    q = tl.load(Q_block_ptr)\n-    q = (q * qk_scale).to(tl.float16)\n-    # loop over k, v and update accumulator\n-    lo = 0\n-    hi = (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX\n-    for start_n in range(lo, hi, BLOCK_N):\n-        # -- load k, v --\n-        k = tl.load(K_block_ptr)\n-        v = tl.load(V_block_ptr)\n-        # -- compute qk ---\n-        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-        if IS_CAUSAL:\n-            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n-        qk += tl.dot(q, k)\n-        # -- compute scaling constant ---\n-        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n-        alpha = tl.math.exp2(m_i - m_i_new)\n-        p = tl.math.exp2(qk - m_i_new[:, None])\n-        # -- scale and update acc --\n-        acc_scale = l_i * 0 + alpha  # workaround some compiler bug\n-        acc *= acc_scale[:, None]\n-        acc += tl.dot(p.to(tl.float16), v)\n-        # -- update m_i and l_i --\n-        l_i = l_i * alpha + tl.sum(p, 1)\n-        m_i = m_i_new\n-        # update pointers\n-        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n-        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n-    # write back l and m\n-    acc = acc / l_i[:, None]\n-    l_ptrs = L + off_hz * N_CTX + offs_m\n-    tl.store(l_ptrs, m_i + tl.math.log2(l_i))\n-    # write back O\n-    O_block_ptr = tl.make_block_ptr(\n-        base=Out + qvk_offset,\n-        shape=(N_CTX, BLOCK_DMODEL),\n-        strides=(stride_om, stride_on),\n-        offsets=(start_m * BLOCK_M, 0),\n-        block_shape=(BLOCK_M, BLOCK_DMODEL),\n-        order=(1, 0)\n-    )\n-    tl.store(O_block_ptr, acc.to(tl.float16))\n-\n-\n-@triton.jit\n-def _bwd_preprocess(\n-    Out, DO,\n-    Delta,\n-    BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\n-):\n-    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n-    off_n = tl.arange(0, D_HEAD)\n-    # load\n-    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n-    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n-    # compute\n-    delta = tl.sum(o * do, axis=1)\n-    # write-back\n-    tl.store(Delta + off_m, delta)\n-\n-\n-@triton.jit\n-def _bwd_kernel(\n-    Q, K, V, sm_scale, Out, DO,\n-    DQ, DK, DV,\n-    L,\n-    D,\n-    stride_qz, stride_qh, stride_qm, stride_qk,\n-    stride_kz, stride_kh, stride_kn, stride_kk,\n-    stride_vz, stride_vh, stride_vk, stride_vn,\n-    Z, H, N_CTX,\n-    num_block,\n-    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n-    BLOCK_N: tl.constexpr,\n-    CAUSAL: tl.constexpr,\n-):\n-    off_hz = tl.program_id(0)\n-    off_z = off_hz // H\n-    off_h = off_hz % H\n-    qk_scale = sm_scale * 1.44269504\n-    # offset pointers for batch/head\n-    Q += off_z * stride_qz + off_h * stride_qh\n-    K += off_z * stride_qz + off_h * stride_qh\n-    V += off_z * stride_qz + off_h * stride_qh\n-    DO += off_z * stride_qz + off_h * stride_qh\n-    DQ += off_z * stride_qz + off_h * stride_qh\n-    DK += off_z * stride_qz + off_h * stride_qh\n-    DV += off_z * stride_qz + off_h * stride_qh\n-    for start_n in range(0, num_block):\n-        if CAUSAL:\n-            lo = start_n * BLOCK_M\n-        else:\n-            lo = 0\n-        # initialize row/col offsets\n-        offs_qm = lo + tl.arange(0, BLOCK_M)\n-        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n-        offs_m = tl.arange(0, BLOCK_N)\n-        offs_k = tl.arange(0, BLOCK_DMODEL)\n-        # initialize pointers to value-like data\n-        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n-        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n-        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n-        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n-        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n-        # pointer to row-wise quantities in value-like data\n-        D_ptrs = D + off_hz * N_CTX\n-        l_ptrs = L + off_hz * N_CTX\n-        # initialize dv amd dk\n-        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n-        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n-        # k and v stay in SRAM throughout\n-        k = tl.load(k_ptrs)\n-        v = tl.load(v_ptrs)\n-        # loop over rows\n-        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n-            offs_m_curr = start_m + offs_m\n-            # load q, k, v, do on-chip\n-            q = tl.load(q_ptrs)\n-            # recompute p = softmax(qk, dim=-1).T\n-            if CAUSAL:\n-                qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\"-inf\"))\n-            else:\n-                qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-            qk += tl.dot(q, tl.trans(k))\n-            qk *= qk_scale\n-            l_i = tl.load(l_ptrs + offs_m_curr)\n-            p = tl.math.exp2(qk - l_i[:, None])\n-            # compute dv\n-            do = tl.load(do_ptrs)\n-            dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\n-            # compute dp = dot(v, do)\n-            Di = tl.load(D_ptrs + offs_m_curr)\n-            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n-            dp += tl.dot(do, tl.trans(v))\n-            # compute ds = p * (dp - delta[:, None])\n-            ds = p * dp * sm_scale\n-            # compute dk = dot(ds.T, q)\n-            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)\n-            # compute dq\n-            dq = tl.load(dq_ptrs)\n-            dq += tl.dot(ds.to(Q.dtype.element_ty), k)\n-            tl.store(dq_ptrs, dq)\n-            # increment pointers\n-            dq_ptrs += BLOCK_M * stride_qm\n-            q_ptrs += BLOCK_M * stride_qm\n-            do_ptrs += BLOCK_M * stride_qm\n-        # write-back\n-        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n-        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n-        tl.store(dv_ptrs, dv)\n-        tl.store(dk_ptrs, dk)\n-\n-\n-empty = torch.empty(128, device=\"cuda\")\n-\n-\n-class _attention(torch.autograd.Function):\n-\n-    @staticmethod\n-    def forward(ctx, q, k, v, causal, sm_scale):\n-        # shape constraints\n-        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n-        assert Lq == Lk and Lk == Lv\n-        assert Lk in {16, 32, 64, 128}\n-        o = torch.empty_like(q)\n-        BLOCK_M = 128\n-        BLOCK_N = 64\n-        grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n-        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n-\n-        num_warps = 4 if Lk <= 64 else 8\n-        _fwd_kernel[grid](\n-            q, k, v, sm_scale,\n-            L,\n-            o,\n-            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n-            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n-            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n-            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n-            q.shape[0], q.shape[1], q.shape[2],\n-            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=Lk,\n-            IS_CAUSAL=causal,\n-            num_warps=num_warps,\n-            num_stages=4)\n-\n-        ctx.save_for_backward(q, k, v, o, L)\n-        ctx.grid = grid\n-        ctx.sm_scale = sm_scale\n-        ctx.BLOCK_DMODEL = Lk\n-        ctx.causal = causal\n-        return o\n-\n-    @staticmethod\n-    def backward(ctx, do):\n-        BLOCK = 128\n-        q, k, v, o, L = ctx.saved_tensors\n-        do = do.contiguous()\n-        dq = torch.zeros_like(q, dtype=torch.float32)\n-        dk = torch.empty_like(k)\n-        dv = torch.empty_like(v)\n-        delta = torch.empty_like(L)\n-        _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n-            o, do,\n-            delta,\n-            BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n-        )\n-        _bwd_kernel[(ctx.grid[1],)](\n-            q, k, v, ctx.sm_scale,\n-            o, do,\n-            dq, dk, dv,\n-            L, delta,\n-            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n-            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n-            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n-            q.shape[0], q.shape[1], q.shape[2],\n-            ctx.grid[0],\n-            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n-            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n-            CAUSAL=ctx.causal,\n-            num_stages=1,\n-        )\n-        return dq, dk, dv, None, None\n-\n-\n-attention = _attention.apply\n-\n-\n-@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(6, 9, 1024, 64)])\n-@pytest.mark.parametrize('causal', [False, True])\n-def test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\n-    torch.manual_seed(20)\n-    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n-    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n-    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n-    sm_scale = 0.5\n-    dout = torch.randn_like(q)\n-    # reference implementation\n-    M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n-    p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n-    if causal:\n-        p[:, :, M == 0] = float(\"-inf\")\n-    p = torch.softmax(p.float(), dim=-1).half()\n-    # p = torch.exp(p)\n-    ref_out = torch.matmul(p, v)\n-    ref_out.backward(dout)\n-    ref_dv, v.grad = v.grad.clone(), None\n-    ref_dk, k.grad = k.grad.clone(), None\n-    ref_dq, q.grad = q.grad.clone(), None\n-    # triton implementation\n-    tri_out = attention(q, k, v, causal, sm_scale).half()\n-    tri_out.backward(dout)\n-    tri_dv, v.grad = v.grad.clone(), None\n-    tri_dk, k.grad = k.grad.clone(), None\n-    tri_dq, q.grad = q.grad.clone(), None\n-    # compare\n-    assert torch.allclose(ref_out, tri_out, atol=1e-2, rtol=0)\n-    assert torch.allclose(ref_dv, tri_dv, atol=1e-2, rtol=0)\n-    assert torch.allclose(ref_dk, tri_dk, atol=1e-2, rtol=0)\n-    assert torch.allclose(ref_dq, tri_dq, atol=1e-2, rtol=0)\n-\n-\n-try:\n-    from flash_attn.flash_attn_interface import \\\n-        flash_attn_qkvpacked_func as flash_attn_func\n-    FLASH_VER = 2\n-except BaseException:\n-    try:\n-        from flash_attn.flash_attn_interface import flash_attn_func\n-        FLASH_VER = 1\n-    except BaseException:\n-        FLASH_VER = None\n-HAS_FLASH = FLASH_VER is not None\n-\n-BATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\n-# vary seq length for fixed head and batch=4\n-configs = [triton.testing.Benchmark(\n-    x_names=['N_CTX'],\n-    x_vals=[2**i for i in range(10, 15)],\n-    line_arg='provider',\n-    line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n-    line_names=['Triton'] + ([f'Flash-{FLASH_VER}'] if HAS_FLASH else []),\n-    styles=[('red', '-'), ('blue', '-')],\n-    ylabel='ms',\n-    plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n-    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode, 'causal': causal}\n-) for mode in ['fwd', 'bwd'] for causal in [False, True]]\n-\n-\n-@triton.testing.perf_report(configs)\n-def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, causal, mode, provider, dtype=torch.float16, device=\"cuda\"):\n-    assert mode in ['fwd', 'bwd']\n-    warmup = 25\n-    rep = 100\n-    if provider == \"triton\":\n-        q = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n-        k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n-        v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n-        sm_scale = 1.3\n-        fn = lambda: attention(q, k, v, causal, sm_scale)\n-        if mode == 'bwd':\n-            o = fn()\n-            do = torch.randn_like(o)\n-            fn = lambda: o.backward(do, retain_graph=True)\n-        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n-    if provider == \"flash\":\n-        qkv = torch.randn((BATCH, N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\n-        if FLASH_VER == 1:\n-            lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n-            cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n-            cu_seqlens[1:] = lengths.cumsum(0)\n-            qkv = qkv.reshape(BATCH * N_CTX, 3, H, D_HEAD)\n-            fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=causal)\n-        elif FLASH_VER == 2:\n-            fn = lambda: flash_attn_func(qkv, causal=causal)\n-        else:\n-            raise ValueError(f'unknown {FLASH_VER = }')\n-        if mode == 'bwd':\n-            o = fn()\n-            do = torch.randn_like(o)\n-            fn = lambda: o.backward(do, retain_graph=True)\n-        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n-    flops_per_matmul = 2. * BATCH * H * N_CTX * N_CTX * D_HEAD\n-    total_flops = 2 * flops_per_matmul\n-    if causal:\n-        total_flops *= 0.5\n-    if mode == 'bwd':\n-        total_flops *= 2.5  # 2.0(bwd) + 0.5(recompute)\n-    return total_flops / ms * 1e-9\n-\n-\n-# only works on post-Ampere GPUs right now\n-bench_flash_attention.run(save_path='.', print_data=True)"}, {"filename": "main/_downloads/62d97d49a32414049819dd8bb8378080/01-vector-add.py", "status": "removed", "additions": 0, "deletions": 139, "changes": 139, "file_content_changes": "@@ -1,139 +0,0 @@\n-\"\"\"\n-Vector Addition\n-===============\n-\n-In this tutorial, you will write a simple vector addition using Triton.\n-\n-In doing so, you will learn about:\n-\n-* The basic programming model of Triton.\n-\n-* The `triton.jit` decorator, which is used to define Triton kernels.\n-\n-* The best practices for validating and benchmarking your custom ops against native reference implementations.\n-\n-\"\"\"\n-\n-# %%\n-# Compute Kernel\n-# --------------\n-\n-import torch\n-\n-import triton\n-import triton.language as tl\n-\n-\n-@triton.jit\n-def add_kernel(\n-    x_ptr,  # *Pointer* to first input vector.\n-    y_ptr,  # *Pointer* to second input vector.\n-    output_ptr,  # *Pointer* to output vector.\n-    n_elements,  # Size of the vector.\n-    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n-                 # NOTE: `constexpr` so it can be used as a shape value.\n-):\n-    # There are multiple 'programs' processing different data. We identify which program\n-    # we are here:\n-    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n-    # This program will process inputs that are offset from the initial data.\n-    # For instance, if you had a vector of length 256 and block_size of 64, the programs\n-    # would each access the elements [0:64, 64:128, 128:192, 192:256].\n-    # Note that offsets is a list of pointers:\n-    block_start = pid * BLOCK_SIZE\n-    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n-    # Create a mask to guard memory operations against out-of-bounds accesses.\n-    mask = offsets < n_elements\n-    # Load x and y from DRAM, masking out any extra elements in case the input is not a\n-    # multiple of the block size.\n-    x = tl.load(x_ptr + offsets, mask=mask)\n-    y = tl.load(y_ptr + offsets, mask=mask)\n-    output = x + y\n-    # Write x + y back to DRAM.\n-    tl.store(output_ptr + offsets, output, mask=mask)\n-\n-\n-# %%\n-# Let's also declare a helper function to (1) allocate the `z` tensor\n-# and (2) enqueue the above kernel with appropriate grid/block sizes:\n-\n-\n-def add(x: torch.Tensor, y: torch.Tensor):\n-    # We need to preallocate the output.\n-    output = torch.empty_like(x)\n-    assert x.is_cuda and y.is_cuda and output.is_cuda\n-    n_elements = output.numel()\n-    # The SPMD launch grid denotes the number of kernel instances that run in parallel.\n-    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\n-    # In this case, we use a 1D grid where the size is the number of blocks:\n-    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n-    # NOTE:\n-    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.\n-    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.\n-    #  - Don't forget to pass meta-parameters as keywords arguments.\n-    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n-    # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\n-    # running asynchronously at this point.\n-    return output\n-\n-\n-# %%\n-# We can now use the above function to compute the element-wise sum of two `torch.tensor` objects and test its correctness:\n-\n-torch.manual_seed(0)\n-size = 98432\n-x = torch.rand(size, device='cuda')\n-y = torch.rand(size, device='cuda')\n-output_torch = x + y\n-output_triton = add(x, y)\n-print(output_torch)\n-print(output_triton)\n-print(\n-    f'The maximum difference between torch and triton is '\n-    f'{torch.max(torch.abs(output_torch - output_triton))}'\n-)\n-\n-# %%\n-# Seems like we're good to go!\n-\n-# %%\n-# Benchmark\n-# ---------\n-#\n-# We can now benchmark our custom op on vectors of increasing sizes to get a sense of how it does relative to PyTorch.\n-# To make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of our custom ops.\n-# for different problem sizes.\n-\n-\n-@triton.testing.perf_report(\n-    triton.testing.Benchmark(\n-        x_names=['size'],  # Argument names to use as an x-axis for the plot.\n-        x_vals=[\n-            2 ** i for i in range(12, 28, 1)\n-        ],  # Different possible values for `x_name`.\n-        x_log=True,  # x axis is logarithmic.\n-        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n-        line_vals=['triton', 'torch'],  # Possible values for `line_arg`.\n-        line_names=['Triton', 'Torch'],  # Label name for the lines.\n-        styles=[('blue', '-'), ('green', '-')],  # Line styles.\n-        ylabel='GB/s',  # Label name for the y-axis.\n-        plot_name='vector-add-performance',  # Name for the plot. Used also as a file name for saving the plot.\n-        args={},  # Values for function arguments not in `x_names` and `y_name`.\n-    )\n-)\n-def benchmark(size, provider):\n-    x = torch.rand(size, device='cuda', dtype=torch.float32)\n-    y = torch.rand(size, device='cuda', dtype=torch.float32)\n-    quantiles = [0.5, 0.2, 0.8]\n-    if provider == 'torch':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: x + y, quantiles=quantiles)\n-    if provider == 'triton':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: add(x, y), quantiles=quantiles)\n-    gbps = lambda ms: 12 * size / ms * 1e-6\n-    return gbps(ms), gbps(max_ms), gbps(min_ms)\n-\n-\n-# %%\n-# We can now run the decorated function above. Pass `print_data=True` to see the performance number, `show_plots=True` to plot them, and/or\n-# `save_path='/path/to/results/' to save them to disk along with raw CSV data:\n-benchmark.run(print_data=True, show_plots=True)"}, {"filename": "main/_downloads/662999063954282841dc90b8945f85ce/tutorials_jupyter.zip", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/763344228ae6bc253ed1a6cf586aa30d/tutorials_python.zip", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/77571465f7f4bd281d3a847dc2633146/07-math-functions.py", "status": "removed", "additions": 0, "deletions": 73, "changes": 73, "file_content_changes": "@@ -1,73 +0,0 @@\n-\"\"\"\n-Libdevice (`tl.math`) function\n-==============================\n-Triton can invoke a custom function from an external library.\n-In this example, we will use the `libdevice` library (a.k.a `math` in triton) to apply `asin` on a tensor.\n-Please refer to https://docs.nvidia.com/cuda/libdevice-users-guide/index.html regarding the semantics of all available libdevice functions.\n-In `triton/language/math.py`, we try to aggregate functions with the same computation but different data types together.\n-For example, both `__nv_asin` and `__nvasinf` calculate the principal value of the arc sine of the input, but `__nv_asin` operates on `double` and `__nv_asinf` operates on `float`.\n-Using triton, you can simply call `tl.math.asin`.\n-Triton automatically selects the correct underlying device function to invoke based on input and output types.\n-\"\"\"\n-\n-# %%\n-#  asin Kernel\n-# ------------\n-\n-import torch\n-\n-import triton\n-import triton.language as tl\n-\n-\n-@triton.jit\n-def asin_kernel(\n-        x_ptr,\n-        y_ptr,\n-        n_elements,\n-        BLOCK_SIZE: tl.constexpr,\n-):\n-    pid = tl.program_id(axis=0)\n-    block_start = pid * BLOCK_SIZE\n-    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n-    mask = offsets < n_elements\n-    x = tl.load(x_ptr + offsets, mask=mask)\n-    x = tl.math.asin(x)\n-    tl.store(y_ptr + offsets, x, mask=mask)\n-\n-# %%\n-#  Using the default libdevice library path\n-# -----------------------------------------\n-# We can use the default libdevice library path encoded in `triton/language/math.py`\n-\n-\n-torch.manual_seed(0)\n-size = 98432\n-x = torch.rand(size, device='cuda')\n-output_triton = torch.zeros(size, device='cuda')\n-output_torch = torch.asin(x)\n-assert x.is_cuda and output_triton.is_cuda\n-n_elements = output_torch.numel()\n-grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n-asin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024)\n-print(output_torch)\n-print(output_triton)\n-print(\n-    f'The maximum difference between torch and triton is '\n-    f'{torch.max(torch.abs(output_torch - output_triton))}'\n-)\n-\n-# %%\n-#  Customize the libdevice library path\n-# -------------------------------------\n-# We can also customize the libdevice library path by passing the path to the `libdevice` library to the `asin` kernel.\n-\n-output_triton = torch.empty_like(x)\n-asin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024,\n-                  extern_libs={'libdevice': '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'})\n-print(output_torch)\n-print(output_triton)\n-print(\n-    f'The maximum difference between torch and triton is '\n-    f'{torch.max(torch.abs(output_torch - output_triton))}'\n-)"}, {"filename": "main/_downloads/935c0dd0fbeb4b2e69588471cbb2d4b2/05-layer-norm.py", "status": "removed", "additions": 0, "deletions": 374, "changes": 374, "file_content_changes": "@@ -1,374 +0,0 @@\n-\"\"\"\n-Layer Normalization\n-====================\n-In this tutorial, you will write a high-performance layer normalization\n-kernel that runs faster than the PyTorch implementation.\n-\n-In doing so, you will learn about:\n-\n-* Implementing backward pass in Triton.\n-\n-* Implementing parallel reduction in Triton.\n-\n-\"\"\"\n-\n-# %%\n-# Motivations\n-# -----------\n-#\n-# The *LayerNorm* operator was first introduced in [BA2016]_ as a way to improve the performance\n-# of sequential models (e.g., Transformers) or neural networks with small batch size.\n-# It takes a vector :math:`x` as input and produces a vector :math:`y` of the same shape as output.\n-# The normalization is performed by subtracting the mean and dividing by the standard deviation of :math:`x`.\n-# After the normalization, a learnable linear transformation with weights :math:`w` and biases :math:`b` is applied.\n-# The forward pass can be expressed as follows:\n-#\n-# .. math::\n-#    y = \\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} } * w + b\n-#\n-# where :math:`\\epsilon` is a small constant added to the denominator for numerical stability.\n-# Let\u2019s first take a look at the forward pass implementation.\n-\n-import torch\n-\n-import triton\n-import triton.language as tl\n-\n-try:\n-    # This is https://github.com/NVIDIA/apex, NOT the apex on PyPi, so it\n-    # should not be added to extras_require in setup.py.\n-    import apex\n-    HAS_APEX = True\n-except ModuleNotFoundError:\n-    HAS_APEX = False\n-\n-\n-@triton.jit\n-def _layer_norm_fwd_fused(\n-    X,  # pointer to the input\n-    Y,  # pointer to the output\n-    W,  # pointer to the weights\n-    B,  # pointer to the biases\n-    Mean,  # pointer to the mean\n-    Rstd,  # pointer to the 1/std\n-    stride,  # how much to increase the pointer when moving by 1 row\n-    N,  # number of columns in X\n-    eps,  # epsilon to avoid division by zero\n-    BLOCK_SIZE: tl.constexpr,\n-):\n-    # Map the program id to the row of X and Y it should compute.\n-    row = tl.program_id(0)\n-    Y += row * stride\n-    X += row * stride\n-    # Compute mean\n-    mean = 0\n-    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n-    for off in range(0, N, BLOCK_SIZE):\n-        cols = off + tl.arange(0, BLOCK_SIZE)\n-        a = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n-        _mean += a\n-    mean = tl.sum(_mean, axis=0) / N\n-    # Compute variance\n-    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n-    for off in range(0, N, BLOCK_SIZE):\n-        cols = off + tl.arange(0, BLOCK_SIZE)\n-        x = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n-        x = tl.where(cols < N, x - mean, 0.)\n-        _var += x * x\n-    var = tl.sum(_var, axis=0) / N\n-    rstd = 1 / tl.sqrt(var + eps)\n-    # Write mean / rstd\n-    tl.store(Mean + row, mean)\n-    tl.store(Rstd + row, rstd)\n-    # Normalize and apply linear transformation\n-    for off in range(0, N, BLOCK_SIZE):\n-        cols = off + tl.arange(0, BLOCK_SIZE)\n-        mask = cols < N\n-        w = tl.load(W + cols, mask=mask)\n-        b = tl.load(B + cols, mask=mask)\n-        x = tl.load(X + cols, mask=mask, other=0.).to(tl.float32)\n-        x_hat = (x - mean) * rstd\n-        y = x_hat * w + b\n-        # Write output\n-        tl.store(Y + cols, y, mask=mask)\n-\n-\n-# %%\n-# Backward pass\n-# -------------\n-#\n-# The backward pass for the layer normalization operator is a bit more involved than the forward pass.\n-# Let :math:`\\hat{x}` be the normalized inputs :math:`\\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} }` before the linear transformation,\n-# the Vector-Jacobian Products (VJP) :math:`\\nabla_{x}` of :math:`x` are given by:\n-#\n-# .. math::\n-#    \\nabla_{x} = \\frac{1}{\\sigma}\\Big( \\nabla_{y} \\odot w - \\underbrace{ \\big( \\frac{1}{N} \\hat{x} \\cdot (\\nabla_{y} \\odot w) \\big) }_{c_1} \\odot \\hat{x} - \\underbrace{ \\frac{1}{N} \\nabla_{y} \\cdot w }_{c_2} \\Big)\n-#\n-# where :math:`\\odot` denotes the element-wise multiplication, :math:`\\cdot` denotes the dot product, and :math:`\\sigma` is the standard deviation.\n-# :math:`c_1` and :math:`c_2` are intermediate constants that improve the readability of the following implementation.\n-#\n-# For the weights :math:`w` and biases :math:`b`, the VJPs :math:`\\nabla_{w}` and :math:`\\nabla_{b}` are more straightforward:\n-#\n-# .. math::\n-#    \\nabla_{w} = \\nabla_{y} \\odot \\hat{x} \\quad \\text{and} \\quad \\nabla_{b} = \\nabla_{y}\n-#\n-# Since the same weights :math:`w` and biases :math:`b` are used for all rows in the same batch, their gradients need to sum up.\n-# To perform this step efficiently, we use a parallel reduction strategy: each kernel instance accumulates\n-# partial :math:`\\nabla_{w}` and :math:`\\nabla_{b}` across certain rows into one of :math:`\\text{GROUP_SIZE_M}` independent buffers.\n-# These buffers stay in the L2 cache and then are further reduced by another function to compute the actual :math:`\\nabla_{w}` and :math:`\\nabla_{b}`.\n-#\n-# Let the number of input rows :math:`M = 4` and :math:`\\text{GROUP_SIZE_M} = 2`,\n-# here's a diagram of the parallel reduction strategy for :math:`\\nabla_{w}` (:math:`\\nabla_{b}` is omitted for brevity):\n-#\n-#   .. image:: parallel_reduction.png\n-#\n-# In Stage 1, the rows of X that have the same color share the same buffer and thus a lock is used to ensure that only one kernel instance writes to the buffer at a time.\n-# In Stage 2, the buffers are further reduced to compute the final :math:`\\nabla_{w}` and :math:`\\nabla_{b}`.\n-# In the following implementation, Stage 1 is implemented by the function :code:`_layer_norm_bwd_dx_fused` and Stage 2 is implemented by the function :code:`_layer_norm_bwd_dwdb`.\n-\n-@triton.jit\n-def _layer_norm_bwd_dx_fused(\n-    DX,  # pointer to the input gradient\n-    DY,  # pointer to the output gradient\n-    DW,  # pointer to the partial sum of weights gradient\n-    DB,  # pointer to the partial sum of biases gradient\n-    X,   # pointer to the input\n-    W,   # pointer to the weights\n-    B,   # pointer to the biases\n-    Mean,   # pointer to the mean\n-    Rstd,   # pointer to the 1/std\n-    Lock,  # pointer to the lock\n-    stride,  # how much to increase the pointer when moving by 1 row\n-    N,  # number of columns in X\n-    eps,  # epsilon to avoid division by zero\n-    GROUP_SIZE_M: tl.constexpr,\n-    BLOCK_SIZE_N: tl.constexpr\n-):\n-    # Map the program id to the elements of X, DX, and DY it should compute.\n-    row = tl.program_id(0)\n-    cols = tl.arange(0, BLOCK_SIZE_N)\n-    mask = cols < N\n-    X += row * stride\n-    DY += row * stride\n-    DX += row * stride\n-    # Offset locks and weights/biases gradient pointer for parallel reduction\n-    lock_id = row % GROUP_SIZE_M\n-    Lock += lock_id\n-    Count = Lock + GROUP_SIZE_M\n-    DW = DW + lock_id * N + cols\n-    DB = DB + lock_id * N + cols\n-    # Load data to SRAM\n-    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n-    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n-    w = tl.load(W + cols, mask=mask).to(tl.float32)\n-    mean = tl.load(Mean + row)\n-    rstd = tl.load(Rstd + row)\n-    # Compute dx\n-    xhat = (x - mean) * rstd\n-    wdy = w * dy\n-    xhat = tl.where(mask, xhat, 0.)\n-    wdy = tl.where(mask, wdy, 0.)\n-    c1 = tl.sum(xhat * wdy, axis=0) / N\n-    c2 = tl.sum(wdy, axis=0) / N\n-    dx = (wdy - (xhat * c1 + c2)) * rstd\n-    # Write dx\n-    tl.store(DX + cols, dx, mask=mask)\n-    # Accumulate partial sums for dw/db\n-    partial_dw = (dy * xhat).to(w.dtype)\n-    partial_db = (dy).to(w.dtype)\n-    while tl.atomic_cas(Lock, 0, 1) == 1:\n-        pass\n-    count = tl.load(Count)\n-    # First store doesn't accumulate\n-    if count == 0:\n-        tl.atomic_xchg(Count, 1)\n-    else:\n-        partial_dw += tl.load(DW, mask=mask)\n-        partial_db += tl.load(DB, mask=mask)\n-    tl.store(DW, partial_dw, mask=mask)\n-    tl.store(DB, partial_db, mask=mask)\n-    # Release the lock\n-    tl.atomic_xchg(Lock, 0)\n-\n-\n-@triton.jit\n-def _layer_norm_bwd_dwdb(\n-    DW,  # pointer to the partial sum of weights gradient\n-    DB,  # pointer to the partial sum of biases gradient\n-    FINAL_DW,  # pointer to the weights gradient\n-    FINAL_DB,  # pointer to the biases gradient\n-    M,  # GROUP_SIZE_M\n-    N,  # number of columns\n-    BLOCK_SIZE_M: tl.constexpr,\n-    BLOCK_SIZE_N: tl.constexpr\n-):\n-    # Map the program id to the elements of DW and DB it should compute.\n-    pid = tl.program_id(0)\n-    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n-    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-    db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-    # Iterate through the rows of DW and DB to sum the partial sums.\n-    for i in range(0, M, BLOCK_SIZE_M):\n-        rows = i + tl.arange(0, BLOCK_SIZE_M)\n-        mask = (rows[:, None] < M) & (cols[None, :] < N)\n-        offs = rows[:, None] * N + cols[None, :]\n-        dw += tl.load(DW + offs, mask=mask, other=0.)\n-        db += tl.load(DB + offs, mask=mask, other=0.)\n-    # Write the final sum to the output.\n-    sum_dw = tl.sum(dw, axis=0)\n-    sum_db = tl.sum(db, axis=0)\n-    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n-    tl.store(FINAL_DB + cols, sum_db, mask=cols < N)\n-\n-\n-# %%\n-# Benchmark\n-# ---------\n-#\n-# We can now compare the performance of our kernel against that of PyTorch.\n-# Here we focus on inputs that have Less than 64KB per feature.\n-# Specifically, one can set :code:`'mode': 'backward'` to benchmark the backward pass.\n-\n-\n-class LayerNorm(torch.autograd.Function):\n-\n-    @staticmethod\n-    def forward(ctx, x, normalized_shape, weight, bias, eps):\n-        # allocate output\n-        y = torch.empty_like(x)\n-        # reshape input data into 2D tensor\n-        x_arg = x.reshape(-1, x.shape[-1])\n-        M, N = x_arg.shape\n-        mean = torch.empty((M, ), dtype=torch.float32, device='cuda')\n-        rstd = torch.empty((M, ), dtype=torch.float32, device='cuda')\n-        # Less than 64KB per feature: enqueue fused kernel\n-        MAX_FUSED_SIZE = 65536 // x.element_size()\n-        BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n-        if N > BLOCK_SIZE:\n-            raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n-        # heuristics for number of warps\n-        num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\n-        # enqueue kernel\n-        _layer_norm_fwd_fused[(M,)](x_arg, y, weight, bias, mean, rstd,\n-                                    x_arg.stride(0), N, eps,\n-                                    BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)\n-        ctx.save_for_backward(x, weight, bias, mean, rstd)\n-        ctx.BLOCK_SIZE = BLOCK_SIZE\n-        ctx.num_warps = num_warps\n-        ctx.eps = eps\n-        return y\n-\n-    @staticmethod\n-    def backward(ctx, dy):\n-        x, w, b, m, v = ctx.saved_tensors\n-        # heuristics for amount of parallel reduction stream for DW/DB\n-        N = w.shape[0]\n-        GROUP_SIZE_M = 64\n-        if N <= 8192: GROUP_SIZE_M = 96\n-        if N <= 4096: GROUP_SIZE_M = 128\n-        if N <= 1024: GROUP_SIZE_M = 256\n-        # allocate output\n-        locks = torch.zeros(2 * GROUP_SIZE_M, dtype=torch.int32, device='cuda')\n-        _dw = torch.empty((GROUP_SIZE_M, w.shape[0]), dtype=x.dtype, device=w.device)\n-        _db = torch.empty((GROUP_SIZE_M, w.shape[0]), dtype=x.dtype, device=w.device)\n-        dw = torch.empty((w.shape[0],), dtype=w.dtype, device=w.device)\n-        db = torch.empty((w.shape[0],), dtype=w.dtype, device=w.device)\n-        dx = torch.empty_like(dy)\n-        # enqueue kernel using forward pass heuristics\n-        # also compute partial sums for DW and DB\n-        x_arg = x.reshape(-1, x.shape[-1])\n-        M, N = x_arg.shape\n-        _layer_norm_bwd_dx_fused[(M,)](dx, dy, _dw, _db, x, w, b, m, v, locks,\n-                                       x_arg.stride(0), N, ctx.eps,\n-                                       BLOCK_SIZE_N=ctx.BLOCK_SIZE,\n-                                       GROUP_SIZE_M=GROUP_SIZE_M,\n-                                       num_warps=ctx.num_warps)\n-        grid = lambda meta: [triton.cdiv(N, meta['BLOCK_SIZE_N'])]\n-        # accumulate partial sums in separate kernel\n-        _layer_norm_bwd_dwdb[grid](_dw, _db, dw, db, GROUP_SIZE_M, N,\n-                                   BLOCK_SIZE_M=32,\n-                                   BLOCK_SIZE_N=128)\n-        return dx, None, dw, db, None\n-\n-\n-layer_norm = LayerNorm.apply\n-\n-\n-def test_layer_norm(M, N, dtype, eps=1e-5, device='cuda'):\n-    # create data\n-    x_shape = (M, N)\n-    w_shape = (x_shape[-1], )\n-    weight = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n-    bias = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n-    x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\n-    dy = .1 * torch.randn_like(x)\n-    x.requires_grad_(True)\n-    # forward pass\n-    y_tri = layer_norm(x, w_shape, weight, bias, eps)\n-    y_ref = torch.nn.functional.layer_norm(x, w_shape, weight, bias, eps).to(dtype)\n-    # backward pass (triton)\n-    y_tri.backward(dy, retain_graph=True)\n-    dx_tri, dw_tri, db_tri = [_.grad.clone() for _ in [x, weight, bias]]\n-    x.grad, weight.grad, bias.grad = None, None, None\n-    # backward pass (torch)\n-    y_ref.backward(dy, retain_graph=True)\n-    dx_ref, dw_ref, db_ref = [_.grad.clone() for _ in [x, weight, bias]]\n-    # compare\n-    assert torch.allclose(y_tri, y_ref, atol=1e-2, rtol=0)\n-    assert torch.allclose(dx_tri, dx_ref, atol=1e-2, rtol=0)\n-    assert torch.allclose(db_tri, db_ref, atol=1e-2, rtol=0)\n-    assert torch.allclose(dw_tri, dw_ref, atol=1e-2, rtol=0)\n-\n-\n-@triton.testing.perf_report(\n-    triton.testing.Benchmark(\n-        x_names=['N'],\n-        x_vals=[512 * i for i in range(2, 32)],\n-        line_arg='provider',\n-        line_vals=['triton', 'torch'] + (['apex'] if HAS_APEX else []),\n-        line_names=['Triton', 'Torch'] + (['Apex'] if HAS_APEX else []),\n-        styles=[('blue', '-'), ('green', '-'), ('orange', '-')],\n-        ylabel='GB/s',\n-        plot_name='layer-norm-backward',\n-        args={'M': 4096, 'dtype': torch.float16, 'mode': 'backward'}\n-    )\n-)\n-def bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='cuda'):\n-    # create data\n-    x_shape = (M, N)\n-    w_shape = (x_shape[-1], )\n-    weight = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n-    bias = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n-    x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\n-    dy = .1 * torch.randn_like(x)\n-    x.requires_grad_(True)\n-    quantiles = [0.5, 0.2, 0.8]\n-    # utility functions\n-    if provider == 'triton':\n-        y_fwd = lambda: layer_norm(x, w_shape, weight, bias, eps)\n-    if provider == 'torch':\n-        y_fwd = lambda: torch.nn.functional.layer_norm(x, w_shape, weight, bias, eps)\n-    if provider == 'apex':\n-        apex_layer_norm = apex.normalization.FusedLayerNorm(w_shape).to(x.device).to(x.dtype)\n-        y_fwd = lambda: apex_layer_norm(x)\n-    # forward pass\n-    if mode == 'forward':\n-        gbps = lambda ms: 2 * x.numel() * x.element_size() / ms * 1e-6\n-        ms, min_ms, max_ms = triton.testing.do_bench(y_fwd, quantiles=quantiles, rep=500)\n-    # backward pass\n-    if mode == 'backward':\n-        gbps = lambda ms: 3 * x.numel() * x.element_size() / ms * 1e-6\n-        y = y_fwd()\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: y.backward(dy, retain_graph=True),\n-                                                     quantiles=quantiles, grad_to_none=[x], rep=500)\n-    return gbps(ms), gbps(max_ms), gbps(min_ms)\n-\n-\n-test_layer_norm(1151, 8192, torch.float16)\n-bench_layer_norm.run(save_path='.', print_data=True)\n-\n-# %%\n-# References\n-# ----------\n-#\n-# .. [BA2016] Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \"Layer Normalization\", Arxiv 2016"}, {"filename": "main/_downloads/9705af6f33c6e66c6fa78f2394331536/08-experimental-block-pointer.ipynb", "status": "removed", "additions": 0, "deletions": 107, "changes": 107, "file_content_changes": "@@ -1,107 +0,0 @@\n-{\n-  \"cells\": [\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"%matplotlib inline\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"\\n# Block Pointer (Experimental)\\nThis tutorial will guide you through writing a matrix multiplication algorithm that utilizes block pointer semantics.\\nThese semantics are more friendly for Triton to optimize and can result in better performance on specific hardware.\\nNote that this feature is still experimental and may change in the future.\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Motivations\\nIn the previous matrix multiplication tutorial, we constructed blocks of values by de-referencing blocks of pointers,\\ni.e., :code:`load(block<pointer_type<element_type>>) -> block<element_type>`, which involved loading blocks of\\nelements from memory. This approach allowed for flexibility in using hardware-managed cache and implementing complex\\ndata structures, such as tensors of trees or unstructured look-up tables.\\n\\nHowever, the drawback of this approach is that it relies heavily on complex optimization passes by the compiler to\\noptimize memory access patterns. This can result in brittle code that may suffer from performance degradation when the\\noptimizer fails to perform adequately. Additionally, as memory controllers specialize to accommodate dense spatial\\ndata structures commonly used in machine learning workloads, this problem is likely to worsen.\\n\\nTo address this issue, we will use block pointers :code:`pointer_type<block<element_type>>` and load them into\\n:code:`block<element_type>`, in which way gives better friendliness for the compiler to optimize memory access\\npatterns.\\n\\nLet's start with the previous matrix multiplication example and demonstrate how to rewrite it to utilize block pointer\\nsemantics.\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Make a Block Pointer\\nA block pointer pointers to a block in a parent tensor and is constructed by :code:`make_block_ptr` function,\\nwhich takes the following information as arguments:\\n\\n* :code:`base`: the base pointer to the parent tensor;\\n\\n* :code:`shape`: the shape of the parent tensor;\\n\\n* :code:`strides`: the strides of the parent tensor, which means how much to increase the pointer by when moving by 1 element in a specific axis;\\n\\n* :code:`offsets`: the offsets of the block;\\n\\n* :code:`block_shape`: the shape of the block;\\n\\n* :code:`order`: the order of the block, which means how the block is laid out in memory.\\n\\nFor example, to a block pointer to a :code:`BLOCK_SIZE_M * BLOCK_SIZE_K` block in a row-major 2D matrix A by\\noffsets :code:`(pid_m * BLOCK_SIZE_M, 0)` and strides :code:`(stride_am, stride_ak)`, we can use the following code\\n(exactly the same as the previous matrix multiplication tutorial):\\n\\n```python\\na_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\\n                                offsets=(pid_m * BLOCK_SIZE_M, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K),\\n                                order=(1, 0))\\n```\\nNote that the :code:`order` argument is set to :code:`(1, 0)`, which means the second axis is the inner dimension in\\nterms of storage, and the first axis is the outer dimension. This information may sound redundant, but it is necessary\\nfor some hardware backends to optimize for better performance.\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Load/Store a Block Pointer\\nTo load/store a block pointer, we can use :code:`load/store` function, which takes a block pointer as an argument,\\nde-references it, and loads/stores a block. You may mask some values in the block, here we have an extra argument\\n:code:`boundary_check` to specify whether to check the boundary of each axis for the block pointer. With check on,\\nout-of-bound values will be masked according to the :code:`padding_option` argument (load only), which can be\\n:code:`zero` or :code:`nan`. Temporarily, we do not support other values due to some hardware limitations. In this\\nmode of block pointer load/store does not support :code:`mask` or :code:`other` arguments in the legacy mode.\\n\\nSo to load the block pointer of A in the previous section, we can simply write\\n:code:`a = tl.load(a_block_ptr, boundary_check=(0, 1))`. Boundary check may cost extra performance, so if you can\\nguarantee that the block pointer is always in-bound in some axis, you can turn off the check by not passing the index\\ninto the :code:`boundary_check` argument. For example, if we know that :code:`M` is a multiple of\\n:code:`BLOCK_SIZE_M`, we can replace with :code:`a = tl.load(a_block_ptr, boundary_check=(1, ))`, since axis 0 is\\nalways in bound.\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Advance a Block Pointer\\nTo advance a block pointer, we can use :code:`advance` function, which takes a block pointer and the increment for\\neach axis as arguments and returns a new block pointer with the same shape and strides as the original one,\\nbut with the offsets advanced by the specified amount.\\n\\nFor example, to advance the block pointer by :code:`BLOCK_SIZE_K` in the second axis\\n(no need to multiply with strides), we can write :code:`a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))`.\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Final Result\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.autotune(\\n    configs=[\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\\n    ],\\n    key=['M', 'N', 'K'],\\n)\\n@triton.jit\\ndef matmul_kernel_with_block_pointers(\\n        # Pointers to matrices\\n        a_ptr, b_ptr, c_ptr,\\n        # Matrix dimensions\\n        M, N, K,\\n        # The stride variables represent how much to increase the ptr by when moving by 1\\n        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\\n        # by to get the element one row down (A has M rows).\\n        stride_am, stride_ak,\\n        stride_bk, stride_bn,\\n        stride_cm, stride_cn,\\n        # Meta-parameters\\n        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\\n        GROUP_SIZE_M: tl.constexpr\\n):\\n    \\\"\\\"\\\"Kernel for computing the matmul C = A x B.\\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\\n    \\\"\\\"\\\"\\n    # -----------------------------------------------------------\\n    # Map program ids `pid` to the block of C it should compute.\\n    # This is done in a grouped ordering to promote L2 data reuse.\\n    # See the matrix multiplication tutorial for details.\\n    pid = tl.program_id(axis=0)\\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\\n    group_id = pid // num_pid_in_group\\n    first_pid_m = group_id * GROUP_SIZE_M\\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\\n    pid_m = first_pid_m + (pid % group_size_m)\\n    pid_n = (pid % num_pid_in_group) // group_size_m\\n\\n    # ----------------------------------------------------------\\n    # Create block pointers for the first blocks of A and B.\\n    # We will advance this pointer as we move in the K direction and accumulate.\\n    # See above `Make a Block Pointer` section for details.\\n    a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\\n                                    offsets=(pid_m * BLOCK_SIZE_M, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K),\\n                                    order=(1, 0))\\n    b_block_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(stride_bk, stride_bn),\\n                                    offsets=(0, pid_n * BLOCK_SIZE_N), block_shape=(BLOCK_SIZE_K, BLOCK_SIZE_N),\\n                                    order=(1, 0))\\n\\n    # -----------------------------------------------------------\\n    # Iterate to compute a block of the C matrix.\\n    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block.\\n    # of fp32 values for higher accuracy.\\n    # `accumulator` will be converted back to fp16 after the loop.\\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\\n    for k in range(0, K, BLOCK_SIZE_K):\\n        # Load with boundary checks, no need to calculate the mask manually.\\n        # For better performance, you may remove some axis from the boundary\\n        # check, if you can guarantee that the access is always in-bound in\\n        # that axis.\\n        # See above `Load/Store a Block Pointer` section for details.\\n        a = tl.load(a_block_ptr, boundary_check=(0, 1))\\n        b = tl.load(b_block_ptr, boundary_check=(0, 1))\\n        # We accumulate along the K dimension.\\n        accumulator += tl.dot(a, b)\\n        # Advance the block pointer to the next K block.\\n        # See above `Advance a Block Pointer` section for details.\\n        a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))\\n        b_block_ptr = tl.advance(b_block_ptr, (BLOCK_SIZE_K, 0))\\n    c = accumulator.to(tl.float16)\\n\\n    # ----------------------------------------------------------------\\n    # Write back the block of the output matrix C with boundary checks.\\n    # See above `Load/Store a Block Pointer` section for details.\\n    c_block_ptr = tl.make_block_ptr(base=c_ptr, shape=(M, N), strides=(stride_cm, stride_cn),\\n                                    offsets=(pid_m * BLOCK_SIZE_M, pid_n * BLOCK_SIZE_N),\\n                                    block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_N), order=(1, 0))\\n    tl.store(c_block_ptr, c, boundary_check=(0, 1))\\n\\n\\n# We can now create a convenience wrapper function that only takes two input tensors,\\n# and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\\ndef matmul(a, b):\\n    # Check constraints.\\n    assert a.shape[1] == b.shape[0], \\\"Incompatible dimensions\\\"\\n    assert a.is_contiguous(), \\\"Matrix A must be contiguous\\\"\\n    assert b.is_contiguous(), \\\"Matrix B must be contiguous\\\"\\n    M, K = a.shape\\n    K, N = b.shape\\n    # Allocates output.\\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\\n    # 1D launch kernel where each block gets its own program.\\n    grid = lambda META: (\\n        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\\n    )\\n    matmul_kernel_with_block_pointers[grid](\\n        a, b, c,\\n        M, N, K,\\n        a.stride(0), a.stride(1),\\n        b.stride(0), b.stride(1),\\n        c.stride(0), c.stride(1),\\n    )\\n    return c\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Unit Test\\n\\nStill we can test our matrix multiplication with block pointers against a native torch implementation (i.e., cuBLAS).\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"torch.manual_seed(0)\\na = torch.randn((512, 512), device='cuda', dtype=torch.float16)\\nb = torch.randn((512, 512), device='cuda', dtype=torch.float16)\\ntriton_output = matmul(a, b)\\ntorch_output = torch.matmul(a, b)\\nprint(f\\\"triton_output={triton_output}\\\")\\nprint(f\\\"torch_output={torch_output}\\\")\\nif torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\\n    print(\\\"\\u2705 Triton and Torch match\\\")\\nelse:\\n    print(\\\"\\u274c Triton and Torch differ\\\")\"\n-      ]\n-    }\n-  ],\n-  \"metadata\": {\n-    \"kernelspec\": {\n-      \"display_name\": \"Python 3\",\n-      \"language\": \"python\",\n-      \"name\": \"python3\"\n-    },\n-    \"language_info\": {\n-      \"codemirror_mode\": {\n-        \"name\": \"ipython\",\n-        \"version\": 3\n-      },\n-      \"file_extension\": \".py\",\n-      \"mimetype\": \"text/x-python\",\n-      \"name\": \"python\",\n-      \"nbconvert_exporter\": \"python\",\n-      \"pygments_lexer\": \"ipython3\",\n-      \"version\": \"3.8.10\"\n-    }\n-  },\n-  \"nbformat\": 4,\n-  \"nbformat_minor\": 0\n-}\n\\ No newline at end of file"}, {"filename": "main/_downloads/ae7fff29e1b574187bc930ed94bcc353/05-layer-norm.ipynb", "status": "removed", "additions": 0, "deletions": 104, "changes": 104, "file_content_changes": "@@ -1,104 +0,0 @@\n-{\n-  \"cells\": [\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"%matplotlib inline\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"\\n# Layer Normalization\\nIn this tutorial, you will write a high-performance layer normalization\\nkernel that runs faster than the PyTorch implementation.\\n\\nIn doing so, you will learn about:\\n\\n* Implementing backward pass in Triton.\\n\\n* Implementing parallel reduction in Triton.\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Motivations\\n\\nThe *LayerNorm* operator was first introduced in [BA2016]_ as a way to improve the performance\\nof sequential models (e.g., Transformers) or neural networks with small batch size.\\nIt takes a vector $x$ as input and produces a vector $y$ of the same shape as output.\\nThe normalization is performed by subtracting the mean and dividing by the standard deviation of $x$.\\nAfter the normalization, a learnable linear transformation with weights $w$ and biases $b$ is applied.\\nThe forward pass can be expressed as follows:\\n\\n\\\\begin{align}y = \\\\frac{ x - \\\\text{E}[x] }{ \\\\sqrt{\\\\text{Var}(x) + \\\\epsilon} } * w + b\\\\end{align}\\n\\nwhere $\\\\epsilon$ is a small constant added to the denominator for numerical stability.\\nLet\\u2019s first take a look at the forward pass implementation.\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\ntry:\\n    # This is https://github.com/NVIDIA/apex, NOT the apex on PyPi, so it\\n    # should not be added to extras_require in setup.py.\\n    import apex\\n    HAS_APEX = True\\nexcept ModuleNotFoundError:\\n    HAS_APEX = False\\n\\n\\n@triton.jit\\ndef _layer_norm_fwd_fused(\\n    X,  # pointer to the input\\n    Y,  # pointer to the output\\n    W,  # pointer to the weights\\n    B,  # pointer to the biases\\n    Mean,  # pointer to the mean\\n    Rstd,  # pointer to the 1/std\\n    stride,  # how much to increase the pointer when moving by 1 row\\n    N,  # number of columns in X\\n    eps,  # epsilon to avoid division by zero\\n    BLOCK_SIZE: tl.constexpr,\\n):\\n    # Map the program id to the row of X and Y it should compute.\\n    row = tl.program_id(0)\\n    Y += row * stride\\n    X += row * stride\\n    # Compute mean\\n    mean = 0\\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\\n    for off in range(0, N, BLOCK_SIZE):\\n        cols = off + tl.arange(0, BLOCK_SIZE)\\n        a = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\\n        _mean += a\\n    mean = tl.sum(_mean, axis=0) / N\\n    # Compute variance\\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\\n    for off in range(0, N, BLOCK_SIZE):\\n        cols = off + tl.arange(0, BLOCK_SIZE)\\n        x = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\\n        x = tl.where(cols < N, x - mean, 0.)\\n        _var += x * x\\n    var = tl.sum(_var, axis=0) / N\\n    rstd = 1 / tl.sqrt(var + eps)\\n    # Write mean / rstd\\n    tl.store(Mean + row, mean)\\n    tl.store(Rstd + row, rstd)\\n    # Normalize and apply linear transformation\\n    for off in range(0, N, BLOCK_SIZE):\\n        cols = off + tl.arange(0, BLOCK_SIZE)\\n        mask = cols < N\\n        w = tl.load(W + cols, mask=mask)\\n        b = tl.load(B + cols, mask=mask)\\n        x = tl.load(X + cols, mask=mask, other=0.).to(tl.float32)\\n        x_hat = (x - mean) * rstd\\n        y = x_hat * w + b\\n        # Write output\\n        tl.store(Y + cols, y, mask=mask)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Backward pass\\n\\nThe backward pass for the layer normalization operator is a bit more involved than the forward pass.\\nLet $\\\\hat{x}$ be the normalized inputs $\\\\frac{ x - \\\\text{E}[x] }{ \\\\sqrt{\\\\text{Var}(x) + \\\\epsilon} }$ before the linear transformation,\\nthe Vector-Jacobian Products (VJP) $\\\\nabla_{x}$ of $x$ are given by:\\n\\n\\\\begin{align}\\\\nabla_{x} = \\\\frac{1}{\\\\sigma}\\\\Big( \\\\nabla_{y} \\\\odot w - \\\\underbrace{ \\\\big( \\\\frac{1}{N} \\\\hat{x} \\\\cdot (\\\\nabla_{y} \\\\odot w) \\\\big) }_{c_1} \\\\odot \\\\hat{x} - \\\\underbrace{ \\\\frac{1}{N} \\\\nabla_{y} \\\\cdot w }_{c_2} \\\\Big)\\\\end{align}\\n\\nwhere $\\\\odot$ denotes the element-wise multiplication, $\\\\cdot$ denotes the dot product, and $\\\\sigma$ is the standard deviation.\\n$c_1$ and $c_2$ are intermediate constants that improve the readability of the following implementation.\\n\\nFor the weights $w$ and biases $b$, the VJPs $\\\\nabla_{w}$ and $\\\\nabla_{b}$ are more straightforward:\\n\\n\\\\begin{align}\\\\nabla_{w} = \\\\nabla_{y} \\\\odot \\\\hat{x} \\\\quad \\\\text{and} \\\\quad \\\\nabla_{b} = \\\\nabla_{y}\\\\end{align}\\n\\nSince the same weights $w$ and biases $b$ are used for all rows in the same batch, their gradients need to sum up.\\nTo perform this step efficiently, we use a parallel reduction strategy: each kernel instance accumulates\\npartial $\\\\nabla_{w}$ and $\\\\nabla_{b}$ across certain rows into one of $\\\\text{GROUP_SIZE_M}$ independent buffers.\\nThese buffers stay in the L2 cache and then are further reduced by another function to compute the actual $\\\\nabla_{w}$ and $\\\\nabla_{b}$.\\n\\nLet the number of input rows $M = 4$ and $\\\\text{GROUP_SIZE_M} = 2$,\\nhere's a diagram of the parallel reduction strategy for $\\\\nabla_{w}$ ($\\\\nabla_{b}$ is omitted for brevity):\\n\\n  .. image:: parallel_reduction.png\\n\\nIn Stage 1, the rows of X that have the same color share the same buffer and thus a lock is used to ensure that only one kernel instance writes to the buffer at a time.\\nIn Stage 2, the buffers are further reduced to compute the final $\\\\nabla_{w}$ and $\\\\nabla_{b}$.\\nIn the following implementation, Stage 1 is implemented by the function :code:`_layer_norm_bwd_dx_fused` and Stage 2 is implemented by the function :code:`_layer_norm_bwd_dwdb`.\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"@triton.jit\\ndef _layer_norm_bwd_dx_fused(\\n    DX,  # pointer to the input gradient\\n    DY,  # pointer to the output gradient\\n    DW,  # pointer to the partial sum of weights gradient\\n    DB,  # pointer to the partial sum of biases gradient\\n    X,   # pointer to the input\\n    W,   # pointer to the weights\\n    B,   # pointer to the biases\\n    Mean,   # pointer to the mean\\n    Rstd,   # pointer to the 1/std\\n    Lock,  # pointer to the lock\\n    stride,  # how much to increase the pointer when moving by 1 row\\n    N,  # number of columns in X\\n    eps,  # epsilon to avoid division by zero\\n    GROUP_SIZE_M: tl.constexpr,\\n    BLOCK_SIZE_N: tl.constexpr\\n):\\n    # Map the program id to the elements of X, DX, and DY it should compute.\\n    row = tl.program_id(0)\\n    cols = tl.arange(0, BLOCK_SIZE_N)\\n    mask = cols < N\\n    X += row * stride\\n    DY += row * stride\\n    DX += row * stride\\n    # Offset locks and weights/biases gradient pointer for parallel reduction\\n    lock_id = row % GROUP_SIZE_M\\n    Lock += lock_id\\n    Count = Lock + GROUP_SIZE_M\\n    DW = DW + lock_id * N + cols\\n    DB = DB + lock_id * N + cols\\n    # Load data to SRAM\\n    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\\n    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\\n    mean = tl.load(Mean + row)\\n    rstd = tl.load(Rstd + row)\\n    # Compute dx\\n    xhat = (x - mean) * rstd\\n    wdy = w * dy\\n    xhat = tl.where(mask, xhat, 0.)\\n    wdy = tl.where(mask, wdy, 0.)\\n    c1 = tl.sum(xhat * wdy, axis=0) / N\\n    c2 = tl.sum(wdy, axis=0) / N\\n    dx = (wdy - (xhat * c1 + c2)) * rstd\\n    # Write dx\\n    tl.store(DX + cols, dx, mask=mask)\\n    # Accumulate partial sums for dw/db\\n    partial_dw = (dy * xhat).to(w.dtype)\\n    partial_db = (dy).to(w.dtype)\\n    while tl.atomic_cas(Lock, 0, 1) == 1:\\n        pass\\n    count = tl.load(Count)\\n    # First store doesn't accumulate\\n    if count == 0:\\n        tl.atomic_xchg(Count, 1)\\n    else:\\n        partial_dw += tl.load(DW, mask=mask)\\n        partial_db += tl.load(DB, mask=mask)\\n    tl.store(DW, partial_dw, mask=mask)\\n    tl.store(DB, partial_db, mask=mask)\\n    # Release the lock\\n    tl.atomic_xchg(Lock, 0)\\n\\n\\n@triton.jit\\ndef _layer_norm_bwd_dwdb(\\n    DW,  # pointer to the partial sum of weights gradient\\n    DB,  # pointer to the partial sum of biases gradient\\n    FINAL_DW,  # pointer to the weights gradient\\n    FINAL_DB,  # pointer to the biases gradient\\n    M,  # GROUP_SIZE_M\\n    N,  # number of columns\\n    BLOCK_SIZE_M: tl.constexpr,\\n    BLOCK_SIZE_N: tl.constexpr\\n):\\n    # Map the program id to the elements of DW and DB it should compute.\\n    pid = tl.program_id(0)\\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\\n    db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\\n    # Iterate through the rows of DW and DB to sum the partial sums.\\n    for i in range(0, M, BLOCK_SIZE_M):\\n        rows = i + tl.arange(0, BLOCK_SIZE_M)\\n        mask = (rows[:, None] < M) & (cols[None, :] < N)\\n        offs = rows[:, None] * N + cols[None, :]\\n        dw += tl.load(DW + offs, mask=mask, other=0.)\\n        db += tl.load(DB + offs, mask=mask, other=0.)\\n    # Write the final sum to the output.\\n    sum_dw = tl.sum(dw, axis=0)\\n    sum_db = tl.sum(db, axis=0)\\n    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\\n    tl.store(FINAL_DB + cols, sum_db, mask=cols < N)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Benchmark\\n\\nWe can now compare the performance of our kernel against that of PyTorch.\\nHere we focus on inputs that have Less than 64KB per feature.\\nSpecifically, one can set :code:`'mode': 'backward'` to benchmark the backward pass.\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"class LayerNorm(torch.autograd.Function):\\n\\n    @staticmethod\\n    def forward(ctx, x, normalized_shape, weight, bias, eps):\\n        # allocate output\\n        y = torch.empty_like(x)\\n        # reshape input data into 2D tensor\\n        x_arg = x.reshape(-1, x.shape[-1])\\n        M, N = x_arg.shape\\n        mean = torch.empty((M, ), dtype=torch.float32, device='cuda')\\n        rstd = torch.empty((M, ), dtype=torch.float32, device='cuda')\\n        # Less than 64KB per feature: enqueue fused kernel\\n        MAX_FUSED_SIZE = 65536 // x.element_size()\\n        BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\\n        if N > BLOCK_SIZE:\\n            raise RuntimeError(\\\"This layer norm doesn't support feature dim >= 64KB.\\\")\\n        # heuristics for number of warps\\n        num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\\n        # enqueue kernel\\n        _layer_norm_fwd_fused[(M,)](x_arg, y, weight, bias, mean, rstd,\\n                                    x_arg.stride(0), N, eps,\\n                                    BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)\\n        ctx.save_for_backward(x, weight, bias, mean, rstd)\\n        ctx.BLOCK_SIZE = BLOCK_SIZE\\n        ctx.num_warps = num_warps\\n        ctx.eps = eps\\n        return y\\n\\n    @staticmethod\\n    def backward(ctx, dy):\\n        x, w, b, m, v = ctx.saved_tensors\\n        # heuristics for amount of parallel reduction stream for DW/DB\\n        N = w.shape[0]\\n        GROUP_SIZE_M = 64\\n        if N <= 8192: GROUP_SIZE_M = 96\\n        if N <= 4096: GROUP_SIZE_M = 128\\n        if N <= 1024: GROUP_SIZE_M = 256\\n        # allocate output\\n        locks = torch.zeros(2 * GROUP_SIZE_M, dtype=torch.int32, device='cuda')\\n        _dw = torch.empty((GROUP_SIZE_M, w.shape[0]), dtype=x.dtype, device=w.device)\\n        _db = torch.empty((GROUP_SIZE_M, w.shape[0]), dtype=x.dtype, device=w.device)\\n        dw = torch.empty((w.shape[0],), dtype=w.dtype, device=w.device)\\n        db = torch.empty((w.shape[0],), dtype=w.dtype, device=w.device)\\n        dx = torch.empty_like(dy)\\n        # enqueue kernel using forward pass heuristics\\n        # also compute partial sums for DW and DB\\n        x_arg = x.reshape(-1, x.shape[-1])\\n        M, N = x_arg.shape\\n        _layer_norm_bwd_dx_fused[(M,)](dx, dy, _dw, _db, x, w, b, m, v, locks,\\n                                       x_arg.stride(0), N, ctx.eps,\\n                                       BLOCK_SIZE_N=ctx.BLOCK_SIZE,\\n                                       GROUP_SIZE_M=GROUP_SIZE_M,\\n                                       num_warps=ctx.num_warps)\\n        grid = lambda meta: [triton.cdiv(N, meta['BLOCK_SIZE_N'])]\\n        # accumulate partial sums in separate kernel\\n        _layer_norm_bwd_dwdb[grid](_dw, _db, dw, db, GROUP_SIZE_M, N,\\n                                   BLOCK_SIZE_M=32,\\n                                   BLOCK_SIZE_N=128)\\n        return dx, None, dw, db, None\\n\\n\\nlayer_norm = LayerNorm.apply\\n\\n\\ndef test_layer_norm(M, N, dtype, eps=1e-5, device='cuda'):\\n    # create data\\n    x_shape = (M, N)\\n    w_shape = (x_shape[-1], )\\n    weight = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\\n    bias = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\\n    x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\\n    dy = .1 * torch.randn_like(x)\\n    x.requires_grad_(True)\\n    # forward pass\\n    y_tri = layer_norm(x, w_shape, weight, bias, eps)\\n    y_ref = torch.nn.functional.layer_norm(x, w_shape, weight, bias, eps).to(dtype)\\n    # backward pass (triton)\\n    y_tri.backward(dy, retain_graph=True)\\n    dx_tri, dw_tri, db_tri = [_.grad.clone() for _ in [x, weight, bias]]\\n    x.grad, weight.grad, bias.grad = None, None, None\\n    # backward pass (torch)\\n    y_ref.backward(dy, retain_graph=True)\\n    dx_ref, dw_ref, db_ref = [_.grad.clone() for _ in [x, weight, bias]]\\n    # compare\\n    assert torch.allclose(y_tri, y_ref, atol=1e-2, rtol=0)\\n    assert torch.allclose(dx_tri, dx_ref, atol=1e-2, rtol=0)\\n    assert torch.allclose(db_tri, db_ref, atol=1e-2, rtol=0)\\n    assert torch.allclose(dw_tri, dw_ref, atol=1e-2, rtol=0)\\n\\n\\n@triton.testing.perf_report(\\n    triton.testing.Benchmark(\\n        x_names=['N'],\\n        x_vals=[512 * i for i in range(2, 32)],\\n        line_arg='provider',\\n        line_vals=['triton', 'torch'] + (['apex'] if HAS_APEX else []),\\n        line_names=['Triton', 'Torch'] + (['Apex'] if HAS_APEX else []),\\n        styles=[('blue', '-'), ('green', '-'), ('orange', '-')],\\n        ylabel='GB/s',\\n        plot_name='layer-norm-backward',\\n        args={'M': 4096, 'dtype': torch.float16, 'mode': 'backward'}\\n    )\\n)\\ndef bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='cuda'):\\n    # create data\\n    x_shape = (M, N)\\n    w_shape = (x_shape[-1], )\\n    weight = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\\n    bias = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\\n    x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\\n    dy = .1 * torch.randn_like(x)\\n    x.requires_grad_(True)\\n    quantiles = [0.5, 0.2, 0.8]\\n    # utility functions\\n    if provider == 'triton':\\n        y_fwd = lambda: layer_norm(x, w_shape, weight, bias, eps)\\n    if provider == 'torch':\\n        y_fwd = lambda: torch.nn.functional.layer_norm(x, w_shape, weight, bias, eps)\\n    if provider == 'apex':\\n        apex_layer_norm = apex.normalization.FusedLayerNorm(w_shape).to(x.device).to(x.dtype)\\n        y_fwd = lambda: apex_layer_norm(x)\\n    # forward pass\\n    if mode == 'forward':\\n        gbps = lambda ms: 2 * x.numel() * x.element_size() / ms * 1e-6\\n        ms, min_ms, max_ms = triton.testing.do_bench(y_fwd, quantiles=quantiles, rep=500)\\n    # backward pass\\n    if mode == 'backward':\\n        gbps = lambda ms: 3 * x.numel() * x.element_size() / ms * 1e-6\\n        y = y_fwd()\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: y.backward(dy, retain_graph=True),\\n                                                     quantiles=quantiles, grad_to_none=[x], rep=500)\\n    return gbps(ms), gbps(max_ms), gbps(min_ms)\\n\\n\\ntest_layer_norm(1151, 8192, torch.float16)\\nbench_layer_norm.run(save_path='.', print_data=True)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## References\\n\\n.. [BA2016] Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \\\"Layer Normalization\\\", Arxiv 2016\\n\\n\"\n-      ]\n-    }\n-  ],\n-  \"metadata\": {\n-    \"kernelspec\": {\n-      \"display_name\": \"Python 3\",\n-      \"language\": \"python\",\n-      \"name\": \"python3\"\n-    },\n-    \"language_info\": {\n-      \"codemirror_mode\": {\n-        \"name\": \"ipython\",\n-        \"version\": 3\n-      },\n-      \"file_extension\": \".py\",\n-      \"mimetype\": \"text/x-python\",\n-      \"name\": \"python\",\n-      \"nbconvert_exporter\": \"python\",\n-      \"pygments_lexer\": \"ipython3\",\n-      \"version\": \"3.8.10\"\n-    }\n-  },\n-  \"nbformat\": 4,\n-  \"nbformat_minor\": 0\n-}\n\\ No newline at end of file"}, {"filename": "main/_downloads/b51b68bc1c6b1a5e509f67800b6235af/03-matrix-multiplication.ipynb", "status": "removed", "additions": 0, "deletions": 129, "changes": 129, "file_content_changes": "@@ -1,129 +0,0 @@\n-{\n-  \"cells\": [\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"%matplotlib inline\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"\\n# Matrix Multiplication\\nIn this tutorial, you will write a very short high-performance FP16 matrix multiplication kernel that achieves\\nperformance on parallel with cuBLAS.\\n\\nYou will specifically learn about:\\n\\n* Block-level matrix multiplications.\\n\\n* Multi-dimensional pointer arithmetics.\\n\\n* Program re-ordering for improved L2 cache hit rate.\\n\\n* Automatic performance tuning.\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Motivations\\n\\nMatrix multiplications are a key building block of most modern high-performance computing systems.\\nThey are notoriously hard to optimize, hence their implementation is generally done by\\nhardware vendors themselves as part of so-called \\\"kernel libraries\\\" (e.g., cuBLAS).\\nUnfortunately, these libraries are often proprietary and cannot be easily customized\\nto accommodate the needs of modern deep learning workloads (e.g., fused activation functions).\\nIn this tutorial, you will learn how to implement efficient matrix multiplications by\\nyourself with Triton, in a way that is easy to customize and extend.\\n\\nRoughly speaking, the kernel that we will write will implement the following blocked\\nalgorithm to multiply a (M, K) by a (K, N) matrix:\\n\\n```python\\n# Do in parallel\\nfor m in range(0, M, BLOCK_SIZE_M):\\n  # Do in parallel\\n  for n in range(0, N, BLOCK_SIZE_N):\\n    acc = zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=float32)\\n    for k in range(0, K, BLOCK_SIZE_K):\\n      a = A[m : m+BLOCK_SIZE_M, k : k+BLOCK_SIZE_K]\\n      b = B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]\\n      acc += dot(a, b)\\n    C[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N] = acc\\n```\\nwhere each iteration of the doubly-nested for-loop is performed by a dedicated Triton program instance.\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Compute Kernel\\n\\nThe above algorithm is, actually, fairly straightforward to implement in Triton.\\nThe main difficulty comes from the computation of the memory locations at which blocks\\nof :code:`A` and :code:`B` must be read in the inner loop. For that, we need\\nmulti-dimensional pointer arithmetics.\\n\\n### Pointer Arithmetics\\n\\nFor a row-major 2D tensor :code:`X`, the memory location of :code:`X[i, j]` is given b\\ny :code:`&X[i, j] = X + i*stride_xi + j*stride_xj`.\\nTherefore, blocks of pointers for :code:`A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K]` and\\n:code:`B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]` can be defined in pseudo-code as:\\n\\n```python\\n&A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K] =  a_ptr + (m : m+BLOCK_SIZE_M)[:, None]*A.stride(0) + (k : k+BLOCK_SIZE_K)[None, :]*A.stride(1);\\n&B[k : k+BLOCK_SIZE_K, n:n+BLOCK_SIZE_N] =  b_ptr + (k : k+BLOCK_SIZE_K)[:, None]*B.stride(0) + (n : n+BLOCK_SIZE_N)[None, :]*B.stride(1);\\n```\\nWhich means that pointers for blocks of A and B can be initialized (i.e., :code:`k=0`) in Triton as the following\\ncode. Also note that we need an extra modulo to handle the case where :code:`M` is not a multiple of\\n:code:`BLOCK_SIZE_M` or :code:`N` is not a multiple of :code:`BLOCK_SIZE_N`, in which case we can pad the data with\\nsome useless values, which will not contribute to the results. For the :code:`K` dimension, we will handle that later\\nusing masking load semantics.\\n\\n```python\\noffs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\\noffs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\\noffs_k = tl.arange(0, BLOCK_SIZE_K)\\na_ptrs = a_ptr + (offs_am[:, None]*stride_am + offs_k [None, :]*stride_ak)\\nb_ptrs = b_ptr + (offs_k [:, None]*stride_bk + offs_bn[None, :]*stride_bn)\\n```\\nAnd then updated in the inner loop as follows:\\n\\n```python\\na_ptrs += BLOCK_SIZE_K * stride_ak;\\nb_ptrs += BLOCK_SIZE_K * stride_bk;\\n```\\n### L2 Cache Optimizations\\n\\nAs mentioned above, each program instance computes a :code:`[BLOCK_SIZE_M, BLOCK_SIZE_N]`\\nblock of :code:`C`.\\nIt is important to remember that the order in which these blocks are computed does\\nmatter, since it affects the L2 cache hit rate of our program. and unfortunately, a\\na simple row-major ordering\\n\\n```Python\\npid = triton.program_id(0);\\ngrid_m = (M + BLOCK_SIZE_M - 1) // BLOCK_SIZE_M;\\ngrid_n = (N + BLOCK_SIZE_N - 1) // BLOCK_SIZE_N;\\npid_m = pid / grid_n;\\npid_n = pid % grid_n;\\n```\\nis just not going to cut it.\\n\\nOne possible solution is to launch blocks in an order that promotes data reuse.\\nThis can be done by 'super-grouping' blocks in groups of :code:`GROUP_M` rows before\\nswitching to the next column:\\n\\n```python\\n# Program ID\\npid = tl.program_id(axis=0)\\n# Number of program ids along the M axis\\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\\n# Number of programs ids along the N axis\\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\\n# Number of programs in group\\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\\n# Id of the group this program is in\\ngroup_id = pid // num_pid_in_group\\n# Row-id of the first program in the group\\nfirst_pid_m = group_id * GROUP_SIZE_M\\n# If `num_pid_m` isn't divisible by `GROUP_SIZE_M`, the last group is smaller\\ngroup_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\\n# *Within groups*, programs are ordered in a column-major order\\n# Row-id of the program in the *launch grid*\\npid_m = first_pid_m + (pid % group_size_m)\\n# Col-id of the program in the *launch grid*\\npid_n = (pid % num_pid_in_group) // group_size_m\\n```\\nFor example, in the following matmul where each matrix is 9 blocks by 9 blocks,\\nwe can see that if we compute the output in row-major ordering, we need to load 90\\nblocks into SRAM to compute the first 9 output blocks, but if we do it in grouped\\nordering, we only need to load 54 blocks.\\n\\n  .. image:: grouped_vs_row_major_ordering.png\\n\\nIn practice, this can improve the performance of our matrix multiplication kernel by\\nmore than 10\\\\% on some hardware architecture (e.g., 220 to 245 TFLOPS on A100).\\n\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Final Result\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n# `triton.jit`'ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:\\n#   - A list of `triton.Config` objects that define different configurations of\\n#       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try\\n#   - An auto-tuning *key* whose change in values will trigger evaluation of all the\\n#       provided configs\\n@triton.autotune(\\n    configs=[\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\\n    ],\\n    key=['M', 'N', 'K'],\\n)\\n@triton.jit\\ndef matmul_kernel(\\n    # Pointers to matrices\\n    a_ptr, b_ptr, c_ptr,\\n    # Matrix dimensions\\n    M, N, K,\\n    # The stride variables represent how much to increase the ptr by when moving by 1\\n    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\\n    # by to get the element one row down (A has M rows).\\n    stride_am, stride_ak,\\n    stride_bk, stride_bn,\\n    stride_cm, stride_cn,\\n    # Meta-parameters\\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\\n    GROUP_SIZE_M: tl.constexpr,\\n    ACTIVATION: tl.constexpr,\\n):\\n    \\\"\\\"\\\"Kernel for computing the matmul C = A x B.\\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\\n    \\\"\\\"\\\"\\n    # -----------------------------------------------------------\\n    # Map program ids `pid` to the block of C it should compute.\\n    # This is done in a grouped ordering to promote L2 data reuse.\\n    # See above `L2 Cache Optimizations` section for details.\\n    pid = tl.program_id(axis=0)\\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\\n    group_id = pid // num_pid_in_group\\n    first_pid_m = group_id * GROUP_SIZE_M\\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\\n    pid_m = first_pid_m + (pid % group_size_m)\\n    pid_n = (pid % num_pid_in_group) // group_size_m\\n\\n    # ----------------------------------------------------------\\n    # Create pointers for the first blocks of A and B.\\n    # We will advance this pointer as we move in the K direction\\n    # and accumulate\\n    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\\n    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\\n    # See above `Pointer Arithmetics` section for details\\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\\n\\n    # -----------------------------------------------------------\\n    # Iterate to compute a block of the C matrix.\\n    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\\n    # of fp32 values for higher accuracy.\\n    # `accumulator` will be converted back to fp16 after the loop.\\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\\n        # Load the next block of A and B, generate a mask by checking the K dimension.\\n        # If it is out of bounds, set it to 0.\\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\\n        # We accumulate along the K dimension.\\n        accumulator += tl.dot(a, b)\\n        # Advance the ptrs to the next K block.\\n        a_ptrs += BLOCK_SIZE_K * stride_ak\\n        b_ptrs += BLOCK_SIZE_K * stride_bk\\n    # You can fuse arbitrary activation functions here\\n    # while the accumulator is still in FP32!\\n    if ACTIVATION == \\\"leaky_relu\\\":\\n        accumulator = leaky_relu(accumulator)\\n    c = accumulator.to(tl.float16)\\n\\n    # -----------------------------------------------------------\\n    # Write back the block of the output matrix C with masks.\\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\\n    tl.store(c_ptrs, c, mask=c_mask)\\n\\n\\n# We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.\\n@triton.jit\\ndef leaky_relu(x):\\n    x = x + 1\\n    return tl.where(x >= 0, x, 0.01 * x)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"We can now create a convenience wrapper function that only takes two input tensors,\\nand (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"def matmul(a, b, activation=\\\"\\\"):\\n    # Check constraints.\\n    assert a.shape[1] == b.shape[0], \\\"Incompatible dimensions\\\"\\n    assert a.is_contiguous(), \\\"Matrix A must be contiguous\\\"\\n    assert b.is_contiguous(), \\\"Matrix B must be contiguous\\\"\\n    M, K = a.shape\\n    K, N = b.shape\\n    # Allocates output.\\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\\n    # 1D launch kernel where each block gets its own program.\\n    grid = lambda META: (\\n        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\\n    )\\n    matmul_kernel[grid](\\n        a, b, c,\\n        M, N, K,\\n        a.stride(0), a.stride(1),\\n        b.stride(0), b.stride(1),\\n        c.stride(0), c.stride(1),\\n        ACTIVATION=activation\\n    )\\n    return c\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Unit Test\\n\\nWe can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS).\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"torch.manual_seed(0)\\na = torch.randn((512, 512), device='cuda', dtype=torch.float16)\\nb = torch.randn((512, 512), device='cuda', dtype=torch.float16)\\ntriton_output = matmul(a, b)\\ntorch_output = torch.matmul(a, b)\\nprint(f\\\"triton_output={triton_output}\\\")\\nprint(f\\\"torch_output={torch_output}\\\")\\nif torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\\n    print(\\\"\\u2705 Triton and Torch match\\\")\\nelse:\\n    print(\\\"\\u274c Triton and Torch differ\\\")\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Benchmark\\n\\n### Square Matrix Performance\\n\\nWe can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices,\\nbut feel free to arrange this script as you wish to benchmark any other matrix shape.\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"@triton.testing.perf_report(\\n    triton.testing.Benchmark(\\n        x_names=['M', 'N', 'K'],  # Argument names to use as an x-axis for the plot\\n        x_vals=[\\n            128 * i for i in range(2, 33)\\n        ],  # Different possible values for `x_name`\\n        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot\\n        # Possible values for `line_arg`\\n        line_vals=['cublas', 'triton'],\\n        # Label name for the lines\\n        line_names=[\\\"cuBLAS\\\", \\\"Triton\\\"],\\n        # Line styles\\n        styles=[('green', '-'), ('blue', '-')],\\n        ylabel=\\\"TFLOPS\\\",  # Label name for the y-axis\\n        plot_name=\\\"matmul-performance\\\",  # Name for the plot, used also as a file name for saving the plot.\\n        args={},\\n    )\\n)\\ndef benchmark(M, N, K, provider):\\n    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\\n    b = torch.randn((K, N), device='cuda', dtype=torch.float16)\\n    quantiles = [0.5, 0.2, 0.8]\\n    if provider == 'cublas':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\\n    if provider == 'triton':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\\n    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\\n    return perf(ms), perf(max_ms), perf(min_ms)\\n\\n\\nbenchmark.run(show_plots=True, print_data=True)\"\n-      ]\n-    }\n-  ],\n-  \"metadata\": {\n-    \"kernelspec\": {\n-      \"display_name\": \"Python 3\",\n-      \"language\": \"python\",\n-      \"name\": \"python3\"\n-    },\n-    \"language_info\": {\n-      \"codemirror_mode\": {\n-        \"name\": \"ipython\",\n-        \"version\": 3\n-      },\n-      \"file_extension\": \".py\",\n-      \"mimetype\": \"text/x-python\",\n-      \"name\": \"python\",\n-      \"nbconvert_exporter\": \"python\",\n-      \"pygments_lexer\": \"ipython3\",\n-      \"version\": \"3.8.10\"\n-    }\n-  },\n-  \"nbformat\": 4,\n-  \"nbformat_minor\": 0\n-}\n\\ No newline at end of file"}, {"filename": "main/_downloads/bc847dec325798bdc436c4ef5ac8b78a/04-low-memory-dropout.ipynb", "status": "removed", "additions": 0, "deletions": 100, "changes": 100, "file_content_changes": "@@ -1,100 +0,0 @@\n-{\n-  \"cells\": [\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"%matplotlib inline\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"\\n# Low-Memory Dropout\\n\\nIn this tutorial, you will write a memory-efficient implementation of dropout whose state\\nwill be composed of a single int32 seed. This differs from more traditional implementations of dropout,\\nwhose state is generally composed of a bit mask tensor of the same shape as the input.\\n\\nIn doing so, you will learn about:\\n\\n* The limitations of naive implementations of Dropout with PyTorch.\\n\\n* Parallel pseudo-random number generation in Triton.\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Baseline\\n\\nThe *dropout* operator was first introduced in [SRIVASTAVA2014]_ as a way to improve the performance\\nof deep neural networks in low-data regime (i.e. regularization).\\n\\nIt takes a vector as input and produces a vector of the same shape as output. Each scalar in the\\noutput has a probability $p$ of being changed to zero and otherwise it is copied from the input.\\nThis forces the network to perform well even when only $1 - p$ scalars from the input are available.\\n\\nAt evaluation time we want to use the full power of the network so we set $p=0$. Naively this would\\nincrease the norm of the output (which can be a bad thing, e.g. it can lead to artificial decrease\\nin the output softmax temperature). To prevent this we multiply the output by $\\\\frac{1}{1 - p}$, which\\nkeeps the norm consistent regardless of the dropout probability.\\n\\nLet's first take a look at the baseline implementation.\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"import tabulate\\nimport torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.jit\\ndef _dropout(\\n    x_ptr,  # pointer to the input\\n    x_keep_ptr,  # pointer to a mask of 0s and 1s\\n    output_ptr,  # pointer to the output\\n    n_elements,  # number of elements in the `x` tensor\\n    p,  # probability that an element of `x` is changed to zero\\n    BLOCK_SIZE: tl.constexpr,\\n):\\n    pid = tl.program_id(axis=0)\\n    block_start = pid * BLOCK_SIZE\\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\\n    mask = offsets < n_elements\\n    # Load data\\n    x = tl.load(x_ptr + offsets, mask=mask)\\n    x_keep = tl.load(x_keep_ptr + offsets, mask=mask)\\n    # The line below is the crucial part, described in the paragraph above!\\n    output = tl.where(x_keep, x / (1 - p), 0.0)\\n    # Write-back output\\n    tl.store(output_ptr + offsets, output, mask=mask)\\n\\n\\ndef dropout(x, x_keep, p):\\n    output = torch.empty_like(x)\\n    assert x.is_contiguous()\\n    n_elements = x.numel()\\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\\n    _dropout[grid](x, x_keep, output, n_elements, p, BLOCK_SIZE=1024)\\n    return output\\n\\n\\n# Input tensor\\nx = torch.randn(size=(10,)).cuda()\\n# Dropout mask\\np = 0.5\\nx_keep = (torch.rand(size=(10,)) > p).to(torch.int32).cuda()\\n#\\noutput = dropout(x, x_keep=x_keep, p=p)\\nprint(tabulate.tabulate([\\n    [\\\"input\\\"] + x.tolist(),\\n    [\\\"keep mask\\\"] + x_keep.tolist(),\\n    [\\\"output\\\"] + output.tolist()\\n]))\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Seeded dropout\\n\\nThe above implementation of dropout works fine, but it can be a bit awkward to deal with. Firstly\\nwe need to store the dropout mask for backpropagation. Secondly, dropout state management can get\\nvery tricky when using recompute/checkpointing (e.g. see all the notes about `preserve_rng_state` in\\nhttps://pytorch.org/docs/1.9.0/checkpoint.html). In this tutorial we'll describe an alternative implementation\\nthat (1) has a smaller memory footprint; (2) requires less data movement; and (3) simplifies the management\\nof persisting randomness across multiple invocations of the kernel.\\n\\nPseudo-random number generation in Triton is simple! In this tutorial we will use the\\n:code:`triton.language.rand` function which generates a block of uniformly distributed :code:`float32`\\nvalues in [0, 1), given a seed and a block of :code:`int32` offsets. But if you need it, Triton also provides\\nother `random number generation strategies <Random Number Generation>`.\\n\\n<div class=\\\"alert alert-info\\\"><h4>Note</h4><p>Triton's implementation of PRNG is based on the Philox algorithm (described on [SALMON2011]_).</p></div>\\n\\nLet's put it all together.\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"@triton.jit\\ndef _seeded_dropout(\\n    x_ptr,\\n    output_ptr,\\n    n_elements,\\n    p,\\n    seed,\\n    BLOCK_SIZE: tl.constexpr,\\n):\\n    # compute memory offsets of elements handled by this instance\\n    pid = tl.program_id(axis=0)\\n    block_start = pid * BLOCK_SIZE\\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\\n    # load data from x\\n    mask = offsets < n_elements\\n    x = tl.load(x_ptr + offsets, mask=mask)\\n    # randomly prune it\\n    random = tl.rand(seed, offsets)\\n    x_keep = random > p\\n    # write-back\\n    output = tl.where(x_keep, x / (1 - p), 0.0)\\n    tl.store(output_ptr + offsets, output, mask=mask)\\n\\n\\ndef seeded_dropout(x, p, seed):\\n    output = torch.empty_like(x)\\n    assert x.is_contiguous()\\n    n_elements = x.numel()\\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\\n    _seeded_dropout[grid](x, output, n_elements, p, seed, BLOCK_SIZE=1024)\\n    return output\\n\\n\\nx = torch.randn(size=(10,)).cuda()\\n# Compare this to the baseline - dropout mask is never instantiated!\\noutput = seeded_dropout(x, p=0.5, seed=123)\\noutput2 = seeded_dropout(x, p=0.5, seed=123)\\noutput3 = seeded_dropout(x, p=0.5, seed=512)\\n\\nprint(tabulate.tabulate([\\n    [\\\"input\\\"] + x.tolist(),\\n    [\\\"output (seed = 123)\\\"] + output.tolist(),\\n    [\\\"output (seed = 123)\\\"] + output2.tolist(),\\n    [\\\"output (seed = 512)\\\"] + output3.tolist()\\n]))\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"Et Voil\\u00e0! We have a triton kernel that applies the same dropout mask provided the seed is the same!\\nIf you'd like explore further applications of pseudorandomness in GPU programming, we encourage you\\nto explore the `triton/language/random` folder!\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Exercises\\n\\n1. Extend the kernel to operate over a matrix and use a vector of seeds - one per row.\\n2. Add support for striding.\\n3. (challenge) Implement a kernel for sparse Johnson-Lindenstrauss transform which generates the projection matrix one the fly each time using a seed.\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## References\\n\\n.. [SALMON2011] John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw, \\\"Parallel Random Numbers: As Easy as 1, 2, 3\\\", 2011\\n.. [SRIVASTAVA2014] Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \\\"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\\\", JMLR 2014\\n\\n\"\n-      ]\n-    }\n-  ],\n-  \"metadata\": {\n-    \"kernelspec\": {\n-      \"display_name\": \"Python 3\",\n-      \"language\": \"python\",\n-      \"name\": \"python3\"\n-    },\n-    \"language_info\": {\n-      \"codemirror_mode\": {\n-        \"name\": \"ipython\",\n-        \"version\": 3\n-      },\n-      \"file_extension\": \".py\",\n-      \"mimetype\": \"text/x-python\",\n-      \"name\": \"python\",\n-      \"nbconvert_exporter\": \"python\",\n-      \"pygments_lexer\": \"ipython3\",\n-      \"version\": \"3.8.10\"\n-    }\n-  },\n-  \"nbformat\": 4,\n-  \"nbformat_minor\": 0\n-}\n\\ No newline at end of file"}, {"filename": "main/_downloads/c9aed78977a4c05741d675a38dde3d7d/04-low-memory-dropout.py", "status": "removed", "additions": 0, "deletions": 173, "changes": 173, "file_content_changes": "@@ -1,173 +0,0 @@\n-\"\"\"\n-Low-Memory Dropout\n-==================\n-\n-In this tutorial, you will write a memory-efficient implementation of dropout whose state\n-will be composed of a single int32 seed. This differs from more traditional implementations of dropout,\n-whose state is generally composed of a bit mask tensor of the same shape as the input.\n-\n-In doing so, you will learn about:\n-\n-* The limitations of naive implementations of Dropout with PyTorch.\n-\n-* Parallel pseudo-random number generation in Triton.\n-\n-\"\"\"\n-\n-# %%\n-# Baseline\n-# --------\n-#\n-# The *dropout* operator was first introduced in [SRIVASTAVA2014]_ as a way to improve the performance\n-# of deep neural networks in low-data regime (i.e. regularization).\n-#\n-# It takes a vector as input and produces a vector of the same shape as output. Each scalar in the\n-# output has a probability :math:`p` of being changed to zero and otherwise it is copied from the input.\n-# This forces the network to perform well even when only :math:`1 - p` scalars from the input are available.\n-#\n-# At evaluation time we want to use the full power of the network so we set :math:`p=0`. Naively this would\n-# increase the norm of the output (which can be a bad thing, e.g. it can lead to artificial decrease\n-# in the output softmax temperature). To prevent this we multiply the output by :math:`\\frac{1}{1 - p}`, which\n-# keeps the norm consistent regardless of the dropout probability.\n-#\n-# Let's first take a look at the baseline implementation.\n-\n-\n-import tabulate\n-import torch\n-\n-import triton\n-import triton.language as tl\n-\n-\n-@triton.jit\n-def _dropout(\n-    x_ptr,  # pointer to the input\n-    x_keep_ptr,  # pointer to a mask of 0s and 1s\n-    output_ptr,  # pointer to the output\n-    n_elements,  # number of elements in the `x` tensor\n-    p,  # probability that an element of `x` is changed to zero\n-    BLOCK_SIZE: tl.constexpr,\n-):\n-    pid = tl.program_id(axis=0)\n-    block_start = pid * BLOCK_SIZE\n-    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n-    mask = offsets < n_elements\n-    # Load data\n-    x = tl.load(x_ptr + offsets, mask=mask)\n-    x_keep = tl.load(x_keep_ptr + offsets, mask=mask)\n-    # The line below is the crucial part, described in the paragraph above!\n-    output = tl.where(x_keep, x / (1 - p), 0.0)\n-    # Write-back output\n-    tl.store(output_ptr + offsets, output, mask=mask)\n-\n-\n-def dropout(x, x_keep, p):\n-    output = torch.empty_like(x)\n-    assert x.is_contiguous()\n-    n_elements = x.numel()\n-    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n-    _dropout[grid](x, x_keep, output, n_elements, p, BLOCK_SIZE=1024)\n-    return output\n-\n-\n-# Input tensor\n-x = torch.randn(size=(10,)).cuda()\n-# Dropout mask\n-p = 0.5\n-x_keep = (torch.rand(size=(10,)) > p).to(torch.int32).cuda()\n-#\n-output = dropout(x, x_keep=x_keep, p=p)\n-print(tabulate.tabulate([\n-    [\"input\"] + x.tolist(),\n-    [\"keep mask\"] + x_keep.tolist(),\n-    [\"output\"] + output.tolist()\n-]))\n-\n-# %%\n-# Seeded dropout\n-# --------------\n-#\n-# The above implementation of dropout works fine, but it can be a bit awkward to deal with. Firstly\n-# we need to store the dropout mask for backpropagation. Secondly, dropout state management can get\n-# very tricky when using recompute/checkpointing (e.g. see all the notes about `preserve_rng_state` in\n-# https://pytorch.org/docs/1.9.0/checkpoint.html). In this tutorial we'll describe an alternative implementation\n-# that (1) has a smaller memory footprint; (2) requires less data movement; and (3) simplifies the management\n-# of persisting randomness across multiple invocations of the kernel.\n-#\n-# Pseudo-random number generation in Triton is simple! In this tutorial we will use the\n-# :code:`triton.language.rand` function which generates a block of uniformly distributed :code:`float32`\n-# values in [0, 1), given a seed and a block of :code:`int32` offsets. But if you need it, Triton also provides\n-# other :ref:`random number generation strategies <Random Number Generation>`.\n-#\n-# .. note::\n-#    Triton's implementation of PRNG is based on the Philox algorithm (described on [SALMON2011]_).\n-#\n-# Let's put it all together.\n-\n-\n-@triton.jit\n-def _seeded_dropout(\n-    x_ptr,\n-    output_ptr,\n-    n_elements,\n-    p,\n-    seed,\n-    BLOCK_SIZE: tl.constexpr,\n-):\n-    # compute memory offsets of elements handled by this instance\n-    pid = tl.program_id(axis=0)\n-    block_start = pid * BLOCK_SIZE\n-    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n-    # load data from x\n-    mask = offsets < n_elements\n-    x = tl.load(x_ptr + offsets, mask=mask)\n-    # randomly prune it\n-    random = tl.rand(seed, offsets)\n-    x_keep = random > p\n-    # write-back\n-    output = tl.where(x_keep, x / (1 - p), 0.0)\n-    tl.store(output_ptr + offsets, output, mask=mask)\n-\n-\n-def seeded_dropout(x, p, seed):\n-    output = torch.empty_like(x)\n-    assert x.is_contiguous()\n-    n_elements = x.numel()\n-    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n-    _seeded_dropout[grid](x, output, n_elements, p, seed, BLOCK_SIZE=1024)\n-    return output\n-\n-\n-x = torch.randn(size=(10,)).cuda()\n-# Compare this to the baseline - dropout mask is never instantiated!\n-output = seeded_dropout(x, p=0.5, seed=123)\n-output2 = seeded_dropout(x, p=0.5, seed=123)\n-output3 = seeded_dropout(x, p=0.5, seed=512)\n-\n-print(tabulate.tabulate([\n-    [\"input\"] + x.tolist(),\n-    [\"output (seed = 123)\"] + output.tolist(),\n-    [\"output (seed = 123)\"] + output2.tolist(),\n-    [\"output (seed = 512)\"] + output3.tolist()\n-]))\n-\n-# %%\n-# Et Voil\u00e0! We have a triton kernel that applies the same dropout mask provided the seed is the same!\n-# If you'd like explore further applications of pseudorandomness in GPU programming, we encourage you\n-# to explore the `triton/language/random` folder!\n-\n-# %%\n-# Exercises\n-# ---------\n-#\n-# 1. Extend the kernel to operate over a matrix and use a vector of seeds - one per row.\n-# 2. Add support for striding.\n-# 3. (challenge) Implement a kernel for sparse Johnson-Lindenstrauss transform which generates the projection matrix one the fly each time using a seed.\n-\n-# %%\n-# References\n-# ----------\n-#\n-# .. [SALMON2011] John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw, \"Parallel Random Numbers: As Easy as 1, 2, 3\", 2011\n-# .. [SRIVASTAVA2014] Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\", JMLR 2014"}, {"filename": "main/_downloads/d5fee5b55a64e47f1b5724ec39adf171/03-matrix-multiplication.py", "status": "removed", "additions": 0, "deletions": 350, "changes": 350, "file_content_changes": "@@ -1,350 +0,0 @@\n-\"\"\"\n-Matrix Multiplication\n-=====================\n-In this tutorial, you will write a very short high-performance FP16 matrix multiplication kernel that achieves\n-performance on parallel with cuBLAS.\n-\n-You will specifically learn about:\n-\n-* Block-level matrix multiplications.\n-\n-* Multi-dimensional pointer arithmetics.\n-\n-* Program re-ordering for improved L2 cache hit rate.\n-\n-* Automatic performance tuning.\n-\n-\"\"\"\n-\n-# %%\n-# Motivations\n-# -----------\n-#\n-# Matrix multiplications are a key building block of most modern high-performance computing systems.\n-# They are notoriously hard to optimize, hence their implementation is generally done by\n-# hardware vendors themselves as part of so-called \"kernel libraries\" (e.g., cuBLAS).\n-# Unfortunately, these libraries are often proprietary and cannot be easily customized\n-# to accommodate the needs of modern deep learning workloads (e.g., fused activation functions).\n-# In this tutorial, you will learn how to implement efficient matrix multiplications by\n-# yourself with Triton, in a way that is easy to customize and extend.\n-#\n-# Roughly speaking, the kernel that we will write will implement the following blocked\n-# algorithm to multiply a (M, K) by a (K, N) matrix:\n-#\n-#  .. code-block:: python\n-#\n-#    # Do in parallel\n-#    for m in range(0, M, BLOCK_SIZE_M):\n-#      # Do in parallel\n-#      for n in range(0, N, BLOCK_SIZE_N):\n-#        acc = zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=float32)\n-#        for k in range(0, K, BLOCK_SIZE_K):\n-#          a = A[m : m+BLOCK_SIZE_M, k : k+BLOCK_SIZE_K]\n-#          b = B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]\n-#          acc += dot(a, b)\n-#        C[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N] = acc\n-#\n-# where each iteration of the doubly-nested for-loop is performed by a dedicated Triton program instance.\n-\n-# %%\n-# Compute Kernel\n-# --------------\n-#\n-# The above algorithm is, actually, fairly straightforward to implement in Triton.\n-# The main difficulty comes from the computation of the memory locations at which blocks\n-# of :code:`A` and :code:`B` must be read in the inner loop. For that, we need\n-# multi-dimensional pointer arithmetics.\n-#\n-# Pointer Arithmetics\n-# ~~~~~~~~~~~~~~~~~~~\n-#\n-# For a row-major 2D tensor :code:`X`, the memory location of :code:`X[i, j]` is given b\n-# y :code:`&X[i, j] = X + i*stride_xi + j*stride_xj`.\n-# Therefore, blocks of pointers for :code:`A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K]` and\n-# :code:`B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]` can be defined in pseudo-code as:\n-#\n-#  .. code-block:: python\n-#\n-#    &A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K] =  a_ptr + (m : m+BLOCK_SIZE_M)[:, None]*A.stride(0) + (k : k+BLOCK_SIZE_K)[None, :]*A.stride(1);\n-#    &B[k : k+BLOCK_SIZE_K, n:n+BLOCK_SIZE_N] =  b_ptr + (k : k+BLOCK_SIZE_K)[:, None]*B.stride(0) + (n : n+BLOCK_SIZE_N)[None, :]*B.stride(1);\n-#\n-# Which means that pointers for blocks of A and B can be initialized (i.e., :code:`k=0`) in Triton as the following\n-# code. Also note that we need an extra modulo to handle the case where :code:`M` is not a multiple of\n-# :code:`BLOCK_SIZE_M` or :code:`N` is not a multiple of :code:`BLOCK_SIZE_N`, in which case we can pad the data with\n-# some useless values, which will not contribute to the results. For the :code:`K` dimension, we will handle that later\n-# using masking load semantics.\n-#\n-#  .. code-block:: python\n-#\n-#    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n-#    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n-#    offs_k = tl.arange(0, BLOCK_SIZE_K)\n-#    a_ptrs = a_ptr + (offs_am[:, None]*stride_am + offs_k [None, :]*stride_ak)\n-#    b_ptrs = b_ptr + (offs_k [:, None]*stride_bk + offs_bn[None, :]*stride_bn)\n-#\n-# And then updated in the inner loop as follows:\n-#\n-#  .. code-block:: python\n-#\n-#    a_ptrs += BLOCK_SIZE_K * stride_ak;\n-#    b_ptrs += BLOCK_SIZE_K * stride_bk;\n-#\n-#\n-# L2 Cache Optimizations\n-# ~~~~~~~~~~~~~~~~~~~~~~\n-#\n-# As mentioned above, each program instance computes a :code:`[BLOCK_SIZE_M, BLOCK_SIZE_N]`\n-# block of :code:`C`.\n-# It is important to remember that the order in which these blocks are computed does\n-# matter, since it affects the L2 cache hit rate of our program. and unfortunately, a\n-# a simple row-major ordering\n-#\n-#  .. code-block:: Python\n-#\n-#    pid = triton.program_id(0);\n-#    grid_m = (M + BLOCK_SIZE_M - 1) // BLOCK_SIZE_M;\n-#    grid_n = (N + BLOCK_SIZE_N - 1) // BLOCK_SIZE_N;\n-#    pid_m = pid / grid_n;\n-#    pid_n = pid % grid_n;\n-#\n-# is just not going to cut it.\n-#\n-# One possible solution is to launch blocks in an order that promotes data reuse.\n-# This can be done by 'super-grouping' blocks in groups of :code:`GROUP_M` rows before\n-# switching to the next column:\n-#\n-#  .. code-block:: python\n-#\n-#    # Program ID\n-#    pid = tl.program_id(axis=0)\n-#    # Number of program ids along the M axis\n-#    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n-#    # Number of programs ids along the N axis\n-#    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n-#    # Number of programs in group\n-#    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n-#    # Id of the group this program is in\n-#    group_id = pid // num_pid_in_group\n-#    # Row-id of the first program in the group\n-#    first_pid_m = group_id * GROUP_SIZE_M\n-#    # If `num_pid_m` isn't divisible by `GROUP_SIZE_M`, the last group is smaller\n-#    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n-#    # *Within groups*, programs are ordered in a column-major order\n-#    # Row-id of the program in the *launch grid*\n-#    pid_m = first_pid_m + (pid % group_size_m)\n-#    # Col-id of the program in the *launch grid*\n-#    pid_n = (pid % num_pid_in_group) // group_size_m\n-#\n-# For example, in the following matmul where each matrix is 9 blocks by 9 blocks,\n-# we can see that if we compute the output in row-major ordering, we need to load 90\n-# blocks into SRAM to compute the first 9 output blocks, but if we do it in grouped\n-# ordering, we only need to load 54 blocks.\n-#\n-#   .. image:: grouped_vs_row_major_ordering.png\n-#\n-# In practice, this can improve the performance of our matrix multiplication kernel by\n-# more than 10\\% on some hardware architecture (e.g., 220 to 245 TFLOPS on A100).\n-#\n-\n-# %%\n-# Final Result\n-# ------------\n-\n-import torch\n-\n-import triton\n-import triton.language as tl\n-\n-\n-# `triton.jit`'ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:\n-#   - A list of `triton.Config` objects that define different configurations of\n-#       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try\n-#   - An auto-tuning *key* whose change in values will trigger evaluation of all the\n-#       provided configs\n-@triton.autotune(\n-    configs=[\n-        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n-        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n-        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n-    ],\n-    key=['M', 'N', 'K'],\n-)\n-@triton.jit\n-def matmul_kernel(\n-    # Pointers to matrices\n-    a_ptr, b_ptr, c_ptr,\n-    # Matrix dimensions\n-    M, N, K,\n-    # The stride variables represent how much to increase the ptr by when moving by 1\n-    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n-    # by to get the element one row down (A has M rows).\n-    stride_am, stride_ak,\n-    stride_bk, stride_bn,\n-    stride_cm, stride_cn,\n-    # Meta-parameters\n-    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n-    GROUP_SIZE_M: tl.constexpr,\n-    ACTIVATION: tl.constexpr,\n-):\n-    \"\"\"Kernel for computing the matmul C = A x B.\n-    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n-    \"\"\"\n-    # -----------------------------------------------------------\n-    # Map program ids `pid` to the block of C it should compute.\n-    # This is done in a grouped ordering to promote L2 data reuse.\n-    # See above `L2 Cache Optimizations` section for details.\n-    pid = tl.program_id(axis=0)\n-    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n-    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n-    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n-    group_id = pid // num_pid_in_group\n-    first_pid_m = group_id * GROUP_SIZE_M\n-    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n-    pid_m = first_pid_m + (pid % group_size_m)\n-    pid_n = (pid % num_pid_in_group) // group_size_m\n-\n-    # ----------------------------------------------------------\n-    # Create pointers for the first blocks of A and B.\n-    # We will advance this pointer as we move in the K direction\n-    # and accumulate\n-    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n-    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n-    # See above `Pointer Arithmetics` section for details\n-    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n-    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n-    offs_k = tl.arange(0, BLOCK_SIZE_K)\n-    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n-    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n-\n-    # -----------------------------------------------------------\n-    # Iterate to compute a block of the C matrix.\n-    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n-    # of fp32 values for higher accuracy.\n-    # `accumulator` will be converted back to fp16 after the loop.\n-    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n-        # Load the next block of A and B, generate a mask by checking the K dimension.\n-        # If it is out of bounds, set it to 0.\n-        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n-        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n-        # We accumulate along the K dimension.\n-        accumulator += tl.dot(a, b)\n-        # Advance the ptrs to the next K block.\n-        a_ptrs += BLOCK_SIZE_K * stride_ak\n-        b_ptrs += BLOCK_SIZE_K * stride_bk\n-    # You can fuse arbitrary activation functions here\n-    # while the accumulator is still in FP32!\n-    if ACTIVATION == \"leaky_relu\":\n-        accumulator = leaky_relu(accumulator)\n-    c = accumulator.to(tl.float16)\n-\n-    # -----------------------------------------------------------\n-    # Write back the block of the output matrix C with masks.\n-    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n-    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n-    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n-    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n-    tl.store(c_ptrs, c, mask=c_mask)\n-\n-\n-# We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.\n-@triton.jit\n-def leaky_relu(x):\n-    x = x + 1\n-    return tl.where(x >= 0, x, 0.01 * x)\n-\n-\n-# %%\n-# We can now create a convenience wrapper function that only takes two input tensors,\n-# and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\n-\n-\n-def matmul(a, b, activation=\"\"):\n-    # Check constraints.\n-    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n-    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n-    assert b.is_contiguous(), \"Matrix B must be contiguous\"\n-    M, K = a.shape\n-    K, N = b.shape\n-    # Allocates output.\n-    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n-    # 1D launch kernel where each block gets its own program.\n-    grid = lambda META: (\n-        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n-    )\n-    matmul_kernel[grid](\n-        a, b, c,\n-        M, N, K,\n-        a.stride(0), a.stride(1),\n-        b.stride(0), b.stride(1),\n-        c.stride(0), c.stride(1),\n-        ACTIVATION=activation\n-    )\n-    return c\n-\n-\n-# %%\n-# Unit Test\n-# ---------\n-#\n-# We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS).\n-\n-torch.manual_seed(0)\n-a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n-b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n-triton_output = matmul(a, b)\n-torch_output = torch.matmul(a, b)\n-print(f\"triton_output={triton_output}\")\n-print(f\"torch_output={torch_output}\")\n-if torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\n-    print(\"\u2705 Triton and Torch match\")\n-else:\n-    print(\"\u274c Triton and Torch differ\")\n-\n-# %%\n-# Benchmark\n-# ---------\n-#\n-# Square Matrix Performance\n-# ~~~~~~~~~~~~~~~~~~~~~~~~~~\n-#\n-# We can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices,\n-# but feel free to arrange this script as you wish to benchmark any other matrix shape.\n-\n-\n-@triton.testing.perf_report(\n-    triton.testing.Benchmark(\n-        x_names=['M', 'N', 'K'],  # Argument names to use as an x-axis for the plot\n-        x_vals=[\n-            128 * i for i in range(2, 33)\n-        ],  # Different possible values for `x_name`\n-        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot\n-        # Possible values for `line_arg`\n-        line_vals=['cublas', 'triton'],\n-        # Label name for the lines\n-        line_names=[\"cuBLAS\", \"Triton\"],\n-        # Line styles\n-        styles=[('green', '-'), ('blue', '-')],\n-        ylabel=\"TFLOPS\",  # Label name for the y-axis\n-        plot_name=\"matmul-performance\",  # Name for the plot, used also as a file name for saving the plot.\n-        args={},\n-    )\n-)\n-def benchmark(M, N, K, provider):\n-    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n-    b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n-    quantiles = [0.5, 0.2, 0.8]\n-    if provider == 'cublas':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\n-    if provider == 'triton':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\n-    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n-    return perf(ms), perf(max_ms), perf(min_ms)\n-\n-\n-benchmark.run(show_plots=True, print_data=True)"}, {"filename": "main/_downloads/d91442ac2982c4e0cc3ab0f43534afbc/02-fused-softmax.py", "status": "removed", "additions": 0, "deletions": 200, "changes": 200, "file_content_changes": "@@ -1,200 +0,0 @@\n-\"\"\"\n-Fused Softmax\n-=============\n-\n-In this tutorial, you will write a fused softmax operation that is significantly faster\n-than PyTorch's native op for a particular class of matrices: those whose rows can fit in\n-the GPU's SRAM.\n-\n-In doing so, you will learn about:\n-\n-* The benefits of kernel fusion for bandwidth-bound operations.\n-\n-* Reduction operators in Triton.\n-\n-\"\"\"\n-\n-# %%\n-# Motivations\n-# -----------\n-#\n-# Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.\n-# Let us consider instead the case of a simple (numerically stabilized) softmax operation:\n-\n-import torch\n-\n-import triton\n-import triton.language as tl\n-\n-\n-@torch.jit.script\n-def naive_softmax(x):\n-    \"\"\"Compute row-wise softmax of X using native pytorch\n-\n-    We subtract the maximum element in order to avoid overflows. Softmax is invariant to\n-    this shift.\n-    \"\"\"\n-    # read  MN elements ; write M  elements\n-    x_max = x.max(dim=1)[0]\n-    # read MN + M elements ; write MN elements\n-    z = x - x_max[:, None]\n-    # read  MN elements ; write MN elements\n-    numerator = torch.exp(z)\n-    # read  MN elements ; write M  elements\n-    denominator = numerator.sum(dim=1)\n-    # read MN + M elements ; write MN elements\n-    ret = numerator / denominator[:, None]\n-    # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements\n-    return ret\n-\n-\n-# %%\n-# When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \\in R^{M \\times N}`\n-# requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.\n-# This is obviously wasteful; we'd prefer to have a custom \"fused\" kernel that only reads\n-# X once and does all the necessary computations on-chip.\n-# Doing so would require reading and writing back only :math:`MN` bytes, so we could\n-# expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).\n-# The `torch.jit.script` flags aims to perform this kind of \"kernel fusion\" automatically\n-# but, as we will see later, it is still far from ideal.\n-\n-# %%\n-# Compute Kernel\n-# --------------\n-#\n-# Our softmax kernel works as follows: each program loads a row of the input matrix X,\n-# normalizes it and writes back the result to the output Y.\n-#\n-# Note that one important limitation of Triton is that each block must have a\n-# power-of-two number of elements, so we need to internally \"pad\" each row and guard the\n-# memory operations properly if we want to handle any possible input shapes:\n-\n-\n-@triton.jit\n-def softmax_kernel(\n-    output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols,\n-    BLOCK_SIZE: tl.constexpr\n-):\n-    # The rows of the softmax are independent, so we parallelize across those\n-    row_idx = tl.program_id(0)\n-    # The stride represents how much we need to increase the pointer to advance 1 row\n-    row_start_ptr = input_ptr + row_idx * input_row_stride\n-    # The block size is the next power of two greater than n_cols, so we can fit each\n-    # row in a single block\n-    col_offsets = tl.arange(0, BLOCK_SIZE)\n-    input_ptrs = row_start_ptr + col_offsets\n-    # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n-    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n-    # Subtract maximum for numerical stability\n-    row_minus_max = row - tl.max(row, axis=0)\n-    # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n-    numerator = tl.exp(row_minus_max)\n-    denominator = tl.sum(numerator, axis=0)\n-    softmax_output = numerator / denominator\n-    # Write back output to DRAM\n-    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n-    output_ptrs = output_row_start_ptr + col_offsets\n-    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)\n-\n-\n-# %%\n-# We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.\n-\n-\n-def softmax(x):\n-    n_rows, n_cols = x.shape\n-    # The block size is the smallest power of two greater than the number of columns in `x`\n-    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n-    # Another trick we can use is to ask the compiler to use more threads per row by\n-    # increasing the number of warps (`num_warps`) over which each row is distributed.\n-    # You will see in the next tutorial how to auto-tune this value in a more natural\n-    # way so you don't have to come up with manual heuristics yourself.\n-    num_warps = 4\n-    if BLOCK_SIZE >= 2048:\n-        num_warps = 8\n-    if BLOCK_SIZE >= 4096:\n-        num_warps = 16\n-    # Allocate output\n-    y = torch.empty_like(x)\n-    # Enqueue kernel. The 1D launch grid is simple: we have one kernel instance per row o\n-    # f the input matrix\n-    softmax_kernel[(n_rows,)](\n-        y,\n-        x,\n-        x.stride(0),\n-        y.stride(0),\n-        n_cols,\n-        num_warps=num_warps,\n-        BLOCK_SIZE=BLOCK_SIZE,\n-    )\n-    return y\n-\n-\n-# %%\n-# Unit Test\n-# ---------\n-\n-# %%\n-# We make sure that we test our kernel on a matrix with an irregular number of rows and columns.\n-# This will allow us to verify that our padding mechanism works.\n-\n-torch.manual_seed(0)\n-x = torch.randn(1823, 781, device='cuda')\n-y_triton = softmax(x)\n-y_torch = torch.softmax(x, axis=1)\n-assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)\n-\n-# %%\n-# As expected, the results are identical.\n-\n-# %%\n-# Benchmark\n-# ---------\n-#\n-# Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.\n-# We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.\n-\n-\n-@triton.testing.perf_report(\n-    triton.testing.Benchmark(\n-        x_names=['N'],  # argument names to use as an x-axis for the plot\n-        x_vals=[\n-            128 * i for i in range(2, 100)\n-        ],  # different possible values for `x_name`\n-        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n-        line_vals=[\n-            'triton',\n-            'torch-native',\n-            'torch-jit',\n-        ],  # possible values for `line_arg``\n-        line_names=[\n-            \"Triton\",\n-            \"Torch (native)\",\n-            \"Torch (jit)\",\n-        ],  # label name for the lines\n-        styles=[('blue', '-'), ('green', '-'), ('green', '--')],  # line styles\n-        ylabel=\"GB/s\",  # label name for the y-axis\n-        plot_name=\"softmax-performance\",  # name for the plot. Used also as a file name for saving the plot.\n-        args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\n-    )\n-)\n-def benchmark(M, N, provider):\n-    x = torch.randn(M, N, device='cuda', dtype=torch.float32)\n-    quantiles = [0.5, 0.2, 0.8]\n-    if provider == 'torch-native':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1), quantiles=quantiles)\n-    if provider == 'triton':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: softmax(x), quantiles=quantiles)\n-    if provider == 'torch-jit':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_softmax(x), quantiles=quantiles)\n-    gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\n-    return gbps(ms), gbps(max_ms), gbps(min_ms)\n-\n-\n-benchmark.run(show_plots=True, print_data=True)\n-\n-# %%\n-# In the above plot, we can see that:\n-#  - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.\n-#  - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.\n-#    Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape."}, {"filename": "main/_downloads/f191ee1e78dc52eb5f7cba88f71cef2f/01-vector-add.ipynb", "status": "removed", "additions": 0, "deletions": 140, "changes": 140, "file_content_changes": "@@ -1,140 +0,0 @@\n-{\n-  \"cells\": [\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"%matplotlib inline\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"\\n# Vector Addition\\n\\nIn this tutorial, you will write a simple vector addition using Triton.\\n\\nIn doing so, you will learn about:\\n\\n* The basic programming model of Triton.\\n\\n* The `triton.jit` decorator, which is used to define Triton kernels.\\n\\n* The best practices for validating and benchmarking your custom ops against native reference implementations.\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Compute Kernel\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.jit\\ndef add_kernel(\\n    x_ptr,  # *Pointer* to first input vector.\\n    y_ptr,  # *Pointer* to second input vector.\\n    output_ptr,  # *Pointer* to output vector.\\n    n_elements,  # Size of the vector.\\n    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\\n                 # NOTE: `constexpr` so it can be used as a shape value.\\n):\\n    # There are multiple 'programs' processing different data. We identify which program\\n    # we are here:\\n    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\\n    # This program will process inputs that are offset from the initial data.\\n    # For instance, if you had a vector of length 256 and block_size of 64, the programs\\n    # would each access the elements [0:64, 64:128, 128:192, 192:256].\\n    # Note that offsets is a list of pointers:\\n    block_start = pid * BLOCK_SIZE\\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\\n    # Create a mask to guard memory operations against out-of-bounds accesses.\\n    mask = offsets < n_elements\\n    # Load x and y from DRAM, masking out any extra elements in case the input is not a\\n    # multiple of the block size.\\n    x = tl.load(x_ptr + offsets, mask=mask)\\n    y = tl.load(y_ptr + offsets, mask=mask)\\n    output = x + y\\n    # Write x + y back to DRAM.\\n    tl.store(output_ptr + offsets, output, mask=mask)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"Let's also declare a helper function to (1) allocate the `z` tensor\\nand (2) enqueue the above kernel with appropriate grid/block sizes:\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"def add(x: torch.Tensor, y: torch.Tensor):\\n    # We need to preallocate the output.\\n    output = torch.empty_like(x)\\n    assert x.is_cuda and y.is_cuda and output.is_cuda\\n    n_elements = output.numel()\\n    # The SPMD launch grid denotes the number of kernel instances that run in parallel.\\n    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\\n    # In this case, we use a 1D grid where the size is the number of blocks:\\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\\n    # NOTE:\\n    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.\\n    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.\\n    #  - Don't forget to pass meta-parameters as keywords arguments.\\n    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\\n    # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\\n    # running asynchronously at this point.\\n    return output\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"We can now use the above function to compute the element-wise sum of two `torch.tensor` objects and test its correctness:\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"torch.manual_seed(0)\\nsize = 98432\\nx = torch.rand(size, device='cuda')\\ny = torch.rand(size, device='cuda')\\noutput_torch = x + y\\noutput_triton = add(x, y)\\nprint(output_torch)\\nprint(output_triton)\\nprint(\\n    f'The maximum difference between torch and triton is '\\n    f'{torch.max(torch.abs(output_torch - output_triton))}'\\n)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"Seems like we're good to go!\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"## Benchmark\\n\\nWe can now benchmark our custom op on vectors of increasing sizes to get a sense of how it does relative to PyTorch.\\nTo make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of our custom ops.\\nfor different problem sizes.\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"@triton.testing.perf_report(\\n    triton.testing.Benchmark(\\n        x_names=['size'],  # Argument names to use as an x-axis for the plot.\\n        x_vals=[\\n            2 ** i for i in range(12, 28, 1)\\n        ],  # Different possible values for `x_name`.\\n        x_log=True,  # x axis is logarithmic.\\n        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\\n        line_vals=['triton', 'torch'],  # Possible values for `line_arg`.\\n        line_names=['Triton', 'Torch'],  # Label name for the lines.\\n        styles=[('blue', '-'), ('green', '-')],  # Line styles.\\n        ylabel='GB/s',  # Label name for the y-axis.\\n        plot_name='vector-add-performance',  # Name for the plot. Used also as a file name for saving the plot.\\n        args={},  # Values for function arguments not in `x_names` and `y_name`.\\n    )\\n)\\ndef benchmark(size, provider):\\n    x = torch.rand(size, device='cuda', dtype=torch.float32)\\n    y = torch.rand(size, device='cuda', dtype=torch.float32)\\n    quantiles = [0.5, 0.2, 0.8]\\n    if provider == 'torch':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: x + y, quantiles=quantiles)\\n    if provider == 'triton':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: add(x, y), quantiles=quantiles)\\n    gbps = lambda ms: 12 * size / ms * 1e-6\\n    return gbps(ms), gbps(max_ms), gbps(min_ms)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {},\n-      \"source\": [\n-        \"We can now run the decorated function above. Pass `print_data=True` to see the performance number, `show_plots=True` to plot them, and/or\\n`save_path='/path/to/results/' to save them to disk along with raw CSV data:\\n\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": null,\n-      \"metadata\": {\n-        \"collapsed\": false\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"benchmark.run(print_data=True, show_plots=True)\"\n-      ]\n-    }\n-  ],\n-  \"metadata\": {\n-    \"kernelspec\": {\n-      \"display_name\": \"Python 3\",\n-      \"language\": \"python\",\n-      \"name\": \"python3\"\n-    },\n-    \"language_info\": {\n-      \"codemirror_mode\": {\n-        \"name\": \"ipython\",\n-        \"version\": 3\n-      },\n-      \"file_extension\": \".py\",\n-      \"mimetype\": \"text/x-python\",\n-      \"name\": \"python\",\n-      \"nbconvert_exporter\": \"python\",\n-      \"pygments_lexer\": \"ipython3\",\n-      \"version\": \"3.8.10\"\n-    }\n-  },\n-  \"nbformat\": 4,\n-  \"nbformat_minor\": 0\n-}\n\\ No newline at end of file"}, {"filename": "main/_images/cuda-parallel-matmul.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/grouped_vs_row_major_ordering.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/halide-iteration.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/parallel_reduction.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/polyhedral-iteration.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_01-vector-add_001.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_01-vector-add_thumb.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_02-fused-softmax_001.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_02-fused-softmax_thumb.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_03-matrix-multiplication_001.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_03-matrix-multiplication_thumb.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_04-low-memory-dropout_thumb.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_05-layer-norm_001.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_05-layer-norm_thumb.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_001.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_002.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_003.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_004.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_thumb.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_07-math-functions_thumb.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_08-experimental-block-pointer_thumb.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/triton-parallel-matmul.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_sources/getting-started/installation.rst.txt", "status": "removed", "additions": 0, "deletions": 55, "changes": 55, "file_content_changes": "@@ -1,55 +0,0 @@\n-============\n-Installation\n-============\n-\n---------------------\n-Binary Distributions\n---------------------\n-\n-You can install the latest stable release of Triton from pip:\n-\n-.. code-block:: bash\n-\n-      pip install triton\n-\n-Binary wheels are available for CPython 3.7-3.11 and PyPy 3.8-3.9.\n-\n-And the latest nightly release:\n-\n-.. code-block:: bash\n-\n-      pip install -U --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/ triton-nightly\n-\n-\n------------\n-From Source\n------------\n-\n-++++++++++++++\n-Python Package\n-++++++++++++++\n-\n-You can install the Python package from source by running the following commands:\n-\n-.. code-block:: bash\n-\n-      git clone https://github.com/openai/triton.git;\n-      cd triton/python;\n-      pip install cmake; # build-time dependency\n-      pip install -e .\n-\n-Note that, if llvm-11 is not present on your system, the setup.py script will download the official LLVM11 static libraries link against that.\n-\n-You can then test your installation by running the unit tests:\n-\n-.. code-block:: bash\n-\n-      pip install -e '.[tests]'\n-      pytest -vs test/unit/\n-\n-and the benchmarks\n-\n-.. code-block:: bash\n-\n-      cd bench\n-      python -m run --with-plots --result-dir /tmp/triton-bench"}, {"filename": "main/_sources/getting-started/tutorials/01-vector-add.rst.txt", "status": "removed", "additions": 0, "deletions": 287, "changes": 287, "file_content_changes": "@@ -1,287 +0,0 @@\n-\n-.. DO NOT EDIT.\n-.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.\n-.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:\n-.. \"getting-started/tutorials/01-vector-add.py\"\n-.. LINE NUMBERS ARE GIVEN BELOW.\n-\n-.. only:: html\n-\n-    .. note::\n-        :class: sphx-glr-download-link-note\n-\n-        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_01-vector-add.py>`\n-        to download the full example code\n-\n-.. rst-class:: sphx-glr-example-title\n-\n-.. _sphx_glr_getting-started_tutorials_01-vector-add.py:\n-\n-\n-Vector Addition\n-===============\n-\n-In this tutorial, you will write a simple vector addition using Triton.\n-\n-In doing so, you will learn about:\n-\n-* The basic programming model of Triton.\n-\n-* The `triton.jit` decorator, which is used to define Triton kernels.\n-\n-* The best practices for validating and benchmarking your custom ops against native reference implementations.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 18-20\n-\n-Compute Kernel\n---------------\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 20-56\n-\n-.. code-block:: default\n-\n-\n-    import torch\n-\n-    import triton\n-    import triton.language as tl\n-\n-\n-    @triton.jit\n-    def add_kernel(\n-        x_ptr,  # *Pointer* to first input vector.\n-        y_ptr,  # *Pointer* to second input vector.\n-        output_ptr,  # *Pointer* to output vector.\n-        n_elements,  # Size of the vector.\n-        BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n-                     # NOTE: `constexpr` so it can be used as a shape value.\n-    ):\n-        # There are multiple 'programs' processing different data. We identify which program\n-        # we are here:\n-        pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n-        # This program will process inputs that are offset from the initial data.\n-        # For instance, if you had a vector of length 256 and block_size of 64, the programs\n-        # would each access the elements [0:64, 64:128, 128:192, 192:256].\n-        # Note that offsets is a list of pointers:\n-        block_start = pid * BLOCK_SIZE\n-        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n-        # Create a mask to guard memory operations against out-of-bounds accesses.\n-        mask = offsets < n_elements\n-        # Load x and y from DRAM, masking out any extra elements in case the input is not a\n-        # multiple of the block size.\n-        x = tl.load(x_ptr + offsets, mask=mask)\n-        y = tl.load(y_ptr + offsets, mask=mask)\n-        output = x + y\n-        # Write x + y back to DRAM.\n-        tl.store(output_ptr + offsets, output, mask=mask)\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 57-59\n-\n-Let's also declare a helper function to (1) allocate the `z` tensor\n-and (2) enqueue the above kernel with appropriate grid/block sizes:\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 59-80\n-\n-.. code-block:: default\n-\n-\n-\n-    def add(x: torch.Tensor, y: torch.Tensor):\n-        # We need to preallocate the output.\n-        output = torch.empty_like(x)\n-        assert x.is_cuda and y.is_cuda and output.is_cuda\n-        n_elements = output.numel()\n-        # The SPMD launch grid denotes the number of kernel instances that run in parallel.\n-        # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\n-        # In this case, we use a 1D grid where the size is the number of blocks:\n-        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n-        # NOTE:\n-        #  - Each torch.tensor object is implicitly converted into a pointer to its first element.\n-        #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.\n-        #  - Don't forget to pass meta-parameters as keywords arguments.\n-        add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n-        # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\n-        # running asynchronously at this point.\n-        return output\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 81-82\n-\n-We can now use the above function to compute the element-wise sum of two `torch.tensor` objects and test its correctness:\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 82-96\n-\n-.. code-block:: default\n-\n-\n-    torch.manual_seed(0)\n-    size = 98432\n-    x = torch.rand(size, device='cuda')\n-    y = torch.rand(size, device='cuda')\n-    output_torch = x + y\n-    output_triton = add(x, y)\n-    print(output_torch)\n-    print(output_triton)\n-    print(\n-        f'The maximum difference between torch and triton is '\n-        f'{torch.max(torch.abs(output_torch - output_triton))}'\n-    )\n-\n-\n-\n-\n-\n-.. rst-class:: sphx-glr-script-out\n-\n- .. code-block:: none\n-\n-    tensor([1.3713, 1.3076, 0.4940,  ..., 0.6724, 1.2141, 0.9733], device='cuda:0')\n-    tensor([1.3713, 1.3076, 0.4940,  ..., 0.6724, 1.2141, 0.9733], device='cuda:0')\n-    The maximum difference between torch and triton is 0.0\n-\n-\n-\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 97-98\n-\n-Seems like we're good to go!\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 100-106\n-\n-Benchmark\n----------\n-\n-We can now benchmark our custom op on vectors of increasing sizes to get a sense of how it does relative to PyTorch.\n-To make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of our custom ops.\n-for different problem sizes.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 106-136\n-\n-.. code-block:: default\n-\n-\n-\n-    @triton.testing.perf_report(\n-        triton.testing.Benchmark(\n-            x_names=['size'],  # Argument names to use as an x-axis for the plot.\n-            x_vals=[\n-                2 ** i for i in range(12, 28, 1)\n-            ],  # Different possible values for `x_name`.\n-            x_log=True,  # x axis is logarithmic.\n-            line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n-            line_vals=['triton', 'torch'],  # Possible values for `line_arg`.\n-            line_names=['Triton', 'Torch'],  # Label name for the lines.\n-            styles=[('blue', '-'), ('green', '-')],  # Line styles.\n-            ylabel='GB/s',  # Label name for the y-axis.\n-            plot_name='vector-add-performance',  # Name for the plot. Used also as a file name for saving the plot.\n-            args={},  # Values for function arguments not in `x_names` and `y_name`.\n-        )\n-    )\n-    def benchmark(size, provider):\n-        x = torch.rand(size, device='cuda', dtype=torch.float32)\n-        y = torch.rand(size, device='cuda', dtype=torch.float32)\n-        quantiles = [0.5, 0.2, 0.8]\n-        if provider == 'torch':\n-            ms, min_ms, max_ms = triton.testing.do_bench(lambda: x + y, quantiles=quantiles)\n-        if provider == 'triton':\n-            ms, min_ms, max_ms = triton.testing.do_bench(lambda: add(x, y), quantiles=quantiles)\n-        gbps = lambda ms: 12 * size / ms * 1e-6\n-        return gbps(ms), gbps(max_ms), gbps(min_ms)\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 137-139\n-\n-We can now run the decorated function above. Pass `print_data=True` to see the performance number, `show_plots=True` to plot them, and/or\n-`save_path='/path/to/results/' to save them to disk along with raw CSV data:\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 139-140\n-\n-.. code-block:: default\n-\n-    benchmark.run(print_data=True, show_plots=True)\n-\n-\n-\n-.. image-sg:: /getting-started/tutorials/images/sphx_glr_01-vector-add_001.png\n-   :alt: 01 vector add\n-   :srcset: /getting-started/tutorials/images/sphx_glr_01-vector-add_001.png\n-   :class: sphx-glr-single-img\n-\n-\n-.. rst-class:: sphx-glr-script-out\n-\n- .. code-block:: none\n-\n-    vector-add-performance:\n-               size       Triton        Torch\n-    0        4096.0     8.000000     9.600000\n-    1        8192.0    15.999999    15.999999\n-    2       16384.0    31.999999    31.999999\n-    3       32768.0    63.999998    63.999998\n-    4       65536.0   127.999995   127.999995\n-    5      131072.0   219.428568   219.428568\n-    6      262144.0   384.000001   384.000001\n-    7      524288.0   614.400016   614.400016\n-    8     1048576.0   819.200021   819.200021\n-    9     2097152.0  1023.999964  1023.999964\n-    10    4194304.0  1228.800031  1228.800031\n-    11    8388608.0  1424.695621  1424.695621\n-    12   16777216.0  1560.380965  1560.380965\n-    13   33554432.0  1631.601649  1624.859540\n-    14   67108864.0  1669.706983  1662.646960\n-    15  134217728.0  1684.008546  1678.616907\n-\n-\n-\n-\n-\n-.. rst-class:: sphx-glr-timing\n-\n-   **Total running time of the script:** ( 0 minutes  5.608 seconds)\n-\n-\n-.. _sphx_glr_download_getting-started_tutorials_01-vector-add.py:\n-\n-.. only:: html\n-\n-  .. container:: sphx-glr-footer sphx-glr-footer-example\n-\n-\n-\n-\n-    .. container:: sphx-glr-download sphx-glr-download-python\n-\n-      :download:`Download Python source code: 01-vector-add.py <01-vector-add.py>`\n-\n-    .. container:: sphx-glr-download sphx-glr-download-jupyter\n-\n-      :download:`Download Jupyter notebook: 01-vector-add.ipynb <01-vector-add.ipynb>`\n-\n-\n-.. only:: html\n-\n- .. rst-class:: sphx-glr-signature\n-\n-    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"}, {"filename": "main/_sources/getting-started/tutorials/02-fused-softmax.rst.txt", "status": "removed", "additions": 0, "deletions": 341, "changes": 341, "file_content_changes": "@@ -1,341 +0,0 @@\n-\n-.. DO NOT EDIT.\n-.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.\n-.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:\n-.. \"getting-started/tutorials/02-fused-softmax.py\"\n-.. LINE NUMBERS ARE GIVEN BELOW.\n-\n-.. only:: html\n-\n-    .. note::\n-        :class: sphx-glr-download-link-note\n-\n-        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`\n-        to download the full example code\n-\n-.. rst-class:: sphx-glr-example-title\n-\n-.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:\n-\n-\n-Fused Softmax\n-=============\n-\n-In this tutorial, you will write a fused softmax operation that is significantly faster\n-than PyTorch's native op for a particular class of matrices: those whose rows can fit in\n-the GPU's SRAM.\n-\n-In doing so, you will learn about:\n-\n-* The benefits of kernel fusion for bandwidth-bound operations.\n-\n-* Reduction operators in Triton.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 18-23\n-\n-Motivations\n------------\n-\n-Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.\n-Let us consider instead the case of a simple (numerically stabilized) softmax operation:\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 23-51\n-\n-.. code-block:: default\n-\n-\n-    import torch\n-\n-    import triton\n-    import triton.language as tl\n-\n-\n-    @torch.jit.script\n-    def naive_softmax(x):\n-        \"\"\"Compute row-wise softmax of X using native pytorch\n-\n-        We subtract the maximum element in order to avoid overflows. Softmax is invariant to\n-        this shift.\n-        \"\"\"\n-        # read  MN elements ; write M  elements\n-        x_max = x.max(dim=1)[0]\n-        # read MN + M elements ; write MN elements\n-        z = x - x_max[:, None]\n-        # read  MN elements ; write MN elements\n-        numerator = torch.exp(z)\n-        # read  MN elements ; write M  elements\n-        denominator = numerator.sum(dim=1)\n-        # read MN + M elements ; write MN elements\n-        ret = numerator / denominator[:, None]\n-        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements\n-        return ret\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 52-60\n-\n-When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \\in R^{M \\times N}`\n-requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.\n-This is obviously wasteful; we'd prefer to have a custom \"fused\" kernel that only reads\n-X once and does all the necessary computations on-chip.\n-Doing so would require reading and writing back only :math:`MN` bytes, so we could\n-expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).\n-The `torch.jit.script` flags aims to perform this kind of \"kernel fusion\" automatically\n-but, as we will see later, it is still far from ideal.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 62-71\n-\n-Compute Kernel\n---------------\n-\n-Our softmax kernel works as follows: each program loads a row of the input matrix X,\n-normalizes it and writes back the result to the output Y.\n-\n-Note that one important limitation of Triton is that each block must have a\n-power-of-two number of elements, so we need to internally \"pad\" each row and guard the\n-memory operations properly if we want to handle any possible input shapes:\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 71-100\n-\n-.. code-block:: default\n-\n-\n-\n-    @triton.jit\n-    def softmax_kernel(\n-        output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols,\n-        BLOCK_SIZE: tl.constexpr\n-    ):\n-        # The rows of the softmax are independent, so we parallelize across those\n-        row_idx = tl.program_id(0)\n-        # The stride represents how much we need to increase the pointer to advance 1 row\n-        row_start_ptr = input_ptr + row_idx * input_row_stride\n-        # The block size is the next power of two greater than n_cols, so we can fit each\n-        # row in a single block\n-        col_offsets = tl.arange(0, BLOCK_SIZE)\n-        input_ptrs = row_start_ptr + col_offsets\n-        # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n-        row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n-        # Subtract maximum for numerical stability\n-        row_minus_max = row - tl.max(row, axis=0)\n-        # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n-        numerator = tl.exp(row_minus_max)\n-        denominator = tl.sum(numerator, axis=0)\n-        softmax_output = numerator / denominator\n-        # Write back output to DRAM\n-        output_row_start_ptr = output_ptr + row_idx * output_row_stride\n-        output_ptrs = output_row_start_ptr + col_offsets\n-        tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 101-102\n-\n-We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 102-133\n-\n-.. code-block:: default\n-\n-\n-\n-    def softmax(x):\n-        n_rows, n_cols = x.shape\n-        # The block size is the smallest power of two greater than the number of columns in `x`\n-        BLOCK_SIZE = triton.next_power_of_2(n_cols)\n-        # Another trick we can use is to ask the compiler to use more threads per row by\n-        # increasing the number of warps (`num_warps`) over which each row is distributed.\n-        # You will see in the next tutorial how to auto-tune this value in a more natural\n-        # way so you don't have to come up with manual heuristics yourself.\n-        num_warps = 4\n-        if BLOCK_SIZE >= 2048:\n-            num_warps = 8\n-        if BLOCK_SIZE >= 4096:\n-            num_warps = 16\n-        # Allocate output\n-        y = torch.empty_like(x)\n-        # Enqueue kernel. The 1D launch grid is simple: we have one kernel instance per row o\n-        # f the input matrix\n-        softmax_kernel[(n_rows,)](\n-            y,\n-            x,\n-            x.stride(0),\n-            y.stride(0),\n-            n_cols,\n-            num_warps=num_warps,\n-            BLOCK_SIZE=BLOCK_SIZE,\n-        )\n-        return y\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 134-136\n-\n-Unit Test\n----------\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 138-140\n-\n-We make sure that we test our kernel on a matrix with an irregular number of rows and columns.\n-This will allow us to verify that our padding mechanism works.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 140-147\n-\n-.. code-block:: default\n-\n-\n-    torch.manual_seed(0)\n-    x = torch.randn(1823, 781, device='cuda')\n-    y_triton = softmax(x)\n-    y_torch = torch.softmax(x, axis=1)\n-    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)\n-\n-\n-\n-\n-\n-\n-\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 148-149\n-\n-As expected, the results are identical.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 151-156\n-\n-Benchmark\n----------\n-\n-Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.\n-We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 156-196\n-\n-.. code-block:: default\n-\n-\n-\n-    @triton.testing.perf_report(\n-        triton.testing.Benchmark(\n-            x_names=['N'],  # argument names to use as an x-axis for the plot\n-            x_vals=[\n-                128 * i for i in range(2, 100)\n-            ],  # different possible values for `x_name`\n-            line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n-            line_vals=[\n-                'triton',\n-                'torch-native',\n-                'torch-jit',\n-            ],  # possible values for `line_arg``\n-            line_names=[\n-                \"Triton\",\n-                \"Torch (native)\",\n-                \"Torch (jit)\",\n-            ],  # label name for the lines\n-            styles=[('blue', '-'), ('green', '-'), ('green', '--')],  # line styles\n-            ylabel=\"GB/s\",  # label name for the y-axis\n-            plot_name=\"softmax-performance\",  # name for the plot. Used also as a file name for saving the plot.\n-            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\n-        )\n-    )\n-    def benchmark(M, N, provider):\n-        x = torch.randn(M, N, device='cuda', dtype=torch.float32)\n-        quantiles = [0.5, 0.2, 0.8]\n-        if provider == 'torch-native':\n-            ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1), quantiles=quantiles)\n-        if provider == 'triton':\n-            ms, min_ms, max_ms = triton.testing.do_bench(lambda: softmax(x), quantiles=quantiles)\n-        if provider == 'torch-jit':\n-            ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_softmax(x), quantiles=quantiles)\n-        gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\n-        return gbps(ms), gbps(max_ms), gbps(min_ms)\n-\n-\n-    benchmark.run(show_plots=True, print_data=True)\n-\n-\n-\n-\n-.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png\n-   :alt: 02 fused softmax\n-   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png\n-   :class: sphx-glr-single-img\n-\n-\n-.. rst-class:: sphx-glr-script-out\n-\n- .. code-block:: none\n-\n-    softmax-performance:\n-              N       Triton  Torch (native)  Torch (jit)\n-    0     256.0   682.666643      744.727267   260.063494\n-    1     384.0   877.714274      819.200021   315.076934\n-    2     512.0   910.222190      963.764689   341.333321\n-    3     640.0  1024.000026      930.909084   372.363633\n-    4     768.0  1068.521715     1023.999964   384.000001\n-    ..      ...          ...             ...          ...\n-    93  12160.0  1601.316858     1066.082150   463.238099\n-    94  12288.0  1604.963246     1021.340281   462.063445\n-    95  12416.0  1602.064538     1031.979242   461.454135\n-    96  12544.0  1599.235121     1018.802024   460.330273\n-    97  12672.0  1602.782573     1008.716405   461.324232\n-\n-    [98 rows x 4 columns]\n-\n-\n-\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 197-201\n-\n-In the above plot, we can see that:\n- - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.\n- - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.\n-   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.\n-\n-\n-.. rst-class:: sphx-glr-timing\n-\n-   **Total running time of the script:** ( 0 minutes  37.028 seconds)\n-\n-\n-.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:\n-\n-.. only:: html\n-\n-  .. container:: sphx-glr-footer sphx-glr-footer-example\n-\n-\n-\n-\n-    .. container:: sphx-glr-download sphx-glr-download-python\n-\n-      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`\n-\n-    .. container:: sphx-glr-download sphx-glr-download-jupyter\n-\n-      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`\n-\n-\n-.. only:: html\n-\n- .. rst-class:: sphx-glr-signature\n-\n-    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"}, {"filename": "main/_sources/getting-started/tutorials/03-matrix-multiplication.rst.txt", "status": "removed", "additions": 0, "deletions": 515, "changes": 515, "file_content_changes": "@@ -1,515 +0,0 @@\n-\n-.. DO NOT EDIT.\n-.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.\n-.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:\n-.. \"getting-started/tutorials/03-matrix-multiplication.py\"\n-.. LINE NUMBERS ARE GIVEN BELOW.\n-\n-.. only:: html\n-\n-    .. note::\n-        :class: sphx-glr-download-link-note\n-\n-        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_03-matrix-multiplication.py>`\n-        to download the full example code\n-\n-.. rst-class:: sphx-glr-example-title\n-\n-.. _sphx_glr_getting-started_tutorials_03-matrix-multiplication.py:\n-\n-\n-Matrix Multiplication\n-=====================\n-In this tutorial, you will write a very short high-performance FP16 matrix multiplication kernel that achieves\n-performance on parallel with cuBLAS.\n-\n-You will specifically learn about:\n-\n-* Block-level matrix multiplications.\n-\n-* Multi-dimensional pointer arithmetics.\n-\n-* Program re-ordering for improved L2 cache hit rate.\n-\n-* Automatic performance tuning.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 20-48\n-\n-Motivations\n------------\n-\n-Matrix multiplications are a key building block of most modern high-performance computing systems.\n-They are notoriously hard to optimize, hence their implementation is generally done by\n-hardware vendors themselves as part of so-called \"kernel libraries\" (e.g., cuBLAS).\n-Unfortunately, these libraries are often proprietary and cannot be easily customized\n-to accommodate the needs of modern deep learning workloads (e.g., fused activation functions).\n-In this tutorial, you will learn how to implement efficient matrix multiplications by\n-yourself with Triton, in a way that is easy to customize and extend.\n-\n-Roughly speaking, the kernel that we will write will implement the following blocked\n-algorithm to multiply a (M, K) by a (K, N) matrix:\n-\n- .. code-block:: python\n-\n-   # Do in parallel\n-   for m in range(0, M, BLOCK_SIZE_M):\n-     # Do in parallel\n-     for n in range(0, N, BLOCK_SIZE_N):\n-       acc = zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=float32)\n-       for k in range(0, K, BLOCK_SIZE_K):\n-         a = A[m : m+BLOCK_SIZE_M, k : k+BLOCK_SIZE_K]\n-         b = B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]\n-         acc += dot(a, b)\n-       C[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N] = acc\n-\n-where each iteration of the doubly-nested for-loop is performed by a dedicated Triton program instance.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 50-148\n-\n-Compute Kernel\n---------------\n-\n-The above algorithm is, actually, fairly straightforward to implement in Triton.\n-The main difficulty comes from the computation of the memory locations at which blocks\n-of :code:`A` and :code:`B` must be read in the inner loop. For that, we need\n-multi-dimensional pointer arithmetics.\n-\n-Pointer Arithmetics\n-~~~~~~~~~~~~~~~~~~~\n-\n-For a row-major 2D tensor :code:`X`, the memory location of :code:`X[i, j]` is given b\n-y :code:`&X[i, j] = X + i*stride_xi + j*stride_xj`.\n-Therefore, blocks of pointers for :code:`A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K]` and\n-:code:`B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]` can be defined in pseudo-code as:\n-\n- .. code-block:: python\n-\n-   &A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K] =  a_ptr + (m : m+BLOCK_SIZE_M)[:, None]*A.stride(0) + (k : k+BLOCK_SIZE_K)[None, :]*A.stride(1);\n-   &B[k : k+BLOCK_SIZE_K, n:n+BLOCK_SIZE_N] =  b_ptr + (k : k+BLOCK_SIZE_K)[:, None]*B.stride(0) + (n : n+BLOCK_SIZE_N)[None, :]*B.stride(1);\n-\n-Which means that pointers for blocks of A and B can be initialized (i.e., :code:`k=0`) in Triton as the following\n-code. Also note that we need an extra modulo to handle the case where :code:`M` is not a multiple of\n-:code:`BLOCK_SIZE_M` or :code:`N` is not a multiple of :code:`BLOCK_SIZE_N`, in which case we can pad the data with\n-some useless values, which will not contribute to the results. For the :code:`K` dimension, we will handle that later\n-using masking load semantics.\n-\n- .. code-block:: python\n-\n-   offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n-   offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n-   offs_k = tl.arange(0, BLOCK_SIZE_K)\n-   a_ptrs = a_ptr + (offs_am[:, None]*stride_am + offs_k [None, :]*stride_ak)\n-   b_ptrs = b_ptr + (offs_k [:, None]*stride_bk + offs_bn[None, :]*stride_bn)\n-\n-And then updated in the inner loop as follows:\n-\n- .. code-block:: python\n-\n-   a_ptrs += BLOCK_SIZE_K * stride_ak;\n-   b_ptrs += BLOCK_SIZE_K * stride_bk;\n-\n-\n-L2 Cache Optimizations\n-~~~~~~~~~~~~~~~~~~~~~~\n-\n-As mentioned above, each program instance computes a :code:`[BLOCK_SIZE_M, BLOCK_SIZE_N]`\n-block of :code:`C`.\n-It is important to remember that the order in which these blocks are computed does\n-matter, since it affects the L2 cache hit rate of our program. and unfortunately, a\n-a simple row-major ordering\n-\n- .. code-block:: Python\n-\n-   pid = triton.program_id(0);\n-   grid_m = (M + BLOCK_SIZE_M - 1) // BLOCK_SIZE_M;\n-   grid_n = (N + BLOCK_SIZE_N - 1) // BLOCK_SIZE_N;\n-   pid_m = pid / grid_n;\n-   pid_n = pid % grid_n;\n-\n-is just not going to cut it.\n-\n-One possible solution is to launch blocks in an order that promotes data reuse.\n-This can be done by 'super-grouping' blocks in groups of :code:`GROUP_M` rows before\n-switching to the next column:\n-\n- .. code-block:: python\n-\n-   # Program ID\n-   pid = tl.program_id(axis=0)\n-   # Number of program ids along the M axis\n-   num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n-   # Number of programs ids along the N axis\n-   num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n-   # Number of programs in group\n-   num_pid_in_group = GROUP_SIZE_M * num_pid_n\n-   # Id of the group this program is in\n-   group_id = pid // num_pid_in_group\n-   # Row-id of the first program in the group\n-   first_pid_m = group_id * GROUP_SIZE_M\n-   # If `num_pid_m` isn't divisible by `GROUP_SIZE_M`, the last group is smaller\n-   group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n-   # *Within groups*, programs are ordered in a column-major order\n-   # Row-id of the program in the *launch grid*\n-   pid_m = first_pid_m + (pid % group_size_m)\n-   # Col-id of the program in the *launch grid*\n-   pid_n = (pid % num_pid_in_group) // group_size_m\n-\n-For example, in the following matmul where each matrix is 9 blocks by 9 blocks,\n-we can see that if we compute the output in row-major ordering, we need to load 90\n-blocks into SRAM to compute the first 9 output blocks, but if we do it in grouped\n-ordering, we only need to load 54 blocks.\n-\n-  .. image:: grouped_vs_row_major_ordering.png\n-\n-In practice, this can improve the performance of our matrix multiplication kernel by\n-more than 10\\% on some hardware architecture (e.g., 220 to 245 TFLOPS on A100).\n-\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 150-152\n-\n-Final Result\n-------------\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 152-262\n-\n-.. code-block:: default\n-\n-\n-    import torch\n-\n-    import triton\n-    import triton.language as tl\n-\n-\n-    # `triton.jit`'ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:\n-    #   - A list of `triton.Config` objects that define different configurations of\n-    #       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try\n-    #   - An auto-tuning *key* whose change in values will trigger evaluation of all the\n-    #       provided configs\n-    @triton.autotune(\n-        configs=[\n-            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n-            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n-            triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n-        ],\n-        key=['M', 'N', 'K'],\n-    )\n-    @triton.jit\n-    def matmul_kernel(\n-        # Pointers to matrices\n-        a_ptr, b_ptr, c_ptr,\n-        # Matrix dimensions\n-        M, N, K,\n-        # The stride variables represent how much to increase the ptr by when moving by 1\n-        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n-        # by to get the element one row down (A has M rows).\n-        stride_am, stride_ak,\n-        stride_bk, stride_bn,\n-        stride_cm, stride_cn,\n-        # Meta-parameters\n-        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n-        GROUP_SIZE_M: tl.constexpr,\n-        ACTIVATION: tl.constexpr,\n-    ):\n-        \"\"\"Kernel for computing the matmul C = A x B.\n-        A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n-        \"\"\"\n-        # -----------------------------------------------------------\n-        # Map program ids `pid` to the block of C it should compute.\n-        # This is done in a grouped ordering to promote L2 data reuse.\n-        # See above `L2 Cache Optimizations` section for details.\n-        pid = tl.program_id(axis=0)\n-        num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n-        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n-        num_pid_in_group = GROUP_SIZE_M * num_pid_n\n-        group_id = pid // num_pid_in_group\n-        first_pid_m = group_id * GROUP_SIZE_M\n-        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n-        pid_m = first_pid_m + (pid % group_size_m)\n-        pid_n = (pid % num_pid_in_group) // group_size_m\n-\n-        # ----------------------------------------------------------\n-        # Create pointers for the first blocks of A and B.\n-        # We will advance this pointer as we move in the K direction\n-        # and accumulate\n-        # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n-        # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n-        # See above `Pointer Arithmetics` section for details\n-        offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n-        offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n-        offs_k = tl.arange(0, BLOCK_SIZE_K)\n-        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n-        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n-\n-        # -----------------------------------------------------------\n-        # Iterate to compute a block of the C matrix.\n-        # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n-        # of fp32 values for higher accuracy.\n-        # `accumulator` will be converted back to fp16 after the loop.\n-        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n-            # Load the next block of A and B, generate a mask by checking the K dimension.\n-            # If it is out of bounds, set it to 0.\n-            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n-            b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n-            # We accumulate along the K dimension.\n-            accumulator += tl.dot(a, b)\n-            # Advance the ptrs to the next K block.\n-            a_ptrs += BLOCK_SIZE_K * stride_ak\n-            b_ptrs += BLOCK_SIZE_K * stride_bk\n-        # You can fuse arbitrary activation functions here\n-        # while the accumulator is still in FP32!\n-        if ACTIVATION == \"leaky_relu\":\n-            accumulator = leaky_relu(accumulator)\n-        c = accumulator.to(tl.float16)\n-\n-        # -----------------------------------------------------------\n-        # Write back the block of the output matrix C with masks.\n-        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n-        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n-        c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n-        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n-        tl.store(c_ptrs, c, mask=c_mask)\n-\n-\n-    # We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.\n-    @triton.jit\n-    def leaky_relu(x):\n-        x = x + 1\n-        return tl.where(x >= 0, x, 0.01 * x)\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 263-265\n-\n-We can now create a convenience wrapper function that only takes two input tensors,\n-and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 265-291\n-\n-.. code-block:: default\n-\n-\n-\n-    def matmul(a, b, activation=\"\"):\n-        # Check constraints.\n-        assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n-        assert a.is_contiguous(), \"Matrix A must be contiguous\"\n-        assert b.is_contiguous(), \"Matrix B must be contiguous\"\n-        M, K = a.shape\n-        K, N = b.shape\n-        # Allocates output.\n-        c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n-        # 1D launch kernel where each block gets its own program.\n-        grid = lambda META: (\n-            triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n-        )\n-        matmul_kernel[grid](\n-            a, b, c,\n-            M, N, K,\n-            a.stride(0), a.stride(1),\n-            b.stride(0), b.stride(1),\n-            c.stride(0), c.stride(1),\n-            ACTIVATION=activation\n-        )\n-        return c\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 292-296\n-\n-Unit Test\n----------\n-\n-We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS).\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 296-309\n-\n-.. code-block:: default\n-\n-\n-    torch.manual_seed(0)\n-    a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n-    b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n-    triton_output = matmul(a, b)\n-    torch_output = torch.matmul(a, b)\n-    print(f\"triton_output={triton_output}\")\n-    print(f\"torch_output={torch_output}\")\n-    if torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\n-        print(\"\u2705 Triton and Torch match\")\n-    else:\n-        print(\"\u274c Triton and Torch differ\")\n-\n-\n-\n-\n-\n-.. rst-class:: sphx-glr-script-out\n-\n- .. code-block:: none\n-\n-    triton_output=tensor([[-10.9531,  -4.7109,  15.6953,  ..., -28.4062,   4.3320, -26.4219],\n-            [ 26.8438,  10.0469,  -5.4297,  ..., -11.2969,  -8.5312,  30.7500],\n-            [-13.2578,  15.8516,  18.0781,  ..., -21.7656,  -8.6406,  10.2031],\n-            ...,\n-            [ 40.2812,  18.6094, -25.6094,  ...,  -2.7598,  -3.2441,  41.0000],\n-            [ -6.1211, -16.8281,   4.4844,  ..., -21.0312,  24.7031,  15.0234],\n-            [-17.0938, -19.0000,  -0.3831,  ...,  21.5469, -30.2344, -13.2188]],\n-           device='cuda:0', dtype=torch.float16)\n-    torch_output=tensor([[-10.9531,  -4.7109,  15.6953,  ..., -28.4062,   4.3320, -26.4219],\n-            [ 26.8438,  10.0469,  -5.4297,  ..., -11.2969,  -8.5312,  30.7500],\n-            [-13.2578,  15.8516,  18.0781,  ..., -21.7656,  -8.6406,  10.2031],\n-            ...,\n-            [ 40.2812,  18.6094, -25.6094,  ...,  -2.7598,  -3.2441,  41.0000],\n-            [ -6.1211, -16.8281,   4.4844,  ..., -21.0312,  24.7031,  15.0234],\n-            [-17.0938, -19.0000,  -0.3831,  ...,  21.5469, -30.2344, -13.2188]],\n-           device='cuda:0', dtype=torch.float16)\n-    \u2705 Triton and Torch match\n-\n-\n-\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 310-318\n-\n-Benchmark\n----------\n-\n-Square Matrix Performance\n-~~~~~~~~~~~~~~~~~~~~~~~~~~\n-\n-We can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices,\n-but feel free to arrange this script as you wish to benchmark any other matrix shape.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 318-351\n-\n-.. code-block:: default\n-\n-\n-\n-    @triton.testing.perf_report(\n-        triton.testing.Benchmark(\n-            x_names=['M', 'N', 'K'],  # Argument names to use as an x-axis for the plot\n-            x_vals=[\n-                128 * i for i in range(2, 33)\n-            ],  # Different possible values for `x_name`\n-            line_arg='provider',  # Argument name whose value corresponds to a different line in the plot\n-            # Possible values for `line_arg`\n-            line_vals=['cublas', 'triton'],\n-            # Label name for the lines\n-            line_names=[\"cuBLAS\", \"Triton\"],\n-            # Line styles\n-            styles=[('green', '-'), ('blue', '-')],\n-            ylabel=\"TFLOPS\",  # Label name for the y-axis\n-            plot_name=\"matmul-performance\",  # Name for the plot, used also as a file name for saving the plot.\n-            args={},\n-        )\n-    )\n-    def benchmark(M, N, K, provider):\n-        a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n-        b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n-        quantiles = [0.5, 0.2, 0.8]\n-        if provider == 'cublas':\n-            ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\n-        if provider == 'triton':\n-            ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\n-        perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n-        return perf(ms), perf(max_ms), perf(min_ms)\n-\n-\n-    benchmark.run(show_plots=True, print_data=True)\n-\n-\n-\n-.. image-sg:: /getting-started/tutorials/images/sphx_glr_03-matrix-multiplication_001.png\n-   :alt: 03 matrix multiplication\n-   :srcset: /getting-started/tutorials/images/sphx_glr_03-matrix-multiplication_001.png\n-   :class: sphx-glr-single-img\n-\n-\n-.. rst-class:: sphx-glr-script-out\n-\n- .. code-block:: none\n-\n-    matmul-performance:\n-             M      cuBLAS      Triton\n-    0    256.0    4.096000    4.096000\n-    1    384.0   11.059200   12.288000\n-    2    512.0   26.214401   23.831273\n-    3    640.0   42.666665   42.666665\n-    4    768.0   68.056616   58.982401\n-    5    896.0   78.051553   82.642822\n-    6   1024.0  104.857603   99.864382\n-    7   1152.0  135.726544  129.825388\n-    8   1280.0  157.538463  163.840004\n-    9   1408.0  155.765024  132.970149\n-    10  1536.0  181.484314  157.286398\n-    11  1664.0  179.978245  179.978245\n-    12  1792.0  172.914215  208.137481\n-    13  1920.0  200.347822  168.585369\n-    14  2048.0  223.696203  190.650180\n-    15  2176.0  211.827867  209.621326\n-    16  2304.0  225.357284  225.357284\n-    17  2432.0  203.583068  203.583068\n-    18  2560.0  224.438347  218.453323\n-    19  2688.0  197.567993  194.528492\n-    20  2816.0  205.727397  210.696652\n-    21  2944.0  218.579083  219.541994\n-    22  3072.0  204.785179  207.410628\n-    23  3200.0  213.333323  218.430042\n-    24  3328.0  203.941342  206.278780\n-    25  3456.0  216.143621  215.565692\n-    26  3584.0  218.772251  206.227962\n-    27  3712.0  208.553950  216.228019\n-    28  3840.0  208.664143  210.250955\n-    29  3968.0  209.663117  215.971570\n-    30  4096.0  217.005221  211.366499\n-\n-\n-\n-\n-\n-.. rst-class:: sphx-glr-timing\n-\n-   **Total running time of the script:** ( 0 minutes  40.533 seconds)\n-\n-\n-.. _sphx_glr_download_getting-started_tutorials_03-matrix-multiplication.py:\n-\n-.. only:: html\n-\n-  .. container:: sphx-glr-footer sphx-glr-footer-example\n-\n-\n-\n-\n-    .. container:: sphx-glr-download sphx-glr-download-python\n-\n-      :download:`Download Python source code: 03-matrix-multiplication.py <03-matrix-multiplication.py>`\n-\n-    .. container:: sphx-glr-download sphx-glr-download-jupyter\n-\n-      :download:`Download Jupyter notebook: 03-matrix-multiplication.ipynb <03-matrix-multiplication.ipynb>`\n-\n-\n-.. only:: html\n-\n- .. rst-class:: sphx-glr-signature\n-\n-    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"}, {"filename": "main/_sources/getting-started/tutorials/04-low-memory-dropout.rst.txt", "status": "removed", "additions": 0, "deletions": 270, "changes": 270, "file_content_changes": "@@ -1,270 +0,0 @@\n-\n-.. DO NOT EDIT.\n-.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.\n-.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:\n-.. \"getting-started/tutorials/04-low-memory-dropout.py\"\n-.. LINE NUMBERS ARE GIVEN BELOW.\n-\n-.. only:: html\n-\n-    .. note::\n-        :class: sphx-glr-download-link-note\n-\n-        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_04-low-memory-dropout.py>`\n-        to download the full example code\n-\n-.. rst-class:: sphx-glr-example-title\n-\n-.. _sphx_glr_getting-started_tutorials_04-low-memory-dropout.py:\n-\n-\n-Low-Memory Dropout\n-==================\n-\n-In this tutorial, you will write a memory-efficient implementation of dropout whose state\n-will be composed of a single int32 seed. This differs from more traditional implementations of dropout,\n-whose state is generally composed of a bit mask tensor of the same shape as the input.\n-\n-In doing so, you will learn about:\n-\n-* The limitations of naive implementations of Dropout with PyTorch.\n-\n-* Parallel pseudo-random number generation in Triton.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 18-34\n-\n-Baseline\n---------\n-\n-The *dropout* operator was first introduced in [SRIVASTAVA2014]_ as a way to improve the performance\n-of deep neural networks in low-data regime (i.e. regularization).\n-\n-It takes a vector as input and produces a vector of the same shape as output. Each scalar in the\n-output has a probability :math:`p` of being changed to zero and otherwise it is copied from the input.\n-This forces the network to perform well even when only :math:`1 - p` scalars from the input are available.\n-\n-At evaluation time we want to use the full power of the network so we set :math:`p=0`. Naively this would\n-increase the norm of the output (which can be a bad thing, e.g. it can lead to artificial decrease\n-in the output softmax temperature). To prevent this we multiply the output by :math:`\\frac{1}{1 - p}`, which\n-keeps the norm consistent regardless of the dropout probability.\n-\n-Let's first take a look at the baseline implementation.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 34-87\n-\n-.. code-block:: default\n-\n-\n-\n-    import tabulate\n-    import torch\n-\n-    import triton\n-    import triton.language as tl\n-\n-\n-    @triton.jit\n-    def _dropout(\n-        x_ptr,  # pointer to the input\n-        x_keep_ptr,  # pointer to a mask of 0s and 1s\n-        output_ptr,  # pointer to the output\n-        n_elements,  # number of elements in the `x` tensor\n-        p,  # probability that an element of `x` is changed to zero\n-        BLOCK_SIZE: tl.constexpr,\n-    ):\n-        pid = tl.program_id(axis=0)\n-        block_start = pid * BLOCK_SIZE\n-        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n-        mask = offsets < n_elements\n-        # Load data\n-        x = tl.load(x_ptr + offsets, mask=mask)\n-        x_keep = tl.load(x_keep_ptr + offsets, mask=mask)\n-        # The line below is the crucial part, described in the paragraph above!\n-        output = tl.where(x_keep, x / (1 - p), 0.0)\n-        # Write-back output\n-        tl.store(output_ptr + offsets, output, mask=mask)\n-\n-\n-    def dropout(x, x_keep, p):\n-        output = torch.empty_like(x)\n-        assert x.is_contiguous()\n-        n_elements = x.numel()\n-        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n-        _dropout[grid](x, x_keep, output, n_elements, p, BLOCK_SIZE=1024)\n-        return output\n-\n-\n-    # Input tensor\n-    x = torch.randn(size=(10,)).cuda()\n-    # Dropout mask\n-    p = 0.5\n-    x_keep = (torch.rand(size=(10,)) > p).to(torch.int32).cuda()\n-    #\n-    output = dropout(x, x_keep=x_keep, p=p)\n-    print(tabulate.tabulate([\n-        [\"input\"] + x.tolist(),\n-        [\"keep mask\"] + x_keep.tolist(),\n-        [\"output\"] + output.tolist()\n-    ]))\n-\n-\n-\n-\n-\n-.. rst-class:: sphx-glr-script-out\n-\n- .. code-block:: none\n-\n-    ---------  -------  ---------  --------  --------  --------  --------  --------  --------  ---------  ---------\n-    input      1.541    -0.293429  -2.17879  0.568431  -1.08452  -1.3986   0.403347  0.838026  -0.719258  -0.403344\n-    keep mask  1         1          0        1          0         1        1         0          0          0\n-    output     3.08199  -0.586858   0        1.13686    0        -2.79719  0.806694  0          0          0\n-    ---------  -------  ---------  --------  --------  --------  --------  --------  --------  ---------  ---------\n-\n-\n-\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 88-107\n-\n-Seeded dropout\n---------------\n-\n-The above implementation of dropout works fine, but it can be a bit awkward to deal with. Firstly\n-we need to store the dropout mask for backpropagation. Secondly, dropout state management can get\n-very tricky when using recompute/checkpointing (e.g. see all the notes about `preserve_rng_state` in\n-https://pytorch.org/docs/1.9.0/checkpoint.html). In this tutorial we'll describe an alternative implementation\n-that (1) has a smaller memory footprint; (2) requires less data movement; and (3) simplifies the management\n-of persisting randomness across multiple invocations of the kernel.\n-\n-Pseudo-random number generation in Triton is simple! In this tutorial we will use the\n-:code:`triton.language.rand` function which generates a block of uniformly distributed :code:`float32`\n-values in [0, 1), given a seed and a block of :code:`int32` offsets. But if you need it, Triton also provides\n-other :ref:`random number generation strategies <Random Number Generation>`.\n-\n-.. note::\n-   Triton's implementation of PRNG is based on the Philox algorithm (described on [SALMON2011]_).\n-\n-Let's put it all together.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 107-155\n-\n-.. code-block:: default\n-\n-\n-\n-    @triton.jit\n-    def _seeded_dropout(\n-        x_ptr,\n-        output_ptr,\n-        n_elements,\n-        p,\n-        seed,\n-        BLOCK_SIZE: tl.constexpr,\n-    ):\n-        # compute memory offsets of elements handled by this instance\n-        pid = tl.program_id(axis=0)\n-        block_start = pid * BLOCK_SIZE\n-        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n-        # load data from x\n-        mask = offsets < n_elements\n-        x = tl.load(x_ptr + offsets, mask=mask)\n-        # randomly prune it\n-        random = tl.rand(seed, offsets)\n-        x_keep = random > p\n-        # write-back\n-        output = tl.where(x_keep, x / (1 - p), 0.0)\n-        tl.store(output_ptr + offsets, output, mask=mask)\n-\n-\n-    def seeded_dropout(x, p, seed):\n-        output = torch.empty_like(x)\n-        assert x.is_contiguous()\n-        n_elements = x.numel()\n-        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n-        _seeded_dropout[grid](x, output, n_elements, p, seed, BLOCK_SIZE=1024)\n-        return output\n-\n-\n-    x = torch.randn(size=(10,)).cuda()\n-    # Compare this to the baseline - dropout mask is never instantiated!\n-    output = seeded_dropout(x, p=0.5, seed=123)\n-    output2 = seeded_dropout(x, p=0.5, seed=123)\n-    output3 = seeded_dropout(x, p=0.5, seed=512)\n-\n-    print(tabulate.tabulate([\n-        [\"input\"] + x.tolist(),\n-        [\"output (seed = 123)\"] + output.tolist(),\n-        [\"output (seed = 123)\"] + output2.tolist(),\n-        [\"output (seed = 512)\"] + output3.tolist()\n-    ]))\n-\n-\n-\n-\n-\n-.. rst-class:: sphx-glr-script-out\n-\n- .. code-block:: none\n-\n-    -------------------  ---------  --------  --------  -------  --------  --------  ---------  ---------  ---------  ---------\n-    input                -0.952835  0.371721  0.408716  1.42142  0.149397  -0.67086  -0.214186  -0.431969  -0.707878  -0.106434\n-    output (seed = 123)   0         0.743443  0         0        0         -1.34172   0          0         -1.41576   -0.212868\n-    output (seed = 123)   0         0.743443  0         0        0         -1.34172   0          0         -1.41576   -0.212868\n-    output (seed = 512)   0         0         0.817432  2.84284  0         -1.34172  -0.428372   0          0          0\n-    -------------------  ---------  --------  --------  -------  --------  --------  ---------  ---------  ---------  ---------\n-\n-\n-\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 156-159\n-\n-Et Voil\u00e0! We have a triton kernel that applies the same dropout mask provided the seed is the same!\n-If you'd like explore further applications of pseudorandomness in GPU programming, we encourage you\n-to explore the `triton/language/random` folder!\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 161-167\n-\n-Exercises\n----------\n-\n-1. Extend the kernel to operate over a matrix and use a vector of seeds - one per row.\n-2. Add support for striding.\n-3. (challenge) Implement a kernel for sparse Johnson-Lindenstrauss transform which generates the projection matrix one the fly each time using a seed.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 169-174\n-\n-References\n-----------\n-\n-.. [SALMON2011] John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw, \"Parallel Random Numbers: As Easy as 1, 2, 3\", 2011\n-.. [SRIVASTAVA2014] Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\", JMLR 2014\n-\n-\n-.. rst-class:: sphx-glr-timing\n-\n-   **Total running time of the script:** ( 0 minutes  0.607 seconds)\n-\n-\n-.. _sphx_glr_download_getting-started_tutorials_04-low-memory-dropout.py:\n-\n-.. only:: html\n-\n-  .. container:: sphx-glr-footer sphx-glr-footer-example\n-\n-\n-\n-\n-    .. container:: sphx-glr-download sphx-glr-download-python\n-\n-      :download:`Download Python source code: 04-low-memory-dropout.py <04-low-memory-dropout.py>`\n-\n-    .. container:: sphx-glr-download sphx-glr-download-jupyter\n-\n-      :download:`Download Jupyter notebook: 04-low-memory-dropout.ipynb <04-low-memory-dropout.ipynb>`\n-\n-\n-.. only:: html\n-\n- .. rst-class:: sphx-glr-signature\n-\n-    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"}, {"filename": "main/_sources/getting-started/tutorials/05-layer-norm.rst.txt", "status": "removed", "additions": 0, "deletions": 503, "changes": 503, "file_content_changes": "@@ -1,503 +0,0 @@\n-\n-.. DO NOT EDIT.\n-.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.\n-.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:\n-.. \"getting-started/tutorials/05-layer-norm.py\"\n-.. LINE NUMBERS ARE GIVEN BELOW.\n-\n-.. only:: html\n-\n-    .. note::\n-        :class: sphx-glr-download-link-note\n-\n-        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_05-layer-norm.py>`\n-        to download the full example code\n-\n-.. rst-class:: sphx-glr-example-title\n-\n-.. _sphx_glr_getting-started_tutorials_05-layer-norm.py:\n-\n-\n-Layer Normalization\n-====================\n-In this tutorial, you will write a high-performance layer normalization\n-kernel that runs faster than the PyTorch implementation.\n-\n-In doing so, you will learn about:\n-\n-* Implementing backward pass in Triton.\n-\n-* Implementing parallel reduction in Triton.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 16-31\n-\n-Motivations\n------------\n-\n-The *LayerNorm* operator was first introduced in [BA2016]_ as a way to improve the performance\n-of sequential models (e.g., Transformers) or neural networks with small batch size.\n-It takes a vector :math:`x` as input and produces a vector :math:`y` of the same shape as output.\n-The normalization is performed by subtracting the mean and dividing by the standard deviation of :math:`x`.\n-After the normalization, a learnable linear transformation with weights :math:`w` and biases :math:`b` is applied.\n-The forward pass can be expressed as follows:\n-\n-.. math::\n-   y = \\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} } * w + b\n-\n-where :math:`\\epsilon` is a small constant added to the denominator for numerical stability.\n-Let\u2019s first take a look at the forward pass implementation.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 31-96\n-\n-.. code-block:: default\n-\n-\n-    import torch\n-\n-    import triton\n-    import triton.language as tl\n-\n-    try:\n-        # This is https://github.com/NVIDIA/apex, NOT the apex on PyPi, so it\n-        # should not be added to extras_require in setup.py.\n-        import apex\n-        HAS_APEX = True\n-    except ModuleNotFoundError:\n-        HAS_APEX = False\n-\n-\n-    @triton.jit\n-    def _layer_norm_fwd_fused(\n-        X,  # pointer to the input\n-        Y,  # pointer to the output\n-        W,  # pointer to the weights\n-        B,  # pointer to the biases\n-        Mean,  # pointer to the mean\n-        Rstd,  # pointer to the 1/std\n-        stride,  # how much to increase the pointer when moving by 1 row\n-        N,  # number of columns in X\n-        eps,  # epsilon to avoid division by zero\n-        BLOCK_SIZE: tl.constexpr,\n-    ):\n-        # Map the program id to the row of X and Y it should compute.\n-        row = tl.program_id(0)\n-        Y += row * stride\n-        X += row * stride\n-        # Compute mean\n-        mean = 0\n-        _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n-        for off in range(0, N, BLOCK_SIZE):\n-            cols = off + tl.arange(0, BLOCK_SIZE)\n-            a = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n-            _mean += a\n-        mean = tl.sum(_mean, axis=0) / N\n-        # Compute variance\n-        _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n-        for off in range(0, N, BLOCK_SIZE):\n-            cols = off + tl.arange(0, BLOCK_SIZE)\n-            x = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n-            x = tl.where(cols < N, x - mean, 0.)\n-            _var += x * x\n-        var = tl.sum(_var, axis=0) / N\n-        rstd = 1 / tl.sqrt(var + eps)\n-        # Write mean / rstd\n-        tl.store(Mean + row, mean)\n-        tl.store(Rstd + row, rstd)\n-        # Normalize and apply linear transformation\n-        for off in range(0, N, BLOCK_SIZE):\n-            cols = off + tl.arange(0, BLOCK_SIZE)\n-            mask = cols < N\n-            w = tl.load(W + cols, mask=mask)\n-            b = tl.load(B + cols, mask=mask)\n-            x = tl.load(X + cols, mask=mask, other=0.).to(tl.float32)\n-            x_hat = (x - mean) * rstd\n-            y = x_hat * w + b\n-            # Write output\n-            tl.store(Y + cols, y, mask=mask)\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 97-128\n-\n-Backward pass\n--------------\n-\n-The backward pass for the layer normalization operator is a bit more involved than the forward pass.\n-Let :math:`\\hat{x}` be the normalized inputs :math:`\\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} }` before the linear transformation,\n-the Vector-Jacobian Products (VJP) :math:`\\nabla_{x}` of :math:`x` are given by:\n-\n-.. math::\n-   \\nabla_{x} = \\frac{1}{\\sigma}\\Big( \\nabla_{y} \\odot w - \\underbrace{ \\big( \\frac{1}{N} \\hat{x} \\cdot (\\nabla_{y} \\odot w) \\big) }_{c_1} \\odot \\hat{x} - \\underbrace{ \\frac{1}{N} \\nabla_{y} \\cdot w }_{c_2} \\Big)\n-\n-where :math:`\\odot` denotes the element-wise multiplication, :math:`\\cdot` denotes the dot product, and :math:`\\sigma` is the standard deviation.\n-:math:`c_1` and :math:`c_2` are intermediate constants that improve the readability of the following implementation.\n-\n-For the weights :math:`w` and biases :math:`b`, the VJPs :math:`\\nabla_{w}` and :math:`\\nabla_{b}` are more straightforward:\n-\n-.. math::\n-   \\nabla_{w} = \\nabla_{y} \\odot \\hat{x} \\quad \\text{and} \\quad \\nabla_{b} = \\nabla_{y}\n-\n-Since the same weights :math:`w` and biases :math:`b` are used for all rows in the same batch, their gradients need to sum up.\n-To perform this step efficiently, we use a parallel reduction strategy: each kernel instance accumulates\n-partial :math:`\\nabla_{w}` and :math:`\\nabla_{b}` across certain rows into one of :math:`\\text{GROUP_SIZE_M}` independent buffers.\n-These buffers stay in the L2 cache and then are further reduced by another function to compute the actual :math:`\\nabla_{w}` and :math:`\\nabla_{b}`.\n-\n-Let the number of input rows :math:`M = 4` and :math:`\\text{GROUP_SIZE_M} = 2`,\n-here's a diagram of the parallel reduction strategy for :math:`\\nabla_{w}` (:math:`\\nabla_{b}` is omitted for brevity):\n-\n-  .. image:: parallel_reduction.png\n-\n-In Stage 1, the rows of X that have the same color share the same buffer and thus a lock is used to ensure that only one kernel instance writes to the buffer at a time.\n-In Stage 2, the buffers are further reduced to compute the final :math:`\\nabla_{w}` and :math:`\\nabla_{b}`.\n-In the following implementation, Stage 1 is implemented by the function :code:`_layer_norm_bwd_dx_fused` and Stage 2 is implemented by the function :code:`_layer_norm_bwd_dwdb`.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 128-224\n-\n-.. code-block:: default\n-\n-\n-    @triton.jit\n-    def _layer_norm_bwd_dx_fused(\n-        DX,  # pointer to the input gradient\n-        DY,  # pointer to the output gradient\n-        DW,  # pointer to the partial sum of weights gradient\n-        DB,  # pointer to the partial sum of biases gradient\n-        X,   # pointer to the input\n-        W,   # pointer to the weights\n-        B,   # pointer to the biases\n-        Mean,   # pointer to the mean\n-        Rstd,   # pointer to the 1/std\n-        Lock,  # pointer to the lock\n-        stride,  # how much to increase the pointer when moving by 1 row\n-        N,  # number of columns in X\n-        eps,  # epsilon to avoid division by zero\n-        GROUP_SIZE_M: tl.constexpr,\n-        BLOCK_SIZE_N: tl.constexpr\n-    ):\n-        # Map the program id to the elements of X, DX, and DY it should compute.\n-        row = tl.program_id(0)\n-        cols = tl.arange(0, BLOCK_SIZE_N)\n-        mask = cols < N\n-        X += row * stride\n-        DY += row * stride\n-        DX += row * stride\n-        # Offset locks and weights/biases gradient pointer for parallel reduction\n-        lock_id = row % GROUP_SIZE_M\n-        Lock += lock_id\n-        Count = Lock + GROUP_SIZE_M\n-        DW = DW + lock_id * N + cols\n-        DB = DB + lock_id * N + cols\n-        # Load data to SRAM\n-        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n-        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n-        w = tl.load(W + cols, mask=mask).to(tl.float32)\n-        mean = tl.load(Mean + row)\n-        rstd = tl.load(Rstd + row)\n-        # Compute dx\n-        xhat = (x - mean) * rstd\n-        wdy = w * dy\n-        xhat = tl.where(mask, xhat, 0.)\n-        wdy = tl.where(mask, wdy, 0.)\n-        c1 = tl.sum(xhat * wdy, axis=0) / N\n-        c2 = tl.sum(wdy, axis=0) / N\n-        dx = (wdy - (xhat * c1 + c2)) * rstd\n-        # Write dx\n-        tl.store(DX + cols, dx, mask=mask)\n-        # Accumulate partial sums for dw/db\n-        partial_dw = (dy * xhat).to(w.dtype)\n-        partial_db = (dy).to(w.dtype)\n-        while tl.atomic_cas(Lock, 0, 1) == 1:\n-            pass\n-        count = tl.load(Count)\n-        # First store doesn't accumulate\n-        if count == 0:\n-            tl.atomic_xchg(Count, 1)\n-        else:\n-            partial_dw += tl.load(DW, mask=mask)\n-            partial_db += tl.load(DB, mask=mask)\n-        tl.store(DW, partial_dw, mask=mask)\n-        tl.store(DB, partial_db, mask=mask)\n-        # Release the lock\n-        tl.atomic_xchg(Lock, 0)\n-\n-\n-    @triton.jit\n-    def _layer_norm_bwd_dwdb(\n-        DW,  # pointer to the partial sum of weights gradient\n-        DB,  # pointer to the partial sum of biases gradient\n-        FINAL_DW,  # pointer to the weights gradient\n-        FINAL_DB,  # pointer to the biases gradient\n-        M,  # GROUP_SIZE_M\n-        N,  # number of columns\n-        BLOCK_SIZE_M: tl.constexpr,\n-        BLOCK_SIZE_N: tl.constexpr\n-    ):\n-        # Map the program id to the elements of DW and DB it should compute.\n-        pid = tl.program_id(0)\n-        cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n-        dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-        db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-        # Iterate through the rows of DW and DB to sum the partial sums.\n-        for i in range(0, M, BLOCK_SIZE_M):\n-            rows = i + tl.arange(0, BLOCK_SIZE_M)\n-            mask = (rows[:, None] < M) & (cols[None, :] < N)\n-            offs = rows[:, None] * N + cols[None, :]\n-            dw += tl.load(DW + offs, mask=mask, other=0.)\n-            db += tl.load(DB + offs, mask=mask, other=0.)\n-        # Write the final sum to the output.\n-        sum_dw = tl.sum(dw, axis=0)\n-        sum_db = tl.sum(db, axis=0)\n-        tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n-        tl.store(FINAL_DB + cols, sum_db, mask=cols < N)\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 225-231\n-\n-Benchmark\n----------\n-\n-We can now compare the performance of our kernel against that of PyTorch.\n-Here we focus on inputs that have Less than 64KB per feature.\n-Specifically, one can set :code:`'mode': 'backward'` to benchmark the backward pass.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 231-370\n-\n-.. code-block:: default\n-\n-\n-\n-    class LayerNorm(torch.autograd.Function):\n-\n-        @staticmethod\n-        def forward(ctx, x, normalized_shape, weight, bias, eps):\n-            # allocate output\n-            y = torch.empty_like(x)\n-            # reshape input data into 2D tensor\n-            x_arg = x.reshape(-1, x.shape[-1])\n-            M, N = x_arg.shape\n-            mean = torch.empty((M, ), dtype=torch.float32, device='cuda')\n-            rstd = torch.empty((M, ), dtype=torch.float32, device='cuda')\n-            # Less than 64KB per feature: enqueue fused kernel\n-            MAX_FUSED_SIZE = 65536 // x.element_size()\n-            BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n-            if N > BLOCK_SIZE:\n-                raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n-            # heuristics for number of warps\n-            num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\n-            # enqueue kernel\n-            _layer_norm_fwd_fused[(M,)](x_arg, y, weight, bias, mean, rstd,\n-                                        x_arg.stride(0), N, eps,\n-                                        BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)\n-            ctx.save_for_backward(x, weight, bias, mean, rstd)\n-            ctx.BLOCK_SIZE = BLOCK_SIZE\n-            ctx.num_warps = num_warps\n-            ctx.eps = eps\n-            return y\n-\n-        @staticmethod\n-        def backward(ctx, dy):\n-            x, w, b, m, v = ctx.saved_tensors\n-            # heuristics for amount of parallel reduction stream for DW/DB\n-            N = w.shape[0]\n-            GROUP_SIZE_M = 64\n-            if N <= 8192: GROUP_SIZE_M = 96\n-            if N <= 4096: GROUP_SIZE_M = 128\n-            if N <= 1024: GROUP_SIZE_M = 256\n-            # allocate output\n-            locks = torch.zeros(2 * GROUP_SIZE_M, dtype=torch.int32, device='cuda')\n-            _dw = torch.empty((GROUP_SIZE_M, w.shape[0]), dtype=x.dtype, device=w.device)\n-            _db = torch.empty((GROUP_SIZE_M, w.shape[0]), dtype=x.dtype, device=w.device)\n-            dw = torch.empty((w.shape[0],), dtype=w.dtype, device=w.device)\n-            db = torch.empty((w.shape[0],), dtype=w.dtype, device=w.device)\n-            dx = torch.empty_like(dy)\n-            # enqueue kernel using forward pass heuristics\n-            # also compute partial sums for DW and DB\n-            x_arg = x.reshape(-1, x.shape[-1])\n-            M, N = x_arg.shape\n-            _layer_norm_bwd_dx_fused[(M,)](dx, dy, _dw, _db, x, w, b, m, v, locks,\n-                                           x_arg.stride(0), N, ctx.eps,\n-                                           BLOCK_SIZE_N=ctx.BLOCK_SIZE,\n-                                           GROUP_SIZE_M=GROUP_SIZE_M,\n-                                           num_warps=ctx.num_warps)\n-            grid = lambda meta: [triton.cdiv(N, meta['BLOCK_SIZE_N'])]\n-            # accumulate partial sums in separate kernel\n-            _layer_norm_bwd_dwdb[grid](_dw, _db, dw, db, GROUP_SIZE_M, N,\n-                                       BLOCK_SIZE_M=32,\n-                                       BLOCK_SIZE_N=128)\n-            return dx, None, dw, db, None\n-\n-\n-    layer_norm = LayerNorm.apply\n-\n-\n-    def test_layer_norm(M, N, dtype, eps=1e-5, device='cuda'):\n-        # create data\n-        x_shape = (M, N)\n-        w_shape = (x_shape[-1], )\n-        weight = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n-        bias = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n-        x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\n-        dy = .1 * torch.randn_like(x)\n-        x.requires_grad_(True)\n-        # forward pass\n-        y_tri = layer_norm(x, w_shape, weight, bias, eps)\n-        y_ref = torch.nn.functional.layer_norm(x, w_shape, weight, bias, eps).to(dtype)\n-        # backward pass (triton)\n-        y_tri.backward(dy, retain_graph=True)\n-        dx_tri, dw_tri, db_tri = [_.grad.clone() for _ in [x, weight, bias]]\n-        x.grad, weight.grad, bias.grad = None, None, None\n-        # backward pass (torch)\n-        y_ref.backward(dy, retain_graph=True)\n-        dx_ref, dw_ref, db_ref = [_.grad.clone() for _ in [x, weight, bias]]\n-        # compare\n-        assert torch.allclose(y_tri, y_ref, atol=1e-2, rtol=0)\n-        assert torch.allclose(dx_tri, dx_ref, atol=1e-2, rtol=0)\n-        assert torch.allclose(db_tri, db_ref, atol=1e-2, rtol=0)\n-        assert torch.allclose(dw_tri, dw_ref, atol=1e-2, rtol=0)\n-\n-\n-    @triton.testing.perf_report(\n-        triton.testing.Benchmark(\n-            x_names=['N'],\n-            x_vals=[512 * i for i in range(2, 32)],\n-            line_arg='provider',\n-            line_vals=['triton', 'torch'] + (['apex'] if HAS_APEX else []),\n-            line_names=['Triton', 'Torch'] + (['Apex'] if HAS_APEX else []),\n-            styles=[('blue', '-'), ('green', '-'), ('orange', '-')],\n-            ylabel='GB/s',\n-            plot_name='layer-norm-backward',\n-            args={'M': 4096, 'dtype': torch.float16, 'mode': 'backward'}\n-        )\n-    )\n-    def bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='cuda'):\n-        # create data\n-        x_shape = (M, N)\n-        w_shape = (x_shape[-1], )\n-        weight = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n-        bias = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n-        x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\n-        dy = .1 * torch.randn_like(x)\n-        x.requires_grad_(True)\n-        quantiles = [0.5, 0.2, 0.8]\n-        # utility functions\n-        if provider == 'triton':\n-            y_fwd = lambda: layer_norm(x, w_shape, weight, bias, eps)\n-        if provider == 'torch':\n-            y_fwd = lambda: torch.nn.functional.layer_norm(x, w_shape, weight, bias, eps)\n-        if provider == 'apex':\n-            apex_layer_norm = apex.normalization.FusedLayerNorm(w_shape).to(x.device).to(x.dtype)\n-            y_fwd = lambda: apex_layer_norm(x)\n-        # forward pass\n-        if mode == 'forward':\n-            gbps = lambda ms: 2 * x.numel() * x.element_size() / ms * 1e-6\n-            ms, min_ms, max_ms = triton.testing.do_bench(y_fwd, quantiles=quantiles, rep=500)\n-        # backward pass\n-        if mode == 'backward':\n-            gbps = lambda ms: 3 * x.numel() * x.element_size() / ms * 1e-6\n-            y = y_fwd()\n-            ms, min_ms, max_ms = triton.testing.do_bench(lambda: y.backward(dy, retain_graph=True),\n-                                                         quantiles=quantiles, grad_to_none=[x], rep=500)\n-        return gbps(ms), gbps(max_ms), gbps(min_ms)\n-\n-\n-    test_layer_norm(1151, 8192, torch.float16)\n-    bench_layer_norm.run(save_path='.', print_data=True)\n-\n-\n-\n-\n-.. image-sg:: /getting-started/tutorials/images/sphx_glr_05-layer-norm_001.png\n-   :alt: 05 layer norm\n-   :srcset: /getting-started/tutorials/images/sphx_glr_05-layer-norm_001.png\n-   :class: sphx-glr-single-img\n-\n-\n-.. rst-class:: sphx-glr-script-out\n-\n- .. code-block:: none\n-\n-    layer-norm-backward:\n-              N       Triton       Torch\n-    0    1024.0   170.666661  372.363633\n-    1    1536.0   261.446814  438.857146\n-    2    2048.0   346.140834  496.484863\n-    3    2560.0   429.650357  529.655159\n-    4    3072.0   511.999982  538.160602\n-    5    3584.0   597.333312  470.032796\n-    6    4096.0   668.734699  472.615390\n-    7    4608.0   713.496767  480.834772\n-    8    5120.0   768.000019  481.882369\n-    9    5632.0   819.199976  489.739120\n-    10   6144.0   862.315754  494.818794\n-    11   6656.0   907.636357  499.200013\n-    12   7168.0   945.230752  475.226530\n-    13   7680.0   975.238103  479.999983\n-    14   8192.0  1003.102074  487.861027\n-    15   8704.0   673.858058  488.074767\n-    16   9216.0   699.949388  491.520008\n-    17   9728.0   727.327104  497.808111\n-    18  10240.0   751.559663  498.498957\n-    19  10752.0   774.918911  485.052653\n-    20  11264.0   806.973159  488.853509\n-    21  11776.0   819.199982  493.235604\n-    22  12288.0   837.818175  499.005061\n-    23  12800.0   843.956028  499.512174\n-    24  13312.0   861.153634  499.200013\n-    25  13824.0   861.755862  501.930388\n-    26  14336.0   873.258878  492.928354\n-    27  14848.0   877.714272  496.311981\n-    28  15360.0   894.757295  501.551014\n-    29  15872.0   887.944041  501.881412\n-\n-\n-\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 371-375\n-\n-References\n-----------\n-\n-.. [BA2016] Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \"Layer Normalization\", Arxiv 2016\n-\n-\n-.. rst-class:: sphx-glr-timing\n-\n-   **Total running time of the script:** ( 0 minutes  28.955 seconds)\n-\n-\n-.. _sphx_glr_download_getting-started_tutorials_05-layer-norm.py:\n-\n-.. only:: html\n-\n-  .. container:: sphx-glr-footer sphx-glr-footer-example\n-\n-\n-\n-\n-    .. container:: sphx-glr-download sphx-glr-download-python\n-\n-      :download:`Download Python source code: 05-layer-norm.py <05-layer-norm.py>`\n-\n-    .. container:: sphx-glr-download sphx-glr-download-jupyter\n-\n-      :download:`Download Jupyter notebook: 05-layer-norm.ipynb <05-layer-norm.ipynb>`\n-\n-\n-.. only:: html\n-\n- .. rst-class:: sphx-glr-signature\n-\n-    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"}, {"filename": "main/_sources/getting-started/tutorials/06-fused-attention.rst.txt", "status": "removed", "additions": 0, "deletions": 536, "changes": 536, "file_content_changes": "@@ -1,536 +0,0 @@\n-\n-.. DO NOT EDIT.\n-.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.\n-.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:\n-.. \"getting-started/tutorials/06-fused-attention.py\"\n-.. LINE NUMBERS ARE GIVEN BELOW.\n-\n-.. only:: html\n-\n-    .. note::\n-        :class: sphx-glr-download-link-note\n-\n-        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_06-fused-attention.py>`\n-        to download the full example code\n-\n-.. rst-class:: sphx-glr-example-title\n-\n-.. _sphx_glr_getting-started_tutorials_06-fused-attention.py:\n-\n-\n-Fused Attention\n-===============\n-\n-This is a Triton implementation of the Flash Attention v2 algorithm from Tri Dao (https://tridao.me/publications/flash2/flash2.pdf)\n-\n-Extra Credits:\n-- Original flash attention paper (https://arxiv.org/abs/2205.14135)\n-- Rabe and Staats (https://arxiv.org/pdf/2112.05682v2.pdf)\n-- Adam P. Goucher for simplified vector math\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 13-411\n-\n-\n-\n-.. rst-class:: sphx-glr-horizontal\n-\n-\n-    *\n-\n-      .. image-sg:: /getting-started/tutorials/images/sphx_glr_06-fused-attention_001.png\n-         :alt: 06 fused attention\n-         :srcset: /getting-started/tutorials/images/sphx_glr_06-fused-attention_001.png\n-         :class: sphx-glr-multi-img\n-\n-    *\n-\n-      .. image-sg:: /getting-started/tutorials/images/sphx_glr_06-fused-attention_002.png\n-         :alt: 06 fused attention\n-         :srcset: /getting-started/tutorials/images/sphx_glr_06-fused-attention_002.png\n-         :class: sphx-glr-multi-img\n-\n-    *\n-\n-      .. image-sg:: /getting-started/tutorials/images/sphx_glr_06-fused-attention_003.png\n-         :alt: 06 fused attention\n-         :srcset: /getting-started/tutorials/images/sphx_glr_06-fused-attention_003.png\n-         :class: sphx-glr-multi-img\n-\n-    *\n-\n-      .. image-sg:: /getting-started/tutorials/images/sphx_glr_06-fused-attention_004.png\n-         :alt: 06 fused attention\n-         :srcset: /getting-started/tutorials/images/sphx_glr_06-fused-attention_004.png\n-         :class: sphx-glr-multi-img\n-\n-\n-.. rst-class:: sphx-glr-script-out\n-\n- .. code-block:: none\n-\n-    fused-attention-batch4-head48-d64-fwd:\n-         N_CTX      Triton\n-    0   1024.0  158.035451\n-    1   2048.0  165.272870\n-    2   4096.0  171.261609\n-    3   8192.0  174.603507\n-    4  16384.0  175.639330\n-    fused-attention-batch4-head48-d64-fwd:\n-         N_CTX      Triton\n-    0   1024.0  119.745775\n-    1   2048.0  137.831452\n-    2   4096.0  151.969170\n-    3   8192.0  157.987534\n-    4  16384.0  156.743005\n-    fused-attention-batch4-head48-d64-bwd:\n-         N_CTX     Triton\n-    0   1024.0  77.893118\n-    1   2048.0  88.003256\n-    2   4096.0  94.209920\n-    3   8192.0  96.223771\n-    4  16384.0  97.642774\n-    fused-attention-batch4-head48-d64-bwd:\n-         N_CTX     Triton\n-    0   1024.0  54.514088\n-    1   2048.0  68.421831\n-    2   4096.0  77.616682\n-    3   8192.0  82.529504\n-    4  16384.0  85.407396\n-\n-\n-\n-\n-\n-\n-|\n-\n-.. code-block:: default\n-\n-\n-    import pytest\n-    import torch\n-\n-    import triton\n-    import triton.language as tl\n-\n-\n-    @triton.jit\n-    def max_fn(x, y):\n-        return tl.math.max(x, y)\n-\n-\n-    @triton.jit\n-    def _fwd_kernel(\n-        Q, K, V, sm_scale,\n-        L,\n-        Out,\n-        stride_qz, stride_qh, stride_qm, stride_qk,\n-        stride_kz, stride_kh, stride_kn, stride_kk,\n-        stride_vz, stride_vh, stride_vk, stride_vn,\n-        stride_oz, stride_oh, stride_om, stride_on,\n-        Z, H, N_CTX,\n-        BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n-        BLOCK_N: tl.constexpr,\n-        IS_CAUSAL: tl.constexpr,\n-    ):\n-        start_m = tl.program_id(0)\n-        off_hz = tl.program_id(1)\n-        qvk_offset = off_hz * stride_qh\n-        Q_block_ptr = tl.make_block_ptr(\n-            base=Q + qvk_offset,\n-            shape=(N_CTX, BLOCK_DMODEL),\n-            strides=(stride_qm, stride_qk),\n-            offsets=(start_m * BLOCK_M, 0),\n-            block_shape=(BLOCK_M, BLOCK_DMODEL),\n-            order=(1, 0)\n-        )\n-        K_block_ptr = tl.make_block_ptr(\n-            base=K + qvk_offset,\n-            shape=(BLOCK_DMODEL, N_CTX),\n-            strides=(stride_kk, stride_kn),\n-            offsets=(0, 0),\n-            block_shape=(BLOCK_DMODEL, BLOCK_N),\n-            order=(0, 1)\n-        )\n-        V_block_ptr = tl.make_block_ptr(\n-            base=V + qvk_offset,\n-            shape=(N_CTX, BLOCK_DMODEL),\n-            strides=(stride_vk, stride_vn),\n-            offsets=(0, 0),\n-            block_shape=(BLOCK_N, BLOCK_DMODEL),\n-            order=(1, 0)\n-        )\n-        # initialize offsets\n-        offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n-        offs_n = tl.arange(0, BLOCK_N)\n-        # initialize pointer to m and l\n-        m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n-        l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n-        acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n-        # scale sm_scale by log_2(e) and use\n-        # 2^x instead of exp in the loop because CSE and LICM\n-        # don't work as expected with `exp` in the loop\n-        qk_scale = sm_scale * 1.44269504\n-        # load q: it will stay in SRAM throughout\n-        q = tl.load(Q_block_ptr)\n-        q = (q * qk_scale).to(tl.float16)\n-        # loop over k, v and update accumulator\n-        lo = 0\n-        hi = (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX\n-        for start_n in range(lo, hi, BLOCK_N):\n-            # -- load k, v --\n-            k = tl.load(K_block_ptr)\n-            v = tl.load(V_block_ptr)\n-            # -- compute qk ---\n-            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-            if IS_CAUSAL:\n-                qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n-            qk += tl.dot(q, k)\n-            # -- compute scaling constant ---\n-            m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n-            alpha = tl.math.exp2(m_i - m_i_new)\n-            p = tl.math.exp2(qk - m_i_new[:, None])\n-            # -- scale and update acc --\n-            acc_scale = l_i * 0 + alpha  # workaround some compiler bug\n-            acc *= acc_scale[:, None]\n-            acc += tl.dot(p.to(tl.float16), v)\n-            # -- update m_i and l_i --\n-            l_i = l_i * alpha + tl.sum(p, 1)\n-            m_i = m_i_new\n-            # update pointers\n-            K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n-            V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n-        # write back l and m\n-        acc = acc / l_i[:, None]\n-        l_ptrs = L + off_hz * N_CTX + offs_m\n-        tl.store(l_ptrs, m_i + tl.math.log2(l_i))\n-        # write back O\n-        O_block_ptr = tl.make_block_ptr(\n-            base=Out + qvk_offset,\n-            shape=(N_CTX, BLOCK_DMODEL),\n-            strides=(stride_om, stride_on),\n-            offsets=(start_m * BLOCK_M, 0),\n-            block_shape=(BLOCK_M, BLOCK_DMODEL),\n-            order=(1, 0)\n-        )\n-        tl.store(O_block_ptr, acc.to(tl.float16))\n-\n-\n-    @triton.jit\n-    def _bwd_preprocess(\n-        Out, DO,\n-        Delta,\n-        BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\n-    ):\n-        off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n-        off_n = tl.arange(0, D_HEAD)\n-        # load\n-        o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n-        do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n-        # compute\n-        delta = tl.sum(o * do, axis=1)\n-        # write-back\n-        tl.store(Delta + off_m, delta)\n-\n-\n-    @triton.jit\n-    def _bwd_kernel(\n-        Q, K, V, sm_scale, Out, DO,\n-        DQ, DK, DV,\n-        L,\n-        D,\n-        stride_qz, stride_qh, stride_qm, stride_qk,\n-        stride_kz, stride_kh, stride_kn, stride_kk,\n-        stride_vz, stride_vh, stride_vk, stride_vn,\n-        Z, H, N_CTX,\n-        num_block,\n-        BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n-        BLOCK_N: tl.constexpr,\n-        CAUSAL: tl.constexpr,\n-    ):\n-        off_hz = tl.program_id(0)\n-        off_z = off_hz // H\n-        off_h = off_hz % H\n-        qk_scale = sm_scale * 1.44269504\n-        # offset pointers for batch/head\n-        Q += off_z * stride_qz + off_h * stride_qh\n-        K += off_z * stride_qz + off_h * stride_qh\n-        V += off_z * stride_qz + off_h * stride_qh\n-        DO += off_z * stride_qz + off_h * stride_qh\n-        DQ += off_z * stride_qz + off_h * stride_qh\n-        DK += off_z * stride_qz + off_h * stride_qh\n-        DV += off_z * stride_qz + off_h * stride_qh\n-        for start_n in range(0, num_block):\n-            if CAUSAL:\n-                lo = start_n * BLOCK_M\n-            else:\n-                lo = 0\n-            # initialize row/col offsets\n-            offs_qm = lo + tl.arange(0, BLOCK_M)\n-            offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n-            offs_m = tl.arange(0, BLOCK_N)\n-            offs_k = tl.arange(0, BLOCK_DMODEL)\n-            # initialize pointers to value-like data\n-            q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n-            k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n-            v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n-            do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n-            dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n-            # pointer to row-wise quantities in value-like data\n-            D_ptrs = D + off_hz * N_CTX\n-            l_ptrs = L + off_hz * N_CTX\n-            # initialize dv amd dk\n-            dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n-            dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n-            # k and v stay in SRAM throughout\n-            k = tl.load(k_ptrs)\n-            v = tl.load(v_ptrs)\n-            # loop over rows\n-            for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n-                offs_m_curr = start_m + offs_m\n-                # load q, k, v, do on-chip\n-                q = tl.load(q_ptrs)\n-                # recompute p = softmax(qk, dim=-1).T\n-                if CAUSAL:\n-                    qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\"-inf\"))\n-                else:\n-                    qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-                qk += tl.dot(q, tl.trans(k))\n-                qk *= qk_scale\n-                l_i = tl.load(l_ptrs + offs_m_curr)\n-                p = tl.math.exp2(qk - l_i[:, None])\n-                # compute dv\n-                do = tl.load(do_ptrs)\n-                dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\n-                # compute dp = dot(v, do)\n-                Di = tl.load(D_ptrs + offs_m_curr)\n-                dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n-                dp += tl.dot(do, tl.trans(v))\n-                # compute ds = p * (dp - delta[:, None])\n-                ds = p * dp * sm_scale\n-                # compute dk = dot(ds.T, q)\n-                dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)\n-                # compute dq\n-                dq = tl.load(dq_ptrs)\n-                dq += tl.dot(ds.to(Q.dtype.element_ty), k)\n-                tl.store(dq_ptrs, dq)\n-                # increment pointers\n-                dq_ptrs += BLOCK_M * stride_qm\n-                q_ptrs += BLOCK_M * stride_qm\n-                do_ptrs += BLOCK_M * stride_qm\n-            # write-back\n-            dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n-            dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n-            tl.store(dv_ptrs, dv)\n-            tl.store(dk_ptrs, dk)\n-\n-\n-    empty = torch.empty(128, device=\"cuda\")\n-\n-\n-    class _attention(torch.autograd.Function):\n-\n-        @staticmethod\n-        def forward(ctx, q, k, v, causal, sm_scale):\n-            # shape constraints\n-            Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n-            assert Lq == Lk and Lk == Lv\n-            assert Lk in {16, 32, 64, 128}\n-            o = torch.empty_like(q)\n-            BLOCK_M = 128\n-            BLOCK_N = 64\n-            grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n-            L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n-\n-            num_warps = 4 if Lk <= 64 else 8\n-            _fwd_kernel[grid](\n-                q, k, v, sm_scale,\n-                L,\n-                o,\n-                q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n-                k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n-                v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n-                o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n-                q.shape[0], q.shape[1], q.shape[2],\n-                BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=Lk,\n-                IS_CAUSAL=causal,\n-                num_warps=num_warps,\n-                num_stages=4)\n-\n-            ctx.save_for_backward(q, k, v, o, L)\n-            ctx.grid = grid\n-            ctx.sm_scale = sm_scale\n-            ctx.BLOCK_DMODEL = Lk\n-            ctx.causal = causal\n-            return o\n-\n-        @staticmethod\n-        def backward(ctx, do):\n-            BLOCK = 128\n-            q, k, v, o, L = ctx.saved_tensors\n-            do = do.contiguous()\n-            dq = torch.zeros_like(q, dtype=torch.float32)\n-            dk = torch.empty_like(k)\n-            dv = torch.empty_like(v)\n-            delta = torch.empty_like(L)\n-            _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n-                o, do,\n-                delta,\n-                BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n-            )\n-            _bwd_kernel[(ctx.grid[1],)](\n-                q, k, v, ctx.sm_scale,\n-                o, do,\n-                dq, dk, dv,\n-                L, delta,\n-                q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n-                k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n-                v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n-                q.shape[0], q.shape[1], q.shape[2],\n-                ctx.grid[0],\n-                BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n-                BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n-                CAUSAL=ctx.causal,\n-                num_stages=1,\n-            )\n-            return dq, dk, dv, None, None\n-\n-\n-    attention = _attention.apply\n-\n-\n-    @pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(6, 9, 1024, 64)])\n-    @pytest.mark.parametrize('causal', [False, True])\n-    def test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\n-        torch.manual_seed(20)\n-        q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n-        k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n-        v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n-        sm_scale = 0.5\n-        dout = torch.randn_like(q)\n-        # reference implementation\n-        M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n-        p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n-        if causal:\n-            p[:, :, M == 0] = float(\"-inf\")\n-        p = torch.softmax(p.float(), dim=-1).half()\n-        # p = torch.exp(p)\n-        ref_out = torch.matmul(p, v)\n-        ref_out.backward(dout)\n-        ref_dv, v.grad = v.grad.clone(), None\n-        ref_dk, k.grad = k.grad.clone(), None\n-        ref_dq, q.grad = q.grad.clone(), None\n-        # triton implementation\n-        tri_out = attention(q, k, v, causal, sm_scale).half()\n-        tri_out.backward(dout)\n-        tri_dv, v.grad = v.grad.clone(), None\n-        tri_dk, k.grad = k.grad.clone(), None\n-        tri_dq, q.grad = q.grad.clone(), None\n-        # compare\n-        assert torch.allclose(ref_out, tri_out, atol=1e-2, rtol=0)\n-        assert torch.allclose(ref_dv, tri_dv, atol=1e-2, rtol=0)\n-        assert torch.allclose(ref_dk, tri_dk, atol=1e-2, rtol=0)\n-        assert torch.allclose(ref_dq, tri_dq, atol=1e-2, rtol=0)\n-\n-\n-    try:\n-        from flash_attn.flash_attn_interface import \\\n-            flash_attn_qkvpacked_func as flash_attn_func\n-        FLASH_VER = 2\n-    except BaseException:\n-        try:\n-            from flash_attn.flash_attn_interface import flash_attn_func\n-            FLASH_VER = 1\n-        except BaseException:\n-            FLASH_VER = None\n-    HAS_FLASH = FLASH_VER is not None\n-\n-    BATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\n-    # vary seq length for fixed head and batch=4\n-    configs = [triton.testing.Benchmark(\n-        x_names=['N_CTX'],\n-        x_vals=[2**i for i in range(10, 15)],\n-        line_arg='provider',\n-        line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n-        line_names=['Triton'] + ([f'Flash-{FLASH_VER}'] if HAS_FLASH else []),\n-        styles=[('red', '-'), ('blue', '-')],\n-        ylabel='ms',\n-        plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n-        args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode, 'causal': causal}\n-    ) for mode in ['fwd', 'bwd'] for causal in [False, True]]\n-\n-\n-    @triton.testing.perf_report(configs)\n-    def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, causal, mode, provider, dtype=torch.float16, device=\"cuda\"):\n-        assert mode in ['fwd', 'bwd']\n-        warmup = 25\n-        rep = 100\n-        if provider == \"triton\":\n-            q = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n-            k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n-            v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n-            sm_scale = 1.3\n-            fn = lambda: attention(q, k, v, causal, sm_scale)\n-            if mode == 'bwd':\n-                o = fn()\n-                do = torch.randn_like(o)\n-                fn = lambda: o.backward(do, retain_graph=True)\n-            ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n-        if provider == \"flash\":\n-            qkv = torch.randn((BATCH, N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\n-            if FLASH_VER == 1:\n-                lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n-                cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n-                cu_seqlens[1:] = lengths.cumsum(0)\n-                qkv = qkv.reshape(BATCH * N_CTX, 3, H, D_HEAD)\n-                fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=causal)\n-            elif FLASH_VER == 2:\n-                fn = lambda: flash_attn_func(qkv, causal=causal)\n-            else:\n-                raise ValueError(f'unknown {FLASH_VER = }')\n-            if mode == 'bwd':\n-                o = fn()\n-                do = torch.randn_like(o)\n-                fn = lambda: o.backward(do, retain_graph=True)\n-            ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n-        flops_per_matmul = 2. * BATCH * H * N_CTX * N_CTX * D_HEAD\n-        total_flops = 2 * flops_per_matmul\n-        if causal:\n-            total_flops *= 0.5\n-        if mode == 'bwd':\n-            total_flops *= 2.5  # 2.0(bwd) + 0.5(recompute)\n-        return total_flops / ms * 1e-9\n-\n-\n-    # only works on post-Ampere GPUs right now\n-    bench_flash_attention.run(save_path='.', print_data=True)\n-\n-\n-.. rst-class:: sphx-glr-timing\n-\n-   **Total running time of the script:** ( 0 minutes  14.145 seconds)\n-\n-\n-.. _sphx_glr_download_getting-started_tutorials_06-fused-attention.py:\n-\n-.. only:: html\n-\n-  .. container:: sphx-glr-footer sphx-glr-footer-example\n-\n-\n-\n-\n-    .. container:: sphx-glr-download sphx-glr-download-python\n-\n-      :download:`Download Python source code: 06-fused-attention.py <06-fused-attention.py>`\n-\n-    .. container:: sphx-glr-download sphx-glr-download-jupyter\n-\n-      :download:`Download Jupyter notebook: 06-fused-attention.ipynb <06-fused-attention.ipynb>`\n-\n-\n-.. only:: html\n-\n- .. rst-class:: sphx-glr-signature\n-\n-    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"}, {"filename": "main/_sources/getting-started/tutorials/07-math-functions.rst.txt", "status": "removed", "additions": 0, "deletions": 175, "changes": 175, "file_content_changes": "@@ -1,175 +0,0 @@\n-\n-.. DO NOT EDIT.\n-.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.\n-.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:\n-.. \"getting-started/tutorials/07-math-functions.py\"\n-.. LINE NUMBERS ARE GIVEN BELOW.\n-\n-.. only:: html\n-\n-    .. note::\n-        :class: sphx-glr-download-link-note\n-\n-        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_07-math-functions.py>`\n-        to download the full example code\n-\n-.. rst-class:: sphx-glr-example-title\n-\n-.. _sphx_glr_getting-started_tutorials_07-math-functions.py:\n-\n-\n-Libdevice (`tl.math`) function\n-==============================\n-Triton can invoke a custom function from an external library.\n-In this example, we will use the `libdevice` library (a.k.a `math` in triton) to apply `asin` on a tensor.\n-Please refer to https://docs.nvidia.com/cuda/libdevice-users-guide/index.html regarding the semantics of all available libdevice functions.\n-In `triton/language/math.py`, we try to aggregate functions with the same computation but different data types together.\n-For example, both `__nv_asin` and `__nvasinf` calculate the principal value of the arc sine of the input, but `__nv_asin` operates on `double` and `__nv_asinf` operates on `float`.\n-Using triton, you can simply call `tl.math.asin`.\n-Triton automatically selects the correct underlying device function to invoke based on input and output types.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 14-16\n-\n-asin Kernel\n-------------\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 16-38\n-\n-.. code-block:: default\n-\n-\n-    import torch\n-\n-    import triton\n-    import triton.language as tl\n-\n-\n-    @triton.jit\n-    def asin_kernel(\n-            x_ptr,\n-            y_ptr,\n-            n_elements,\n-            BLOCK_SIZE: tl.constexpr,\n-    ):\n-        pid = tl.program_id(axis=0)\n-        block_start = pid * BLOCK_SIZE\n-        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n-        mask = offsets < n_elements\n-        x = tl.load(x_ptr + offsets, mask=mask)\n-        x = tl.math.asin(x)\n-        tl.store(y_ptr + offsets, x, mask=mask)\n-\n-\n-\n-\n-\n-\n-\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 39-42\n-\n-Using the default libdevice library path\n------------------------------------------\n-We can use the default libdevice library path encoded in `triton/language/math.py`\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 42-60\n-\n-.. code-block:: default\n-\n-\n-\n-    torch.manual_seed(0)\n-    size = 98432\n-    x = torch.rand(size, device='cuda')\n-    output_triton = torch.zeros(size, device='cuda')\n-    output_torch = torch.asin(x)\n-    assert x.is_cuda and output_triton.is_cuda\n-    n_elements = output_torch.numel()\n-    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n-    asin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024)\n-    print(output_torch)\n-    print(output_triton)\n-    print(\n-        f'The maximum difference between torch and triton is '\n-        f'{torch.max(torch.abs(output_torch - output_triton))}'\n-    )\n-\n-\n-\n-\n-\n-.. rst-class:: sphx-glr-script-out\n-\n- .. code-block:: none\n-\n-    tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device='cuda:0')\n-    tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device='cuda:0')\n-    The maximum difference between torch and triton is 2.384185791015625e-07\n-\n-\n-\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 61-64\n-\n-Customize the libdevice library path\n--------------------------------------\n-We can also customize the libdevice library path by passing the path to the `libdevice` library to the `asin` kernel.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 64-74\n-\n-.. code-block:: default\n-\n-\n-    output_triton = torch.empty_like(x)\n-    asin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024,\n-                      extern_libs={'libdevice': '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'})\n-    print(output_torch)\n-    print(output_triton)\n-    print(\n-        f'The maximum difference between torch and triton is '\n-        f'{torch.max(torch.abs(output_torch - output_triton))}'\n-    )\n-\n-\n-\n-\n-.. rst-class:: sphx-glr-script-out\n-\n- .. code-block:: none\n-\n-    tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device='cuda:0')\n-    tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device='cuda:0')\n-    The maximum difference between torch and triton is 2.384185791015625e-07\n-\n-\n-\n-\n-\n-.. rst-class:: sphx-glr-timing\n-\n-   **Total running time of the script:** ( 0 minutes  0.215 seconds)\n-\n-\n-.. _sphx_glr_download_getting-started_tutorials_07-math-functions.py:\n-\n-.. only:: html\n-\n-  .. container:: sphx-glr-footer sphx-glr-footer-example\n-\n-\n-\n-\n-    .. container:: sphx-glr-download sphx-glr-download-python\n-\n-      :download:`Download Python source code: 07-math-functions.py <07-math-functions.py>`\n-\n-    .. container:: sphx-glr-download sphx-glr-download-jupyter\n-\n-      :download:`Download Jupyter notebook: 07-math-functions.ipynb <07-math-functions.ipynb>`\n-\n-\n-.. only:: html\n-\n- .. rst-class:: sphx-glr-signature\n-\n-    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"}, {"filename": "main/_sources/getting-started/tutorials/08-experimental-block-pointer.rst.txt", "status": "removed", "additions": 0, "deletions": 326, "changes": 326, "file_content_changes": "@@ -1,326 +0,0 @@\n-\n-.. DO NOT EDIT.\n-.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.\n-.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:\n-.. \"getting-started/tutorials/08-experimental-block-pointer.py\"\n-.. LINE NUMBERS ARE GIVEN BELOW.\n-\n-.. only:: html\n-\n-    .. note::\n-        :class: sphx-glr-download-link-note\n-\n-        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_08-experimental-block-pointer.py>`\n-        to download the full example code\n-\n-.. rst-class:: sphx-glr-example-title\n-\n-.. _sphx_glr_getting-started_tutorials_08-experimental-block-pointer.py:\n-\n-\n-Block Pointer (Experimental)\n-============================\n-This tutorial will guide you through writing a matrix multiplication algorithm that utilizes block pointer semantics.\n-These semantics are more friendly for Triton to optimize and can result in better performance on specific hardware.\n-Note that this feature is still experimental and may change in the future.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 11-29\n-\n-Motivations\n------------\n-In the previous matrix multiplication tutorial, we constructed blocks of values by de-referencing blocks of pointers,\n-i.e., :code:`load(block<pointer_type<element_type>>) -> block<element_type>`, which involved loading blocks of\n-elements from memory. This approach allowed for flexibility in using hardware-managed cache and implementing complex\n-data structures, such as tensors of trees or unstructured look-up tables.\n-\n-However, the drawback of this approach is that it relies heavily on complex optimization passes by the compiler to\n-optimize memory access patterns. This can result in brittle code that may suffer from performance degradation when the\n-optimizer fails to perform adequately. Additionally, as memory controllers specialize to accommodate dense spatial\n-data structures commonly used in machine learning workloads, this problem is likely to worsen.\n-\n-To address this issue, we will use block pointers :code:`pointer_type<block<element_type>>` and load them into\n-:code:`block<element_type>`, in which way gives better friendliness for the compiler to optimize memory access\n-patterns.\n-\n-Let's start with the previous matrix multiplication example and demonstrate how to rewrite it to utilize block pointer\n-semantics.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 31-61\n-\n-Make a Block Pointer\n---------------------\n-A block pointer pointers to a block in a parent tensor and is constructed by :code:`make_block_ptr` function,\n-which takes the following information as arguments:\n-\n-* :code:`base`: the base pointer to the parent tensor;\n-\n-* :code:`shape`: the shape of the parent tensor;\n-\n-* :code:`strides`: the strides of the parent tensor, which means how much to increase the pointer by when moving by 1 element in a specific axis;\n-\n-* :code:`offsets`: the offsets of the block;\n-\n-* :code:`block_shape`: the shape of the block;\n-\n-* :code:`order`: the order of the block, which means how the block is laid out in memory.\n-\n-For example, to a block pointer to a :code:`BLOCK_SIZE_M * BLOCK_SIZE_K` block in a row-major 2D matrix A by\n-offsets :code:`(pid_m * BLOCK_SIZE_M, 0)` and strides :code:`(stride_am, stride_ak)`, we can use the following code\n-(exactly the same as the previous matrix multiplication tutorial):\n-\n-.. code-block:: python\n-\n-    a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\n-                                    offsets=(pid_m * BLOCK_SIZE_M, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K),\n-                                    order=(1, 0))\n-\n-Note that the :code:`order` argument is set to :code:`(1, 0)`, which means the second axis is the inner dimension in\n-terms of storage, and the first axis is the outer dimension. This information may sound redundant, but it is necessary\n-for some hardware backends to optimize for better performance.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 63-78\n-\n-Load/Store a Block Pointer\n---------------------------\n-To load/store a block pointer, we can use :code:`load/store` function, which takes a block pointer as an argument,\n-de-references it, and loads/stores a block. You may mask some values in the block, here we have an extra argument\n-:code:`boundary_check` to specify whether to check the boundary of each axis for the block pointer. With check on,\n-out-of-bound values will be masked according to the :code:`padding_option` argument (load only), which can be\n-:code:`zero` or :code:`nan`. Temporarily, we do not support other values due to some hardware limitations. In this\n-mode of block pointer load/store does not support :code:`mask` or :code:`other` arguments in the legacy mode.\n-\n-So to load the block pointer of A in the previous section, we can simply write\n-:code:`a = tl.load(a_block_ptr, boundary_check=(0, 1))`. Boundary check may cost extra performance, so if you can\n-guarantee that the block pointer is always in-bound in some axis, you can turn off the check by not passing the index\n-into the :code:`boundary_check` argument. For example, if we know that :code:`M` is a multiple of\n-:code:`BLOCK_SIZE_M`, we can replace with :code:`a = tl.load(a_block_ptr, boundary_check=(1, ))`, since axis 0 is\n-always in bound.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 80-88\n-\n-Advance a Block Pointer\n------------------------\n-To advance a block pointer, we can use :code:`advance` function, which takes a block pointer and the increment for\n-each axis as arguments and returns a new block pointer with the same shape and strides as the original one,\n-but with the offsets advanced by the specified amount.\n-\n-For example, to advance the block pointer by :code:`BLOCK_SIZE_K` in the second axis\n-(no need to multiply with strides), we can write :code:`a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))`.\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 90-92\n-\n-Final Result\n-------------\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 92-212\n-\n-.. code-block:: default\n-\n-\n-    import torch\n-\n-    import triton\n-    import triton.language as tl\n-\n-\n-    @triton.autotune(\n-        configs=[\n-            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n-            triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n-        ],\n-        key=['M', 'N', 'K'],\n-    )\n-    @triton.jit\n-    def matmul_kernel_with_block_pointers(\n-            # Pointers to matrices\n-            a_ptr, b_ptr, c_ptr,\n-            # Matrix dimensions\n-            M, N, K,\n-            # The stride variables represent how much to increase the ptr by when moving by 1\n-            # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n-            # by to get the element one row down (A has M rows).\n-            stride_am, stride_ak,\n-            stride_bk, stride_bn,\n-            stride_cm, stride_cn,\n-            # Meta-parameters\n-            BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n-            GROUP_SIZE_M: tl.constexpr\n-    ):\n-        \"\"\"Kernel for computing the matmul C = A x B.\n-        A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n-        \"\"\"\n-        # -----------------------------------------------------------\n-        # Map program ids `pid` to the block of C it should compute.\n-        # This is done in a grouped ordering to promote L2 data reuse.\n-        # See the matrix multiplication tutorial for details.\n-        pid = tl.program_id(axis=0)\n-        num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n-        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n-        num_pid_in_group = GROUP_SIZE_M * num_pid_n\n-        group_id = pid // num_pid_in_group\n-        first_pid_m = group_id * GROUP_SIZE_M\n-        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n-        pid_m = first_pid_m + (pid % group_size_m)\n-        pid_n = (pid % num_pid_in_group) // group_size_m\n-\n-        # ----------------------------------------------------------\n-        # Create block pointers for the first blocks of A and B.\n-        # We will advance this pointer as we move in the K direction and accumulate.\n-        # See above `Make a Block Pointer` section for details.\n-        a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\n-                                        offsets=(pid_m * BLOCK_SIZE_M, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K),\n-                                        order=(1, 0))\n-        b_block_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(stride_bk, stride_bn),\n-                                        offsets=(0, pid_n * BLOCK_SIZE_N), block_shape=(BLOCK_SIZE_K, BLOCK_SIZE_N),\n-                                        order=(1, 0))\n-\n-        # -----------------------------------------------------------\n-        # Iterate to compute a block of the C matrix.\n-        # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block.\n-        # of fp32 values for higher accuracy.\n-        # `accumulator` will be converted back to fp16 after the loop.\n-        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-        for k in range(0, K, BLOCK_SIZE_K):\n-            # Load with boundary checks, no need to calculate the mask manually.\n-            # For better performance, you may remove some axis from the boundary\n-            # check, if you can guarantee that the access is always in-bound in\n-            # that axis.\n-            # See above `Load/Store a Block Pointer` section for details.\n-            a = tl.load(a_block_ptr, boundary_check=(0, 1))\n-            b = tl.load(b_block_ptr, boundary_check=(0, 1))\n-            # We accumulate along the K dimension.\n-            accumulator += tl.dot(a, b)\n-            # Advance the block pointer to the next K block.\n-            # See above `Advance a Block Pointer` section for details.\n-            a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))\n-            b_block_ptr = tl.advance(b_block_ptr, (BLOCK_SIZE_K, 0))\n-        c = accumulator.to(tl.float16)\n-\n-        # ----------------------------------------------------------------\n-        # Write back the block of the output matrix C with boundary checks.\n-        # See above `Load/Store a Block Pointer` section for details.\n-        c_block_ptr = tl.make_block_ptr(base=c_ptr, shape=(M, N), strides=(stride_cm, stride_cn),\n-                                        offsets=(pid_m * BLOCK_SIZE_M, pid_n * BLOCK_SIZE_N),\n-                                        block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_N), order=(1, 0))\n-        tl.store(c_block_ptr, c, boundary_check=(0, 1))\n-\n-\n-    # We can now create a convenience wrapper function that only takes two input tensors,\n-    # and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\n-    def matmul(a, b):\n-        # Check constraints.\n-        assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n-        assert a.is_contiguous(), \"Matrix A must be contiguous\"\n-        assert b.is_contiguous(), \"Matrix B must be contiguous\"\n-        M, K = a.shape\n-        K, N = b.shape\n-        # Allocates output.\n-        c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n-        # 1D launch kernel where each block gets its own program.\n-        grid = lambda META: (\n-            triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n-        )\n-        matmul_kernel_with_block_pointers[grid](\n-            a, b, c,\n-            M, N, K,\n-            a.stride(0), a.stride(1),\n-            b.stride(0), b.stride(1),\n-            c.stride(0), c.stride(1),\n-        )\n-        return c\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 213-217\n-\n-Unit Test\n----------\n-\n-Still we can test our matrix multiplication with block pointers against a native torch implementation (i.e., cuBLAS).\n-\n-.. GENERATED FROM PYTHON SOURCE LINES 217-229\n-\n-.. code-block:: default\n-\n-\n-    torch.manual_seed(0)\n-    a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n-    b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n-    triton_output = matmul(a, b)\n-    torch_output = torch.matmul(a, b)\n-    print(f\"triton_output={triton_output}\")\n-    print(f\"torch_output={torch_output}\")\n-    if torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\n-        print(\"\u2705 Triton and Torch match\")\n-    else:\n-        print(\"\u274c Triton and Torch differ\")\n-\n-\n-\n-\n-.. rst-class:: sphx-glr-script-out\n-\n- .. code-block:: none\n-\n-    triton_output=tensor([[-10.9531,  -4.7109,  15.6953,  ..., -28.4062,   4.3320, -26.4219],\n-            [ 26.8438,  10.0469,  -5.4297,  ..., -11.2969,  -8.5312,  30.7500],\n-            [-13.2578,  15.8516,  18.0781,  ..., -21.7656,  -8.6406,  10.2031],\n-            ...,\n-            [ 40.2812,  18.6094, -25.6094,  ...,  -2.7598,  -3.2441,  41.0000],\n-            [ -6.1211, -16.8281,   4.4844,  ..., -21.0312,  24.7031,  15.0234],\n-            [-17.0938, -19.0000,  -0.3831,  ...,  21.5469, -30.2344, -13.2188]],\n-           device='cuda:0', dtype=torch.float16)\n-    torch_output=tensor([[-10.9531,  -4.7109,  15.6953,  ..., -28.4062,   4.3320, -26.4219],\n-            [ 26.8438,  10.0469,  -5.4297,  ..., -11.2969,  -8.5312,  30.7500],\n-            [-13.2578,  15.8516,  18.0781,  ..., -21.7656,  -8.6406,  10.2031],\n-            ...,\n-            [ 40.2812,  18.6094, -25.6094,  ...,  -2.7598,  -3.2441,  41.0000],\n-            [ -6.1211, -16.8281,   4.4844,  ..., -21.0312,  24.7031,  15.0234],\n-            [-17.0938, -19.0000,  -0.3831,  ...,  21.5469, -30.2344, -13.2188]],\n-           device='cuda:0', dtype=torch.float16)\n-    \u2705 Triton and Torch match\n-\n-\n-\n-\n-\n-.. rst-class:: sphx-glr-timing\n-\n-   **Total running time of the script:** ( 0 minutes  6.345 seconds)\n-\n-\n-.. _sphx_glr_download_getting-started_tutorials_08-experimental-block-pointer.py:\n-\n-.. only:: html\n-\n-  .. container:: sphx-glr-footer sphx-glr-footer-example\n-\n-\n-\n-\n-    .. container:: sphx-glr-download sphx-glr-download-python\n-\n-      :download:`Download Python source code: 08-experimental-block-pointer.py <08-experimental-block-pointer.py>`\n-\n-    .. container:: sphx-glr-download sphx-glr-download-jupyter\n-\n-      :download:`Download Jupyter notebook: 08-experimental-block-pointer.ipynb <08-experimental-block-pointer.ipynb>`\n-\n-\n-.. only:: html\n-\n- .. rst-class:: sphx-glr-signature\n-\n-    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"}, {"filename": "main/_sources/getting-started/tutorials/index.rst.txt", "status": "removed", "additions": 0, "deletions": 193, "changes": 193, "file_content_changes": "@@ -1,193 +0,0 @@\n-:orphan:\n-\n-Tutorials\n-=========\n-\n-Below is a gallery of tutorials for writing various basic operations with Triton. It is recommended that you read through the tutorials in order, starting with the simplest one.\n-\n-To install the dependencies for the tutorials:\n-\n-.. code-block:: bash\n-\n-    cd triton\n-    pip install -e './python[tutorials]'\n-\n-\n-\n-.. raw:: html\n-\n-    <div class=\"sphx-glr-thumbnails\">\n-\n-\n-.. raw:: html\n-\n-    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"In this tutorial, you will write a simple vector addition using Triton.\">\n-\n-.. only:: html\n-\n-  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_01-vector-add_thumb.png\n-    :alt:\n-\n-  :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py`\n-\n-.. raw:: html\n-\n-      <div class=\"sphx-glr-thumbnail-title\">Vector Addition</div>\n-    </div>\n-\n-\n-.. raw:: html\n-\n-    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"In this tutorial, you will write a fused softmax operation that is significantly faster than Py...\">\n-\n-.. only:: html\n-\n-  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_02-fused-softmax_thumb.png\n-    :alt:\n-\n-  :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py`\n-\n-.. raw:: html\n-\n-      <div class=\"sphx-glr-thumbnail-title\">Fused Softmax</div>\n-    </div>\n-\n-\n-.. raw:: html\n-\n-    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"You will specifically learn about:\">\n-\n-.. only:: html\n-\n-  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_03-matrix-multiplication_thumb.png\n-    :alt:\n-\n-  :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py`\n-\n-.. raw:: html\n-\n-      <div class=\"sphx-glr-thumbnail-title\">Matrix Multiplication</div>\n-    </div>\n-\n-\n-.. raw:: html\n-\n-    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"In this tutorial, you will write a memory-efficient implementation of dropout whose state will ...\">\n-\n-.. only:: html\n-\n-  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_04-low-memory-dropout_thumb.png\n-    :alt:\n-\n-  :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py`\n-\n-.. raw:: html\n-\n-      <div class=\"sphx-glr-thumbnail-title\">Low-Memory Dropout</div>\n-    </div>\n-\n-\n-.. raw:: html\n-\n-    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"In doing so, you will learn about:\">\n-\n-.. only:: html\n-\n-  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_05-layer-norm_thumb.png\n-    :alt:\n-\n-  :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py`\n-\n-.. raw:: html\n-\n-      <div class=\"sphx-glr-thumbnail-title\">Layer Normalization</div>\n-    </div>\n-\n-\n-.. raw:: html\n-\n-    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"This is a Triton implementation of the Flash Attention v2 algorithm from Tri Dao (https://trida...\">\n-\n-.. only:: html\n-\n-  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_06-fused-attention_thumb.png\n-    :alt:\n-\n-  :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py`\n-\n-.. raw:: html\n-\n-      <div class=\"sphx-glr-thumbnail-title\">Fused Attention</div>\n-    </div>\n-\n-\n-.. raw:: html\n-\n-    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"Libdevice (`tl.math`) function\">\n-\n-.. only:: html\n-\n-  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_07-math-functions_thumb.png\n-    :alt:\n-\n-  :ref:`sphx_glr_getting-started_tutorials_07-math-functions.py`\n-\n-.. raw:: html\n-\n-      <div class=\"sphx-glr-thumbnail-title\">Libdevice (`tl.math`) function</div>\n-    </div>\n-\n-\n-.. raw:: html\n-\n-    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"Block Pointer (Experimental)\">\n-\n-.. only:: html\n-\n-  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_08-experimental-block-pointer_thumb.png\n-    :alt:\n-\n-  :ref:`sphx_glr_getting-started_tutorials_08-experimental-block-pointer.py`\n-\n-.. raw:: html\n-\n-      <div class=\"sphx-glr-thumbnail-title\">Block Pointer (Experimental)</div>\n-    </div>\n-\n-\n-.. raw:: html\n-\n-    </div>\n-\n-\n-.. toctree::\n-   :hidden:\n-\n-   /getting-started/tutorials/01-vector-add\n-   /getting-started/tutorials/02-fused-softmax\n-   /getting-started/tutorials/03-matrix-multiplication\n-   /getting-started/tutorials/04-low-memory-dropout\n-   /getting-started/tutorials/05-layer-norm\n-   /getting-started/tutorials/06-fused-attention\n-   /getting-started/tutorials/07-math-functions\n-   /getting-started/tutorials/08-experimental-block-pointer\n-\n-\n-.. only:: html\n-\n-  .. container:: sphx-glr-footer sphx-glr-footer-gallery\n-\n-    .. container:: sphx-glr-download sphx-glr-download-python\n-\n-      :download:`Download all examples in Python source code: tutorials_python.zip </getting-started/tutorials/tutorials_python.zip>`\n-\n-    .. container:: sphx-glr-download sphx-glr-download-jupyter\n-\n-      :download:`Download all examples in Jupyter notebooks: tutorials_jupyter.zip </getting-started/tutorials/tutorials_jupyter.zip>`\n-\n-\n-.. only:: html\n-\n- .. rst-class:: sphx-glr-signature\n-\n-    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"}, {"filename": "main/_sources/getting-started/tutorials/sg_execution_times.rst.txt", "status": "removed", "additions": 0, "deletions": 27, "changes": 27, "file_content_changes": "@@ -1,27 +0,0 @@\n-\n-:orphan:\n-\n-.. _sphx_glr_getting-started_tutorials_sg_execution_times:\n-\n-\n-Computation times\n-=================\n-**02:13.436** total execution time for **getting-started_tutorials** files:\n-\n-+-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``)           | 00:40.533 | 0.0 MB |\n-+-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                           | 00:37.028 | 0.0 MB |\n-+-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                                 | 00:28.955 | 0.0 MB |\n-+-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py` (``06-fused-attention.py``)                       | 00:14.145 | 0.0 MB |\n-+-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_08-experimental-block-pointer.py` (``08-experimental-block-pointer.py``) | 00:06.345 | 0.0 MB |\n-+-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                                 | 00:05.608 | 0.0 MB |\n-+-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)                 | 00:00.607 | 0.0 MB |\n-+-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_07-math-functions.py` (``07-math-functions.py``)                         | 00:00.215 | 0.0 MB |\n-+-------------------------------------------------------------------------------------------------------------------+-----------+--------+"}, {"filename": "main/_sources/index.rst.txt", "status": "removed", "additions": 0, "deletions": 56, "changes": 56, "file_content_changes": "@@ -1,56 +0,0 @@\n-Welcome to Triton's documentation!\n-==================================\n-\n-Triton_ is a language and compiler for parallel programming. It aims to provide a Python-based programming environment for productively writing custom DNN compute kernels capable of running at maximal throughput on modern GPU hardware.\n-\n-\n-Getting Started\n----------------\n-\n-- Follow the :doc:`installation instructions <getting-started/installation>` for your platform of choice.\n-- Take a look at the :doc:`tutorials <getting-started/tutorials/index>` to learn how to write your first Triton program.\n-\n-.. toctree::\n-   :maxdepth: 1\n-   :caption: Getting Started\n-   :hidden:\n-\n-   getting-started/installation\n-   getting-started/tutorials/index\n-\n-\n-Python API\n-----------\n-\n-- :doc:`triton <python-api/triton>`\n-- :doc:`triton.language <python-api/triton.language>`\n-- :doc:`triton.testing <python-api/triton.testing>`\n-\n-\n-.. toctree::\n-   :maxdepth: 1\n-   :caption: Python API\n-   :hidden:\n-\n-   python-api/triton\n-   python-api/triton.language\n-   python-api/triton.testing\n-\n-\n-Going Further\n--------------\n-\n-Check out the following documents to learn more about Triton and how it compares against other DSLs for DNNs:\n-\n-- Chapter 1: :doc:`Introduction <programming-guide/chapter-1/introduction>`\n-- Chapter 2: :doc:`Related Work <programming-guide/chapter-2/related-work>`\n-\n-.. toctree::\n-   :maxdepth: 1\n-   :caption: Programming Guide\n-   :hidden:\n-\n-   programming-guide/chapter-1/introduction\n-   programming-guide/chapter-2/related-work\n-\n-.. _Triton: https://github.com/openai/triton"}, {"filename": "main/_sources/programming-guide/chapter-1/introduction.rst.txt", "status": "removed", "additions": 0, "deletions": 71, "changes": 71, "file_content_changes": "@@ -1,71 +0,0 @@\n-============\n-Introduction\n-============\n-\n------------\n-Motivations\n------------\n-\n-Over the past decade, Deep Neural Networks (DNNs) have emerged as an important class of Machine Learning (ML) models, capable of achieving state-of-the-art performance across many domains ranging from natural language processing [SUTSKEVER2014]_ to computer vision [REDMON2016]_ to computational neuroscience [LEE2017]_. The strength of these models lies in their hierarchical structure, composed of a sequence of parametric (e.g., convolutional) and non-parametric (e.g., rectified linearity) *layers*. This pattern, though notoriously computationally expensive, also generates a large amount of highly parallelizable work particularly well suited for multi- and many- core processors.\n-\n-As a consequence, Graphics Processing Units (GPUs) have become a cheap and accessible resource for exploring and/or deploying novel research ideas in the field. This trend has been accelerated by the release of several frameworks for General-Purpose GPU (GPGPU) computing, such as CUDA and OpenCL, which have made the development of high-performance programs easier. Yet, GPUs remain incredibly challenging to optimize for locality and parallelism, especially for computations that cannot be efficiently implemented using a combination of pre-existing optimized primitives. To make matters worse, GPU architectures are also rapidly evolving and specializing, as evidenced by the addition of tensor cores to NVIDIA (and more recently AMD) micro-architectures.\n-\n-This tension between the computational opportunities offered by DNNs and the practical difficulty of GPU programming has created substantial academic and industrial interest for Domain-Specific Languages (DSLs) and compilers. Regrettably, these systems -- whether they be based on polyhedral machinery (e.g., Tiramisu [BAGHDADI2021]_, Tensor Comprehensions [VASILACHE2018]_) or scheduling languages (e.g., Halide [JRK2013]_, TVM [CHEN2018]_) -- remain less flexible and (for the same algorithm) markedly slower than the best handwritten compute kernels available in libraries like `cuBLAS <https://docs.nvidia.com/cuda/cublas/index.html>`_, `cuDNN <https://docs.nvidia.com/deeplearning/cudnn/api/index.html>`_ or `TensorRT <https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html>`_.\n-\n-The main premise of this project is the following: programming paradigms based on blocked algorithms [LAM1991]_ can facilitate the construction of high-performance compute kernels for neural networks. We specifically revisit traditional \"Single Program, Multiple Data\" (SPMD [AUGUIN1983]_) execution models for GPUs, and propose a variant in which programs -- rather than threads -- are blocked. For example, in the case of matrix multiplication, CUDA and Triton differ as follows:\n-\n-.. table::\n-    :widths: 50 50\n-\n-    +-----------------------------------------------------+-----------------------------------------------------+\n-    | CUDA Programming Model                              | Triton Programming Model                            |\n-    |                                                     |                                                     |\n-    | (Scalar Program, Blocked Threads)                   | (Blocked Program, Scalar Threads)                   |\n-    +=====================================================+=====================================================+\n-    |                                                     |                                                     |\n-    |.. code-block:: C                                    |.. code-block:: C                                    |\n-    |                                                     |   :force:                                           |\n-    |                                                     |                                                     |\n-    |   #pragma parallel                                  |   #pragma parallel                                  |\n-    |   for(int m = 0; m < M; m++)                        |   for(int m = 0; m < M; m += MB)                    |\n-    |   #pragma parallel                                  |   #pragma parallel                                  |\n-    |   for(int n = 0; n < N; n++){                       |   for(int n = 0; n < N; n += NB){                   |\n-    |     float acc = 0;                                  |     float acc[MB, NB] = 0;                          |\n-    |     for(int k = 0; k < K; k++)                      |     for(int k = 0; k < K; k += KB)                  |\n-    |       acc += A[m, k] * B[k, n];                     |       acc +=  A[m:m+MB, k:k+KB]                     |\n-    |                                                     |             @ B[k:k+KB, n:n+NB];                    |\n-    |     C[m, n] = acc;                                  |     C[m:m+MB, n:n+NB] = acc;                        |\n-    |   }                                                 |   }                                                 |\n-    |                                                     |                                                     |\n-    +-----------------------------------------------------+-----------------------------------------------------+\n-    | |pic1|                                              | |pic2|                                              |\n-    +-----------------------------------------------------+-----------------------------------------------------+\n-\n-\n-.. |pic1| image:: cuda-parallel-matmul.png\n-\n-.. |pic2| image:: triton-parallel-matmul.png\n-\n-A key benefit of this approach is that it leads to block-structured iteration spaces that offer programmers more flexibility than existing DSLs when implementing sparse operations, all while allowing compilers to aggressively optimize programs for data locality and parallelism.\n-\n-\n-----------\n-Challenges\n-----------\n-\n-The main challenge posed by our proposed paradigm is that of work scheduling, i.e., how the work done by each program instance should be partitioned for efficient execution on modern GPUs. To address this issue, the Triton compiler makes heavy use of *block-level data-flow analysis*, a technique for scheduling iteration blocks statically based on the control- and data-flow structure of the target program. The resulting system actually works surprisingly well: our compiler manages to apply a broad range of interesting optimization automatically (e.g., automatic coalescing, thread swizzling, pre-fetching, automatic vectorization, tensor core-aware instruction selection, shared memory allocation/synchronization, asynchronous copy scheduling). Of course doing all this is not trivial; one of the purposes of this guide is to give you a sense of how it works.\n-\n-\n-----------\n-References\n-----------\n-\n-.. [SUTSKEVER2014] I. Sutskever et al., \"Sequence to Sequence Learning with Neural Networks\", NIPS 2014\n-.. [REDMON2016] J. Redmon et al., \"You Only Look Once: Unified, Real-Time Object Detection\", CVPR 2016\n-.. [LEE2017] K. Lee et al., \"Superhuman Accuracy on the SNEMI3D Connectomics Challenge\", ArXiV 2017\n-.. [BAGHDADI2021] R. Baghdadi et al., \"Tiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code\", CGO 2021\n-.. [VASILACHE2018] N. Vasilache et al., \"Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions\", ArXiV 2018\n-.. [JRK2013] J. Ragan-Kelley et al., \"Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines\", PLDI 2013\n-.. [CHEN2018] T. Chen et al., \"TVM: An Automated End-to-End Optimizing Compiler for Deep Learning\", OSDI 2018\n-.. [LAM1991] M. Lam et al., \"The Cache Performance and Optimizations of Blocked Algorithms\", ASPLOS 1991\n-.. [AUGUIN1983] M. Auguin et al., \"Opsila: an advanced SIMD for numerical analysis and signal processing\", EUROMICRO 1983"}, {"filename": "main/_sources/programming-guide/chapter-2/related-work.rst.txt", "status": "removed", "additions": 0, "deletions": 213, "changes": 213, "file_content_changes": "@@ -1,213 +0,0 @@\n-============\n-Related Work\n-============\n-\n-At first sight, Triton may seem like just yet another DSL for DNNs. The purpose of this section is to contextualize Triton and highlight its differences with the two leading approaches in this domain: polyhedral compilation and scheduling languages.\n-\n-\n-----------------------\n-Polyhedral Compilation\n-----------------------\n-\n-Traditional compilers typically rely on intermediate representations, such as LLVM-IR [LATTNER2004]_, that encode control flow information using (un)conditional branches. This relatively low-level format makes it difficult to statically analyze the runtime behavior (e.g., cache misses) of input programs, and to  automatically optimize loops accordingly through the use of tiling [WOLFE1989]_, fusion [DARTE1999]_ and interchange [ALLEN1984]_. To solve this issue, polyhedral compilers [ANCOURT1991]_ rely on program representations that have statically predictable control flow, thereby enabling aggressive compile-time program transformations for data locality and parallelism. Though this strategy has been adopted by many languages and compilers for DNNs such as Tiramisu [BAGHDADI2021]_, Tensor Comprehensions [VASILACHE2018]_, Diesel [ELANGO2018]_ and the Affine dialect in MLIR [LATTNER2019]_, it also comes with a number of limitations that will be described later in this section.\n-\n-++++++++++++++++++++++\n-Program Representation\n-++++++++++++++++++++++\n-\n-Polyhedral compilation is a vast area of research. In this section we only outline the most basic aspects of this topic, but readers interested in the solid mathematical foundations underneath may refer to the ample literature on linear and integer programming.\n-\n-.. table::\n-    :widths: 50 50\n-\n-    +-----------------------------------------------------+-----------------------------------------------------+\n-    |                                                     |                                                     |\n-    |.. code-block:: C                                    | |pic1|                                              |\n-    |                                                     |                                                     |\n-    |   for(int i = 0; i < 3; i++)                        |                                                     |\n-    |   for(int j = i; j < 5; j++)                        |                                                     |\n-    |     A[i][j] = 0;                                    |                                                     |\n-    +-----------------------------------------------------+-----------------------------------------------------+\n-\n-.. |pic1| image:: polyhedral-iteration.png\n-    :width: 300\n-\n-Polyhedral compilers focus on a class of programs commonly known as **Static Control Parts** (SCoP), *i.e.*, maximal sets of consecutive statements in which conditionals and loop bounds are affine functions of surrounding loop indices and global invariant parameters. As shown above, programs in this format always lead to iteration domains that are bounded by affine inequalities, i.e., polyhedral. These polyhedra can also be defined algebraically; for the above example:\n-\n-.. math::\n-\n-  \\mathcal{P} = \\{ i, j \\in \\mathbb{Z}^2\n-  ~|~\n-  \\begin{pmatrix}\n-  1 & 0 \\\\\n-  -1 & 0 \\\\\n-  -1 & 1 \\\\\n-  0 & -1 \\\\\n-  \\end{pmatrix}\n-  \\begin{pmatrix}\n-  i \\\\\n-  j\n-  \\end{pmatrix}\n-  +\n-  \\begin{pmatrix}\n-  0 \\\\\n-  2 \\\\\n-  0 \\\\\n-  4\n-  \\end{pmatrix}\n-  \\geq\n-  0\n-  \\}\n-\n-\n-Each point :math:`(i, j)` in :math:`\\mathcal{P}` represents a *polyhedral statement*, that is a program statement which (1) does not induce control-flow side effects (e.g., :code:`for`, :code:`if`, :code:`break`) and (2) contains only affine functions of loop indices and global parameters in array accesses. To facilitate alias analysis, array accesses are also mathematically abstracted, using so-called *access function*. In other words, :code:`A[i][j]` is simply :code:`A[f(i,j)]` where the access function :math:`f` is defined by:\n-\n-.. math::\n-\n-  f(i, j) = \\begin{pmatrix}\n-  1 & 0\\\\\n-  0 & 1\\\\\n-  \\end{pmatrix}\n-  \\begin{pmatrix}\n-  i\\\\\n-  j\n-  \\end{pmatrix}\n-  =\n-  (i, j)\n-\n-\n-Note that the iteration domains of an SCoP does not specify the order in which its statements shall execute. In fact, this iteration domain may be traversed in many different possible legal orders, i.e. *schedules*. Formally, a schedule is defined as a p-dimensional affine transformation :math:`\\Theta` of loop indices :math:`\\mathbf{x}` and global invariant parameters :math:`\\mathbf{g}`:\n-\n-.. math::\n-  \\Theta_S(\\mathbf{x}) = T_S \\begin{pmatrix}\n-  \\vec{x}\\\\\n-  \\vec{g}\\\\\n-  1\n-  \\end{pmatrix}\n-  \\qquad\n-  T_S \\in \\mathbb{Z} ^{p \\times (\\text{dim}(\\mathbf{x}) + \\text{dim}(\\mathbf{g}) + 1)}\n-\n-\n-Where :math:`\\Theta_S(\\mathbf{x})` is a p-dimensional vector representing the slowest to fastest growing indices (from left to right) when traversing the loop nest surrounding :math:`S`. For the code shown above, the original schedule defined by the loop nest in C can be retrieved by using:\n-\n-.. math::\n-  \\Theta_S(\\mathbf{x}) = \\begin{pmatrix}\n-  1 & 0 \\\\\n-  0 & 1 \\\\\n-  \\end{pmatrix}\n-  \\begin{pmatrix}\n-  i & j\n-  \\end{pmatrix}^T\n-  =\n-  \\begin{pmatrix}\n-  i & j\n-  \\end{pmatrix}^T\n-\n-\n-where :math:`i` and :math:`j` are respectively the slowest and fastest growing loop indices in the nest. If :math:`T_S` is a vector (resp. tensor), then :math:`\\Theta_S` is a said to be one-dimensional (resp. multi-dimensional).\n-\n-++++++++++\n-Advantages\n-++++++++++\n-\n-Programs amenable to polyhedral compilation can be aggressively transformed and optimized. Most of these transformations actually boil down to the production of  schedules and iteration domains that enable loop transformations promoting parallelism and spatial/temporal data locality (e.g., fusion, interchange, tiling, parallelization).\n-\n-Polyhedral compilers can also automatically go through complex verification processes to ensure that the semantics of their input program is preserved throughout this optimization phase. Note that polyhedral optimizers are not incompatible with more standard optimization techniques. In fact, it is not uncommon for these systems to be implemented as a set of LLVM passes that can be run ahead of more traditional compilation techniques [GROSSER2012]_.\n-\n-All in all, polyhedral machinery is extremely powerful, when applicable. It has been shown to support most common loop transformations, and has indeed achieved performance comparable to state-of-the-art GPU libraries for dense matrix multiplication [ELANGO2018]_. Additionally, it is also fully automatic and doesn't require any hint from programmers apart from source-code in a C-like format.\n-\n-+++++++++++\n-Limitations\n-+++++++++++\n-\n-Unfortunately, polyhedral compilers suffer from two major limitations that have prevented its adoption as a universal method for code generation in neural networks.\n-\n-First, the set of possible program transformations :math:`\\Omega = \\{ \\Theta_S ~|~ S \\in \\text{program} \\}` is large, and grows with the number of statements in the program as well as with the size of their iteration domain. Verifying the legality of each transformation can also require the resolution of complex integer linear programs, making polyhedral compilation very computationally expensive. To make matters worse, hardware properties (e.g., cache size, number of SMs) and contextual characteristics (e.g., input tensor shapes) also have to be taken into account by this framework, leading to expensive auto-tuning procedures [SATO2019]_.\n-\n-Second, the polyhedral framework is not very generally applicable; SCoPs are relatively common [GIRBAL2006]_ but require loop bounds and array subscripts to be affine functions of loop indices, which typically only occurs in regular, dense computations. For this reason, this framework still has to be successfully applied to sparse -- or even structured-sparse -- neural networks, whose importance has been rapidly rising over the past few years.\n-\n-On the other hand, blocked program representations advocated by this dissertation are less restricted in scope and can achieve close to peak performance using standard dataflow analysis.\n-\n-\n---------------------\n-Scheduling Languages\n---------------------\n-\n-Separation of concerns [DIJKSTRA82]_ is a well-known design principle in computer science: programs should be decomposed into modular layers of abstraction that separate the semantics of their algorithms from the details of their implementation. Systems like Halide and TVM push this philosophy one step further, and enforce this separation at the grammatical level through the use of a  **scheduling language**. The benefits of this methodology are particularly visible in the case of matrix multiplication, where, as one can see below, the definition of the algorithm (Line 1-7) is completely disjoint from its implementation (Line 8-16), meaning that both can be maintained, optimized and distributed independently.\n-\n-.. code-block:: python\n-  :linenos:\n-\n-  // algorithm\n-  Var x(\"x\"), y(\"y\");\n-  Func matmul(\"matmul\");\n-  RDom k(0, matrix_size);\n-  RVar ki;\n-  matmul(x, y) = 0.0f;\n-  matmul(x, y) += A(k, y) * B(x, k);\n-  // schedule\n-  Var xi(\"xi\"), xo(\"xo\"), yo(\"yo\"), yi(\"yo\"), yii(\"yii\"), xii(\"xii\");\n-  matmul.vectorize(x, 8);\n-  matmul.update(0)\n-      .split(x, x, xi, block_size).split(xi, xi, xii, 8)\n-      .split(y, y, yi, block_size).split(yi, yi, yii, 4)\n-      .split(k, k, ki, block_size)\n-      .reorder(xii, yii, xi, ki, yi, k, x, y)\n-      .parallel(y).vectorize(xii).unroll(xi).unroll(yii);\n-\n-\n-The resulting code may however not be completely portable, as schedules can sometimes rely on execution models (e.g., SPMD) or hardware intrinsics (e.g., matrix-multiply-accumulate) that are not widely available. This issue can be mitigated by auto-scheduling mechanisms [MULLAPUDI2016]_.\n-\n-++++++++++\n-Advantages\n-++++++++++\n-\n-The main advantage of this approach is that it allows programmers to write an algorithm *only once*, and focus on performance optimization separately. It makes it possible to manually specify optimizations that a polyhedral compiler wouldn't be able to figure out automatically using static data-flow analysis.\n-\n-Scheduling languages are, without a doubt, one of the most popular approaches for neural network code generation. The most popular system for this purpose is probably TVM, which provides good performance across a wide range of platforms as well as built-in automatic scheduling mechanisms.\n-\n-+++++++++++\n-Limitations\n-+++++++++++\n-\n-This ease-of-development comes at a cost. First of all, existing systems that follow this paradigm tend to be noticeably slower than Triton on modern hardware when applicable (e.g., V100/A100 tensor cores w/ equal tile sizes). I do believe that this is not a fundamental issue of scheduling languages -- in the sense that it could probably be solved with more efforts -- but it could mean that these systems are harder to engineer. More importantly, existing scheduling languages generate loops whose bounds and increments cannot depend on surrounding loop indices without at least imposing severe constraints on possible schedules -- if not breaking the system entirely. This is problematic for sparse computations, whose iteration spaces may be irregular.\n-\n-.. table::\n-    :widths: 50 50\n-\n-    +-----------------------------------------------------+-----------------------------------------------------+\n-    |                                                     |                                                     |\n-    |.. code-block:: C                                    | |pic2|                                              |\n-    |                                                     |                                                     |\n-    |   for(int i = 0; i < 4; i++)                        |                                                     |\n-    |   for(int j = 0; j < 4; j++)                        |                                                     |\n-    |     float acc = 0;                                  |                                                     |\n-    |     for(int k = 0; k < K[i]; k++)                   |                                                     |\n-    |       acc += A[i][col[i, k]] * B[k][j]              |                                                     |\n-    |     C[i][j] = acc;                                  |                                                     |\n-    +-----------------------------------------------------+-----------------------------------------------------+\n-\n-.. |pic2| image:: halide-iteration.png\n-    :width: 300\n-\n-On the other hand, the block-based program representation that we advocate for through this work allows for block-structured iteration spaces and allows programmers to manually handle load-balancing as they wish.\n-\n-\n-----------\n-References\n-----------\n-\n-.. [LATTNER2004] C. Lattner et al., \"LLVM: a compilation framework for lifelong program analysis transformation\", CGO 2004\n-.. [WOLFE1989] M. Wolfe, \"More Iteration Space Tiling\", SC 1989\n-.. [DARTE1999] A. Darte, \"On the Complexity of Loop Fusion\", PACT 1999\n-.. [ALLEN1984] J. Allen et al., \"Automatic Loop Interchange\", SIGPLAN Notices 1984\n-.. [ANCOURT1991] C. Ancourt et al., \"Scanning Polyhedra with DO Loops\", PPoPP 1991\n-.. [BAGHDADI2021] R. Baghdadi et al., \"Tiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code\", CGO 2021\n-.. [VASILACHE2018] N. Vasilache et al., \"Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions\", ArXiV 2018\n-.. [ELANGO2018] V. Elango et al. \"Diesel: DSL for Linear Algebra and Neural Net Computations on GPUs\", MAPL 2018\n-.. [LATTNER2019] C. Lattner et al., \"MLIR Primer: A Compiler Infrastructure for the End of Moore\u2019s Law\", Arxiv 2019\n-.. [GROSSER2012] T. Grosser et al., \"Polly - Performing Polyhedral Optimizations on a Low-Level Intermediate Representation\", Parallel Processing Letters 2012\n-.. [SATO2019] Y. Sato et al., \"An Autotuning Framework for Scalable Execution of Tiled Code via Iterative Polyhedral Compilation\", TACO 2019\n-.. [GIRBAL2006] S. Girbal et al., \"Semi-Automatic Composition of Loop Transformations for Deep Parallelism and Memory Hierarchies\", International Journal of Parallel Programming 2006\n-.. [DIJKSTRA82] E. W. Dijkstra et al., \"On the role of scientific thought\", Selected writings on computing: a personal perspective 1982\n-.. [MULLAPUDI2016] R. Mullapudi et al., \"Automatically scheduling halide image processing pipelines\", TOG 2016"}, {"filename": "main/_sources/python-api/generated/triton.Config.rst.txt", "status": "removed", "additions": 0, "deletions": 22, "changes": 22, "file_content_changes": "@@ -1,22 +0,0 @@\n-\ufefftriton.Config\n-=============\n-\n-.. currentmodule:: triton\n-\n-.. autoclass:: Config\n-\n-   \n-   .. automethod:: __init__\n-\n-   \n-   .. rubric:: Methods\n-\n-   .. autosummary::\n-   \n-      ~Config.__init__\n-   \n-   \n-\n-   \n-   \n-   \n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.autotune.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.autotune\n-===============\n-\n-.. currentmodule:: triton\n-\n-.. autofunction:: autotune\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.heuristics.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.heuristics\n-=================\n-\n-.. currentmodule:: triton\n-\n-.. autofunction:: heuristics\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.jit.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.jit\n-==========\n-\n-.. currentmodule:: triton\n-\n-.. autofunction:: jit\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.abs.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.abs\n-===================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: abs\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.arange.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.arange\n-======================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: arange\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.argmax.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.argmax\n-======================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: argmax\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.argmin.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.argmin\n-======================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: argmin\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.associative_scan.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.associative\\_scan\n-=================================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: associative_scan\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.atomic_add.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.atomic\\_add\n-===========================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: atomic_add\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.atomic_cas.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.atomic\\_cas\n-===========================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: atomic_cas\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.atomic_max.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.atomic\\_max\n-===========================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: atomic_max\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.atomic_min.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.atomic\\_min\n-===========================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: atomic_min\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.atomic_xchg.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.atomic\\_xchg\n-============================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: atomic_xchg\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.broadcast.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.broadcast\n-=========================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: broadcast\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.broadcast_to.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.broadcast\\_to\n-=============================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: broadcast_to\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.cat.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.cat\n-===================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: cat\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.cos.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.cos\n-===================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: cos\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.cumprod.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.cumprod\n-=======================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: cumprod\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.cumsum.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.cumsum\n-======================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: cumsum\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.debug_barrier.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.debug\\_barrier\n-==============================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: debug_barrier\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.device_assert.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.device\\_assert\n-==============================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: device_assert\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.device_print.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.device\\_print\n-=============================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: device_print\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.dot.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.dot\n-===================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: dot\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.exp.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.exp\n-===================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: exp\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.expand_dims.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.expand\\_dims\n-============================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: expand_dims\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.fdiv.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.fdiv\n-====================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: fdiv\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.full.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.full\n-====================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: full\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.load.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.load\n-====================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: load\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.log.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.log\n-===================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: log\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.max.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.max\n-===================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: max\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.max_constancy.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.max\\_constancy\n-==============================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: max_constancy\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.max_contiguous.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.max\\_contiguous\n-===============================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: max_contiguous\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.maximum.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.maximum\n-=======================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: maximum\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.min.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.min\n-===================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: min\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.minimum.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.minimum\n-=======================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: minimum\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.multiple_of.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.multiple\\_of\n-============================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: multiple_of\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.num_programs.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.num\\_programs\n-=============================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: num_programs\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.program_id.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.program\\_id\n-===========================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: program_id\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.rand.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.rand\n-====================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: rand\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.randint.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.randint\n-=======================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: randint\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.randint4x.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.randint4x\n-=========================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: randint4x\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.randn.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.randn\n-=====================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: randn\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.ravel.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.ravel\n-=====================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: ravel\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.reduce.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.reduce\n-======================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: reduce\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.reshape.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.reshape\n-=======================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: reshape\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.sigmoid.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.sigmoid\n-=======================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: sigmoid\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.sin.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.sin\n-===================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: sin\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.softmax.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.softmax\n-=======================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: softmax\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.sqrt.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.sqrt\n-====================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: sqrt\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.static_assert.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.static\\_assert\n-==============================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: static_assert\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.static_print.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.static\\_print\n-=============================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: static_print\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.static_range.rst.txt", "status": "removed", "additions": 0, "deletions": 22, "changes": 22, "file_content_changes": "@@ -1,22 +0,0 @@\n-\ufefftriton.language.static\\_range\n-=============================\n-\n-.. currentmodule:: triton.language\n-\n-.. autoclass:: static_range\n-\n-   \n-   .. automethod:: __init__\n-\n-   \n-   .. rubric:: Methods\n-\n-   .. autosummary::\n-   \n-      ~static_range.__init__\n-   \n-   \n-\n-   \n-   \n-   \n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.store.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.store\n-=====================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: store\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.sum.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.sum\n-===================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: sum\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.trans.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.trans\n-=====================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: trans\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.umulhi.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.umulhi\n-======================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: umulhi\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.view.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.view\n-====================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: view\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.where.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.where\n-=====================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: where\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.xor_sum.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.xor\\_sum\n-========================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: xor_sum\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.language.zeros.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.language.zeros\n-=====================\n-\n-.. currentmodule:: triton.language\n-\n-.. autofunction:: zeros\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.testing.Benchmark.rst.txt", "status": "removed", "additions": 0, "deletions": 22, "changes": 22, "file_content_changes": "@@ -1,22 +0,0 @@\n-\ufefftriton.testing.Benchmark\n-========================\n-\n-.. currentmodule:: triton.testing\n-\n-.. autoclass:: Benchmark\n-\n-   \n-   .. automethod:: __init__\n-\n-   \n-   .. rubric:: Methods\n-\n-   .. autosummary::\n-   \n-      ~Benchmark.__init__\n-   \n-   \n-\n-   \n-   \n-   \n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.testing.do_bench.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.testing.do\\_bench\n-========================\n-\n-.. currentmodule:: triton.testing\n-\n-.. autofunction:: do_bench\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/generated/triton.testing.perf_report.rst.txt", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-\ufefftriton.testing.perf\\_report\n-===========================\n-\n-.. currentmodule:: triton.testing\n-\n-.. autofunction:: perf_report\n\\ No newline at end of file"}, {"filename": "main/_sources/python-api/triton.language.rst.txt", "status": "removed", "additions": 0, "deletions": 194, "changes": 194, "file_content_changes": "@@ -1,194 +0,0 @@\n-triton.language\n-===============\n-\n-.. currentmodule:: triton.language\n-\n-\n-Programming Model\n------------------\n-\n-.. autosummary::\n-    :toctree: generated\n-    :nosignatures:\n-\n-    program_id\n-    num_programs\n-\n-\n-Creation Ops\n-------------\n-\n-.. autosummary::\n-    :toctree: generated\n-    :nosignatures:\n-\n-    arange\n-    cat\n-    full\n-    zeros\n-\n-\n-Shape Manipulation Ops\n-----------------------\n-\n-.. autosummary::\n-    :toctree: generated\n-    :nosignatures:\n-\n-    broadcast\n-    broadcast_to\n-    expand_dims\n-    ravel\n-    reshape\n-    trans\n-    view\n-\n-\n-Linear Algebra Ops\n-------------------\n-\n-.. autosummary::\n-    :toctree: generated\n-    :nosignatures:\n-\n-    dot\n-\n-\n-Memory Ops\n-----------\n-\n-.. autosummary::\n-    :toctree: generated\n-    :nosignatures:\n-\n-    load\n-    store\n-    atomic_cas\n-    atomic_xchg\n-\n-\n-Indexing Ops\n-------------\n-\n-.. autosummary::\n-    :toctree: generated\n-    :nosignatures:\n-\n-    where\n-\n-\n-Math Ops\n---------\n-\n-.. autosummary::\n-    :toctree: generated\n-    :nosignatures:\n-\n-    abs\n-    exp\n-    log\n-    fdiv\n-    cos\n-    sin\n-    sqrt\n-    sigmoid\n-    softmax\n-    umulhi\n-\n-\n-Reduction Ops\n--------------\n-\n-.. autosummary::\n-    :toctree: generated\n-    :nosignatures:\n-\n-    argmax\n-    argmin\n-    max\n-    min\n-    reduce\n-    sum\n-    xor_sum\n-\n-Scan Ops\n--------------\n-\n-.. autosummary::\n-    :toctree: generated\n-    :nosignatures:\n-\n-    associative_scan\n-    cumsum\n-    cumprod\n-\n-Atomic Ops\n-----------\n-\n-.. autosummary::\n-    :toctree: generated\n-    :nosignatures:\n-\n-    atomic_cas\n-    atomic_add\n-    atomic_max\n-    atomic_min\n-\n-\n-Comparison ops\n---------------\n-\n-.. autosummary::\n-    :toctree: generated\n-    :nosignatures:\n-\n-    minimum\n-    maximum\n-\n-.. _Random Number Generation:\n-\n-Random Number Generation\n-------------------------\n-\n-.. autosummary::\n-    :toctree: generated\n-    :nosignatures:\n-\n-    randint4x\n-    randint\n-    rand\n-    randn\n-\n-\n-Compiler Hint Ops\n------------------\n-\n-.. autosummary::\n-    :toctree: generated\n-    :nosignatures:\n-\n-    debug_barrier\n-    max_constancy\n-    max_contiguous\n-    multiple_of\n-\n-Debug Ops\n------------------\n-\n-.. autosummary::\n-    :toctree: generated\n-    :nosignatures:\n-\n-    static_print\n-    static_assert\n-    device_print\n-    device_assert\n-\n-Iterators\n------------------\n-\n-.. autosummary::\n-    :toctree: generated\n-    :nosignatures:\n-\n-    static_range"}, {"filename": "main/_sources/python-api/triton.rst.txt", "status": "removed", "additions": 0, "deletions": 13, "changes": 13, "file_content_changes": "@@ -1,13 +0,0 @@\n-triton\n-======\n-\n-.. currentmodule:: triton\n-\n-.. autosummary::\n-    :toctree: generated\n-    :nosignatures:\n-\n-    jit\n-    autotune\n-    heuristics\n-    Config"}, {"filename": "main/_sources/python-api/triton.testing.rst.txt", "status": "removed", "additions": 0, "deletions": 12, "changes": 12, "file_content_changes": "@@ -1,12 +0,0 @@\n-triton.testing\n-==============\n-\n-.. currentmodule:: triton.testing\n-\n-.. autosummary::\n-    :toctree: generated\n-    :nosignatures:\n-\n-    do_bench\n-    Benchmark\n-    perf_report"}, {"filename": "main/_static/basic.css", "status": "removed", "additions": 0, "deletions": 903, "changes": 903, "file_content_changes": "@@ -1,903 +0,0 @@\n-/*\n- * basic.css\n- * ~~~~~~~~~\n- *\n- * Sphinx stylesheet -- basic theme.\n- *\n- * :copyright: Copyright 2007-2023 by the Sphinx team, see AUTHORS.\n- * :license: BSD, see LICENSE for details.\n- *\n- */\n-\n-/* -- main layout ----------------------------------------------------------- */\n-\n-div.clearer {\n-    clear: both;\n-}\n-\n-div.section::after {\n-    display: block;\n-    content: '';\n-    clear: left;\n-}\n-\n-/* -- relbar ---------------------------------------------------------------- */\n-\n-div.related {\n-    width: 100%;\n-    font-size: 90%;\n-}\n-\n-div.related h3 {\n-    display: none;\n-}\n-\n-div.related ul {\n-    margin: 0;\n-    padding: 0 0 0 10px;\n-    list-style: none;\n-}\n-\n-div.related li {\n-    display: inline;\n-}\n-\n-div.related li.right {\n-    float: right;\n-    margin-right: 5px;\n-}\n-\n-/* -- sidebar --------------------------------------------------------------- */\n-\n-div.sphinxsidebarwrapper {\n-    padding: 10px 5px 0 10px;\n-}\n-\n-div.sphinxsidebar {\n-    float: left;\n-    width: 230px;\n-    margin-left: -100%;\n-    font-size: 90%;\n-    word-wrap: break-word;\n-    overflow-wrap : break-word;\n-}\n-\n-div.sphinxsidebar ul {\n-    list-style: none;\n-}\n-\n-div.sphinxsidebar ul ul,\n-div.sphinxsidebar ul.want-points {\n-    margin-left: 20px;\n-    list-style: square;\n-}\n-\n-div.sphinxsidebar ul ul {\n-    margin-top: 0;\n-    margin-bottom: 0;\n-}\n-\n-div.sphinxsidebar form {\n-    margin-top: 10px;\n-}\n-\n-div.sphinxsidebar input {\n-    border: 1px solid #98dbcc;\n-    font-family: sans-serif;\n-    font-size: 1em;\n-}\n-\n-div.sphinxsidebar #searchbox form.search {\n-    overflow: hidden;\n-}\n-\n-div.sphinxsidebar #searchbox input[type=\"text\"] {\n-    float: left;\n-    width: 80%;\n-    padding: 0.25em;\n-    box-sizing: border-box;\n-}\n-\n-div.sphinxsidebar #searchbox input[type=\"submit\"] {\n-    float: left;\n-    width: 20%;\n-    border-left: none;\n-    padding: 0.25em;\n-    box-sizing: border-box;\n-}\n-\n-\n-img {\n-    border: 0;\n-    max-width: 100%;\n-}\n-\n-/* -- search page ----------------------------------------------------------- */\n-\n-ul.search {\n-    margin: 10px 0 0 20px;\n-    padding: 0;\n-}\n-\n-ul.search li {\n-    padding: 5px 0 5px 20px;\n-    background-image: url(file.png);\n-    background-repeat: no-repeat;\n-    background-position: 0 7px;\n-}\n-\n-ul.search li a {\n-    font-weight: bold;\n-}\n-\n-ul.search li p.context {\n-    color: #888;\n-    margin: 2px 0 0 30px;\n-    text-align: left;\n-}\n-\n-ul.keywordmatches li.goodmatch a {\n-    font-weight: bold;\n-}\n-\n-/* -- index page ------------------------------------------------------------ */\n-\n-table.contentstable {\n-    width: 90%;\n-    margin-left: auto;\n-    margin-right: auto;\n-}\n-\n-table.contentstable p.biglink {\n-    line-height: 150%;\n-}\n-\n-a.biglink {\n-    font-size: 1.3em;\n-}\n-\n-span.linkdescr {\n-    font-style: italic;\n-    padding-top: 5px;\n-    font-size: 90%;\n-}\n-\n-/* -- general index --------------------------------------------------------- */\n-\n-table.indextable {\n-    width: 100%;\n-}\n-\n-table.indextable td {\n-    text-align: left;\n-    vertical-align: top;\n-}\n-\n-table.indextable ul {\n-    margin-top: 0;\n-    margin-bottom: 0;\n-    list-style-type: none;\n-}\n-\n-table.indextable > tbody > tr > td > ul {\n-    padding-left: 0em;\n-}\n-\n-table.indextable tr.pcap {\n-    height: 10px;\n-}\n-\n-table.indextable tr.cap {\n-    margin-top: 10px;\n-    background-color: #f2f2f2;\n-}\n-\n-img.toggler {\n-    margin-right: 3px;\n-    margin-top: 3px;\n-    cursor: pointer;\n-}\n-\n-div.modindex-jumpbox {\n-    border-top: 1px solid #ddd;\n-    border-bottom: 1px solid #ddd;\n-    margin: 1em 0 1em 0;\n-    padding: 0.4em;\n-}\n-\n-div.genindex-jumpbox {\n-    border-top: 1px solid #ddd;\n-    border-bottom: 1px solid #ddd;\n-    margin: 1em 0 1em 0;\n-    padding: 0.4em;\n-}\n-\n-/* -- domain module index --------------------------------------------------- */\n-\n-table.modindextable td {\n-    padding: 2px;\n-    border-collapse: collapse;\n-}\n-\n-/* -- general body styles --------------------------------------------------- */\n-\n-div.body {\n-    min-width: 360px;\n-    max-width: 800px;\n-}\n-\n-div.body p, div.body dd, div.body li, div.body blockquote {\n-    -moz-hyphens: auto;\n-    -ms-hyphens: auto;\n-    -webkit-hyphens: auto;\n-    hyphens: auto;\n-}\n-\n-a.headerlink {\n-    visibility: hidden;\n-}\n-\n-h1:hover > a.headerlink,\n-h2:hover > a.headerlink,\n-h3:hover > a.headerlink,\n-h4:hover > a.headerlink,\n-h5:hover > a.headerlink,\n-h6:hover > a.headerlink,\n-dt:hover > a.headerlink,\n-caption:hover > a.headerlink,\n-p.caption:hover > a.headerlink,\n-div.code-block-caption:hover > a.headerlink {\n-    visibility: visible;\n-}\n-\n-div.body p.caption {\n-    text-align: inherit;\n-}\n-\n-div.body td {\n-    text-align: left;\n-}\n-\n-.first {\n-    margin-top: 0 !important;\n-}\n-\n-p.rubric {\n-    margin-top: 30px;\n-    font-weight: bold;\n-}\n-\n-img.align-left, figure.align-left, .figure.align-left, object.align-left {\n-    clear: left;\n-    float: left;\n-    margin-right: 1em;\n-}\n-\n-img.align-right, figure.align-right, .figure.align-right, object.align-right {\n-    clear: right;\n-    float: right;\n-    margin-left: 1em;\n-}\n-\n-img.align-center, figure.align-center, .figure.align-center, object.align-center {\n-  display: block;\n-  margin-left: auto;\n-  margin-right: auto;\n-}\n-\n-img.align-default, figure.align-default, .figure.align-default {\n-  display: block;\n-  margin-left: auto;\n-  margin-right: auto;\n-}\n-\n-.align-left {\n-    text-align: left;\n-}\n-\n-.align-center {\n-    text-align: center;\n-}\n-\n-.align-default {\n-    text-align: center;\n-}\n-\n-.align-right {\n-    text-align: right;\n-}\n-\n-/* -- sidebars -------------------------------------------------------------- */\n-\n-div.sidebar,\n-aside.sidebar {\n-    margin: 0 0 0.5em 1em;\n-    border: 1px solid #ddb;\n-    padding: 7px;\n-    background-color: #ffe;\n-    width: 40%;\n-    float: right;\n-    clear: right;\n-    overflow-x: auto;\n-}\n-\n-p.sidebar-title {\n-    font-weight: bold;\n-}\n-\n-nav.contents,\n-aside.topic,\n-div.admonition, div.topic, blockquote {\n-    clear: left;\n-}\n-\n-/* -- topics ---------------------------------------------------------------- */\n-\n-nav.contents,\n-aside.topic,\n-div.topic {\n-    border: 1px solid #ccc;\n-    padding: 7px;\n-    margin: 10px 0 10px 0;\n-}\n-\n-p.topic-title {\n-    font-size: 1.1em;\n-    font-weight: bold;\n-    margin-top: 10px;\n-}\n-\n-/* -- admonitions ----------------------------------------------------------- */\n-\n-div.admonition {\n-    margin-top: 10px;\n-    margin-bottom: 10px;\n-    padding: 7px;\n-}\n-\n-div.admonition dt {\n-    font-weight: bold;\n-}\n-\n-p.admonition-title {\n-    margin: 0px 10px 5px 0px;\n-    font-weight: bold;\n-}\n-\n-div.body p.centered {\n-    text-align: center;\n-    margin-top: 25px;\n-}\n-\n-/* -- content of sidebars/topics/admonitions -------------------------------- */\n-\n-div.sidebar > :last-child,\n-aside.sidebar > :last-child,\n-nav.contents > :last-child,\n-aside.topic > :last-child,\n-div.topic > :last-child,\n-div.admonition > :last-child {\n-    margin-bottom: 0;\n-}\n-\n-div.sidebar::after,\n-aside.sidebar::after,\n-nav.contents::after,\n-aside.topic::after,\n-div.topic::after,\n-div.admonition::after,\n-blockquote::after {\n-    display: block;\n-    content: '';\n-    clear: both;\n-}\n-\n-/* -- tables ---------------------------------------------------------------- */\n-\n-table.docutils {\n-    margin-top: 10px;\n-    margin-bottom: 10px;\n-    border: 0;\n-    border-collapse: collapse;\n-}\n-\n-table.align-center {\n-    margin-left: auto;\n-    margin-right: auto;\n-}\n-\n-table.align-default {\n-    margin-left: auto;\n-    margin-right: auto;\n-}\n-\n-table caption span.caption-number {\n-    font-style: italic;\n-}\n-\n-table caption span.caption-text {\n-}\n-\n-table.docutils td, table.docutils th {\n-    padding: 1px 8px 1px 5px;\n-    border-top: 0;\n-    border-left: 0;\n-    border-right: 0;\n-    border-bottom: 1px solid #aaa;\n-}\n-\n-th {\n-    text-align: left;\n-    padding-right: 5px;\n-}\n-\n-table.citation {\n-    border-left: solid 1px gray;\n-    margin-left: 1px;\n-}\n-\n-table.citation td {\n-    border-bottom: none;\n-}\n-\n-th > :first-child,\n-td > :first-child {\n-    margin-top: 0px;\n-}\n-\n-th > :last-child,\n-td > :last-child {\n-    margin-bottom: 0px;\n-}\n-\n-/* -- figures --------------------------------------------------------------- */\n-\n-div.figure, figure {\n-    margin: 0.5em;\n-    padding: 0.5em;\n-}\n-\n-div.figure p.caption, figcaption {\n-    padding: 0.3em;\n-}\n-\n-div.figure p.caption span.caption-number,\n-figcaption span.caption-number {\n-    font-style: italic;\n-}\n-\n-div.figure p.caption span.caption-text,\n-figcaption span.caption-text {\n-}\n-\n-/* -- field list styles ----------------------------------------------------- */\n-\n-table.field-list td, table.field-list th {\n-    border: 0 !important;\n-}\n-\n-.field-list ul {\n-    margin: 0;\n-    padding-left: 1em;\n-}\n-\n-.field-list p {\n-    margin: 0;\n-}\n-\n-.field-name {\n-    -moz-hyphens: manual;\n-    -ms-hyphens: manual;\n-    -webkit-hyphens: manual;\n-    hyphens: manual;\n-}\n-\n-/* -- hlist styles ---------------------------------------------------------- */\n-\n-table.hlist {\n-    margin: 1em 0;\n-}\n-\n-table.hlist td {\n-    vertical-align: top;\n-}\n-\n-/* -- object description styles --------------------------------------------- */\n-\n-.sig {\n-\tfont-family: 'Consolas', 'Menlo', 'DejaVu Sans Mono', 'Bitstream Vera Sans Mono', monospace;\n-}\n-\n-.sig-name, code.descname {\n-    background-color: transparent;\n-    font-weight: bold;\n-}\n-\n-.sig-name {\n-\tfont-size: 1.1em;\n-}\n-\n-code.descname {\n-    font-size: 1.2em;\n-}\n-\n-.sig-prename, code.descclassname {\n-    background-color: transparent;\n-}\n-\n-.optional {\n-    font-size: 1.3em;\n-}\n-\n-.sig-paren {\n-    font-size: larger;\n-}\n-\n-.sig-param.n {\n-\tfont-style: italic;\n-}\n-\n-/* C++ specific styling */\n-\n-.sig-inline.c-texpr,\n-.sig-inline.cpp-texpr {\n-\tfont-family: unset;\n-}\n-\n-.sig.c   .k, .sig.c   .kt,\n-.sig.cpp .k, .sig.cpp .kt {\n-\tcolor: #0033B3;\n-}\n-\n-.sig.c   .m,\n-.sig.cpp .m {\n-\tcolor: #1750EB;\n-}\n-\n-.sig.c   .s, .sig.c   .sc,\n-.sig.cpp .s, .sig.cpp .sc {\n-\tcolor: #067D17;\n-}\n-\n-\n-/* -- other body styles ----------------------------------------------------- */\n-\n-ol.arabic {\n-    list-style: decimal;\n-}\n-\n-ol.loweralpha {\n-    list-style: lower-alpha;\n-}\n-\n-ol.upperalpha {\n-    list-style: upper-alpha;\n-}\n-\n-ol.lowerroman {\n-    list-style: lower-roman;\n-}\n-\n-ol.upperroman {\n-    list-style: upper-roman;\n-}\n-\n-:not(li) > ol > li:first-child > :first-child,\n-:not(li) > ul > li:first-child > :first-child {\n-    margin-top: 0px;\n-}\n-\n-:not(li) > ol > li:last-child > :last-child,\n-:not(li) > ul > li:last-child > :last-child {\n-    margin-bottom: 0px;\n-}\n-\n-ol.simple ol p,\n-ol.simple ul p,\n-ul.simple ol p,\n-ul.simple ul p {\n-    margin-top: 0;\n-}\n-\n-ol.simple > li:not(:first-child) > p,\n-ul.simple > li:not(:first-child) > p {\n-    margin-top: 0;\n-}\n-\n-ol.simple p,\n-ul.simple p {\n-    margin-bottom: 0;\n-}\n-\n-aside.footnote > span,\n-div.citation > span {\n-    float: left;\n-}\n-aside.footnote > span:last-of-type,\n-div.citation > span:last-of-type {\n-  padding-right: 0.5em;\n-}\n-aside.footnote > p {\n-  margin-left: 2em;\n-}\n-div.citation > p {\n-  margin-left: 4em;\n-}\n-aside.footnote > p:last-of-type,\n-div.citation > p:last-of-type {\n-    margin-bottom: 0em;\n-}\n-aside.footnote > p:last-of-type:after,\n-div.citation > p:last-of-type:after {\n-    content: \"\";\n-    clear: both;\n-}\n-\n-dl.field-list {\n-    display: grid;\n-    grid-template-columns: fit-content(30%) auto;\n-}\n-\n-dl.field-list > dt {\n-    font-weight: bold;\n-    word-break: break-word;\n-    padding-left: 0.5em;\n-    padding-right: 5px;\n-}\n-\n-dl.field-list > dd {\n-    padding-left: 0.5em;\n-    margin-top: 0em;\n-    margin-left: 0em;\n-    margin-bottom: 0em;\n-}\n-\n-dl {\n-    margin-bottom: 15px;\n-}\n-\n-dd > :first-child {\n-    margin-top: 0px;\n-}\n-\n-dd ul, dd table {\n-    margin-bottom: 10px;\n-}\n-\n-dd {\n-    margin-top: 3px;\n-    margin-bottom: 10px;\n-    margin-left: 30px;\n-}\n-\n-dl > dd:last-child,\n-dl > dd:last-child > :last-child {\n-    margin-bottom: 0;\n-}\n-\n-dt:target, span.highlighted {\n-    background-color: #fbe54e;\n-}\n-\n-rect.highlighted {\n-    fill: #fbe54e;\n-}\n-\n-dl.glossary dt {\n-    font-weight: bold;\n-    font-size: 1.1em;\n-}\n-\n-.versionmodified {\n-    font-style: italic;\n-}\n-\n-.system-message {\n-    background-color: #fda;\n-    padding: 5px;\n-    border: 3px solid red;\n-}\n-\n-.footnote:target  {\n-    background-color: #ffa;\n-}\n-\n-.line-block {\n-    display: block;\n-    margin-top: 1em;\n-    margin-bottom: 1em;\n-}\n-\n-.line-block .line-block {\n-    margin-top: 0;\n-    margin-bottom: 0;\n-    margin-left: 1.5em;\n-}\n-\n-.guilabel, .menuselection {\n-    font-family: sans-serif;\n-}\n-\n-.accelerator {\n-    text-decoration: underline;\n-}\n-\n-.classifier {\n-    font-style: oblique;\n-}\n-\n-.classifier:before {\n-    font-style: normal;\n-    margin: 0 0.5em;\n-    content: \":\";\n-    display: inline-block;\n-}\n-\n-abbr, acronym {\n-    border-bottom: dotted 1px;\n-    cursor: help;\n-}\n-\n-/* -- code displays --------------------------------------------------------- */\n-\n-pre {\n-    overflow: auto;\n-    overflow-y: hidden;  /* fixes display issues on Chrome browsers */\n-}\n-\n-pre, div[class*=\"highlight-\"] {\n-    clear: both;\n-}\n-\n-span.pre {\n-    -moz-hyphens: none;\n-    -ms-hyphens: none;\n-    -webkit-hyphens: none;\n-    hyphens: none;\n-    white-space: nowrap;\n-}\n-\n-div[class*=\"highlight-\"] {\n-    margin: 1em 0;\n-}\n-\n-td.linenos pre {\n-    border: 0;\n-    background-color: transparent;\n-    color: #aaa;\n-}\n-\n-table.highlighttable {\n-    display: block;\n-}\n-\n-table.highlighttable tbody {\n-    display: block;\n-}\n-\n-table.highlighttable tr {\n-    display: flex;\n-}\n-\n-table.highlighttable td {\n-    margin: 0;\n-    padding: 0;\n-}\n-\n-table.highlighttable td.linenos {\n-    padding-right: 0.5em;\n-}\n-\n-table.highlighttable td.code {\n-    flex: 1;\n-    overflow: hidden;\n-}\n-\n-.highlight .hll {\n-    display: block;\n-}\n-\n-div.highlight pre,\n-table.highlighttable pre {\n-    margin: 0;\n-}\n-\n-div.code-block-caption + div {\n-    margin-top: 0;\n-}\n-\n-div.code-block-caption {\n-    margin-top: 1em;\n-    padding: 2px 5px;\n-    font-size: small;\n-}\n-\n-div.code-block-caption code {\n-    background-color: transparent;\n-}\n-\n-table.highlighttable td.linenos,\n-span.linenos,\n-div.highlight span.gp {  /* gp: Generic.Prompt */\n-  user-select: none;\n-  -webkit-user-select: text; /* Safari fallback only */\n-  -webkit-user-select: none; /* Chrome/Safari */\n-  -moz-user-select: none; /* Firefox */\n-  -ms-user-select: none; /* IE10+ */\n-}\n-\n-div.code-block-caption span.caption-number {\n-    padding: 0.1em 0.3em;\n-    font-style: italic;\n-}\n-\n-div.code-block-caption span.caption-text {\n-}\n-\n-div.literal-block-wrapper {\n-    margin: 1em 0;\n-}\n-\n-code.xref, a code {\n-    background-color: transparent;\n-    font-weight: bold;\n-}\n-\n-h1 code, h2 code, h3 code, h4 code, h5 code, h6 code {\n-    background-color: transparent;\n-}\n-\n-.viewcode-link {\n-    float: right;\n-}\n-\n-.viewcode-back {\n-    float: right;\n-    font-family: sans-serif;\n-}\n-\n-div.viewcode-block:target {\n-    margin: -1px -10px;\n-    padding: 0 10px;\n-}\n-\n-/* -- math display ---------------------------------------------------------- */\n-\n-img.math {\n-    vertical-align: middle;\n-}\n-\n-div.body div.math p {\n-    text-align: center;\n-}\n-\n-span.eqno {\n-    float: right;\n-}\n-\n-span.eqno a.headerlink {\n-    position: absolute;\n-    z-index: 1;\n-}\n-\n-div.math:hover a.headerlink {\n-    visibility: visible;\n-}\n-\n-/* -- printout stylesheet --------------------------------------------------- */\n-\n-@media print {\n-    div.document,\n-    div.documentwrapper,\n-    div.bodywrapper {\n-        margin: 0 !important;\n-        width: 100%;\n-    }\n-\n-    div.sphinxsidebar,\n-    div.related,\n-    div.footer,\n-    #top-link {\n-        display: none;\n-    }\n-}\n\\ No newline at end of file"}, {"filename": "main/_static/binder_badge_logo.svg", "status": "removed", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1 +0,0 @@\n-<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"109\" height=\"20\"><linearGradient id=\"b\" x2=\"0\" y2=\"100%\"><stop offset=\"0\" stop-color=\"#bbb\" stop-opacity=\".1\"/><stop offset=\"1\" stop-opacity=\".1\"/></linearGradient><clipPath id=\"a\"><rect width=\"109\" height=\"20\" rx=\"3\" fill=\"#fff\"/></clipPath><g clip-path=\"url(#a)\"><path fill=\"#555\" d=\"M0 0h64v20H0z\"/><path fill=\"#579aca\" d=\"M64 0h45v20H64z\"/><path fill=\"url(#b)\" d=\"M0 0h109v20H0z\"/></g><g fill=\"#fff\" text-anchor=\"middle\" font-family=\"DejaVu Sans,Verdana,Geneva,sans-serif\" font-size=\"110\"><image x=\"5\" y=\"3\" width=\"14\" height=\"14\" xlink:href=\"data:image/png;base64,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\"/> <text x=\"415\" y=\"150\" fill=\"#010101\" fill-opacity=\".3\" transform=\"scale(.1)\" textLength=\"370\">launch</text><text x=\"415\" y=\"140\" transform=\"scale(.1)\" textLength=\"370\">launch</text><text x=\"855\" y=\"150\" fill=\"#010101\" fill-opacity=\".3\" transform=\"scale(.1)\" textLength=\"350\">binder</text><text x=\"855\" y=\"140\" transform=\"scale(.1)\" textLength=\"350\">binder</text></g> </svg>\n\\ No newline at end of file"}, {"filename": "main/_static/broken_example.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_static/css/badge_only.css", "status": "removed", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1 +0,0 @@\n-.clearfix{*zoom:1}.clearfix:after,.clearfix:before{display:table;content:\"\"}.clearfix:after{clear:both}@font-face{font-family:FontAwesome;font-style:normal;font-weight:400;src:url(fonts/fontawesome-webfont.eot?674f50d287a8c48dc19ba404d20fe713?#iefix) format(\"embedded-opentype\"),url(fonts/fontawesome-webfont.woff2?af7ae505a9eed503f8b8e6982036873e) format(\"woff2\"),url(fonts/fontawesome-webfont.woff?fee66e712a8a08eef5805a46892932ad) format(\"woff\"),url(fonts/fontawesome-webfont.ttf?b06871f281fee6b241d60582ae9369b9) format(\"truetype\"),url(fonts/fontawesome-webfont.svg?912ec66d7572ff821749319396470bde#FontAwesome) format(\"svg\")}.fa:before{font-family:FontAwesome;font-style:normal;font-weight:400;line-height:1}.fa:before,a .fa{text-decoration:inherit}.fa:before,a .fa,li .fa{display:inline-block}li .fa-large:before{width:1.875em}ul.fas{list-style-type:none;margin-left:2em;text-indent:-.8em}ul.fas li .fa{width:.8em}ul.fas li .fa-large:before{vertical-align:baseline}.fa-book:before,.icon-book:before{content:\"\\f02d\"}.fa-caret-down:before,.icon-caret-down:before{content:\"\\f0d7\"}.fa-caret-up:before,.icon-caret-up:before{content:\"\\f0d8\"}.fa-caret-left:before,.icon-caret-left:before{content:\"\\f0d9\"}.fa-caret-right:before,.icon-caret-right:before{content:\"\\f0da\"}.rst-versions{position:fixed;bottom:0;left:0;width:300px;color:#fcfcfc;background:#1f1d1d;font-family:Lato,proxima-nova,Helvetica Neue,Arial,sans-serif;z-index:400}.rst-versions a{color:#2980b9;text-decoration:none}.rst-versions .rst-badge-small{display:none}.rst-versions .rst-current-version{padding:12px;background-color:#272525;display:block;text-align:right;font-size:90%;cursor:pointer;color:#27ae60}.rst-versions .rst-current-version:after{clear:both;content:\"\";display:block}.rst-versions .rst-current-version .fa{color:#fcfcfc}.rst-versions .rst-current-version .fa-book,.rst-versions .rst-current-version .icon-book{float:left}.rst-versions .rst-current-version.rst-out-of-date{background-color:#e74c3c;color:#fff}.rst-versions .rst-current-version.rst-active-old-version{background-color:#f1c40f;color:#000}.rst-versions.shift-up{height:auto;max-height:100%;overflow-y:scroll}.rst-versions.shift-up .rst-other-versions{display:block}.rst-versions .rst-other-versions{font-size:90%;padding:12px;color:grey;display:none}.rst-versions .rst-other-versions hr{display:block;height:1px;border:0;margin:20px 0;padding:0;border-top:1px solid #413d3d}.rst-versions .rst-other-versions dd{display:inline-block;margin:0}.rst-versions .rst-other-versions dd a{display:inline-block;padding:6px;color:#fcfcfc}.rst-versions.rst-badge{width:auto;bottom:20px;right:20px;left:auto;border:none;max-width:300px;max-height:90%}.rst-versions.rst-badge .fa-book,.rst-versions.rst-badge .icon-book{float:none;line-height:30px}.rst-versions.rst-badge.shift-up .rst-current-version{text-align:right}.rst-versions.rst-badge.shift-up .rst-current-version .fa-book,.rst-versions.rst-badge.shift-up .rst-current-version .icon-book{float:left}.rst-versions.rst-badge>.rst-current-version{width:auto;height:30px;line-height:30px;padding:0 6px;display:block;text-align:center}@media screen and (max-width:768px){.rst-versions{width:85%;display:none}.rst-versions.shift{display:block}}\n\\ No newline at end of file"}, {"filename": "main/_static/css/fonts/Roboto-Slab-Bold.woff", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_static/css/fonts/Roboto-Slab-Bold.woff2", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_static/css/fonts/Roboto-Slab-Regular.woff", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_static/css/fonts/Roboto-Slab-Regular.woff2", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_static/css/fonts/fontawesome-webfont.eot", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_static/css/fonts/fontawesome-webfont.svg", "status": "removed", "additions": 0, "deletions": 2671, "changes": 2671, "file_content_changes": "N/A"}, {"filename": "main/_static/css/fonts/fontawesome-webfont.ttf", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_static/css/fonts/fontawesome-webfont.woff", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_static/css/fonts/fontawesome-webfont.woff2", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_static/css/fonts/lato-bold-italic.woff", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_static/css/fonts/lato-bold-italic.woff2", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_static/css/fonts/lato-bold.woff", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_static/css/fonts/lato-bold.woff2", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_static/css/fonts/lato-normal-italic.woff", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_static/css/fonts/lato-normal-italic.woff2", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_static/css/fonts/lato-normal.woff", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_static/css/fonts/lato-normal.woff2", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_static/css/theme.css", "status": "removed", "additions": 0, "deletions": 4, "changes": 4, "file_content_changes": "N/A"}, {"filename": "main/_static/doctools.js", "status": "removed", "additions": 0, "deletions": 156, "changes": 156, "file_content_changes": "@@ -1,156 +0,0 @@\n-/*\n- * doctools.js\n- * ~~~~~~~~~~~\n- *\n- * Base JavaScript utilities for all Sphinx HTML documentation.\n- *\n- * :copyright: Copyright 2007-2023 by the Sphinx team, see AUTHORS.\n- * :license: BSD, see LICENSE for details.\n- *\n- */\n-\"use strict\";\n-\n-const BLACKLISTED_KEY_CONTROL_ELEMENTS = new Set([\n-  \"TEXTAREA\",\n-  \"INPUT\",\n-  \"SELECT\",\n-  \"BUTTON\",\n-]);\n-\n-const _ready = (callback) => {\n-  if (document.readyState !== \"loading\") {\n-    callback();\n-  } else {\n-    document.addEventListener(\"DOMContentLoaded\", callback);\n-  }\n-};\n-\n-/**\n- * Small JavaScript module for the documentation.\n- */\n-const Documentation = {\n-  init: () => {\n-    Documentation.initDomainIndexTable();\n-    Documentation.initOnKeyListeners();\n-  },\n-\n-  /**\n-   * i18n support\n-   */\n-  TRANSLATIONS: {},\n-  PLURAL_EXPR: (n) => (n === 1 ? 0 : 1),\n-  LOCALE: \"unknown\",\n-\n-  // gettext and ngettext don't access this so that the functions\n-  // can safely bound to a different name (_ = Documentation.gettext)\n-  gettext: (string) => {\n-    const translated = Documentation.TRANSLATIONS[string];\n-    switch (typeof translated) {\n-      case \"undefined\":\n-        return string; // no translation\n-      case \"string\":\n-        return translated; // translation exists\n-      default:\n-        return translated[0]; // (singular, plural) translation tuple exists\n-    }\n-  },\n-\n-  ngettext: (singular, plural, n) => {\n-    const translated = Documentation.TRANSLATIONS[singular];\n-    if (typeof translated !== \"undefined\")\n-      return translated[Documentation.PLURAL_EXPR(n)];\n-    return n === 1 ? singular : plural;\n-  },\n-\n-  addTranslations: (catalog) => {\n-    Object.assign(Documentation.TRANSLATIONS, catalog.messages);\n-    Documentation.PLURAL_EXPR = new Function(\n-      \"n\",\n-      `return (${catalog.plural_expr})`\n-    );\n-    Documentation.LOCALE = catalog.locale;\n-  },\n-\n-  /**\n-   * helper function to focus on search bar\n-   */\n-  focusSearchBar: () => {\n-    document.querySelectorAll(\"input[name=q]\")[0]?.focus();\n-  },\n-\n-  /**\n-   * Initialise the domain index toggle buttons\n-   */\n-  initDomainIndexTable: () => {\n-    const toggler = (el) => {\n-      const idNumber = el.id.substr(7);\n-      const toggledRows = document.querySelectorAll(`tr.cg-${idNumber}`);\n-      if (el.src.substr(-9) === \"minus.png\") {\n-        el.src = `${el.src.substr(0, el.src.length - 9)}plus.png`;\n-        toggledRows.forEach((el) => (el.style.display = \"none\"));\n-      } else {\n-        el.src = `${el.src.substr(0, el.src.length - 8)}minus.png`;\n-        toggledRows.forEach((el) => (el.style.display = \"\"));\n-      }\n-    };\n-\n-    const togglerElements = document.querySelectorAll(\"img.toggler\");\n-    togglerElements.forEach((el) =>\n-      el.addEventListener(\"click\", (event) => toggler(event.currentTarget))\n-    );\n-    togglerElements.forEach((el) => (el.style.display = \"\"));\n-    if (DOCUMENTATION_OPTIONS.COLLAPSE_INDEX) togglerElements.forEach(toggler);\n-  },\n-\n-  initOnKeyListeners: () => {\n-    // only install a listener if it is really needed\n-    if (\n-      !DOCUMENTATION_OPTIONS.NAVIGATION_WITH_KEYS &&\n-      !DOCUMENTATION_OPTIONS.ENABLE_SEARCH_SHORTCUTS\n-    )\n-      return;\n-\n-    document.addEventListener(\"keydown\", (event) => {\n-      // bail for input elements\n-      if (BLACKLISTED_KEY_CONTROL_ELEMENTS.has(document.activeElement.tagName)) return;\n-      // bail with special keys\n-      if (event.altKey || event.ctrlKey || event.metaKey) return;\n-\n-      if (!event.shiftKey) {\n-        switch (event.key) {\n-          case \"ArrowLeft\":\n-            if (!DOCUMENTATION_OPTIONS.NAVIGATION_WITH_KEYS) break;\n-\n-            const prevLink = document.querySelector('link[rel=\"prev\"]');\n-            if (prevLink && prevLink.href) {\n-              window.location.href = prevLink.href;\n-              event.preventDefault();\n-            }\n-            break;\n-          case \"ArrowRight\":\n-            if (!DOCUMENTATION_OPTIONS.NAVIGATION_WITH_KEYS) break;\n-\n-            const nextLink = document.querySelector('link[rel=\"next\"]');\n-            if (nextLink && nextLink.href) {\n-              window.location.href = nextLink.href;\n-              event.preventDefault();\n-            }\n-            break;\n-        }\n-      }\n-\n-      // some keyboard layouts may need Shift to get /\n-      switch (event.key) {\n-        case \"/\":\n-          if (!DOCUMENTATION_OPTIONS.ENABLE_SEARCH_SHORTCUTS) break;\n-          Documentation.focusSearchBar();\n-          event.preventDefault();\n-      }\n-    });\n-  },\n-};\n-\n-// quick alias for translations\n-const _ = Documentation.gettext;\n-\n-_ready(Documentation.init);"}, {"filename": "main/_static/documentation_options.js", "status": "removed", "additions": 0, "deletions": 14, "changes": 14, "file_content_changes": "@@ -1,14 +0,0 @@\n-var DOCUMENTATION_OPTIONS = {\n-    URL_ROOT: document.getElementById(\"documentation_options\").getAttribute('data-url_root'),\n-    VERSION: '',\n-    LANGUAGE: 'en',\n-    COLLAPSE_INDEX: false,\n-    BUILDER: 'html',\n-    FILE_SUFFIX: '.html',\n-    LINK_SUFFIX: '.html',\n-    HAS_SOURCE: true,\n-    SOURCELINK_SUFFIX: '.txt',\n-    NAVIGATION_WITH_KEYS: false,\n-    SHOW_SEARCH_SUMMARY: true,\n-    ENABLE_SEARCH_SHORTCUTS: true,\n-};\n\\ No newline at end of file"}, {"filename": "main/_static/file.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_static/js/badge_only.js", "status": "removed", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1 +0,0 @@\n-!function(e){var t={};function r(n){if(t[n])return t[n].exports;var o=t[n]={i:n,l:!1,exports:{}};return e[n].call(o.exports,o,o.exports,r),o.l=!0,o.exports}r.m=e,r.c=t,r.d=function(e,t,n){r.o(e,t)||Object.defineProperty(e,t,{enumerable:!0,get:n})},r.r=function(e){\"undefined\"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:\"Module\"}),Object.defineProperty(e,\"__esModule\",{value:!0})},r.t=function(e,t){if(1&t&&(e=r(e)),8&t)return e;if(4&t&&\"object\"==typeof e&&e&&e.__esModule)return e;var n=Object.create(null);if(r.r(n),Object.defineProperty(n,\"default\",{enumerable:!0,value:e}),2&t&&\"string\"!=typeof e)for(var o in e)r.d(n,o,function(t){return e[t]}.bind(null,o));return n},r.n=function(e){var t=e&&e.__esModule?function(){return e.default}:function(){return e};return r.d(t,\"a\",t),t},r.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)},r.p=\"\",r(r.s=4)}({4:function(e,t,r){}});\n\\ No newline at end of file"}, {"filename": "main/_static/js/html5shiv-printshiv.min.js", "status": "removed", "additions": 0, "deletions": 4, "changes": 4, "file_content_changes": "@@ -1,4 +0,0 @@\n-/**\n-* @preserve HTML5 Shiv 3.7.3-pre | @afarkas @jdalton @jon_neal @rem | MIT/GPL2 Licensed\n-*/\n-!function(a,b){function c(a,b){var c=a.createElement(\"p\"),d=a.getElementsByTagName(\"head\")[0]||a.documentElement;return c.innerHTML=\"x<style>\"+b+\"</style>\",d.insertBefore(c.lastChild,d.firstChild)}function d(){var a=y.elements;return\"string\"==typeof a?a.split(\" \"):a}function e(a,b){var c=y.elements;\"string\"!=typeof c&&(c=c.join(\" \")),\"string\"!=typeof a&&(a=a.join(\" \")),y.elements=c+\" \"+a,j(b)}function f(a){var b=x[a[v]];return b||(b={},w++,a[v]=w,x[w]=b),b}function g(a,c,d){if(c||(c=b),q)return c.createElement(a);d||(d=f(c));var e;return e=d.cache[a]?d.cache[a].cloneNode():u.test(a)?(d.cache[a]=d.createElem(a)).cloneNode():d.createElem(a),!e.canHaveChildren||t.test(a)||e.tagUrn?e:d.frag.appendChild(e)}function h(a,c){if(a||(a=b),q)return a.createDocumentFragment();c=c||f(a);for(var e=c.frag.cloneNode(),g=0,h=d(),i=h.length;i>g;g++)e.createElement(h[g]);return e}function i(a,b){b.cache||(b.cache={},b.createElem=a.createElement,b.createFrag=a.createDocumentFragment,b.frag=b.createFrag()),a.createElement=function(c){return y.shivMethods?g(c,a,b):b.createElem(c)},a.createDocumentFragment=Function(\"h,f\",\"return function(){var n=f.cloneNode(),c=n.createElement;h.shivMethods&&(\"+d().join().replace(/[\\w\\-:]+/g,function(a){return b.createElem(a),b.frag.createElement(a),'c(\"'+a+'\")'})+\");return n}\")(y,b.frag)}function j(a){a||(a=b);var d=f(a);return!y.shivCSS||p||d.hasCSS||(d.hasCSS=!!c(a,\"article,aside,dialog,figcaption,figure,footer,header,hgroup,main,nav,section{display:block}mark{background:#FF0;color:#000}template{display:none}\")),q||i(a,d),a}function k(a){for(var b,c=a.getElementsByTagName(\"*\"),e=c.length,f=RegExp(\"^(?:\"+d().join(\"|\")+\")$\",\"i\"),g=[];e--;)b=c[e],f.test(b.nodeName)&&g.push(b.applyElement(l(b)));return g}function l(a){for(var b,c=a.attributes,d=c.length,e=a.ownerDocument.createElement(A+\":\"+a.nodeName);d--;)b=c[d],b.specified&&e.setAttribute(b.nodeName,b.nodeValue);return e.style.cssText=a.style.cssText,e}function m(a){for(var b,c=a.split(\"{\"),e=c.length,f=RegExp(\"(^|[\\\\s,>+~])(\"+d().join(\"|\")+\")(?=[[\\\\s,>+~#.:]|$)\",\"gi\"),g=\"$1\"+A+\"\\\\:$2\";e--;)b=c[e]=c[e].split(\"}\"),b[b.length-1]=b[b.length-1].replace(f,g),c[e]=b.join(\"}\");return c.join(\"{\")}function n(a){for(var b=a.length;b--;)a[b].removeNode()}function o(a){function b(){clearTimeout(g._removeSheetTimer),d&&d.removeNode(!0),d=null}var d,e,g=f(a),h=a.namespaces,i=a.parentWindow;return!B||a.printShived?a:(\"undefined\"==typeof h[A]&&h.add(A),i.attachEvent(\"onbeforeprint\",function(){b();for(var f,g,h,i=a.styleSheets,j=[],l=i.length,n=Array(l);l--;)n[l]=i[l];for(;h=n.pop();)if(!h.disabled&&z.test(h.media)){try{f=h.imports,g=f.length}catch(o){g=0}for(l=0;g>l;l++)n.push(f[l]);try{j.push(h.cssText)}catch(o){}}j=m(j.reverse().join(\"\")),e=k(a),d=c(a,j)}),i.attachEvent(\"onafterprint\",function(){n(e),clearTimeout(g._removeSheetTimer),g._removeSheetTimer=setTimeout(b,500)}),a.printShived=!0,a)}var p,q,r=\"3.7.3\",s=a.html5||{},t=/^<|^(?:button|map|select|textarea|object|iframe|option|optgroup)$/i,u=/^(?:a|b|code|div|fieldset|h1|h2|h3|h4|h5|h6|i|label|li|ol|p|q|span|strong|style|table|tbody|td|th|tr|ul)$/i,v=\"_html5shiv\",w=0,x={};!function(){try{var a=b.createElement(\"a\");a.innerHTML=\"<xyz></xyz>\",p=\"hidden\"in a,q=1==a.childNodes.length||function(){b.createElement(\"a\");var a=b.createDocumentFragment();return\"undefined\"==typeof a.cloneNode||\"undefined\"==typeof a.createDocumentFragment||\"undefined\"==typeof a.createElement}()}catch(c){p=!0,q=!0}}();var y={elements:s.elements||\"abbr article aside audio bdi canvas data datalist details dialog figcaption figure footer header hgroup main mark meter nav output picture progress section summary template time video\",version:r,shivCSS:s.shivCSS!==!1,supportsUnknownElements:q,shivMethods:s.shivMethods!==!1,type:\"default\",shivDocument:j,createElement:g,createDocumentFragment:h,addElements:e};a.html5=y,j(b);var z=/^$|\\b(?:all|print)\\b/,A=\"html5shiv\",B=!q&&function(){var c=b.documentElement;return!(\"undefined\"==typeof b.namespaces||\"undefined\"==typeof b.parentWindow||\"undefined\"==typeof c.applyElement||\"undefined\"==typeof c.removeNode||\"undefined\"==typeof a.attachEvent)}();y.type+=\" print\",y.shivPrint=o,o(b),\"object\"==typeof module&&module.exports&&(module.exports=y)}(\"undefined\"!=typeof window?window:this,document);\n\\ No newline at end of file"}, {"filename": "main/_static/js/html5shiv.min.js", "status": "removed", "additions": 0, "deletions": 4, "changes": 4, "file_content_changes": "@@ -1,4 +0,0 @@\n-/**\n-* @preserve HTML5 Shiv 3.7.3 | @afarkas @jdalton @jon_neal @rem | MIT/GPL2 Licensed\n-*/\n-!function(a,b){function c(a,b){var c=a.createElement(\"p\"),d=a.getElementsByTagName(\"head\")[0]||a.documentElement;return c.innerHTML=\"x<style>\"+b+\"</style>\",d.insertBefore(c.lastChild,d.firstChild)}function d(){var a=t.elements;return\"string\"==typeof a?a.split(\" \"):a}function e(a,b){var c=t.elements;\"string\"!=typeof c&&(c=c.join(\" \")),\"string\"!=typeof a&&(a=a.join(\" \")),t.elements=c+\" \"+a,j(b)}function f(a){var b=s[a[q]];return b||(b={},r++,a[q]=r,s[r]=b),b}function g(a,c,d){if(c||(c=b),l)return c.createElement(a);d||(d=f(c));var e;return e=d.cache[a]?d.cache[a].cloneNode():p.test(a)?(d.cache[a]=d.createElem(a)).cloneNode():d.createElem(a),!e.canHaveChildren||o.test(a)||e.tagUrn?e:d.frag.appendChild(e)}function h(a,c){if(a||(a=b),l)return a.createDocumentFragment();c=c||f(a);for(var e=c.frag.cloneNode(),g=0,h=d(),i=h.length;i>g;g++)e.createElement(h[g]);return e}function i(a,b){b.cache||(b.cache={},b.createElem=a.createElement,b.createFrag=a.createDocumentFragment,b.frag=b.createFrag()),a.createElement=function(c){return t.shivMethods?g(c,a,b):b.createElem(c)},a.createDocumentFragment=Function(\"h,f\",\"return function(){var n=f.cloneNode(),c=n.createElement;h.shivMethods&&(\"+d().join().replace(/[\\w\\-:]+/g,function(a){return b.createElem(a),b.frag.createElement(a),'c(\"'+a+'\")'})+\");return n}\")(t,b.frag)}function j(a){a||(a=b);var d=f(a);return!t.shivCSS||k||d.hasCSS||(d.hasCSS=!!c(a,\"article,aside,dialog,figcaption,figure,footer,header,hgroup,main,nav,section{display:block}mark{background:#FF0;color:#000}template{display:none}\")),l||i(a,d),a}var k,l,m=\"3.7.3-pre\",n=a.html5||{},o=/^<|^(?:button|map|select|textarea|object|iframe|option|optgroup)$/i,p=/^(?:a|b|code|div|fieldset|h1|h2|h3|h4|h5|h6|i|label|li|ol|p|q|span|strong|style|table|tbody|td|th|tr|ul)$/i,q=\"_html5shiv\",r=0,s={};!function(){try{var a=b.createElement(\"a\");a.innerHTML=\"<xyz></xyz>\",k=\"hidden\"in a,l=1==a.childNodes.length||function(){b.createElement(\"a\");var a=b.createDocumentFragment();return\"undefined\"==typeof a.cloneNode||\"undefined\"==typeof a.createDocumentFragment||\"undefined\"==typeof a.createElement}()}catch(c){k=!0,l=!0}}();var t={elements:n.elements||\"abbr article aside audio bdi canvas data datalist details dialog figcaption figure footer header hgroup main mark meter nav output picture progress section summary template time video\",version:m,shivCSS:n.shivCSS!==!1,supportsUnknownElements:l,shivMethods:n.shivMethods!==!1,type:\"default\",shivDocument:j,createElement:g,createDocumentFragment:h,addElements:e};a.html5=t,j(b),\"object\"==typeof module&&module.exports&&(module.exports=t)}(\"undefined\"!=typeof window?window:this,document);\n\\ No newline at end of file"}, {"filename": "main/_static/js/theme.js", "status": "removed", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1 +0,0 @@\n-!function(n){var e={};function t(i){if(e[i])return e[i].exports;var o=e[i]={i:i,l:!1,exports:{}};return n[i].call(o.exports,o,o.exports,t),o.l=!0,o.exports}t.m=n,t.c=e,t.d=function(n,e,i){t.o(n,e)||Object.defineProperty(n,e,{enumerable:!0,get:i})},t.r=function(n){\"undefined\"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(n,Symbol.toStringTag,{value:\"Module\"}),Object.defineProperty(n,\"__esModule\",{value:!0})},t.t=function(n,e){if(1&e&&(n=t(n)),8&e)return n;if(4&e&&\"object\"==typeof n&&n&&n.__esModule)return n;var i=Object.create(null);if(t.r(i),Object.defineProperty(i,\"default\",{enumerable:!0,value:n}),2&e&&\"string\"!=typeof n)for(var o in n)t.d(i,o,function(e){return n[e]}.bind(null,o));return i},t.n=function(n){var e=n&&n.__esModule?function(){return n.default}:function(){return n};return t.d(e,\"a\",e),e},t.o=function(n,e){return Object.prototype.hasOwnProperty.call(n,e)},t.p=\"\",t(t.s=0)}([function(n,e,t){t(1),n.exports=t(3)},function(n,e,t){(function(){var e=\"undefined\"!=typeof window?window.jQuery:t(2);n.exports.ThemeNav={navBar:null,win:null,winScroll:!1,winResize:!1,linkScroll:!1,winPosition:0,winHeight:null,docHeight:null,isRunning:!1,enable:function(n){var t=this;void 0===n&&(n=!0),t.isRunning||(t.isRunning=!0,e((function(e){t.init(e),t.reset(),t.win.on(\"hashchange\",t.reset),n&&t.win.on(\"scroll\",(function(){t.linkScroll||t.winScroll||(t.winScroll=!0,requestAnimationFrame((function(){t.onScroll()})))})),t.win.on(\"resize\",(function(){t.winResize||(t.winResize=!0,requestAnimationFrame((function(){t.onResize()})))})),t.onResize()})))},enableSticky:function(){this.enable(!0)},init:function(n){n(document);var e=this;this.navBar=n(\"div.wy-side-scroll:first\"),this.win=n(window),n(document).on(\"click\",\"[data-toggle='wy-nav-top']\",(function(){n(\"[data-toggle='wy-nav-shift']\").toggleClass(\"shift\"),n(\"[data-toggle='rst-versions']\").toggleClass(\"shift\")})).on(\"click\",\".wy-menu-vertical .current ul li a\",(function(){var t=n(this);n(\"[data-toggle='wy-nav-shift']\").removeClass(\"shift\"),n(\"[data-toggle='rst-versions']\").toggleClass(\"shift\"),e.toggleCurrent(t),e.hashChange()})).on(\"click\",\"[data-toggle='rst-current-version']\",(function(){n(\"[data-toggle='rst-versions']\").toggleClass(\"shift-up\")})),n(\"table.docutils:not(.field-list,.footnote,.citation)\").wrap(\"<div class='wy-table-responsive'></div>\"),n(\"table.docutils.footnote\").wrap(\"<div class='wy-table-responsive footnote'></div>\"),n(\"table.docutils.citation\").wrap(\"<div class='wy-table-responsive citation'></div>\"),n(\".wy-menu-vertical ul\").not(\".simple\").siblings(\"a\").each((function(){var t=n(this);expand=n('<button class=\"toctree-expand\" title=\"Open/close menu\"></button>'),expand.on(\"click\",(function(n){return e.toggleCurrent(t),n.stopPropagation(),!1})),t.prepend(expand)}))},reset:function(){var n=encodeURI(window.location.hash)||\"#\";try{var e=$(\".wy-menu-vertical\"),t=e.find('[href=\"'+n+'\"]');if(0===t.length){var i=$('.document [id=\"'+n.substring(1)+'\"]').closest(\"div.section\");0===(t=e.find('[href=\"#'+i.attr(\"id\")+'\"]')).length&&(t=e.find('[href=\"#\"]'))}if(t.length>0){$(\".wy-menu-vertical .current\").removeClass(\"current\").attr(\"aria-expanded\",\"false\"),t.addClass(\"current\").attr(\"aria-expanded\",\"true\"),t.closest(\"li.toctree-l1\").parent().addClass(\"current\").attr(\"aria-expanded\",\"true\");for(let n=1;n<=10;n++)t.closest(\"li.toctree-l\"+n).addClass(\"current\").attr(\"aria-expanded\",\"true\");t[0].scrollIntoView()}}catch(n){console.log(\"Error expanding nav for anchor\",n)}},onScroll:function(){this.winScroll=!1;var n=this.win.scrollTop(),e=n+this.winHeight,t=this.navBar.scrollTop()+(n-this.winPosition);n<0||e>this.docHeight||(this.navBar.scrollTop(t),this.winPosition=n)},onResize:function(){this.winResize=!1,this.winHeight=this.win.height(),this.docHeight=$(document).height()},hashChange:function(){this.linkScroll=!0,this.win.one(\"hashchange\",(function(){this.linkScroll=!1}))},toggleCurrent:function(n){var e=n.closest(\"li\");e.siblings(\"li.current\").removeClass(\"current\").attr(\"aria-expanded\",\"false\"),e.siblings().find(\"li.current\").removeClass(\"current\").attr(\"aria-expanded\",\"false\");var t=e.find(\"> ul li\");t.length&&(t.removeClass(\"current\").attr(\"aria-expanded\",\"false\"),e.toggleClass(\"current\").attr(\"aria-expanded\",(function(n,e){return\"true\"==e?\"false\":\"true\"})))}},\"undefined\"!=typeof window&&(window.SphinxRtdTheme={Navigation:n.exports.ThemeNav,StickyNav:n.exports.ThemeNav}),function(){for(var n=0,e=[\"ms\",\"moz\",\"webkit\",\"o\"],t=0;t<e.length&&!window.requestAnimationFrame;++t)window.requestAnimationFrame=window[e[t]+\"RequestAnimationFrame\"],window.cancelAnimationFrame=window[e[t]+\"CancelAnimationFrame\"]||window[e[t]+\"CancelRequestAnimationFrame\"];window.requestAnimationFrame||(window.requestAnimationFrame=function(e,t){var i=(new Date).getTime(),o=Math.max(0,16-(i-n)),r=window.setTimeout((function(){e(i+o)}),o);return n=i+o,r}),window.cancelAnimationFrame||(window.cancelAnimationFrame=function(n){clearTimeout(n)})}()}).call(window)},function(n,e){n.exports=jQuery},function(n,e,t){}]);\n\\ No newline at end of file"}, {"filename": "main/_static/jupyterlite_badge_logo.svg", "status": "removed", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -1,3 +0,0 @@\n-<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n-<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n-<svg width=\"91px\" height=\"20px\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:space=\"preserve\" xmlns:serif=\"http://www.serif.com/\" style=\"fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;\"><g transform=\"matrix(1,0,0,1,-17102,-9779)\"><g id=\"lite-badge-launch\" transform=\"matrix(0.0930728,0,0,0.0635416,15477.2,9173.48)\"><rect x=\"17457.1\" y=\"9529.53\" width=\"977.729\" height=\"314.755\" style=\"fill:none;\"/><clipPath id=\"_clip1\"><rect x=\"17457.1\" y=\"9529.53\" width=\"977.729\" height=\"314.755\"/></clipPath><g clip-path=\"url(#_clip1)\"><g transform=\"matrix(1.03361,0,0,1,-586.737,-1.01863e-10)\"><g transform=\"matrix(2.43224,0,0,2.42119,-20084.1,-14912.4)\"><path d=\"M15802.4,10095L15447.7,10095C15440.6,10095 15434.8,10103.8 15434.8,10114.5L15434.8,10205.5C15434.8,10216.3 15440.6,10225 15447.7,10225L15802.4,10225L15802.4,10095Z\" style=\"fill:rgb(85,85,85);\"/></g><g transform=\"matrix(-0.76361,0,0,2.42119,30189.2,-14912.4)\"><path d=\"M15802.4,10095L15475.7,10095C15453.1,10095 15434.8,10103.8 15434.8,10114.5L15434.8,10205.5C15434.8,10216.3 15453.1,10225 15475.7,10225L15802.4,10225L15802.4,10095Z\" style=\"fill:rgb(247,220,30);\"/></g><g transform=\"matrix(2.74885,0,0,12.7513,10841.5,-10210.3)\"><g transform=\"matrix(0.845162,0,0,0.273799,2185.28,1469.95)\"><circle cx=\"316.432\" cy=\"323.632\" r=\"24.936\" style=\"fill:rgb(247,220,30);\"/></g><g transform=\"matrix(0.845162,0,0,0.273799,2185.28,1469.95)\"><path d=\"M326.851,350.903C326.851,349.85 326.002,348.995 324.957,348.995L307.381,348.995C306.336,348.995 305.487,349.85 305.487,350.903C305.487,351.957 306.336,352.812 307.381,352.812L324.957,352.812C326.002,352.812 326.851,351.957 326.851,350.903Z\" style=\"fill:rgb(249,249,249);\"/></g><g transform=\"matrix(0.845162,0,0,0.273799,2185.28,1469.95)\"><path d=\"M326.828,355.958C326.828,354.883 325.962,354.01 324.895,354.01L307.443,354.01C306.376,354.01 305.509,354.883 305.509,355.958C305.509,357.033 306.376,357.906 307.443,357.906L324.895,357.906C325.962,357.906 326.828,357.033 326.828,355.958Z\" style=\"fill:rgb(249,249,249);\"/></g><g transform=\"matrix(0.845162,0,0,0.273799,2185.28,1469.95)\"><path d=\"M322.293,361.032C322.293,359.968 321.436,359.104 320.38,359.104L311.962,359.104C310.906,359.104 310.048,359.968 310.048,361.032C310.048,362.096 310.906,362.959 311.962,362.959L320.38,362.959C321.436,362.959 322.293,362.096 322.293,361.032Z\" style=\"fill:rgb(249,249,249);\"/></g><g transform=\"matrix(0.845162,0,0,0.273799,2425.45,1551.72)\"><g transform=\"matrix(1,0,0,1,-288.772,-300.351)\"><path d=\"M329.047,341.228C326.537,341.125 324.679,339.593 323.472,336.633C322.575,334.405 322.127,331.677 322.127,328.447C322.127,328.28 322.137,328.066 322.157,327.804C322.303,325.607 322.71,323.592 323.378,321.757C324.461,318.802 325.992,317.324 327.97,317.324C328.076,317.324 328.185,317.329 328.299,317.339C330.1,317.453 331.001,318.498 331.001,320.476C331.001,322.635 330.074,325.031 328.219,327.664C327.043,329.342 325.808,330.703 324.515,331.746C324.871,333.724 325.261,335.208 325.687,336.198C326.481,338.03 327.659,338.946 329.221,338.946C329.324,338.946 329.434,338.936 329.55,338.915C329.892,338.852 330.371,338.603 330.986,338.167C331.58,337.759 332.044,337.555 332.376,337.555C332.523,337.555 332.664,337.603 332.8,337.698C333.102,337.905 333.253,338.186 333.253,338.541C333.231,339.083 332.845,339.64 332.097,340.212C331.159,340.889 330.143,341.228 329.047,341.228ZM327.989,319.683C327.371,319.693 326.83,319.957 326.364,320.476C325.174,321.822 324.523,324.631 324.41,328.904C325.035,328.267 325.659,327.516 326.284,326.652C328.013,324.286 328.877,322.29 328.877,320.665C328.877,320.582 328.855,320.484 328.813,320.37C328.646,319.912 328.37,319.683 327.985,319.683L327.989,319.683Z\" style=\"fill:rgb(85,85,85);fill-rule:nonzero;\"/></g><g transform=\"matrix(1,0,0,1,-288.772,-300.351)\"><path d=\"M321.235,316.187C321.235,316.188 321.235,316.189 321.235,316.19C321.235,317.629 320.051,318.813 318.612,318.813L318.612,318.813C317.173,318.813 315.989,317.629 315.989,316.19C315.989,316.189 315.989,316.188 315.989,316.187C315.989,314.748 317.173,313.564 318.612,313.564C320.051,313.564 321.235,314.748 321.235,316.187Z\" style=\"fill:rgb(85,85,85);fill-rule:nonzero;\"/></g><g transform=\"matrix(1,0,0,1,-288.772,-300.351)\"><path d=\"M318.049,319.981C318.727,319.991 319.076,320.432 319.096,321.304L317.361,333.711C319.966,331.721 321.43,330.596 321.753,330.336C322.138,330.004 322.534,329.832 322.94,329.822C323.086,329.822 323.227,329.869 323.363,329.962C323.665,330.171 323.816,330.437 323.816,330.76C323.804,331.261 323.522,331.746 322.97,332.215C321.44,333.505 319.415,335.092 316.893,336.977L316.175,342.15C316.144,342.39 316.098,342.624 316.035,342.853C315.556,344.685 314.353,345.601 312.426,345.601C311.468,345.601 310.624,345.242 309.893,344.524C309.175,343.806 308.816,342.999 308.816,342.105C308.816,341.165 309.041,340.394 309.489,339.792C309.738,339.48 310.227,339.051 310.955,338.507L314.704,335.695C316.027,326.958 316.705,322.365 316.738,321.916C316.738,321.853 316.743,321.796 316.753,321.746C316.866,320.569 317.298,319.981 318.049,319.981ZM313.941,340.899L314.206,338.995L312.471,340.321C311.72,340.883 311.34,341.415 311.33,341.916C311.33,342.092 311.361,342.259 311.424,342.415C311.6,342.874 311.934,343.103 312.426,343.103C312.622,343.103 312.798,343.071 312.955,343.008C313.456,342.789 313.784,342.086 313.937,340.899L313.941,340.899ZM317.331,316.995C317.331,316.809 317.378,316.633 317.471,316.466C317.69,316.058 318.029,315.854 318.488,315.854C318.674,315.854 318.855,315.901 319.032,315.994C319.43,316.213 319.629,316.547 319.629,316.995C319.629,317.182 319.581,317.365 319.485,317.543C319.269,317.939 318.936,318.137 318.488,318.137C318.299,318.137 318.116,318.089 317.94,317.993C317.534,317.787 317.331,317.454 317.331,316.995Z\" style=\"fill:rgb(85,85,85);fill-rule:nonzero;\"/></g></g></g><g transform=\"matrix(3.03205,0,0,14.065,10159.7,-12659.1)\"><g transform=\"matrix(0.295204,0,0,0.0956347,2538.31,1594.22)\"><text x=\"-181.194px\" y=\"0px\" style=\"font-family:'Verdana', sans-serif;font-size:130.5px;fill:rgb(1,1,1);fill-opacity:0.3;\">launch</text></g><g transform=\"matrix(0.295204,0,0,0.0956347,2538.31,1593.26)\"><text x=\"-181.194px\" y=\"0px\" style=\"font-family:'Verdana', sans-serif;font-size:130.5px;fill:white;\">launch</text></g></g><g transform=\"matrix(3.5905,0,0,16.6556,9244.95,-16787)\"><g transform=\"matrix(0.295204,0,0,0.0956347,2538.31,1594.22)\"><text x=\"-181.194px\" y=\"0px\" style=\"font-family:'Verdana', sans-serif;font-size:110px;fill:rgb(1,1,1);fill-opacity:0.1;\">lite</text></g><g transform=\"matrix(0.295204,0,0,0.0956347,2538.31,1593.26)\"><text x=\"-181.194px\" y=\"0px\" style=\"font-family:'Verdana', sans-serif;font-size:110px;fill:rgb(56,56,56);\">lite</text></g></g></g></g></g></g></svg>\n\\ No newline at end of file"}, {"filename": "main/_static/language_data.js", "status": "removed", "additions": 0, "deletions": 199, "changes": 199, "file_content_changes": "@@ -1,199 +0,0 @@\n-/*\n- * language_data.js\n- * ~~~~~~~~~~~~~~~~\n- *\n- * This script contains the language-specific data used by searchtools.js,\n- * namely the list of stopwords, stemmer, scorer and splitter.\n- *\n- * :copyright: Copyright 2007-2023 by the Sphinx team, see AUTHORS.\n- * :license: BSD, see LICENSE for details.\n- *\n- */\n-\n-var stopwords = [\"a\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"if\", \"in\", \"into\", \"is\", \"it\", \"near\", \"no\", \"not\", \"of\", \"on\", \"or\", \"such\", \"that\", \"the\", \"their\", \"then\", \"there\", \"these\", \"they\", \"this\", \"to\", \"was\", \"will\", \"with\"];\n-\n-\n-/* Non-minified version is copied as a separate JS file, is available */\n-\n-/**\n- * Porter Stemmer\n- */\n-var Stemmer = function() {\n-\n-  var step2list = {\n-    ational: 'ate',\n-    tional: 'tion',\n-    enci: 'ence',\n-    anci: 'ance',\n-    izer: 'ize',\n-    bli: 'ble',\n-    alli: 'al',\n-    entli: 'ent',\n-    eli: 'e',\n-    ousli: 'ous',\n-    ization: 'ize',\n-    ation: 'ate',\n-    ator: 'ate',\n-    alism: 'al',\n-    iveness: 'ive',\n-    fulness: 'ful',\n-    ousness: 'ous',\n-    aliti: 'al',\n-    iviti: 'ive',\n-    biliti: 'ble',\n-    logi: 'log'\n-  };\n-\n-  var step3list = {\n-    icate: 'ic',\n-    ative: '',\n-    alize: 'al',\n-    iciti: 'ic',\n-    ical: 'ic',\n-    ful: '',\n-    ness: ''\n-  };\n-\n-  var c = \"[^aeiou]\";          // consonant\n-  var v = \"[aeiouy]\";          // vowel\n-  var C = c + \"[^aeiouy]*\";    // consonant sequence\n-  var V = v + \"[aeiou]*\";      // vowel sequence\n-\n-  var mgr0 = \"^(\" + C + \")?\" + V + C;                      // [C]VC... is m>0\n-  var meq1 = \"^(\" + C + \")?\" + V + C + \"(\" + V + \")?$\";    // [C]VC[V] is m=1\n-  var mgr1 = \"^(\" + C + \")?\" + V + C + V + C;              // [C]VCVC... is m>1\n-  var s_v   = \"^(\" + C + \")?\" + v;                         // vowel in stem\n-\n-  this.stemWord = function (w) {\n-    var stem;\n-    var suffix;\n-    var firstch;\n-    var origword = w;\n-\n-    if (w.length < 3)\n-      return w;\n-\n-    var re;\n-    var re2;\n-    var re3;\n-    var re4;\n-\n-    firstch = w.substr(0,1);\n-    if (firstch == \"y\")\n-      w = firstch.toUpperCase() + w.substr(1);\n-\n-    // Step 1a\n-    re = /^(.+?)(ss|i)es$/;\n-    re2 = /^(.+?)([^s])s$/;\n-\n-    if (re.test(w))\n-      w = w.replace(re,\"$1$2\");\n-    else if (re2.test(w))\n-      w = w.replace(re2,\"$1$2\");\n-\n-    // Step 1b\n-    re = /^(.+?)eed$/;\n-    re2 = /^(.+?)(ed|ing)$/;\n-    if (re.test(w)) {\n-      var fp = re.exec(w);\n-      re = new RegExp(mgr0);\n-      if (re.test(fp[1])) {\n-        re = /.$/;\n-        w = w.replace(re,\"\");\n-      }\n-    }\n-    else if (re2.test(w)) {\n-      var fp = re2.exec(w);\n-      stem = fp[1];\n-      re2 = new RegExp(s_v);\n-      if (re2.test(stem)) {\n-        w = stem;\n-        re2 = /(at|bl|iz)$/;\n-        re3 = new RegExp(\"([^aeiouylsz])\\\\1$\");\n-        re4 = new RegExp(\"^\" + C + v + \"[^aeiouwxy]$\");\n-        if (re2.test(w))\n-          w = w + \"e\";\n-        else if (re3.test(w)) {\n-          re = /.$/;\n-          w = w.replace(re,\"\");\n-        }\n-        else if (re4.test(w))\n-          w = w + \"e\";\n-      }\n-    }\n-\n-    // Step 1c\n-    re = /^(.+?)y$/;\n-    if (re.test(w)) {\n-      var fp = re.exec(w);\n-      stem = fp[1];\n-      re = new RegExp(s_v);\n-      if (re.test(stem))\n-        w = stem + \"i\";\n-    }\n-\n-    // Step 2\n-    re = /^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/;\n-    if (re.test(w)) {\n-      var fp = re.exec(w);\n-      stem = fp[1];\n-      suffix = fp[2];\n-      re = new RegExp(mgr0);\n-      if (re.test(stem))\n-        w = stem + step2list[suffix];\n-    }\n-\n-    // Step 3\n-    re = /^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/;\n-    if (re.test(w)) {\n-      var fp = re.exec(w);\n-      stem = fp[1];\n-      suffix = fp[2];\n-      re = new RegExp(mgr0);\n-      if (re.test(stem))\n-        w = stem + step3list[suffix];\n-    }\n-\n-    // Step 4\n-    re = /^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/;\n-    re2 = /^(.+?)(s|t)(ion)$/;\n-    if (re.test(w)) {\n-      var fp = re.exec(w);\n-      stem = fp[1];\n-      re = new RegExp(mgr1);\n-      if (re.test(stem))\n-        w = stem;\n-    }\n-    else if (re2.test(w)) {\n-      var fp = re2.exec(w);\n-      stem = fp[1] + fp[2];\n-      re2 = new RegExp(mgr1);\n-      if (re2.test(stem))\n-        w = stem;\n-    }\n-\n-    // Step 5\n-    re = /^(.+?)e$/;\n-    if (re.test(w)) {\n-      var fp = re.exec(w);\n-      stem = fp[1];\n-      re = new RegExp(mgr1);\n-      re2 = new RegExp(meq1);\n-      re3 = new RegExp(\"^\" + C + v + \"[^aeiouwxy]$\");\n-      if (re.test(stem) || (re2.test(stem) && !(re3.test(stem))))\n-        w = stem;\n-    }\n-    re = /ll$/;\n-    re2 = new RegExp(mgr1);\n-    if (re.test(w) && re2.test(w)) {\n-      re = /.$/;\n-      w = w.replace(re,\"\");\n-    }\n-\n-    // and turn initial Y back to y\n-    if (firstch == \"y\")\n-      w = firstch.toLowerCase() + w.substr(1);\n-    return w;\n-  }\n-}\n-"}, {"filename": "main/_static/minus.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_static/no_image.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_static/plus.png", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_static/pygments.css", "status": "removed", "additions": 0, "deletions": 74, "changes": 74, "file_content_changes": "@@ -1,74 +0,0 @@\n-pre { line-height: 125%; }\n-td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n-span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n-td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n-span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n-.highlight .hll { background-color: #ffffcc }\n-.highlight { background: #eeffcc; }\n-.highlight .c { color: #408090; font-style: italic } /* Comment */\n-.highlight .err { border: 1px solid #FF0000 } /* Error */\n-.highlight .k { color: #007020; font-weight: bold } /* Keyword */\n-.highlight .o { color: #666666 } /* Operator */\n-.highlight .ch { color: #408090; font-style: italic } /* Comment.Hashbang */\n-.highlight .cm { color: #408090; font-style: italic } /* Comment.Multiline */\n-.highlight .cp { color: #007020 } /* Comment.Preproc */\n-.highlight .cpf { color: #408090; font-style: italic } /* Comment.PreprocFile */\n-.highlight .c1 { color: #408090; font-style: italic } /* Comment.Single */\n-.highlight .cs { color: #408090; background-color: #fff0f0 } /* Comment.Special */\n-.highlight .gd { color: #A00000 } /* Generic.Deleted */\n-.highlight .ge { font-style: italic } /* Generic.Emph */\n-.highlight .gr { color: #FF0000 } /* Generic.Error */\n-.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n-.highlight .gi { color: #00A000 } /* Generic.Inserted */\n-.highlight .go { color: #333333 } /* Generic.Output */\n-.highlight .gp { color: #c65d09; font-weight: bold } /* Generic.Prompt */\n-.highlight .gs { font-weight: bold } /* Generic.Strong */\n-.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n-.highlight .gt { color: #0044DD } /* Generic.Traceback */\n-.highlight .kc { color: #007020; font-weight: bold } /* Keyword.Constant */\n-.highlight .kd { color: #007020; font-weight: bold } /* Keyword.Declaration */\n-.highlight .kn { color: #007020; font-weight: bold } /* Keyword.Namespace */\n-.highlight .kp { color: #007020 } /* Keyword.Pseudo */\n-.highlight .kr { color: #007020; font-weight: bold } /* Keyword.Reserved */\n-.highlight .kt { color: #902000 } /* Keyword.Type */\n-.highlight .m { color: #208050 } /* Literal.Number */\n-.highlight .s { color: #4070a0 } /* Literal.String */\n-.highlight .na { color: #4070a0 } /* Name.Attribute */\n-.highlight .nb { color: #007020 } /* Name.Builtin */\n-.highlight .nc { color: #0e84b5; font-weight: bold } /* Name.Class */\n-.highlight .no { color: #60add5 } /* Name.Constant */\n-.highlight .nd { color: #555555; font-weight: bold } /* Name.Decorator */\n-.highlight .ni { color: #d55537; font-weight: bold } /* Name.Entity */\n-.highlight .ne { color: #007020 } /* Name.Exception */\n-.highlight .nf { color: #06287e } /* Name.Function */\n-.highlight .nl { color: #002070; font-weight: bold } /* Name.Label */\n-.highlight .nn { color: #0e84b5; font-weight: bold } /* Name.Namespace */\n-.highlight .nt { color: #062873; font-weight: bold } /* Name.Tag */\n-.highlight .nv { color: #bb60d5 } /* Name.Variable */\n-.highlight .ow { color: #007020; font-weight: bold } /* Operator.Word */\n-.highlight .w { color: #bbbbbb } /* Text.Whitespace */\n-.highlight .mb { color: #208050 } /* Literal.Number.Bin */\n-.highlight .mf { color: #208050 } /* Literal.Number.Float */\n-.highlight .mh { color: #208050 } /* Literal.Number.Hex */\n-.highlight .mi { color: #208050 } /* Literal.Number.Integer */\n-.highlight .mo { color: #208050 } /* Literal.Number.Oct */\n-.highlight .sa { color: #4070a0 } /* Literal.String.Affix */\n-.highlight .sb { color: #4070a0 } /* Literal.String.Backtick */\n-.highlight .sc { color: #4070a0 } /* Literal.String.Char */\n-.highlight .dl { color: #4070a0 } /* Literal.String.Delimiter */\n-.highlight .sd { color: #4070a0; font-style: italic } /* Literal.String.Doc */\n-.highlight .s2 { color: #4070a0 } /* Literal.String.Double */\n-.highlight .se { color: #4070a0; font-weight: bold } /* Literal.String.Escape */\n-.highlight .sh { color: #4070a0 } /* Literal.String.Heredoc */\n-.highlight .si { color: #70a0d0; font-style: italic } /* Literal.String.Interpol */\n-.highlight .sx { color: #c65d09 } /* Literal.String.Other */\n-.highlight .sr { color: #235388 } /* Literal.String.Regex */\n-.highlight .s1 { color: #4070a0 } /* Literal.String.Single */\n-.highlight .ss { color: #517918 } /* Literal.String.Symbol */\n-.highlight .bp { color: #007020 } /* Name.Builtin.Pseudo */\n-.highlight .fm { color: #06287e } /* Name.Function.Magic */\n-.highlight .vc { color: #bb60d5 } /* Name.Variable.Class */\n-.highlight .vg { color: #bb60d5 } /* Name.Variable.Global */\n-.highlight .vi { color: #bb60d5 } /* Name.Variable.Instance */\n-.highlight .vm { color: #bb60d5 } /* Name.Variable.Magic */\n-.highlight .il { color: #208050 } /* Literal.Number.Integer.Long */\n\\ No newline at end of file"}, {"filename": "main/_static/searchtools.js", "status": "removed", "additions": 0, "deletions": 566, "changes": 566, "file_content_changes": "@@ -1,566 +0,0 @@\n-/*\n- * searchtools.js\n- * ~~~~~~~~~~~~~~~~\n- *\n- * Sphinx JavaScript utilities for the full-text search.\n- *\n- * :copyright: Copyright 2007-2023 by the Sphinx team, see AUTHORS.\n- * :license: BSD, see LICENSE for details.\n- *\n- */\n-\"use strict\";\n-\n-/**\n- * Simple result scoring code.\n- */\n-if (typeof Scorer === \"undefined\") {\n-  var Scorer = {\n-    // Implement the following function to further tweak the score for each result\n-    // The function takes a result array [docname, title, anchor, descr, score, filename]\n-    // and returns the new score.\n-    /*\n-    score: result => {\n-      const [docname, title, anchor, descr, score, filename] = result\n-      return score\n-    },\n-    */\n-\n-    // query matches the full name of an object\n-    objNameMatch: 11,\n-    // or matches in the last dotted part of the object name\n-    objPartialMatch: 6,\n-    // Additive scores depending on the priority of the object\n-    objPrio: {\n-      0: 15, // used to be importantResults\n-      1: 5, // used to be objectResults\n-      2: -5, // used to be unimportantResults\n-    },\n-    //  Used when the priority is not in the mapping.\n-    objPrioDefault: 0,\n-\n-    // query found in title\n-    title: 15,\n-    partialTitle: 7,\n-    // query found in terms\n-    term: 5,\n-    partialTerm: 2,\n-  };\n-}\n-\n-const _removeChildren = (element) => {\n-  while (element && element.lastChild) element.removeChild(element.lastChild);\n-};\n-\n-/**\n- * See https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_Expressions#escaping\n- */\n-const _escapeRegExp = (string) =>\n-  string.replace(/[.*+\\-?^${}()|[\\]\\\\]/g, \"\\\\$&\"); // $& means the whole matched string\n-\n-const _displayItem = (item, searchTerms) => {\n-  const docBuilder = DOCUMENTATION_OPTIONS.BUILDER;\n-  const docUrlRoot = DOCUMENTATION_OPTIONS.URL_ROOT;\n-  const docFileSuffix = DOCUMENTATION_OPTIONS.FILE_SUFFIX;\n-  const docLinkSuffix = DOCUMENTATION_OPTIONS.LINK_SUFFIX;\n-  const showSearchSummary = DOCUMENTATION_OPTIONS.SHOW_SEARCH_SUMMARY;\n-\n-  const [docName, title, anchor, descr, score, _filename] = item;\n-\n-  let listItem = document.createElement(\"li\");\n-  let requestUrl;\n-  let linkUrl;\n-  if (docBuilder === \"dirhtml\") {\n-    // dirhtml builder\n-    let dirname = docName + \"/\";\n-    if (dirname.match(/\\/index\\/$/))\n-      dirname = dirname.substring(0, dirname.length - 6);\n-    else if (dirname === \"index/\") dirname = \"\";\n-    requestUrl = docUrlRoot + dirname;\n-    linkUrl = requestUrl;\n-  } else {\n-    // normal html builders\n-    requestUrl = docUrlRoot + docName + docFileSuffix;\n-    linkUrl = docName + docLinkSuffix;\n-  }\n-  let linkEl = listItem.appendChild(document.createElement(\"a\"));\n-  linkEl.href = linkUrl + anchor;\n-  linkEl.dataset.score = score;\n-  linkEl.innerHTML = title;\n-  if (descr)\n-    listItem.appendChild(document.createElement(\"span\")).innerHTML =\n-      \" (\" + descr + \")\";\n-  else if (showSearchSummary)\n-    fetch(requestUrl)\n-      .then((responseData) => responseData.text())\n-      .then((data) => {\n-        if (data)\n-          listItem.appendChild(\n-            Search.makeSearchSummary(data, searchTerms)\n-          );\n-      });\n-  Search.output.appendChild(listItem);\n-};\n-const _finishSearch = (resultCount) => {\n-  Search.stopPulse();\n-  Search.title.innerText = _(\"Search Results\");\n-  if (!resultCount)\n-    Search.status.innerText = Documentation.gettext(\n-      \"Your search did not match any documents. Please make sure that all words are spelled correctly and that you've selected enough categories.\"\n-    );\n-  else\n-    Search.status.innerText = _(\n-      `Search finished, found ${resultCount} page(s) matching the search query.`\n-    );\n-};\n-const _displayNextItem = (\n-  results,\n-  resultCount,\n-  searchTerms\n-) => {\n-  // results left, load the summary and display it\n-  // this is intended to be dynamic (don't sub resultsCount)\n-  if (results.length) {\n-    _displayItem(results.pop(), searchTerms);\n-    setTimeout(\n-      () => _displayNextItem(results, resultCount, searchTerms),\n-      5\n-    );\n-  }\n-  // search finished, update title and status message\n-  else _finishSearch(resultCount);\n-};\n-\n-/**\n- * Default splitQuery function. Can be overridden in ``sphinx.search`` with a\n- * custom function per language.\n- *\n- * The regular expression works by splitting the string on consecutive characters\n- * that are not Unicode letters, numbers, underscores, or emoji characters.\n- * This is the same as ``\\W+`` in Python, preserving the surrogate pair area.\n- */\n-if (typeof splitQuery === \"undefined\") {\n-  var splitQuery = (query) => query\n-      .split(/[^\\p{Letter}\\p{Number}_\\p{Emoji_Presentation}]+/gu)\n-      .filter(term => term)  // remove remaining empty strings\n-}\n-\n-/**\n- * Search Module\n- */\n-const Search = {\n-  _index: null,\n-  _queued_query: null,\n-  _pulse_status: -1,\n-\n-  htmlToText: (htmlString) => {\n-    const htmlElement = new DOMParser().parseFromString(htmlString, 'text/html');\n-    htmlElement.querySelectorAll(\".headerlink\").forEach((el) => { el.remove() });\n-    const docContent = htmlElement.querySelector('[role=\"main\"]');\n-    if (docContent !== undefined) return docContent.textContent;\n-    console.warn(\n-      \"Content block not found. Sphinx search tries to obtain it via '[role=main]'. Could you check your theme or template.\"\n-    );\n-    return \"\";\n-  },\n-\n-  init: () => {\n-    const query = new URLSearchParams(window.location.search).get(\"q\");\n-    document\n-      .querySelectorAll('input[name=\"q\"]')\n-      .forEach((el) => (el.value = query));\n-    if (query) Search.performSearch(query);\n-  },\n-\n-  loadIndex: (url) =>\n-    (document.body.appendChild(document.createElement(\"script\")).src = url),\n-\n-  setIndex: (index) => {\n-    Search._index = index;\n-    if (Search._queued_query !== null) {\n-      const query = Search._queued_query;\n-      Search._queued_query = null;\n-      Search.query(query);\n-    }\n-  },\n-\n-  hasIndex: () => Search._index !== null,\n-\n-  deferQuery: (query) => (Search._queued_query = query),\n-\n-  stopPulse: () => (Search._pulse_status = -1),\n-\n-  startPulse: () => {\n-    if (Search._pulse_status >= 0) return;\n-\n-    const pulse = () => {\n-      Search._pulse_status = (Search._pulse_status + 1) % 4;\n-      Search.dots.innerText = \".\".repeat(Search._pulse_status);\n-      if (Search._pulse_status >= 0) window.setTimeout(pulse, 500);\n-    };\n-    pulse();\n-  },\n-\n-  /**\n-   * perform a search for something (or wait until index is loaded)\n-   */\n-  performSearch: (query) => {\n-    // create the required interface elements\n-    const searchText = document.createElement(\"h2\");\n-    searchText.textContent = _(\"Searching\");\n-    const searchSummary = document.createElement(\"p\");\n-    searchSummary.classList.add(\"search-summary\");\n-    searchSummary.innerText = \"\";\n-    const searchList = document.createElement(\"ul\");\n-    searchList.classList.add(\"search\");\n-\n-    const out = document.getElementById(\"search-results\");\n-    Search.title = out.appendChild(searchText);\n-    Search.dots = Search.title.appendChild(document.createElement(\"span\"));\n-    Search.status = out.appendChild(searchSummary);\n-    Search.output = out.appendChild(searchList);\n-\n-    const searchProgress = document.getElementById(\"search-progress\");\n-    // Some themes don't use the search progress node\n-    if (searchProgress) {\n-      searchProgress.innerText = _(\"Preparing search...\");\n-    }\n-    Search.startPulse();\n-\n-    // index already loaded, the browser was quick!\n-    if (Search.hasIndex()) Search.query(query);\n-    else Search.deferQuery(query);\n-  },\n-\n-  /**\n-   * execute search (requires search index to be loaded)\n-   */\n-  query: (query) => {\n-    const filenames = Search._index.filenames;\n-    const docNames = Search._index.docnames;\n-    const titles = Search._index.titles;\n-    const allTitles = Search._index.alltitles;\n-    const indexEntries = Search._index.indexentries;\n-\n-    // stem the search terms and add them to the correct list\n-    const stemmer = new Stemmer();\n-    const searchTerms = new Set();\n-    const excludedTerms = new Set();\n-    const highlightTerms = new Set();\n-    const objectTerms = new Set(splitQuery(query.toLowerCase().trim()));\n-    splitQuery(query.trim()).forEach((queryTerm) => {\n-      const queryTermLower = queryTerm.toLowerCase();\n-\n-      // maybe skip this \"word\"\n-      // stopwords array is from language_data.js\n-      if (\n-        stopwords.indexOf(queryTermLower) !== -1 ||\n-        queryTerm.match(/^\\d+$/)\n-      )\n-        return;\n-\n-      // stem the word\n-      let word = stemmer.stemWord(queryTermLower);\n-      // select the correct list\n-      if (word[0] === \"-\") excludedTerms.add(word.substr(1));\n-      else {\n-        searchTerms.add(word);\n-        highlightTerms.add(queryTermLower);\n-      }\n-    });\n-\n-    if (SPHINX_HIGHLIGHT_ENABLED) {  // set in sphinx_highlight.js\n-      localStorage.setItem(\"sphinx_highlight_terms\", [...highlightTerms].join(\" \"))\n-    }\n-\n-    // console.debug(\"SEARCH: searching for:\");\n-    // console.info(\"required: \", [...searchTerms]);\n-    // console.info(\"excluded: \", [...excludedTerms]);\n-\n-    // array of [docname, title, anchor, descr, score, filename]\n-    let results = [];\n-    _removeChildren(document.getElementById(\"search-progress\"));\n-\n-    const queryLower = query.toLowerCase();\n-    for (const [title, foundTitles] of Object.entries(allTitles)) {\n-      if (title.toLowerCase().includes(queryLower) && (queryLower.length >= title.length/2)) {\n-        for (const [file, id] of foundTitles) {\n-          let score = Math.round(100 * queryLower.length / title.length)\n-          results.push([\n-            docNames[file],\n-            titles[file] !== title ? `${titles[file]} > ${title}` : title,\n-            id !== null ? \"#\" + id : \"\",\n-            null,\n-            score,\n-            filenames[file],\n-          ]);\n-        }\n-      }\n-    }\n-\n-    // search for explicit entries in index directives\n-    for (const [entry, foundEntries] of Object.entries(indexEntries)) {\n-      if (entry.includes(queryLower) && (queryLower.length >= entry.length/2)) {\n-        for (const [file, id] of foundEntries) {\n-          let score = Math.round(100 * queryLower.length / entry.length)\n-          results.push([\n-            docNames[file],\n-            titles[file],\n-            id ? \"#\" + id : \"\",\n-            null,\n-            score,\n-            filenames[file],\n-          ]);\n-        }\n-      }\n-    }\n-\n-    // lookup as object\n-    objectTerms.forEach((term) =>\n-      results.push(...Search.performObjectSearch(term, objectTerms))\n-    );\n-\n-    // lookup as search terms in fulltext\n-    results.push(...Search.performTermsSearch(searchTerms, excludedTerms));\n-\n-    // let the scorer override scores with a custom scoring function\n-    if (Scorer.score) results.forEach((item) => (item[4] = Scorer.score(item)));\n-\n-    // now sort the results by score (in opposite order of appearance, since the\n-    // display function below uses pop() to retrieve items) and then\n-    // alphabetically\n-    results.sort((a, b) => {\n-      const leftScore = a[4];\n-      const rightScore = b[4];\n-      if (leftScore === rightScore) {\n-        // same score: sort alphabetically\n-        const leftTitle = a[1].toLowerCase();\n-        const rightTitle = b[1].toLowerCase();\n-        if (leftTitle === rightTitle) return 0;\n-        return leftTitle > rightTitle ? -1 : 1; // inverted is intentional\n-      }\n-      return leftScore > rightScore ? 1 : -1;\n-    });\n-\n-    // remove duplicate search results\n-    // note the reversing of results, so that in the case of duplicates, the highest-scoring entry is kept\n-    let seen = new Set();\n-    results = results.reverse().reduce((acc, result) => {\n-      let resultStr = result.slice(0, 4).concat([result[5]]).map(v => String(v)).join(',');\n-      if (!seen.has(resultStr)) {\n-        acc.push(result);\n-        seen.add(resultStr);\n-      }\n-      return acc;\n-    }, []);\n-\n-    results = results.reverse();\n-\n-    // for debugging\n-    //Search.lastresults = results.slice();  // a copy\n-    // console.info(\"search results:\", Search.lastresults);\n-\n-    // print the results\n-    _displayNextItem(results, results.length, searchTerms);\n-  },\n-\n-  /**\n-   * search for object names\n-   */\n-  performObjectSearch: (object, objectTerms) => {\n-    const filenames = Search._index.filenames;\n-    const docNames = Search._index.docnames;\n-    const objects = Search._index.objects;\n-    const objNames = Search._index.objnames;\n-    const titles = Search._index.titles;\n-\n-    const results = [];\n-\n-    const objectSearchCallback = (prefix, match) => {\n-      const name = match[4]\n-      const fullname = (prefix ? prefix + \".\" : \"\") + name;\n-      const fullnameLower = fullname.toLowerCase();\n-      if (fullnameLower.indexOf(object) < 0) return;\n-\n-      let score = 0;\n-      const parts = fullnameLower.split(\".\");\n-\n-      // check for different match types: exact matches of full name or\n-      // \"last name\" (i.e. last dotted part)\n-      if (fullnameLower === object || parts.slice(-1)[0] === object)\n-        score += Scorer.objNameMatch;\n-      else if (parts.slice(-1)[0].indexOf(object) > -1)\n-        score += Scorer.objPartialMatch; // matches in last name\n-\n-      const objName = objNames[match[1]][2];\n-      const title = titles[match[0]];\n-\n-      // If more than one term searched for, we require other words to be\n-      // found in the name/title/description\n-      const otherTerms = new Set(objectTerms);\n-      otherTerms.delete(object);\n-      if (otherTerms.size > 0) {\n-        const haystack = `${prefix} ${name} ${objName} ${title}`.toLowerCase();\n-        if (\n-          [...otherTerms].some((otherTerm) => haystack.indexOf(otherTerm) < 0)\n-        )\n-          return;\n-      }\n-\n-      let anchor = match[3];\n-      if (anchor === \"\") anchor = fullname;\n-      else if (anchor === \"-\") anchor = objNames[match[1]][1] + \"-\" + fullname;\n-\n-      const descr = objName + _(\", in \") + title;\n-\n-      // add custom score for some objects according to scorer\n-      if (Scorer.objPrio.hasOwnProperty(match[2]))\n-        score += Scorer.objPrio[match[2]];\n-      else score += Scorer.objPrioDefault;\n-\n-      results.push([\n-        docNames[match[0]],\n-        fullname,\n-        \"#\" + anchor,\n-        descr,\n-        score,\n-        filenames[match[0]],\n-      ]);\n-    };\n-    Object.keys(objects).forEach((prefix) =>\n-      objects[prefix].forEach((array) =>\n-        objectSearchCallback(prefix, array)\n-      )\n-    );\n-    return results;\n-  },\n-\n-  /**\n-   * search for full-text terms in the index\n-   */\n-  performTermsSearch: (searchTerms, excludedTerms) => {\n-    // prepare search\n-    const terms = Search._index.terms;\n-    const titleTerms = Search._index.titleterms;\n-    const filenames = Search._index.filenames;\n-    const docNames = Search._index.docnames;\n-    const titles = Search._index.titles;\n-\n-    const scoreMap = new Map();\n-    const fileMap = new Map();\n-\n-    // perform the search on the required terms\n-    searchTerms.forEach((word) => {\n-      const files = [];\n-      const arr = [\n-        { files: terms[word], score: Scorer.term },\n-        { files: titleTerms[word], score: Scorer.title },\n-      ];\n-      // add support for partial matches\n-      if (word.length > 2) {\n-        const escapedWord = _escapeRegExp(word);\n-        Object.keys(terms).forEach((term) => {\n-          if (term.match(escapedWord) && !terms[word])\n-            arr.push({ files: terms[term], score: Scorer.partialTerm });\n-        });\n-        Object.keys(titleTerms).forEach((term) => {\n-          if (term.match(escapedWord) && !titleTerms[word])\n-            arr.push({ files: titleTerms[word], score: Scorer.partialTitle });\n-        });\n-      }\n-\n-      // no match but word was a required one\n-      if (arr.every((record) => record.files === undefined)) return;\n-\n-      // found search word in contents\n-      arr.forEach((record) => {\n-        if (record.files === undefined) return;\n-\n-        let recordFiles = record.files;\n-        if (recordFiles.length === undefined) recordFiles = [recordFiles];\n-        files.push(...recordFiles);\n-\n-        // set score for the word in each file\n-        recordFiles.forEach((file) => {\n-          if (!scoreMap.has(file)) scoreMap.set(file, {});\n-          scoreMap.get(file)[word] = record.score;\n-        });\n-      });\n-\n-      // create the mapping\n-      files.forEach((file) => {\n-        if (fileMap.has(file) && fileMap.get(file).indexOf(word) === -1)\n-          fileMap.get(file).push(word);\n-        else fileMap.set(file, [word]);\n-      });\n-    });\n-\n-    // now check if the files don't contain excluded terms\n-    const results = [];\n-    for (const [file, wordList] of fileMap) {\n-      // check if all requirements are matched\n-\n-      // as search terms with length < 3 are discarded\n-      const filteredTermCount = [...searchTerms].filter(\n-        (term) => term.length > 2\n-      ).length;\n-      if (\n-        wordList.length !== searchTerms.size &&\n-        wordList.length !== filteredTermCount\n-      )\n-        continue;\n-\n-      // ensure that none of the excluded terms is in the search result\n-      if (\n-        [...excludedTerms].some(\n-          (term) =>\n-            terms[term] === file ||\n-            titleTerms[term] === file ||\n-            (terms[term] || []).includes(file) ||\n-            (titleTerms[term] || []).includes(file)\n-        )\n-      )\n-        break;\n-\n-      // select one (max) score for the file.\n-      const score = Math.max(...wordList.map((w) => scoreMap.get(file)[w]));\n-      // add result to the result list\n-      results.push([\n-        docNames[file],\n-        titles[file],\n-        \"\",\n-        null,\n-        score,\n-        filenames[file],\n-      ]);\n-    }\n-    return results;\n-  },\n-\n-  /**\n-   * helper function to return a node containing the\n-   * search summary for a given text. keywords is a list\n-   * of stemmed words.\n-   */\n-  makeSearchSummary: (htmlText, keywords) => {\n-    const text = Search.htmlToText(htmlText);\n-    if (text === \"\") return null;\n-\n-    const textLower = text.toLowerCase();\n-    const actualStartPosition = [...keywords]\n-      .map((k) => textLower.indexOf(k.toLowerCase()))\n-      .filter((i) => i > -1)\n-      .slice(-1)[0];\n-    const startWithContext = Math.max(actualStartPosition - 120, 0);\n-\n-    const top = startWithContext === 0 ? \"\" : \"...\";\n-    const tail = startWithContext + 240 < text.length ? \"...\" : \"\";\n-\n-    let summary = document.createElement(\"p\");\n-    summary.classList.add(\"context\");\n-    summary.textContent = top + text.substr(startWithContext, 240).trim() + tail;\n-\n-    return summary;\n-  },\n-};\n-\n-_ready(Search.init);"}, {"filename": "main/_static/sg_gallery-binder.css", "status": "removed", "additions": 0, "deletions": 11, "changes": 11, "file_content_changes": "@@ -1,11 +0,0 @@\n-/* CSS for binder integration */\n-\n-div.binder-badge {\n-  margin: 1em auto;\n-  vertical-align: middle;\n-}\n-\n-div.lite-badge {\n-  margin: 1em auto;\n-  vertical-align: middle;\n-}"}, {"filename": "main/_static/sg_gallery-dataframe.css", "status": "removed", "additions": 0, "deletions": 46, "changes": 46, "file_content_changes": "@@ -1,46 +0,0 @@\n-/* Pandas dataframe css */\n-/* Taken from: https://github.com/spatialaudio/nbsphinx/blob/fb3ba670fc1ba5f54d4c487573dbc1b4ecf7e9ff/src/nbsphinx.py#L587-L619 */\n-html[data-theme=\"light\"] {\n-  --sg-text-color: #000;\n-  --sg-tr-odd-color: #f5f5f5;\n-  --sg-tr-hover-color: rgba(66, 165, 245, 0.2);\n-}\n-html[data-theme=\"dark\"] {\n-  --sg-text-color: #fff;\n-  --sg-tr-odd-color: #373737;\n-  --sg-tr-hover-color: rgba(30, 81, 122, 0.2);\n-}\n-\n-table.dataframe {\n-  border: none !important;\n-  border-collapse: collapse;\n-  border-spacing: 0;\n-  border-color: transparent;\n-  color: var(--sg-text-color);\n-  font-size: 12px;\n-  table-layout: fixed;\n-}\n-table.dataframe thead {\n-  border-bottom: 1px solid var(--sg-text-color);\n-  vertical-align: bottom;\n-}\n-table.dataframe tr,\n-table.dataframe th,\n-table.dataframe td {\n-  text-align: right;\n-  vertical-align: middle;\n-  padding: 0.5em 0.5em;\n-  line-height: normal;\n-  white-space: normal;\n-  max-width: none;\n-  border: none;\n-}\n-table.dataframe th {\n-  font-weight: bold;\n-}\n-table.dataframe tbody tr:nth-child(odd) {\n-  background: var(--sg-tr-odd-color);\n-}\n-table.dataframe tbody tr:hover {\n-  background: var(--sg-tr-hover-color);\n-}"}, {"filename": "main/_static/sg_gallery-rendered-html.css", "status": "removed", "additions": 0, "deletions": 224, "changes": 224, "file_content_changes": "@@ -1,224 +0,0 @@\n-/* Adapted from notebook/static/style/style.min.css */\n-html[data-theme=\"light\"] {\n-  --sg-text-color: #000;\n-  --sg-background-color: #ffffff;\n-  --sg-code-background-color: #eff0f1;\n-  --sg-tr-hover-color: rgba(66, 165, 245, 0.2);\n-  --sg-tr-odd-color: #f5f5f5;\n-}\n-html[data-theme=\"dark\"] {\n-  --sg-text-color: #fff;\n-  --sg-background-color: #121212;\n-  --sg-code-background-color: #2f2f30;\n-  --sg-tr-hover-color: rgba(66, 165, 245, 0.2);\n-  --sg-tr-odd-color: #1f1f1f;\n-}\n-\n-.rendered_html {\n-  color: var(--sg-text-color);\n-  /* any extras will just be numbers: */\n-}\n-.rendered_html em {\n-  font-style: italic;\n-}\n-.rendered_html strong {\n-  font-weight: bold;\n-}\n-.rendered_html u {\n-  text-decoration: underline;\n-}\n-.rendered_html :link {\n-  text-decoration: underline;\n-}\n-.rendered_html :visited {\n-  text-decoration: underline;\n-}\n-.rendered_html h1 {\n-  font-size: 185.7%;\n-  margin: 1.08em 0 0 0;\n-  font-weight: bold;\n-  line-height: 1.0;\n-}\n-.rendered_html h2 {\n-  font-size: 157.1%;\n-  margin: 1.27em 0 0 0;\n-  font-weight: bold;\n-  line-height: 1.0;\n-}\n-.rendered_html h3 {\n-  font-size: 128.6%;\n-  margin: 1.55em 0 0 0;\n-  font-weight: bold;\n-  line-height: 1.0;\n-}\n-.rendered_html h4 {\n-  font-size: 100%;\n-  margin: 2em 0 0 0;\n-  font-weight: bold;\n-  line-height: 1.0;\n-}\n-.rendered_html h5 {\n-  font-size: 100%;\n-  margin: 2em 0 0 0;\n-  font-weight: bold;\n-  line-height: 1.0;\n-  font-style: italic;\n-}\n-.rendered_html h6 {\n-  font-size: 100%;\n-  margin: 2em 0 0 0;\n-  font-weight: bold;\n-  line-height: 1.0;\n-  font-style: italic;\n-}\n-.rendered_html h1:first-child {\n-  margin-top: 0.538em;\n-}\n-.rendered_html h2:first-child {\n-  margin-top: 0.636em;\n-}\n-.rendered_html h3:first-child {\n-  margin-top: 0.777em;\n-}\n-.rendered_html h4:first-child {\n-  margin-top: 1em;\n-}\n-.rendered_html h5:first-child {\n-  margin-top: 1em;\n-}\n-.rendered_html h6:first-child {\n-  margin-top: 1em;\n-}\n-.rendered_html ul:not(.list-inline),\n-.rendered_html ol:not(.list-inline) {\n-  padding-left: 2em;\n-}\n-.rendered_html ul {\n-  list-style: disc;\n-}\n-.rendered_html ul ul {\n-  list-style: square;\n-  margin-top: 0;\n-}\n-.rendered_html ul ul ul {\n-  list-style: circle;\n-}\n-.rendered_html ol {\n-  list-style: decimal;\n-}\n-.rendered_html ol ol {\n-  list-style: upper-alpha;\n-  margin-top: 0;\n-}\n-.rendered_html ol ol ol {\n-  list-style: lower-alpha;\n-}\n-.rendered_html ol ol ol ol {\n-  list-style: lower-roman;\n-}\n-.rendered_html ol ol ol ol ol {\n-  list-style: decimal;\n-}\n-.rendered_html * + ul {\n-  margin-top: 1em;\n-}\n-.rendered_html * + ol {\n-  margin-top: 1em;\n-}\n-.rendered_html hr {\n-  color: var(--sg-text-color);\n-  background-color: var(--sg-text-color);\n-}\n-.rendered_html pre {\n-  margin: 1em 2em;\n-  padding: 0px;\n-  background-color: var(--sg-background-color);\n-}\n-.rendered_html code {\n-  background-color: var(--sg-code-background-color);\n-}\n-.rendered_html p code {\n-  padding: 1px 5px;\n-}\n-.rendered_html pre code {\n-  background-color: var(--sg-background-color);\n-}\n-.rendered_html pre,\n-.rendered_html code {\n-  border: 0;\n-  color: var(--sg-text-color);\n-  font-size: 100%;\n-}\n-.rendered_html blockquote {\n-  margin: 1em 2em;\n-}\n-.rendered_html table {\n-  margin-left: auto;\n-  margin-right: auto;\n-  border: none;\n-  border-collapse: collapse;\n-  border-spacing: 0;\n-  color: var(--sg-text-color);\n-  font-size: 12px;\n-  table-layout: fixed;\n-}\n-.rendered_html thead {\n-  border-bottom: 1px solid var(--sg-text-color);\n-  vertical-align: bottom;\n-}\n-.rendered_html tr,\n-.rendered_html th,\n-.rendered_html td {\n-  text-align: right;\n-  vertical-align: middle;\n-  padding: 0.5em 0.5em;\n-  line-height: normal;\n-  white-space: normal;\n-  max-width: none;\n-  border: none;\n-}\n-.rendered_html th {\n-  font-weight: bold;\n-}\n-.rendered_html tbody tr:nth-child(odd) {\n-  background: var(--sg-tr-odd-color);\n-}\n-.rendered_html tbody tr:hover {\n-  color: var(--sg-text-color);\n-  background: var(--sg-tr-hover-color);\n-}\n-.rendered_html * + table {\n-  margin-top: 1em;\n-}\n-.rendered_html p {\n-  text-align: left;\n-}\n-.rendered_html * + p {\n-  margin-top: 1em;\n-}\n-.rendered_html img {\n-  display: block;\n-  margin-left: auto;\n-  margin-right: auto;\n-}\n-.rendered_html * + img {\n-  margin-top: 1em;\n-}\n-.rendered_html img,\n-.rendered_html svg {\n-  max-width: 100%;\n-  height: auto;\n-}\n-.rendered_html img.unconfined,\n-.rendered_html svg.unconfined {\n-  max-width: none;\n-}\n-.rendered_html .alert {\n-  margin-bottom: initial;\n-}\n-.rendered_html * + .alert {\n-  margin-top: 1em;\n-}\n-[dir=\"rtl\"] .rendered_html p {\n-  text-align: right;\n-}"}, {"filename": "main/_static/sg_gallery.css", "status": "removed", "additions": 0, "deletions": 342, "changes": 342, "file_content_changes": "@@ -1,342 +0,0 @@\n-/*\n-Sphinx-Gallery has compatible CSS to fix default sphinx themes\n-Tested for Sphinx 1.3.1 for all themes: default, alabaster, sphinxdoc,\n-scrolls, agogo, traditional, nature, haiku, pyramid\n-Tested for Read the Docs theme 0.1.7 */\n-\n-/* Define light colors */\n-:root, html[data-theme=\"light\"], body[data-theme=\"light\"]{\n-  --sg-tooltip-foreground: black;\n-  --sg-tooltip-background: rgba(250, 250, 250, 0.9);\n-  --sg-tooltip-border: #ccc transparent;\n-  --sg-thumb-box-shadow-color: #6c757d40;\n-  --sg-thumb-hover-border: #0069d9;\n-  --sg-script-out: #888;\n-  --sg-script-pre: #fafae2;\n-  --sg-pytb-foreground: #000;\n-  --sg-pytb-background: #ffe4e4;\n-  --sg-pytb-border-color: #f66;\n-  --sg-download-a-background-color: #ffc;\n-  --sg-download-a-background-image: linear-gradient(to bottom, #ffc, #d5d57e);\n-  --sg-download-a-border-color: 1px solid #c2c22d;\n-  --sg-download-a-color: #000;\n-  --sg-download-a-hover-background-color: #d5d57e;\n-  --sg-download-a-hover-box-shadow-1: rgba(255, 255, 255, 0.1);\n-  --sg-download-a-hover-box-shadow-2: rgba(0, 0, 0, 0.25);\n-}\n-@media(prefers-color-scheme: light) {\n-  :root[data-theme=\"auto\"], html[data-theme=\"auto\"], body[data-theme=\"auto\"] {\n-    --sg-tooltip-foreground: black;\n-    --sg-tooltip-background: rgba(250, 250, 250, 0.9);\n-    --sg-tooltip-border: #ccc transparent;\n-    --sg-thumb-box-shadow-color: #6c757d40;\n-    --sg-thumb-hover-border: #0069d9;\n-    --sg-script-out: #888;\n-    --sg-script-pre: #fafae2;\n-    --sg-pytb-foreground: #000;\n-    --sg-pytb-background: #ffe4e4;\n-    --sg-pytb-border-color: #f66;\n-    --sg-download-a-background-color: #ffc;\n-    --sg-download-a-background-image: linear-gradient(to bottom, #ffc, #d5d57e);\n-    --sg-download-a-border-color: 1px solid #c2c22d;\n-    --sg-download-a-color: #000;\n-    --sg-download-a-hover-background-color: #d5d57e;\n-    --sg-download-a-hover-box-shadow-1: rgba(255, 255, 255, 0.1);\n-    --sg-download-a-hover-box-shadow-2: rgba(0, 0, 0, 0.25);\n-  }\n-}\n-\n-html[data-theme=\"dark\"], body[data-theme=\"dark\"] {\n-  --sg-tooltip-foreground: white;\n-  --sg-tooltip-background: rgba(10, 10, 10, 0.9);\n-  --sg-tooltip-border: #333 transparent;\n-  --sg-thumb-box-shadow-color: #79848d40;\n-  --sg-thumb-hover-border: #003975;\n-  --sg-script-out: rgb(179, 179, 179);\n-  --sg-script-pre: #2e2e22;\n-  --sg-pytb-foreground: #fff;\n-  --sg-pytb-background: #1b1717;\n-  --sg-pytb-border-color: #622;\n-  --sg-download-a-background-color: #443;\n-  --sg-download-a-background-image: linear-gradient(to bottom, #443, #221);\n-  --sg-download-a-border-color: 1px solid #3a3a0d;\n-  --sg-download-a-color: #fff;\n-  --sg-download-a-hover-background-color: #616135;\n-  --sg-download-a-hover-box-shadow-1: rgba(0, 0, 0, 0.1);\n-  --sg-download-a-hover-box-shadow-2: rgba(255, 255, 255, 0.25);\n-}\n-@media(prefers-color-scheme: dark){\n-  html[data-theme=\"auto\"], body[data-theme=\"auto\"] {\n-    --sg-tooltip-foreground: white;\n-    --sg-tooltip-background: rgba(10, 10, 10, 0.9);\n-    --sg-tooltip-border: #333 transparent;\n-    --sg-thumb-box-shadow-color: #79848d40;\n-    --sg-thumb-hover-border: #003975;\n-    --sg-script-out: rgb(179, 179, 179);\n-    --sg-script-pre: #2e2e22;\n-    --sg-pytb-foreground: #fff;\n-    --sg-pytb-background: #1b1717;\n-    --sg-pytb-border-color: #622;\n-    --sg-download-a-background-color: #443;\n-    --sg-download-a-background-image: linear-gradient(to bottom, #443, #221);\n-    --sg-download-a-border-color: 1px solid #3a3a0d;\n-    --sg-download-a-color: #fff;\n-    --sg-download-a-hover-background-color: #616135;\n-    --sg-download-a-hover-box-shadow-1: rgba(0, 0, 0, 0.1);\n-    --sg-download-a-hover-box-shadow-2: rgba(255, 255, 255, 0.25);\n-  }\n-}\n-\n-.sphx-glr-thumbnails {\n-  width: 100%;\n-  margin: 0px 0px 20px 0px;\n-\n-  /* align thumbnails on a grid */\n-  justify-content: space-between;\n-  display: grid;\n-  /* each grid column should be at least 160px (this will determine\n-  the actual number of columns) and then take as much of the\n-  remaining width as possible */\n-  grid-template-columns: repeat(auto-fill, minmax(160px, 1fr));\n-  gap: 15px;\n-}\n-.sphx-glr-thumbnails .toctree-wrapper {\n-  /* hide empty toctree divs added to the DOM\n-  by sphinx even though the toctree is hidden\n-  (they would fill grid places with empty divs) */\n-  display: none;\n-}\n-.sphx-glr-thumbcontainer {\n-  background: transparent;\n-  -moz-border-radius: 5px;\n-  -webkit-border-radius: 5px;\n-  border-radius: 5px;\n-  box-shadow: 0 0 10px var(--sg-thumb-box-shadow-color);\n-\n-  /* useful to absolutely position link in div */\n-  position: relative;\n-\n-  /* thumbnail width should include padding and borders\n-  and take all available space */\n-  box-sizing: border-box;\n-  width: 100%;\n-  padding: 10px;\n-  border: 1px solid transparent;\n-\n-  /* align content in thumbnail */\n-  display: flex;\n-  flex-direction: column;\n-  align-items: center;\n-  gap: 7px;\n-}\n-.sphx-glr-thumbcontainer p {\n-  position: absolute;\n-  top: 0;\n-  left: 0;\n-}\n-.sphx-glr-thumbcontainer p,\n-.sphx-glr-thumbcontainer p a {\n-  /* link should cover the whole thumbnail div */\n-  width: 100%;\n-  height: 100%;\n-}\n-.sphx-glr-thumbcontainer p a span {\n-  /* text within link should be masked\n-  (we are just interested in the href) */\n-  display: none;\n-}\n-.sphx-glr-thumbcontainer:hover {\n-  border: 1px solid;\n-  border-color: var(--sg-thumb-hover-border);\n-  cursor: pointer;\n-}\n-.sphx-glr-thumbcontainer a.internal {\n-  bottom: 0;\n-  display: block;\n-  left: 0;\n-  box-sizing: border-box;\n-  padding: 150px 10px 0;\n-  position: absolute;\n-  right: 0;\n-  top: 0;\n-}\n-/* Next one is to avoid Sphinx traditional theme to cover all the\n-thumbnail with its default link Background color */\n-.sphx-glr-thumbcontainer a.internal:hover {\n-  background-color: transparent;\n-}\n-\n-.sphx-glr-thumbcontainer p {\n-  margin: 0 0 0.1em 0;\n-}\n-.sphx-glr-thumbcontainer .figure {\n-  margin: 10px;\n-  width: 160px;\n-}\n-.sphx-glr-thumbcontainer img {\n-  display: inline;\n-  max-height: 112px;\n-  max-width: 160px;\n-}\n-.sphx-glr-thumbcontainer[tooltip]:hover:after {\n-  background: var(--sg-tooltip-background);\n-  -webkit-border-radius: 4px;\n-  -moz-border-radius: 4px;\n-  border-radius: 4px;\n-  color: var(--sg-tooltip-foreground);\n-  content: attr(tooltip);\n-  padding: 10px;\n-  z-index: 98;\n-  width: 100%;\n-  height: 100%;\n-  position: absolute;\n-  pointer-events: none;\n-  top: 0;\n-  box-sizing: border-box;\n-  overflow: hidden;\n-  backdrop-filter: blur(3px);\n-}\n-\n-.sphx-glr-script-out {\n-  color: var(--sg-script-out);\n-  display: flex;\n-  gap: 0.5em;\n-}\n-.sphx-glr-script-out::before {\n-  content: \"Out:\";\n-  /* These numbers come from the pre style in the pydata sphinx theme. This\n-   * turns out to match perfectly on the rtd theme, but be a bit too low for\n-   * the pydata sphinx theme. As I could not find a dimension to use that was\n-   * scaled the same way, I just picked one option that worked pretty close for\n-   * both. */\n-  line-height: 1.4;\n-  padding-top: 10px;\n-}\n-.sphx-glr-script-out .highlight {\n-  background-color: transparent;\n-  /* These options make the div expand... */\n-  flex-grow: 1;\n-  /* ... but also keep it from overflowing its flex container. */\n-  overflow: auto;\n-}\n-.sphx-glr-script-out .highlight pre {\n-  background-color: var(--sg-script-pre);\n-  border: 0;\n-  max-height: 30em;\n-  overflow: auto;\n-  padding-left: 1ex;\n-  /* This margin is necessary in the pydata sphinx theme because pre has a box\n-   * shadow which would be clipped by the overflow:auto in the parent div\n-   * above. */\n-  margin: 2px;\n-  word-break: break-word;\n-}\n-.sphx-glr-script-out + p {\n-  margin-top: 1.8em;\n-}\n-blockquote.sphx-glr-script-out {\n-  margin-left: 0pt;\n-}\n-.sphx-glr-script-out.highlight-pytb .highlight pre {\n-  color: var(--sg-pytb-foreground);\n-  background-color: var(--sg-pytb-background);\n-  border: 1px solid var(--sg-pytb-border-color);\n-  margin-top: 10px;\n-  padding: 7px;\n-}\n-\n-div.sphx-glr-footer {\n-  text-align: center;\n-}\n-\n-div.sphx-glr-download {\n-  margin: 1em auto;\n-  vertical-align: middle;\n-}\n-\n-div.sphx-glr-download a {\n-  background-color: var(--sg-download-a-background-color);\n-  background-image: var(--sg-download-a-background-image);\n-  border-radius: 4px;\n-  border: 1px solid var(--sg-download-a-border-color);\n-  color: var(--sg-download-a-color);\n-  display: inline-block;\n-  font-weight: bold;\n-  padding: 1ex;\n-  text-align: center;\n-}\n-\n-div.sphx-glr-download code.download {\n-  display: inline-block;\n-  white-space: normal;\n-  word-break: normal;\n-  overflow-wrap: break-word;\n-  /* border and background are given by the enclosing 'a' */\n-  border: none;\n-  background: none;\n-}\n-\n-div.sphx-glr-download a:hover {\n-  box-shadow: inset 0 1px 0 var(--sg-download-a-hover-box-shadow-1), 0 1px 5px var(--sg-download-a-hover-box-shadow-2);\n-  text-decoration: none;\n-  background-image: none;\n-  background-color: var(--sg-download-a-hover-background-color);\n-}\n-\n-.sphx-glr-example-title:target::before {\n-  display: block;\n-  content: \"\";\n-  margin-top: -50px;\n-  height: 50px;\n-  visibility: hidden;\n-}\n-\n-ul.sphx-glr-horizontal {\n-  list-style: none;\n-  padding: 0;\n-}\n-ul.sphx-glr-horizontal li {\n-  display: inline;\n-}\n-ul.sphx-glr-horizontal img {\n-  height: auto !important;\n-}\n-\n-.sphx-glr-single-img {\n-  margin: auto;\n-  display: block;\n-  max-width: 100%;\n-}\n-\n-.sphx-glr-multi-img {\n-  max-width: 42%;\n-  height: auto;\n-}\n-\n-div.sphx-glr-animation {\n-  margin: auto;\n-  display: block;\n-  max-width: 100%;\n-}\n-div.sphx-glr-animation .animation {\n-  display: block;\n-}\n-\n-p.sphx-glr-signature a.reference.external {\n-  -moz-border-radius: 5px;\n-  -webkit-border-radius: 5px;\n-  border-radius: 5px;\n-  padding: 3px;\n-  font-size: 75%;\n-  text-align: right;\n-  margin-left: auto;\n-  display: table;\n-}\n-\n-.sphx-glr-clear {\n-  clear: both;\n-}\n-\n-a.sphx-glr-backref-instance {\n-  text-decoration: none;\n-}"}, {"filename": "main/_static/sphinx_highlight.js", "status": "removed", "additions": 0, "deletions": 144, "changes": 144, "file_content_changes": "@@ -1,144 +0,0 @@\n-/* Highlighting utilities for Sphinx HTML documentation. */\n-\"use strict\";\n-\n-const SPHINX_HIGHLIGHT_ENABLED = true\n-\n-/**\n- * highlight a given string on a node by wrapping it in\n- * span elements with the given class name.\n- */\n-const _highlight = (node, addItems, text, className) => {\n-  if (node.nodeType === Node.TEXT_NODE) {\n-    const val = node.nodeValue;\n-    const parent = node.parentNode;\n-    const pos = val.toLowerCase().indexOf(text);\n-    if (\n-      pos >= 0 &&\n-      !parent.classList.contains(className) &&\n-      !parent.classList.contains(\"nohighlight\")\n-    ) {\n-      let span;\n-\n-      const closestNode = parent.closest(\"body, svg, foreignObject\");\n-      const isInSVG = closestNode && closestNode.matches(\"svg\");\n-      if (isInSVG) {\n-        span = document.createElementNS(\"http://www.w3.org/2000/svg\", \"tspan\");\n-      } else {\n-        span = document.createElement(\"span\");\n-        span.classList.add(className);\n-      }\n-\n-      span.appendChild(document.createTextNode(val.substr(pos, text.length)));\n-      parent.insertBefore(\n-        span,\n-        parent.insertBefore(\n-          document.createTextNode(val.substr(pos + text.length)),\n-          node.nextSibling\n-        )\n-      );\n-      node.nodeValue = val.substr(0, pos);\n-\n-      if (isInSVG) {\n-        const rect = document.createElementNS(\n-          \"http://www.w3.org/2000/svg\",\n-          \"rect\"\n-        );\n-        const bbox = parent.getBBox();\n-        rect.x.baseVal.value = bbox.x;\n-        rect.y.baseVal.value = bbox.y;\n-        rect.width.baseVal.value = bbox.width;\n-        rect.height.baseVal.value = bbox.height;\n-        rect.setAttribute(\"class\", className);\n-        addItems.push({ parent: parent, target: rect });\n-      }\n-    }\n-  } else if (node.matches && !node.matches(\"button, select, textarea\")) {\n-    node.childNodes.forEach((el) => _highlight(el, addItems, text, className));\n-  }\n-};\n-const _highlightText = (thisNode, text, className) => {\n-  let addItems = [];\n-  _highlight(thisNode, addItems, text, className);\n-  addItems.forEach((obj) =>\n-    obj.parent.insertAdjacentElement(\"beforebegin\", obj.target)\n-  );\n-};\n-\n-/**\n- * Small JavaScript module for the documentation.\n- */\n-const SphinxHighlight = {\n-\n-  /**\n-   * highlight the search words provided in localstorage in the text\n-   */\n-  highlightSearchWords: () => {\n-    if (!SPHINX_HIGHLIGHT_ENABLED) return;  // bail if no highlight\n-\n-    // get and clear terms from localstorage\n-    const url = new URL(window.location);\n-    const highlight =\n-        localStorage.getItem(\"sphinx_highlight_terms\")\n-        || url.searchParams.get(\"highlight\")\n-        || \"\";\n-    localStorage.removeItem(\"sphinx_highlight_terms\")\n-    url.searchParams.delete(\"highlight\");\n-    window.history.replaceState({}, \"\", url);\n-\n-    // get individual terms from highlight string\n-    const terms = highlight.toLowerCase().split(/\\s+/).filter(x => x);\n-    if (terms.length === 0) return; // nothing to do\n-\n-    // There should never be more than one element matching \"div.body\"\n-    const divBody = document.querySelectorAll(\"div.body\");\n-    const body = divBody.length ? divBody[0] : document.querySelector(\"body\");\n-    window.setTimeout(() => {\n-      terms.forEach((term) => _highlightText(body, term, \"highlighted\"));\n-    }, 10);\n-\n-    const searchBox = document.getElementById(\"searchbox\");\n-    if (searchBox === null) return;\n-    searchBox.appendChild(\n-      document\n-        .createRange()\n-        .createContextualFragment(\n-          '<p class=\"highlight-link\">' +\n-            '<a href=\"javascript:SphinxHighlight.hideSearchWords()\">' +\n-            _(\"Hide Search Matches\") +\n-            \"</a></p>\"\n-        )\n-    );\n-  },\n-\n-  /**\n-   * helper function to hide the search marks again\n-   */\n-  hideSearchWords: () => {\n-    document\n-      .querySelectorAll(\"#searchbox .highlight-link\")\n-      .forEach((el) => el.remove());\n-    document\n-      .querySelectorAll(\"span.highlighted\")\n-      .forEach((el) => el.classList.remove(\"highlighted\"));\n-    localStorage.removeItem(\"sphinx_highlight_terms\")\n-  },\n-\n-  initEscapeListener: () => {\n-    // only install a listener if it is really needed\n-    if (!DOCUMENTATION_OPTIONS.ENABLE_SEARCH_SHORTCUTS) return;\n-\n-    document.addEventListener(\"keydown\", (event) => {\n-      // bail for input elements\n-      if (BLACKLISTED_KEY_CONTROL_ELEMENTS.has(document.activeElement.tagName)) return;\n-      // bail with special keys\n-      if (event.shiftKey || event.altKey || event.ctrlKey || event.metaKey) return;\n-      if (DOCUMENTATION_OPTIONS.ENABLE_SEARCH_SHORTCUTS && (event.key === \"Escape\")) {\n-        SphinxHighlight.hideSearchWords();\n-        event.preventDefault();\n-      }\n-    });\n-  },\n-};\n-\n-_ready(SphinxHighlight.highlightSearchWords);\n-_ready(SphinxHighlight.initEscapeListener);"}, {"filename": "main/genindex.html", "status": "removed", "additions": 0, "deletions": 438, "changes": 438, "file_content_changes": "@@ -1,438 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" />\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>Index &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"./\" id=\"documentation_options\" src=\"_static/documentation_options.js\"></script>\n-        <script src=\"_static/doctools.js\"></script>\n-        <script src=\"_static/sphinx_highlight.js\"></script>\n-    <script src=\"_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"#\" />\n-    <link rel=\"search\" title=\"Search\" href=\"search.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"getting-started/installation.html\">Installation</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"getting-started/tutorials/index.html\">Tutorials</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"python-api/triton.html\">triton</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"python-api/triton.language.html\">triton.language</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"python-api/triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-      <li class=\"breadcrumb-item active\">Index</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-\n-<h1 id=\"index\">Index</h1>\n-\n-<div class=\"genindex-jumpbox\">\n- <a href=\"#_\"><strong>_</strong></a>\n- | <a href=\"#A\"><strong>A</strong></a>\n- | <a href=\"#B\"><strong>B</strong></a>\n- | <a href=\"#C\"><strong>C</strong></a>\n- | <a href=\"#D\"><strong>D</strong></a>\n- | <a href=\"#E\"><strong>E</strong></a>\n- | <a href=\"#F\"><strong>F</strong></a>\n- | <a href=\"#H\"><strong>H</strong></a>\n- | <a href=\"#J\"><strong>J</strong></a>\n- | <a href=\"#L\"><strong>L</strong></a>\n- | <a href=\"#M\"><strong>M</strong></a>\n- | <a href=\"#N\"><strong>N</strong></a>\n- | <a href=\"#P\"><strong>P</strong></a>\n- | <a href=\"#R\"><strong>R</strong></a>\n- | <a href=\"#S\"><strong>S</strong></a>\n- | <a href=\"#T\"><strong>T</strong></a>\n- | <a href=\"#U\"><strong>U</strong></a>\n- | <a href=\"#V\"><strong>V</strong></a>\n- | <a href=\"#W\"><strong>W</strong></a>\n- | <a href=\"#X\"><strong>X</strong></a>\n- | <a href=\"#Z\"><strong>Z</strong></a>\n- \n-</div>\n-<h2 id=\"_\">_</h2>\n-<table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.Config.html#triton.Config.__init__\">__init__() (triton.Config method)</a>\n-\n-      <ul>\n-        <li><a href=\"python-api/generated/triton.language.static_range.html#triton.language.static_range.__init__\">(triton.language.static_range method)</a>\n-</li>\n-        <li><a href=\"python-api/generated/triton.testing.Benchmark.html#triton.testing.Benchmark.__init__\">(triton.testing.Benchmark method)</a>\n-</li>\n-      </ul></li>\n-  </ul></td>\n-</tr></table>\n-\n-<h2 id=\"A\">A</h2>\n-<table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.abs.html#triton.language.abs\">abs() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.arange.html#triton.language.arange\">arange() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.argmax.html#triton.language.argmax\">argmax() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.argmin.html#triton.language.argmin\">argmin() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.associative_scan.html#triton.language.associative_scan\">associative_scan() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.atomic_add.html#triton.language.atomic_add\">atomic_add() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.atomic_cas.html#triton.language.atomic_cas\">atomic_cas() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.atomic_max.html#triton.language.atomic_max\">atomic_max() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.atomic_min.html#triton.language.atomic_min\">atomic_min() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.atomic_xchg.html#triton.language.atomic_xchg\">atomic_xchg() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.autotune.html#triton.autotune\">autotune() (in module triton)</a>\n-</li>\n-  </ul></td>\n-</tr></table>\n-\n-<h2 id=\"B\">B</h2>\n-<table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.testing.Benchmark.html#triton.testing.Benchmark\">Benchmark (class in triton.testing)</a>\n-</li>\n-  </ul></td>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.broadcast.html#triton.language.broadcast\">broadcast() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.broadcast_to.html#triton.language.broadcast_to\">broadcast_to() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-</tr></table>\n-\n-<h2 id=\"C\">C</h2>\n-<table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.cat.html#triton.language.cat\">cat() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.Config.html#triton.Config\">Config (class in triton)</a>\n-</li>\n-  </ul></td>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.cos.html#triton.language.cos\">cos() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.cumprod.html#triton.language.cumprod\">cumprod() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.cumsum.html#triton.language.cumsum\">cumsum() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-</tr></table>\n-\n-<h2 id=\"D\">D</h2>\n-<table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.debug_barrier.html#triton.language.debug_barrier\">debug_barrier() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.device_assert.html#triton.language.device_assert\">device_assert() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.device_print.html#triton.language.device_print\">device_print() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.testing.do_bench.html#triton.testing.do_bench\">do_bench() (in module triton.testing)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.dot.html#triton.language.dot\">dot() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-</tr></table>\n-\n-<h2 id=\"E\">E</h2>\n-<table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.exp.html#triton.language.exp\">exp() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.expand_dims.html#triton.language.expand_dims\">expand_dims() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-</tr></table>\n-\n-<h2 id=\"F\">F</h2>\n-<table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.fdiv.html#triton.language.fdiv\">fdiv() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.full.html#triton.language.full\">full() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-</tr></table>\n-\n-<h2 id=\"H\">H</h2>\n-<table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.heuristics.html#triton.heuristics\">heuristics() (in module triton)</a>\n-</li>\n-  </ul></td>\n-</tr></table>\n-\n-<h2 id=\"J\">J</h2>\n-<table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.jit.html#triton.jit\">jit() (in module triton)</a>\n-</li>\n-  </ul></td>\n-</tr></table>\n-\n-<h2 id=\"L\">L</h2>\n-<table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.load.html#triton.language.load\">load() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.log.html#triton.language.log\">log() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-</tr></table>\n-\n-<h2 id=\"M\">M</h2>\n-<table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.max.html#triton.language.max\">max() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.max_constancy.html#triton.language.max_constancy\">max_constancy() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.max_contiguous.html#triton.language.max_contiguous\">max_contiguous() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.maximum.html#triton.language.maximum\">maximum() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.min.html#triton.language.min\">min() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.minimum.html#triton.language.minimum\">minimum() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.multiple_of.html#triton.language.multiple_of\">multiple_of() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-</tr></table>\n-\n-<h2 id=\"N\">N</h2>\n-<table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.num_programs.html#triton.language.num_programs\">num_programs() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-</tr></table>\n-\n-<h2 id=\"P\">P</h2>\n-<table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.testing.perf_report.html#triton.testing.perf_report\">perf_report() (in module triton.testing)</a>\n-</li>\n-  </ul></td>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.program_id.html#triton.language.program_id\">program_id() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-</tr></table>\n-\n-<h2 id=\"R\">R</h2>\n-<table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.rand.html#triton.language.rand\">rand() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.randint.html#triton.language.randint\">randint() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.randint4x.html#triton.language.randint4x\">randint4x() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.randn.html#triton.language.randn\">randn() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.ravel.html#triton.language.ravel\">ravel() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.reduce.html#triton.language.reduce\">reduce() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.reshape.html#triton.language.reshape\">reshape() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-</tr></table>\n-\n-<h2 id=\"S\">S</h2>\n-<table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.sigmoid.html#triton.language.sigmoid\">sigmoid() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.sin.html#triton.language.sin\">sin() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.softmax.html#triton.language.softmax\">softmax() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.sqrt.html#triton.language.sqrt\">sqrt() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.static_assert.html#triton.language.static_assert\">static_assert() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.static_print.html#triton.language.static_print\">static_print() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.static_range.html#triton.language.static_range\">static_range (class in triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.store.html#triton.language.store\">store() (in module triton.language)</a>\n-</li>\n-      <li><a href=\"python-api/generated/triton.language.sum.html#triton.language.sum\">sum() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-</tr></table>\n-\n-<h2 id=\"T\">T</h2>\n-<table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.trans.html#triton.language.trans\">trans() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-</tr></table>\n-\n-<h2 id=\"U\">U</h2>\n-<table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.umulhi.html#triton.language.umulhi\">umulhi() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-</tr></table>\n-\n-<h2 id=\"V\">V</h2>\n-<table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.view.html#triton.language.view\">view() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-</tr></table>\n-\n-<h2 id=\"W\">W</h2>\n-<table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.where.html#triton.language.where\">where() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-</tr></table>\n-\n-<h2 id=\"X\">X</h2>\n-<table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.xor_sum.html#triton.language.xor_sum\">xor_sum() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-</tr></table>\n-\n-<h2 id=\"Z\">Z</h2>\n-<table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n-  <td style=\"width: 33%; vertical-align: top;\"><ul>\n-      <li><a href=\"python-api/generated/triton.language.zeros.html#triton.language.zeros\">zeros() (in module triton.language)</a>\n-</li>\n-  </ul></td>\n-</tr></table>\n-\n-\n-\n-           </div>\n-          </div>\n-          <footer>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"genindex.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/getting-started/installation.html", "status": "removed", "additions": 0, "deletions": 180, "changes": 180, "file_content_changes": "@@ -1,180 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>Installation &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../\" id=\"documentation_options\" src=\"../_static/documentation_options.js\"></script>\n-        <script src=\"../_static/doctools.js\"></script>\n-        <script src=\"../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../search.html\" />\n-    <link rel=\"next\" title=\"Tutorials\" href=\"tutorials/index.html\" />\n-    <link rel=\"prev\" title=\"Welcome to Triton\u2019s documentation!\" href=\"../index.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1 current\"><a class=\"current reference internal\" href=\"#\">Installation</a><ul>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"#binary-distributions\">Binary Distributions</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"#from-source\">From Source</a><ul>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#python-package\">Python Package</a></li>\n-</ul>\n-</li>\n-</ul>\n-</li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/index.html\">Tutorials</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../python-api/triton.html\">triton</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../python-api/triton.language.html\">triton.language</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../python-api/triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-      <li class=\"breadcrumb-item active\">Installation</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../_sources/getting-started/installation.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <section id=\"installation\">\n-<h1>Installation<a class=\"headerlink\" href=\"#installation\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<section id=\"binary-distributions\">\n-<h2>Binary Distributions<a class=\"headerlink\" href=\"#binary-distributions\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>You can install the latest stable release of Triton from pip:</p>\n-<div class=\"highlight-bash notranslate\"><div class=\"highlight\"><pre><span></span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span>triton\n-</pre></div>\n-</div>\n-<p>Binary wheels are available for CPython 3.7-3.11 and PyPy 3.8-3.9.</p>\n-<p>And the latest nightly release:</p>\n-<div class=\"highlight-bash notranslate\"><div class=\"highlight\"><pre><span></span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span>-U<span class=\"w\"> </span>--index-url<span class=\"w\"> </span>https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/<span class=\"w\"> </span>triton-nightly\n-</pre></div>\n-</div>\n-</section>\n-<section id=\"from-source\">\n-<h2>From Source<a class=\"headerlink\" href=\"#from-source\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<section id=\"python-package\">\n-<h3>Python Package<a class=\"headerlink\" href=\"#python-package\" title=\"Permalink to this heading\">\u00b6</a></h3>\n-<p>You can install the Python package from source by running the following commands:</p>\n-<div class=\"highlight-bash notranslate\"><div class=\"highlight\"><pre><span></span>git<span class=\"w\"> </span>clone<span class=\"w\"> </span>https://github.com/openai/triton.git<span class=\"p\">;</span>\n-<span class=\"nb\">cd</span><span class=\"w\"> </span>triton/python<span class=\"p\">;</span>\n-pip<span class=\"w\"> </span>install<span class=\"w\"> </span>cmake<span class=\"p\">;</span><span class=\"w\"> </span><span class=\"c1\"># build-time dependency</span>\n-pip<span class=\"w\"> </span>install<span class=\"w\"> </span>-e<span class=\"w\"> </span>.\n-</pre></div>\n-</div>\n-<p>Note that, if llvm-11 is not present on your system, the setup.py script will download the official LLVM11 static libraries link against that.</p>\n-<p>You can then test your installation by running the unit tests:</p>\n-<div class=\"highlight-bash notranslate\"><div class=\"highlight\"><pre><span></span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span>-e<span class=\"w\"> </span><span class=\"s1\">&#39;.[tests]&#39;</span>\n-pytest<span class=\"w\"> </span>-vs<span class=\"w\"> </span>test/unit/\n-</pre></div>\n-</div>\n-<p>and the benchmarks</p>\n-<div class=\"highlight-bash notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nb\">cd</span><span class=\"w\"> </span>bench\n-python<span class=\"w\"> </span>-m<span class=\"w\"> </span>run<span class=\"w\"> </span>--with-plots<span class=\"w\"> </span>--result-dir<span class=\"w\"> </span>/tmp/triton-bench\n-</pre></div>\n-</div>\n-</section>\n-</section>\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"../index.html\" class=\"btn btn-neutral float-left\" title=\"Welcome to Triton\u2019s documentation!\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"tutorials/index.html\" class=\"btn btn-neutral float-right\" title=\"Tutorials\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"installation.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/getting-started/tutorials/01-vector-add.html", "status": "removed", "additions": 0, "deletions": 313, "changes": 313, "file_content_changes": "@@ -1,313 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>Vector Addition &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"Fused Softmax\" href=\"02-fused-softmax.html\" />\n-    <link rel=\"prev\" title=\"Tutorials\" href=\"index.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../installation.html\">Installation</a></li>\n-<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"index.html\">Tutorials</a><ul class=\"current\">\n-<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Vector Addition</a><ul>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#compute-kernel\">Compute Kernel</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#benchmark\">Benchmark</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"02-fused-softmax.html\">Fused Softmax</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"08-experimental-block-pointer.html\">Block Pointer (Experimental)</a></li>\n-</ul>\n-</li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-          <li class=\"breadcrumb-item\"><a href=\"index.html\">Tutorials</a></li>\n-      <li class=\"breadcrumb-item active\">Vector Addition</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/getting-started/tutorials/01-vector-add.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <div class=\"sphx-glr-download-link-note admonition note\">\n-<p class=\"admonition-title\">Note</p>\n-<p><a class=\"reference internal\" href=\"#sphx-glr-download-getting-started-tutorials-01-vector-add-py\"><span class=\"std std-ref\">Go to the end</span></a>\n-to download the full example code</p>\n-</div>\n-<section class=\"sphx-glr-example-title\" id=\"vector-addition\">\n-<span id=\"sphx-glr-getting-started-tutorials-01-vector-add-py\"></span><h1>Vector Addition<a class=\"headerlink\" href=\"#vector-addition\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<p>In this tutorial, you will write a simple vector addition using Triton.</p>\n-<p>In doing so, you will learn about:</p>\n-<ul class=\"simple\">\n-<li><p>The basic programming model of Triton.</p></li>\n-<li><p>The <cite>triton.jit</cite> decorator, which is used to define Triton kernels.</p></li>\n-<li><p>The best practices for validating and benchmarking your custom ops against native reference implementations.</p></li>\n-</ul>\n-<section id=\"compute-kernel\">\n-<h2>Compute Kernel<a class=\"headerlink\" href=\"#compute-kernel\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n-\n-<span class=\"kn\">import</span> <span class=\"nn\">triton</span>\n-<span class=\"kn\">import</span> <span class=\"nn\">triton.language</span> <span class=\"k\">as</span> <span class=\"nn\">tl</span>\n-\n-\n-<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n-<span class=\"k\">def</span> <span class=\"nf\">add_kernel</span><span class=\"p\">(</span>\n-    <span class=\"n\">x_ptr</span><span class=\"p\">,</span>  <span class=\"c1\"># *Pointer* to first input vector.</span>\n-    <span class=\"n\">y_ptr</span><span class=\"p\">,</span>  <span class=\"c1\"># *Pointer* to second input vector.</span>\n-    <span class=\"n\">output_ptr</span><span class=\"p\">,</span>  <span class=\"c1\"># *Pointer* to output vector.</span>\n-    <span class=\"n\">n_elements</span><span class=\"p\">,</span>  <span class=\"c1\"># Size of the vector.</span>\n-    <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>  <span class=\"c1\"># Number of elements each program should process.</span>\n-                 <span class=\"c1\"># NOTE: `constexpr` so it can be used as a shape value.</span>\n-<span class=\"p\">):</span>\n-    <span class=\"c1\"># There are multiple &#39;programs&#39; processing different data. We identify which program</span>\n-    <span class=\"c1\"># we are here:</span>\n-    <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>  <span class=\"c1\"># We use a 1D launch grid so axis is 0.</span>\n-    <span class=\"c1\"># This program will process inputs that are offset from the initial data.</span>\n-    <span class=\"c1\"># For instance, if you had a vector of length 256 and block_size of 64, the programs</span>\n-    <span class=\"c1\"># would each access the elements [0:64, 64:128, 128:192, 192:256].</span>\n-    <span class=\"c1\"># Note that offsets is a list of pointers:</span>\n-    <span class=\"n\">block_start</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE</span>\n-    <span class=\"n\">offsets</span> <span class=\"o\">=</span> <span class=\"n\">block_start</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># Create a mask to guard memory operations against out-of-bounds accesses.</span>\n-    <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">offsets</span> <span class=\"o\">&lt;</span> <span class=\"n\">n_elements</span>\n-    <span class=\"c1\"># Load x and y from DRAM, masking out any extra elements in case the input is not a</span>\n-    <span class=\"c1\"># multiple of the block size.</span>\n-    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">x_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n-    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">y_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n-    <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"n\">y</span>\n-    <span class=\"c1\"># Write x + y back to DRAM.</span>\n-    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">output_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n-</pre></div>\n-</div>\n-<p>Let\u2019s also declare a helper function to (1) allocate the <cite>z</cite> tensor\n-and (2) enqueue the above kernel with appropriate grid/block sizes:</p>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">add</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">):</span>\n-    <span class=\"c1\"># We need to preallocate the output.</span>\n-    <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n-    <span class=\"k\">assert</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">is_cuda</span> <span class=\"ow\">and</span> <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">is_cuda</span> <span class=\"ow\">and</span> <span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">is_cuda</span>\n-    <span class=\"n\">n_elements</span> <span class=\"o\">=</span> <span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">numel</span><span class=\"p\">()</span>\n-    <span class=\"c1\"># The SPMD launch grid denotes the number of kernel instances that run in parallel.</span>\n-    <span class=\"c1\"># It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -&gt; Tuple[int].</span>\n-    <span class=\"c1\"># In this case, we use a 1D grid where the size is the number of blocks:</span>\n-    <span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">meta</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">meta</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">]),)</span>\n-    <span class=\"c1\"># NOTE:</span>\n-    <span class=\"c1\">#  - Each torch.tensor object is implicitly converted into a pointer to its first element.</span>\n-    <span class=\"c1\">#  - `triton.jit`&#39;ed functions can be indexed with a launch grid to obtain a callable GPU kernel.</span>\n-    <span class=\"c1\">#  - Don&#39;t forget to pass meta-parameters as keywords arguments.</span>\n-    <span class=\"n\">add_kernel</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"o\">=</span><span class=\"mi\">1024</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># We return a handle to z but, since `torch.cuda.synchronize()` hasn&#39;t been called, the kernel is still</span>\n-    <span class=\"c1\"># running asynchronously at this point.</span>\n-    <span class=\"k\">return</span> <span class=\"n\">output</span>\n-</pre></div>\n-</div>\n-<p>We can now use the above function to compute the element-wise sum of two <cite>torch.tensor</cite> objects and test its correctness:</p>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-<span class=\"n\">size</span> <span class=\"o\">=</span> <span class=\"mi\">98432</span>\n-<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n-<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n-<span class=\"n\">output_torch</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"n\">y</span>\n-<span class=\"n\">output_triton</span> <span class=\"o\">=</span> <span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n-<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">output_torch</span><span class=\"p\">)</span>\n-<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">output_triton</span><span class=\"p\">)</span>\n-<span class=\"nb\">print</span><span class=\"p\">(</span>\n-    <span class=\"sa\">f</span><span class=\"s1\">&#39;The maximum difference between torch and triton is &#39;</span>\n-    <span class=\"sa\">f</span><span class=\"s1\">&#39;</span><span class=\"si\">{</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">abs</span><span class=\"p\">(</span><span class=\"n\">output_torch</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"w\"> </span><span class=\"n\">output_triton</span><span class=\"p\">))</span><span class=\"si\">}</span><span class=\"s1\">&#39;</span>\n-<span class=\"p\">)</span>\n-</pre></div>\n-</div>\n-<div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>tensor([1.3713, 1.3076, 0.4940,  ..., 0.6724, 1.2141, 0.9733], device=&#39;cuda:0&#39;)\n-tensor([1.3713, 1.3076, 0.4940,  ..., 0.6724, 1.2141, 0.9733], device=&#39;cuda:0&#39;)\n-The maximum difference between torch and triton is 0.0\n-</pre></div>\n-</div>\n-<p>Seems like we\u2019re good to go!</p>\n-</section>\n-<section id=\"benchmark\">\n-<h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>We can now benchmark our custom op on vectors of increasing sizes to get a sense of how it does relative to PyTorch.\n-To make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of our custom ops.\n-for different problem sizes.</p>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">perf_report</span><span class=\"p\">(</span>\n-    <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">Benchmark</span><span class=\"p\">(</span>\n-        <span class=\"n\">x_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;size&#39;</span><span class=\"p\">],</span>  <span class=\"c1\"># Argument names to use as an x-axis for the plot.</span>\n-        <span class=\"n\">x_vals</span><span class=\"o\">=</span><span class=\"p\">[</span>\n-            <span class=\"mi\">2</span> <span class=\"o\">**</span> <span class=\"n\">i</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">12</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n-        <span class=\"p\">],</span>  <span class=\"c1\"># Different possible values for `x_name`.</span>\n-        <span class=\"n\">x_log</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>  <span class=\"c1\"># x axis is logarithmic.</span>\n-        <span class=\"n\">line_arg</span><span class=\"o\">=</span><span class=\"s1\">&#39;provider&#39;</span><span class=\"p\">,</span>  <span class=\"c1\"># Argument name whose value corresponds to a different line in the plot.</span>\n-        <span class=\"n\">line_vals</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;torch&#39;</span><span class=\"p\">],</span>  <span class=\"c1\"># Possible values for `line_arg`.</span>\n-        <span class=\"n\">line_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;Triton&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;Torch&#39;</span><span class=\"p\">],</span>  <span class=\"c1\"># Label name for the lines.</span>\n-        <span class=\"n\">styles</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s1\">&#39;blue&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;green&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">)],</span>  <span class=\"c1\"># Line styles.</span>\n-        <span class=\"n\">ylabel</span><span class=\"o\">=</span><span class=\"s1\">&#39;GB/s&#39;</span><span class=\"p\">,</span>  <span class=\"c1\"># Label name for the y-axis.</span>\n-        <span class=\"n\">plot_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;vector-add-performance&#39;</span><span class=\"p\">,</span>  <span class=\"c1\"># Name for the plot. Used also as a file name for saving the plot.</span>\n-        <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{},</span>  <span class=\"c1\"># Values for function arguments not in `x_names` and `y_name`.</span>\n-    <span class=\"p\">)</span>\n-<span class=\"p\">)</span>\n-<span class=\"k\">def</span> <span class=\"nf\">benchmark</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">provider</span><span class=\"p\">):</span>\n-    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n-    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n-    <span class=\"n\">quantiles</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">]</span>\n-    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;torch&#39;</span><span class=\"p\">:</span>\n-        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">)</span>\n-    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">:</span>\n-        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">),</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">)</span>\n-    <span class=\"n\">gbps</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">ms</span><span class=\"p\">:</span> <span class=\"mi\">12</span> <span class=\"o\">*</span> <span class=\"n\">size</span> <span class=\"o\">/</span> <span class=\"n\">ms</span> <span class=\"o\">*</span> <span class=\"mf\">1e-6</span>\n-    <span class=\"k\">return</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">ms</span><span class=\"p\">),</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">max_ms</span><span class=\"p\">),</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">min_ms</span><span class=\"p\">)</span>\n-</pre></div>\n-</div>\n-<p>We can now run the decorated function above. Pass <cite>print_data=True</cite> to see the performance number, <cite>show_plots=True</cite> to plot them, and/or\n-<a href=\"#id1\"><span class=\"problematic\" id=\"id2\">`</span></a>save_path=\u2019/path/to/results/\u2019 to save them to disk along with raw CSV data:</p>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">benchmark</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">print_data</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">show_plots</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n-</pre></div>\n-</div>\n-<img src=\"../../_images/sphx_glr_01-vector-add_001.png\" srcset=\"../../_images/sphx_glr_01-vector-add_001.png\" alt=\"01 vector add\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>vector-add-performance:\n-           size       Triton        Torch\n-0        4096.0     8.000000     9.600000\n-1        8192.0    15.999999    15.999999\n-2       16384.0    31.999999    31.999999\n-3       32768.0    63.999998    63.999998\n-4       65536.0   127.999995   127.999995\n-5      131072.0   219.428568   219.428568\n-6      262144.0   384.000001   384.000001\n-7      524288.0   614.400016   614.400016\n-8     1048576.0   819.200021   819.200021\n-9     2097152.0  1023.999964  1023.999964\n-10    4194304.0  1228.800031  1228.800031\n-11    8388608.0  1424.695621  1424.695621\n-12   16777216.0  1560.380965  1560.380965\n-13   33554432.0  1631.601649  1624.859540\n-14   67108864.0  1669.706983  1662.646960\n-15  134217728.0  1684.008546  1678.616907\n-</pre></div>\n-</div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  5.608 seconds)</p>\n-<div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-01-vector-add-py\">\n-<div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n-<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/62d97d49a32414049819dd8bb8378080/01-vector-add.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">01-vector-add.py</span></code></a></p>\n-</div>\n-<div class=\"sphx-glr-download sphx-glr-download-jupyter docutils container\">\n-<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/f191ee1e78dc52eb5f7cba88f71cef2f/01-vector-add.ipynb\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Jupyter</span> <span class=\"pre\">notebook:</span> <span class=\"pre\">01-vector-add.ipynb</span></code></a></p>\n-</div>\n-</div>\n-<p class=\"sphx-glr-signature\"><a class=\"reference external\" href=\"https://sphinx-gallery.github.io\">Gallery generated by Sphinx-Gallery</a></p>\n-</section>\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"index.html\" class=\"btn btn-neutral float-left\" title=\"Tutorials\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"02-fused-softmax.html\" class=\"btn btn-neutral float-right\" title=\"Fused Softmax\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"01-vector-add.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/getting-started/tutorials/02-fused-softmax.html", "status": "removed", "additions": 0, "deletions": 365, "changes": 365, "file_content_changes": "@@ -1,365 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>Fused Softmax &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-        <script async=\"async\" src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"Matrix Multiplication\" href=\"03-matrix-multiplication.html\" />\n-    <link rel=\"prev\" title=\"Vector Addition\" href=\"01-vector-add.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../installation.html\">Installation</a></li>\n-<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"index.html\">Tutorials</a><ul class=\"current\">\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"01-vector-add.html\">Vector Addition</a></li>\n-<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Fused Softmax</a><ul>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#motivations\">Motivations</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#compute-kernel\">Compute Kernel</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#unit-test\">Unit Test</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#benchmark\">Benchmark</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"08-experimental-block-pointer.html\">Block Pointer (Experimental)</a></li>\n-</ul>\n-</li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-          <li class=\"breadcrumb-item\"><a href=\"index.html\">Tutorials</a></li>\n-      <li class=\"breadcrumb-item active\">Fused Softmax</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/getting-started/tutorials/02-fused-softmax.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <div class=\"sphx-glr-download-link-note admonition note\">\n-<p class=\"admonition-title\">Note</p>\n-<p><a class=\"reference internal\" href=\"#sphx-glr-download-getting-started-tutorials-02-fused-softmax-py\"><span class=\"std std-ref\">Go to the end</span></a>\n-to download the full example code</p>\n-</div>\n-<section class=\"sphx-glr-example-title\" id=\"fused-softmax\">\n-<span id=\"sphx-glr-getting-started-tutorials-02-fused-softmax-py\"></span><h1>Fused Softmax<a class=\"headerlink\" href=\"#fused-softmax\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<p>In this tutorial, you will write a fused softmax operation that is significantly faster\n-than PyTorch\u2019s native op for a particular class of matrices: those whose rows can fit in\n-the GPU\u2019s SRAM.</p>\n-<p>In doing so, you will learn about:</p>\n-<ul class=\"simple\">\n-<li><p>The benefits of kernel fusion for bandwidth-bound operations.</p></li>\n-<li><p>Reduction operators in Triton.</p></li>\n-</ul>\n-<section id=\"motivations\">\n-<h2>Motivations<a class=\"headerlink\" href=\"#motivations\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>Custom GPU kernels for elementwise additions are educationally valuable but won\u2019t get you very far in practice.\n-Let us consider instead the case of a simple (numerically stabilized) softmax operation:</p>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n-\n-<span class=\"kn\">import</span> <span class=\"nn\">triton</span>\n-<span class=\"kn\">import</span> <span class=\"nn\">triton.language</span> <span class=\"k\">as</span> <span class=\"nn\">tl</span>\n-\n-\n-<span class=\"nd\">@torch</span><span class=\"o\">.</span><span class=\"n\">jit</span><span class=\"o\">.</span><span class=\"n\">script</span>\n-<span class=\"k\">def</span> <span class=\"nf\">naive_softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n-<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Compute row-wise softmax of X using native pytorch</span>\n-\n-<span class=\"sd\">    We subtract the maximum element in order to avoid overflows. Softmax is invariant to</span>\n-<span class=\"sd\">    this shift.</span>\n-<span class=\"sd\">    &quot;&quot;&quot;</span>\n-    <span class=\"c1\"># read  MN elements ; write M  elements</span>\n-    <span class=\"n\">x_max</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n-    <span class=\"c1\"># read MN + M elements ; write MN elements</span>\n-    <span class=\"n\">z</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">x_max</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n-    <span class=\"c1\"># read  MN elements ; write MN elements</span>\n-    <span class=\"n\">numerator</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">z</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># read  MN elements ; write M  elements</span>\n-    <span class=\"n\">denominator</span> <span class=\"o\">=</span> <span class=\"n\">numerator</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># read MN + M elements ; write MN elements</span>\n-    <span class=\"n\">ret</span> <span class=\"o\">=</span> <span class=\"n\">numerator</span> <span class=\"o\">/</span> <span class=\"n\">denominator</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n-    <span class=\"c1\"># in total: read 5MN + 2M elements ; wrote 3MN + 2M elements</span>\n-    <span class=\"k\">return</span> <span class=\"n\">ret</span>\n-</pre></div>\n-</div>\n-<p>When implemented naively in PyTorch, computing <code class=\"code docutils literal notranslate\"><span class=\"pre\">y</span> <span class=\"pre\">=</span> <span class=\"pre\">naive_softmax(x)</span></code> for <span class=\"math notranslate nohighlight\">\\(x \\in R^{M \\times N}\\)</span>\n-requires reading <span class=\"math notranslate nohighlight\">\\(5MN + 2M\\)</span> elements from DRAM and writing back <span class=\"math notranslate nohighlight\">\\(3MN + 2M\\)</span> elements.\n-This is obviously wasteful; we\u2019d prefer to have a custom \u201cfused\u201d kernel that only reads\n-X once and does all the necessary computations on-chip.\n-Doing so would require reading and writing back only <span class=\"math notranslate nohighlight\">\\(MN\\)</span> bytes, so we could\n-expect a theoretical speed-up of ~4x (i.e., <span class=\"math notranslate nohighlight\">\\((8MN + 4M) / 2MN\\)</span>).\n-The <cite>torch.jit.script</cite> flags aims to perform this kind of \u201ckernel fusion\u201d automatically\n-but, as we will see later, it is still far from ideal.</p>\n-</section>\n-<section id=\"compute-kernel\">\n-<h2>Compute Kernel<a class=\"headerlink\" href=\"#compute-kernel\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>Our softmax kernel works as follows: each program loads a row of the input matrix X,\n-normalizes it and writes back the result to the output Y.</p>\n-<p>Note that one important limitation of Triton is that each block must have a\n-power-of-two number of elements, so we need to internally \u201cpad\u201d each row and guard the\n-memory operations properly if we want to handle any possible input shapes:</p>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n-<span class=\"k\">def</span> <span class=\"nf\">softmax_kernel</span><span class=\"p\">(</span>\n-    <span class=\"n\">output_ptr</span><span class=\"p\">,</span> <span class=\"n\">input_ptr</span><span class=\"p\">,</span> <span class=\"n\">input_row_stride</span><span class=\"p\">,</span> <span class=\"n\">output_row_stride</span><span class=\"p\">,</span> <span class=\"n\">n_cols</span><span class=\"p\">,</span>\n-    <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span>\n-<span class=\"p\">):</span>\n-    <span class=\"c1\"># The rows of the softmax are independent, so we parallelize across those</span>\n-    <span class=\"n\">row_idx</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># The stride represents how much we need to increase the pointer to advance 1 row</span>\n-    <span class=\"n\">row_start_ptr</span> <span class=\"o\">=</span> <span class=\"n\">input_ptr</span> <span class=\"o\">+</span> <span class=\"n\">row_idx</span> <span class=\"o\">*</span> <span class=\"n\">input_row_stride</span>\n-    <span class=\"c1\"># The block size is the next power of two greater than n_cols, so we can fit each</span>\n-    <span class=\"c1\"># row in a single block</span>\n-    <span class=\"n\">col_offsets</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">)</span>\n-    <span class=\"n\">input_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">row_start_ptr</span> <span class=\"o\">+</span> <span class=\"n\">col_offsets</span>\n-    <span class=\"c1\"># Load the row into SRAM, using a mask since BLOCK_SIZE may be &gt; than n_cols</span>\n-    <span class=\"n\">row</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">input_ptrs</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">col_offsets</span> <span class=\"o\">&lt;</span> <span class=\"n\">n_cols</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=-</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s1\">&#39;inf&#39;</span><span class=\"p\">))</span>\n-    <span class=\"c1\"># Subtract maximum for numerical stability</span>\n-    <span class=\"n\">row_minus_max</span> <span class=\"o\">=</span> <span class=\"n\">row</span> <span class=\"o\">-</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">row</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)</span>\n-    <span class=\"n\">numerator</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">row_minus_max</span><span class=\"p\">)</span>\n-    <span class=\"n\">denominator</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">numerator</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-    <span class=\"n\">softmax_output</span> <span class=\"o\">=</span> <span class=\"n\">numerator</span> <span class=\"o\">/</span> <span class=\"n\">denominator</span>\n-    <span class=\"c1\"># Write back output to DRAM</span>\n-    <span class=\"n\">output_row_start_ptr</span> <span class=\"o\">=</span> <span class=\"n\">output_ptr</span> <span class=\"o\">+</span> <span class=\"n\">row_idx</span> <span class=\"o\">*</span> <span class=\"n\">output_row_stride</span>\n-    <span class=\"n\">output_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">output_row_start_ptr</span> <span class=\"o\">+</span> <span class=\"n\">col_offsets</span>\n-    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">output_ptrs</span><span class=\"p\">,</span> <span class=\"n\">softmax_output</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">col_offsets</span> <span class=\"o\">&lt;</span> <span class=\"n\">n_cols</span><span class=\"p\">)</span>\n-</pre></div>\n-</div>\n-<p>We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.</p>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n-    <span class=\"n\">n_rows</span><span class=\"p\">,</span> <span class=\"n\">n_cols</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n-    <span class=\"c1\"># The block size is the smallest power of two greater than the number of columns in `x`</span>\n-    <span class=\"n\">BLOCK_SIZE</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">next_power_of_2</span><span class=\"p\">(</span><span class=\"n\">n_cols</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># Another trick we can use is to ask the compiler to use more threads per row by</span>\n-    <span class=\"c1\"># increasing the number of warps (`num_warps`) over which each row is distributed.</span>\n-    <span class=\"c1\"># You will see in the next tutorial how to auto-tune this value in a more natural</span>\n-    <span class=\"c1\"># way so you don&#39;t have to come up with manual heuristics yourself.</span>\n-    <span class=\"n\">num_warps</span> <span class=\"o\">=</span> <span class=\"mi\">4</span>\n-    <span class=\"k\">if</span> <span class=\"n\">BLOCK_SIZE</span> <span class=\"o\">&gt;=</span> <span class=\"mi\">2048</span><span class=\"p\">:</span>\n-        <span class=\"n\">num_warps</span> <span class=\"o\">=</span> <span class=\"mi\">8</span>\n-    <span class=\"k\">if</span> <span class=\"n\">BLOCK_SIZE</span> <span class=\"o\">&gt;=</span> <span class=\"mi\">4096</span><span class=\"p\">:</span>\n-        <span class=\"n\">num_warps</span> <span class=\"o\">=</span> <span class=\"mi\">16</span>\n-    <span class=\"c1\"># Allocate output</span>\n-    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># Enqueue kernel. The 1D launch grid is simple: we have one kernel instance per row o</span>\n-    <span class=\"c1\"># f the input matrix</span>\n-    <span class=\"n\">softmax_kernel</span><span class=\"p\">[(</span><span class=\"n\">n_rows</span><span class=\"p\">,)](</span>\n-        <span class=\"n\">y</span><span class=\"p\">,</span>\n-        <span class=\"n\">x</span><span class=\"p\">,</span>\n-        <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span>\n-        <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span>\n-        <span class=\"n\">n_cols</span><span class=\"p\">,</span>\n-        <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"n\">num_warps</span><span class=\"p\">,</span>\n-        <span class=\"n\">BLOCK_SIZE</span><span class=\"o\">=</span><span class=\"n\">BLOCK_SIZE</span><span class=\"p\">,</span>\n-    <span class=\"p\">)</span>\n-    <span class=\"k\">return</span> <span class=\"n\">y</span>\n-</pre></div>\n-</div>\n-</section>\n-<section id=\"unit-test\">\n-<h2>Unit Test<a class=\"headerlink\" href=\"#unit-test\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>We make sure that we test our kernel on a matrix with an irregular number of rows and columns.\n-This will allow us to verify that our padding mechanism works.</p>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">1823</span><span class=\"p\">,</span> <span class=\"mi\">781</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n-<span class=\"n\">y_triton</span> <span class=\"o\">=</span> <span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n-<span class=\"n\">y_torch</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n-<span class=\"k\">assert</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">y_triton</span><span class=\"p\">,</span> <span class=\"n\">y_torch</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"n\">y_triton</span><span class=\"p\">,</span> <span class=\"n\">y_torch</span><span class=\"p\">)</span>\n-</pre></div>\n-</div>\n-<p>As expected, the results are identical.</p>\n-</section>\n-<section id=\"benchmark\">\n-<h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>Here we will benchmark our operation as a function of the number of columns in the input matrix \u2013 assuming 4096 rows.\n-We will then compare its performance against (1) <code class=\"code docutils literal notranslate\"><span class=\"pre\">torch.softmax</span></code> and (2) the <code class=\"code docutils literal notranslate\"><span class=\"pre\">naive_softmax</span></code> defined above.</p>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">perf_report</span><span class=\"p\">(</span>\n-    <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">Benchmark</span><span class=\"p\">(</span>\n-        <span class=\"n\">x_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;N&#39;</span><span class=\"p\">],</span>  <span class=\"c1\"># argument names to use as an x-axis for the plot</span>\n-        <span class=\"n\">x_vals</span><span class=\"o\">=</span><span class=\"p\">[</span>\n-            <span class=\"mi\">128</span> <span class=\"o\">*</span> <span class=\"n\">i</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">)</span>\n-        <span class=\"p\">],</span>  <span class=\"c1\"># different possible values for `x_name`</span>\n-        <span class=\"n\">line_arg</span><span class=\"o\">=</span><span class=\"s1\">&#39;provider&#39;</span><span class=\"p\">,</span>  <span class=\"c1\"># argument name whose value corresponds to a different line in the plot</span>\n-        <span class=\"n\">line_vals</span><span class=\"o\">=</span><span class=\"p\">[</span>\n-            <span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">,</span>\n-            <span class=\"s1\">&#39;torch-native&#39;</span><span class=\"p\">,</span>\n-            <span class=\"s1\">&#39;torch-jit&#39;</span><span class=\"p\">,</span>\n-        <span class=\"p\">],</span>  <span class=\"c1\"># possible values for `line_arg``</span>\n-        <span class=\"n\">line_names</span><span class=\"o\">=</span><span class=\"p\">[</span>\n-            <span class=\"s2\">&quot;Triton&quot;</span><span class=\"p\">,</span>\n-            <span class=\"s2\">&quot;Torch (native)&quot;</span><span class=\"p\">,</span>\n-            <span class=\"s2\">&quot;Torch (jit)&quot;</span><span class=\"p\">,</span>\n-        <span class=\"p\">],</span>  <span class=\"c1\"># label name for the lines</span>\n-        <span class=\"n\">styles</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s1\">&#39;blue&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;green&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;green&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;--&#39;</span><span class=\"p\">)],</span>  <span class=\"c1\"># line styles</span>\n-        <span class=\"n\">ylabel</span><span class=\"o\">=</span><span class=\"s2\">&quot;GB/s&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># label name for the y-axis</span>\n-        <span class=\"n\">plot_name</span><span class=\"o\">=</span><span class=\"s2\">&quot;softmax-performance&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># name for the plot. Used also as a file name for saving the plot.</span>\n-        <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">4096</span><span class=\"p\">},</span>  <span class=\"c1\"># values for function arguments not in `x_names` and `y_name`</span>\n-    <span class=\"p\">)</span>\n-<span class=\"p\">)</span>\n-<span class=\"k\">def</span> <span class=\"nf\">benchmark</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">provider</span><span class=\"p\">):</span>\n-    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n-    <span class=\"n\">quantiles</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">]</span>\n-    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;torch-native&#39;</span><span class=\"p\">:</span>\n-        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">)</span>\n-    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">:</span>\n-        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">),</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">)</span>\n-    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;torch-jit&#39;</span><span class=\"p\">:</span>\n-        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">naive_softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">),</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">)</span>\n-    <span class=\"n\">gbps</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">ms</span><span class=\"p\">:</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">nelement</span><span class=\"p\">()</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">element_size</span><span class=\"p\">()</span> <span class=\"o\">*</span> <span class=\"mf\">1e-9</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">ms</span> <span class=\"o\">*</span> <span class=\"mf\">1e-3</span><span class=\"p\">)</span>\n-    <span class=\"k\">return</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">ms</span><span class=\"p\">),</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">max_ms</span><span class=\"p\">),</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">min_ms</span><span class=\"p\">)</span>\n-\n-\n-<span class=\"n\">benchmark</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">show_plots</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">print_data</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n-</pre></div>\n-</div>\n-<img src=\"../../_images/sphx_glr_02-fused-softmax_001.png\" srcset=\"../../_images/sphx_glr_02-fused-softmax_001.png\" alt=\"02 fused softmax\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>softmax-performance:\n-          N       Triton  Torch (native)  Torch (jit)\n-0     256.0   682.666643      744.727267   260.063494\n-1     384.0   877.714274      819.200021   315.076934\n-2     512.0   910.222190      963.764689   341.333321\n-3     640.0  1024.000026      930.909084   372.363633\n-4     768.0  1068.521715     1023.999964   384.000001\n-..      ...          ...             ...          ...\n-93  12160.0  1601.316858     1066.082150   463.238099\n-94  12288.0  1604.963246     1021.340281   462.063445\n-95  12416.0  1602.064538     1031.979242   461.454135\n-96  12544.0  1599.235121     1018.802024   460.330273\n-97  12672.0  1602.782573     1008.716405   461.324232\n-\n-[98 rows x 4 columns]\n-</pre></div>\n-</div>\n-<dl class=\"simple\">\n-<dt>In the above plot, we can see that:</dt><dd><ul class=\"simple\">\n-<li><p>Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.</p></li>\n-<li><p>Triton is noticeably faster than <code class=\"code docutils literal notranslate\"><span class=\"pre\">torch.softmax</span></code> \u2013 in addition to being <strong>easier to read, understand and maintain</strong>.\n-Note however that the PyTorch <cite>softmax</cite> operation is more general and will work on tensors of any shape.</p></li>\n-</ul>\n-</dd>\n-</dl>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  37.028 seconds)</p>\n-<div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-02-fused-softmax-py\">\n-<div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n-<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d91442ac2982c4e0cc3ab0f43534afbc/02-fused-softmax.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">02-fused-softmax.py</span></code></a></p>\n-</div>\n-<div class=\"sphx-glr-download sphx-glr-download-jupyter docutils container\">\n-<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/034d953b6214fedce6ea03803c712b89/02-fused-softmax.ipynb\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Jupyter</span> <span class=\"pre\">notebook:</span> <span class=\"pre\">02-fused-softmax.ipynb</span></code></a></p>\n-</div>\n-</div>\n-<p class=\"sphx-glr-signature\"><a class=\"reference external\" href=\"https://sphinx-gallery.github.io\">Gallery generated by Sphinx-Gallery</a></p>\n-</section>\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"01-vector-add.html\" class=\"btn btn-neutral float-left\" title=\"Vector Addition\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"03-matrix-multiplication.html\" class=\"btn btn-neutral float-right\" title=\"Matrix Multiplication\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"02-fused-softmax.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/getting-started/tutorials/03-matrix-multiplication.html", "status": "removed", "additions": 0, "deletions": 557, "changes": 557, "file_content_changes": "@@ -1,557 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>Matrix Multiplication &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"Low-Memory Dropout\" href=\"04-low-memory-dropout.html\" />\n-    <link rel=\"prev\" title=\"Fused Softmax\" href=\"02-fused-softmax.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../installation.html\">Installation</a></li>\n-<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"index.html\">Tutorials</a><ul class=\"current\">\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"01-vector-add.html\">Vector Addition</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"02-fused-softmax.html\">Fused Softmax</a></li>\n-<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Matrix Multiplication</a><ul>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#motivations\">Motivations</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#compute-kernel\">Compute Kernel</a><ul>\n-<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#pointer-arithmetics\">Pointer Arithmetics</a></li>\n-<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#l2-cache-optimizations\">L2 Cache Optimizations</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#final-result\">Final Result</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#unit-test\">Unit Test</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#benchmark\">Benchmark</a><ul>\n-<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#square-matrix-performance\">Square Matrix Performance</a></li>\n-</ul>\n-</li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"08-experimental-block-pointer.html\">Block Pointer (Experimental)</a></li>\n-</ul>\n-</li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-          <li class=\"breadcrumb-item\"><a href=\"index.html\">Tutorials</a></li>\n-      <li class=\"breadcrumb-item active\">Matrix Multiplication</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/getting-started/tutorials/03-matrix-multiplication.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <div class=\"sphx-glr-download-link-note admonition note\">\n-<p class=\"admonition-title\">Note</p>\n-<p><a class=\"reference internal\" href=\"#sphx-glr-download-getting-started-tutorials-03-matrix-multiplication-py\"><span class=\"std std-ref\">Go to the end</span></a>\n-to download the full example code</p>\n-</div>\n-<section class=\"sphx-glr-example-title\" id=\"matrix-multiplication\">\n-<span id=\"sphx-glr-getting-started-tutorials-03-matrix-multiplication-py\"></span><h1>Matrix Multiplication<a class=\"headerlink\" href=\"#matrix-multiplication\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<p>In this tutorial, you will write a very short high-performance FP16 matrix multiplication kernel that achieves\n-performance on parallel with cuBLAS.</p>\n-<p>You will specifically learn about:</p>\n-<ul class=\"simple\">\n-<li><p>Block-level matrix multiplications.</p></li>\n-<li><p>Multi-dimensional pointer arithmetics.</p></li>\n-<li><p>Program re-ordering for improved L2 cache hit rate.</p></li>\n-<li><p>Automatic performance tuning.</p></li>\n-</ul>\n-<section id=\"motivations\">\n-<h2>Motivations<a class=\"headerlink\" href=\"#motivations\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>Matrix multiplications are a key building block of most modern high-performance computing systems.\n-They are notoriously hard to optimize, hence their implementation is generally done by\n-hardware vendors themselves as part of so-called \u201ckernel libraries\u201d (e.g., cuBLAS).\n-Unfortunately, these libraries are often proprietary and cannot be easily customized\n-to accommodate the needs of modern deep learning workloads (e.g., fused activation functions).\n-In this tutorial, you will learn how to implement efficient matrix multiplications by\n-yourself with Triton, in a way that is easy to customize and extend.</p>\n-<p>Roughly speaking, the kernel that we will write will implement the following blocked\n-algorithm to multiply a (M, K) by a (K, N) matrix:</p>\n-<blockquote>\n-<div><div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"c1\"># Do in parallel</span>\n-<span class=\"k\">for</span> <span class=\"n\">m</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">):</span>\n-  <span class=\"c1\"># Do in parallel</span>\n-  <span class=\"k\">for</span> <span class=\"n\">n</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">):</span>\n-    <span class=\"n\">acc</span> <span class=\"o\">=</span> <span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n-    <span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">):</span>\n-      <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">m</span> <span class=\"p\">:</span> <span class=\"n\">m</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">k</span> <span class=\"p\">:</span> <span class=\"n\">k</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">]</span>\n-      <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">B</span><span class=\"p\">[</span><span class=\"n\">k</span> <span class=\"p\">:</span> <span class=\"n\">k</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">,</span> <span class=\"n\">n</span> <span class=\"p\">:</span> <span class=\"n\">n</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">]</span>\n-      <span class=\"n\">acc</span> <span class=\"o\">+=</span> <span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n-    <span class=\"n\">C</span><span class=\"p\">[</span><span class=\"n\">m</span> <span class=\"p\">:</span> <span class=\"n\">m</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">n</span> <span class=\"p\">:</span> <span class=\"n\">n</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">acc</span>\n-</pre></div>\n-</div>\n-</div></blockquote>\n-<p>where each iteration of the doubly-nested for-loop is performed by a dedicated Triton program instance.</p>\n-</section>\n-<section id=\"compute-kernel\">\n-<h2>Compute Kernel<a class=\"headerlink\" href=\"#compute-kernel\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>The above algorithm is, actually, fairly straightforward to implement in Triton.\n-The main difficulty comes from the computation of the memory locations at which blocks\n-of <code class=\"code docutils literal notranslate\"><span class=\"pre\">A</span></code> and <code class=\"code docutils literal notranslate\"><span class=\"pre\">B</span></code> must be read in the inner loop. For that, we need\n-multi-dimensional pointer arithmetics.</p>\n-<section id=\"pointer-arithmetics\">\n-<h3>Pointer Arithmetics<a class=\"headerlink\" href=\"#pointer-arithmetics\" title=\"Permalink to this heading\">\u00b6</a></h3>\n-<p>For a row-major 2D tensor <code class=\"code docutils literal notranslate\"><span class=\"pre\">X</span></code>, the memory location of <code class=\"code docutils literal notranslate\"><span class=\"pre\">X[i,</span> <span class=\"pre\">j]</span></code> is given b\n-y <code class=\"code docutils literal notranslate\"><span class=\"pre\">&amp;X[i,</span> <span class=\"pre\">j]</span> <span class=\"pre\">=</span> <span class=\"pre\">X</span> <span class=\"pre\">+</span> <span class=\"pre\">i*stride_xi</span> <span class=\"pre\">+</span> <span class=\"pre\">j*stride_xj</span></code>.\n-Therefore, blocks of pointers for <code class=\"code docutils literal notranslate\"><span class=\"pre\">A[m</span> <span class=\"pre\">:</span> <span class=\"pre\">m+BLOCK_SIZE_M,</span> <span class=\"pre\">k:k+BLOCK_SIZE_K]</span></code> and\n-<code class=\"code docutils literal notranslate\"><span class=\"pre\">B[k</span> <span class=\"pre\">:</span> <span class=\"pre\">k+BLOCK_SIZE_K,</span> <span class=\"pre\">n</span> <span class=\"pre\">:</span> <span class=\"pre\">n+BLOCK_SIZE_N]</span></code> can be defined in pseudo-code as:</p>\n-<blockquote>\n-<div><div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"o\">&amp;</span><span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">m</span> <span class=\"p\">:</span> <span class=\"n\">m</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">:</span><span class=\"n\">k</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">]</span> <span class=\"o\">=</span>  <span class=\"n\">a_ptr</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">m</span> <span class=\"p\">:</span> <span class=\"n\">m</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">)[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"n\">A</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">k</span> <span class=\"p\">:</span> <span class=\"n\">k</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">)[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span><span class=\"o\">*</span><span class=\"n\">A</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">);</span>\n-<span class=\"o\">&amp;</span><span class=\"n\">B</span><span class=\"p\">[</span><span class=\"n\">k</span> <span class=\"p\">:</span> <span class=\"n\">k</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">:</span><span class=\"n\">n</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">]</span> <span class=\"o\">=</span>  <span class=\"n\">b_ptr</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">k</span> <span class=\"p\">:</span> <span class=\"n\">k</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">)[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"n\">B</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">n</span> <span class=\"p\">:</span> <span class=\"n\">n</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span><span class=\"o\">*</span><span class=\"n\">B</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">);</span>\n-</pre></div>\n-</div>\n-</div></blockquote>\n-<p>Which means that pointers for blocks of A and B can be initialized (i.e., <code class=\"code docutils literal notranslate\"><span class=\"pre\">k=0</span></code>) in Triton as the following\n-code. Also note that we need an extra modulo to handle the case where <code class=\"code docutils literal notranslate\"><span class=\"pre\">M</span></code> is not a multiple of\n-<code class=\"code docutils literal notranslate\"><span class=\"pre\">BLOCK_SIZE_M</span></code> or <code class=\"code docutils literal notranslate\"><span class=\"pre\">N</span></code> is not a multiple of <code class=\"code docutils literal notranslate\"><span class=\"pre\">BLOCK_SIZE_N</span></code>, in which case we can pad the data with\n-some useless values, which will not contribute to the results. For the <code class=\"code docutils literal notranslate\"><span class=\"pre\">K</span></code> dimension, we will handle that later\n-using masking load semantics.</p>\n-<blockquote>\n-<div><div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">offs_am</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">pid_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">))</span> <span class=\"o\">%</span> <span class=\"n\">M</span>\n-<span class=\"n\">offs_bn</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">pid_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_N</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">))</span> <span class=\"o\">%</span> <span class=\"n\">N</span>\n-<span class=\"n\">offs_k</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">)</span>\n-<span class=\"n\">a_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">a_ptr</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_am</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"n\">stride_am</span> <span class=\"o\">+</span> <span class=\"n\">offs_k</span> <span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span><span class=\"o\">*</span><span class=\"n\">stride_ak</span><span class=\"p\">)</span>\n-<span class=\"n\">b_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">b_ptr</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_k</span> <span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"n\">stride_bk</span> <span class=\"o\">+</span> <span class=\"n\">offs_bn</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span><span class=\"o\">*</span><span class=\"n\">stride_bn</span><span class=\"p\">)</span>\n-</pre></div>\n-</div>\n-</div></blockquote>\n-<p>And then updated in the inner loop as follows:</p>\n-<blockquote>\n-<div><div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">a_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_SIZE_K</span> <span class=\"o\">*</span> <span class=\"n\">stride_ak</span><span class=\"p\">;</span>\n-<span class=\"n\">b_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_SIZE_K</span> <span class=\"o\">*</span> <span class=\"n\">stride_bk</span><span class=\"p\">;</span>\n-</pre></div>\n-</div>\n-</div></blockquote>\n-</section>\n-<section id=\"l2-cache-optimizations\">\n-<h3>L2 Cache Optimizations<a class=\"headerlink\" href=\"#l2-cache-optimizations\" title=\"Permalink to this heading\">\u00b6</a></h3>\n-<p>As mentioned above, each program instance computes a <code class=\"code docutils literal notranslate\"><span class=\"pre\">[BLOCK_SIZE_M,</span> <span class=\"pre\">BLOCK_SIZE_N]</span></code>\n-block of <code class=\"code docutils literal notranslate\"><span class=\"pre\">C</span></code>.\n-It is important to remember that the order in which these blocks are computed does\n-matter, since it affects the L2 cache hit rate of our program. and unfortunately, a\n-a simple row-major ordering</p>\n-<blockquote>\n-<div><div class=\"highlight-Python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">);</span>\n-<span class=\"n\">grid_m</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">M</span> <span class=\"o\">+</span> <span class=\"n\">BLOCK_SIZE_M</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">;</span>\n-<span class=\"n\">grid_n</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">N</span> <span class=\"o\">+</span> <span class=\"n\">BLOCK_SIZE_N</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">;</span>\n-<span class=\"n\">pid_m</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">/</span> <span class=\"n\">grid_n</span><span class=\"p\">;</span>\n-<span class=\"n\">pid_n</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">%</span> <span class=\"n\">grid_n</span><span class=\"p\">;</span>\n-</pre></div>\n-</div>\n-</div></blockquote>\n-<p>is just not going to cut it.</p>\n-<p>One possible solution is to launch blocks in an order that promotes data reuse.\n-This can be done by \u2018super-grouping\u2019 blocks in groups of <code class=\"code docutils literal notranslate\"><span class=\"pre\">GROUP_M</span></code> rows before\n-switching to the next column:</p>\n-<blockquote>\n-<div><div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"c1\"># Program ID</span>\n-<span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-<span class=\"c1\"># Number of program ids along the M axis</span>\n-<span class=\"n\">num_pid_m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">)</span>\n-<span class=\"c1\"># Number of programs ids along the N axis</span>\n-<span class=\"n\">num_pid_n</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)</span>\n-<span class=\"c1\"># Number of programs in group</span>\n-<span class=\"n\">num_pid_in_group</span> <span class=\"o\">=</span> <span class=\"n\">GROUP_SIZE_M</span> <span class=\"o\">*</span> <span class=\"n\">num_pid_n</span>\n-<span class=\"c1\"># Id of the group this program is in</span>\n-<span class=\"n\">group_id</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">//</span> <span class=\"n\">num_pid_in_group</span>\n-<span class=\"c1\"># Row-id of the first program in the group</span>\n-<span class=\"n\">first_pid_m</span> <span class=\"o\">=</span> <span class=\"n\">group_id</span> <span class=\"o\">*</span> <span class=\"n\">GROUP_SIZE_M</span>\n-<span class=\"c1\"># If `num_pid_m` isn&#39;t divisible by `GROUP_SIZE_M`, the last group is smaller</span>\n-<span class=\"n\">group_size_m</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">num_pid_m</span> <span class=\"o\">-</span> <span class=\"n\">first_pid_m</span><span class=\"p\">,</span> <span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">)</span>\n-<span class=\"c1\"># *Within groups*, programs are ordered in a column-major order</span>\n-<span class=\"c1\"># Row-id of the program in the *launch grid*</span>\n-<span class=\"n\">pid_m</span> <span class=\"o\">=</span> <span class=\"n\">first_pid_m</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">pid</span> <span class=\"o\">%</span> <span class=\"n\">group_size_m</span><span class=\"p\">)</span>\n-<span class=\"c1\"># Col-id of the program in the *launch grid*</span>\n-<span class=\"n\">pid_n</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">pid</span> <span class=\"o\">%</span> <span class=\"n\">num_pid_in_group</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"n\">group_size_m</span>\n-</pre></div>\n-</div>\n-</div></blockquote>\n-<p>For example, in the following matmul where each matrix is 9 blocks by 9 blocks,\n-we can see that if we compute the output in row-major ordering, we need to load 90\n-blocks into SRAM to compute the first 9 output blocks, but if we do it in grouped\n-ordering, we only need to load 54 blocks.</p>\n-<blockquote>\n-<div><img alt=\"../../_images/grouped_vs_row_major_ordering.png\" src=\"../../_images/grouped_vs_row_major_ordering.png\" />\n-</div></blockquote>\n-<p>In practice, this can improve the performance of our matrix multiplication kernel by\n-more than 10% on some hardware architecture (e.g., 220 to 245 TFLOPS on A100).</p>\n-</section>\n-</section>\n-<section id=\"final-result\">\n-<h2>Final Result<a class=\"headerlink\" href=\"#final-result\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n-\n-<span class=\"kn\">import</span> <span class=\"nn\">triton</span>\n-<span class=\"kn\">import</span> <span class=\"nn\">triton.language</span> <span class=\"k\">as</span> <span class=\"nn\">tl</span>\n-\n-\n-<span class=\"c1\"># `triton.jit`&#39;ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:</span>\n-<span class=\"c1\">#   - A list of `triton.Config` objects that define different configurations of</span>\n-<span class=\"c1\">#       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try</span>\n-<span class=\"c1\">#   - An auto-tuning *key* whose change in values will trigger evaluation of all the</span>\n-<span class=\"c1\">#       provided configs</span>\n-<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">autotune</span><span class=\"p\">(</span>\n-    <span class=\"n\">configs</span><span class=\"o\">=</span><span class=\"p\">[</span>\n-        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">),</span>\n-        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n-        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n-        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n-        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n-        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n-        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n-        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n-    <span class=\"p\">],</span>\n-    <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;M&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;K&#39;</span><span class=\"p\">],</span>\n-<span class=\"p\">)</span>\n-<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n-<span class=\"k\">def</span> <span class=\"nf\">matmul_kernel</span><span class=\"p\">(</span>\n-    <span class=\"c1\"># Pointers to matrices</span>\n-    <span class=\"n\">a_ptr</span><span class=\"p\">,</span> <span class=\"n\">b_ptr</span><span class=\"p\">,</span> <span class=\"n\">c_ptr</span><span class=\"p\">,</span>\n-    <span class=\"c1\"># Matrix dimensions</span>\n-    <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span>\n-    <span class=\"c1\"># The stride variables represent how much to increase the ptr by when moving by 1</span>\n-    <span class=\"c1\"># element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`</span>\n-    <span class=\"c1\"># by to get the element one row down (A has M rows).</span>\n-    <span class=\"n\">stride_am</span><span class=\"p\">,</span> <span class=\"n\">stride_ak</span><span class=\"p\">,</span>\n-    <span class=\"n\">stride_bk</span><span class=\"p\">,</span> <span class=\"n\">stride_bn</span><span class=\"p\">,</span>\n-    <span class=\"n\">stride_cm</span><span class=\"p\">,</span> <span class=\"n\">stride_cn</span><span class=\"p\">,</span>\n-    <span class=\"c1\"># Meta-parameters</span>\n-    <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n-    <span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n-    <span class=\"n\">ACTIVATION</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n-<span class=\"p\">):</span>\n-<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Kernel for computing the matmul C = A x B.</span>\n-<span class=\"sd\">    A has shape (M, K), B has shape (K, N) and C has shape (M, N)</span>\n-<span class=\"sd\">    &quot;&quot;&quot;</span>\n-    <span class=\"c1\"># -----------------------------------------------------------</span>\n-    <span class=\"c1\"># Map program ids `pid` to the block of C it should compute.</span>\n-    <span class=\"c1\"># This is done in a grouped ordering to promote L2 data reuse.</span>\n-    <span class=\"c1\"># See above `L2 Cache Optimizations` section for details.</span>\n-    <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-    <span class=\"n\">num_pid_m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">)</span>\n-    <span class=\"n\">num_pid_n</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)</span>\n-    <span class=\"n\">num_pid_in_group</span> <span class=\"o\">=</span> <span class=\"n\">GROUP_SIZE_M</span> <span class=\"o\">*</span> <span class=\"n\">num_pid_n</span>\n-    <span class=\"n\">group_id</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">//</span> <span class=\"n\">num_pid_in_group</span>\n-    <span class=\"n\">first_pid_m</span> <span class=\"o\">=</span> <span class=\"n\">group_id</span> <span class=\"o\">*</span> <span class=\"n\">GROUP_SIZE_M</span>\n-    <span class=\"n\">group_size_m</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">num_pid_m</span> <span class=\"o\">-</span> <span class=\"n\">first_pid_m</span><span class=\"p\">,</span> <span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">)</span>\n-    <span class=\"n\">pid_m</span> <span class=\"o\">=</span> <span class=\"n\">first_pid_m</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">pid</span> <span class=\"o\">%</span> <span class=\"n\">group_size_m</span><span class=\"p\">)</span>\n-    <span class=\"n\">pid_n</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">pid</span> <span class=\"o\">%</span> <span class=\"n\">num_pid_in_group</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"n\">group_size_m</span>\n-\n-    <span class=\"c1\"># ----------------------------------------------------------</span>\n-    <span class=\"c1\"># Create pointers for the first blocks of A and B.</span>\n-    <span class=\"c1\"># We will advance this pointer as we move in the K direction</span>\n-    <span class=\"c1\"># and accumulate</span>\n-    <span class=\"c1\"># `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers</span>\n-    <span class=\"c1\"># `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers</span>\n-    <span class=\"c1\"># See above `Pointer Arithmetics` section for details</span>\n-    <span class=\"n\">offs_am</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">pid_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">))</span> <span class=\"o\">%</span> <span class=\"n\">M</span>\n-    <span class=\"n\">offs_bn</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">pid_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_N</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">))</span> <span class=\"o\">%</span> <span class=\"n\">N</span>\n-    <span class=\"n\">offs_k</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">)</span>\n-    <span class=\"n\">a_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">a_ptr</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_am</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_am</span> <span class=\"o\">+</span> <span class=\"n\">offs_k</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_ak</span><span class=\"p\">)</span>\n-    <span class=\"n\">b_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">b_ptr</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_k</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_bk</span> <span class=\"o\">+</span> <span class=\"n\">offs_bn</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_bn</span><span class=\"p\">)</span>\n-\n-    <span class=\"c1\"># -----------------------------------------------------------</span>\n-    <span class=\"c1\"># Iterate to compute a block of the C matrix.</span>\n-    <span class=\"c1\"># We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block</span>\n-    <span class=\"c1\"># of fp32 values for higher accuracy.</span>\n-    <span class=\"c1\"># `accumulator` will be converted back to fp16 after the loop.</span>\n-    <span class=\"n\">accumulator</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n-    <span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">)):</span>\n-        <span class=\"c1\"># Load the next block of A and B, generate a mask by checking the K dimension.</span>\n-        <span class=\"c1\"># If it is out of bounds, set it to 0.</span>\n-        <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">a_ptrs</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">offs_k</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">&lt;</span> <span class=\"n\">K</span> <span class=\"o\">-</span> <span class=\"n\">k</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">)</span>\n-        <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">b_ptrs</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">offs_k</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&lt;</span> <span class=\"n\">K</span> <span class=\"o\">-</span> <span class=\"n\">k</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">)</span>\n-        <span class=\"c1\"># We accumulate along the K dimension.</span>\n-        <span class=\"n\">accumulator</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n-        <span class=\"c1\"># Advance the ptrs to the next K block.</span>\n-        <span class=\"n\">a_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_SIZE_K</span> <span class=\"o\">*</span> <span class=\"n\">stride_ak</span>\n-        <span class=\"n\">b_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_SIZE_K</span> <span class=\"o\">*</span> <span class=\"n\">stride_bk</span>\n-    <span class=\"c1\"># You can fuse arbitrary activation functions here</span>\n-    <span class=\"c1\"># while the accumulator is still in FP32!</span>\n-    <span class=\"k\">if</span> <span class=\"n\">ACTIVATION</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;leaky_relu&quot;</span><span class=\"p\">:</span>\n-        <span class=\"n\">accumulator</span> <span class=\"o\">=</span> <span class=\"n\">leaky_relu</span><span class=\"p\">(</span><span class=\"n\">accumulator</span><span class=\"p\">)</span>\n-    <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">accumulator</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n-\n-    <span class=\"c1\"># -----------------------------------------------------------</span>\n-    <span class=\"c1\"># Write back the block of the output matrix C with masks.</span>\n-    <span class=\"n\">offs_cm</span> <span class=\"o\">=</span> <span class=\"n\">pid_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">)</span>\n-    <span class=\"n\">offs_cn</span> <span class=\"o\">=</span> <span class=\"n\">pid_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_N</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)</span>\n-    <span class=\"n\">c_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">c_ptr</span> <span class=\"o\">+</span> <span class=\"n\">stride_cm</span> <span class=\"o\">*</span> <span class=\"n\">offs_cm</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">stride_cn</span> <span class=\"o\">*</span> <span class=\"n\">offs_cn</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n-    <span class=\"n\">c_mask</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">offs_cm</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&lt;</span> <span class=\"n\">M</span><span class=\"p\">)</span> <span class=\"o\">&amp;</span> <span class=\"p\">(</span><span class=\"n\">offs_cn</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n-    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">c_ptrs</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">c_mask</span><span class=\"p\">)</span>\n-\n-\n-<span class=\"c1\"># We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.</span>\n-<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n-<span class=\"k\">def</span> <span class=\"nf\">leaky_relu</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n-    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n-    <span class=\"k\">return</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">&gt;=</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"mf\">0.01</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"p\">)</span>\n-</pre></div>\n-</div>\n-<p>We can now create a convenience wrapper function that only takes two input tensors,\n-and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.</p>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s2\">&quot;&quot;</span><span class=\"p\">):</span>\n-    <span class=\"c1\"># Check constraints.</span>\n-    <span class=\"k\">assert</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"s2\">&quot;Incompatible dimensions&quot;</span>\n-    <span class=\"k\">assert</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">is_contiguous</span><span class=\"p\">(),</span> <span class=\"s2\">&quot;Matrix A must be contiguous&quot;</span>\n-    <span class=\"k\">assert</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">is_contiguous</span><span class=\"p\">(),</span> <span class=\"s2\">&quot;Matrix B must be contiguous&quot;</span>\n-    <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">K</span> <span class=\"o\">=</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n-    <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n-    <span class=\"c1\"># Allocates output.</span>\n-    <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># 1D launch kernel where each block gets its own program.</span>\n-    <span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">META</span><span class=\"p\">:</span> <span class=\"p\">(</span>\n-        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">META</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">META</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">]),</span>\n-    <span class=\"p\">)</span>\n-    <span class=\"n\">matmul_kernel</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span>\n-        <span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">,</span>\n-        <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span>\n-        <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n-        <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n-        <span class=\"n\">c</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">c</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n-        <span class=\"n\">ACTIVATION</span><span class=\"o\">=</span><span class=\"n\">activation</span>\n-    <span class=\"p\">)</span>\n-    <span class=\"k\">return</span> <span class=\"n\">c</span>\n-</pre></div>\n-</div>\n-</section>\n-<section id=\"unit-test\">\n-<h2>Unit Test<a class=\"headerlink\" href=\"#unit-test\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS).</p>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-<span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n-<span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n-<span class=\"n\">triton_output</span> <span class=\"o\">=</span> <span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n-<span class=\"n\">torch_output</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n-<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;triton_output=</span><span class=\"si\">{</span><span class=\"n\">triton_output</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n-<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;torch_output=</span><span class=\"si\">{</span><span class=\"n\">torch_output</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n-<span class=\"k\">if</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">triton_output</span><span class=\"p\">,</span> <span class=\"n\">torch_output</span><span class=\"p\">,</span> <span class=\"n\">atol</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">,</span> <span class=\"n\">rtol</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">):</span>\n-    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;\u2705 Triton and Torch match&quot;</span><span class=\"p\">)</span>\n-<span class=\"k\">else</span><span class=\"p\">:</span>\n-    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;\u274c Triton and Torch differ&quot;</span><span class=\"p\">)</span>\n-</pre></div>\n-</div>\n-<div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>triton_output=tensor([[-10.9531,  -4.7109,  15.6953,  ..., -28.4062,   4.3320, -26.4219],\n-        [ 26.8438,  10.0469,  -5.4297,  ..., -11.2969,  -8.5312,  30.7500],\n-        [-13.2578,  15.8516,  18.0781,  ..., -21.7656,  -8.6406,  10.2031],\n-        ...,\n-        [ 40.2812,  18.6094, -25.6094,  ...,  -2.7598,  -3.2441,  41.0000],\n-        [ -6.1211, -16.8281,   4.4844,  ..., -21.0312,  24.7031,  15.0234],\n-        [-17.0938, -19.0000,  -0.3831,  ...,  21.5469, -30.2344, -13.2188]],\n-       device=&#39;cuda:0&#39;, dtype=torch.float16)\n-torch_output=tensor([[-10.9531,  -4.7109,  15.6953,  ..., -28.4062,   4.3320, -26.4219],\n-        [ 26.8438,  10.0469,  -5.4297,  ..., -11.2969,  -8.5312,  30.7500],\n-        [-13.2578,  15.8516,  18.0781,  ..., -21.7656,  -8.6406,  10.2031],\n-        ...,\n-        [ 40.2812,  18.6094, -25.6094,  ...,  -2.7598,  -3.2441,  41.0000],\n-        [ -6.1211, -16.8281,   4.4844,  ..., -21.0312,  24.7031,  15.0234],\n-        [-17.0938, -19.0000,  -0.3831,  ...,  21.5469, -30.2344, -13.2188]],\n-       device=&#39;cuda:0&#39;, dtype=torch.float16)\n-\u2705 Triton and Torch match\n-</pre></div>\n-</div>\n-</section>\n-<section id=\"benchmark\">\n-<h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<section id=\"square-matrix-performance\">\n-<h3>Square Matrix Performance<a class=\"headerlink\" href=\"#square-matrix-performance\" title=\"Permalink to this heading\">\u00b6</a></h3>\n-<p>We can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices,\n-but feel free to arrange this script as you wish to benchmark any other matrix shape.</p>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">perf_report</span><span class=\"p\">(</span>\n-    <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">Benchmark</span><span class=\"p\">(</span>\n-        <span class=\"n\">x_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;M&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;K&#39;</span><span class=\"p\">],</span>  <span class=\"c1\"># Argument names to use as an x-axis for the plot</span>\n-        <span class=\"n\">x_vals</span><span class=\"o\">=</span><span class=\"p\">[</span>\n-            <span class=\"mi\">128</span> <span class=\"o\">*</span> <span class=\"n\">i</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">33</span><span class=\"p\">)</span>\n-        <span class=\"p\">],</span>  <span class=\"c1\"># Different possible values for `x_name`</span>\n-        <span class=\"n\">line_arg</span><span class=\"o\">=</span><span class=\"s1\">&#39;provider&#39;</span><span class=\"p\">,</span>  <span class=\"c1\"># Argument name whose value corresponds to a different line in the plot</span>\n-        <span class=\"c1\"># Possible values for `line_arg`</span>\n-        <span class=\"n\">line_vals</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;cublas&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">],</span>\n-        <span class=\"c1\"># Label name for the lines</span>\n-        <span class=\"n\">line_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">&quot;cuBLAS&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;Triton&quot;</span><span class=\"p\">],</span>\n-        <span class=\"c1\"># Line styles</span>\n-        <span class=\"n\">styles</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s1\">&#39;green&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;blue&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">)],</span>\n-        <span class=\"n\">ylabel</span><span class=\"o\">=</span><span class=\"s2\">&quot;TFLOPS&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># Label name for the y-axis</span>\n-        <span class=\"n\">plot_name</span><span class=\"o\">=</span><span class=\"s2\">&quot;matmul-performance&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># Name for the plot, used also as a file name for saving the plot.</span>\n-        <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{},</span>\n-    <span class=\"p\">)</span>\n-<span class=\"p\">)</span>\n-<span class=\"k\">def</span> <span class=\"nf\">benchmark</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">provider</span><span class=\"p\">):</span>\n-    <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n-    <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n-    <span class=\"n\">quantiles</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">]</span>\n-    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;cublas&#39;</span><span class=\"p\">:</span>\n-        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">),</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">)</span>\n-    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">:</span>\n-        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">),</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">)</span>\n-    <span class=\"n\">perf</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">ms</span><span class=\"p\">:</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">M</span> <span class=\"o\">*</span> <span class=\"n\">N</span> <span class=\"o\">*</span> <span class=\"n\">K</span> <span class=\"o\">*</span> <span class=\"mf\">1e-12</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">ms</span> <span class=\"o\">*</span> <span class=\"mf\">1e-3</span><span class=\"p\">)</span>\n-    <span class=\"k\">return</span> <span class=\"n\">perf</span><span class=\"p\">(</span><span class=\"n\">ms</span><span class=\"p\">),</span> <span class=\"n\">perf</span><span class=\"p\">(</span><span class=\"n\">max_ms</span><span class=\"p\">),</span> <span class=\"n\">perf</span><span class=\"p\">(</span><span class=\"n\">min_ms</span><span class=\"p\">)</span>\n-\n-\n-<span class=\"n\">benchmark</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">show_plots</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">print_data</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n-</pre></div>\n-</div>\n-<img src=\"../../_images/sphx_glr_03-matrix-multiplication_001.png\" srcset=\"../../_images/sphx_glr_03-matrix-multiplication_001.png\" alt=\"03 matrix multiplication\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>matmul-performance:\n-         M      cuBLAS      Triton\n-0    256.0    4.096000    4.096000\n-1    384.0   11.059200   12.288000\n-2    512.0   26.214401   23.831273\n-3    640.0   42.666665   42.666665\n-4    768.0   68.056616   58.982401\n-5    896.0   78.051553   82.642822\n-6   1024.0  104.857603   99.864382\n-7   1152.0  135.726544  129.825388\n-8   1280.0  157.538463  163.840004\n-9   1408.0  155.765024  132.970149\n-10  1536.0  181.484314  157.286398\n-11  1664.0  179.978245  179.978245\n-12  1792.0  172.914215  208.137481\n-13  1920.0  200.347822  168.585369\n-14  2048.0  223.696203  190.650180\n-15  2176.0  211.827867  209.621326\n-16  2304.0  225.357284  225.357284\n-17  2432.0  203.583068  203.583068\n-18  2560.0  224.438347  218.453323\n-19  2688.0  197.567993  194.528492\n-20  2816.0  205.727397  210.696652\n-21  2944.0  218.579083  219.541994\n-22  3072.0  204.785179  207.410628\n-23  3200.0  213.333323  218.430042\n-24  3328.0  203.941342  206.278780\n-25  3456.0  216.143621  215.565692\n-26  3584.0  218.772251  206.227962\n-27  3712.0  208.553950  216.228019\n-28  3840.0  208.664143  210.250955\n-29  3968.0  209.663117  215.971570\n-30  4096.0  217.005221  211.366499\n-</pre></div>\n-</div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  40.533 seconds)</p>\n-<div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-03-matrix-multiplication-py\">\n-<div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n-<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d5fee5b55a64e47f1b5724ec39adf171/03-matrix-multiplication.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">03-matrix-multiplication.py</span></code></a></p>\n-</div>\n-<div class=\"sphx-glr-download sphx-glr-download-jupyter docutils container\">\n-<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/b51b68bc1c6b1a5e509f67800b6235af/03-matrix-multiplication.ipynb\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Jupyter</span> <span class=\"pre\">notebook:</span> <span class=\"pre\">03-matrix-multiplication.ipynb</span></code></a></p>\n-</div>\n-</div>\n-<p class=\"sphx-glr-signature\"><a class=\"reference external\" href=\"https://sphinx-gallery.github.io\">Gallery generated by Sphinx-Gallery</a></p>\n-</section>\n-</section>\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"02-fused-softmax.html\" class=\"btn btn-neutral float-left\" title=\"Fused Softmax\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"04-low-memory-dropout.html\" class=\"btn btn-neutral float-right\" title=\"Low-Memory Dropout\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"03-matrix-multiplication.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/getting-started/tutorials/04-low-memory-dropout.html", "status": "removed", "additions": 0, "deletions": 347, "changes": 347, "file_content_changes": "@@ -1,347 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>Low-Memory Dropout &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-        <script async=\"async\" src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"Layer Normalization\" href=\"05-layer-norm.html\" />\n-    <link rel=\"prev\" title=\"Matrix Multiplication\" href=\"03-matrix-multiplication.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../installation.html\">Installation</a></li>\n-<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"index.html\">Tutorials</a><ul class=\"current\">\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"01-vector-add.html\">Vector Addition</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"02-fused-softmax.html\">Fused Softmax</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n-<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Low-Memory Dropout</a><ul>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#baseline\">Baseline</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#seeded-dropout\">Seeded dropout</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#exercises\">Exercises</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#references\">References</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"08-experimental-block-pointer.html\">Block Pointer (Experimental)</a></li>\n-</ul>\n-</li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-          <li class=\"breadcrumb-item\"><a href=\"index.html\">Tutorials</a></li>\n-      <li class=\"breadcrumb-item active\">Low-Memory Dropout</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/getting-started/tutorials/04-low-memory-dropout.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <div class=\"sphx-glr-download-link-note admonition note\">\n-<p class=\"admonition-title\">Note</p>\n-<p><a class=\"reference internal\" href=\"#sphx-glr-download-getting-started-tutorials-04-low-memory-dropout-py\"><span class=\"std std-ref\">Go to the end</span></a>\n-to download the full example code</p>\n-</div>\n-<section class=\"sphx-glr-example-title\" id=\"low-memory-dropout\">\n-<span id=\"sphx-glr-getting-started-tutorials-04-low-memory-dropout-py\"></span><h1>Low-Memory Dropout<a class=\"headerlink\" href=\"#low-memory-dropout\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<p>In this tutorial, you will write a memory-efficient implementation of dropout whose state\n-will be composed of a single int32 seed. This differs from more traditional implementations of dropout,\n-whose state is generally composed of a bit mask tensor of the same shape as the input.</p>\n-<p>In doing so, you will learn about:</p>\n-<ul class=\"simple\">\n-<li><p>The limitations of naive implementations of Dropout with PyTorch.</p></li>\n-<li><p>Parallel pseudo-random number generation in Triton.</p></li>\n-</ul>\n-<section id=\"baseline\">\n-<h2>Baseline<a class=\"headerlink\" href=\"#baseline\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>The <em>dropout</em> operator was first introduced in <a class=\"reference internal\" href=\"#srivastava2014\" id=\"id1\"><span>[SRIVASTAVA2014]</span></a> as a way to improve the performance\n-of deep neural networks in low-data regime (i.e. regularization).</p>\n-<p>It takes a vector as input and produces a vector of the same shape as output. Each scalar in the\n-output has a probability <span class=\"math notranslate nohighlight\">\\(p\\)</span> of being changed to zero and otherwise it is copied from the input.\n-This forces the network to perform well even when only <span class=\"math notranslate nohighlight\">\\(1 - p\\)</span> scalars from the input are available.</p>\n-<p>At evaluation time we want to use the full power of the network so we set <span class=\"math notranslate nohighlight\">\\(p=0\\)</span>. Naively this would\n-increase the norm of the output (which can be a bad thing, e.g. it can lead to artificial decrease\n-in the output softmax temperature). To prevent this we multiply the output by <span class=\"math notranslate nohighlight\">\\(\\frac{1}{1 - p}\\)</span>, which\n-keeps the norm consistent regardless of the dropout probability.</p>\n-<p>Let\u2019s first take a look at the baseline implementation.</p>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">tabulate</span>\n-<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n-\n-<span class=\"kn\">import</span> <span class=\"nn\">triton</span>\n-<span class=\"kn\">import</span> <span class=\"nn\">triton.language</span> <span class=\"k\">as</span> <span class=\"nn\">tl</span>\n-\n-\n-<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n-<span class=\"k\">def</span> <span class=\"nf\">_dropout</span><span class=\"p\">(</span>\n-    <span class=\"n\">x_ptr</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the input</span>\n-    <span class=\"n\">x_keep_ptr</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to a mask of 0s and 1s</span>\n-    <span class=\"n\">output_ptr</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the output</span>\n-    <span class=\"n\">n_elements</span><span class=\"p\">,</span>  <span class=\"c1\"># number of elements in the `x` tensor</span>\n-    <span class=\"n\">p</span><span class=\"p\">,</span>  <span class=\"c1\"># probability that an element of `x` is changed to zero</span>\n-    <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n-<span class=\"p\">):</span>\n-    <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-    <span class=\"n\">block_start</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE</span>\n-    <span class=\"n\">offsets</span> <span class=\"o\">=</span> <span class=\"n\">block_start</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">)</span>\n-    <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">offsets</span> <span class=\"o\">&lt;</span> <span class=\"n\">n_elements</span>\n-    <span class=\"c1\"># Load data</span>\n-    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">x_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n-    <span class=\"n\">x_keep</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">x_keep_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># The line below is the crucial part, described in the paragraph above!</span>\n-    <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">x_keep</span><span class=\"p\">,</span> <span class=\"n\">x</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">p</span><span class=\"p\">),</span> <span class=\"mf\">0.0</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># Write-back output</span>\n-    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">output_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n-\n-\n-<span class=\"k\">def</span> <span class=\"nf\">dropout</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">x_keep</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"p\">):</span>\n-    <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n-    <span class=\"k\">assert</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">is_contiguous</span><span class=\"p\">()</span>\n-    <span class=\"n\">n_elements</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">numel</span><span class=\"p\">()</span>\n-    <span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">meta</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">meta</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">]),)</span>\n-    <span class=\"n\">_dropout</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">x_keep</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"o\">=</span><span class=\"mi\">1024</span><span class=\"p\">)</span>\n-    <span class=\"k\">return</span> <span class=\"n\">output</span>\n-\n-\n-<span class=\"c1\"># Input tensor</span>\n-<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,))</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"p\">()</span>\n-<span class=\"c1\"># Dropout mask</span>\n-<span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>\n-<span class=\"n\">x_keep</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,))</span> <span class=\"o\">&gt;</span> <span class=\"n\">p</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"p\">()</span>\n-<span class=\"c1\">#</span>\n-<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">dropout</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">x_keep</span><span class=\"o\">=</span><span class=\"n\">x_keep</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"o\">=</span><span class=\"n\">p</span><span class=\"p\">)</span>\n-<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">tabulate</span><span class=\"o\">.</span><span class=\"n\">tabulate</span><span class=\"p\">([</span>\n-    <span class=\"p\">[</span><span class=\"s2\">&quot;input&quot;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">(),</span>\n-    <span class=\"p\">[</span><span class=\"s2\">&quot;keep mask&quot;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">x_keep</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">(),</span>\n-    <span class=\"p\">[</span><span class=\"s2\">&quot;output&quot;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">()</span>\n-<span class=\"p\">]))</span>\n-</pre></div>\n-</div>\n-<div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>---------  -------  ---------  --------  --------  --------  --------  --------  --------  ---------  ---------\n-input      1.541    -0.293429  -2.17879  0.568431  -1.08452  -1.3986   0.403347  0.838026  -0.719258  -0.403344\n-keep mask  1         1          0        1          0         1        1         0          0          0\n-output     3.08199  -0.586858   0        1.13686    0        -2.79719  0.806694  0          0          0\n----------  -------  ---------  --------  --------  --------  --------  --------  --------  ---------  ---------\n-</pre></div>\n-</div>\n-</section>\n-<section id=\"seeded-dropout\">\n-<h2>Seeded dropout<a class=\"headerlink\" href=\"#seeded-dropout\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>The above implementation of dropout works fine, but it can be a bit awkward to deal with. Firstly\n-we need to store the dropout mask for backpropagation. Secondly, dropout state management can get\n-very tricky when using recompute/checkpointing (e.g. see all the notes about <cite>preserve_rng_state</cite> in\n-<a class=\"reference external\" href=\"https://pytorch.org/docs/1.9.0/checkpoint.html\">https://pytorch.org/docs/1.9.0/checkpoint.html</a>). In this tutorial we\u2019ll describe an alternative implementation\n-that (1) has a smaller memory footprint; (2) requires less data movement; and (3) simplifies the management\n-of persisting randomness across multiple invocations of the kernel.</p>\n-<p>Pseudo-random number generation in Triton is simple! In this tutorial we will use the\n-<code class=\"code docutils literal notranslate\"><span class=\"pre\">triton.language.rand</span></code> function which generates a block of uniformly distributed <code class=\"code docutils literal notranslate\"><span class=\"pre\">float32</span></code>\n-values in [0, 1), given a seed and a block of <code class=\"code docutils literal notranslate\"><span class=\"pre\">int32</span></code> offsets. But if you need it, Triton also provides\n-other <a class=\"reference internal\" href=\"../../python-api/triton.language.html#random-number-generation\"><span class=\"std std-ref\">random number generation strategies</span></a>.</p>\n-<div class=\"admonition note\">\n-<p class=\"admonition-title\">Note</p>\n-<p>Triton\u2019s implementation of PRNG is based on the Philox algorithm (described on <a class=\"reference internal\" href=\"#salmon2011\" id=\"id2\"><span>[SALMON2011]</span></a>).</p>\n-</div>\n-<p>Let\u2019s put it all together.</p>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n-<span class=\"k\">def</span> <span class=\"nf\">_seeded_dropout</span><span class=\"p\">(</span>\n-    <span class=\"n\">x_ptr</span><span class=\"p\">,</span>\n-    <span class=\"n\">output_ptr</span><span class=\"p\">,</span>\n-    <span class=\"n\">n_elements</span><span class=\"p\">,</span>\n-    <span class=\"n\">p</span><span class=\"p\">,</span>\n-    <span class=\"n\">seed</span><span class=\"p\">,</span>\n-    <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n-<span class=\"p\">):</span>\n-    <span class=\"c1\"># compute memory offsets of elements handled by this instance</span>\n-    <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-    <span class=\"n\">block_start</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE</span>\n-    <span class=\"n\">offsets</span> <span class=\"o\">=</span> <span class=\"n\">block_start</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># load data from x</span>\n-    <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">offsets</span> <span class=\"o\">&lt;</span> <span class=\"n\">n_elements</span>\n-    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">x_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># randomly prune it</span>\n-    <span class=\"n\">random</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">seed</span><span class=\"p\">,</span> <span class=\"n\">offsets</span><span class=\"p\">)</span>\n-    <span class=\"n\">x_keep</span> <span class=\"o\">=</span> <span class=\"n\">random</span> <span class=\"o\">&gt;</span> <span class=\"n\">p</span>\n-    <span class=\"c1\"># write-back</span>\n-    <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">x_keep</span><span class=\"p\">,</span> <span class=\"n\">x</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">p</span><span class=\"p\">),</span> <span class=\"mf\">0.0</span><span class=\"p\">)</span>\n-    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">output_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n-\n-\n-<span class=\"k\">def</span> <span class=\"nf\">seeded_dropout</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"p\">):</span>\n-    <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n-    <span class=\"k\">assert</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">is_contiguous</span><span class=\"p\">()</span>\n-    <span class=\"n\">n_elements</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">numel</span><span class=\"p\">()</span>\n-    <span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">meta</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">meta</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">]),)</span>\n-    <span class=\"n\">_seeded_dropout</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"o\">=</span><span class=\"mi\">1024</span><span class=\"p\">)</span>\n-    <span class=\"k\">return</span> <span class=\"n\">output</span>\n-\n-\n-<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,))</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"p\">()</span>\n-<span class=\"c1\"># Compare this to the baseline - dropout mask is never instantiated!</span>\n-<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">seeded_dropout</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"mi\">123</span><span class=\"p\">)</span>\n-<span class=\"n\">output2</span> <span class=\"o\">=</span> <span class=\"n\">seeded_dropout</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"mi\">123</span><span class=\"p\">)</span>\n-<span class=\"n\">output3</span> <span class=\"o\">=</span> <span class=\"n\">seeded_dropout</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"mi\">512</span><span class=\"p\">)</span>\n-\n-<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">tabulate</span><span class=\"o\">.</span><span class=\"n\">tabulate</span><span class=\"p\">([</span>\n-    <span class=\"p\">[</span><span class=\"s2\">&quot;input&quot;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">(),</span>\n-    <span class=\"p\">[</span><span class=\"s2\">&quot;output (seed = 123)&quot;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">(),</span>\n-    <span class=\"p\">[</span><span class=\"s2\">&quot;output (seed = 123)&quot;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">output2</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">(),</span>\n-    <span class=\"p\">[</span><span class=\"s2\">&quot;output (seed = 512)&quot;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">output3</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">()</span>\n-<span class=\"p\">]))</span>\n-</pre></div>\n-</div>\n-<div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>-------------------  ---------  --------  --------  -------  --------  --------  ---------  ---------  ---------  ---------\n-input                -0.952835  0.371721  0.408716  1.42142  0.149397  -0.67086  -0.214186  -0.431969  -0.707878  -0.106434\n-output (seed = 123)   0         0.743443  0         0        0         -1.34172   0          0         -1.41576   -0.212868\n-output (seed = 123)   0         0.743443  0         0        0         -1.34172   0          0         -1.41576   -0.212868\n-output (seed = 512)   0         0         0.817432  2.84284  0         -1.34172  -0.428372   0          0          0\n--------------------  ---------  --------  --------  -------  --------  --------  ---------  ---------  ---------  ---------\n-</pre></div>\n-</div>\n-<p>Et Voil\u00e0! We have a triton kernel that applies the same dropout mask provided the seed is the same!\n-If you\u2019d like explore further applications of pseudorandomness in GPU programming, we encourage you\n-to explore the <cite>triton/language/random</cite> folder!</p>\n-</section>\n-<section id=\"exercises\">\n-<h2>Exercises<a class=\"headerlink\" href=\"#exercises\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<ol class=\"arabic simple\">\n-<li><p>Extend the kernel to operate over a matrix and use a vector of seeds - one per row.</p></li>\n-<li><p>Add support for striding.</p></li>\n-<li><p>(challenge) Implement a kernel for sparse Johnson-Lindenstrauss transform which generates the projection matrix one the fly each time using a seed.</p></li>\n-</ol>\n-</section>\n-<section id=\"references\">\n-<h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<div role=\"list\" class=\"citation-list\">\n-<div class=\"citation\" id=\"salmon2011\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id2\">SALMON2011</a><span class=\"fn-bracket\">]</span></span>\n-<p>John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw, \u201cParallel Random Numbers: As Easy as 1, 2, 3\u201d, 2011</p>\n-</div>\n-<div class=\"citation\" id=\"srivastava2014\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id1\">SRIVASTAVA2014</a><span class=\"fn-bracket\">]</span></span>\n-<p>Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \u201cDropout: A Simple Way to Prevent Neural Networks from Overfitting\u201d, JMLR 2014</p>\n-</div>\n-</div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.607 seconds)</p>\n-<div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-04-low-memory-dropout-py\">\n-<div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n-<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/c9aed78977a4c05741d675a38dde3d7d/04-low-memory-dropout.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">04-low-memory-dropout.py</span></code></a></p>\n-</div>\n-<div class=\"sphx-glr-download sphx-glr-download-jupyter docutils container\">\n-<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/bc847dec325798bdc436c4ef5ac8b78a/04-low-memory-dropout.ipynb\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Jupyter</span> <span class=\"pre\">notebook:</span> <span class=\"pre\">04-low-memory-dropout.ipynb</span></code></a></p>\n-</div>\n-</div>\n-<p class=\"sphx-glr-signature\"><a class=\"reference external\" href=\"https://sphinx-gallery.github.io\">Gallery generated by Sphinx-Gallery</a></p>\n-</section>\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"03-matrix-multiplication.html\" class=\"btn btn-neutral float-left\" title=\"Matrix Multiplication\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"05-layer-norm.html\" class=\"btn btn-neutral float-right\" title=\"Layer Normalization\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"04-low-memory-dropout.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/getting-started/tutorials/05-layer-norm.html", "status": "removed", "additions": 0, "deletions": 561, "changes": 561, "file_content_changes": "@@ -1,561 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>Layer Normalization &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-        <script async=\"async\" src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"Fused Attention\" href=\"06-fused-attention.html\" />\n-    <link rel=\"prev\" title=\"Low-Memory Dropout\" href=\"04-low-memory-dropout.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../installation.html\">Installation</a></li>\n-<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"index.html\">Tutorials</a><ul class=\"current\">\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"01-vector-add.html\">Vector Addition</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"02-fused-softmax.html\">Fused Softmax</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n-<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Layer Normalization</a><ul>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#motivations\">Motivations</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#backward-pass\">Backward pass</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#benchmark\">Benchmark</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#references\">References</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"08-experimental-block-pointer.html\">Block Pointer (Experimental)</a></li>\n-</ul>\n-</li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-          <li class=\"breadcrumb-item\"><a href=\"index.html\">Tutorials</a></li>\n-      <li class=\"breadcrumb-item active\">Layer Normalization</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/getting-started/tutorials/05-layer-norm.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <div class=\"sphx-glr-download-link-note admonition note\">\n-<p class=\"admonition-title\">Note</p>\n-<p><a class=\"reference internal\" href=\"#sphx-glr-download-getting-started-tutorials-05-layer-norm-py\"><span class=\"std std-ref\">Go to the end</span></a>\n-to download the full example code</p>\n-</div>\n-<section class=\"sphx-glr-example-title\" id=\"layer-normalization\">\n-<span id=\"sphx-glr-getting-started-tutorials-05-layer-norm-py\"></span><h1>Layer Normalization<a class=\"headerlink\" href=\"#layer-normalization\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<p>In this tutorial, you will write a high-performance layer normalization\n-kernel that runs faster than the PyTorch implementation.</p>\n-<p>In doing so, you will learn about:</p>\n-<ul class=\"simple\">\n-<li><p>Implementing backward pass in Triton.</p></li>\n-<li><p>Implementing parallel reduction in Triton.</p></li>\n-</ul>\n-<section id=\"motivations\">\n-<h2>Motivations<a class=\"headerlink\" href=\"#motivations\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>The <em>LayerNorm</em> operator was first introduced in <a class=\"reference internal\" href=\"#ba2016\" id=\"id1\"><span>[BA2016]</span></a> as a way to improve the performance\n-of sequential models (e.g., Transformers) or neural networks with small batch size.\n-It takes a vector <span class=\"math notranslate nohighlight\">\\(x\\)</span> as input and produces a vector <span class=\"math notranslate nohighlight\">\\(y\\)</span> of the same shape as output.\n-The normalization is performed by subtracting the mean and dividing by the standard deviation of <span class=\"math notranslate nohighlight\">\\(x\\)</span>.\n-After the normalization, a learnable linear transformation with weights <span class=\"math notranslate nohighlight\">\\(w\\)</span> and biases <span class=\"math notranslate nohighlight\">\\(b\\)</span> is applied.\n-The forward pass can be expressed as follows:</p>\n-<div class=\"math notranslate nohighlight\">\n-\\[y = \\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} } * w + b\\]</div>\n-<p>where <span class=\"math notranslate nohighlight\">\\(\\epsilon\\)</span> is a small constant added to the denominator for numerical stability.\n-Let\u2019s first take a look at the forward pass implementation.</p>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n-\n-<span class=\"kn\">import</span> <span class=\"nn\">triton</span>\n-<span class=\"kn\">import</span> <span class=\"nn\">triton.language</span> <span class=\"k\">as</span> <span class=\"nn\">tl</span>\n-\n-<span class=\"k\">try</span><span class=\"p\">:</span>\n-    <span class=\"c1\"># This is https://github.com/NVIDIA/apex, NOT the apex on PyPi, so it</span>\n-    <span class=\"c1\"># should not be added to extras_require in setup.py.</span>\n-    <span class=\"kn\">import</span> <span class=\"nn\">apex</span>\n-    <span class=\"n\">HAS_APEX</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>\n-<span class=\"k\">except</span> <span class=\"ne\">ModuleNotFoundError</span><span class=\"p\">:</span>\n-    <span class=\"n\">HAS_APEX</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n-\n-\n-<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n-<span class=\"k\">def</span> <span class=\"nf\">_layer_norm_fwd_fused</span><span class=\"p\">(</span>\n-    <span class=\"n\">X</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the input</span>\n-    <span class=\"n\">Y</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the output</span>\n-    <span class=\"n\">W</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the weights</span>\n-    <span class=\"n\">B</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the biases</span>\n-    <span class=\"n\">Mean</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the mean</span>\n-    <span class=\"n\">Rstd</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the 1/std</span>\n-    <span class=\"n\">stride</span><span class=\"p\">,</span>  <span class=\"c1\"># how much to increase the pointer when moving by 1 row</span>\n-    <span class=\"n\">N</span><span class=\"p\">,</span>  <span class=\"c1\"># number of columns in X</span>\n-    <span class=\"n\">eps</span><span class=\"p\">,</span>  <span class=\"c1\"># epsilon to avoid division by zero</span>\n-    <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n-<span class=\"p\">):</span>\n-    <span class=\"c1\"># Map the program id to the row of X and Y it should compute.</span>\n-    <span class=\"n\">row</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-    <span class=\"n\">Y</span> <span class=\"o\">+=</span> <span class=\"n\">row</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n-    <span class=\"n\">X</span> <span class=\"o\">+=</span> <span class=\"n\">row</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n-    <span class=\"c1\"># Compute mean</span>\n-    <span class=\"n\">mean</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n-    <span class=\"n\">_mean</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_SIZE</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n-    <span class=\"k\">for</span> <span class=\"n\">off</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">):</span>\n-        <span class=\"n\">cols</span> <span class=\"o\">=</span> <span class=\"n\">off</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">)</span>\n-        <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">X</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">cols</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n-        <span class=\"n\">_mean</span> <span class=\"o\">+=</span> <span class=\"n\">a</span>\n-    <span class=\"n\">mean</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">_mean</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">N</span>\n-    <span class=\"c1\"># Compute variance</span>\n-    <span class=\"n\">_var</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_SIZE</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n-    <span class=\"k\">for</span> <span class=\"n\">off</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">):</span>\n-        <span class=\"n\">cols</span> <span class=\"o\">=</span> <span class=\"n\">off</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">)</span>\n-        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">X</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">cols</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n-        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">cols</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">mean</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">)</span>\n-        <span class=\"n\">_var</span> <span class=\"o\">+=</span> <span class=\"n\">x</span> <span class=\"o\">*</span> <span class=\"n\">x</span>\n-    <span class=\"n\">var</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">_var</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">N</span>\n-    <span class=\"n\">rstd</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"o\">/</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">var</span> <span class=\"o\">+</span> <span class=\"n\">eps</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># Write mean / rstd</span>\n-    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">Mean</span> <span class=\"o\">+</span> <span class=\"n\">row</span><span class=\"p\">,</span> <span class=\"n\">mean</span><span class=\"p\">)</span>\n-    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">Rstd</span> <span class=\"o\">+</span> <span class=\"n\">row</span><span class=\"p\">,</span> <span class=\"n\">rstd</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># Normalize and apply linear transformation</span>\n-    <span class=\"k\">for</span> <span class=\"n\">off</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">):</span>\n-        <span class=\"n\">cols</span> <span class=\"o\">=</span> <span class=\"n\">off</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">)</span>\n-        <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">cols</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span>\n-        <span class=\"n\">w</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">W</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n-        <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">B</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n-        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">X</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n-        <span class=\"n\">x_hat</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">mean</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">rstd</span>\n-        <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">x_hat</span> <span class=\"o\">*</span> <span class=\"n\">w</span> <span class=\"o\">+</span> <span class=\"n\">b</span>\n-        <span class=\"c1\"># Write output</span>\n-        <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">Y</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n-</pre></div>\n-</div>\n-</section>\n-<section id=\"backward-pass\">\n-<h2>Backward pass<a class=\"headerlink\" href=\"#backward-pass\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>The backward pass for the layer normalization operator is a bit more involved than the forward pass.\n-Let <span class=\"math notranslate nohighlight\">\\(\\hat{x}\\)</span> be the normalized inputs <span class=\"math notranslate nohighlight\">\\(\\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} }\\)</span> before the linear transformation,\n-the Vector-Jacobian Products (VJP) <span class=\"math notranslate nohighlight\">\\(\\nabla_{x}\\)</span> of <span class=\"math notranslate nohighlight\">\\(x\\)</span> are given by:</p>\n-<div class=\"math notranslate nohighlight\">\n-\\[\\nabla_{x} = \\frac{1}{\\sigma}\\Big( \\nabla_{y} \\odot w - \\underbrace{ \\big( \\frac{1}{N} \\hat{x} \\cdot (\\nabla_{y} \\odot w) \\big) }_{c_1} \\odot \\hat{x} - \\underbrace{ \\frac{1}{N} \\nabla_{y} \\cdot w }_{c_2} \\Big)\\]</div>\n-<p>where <span class=\"math notranslate nohighlight\">\\(\\odot\\)</span> denotes the element-wise multiplication, <span class=\"math notranslate nohighlight\">\\(\\cdot\\)</span> denotes the dot product, and <span class=\"math notranslate nohighlight\">\\(\\sigma\\)</span> is the standard deviation.\n-<span class=\"math notranslate nohighlight\">\\(c_1\\)</span> and <span class=\"math notranslate nohighlight\">\\(c_2\\)</span> are intermediate constants that improve the readability of the following implementation.</p>\n-<p>For the weights <span class=\"math notranslate nohighlight\">\\(w\\)</span> and biases <span class=\"math notranslate nohighlight\">\\(b\\)</span>, the VJPs <span class=\"math notranslate nohighlight\">\\(\\nabla_{w}\\)</span> and <span class=\"math notranslate nohighlight\">\\(\\nabla_{b}\\)</span> are more straightforward:</p>\n-<div class=\"math notranslate nohighlight\">\n-\\[\\nabla_{w} = \\nabla_{y} \\odot \\hat{x} \\quad \\text{and} \\quad \\nabla_{b} = \\nabla_{y}\\]</div>\n-<p>Since the same weights <span class=\"math notranslate nohighlight\">\\(w\\)</span> and biases <span class=\"math notranslate nohighlight\">\\(b\\)</span> are used for all rows in the same batch, their gradients need to sum up.\n-To perform this step efficiently, we use a parallel reduction strategy: each kernel instance accumulates\n-partial <span class=\"math notranslate nohighlight\">\\(\\nabla_{w}\\)</span> and <span class=\"math notranslate nohighlight\">\\(\\nabla_{b}\\)</span> across certain rows into one of <span class=\"math notranslate nohighlight\">\\(\\text{GROUP_SIZE_M}\\)</span> independent buffers.\n-These buffers stay in the L2 cache and then are further reduced by another function to compute the actual <span class=\"math notranslate nohighlight\">\\(\\nabla_{w}\\)</span> and <span class=\"math notranslate nohighlight\">\\(\\nabla_{b}\\)</span>.</p>\n-<p>Let the number of input rows <span class=\"math notranslate nohighlight\">\\(M = 4\\)</span> and <span class=\"math notranslate nohighlight\">\\(\\text{GROUP_SIZE_M} = 2\\)</span>,\n-here\u2019s a diagram of the parallel reduction strategy for <span class=\"math notranslate nohighlight\">\\(\\nabla_{w}\\)</span> (<span class=\"math notranslate nohighlight\">\\(\\nabla_{b}\\)</span> is omitted for brevity):</p>\n-<blockquote>\n-<div><img alt=\"../../_images/parallel_reduction.png\" src=\"../../_images/parallel_reduction.png\" />\n-</div></blockquote>\n-<p>In Stage 1, the rows of X that have the same color share the same buffer and thus a lock is used to ensure that only one kernel instance writes to the buffer at a time.\n-In Stage 2, the buffers are further reduced to compute the final <span class=\"math notranslate nohighlight\">\\(\\nabla_{w}\\)</span> and <span class=\"math notranslate nohighlight\">\\(\\nabla_{b}\\)</span>.\n-In the following implementation, Stage 1 is implemented by the function <code class=\"code docutils literal notranslate\"><span class=\"pre\">_layer_norm_bwd_dx_fused</span></code> and Stage 2 is implemented by the function <code class=\"code docutils literal notranslate\"><span class=\"pre\">_layer_norm_bwd_dwdb</span></code>.</p>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n-<span class=\"k\">def</span> <span class=\"nf\">_layer_norm_bwd_dx_fused</span><span class=\"p\">(</span>\n-    <span class=\"n\">DX</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the input gradient</span>\n-    <span class=\"n\">DY</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the output gradient</span>\n-    <span class=\"n\">DW</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the partial sum of weights gradient</span>\n-    <span class=\"n\">DB</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the partial sum of biases gradient</span>\n-    <span class=\"n\">X</span><span class=\"p\">,</span>   <span class=\"c1\"># pointer to the input</span>\n-    <span class=\"n\">W</span><span class=\"p\">,</span>   <span class=\"c1\"># pointer to the weights</span>\n-    <span class=\"n\">B</span><span class=\"p\">,</span>   <span class=\"c1\"># pointer to the biases</span>\n-    <span class=\"n\">Mean</span><span class=\"p\">,</span>   <span class=\"c1\"># pointer to the mean</span>\n-    <span class=\"n\">Rstd</span><span class=\"p\">,</span>   <span class=\"c1\"># pointer to the 1/std</span>\n-    <span class=\"n\">Lock</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the lock</span>\n-    <span class=\"n\">stride</span><span class=\"p\">,</span>  <span class=\"c1\"># how much to increase the pointer when moving by 1 row</span>\n-    <span class=\"n\">N</span><span class=\"p\">,</span>  <span class=\"c1\"># number of columns in X</span>\n-    <span class=\"n\">eps</span><span class=\"p\">,</span>  <span class=\"c1\"># epsilon to avoid division by zero</span>\n-    <span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n-    <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span>\n-<span class=\"p\">):</span>\n-    <span class=\"c1\"># Map the program id to the elements of X, DX, and DY it should compute.</span>\n-    <span class=\"n\">row</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-    <span class=\"n\">cols</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)</span>\n-    <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">cols</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span>\n-    <span class=\"n\">X</span> <span class=\"o\">+=</span> <span class=\"n\">row</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n-    <span class=\"n\">DY</span> <span class=\"o\">+=</span> <span class=\"n\">row</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n-    <span class=\"n\">DX</span> <span class=\"o\">+=</span> <span class=\"n\">row</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n-    <span class=\"c1\"># Offset locks and weights/biases gradient pointer for parallel reduction</span>\n-    <span class=\"n\">lock_id</span> <span class=\"o\">=</span> <span class=\"n\">row</span> <span class=\"o\">%</span> <span class=\"n\">GROUP_SIZE_M</span>\n-    <span class=\"n\">Lock</span> <span class=\"o\">+=</span> <span class=\"n\">lock_id</span>\n-    <span class=\"n\">Count</span> <span class=\"o\">=</span> <span class=\"n\">Lock</span> <span class=\"o\">+</span> <span class=\"n\">GROUP_SIZE_M</span>\n-    <span class=\"n\">DW</span> <span class=\"o\">=</span> <span class=\"n\">DW</span> <span class=\"o\">+</span> <span class=\"n\">lock_id</span> <span class=\"o\">*</span> <span class=\"n\">N</span> <span class=\"o\">+</span> <span class=\"n\">cols</span>\n-    <span class=\"n\">DB</span> <span class=\"o\">=</span> <span class=\"n\">DB</span> <span class=\"o\">+</span> <span class=\"n\">lock_id</span> <span class=\"o\">*</span> <span class=\"n\">N</span> <span class=\"o\">+</span> <span class=\"n\">cols</span>\n-    <span class=\"c1\"># Load data to SRAM</span>\n-    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">X</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n-    <span class=\"n\">dy</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">DY</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n-    <span class=\"n\">w</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">W</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n-    <span class=\"n\">mean</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">Mean</span> <span class=\"o\">+</span> <span class=\"n\">row</span><span class=\"p\">)</span>\n-    <span class=\"n\">rstd</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">Rstd</span> <span class=\"o\">+</span> <span class=\"n\">row</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># Compute dx</span>\n-    <span class=\"n\">xhat</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">mean</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">rstd</span>\n-    <span class=\"n\">wdy</span> <span class=\"o\">=</span> <span class=\"n\">w</span> <span class=\"o\">*</span> <span class=\"n\">dy</span>\n-    <span class=\"n\">xhat</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">xhat</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">)</span>\n-    <span class=\"n\">wdy</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">wdy</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">)</span>\n-    <span class=\"n\">c1</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">xhat</span> <span class=\"o\">*</span> <span class=\"n\">wdy</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">N</span>\n-    <span class=\"n\">c2</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">wdy</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">N</span>\n-    <span class=\"n\">dx</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">wdy</span> <span class=\"o\">-</span> <span class=\"p\">(</span><span class=\"n\">xhat</span> <span class=\"o\">*</span> <span class=\"n\">c1</span> <span class=\"o\">+</span> <span class=\"n\">c2</span><span class=\"p\">))</span> <span class=\"o\">*</span> <span class=\"n\">rstd</span>\n-    <span class=\"c1\"># Write dx</span>\n-    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">DX</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">dx</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># Accumulate partial sums for dw/db</span>\n-    <span class=\"n\">partial_dw</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">dy</span> <span class=\"o\">*</span> <span class=\"n\">xhat</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n-    <span class=\"n\">partial_db</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">dy</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n-    <span class=\"k\">while</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">atomic_cas</span><span class=\"p\">(</span><span class=\"n\">Lock</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n-        <span class=\"k\">pass</span>\n-    <span class=\"n\">count</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">Count</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># First store doesn&#39;t accumulate</span>\n-    <span class=\"k\">if</span> <span class=\"n\">count</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n-        <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">atomic_xchg</span><span class=\"p\">(</span><span class=\"n\">Count</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n-    <span class=\"k\">else</span><span class=\"p\">:</span>\n-        <span class=\"n\">partial_dw</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">DW</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n-        <span class=\"n\">partial_db</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">DB</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n-    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">DW</span><span class=\"p\">,</span> <span class=\"n\">partial_dw</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n-    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">DB</span><span class=\"p\">,</span> <span class=\"n\">partial_db</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># Release the lock</span>\n-    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">atomic_xchg</span><span class=\"p\">(</span><span class=\"n\">Lock</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n-\n-\n-<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n-<span class=\"k\">def</span> <span class=\"nf\">_layer_norm_bwd_dwdb</span><span class=\"p\">(</span>\n-    <span class=\"n\">DW</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the partial sum of weights gradient</span>\n-    <span class=\"n\">DB</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the partial sum of biases gradient</span>\n-    <span class=\"n\">FINAL_DW</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the weights gradient</span>\n-    <span class=\"n\">FINAL_DB</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the biases gradient</span>\n-    <span class=\"n\">M</span><span class=\"p\">,</span>  <span class=\"c1\"># GROUP_SIZE_M</span>\n-    <span class=\"n\">N</span><span class=\"p\">,</span>  <span class=\"c1\"># number of columns</span>\n-    <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n-    <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span>\n-<span class=\"p\">):</span>\n-    <span class=\"c1\"># Map the program id to the elements of DW and DB it should compute.</span>\n-    <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-    <span class=\"n\">cols</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_N</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)</span>\n-    <span class=\"n\">dw</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n-    <span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># Iterate through the rows of DW and DB to sum the partial sums.</span>\n-    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">):</span>\n-        <span class=\"n\">rows</span> <span class=\"o\">=</span> <span class=\"n\">i</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">)</span>\n-        <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">rows</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&lt;</span> <span class=\"n\">M</span><span class=\"p\">)</span> <span class=\"o\">&amp;</span> <span class=\"p\">(</span><span class=\"n\">cols</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n-        <span class=\"n\">offs</span> <span class=\"o\">=</span> <span class=\"n\">rows</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">N</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n-        <span class=\"n\">dw</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">DW</span> <span class=\"o\">+</span> <span class=\"n\">offs</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">)</span>\n-        <span class=\"n\">db</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">DB</span> <span class=\"o\">+</span> <span class=\"n\">offs</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># Write the final sum to the output.</span>\n-    <span class=\"n\">sum_dw</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">dw</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-    <span class=\"n\">sum_db</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">db</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">FINAL_DW</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">sum_dw</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">cols</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n-    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">FINAL_DB</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">sum_db</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">cols</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n-</pre></div>\n-</div>\n-</section>\n-<section id=\"benchmark\">\n-<h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>We can now compare the performance of our kernel against that of PyTorch.\n-Here we focus on inputs that have Less than 64KB per feature.\n-Specifically, one can set <code class=\"code docutils literal notranslate\"><span class=\"pre\">'mode':</span> <span class=\"pre\">'backward'</span></code> to benchmark the backward pass.</p>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"k\">class</span> <span class=\"nc\">LayerNorm</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">autograd</span><span class=\"o\">.</span><span class=\"n\">Function</span><span class=\"p\">):</span>\n-\n-    <span class=\"nd\">@staticmethod</span>\n-    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">normalized_shape</span><span class=\"p\">,</span> <span class=\"n\">weight</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"p\">):</span>\n-        <span class=\"c1\"># allocate output</span>\n-        <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n-        <span class=\"c1\"># reshape input data into 2D tensor</span>\n-        <span class=\"n\">x_arg</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n-        <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"n\">x_arg</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n-        <span class=\"n\">mean</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n-        <span class=\"n\">rstd</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n-        <span class=\"c1\"># Less than 64KB per feature: enqueue fused kernel</span>\n-        <span class=\"n\">MAX_FUSED_SIZE</span> <span class=\"o\">=</span> <span class=\"mi\">65536</span> <span class=\"o\">//</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">element_size</span><span class=\"p\">()</span>\n-        <span class=\"n\">BLOCK_SIZE</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">MAX_FUSED_SIZE</span><span class=\"p\">,</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">next_power_of_2</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">))</span>\n-        <span class=\"k\">if</span> <span class=\"n\">N</span> <span class=\"o\">&gt;</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">:</span>\n-            <span class=\"k\">raise</span> <span class=\"ne\">RuntimeError</span><span class=\"p\">(</span><span class=\"s2\">&quot;This layer norm doesn&#39;t support feature dim &gt;= 64KB.&quot;</span><span class=\"p\">)</span>\n-        <span class=\"c1\"># heuristics for number of warps</span>\n-        <span class=\"n\">num_warps</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"nb\">max</span><span class=\"p\">(</span><span class=\"n\">BLOCK_SIZE</span> <span class=\"o\">//</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"mi\">8</span><span class=\"p\">)</span>\n-        <span class=\"c1\"># enqueue kernel</span>\n-        <span class=\"n\">_layer_norm_fwd_fused</span><span class=\"p\">[(</span><span class=\"n\">M</span><span class=\"p\">,)](</span><span class=\"n\">x_arg</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">weight</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"p\">,</span> <span class=\"n\">mean</span><span class=\"p\">,</span> <span class=\"n\">rstd</span><span class=\"p\">,</span>\n-                                    <span class=\"n\">x_arg</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"p\">,</span>\n-                                    <span class=\"n\">BLOCK_SIZE</span><span class=\"o\">=</span><span class=\"n\">BLOCK_SIZE</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"n\">num_warps</span><span class=\"p\">)</span>\n-        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">save_for_backward</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">weight</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"p\">,</span> <span class=\"n\">mean</span><span class=\"p\">,</span> <span class=\"n\">rstd</span><span class=\"p\">)</span>\n-        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">BLOCK_SIZE</span> <span class=\"o\">=</span> <span class=\"n\">BLOCK_SIZE</span>\n-        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">num_warps</span> <span class=\"o\">=</span> <span class=\"n\">num_warps</span>\n-        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">eps</span> <span class=\"o\">=</span> <span class=\"n\">eps</span>\n-        <span class=\"k\">return</span> <span class=\"n\">y</span>\n-\n-    <span class=\"nd\">@staticmethod</span>\n-    <span class=\"k\">def</span> <span class=\"nf\">backward</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">,</span> <span class=\"n\">dy</span><span class=\"p\">):</span>\n-        <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">saved_tensors</span>\n-        <span class=\"c1\"># heuristics for amount of parallel reduction stream for DW/DB</span>\n-        <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n-        <span class=\"n\">GROUP_SIZE_M</span> <span class=\"o\">=</span> <span class=\"mi\">64</span>\n-        <span class=\"k\">if</span> <span class=\"n\">N</span> <span class=\"o\">&lt;=</span> <span class=\"mi\">8192</span><span class=\"p\">:</span> <span class=\"n\">GROUP_SIZE_M</span> <span class=\"o\">=</span> <span class=\"mi\">96</span>\n-        <span class=\"k\">if</span> <span class=\"n\">N</span> <span class=\"o\">&lt;=</span> <span class=\"mi\">4096</span><span class=\"p\">:</span> <span class=\"n\">GROUP_SIZE_M</span> <span class=\"o\">=</span> <span class=\"mi\">128</span>\n-        <span class=\"k\">if</span> <span class=\"n\">N</span> <span class=\"o\">&lt;=</span> <span class=\"mi\">1024</span><span class=\"p\">:</span> <span class=\"n\">GROUP_SIZE_M</span> <span class=\"o\">=</span> <span class=\"mi\">256</span>\n-        <span class=\"c1\"># allocate output</span>\n-        <span class=\"n\">locks</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n-        <span class=\"n\">_dw</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span>\n-        <span class=\"n\">_db</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span>\n-        <span class=\"n\">dw</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span>\n-        <span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span>\n-        <span class=\"n\">dx</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">dy</span><span class=\"p\">)</span>\n-        <span class=\"c1\"># enqueue kernel using forward pass heuristics</span>\n-        <span class=\"c1\"># also compute partial sums for DW and DB</span>\n-        <span class=\"n\">x_arg</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n-        <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"n\">x_arg</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n-        <span class=\"n\">_layer_norm_bwd_dx_fused</span><span class=\"p\">[(</span><span class=\"n\">M</span><span class=\"p\">,)](</span><span class=\"n\">dx</span><span class=\"p\">,</span> <span class=\"n\">dy</span><span class=\"p\">,</span> <span class=\"n\">_dw</span><span class=\"p\">,</span> <span class=\"n\">_db</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">locks</span><span class=\"p\">,</span>\n-                                       <span class=\"n\">x_arg</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">eps</span><span class=\"p\">,</span>\n-                                       <span class=\"n\">BLOCK_SIZE_N</span><span class=\"o\">=</span><span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">BLOCK_SIZE</span><span class=\"p\">,</span>\n-                                       <span class=\"n\">GROUP_SIZE_M</span><span class=\"o\">=</span><span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">,</span>\n-                                       <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">num_warps</span><span class=\"p\">)</span>\n-        <span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">meta</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">meta</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">])]</span>\n-        <span class=\"c1\"># accumulate partial sums in separate kernel</span>\n-        <span class=\"n\">_layer_norm_bwd_dwdb</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span><span class=\"n\">_dw</span><span class=\"p\">,</span> <span class=\"n\">_db</span><span class=\"p\">,</span> <span class=\"n\">dw</span><span class=\"p\">,</span> <span class=\"n\">db</span><span class=\"p\">,</span> <span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span>\n-                                   <span class=\"n\">BLOCK_SIZE_M</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">,</span>\n-                                   <span class=\"n\">BLOCK_SIZE_N</span><span class=\"o\">=</span><span class=\"mi\">128</span><span class=\"p\">)</span>\n-        <span class=\"k\">return</span> <span class=\"n\">dx</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">dw</span><span class=\"p\">,</span> <span class=\"n\">db</span><span class=\"p\">,</span> <span class=\"kc\">None</span>\n-\n-\n-<span class=\"n\">layer_norm</span> <span class=\"o\">=</span> <span class=\"n\">LayerNorm</span><span class=\"o\">.</span><span class=\"n\">apply</span>\n-\n-\n-<span class=\"k\">def</span> <span class=\"nf\">test_layer_norm</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-5</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">):</span>\n-    <span class=\"c1\"># create data</span>\n-    <span class=\"n\">x_shape</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n-    <span class=\"n\">w_shape</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">x_shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">)</span>\n-    <span class=\"n\">weight</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">w_shape</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n-    <span class=\"n\">bias</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">w_shape</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n-    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mf\">2.3</span> <span class=\"o\">+</span> <span class=\"mf\">0.5</span> <span class=\"o\">*</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">x_shape</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n-    <span class=\"n\">dy</span> <span class=\"o\">=</span> <span class=\"mf\">.1</span> <span class=\"o\">*</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn_like</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n-    <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">requires_grad_</span><span class=\"p\">(</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># forward pass</span>\n-    <span class=\"n\">y_tri</span> <span class=\"o\">=</span> <span class=\"n\">layer_norm</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">w_shape</span><span class=\"p\">,</span> <span class=\"n\">weight</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"p\">)</span>\n-    <span class=\"n\">y_ref</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">functional</span><span class=\"o\">.</span><span class=\"n\">layer_norm</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">w_shape</span><span class=\"p\">,</span> <span class=\"n\">weight</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># backward pass (triton)</span>\n-    <span class=\"n\">y_tri</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">(</span><span class=\"n\">dy</span><span class=\"p\">,</span> <span class=\"n\">retain_graph</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n-    <span class=\"n\">dx_tri</span><span class=\"p\">,</span> <span class=\"n\">dw_tri</span><span class=\"p\">,</span> <span class=\"n\">db_tri</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">_</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">clone</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">weight</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"p\">]]</span>\n-    <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"p\">,</span> <span class=\"n\">weight</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span>\n-    <span class=\"c1\"># backward pass (torch)</span>\n-    <span class=\"n\">y_ref</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">(</span><span class=\"n\">dy</span><span class=\"p\">,</span> <span class=\"n\">retain_graph</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n-    <span class=\"n\">dx_ref</span><span class=\"p\">,</span> <span class=\"n\">dw_ref</span><span class=\"p\">,</span> <span class=\"n\">db_ref</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">_</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">clone</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">weight</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"p\">]]</span>\n-    <span class=\"c1\"># compare</span>\n-    <span class=\"k\">assert</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">y_tri</span><span class=\"p\">,</span> <span class=\"n\">y_ref</span><span class=\"p\">,</span> <span class=\"n\">atol</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">,</span> <span class=\"n\">rtol</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-    <span class=\"k\">assert</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">dx_tri</span><span class=\"p\">,</span> <span class=\"n\">dx_ref</span><span class=\"p\">,</span> <span class=\"n\">atol</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">,</span> <span class=\"n\">rtol</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-    <span class=\"k\">assert</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">db_tri</span><span class=\"p\">,</span> <span class=\"n\">db_ref</span><span class=\"p\">,</span> <span class=\"n\">atol</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">,</span> <span class=\"n\">rtol</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-    <span class=\"k\">assert</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">dw_tri</span><span class=\"p\">,</span> <span class=\"n\">dw_ref</span><span class=\"p\">,</span> <span class=\"n\">atol</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">,</span> <span class=\"n\">rtol</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-\n-\n-<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">perf_report</span><span class=\"p\">(</span>\n-    <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">Benchmark</span><span class=\"p\">(</span>\n-        <span class=\"n\">x_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;N&#39;</span><span class=\"p\">],</span>\n-        <span class=\"n\">x_vals</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">512</span> <span class=\"o\">*</span> <span class=\"n\">i</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">32</span><span class=\"p\">)],</span>\n-        <span class=\"n\">line_arg</span><span class=\"o\">=</span><span class=\"s1\">&#39;provider&#39;</span><span class=\"p\">,</span>\n-        <span class=\"n\">line_vals</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;torch&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"p\">([</span><span class=\"s1\">&#39;apex&#39;</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">HAS_APEX</span> <span class=\"k\">else</span> <span class=\"p\">[]),</span>\n-        <span class=\"n\">line_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;Triton&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;Torch&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"p\">([</span><span class=\"s1\">&#39;Apex&#39;</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">HAS_APEX</span> <span class=\"k\">else</span> <span class=\"p\">[]),</span>\n-        <span class=\"n\">styles</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s1\">&#39;blue&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;green&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;orange&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">)],</span>\n-        <span class=\"n\">ylabel</span><span class=\"o\">=</span><span class=\"s1\">&#39;GB/s&#39;</span><span class=\"p\">,</span>\n-        <span class=\"n\">plot_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;layer-norm-backward&#39;</span><span class=\"p\">,</span>\n-        <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">4096</span><span class=\"p\">,</span> <span class=\"s1\">&#39;dtype&#39;</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">,</span> <span class=\"s1\">&#39;mode&#39;</span><span class=\"p\">:</span> <span class=\"s1\">&#39;backward&#39;</span><span class=\"p\">}</span>\n-    <span class=\"p\">)</span>\n-<span class=\"p\">)</span>\n-<span class=\"k\">def</span> <span class=\"nf\">bench_layer_norm</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">provider</span><span class=\"p\">,</span> <span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;backward&#39;</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-5</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">):</span>\n-    <span class=\"c1\"># create data</span>\n-    <span class=\"n\">x_shape</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n-    <span class=\"n\">w_shape</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">x_shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">)</span>\n-    <span class=\"n\">weight</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">w_shape</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n-    <span class=\"n\">bias</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">w_shape</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n-    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mf\">2.3</span> <span class=\"o\">+</span> <span class=\"mf\">0.5</span> <span class=\"o\">*</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">x_shape</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n-    <span class=\"n\">dy</span> <span class=\"o\">=</span> <span class=\"mf\">.1</span> <span class=\"o\">*</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn_like</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n-    <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">requires_grad_</span><span class=\"p\">(</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n-    <span class=\"n\">quantiles</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">]</span>\n-    <span class=\"c1\"># utility functions</span>\n-    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">:</span>\n-        <span class=\"n\">y_fwd</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">layer_norm</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">w_shape</span><span class=\"p\">,</span> <span class=\"n\">weight</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"p\">)</span>\n-    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;torch&#39;</span><span class=\"p\">:</span>\n-        <span class=\"n\">y_fwd</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">functional</span><span class=\"o\">.</span><span class=\"n\">layer_norm</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">w_shape</span><span class=\"p\">,</span> <span class=\"n\">weight</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"p\">)</span>\n-    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;apex&#39;</span><span class=\"p\">:</span>\n-        <span class=\"n\">apex_layer_norm</span> <span class=\"o\">=</span> <span class=\"n\">apex</span><span class=\"o\">.</span><span class=\"n\">normalization</span><span class=\"o\">.</span><span class=\"n\">FusedLayerNorm</span><span class=\"p\">(</span><span class=\"n\">w_shape</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n-        <span class=\"n\">y_fwd</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">apex_layer_norm</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># forward pass</span>\n-    <span class=\"k\">if</span> <span class=\"n\">mode</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;forward&#39;</span><span class=\"p\">:</span>\n-        <span class=\"n\">gbps</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">ms</span><span class=\"p\">:</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">numel</span><span class=\"p\">()</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">element_size</span><span class=\"p\">()</span> <span class=\"o\">/</span> <span class=\"n\">ms</span> <span class=\"o\">*</span> <span class=\"mf\">1e-6</span>\n-        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"n\">y_fwd</span><span class=\"p\">,</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">,</span> <span class=\"n\">rep</span><span class=\"o\">=</span><span class=\"mi\">500</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># backward pass</span>\n-    <span class=\"k\">if</span> <span class=\"n\">mode</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;backward&#39;</span><span class=\"p\">:</span>\n-        <span class=\"n\">gbps</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">ms</span><span class=\"p\">:</span> <span class=\"mi\">3</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">numel</span><span class=\"p\">()</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">element_size</span><span class=\"p\">()</span> <span class=\"o\">/</span> <span class=\"n\">ms</span> <span class=\"o\">*</span> <span class=\"mf\">1e-6</span>\n-        <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">y_fwd</span><span class=\"p\">()</span>\n-        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">(</span><span class=\"n\">dy</span><span class=\"p\">,</span> <span class=\"n\">retain_graph</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">),</span>\n-                                                     <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">,</span> <span class=\"n\">grad_to_none</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">x</span><span class=\"p\">],</span> <span class=\"n\">rep</span><span class=\"o\">=</span><span class=\"mi\">500</span><span class=\"p\">)</span>\n-    <span class=\"k\">return</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">ms</span><span class=\"p\">),</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">max_ms</span><span class=\"p\">),</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">min_ms</span><span class=\"p\">)</span>\n-\n-\n-<span class=\"n\">test_layer_norm</span><span class=\"p\">(</span><span class=\"mi\">1151</span><span class=\"p\">,</span> <span class=\"mi\">8192</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n-<span class=\"n\">bench_layer_norm</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;.&#39;</span><span class=\"p\">,</span> <span class=\"n\">print_data</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n-</pre></div>\n-</div>\n-<img src=\"../../_images/sphx_glr_05-layer-norm_001.png\" srcset=\"../../_images/sphx_glr_05-layer-norm_001.png\" alt=\"05 layer norm\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>layer-norm-backward:\n-          N       Triton       Torch\n-0    1024.0   170.666661  372.363633\n-1    1536.0   261.446814  438.857146\n-2    2048.0   346.140834  496.484863\n-3    2560.0   429.650357  529.655159\n-4    3072.0   511.999982  538.160602\n-5    3584.0   597.333312  470.032796\n-6    4096.0   668.734699  472.615390\n-7    4608.0   713.496767  480.834772\n-8    5120.0   768.000019  481.882369\n-9    5632.0   819.199976  489.739120\n-10   6144.0   862.315754  494.818794\n-11   6656.0   907.636357  499.200013\n-12   7168.0   945.230752  475.226530\n-13   7680.0   975.238103  479.999983\n-14   8192.0  1003.102074  487.861027\n-15   8704.0   673.858058  488.074767\n-16   9216.0   699.949388  491.520008\n-17   9728.0   727.327104  497.808111\n-18  10240.0   751.559663  498.498957\n-19  10752.0   774.918911  485.052653\n-20  11264.0   806.973159  488.853509\n-21  11776.0   819.199982  493.235604\n-22  12288.0   837.818175  499.005061\n-23  12800.0   843.956028  499.512174\n-24  13312.0   861.153634  499.200013\n-25  13824.0   861.755862  501.930388\n-26  14336.0   873.258878  492.928354\n-27  14848.0   877.714272  496.311981\n-28  15360.0   894.757295  501.551014\n-29  15872.0   887.944041  501.881412\n-</pre></div>\n-</div>\n-</section>\n-<section id=\"references\">\n-<h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<div role=\"list\" class=\"citation-list\">\n-<div class=\"citation\" id=\"ba2016\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id1\">BA2016</a><span class=\"fn-bracket\">]</span></span>\n-<p>Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \u201cLayer Normalization\u201d, Arxiv 2016</p>\n-</div>\n-</div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  28.955 seconds)</p>\n-<div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-05-layer-norm-py\">\n-<div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n-<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/935c0dd0fbeb4b2e69588471cbb2d4b2/05-layer-norm.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">05-layer-norm.py</span></code></a></p>\n-</div>\n-<div class=\"sphx-glr-download sphx-glr-download-jupyter docutils container\">\n-<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/ae7fff29e1b574187bc930ed94bcc353/05-layer-norm.ipynb\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Jupyter</span> <span class=\"pre\">notebook:</span> <span class=\"pre\">05-layer-norm.ipynb</span></code></a></p>\n-</div>\n-</div>\n-<p class=\"sphx-glr-signature\"><a class=\"reference external\" href=\"https://sphinx-gallery.github.io\">Gallery generated by Sphinx-Gallery</a></p>\n-</section>\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"04-low-memory-dropout.html\" class=\"btn btn-neutral float-left\" title=\"Low-Memory Dropout\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"06-fused-attention.html\" class=\"btn btn-neutral float-right\" title=\"Fused Attention\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"05-layer-norm.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/getting-started/tutorials/06-fused-attention.html", "status": "removed", "additions": 0, "deletions": 606, "changes": 606, "file_content_changes": "N/A"}, {"filename": "main/getting-started/tutorials/07-math-functions.html", "status": "removed", "additions": 0, "deletions": 246, "changes": 246, "file_content_changes": "@@ -1,246 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>Libdevice (tl.math) function &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"Block Pointer (Experimental)\" href=\"08-experimental-block-pointer.html\" />\n-    <link rel=\"prev\" title=\"Fused Attention\" href=\"06-fused-attention.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../installation.html\">Installation</a></li>\n-<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"index.html\">Tutorials</a><ul class=\"current\">\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"01-vector-add.html\">Vector Addition</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"02-fused-softmax.html\">Fused Softmax</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n-<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Libdevice (<cite>tl.math</cite>) function</a><ul>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#asin-kernel\">asin Kernel</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#using-the-default-libdevice-library-path\">Using the default libdevice library path</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#customize-the-libdevice-library-path\">Customize the libdevice library path</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"08-experimental-block-pointer.html\">Block Pointer (Experimental)</a></li>\n-</ul>\n-</li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-          <li class=\"breadcrumb-item\"><a href=\"index.html\">Tutorials</a></li>\n-      <li class=\"breadcrumb-item active\">Libdevice (<cite>tl.math</cite>) function</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/getting-started/tutorials/07-math-functions.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <div class=\"sphx-glr-download-link-note admonition note\">\n-<p class=\"admonition-title\">Note</p>\n-<p><a class=\"reference internal\" href=\"#sphx-glr-download-getting-started-tutorials-07-math-functions-py\"><span class=\"std std-ref\">Go to the end</span></a>\n-to download the full example code</p>\n-</div>\n-<section class=\"sphx-glr-example-title\" id=\"libdevice-tl-math-function\">\n-<span id=\"sphx-glr-getting-started-tutorials-07-math-functions-py\"></span><h1>Libdevice (<cite>tl.math</cite>) function<a class=\"headerlink\" href=\"#libdevice-tl-math-function\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<p>Triton can invoke a custom function from an external library.\n-In this example, we will use the <cite>libdevice</cite> library (a.k.a <cite>math</cite> in triton) to apply <cite>asin</cite> on a tensor.\n-Please refer to <a class=\"reference external\" href=\"https://docs.nvidia.com/cuda/libdevice-users-guide/index.html\">https://docs.nvidia.com/cuda/libdevice-users-guide/index.html</a> regarding the semantics of all available libdevice functions.\n-In <cite>triton/language/math.py</cite>, we try to aggregate functions with the same computation but different data types together.\n-For example, both <cite>__nv_asin</cite> and <cite>__nvasinf</cite> calculate the principal value of the arc sine of the input, but <cite>__nv_asin</cite> operates on <cite>double</cite> and <cite>__nv_asinf</cite> operates on <cite>float</cite>.\n-Using triton, you can simply call <cite>tl.math.asin</cite>.\n-Triton automatically selects the correct underlying device function to invoke based on input and output types.</p>\n-<section id=\"asin-kernel\">\n-<h2>asin Kernel<a class=\"headerlink\" href=\"#asin-kernel\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n-\n-<span class=\"kn\">import</span> <span class=\"nn\">triton</span>\n-<span class=\"kn\">import</span> <span class=\"nn\">triton.language</span> <span class=\"k\">as</span> <span class=\"nn\">tl</span>\n-\n-\n-<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n-<span class=\"k\">def</span> <span class=\"nf\">asin_kernel</span><span class=\"p\">(</span>\n-        <span class=\"n\">x_ptr</span><span class=\"p\">,</span>\n-        <span class=\"n\">y_ptr</span><span class=\"p\">,</span>\n-        <span class=\"n\">n_elements</span><span class=\"p\">,</span>\n-        <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n-<span class=\"p\">):</span>\n-    <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-    <span class=\"n\">block_start</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE</span>\n-    <span class=\"n\">offsets</span> <span class=\"o\">=</span> <span class=\"n\">block_start</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">)</span>\n-    <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">offsets</span> <span class=\"o\">&lt;</span> <span class=\"n\">n_elements</span>\n-    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">x_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n-    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">asin</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n-    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">y_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n-</pre></div>\n-</div>\n-</section>\n-<section id=\"using-the-default-libdevice-library-path\">\n-<h2>Using the default libdevice library path<a class=\"headerlink\" href=\"#using-the-default-libdevice-library-path\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>We can use the default libdevice library path encoded in <cite>triton/language/math.py</cite></p>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-<span class=\"n\">size</span> <span class=\"o\">=</span> <span class=\"mi\">98432</span>\n-<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n-<span class=\"n\">output_triton</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n-<span class=\"n\">output_torch</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">asin</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n-<span class=\"k\">assert</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">is_cuda</span> <span class=\"ow\">and</span> <span class=\"n\">output_triton</span><span class=\"o\">.</span><span class=\"n\">is_cuda</span>\n-<span class=\"n\">n_elements</span> <span class=\"o\">=</span> <span class=\"n\">output_torch</span><span class=\"o\">.</span><span class=\"n\">numel</span><span class=\"p\">()</span>\n-<span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">meta</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">meta</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">]),)</span>\n-<span class=\"n\">asin_kernel</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">output_triton</span><span class=\"p\">,</span> <span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"o\">=</span><span class=\"mi\">1024</span><span class=\"p\">)</span>\n-<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">output_torch</span><span class=\"p\">)</span>\n-<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">output_triton</span><span class=\"p\">)</span>\n-<span class=\"nb\">print</span><span class=\"p\">(</span>\n-    <span class=\"sa\">f</span><span class=\"s1\">&#39;The maximum difference between torch and triton is &#39;</span>\n-    <span class=\"sa\">f</span><span class=\"s1\">&#39;</span><span class=\"si\">{</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">abs</span><span class=\"p\">(</span><span class=\"n\">output_torch</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"w\"> </span><span class=\"n\">output_triton</span><span class=\"p\">))</span><span class=\"si\">}</span><span class=\"s1\">&#39;</span>\n-<span class=\"p\">)</span>\n-</pre></div>\n-</div>\n-<div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device=&#39;cuda:0&#39;)\n-tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device=&#39;cuda:0&#39;)\n-The maximum difference between torch and triton is 2.384185791015625e-07\n-</pre></div>\n-</div>\n-</section>\n-<section id=\"customize-the-libdevice-library-path\">\n-<h2>Customize the libdevice library path<a class=\"headerlink\" href=\"#customize-the-libdevice-library-path\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>We can also customize the libdevice library path by passing the path to the <cite>libdevice</cite> library to the <cite>asin</cite> kernel.</p>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">output_triton</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n-<span class=\"n\">asin_kernel</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">output_triton</span><span class=\"p\">,</span> <span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"o\">=</span><span class=\"mi\">1024</span><span class=\"p\">,</span>\n-                  <span class=\"n\">extern_libs</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;libdevice&#39;</span><span class=\"p\">:</span> <span class=\"s1\">&#39;/usr/local/cuda/nvvm/libdevice/libdevice.10.bc&#39;</span><span class=\"p\">})</span>\n-<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">output_torch</span><span class=\"p\">)</span>\n-<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">output_triton</span><span class=\"p\">)</span>\n-<span class=\"nb\">print</span><span class=\"p\">(</span>\n-    <span class=\"sa\">f</span><span class=\"s1\">&#39;The maximum difference between torch and triton is &#39;</span>\n-    <span class=\"sa\">f</span><span class=\"s1\">&#39;</span><span class=\"si\">{</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">abs</span><span class=\"p\">(</span><span class=\"n\">output_torch</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"w\"> </span><span class=\"n\">output_triton</span><span class=\"p\">))</span><span class=\"si\">}</span><span class=\"s1\">&#39;</span>\n-<span class=\"p\">)</span>\n-</pre></div>\n-</div>\n-<div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device=&#39;cuda:0&#39;)\n-tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device=&#39;cuda:0&#39;)\n-The maximum difference between torch and triton is 2.384185791015625e-07\n-</pre></div>\n-</div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.215 seconds)</p>\n-<div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-07-math-functions-py\">\n-<div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n-<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/77571465f7f4bd281d3a847dc2633146/07-math-functions.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">07-math-functions.py</span></code></a></p>\n-</div>\n-<div class=\"sphx-glr-download sphx-glr-download-jupyter docutils container\">\n-<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/302f1761fdc89516b532aad33b963ca0/07-math-functions.ipynb\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Jupyter</span> <span class=\"pre\">notebook:</span> <span class=\"pre\">07-math-functions.ipynb</span></code></a></p>\n-</div>\n-</div>\n-<p class=\"sphx-glr-signature\"><a class=\"reference external\" href=\"https://sphinx-gallery.github.io\">Gallery generated by Sphinx-Gallery</a></p>\n-</section>\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"06-fused-attention.html\" class=\"btn btn-neutral float-left\" title=\"Fused Attention\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"08-experimental-block-pointer.html\" class=\"btn btn-neutral float-right\" title=\"Block Pointer (Experimental)\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"07-math-functions.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/getting-started/tutorials/08-experimental-block-pointer.html", "status": "removed", "additions": 0, "deletions": 395, "changes": 395, "file_content_changes": "@@ -1,395 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>Block Pointer (Experimental) &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton\" href=\"../../python-api/triton.html\" />\n-    <link rel=\"prev\" title=\"Libdevice (tl.math) function\" href=\"07-math-functions.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../installation.html\">Installation</a></li>\n-<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"index.html\">Tutorials</a><ul class=\"current\">\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"01-vector-add.html\">Vector Addition</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"02-fused-softmax.html\">Fused Softmax</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n-<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Block Pointer (Experimental)</a><ul>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#motivations\">Motivations</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#make-a-block-pointer\">Make a Block Pointer</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#load-store-a-block-pointer\">Load/Store a Block Pointer</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#advance-a-block-pointer\">Advance a Block Pointer</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#final-result\">Final Result</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#unit-test\">Unit Test</a></li>\n-</ul>\n-</li>\n-</ul>\n-</li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-          <li class=\"breadcrumb-item\"><a href=\"index.html\">Tutorials</a></li>\n-      <li class=\"breadcrumb-item active\">Block Pointer (Experimental)</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/getting-started/tutorials/08-experimental-block-pointer.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <div class=\"sphx-glr-download-link-note admonition note\">\n-<p class=\"admonition-title\">Note</p>\n-<p><a class=\"reference internal\" href=\"#sphx-glr-download-getting-started-tutorials-08-experimental-block-pointer-py\"><span class=\"std std-ref\">Go to the end</span></a>\n-to download the full example code</p>\n-</div>\n-<section class=\"sphx-glr-example-title\" id=\"block-pointer-experimental\">\n-<span id=\"sphx-glr-getting-started-tutorials-08-experimental-block-pointer-py\"></span><h1>Block Pointer (Experimental)<a class=\"headerlink\" href=\"#block-pointer-experimental\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<p>This tutorial will guide you through writing a matrix multiplication algorithm that utilizes block pointer semantics.\n-These semantics are more friendly for Triton to optimize and can result in better performance on specific hardware.\n-Note that this feature is still experimental and may change in the future.</p>\n-<section id=\"motivations\">\n-<h2>Motivations<a class=\"headerlink\" href=\"#motivations\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>In the previous matrix multiplication tutorial, we constructed blocks of values by de-referencing blocks of pointers,\n-i.e., <code class=\"code docutils literal notranslate\"><span class=\"pre\">load(block&lt;pointer_type&lt;element_type&gt;&gt;)</span> <span class=\"pre\">-&gt;</span> <span class=\"pre\">block&lt;element_type&gt;</span></code>, which involved loading blocks of\n-elements from memory. This approach allowed for flexibility in using hardware-managed cache and implementing complex\n-data structures, such as tensors of trees or unstructured look-up tables.</p>\n-<p>However, the drawback of this approach is that it relies heavily on complex optimization passes by the compiler to\n-optimize memory access patterns. This can result in brittle code that may suffer from performance degradation when the\n-optimizer fails to perform adequately. Additionally, as memory controllers specialize to accommodate dense spatial\n-data structures commonly used in machine learning workloads, this problem is likely to worsen.</p>\n-<p>To address this issue, we will use block pointers <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer_type&lt;block&lt;element_type&gt;&gt;</span></code> and load them into\n-<code class=\"code docutils literal notranslate\"><span class=\"pre\">block&lt;element_type&gt;</span></code>, in which way gives better friendliness for the compiler to optimize memory access\n-patterns.</p>\n-<p>Let\u2019s start with the previous matrix multiplication example and demonstrate how to rewrite it to utilize block pointer\n-semantics.</p>\n-</section>\n-<section id=\"make-a-block-pointer\">\n-<h2>Make a Block Pointer<a class=\"headerlink\" href=\"#make-a-block-pointer\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>A block pointer pointers to a block in a parent tensor and is constructed by <code class=\"code docutils literal notranslate\"><span class=\"pre\">make_block_ptr</span></code> function,\n-which takes the following information as arguments:</p>\n-<ul class=\"simple\">\n-<li><p><code class=\"code docutils literal notranslate\"><span class=\"pre\">base</span></code>: the base pointer to the parent tensor;</p></li>\n-<li><p><code class=\"code docutils literal notranslate\"><span class=\"pre\">shape</span></code>: the shape of the parent tensor;</p></li>\n-<li><p><code class=\"code docutils literal notranslate\"><span class=\"pre\">strides</span></code>: the strides of the parent tensor, which means how much to increase the pointer by when moving by 1 element in a specific axis;</p></li>\n-<li><p><code class=\"code docutils literal notranslate\"><span class=\"pre\">offsets</span></code>: the offsets of the block;</p></li>\n-<li><p><code class=\"code docutils literal notranslate\"><span class=\"pre\">block_shape</span></code>: the shape of the block;</p></li>\n-<li><p><code class=\"code docutils literal notranslate\"><span class=\"pre\">order</span></code>: the order of the block, which means how the block is laid out in memory.</p></li>\n-</ul>\n-<p>For example, to a block pointer to a <code class=\"code docutils literal notranslate\"><span class=\"pre\">BLOCK_SIZE_M</span> <span class=\"pre\">*</span> <span class=\"pre\">BLOCK_SIZE_K</span></code> block in a row-major 2D matrix A by\n-offsets <code class=\"code docutils literal notranslate\"><span class=\"pre\">(pid_m</span> <span class=\"pre\">*</span> <span class=\"pre\">BLOCK_SIZE_M,</span> <span class=\"pre\">0)</span></code> and strides <code class=\"code docutils literal notranslate\"><span class=\"pre\">(stride_am,</span> <span class=\"pre\">stride_ak)</span></code>, we can use the following code\n-(exactly the same as the previous matrix multiplication tutorial):</p>\n-<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">a_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span><span class=\"n\">base</span><span class=\"o\">=</span><span class=\"n\">a_ptr</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">),</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_am</span><span class=\"p\">,</span> <span class=\"n\">stride_ak</span><span class=\"p\">),</span>\n-                                <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">pid_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">),</span>\n-                                <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n-</pre></div>\n-</div>\n-<p>Note that the <code class=\"code docutils literal notranslate\"><span class=\"pre\">order</span></code> argument is set to <code class=\"code docutils literal notranslate\"><span class=\"pre\">(1,</span> <span class=\"pre\">0)</span></code>, which means the second axis is the inner dimension in\n-terms of storage, and the first axis is the outer dimension. This information may sound redundant, but it is necessary\n-for some hardware backends to optimize for better performance.</p>\n-</section>\n-<section id=\"load-store-a-block-pointer\">\n-<h2>Load/Store a Block Pointer<a class=\"headerlink\" href=\"#load-store-a-block-pointer\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>To load/store a block pointer, we can use <code class=\"code docutils literal notranslate\"><span class=\"pre\">load/store</span></code> function, which takes a block pointer as an argument,\n-de-references it, and loads/stores a block. You may mask some values in the block, here we have an extra argument\n-<code class=\"code docutils literal notranslate\"><span class=\"pre\">boundary_check</span></code> to specify whether to check the boundary of each axis for the block pointer. With check on,\n-out-of-bound values will be masked according to the <code class=\"code docutils literal notranslate\"><span class=\"pre\">padding_option</span></code> argument (load only), which can be\n-<code class=\"code docutils literal notranslate\"><span class=\"pre\">zero</span></code> or <code class=\"code docutils literal notranslate\"><span class=\"pre\">nan</span></code>. Temporarily, we do not support other values due to some hardware limitations. In this\n-mode of block pointer load/store does not support <code class=\"code docutils literal notranslate\"><span class=\"pre\">mask</span></code> or <code class=\"code docutils literal notranslate\"><span class=\"pre\">other</span></code> arguments in the legacy mode.</p>\n-<p>So to load the block pointer of A in the previous section, we can simply write\n-<code class=\"code docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">=</span> <span class=\"pre\">tl.load(a_block_ptr,</span> <span class=\"pre\">boundary_check=(0,</span> <span class=\"pre\">1))</span></code>. Boundary check may cost extra performance, so if you can\n-guarantee that the block pointer is always in-bound in some axis, you can turn off the check by not passing the index\n-into the <code class=\"code docutils literal notranslate\"><span class=\"pre\">boundary_check</span></code> argument. For example, if we know that <code class=\"code docutils literal notranslate\"><span class=\"pre\">M</span></code> is a multiple of\n-<code class=\"code docutils literal notranslate\"><span class=\"pre\">BLOCK_SIZE_M</span></code>, we can replace with <code class=\"code docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">=</span> <span class=\"pre\">tl.load(a_block_ptr,</span> <span class=\"pre\">boundary_check=(1,</span> <span class=\"pre\">))</span></code>, since axis 0 is\n-always in bound.</p>\n-</section>\n-<section id=\"advance-a-block-pointer\">\n-<h2>Advance a Block Pointer<a class=\"headerlink\" href=\"#advance-a-block-pointer\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>To advance a block pointer, we can use <code class=\"code docutils literal notranslate\"><span class=\"pre\">advance</span></code> function, which takes a block pointer and the increment for\n-each axis as arguments and returns a new block pointer with the same shape and strides as the original one,\n-but with the offsets advanced by the specified amount.</p>\n-<p>For example, to advance the block pointer by <code class=\"code docutils literal notranslate\"><span class=\"pre\">BLOCK_SIZE_K</span></code> in the second axis\n-(no need to multiply with strides), we can write <code class=\"code docutils literal notranslate\"><span class=\"pre\">a_block_ptr</span> <span class=\"pre\">=</span> <span class=\"pre\">tl.advance(a_block_ptr,</span> <span class=\"pre\">(0,</span> <span class=\"pre\">BLOCK_SIZE_K))</span></code>.</p>\n-</section>\n-<section id=\"final-result\">\n-<h2>Final Result<a class=\"headerlink\" href=\"#final-result\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n-\n-<span class=\"kn\">import</span> <span class=\"nn\">triton</span>\n-<span class=\"kn\">import</span> <span class=\"nn\">triton.language</span> <span class=\"k\">as</span> <span class=\"nn\">tl</span>\n-\n-\n-<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">autotune</span><span class=\"p\">(</span>\n-    <span class=\"n\">configs</span><span class=\"o\">=</span><span class=\"p\">[</span>\n-        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n-        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n-        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n-        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n-        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n-        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n-        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n-        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n-    <span class=\"p\">],</span>\n-    <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;M&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;K&#39;</span><span class=\"p\">],</span>\n-<span class=\"p\">)</span>\n-<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n-<span class=\"k\">def</span> <span class=\"nf\">matmul_kernel_with_block_pointers</span><span class=\"p\">(</span>\n-        <span class=\"c1\"># Pointers to matrices</span>\n-        <span class=\"n\">a_ptr</span><span class=\"p\">,</span> <span class=\"n\">b_ptr</span><span class=\"p\">,</span> <span class=\"n\">c_ptr</span><span class=\"p\">,</span>\n-        <span class=\"c1\"># Matrix dimensions</span>\n-        <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span>\n-        <span class=\"c1\"># The stride variables represent how much to increase the ptr by when moving by 1</span>\n-        <span class=\"c1\"># element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`</span>\n-        <span class=\"c1\"># by to get the element one row down (A has M rows).</span>\n-        <span class=\"n\">stride_am</span><span class=\"p\">,</span> <span class=\"n\">stride_ak</span><span class=\"p\">,</span>\n-        <span class=\"n\">stride_bk</span><span class=\"p\">,</span> <span class=\"n\">stride_bn</span><span class=\"p\">,</span>\n-        <span class=\"n\">stride_cm</span><span class=\"p\">,</span> <span class=\"n\">stride_cn</span><span class=\"p\">,</span>\n-        <span class=\"c1\"># Meta-parameters</span>\n-        <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n-        <span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span>\n-<span class=\"p\">):</span>\n-<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Kernel for computing the matmul C = A x B.</span>\n-<span class=\"sd\">    A has shape (M, K), B has shape (K, N) and C has shape (M, N)</span>\n-<span class=\"sd\">    &quot;&quot;&quot;</span>\n-    <span class=\"c1\"># -----------------------------------------------------------</span>\n-    <span class=\"c1\"># Map program ids `pid` to the block of C it should compute.</span>\n-    <span class=\"c1\"># This is done in a grouped ordering to promote L2 data reuse.</span>\n-    <span class=\"c1\"># See the matrix multiplication tutorial for details.</span>\n-    <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-    <span class=\"n\">num_pid_m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">)</span>\n-    <span class=\"n\">num_pid_n</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)</span>\n-    <span class=\"n\">num_pid_in_group</span> <span class=\"o\">=</span> <span class=\"n\">GROUP_SIZE_M</span> <span class=\"o\">*</span> <span class=\"n\">num_pid_n</span>\n-    <span class=\"n\">group_id</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">//</span> <span class=\"n\">num_pid_in_group</span>\n-    <span class=\"n\">first_pid_m</span> <span class=\"o\">=</span> <span class=\"n\">group_id</span> <span class=\"o\">*</span> <span class=\"n\">GROUP_SIZE_M</span>\n-    <span class=\"n\">group_size_m</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">num_pid_m</span> <span class=\"o\">-</span> <span class=\"n\">first_pid_m</span><span class=\"p\">,</span> <span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">)</span>\n-    <span class=\"n\">pid_m</span> <span class=\"o\">=</span> <span class=\"n\">first_pid_m</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">pid</span> <span class=\"o\">%</span> <span class=\"n\">group_size_m</span><span class=\"p\">)</span>\n-    <span class=\"n\">pid_n</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">pid</span> <span class=\"o\">%</span> <span class=\"n\">num_pid_in_group</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"n\">group_size_m</span>\n-\n-    <span class=\"c1\"># ----------------------------------------------------------</span>\n-    <span class=\"c1\"># Create block pointers for the first blocks of A and B.</span>\n-    <span class=\"c1\"># We will advance this pointer as we move in the K direction and accumulate.</span>\n-    <span class=\"c1\"># See above `Make a Block Pointer` section for details.</span>\n-    <span class=\"n\">a_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span><span class=\"n\">base</span><span class=\"o\">=</span><span class=\"n\">a_ptr</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">),</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_am</span><span class=\"p\">,</span> <span class=\"n\">stride_ak</span><span class=\"p\">),</span>\n-                                    <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">pid_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">),</span>\n-                                    <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n-    <span class=\"n\">b_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span><span class=\"n\">base</span><span class=\"o\">=</span><span class=\"n\">b_ptr</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">),</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_bk</span><span class=\"p\">,</span> <span class=\"n\">stride_bn</span><span class=\"p\">),</span>\n-                                    <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">pid_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span> <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span>\n-                                    <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n-\n-    <span class=\"c1\"># -----------------------------------------------------------</span>\n-    <span class=\"c1\"># Iterate to compute a block of the C matrix.</span>\n-    <span class=\"c1\"># We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block.</span>\n-    <span class=\"c1\"># of fp32 values for higher accuracy.</span>\n-    <span class=\"c1\"># `accumulator` will be converted back to fp16 after the loop.</span>\n-    <span class=\"n\">accumulator</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n-    <span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">):</span>\n-        <span class=\"c1\"># Load with boundary checks, no need to calculate the mask manually.</span>\n-        <span class=\"c1\"># For better performance, you may remove some axis from the boundary</span>\n-        <span class=\"c1\"># check, if you can guarantee that the access is always in-bound in</span>\n-        <span class=\"c1\"># that axis.</span>\n-        <span class=\"c1\"># See above `Load/Store a Block Pointer` section for details.</span>\n-        <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">a_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n-        <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">b_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n-        <span class=\"c1\"># We accumulate along the K dimension.</span>\n-        <span class=\"n\">accumulator</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n-        <span class=\"c1\"># Advance the block pointer to the next K block.</span>\n-        <span class=\"c1\"># See above `Advance a Block Pointer` section for details.</span>\n-        <span class=\"n\">a_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">advance</span><span class=\"p\">(</span><span class=\"n\">a_block_ptr</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">))</span>\n-        <span class=\"n\">b_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">advance</span><span class=\"p\">(</span><span class=\"n\">b_block_ptr</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n-    <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">accumulator</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n-\n-    <span class=\"c1\"># ----------------------------------------------------------------</span>\n-    <span class=\"c1\"># Write back the block of the output matrix C with boundary checks.</span>\n-    <span class=\"c1\"># See above `Load/Store a Block Pointer` section for details.</span>\n-    <span class=\"n\">c_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span><span class=\"n\">base</span><span class=\"o\">=</span><span class=\"n\">c_ptr</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">),</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_cm</span><span class=\"p\">,</span> <span class=\"n\">stride_cn</span><span class=\"p\">),</span>\n-                                    <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">pid_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">pid_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span>\n-                                    <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span> <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n-    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">c_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n-\n-\n-<span class=\"c1\"># We can now create a convenience wrapper function that only takes two input tensors,</span>\n-<span class=\"c1\"># and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.</span>\n-<span class=\"k\">def</span> <span class=\"nf\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">):</span>\n-    <span class=\"c1\"># Check constraints.</span>\n-    <span class=\"k\">assert</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"s2\">&quot;Incompatible dimensions&quot;</span>\n-    <span class=\"k\">assert</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">is_contiguous</span><span class=\"p\">(),</span> <span class=\"s2\">&quot;Matrix A must be contiguous&quot;</span>\n-    <span class=\"k\">assert</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">is_contiguous</span><span class=\"p\">(),</span> <span class=\"s2\">&quot;Matrix B must be contiguous&quot;</span>\n-    <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">K</span> <span class=\"o\">=</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n-    <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n-    <span class=\"c1\"># Allocates output.</span>\n-    <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># 1D launch kernel where each block gets its own program.</span>\n-    <span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">META</span><span class=\"p\">:</span> <span class=\"p\">(</span>\n-        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">META</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">META</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">]),</span>\n-    <span class=\"p\">)</span>\n-    <span class=\"n\">matmul_kernel_with_block_pointers</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span>\n-        <span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">,</span>\n-        <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span>\n-        <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n-        <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n-        <span class=\"n\">c</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">c</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n-    <span class=\"p\">)</span>\n-    <span class=\"k\">return</span> <span class=\"n\">c</span>\n-</pre></div>\n-</div>\n-</section>\n-<section id=\"unit-test\">\n-<h2>Unit Test<a class=\"headerlink\" href=\"#unit-test\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>Still we can test our matrix multiplication with block pointers against a native torch implementation (i.e., cuBLAS).</p>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-<span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n-<span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n-<span class=\"n\">triton_output</span> <span class=\"o\">=</span> <span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n-<span class=\"n\">torch_output</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n-<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;triton_output=</span><span class=\"si\">{</span><span class=\"n\">triton_output</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n-<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;torch_output=</span><span class=\"si\">{</span><span class=\"n\">torch_output</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n-<span class=\"k\">if</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">triton_output</span><span class=\"p\">,</span> <span class=\"n\">torch_output</span><span class=\"p\">,</span> <span class=\"n\">atol</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">,</span> <span class=\"n\">rtol</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">):</span>\n-    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;\u2705 Triton and Torch match&quot;</span><span class=\"p\">)</span>\n-<span class=\"k\">else</span><span class=\"p\">:</span>\n-    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;\u274c Triton and Torch differ&quot;</span><span class=\"p\">)</span>\n-</pre></div>\n-</div>\n-<div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>triton_output=tensor([[-10.9531,  -4.7109,  15.6953,  ..., -28.4062,   4.3320, -26.4219],\n-        [ 26.8438,  10.0469,  -5.4297,  ..., -11.2969,  -8.5312,  30.7500],\n-        [-13.2578,  15.8516,  18.0781,  ..., -21.7656,  -8.6406,  10.2031],\n-        ...,\n-        [ 40.2812,  18.6094, -25.6094,  ...,  -2.7598,  -3.2441,  41.0000],\n-        [ -6.1211, -16.8281,   4.4844,  ..., -21.0312,  24.7031,  15.0234],\n-        [-17.0938, -19.0000,  -0.3831,  ...,  21.5469, -30.2344, -13.2188]],\n-       device=&#39;cuda:0&#39;, dtype=torch.float16)\n-torch_output=tensor([[-10.9531,  -4.7109,  15.6953,  ..., -28.4062,   4.3320, -26.4219],\n-        [ 26.8438,  10.0469,  -5.4297,  ..., -11.2969,  -8.5312,  30.7500],\n-        [-13.2578,  15.8516,  18.0781,  ..., -21.7656,  -8.6406,  10.2031],\n-        ...,\n-        [ 40.2812,  18.6094, -25.6094,  ...,  -2.7598,  -3.2441,  41.0000],\n-        [ -6.1211, -16.8281,   4.4844,  ..., -21.0312,  24.7031,  15.0234],\n-        [-17.0938, -19.0000,  -0.3831,  ...,  21.5469, -30.2344, -13.2188]],\n-       device=&#39;cuda:0&#39;, dtype=torch.float16)\n-\u2705 Triton and Torch match\n-</pre></div>\n-</div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  6.345 seconds)</p>\n-<div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-08-experimental-block-pointer-py\">\n-<div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n-<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/4d6052117d61c2ca779cd4b75567fee5/08-experimental-block-pointer.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">08-experimental-block-pointer.py</span></code></a></p>\n-</div>\n-<div class=\"sphx-glr-download sphx-glr-download-jupyter docutils container\">\n-<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/9705af6f33c6e66c6fa78f2394331536/08-experimental-block-pointer.ipynb\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Jupyter</span> <span class=\"pre\">notebook:</span> <span class=\"pre\">08-experimental-block-pointer.ipynb</span></code></a></p>\n-</div>\n-</div>\n-<p class=\"sphx-glr-signature\"><a class=\"reference external\" href=\"https://sphinx-gallery.github.io\">Gallery generated by Sphinx-Gallery</a></p>\n-</section>\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"07-math-functions.html\" class=\"btn btn-neutral float-left\" title=\"Libdevice (tl.math) function\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"../../python-api/triton.html\" class=\"btn btn-neutral float-right\" title=\"triton\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"08-experimental-block-pointer.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/getting-started/tutorials/index.html", "status": "removed", "additions": 0, "deletions": 188, "changes": 188, "file_content_changes": "@@ -1,188 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>Tutorials &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"Vector Addition\" href=\"01-vector-add.html\" />\n-    <link rel=\"prev\" title=\"Installation\" href=\"../installation.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../installation.html\">Installation</a></li>\n-<li class=\"toctree-l1 current\"><a class=\"current reference internal\" href=\"#\">Tutorials</a><ul>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"01-vector-add.html\">Vector Addition</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"02-fused-softmax.html\">Fused Softmax</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"08-experimental-block-pointer.html\">Block Pointer (Experimental)</a></li>\n-</ul>\n-</li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-      <li class=\"breadcrumb-item active\">Tutorials</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/getting-started/tutorials/index.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <section id=\"tutorials\">\n-<h1>Tutorials<a class=\"headerlink\" href=\"#tutorials\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<p>Below is a gallery of tutorials for writing various basic operations with Triton. It is recommended that you read through the tutorials in order, starting with the simplest one.</p>\n-<p>To install the dependencies for the tutorials:</p>\n-<div class=\"highlight-bash notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nb\">cd</span><span class=\"w\"> </span>triton\n-pip<span class=\"w\"> </span>install<span class=\"w\"> </span>-e<span class=\"w\"> </span><span class=\"s1\">&#39;./python[tutorials]&#39;</span>\n-</pre></div>\n-</div>\n-<div class=\"sphx-glr-thumbnails\"><div class=\"sphx-glr-thumbcontainer\" tooltip=\"In this tutorial, you will write a simple vector addition using Triton.\"><img alt=\"\" src=\"../../_images/sphx_glr_01-vector-add_thumb.png\" />\n-<p><a class=\"reference internal\" href=\"01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py\"><span class=\"std std-ref\">Vector Addition</span></a></p>\n-  <div class=\"sphx-glr-thumbnail-title\">Vector Addition</div>\n-</div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"In this tutorial, you will write a fused softmax operation that is significantly faster than Py...\"><img alt=\"\" src=\"../../_images/sphx_glr_02-fused-softmax_thumb.png\" />\n-<p><a class=\"reference internal\" href=\"02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py\"><span class=\"std std-ref\">Fused Softmax</span></a></p>\n-  <div class=\"sphx-glr-thumbnail-title\">Fused Softmax</div>\n-</div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"You will specifically learn about:\"><img alt=\"\" src=\"../../_images/sphx_glr_03-matrix-multiplication_thumb.png\" />\n-<p><a class=\"reference internal\" href=\"03-matrix-multiplication.html#sphx-glr-getting-started-tutorials-03-matrix-multiplication-py\"><span class=\"std std-ref\">Matrix Multiplication</span></a></p>\n-  <div class=\"sphx-glr-thumbnail-title\">Matrix Multiplication</div>\n-</div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"In this tutorial, you will write a memory-efficient implementation of dropout whose state will ...\"><img alt=\"\" src=\"../../_images/sphx_glr_04-low-memory-dropout_thumb.png\" />\n-<p><a class=\"reference internal\" href=\"04-low-memory-dropout.html#sphx-glr-getting-started-tutorials-04-low-memory-dropout-py\"><span class=\"std std-ref\">Low-Memory Dropout</span></a></p>\n-  <div class=\"sphx-glr-thumbnail-title\">Low-Memory Dropout</div>\n-</div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"In doing so, you will learn about:\"><img alt=\"\" src=\"../../_images/sphx_glr_05-layer-norm_thumb.png\" />\n-<p><a class=\"reference internal\" href=\"05-layer-norm.html#sphx-glr-getting-started-tutorials-05-layer-norm-py\"><span class=\"std std-ref\">Layer Normalization</span></a></p>\n-  <div class=\"sphx-glr-thumbnail-title\">Layer Normalization</div>\n-</div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"This is a Triton implementation of the Flash Attention v2 algorithm from Tri Dao (https://trida...\"><img alt=\"\" src=\"../../_images/sphx_glr_06-fused-attention_thumb.png\" />\n-<p><a class=\"reference internal\" href=\"06-fused-attention.html#sphx-glr-getting-started-tutorials-06-fused-attention-py\"><span class=\"std std-ref\">Fused Attention</span></a></p>\n-  <div class=\"sphx-glr-thumbnail-title\">Fused Attention</div>\n-</div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"Libdevice (`tl.math`) function\"><img alt=\"\" src=\"../../_images/sphx_glr_07-math-functions_thumb.png\" />\n-<p><a class=\"reference internal\" href=\"07-math-functions.html#sphx-glr-getting-started-tutorials-07-math-functions-py\"><span class=\"std std-ref\">Libdevice (tl.math) function</span></a></p>\n-  <div class=\"sphx-glr-thumbnail-title\">Libdevice (`tl.math`) function</div>\n-</div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"Block Pointer (Experimental)\"><img alt=\"\" src=\"../../_images/sphx_glr_08-experimental-block-pointer_thumb.png\" />\n-<p><a class=\"reference internal\" href=\"08-experimental-block-pointer.html#sphx-glr-getting-started-tutorials-08-experimental-block-pointer-py\"><span class=\"std std-ref\">Block Pointer (Experimental)</span></a></p>\n-  <div class=\"sphx-glr-thumbnail-title\">Block Pointer (Experimental)</div>\n-</div></div><div class=\"toctree-wrapper compound\">\n-</div>\n-<div class=\"sphx-glr-footer sphx-glr-footer-gallery docutils container\">\n-<div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n-<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/763344228ae6bc253ed1a6cf586aa30d/tutorials_python.zip\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">all</span> <span class=\"pre\">examples</span> <span class=\"pre\">in</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">tutorials_python.zip</span></code></a></p>\n-</div>\n-<div class=\"sphx-glr-download sphx-glr-download-jupyter docutils container\">\n-<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/662999063954282841dc90b8945f85ce/tutorials_jupyter.zip\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">all</span> <span class=\"pre\">examples</span> <span class=\"pre\">in</span> <span class=\"pre\">Jupyter</span> <span class=\"pre\">notebooks:</span> <span class=\"pre\">tutorials_jupyter.zip</span></code></a></p>\n-</div>\n-</div>\n-<p class=\"sphx-glr-signature\"><a class=\"reference external\" href=\"https://sphinx-gallery.github.io\">Gallery generated by Sphinx-Gallery</a></p>\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"../installation.html\" class=\"btn btn-neutral float-left\" title=\"Installation\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"01-vector-add.html\" class=\"btn btn-neutral float-right\" title=\"Vector Addition\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"index.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/getting-started/tutorials/sg_execution_times.html", "status": "removed", "additions": 0, "deletions": 169, "changes": 169, "file_content_changes": "@@ -1,169 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>Computation times &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../installation.html\">Installation</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"index.html\">Tutorials</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-      <li class=\"breadcrumb-item active\">Computation times</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/getting-started/tutorials/sg_execution_times.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <section id=\"computation-times\">\n-<span id=\"sphx-glr-getting-started-tutorials-sg-execution-times\"></span><h1>Computation times<a class=\"headerlink\" href=\"#computation-times\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<p><strong>02:13.436</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n-<table class=\"docutils align-default\">\n-<tbody>\n-<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"03-matrix-multiplication.html#sphx-glr-getting-started-tutorials-03-matrix-multiplication-py\"><span class=\"std std-ref\">Matrix Multiplication</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">03-matrix-multiplication.py</span></code>)</p></td>\n-<td><p>00:40.533</p></td>\n-<td><p>0.0 MB</p></td>\n-</tr>\n-<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py\"><span class=\"std std-ref\">Fused Softmax</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">02-fused-softmax.py</span></code>)</p></td>\n-<td><p>00:37.028</p></td>\n-<td><p>0.0 MB</p></td>\n-</tr>\n-<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"05-layer-norm.html#sphx-glr-getting-started-tutorials-05-layer-norm-py\"><span class=\"std std-ref\">Layer Normalization</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">05-layer-norm.py</span></code>)</p></td>\n-<td><p>00:28.955</p></td>\n-<td><p>0.0 MB</p></td>\n-</tr>\n-<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"06-fused-attention.html#sphx-glr-getting-started-tutorials-06-fused-attention-py\"><span class=\"std std-ref\">Fused Attention</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">06-fused-attention.py</span></code>)</p></td>\n-<td><p>00:14.145</p></td>\n-<td><p>0.0 MB</p></td>\n-</tr>\n-<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"08-experimental-block-pointer.html#sphx-glr-getting-started-tutorials-08-experimental-block-pointer-py\"><span class=\"std std-ref\">Block Pointer (Experimental)</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">08-experimental-block-pointer.py</span></code>)</p></td>\n-<td><p>00:06.345</p></td>\n-<td><p>0.0 MB</p></td>\n-</tr>\n-<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py\"><span class=\"std std-ref\">Vector Addition</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">01-vector-add.py</span></code>)</p></td>\n-<td><p>00:05.608</p></td>\n-<td><p>0.0 MB</p></td>\n-</tr>\n-<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"04-low-memory-dropout.html#sphx-glr-getting-started-tutorials-04-low-memory-dropout-py\"><span class=\"std std-ref\">Low-Memory Dropout</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">04-low-memory-dropout.py</span></code>)</p></td>\n-<td><p>00:00.607</p></td>\n-<td><p>0.0 MB</p></td>\n-</tr>\n-<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"07-math-functions.html#sphx-glr-getting-started-tutorials-07-math-functions-py\"><span class=\"std std-ref\">Libdevice (tl.math) function</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">07-math-functions.py</span></code>)</p></td>\n-<td><p>00:00.215</p></td>\n-<td><p>0.0 MB</p></td>\n-</tr>\n-</tbody>\n-</table>\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"sg_execution_times.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/index.html", "status": "removed", "additions": 0, "deletions": 165, "changes": 165, "file_content_changes": "@@ -1,165 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>Welcome to Triton\u2019s documentation! &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"./\" id=\"documentation_options\" src=\"_static/documentation_options.js\"></script>\n-        <script src=\"_static/doctools.js\"></script>\n-        <script src=\"_static/sphinx_highlight.js\"></script>\n-    <script src=\"_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"search.html\" />\n-    <link rel=\"next\" title=\"Installation\" href=\"getting-started/installation.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"#\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"getting-started/installation.html\">Installation</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"getting-started/tutorials/index.html\">Tutorials</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"python-api/triton.html\">triton</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"python-api/triton.language.html\">triton.language</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"python-api/triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"#\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"#\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-      <li class=\"breadcrumb-item active\">Welcome to Triton\u2019s documentation!</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"_sources/index.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <section id=\"welcome-to-triton-s-documentation\">\n-<h1>Welcome to Triton\u2019s documentation!<a class=\"headerlink\" href=\"#welcome-to-triton-s-documentation\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<p><a class=\"reference external\" href=\"https://github.com/openai/triton\">Triton</a> is a language and compiler for parallel programming. It aims to provide a Python-based programming environment for productively writing custom DNN compute kernels capable of running at maximal throughput on modern GPU hardware.</p>\n-<section id=\"getting-started\">\n-<h2>Getting Started<a class=\"headerlink\" href=\"#getting-started\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<ul class=\"simple\">\n-<li><p>Follow the <a class=\"reference internal\" href=\"getting-started/installation.html\"><span class=\"doc\">installation instructions</span></a> for your platform of choice.</p></li>\n-<li><p>Take a look at the <a class=\"reference internal\" href=\"getting-started/tutorials/index.html\"><span class=\"doc\">tutorials</span></a> to learn how to write your first Triton program.</p></li>\n-</ul>\n-<div class=\"toctree-wrapper compound\">\n-</div>\n-</section>\n-<section id=\"python-api\">\n-<h2>Python API<a class=\"headerlink\" href=\"#python-api\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<ul class=\"simple\">\n-<li><p><a class=\"reference internal\" href=\"python-api/triton.html\"><span class=\"doc\">triton</span></a></p></li>\n-<li><p><a class=\"reference internal\" href=\"python-api/triton.language.html\"><span class=\"doc\">triton.language</span></a></p></li>\n-<li><p><a class=\"reference internal\" href=\"python-api/triton.testing.html\"><span class=\"doc\">triton.testing</span></a></p></li>\n-</ul>\n-<div class=\"toctree-wrapper compound\">\n-</div>\n-</section>\n-<section id=\"going-further\">\n-<h2>Going Further<a class=\"headerlink\" href=\"#going-further\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>Check out the following documents to learn more about Triton and how it compares against other DSLs for DNNs:</p>\n-<ul class=\"simple\">\n-<li><p>Chapter 1: <a class=\"reference internal\" href=\"programming-guide/chapter-1/introduction.html\"><span class=\"doc\">Introduction</span></a></p></li>\n-<li><p>Chapter 2: <a class=\"reference internal\" href=\"programming-guide/chapter-2/related-work.html\"><span class=\"doc\">Related Work</span></a></p></li>\n-</ul>\n-<div class=\"toctree-wrapper compound\">\n-</div>\n-</section>\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"getting-started/installation.html\" class=\"btn btn-neutral float-right\" title=\"Installation\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"index.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/objects.inv", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/programming-guide/chapter-1/introduction.html", "status": "removed", "additions": 0, "deletions": 260, "changes": 260, "file_content_changes": "@@ -1,260 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>Introduction &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"Related Work\" href=\"../chapter-2/related-work.html\" />\n-    <link rel=\"prev\" title=\"triton.testing.perf_report\" href=\"../../python-api/generated/triton.testing.perf_report.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/installation.html\">Installation</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/tutorials/index.html\">Tutorials</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1 current\"><a class=\"current reference internal\" href=\"#\">Introduction</a><ul>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"#motivations\">Motivations</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"#challenges\">Challenges</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"#references\">References</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-      <li class=\"breadcrumb-item active\">Introduction</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/programming-guide/chapter-1/introduction.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <section id=\"introduction\">\n-<h1>Introduction<a class=\"headerlink\" href=\"#introduction\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<section id=\"motivations\">\n-<h2>Motivations<a class=\"headerlink\" href=\"#motivations\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>Over the past decade, Deep Neural Networks (DNNs) have emerged as an important class of Machine Learning (ML) models, capable of achieving state-of-the-art performance across many domains ranging from natural language processing <a class=\"reference internal\" href=\"#sutskever2014\" id=\"id1\"><span>[SUTSKEVER2014]</span></a> to computer vision <a class=\"reference internal\" href=\"#redmon2016\" id=\"id2\"><span>[REDMON2016]</span></a> to computational neuroscience <a class=\"reference internal\" href=\"#lee2017\" id=\"id3\"><span>[LEE2017]</span></a>. The strength of these models lies in their hierarchical structure, composed of a sequence of parametric (e.g., convolutional) and non-parametric (e.g., rectified linearity) <em>layers</em>. This pattern, though notoriously computationally expensive, also generates a large amount of highly parallelizable work particularly well suited for multi- and many- core processors.</p>\n-<p>As a consequence, Graphics Processing Units (GPUs) have become a cheap and accessible resource for exploring and/or deploying novel research ideas in the field. This trend has been accelerated by the release of several frameworks for General-Purpose GPU (GPGPU) computing, such as CUDA and OpenCL, which have made the development of high-performance programs easier. Yet, GPUs remain incredibly challenging to optimize for locality and parallelism, especially for computations that cannot be efficiently implemented using a combination of pre-existing optimized primitives. To make matters worse, GPU architectures are also rapidly evolving and specializing, as evidenced by the addition of tensor cores to NVIDIA (and more recently AMD) micro-architectures.</p>\n-<p>This tension between the computational opportunities offered by DNNs and the practical difficulty of GPU programming has created substantial academic and industrial interest for Domain-Specific Languages (DSLs) and compilers. Regrettably, these systems \u2013 whether they be based on polyhedral machinery (e.g., Tiramisu <a class=\"reference internal\" href=\"../chapter-2/related-work.html#baghdadi2021\" id=\"id4\"><span>[BAGHDADI2021]</span></a>, Tensor Comprehensions <a class=\"reference internal\" href=\"../chapter-2/related-work.html#vasilache2018\" id=\"id5\"><span>[VASILACHE2018]</span></a>) or scheduling languages (e.g., Halide <a class=\"reference internal\" href=\"#jrk2013\" id=\"id6\"><span>[JRK2013]</span></a>, TVM <a class=\"reference internal\" href=\"#chen2018\" id=\"id7\"><span>[CHEN2018]</span></a>) \u2013 remain less flexible and (for the same algorithm) markedly slower than the best handwritten compute kernels available in libraries like <a class=\"reference external\" href=\"https://docs.nvidia.com/cuda/cublas/index.html\">cuBLAS</a>, <a class=\"reference external\" href=\"https://docs.nvidia.com/deeplearning/cudnn/api/index.html\">cuDNN</a> or <a class=\"reference external\" href=\"https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html\">TensorRT</a>.</p>\n-<p>The main premise of this project is the following: programming paradigms based on blocked algorithms <a class=\"reference internal\" href=\"#lam1991\" id=\"id8\"><span>[LAM1991]</span></a> can facilitate the construction of high-performance compute kernels for neural networks. We specifically revisit traditional \u201cSingle Program, Multiple Data\u201d (SPMD <a class=\"reference internal\" href=\"#auguin1983\" id=\"id9\"><span>[AUGUIN1983]</span></a>) execution models for GPUs, and propose a variant in which programs \u2013 rather than threads \u2013 are blocked. For example, in the case of matrix multiplication, CUDA and Triton differ as follows:</p>\n-<table class=\"docutils align-default\">\n-<colgroup>\n-<col style=\"width: 50%\" />\n-<col style=\"width: 50%\" />\n-</colgroup>\n-<thead>\n-<tr class=\"row-odd\"><th class=\"head\"><p>CUDA Programming Model</p>\n-<p>(Scalar Program, Blocked Threads)</p>\n-</th>\n-<th class=\"head\"><p>Triton Programming Model</p>\n-<p>(Blocked Program, Scalar Threads)</p>\n-</th>\n-</tr>\n-</thead>\n-<tbody>\n-<tr class=\"row-even\"><td><div class=\"highlight-C notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"cp\">#pragma parallel</span>\n-<span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kt\">int</span><span class=\"w\"> </span><span class=\"n\">m</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">m</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"w\"> </span><span class=\"n\">M</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">m</span><span class=\"o\">++</span><span class=\"p\">)</span>\n-<span class=\"cp\">#pragma parallel</span>\n-<span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kt\">int</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"w\"> </span><span class=\"n\">N</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"o\">++</span><span class=\"p\">){</span>\n-<span class=\"w\">  </span><span class=\"kt\">float</span><span class=\"w\"> </span><span class=\"n\">acc</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">;</span>\n-<span class=\"w\">  </span><span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kt\">int</span><span class=\"w\"> </span><span class=\"n\">k</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">k</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"w\"> </span><span class=\"n\">K</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">k</span><span class=\"o\">++</span><span class=\"p\">)</span>\n-<span class=\"w\">    </span><span class=\"n\">acc</span><span class=\"w\"> </span><span class=\"o\">+=</span><span class=\"w\"> </span><span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">m</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">k</span><span class=\"p\">]</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"n\">B</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"p\">];</span>\n-\n-<span class=\"w\">  </span><span class=\"n\">C</span><span class=\"p\">[</span><span class=\"n\">m</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"p\">]</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">acc</span><span class=\"p\">;</span>\n-<span class=\"p\">}</span>\n-</pre></div>\n-</div>\n-</td>\n-<td><div class=\"highlight-C notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"cp\">#pragma parallel</span>\n-<span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kt\">int</span><span class=\"w\"> </span><span class=\"n\">m</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">m</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"w\"> </span><span class=\"n\">M</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">m</span><span class=\"w\"> </span><span class=\"o\">+=</span><span class=\"w\"> </span><span class=\"n\">MB</span><span class=\"p\">)</span>\n-<span class=\"cp\">#pragma parallel</span>\n-<span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kt\">int</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"w\"> </span><span class=\"n\">N</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"w\"> </span><span class=\"o\">+=</span><span class=\"w\"> </span><span class=\"n\">NB</span><span class=\"p\">){</span>\n-<span class=\"w\">  </span><span class=\"kt\">float</span><span class=\"w\"> </span><span class=\"n\">acc</span><span class=\"p\">[</span><span class=\"n\">MB</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">NB</span><span class=\"p\">]</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">;</span>\n-<span class=\"w\">  </span><span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kt\">int</span><span class=\"w\"> </span><span class=\"n\">k</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">k</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"w\"> </span><span class=\"n\">K</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">k</span><span class=\"w\"> </span><span class=\"o\">+=</span><span class=\"w\"> </span><span class=\"n\">KB</span><span class=\"p\">)</span>\n-<span class=\"w\">    </span><span class=\"n\">acc</span><span class=\"w\"> </span><span class=\"o\">+=</span><span class=\"w\">  </span><span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">m</span><span class=\"o\">:</span><span class=\"n\">m</span><span class=\"o\">+</span><span class=\"n\">MB</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">k</span><span class=\"o\">:</span><span class=\"n\">k</span><span class=\"o\">+</span><span class=\"n\">KB</span><span class=\"p\">]</span>\n-<span class=\"w\">          </span><span class=\"err\">@</span><span class=\"w\"> </span><span class=\"n\">B</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"o\">:</span><span class=\"n\">k</span><span class=\"o\">+</span><span class=\"n\">KB</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"o\">:</span><span class=\"n\">n</span><span class=\"o\">+</span><span class=\"n\">NB</span><span class=\"p\">];</span>\n-<span class=\"w\">  </span><span class=\"n\">C</span><span class=\"p\">[</span><span class=\"n\">m</span><span class=\"o\">:</span><span class=\"n\">m</span><span class=\"o\">+</span><span class=\"n\">MB</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"o\">:</span><span class=\"n\">n</span><span class=\"o\">+</span><span class=\"n\">NB</span><span class=\"p\">]</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">acc</span><span class=\"p\">;</span>\n-<span class=\"p\">}</span>\n-</pre></div>\n-</div>\n-</td>\n-</tr>\n-<tr class=\"row-odd\"><td><p><img alt=\"pic1\" src=\"../../_images/cuda-parallel-matmul.png\" /></p></td>\n-<td><p><img alt=\"pic2\" src=\"../../_images/triton-parallel-matmul.png\" /></p></td>\n-</tr>\n-</tbody>\n-</table>\n-<p>A key benefit of this approach is that it leads to block-structured iteration spaces that offer programmers more flexibility than existing DSLs when implementing sparse operations, all while allowing compilers to aggressively optimize programs for data locality and parallelism.</p>\n-</section>\n-<section id=\"challenges\">\n-<h2>Challenges<a class=\"headerlink\" href=\"#challenges\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>The main challenge posed by our proposed paradigm is that of work scheduling, i.e., how the work done by each program instance should be partitioned for efficient execution on modern GPUs. To address this issue, the Triton compiler makes heavy use of <em>block-level data-flow analysis</em>, a technique for scheduling iteration blocks statically based on the control- and data-flow structure of the target program. The resulting system actually works surprisingly well: our compiler manages to apply a broad range of interesting optimization automatically (e.g., automatic coalescing, thread swizzling, pre-fetching, automatic vectorization, tensor core-aware instruction selection, shared memory allocation/synchronization, asynchronous copy scheduling). Of course doing all this is not trivial; one of the purposes of this guide is to give you a sense of how it works.</p>\n-</section>\n-<section id=\"references\">\n-<h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<div role=\"list\" class=\"citation-list\">\n-<div class=\"citation\" id=\"sutskever2014\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id1\">SUTSKEVER2014</a><span class=\"fn-bracket\">]</span></span>\n-<ol class=\"upperroman simple\">\n-<li><p>Sutskever et al., \u201cSequence to Sequence Learning with Neural Networks\u201d, NIPS 2014</p></li>\n-</ol>\n-</div>\n-<div class=\"citation\" id=\"redmon2016\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id2\">REDMON2016</a><span class=\"fn-bracket\">]</span></span>\n-<ol class=\"upperalpha simple\" start=\"10\">\n-<li><p>Redmon et al., \u201cYou Only Look Once: Unified, Real-Time Object Detection\u201d, CVPR 2016</p></li>\n-</ol>\n-</div>\n-<div class=\"citation\" id=\"lee2017\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id3\">LEE2017</a><span class=\"fn-bracket\">]</span></span>\n-<ol class=\"upperalpha simple\" start=\"11\">\n-<li><p>Lee et al., \u201cSuperhuman Accuracy on the SNEMI3D Connectomics Challenge\u201d, ArXiV 2017</p></li>\n-</ol>\n-</div>\n-<div class=\"citation\" id=\"baghdadi2021\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id4\">BAGHDADI2021</a><span class=\"fn-bracket\">]</span></span>\n-<ol class=\"upperalpha simple\" start=\"18\">\n-<li><p>Baghdadi et al., \u201cTiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code\u201d, CGO 2021</p></li>\n-</ol>\n-</div>\n-<div class=\"citation\" id=\"vasilache2018\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id5\">VASILACHE2018</a><span class=\"fn-bracket\">]</span></span>\n-<ol class=\"upperalpha simple\" start=\"14\">\n-<li><p>Vasilache et al., \u201cTensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions\u201d, ArXiV 2018</p></li>\n-</ol>\n-</div>\n-<div class=\"citation\" id=\"jrk2013\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id6\">JRK2013</a><span class=\"fn-bracket\">]</span></span>\n-<ol class=\"upperalpha simple\" start=\"10\">\n-<li><p>Ragan-Kelley et al., \u201cHalide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines\u201d, PLDI 2013</p></li>\n-</ol>\n-</div>\n-<div class=\"citation\" id=\"chen2018\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id7\">CHEN2018</a><span class=\"fn-bracket\">]</span></span>\n-<ol class=\"upperalpha simple\" start=\"20\">\n-<li><p>Chen et al., \u201cTVM: An Automated End-to-End Optimizing Compiler for Deep Learning\u201d, OSDI 2018</p></li>\n-</ol>\n-</div>\n-<div class=\"citation\" id=\"lam1991\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id8\">LAM1991</a><span class=\"fn-bracket\">]</span></span>\n-<ol class=\"upperalpha simple\" start=\"13\">\n-<li><p>Lam et al., \u201cThe Cache Performance and Optimizations of Blocked Algorithms\u201d, ASPLOS 1991</p></li>\n-</ol>\n-</div>\n-<div class=\"citation\" id=\"auguin1983\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id9\">AUGUIN1983</a><span class=\"fn-bracket\">]</span></span>\n-<ol class=\"upperalpha simple\" start=\"13\">\n-<li><p>Auguin et al., \u201cOpsila: an advanced SIMD for numerical analysis and signal processing\u201d, EUROMICRO 1983</p></li>\n-</ol>\n-</div>\n-</div>\n-</section>\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"../../python-api/generated/triton.testing.perf_report.html\" class=\"btn btn-neutral float-left\" title=\"triton.testing.perf_report\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"../chapter-2/related-work.html\" class=\"btn btn-neutral float-right\" title=\"Related Work\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"introduction.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/programming-guide/chapter-2/related-work.html", "status": "removed", "additions": 0, "deletions": 392, "changes": 392, "file_content_changes": "@@ -1,392 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>Related Work &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-        <script async=\"async\" src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"prev\" title=\"Introduction\" href=\"../chapter-1/introduction.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/installation.html\">Installation</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/tutorials/index.html\">Tutorials</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1 current\"><a class=\"current reference internal\" href=\"#\">Related Work</a><ul>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"#polyhedral-compilation\">Polyhedral Compilation</a><ul>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#program-representation\">Program Representation</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#advantages\">Advantages</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#limitations\">Limitations</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"#scheduling-languages\">Scheduling Languages</a><ul>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#id16\">Advantages</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#id17\">Limitations</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"#references\">References</a></li>\n-</ul>\n-</li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-      <li class=\"breadcrumb-item active\">Related Work</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/programming-guide/chapter-2/related-work.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <section id=\"related-work\">\n-<h1>Related Work<a class=\"headerlink\" href=\"#related-work\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<p>At first sight, Triton may seem like just yet another DSL for DNNs. The purpose of this section is to contextualize Triton and highlight its differences with the two leading approaches in this domain: polyhedral compilation and scheduling languages.</p>\n-<section id=\"polyhedral-compilation\">\n-<h2>Polyhedral Compilation<a class=\"headerlink\" href=\"#polyhedral-compilation\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>Traditional compilers typically rely on intermediate representations, such as LLVM-IR <a class=\"reference internal\" href=\"#lattner2004\" id=\"id1\"><span>[LATTNER2004]</span></a>, that encode control flow information using (un)conditional branches. This relatively low-level format makes it difficult to statically analyze the runtime behavior (e.g., cache misses) of input programs, and to  automatically optimize loops accordingly through the use of tiling <a class=\"reference internal\" href=\"#wolfe1989\" id=\"id2\"><span>[WOLFE1989]</span></a>, fusion <a class=\"reference internal\" href=\"#darte1999\" id=\"id3\"><span>[DARTE1999]</span></a> and interchange <a class=\"reference internal\" href=\"#allen1984\" id=\"id4\"><span>[ALLEN1984]</span></a>. To solve this issue, polyhedral compilers <a class=\"reference internal\" href=\"#ancourt1991\" id=\"id5\"><span>[ANCOURT1991]</span></a> rely on program representations that have statically predictable control flow, thereby enabling aggressive compile-time program transformations for data locality and parallelism. Though this strategy has been adopted by many languages and compilers for DNNs such as Tiramisu <a class=\"reference internal\" href=\"#baghdadi2021\" id=\"id6\"><span>[BAGHDADI2021]</span></a>, Tensor Comprehensions <a class=\"reference internal\" href=\"#vasilache2018\" id=\"id7\"><span>[VASILACHE2018]</span></a>, Diesel <a class=\"reference internal\" href=\"#elango2018\" id=\"id8\"><span>[ELANGO2018]</span></a> and the Affine dialect in MLIR <a class=\"reference internal\" href=\"#lattner2019\" id=\"id9\"><span>[LATTNER2019]</span></a>, it also comes with a number of limitations that will be described later in this section.</p>\n-<section id=\"program-representation\">\n-<h3>Program Representation<a class=\"headerlink\" href=\"#program-representation\" title=\"Permalink to this heading\">\u00b6</a></h3>\n-<p>Polyhedral compilation is a vast area of research. In this section we only outline the most basic aspects of this topic, but readers interested in the solid mathematical foundations underneath may refer to the ample literature on linear and integer programming.</p>\n-<table class=\"docutils align-default\">\n-<colgroup>\n-<col style=\"width: 50%\" />\n-<col style=\"width: 50%\" />\n-</colgroup>\n-<tbody>\n-<tr class=\"row-odd\"><td><div class=\"highlight-C notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kt\">int</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"w\"> </span><span class=\"mi\">3</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"o\">++</span><span class=\"p\">)</span>\n-<span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kt\">int</span><span class=\"w\"> </span><span class=\"n\">j</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">j</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"w\"> </span><span class=\"mi\">5</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">j</span><span class=\"o\">++</span><span class=\"p\">)</span>\n-<span class=\"w\">  </span><span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">][</span><span class=\"n\">j</span><span class=\"p\">]</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">;</span>\n-</pre></div>\n-</div>\n-</td>\n-<td><p><a class=\"reference internal\" href=\"../../_images/polyhedral-iteration.png\"><img alt=\"pic1\" src=\"../../_images/polyhedral-iteration.png\" style=\"width: 300px;\" /></a></p></td>\n-</tr>\n-</tbody>\n-</table>\n-<p>Polyhedral compilers focus on a class of programs commonly known as <strong>Static Control Parts</strong> (SCoP), <em>i.e.</em>, maximal sets of consecutive statements in which conditionals and loop bounds are affine functions of surrounding loop indices and global invariant parameters. As shown above, programs in this format always lead to iteration domains that are bounded by affine inequalities, i.e., polyhedral. These polyhedra can also be defined algebraically; for the above example:</p>\n-<div class=\"math notranslate nohighlight\">\n-\\[\\begin{split}\\mathcal{P} = \\{ i, j \\in \\mathbb{Z}^2\n-~|~\n-\\begin{pmatrix}\n-1 &amp; 0 \\\\\n--1 &amp; 0 \\\\\n--1 &amp; 1 \\\\\n-0 &amp; -1 \\\\\n-\\end{pmatrix}\n-\\begin{pmatrix}\n-i \\\\\n-j\n-\\end{pmatrix}\n-+\n-\\begin{pmatrix}\n-0 \\\\\n-2 \\\\\n-0 \\\\\n-4\n-\\end{pmatrix}\n-\\geq\n-0\n-\\}\\end{split}\\]</div>\n-<p>Each point <span class=\"math notranslate nohighlight\">\\((i, j)\\)</span> in <span class=\"math notranslate nohighlight\">\\(\\mathcal{P}\\)</span> represents a <em>polyhedral statement</em>, that is a program statement which (1) does not induce control-flow side effects (e.g., <code class=\"code docutils literal notranslate\"><span class=\"pre\">for</span></code>, <code class=\"code docutils literal notranslate\"><span class=\"pre\">if</span></code>, <code class=\"code docutils literal notranslate\"><span class=\"pre\">break</span></code>) and (2) contains only affine functions of loop indices and global parameters in array accesses. To facilitate alias analysis, array accesses are also mathematically abstracted, using so-called <em>access function</em>. In other words, <code class=\"code docutils literal notranslate\"><span class=\"pre\">A[i][j]</span></code> is simply <code class=\"code docutils literal notranslate\"><span class=\"pre\">A[f(i,j)]</span></code> where the access function <span class=\"math notranslate nohighlight\">\\(f\\)</span> is defined by:</p>\n-<div class=\"math notranslate nohighlight\">\n-\\[\\begin{split}f(i, j) = \\begin{pmatrix}\n-1 &amp; 0\\\\\n-0 &amp; 1\\\\\n-\\end{pmatrix}\n-\\begin{pmatrix}\n-i\\\\\n-j\n-\\end{pmatrix}\n-=\n-(i, j)\\end{split}\\]</div>\n-<p>Note that the iteration domains of an SCoP does not specify the order in which its statements shall execute. In fact, this iteration domain may be traversed in many different possible legal orders, i.e. <em>schedules</em>. Formally, a schedule is defined as a p-dimensional affine transformation <span class=\"math notranslate nohighlight\">\\(\\Theta\\)</span> of loop indices <span class=\"math notranslate nohighlight\">\\(\\mathbf{x}\\)</span> and global invariant parameters <span class=\"math notranslate nohighlight\">\\(\\mathbf{g}\\)</span>:</p>\n-<div class=\"math notranslate nohighlight\">\n-\\[\\begin{split}\\Theta_S(\\mathbf{x}) = T_S \\begin{pmatrix}\n-\\vec{x}\\\\\n-\\vec{g}\\\\\n-1\n-\\end{pmatrix}\n-\\qquad\n-T_S \\in \\mathbb{Z} ^{p \\times (\\text{dim}(\\mathbf{x}) + \\text{dim}(\\mathbf{g}) + 1)}\\end{split}\\]</div>\n-<p>Where <span class=\"math notranslate nohighlight\">\\(\\Theta_S(\\mathbf{x})\\)</span> is a p-dimensional vector representing the slowest to fastest growing indices (from left to right) when traversing the loop nest surrounding <span class=\"math notranslate nohighlight\">\\(S\\)</span>. For the code shown above, the original schedule defined by the loop nest in C can be retrieved by using:</p>\n-<div class=\"math notranslate nohighlight\">\n-\\[\\begin{split}\\Theta_S(\\mathbf{x}) = \\begin{pmatrix}\n-1 &amp; 0 \\\\\n-0 &amp; 1 \\\\\n-\\end{pmatrix}\n-\\begin{pmatrix}\n-i &amp; j\n-\\end{pmatrix}^T\n-=\n-\\begin{pmatrix}\n-i &amp; j\n-\\end{pmatrix}^T\\end{split}\\]</div>\n-<p>where <span class=\"math notranslate nohighlight\">\\(i\\)</span> and <span class=\"math notranslate nohighlight\">\\(j\\)</span> are respectively the slowest and fastest growing loop indices in the nest. If <span class=\"math notranslate nohighlight\">\\(T_S\\)</span> is a vector (resp. tensor), then <span class=\"math notranslate nohighlight\">\\(\\Theta_S\\)</span> is a said to be one-dimensional (resp. multi-dimensional).</p>\n-</section>\n-<section id=\"advantages\">\n-<h3>Advantages<a class=\"headerlink\" href=\"#advantages\" title=\"Permalink to this heading\">\u00b6</a></h3>\n-<p>Programs amenable to polyhedral compilation can be aggressively transformed and optimized. Most of these transformations actually boil down to the production of  schedules and iteration domains that enable loop transformations promoting parallelism and spatial/temporal data locality (e.g., fusion, interchange, tiling, parallelization).</p>\n-<p>Polyhedral compilers can also automatically go through complex verification processes to ensure that the semantics of their input program is preserved throughout this optimization phase. Note that polyhedral optimizers are not incompatible with more standard optimization techniques. In fact, it is not uncommon for these systems to be implemented as a set of LLVM passes that can be run ahead of more traditional compilation techniques <a class=\"reference internal\" href=\"#grosser2012\" id=\"id10\"><span>[GROSSER2012]</span></a>.</p>\n-<p>All in all, polyhedral machinery is extremely powerful, when applicable. It has been shown to support most common loop transformations, and has indeed achieved performance comparable to state-of-the-art GPU libraries for dense matrix multiplication <a class=\"reference internal\" href=\"#elango2018\" id=\"id11\"><span>[ELANGO2018]</span></a>. Additionally, it is also fully automatic and doesn\u2019t require any hint from programmers apart from source-code in a C-like format.</p>\n-</section>\n-<section id=\"limitations\">\n-<h3>Limitations<a class=\"headerlink\" href=\"#limitations\" title=\"Permalink to this heading\">\u00b6</a></h3>\n-<p>Unfortunately, polyhedral compilers suffer from two major limitations that have prevented its adoption as a universal method for code generation in neural networks.</p>\n-<p>First, the set of possible program transformations <span class=\"math notranslate nohighlight\">\\(\\Omega = \\{ \\Theta_S ~|~ S \\in \\text{program} \\}\\)</span> is large, and grows with the number of statements in the program as well as with the size of their iteration domain. Verifying the legality of each transformation can also require the resolution of complex integer linear programs, making polyhedral compilation very computationally expensive. To make matters worse, hardware properties (e.g., cache size, number of SMs) and contextual characteristics (e.g., input tensor shapes) also have to be taken into account by this framework, leading to expensive auto-tuning procedures <a class=\"reference internal\" href=\"#sato2019\" id=\"id12\"><span>[SATO2019]</span></a>.</p>\n-<p>Second, the polyhedral framework is not very generally applicable; SCoPs are relatively common <a class=\"reference internal\" href=\"#girbal2006\" id=\"id13\"><span>[GIRBAL2006]</span></a> but require loop bounds and array subscripts to be affine functions of loop indices, which typically only occurs in regular, dense computations. For this reason, this framework still has to be successfully applied to sparse \u2013 or even structured-sparse \u2013 neural networks, whose importance has been rapidly rising over the past few years.</p>\n-<p>On the other hand, blocked program representations advocated by this dissertation are less restricted in scope and can achieve close to peak performance using standard dataflow analysis.</p>\n-</section>\n-</section>\n-<section id=\"scheduling-languages\">\n-<h2>Scheduling Languages<a class=\"headerlink\" href=\"#scheduling-languages\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<p>Separation of concerns <a class=\"reference internal\" href=\"#dijkstra82\" id=\"id14\"><span>[DIJKSTRA82]</span></a> is a well-known design principle in computer science: programs should be decomposed into modular layers of abstraction that separate the semantics of their algorithms from the details of their implementation. Systems like Halide and TVM push this philosophy one step further, and enforce this separation at the grammatical level through the use of a  <strong>scheduling language</strong>. The benefits of this methodology are particularly visible in the case of matrix multiplication, where, as one can see below, the definition of the algorithm (Line 1-7) is completely disjoint from its implementation (Line 8-16), meaning that both can be maintained, optimized and distributed independently.</p>\n-<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"linenos\"> 1</span><span class=\"o\">//</span> <span class=\"n\">algorithm</span>\n-<span class=\"linenos\"> 2</span><span class=\"n\">Var</span> <span class=\"n\">x</span><span class=\"p\">(</span><span class=\"s2\">&quot;x&quot;</span><span class=\"p\">),</span> <span class=\"n\">y</span><span class=\"p\">(</span><span class=\"s2\">&quot;y&quot;</span><span class=\"p\">);</span>\n-<span class=\"linenos\"> 3</span><span class=\"n\">Func</span> <span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"s2\">&quot;matmul&quot;</span><span class=\"p\">);</span>\n-<span class=\"linenos\"> 4</span><span class=\"n\">RDom</span> <span class=\"n\">k</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">matrix_size</span><span class=\"p\">);</span>\n-<span class=\"linenos\"> 5</span><span class=\"n\">RVar</span> <span class=\"n\">ki</span><span class=\"p\">;</span>\n-<span class=\"linenos\"> 6</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span><span class=\"n\">f</span><span class=\"p\">;</span>\n-<span class=\"linenos\"> 7</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span> <span class=\"o\">+=</span> <span class=\"n\">A</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">B</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">);</span>\n-<span class=\"linenos\"> 8</span><span class=\"o\">//</span> <span class=\"n\">schedule</span>\n-<span class=\"linenos\"> 9</span><span class=\"n\">Var</span> <span class=\"n\">xi</span><span class=\"p\">(</span><span class=\"s2\">&quot;xi&quot;</span><span class=\"p\">),</span> <span class=\"n\">xo</span><span class=\"p\">(</span><span class=\"s2\">&quot;xo&quot;</span><span class=\"p\">),</span> <span class=\"n\">yo</span><span class=\"p\">(</span><span class=\"s2\">&quot;yo&quot;</span><span class=\"p\">),</span> <span class=\"n\">yi</span><span class=\"p\">(</span><span class=\"s2\">&quot;yo&quot;</span><span class=\"p\">),</span> <span class=\"n\">yii</span><span class=\"p\">(</span><span class=\"s2\">&quot;yii&quot;</span><span class=\"p\">),</span> <span class=\"n\">xii</span><span class=\"p\">(</span><span class=\"s2\">&quot;xii&quot;</span><span class=\"p\">);</span>\n-<span class=\"linenos\">10</span><span class=\"n\">matmul</span><span class=\"o\">.</span><span class=\"n\">vectorize</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">);</span>\n-<span class=\"linenos\">11</span><span class=\"n\">matmul</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-<span class=\"linenos\">12</span>    <span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">xi</span><span class=\"p\">,</span> <span class=\"n\">block_size</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">xi</span><span class=\"p\">,</span> <span class=\"n\">xi</span><span class=\"p\">,</span> <span class=\"n\">xii</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">)</span>\n-<span class=\"linenos\">13</span>    <span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">yi</span><span class=\"p\">,</span> <span class=\"n\">block_size</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">yi</span><span class=\"p\">,</span> <span class=\"n\">yi</span><span class=\"p\">,</span> <span class=\"n\">yii</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">)</span>\n-<span class=\"linenos\">14</span>    <span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">ki</span><span class=\"p\">,</span> <span class=\"n\">block_size</span><span class=\"p\">)</span>\n-<span class=\"linenos\">15</span>    <span class=\"o\">.</span><span class=\"n\">reorder</span><span class=\"p\">(</span><span class=\"n\">xii</span><span class=\"p\">,</span> <span class=\"n\">yii</span><span class=\"p\">,</span> <span class=\"n\">xi</span><span class=\"p\">,</span> <span class=\"n\">ki</span><span class=\"p\">,</span> <span class=\"n\">yi</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n-<span class=\"linenos\">16</span>    <span class=\"o\">.</span><span class=\"n\">parallel</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">vectorize</span><span class=\"p\">(</span><span class=\"n\">xii</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">unroll</span><span class=\"p\">(</span><span class=\"n\">xi</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">unroll</span><span class=\"p\">(</span><span class=\"n\">yii</span><span class=\"p\">);</span>\n-</pre></div>\n-</div>\n-<p>The resulting code may however not be completely portable, as schedules can sometimes rely on execution models (e.g., SPMD) or hardware intrinsics (e.g., matrix-multiply-accumulate) that are not widely available. This issue can be mitigated by auto-scheduling mechanisms <a class=\"reference internal\" href=\"#mullapudi2016\" id=\"id15\"><span>[MULLAPUDI2016]</span></a>.</p>\n-<section id=\"id16\">\n-<h3>Advantages<a class=\"headerlink\" href=\"#id16\" title=\"Permalink to this heading\">\u00b6</a></h3>\n-<p>The main advantage of this approach is that it allows programmers to write an algorithm <em>only once</em>, and focus on performance optimization separately. It makes it possible to manually specify optimizations that a polyhedral compiler wouldn\u2019t be able to figure out automatically using static data-flow analysis.</p>\n-<p>Scheduling languages are, without a doubt, one of the most popular approaches for neural network code generation. The most popular system for this purpose is probably TVM, which provides good performance across a wide range of platforms as well as built-in automatic scheduling mechanisms.</p>\n-</section>\n-<section id=\"id17\">\n-<h3>Limitations<a class=\"headerlink\" href=\"#id17\" title=\"Permalink to this heading\">\u00b6</a></h3>\n-<p>This ease-of-development comes at a cost. First of all, existing systems that follow this paradigm tend to be noticeably slower than Triton on modern hardware when applicable (e.g., V100/A100 tensor cores w/ equal tile sizes). I do believe that this is not a fundamental issue of scheduling languages \u2013 in the sense that it could probably be solved with more efforts \u2013 but it could mean that these systems are harder to engineer. More importantly, existing scheduling languages generate loops whose bounds and increments cannot depend on surrounding loop indices without at least imposing severe constraints on possible schedules \u2013 if not breaking the system entirely. This is problematic for sparse computations, whose iteration spaces may be irregular.</p>\n-<table class=\"docutils align-default\">\n-<colgroup>\n-<col style=\"width: 50%\" />\n-<col style=\"width: 50%\" />\n-</colgroup>\n-<tbody>\n-<tr class=\"row-odd\"><td><div class=\"highlight-C notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kt\">int</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"w\"> </span><span class=\"mi\">4</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"o\">++</span><span class=\"p\">)</span>\n-<span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kt\">int</span><span class=\"w\"> </span><span class=\"n\">j</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">j</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"w\"> </span><span class=\"mi\">4</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">j</span><span class=\"o\">++</span><span class=\"p\">)</span>\n-<span class=\"w\">  </span><span class=\"kt\">float</span><span class=\"w\"> </span><span class=\"n\">acc</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">;</span>\n-<span class=\"w\">  </span><span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kt\">int</span><span class=\"w\"> </span><span class=\"n\">k</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">k</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"w\"> </span><span class=\"n\">K</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">];</span><span class=\"w\"> </span><span class=\"n\">k</span><span class=\"o\">++</span><span class=\"p\">)</span>\n-<span class=\"w\">    </span><span class=\"n\">acc</span><span class=\"w\"> </span><span class=\"o\">+=</span><span class=\"w\"> </span><span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">][</span><span class=\"n\">col</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">k</span><span class=\"p\">]]</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"n\">B</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"p\">][</span><span class=\"n\">j</span><span class=\"p\">]</span>\n-<span class=\"w\">  </span><span class=\"n\">C</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">][</span><span class=\"n\">j</span><span class=\"p\">]</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">acc</span><span class=\"p\">;</span>\n-</pre></div>\n-</div>\n-</td>\n-<td><p><a class=\"reference internal\" href=\"../../_images/halide-iteration.png\"><img alt=\"pic2\" src=\"../../_images/halide-iteration.png\" style=\"width: 300px;\" /></a></p></td>\n-</tr>\n-</tbody>\n-</table>\n-<p>On the other hand, the block-based program representation that we advocate for through this work allows for block-structured iteration spaces and allows programmers to manually handle load-balancing as they wish.</p>\n-</section>\n-</section>\n-<section id=\"references\">\n-<h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this heading\">\u00b6</a></h2>\n-<div role=\"list\" class=\"citation-list\">\n-<div class=\"citation\" id=\"lattner2004\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id1\">LATTNER2004</a><span class=\"fn-bracket\">]</span></span>\n-<ol class=\"upperalpha simple\" start=\"3\">\n-<li><p>Lattner et al., \u201cLLVM: a compilation framework for lifelong program analysis transformation\u201d, CGO 2004</p></li>\n-</ol>\n-</div>\n-<div class=\"citation\" id=\"wolfe1989\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id2\">WOLFE1989</a><span class=\"fn-bracket\">]</span></span>\n-<ol class=\"upperalpha simple\" start=\"13\">\n-<li><p>Wolfe, \u201cMore Iteration Space Tiling\u201d, SC 1989</p></li>\n-</ol>\n-</div>\n-<div class=\"citation\" id=\"darte1999\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id3\">DARTE1999</a><span class=\"fn-bracket\">]</span></span>\n-<ol class=\"upperalpha simple\">\n-<li><p>Darte, \u201cOn the Complexity of Loop Fusion\u201d, PACT 1999</p></li>\n-</ol>\n-</div>\n-<div class=\"citation\" id=\"allen1984\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id4\">ALLEN1984</a><span class=\"fn-bracket\">]</span></span>\n-<ol class=\"upperalpha simple\" start=\"10\">\n-<li><p>Allen et al., \u201cAutomatic Loop Interchange\u201d, SIGPLAN Notices 1984</p></li>\n-</ol>\n-</div>\n-<div class=\"citation\" id=\"ancourt1991\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id5\">ANCOURT1991</a><span class=\"fn-bracket\">]</span></span>\n-<ol class=\"upperalpha simple\" start=\"3\">\n-<li><p>Ancourt et al., \u201cScanning Polyhedra with DO Loops\u201d, PPoPP 1991</p></li>\n-</ol>\n-</div>\n-<div class=\"citation\" id=\"baghdadi2021\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id6\">BAGHDADI2021</a><span class=\"fn-bracket\">]</span></span>\n-<ol class=\"upperalpha simple\" start=\"18\">\n-<li><p>Baghdadi et al., \u201cTiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code\u201d, CGO 2021</p></li>\n-</ol>\n-</div>\n-<div class=\"citation\" id=\"vasilache2018\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id7\">VASILACHE2018</a><span class=\"fn-bracket\">]</span></span>\n-<ol class=\"upperalpha simple\" start=\"14\">\n-<li><p>Vasilache et al., \u201cTensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions\u201d, ArXiV 2018</p></li>\n-</ol>\n-</div>\n-<div class=\"citation\" id=\"elango2018\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span>ELANGO2018<span class=\"fn-bracket\">]</span></span>\n-<span class=\"backrefs\">(<a role=\"doc-backlink\" href=\"#id8\">1</a>,<a role=\"doc-backlink\" href=\"#id11\">2</a>)</span>\n-<ol class=\"upperalpha simple\" start=\"22\">\n-<li><p>Elango et al. \u201cDiesel: DSL for Linear Algebra and Neural Net Computations on GPUs\u201d, MAPL 2018</p></li>\n-</ol>\n-</div>\n-<div class=\"citation\" id=\"lattner2019\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id9\">LATTNER2019</a><span class=\"fn-bracket\">]</span></span>\n-<ol class=\"upperalpha simple\" start=\"3\">\n-<li><p>Lattner et al., \u201cMLIR Primer: A Compiler Infrastructure for the End of Moore\u2019s Law\u201d, Arxiv 2019</p></li>\n-</ol>\n-</div>\n-<div class=\"citation\" id=\"grosser2012\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id10\">GROSSER2012</a><span class=\"fn-bracket\">]</span></span>\n-<ol class=\"upperalpha simple\" start=\"20\">\n-<li><p>Grosser et al., \u201cPolly - Performing Polyhedral Optimizations on a Low-Level Intermediate Representation\u201d, Parallel Processing Letters 2012</p></li>\n-</ol>\n-</div>\n-<div class=\"citation\" id=\"sato2019\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id12\">SATO2019</a><span class=\"fn-bracket\">]</span></span>\n-<ol class=\"upperalpha simple\" start=\"25\">\n-<li><p>Sato et al., \u201cAn Autotuning Framework for Scalable Execution of Tiled Code via Iterative Polyhedral Compilation\u201d, TACO 2019</p></li>\n-</ol>\n-</div>\n-<div class=\"citation\" id=\"girbal2006\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id13\">GIRBAL2006</a><span class=\"fn-bracket\">]</span></span>\n-<ol class=\"upperalpha simple\" start=\"19\">\n-<li><p>Girbal et al., \u201cSemi-Automatic Composition of Loop Transformations for Deep Parallelism and Memory Hierarchies\u201d, International Journal of Parallel Programming 2006</p></li>\n-</ol>\n-</div>\n-<div class=\"citation\" id=\"dijkstra82\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id14\">DIJKSTRA82</a><span class=\"fn-bracket\">]</span></span>\n-<ol class=\"upperalpha simple\" start=\"5\">\n-<li><ol class=\"upperalpha simple\" start=\"23\">\n-<li><p>Dijkstra et al., \u201cOn the role of scientific thought\u201d, Selected writings on computing: a personal perspective 1982</p></li>\n-</ol>\n-</li>\n-</ol>\n-</div>\n-<div class=\"citation\" id=\"mullapudi2016\" role=\"doc-biblioentry\">\n-<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id15\">MULLAPUDI2016</a><span class=\"fn-bracket\">]</span></span>\n-<ol class=\"upperalpha simple\" start=\"18\">\n-<li><p>Mullapudi et al., \u201cAutomatically scheduling halide image processing pipelines\u201d, TOG 2016</p></li>\n-</ol>\n-</div>\n-</div>\n-</section>\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"../chapter-1/introduction.html\" class=\"btn btn-neutral float-left\" title=\"Introduction\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"related-work.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/python-api/generated/triton.Config.html", "status": "removed", "additions": 0, "deletions": 183, "changes": 183, "file_content_changes": "@@ -1,183 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>triton.Config &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton.language\" href=\"../triton.language.html\" />\n-    <link rel=\"prev\" title=\"triton.heuristics\" href=\"triton.heuristics.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/installation.html\">Installation</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/tutorials/index.html\">Tutorials</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"../triton.html\">triton</a><ul class=\"current\">\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"triton.jit.html\">triton.jit</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"triton.autotune.html\">triton.autotune</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"triton.heuristics.html\">triton.heuristics</a></li>\n-<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">triton.Config</a><ul>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#triton.Config\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Config</span></code></a><ul>\n-<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#triton.Config.__init__\"><code class=\"docutils literal notranslate\"><span class=\"pre\">Config.__init__()</span></code></a></li>\n-</ul>\n-</li>\n-</ul>\n-</li>\n-</ul>\n-</li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.language.html\">triton.language</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-          <li class=\"breadcrumb-item\"><a href=\"../triton.html\">triton</a></li>\n-      <li class=\"breadcrumb-item active\">triton.Config</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/python-api/generated/triton.Config.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <section id=\"triton-config\">\n-<h1>triton.Config<a class=\"headerlink\" href=\"#triton-config\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<dl class=\"py class\">\n-<dt class=\"sig sig-object py\" id=\"triton.Config\">\n-<em class=\"property\"><span class=\"pre\">class</span><span class=\"w\"> </span></em><span class=\"sig-prename descclassname\"><span class=\"pre\">triton.</span></span><span class=\"sig-name descname\"><span class=\"pre\">Config</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">self</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">kwargs</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">num_warps</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">4</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">num_stages</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">2</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">pre_hook</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.Config\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>An object that represents a possible kernel configuration for the auto-tuner to try.</p>\n-<dl class=\"field-list simple\">\n-<dt class=\"field-odd\">Variables<span class=\"colon\">:</span></dt>\n-<dd class=\"field-odd\"><ul class=\"simple\">\n-<li><p><strong>meta</strong> \u2013 a dictionary of meta-parameters to pass to the kernel as keyword arguments.</p></li>\n-<li><p><strong>num_warps</strong> \u2013 the number of warps to use for the kernel when compiled for GPUs. For example, if\n-<cite>num_warps=8</cite>, then each kernel instance will be automatically parallelized to\n-cooperatively execute using <cite>8 * 32 = 256</cite> threads.</p></li>\n-<li><p><strong>num_stages</strong> \u2013 the number of stages that the compiler should use when software-pipelining loops.\n-Mostly useful for matrix multiplication workloads on SM80+ GPUs.</p></li>\n-<li><p><strong>pre_hook</strong> \u2013 a function that will be called before the kernel is called. Parameters of this\n-function are args.</p></li>\n-</ul>\n-</dd>\n-</dl>\n-<dl class=\"py method\">\n-<dt class=\"sig sig-object py\" id=\"triton.Config.__init__\">\n-<span class=\"sig-name descname\"><span class=\"pre\">__init__</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">self</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">kwargs</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">num_warps</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">4</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">num_stages</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">2</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">pre_hook</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.Config.__init__\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd></dd></dl>\n-\n-<p class=\"rubric\">Methods</p>\n-<table class=\"autosummary longtable docutils align-default\">\n-<tbody>\n-<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"#triton.Config.__init__\" title=\"triton.Config.__init__\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">__init__</span></code></a>(self,\u00a0kwargs[,\u00a0num_warps,\u00a0...])</p></td>\n-<td><p></p></td>\n-</tr>\n-</tbody>\n-</table>\n-</dd></dl>\n-\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"triton.heuristics.html\" class=\"btn btn-neutral float-left\" title=\"triton.heuristics\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"../triton.language.html\" class=\"btn btn-neutral float-right\" title=\"triton.language\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.Config.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/python-api/generated/triton.autotune.html", "status": "removed", "additions": 0, "deletions": 186, "changes": 186, "file_content_changes": "@@ -1,186 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>triton.autotune &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton.heuristics\" href=\"triton.heuristics.html\" />\n-    <link rel=\"prev\" title=\"triton.jit\" href=\"triton.jit.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/installation.html\">Installation</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/tutorials/index.html\">Tutorials</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"../triton.html\">triton</a><ul class=\"current\">\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"triton.jit.html\">triton.jit</a></li>\n-<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">triton.autotune</a><ul>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#triton.autotune\"><code class=\"docutils literal notranslate\"><span class=\"pre\">autotune()</span></code></a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"triton.heuristics.html\">triton.heuristics</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"triton.Config.html\">triton.Config</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.language.html\">triton.language</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-          <li class=\"breadcrumb-item\"><a href=\"../triton.html\">triton</a></li>\n-      <li class=\"breadcrumb-item active\">triton.autotune</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/python-api/generated/triton.autotune.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <section id=\"triton-autotune\">\n-<h1>triton.autotune<a class=\"headerlink\" href=\"#triton-autotune\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<dl class=\"py function\">\n-<dt class=\"sig sig-object py\" id=\"triton.autotune\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.</span></span><span class=\"sig-name descname\"><span class=\"pre\">autotune</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">configs</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">key</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prune_configs_by</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">reset_to_zero</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">warmup</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">25</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">rep</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">100</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.autotune\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Decorator for auto-tuning a <code class=\"code docutils literal notranslate\"><span class=\"pre\">triton.jit</span></code>\u2019d function.</p>\n-<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">autotune</span><span class=\"p\">(</span><span class=\"n\">configs</span><span class=\"o\">=</span><span class=\"p\">[</span>\n-    <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">(</span><span class=\"n\">meta</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">},</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n-    <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">(</span><span class=\"n\">meta</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">:</span> <span class=\"mi\">1024</span><span class=\"p\">},</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">),</span>\n-  <span class=\"p\">],</span>\n-  <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;x_size&#39;</span><span class=\"p\">]</span> <span class=\"c1\"># the two above configs will be evaluated anytime</span>\n-                 <span class=\"c1\"># the value of x_size changes</span>\n-<span class=\"p\">)</span>\n-<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n-<span class=\"k\">def</span> <span class=\"nf\">kernel</span><span class=\"p\">(</span><span class=\"n\">x_ptr</span><span class=\"p\">,</span> <span class=\"n\">x_size</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">META</span><span class=\"p\">):</span>\n-    <span class=\"n\">BLOCK_SIZE</span> <span class=\"o\">=</span> <span class=\"n\">META</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">]</span>\n-</pre></div>\n-</div>\n-<dl class=\"field-list simple\">\n-<dt class=\"field-odd\">Note<span class=\"colon\">:</span></dt>\n-<dd class=\"field-odd\"><p>When all the configurations are evaluated, the kernel will run multiple times.\n-This means that whatever value the kernel updates will be updated multiple times.\n-To avoid this undesired behavior, you can use the <cite>reset_to_zero</cite> argument, which\n-resets the value of the provided tensor to <cite>zero</cite> before running any configuration.</p>\n-</dd>\n-<dt class=\"field-even\">Parameters<span class=\"colon\">:</span></dt>\n-<dd class=\"field-even\"><ul class=\"simple\">\n-<li><p><strong>configs</strong> (<em>list</em><em>[</em><a class=\"reference internal\" href=\"triton.Config.html#triton.Config\" title=\"triton.Config\"><em>triton.Config</em></a><em>]</em>) \u2013 a list of <code class=\"code docutils literal notranslate\"><span class=\"pre\">triton.Config</span></code> objects</p></li>\n-<li><p><strong>key</strong> (<em>list</em><em>[</em><em>str</em><em>]</em>) \u2013 a list of argument names whose change in value will trigger the evaluation of all provided configs.</p></li>\n-<li><p><strong>prune_configs_by</strong> \u2013 a dict of functions that are used to prune configs, fields:\n-\u2018perf_model\u2019: performance model used to predicate running time with different configs, returns running time\n-\u2018top_k\u2019: number of configs to bench\n-\u2018early_config_prune\u2019(optional): a function used to do early prune (eg, num_stages). It takes configs:List[Config] as its input, and returns pruned configs.</p></li>\n-<li><p><strong>reset_to_zero</strong> (<em>list</em><em>[</em><em>str</em><em>]</em>) \u2013 a list of argument names whose value will be reset to zero before evaluating any configs.</p></li>\n-<li><p><strong>warmup</strong> (<em>int</em>) \u2013 Warmup time (in ms) to pass to benchmarking, defaults to 25.</p></li>\n-<li><p><strong>rep</strong> (<em>int</em>) \u2013 Repetition time (in ms) to pass to benchmarking, defaults to 100.</p></li>\n-</ul>\n-</dd>\n-</dl>\n-</dd></dl>\n-\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"triton.jit.html\" class=\"btn btn-neutral float-left\" title=\"triton.jit\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"triton.heuristics.html\" class=\"btn btn-neutral float-right\" title=\"triton.heuristics\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.autotune.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/python-api/generated/triton.heuristics.html", "status": "removed", "additions": 0, "deletions": 166, "changes": 166, "file_content_changes": "@@ -1,166 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>triton.heuristics &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton.Config\" href=\"triton.Config.html\" />\n-    <link rel=\"prev\" title=\"triton.autotune\" href=\"triton.autotune.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/installation.html\">Installation</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/tutorials/index.html\">Tutorials</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"../triton.html\">triton</a><ul class=\"current\">\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"triton.jit.html\">triton.jit</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"triton.autotune.html\">triton.autotune</a></li>\n-<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">triton.heuristics</a><ul>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#triton.heuristics\"><code class=\"docutils literal notranslate\"><span class=\"pre\">heuristics()</span></code></a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"triton.Config.html\">triton.Config</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.language.html\">triton.language</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-          <li class=\"breadcrumb-item\"><a href=\"../triton.html\">triton</a></li>\n-      <li class=\"breadcrumb-item active\">triton.heuristics</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/python-api/generated/triton.heuristics.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <section id=\"triton-heuristics\">\n-<h1>triton.heuristics<a class=\"headerlink\" href=\"#triton-heuristics\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<dl class=\"py function\">\n-<dt class=\"sig sig-object py\" id=\"triton.heuristics\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.</span></span><span class=\"sig-name descname\"><span class=\"pre\">heuristics</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">values</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.heuristics\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Decorator for specifying how the values of certain meta-parameters may be computed.\n-This is useful for cases where auto-tuning is prohibitevely expensive, or just not applicable.</p>\n-<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">heuristics</span><span class=\"p\">(</span><span class=\"n\">values</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">:</span> <span class=\"k\">lambda</span> <span class=\"n\">args</span><span class=\"p\">:</span> <span class=\"mi\">2</span> <span class=\"o\">**</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">ceil</span><span class=\"p\">(</span><span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">log2</span><span class=\"p\">(</span><span class=\"n\">args</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])))})</span>\n-<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n-<span class=\"k\">def</span> <span class=\"nf\">kernel</span><span class=\"p\">(</span><span class=\"n\">x_ptr</span><span class=\"p\">,</span> <span class=\"n\">x_size</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">META</span><span class=\"p\">):</span>\n-    <span class=\"n\">BLOCK_SIZE</span> <span class=\"o\">=</span> <span class=\"n\">META</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">]</span> <span class=\"c1\"># smallest power-of-two &gt;= x_size</span>\n-</pre></div>\n-</div>\n-<dl class=\"field-list simple\">\n-<dt class=\"field-odd\">Parameters<span class=\"colon\">:</span></dt>\n-<dd class=\"field-odd\"><p><strong>values</strong> (<em>dict</em><em>[</em><em>str</em><em>, </em><em>Callable</em><em>[</em><em>[</em><em>list</em><em>[</em><em>Any</em><em>]</em><em>]</em><em>, </em><em>Any</em><em>]</em><em>]</em>) \u2013 a dictionary of meta-parameter names and functions that compute the value of the meta-parameter.\n-each such function takes a list of positional arguments as input.</p>\n-</dd>\n-</dl>\n-</dd></dl>\n-\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"triton.autotune.html\" class=\"btn btn-neutral float-left\" title=\"triton.autotune\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"triton.Config.html\" class=\"btn btn-neutral float-right\" title=\"triton.Config\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.heuristics.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/python-api/generated/triton.jit.html", "status": "removed", "additions": 0, "deletions": 174, "changes": 174, "file_content_changes": "@@ -1,174 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>triton.jit &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton.autotune\" href=\"triton.autotune.html\" />\n-    <link rel=\"prev\" title=\"triton\" href=\"../triton.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/installation.html\">Installation</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/tutorials/index.html\">Tutorials</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"../triton.html\">triton</a><ul class=\"current\">\n-<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">triton.jit</a><ul>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#triton.jit\"><code class=\"docutils literal notranslate\"><span class=\"pre\">jit()</span></code></a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"triton.autotune.html\">triton.autotune</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"triton.heuristics.html\">triton.heuristics</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"triton.Config.html\">triton.Config</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.language.html\">triton.language</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-          <li class=\"breadcrumb-item\"><a href=\"../triton.html\">triton</a></li>\n-      <li class=\"breadcrumb-item active\">triton.jit</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/python-api/generated/triton.jit.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <section id=\"triton-jit\">\n-<h1>triton.jit<a class=\"headerlink\" href=\"#triton-jit\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<dl class=\"py function\">\n-<dt class=\"sig sig-object py\" id=\"triton.jit\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.</span></span><span class=\"sig-name descname\"><span class=\"pre\">jit</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">fn</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"> </span><span class=\"n\"><span class=\"pre\">T</span></span></em><span class=\"sig-paren\">)</span> <span class=\"sig-return\"><span class=\"sig-return-icon\">&#x2192;</span> <span class=\"sig-return-typehint\"><span class=\"pre\">JITFunction</span><span class=\"p\"><span class=\"pre\">[</span></span><span class=\"pre\">T</span><span class=\"p\"><span class=\"pre\">]</span></span></span></span><a class=\"headerlink\" href=\"#triton.jit\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dt class=\"sig sig-object py\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.</span></span><span class=\"sig-name descname\"><span class=\"pre\">jit</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">version</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">do_not_specialize</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"> </span><span class=\"n\"><span class=\"pre\">Iterable</span><span class=\"p\"><span class=\"pre\">[</span></span><span class=\"pre\">int</span><span class=\"p\"><span class=\"pre\">]</span></span><span class=\"w\"> </span><span class=\"p\"><span class=\"pre\">|</span></span><span class=\"w\"> </span><span class=\"pre\">None</span></span><span class=\"w\"> </span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"> </span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">debug</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"> </span><span class=\"n\"><span class=\"pre\">bool</span><span class=\"w\"> </span><span class=\"p\"><span class=\"pre\">|</span></span><span class=\"w\"> </span><span class=\"pre\">None</span></span><span class=\"w\"> </span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"> </span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">noinline</span></span><span class=\"p\"><span class=\"pre\">:</span></span><span class=\"w\"> </span><span class=\"n\"><span class=\"pre\">bool</span><span class=\"w\"> </span><span class=\"p\"><span class=\"pre\">|</span></span><span class=\"w\"> </span><span class=\"pre\">None</span></span><span class=\"w\"> </span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"w\"> </span><span class=\"default_value\"><span class=\"pre\">None</span></span></em><span class=\"sig-paren\">)</span> <span class=\"sig-return\"><span class=\"sig-return-icon\">&#x2192;</span> <span class=\"sig-return-typehint\"><span class=\"pre\">Callable</span><span class=\"p\"><span class=\"pre\">[</span></span><span class=\"p\"><span class=\"pre\">[</span></span><span class=\"pre\">T</span><span class=\"p\"><span class=\"pre\">]</span></span><span class=\"p\"><span class=\"pre\">,</span></span><span class=\"w\"> </span><span class=\"pre\">JITFunction</span><span class=\"p\"><span class=\"pre\">[</span></span><span class=\"pre\">T</span><span class=\"p\"><span class=\"pre\">]</span></span><span class=\"p\"><span class=\"pre\">]</span></span></span></span></dt>\n-<dd><p>Decorator for JIT-compiling a function using the Triton compiler.</p>\n-<dl class=\"field-list simple\">\n-<dt class=\"field-odd\">Note<span class=\"colon\">:</span></dt>\n-<dd class=\"field-odd\"><p>When a jit\u2019d function is called, arguments are\n-implicitly converted to pointers if they have a <code class=\"code docutils literal notranslate\"><span class=\"pre\">.data_ptr()</span></code> method\n-and a <cite>.dtype</cite> attribute.</p>\n-</dd>\n-<dt class=\"field-even\">Note<span class=\"colon\">:</span></dt>\n-<dd class=\"field-even\"><p>This function will be compiled and run on the GPU. It will only have access to:</p>\n-<ul class=\"simple\">\n-<li><p>python primitives,</p></li>\n-<li><p>builtins within the triton package,</p></li>\n-<li><p>arguments to this function,</p></li>\n-<li><p>other jit\u2019d functions</p></li>\n-</ul>\n-</dd>\n-<dt class=\"field-odd\">Parameters<span class=\"colon\">:</span></dt>\n-<dd class=\"field-odd\"><p><strong>fn</strong> (<em>Callable</em>) \u2013 the function to be jit-compiled</p>\n-</dd>\n-</dl>\n-</dd></dl>\n-\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"../triton.html\" class=\"btn btn-neutral float-left\" title=\"triton\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"triton.autotune.html\" class=\"btn btn-neutral float-right\" title=\"triton.autotune\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.jit.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/python-api/generated/triton.language.abs.html", "status": "removed", "additions": 0, "deletions": 181, "changes": 181, "file_content_changes": "@@ -1,181 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>triton.language.abs &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton.language.exp\" href=\"triton.language.exp.html\" />\n-    <link rel=\"prev\" title=\"triton.language.where\" href=\"triton.language.where.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/installation.html\">Installation</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/tutorials/index.html\">Tutorials</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.html\">triton</a></li>\n-<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"../triton.language.html\">triton.language</a><ul class=\"current\">\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#programming-model\">Programming Model</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#creation-ops\">Creation Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#shape-manipulation-ops\">Shape Manipulation Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#linear-algebra-ops\">Linear Algebra Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n-<li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a><ul class=\"current\">\n-<li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.abs</a><ul>\n-<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#triton.language.abs\"><code class=\"docutils literal notranslate\"><span class=\"pre\">abs()</span></code></a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.exp.html\">triton.language.exp</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.log.html\">triton.language.log</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.fdiv.html\">triton.language.fdiv</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.cos.html\">triton.language.cos</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.sin.html\">triton.language.sin</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.sqrt.html\">triton.language.sqrt</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.sigmoid.html\">triton.language.sigmoid</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.softmax.html\">triton.language.softmax</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.umulhi.html\">triton.language.umulhi</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#reduction-ops\">Reduction Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#scan-ops\">Scan Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#atomic-ops\">Atomic Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#comparison-ops\">Comparison ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#random-number-generation\">Random Number Generation</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#compiler-hint-ops\">Compiler Hint Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#debug-ops\">Debug Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#iterators\">Iterators</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-          <li class=\"breadcrumb-item\"><a href=\"../triton.language.html\">triton.language</a></li>\n-      <li class=\"breadcrumb-item active\">triton.language.abs</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/python-api/generated/triton.language.abs.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <section id=\"triton-language-abs\">\n-<h1>triton.language.abs<a class=\"headerlink\" href=\"#triton-language-abs\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<dl class=\"py function\">\n-<dt class=\"sig sig-object py\" id=\"triton.language.abs\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">abs</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">x</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.abs\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Computes the element-wise absolute value of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code>.</p>\n-<dl class=\"field-list simple\">\n-<dt class=\"field-odd\">Parameters<span class=\"colon\">:</span></dt>\n-<dd class=\"field-odd\"><p><strong>x</strong> (<em>Block</em>) \u2013 the input values</p>\n-</dd>\n-</dl>\n-</dd></dl>\n-\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"triton.language.where.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.where\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"triton.language.exp.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.exp\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.abs.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/python-api/generated/triton.language.arange.html", "status": "removed", "additions": 0, "deletions": 178, "changes": 178, "file_content_changes": "@@ -1,178 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>triton.language.arange &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton.language.cat\" href=\"triton.language.cat.html\" />\n-    <link rel=\"prev\" title=\"triton.language.num_programs\" href=\"triton.language.num_programs.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/installation.html\">Installation</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/tutorials/index.html\">Tutorials</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.html\">triton</a></li>\n-<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"../triton.language.html\">triton.language</a><ul class=\"current\">\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#programming-model\">Programming Model</a></li>\n-<li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#creation-ops\">Creation Ops</a><ul class=\"current\">\n-<li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.arange</a><ul>\n-<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#triton.language.arange\"><code class=\"docutils literal notranslate\"><span class=\"pre\">arange()</span></code></a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.cat.html\">triton.language.cat</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.full.html\">triton.language.full</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.zeros.html\">triton.language.zeros</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#shape-manipulation-ops\">Shape Manipulation Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#linear-algebra-ops\">Linear Algebra Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#reduction-ops\">Reduction Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#scan-ops\">Scan Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#atomic-ops\">Atomic Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#comparison-ops\">Comparison ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#random-number-generation\">Random Number Generation</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#compiler-hint-ops\">Compiler Hint Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#debug-ops\">Debug Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#iterators\">Iterators</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-          <li class=\"breadcrumb-item\"><a href=\"../triton.language.html\">triton.language</a></li>\n-      <li class=\"breadcrumb-item active\">triton.language.arange</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/python-api/generated/triton.language.arange.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <section id=\"triton-language-arange\">\n-<h1>triton.language.arange<a class=\"headerlink\" href=\"#triton-language-arange\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<dl class=\"py function\">\n-<dt class=\"sig sig-object py\" id=\"triton.language.arange\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">arange</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">start</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">end</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.arange\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Returns contiguous values within the left-closed and right-open interval [<code class=\"code docutils literal notranslate\"><span class=\"pre\">start</span></code>, <code class=\"code docutils literal notranslate\"><span class=\"pre\">end</span></code>).     End - Start must be less than or equal to TRITON_MAX_TENSOR_NUMEL = 131072</p>\n-<dl class=\"field-list simple\">\n-<dt class=\"field-odd\">Parameters<span class=\"colon\">:</span></dt>\n-<dd class=\"field-odd\"><ul class=\"simple\">\n-<li><p><strong>start</strong> (<em>int32</em>) \u2013 Start of the interval. Must be a power of two.</p></li>\n-<li><p><strong>end</strong> (<em>int32</em>) \u2013 End of the interval. Must be a power of two &gt; start.</p></li>\n-</ul>\n-</dd>\n-</dl>\n-</dd></dl>\n-\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"triton.language.num_programs.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.num_programs\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"triton.language.cat.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.cat\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.arange.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/python-api/generated/triton.language.argmax.html", "status": "removed", "additions": 0, "deletions": 182, "changes": 182, "file_content_changes": "@@ -1,182 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>triton.language.argmax &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton.language.argmin\" href=\"triton.language.argmin.html\" />\n-    <link rel=\"prev\" title=\"triton.language.umulhi\" href=\"triton.language.umulhi.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/installation.html\">Installation</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/tutorials/index.html\">Tutorials</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.html\">triton</a></li>\n-<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"../triton.language.html\">triton.language</a><ul class=\"current\">\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#programming-model\">Programming Model</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#creation-ops\">Creation Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#shape-manipulation-ops\">Shape Manipulation Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#linear-algebra-ops\">Linear Algebra Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a></li>\n-<li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#reduction-ops\">Reduction Ops</a><ul class=\"current\">\n-<li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.argmax</a><ul>\n-<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#triton.language.argmax\"><code class=\"docutils literal notranslate\"><span class=\"pre\">argmax()</span></code></a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.argmin.html\">triton.language.argmin</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.max.html\">triton.language.max</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.min.html\">triton.language.min</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.reduce.html\">triton.language.reduce</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.sum.html\">triton.language.sum</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.xor_sum.html\">triton.language.xor_sum</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#scan-ops\">Scan Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#atomic-ops\">Atomic Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#comparison-ops\">Comparison ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#random-number-generation\">Random Number Generation</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#compiler-hint-ops\">Compiler Hint Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#debug-ops\">Debug Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#iterators\">Iterators</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-          <li class=\"breadcrumb-item\"><a href=\"../triton.language.html\">triton.language</a></li>\n-      <li class=\"breadcrumb-item active\">triton.language.argmax</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/python-api/generated/triton.language.argmax.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <section id=\"triton-language-argmax\">\n-<h1>triton.language.argmax<a class=\"headerlink\" href=\"#triton-language-argmax\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<dl class=\"py function\">\n-<dt class=\"sig sig-object py\" id=\"triton.language.argmax\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">argmax</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">input</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">axis</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">tie_break_left</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.argmax\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Returns the maximum index of all elements in the <code class=\"code docutils literal notranslate\"><span class=\"pre\">input</span></code> tensor along the provided <code class=\"code docutils literal notranslate\"><span class=\"pre\">axis</span></code></p>\n-<dl class=\"field-list simple\">\n-<dt class=\"field-odd\">Parameters<span class=\"colon\">:</span></dt>\n-<dd class=\"field-odd\"><ul class=\"simple\">\n-<li><p><strong>input</strong> \u2013 the input values</p></li>\n-<li><p><strong>axis</strong> \u2013 the dimension along which the reduction should be done</p></li>\n-<li><p><strong>tie_break_left</strong> \u2013 if true, return the left-most indices in case of ties for values that aren\u2019t NaN</p></li>\n-</ul>\n-</dd>\n-</dl>\n-</dd></dl>\n-\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"triton.language.umulhi.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.umulhi\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"triton.language.argmin.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.argmin\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.argmax.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/python-api/generated/triton.language.argmin.html", "status": "removed", "additions": 0, "deletions": 182, "changes": 182, "file_content_changes": "@@ -1,182 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>triton.language.argmin &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton.language.max\" href=\"triton.language.max.html\" />\n-    <link rel=\"prev\" title=\"triton.language.argmax\" href=\"triton.language.argmax.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/installation.html\">Installation</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/tutorials/index.html\">Tutorials</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.html\">triton</a></li>\n-<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"../triton.language.html\">triton.language</a><ul class=\"current\">\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#programming-model\">Programming Model</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#creation-ops\">Creation Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#shape-manipulation-ops\">Shape Manipulation Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#linear-algebra-ops\">Linear Algebra Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a></li>\n-<li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#reduction-ops\">Reduction Ops</a><ul class=\"current\">\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.argmax.html\">triton.language.argmax</a></li>\n-<li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.argmin</a><ul>\n-<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#triton.language.argmin\"><code class=\"docutils literal notranslate\"><span class=\"pre\">argmin()</span></code></a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.max.html\">triton.language.max</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.min.html\">triton.language.min</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.reduce.html\">triton.language.reduce</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.sum.html\">triton.language.sum</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.xor_sum.html\">triton.language.xor_sum</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#scan-ops\">Scan Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#atomic-ops\">Atomic Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#comparison-ops\">Comparison ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#random-number-generation\">Random Number Generation</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#compiler-hint-ops\">Compiler Hint Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#debug-ops\">Debug Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#iterators\">Iterators</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-          <li class=\"breadcrumb-item\"><a href=\"../triton.language.html\">triton.language</a></li>\n-      <li class=\"breadcrumb-item active\">triton.language.argmin</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/python-api/generated/triton.language.argmin.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <section id=\"triton-language-argmin\">\n-<h1>triton.language.argmin<a class=\"headerlink\" href=\"#triton-language-argmin\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<dl class=\"py function\">\n-<dt class=\"sig sig-object py\" id=\"triton.language.argmin\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">argmin</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">input</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">axis</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">tie_break_left</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.argmin\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Returns the minimum index of all elements in the <code class=\"code docutils literal notranslate\"><span class=\"pre\">input</span></code> tensor along the provided <code class=\"code docutils literal notranslate\"><span class=\"pre\">axis</span></code></p>\n-<dl class=\"field-list simple\">\n-<dt class=\"field-odd\">Parameters<span class=\"colon\">:</span></dt>\n-<dd class=\"field-odd\"><ul class=\"simple\">\n-<li><p><strong>input</strong> \u2013 the input values</p></li>\n-<li><p><strong>axis</strong> \u2013 the dimension along which the reduction should be done</p></li>\n-<li><p><strong>tie_break_left</strong> \u2013 if true, return the left-most indices in case of ties for values that aren\u2019t NaN</p></li>\n-</ul>\n-</dd>\n-</dl>\n-</dd></dl>\n-\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"triton.language.argmax.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.argmax\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"triton.language.max.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.max\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.argmin.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/python-api/generated/triton.language.associative_scan.html", "status": "removed", "additions": 0, "deletions": 178, "changes": 178, "file_content_changes": "@@ -1,178 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>triton.language.associative_scan &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton.language.cumsum\" href=\"triton.language.cumsum.html\" />\n-    <link rel=\"prev\" title=\"triton.language.xor_sum\" href=\"triton.language.xor_sum.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/installation.html\">Installation</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/tutorials/index.html\">Tutorials</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.html\">triton</a></li>\n-<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"../triton.language.html\">triton.language</a><ul class=\"current\">\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#programming-model\">Programming Model</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#creation-ops\">Creation Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#shape-manipulation-ops\">Shape Manipulation Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#linear-algebra-ops\">Linear Algebra Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#reduction-ops\">Reduction Ops</a></li>\n-<li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#scan-ops\">Scan Ops</a><ul class=\"current\">\n-<li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.associative_scan</a><ul>\n-<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#triton.language.associative_scan\"><code class=\"docutils literal notranslate\"><span class=\"pre\">associative_scan()</span></code></a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.cumsum.html\">triton.language.cumsum</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.cumprod.html\">triton.language.cumprod</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#atomic-ops\">Atomic Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#comparison-ops\">Comparison ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#random-number-generation\">Random Number Generation</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#compiler-hint-ops\">Compiler Hint Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#debug-ops\">Debug Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#iterators\">Iterators</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-          <li class=\"breadcrumb-item\"><a href=\"../triton.language.html\">triton.language</a></li>\n-      <li class=\"breadcrumb-item active\">triton.language.associative_scan</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/python-api/generated/triton.language.associative_scan.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <section id=\"triton-language-associative-scan\">\n-<h1>triton.language.associative_scan<a class=\"headerlink\" href=\"#triton-language-associative-scan\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<dl class=\"py function\">\n-<dt class=\"sig sig-object py\" id=\"triton.language.associative_scan\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">associative_scan</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">input</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">axis</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">combine_fn</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.associative_scan\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Applies the combine_fn to each elements with a carry in <code class=\"code docutils literal notranslate\"><span class=\"pre\">input</span></code> tensors along the provided <code class=\"code docutils literal notranslate\"><span class=\"pre\">axis</span></code> and update the carry</p>\n-<dl class=\"field-list simple\">\n-<dt class=\"field-odd\">Parameters<span class=\"colon\">:</span></dt>\n-<dd class=\"field-odd\"><ul class=\"simple\">\n-<li><p><strong>input</strong> \u2013 the input tensor, or tuple of tensors</p></li>\n-<li><p><strong>axis</strong> \u2013 the dimension along which the reduction should be done</p></li>\n-<li><p><strong>combine_fn</strong> \u2013 a function to combine two groups of scalar tensors (must be marked with &#64;triton.jit)</p></li>\n-</ul>\n-</dd>\n-</dl>\n-</dd></dl>\n-\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"triton.language.xor_sum.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.xor_sum\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"triton.language.cumsum.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.cumsum\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.associative_scan.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/python-api/generated/triton.language.atomic_add.html", "status": "removed", "additions": 0, "deletions": 180, "changes": 180, "file_content_changes": "@@ -1,180 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>triton.language.atomic_add &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton.language.atomic_max\" href=\"triton.language.atomic_max.html\" />\n-    <link rel=\"prev\" title=\"triton.language.cumprod\" href=\"triton.language.cumprod.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/installation.html\">Installation</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/tutorials/index.html\">Tutorials</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.html\">triton</a></li>\n-<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"../triton.language.html\">triton.language</a><ul class=\"current\">\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#programming-model\">Programming Model</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#creation-ops\">Creation Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#shape-manipulation-ops\">Shape Manipulation Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#linear-algebra-ops\">Linear Algebra Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#reduction-ops\">Reduction Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#scan-ops\">Scan Ops</a></li>\n-<li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#atomic-ops\">Atomic Ops</a><ul class=\"current\">\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.atomic_cas.html\">triton.language.atomic_cas</a></li>\n-<li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.atomic_add</a><ul>\n-<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#triton.language.atomic_add\"><code class=\"docutils literal notranslate\"><span class=\"pre\">atomic_add()</span></code></a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.atomic_max.html\">triton.language.atomic_max</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.atomic_min.html\">triton.language.atomic_min</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#comparison-ops\">Comparison ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#random-number-generation\">Random Number Generation</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#compiler-hint-ops\">Compiler Hint Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#debug-ops\">Debug Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#iterators\">Iterators</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-          <li class=\"breadcrumb-item\"><a href=\"../triton.language.html\">triton.language</a></li>\n-      <li class=\"breadcrumb-item active\">triton.language.atomic_add</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/python-api/generated/triton.language.atomic_add.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <section id=\"triton-language-atomic-add\">\n-<h1>triton.language.atomic_add<a class=\"headerlink\" href=\"#triton-language-atomic-add\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<dl class=\"py function\">\n-<dt class=\"sig sig-object py\" id=\"triton.language.atomic_add\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">atomic_add</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">pointer</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">val</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">mask</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">sem</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.atomic_add\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Performs an atomic add at the memory location specified by <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer</span></code>.</p>\n-<p>Return the data stored at <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer</span></code> before the atomic operation.</p>\n-<dl class=\"field-list simple\">\n-<dt class=\"field-odd\">Parameters<span class=\"colon\">:</span></dt>\n-<dd class=\"field-odd\"><ul class=\"simple\">\n-<li><p><strong>pointer</strong> (<em>Block</em><em> of </em><em>dtype=triton.PointerDType</em>) \u2013 The memory locations to compare-and-swap.</p></li>\n-<li><p><strong>cmp</strong> (<em>Block</em><em> of </em><em>dtype=`pointer.dtype.element_ty`</em>) \u2013 The values expected to be found in the atomic object</p></li>\n-<li><p><strong>val</strong> (<em>Block</em><em> of </em><em>dtype=`pointer.dtype.element_ty`</em>) \u2013 The values to copy in case the expected value matches the contained value.</p></li>\n-</ul>\n-</dd>\n-</dl>\n-</dd></dl>\n-\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"triton.language.cumprod.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.cumprod\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"triton.language.atomic_max.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.atomic_max\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.atomic_add.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/python-api/generated/triton.language.atomic_cas.html", "status": "removed", "additions": 0, "deletions": 189, "changes": 189, "file_content_changes": "@@ -1,189 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>triton.language.atomic_cas &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton.language.atomic_xchg\" href=\"triton.language.atomic_xchg.html\" />\n-    <link rel=\"prev\" title=\"triton.language.store\" href=\"triton.language.store.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/installation.html\">Installation</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/tutorials/index.html\">Tutorials</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.html\">triton</a></li>\n-<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"../triton.language.html\">triton.language</a><ul class=\"current\">\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#programming-model\">Programming Model</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#creation-ops\">Creation Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#shape-manipulation-ops\">Shape Manipulation Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#linear-algebra-ops\">Linear Algebra Ops</a></li>\n-<li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a><ul class=\"current\">\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.load.html\">triton.language.load</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.store.html\">triton.language.store</a></li>\n-<li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.atomic_cas</a><ul>\n-<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#triton.language.atomic_cas\"><code class=\"docutils literal notranslate\"><span class=\"pre\">atomic_cas()</span></code></a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.atomic_xchg.html\">triton.language.atomic_xchg</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#reduction-ops\">Reduction Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#scan-ops\">Scan Ops</a></li>\n-<li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#atomic-ops\">Atomic Ops</a><ul class=\"current\">\n-<li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.atomic_cas</a><ul>\n-<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#triton.language.atomic_cas\"><code class=\"docutils literal notranslate\"><span class=\"pre\">atomic_cas()</span></code></a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.atomic_add.html\">triton.language.atomic_add</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.atomic_max.html\">triton.language.atomic_max</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.atomic_min.html\">triton.language.atomic_min</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#comparison-ops\">Comparison ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#random-number-generation\">Random Number Generation</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#compiler-hint-ops\">Compiler Hint Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#debug-ops\">Debug Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#iterators\">Iterators</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-          <li class=\"breadcrumb-item\"><a href=\"../triton.language.html\">triton.language</a></li>\n-      <li class=\"breadcrumb-item active\">triton.language.atomic_cas</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/python-api/generated/triton.language.atomic_cas.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <section id=\"triton-language-atomic-cas\">\n-<h1>triton.language.atomic_cas<a class=\"headerlink\" href=\"#triton-language-atomic-cas\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<dl class=\"py function\">\n-<dt class=\"sig sig-object py\" id=\"triton.language.atomic_cas\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">atomic_cas</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">pointer</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">cmp</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">val</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">sem</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.atomic_cas\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Performs an atomic compare-and-swap at the memory location specified by <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer</span></code>.</p>\n-<p>Return the data stored at <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer</span></code> before the atomic operation.</p>\n-<dl class=\"field-list simple\">\n-<dt class=\"field-odd\">Parameters<span class=\"colon\">:</span></dt>\n-<dd class=\"field-odd\"><ul class=\"simple\">\n-<li><p><strong>pointer</strong> (<em>Block</em><em> of </em><em>dtype=triton.PointerDType</em>) \u2013 The memory locations to compare-and-swap.</p></li>\n-<li><p><strong>cmp</strong> (<em>Block</em><em> of </em><em>dtype=`pointer.dtype.element_ty`</em>) \u2013 The values expected to be found in the atomic object</p></li>\n-<li><p><strong>val</strong> (<em>Block</em><em> of </em><em>dtype=`pointer.dtype.element_ty`</em>) \u2013 The values to copy in case the expected value matches the contained value.</p></li>\n-</ul>\n-</dd>\n-</dl>\n-</dd></dl>\n-\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"triton.language.store.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.store\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"triton.language.atomic_xchg.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.atomic_xchg\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.atomic_cas.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/python-api/generated/triton.language.atomic_max.html", "status": "removed", "additions": 0, "deletions": 180, "changes": 180, "file_content_changes": "@@ -1,180 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>triton.language.atomic_max &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton.language.atomic_min\" href=\"triton.language.atomic_min.html\" />\n-    <link rel=\"prev\" title=\"triton.language.atomic_add\" href=\"triton.language.atomic_add.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/installation.html\">Installation</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/tutorials/index.html\">Tutorials</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.html\">triton</a></li>\n-<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"../triton.language.html\">triton.language</a><ul class=\"current\">\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#programming-model\">Programming Model</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#creation-ops\">Creation Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#shape-manipulation-ops\">Shape Manipulation Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#linear-algebra-ops\">Linear Algebra Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#reduction-ops\">Reduction Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#scan-ops\">Scan Ops</a></li>\n-<li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#atomic-ops\">Atomic Ops</a><ul class=\"current\">\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.atomic_cas.html\">triton.language.atomic_cas</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.atomic_add.html\">triton.language.atomic_add</a></li>\n-<li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.atomic_max</a><ul>\n-<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#triton.language.atomic_max\"><code class=\"docutils literal notranslate\"><span class=\"pre\">atomic_max()</span></code></a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.atomic_min.html\">triton.language.atomic_min</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#comparison-ops\">Comparison ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#random-number-generation\">Random Number Generation</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#compiler-hint-ops\">Compiler Hint Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#debug-ops\">Debug Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#iterators\">Iterators</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-          <li class=\"breadcrumb-item\"><a href=\"../triton.language.html\">triton.language</a></li>\n-      <li class=\"breadcrumb-item active\">triton.language.atomic_max</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/python-api/generated/triton.language.atomic_max.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <section id=\"triton-language-atomic-max\">\n-<h1>triton.language.atomic_max<a class=\"headerlink\" href=\"#triton-language-atomic-max\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<dl class=\"py function\">\n-<dt class=\"sig sig-object py\" id=\"triton.language.atomic_max\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">atomic_max</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">pointer</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">val</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">mask</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">sem</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.atomic_max\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Performs an atomic max at the memory location specified by <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer</span></code>.</p>\n-<p>Return the data stored at <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer</span></code> before the atomic operation.</p>\n-<dl class=\"field-list simple\">\n-<dt class=\"field-odd\">Parameters<span class=\"colon\">:</span></dt>\n-<dd class=\"field-odd\"><ul class=\"simple\">\n-<li><p><strong>pointer</strong> (<em>Block</em><em> of </em><em>dtype=triton.PointerDType</em>) \u2013 The memory locations to compare-and-swap.</p></li>\n-<li><p><strong>cmp</strong> (<em>Block</em><em> of </em><em>dtype=`pointer.dtype.element_ty`</em>) \u2013 The values expected to be found in the atomic object</p></li>\n-<li><p><strong>val</strong> (<em>Block</em><em> of </em><em>dtype=`pointer.dtype.element_ty`</em>) \u2013 The values to copy in case the expected value matches the contained value.</p></li>\n-</ul>\n-</dd>\n-</dl>\n-</dd></dl>\n-\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"triton.language.atomic_add.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.atomic_add\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"triton.language.atomic_min.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.atomic_min\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.atomic_max.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/python-api/generated/triton.language.atomic_min.html", "status": "removed", "additions": 0, "deletions": 180, "changes": 180, "file_content_changes": "@@ -1,180 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>triton.language.atomic_min &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton.language.minimum\" href=\"triton.language.minimum.html\" />\n-    <link rel=\"prev\" title=\"triton.language.atomic_max\" href=\"triton.language.atomic_max.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/installation.html\">Installation</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/tutorials/index.html\">Tutorials</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.html\">triton</a></li>\n-<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"../triton.language.html\">triton.language</a><ul class=\"current\">\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#programming-model\">Programming Model</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#creation-ops\">Creation Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#shape-manipulation-ops\">Shape Manipulation Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#linear-algebra-ops\">Linear Algebra Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#reduction-ops\">Reduction Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#scan-ops\">Scan Ops</a></li>\n-<li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#atomic-ops\">Atomic Ops</a><ul class=\"current\">\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.atomic_cas.html\">triton.language.atomic_cas</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.atomic_add.html\">triton.language.atomic_add</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.atomic_max.html\">triton.language.atomic_max</a></li>\n-<li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.atomic_min</a><ul>\n-<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#triton.language.atomic_min\"><code class=\"docutils literal notranslate\"><span class=\"pre\">atomic_min()</span></code></a></li>\n-</ul>\n-</li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#comparison-ops\">Comparison ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#random-number-generation\">Random Number Generation</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#compiler-hint-ops\">Compiler Hint Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#debug-ops\">Debug Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#iterators\">Iterators</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-          <li class=\"breadcrumb-item\"><a href=\"../triton.language.html\">triton.language</a></li>\n-      <li class=\"breadcrumb-item active\">triton.language.atomic_min</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/python-api/generated/triton.language.atomic_min.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <section id=\"triton-language-atomic-min\">\n-<h1>triton.language.atomic_min<a class=\"headerlink\" href=\"#triton-language-atomic-min\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<dl class=\"py function\">\n-<dt class=\"sig sig-object py\" id=\"triton.language.atomic_min\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">atomic_min</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">pointer</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">val</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">mask</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">sem</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.atomic_min\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Performs an atomic min at the memory location specified by <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer</span></code>.</p>\n-<p>Return the data stored at <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer</span></code> before the atomic operation.</p>\n-<dl class=\"field-list simple\">\n-<dt class=\"field-odd\">Parameters<span class=\"colon\">:</span></dt>\n-<dd class=\"field-odd\"><ul class=\"simple\">\n-<li><p><strong>pointer</strong> (<em>Block</em><em> of </em><em>dtype=triton.PointerDType</em>) \u2013 The memory locations to compare-and-swap.</p></li>\n-<li><p><strong>cmp</strong> (<em>Block</em><em> of </em><em>dtype=`pointer.dtype.element_ty`</em>) \u2013 The values expected to be found in the atomic object</p></li>\n-<li><p><strong>val</strong> (<em>Block</em><em> of </em><em>dtype=`pointer.dtype.element_ty`</em>) \u2013 The values to copy in case the expected value matches the contained value.</p></li>\n-</ul>\n-</dd>\n-</dl>\n-</dd></dl>\n-\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"triton.language.atomic_max.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.atomic_max\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"triton.language.minimum.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.minimum\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.atomic_min.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/python-api/generated/triton.language.atomic_xchg.html", "status": "removed", "additions": 0, "deletions": 180, "changes": 180, "file_content_changes": "@@ -1,180 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>triton.language.atomic_xchg &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton.language.where\" href=\"triton.language.where.html\" />\n-    <link rel=\"prev\" title=\"triton.language.atomic_cas\" href=\"triton.language.atomic_cas.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/installation.html\">Installation</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/tutorials/index.html\">Tutorials</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.html\">triton</a></li>\n-<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"../triton.language.html\">triton.language</a><ul class=\"current\">\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#programming-model\">Programming Model</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#creation-ops\">Creation Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#shape-manipulation-ops\">Shape Manipulation Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#linear-algebra-ops\">Linear Algebra Ops</a></li>\n-<li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a><ul class=\"current\">\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.load.html\">triton.language.load</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.store.html\">triton.language.store</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.atomic_cas.html\">triton.language.atomic_cas</a></li>\n-<li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.atomic_xchg</a><ul>\n-<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#triton.language.atomic_xchg\"><code class=\"docutils literal notranslate\"><span class=\"pre\">atomic_xchg()</span></code></a></li>\n-</ul>\n-</li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#reduction-ops\">Reduction Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#scan-ops\">Scan Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#atomic-ops\">Atomic Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#comparison-ops\">Comparison ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#random-number-generation\">Random Number Generation</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#compiler-hint-ops\">Compiler Hint Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#debug-ops\">Debug Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#iterators\">Iterators</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-          <li class=\"breadcrumb-item\"><a href=\"../triton.language.html\">triton.language</a></li>\n-      <li class=\"breadcrumb-item active\">triton.language.atomic_xchg</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/python-api/generated/triton.language.atomic_xchg.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <section id=\"triton-language-atomic-xchg\">\n-<h1>triton.language.atomic_xchg<a class=\"headerlink\" href=\"#triton-language-atomic-xchg\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<dl class=\"py function\">\n-<dt class=\"sig sig-object py\" id=\"triton.language.atomic_xchg\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">atomic_xchg</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">pointer</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">val</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">mask</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">sem</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.atomic_xchg\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Performs an atomic exchange at the memory location specified by <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer</span></code>.</p>\n-<p>Return the data stored at <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer</span></code> before the atomic operation.</p>\n-<dl class=\"field-list simple\">\n-<dt class=\"field-odd\">Parameters<span class=\"colon\">:</span></dt>\n-<dd class=\"field-odd\"><ul class=\"simple\">\n-<li><p><strong>pointer</strong> (<em>Block</em><em> of </em><em>dtype=triton.PointerDType</em>) \u2013 The memory locations to compare-and-swap.</p></li>\n-<li><p><strong>cmp</strong> (<em>Block</em><em> of </em><em>dtype=`pointer.dtype.element_ty`</em>) \u2013 The values expected to be found in the atomic object</p></li>\n-<li><p><strong>val</strong> (<em>Block</em><em> of </em><em>dtype=`pointer.dtype.element_ty`</em>) \u2013 The values to copy in case the expected value matches the contained value.</p></li>\n-</ul>\n-</dd>\n-</dl>\n-</dd></dl>\n-\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"triton.language.atomic_cas.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.atomic_cas\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"triton.language.where.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.where\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.atomic_xchg.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/python-api/generated/triton.language.broadcast.html", "status": "removed", "additions": 0, "deletions": 181, "changes": 181, "file_content_changes": "@@ -1,181 +0,0 @@\n-<!DOCTYPE html>\n-<html class=\"writer-html5\" lang=\"en\" >\n-<head>\n-  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n-\n-  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n-  <title>triton.language.broadcast &mdash; Triton  documentation</title>\n-      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n-      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n-  <!--[if lt IE 9]>\n-    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n-  <![endif]-->\n-  \n-        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n-        <script src=\"../../_static/doctools.js\"></script>\n-        <script src=\"../../_static/sphinx_highlight.js\"></script>\n-    <script src=\"../../_static/js/theme.js\"></script>\n-    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n-    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton.language.broadcast_to\" href=\"triton.language.broadcast_to.html\" />\n-    <link rel=\"prev\" title=\"triton.language.zeros\" href=\"triton.language.zeros.html\" /> \n-</head>\n-\n-<body class=\"wy-body-for-nav\"> \n-  <div class=\"wy-grid-for-nav\">\n-    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n-      <div class=\"wy-side-scroll\">\n-        <div class=\"wy-side-nav-search\" >\n-\n-          \n-          \n-          <a href=\"../../index.html\" class=\"icon icon-home\">\n-            Triton\n-          </a>\n-<div role=\"search\">\n-  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n-    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n-    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n-    <input type=\"hidden\" name=\"area\" value=\"default\" />\n-  </form>\n-</div>\n-        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n-              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/installation.html\">Installation</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../getting-started/tutorials/index.html\">Tutorials</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n-<ul class=\"current\">\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.html\">triton</a></li>\n-<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"../triton.language.html\">triton.language</a><ul class=\"current\">\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#programming-model\">Programming Model</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#creation-ops\">Creation Ops</a></li>\n-<li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#shape-manipulation-ops\">Shape Manipulation Ops</a><ul class=\"current\">\n-<li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.broadcast</a><ul>\n-<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#triton.language.broadcast\"><code class=\"docutils literal notranslate\"><span class=\"pre\">broadcast()</span></code></a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.broadcast_to.html\">triton.language.broadcast_to</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.expand_dims.html\">triton.language.expand_dims</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.ravel.html\">triton.language.ravel</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.reshape.html\">triton.language.reshape</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.trans.html\">triton.language.trans</a></li>\n-<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.view.html\">triton.language.view</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#linear-algebra-ops\">Linear Algebra Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#reduction-ops\">Reduction Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#scan-ops\">Scan Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#atomic-ops\">Atomic Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#comparison-ops\">Comparison ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#random-number-generation\">Random Number Generation</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#compiler-hint-ops\">Compiler Hint Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#debug-ops\">Debug Ops</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#iterators\">Iterators</a></li>\n-</ul>\n-</li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../triton.testing.html\">triton.testing</a></li>\n-</ul>\n-<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n-<ul>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n-<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n-</ul>\n-\n-        </div>\n-      </div>\n-    </nav>\n-\n-    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n-          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n-          <a href=\"../../index.html\">Triton</a>\n-      </nav>\n-\n-      <div class=\"wy-nav-content\">\n-        <div class=\"rst-content\">\n-          <div role=\"navigation\" aria-label=\"Page navigation\">\n-  <ul class=\"wy-breadcrumbs\">\n-      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n-          <li class=\"breadcrumb-item\"><a href=\"../triton.language.html\">triton.language</a></li>\n-      <li class=\"breadcrumb-item active\">triton.language.broadcast</li>\n-      <li class=\"wy-breadcrumbs-aside\">\n-            <a href=\"../../_sources/python-api/generated/triton.language.broadcast.rst.txt\" rel=\"nofollow\"> View page source</a>\n-      </li>\n-  </ul>\n-  <hr/>\n-</div>\n-          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n-           <div itemprop=\"articleBody\">\n-             \n-  <section id=\"triton-language-broadcast\">\n-<h1>triton.language.broadcast<a class=\"headerlink\" href=\"#triton-language-broadcast\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<dl class=\"py function\">\n-<dt class=\"sig sig-object py\" id=\"triton.language.broadcast\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">broadcast</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">input</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">other</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.broadcast\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Tries to broadcast the two given blocks to a common compatible shape.</p>\n-<dl class=\"field-list simple\">\n-<dt class=\"field-odd\">Parameters<span class=\"colon\">:</span></dt>\n-<dd class=\"field-odd\"><ul class=\"simple\">\n-<li><p><strong>input</strong> (<em>Block</em>) \u2013 The first input tensor.</p></li>\n-<li><p><strong>other</strong> (<em>Block</em>) \u2013 The second input tensor.</p></li>\n-</ul>\n-</dd>\n-</dl>\n-</dd></dl>\n-\n-</section>\n-\n-\n-           </div>\n-          </div>\n-          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"triton.language.zeros.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.zeros\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"triton.language.broadcast_to.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.broadcast_to\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-    </div>\n-\n-  <hr/>\n-\n-  <div role=\"contentinfo\">\n-    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n-  </div>\n-\n-  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n-    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n-    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n-   \n-\n-</footer>\n-        </div>\n-      </div>\n-    </section>\n-  </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.broadcast.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n-      jQuery(function () {\n-          SphinxRtdTheme.Navigation.enable(true);\n-      });\n-  </script> \n-\n-</body>\n-</html>\n\\ No newline at end of file"}, {"filename": "main/python-api/generated/triton.language.broadcast_to.html", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.cat.html", "status": "removed", "additions": 0, "deletions": 182, "changes": 182, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.cos.html", "status": "removed", "additions": 0, "deletions": 181, "changes": 181, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.cumprod.html", "status": "removed", "additions": 0, "deletions": 177, "changes": 177, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.cumsum.html", "status": "removed", "additions": 0, "deletions": 177, "changes": 177, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.debug_barrier.html", "status": "removed", "additions": 0, "deletions": 170, "changes": 170, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.device_assert.html", "status": "removed", "additions": 0, "deletions": 186, "changes": 186, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.device_print.html", "status": "removed", "additions": 0, "deletions": 186, "changes": 186, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.dot.html", "status": "removed", "additions": 0, "deletions": 176, "changes": 176, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.exp.html", "status": "removed", "additions": 0, "deletions": 181, "changes": 181, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.expand_dims.html", "status": "removed", "additions": 0, "deletions": 183, "changes": 183, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.fdiv.html", "status": "removed", "additions": 0, "deletions": 175, "changes": 175, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.full.html", "status": "removed", "additions": 0, "deletions": 181, "changes": 181, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.load.html", "status": "removed", "additions": 0, "deletions": 200, "changes": 200, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.log.html", "status": "removed", "additions": 0, "deletions": 181, "changes": 181, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.max.html", "status": "removed", "additions": 0, "deletions": 183, "changes": 183, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.max_constancy.html", "status": "removed", "additions": 0, "deletions": 172, "changes": 172, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.max_contiguous.html", "status": "removed", "additions": 0, "deletions": 170, "changes": 170, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.maximum.html", "status": "removed", "additions": 0, "deletions": 176, "changes": 176, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.min.html", "status": "removed", "additions": 0, "deletions": 183, "changes": 183, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.minimum.html", "status": "removed", "additions": 0, "deletions": 176, "changes": 176, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.multiple_of.html", "status": "removed", "additions": 0, "deletions": 170, "changes": 170, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.num_programs.html", "status": "removed", "additions": 0, "deletions": 173, "changes": 173, "file_content_changes": "N/A"}, {"filename": "main/python-api/generated/triton.language.program_id.html", "status": "removed", "additions": 0, "deletions": 173, "changes": 173, "file_content_changes": "N/A"}]