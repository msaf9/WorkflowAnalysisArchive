[{"filename": "main/.buildinfo", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n # Sphinx build info version 1\n # This file hashes the configuration used when building these files. When it is not found, a full rebuild will be done.\n-config: c07d4c592e73a6ccb5526b4e3c30b2ad\n+config: 72888c96ebe6ada997a70746f5887f19\n tags: 645f666f9bcd5a90fca523b33c5a78b7"}, {"filename": "main/.doctrees/environment.pickle", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/installation.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/01-vector-add.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/02-fused-softmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/03-matrix-multiplication.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/04-low-memory-dropout.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/05-layer-norm.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/06-fused-attention.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/07-math-functions.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/08-experimental-block-pointer.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/index.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/sg_execution_times.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/index.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/programming-guide/chapter-1/introduction.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/programming-guide/chapter-2/related-work.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.Config.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.autotune.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.heuristics.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.jit.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.abs.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.arange.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.argmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.argmin.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.associative_scan.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_add.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_cas.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_max.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_min.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_xchg.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.broadcast.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.broadcast_to.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cat.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cos.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cumprod.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cumsum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.debug_barrier.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.device_assert.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.device_print.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.dot.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.exp.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.expand_dims.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.fdiv.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.full.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.load.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.log.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.max.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.max_constancy.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.max_contiguous.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.maximum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.min.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.minimum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.multiple_of.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.num_programs.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.program_id.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.rand.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randint.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randint4x.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randn.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.ravel.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.reduce.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.reshape.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sigmoid.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sin.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.softmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sqrt.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.static_assert.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.static_print.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.static_range.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.store.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.trans.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.umulhi.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.view.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.where.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.xor_sum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.zeros.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.Benchmark.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.do_bench.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.perf_report.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.language.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.testing.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/662999063954282841dc90b8945f85ce/tutorials_jupyter.zip", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/763344228ae6bc253ed1a6cf586aa30d/tutorials_python.zip", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_01-vector-add_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_01-vector-add_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_02-fused-softmax_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_02-fused-softmax_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_03-matrix-multiplication_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_03-matrix-multiplication_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_05-layer-norm_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_05-layer-norm_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_002.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_003.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_004.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_sources/getting-started/tutorials/01-vector-add.rst.txt", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -236,22 +236,22 @@ We can now run the decorated function above. Pass `print_data=True` to see the p\n \n     vector-add-performance:\n                size       Triton        Torch\n-    0        4096.0     8.000000     8.000000\n-    1        8192.0    15.999999    15.999999\n+    0        4096.0     9.600000     8.000000\n+    1        8192.0    19.200000    15.999999\n     2       16384.0    31.999999    31.999999\n-    3       32768.0    54.857142    54.857142\n-    4       65536.0   109.714284   109.714284\n+    3       32768.0    63.999998    63.999998\n+    4       65536.0   127.999995   127.999995\n     5      131072.0   219.428568   219.428568\n     6      262144.0   384.000001   384.000001\n-    7      524288.0   614.400016   558.545450\n+    7      524288.0   614.400016   614.400016\n     8     1048576.0   819.200021   819.200021\n-    9     2097152.0  1023.999964  1023.999964\n-    10    4194304.0  1228.800031  1228.800031\n+    9     2097152.0  1068.521715  1023.999964\n+    10    4194304.0  1228.800031  1260.307736\n     11    8388608.0  1424.695621  1424.695621\n     12   16777216.0  1560.380965  1560.380965\n-    13   33554432.0  1631.601649  1631.601649\n-    14   67108864.0  1680.410210  1673.259578\n-    15  134217728.0  1698.557221  1693.072171\n+    13   33554432.0  1624.859540  1624.859540\n+    14   67108864.0  1669.706983  1666.169441\n+    15  134217728.0  1684.008546  1680.410210\n \n \n "}, {"filename": "main/_sources/getting-started/tutorials/02-fused-softmax.rst.txt", "status": "modified", "additions": 11, "deletions": 11, "changes": 22, "file_content_changes": "@@ -286,17 +286,17 @@ We will then compare its performance against (1) :code:`torch.softmax` and (2) t\n \n     softmax-performance:\n               N       Triton  Torch (native)  Torch (jit)\n-    0     256.0   630.153853      682.666643   248.242422\n-    1     384.0   819.200021      819.200021   307.200008\n-    2     512.0   910.222190      910.222190   348.595735\n-    3     640.0   975.238103      890.434763   379.259259\n-    4     768.0  1068.521715      983.040025   402.885254\n+    0     256.0   682.666643      682.666643   273.066674\n+    1     384.0   877.714274      819.200021   332.108094\n+    2     512.0   910.222190      963.764689   372.363633\n+    3     640.0  1024.000026      975.238103   409.600010\n+    4     768.0  1068.521715     1023.999964   438.857137\n     ..      ...          ...             ...          ...\n-    93  12160.0  1601.316858     1066.082150   580.776137\n-    94  12288.0  1598.438956     1021.340281   580.821292\n-    95  12416.0  1602.064538     1040.083774   578.328973\n-    96  12544.0  1599.235121     1024.000033   580.069387\n-    97  12672.0  1602.782573     1013.760026   579.291449\n+    93  12160.0  1601.316858     1069.010969   590.470399\n+    94  12288.0  1604.963246     1016.061996   592.192778\n+    95  12416.0  1595.630495     1026.645975   586.871514\n+    96  12544.0  1599.235121     1013.656595   589.439056\n+    97  12672.0  1602.782573     1008.716405   588.539906\n \n     [98 rows x 4 columns]\n \n@@ -313,7 +313,7 @@ In the above plot, we can see that:\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  38.427 seconds)\n+   **Total running time of the script:** ( 0 minutes  38.816 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:"}, {"filename": "main/_sources/getting-started/tutorials/03-matrix-multiplication.rst.txt", "status": "modified", "additions": 31, "deletions": 31, "changes": 62, "file_content_changes": "@@ -449,45 +449,45 @@ but feel free to arrange this script as you wish to benchmark any other matrix s\n \n     matmul-performance:\n              M      cuBLAS      Triton\n-    0    256.0    4.096000    3.640889\n-    1    384.0   11.059200   11.059200\n-    2    512.0   23.831273   21.845333\n-    3    640.0   39.384616   36.571428\n-    4    768.0   58.982401   55.296000\n-    5    896.0   73.943582   73.943582\n-    6   1024.0   99.864382   91.180520\n-    7   1152.0  124.415996  119.439363\n-    8   1280.0  146.285712  146.285712\n-    9   1408.0  139.789133  123.903999\n-    10  1536.0  164.602046  144.446699\n-    11  1664.0  166.646518  166.646518\n-    12  1792.0  160.563196  193.783168\n-    13  1920.0  191.999993  157.090908\n-    14  2048.0  186.413508  180.400167\n-    15  2176.0  184.620623  203.269178\n-    16  2304.0  227.503545  225.357284\n-    17  2432.0  202.118452  199.251522\n-    18  2560.0  222.911566  217.006622\n-    19  2688.0  196.544332  197.567993\n-    20  2816.0  208.680416  209.683695\n-    21  2944.0  218.579083  218.579083\n-    22  3072.0  203.680236  206.653671\n+    0    256.0    4.096000    4.096000\n+    1    384.0   12.288000   12.288000\n+    2    512.0   26.214401   23.831273\n+    3    640.0   42.666665   39.384616\n+    4    768.0   68.056616   58.982401\n+    5    896.0   78.051553   82.642822\n+    6   1024.0  104.857603   99.864382\n+    7   1152.0  135.726544  129.825388\n+    8   1280.0  163.840004  163.840004\n+    9   1408.0  155.765024  136.294403\n+    10  1536.0  181.484314  157.286398\n+    11  1664.0  183.651271  179.978245\n+    12  1792.0  172.914215  204.353162\n+    13  1920.0  203.294114  166.554219\n+    14  2048.0  197.379013  184.365008\n+    15  2176.0  188.071477  205.343354\n+    16  2304.0  223.251141  223.251141\n+    17  2432.0  196.464787  196.464787\n+    18  2560.0  219.919464  212.779229\n+    19  2688.0  193.536006  197.567993\n+    20  2816.0  208.680416  210.696652\n+    21  2944.0  221.493479  221.493479\n+    22  3072.0  205.156169  208.941345\n     23  3200.0  213.333323  215.488222\n-    24  3328.0  203.365249  205.689424\n-    25  3456.0  216.143621  214.990846\n-    26  3584.0  216.663602  207.656790\n-    27  3712.0  209.868376  211.199462\n-    28  3840.0  205.561330  207.879708\n-    29  3968.0  207.877238  215.971570\n-    30  4096.0  216.829933  209.715205\n+    24  3328.0  202.222739  203.365249\n+    25  3456.0  213.850319  215.565692\n+    26  3584.0  216.663602  205.286289\n+    27  3712.0  207.686788  213.912940\n+    28  3840.0  206.328356  205.179974\n+    29  3968.0  207.171367  213.328563\n+    30  4096.0  218.952244  206.806979\n \n \n \n \n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  40.955 seconds)\n+   **Total running time of the script:** ( 0 minutes  40.827 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_03-matrix-multiplication.py:"}, {"filename": "main/_sources/getting-started/tutorials/04-low-memory-dropout.rst.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -242,7 +242,7 @@ References\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  0.686 seconds)\n+   **Total running time of the script:** ( 0 minutes  0.649 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_04-low-memory-dropout.py:"}, {"filename": "main/_sources/getting-started/tutorials/05-layer-norm.rst.txt", "status": "modified", "additions": 32, "deletions": 32, "changes": 64, "file_content_changes": "@@ -430,37 +430,37 @@ Specifically, one can set :code:`'mode': 'backward'` to benchmark the backward p\n  .. code-block:: none\n \n     layer-norm-backward:\n-              N      Triton       Torch\n-    0    1024.0  140.434291  356.173905\n-    1    1536.0  215.578939  418.909088\n-    2    2048.0  282.482757  477.203890\n-    3    2560.0  355.144523  512.000013\n-    4    3072.0  426.173427  522.893629\n-    5    3584.0  500.093023  457.531902\n-    6    4096.0  552.269657  461.521112\n-    7    4608.0  601.043465  466.632911\n-    8    5120.0  675.164823  472.615383\n-    9    5632.0  755.128512  479.319160\n-    10   6144.0  797.059472  483.462304\n-    11   6656.0  840.757868  488.513781\n-    12   7168.0  868.848510  466.211399\n-    13   7680.0  903.529428  469.007640\n-    14   8192.0  936.228579  477.203890\n-    15   8704.0  621.714291  479.119264\n-    16   9216.0  646.736816  480.834772\n-    17   9728.0  668.974195  485.388778\n-    18  10240.0  686.480466  486.653462\n-    19  10752.0  712.839796  476.983376\n-    20  11264.0  736.610353  479.319160\n-    21  11776.0  755.679153  484.775287\n-    22  12288.0  774.047263  490.702181\n-    23  12800.0  781.679400  493.890658\n-    24  13312.0  798.720020  494.563489\n-    25  13824.0  795.625894  494.450084\n-    26  14336.0  802.013973  485.280687\n-    27  14848.0  808.054402  488.823031\n-    28  15360.0  822.857132  494.818774\n-    29  15872.0  820.965497  496.646663\n+              N       Triton       Torch\n+    0    1024.0   179.386867  372.363633\n+    1    1536.0   245.759988  438.857146\n+    2    2048.0   369.563926  496.484863\n+    3    2560.0   455.111129  534.260858\n+    4    3072.0   538.160602  542.117638\n+    5    3584.0   623.304334  472.615376\n+    6    4096.0   677.958629  474.898540\n+    7    4608.0   718.129898  480.834772\n+    8    5120.0   772.830175  483.779502\n+    9    5632.0   824.195135  489.739120\n+    10   6144.0   867.388239  494.818794\n+    11   6656.0   907.636357  497.644861\n+    12   7168.0   940.065592  476.542919\n+    13   7680.0   975.238103  478.753257\n+    14   8192.0  1008.246151  486.653476\n+    15   8704.0   676.038845  486.937055\n+    16   9216.0   704.407633  488.264916\n+    17   9728.0   729.600018  493.598316\n+    18  10240.0   749.268305  494.486921\n+    19  10752.0   774.918911  484.142604\n+    20  11264.0   802.183964  486.215841\n+    21  11776.0   819.199982  490.666649\n+    22  12288.0   837.818175  496.484863\n+    23  12800.0   841.643802  499.512174\n+    24  13312.0   858.838688  499.200013\n+    25  13824.0   857.302309  498.911279\n+    26  14336.0   862.315765  490.818842\n+    27  14848.0   864.932052  493.562309\n+    28  15360.0   881.913853  499.512213\n+    29  15872.0   881.777776  501.221037\n \n \n \n@@ -475,7 +475,7 @@ References\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  29.999 seconds)\n+   **Total running time of the script:** ( 0 minutes  29.108 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_05-layer-norm.py:"}, {"filename": "main/_sources/getting-started/tutorials/06-fused-attention.rst.txt", "status": "modified", "additions": 21, "deletions": 21, "changes": 42, "file_content_changes": "@@ -70,32 +70,32 @@ Extra Credits:\n \n     fused-attention-batch4-head48-d64-fwd:\n          N_CTX      Triton\n-    0   1024.0  152.657562\n-    1   2048.0  164.849663\n-    2   4096.0  170.224456\n-    3   8192.0  172.505255\n-    4  16384.0  173.534037\n+    0   1024.0  156.482198\n+    1   2048.0  163.179966\n+    2   4096.0  168.268211\n+    3   8192.0  171.763876\n+    4  16384.0  173.555075\n     fused-attention-batch4-head48-d64-fwd:\n          N_CTX      Triton\n-    0   1024.0  112.929286\n-    1   2048.0  137.056634\n-    2   4096.0  148.889747\n-    3   8192.0  158.030599\n-    4  16384.0  160.130507\n+    0   1024.0  118.318380\n+    1   2048.0  135.642941\n+    2   4096.0  146.344205\n+    3   8192.0  156.282511\n+    4  16384.0  156.282920\n     fused-attention-batch4-head48-d64-bwd:\n          N_CTX     Triton\n-    0   1024.0  74.523399\n-    1   2048.0  85.481737\n-    2   4096.0  91.486093\n-    3   8192.0  94.367787\n-    4  16384.0  95.571148\n+    0   1024.0  77.358365\n+    1   2048.0  86.589881\n+    2   4096.0  91.734123\n+    3   8192.0  95.348793\n+    4  16384.0  95.981783\n     fused-attention-batch4-head48-d64-bwd:\n          N_CTX     Triton\n-    0   1024.0  51.565022\n-    1   2048.0  65.178841\n-    2   4096.0  74.390648\n-    3   8192.0  79.961311\n-    4  16384.0  82.900346\n+    0   1024.0  54.072243\n+    1   2048.0  67.736735\n+    2   4096.0  76.251815\n+    3   8192.0  81.848398\n+    4  16384.0  84.150805\n \n \n \n@@ -508,7 +508,7 @@ Extra Credits:\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  14.355 seconds)\n+   **Total running time of the script:** ( 0 minutes  14.247 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_06-fused-attention.py:"}, {"filename": "main/_sources/getting-started/tutorials/07-math-functions.rst.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -147,7 +147,7 @@ We can also customize the libdevice library path by passing the path to the `lib\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  0.239 seconds)\n+   **Total running time of the script:** ( 0 minutes  0.223 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_07-math-functions.py:"}, {"filename": "main/_sources/getting-started/tutorials/08-experimental-block-pointer.rst.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -298,7 +298,7 @@ Still we can test our matrix multiplication with block pointers against a native\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  6.971 seconds)\n+   **Total running time of the script:** ( 0 minutes  6.405 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_08-experimental-block-pointer.py:"}, {"filename": "main/_sources/getting-started/tutorials/sg_execution_times.rst.txt", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -6,22 +6,22 @@\n \n Computation times\n =================\n-**02:17.323** total execution time for **getting-started_tutorials** files:\n+**02:15.968** total execution time for **getting-started_tutorials** files:\n \n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``)           | 00:40.955 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``)           | 00:40.827 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                           | 00:38.427 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                           | 00:38.816 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                                 | 00:29.999 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                                 | 00:29.108 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py` (``06-fused-attention.py``)                       | 00:14.355 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py` (``06-fused-attention.py``)                       | 00:14.247 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_08-experimental-block-pointer.py` (``08-experimental-block-pointer.py``) | 00:06.971 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_08-experimental-block-pointer.py` (``08-experimental-block-pointer.py``) | 00:06.405 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n | :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                                 | 00:05.691 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)                 | 00:00.686 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)                 | 00:00.649 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_07-math-functions.py` (``07-math-functions.py``)                         | 00:00.239 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_07-math-functions.py` (``07-math-functions.py``)                         | 00:00.223 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+"}, {"filename": "main/getting-started/tutorials/01-vector-add.html", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -235,22 +235,22 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n </div>\n <img src=\"../../_images/sphx_glr_01-vector-add_001.png\" srcset=\"../../_images/sphx_glr_01-vector-add_001.png\" alt=\"01 vector add\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>vector-add-performance:\n            size       Triton        Torch\n-0        4096.0     8.000000     8.000000\n-1        8192.0    15.999999    15.999999\n+0        4096.0     9.600000     8.000000\n+1        8192.0    19.200000    15.999999\n 2       16384.0    31.999999    31.999999\n-3       32768.0    54.857142    54.857142\n-4       65536.0   109.714284   109.714284\n+3       32768.0    63.999998    63.999998\n+4       65536.0   127.999995   127.999995\n 5      131072.0   219.428568   219.428568\n 6      262144.0   384.000001   384.000001\n-7      524288.0   614.400016   558.545450\n+7      524288.0   614.400016   614.400016\n 8     1048576.0   819.200021   819.200021\n-9     2097152.0  1023.999964  1023.999964\n-10    4194304.0  1228.800031  1228.800031\n+9     2097152.0  1068.521715  1023.999964\n+10    4194304.0  1228.800031  1260.307736\n 11    8388608.0  1424.695621  1424.695621\n 12   16777216.0  1560.380965  1560.380965\n-13   33554432.0  1631.601649  1631.601649\n-14   67108864.0  1680.410210  1673.259578\n-15  134217728.0  1698.557221  1693.072171\n+13   33554432.0  1624.859540  1624.859540\n+14   67108864.0  1669.706983  1666.169441\n+15  134217728.0  1684.008546  1680.410210\n </pre></div>\n </div>\n <p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  5.691 seconds)</p>"}, {"filename": "main/getting-started/tutorials/02-fused-softmax.html", "status": "modified", "additions": 11, "deletions": 11, "changes": 22, "file_content_changes": "@@ -282,17 +282,17 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n </div>\n <img src=\"../../_images/sphx_glr_02-fused-softmax_001.png\" srcset=\"../../_images/sphx_glr_02-fused-softmax_001.png\" alt=\"02 fused softmax\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>softmax-performance:\n           N       Triton  Torch (native)  Torch (jit)\n-0     256.0   630.153853      682.666643   248.242422\n-1     384.0   819.200021      819.200021   307.200008\n-2     512.0   910.222190      910.222190   348.595735\n-3     640.0   975.238103      890.434763   379.259259\n-4     768.0  1068.521715      983.040025   402.885254\n+0     256.0   682.666643      682.666643   273.066674\n+1     384.0   877.714274      819.200021   332.108094\n+2     512.0   910.222190      963.764689   372.363633\n+3     640.0  1024.000026      975.238103   409.600010\n+4     768.0  1068.521715     1023.999964   438.857137\n ..      ...          ...             ...          ...\n-93  12160.0  1601.316858     1066.082150   580.776137\n-94  12288.0  1598.438956     1021.340281   580.821292\n-95  12416.0  1602.064538     1040.083774   578.328973\n-96  12544.0  1599.235121     1024.000033   580.069387\n-97  12672.0  1602.782573     1013.760026   579.291449\n+93  12160.0  1601.316858     1069.010969   590.470399\n+94  12288.0  1604.963246     1016.061996   592.192778\n+95  12416.0  1595.630495     1026.645975   586.871514\n+96  12544.0  1599.235121     1013.656595   589.439056\n+97  12672.0  1602.782573     1008.716405   588.539906\n \n [98 rows x 4 columns]\n </pre></div>\n@@ -305,7 +305,7 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n </ul>\n </dd>\n </dl>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  38.427 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  38.816 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-02-fused-softmax-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d91442ac2982c4e0cc3ab0f43534afbc/02-fused-softmax.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">02-fused-softmax.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/03-matrix-multiplication.html", "status": "modified", "additions": 31, "deletions": 31, "changes": 62, "file_content_changes": "@@ -463,40 +463,40 @@ <h3>Square Matrix Performance<a class=\"headerlink\" href=\"#square-matrix-performa\n </div>\n <img src=\"../../_images/sphx_glr_03-matrix-multiplication_001.png\" srcset=\"../../_images/sphx_glr_03-matrix-multiplication_001.png\" alt=\"03 matrix multiplication\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>matmul-performance:\n          M      cuBLAS      Triton\n-0    256.0    4.096000    3.640889\n-1    384.0   11.059200   11.059200\n-2    512.0   23.831273   21.845333\n-3    640.0   39.384616   36.571428\n-4    768.0   58.982401   55.296000\n-5    896.0   73.943582   73.943582\n-6   1024.0   99.864382   91.180520\n-7   1152.0  124.415996  119.439363\n-8   1280.0  146.285712  146.285712\n-9   1408.0  139.789133  123.903999\n-10  1536.0  164.602046  144.446699\n-11  1664.0  166.646518  166.646518\n-12  1792.0  160.563196  193.783168\n-13  1920.0  191.999993  157.090908\n-14  2048.0  186.413508  180.400167\n-15  2176.0  184.620623  203.269178\n-16  2304.0  227.503545  225.357284\n-17  2432.0  202.118452  199.251522\n-18  2560.0  222.911566  217.006622\n-19  2688.0  196.544332  197.567993\n-20  2816.0  208.680416  209.683695\n-21  2944.0  218.579083  218.579083\n-22  3072.0  203.680236  206.653671\n+0    256.0    4.096000    4.096000\n+1    384.0   12.288000   12.288000\n+2    512.0   26.214401   23.831273\n+3    640.0   42.666665   39.384616\n+4    768.0   68.056616   58.982401\n+5    896.0   78.051553   82.642822\n+6   1024.0  104.857603   99.864382\n+7   1152.0  135.726544  129.825388\n+8   1280.0  163.840004  163.840004\n+9   1408.0  155.765024  136.294403\n+10  1536.0  181.484314  157.286398\n+11  1664.0  183.651271  179.978245\n+12  1792.0  172.914215  204.353162\n+13  1920.0  203.294114  166.554219\n+14  2048.0  197.379013  184.365008\n+15  2176.0  188.071477  205.343354\n+16  2304.0  223.251141  223.251141\n+17  2432.0  196.464787  196.464787\n+18  2560.0  219.919464  212.779229\n+19  2688.0  193.536006  197.567993\n+20  2816.0  208.680416  210.696652\n+21  2944.0  221.493479  221.493479\n+22  3072.0  205.156169  208.941345\n 23  3200.0  213.333323  215.488222\n-24  3328.0  203.365249  205.689424\n-25  3456.0  216.143621  214.990846\n-26  3584.0  216.663602  207.656790\n-27  3712.0  209.868376  211.199462\n-28  3840.0  205.561330  207.879708\n-29  3968.0  207.877238  215.971570\n-30  4096.0  216.829933  209.715205\n+24  3328.0  202.222739  203.365249\n+25  3456.0  213.850319  215.565692\n+26  3584.0  216.663602  205.286289\n+27  3712.0  207.686788  213.912940\n+28  3840.0  206.328356  205.179974\n+29  3968.0  207.171367  213.328563\n+30  4096.0  218.952244  206.806979\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  40.955 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  40.827 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-03-matrix-multiplication-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d5fee5b55a64e47f1b5724ec39adf171/03-matrix-multiplication.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">03-matrix-multiplication.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/04-low-memory-dropout.html", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -287,7 +287,7 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n <p>Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \u201cDropout: A Simple Way to Prevent Neural Networks from Overfitting\u201d, JMLR 2014</p>\n </div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.686 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.649 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-04-low-memory-dropout-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/c9aed78977a4c05741d675a38dde3d7d/04-low-memory-dropout.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">04-low-memory-dropout.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/05-layer-norm.html", "status": "modified", "additions": 32, "deletions": 32, "changes": 64, "file_content_changes": "@@ -459,37 +459,37 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n </pre></div>\n </div>\n <img src=\"../../_images/sphx_glr_05-layer-norm_001.png\" srcset=\"../../_images/sphx_glr_05-layer-norm_001.png\" alt=\"05 layer norm\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>layer-norm-backward:\n-          N      Triton       Torch\n-0    1024.0  140.434291  356.173905\n-1    1536.0  215.578939  418.909088\n-2    2048.0  282.482757  477.203890\n-3    2560.0  355.144523  512.000013\n-4    3072.0  426.173427  522.893629\n-5    3584.0  500.093023  457.531902\n-6    4096.0  552.269657  461.521112\n-7    4608.0  601.043465  466.632911\n-8    5120.0  675.164823  472.615383\n-9    5632.0  755.128512  479.319160\n-10   6144.0  797.059472  483.462304\n-11   6656.0  840.757868  488.513781\n-12   7168.0  868.848510  466.211399\n-13   7680.0  903.529428  469.007640\n-14   8192.0  936.228579  477.203890\n-15   8704.0  621.714291  479.119264\n-16   9216.0  646.736816  480.834772\n-17   9728.0  668.974195  485.388778\n-18  10240.0  686.480466  486.653462\n-19  10752.0  712.839796  476.983376\n-20  11264.0  736.610353  479.319160\n-21  11776.0  755.679153  484.775287\n-22  12288.0  774.047263  490.702181\n-23  12800.0  781.679400  493.890658\n-24  13312.0  798.720020  494.563489\n-25  13824.0  795.625894  494.450084\n-26  14336.0  802.013973  485.280687\n-27  14848.0  808.054402  488.823031\n-28  15360.0  822.857132  494.818774\n-29  15872.0  820.965497  496.646663\n+          N       Triton       Torch\n+0    1024.0   179.386867  372.363633\n+1    1536.0   245.759988  438.857146\n+2    2048.0   369.563926  496.484863\n+3    2560.0   455.111129  534.260858\n+4    3072.0   538.160602  542.117638\n+5    3584.0   623.304334  472.615376\n+6    4096.0   677.958629  474.898540\n+7    4608.0   718.129898  480.834772\n+8    5120.0   772.830175  483.779502\n+9    5632.0   824.195135  489.739120\n+10   6144.0   867.388239  494.818794\n+11   6656.0   907.636357  497.644861\n+12   7168.0   940.065592  476.542919\n+13   7680.0   975.238103  478.753257\n+14   8192.0  1008.246151  486.653476\n+15   8704.0   676.038845  486.937055\n+16   9216.0   704.407633  488.264916\n+17   9728.0   729.600018  493.598316\n+18  10240.0   749.268305  494.486921\n+19  10752.0   774.918911  484.142604\n+20  11264.0   802.183964  486.215841\n+21  11776.0   819.199982  490.666649\n+22  12288.0   837.818175  496.484863\n+23  12800.0   841.643802  499.512174\n+24  13312.0   858.838688  499.200013\n+25  13824.0   857.302309  498.911279\n+26  14336.0   862.315765  490.818842\n+27  14848.0   864.932052  493.562309\n+28  15360.0   881.913853  499.512213\n+29  15872.0   881.777776  501.221037\n </pre></div>\n </div>\n </section>\n@@ -501,7 +501,7 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n <p>Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \u201cLayer Normalization\u201d, Arxiv 2016</p>\n </div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  29.999 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  29.108 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-05-layer-norm-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/935c0dd0fbeb4b2e69588471cbb2d4b2/05-layer-norm.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">05-layer-norm.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/06-fused-attention.html", "status": "modified", "additions": 21, "deletions": 21, "changes": 42, "file_content_changes": "@@ -117,32 +117,32 @@\n </ul>\n <div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>fused-attention-batch4-head48-d64-fwd:\n      N_CTX      Triton\n-0   1024.0  152.657562\n-1   2048.0  164.849663\n-2   4096.0  170.224456\n-3   8192.0  172.505255\n-4  16384.0  173.534037\n+0   1024.0  156.482198\n+1   2048.0  163.179966\n+2   4096.0  168.268211\n+3   8192.0  171.763876\n+4  16384.0  173.555075\n fused-attention-batch4-head48-d64-fwd:\n      N_CTX      Triton\n-0   1024.0  112.929286\n-1   2048.0  137.056634\n-2   4096.0  148.889747\n-3   8192.0  158.030599\n-4  16384.0  160.130507\n+0   1024.0  118.318380\n+1   2048.0  135.642941\n+2   4096.0  146.344205\n+3   8192.0  156.282511\n+4  16384.0  156.282920\n fused-attention-batch4-head48-d64-bwd:\n      N_CTX     Triton\n-0   1024.0  74.523399\n-1   2048.0  85.481737\n-2   4096.0  91.486093\n-3   8192.0  94.367787\n-4  16384.0  95.571148\n+0   1024.0  77.358365\n+1   2048.0  86.589881\n+2   4096.0  91.734123\n+3   8192.0  95.348793\n+4  16384.0  95.981783\n fused-attention-batch4-head48-d64-bwd:\n      N_CTX     Triton\n-0   1024.0  51.565022\n-1   2048.0  65.178841\n-2   4096.0  74.390648\n-3   8192.0  79.961311\n-4  16384.0  82.900346\n+0   1024.0  54.072243\n+1   2048.0  67.736735\n+2   4096.0  76.251815\n+3   8192.0  81.848398\n+4  16384.0  84.150805\n </pre></div>\n </div>\n <div class=\"line-block\">\n@@ -547,7 +547,7 @@\n <span class=\"n\">bench_flash_attention</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;.&#39;</span><span class=\"p\">,</span> <span class=\"n\">print_data</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  14.355 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  14.247 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-06-fused-attention-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/54a35f6ec55f9746935b9566fb6bb1df/06-fused-attention.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">06-fused-attention.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/07-math-functions.html", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -186,7 +186,7 @@ <h2>Customize the libdevice library path<a class=\"headerlink\" href=\"#customize-t\n The maximum difference between torch and triton is 2.384185791015625e-07\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.239 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.223 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-07-math-functions-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/77571465f7f4bd281d3a847dc2633146/07-math-functions.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">07-math-functions.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/08-experimental-block-pointer.html", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -335,7 +335,7 @@ <h2>Unit Test<a class=\"headerlink\" href=\"#unit-test\" title=\"Permalink to this he\n \u2705 Triton and Torch match\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  6.971 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  6.405 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-08-experimental-block-pointer-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/4d6052117d61c2ca779cd4b75567fee5/08-experimental-block-pointer.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">08-experimental-block-pointer.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/sg_execution_times.html", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -86,39 +86,39 @@\n              \n   <section id=\"computation-times\">\n <span id=\"sphx-glr-getting-started-tutorials-sg-execution-times\"></span><h1>Computation times<a class=\"headerlink\" href=\"#computation-times\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<p><strong>02:17.323</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n+<p><strong>02:15.968</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n <table class=\"docutils align-default\">\n <tbody>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"03-matrix-multiplication.html#sphx-glr-getting-started-tutorials-03-matrix-multiplication-py\"><span class=\"std std-ref\">Matrix Multiplication</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">03-matrix-multiplication.py</span></code>)</p></td>\n-<td><p>00:40.955</p></td>\n+<td><p>00:40.827</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py\"><span class=\"std std-ref\">Fused Softmax</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">02-fused-softmax.py</span></code>)</p></td>\n-<td><p>00:38.427</p></td>\n+<td><p>00:38.816</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"05-layer-norm.html#sphx-glr-getting-started-tutorials-05-layer-norm-py\"><span class=\"std std-ref\">Layer Normalization</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">05-layer-norm.py</span></code>)</p></td>\n-<td><p>00:29.999</p></td>\n+<td><p>00:29.108</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"06-fused-attention.html#sphx-glr-getting-started-tutorials-06-fused-attention-py\"><span class=\"std std-ref\">Fused Attention</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">06-fused-attention.py</span></code>)</p></td>\n-<td><p>00:14.355</p></td>\n+<td><p>00:14.247</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"08-experimental-block-pointer.html#sphx-glr-getting-started-tutorials-08-experimental-block-pointer-py\"><span class=\"std std-ref\">Block Pointer (Experimental)</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">08-experimental-block-pointer.py</span></code>)</p></td>\n-<td><p>00:06.971</p></td>\n+<td><p>00:06.405</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py\"><span class=\"std std-ref\">Vector Addition</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">01-vector-add.py</span></code>)</p></td>\n <td><p>00:05.691</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"04-low-memory-dropout.html#sphx-glr-getting-started-tutorials-04-low-memory-dropout-py\"><span class=\"std std-ref\">Low-Memory Dropout</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">04-low-memory-dropout.py</span></code>)</p></td>\n-<td><p>00:00.686</p></td>\n+<td><p>00:00.649</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"07-math-functions.html#sphx-glr-getting-started-tutorials-07-math-functions-py\"><span class=\"std std-ref\">Libdevice (tl.math) function</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">07-math-functions.py</span></code>)</p></td>\n-<td><p>00:00.239</p></td>\n+<td><p>00:00.223</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n </tbody>"}, {"filename": "main/searchindex.js", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "N/A"}]