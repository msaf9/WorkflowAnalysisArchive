[{"filename": "main/.buildinfo", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n # Sphinx build info version 1\n # This file hashes the configuration used when building these files. When it is not found, a full rebuild will be done.\n-config: 038e8daa3e8a52869b95f56ffd7293f0\n+config: a5ac9b25f5510b3cb8d14293f4d0fbf7\n tags: 645f666f9bcd5a90fca523b33c5a78b7"}, {"filename": "main/.doctrees/environment.pickle", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/installation.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/01-vector-add.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/02-fused-softmax.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/03-matrix-multiplication.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/04-low-memory-dropout.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/05-layer-norm.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/06-fused-attention.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/07-math-functions.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/08-experimental-block-pointer.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/index.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/sg_execution_times.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/index.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/programming-guide/chapter-1/introduction.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/programming-guide/chapter-2/related-work.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.Config.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.autotune.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.heuristics.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.jit.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.abs.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.arange.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.argmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.argmin.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.associative_scan.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_add.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_cas.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_max.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_min.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_xchg.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.broadcast.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.broadcast_to.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cat.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cos.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cumprod.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cumsum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.debug_barrier.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.device_assert.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.device_print.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.dot.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.exp.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.expand_dims.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.fdiv.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.full.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.load.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.log.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.max.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.max_constancy.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.max_contiguous.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.maximum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.min.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.minimum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.multiple_of.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.num_programs.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.program_id.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.rand.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randint.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randint4x.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randn.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.ravel.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.reduce.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.reshape.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sigmoid.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sin.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.softmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sqrt.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.static_assert.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.static_print.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.static_range.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.store.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.trans.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.umulhi.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.view.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.where.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.xor_sum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.zeros.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.Benchmark.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.do_bench.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.perf_report.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.language.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.testing.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/034d953b6214fedce6ea03803c712b89/02-fused-softmax.ipynb", "status": "added", "additions": 161, "deletions": 0, "changes": 161, "file_content_changes": "@@ -0,0 +1,161 @@\n+{\n+  \"cells\": [\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"%matplotlib inline\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"\\n# Fused Softmax\\n\\nIn this tutorial, you will write a fused softmax operation that is significantly faster\\nthan PyTorch's native op for a particular class of matrices: those whose rows can fit in\\nthe GPU's SRAM.\\n\\nIn doing so, you will learn about:\\n\\n* The benefits of kernel fusion for bandwidth-bound operations.\\n\\n* Reduction operators in Triton.\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Motivations\\n\\nCustom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.\\nLet us consider instead the case of a simple (numerically stabilized) softmax operation:\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@torch.jit.script\\ndef naive_softmax(x):\\n    \\\"\\\"\\\"Compute row-wise softmax of X using native pytorch\\n\\n    We subtract the maximum element in order to avoid overflows. Softmax is invariant to\\n    this shift.\\n    \\\"\\\"\\\"\\n    # read  MN elements ; write M  elements\\n    x_max = x.max(dim=1)[0]\\n    # read MN + M elements ; write MN elements\\n    z = x - x_max[:, None]\\n    # read  MN elements ; write MN elements\\n    numerator = torch.exp(z)\\n    # read  MN elements ; write M  elements\\n    denominator = numerator.sum(dim=1)\\n    # read MN + M elements ; write MN elements\\n    ret = numerator / denominator[:, None]\\n    # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements\\n    return ret\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for $x \\\\in R^{M \\\\times N}$\\nrequires reading $5MN + 2M$ elements from DRAM and writing back $3MN + 2M$ elements.\\nThis is obviously wasteful; we'd prefer to have a custom \\\"fused\\\" kernel that only reads\\nX once and does all the necessary computations on-chip.\\nDoing so would require reading and writing back only $MN$ bytes, so we could\\nexpect a theoretical speed-up of ~4x (i.e., $(8MN + 4M) / 2MN$).\\nThe `torch.jit.script` flags aims to perform this kind of \\\"kernel fusion\\\" automatically\\nbut, as we will see later, it is still far from ideal.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Compute Kernel\\n\\nOur softmax kernel works as follows: each program loads a row of the input matrix X,\\nnormalizes it and writes back the result to the output Y.\\n\\nNote that one important limitation of Triton is that each block must have a\\npower-of-two number of elements, so we need to internally \\\"pad\\\" each row and guard the\\nmemory operations properly if we want to handle any possible input shapes:\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"@triton.jit\\ndef softmax_kernel(\\n    output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols,\\n    BLOCK_SIZE: tl.constexpr\\n):\\n    # The rows of the softmax are independent, so we parallelize across those\\n    row_idx = tl.program_id(0)\\n    # The stride represents how much we need to increase the pointer to advance 1 row\\n    row_start_ptr = input_ptr + row_idx * input_row_stride\\n    # The block size is the next power of two greater than n_cols, so we can fit each\\n    # row in a single block\\n    col_offsets = tl.arange(0, BLOCK_SIZE)\\n    input_ptrs = row_start_ptr + col_offsets\\n    # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\\n    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\\n    # Subtract maximum for numerical stability\\n    row_minus_max = row - tl.max(row, axis=0)\\n    # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\\n    numerator = tl.exp(row_minus_max)\\n    denominator = tl.sum(numerator, axis=0)\\n    softmax_output = numerator / denominator\\n    # Write back output to DRAM\\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\\n    output_ptrs = output_row_start_ptr + col_offsets\\n    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"def softmax(x):\\n    n_rows, n_cols = x.shape\\n    # The block size is the smallest power of two greater than the number of columns in `x`\\n    BLOCK_SIZE = triton.next_power_of_2(n_cols)\\n    # Another trick we can use is to ask the compiler to use more threads per row by\\n    # increasing the number of warps (`num_warps`) over which each row is distributed.\\n    # You will see in the next tutorial how to auto-tune this value in a more natural\\n    # way so you don't have to come up with manual heuristics yourself.\\n    num_warps = 4\\n    if BLOCK_SIZE >= 2048:\\n        num_warps = 8\\n    if BLOCK_SIZE >= 4096:\\n        num_warps = 16\\n    # Allocate output\\n    y = torch.empty_like(x)\\n    # Enqueue kernel. The 1D launch grid is simple: we have one kernel instance per row o\\n    # f the input matrix\\n    softmax_kernel[(n_rows,)](\\n        y,\\n        x,\\n        x.stride(0),\\n        y.stride(0),\\n        n_cols,\\n        num_warps=num_warps,\\n        BLOCK_SIZE=BLOCK_SIZE,\\n    )\\n    return y\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Unit Test\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"We make sure that we test our kernel on a matrix with an irregular number of rows and columns.\\nThis will allow us to verify that our padding mechanism works.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"torch.manual_seed(0)\\nx = torch.randn(1823, 781, device='cuda')\\ny_triton = softmax(x)\\ny_torch = torch.softmax(x, axis=1)\\nassert torch.allclose(y_triton, y_torch), (y_triton, y_torch)\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"As expected, the results are identical.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Benchmark\\n\\nHere we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.\\nWe will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"@triton.testing.perf_report(\\n    triton.testing.Benchmark(\\n        x_names=['N'],  # argument names to use as an x-axis for the plot\\n        x_vals=[\\n            128 * i for i in range(2, 100)\\n        ],  # different possible values for `x_name`\\n        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\\n        line_vals=[\\n            'triton',\\n            'torch-native',\\n            'torch-jit',\\n        ],  # possible values for `line_arg``\\n        line_names=[\\n            \\\"Triton\\\",\\n            \\\"Torch (native)\\\",\\n            \\\"Torch (jit)\\\",\\n        ],  # label name for the lines\\n        styles=[('blue', '-'), ('green', '-'), ('green', '--')],  # line styles\\n        ylabel=\\\"GB/s\\\",  # label name for the y-axis\\n        plot_name=\\\"softmax-performance\\\",  # name for the plot. Used also as a file name for saving the plot.\\n        args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\\n    )\\n)\\ndef benchmark(M, N, provider):\\n    x = torch.randn(M, N, device='cuda', dtype=torch.float32)\\n    quantiles = [0.5, 0.2, 0.8]\\n    if provider == 'torch-native':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1), quantiles=quantiles)\\n    if provider == 'triton':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: softmax(x), quantiles=quantiles)\\n    if provider == 'torch-jit':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_softmax(x), quantiles=quantiles)\\n    gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\\n    return gbps(ms), gbps(max_ms), gbps(min_ms)\\n\\n\\nbenchmark.run(show_plots=True, print_data=True)\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"In the above plot, we can see that:\\n - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.\\n - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.\\n   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.\\n\\n\"\n+      ]\n+    }\n+  ],\n+  \"metadata\": {\n+    \"kernelspec\": {\n+      \"display_name\": \"Python 3\",\n+      \"language\": \"python\",\n+      \"name\": \"python3\"\n+    },\n+    \"language_info\": {\n+      \"codemirror_mode\": {\n+        \"name\": \"ipython\",\n+        \"version\": 3\n+      },\n+      \"file_extension\": \".py\",\n+      \"mimetype\": \"text/x-python\",\n+      \"name\": \"python\",\n+      \"nbconvert_exporter\": \"python\",\n+      \"pygments_lexer\": \"ipython3\",\n+      \"version\": \"3.8.10\"\n+    }\n+  },\n+  \"nbformat\": 4,\n+  \"nbformat_minor\": 0\n+}\n\\ No newline at end of file"}, {"filename": "main/_downloads/302f1761fdc89516b532aad33b963ca0/07-math-functions.ipynb", "status": "added", "additions": 97, "deletions": 0, "changes": 97, "file_content_changes": "@@ -0,0 +1,97 @@\n+{\n+  \"cells\": [\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"%matplotlib inline\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"\\n# Libdevice (`tl.math`) function\\nTriton can invoke a custom function from an external library.\\nIn this example, we will use the `libdevice` library (a.k.a `math` in triton) to apply `asin` on a tensor.\\nPlease refer to https://docs.nvidia.com/cuda/libdevice-users-guide/index.html regarding the semantics of all available libdevice functions.\\nIn `triton/language/math.py`, we try to aggregate functions with the same computation but different data types together.\\nFor example, both `__nv_asin` and `__nvasinf` calculate the principal value of the arc sine of the input, but `__nv_asin` operates on `double` and `__nv_asinf` operates on `float`.\\nUsing triton, you can simply call `tl.math.asin`.\\nTriton automatically selects the correct underlying device function to invoke based on input and output types.\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## asin Kernel\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.jit\\ndef asin_kernel(\\n        x_ptr,\\n        y_ptr,\\n        n_elements,\\n        BLOCK_SIZE: tl.constexpr,\\n):\\n    pid = tl.program_id(axis=0)\\n    block_start = pid * BLOCK_SIZE\\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\\n    mask = offsets < n_elements\\n    x = tl.load(x_ptr + offsets, mask=mask)\\n    x = tl.math.asin(x)\\n    tl.store(y_ptr + offsets, x, mask=mask)\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Using the default libdevice library path\\nWe can use the default libdevice library path encoded in `triton/language/math.py`\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"torch.manual_seed(0)\\nsize = 98432\\nx = torch.rand(size, device='cuda')\\noutput_triton = torch.zeros(size, device='cuda')\\noutput_torch = torch.asin(x)\\nassert x.is_cuda and output_triton.is_cuda\\nn_elements = output_torch.numel()\\ngrid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\\nasin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024)\\nprint(output_torch)\\nprint(output_triton)\\nprint(\\n    f'The maximum difference between torch and triton is '\\n    f'{torch.max(torch.abs(output_torch - output_triton))}'\\n)\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Customize the libdevice library path\\nWe can also customize the libdevice library path by passing the path to the `libdevice` library to the `asin` kernel.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"output_triton = torch.empty_like(x)\\nasin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024,\\n                  extern_libs={'libdevice': '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'})\\nprint(output_torch)\\nprint(output_triton)\\nprint(\\n    f'The maximum difference between torch and triton is '\\n    f'{torch.max(torch.abs(output_torch - output_triton))}'\\n)\"\n+      ]\n+    }\n+  ],\n+  \"metadata\": {\n+    \"kernelspec\": {\n+      \"display_name\": \"Python 3\",\n+      \"language\": \"python\",\n+      \"name\": \"python3\"\n+    },\n+    \"language_info\": {\n+      \"codemirror_mode\": {\n+        \"name\": \"ipython\",\n+        \"version\": 3\n+      },\n+      \"file_extension\": \".py\",\n+      \"mimetype\": \"text/x-python\",\n+      \"name\": \"python\",\n+      \"nbconvert_exporter\": \"python\",\n+      \"pygments_lexer\": \"ipython3\",\n+      \"version\": \"3.8.10\"\n+    }\n+  },\n+  \"nbformat\": 4,\n+  \"nbformat_minor\": 0\n+}\n\\ No newline at end of file"}, {"filename": "main/_downloads/3176accb6c7288b0e45f41d94eebacb9/06-fused-attention.ipynb", "status": "added", "additions": 54, "deletions": 0, "changes": 54, "file_content_changes": "@@ -0,0 +1,54 @@\n+{\n+  \"cells\": [\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"%matplotlib inline\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"\\n# Fused Attention\\n\\nThis is a Triton implementation of the Flash Attention v2 algorithm from Tri Dao (https://tridao.me/publications/flash2/flash2.pdf)\\n\\nExtra Credits:\\n- Original flash attention paper (https://arxiv.org/abs/2205.14135)\\n- Rabe and Staats (https://arxiv.org/pdf/2112.05682v2.pdf)\\n- Adam P. Goucher for simplified vector math\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import pytest\\nimport torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.jit\\ndef max_fn(x, y):\\n    return tl.math.max(x, y)\\n\\n\\n@triton.jit\\ndef _fwd_kernel(\\n    Q, K, V, sm_scale,\\n    L,\\n    Out,\\n    stride_qz, stride_qh, stride_qm, stride_qk,\\n    stride_kz, stride_kh, stride_kn, stride_kk,\\n    stride_vz, stride_vh, stride_vk, stride_vn,\\n    stride_oz, stride_oh, stride_om, stride_on,\\n    Z, H, N_CTX, P_SEQ,\\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\\n    BLOCK_N: tl.constexpr,\\n    IS_CAUSAL: tl.constexpr,\\n):\\n    start_m = tl.program_id(0)\\n    off_hz = tl.program_id(1)\\n    q_offset = off_hz * stride_qh\\n    kv_offset = off_hz * stride_kh\\n    Q_block_ptr = tl.make_block_ptr(\\n        base=Q + q_offset,\\n        shape=(N_CTX, BLOCK_DMODEL),\\n        strides=(stride_qm, stride_qk),\\n        offsets=(start_m * BLOCK_M, 0),\\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\\n        order=(1, 0)\\n    )\\n    K_block_ptr = tl.make_block_ptr(\\n        base=K + kv_offset,\\n        shape=(BLOCK_DMODEL, N_CTX + P_SEQ),\\n        strides=(stride_kk, stride_kn),\\n        offsets=(0, 0),\\n        block_shape=(BLOCK_DMODEL, BLOCK_N),\\n        order=(0, 1)\\n    )\\n    V_block_ptr = tl.make_block_ptr(\\n        base=V + kv_offset,\\n        shape=(N_CTX + P_SEQ, BLOCK_DMODEL),\\n        strides=(stride_vk, stride_vn),\\n        offsets=(0, 0),\\n        block_shape=(BLOCK_N, BLOCK_DMODEL),\\n        order=(1, 0)\\n    )\\n    # initialize offsets\\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\\n    offs_n = tl.arange(0, BLOCK_N)\\n    # initialize pointer to m and l\\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\\\"inf\\\")\\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n    # scale sm_scale by log_2(e) and use\\n    # 2^x instead of exp in the loop because CSE and LICM\\n    # don't work as expected with `exp` in the loop\\n    qk_scale = sm_scale * 1.44269504\\n    # load q: it will stay in SRAM throughout\\n    q = tl.load(Q_block_ptr)\\n    q = (q * qk_scale).to(tl.float16)\\n    # loop over k, v and update accumulator\\n    lo = 0\\n    hi = P_SEQ + (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX + P_SEQ\\n    for start_n in range(lo, hi, BLOCK_N):\\n        # -- load k, v --\\n        k = tl.load(K_block_ptr)\\n        v = tl.load(V_block_ptr)\\n        # -- compute qk ---\\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float16)\\n        if IS_CAUSAL:\\n            qk = tl.where(P_SEQ + offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\\\"-inf\\\"))\\n        qk += tl.dot(q, k, out_dtype=tl.float16)\\n        # -- compute scaling constant ---\\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\\n        alpha = tl.math.exp2(m_i - m_i_new)\\n        p = tl.math.exp2(qk - m_i_new[:, None])\\n        # -- scale and update acc --\\n        acc_scale = l_i * 0 + alpha  # workaround some compiler bug\\n        acc *= acc_scale[:, None]\\n        acc += tl.dot(p.to(tl.float16), v)\\n        # -- update m_i and l_i --\\n        l_i = l_i * alpha + tl.sum(p, 1)\\n        m_i = m_i_new\\n        # update pointers\\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\\n    # write back l and m\\n    acc = acc / l_i[:, None]\\n    l_ptrs = L + off_hz * N_CTX + offs_m\\n    tl.store(l_ptrs, m_i + tl.math.log2(l_i))\\n    # write back O\\n    O_block_ptr = tl.make_block_ptr(\\n        base=Out + q_offset,\\n        shape=(N_CTX, BLOCK_DMODEL),\\n        strides=(stride_om, stride_on),\\n        offsets=(start_m * BLOCK_M, 0),\\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\\n        order=(1, 0)\\n    )\\n    tl.store(O_block_ptr, acc.to(tl.float16))\\n\\n\\n@triton.jit\\ndef _bwd_preprocess(\\n    Out, DO,\\n    Delta,\\n    BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\\n):\\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\\n    off_n = tl.arange(0, D_HEAD)\\n    # load\\n    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\\n    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\\n    # compute\\n    delta = tl.sum(o * do, axis=1)\\n    # write-back\\n    tl.store(Delta + off_m, delta)\\n\\n\\n@triton.jit\\ndef _bwd_kernel(\\n    Q, K, V, sm_scale, Out, DO,\\n    DQ, DK, DV,\\n    L,\\n    D,\\n    stride_qz, stride_qh, stride_qm, stride_qk,\\n    stride_kz, stride_kh, stride_kn, stride_kk,\\n    stride_vz, stride_vh, stride_vk, stride_vn,\\n    Z, H, N_CTX, P_SEQ,\\n    num_block_q, num_block_kv,\\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\\n    BLOCK_N: tl.constexpr,\\n    CAUSAL: tl.constexpr,\\n):\\n    off_hz = tl.program_id(0)\\n    off_z = off_hz // H\\n    off_h = off_hz % H\\n    qk_scale = sm_scale * 1.44269504\\n    # offset pointers for batch/head\\n    Q += off_z * stride_qz + off_h * stride_qh\\n    K += off_z * stride_kz + off_h * stride_kh\\n    V += off_z * stride_vz + off_h * stride_vh\\n    DO += off_z * stride_qz + off_h * stride_qh\\n    DQ += off_z * stride_qz + off_h * stride_qh\\n    DK += off_z * stride_kz + off_h * stride_kh\\n    DV += off_z * stride_vz + off_h * stride_vh\\n    for start_n in range(0, num_block_kv):\\n        if CAUSAL:\\n            lo = tl.math.max(start_n * BLOCK_M - P_SEQ, 0)\\n        else:\\n            lo = 0\\n        # initialize row/col offsets\\n        offs_qm = lo + tl.arange(0, BLOCK_M)\\n        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\\n        offs_m = tl.arange(0, BLOCK_N)\\n        offs_k = tl.arange(0, BLOCK_DMODEL)\\n        # initialize pointers to value-like data\\n        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\\n        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        # pointer to row-wise quantities in value-like data\\n        D_ptrs = D + off_hz * N_CTX\\n        l_ptrs = L + off_hz * N_CTX\\n        # initialize dk amd dv\\n        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n        # k and v stay in SRAM throughout\\n        k = tl.load(k_ptrs)\\n        v = tl.load(v_ptrs)\\n        # loop over rows\\n        for start_m in range(lo, num_block_q * BLOCK_M, BLOCK_M):\\n            offs_m_curr = start_m + offs_m\\n            # load q, k, v, do on-chip\\n            q = tl.load(q_ptrs)\\n            # recompute p = softmax(qk, dim=-1).T\\n            if CAUSAL:\\n                qk = tl.where(P_SEQ + offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\\\"-inf\\\"))\\n            else:\\n                qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\\n            qk += tl.dot(q, tl.trans(k))\\n            qk *= qk_scale\\n            l_i = tl.load(l_ptrs + offs_m_curr)\\n            p = tl.math.exp2(qk - l_i[:, None])\\n            # compute dv\\n            do = tl.load(do_ptrs)\\n            dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\\n            # compute dp = dot(v, do)\\n            Di = tl.load(D_ptrs + offs_m_curr)\\n            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\\n            dp += tl.dot(do, tl.trans(v))\\n            # compute ds = p * (dp - delta[:, None])\\n            ds = p * dp * sm_scale\\n            # compute dk = dot(ds.T, q)\\n            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)\\n            # compute dq\\n            dq = tl.load(dq_ptrs)\\n            dq += tl.dot(ds.to(Q.dtype.element_ty), k)\\n            tl.store(dq_ptrs, dq)\\n            # increment pointers\\n            dq_ptrs += BLOCK_M * stride_qm\\n            q_ptrs += BLOCK_M * stride_qm\\n            do_ptrs += BLOCK_M * stride_qm\\n        # write-back\\n        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\\n        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        tl.store(dk_ptrs, dk)\\n        tl.store(dv_ptrs, dv)\\n\\n\\nempty = torch.empty(128, device=\\\"cuda\\\")\\n\\n\\nclass _attention(torch.autograd.Function):\\n\\n    @staticmethod\\n    def forward(ctx, q, k, v, causal, sm_scale):\\n        # shape constraints\\n        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\\n        assert Lq == Lk and Lk == Lv\\n        assert Lk in {16, 32, 64, 128}\\n        o = torch.empty_like(q)\\n        BLOCK_M = 128\\n        BLOCK_N = 64 if Lk <= 64 else 32\\n        num_stages = 4 if Lk <= 64 else 3\\n        num_warps = 4\\n        grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\\n        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\\n        P_SEQ = 0 if q.shape[-2] == k.shape[-2] else k.shape[-2] - q.shape[-2]\\n        _fwd_kernel[grid](\\n            q, k, v, sm_scale,\\n            L,\\n            o,\\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\\n            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\\n            q.shape[0], q.shape[1], q.shape[2], P_SEQ,\\n            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=Lk,\\n            IS_CAUSAL=causal,\\n            num_warps=num_warps,\\n            num_stages=num_stages)\\n\\n        ctx.save_for_backward(q, k, v, o, L)\\n        ctx.grid = grid\\n        ctx.sm_scale = sm_scale\\n        ctx.BLOCK_DMODEL = Lk\\n        ctx.causal = causal\\n        ctx.P_SEQ = P_SEQ\\n        return o\\n\\n    @staticmethod\\n    def backward(ctx, do):\\n        BLOCK = 128\\n        q, k, v, o, L = ctx.saved_tensors\\n        do = do.contiguous()\\n        dq = torch.zeros_like(q, dtype=torch.float32)\\n        dk = torch.empty_like(k)\\n        dv = torch.empty_like(v)\\n        delta = torch.empty_like(L)\\n        _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\\n            o, do,\\n            delta,\\n            BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\\n        )\\n        _bwd_kernel[(ctx.grid[1],)](\\n            q, k, v, ctx.sm_scale,\\n            o, do,\\n            dq, dk, dv,\\n            L, delta,\\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\\n            q.shape[0], q.shape[1], q.shape[2], ctx.P_SEQ,\\n            ctx.grid[0], triton.cdiv(k.shape[2], BLOCK),\\n            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\\n            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\\n            CAUSAL=ctx.causal,\\n            num_stages=1,\\n        )\\n        return dq, dk, dv, None, None\\n\\n\\nattention = _attention.apply\\n\\n\\n@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD, P_SEQ', [(6, 9, 1024, 64, 128)])\\n@pytest.mark.parametrize('causal', [False, True])\\ndef test_op(Z, H, N_CTX, D_HEAD, P_SEQ, causal, dtype=torch.float16):\\n    torch.manual_seed(20)\\n    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0., std=0.5).requires_grad_()\\n    k = torch.empty((Z, H, N_CTX + P_SEQ, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0., std=0.5).requires_grad_()\\n    v = torch.empty((Z, H, N_CTX + P_SEQ, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0., std=0.5).requires_grad_()\\n    sm_scale = 0.5\\n    dout = torch.randn_like(q)\\n    # reference implementation\\n    M = torch.tril(torch.ones((N_CTX, N_CTX + P_SEQ), device=\\\"cuda\\\"), diagonal=P_SEQ)\\n    p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\\n    if causal:\\n        p[:, :, M == 0] = float(\\\"-inf\\\")\\n    p = torch.softmax(p.float(), dim=-1).half()\\n    # p = torch.exp(p)\\n    ref_out = torch.matmul(p, v)\\n    ref_out.backward(dout)\\n    ref_dv, v.grad = v.grad.clone(), None\\n    ref_dk, k.grad = k.grad.clone(), None\\n    ref_dq, q.grad = q.grad.clone(), None\\n    # triton implementation\\n    tri_out = attention(q, k, v, causal, sm_scale).half()\\n    tri_out.backward(dout)\\n    tri_dv, v.grad = v.grad.clone(), None\\n    tri_dk, k.grad = k.grad.clone(), None\\n    tri_dq, q.grad = q.grad.clone(), None\\n    # compare\\n    assert torch.allclose(ref_out, tri_out, atol=1e-2, rtol=0)\\n    assert torch.allclose(ref_dv, tri_dv, atol=1e-2, rtol=0)\\n    assert torch.allclose(ref_dk, tri_dk, atol=1e-2, rtol=0)\\n    assert torch.allclose(ref_dq, tri_dq, atol=1e-2, rtol=0)\\n\\n\\ntry:\\n    from flash_attn.flash_attn_interface import \\\\\\n        flash_attn_qkvpacked_func as flash_attn_func\\n    FLASH_VER = 2\\nexcept BaseException:\\n    try:\\n        from flash_attn.flash_attn_interface import flash_attn_func\\n        FLASH_VER = 1\\n    except BaseException:\\n        FLASH_VER = None\\nHAS_FLASH = FLASH_VER is not None\\n\\nBATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\\n# vary seq length for fixed head and batch=4\\nconfigs = [triton.testing.Benchmark(\\n    x_names=['N_CTX'],\\n    x_vals=[2**i for i in range(10, 15)],\\n    line_arg='provider',\\n    line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\\n    line_names=['Triton'] + ([f'Flash-{FLASH_VER}'] if HAS_FLASH else []),\\n    styles=[('red', '-'), ('blue', '-')],\\n    ylabel='ms',\\n    plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\\n    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode, 'causal': causal}\\n) for mode in ['fwd', 'bwd'] for causal in [False, True]]\\n\\n\\n@triton.testing.perf_report(configs)\\ndef bench_flash_attention(BATCH, H, N_CTX, D_HEAD, causal, mode, provider, dtype=torch.float16, device=\\\"cuda\\\"):\\n    assert mode in ['fwd', 'bwd']\\n    warmup = 25\\n    rep = 100\\n    if provider == \\\"triton\\\":\\n        q = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        sm_scale = 1.3\\n        fn = lambda: attention(q, k, v, causal, sm_scale)\\n        if mode == 'bwd':\\n            o = fn()\\n            do = torch.randn_like(o)\\n            fn = lambda: o.backward(do, retain_graph=True)\\n        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\\n    if provider == \\\"flash\\\":\\n        qkv = torch.randn((BATCH, N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\\n        if FLASH_VER == 1:\\n            lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\\n            cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\\n            cu_seqlens[1:] = lengths.cumsum(0)\\n            qkv = qkv.reshape(BATCH * N_CTX, 3, H, D_HEAD)\\n            fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=causal)\\n        elif FLASH_VER == 2:\\n            fn = lambda: flash_attn_func(qkv, causal=causal)\\n        else:\\n            raise ValueError(f'unknown {FLASH_VER = }')\\n        if mode == 'bwd':\\n            o = fn()\\n            do = torch.randn_like(o)\\n            fn = lambda: o.backward(do, retain_graph=True)\\n        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\\n    flops_per_matmul = 2. * BATCH * H * N_CTX * N_CTX * D_HEAD\\n    total_flops = 2 * flops_per_matmul\\n    if causal:\\n        total_flops *= 0.5\\n    if mode == 'bwd':\\n        total_flops *= 2.5  # 2.0(bwd) + 0.5(recompute)\\n    return total_flops / ms * 1e-9\\n\\n\\n# only works on post-Ampere GPUs right now\\nbench_flash_attention.run(save_path='.', print_data=True)\"\n+      ]\n+    }\n+  ],\n+  \"metadata\": {\n+    \"kernelspec\": {\n+      \"display_name\": \"Python 3\",\n+      \"language\": \"python\",\n+      \"name\": \"python3\"\n+    },\n+    \"language_info\": {\n+      \"codemirror_mode\": {\n+        \"name\": \"ipython\",\n+        \"version\": 3\n+      },\n+      \"file_extension\": \".py\",\n+      \"mimetype\": \"text/x-python\",\n+      \"name\": \"python\",\n+      \"nbconvert_exporter\": \"python\",\n+      \"pygments_lexer\": \"ipython3\",\n+      \"version\": \"3.8.10\"\n+    }\n+  },\n+  \"nbformat\": 4,\n+  \"nbformat_minor\": 0\n+}\n\\ No newline at end of file"}, {"filename": "main/_downloads/4d6052117d61c2ca779cd4b75567fee5/08-experimental-block-pointer.py", "status": "added", "additions": 228, "deletions": 0, "changes": 228, "file_content_changes": "@@ -0,0 +1,228 @@\n+\"\"\"\n+Block Pointer (Experimental)\n+============================\n+This tutorial will guide you through writing a matrix multiplication algorithm that utilizes block pointer semantics.\n+These semantics are more friendly for Triton to optimize and can result in better performance on specific hardware.\n+Note that this feature is still experimental and may change in the future.\n+\n+\"\"\"\n+\n+# %%\n+# Motivations\n+# -----------\n+# In the previous matrix multiplication tutorial, we constructed blocks of values by de-referencing blocks of pointers,\n+# i.e., :code:`load(block<pointer_type<element_type>>) -> block<element_type>`, which involved loading blocks of\n+# elements from memory. This approach allowed for flexibility in using hardware-managed cache and implementing complex\n+# data structures, such as tensors of trees or unstructured look-up tables.\n+#\n+# However, the drawback of this approach is that it relies heavily on complex optimization passes by the compiler to\n+# optimize memory access patterns. This can result in brittle code that may suffer from performance degradation when the\n+# optimizer fails to perform adequately. Additionally, as memory controllers specialize to accommodate dense spatial\n+# data structures commonly used in machine learning workloads, this problem is likely to worsen.\n+#\n+# To address this issue, we will use block pointers :code:`pointer_type<block<element_type>>` and load them into\n+# :code:`block<element_type>`, in which way gives better friendliness for the compiler to optimize memory access\n+# patterns.\n+#\n+# Let's start with the previous matrix multiplication example and demonstrate how to rewrite it to utilize block pointer\n+# semantics.\n+\n+# %%\n+# Make a Block Pointer\n+# --------------------\n+# A block pointer pointers to a block in a parent tensor and is constructed by :code:`make_block_ptr` function,\n+# which takes the following information as arguments:\n+#\n+# * :code:`base`: the base pointer to the parent tensor;\n+#\n+# * :code:`shape`: the shape of the parent tensor;\n+#\n+# * :code:`strides`: the strides of the parent tensor, which means how much to increase the pointer by when moving by 1 element in a specific axis;\n+#\n+# * :code:`offsets`: the offsets of the block;\n+#\n+# * :code:`block_shape`: the shape of the block;\n+#\n+# * :code:`order`: the order of the block, which means how the block is laid out in memory.\n+#\n+# For example, to a block pointer to a :code:`BLOCK_SIZE_M * BLOCK_SIZE_K` block in a row-major 2D matrix A by\n+# offsets :code:`(pid_m * BLOCK_SIZE_M, 0)` and strides :code:`(stride_am, stride_ak)`, we can use the following code\n+# (exactly the same as the previous matrix multiplication tutorial):\n+#\n+# .. code-block:: python\n+#\n+#     a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\n+#                                     offsets=(pid_m * BLOCK_SIZE_M, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K),\n+#                                     order=(1, 0))\n+#\n+# Note that the :code:`order` argument is set to :code:`(1, 0)`, which means the second axis is the inner dimension in\n+# terms of storage, and the first axis is the outer dimension. This information may sound redundant, but it is necessary\n+# for some hardware backends to optimize for better performance.\n+\n+# %%\n+# Load/Store a Block Pointer\n+# --------------------------\n+# To load/store a block pointer, we can use :code:`load/store` function, which takes a block pointer as an argument,\n+# de-references it, and loads/stores a block. You may mask some values in the block, here we have an extra argument\n+# :code:`boundary_check` to specify whether to check the boundary of each axis for the block pointer. With check on,\n+# out-of-bound values will be masked according to the :code:`padding_option` argument (load only), which can be\n+# :code:`zero` or :code:`nan`. Temporarily, we do not support other values due to some hardware limitations. In this\n+# mode of block pointer load/store does not support :code:`mask` or :code:`other` arguments in the legacy mode.\n+#\n+# So to load the block pointer of A in the previous section, we can simply write\n+# :code:`a = tl.load(a_block_ptr, boundary_check=(0, 1))`. Boundary check may cost extra performance, so if you can\n+# guarantee that the block pointer is always in-bound in some axis, you can turn off the check by not passing the index\n+# into the :code:`boundary_check` argument. For example, if we know that :code:`M` is a multiple of\n+# :code:`BLOCK_SIZE_M`, we can replace with :code:`a = tl.load(a_block_ptr, boundary_check=(1, ))`, since axis 0 is\n+# always in bound.\n+\n+# %%\n+# Advance a Block Pointer\n+# -----------------------\n+# To advance a block pointer, we can use :code:`advance` function, which takes a block pointer and the increment for\n+# each axis as arguments and returns a new block pointer with the same shape and strides as the original one,\n+# but with the offsets advanced by the specified amount.\n+#\n+# For example, to advance the block pointer by :code:`BLOCK_SIZE_K` in the second axis\n+# (no need to multiply with strides), we can write :code:`a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))`.\n+\n+# %%\n+# Final Result\n+# ------------\n+\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.autotune(\n+    configs=[\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n+        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n+    ],\n+    key=['M', 'N', 'K'],\n+)\n+@triton.jit\n+def matmul_kernel_with_block_pointers(\n+        # Pointers to matrices\n+        a_ptr, b_ptr, c_ptr,\n+        # Matrix dimensions\n+        M, N, K,\n+        # The stride variables represent how much to increase the ptr by when moving by 1\n+        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n+        # by to get the element one row down (A has M rows).\n+        stride_am, stride_ak,\n+        stride_bk, stride_bn,\n+        stride_cm, stride_cn,\n+        # Meta-parameters\n+        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+        GROUP_SIZE_M: tl.constexpr\n+):\n+    \"\"\"Kernel for computing the matmul C = A x B.\n+    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n+    \"\"\"\n+    # -----------------------------------------------------------\n+    # Map program ids `pid` to the block of C it should compute.\n+    # This is done in a grouped ordering to promote L2 data reuse.\n+    # See the matrix multiplication tutorial for details.\n+    pid = tl.program_id(axis=0)\n+    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n+    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n+    group_id = pid // num_pid_in_group\n+    first_pid_m = group_id * GROUP_SIZE_M\n+    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n+    pid_m = first_pid_m + (pid % group_size_m)\n+    pid_n = (pid % num_pid_in_group) // group_size_m\n+\n+    # ----------------------------------------------------------\n+    # Create block pointers for the first blocks of A and B.\n+    # We will advance this pointer as we move in the K direction and accumulate.\n+    # See above `Make a Block Pointer` section for details.\n+    a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\n+                                    offsets=(pid_m * BLOCK_SIZE_M, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K),\n+                                    order=(1, 0))\n+    b_block_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(stride_bk, stride_bn),\n+                                    offsets=(0, pid_n * BLOCK_SIZE_N), block_shape=(BLOCK_SIZE_K, BLOCK_SIZE_N),\n+                                    order=(1, 0))\n+\n+    # -----------------------------------------------------------\n+    # Iterate to compute a block of the C matrix.\n+    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block.\n+    # of fp32 values for higher accuracy.\n+    # `accumulator` will be converted back to fp16 after the loop.\n+    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+    for k in range(0, K, BLOCK_SIZE_K):\n+        # Load with boundary checks, no need to calculate the mask manually.\n+        # For better performance, you may remove some axis from the boundary\n+        # check, if you can guarantee that the access is always in-bound in\n+        # that axis.\n+        # See above `Load/Store a Block Pointer` section for details.\n+        a = tl.load(a_block_ptr, boundary_check=(0, 1))\n+        b = tl.load(b_block_ptr, boundary_check=(0, 1))\n+        # We accumulate along the K dimension.\n+        accumulator += tl.dot(a, b)\n+        # Advance the block pointer to the next K block.\n+        # See above `Advance a Block Pointer` section for details.\n+        a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))\n+        b_block_ptr = tl.advance(b_block_ptr, (BLOCK_SIZE_K, 0))\n+    c = accumulator.to(tl.float16)\n+\n+    # ----------------------------------------------------------------\n+    # Write back the block of the output matrix C with boundary checks.\n+    # See above `Load/Store a Block Pointer` section for details.\n+    c_block_ptr = tl.make_block_ptr(base=c_ptr, shape=(M, N), strides=(stride_cm, stride_cn),\n+                                    offsets=(pid_m * BLOCK_SIZE_M, pid_n * BLOCK_SIZE_N),\n+                                    block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_N), order=(1, 0))\n+    tl.store(c_block_ptr, c, boundary_check=(0, 1))\n+\n+\n+# We can now create a convenience wrapper function that only takes two input tensors,\n+# and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\n+def matmul(a, b):\n+    # Check constraints.\n+    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n+    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n+    assert b.is_contiguous(), \"Matrix B must be contiguous\"\n+    M, K = a.shape\n+    K, N = b.shape\n+    # Allocates output.\n+    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n+    # 1D launch kernel where each block gets its own program.\n+    grid = lambda META: (\n+        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n+    )\n+    matmul_kernel_with_block_pointers[grid](\n+        a, b, c,\n+        M, N, K,\n+        a.stride(0), a.stride(1),\n+        b.stride(0), b.stride(1),\n+        c.stride(0), c.stride(1),\n+    )\n+    return c\n+\n+\n+# %%\n+# Unit Test\n+# ---------\n+#\n+# Still we can test our matrix multiplication with block pointers against a native torch implementation (i.e., cuBLAS).\n+\n+torch.manual_seed(0)\n+a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n+b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n+triton_output = matmul(a, b)\n+torch_output = torch.matmul(a, b)\n+print(f\"triton_output={triton_output}\")\n+print(f\"torch_output={torch_output}\")\n+if torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\n+    print(\"\u2705 Triton and Torch match\")\n+else:\n+    print(\"\u274c Triton and Torch differ\")"}, {"filename": "main/_downloads/54a35f6ec55f9746935b9566fb6bb1df/06-fused-attention.py", "status": "added", "additions": 413, "deletions": 0, "changes": 413, "file_content_changes": "@@ -0,0 +1,413 @@\n+\"\"\"\n+Fused Attention\n+===============\n+\n+This is a Triton implementation of the Flash Attention v2 algorithm from Tri Dao (https://tridao.me/publications/flash2/flash2.pdf)\n+\n+Extra Credits:\n+- Original flash attention paper (https://arxiv.org/abs/2205.14135)\n+- Rabe and Staats (https://arxiv.org/pdf/2112.05682v2.pdf)\n+- Adam P. Goucher for simplified vector math\n+\n+\"\"\"\n+\n+import pytest\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def max_fn(x, y):\n+    return tl.math.max(x, y)\n+\n+\n+@triton.jit\n+def _fwd_kernel(\n+    Q, K, V, sm_scale,\n+    L,\n+    Out,\n+    stride_qz, stride_qh, stride_qm, stride_qk,\n+    stride_kz, stride_kh, stride_kn, stride_kk,\n+    stride_vz, stride_vh, stride_vk, stride_vn,\n+    stride_oz, stride_oh, stride_om, stride_on,\n+    Z, H, N_CTX, P_SEQ,\n+    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n+    BLOCK_N: tl.constexpr,\n+    IS_CAUSAL: tl.constexpr,\n+):\n+    start_m = tl.program_id(0)\n+    off_hz = tl.program_id(1)\n+    q_offset = off_hz * stride_qh\n+    kv_offset = off_hz * stride_kh\n+    Q_block_ptr = tl.make_block_ptr(\n+        base=Q + q_offset,\n+        shape=(N_CTX, BLOCK_DMODEL),\n+        strides=(stride_qm, stride_qk),\n+        offsets=(start_m * BLOCK_M, 0),\n+        block_shape=(BLOCK_M, BLOCK_DMODEL),\n+        order=(1, 0)\n+    )\n+    K_block_ptr = tl.make_block_ptr(\n+        base=K + kv_offset,\n+        shape=(BLOCK_DMODEL, N_CTX + P_SEQ),\n+        strides=(stride_kk, stride_kn),\n+        offsets=(0, 0),\n+        block_shape=(BLOCK_DMODEL, BLOCK_N),\n+        order=(0, 1)\n+    )\n+    V_block_ptr = tl.make_block_ptr(\n+        base=V + kv_offset,\n+        shape=(N_CTX + P_SEQ, BLOCK_DMODEL),\n+        strides=(stride_vk, stride_vn),\n+        offsets=(0, 0),\n+        block_shape=(BLOCK_N, BLOCK_DMODEL),\n+        order=(1, 0)\n+    )\n+    # initialize offsets\n+    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+    offs_n = tl.arange(0, BLOCK_N)\n+    # initialize pointer to m and l\n+    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n+    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+    # scale sm_scale by log_2(e) and use\n+    # 2^x instead of exp in the loop because CSE and LICM\n+    # don't work as expected with `exp` in the loop\n+    qk_scale = sm_scale * 1.44269504\n+    # load q: it will stay in SRAM throughout\n+    q = tl.load(Q_block_ptr)\n+    q = (q * qk_scale).to(tl.float16)\n+    # loop over k, v and update accumulator\n+    lo = 0\n+    hi = P_SEQ + (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX + P_SEQ\n+    for start_n in range(lo, hi, BLOCK_N):\n+        # -- load k, v --\n+        k = tl.load(K_block_ptr)\n+        v = tl.load(V_block_ptr)\n+        # -- compute qk ---\n+        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float16)\n+        if IS_CAUSAL:\n+            qk = tl.where(P_SEQ + offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n+        qk += tl.dot(q, k, out_dtype=tl.float16)\n+        # -- compute scaling constant ---\n+        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n+        alpha = tl.math.exp2(m_i - m_i_new)\n+        p = tl.math.exp2(qk - m_i_new[:, None])\n+        # -- scale and update acc --\n+        acc_scale = l_i * 0 + alpha  # workaround some compiler bug\n+        acc *= acc_scale[:, None]\n+        acc += tl.dot(p.to(tl.float16), v)\n+        # -- update m_i and l_i --\n+        l_i = l_i * alpha + tl.sum(p, 1)\n+        m_i = m_i_new\n+        # update pointers\n+        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n+        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n+    # write back l and m\n+    acc = acc / l_i[:, None]\n+    l_ptrs = L + off_hz * N_CTX + offs_m\n+    tl.store(l_ptrs, m_i + tl.math.log2(l_i))\n+    # write back O\n+    O_block_ptr = tl.make_block_ptr(\n+        base=Out + q_offset,\n+        shape=(N_CTX, BLOCK_DMODEL),\n+        strides=(stride_om, stride_on),\n+        offsets=(start_m * BLOCK_M, 0),\n+        block_shape=(BLOCK_M, BLOCK_DMODEL),\n+        order=(1, 0)\n+    )\n+    tl.store(O_block_ptr, acc.to(tl.float16))\n+\n+\n+@triton.jit\n+def _bwd_preprocess(\n+    Out, DO,\n+    Delta,\n+    BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\n+):\n+    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n+    off_n = tl.arange(0, D_HEAD)\n+    # load\n+    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n+    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n+    # compute\n+    delta = tl.sum(o * do, axis=1)\n+    # write-back\n+    tl.store(Delta + off_m, delta)\n+\n+\n+@triton.jit\n+def _bwd_kernel(\n+    Q, K, V, sm_scale, Out, DO,\n+    DQ, DK, DV,\n+    L,\n+    D,\n+    stride_qz, stride_qh, stride_qm, stride_qk,\n+    stride_kz, stride_kh, stride_kn, stride_kk,\n+    stride_vz, stride_vh, stride_vk, stride_vn,\n+    Z, H, N_CTX, P_SEQ,\n+    num_block_q, num_block_kv,\n+    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n+    BLOCK_N: tl.constexpr,\n+    CAUSAL: tl.constexpr,\n+):\n+    off_hz = tl.program_id(0)\n+    off_z = off_hz // H\n+    off_h = off_hz % H\n+    qk_scale = sm_scale * 1.44269504\n+    # offset pointers for batch/head\n+    Q += off_z * stride_qz + off_h * stride_qh\n+    K += off_z * stride_kz + off_h * stride_kh\n+    V += off_z * stride_vz + off_h * stride_vh\n+    DO += off_z * stride_qz + off_h * stride_qh\n+    DQ += off_z * stride_qz + off_h * stride_qh\n+    DK += off_z * stride_kz + off_h * stride_kh\n+    DV += off_z * stride_vz + off_h * stride_vh\n+    for start_n in range(0, num_block_kv):\n+        if CAUSAL:\n+            lo = tl.math.max(start_n * BLOCK_M - P_SEQ, 0)\n+        else:\n+            lo = 0\n+        # initialize row/col offsets\n+        offs_qm = lo + tl.arange(0, BLOCK_M)\n+        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n+        offs_m = tl.arange(0, BLOCK_N)\n+        offs_k = tl.arange(0, BLOCK_DMODEL)\n+        # initialize pointers to value-like data\n+        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n+        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        # pointer to row-wise quantities in value-like data\n+        D_ptrs = D + off_hz * N_CTX\n+        l_ptrs = L + off_hz * N_CTX\n+        # initialize dk amd dv\n+        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+        # k and v stay in SRAM throughout\n+        k = tl.load(k_ptrs)\n+        v = tl.load(v_ptrs)\n+        # loop over rows\n+        for start_m in range(lo, num_block_q * BLOCK_M, BLOCK_M):\n+            offs_m_curr = start_m + offs_m\n+            # load q, k, v, do on-chip\n+            q = tl.load(q_ptrs)\n+            # recompute p = softmax(qk, dim=-1).T\n+            if CAUSAL:\n+                qk = tl.where(P_SEQ + offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\"-inf\"))\n+            else:\n+                qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n+            qk += tl.dot(q, tl.trans(k))\n+            qk *= qk_scale\n+            l_i = tl.load(l_ptrs + offs_m_curr)\n+            p = tl.math.exp2(qk - l_i[:, None])\n+            # compute dv\n+            do = tl.load(do_ptrs)\n+            dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\n+            # compute dp = dot(v, do)\n+            Di = tl.load(D_ptrs + offs_m_curr)\n+            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n+            dp += tl.dot(do, tl.trans(v))\n+            # compute ds = p * (dp - delta[:, None])\n+            ds = p * dp * sm_scale\n+            # compute dk = dot(ds.T, q)\n+            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)\n+            # compute dq\n+            dq = tl.load(dq_ptrs)\n+            dq += tl.dot(ds.to(Q.dtype.element_ty), k)\n+            tl.store(dq_ptrs, dq)\n+            # increment pointers\n+            dq_ptrs += BLOCK_M * stride_qm\n+            q_ptrs += BLOCK_M * stride_qm\n+            do_ptrs += BLOCK_M * stride_qm\n+        # write-back\n+        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n+        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        tl.store(dk_ptrs, dk)\n+        tl.store(dv_ptrs, dv)\n+\n+\n+empty = torch.empty(128, device=\"cuda\")\n+\n+\n+class _attention(torch.autograd.Function):\n+\n+    @staticmethod\n+    def forward(ctx, q, k, v, causal, sm_scale):\n+        # shape constraints\n+        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n+        assert Lq == Lk and Lk == Lv\n+        assert Lk in {16, 32, 64, 128}\n+        o = torch.empty_like(q)\n+        BLOCK_M = 128\n+        BLOCK_N = 64 if Lk <= 64 else 32\n+        num_stages = 4 if Lk <= 64 else 3\n+        num_warps = 4\n+        grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n+        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        P_SEQ = 0 if q.shape[-2] == k.shape[-2] else k.shape[-2] - q.shape[-2]\n+        _fwd_kernel[grid](\n+            q, k, v, sm_scale,\n+            L,\n+            o,\n+            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n+            q.shape[0], q.shape[1], q.shape[2], P_SEQ,\n+            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=Lk,\n+            IS_CAUSAL=causal,\n+            num_warps=num_warps,\n+            num_stages=num_stages)\n+\n+        ctx.save_for_backward(q, k, v, o, L)\n+        ctx.grid = grid\n+        ctx.sm_scale = sm_scale\n+        ctx.BLOCK_DMODEL = Lk\n+        ctx.causal = causal\n+        ctx.P_SEQ = P_SEQ\n+        return o\n+\n+    @staticmethod\n+    def backward(ctx, do):\n+        BLOCK = 128\n+        q, k, v, o, L = ctx.saved_tensors\n+        do = do.contiguous()\n+        dq = torch.zeros_like(q, dtype=torch.float32)\n+        dk = torch.empty_like(k)\n+        dv = torch.empty_like(v)\n+        delta = torch.empty_like(L)\n+        _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n+            o, do,\n+            delta,\n+            BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n+        )\n+        _bwd_kernel[(ctx.grid[1],)](\n+            q, k, v, ctx.sm_scale,\n+            o, do,\n+            dq, dk, dv,\n+            L, delta,\n+            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+            q.shape[0], q.shape[1], q.shape[2], ctx.P_SEQ,\n+            ctx.grid[0], triton.cdiv(k.shape[2], BLOCK),\n+            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n+            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n+            CAUSAL=ctx.causal,\n+            num_stages=1,\n+        )\n+        return dq, dk, dv, None, None\n+\n+\n+attention = _attention.apply\n+\n+\n+@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD, P_SEQ', [(6, 9, 1024, 64, 128)])\n+@pytest.mark.parametrize('causal', [False, True])\n+def test_op(Z, H, N_CTX, D_HEAD, P_SEQ, causal, dtype=torch.float16):\n+    torch.manual_seed(20)\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX + P_SEQ, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX + P_SEQ, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    sm_scale = 0.5\n+    dout = torch.randn_like(q)\n+    # reference implementation\n+    M = torch.tril(torch.ones((N_CTX, N_CTX + P_SEQ), device=\"cuda\"), diagonal=P_SEQ)\n+    p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n+    if causal:\n+        p[:, :, M == 0] = float(\"-inf\")\n+    p = torch.softmax(p.float(), dim=-1).half()\n+    # p = torch.exp(p)\n+    ref_out = torch.matmul(p, v)\n+    ref_out.backward(dout)\n+    ref_dv, v.grad = v.grad.clone(), None\n+    ref_dk, k.grad = k.grad.clone(), None\n+    ref_dq, q.grad = q.grad.clone(), None\n+    # triton implementation\n+    tri_out = attention(q, k, v, causal, sm_scale).half()\n+    tri_out.backward(dout)\n+    tri_dv, v.grad = v.grad.clone(), None\n+    tri_dk, k.grad = k.grad.clone(), None\n+    tri_dq, q.grad = q.grad.clone(), None\n+    # compare\n+    assert torch.allclose(ref_out, tri_out, atol=1e-2, rtol=0)\n+    assert torch.allclose(ref_dv, tri_dv, atol=1e-2, rtol=0)\n+    assert torch.allclose(ref_dk, tri_dk, atol=1e-2, rtol=0)\n+    assert torch.allclose(ref_dq, tri_dq, atol=1e-2, rtol=0)\n+\n+\n+try:\n+    from flash_attn.flash_attn_interface import \\\n+        flash_attn_qkvpacked_func as flash_attn_func\n+    FLASH_VER = 2\n+except BaseException:\n+    try:\n+        from flash_attn.flash_attn_interface import flash_attn_func\n+        FLASH_VER = 1\n+    except BaseException:\n+        FLASH_VER = None\n+HAS_FLASH = FLASH_VER is not None\n+\n+BATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\n+# vary seq length for fixed head and batch=4\n+configs = [triton.testing.Benchmark(\n+    x_names=['N_CTX'],\n+    x_vals=[2**i for i in range(10, 15)],\n+    line_arg='provider',\n+    line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n+    line_names=['Triton'] + ([f'Flash-{FLASH_VER}'] if HAS_FLASH else []),\n+    styles=[('red', '-'), ('blue', '-')],\n+    ylabel='ms',\n+    plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n+    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode, 'causal': causal}\n+) for mode in ['fwd', 'bwd'] for causal in [False, True]]\n+\n+\n+@triton.testing.perf_report(configs)\n+def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, causal, mode, provider, dtype=torch.float16, device=\"cuda\"):\n+    assert mode in ['fwd', 'bwd']\n+    warmup = 25\n+    rep = 100\n+    if provider == \"triton\":\n+        q = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+        k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+        v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+        sm_scale = 1.3\n+        fn = lambda: attention(q, k, v, causal, sm_scale)\n+        if mode == 'bwd':\n+            o = fn()\n+            do = torch.randn_like(o)\n+            fn = lambda: o.backward(do, retain_graph=True)\n+        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n+    if provider == \"flash\":\n+        qkv = torch.randn((BATCH, N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\n+        if FLASH_VER == 1:\n+            lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n+            cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n+            cu_seqlens[1:] = lengths.cumsum(0)\n+            qkv = qkv.reshape(BATCH * N_CTX, 3, H, D_HEAD)\n+            fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=causal)\n+        elif FLASH_VER == 2:\n+            fn = lambda: flash_attn_func(qkv, causal=causal)\n+        else:\n+            raise ValueError(f'unknown {FLASH_VER = }')\n+        if mode == 'bwd':\n+            o = fn()\n+            do = torch.randn_like(o)\n+            fn = lambda: o.backward(do, retain_graph=True)\n+        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n+    flops_per_matmul = 2. * BATCH * H * N_CTX * N_CTX * D_HEAD\n+    total_flops = 2 * flops_per_matmul\n+    if causal:\n+        total_flops *= 0.5\n+    if mode == 'bwd':\n+        total_flops *= 2.5  # 2.0(bwd) + 0.5(recompute)\n+    return total_flops / ms * 1e-9\n+\n+\n+# only works on post-Ampere GPUs right now\n+bench_flash_attention.run(save_path='.', print_data=True)"}, {"filename": "main/_downloads/62d97d49a32414049819dd8bb8378080/01-vector-add.py", "status": "added", "additions": 139, "deletions": 0, "changes": 139, "file_content_changes": "@@ -0,0 +1,139 @@\n+\"\"\"\n+Vector Addition\n+===============\n+\n+In this tutorial, you will write a simple vector addition using Triton.\n+\n+In doing so, you will learn about:\n+\n+* The basic programming model of Triton.\n+\n+* The `triton.jit` decorator, which is used to define Triton kernels.\n+\n+* The best practices for validating and benchmarking your custom ops against native reference implementations.\n+\n+\"\"\"\n+\n+# %%\n+# Compute Kernel\n+# --------------\n+\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def add_kernel(\n+    x_ptr,  # *Pointer* to first input vector.\n+    y_ptr,  # *Pointer* to second input vector.\n+    output_ptr,  # *Pointer* to output vector.\n+    n_elements,  # Size of the vector.\n+    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n+                 # NOTE: `constexpr` so it can be used as a shape value.\n+):\n+    # There are multiple 'programs' processing different data. We identify which program\n+    # we are here:\n+    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n+    # This program will process inputs that are offset from the initial data.\n+    # For instance, if you had a vector of length 256 and block_size of 64, the programs\n+    # would each access the elements [0:64, 64:128, 128:192, 192:256].\n+    # Note that offsets is a list of pointers:\n+    block_start = pid * BLOCK_SIZE\n+    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+    # Create a mask to guard memory operations against out-of-bounds accesses.\n+    mask = offsets < n_elements\n+    # Load x and y from DRAM, masking out any extra elements in case the input is not a\n+    # multiple of the block size.\n+    x = tl.load(x_ptr + offsets, mask=mask)\n+    y = tl.load(y_ptr + offsets, mask=mask)\n+    output = x + y\n+    # Write x + y back to DRAM.\n+    tl.store(output_ptr + offsets, output, mask=mask)\n+\n+\n+# %%\n+# Let's also declare a helper function to (1) allocate the `z` tensor\n+# and (2) enqueue the above kernel with appropriate grid/block sizes:\n+\n+\n+def add(x: torch.Tensor, y: torch.Tensor):\n+    # We need to preallocate the output.\n+    output = torch.empty_like(x)\n+    assert x.is_cuda and y.is_cuda and output.is_cuda\n+    n_elements = output.numel()\n+    # The SPMD launch grid denotes the number of kernel instances that run in parallel.\n+    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\n+    # In this case, we use a 1D grid where the size is the number of blocks:\n+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+    # NOTE:\n+    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.\n+    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.\n+    #  - Don't forget to pass meta-parameters as keywords arguments.\n+    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n+    # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\n+    # running asynchronously at this point.\n+    return output\n+\n+\n+# %%\n+# We can now use the above function to compute the element-wise sum of two `torch.tensor` objects and test its correctness:\n+\n+torch.manual_seed(0)\n+size = 98432\n+x = torch.rand(size, device='cuda')\n+y = torch.rand(size, device='cuda')\n+output_torch = x + y\n+output_triton = add(x, y)\n+print(output_torch)\n+print(output_triton)\n+print(\n+    f'The maximum difference between torch and triton is '\n+    f'{torch.max(torch.abs(output_torch - output_triton))}'\n+)\n+\n+# %%\n+# Seems like we're good to go!\n+\n+# %%\n+# Benchmark\n+# ---------\n+#\n+# We can now benchmark our custom op on vectors of increasing sizes to get a sense of how it does relative to PyTorch.\n+# To make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of our custom ops.\n+# for different problem sizes.\n+\n+\n+@triton.testing.perf_report(\n+    triton.testing.Benchmark(\n+        x_names=['size'],  # Argument names to use as an x-axis for the plot.\n+        x_vals=[\n+            2 ** i for i in range(12, 28, 1)\n+        ],  # Different possible values for `x_name`.\n+        x_log=True,  # x axis is logarithmic.\n+        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n+        line_vals=['triton', 'torch'],  # Possible values for `line_arg`.\n+        line_names=['Triton', 'Torch'],  # Label name for the lines.\n+        styles=[('blue', '-'), ('green', '-')],  # Line styles.\n+        ylabel='GB/s',  # Label name for the y-axis.\n+        plot_name='vector-add-performance',  # Name for the plot. Used also as a file name for saving the plot.\n+        args={},  # Values for function arguments not in `x_names` and `y_name`.\n+    )\n+)\n+def benchmark(size, provider):\n+    x = torch.rand(size, device='cuda', dtype=torch.float32)\n+    y = torch.rand(size, device='cuda', dtype=torch.float32)\n+    quantiles = [0.5, 0.2, 0.8]\n+    if provider == 'torch':\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: x + y, quantiles=quantiles)\n+    if provider == 'triton':\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: add(x, y), quantiles=quantiles)\n+    gbps = lambda ms: 12 * size / ms * 1e-6\n+    return gbps(ms), gbps(max_ms), gbps(min_ms)\n+\n+\n+# %%\n+# We can now run the decorated function above. Pass `print_data=True` to see the performance number, `show_plots=True` to plot them, and/or\n+# `save_path='/path/to/results/' to save them to disk along with raw CSV data:\n+benchmark.run(print_data=True, show_plots=True)"}, {"filename": "main/_downloads/662999063954282841dc90b8945f85ce/tutorials_jupyter.zip", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/763344228ae6bc253ed1a6cf586aa30d/tutorials_python.zip", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/77571465f7f4bd281d3a847dc2633146/07-math-functions.py", "status": "added", "additions": 73, "deletions": 0, "changes": 73, "file_content_changes": "@@ -0,0 +1,73 @@\n+\"\"\"\n+Libdevice (`tl.math`) function\n+==============================\n+Triton can invoke a custom function from an external library.\n+In this example, we will use the `libdevice` library (a.k.a `math` in triton) to apply `asin` on a tensor.\n+Please refer to https://docs.nvidia.com/cuda/libdevice-users-guide/index.html regarding the semantics of all available libdevice functions.\n+In `triton/language/math.py`, we try to aggregate functions with the same computation but different data types together.\n+For example, both `__nv_asin` and `__nvasinf` calculate the principal value of the arc sine of the input, but `__nv_asin` operates on `double` and `__nv_asinf` operates on `float`.\n+Using triton, you can simply call `tl.math.asin`.\n+Triton automatically selects the correct underlying device function to invoke based on input and output types.\n+\"\"\"\n+\n+# %%\n+#  asin Kernel\n+# ------------\n+\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def asin_kernel(\n+        x_ptr,\n+        y_ptr,\n+        n_elements,\n+        BLOCK_SIZE: tl.constexpr,\n+):\n+    pid = tl.program_id(axis=0)\n+    block_start = pid * BLOCK_SIZE\n+    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+    mask = offsets < n_elements\n+    x = tl.load(x_ptr + offsets, mask=mask)\n+    x = tl.math.asin(x)\n+    tl.store(y_ptr + offsets, x, mask=mask)\n+\n+# %%\n+#  Using the default libdevice library path\n+# -----------------------------------------\n+# We can use the default libdevice library path encoded in `triton/language/math.py`\n+\n+\n+torch.manual_seed(0)\n+size = 98432\n+x = torch.rand(size, device='cuda')\n+output_triton = torch.zeros(size, device='cuda')\n+output_torch = torch.asin(x)\n+assert x.is_cuda and output_triton.is_cuda\n+n_elements = output_torch.numel()\n+grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+asin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024)\n+print(output_torch)\n+print(output_triton)\n+print(\n+    f'The maximum difference between torch and triton is '\n+    f'{torch.max(torch.abs(output_torch - output_triton))}'\n+)\n+\n+# %%\n+#  Customize the libdevice library path\n+# -------------------------------------\n+# We can also customize the libdevice library path by passing the path to the `libdevice` library to the `asin` kernel.\n+\n+output_triton = torch.empty_like(x)\n+asin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024,\n+                  extern_libs={'libdevice': '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'})\n+print(output_torch)\n+print(output_triton)\n+print(\n+    f'The maximum difference between torch and triton is '\n+    f'{torch.max(torch.abs(output_torch - output_triton))}'\n+)"}, {"filename": "main/_downloads/935c0dd0fbeb4b2e69588471cbb2d4b2/05-layer-norm.py", "status": "added", "additions": 376, "deletions": 0, "changes": 376, "file_content_changes": "@@ -0,0 +1,376 @@\n+\"\"\"\n+Layer Normalization\n+====================\n+In this tutorial, you will write a high-performance layer normalization\n+kernel that runs faster than the PyTorch implementation.\n+\n+In doing so, you will learn about:\n+\n+* Implementing backward pass in Triton.\n+\n+* Implementing parallel reduction in Triton.\n+\n+\"\"\"\n+\n+# %%\n+# Motivations\n+# -----------\n+#\n+# The *LayerNorm* operator was first introduced in [BA2016]_ as a way to improve the performance\n+# of sequential models (e.g., Transformers) or neural networks with small batch size.\n+# It takes a vector :math:`x` as input and produces a vector :math:`y` of the same shape as output.\n+# The normalization is performed by subtracting the mean and dividing by the standard deviation of :math:`x`.\n+# After the normalization, a learnable linear transformation with weights :math:`w` and biases :math:`b` is applied.\n+# The forward pass can be expressed as follows:\n+#\n+# .. math::\n+#    y = \\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} } * w + b\n+#\n+# where :math:`\\epsilon` is a small constant added to the denominator for numerical stability.\n+# Let\u2019s first take a look at the forward pass implementation.\n+\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+try:\n+    # This is https://github.com/NVIDIA/apex, NOT the apex on PyPi, so it\n+    # should not be added to extras_require in setup.py.\n+    import apex\n+    HAS_APEX = True\n+except ModuleNotFoundError:\n+    HAS_APEX = False\n+\n+\n+@triton.jit\n+def _layer_norm_fwd_fused(\n+    X,  # pointer to the input\n+    Y,  # pointer to the output\n+    W,  # pointer to the weights\n+    B,  # pointer to the biases\n+    Mean,  # pointer to the mean\n+    Rstd,  # pointer to the 1/std\n+    stride,  # how much to increase the pointer when moving by 1 row\n+    N,  # number of columns in X\n+    eps,  # epsilon to avoid division by zero\n+    BLOCK_SIZE: tl.constexpr,\n+):\n+    # Map the program id to the row of X and Y it should compute.\n+    row = tl.program_id(0)\n+    Y += row * stride\n+    X += row * stride\n+    # Compute mean\n+    mean = 0\n+    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n+    for off in range(0, N, BLOCK_SIZE):\n+        cols = off + tl.arange(0, BLOCK_SIZE)\n+        a = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n+        _mean += a\n+    mean = tl.sum(_mean, axis=0) / N\n+    # Compute variance\n+    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n+    for off in range(0, N, BLOCK_SIZE):\n+        cols = off + tl.arange(0, BLOCK_SIZE)\n+        x = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n+        x = tl.where(cols < N, x - mean, 0.)\n+        _var += x * x\n+    var = tl.sum(_var, axis=0) / N\n+    rstd = 1 / tl.sqrt(var + eps)\n+    # Write mean / rstd\n+    tl.store(Mean + row, mean)\n+    tl.store(Rstd + row, rstd)\n+    # Normalize and apply linear transformation\n+    for off in range(0, N, BLOCK_SIZE):\n+        cols = off + tl.arange(0, BLOCK_SIZE)\n+        mask = cols < N\n+        w = tl.load(W + cols, mask=mask)\n+        b = tl.load(B + cols, mask=mask)\n+        x = tl.load(X + cols, mask=mask, other=0.).to(tl.float32)\n+        x_hat = (x - mean) * rstd\n+        y = x_hat * w + b\n+        # Write output\n+        tl.store(Y + cols, y, mask=mask)\n+\n+\n+# %%\n+# Backward pass\n+# -------------\n+#\n+# The backward pass for the layer normalization operator is a bit more involved than the forward pass.\n+# Let :math:`\\hat{x}` be the normalized inputs :math:`\\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} }` before the linear transformation,\n+# the Vector-Jacobian Products (VJP) :math:`\\nabla_{x}` of :math:`x` are given by:\n+#\n+# .. math::\n+#    \\nabla_{x} = \\frac{1}{\\sigma}\\Big( \\nabla_{y} \\odot w - \\underbrace{ \\big( \\frac{1}{N} \\hat{x} \\cdot (\\nabla_{y} \\odot w) \\big) }_{c_1} \\odot \\hat{x} - \\underbrace{ \\frac{1}{N} \\nabla_{y} \\cdot w }_{c_2} \\Big)\n+#\n+# where :math:`\\odot` denotes the element-wise multiplication, :math:`\\cdot` denotes the dot product, and :math:`\\sigma` is the standard deviation.\n+# :math:`c_1` and :math:`c_2` are intermediate constants that improve the readability of the following implementation.\n+#\n+# For the weights :math:`w` and biases :math:`b`, the VJPs :math:`\\nabla_{w}` and :math:`\\nabla_{b}` are more straightforward:\n+#\n+# .. math::\n+#    \\nabla_{w} = \\nabla_{y} \\odot \\hat{x} \\quad \\text{and} \\quad \\nabla_{b} = \\nabla_{y}\n+#\n+# Since the same weights :math:`w` and biases :math:`b` are used for all rows in the same batch, their gradients need to sum up.\n+# To perform this step efficiently, we use a parallel reduction strategy: each kernel instance accumulates\n+# partial :math:`\\nabla_{w}` and :math:`\\nabla_{b}` across certain rows into one of :math:`\\text{GROUP_SIZE_M}` independent buffers.\n+# These buffers stay in the L2 cache and then are further reduced by another function to compute the actual :math:`\\nabla_{w}` and :math:`\\nabla_{b}`.\n+#\n+# Let the number of input rows :math:`M = 4` and :math:`\\text{GROUP_SIZE_M} = 2`,\n+# here's a diagram of the parallel reduction strategy for :math:`\\nabla_{w}` (:math:`\\nabla_{b}` is omitted for brevity):\n+#\n+#   .. image:: parallel_reduction.png\n+#\n+# In Stage 1, the rows of X that have the same color share the same buffer and thus a lock is used to ensure that only one kernel instance writes to the buffer at a time.\n+# In Stage 2, the buffers are further reduced to compute the final :math:`\\nabla_{w}` and :math:`\\nabla_{b}`.\n+# In the following implementation, Stage 1 is implemented by the function :code:`_layer_norm_bwd_dx_fused` and Stage 2 is implemented by the function :code:`_layer_norm_bwd_dwdb`.\n+\n+@triton.jit\n+def _layer_norm_bwd_dx_fused(\n+    DX,  # pointer to the input gradient\n+    DY,  # pointer to the output gradient\n+    DW,  # pointer to the partial sum of weights gradient\n+    DB,  # pointer to the partial sum of biases gradient\n+    X,   # pointer to the input\n+    W,   # pointer to the weights\n+    B,   # pointer to the biases\n+    Mean,   # pointer to the mean\n+    Rstd,   # pointer to the 1/std\n+    Lock,  # pointer to the lock\n+    stride,  # how much to increase the pointer when moving by 1 row\n+    N,  # number of columns in X\n+    eps,  # epsilon to avoid division by zero\n+    GROUP_SIZE_M: tl.constexpr,\n+    BLOCK_SIZE_N: tl.constexpr\n+):\n+    # Map the program id to the elements of X, DX, and DY it should compute.\n+    row = tl.program_id(0)\n+    cols = tl.arange(0, BLOCK_SIZE_N)\n+    mask = cols < N\n+    X += row * stride\n+    DY += row * stride\n+    DX += row * stride\n+    # Offset locks and weights/biases gradient pointer for parallel reduction\n+    lock_id = row % GROUP_SIZE_M\n+    Lock += lock_id\n+    Count = Lock + GROUP_SIZE_M\n+    DW = DW + lock_id * N + cols\n+    DB = DB + lock_id * N + cols\n+    # Load data to SRAM\n+    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n+    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n+    w = tl.load(W + cols, mask=mask).to(tl.float32)\n+    mean = tl.load(Mean + row)\n+    rstd = tl.load(Rstd + row)\n+    # Compute dx\n+    xhat = (x - mean) * rstd\n+    wdy = w * dy\n+    xhat = tl.where(mask, xhat, 0.)\n+    wdy = tl.where(mask, wdy, 0.)\n+    c1 = tl.sum(xhat * wdy, axis=0) / N\n+    c2 = tl.sum(wdy, axis=0) / N\n+    dx = (wdy - (xhat * c1 + c2)) * rstd\n+    # Write dx\n+    tl.store(DX + cols, dx, mask=mask)\n+    # Accumulate partial sums for dw/db\n+    partial_dw = (dy * xhat).to(w.dtype)\n+    partial_db = (dy).to(w.dtype)\n+    while tl.atomic_cas(Lock, 0, 1) == 1:\n+        pass\n+    count = tl.load(Count)\n+    # First store doesn't accumulate\n+    if count == 0:\n+        tl.atomic_xchg(Count, 1)\n+    else:\n+        partial_dw += tl.load(DW, mask=mask)\n+        partial_db += tl.load(DB, mask=mask)\n+    tl.store(DW, partial_dw, mask=mask)\n+    tl.store(DB, partial_db, mask=mask)\n+    # Release the lock\n+    tl.atomic_xchg(Lock, 0)\n+\n+\n+@triton.jit\n+def _layer_norm_bwd_dwdb(\n+    DW,  # pointer to the partial sum of weights gradient\n+    DB,  # pointer to the partial sum of biases gradient\n+    FINAL_DW,  # pointer to the weights gradient\n+    FINAL_DB,  # pointer to the biases gradient\n+    M,  # GROUP_SIZE_M\n+    N,  # number of columns\n+    BLOCK_SIZE_M: tl.constexpr,\n+    BLOCK_SIZE_N: tl.constexpr\n+):\n+    # Map the program id to the elements of DW and DB it should compute.\n+    pid = tl.program_id(0)\n+    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+    db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+    # Iterate through the rows of DW and DB to sum the partial sums.\n+    for i in range(0, M, BLOCK_SIZE_M):\n+        rows = i + tl.arange(0, BLOCK_SIZE_M)\n+        mask = (rows[:, None] < M) & (cols[None, :] < N)\n+        offs = rows[:, None] * N + cols[None, :]\n+        dw += tl.load(DW + offs, mask=mask, other=0.)\n+        db += tl.load(DB + offs, mask=mask, other=0.)\n+    # Write the final sum to the output.\n+    sum_dw = tl.sum(dw, axis=0)\n+    sum_db = tl.sum(db, axis=0)\n+    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n+    tl.store(FINAL_DB + cols, sum_db, mask=cols < N)\n+\n+\n+# %%\n+# Benchmark\n+# ---------\n+#\n+# We can now compare the performance of our kernel against that of PyTorch.\n+# Here we focus on inputs that have Less than 64KB per feature.\n+# Specifically, one can set :code:`'mode': 'backward'` to benchmark the backward pass.\n+\n+\n+class LayerNorm(torch.autograd.Function):\n+\n+    @staticmethod\n+    def forward(ctx, x, normalized_shape, weight, bias, eps):\n+        # allocate output\n+        y = torch.empty_like(x)\n+        # reshape input data into 2D tensor\n+        x_arg = x.reshape(-1, x.shape[-1])\n+        M, N = x_arg.shape\n+        mean = torch.empty((M, ), dtype=torch.float32, device='cuda')\n+        rstd = torch.empty((M, ), dtype=torch.float32, device='cuda')\n+        # Less than 64KB per feature: enqueue fused kernel\n+        MAX_FUSED_SIZE = 65536 // x.element_size()\n+        BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n+        if N > BLOCK_SIZE:\n+            raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n+        # heuristics for number of warps\n+        num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\n+        # enqueue kernel\n+        _layer_norm_fwd_fused[(M,)](x_arg, y, weight, bias, mean, rstd,\n+                                    x_arg.stride(0), N, eps,\n+                                    BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps, num_ctas=1)\n+        ctx.save_for_backward(x, weight, bias, mean, rstd)\n+        ctx.BLOCK_SIZE = BLOCK_SIZE\n+        ctx.num_warps = num_warps\n+        ctx.eps = eps\n+        return y\n+\n+    @staticmethod\n+    def backward(ctx, dy):\n+        x, w, b, m, v = ctx.saved_tensors\n+        # heuristics for amount of parallel reduction stream for DW/DB\n+        N = w.shape[0]\n+        GROUP_SIZE_M = 64\n+        if N <= 8192: GROUP_SIZE_M = 96\n+        if N <= 4096: GROUP_SIZE_M = 128\n+        if N <= 1024: GROUP_SIZE_M = 256\n+        # allocate output\n+        locks = torch.zeros(2 * GROUP_SIZE_M, dtype=torch.int32, device='cuda')\n+        _dw = torch.empty((GROUP_SIZE_M, w.shape[0]), dtype=x.dtype, device=w.device)\n+        _db = torch.empty((GROUP_SIZE_M, w.shape[0]), dtype=x.dtype, device=w.device)\n+        dw = torch.empty((w.shape[0],), dtype=w.dtype, device=w.device)\n+        db = torch.empty((w.shape[0],), dtype=w.dtype, device=w.device)\n+        dx = torch.empty_like(dy)\n+        # enqueue kernel using forward pass heuristics\n+        # also compute partial sums for DW and DB\n+        x_arg = x.reshape(-1, x.shape[-1])\n+        M, N = x_arg.shape\n+        _layer_norm_bwd_dx_fused[(M,)](dx, dy, _dw, _db, x, w, b, m, v, locks,\n+                                       x_arg.stride(0), N, ctx.eps,\n+                                       BLOCK_SIZE_N=ctx.BLOCK_SIZE,\n+                                       GROUP_SIZE_M=GROUP_SIZE_M,\n+                                       num_warps=ctx.num_warps)\n+        grid = lambda meta: [triton.cdiv(N, meta['BLOCK_SIZE_N'])]\n+        # accumulate partial sums in separate kernel\n+        _layer_norm_bwd_dwdb[grid](_dw, _db, dw, db, GROUP_SIZE_M, N,\n+                                   BLOCK_SIZE_M=32,\n+                                   BLOCK_SIZE_N=128, num_ctas=1)\n+        return dx, None, dw, db, None\n+\n+\n+layer_norm = LayerNorm.apply\n+\n+\n+def test_layer_norm(M, N, dtype, eps=1e-5, device='cuda'):\n+    # create data\n+    x_shape = (M, N)\n+    w_shape = (x_shape[-1], )\n+    weight = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n+    bias = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n+    x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\n+    dy = .1 * torch.randn_like(x)\n+    x.requires_grad_(True)\n+    # forward pass\n+    y_tri = layer_norm(x, w_shape, weight, bias, eps)\n+    y_ref = torch.nn.functional.layer_norm(x, w_shape, weight, bias, eps).to(dtype)\n+    # backward pass (triton)\n+    y_tri.backward(dy, retain_graph=True)\n+    dx_tri, dw_tri, db_tri = [_.grad.clone() for _ in [x, weight, bias]]\n+    x.grad, weight.grad, bias.grad = None, None, None\n+    # backward pass (torch)\n+    y_ref.backward(dy, retain_graph=True)\n+    dx_ref, dw_ref, db_ref = [_.grad.clone() for _ in [x, weight, bias]]\n+    # compare\n+    assert torch.allclose(y_tri, y_ref, atol=1e-2, rtol=0)\n+    assert torch.allclose(dx_tri, dx_ref, atol=1e-2, rtol=0)\n+    assert torch.allclose(db_tri, db_ref, atol=1e-2, rtol=0)\n+    assert torch.allclose(dw_tri, dw_ref, atol=1e-2, rtol=0)\n+\n+\n+@triton.testing.perf_report(\n+    triton.testing.Benchmark(\n+        x_names=['N'],\n+        x_vals=[512 * i for i in range(2, 32)],\n+        line_arg='provider',\n+        line_vals=['triton', 'torch'] + (['apex'] if HAS_APEX else []),\n+        line_names=['Triton', 'Torch'] + (['Apex'] if HAS_APEX else []),\n+        styles=[('blue', '-'), ('green', '-'), ('orange', '-')],\n+        ylabel='GB/s',\n+        plot_name='layer-norm-backward',\n+        args={'M': 4096, 'dtype': torch.float16, 'mode': 'backward'}\n+    )\n+)\n+def bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='cuda'):\n+    # create data\n+    x_shape = (M, N)\n+    w_shape = (x_shape[-1], )\n+    weight = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n+    bias = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n+    x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\n+    dy = .1 * torch.randn_like(x)\n+    x.requires_grad_(True)\n+    quantiles = [0.5, 0.2, 0.8]\n+    # utility functions\n+    if provider == 'triton':\n+        def y_fwd(): return layer_norm(x, w_shape, weight, bias, eps)  # noqa: F811, E704\n+    if provider == 'torch':\n+        def y_fwd(): return torch.nn.functional.layer_norm(x, w_shape, weight, bias, eps)  # noqa: F811, E704\n+    if provider == 'apex':\n+        apex_layer_norm = apex.normalization.FusedLayerNorm(\n+            w_shape).to(x.device).to(x.dtype)\n+\n+        def y_fwd(): return apex_layer_norm(x)  # noqa: F811, E704\n+    # forward pass\n+    if mode == 'forward':\n+        gbps = lambda ms: 2 * x.numel() * x.element_size() / ms * 1e-6\n+        ms, min_ms, max_ms = triton.testing.do_bench(y_fwd, quantiles=quantiles, rep=500)\n+    # backward pass\n+    if mode == 'backward':\n+        def gbps(ms): return 3 * x.numel() * x.element_size() / ms * 1e-6  # noqa: F811, E704\n+        y = y_fwd()\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: y.backward(dy, retain_graph=True),\n+                                                     quantiles=quantiles, grad_to_none=[x], rep=500)\n+    return gbps(ms), gbps(max_ms), gbps(min_ms)\n+\n+\n+test_layer_norm(1151, 8192, torch.float16)\n+bench_layer_norm.run(save_path='.', print_data=True)\n+\n+# %%\n+# References\n+# ----------\n+#\n+# .. [BA2016] Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \"Layer Normalization\", Arxiv 2016"}, {"filename": "main/_downloads/9705af6f33c6e66c6fa78f2394331536/08-experimental-block-pointer.ipynb", "status": "added", "additions": 107, "deletions": 0, "changes": 107, "file_content_changes": "@@ -0,0 +1,107 @@\n+{\n+  \"cells\": [\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"%matplotlib inline\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"\\n# Block Pointer (Experimental)\\nThis tutorial will guide you through writing a matrix multiplication algorithm that utilizes block pointer semantics.\\nThese semantics are more friendly for Triton to optimize and can result in better performance on specific hardware.\\nNote that this feature is still experimental and may change in the future.\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Motivations\\nIn the previous matrix multiplication tutorial, we constructed blocks of values by de-referencing blocks of pointers,\\ni.e., :code:`load(block<pointer_type<element_type>>) -> block<element_type>`, which involved loading blocks of\\nelements from memory. This approach allowed for flexibility in using hardware-managed cache and implementing complex\\ndata structures, such as tensors of trees or unstructured look-up tables.\\n\\nHowever, the drawback of this approach is that it relies heavily on complex optimization passes by the compiler to\\noptimize memory access patterns. This can result in brittle code that may suffer from performance degradation when the\\noptimizer fails to perform adequately. Additionally, as memory controllers specialize to accommodate dense spatial\\ndata structures commonly used in machine learning workloads, this problem is likely to worsen.\\n\\nTo address this issue, we will use block pointers :code:`pointer_type<block<element_type>>` and load them into\\n:code:`block<element_type>`, in which way gives better friendliness for the compiler to optimize memory access\\npatterns.\\n\\nLet's start with the previous matrix multiplication example and demonstrate how to rewrite it to utilize block pointer\\nsemantics.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Make a Block Pointer\\nA block pointer pointers to a block in a parent tensor and is constructed by :code:`make_block_ptr` function,\\nwhich takes the following information as arguments:\\n\\n* :code:`base`: the base pointer to the parent tensor;\\n\\n* :code:`shape`: the shape of the parent tensor;\\n\\n* :code:`strides`: the strides of the parent tensor, which means how much to increase the pointer by when moving by 1 element in a specific axis;\\n\\n* :code:`offsets`: the offsets of the block;\\n\\n* :code:`block_shape`: the shape of the block;\\n\\n* :code:`order`: the order of the block, which means how the block is laid out in memory.\\n\\nFor example, to a block pointer to a :code:`BLOCK_SIZE_M * BLOCK_SIZE_K` block in a row-major 2D matrix A by\\noffsets :code:`(pid_m * BLOCK_SIZE_M, 0)` and strides :code:`(stride_am, stride_ak)`, we can use the following code\\n(exactly the same as the previous matrix multiplication tutorial):\\n\\n```python\\na_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\\n                                offsets=(pid_m * BLOCK_SIZE_M, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K),\\n                                order=(1, 0))\\n```\\nNote that the :code:`order` argument is set to :code:`(1, 0)`, which means the second axis is the inner dimension in\\nterms of storage, and the first axis is the outer dimension. This information may sound redundant, but it is necessary\\nfor some hardware backends to optimize for better performance.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Load/Store a Block Pointer\\nTo load/store a block pointer, we can use :code:`load/store` function, which takes a block pointer as an argument,\\nde-references it, and loads/stores a block. You may mask some values in the block, here we have an extra argument\\n:code:`boundary_check` to specify whether to check the boundary of each axis for the block pointer. With check on,\\nout-of-bound values will be masked according to the :code:`padding_option` argument (load only), which can be\\n:code:`zero` or :code:`nan`. Temporarily, we do not support other values due to some hardware limitations. In this\\nmode of block pointer load/store does not support :code:`mask` or :code:`other` arguments in the legacy mode.\\n\\nSo to load the block pointer of A in the previous section, we can simply write\\n:code:`a = tl.load(a_block_ptr, boundary_check=(0, 1))`. Boundary check may cost extra performance, so if you can\\nguarantee that the block pointer is always in-bound in some axis, you can turn off the check by not passing the index\\ninto the :code:`boundary_check` argument. For example, if we know that :code:`M` is a multiple of\\n:code:`BLOCK_SIZE_M`, we can replace with :code:`a = tl.load(a_block_ptr, boundary_check=(1, ))`, since axis 0 is\\nalways in bound.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Advance a Block Pointer\\nTo advance a block pointer, we can use :code:`advance` function, which takes a block pointer and the increment for\\neach axis as arguments and returns a new block pointer with the same shape and strides as the original one,\\nbut with the offsets advanced by the specified amount.\\n\\nFor example, to advance the block pointer by :code:`BLOCK_SIZE_K` in the second axis\\n(no need to multiply with strides), we can write :code:`a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))`.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Final Result\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.autotune(\\n    configs=[\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\\n    ],\\n    key=['M', 'N', 'K'],\\n)\\n@triton.jit\\ndef matmul_kernel_with_block_pointers(\\n        # Pointers to matrices\\n        a_ptr, b_ptr, c_ptr,\\n        # Matrix dimensions\\n        M, N, K,\\n        # The stride variables represent how much to increase the ptr by when moving by 1\\n        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\\n        # by to get the element one row down (A has M rows).\\n        stride_am, stride_ak,\\n        stride_bk, stride_bn,\\n        stride_cm, stride_cn,\\n        # Meta-parameters\\n        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\\n        GROUP_SIZE_M: tl.constexpr\\n):\\n    \\\"\\\"\\\"Kernel for computing the matmul C = A x B.\\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\\n    \\\"\\\"\\\"\\n    # -----------------------------------------------------------\\n    # Map program ids `pid` to the block of C it should compute.\\n    # This is done in a grouped ordering to promote L2 data reuse.\\n    # See the matrix multiplication tutorial for details.\\n    pid = tl.program_id(axis=0)\\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\\n    group_id = pid // num_pid_in_group\\n    first_pid_m = group_id * GROUP_SIZE_M\\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\\n    pid_m = first_pid_m + (pid % group_size_m)\\n    pid_n = (pid % num_pid_in_group) // group_size_m\\n\\n    # ----------------------------------------------------------\\n    # Create block pointers for the first blocks of A and B.\\n    # We will advance this pointer as we move in the K direction and accumulate.\\n    # See above `Make a Block Pointer` section for details.\\n    a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\\n                                    offsets=(pid_m * BLOCK_SIZE_M, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K),\\n                                    order=(1, 0))\\n    b_block_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(stride_bk, stride_bn),\\n                                    offsets=(0, pid_n * BLOCK_SIZE_N), block_shape=(BLOCK_SIZE_K, BLOCK_SIZE_N),\\n                                    order=(1, 0))\\n\\n    # -----------------------------------------------------------\\n    # Iterate to compute a block of the C matrix.\\n    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block.\\n    # of fp32 values for higher accuracy.\\n    # `accumulator` will be converted back to fp16 after the loop.\\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\\n    for k in range(0, K, BLOCK_SIZE_K):\\n        # Load with boundary checks, no need to calculate the mask manually.\\n        # For better performance, you may remove some axis from the boundary\\n        # check, if you can guarantee that the access is always in-bound in\\n        # that axis.\\n        # See above `Load/Store a Block Pointer` section for details.\\n        a = tl.load(a_block_ptr, boundary_check=(0, 1))\\n        b = tl.load(b_block_ptr, boundary_check=(0, 1))\\n        # We accumulate along the K dimension.\\n        accumulator += tl.dot(a, b)\\n        # Advance the block pointer to the next K block.\\n        # See above `Advance a Block Pointer` section for details.\\n        a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))\\n        b_block_ptr = tl.advance(b_block_ptr, (BLOCK_SIZE_K, 0))\\n    c = accumulator.to(tl.float16)\\n\\n    # ----------------------------------------------------------------\\n    # Write back the block of the output matrix C with boundary checks.\\n    # See above `Load/Store a Block Pointer` section for details.\\n    c_block_ptr = tl.make_block_ptr(base=c_ptr, shape=(M, N), strides=(stride_cm, stride_cn),\\n                                    offsets=(pid_m * BLOCK_SIZE_M, pid_n * BLOCK_SIZE_N),\\n                                    block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_N), order=(1, 0))\\n    tl.store(c_block_ptr, c, boundary_check=(0, 1))\\n\\n\\n# We can now create a convenience wrapper function that only takes two input tensors,\\n# and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\\ndef matmul(a, b):\\n    # Check constraints.\\n    assert a.shape[1] == b.shape[0], \\\"Incompatible dimensions\\\"\\n    assert a.is_contiguous(), \\\"Matrix A must be contiguous\\\"\\n    assert b.is_contiguous(), \\\"Matrix B must be contiguous\\\"\\n    M, K = a.shape\\n    K, N = b.shape\\n    # Allocates output.\\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\\n    # 1D launch kernel where each block gets its own program.\\n    grid = lambda META: (\\n        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\\n    )\\n    matmul_kernel_with_block_pointers[grid](\\n        a, b, c,\\n        M, N, K,\\n        a.stride(0), a.stride(1),\\n        b.stride(0), b.stride(1),\\n        c.stride(0), c.stride(1),\\n    )\\n    return c\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Unit Test\\n\\nStill we can test our matrix multiplication with block pointers against a native torch implementation (i.e., cuBLAS).\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"torch.manual_seed(0)\\na = torch.randn((512, 512), device='cuda', dtype=torch.float16)\\nb = torch.randn((512, 512), device='cuda', dtype=torch.float16)\\ntriton_output = matmul(a, b)\\ntorch_output = torch.matmul(a, b)\\nprint(f\\\"triton_output={triton_output}\\\")\\nprint(f\\\"torch_output={torch_output}\\\")\\nif torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\\n    print(\\\"\\u2705 Triton and Torch match\\\")\\nelse:\\n    print(\\\"\\u274c Triton and Torch differ\\\")\"\n+      ]\n+    }\n+  ],\n+  \"metadata\": {\n+    \"kernelspec\": {\n+      \"display_name\": \"Python 3\",\n+      \"language\": \"python\",\n+      \"name\": \"python3\"\n+    },\n+    \"language_info\": {\n+      \"codemirror_mode\": {\n+        \"name\": \"ipython\",\n+        \"version\": 3\n+      },\n+      \"file_extension\": \".py\",\n+      \"mimetype\": \"text/x-python\",\n+      \"name\": \"python\",\n+      \"nbconvert_exporter\": \"python\",\n+      \"pygments_lexer\": \"ipython3\",\n+      \"version\": \"3.8.10\"\n+    }\n+  },\n+  \"nbformat\": 4,\n+  \"nbformat_minor\": 0\n+}\n\\ No newline at end of file"}, {"filename": "main/_downloads/ae7fff29e1b574187bc930ed94bcc353/05-layer-norm.ipynb", "status": "added", "additions": 104, "deletions": 0, "changes": 104, "file_content_changes": "@@ -0,0 +1,104 @@\n+{\n+  \"cells\": [\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"%matplotlib inline\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"\\n# Layer Normalization\\nIn this tutorial, you will write a high-performance layer normalization\\nkernel that runs faster than the PyTorch implementation.\\n\\nIn doing so, you will learn about:\\n\\n* Implementing backward pass in Triton.\\n\\n* Implementing parallel reduction in Triton.\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Motivations\\n\\nThe *LayerNorm* operator was first introduced in [BA2016]_ as a way to improve the performance\\nof sequential models (e.g., Transformers) or neural networks with small batch size.\\nIt takes a vector $x$ as input and produces a vector $y$ of the same shape as output.\\nThe normalization is performed by subtracting the mean and dividing by the standard deviation of $x$.\\nAfter the normalization, a learnable linear transformation with weights $w$ and biases $b$ is applied.\\nThe forward pass can be expressed as follows:\\n\\n\\\\begin{align}y = \\\\frac{ x - \\\\text{E}[x] }{ \\\\sqrt{\\\\text{Var}(x) + \\\\epsilon} } * w + b\\\\end{align}\\n\\nwhere $\\\\epsilon$ is a small constant added to the denominator for numerical stability.\\nLet\\u2019s first take a look at the forward pass implementation.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\ntry:\\n    # This is https://github.com/NVIDIA/apex, NOT the apex on PyPi, so it\\n    # should not be added to extras_require in setup.py.\\n    import apex\\n    HAS_APEX = True\\nexcept ModuleNotFoundError:\\n    HAS_APEX = False\\n\\n\\n@triton.jit\\ndef _layer_norm_fwd_fused(\\n    X,  # pointer to the input\\n    Y,  # pointer to the output\\n    W,  # pointer to the weights\\n    B,  # pointer to the biases\\n    Mean,  # pointer to the mean\\n    Rstd,  # pointer to the 1/std\\n    stride,  # how much to increase the pointer when moving by 1 row\\n    N,  # number of columns in X\\n    eps,  # epsilon to avoid division by zero\\n    BLOCK_SIZE: tl.constexpr,\\n):\\n    # Map the program id to the row of X and Y it should compute.\\n    row = tl.program_id(0)\\n    Y += row * stride\\n    X += row * stride\\n    # Compute mean\\n    mean = 0\\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\\n    for off in range(0, N, BLOCK_SIZE):\\n        cols = off + tl.arange(0, BLOCK_SIZE)\\n        a = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\\n        _mean += a\\n    mean = tl.sum(_mean, axis=0) / N\\n    # Compute variance\\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\\n    for off in range(0, N, BLOCK_SIZE):\\n        cols = off + tl.arange(0, BLOCK_SIZE)\\n        x = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\\n        x = tl.where(cols < N, x - mean, 0.)\\n        _var += x * x\\n    var = tl.sum(_var, axis=0) / N\\n    rstd = 1 / tl.sqrt(var + eps)\\n    # Write mean / rstd\\n    tl.store(Mean + row, mean)\\n    tl.store(Rstd + row, rstd)\\n    # Normalize and apply linear transformation\\n    for off in range(0, N, BLOCK_SIZE):\\n        cols = off + tl.arange(0, BLOCK_SIZE)\\n        mask = cols < N\\n        w = tl.load(W + cols, mask=mask)\\n        b = tl.load(B + cols, mask=mask)\\n        x = tl.load(X + cols, mask=mask, other=0.).to(tl.float32)\\n        x_hat = (x - mean) * rstd\\n        y = x_hat * w + b\\n        # Write output\\n        tl.store(Y + cols, y, mask=mask)\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Backward pass\\n\\nThe backward pass for the layer normalization operator is a bit more involved than the forward pass.\\nLet $\\\\hat{x}$ be the normalized inputs $\\\\frac{ x - \\\\text{E}[x] }{ \\\\sqrt{\\\\text{Var}(x) + \\\\epsilon} }$ before the linear transformation,\\nthe Vector-Jacobian Products (VJP) $\\\\nabla_{x}$ of $x$ are given by:\\n\\n\\\\begin{align}\\\\nabla_{x} = \\\\frac{1}{\\\\sigma}\\\\Big( \\\\nabla_{y} \\\\odot w - \\\\underbrace{ \\\\big( \\\\frac{1}{N} \\\\hat{x} \\\\cdot (\\\\nabla_{y} \\\\odot w) \\\\big) }_{c_1} \\\\odot \\\\hat{x} - \\\\underbrace{ \\\\frac{1}{N} \\\\nabla_{y} \\\\cdot w }_{c_2} \\\\Big)\\\\end{align}\\n\\nwhere $\\\\odot$ denotes the element-wise multiplication, $\\\\cdot$ denotes the dot product, and $\\\\sigma$ is the standard deviation.\\n$c_1$ and $c_2$ are intermediate constants that improve the readability of the following implementation.\\n\\nFor the weights $w$ and biases $b$, the VJPs $\\\\nabla_{w}$ and $\\\\nabla_{b}$ are more straightforward:\\n\\n\\\\begin{align}\\\\nabla_{w} = \\\\nabla_{y} \\\\odot \\\\hat{x} \\\\quad \\\\text{and} \\\\quad \\\\nabla_{b} = \\\\nabla_{y}\\\\end{align}\\n\\nSince the same weights $w$ and biases $b$ are used for all rows in the same batch, their gradients need to sum up.\\nTo perform this step efficiently, we use a parallel reduction strategy: each kernel instance accumulates\\npartial $\\\\nabla_{w}$ and $\\\\nabla_{b}$ across certain rows into one of $\\\\text{GROUP_SIZE_M}$ independent buffers.\\nThese buffers stay in the L2 cache and then are further reduced by another function to compute the actual $\\\\nabla_{w}$ and $\\\\nabla_{b}$.\\n\\nLet the number of input rows $M = 4$ and $\\\\text{GROUP_SIZE_M} = 2$,\\nhere's a diagram of the parallel reduction strategy for $\\\\nabla_{w}$ ($\\\\nabla_{b}$ is omitted for brevity):\\n\\n  .. image:: parallel_reduction.png\\n\\nIn Stage 1, the rows of X that have the same color share the same buffer and thus a lock is used to ensure that only one kernel instance writes to the buffer at a time.\\nIn Stage 2, the buffers are further reduced to compute the final $\\\\nabla_{w}$ and $\\\\nabla_{b}$.\\nIn the following implementation, Stage 1 is implemented by the function :code:`_layer_norm_bwd_dx_fused` and Stage 2 is implemented by the function :code:`_layer_norm_bwd_dwdb`.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"@triton.jit\\ndef _layer_norm_bwd_dx_fused(\\n    DX,  # pointer to the input gradient\\n    DY,  # pointer to the output gradient\\n    DW,  # pointer to the partial sum of weights gradient\\n    DB,  # pointer to the partial sum of biases gradient\\n    X,   # pointer to the input\\n    W,   # pointer to the weights\\n    B,   # pointer to the biases\\n    Mean,   # pointer to the mean\\n    Rstd,   # pointer to the 1/std\\n    Lock,  # pointer to the lock\\n    stride,  # how much to increase the pointer when moving by 1 row\\n    N,  # number of columns in X\\n    eps,  # epsilon to avoid division by zero\\n    GROUP_SIZE_M: tl.constexpr,\\n    BLOCK_SIZE_N: tl.constexpr\\n):\\n    # Map the program id to the elements of X, DX, and DY it should compute.\\n    row = tl.program_id(0)\\n    cols = tl.arange(0, BLOCK_SIZE_N)\\n    mask = cols < N\\n    X += row * stride\\n    DY += row * stride\\n    DX += row * stride\\n    # Offset locks and weights/biases gradient pointer for parallel reduction\\n    lock_id = row % GROUP_SIZE_M\\n    Lock += lock_id\\n    Count = Lock + GROUP_SIZE_M\\n    DW = DW + lock_id * N + cols\\n    DB = DB + lock_id * N + cols\\n    # Load data to SRAM\\n    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\\n    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\\n    mean = tl.load(Mean + row)\\n    rstd = tl.load(Rstd + row)\\n    # Compute dx\\n    xhat = (x - mean) * rstd\\n    wdy = w * dy\\n    xhat = tl.where(mask, xhat, 0.)\\n    wdy = tl.where(mask, wdy, 0.)\\n    c1 = tl.sum(xhat * wdy, axis=0) / N\\n    c2 = tl.sum(wdy, axis=0) / N\\n    dx = (wdy - (xhat * c1 + c2)) * rstd\\n    # Write dx\\n    tl.store(DX + cols, dx, mask=mask)\\n    # Accumulate partial sums for dw/db\\n    partial_dw = (dy * xhat).to(w.dtype)\\n    partial_db = (dy).to(w.dtype)\\n    while tl.atomic_cas(Lock, 0, 1) == 1:\\n        pass\\n    count = tl.load(Count)\\n    # First store doesn't accumulate\\n    if count == 0:\\n        tl.atomic_xchg(Count, 1)\\n    else:\\n        partial_dw += tl.load(DW, mask=mask)\\n        partial_db += tl.load(DB, mask=mask)\\n    tl.store(DW, partial_dw, mask=mask)\\n    tl.store(DB, partial_db, mask=mask)\\n    # Release the lock\\n    tl.atomic_xchg(Lock, 0)\\n\\n\\n@triton.jit\\ndef _layer_norm_bwd_dwdb(\\n    DW,  # pointer to the partial sum of weights gradient\\n    DB,  # pointer to the partial sum of biases gradient\\n    FINAL_DW,  # pointer to the weights gradient\\n    FINAL_DB,  # pointer to the biases gradient\\n    M,  # GROUP_SIZE_M\\n    N,  # number of columns\\n    BLOCK_SIZE_M: tl.constexpr,\\n    BLOCK_SIZE_N: tl.constexpr\\n):\\n    # Map the program id to the elements of DW and DB it should compute.\\n    pid = tl.program_id(0)\\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\\n    db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\\n    # Iterate through the rows of DW and DB to sum the partial sums.\\n    for i in range(0, M, BLOCK_SIZE_M):\\n        rows = i + tl.arange(0, BLOCK_SIZE_M)\\n        mask = (rows[:, None] < M) & (cols[None, :] < N)\\n        offs = rows[:, None] * N + cols[None, :]\\n        dw += tl.load(DW + offs, mask=mask, other=0.)\\n        db += tl.load(DB + offs, mask=mask, other=0.)\\n    # Write the final sum to the output.\\n    sum_dw = tl.sum(dw, axis=0)\\n    sum_db = tl.sum(db, axis=0)\\n    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\\n    tl.store(FINAL_DB + cols, sum_db, mask=cols < N)\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Benchmark\\n\\nWe can now compare the performance of our kernel against that of PyTorch.\\nHere we focus on inputs that have Less than 64KB per feature.\\nSpecifically, one can set :code:`'mode': 'backward'` to benchmark the backward pass.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"class LayerNorm(torch.autograd.Function):\\n\\n    @staticmethod\\n    def forward(ctx, x, normalized_shape, weight, bias, eps):\\n        # allocate output\\n        y = torch.empty_like(x)\\n        # reshape input data into 2D tensor\\n        x_arg = x.reshape(-1, x.shape[-1])\\n        M, N = x_arg.shape\\n        mean = torch.empty((M, ), dtype=torch.float32, device='cuda')\\n        rstd = torch.empty((M, ), dtype=torch.float32, device='cuda')\\n        # Less than 64KB per feature: enqueue fused kernel\\n        MAX_FUSED_SIZE = 65536 // x.element_size()\\n        BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\\n        if N > BLOCK_SIZE:\\n            raise RuntimeError(\\\"This layer norm doesn't support feature dim >= 64KB.\\\")\\n        # heuristics for number of warps\\n        num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\\n        # enqueue kernel\\n        _layer_norm_fwd_fused[(M,)](x_arg, y, weight, bias, mean, rstd,\\n                                    x_arg.stride(0), N, eps,\\n                                    BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps, num_ctas=1)\\n        ctx.save_for_backward(x, weight, bias, mean, rstd)\\n        ctx.BLOCK_SIZE = BLOCK_SIZE\\n        ctx.num_warps = num_warps\\n        ctx.eps = eps\\n        return y\\n\\n    @staticmethod\\n    def backward(ctx, dy):\\n        x, w, b, m, v = ctx.saved_tensors\\n        # heuristics for amount of parallel reduction stream for DW/DB\\n        N = w.shape[0]\\n        GROUP_SIZE_M = 64\\n        if N <= 8192: GROUP_SIZE_M = 96\\n        if N <= 4096: GROUP_SIZE_M = 128\\n        if N <= 1024: GROUP_SIZE_M = 256\\n        # allocate output\\n        locks = torch.zeros(2 * GROUP_SIZE_M, dtype=torch.int32, device='cuda')\\n        _dw = torch.empty((GROUP_SIZE_M, w.shape[0]), dtype=x.dtype, device=w.device)\\n        _db = torch.empty((GROUP_SIZE_M, w.shape[0]), dtype=x.dtype, device=w.device)\\n        dw = torch.empty((w.shape[0],), dtype=w.dtype, device=w.device)\\n        db = torch.empty((w.shape[0],), dtype=w.dtype, device=w.device)\\n        dx = torch.empty_like(dy)\\n        # enqueue kernel using forward pass heuristics\\n        # also compute partial sums for DW and DB\\n        x_arg = x.reshape(-1, x.shape[-1])\\n        M, N = x_arg.shape\\n        _layer_norm_bwd_dx_fused[(M,)](dx, dy, _dw, _db, x, w, b, m, v, locks,\\n                                       x_arg.stride(0), N, ctx.eps,\\n                                       BLOCK_SIZE_N=ctx.BLOCK_SIZE,\\n                                       GROUP_SIZE_M=GROUP_SIZE_M,\\n                                       num_warps=ctx.num_warps)\\n        grid = lambda meta: [triton.cdiv(N, meta['BLOCK_SIZE_N'])]\\n        # accumulate partial sums in separate kernel\\n        _layer_norm_bwd_dwdb[grid](_dw, _db, dw, db, GROUP_SIZE_M, N,\\n                                   BLOCK_SIZE_M=32,\\n                                   BLOCK_SIZE_N=128, num_ctas=1)\\n        return dx, None, dw, db, None\\n\\n\\nlayer_norm = LayerNorm.apply\\n\\n\\ndef test_layer_norm(M, N, dtype, eps=1e-5, device='cuda'):\\n    # create data\\n    x_shape = (M, N)\\n    w_shape = (x_shape[-1], )\\n    weight = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\\n    bias = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\\n    x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\\n    dy = .1 * torch.randn_like(x)\\n    x.requires_grad_(True)\\n    # forward pass\\n    y_tri = layer_norm(x, w_shape, weight, bias, eps)\\n    y_ref = torch.nn.functional.layer_norm(x, w_shape, weight, bias, eps).to(dtype)\\n    # backward pass (triton)\\n    y_tri.backward(dy, retain_graph=True)\\n    dx_tri, dw_tri, db_tri = [_.grad.clone() for _ in [x, weight, bias]]\\n    x.grad, weight.grad, bias.grad = None, None, None\\n    # backward pass (torch)\\n    y_ref.backward(dy, retain_graph=True)\\n    dx_ref, dw_ref, db_ref = [_.grad.clone() for _ in [x, weight, bias]]\\n    # compare\\n    assert torch.allclose(y_tri, y_ref, atol=1e-2, rtol=0)\\n    assert torch.allclose(dx_tri, dx_ref, atol=1e-2, rtol=0)\\n    assert torch.allclose(db_tri, db_ref, atol=1e-2, rtol=0)\\n    assert torch.allclose(dw_tri, dw_ref, atol=1e-2, rtol=0)\\n\\n\\n@triton.testing.perf_report(\\n    triton.testing.Benchmark(\\n        x_names=['N'],\\n        x_vals=[512 * i for i in range(2, 32)],\\n        line_arg='provider',\\n        line_vals=['triton', 'torch'] + (['apex'] if HAS_APEX else []),\\n        line_names=['Triton', 'Torch'] + (['Apex'] if HAS_APEX else []),\\n        styles=[('blue', '-'), ('green', '-'), ('orange', '-')],\\n        ylabel='GB/s',\\n        plot_name='layer-norm-backward',\\n        args={'M': 4096, 'dtype': torch.float16, 'mode': 'backward'}\\n    )\\n)\\ndef bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='cuda'):\\n    # create data\\n    x_shape = (M, N)\\n    w_shape = (x_shape[-1], )\\n    weight = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\\n    bias = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\\n    x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\\n    dy = .1 * torch.randn_like(x)\\n    x.requires_grad_(True)\\n    quantiles = [0.5, 0.2, 0.8]\\n    # utility functions\\n    if provider == 'triton':\\n        def y_fwd(): return layer_norm(x, w_shape, weight, bias, eps)  # noqa: F811, E704\\n    if provider == 'torch':\\n        def y_fwd(): return torch.nn.functional.layer_norm(x, w_shape, weight, bias, eps)  # noqa: F811, E704\\n    if provider == 'apex':\\n        apex_layer_norm = apex.normalization.FusedLayerNorm(\\n            w_shape).to(x.device).to(x.dtype)\\n\\n        def y_fwd(): return apex_layer_norm(x)  # noqa: F811, E704\\n    # forward pass\\n    if mode == 'forward':\\n        gbps = lambda ms: 2 * x.numel() * x.element_size() / ms * 1e-6\\n        ms, min_ms, max_ms = triton.testing.do_bench(y_fwd, quantiles=quantiles, rep=500)\\n    # backward pass\\n    if mode == 'backward':\\n        def gbps(ms): return 3 * x.numel() * x.element_size() / ms * 1e-6  # noqa: F811, E704\\n        y = y_fwd()\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: y.backward(dy, retain_graph=True),\\n                                                     quantiles=quantiles, grad_to_none=[x], rep=500)\\n    return gbps(ms), gbps(max_ms), gbps(min_ms)\\n\\n\\ntest_layer_norm(1151, 8192, torch.float16)\\nbench_layer_norm.run(save_path='.', print_data=True)\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## References\\n\\n.. [BA2016] Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \\\"Layer Normalization\\\", Arxiv 2016\\n\\n\"\n+      ]\n+    }\n+  ],\n+  \"metadata\": {\n+    \"kernelspec\": {\n+      \"display_name\": \"Python 3\",\n+      \"language\": \"python\",\n+      \"name\": \"python3\"\n+    },\n+    \"language_info\": {\n+      \"codemirror_mode\": {\n+        \"name\": \"ipython\",\n+        \"version\": 3\n+      },\n+      \"file_extension\": \".py\",\n+      \"mimetype\": \"text/x-python\",\n+      \"name\": \"python\",\n+      \"nbconvert_exporter\": \"python\",\n+      \"pygments_lexer\": \"ipython3\",\n+      \"version\": \"3.8.10\"\n+    }\n+  },\n+  \"nbformat\": 4,\n+  \"nbformat_minor\": 0\n+}\n\\ No newline at end of file"}, {"filename": "main/_downloads/b51b68bc1c6b1a5e509f67800b6235af/03-matrix-multiplication.ipynb", "status": "added", "additions": 129, "deletions": 0, "changes": 129, "file_content_changes": "@@ -0,0 +1,129 @@\n+{\n+  \"cells\": [\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"%matplotlib inline\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"\\n# Matrix Multiplication\\nIn this tutorial, you will write a very short high-performance FP16 matrix multiplication kernel that achieves\\nperformance on parallel with cuBLAS.\\n\\nYou will specifically learn about:\\n\\n* Block-level matrix multiplications.\\n\\n* Multi-dimensional pointer arithmetics.\\n\\n* Program re-ordering for improved L2 cache hit rate.\\n\\n* Automatic performance tuning.\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Motivations\\n\\nMatrix multiplications are a key building block of most modern high-performance computing systems.\\nThey are notoriously hard to optimize, hence their implementation is generally done by\\nhardware vendors themselves as part of so-called \\\"kernel libraries\\\" (e.g., cuBLAS).\\nUnfortunately, these libraries are often proprietary and cannot be easily customized\\nto accommodate the needs of modern deep learning workloads (e.g., fused activation functions).\\nIn this tutorial, you will learn how to implement efficient matrix multiplications by\\nyourself with Triton, in a way that is easy to customize and extend.\\n\\nRoughly speaking, the kernel that we will write will implement the following blocked\\nalgorithm to multiply a (M, K) by a (K, N) matrix:\\n\\n```python\\n# Do in parallel\\nfor m in range(0, M, BLOCK_SIZE_M):\\n  # Do in parallel\\n  for n in range(0, N, BLOCK_SIZE_N):\\n    acc = zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=float32)\\n    for k in range(0, K, BLOCK_SIZE_K):\\n      a = A[m : m+BLOCK_SIZE_M, k : k+BLOCK_SIZE_K]\\n      b = B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]\\n      acc += dot(a, b)\\n    C[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N] = acc\\n```\\nwhere each iteration of the doubly-nested for-loop is performed by a dedicated Triton program instance.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Compute Kernel\\n\\nThe above algorithm is, actually, fairly straightforward to implement in Triton.\\nThe main difficulty comes from the computation of the memory locations at which blocks\\nof :code:`A` and :code:`B` must be read in the inner loop. For that, we need\\nmulti-dimensional pointer arithmetics.\\n\\n### Pointer Arithmetics\\n\\nFor a row-major 2D tensor :code:`X`, the memory location of :code:`X[i, j]` is given b\\ny :code:`&X[i, j] = X + i*stride_xi + j*stride_xj`.\\nTherefore, blocks of pointers for :code:`A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K]` and\\n:code:`B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]` can be defined in pseudo-code as:\\n\\n```python\\n&A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K] =  a_ptr + (m : m+BLOCK_SIZE_M)[:, None]*A.stride(0) + (k : k+BLOCK_SIZE_K)[None, :]*A.stride(1);\\n&B[k : k+BLOCK_SIZE_K, n:n+BLOCK_SIZE_N] =  b_ptr + (k : k+BLOCK_SIZE_K)[:, None]*B.stride(0) + (n : n+BLOCK_SIZE_N)[None, :]*B.stride(1);\\n```\\nWhich means that pointers for blocks of A and B can be initialized (i.e., :code:`k=0`) in Triton as the following\\ncode. Also note that we need an extra modulo to handle the case where :code:`M` is not a multiple of\\n:code:`BLOCK_SIZE_M` or :code:`N` is not a multiple of :code:`BLOCK_SIZE_N`, in which case we can pad the data with\\nsome useless values, which will not contribute to the results. For the :code:`K` dimension, we will handle that later\\nusing masking load semantics.\\n\\n```python\\noffs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\\noffs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\\noffs_k = tl.arange(0, BLOCK_SIZE_K)\\na_ptrs = a_ptr + (offs_am[:, None]*stride_am + offs_k [None, :]*stride_ak)\\nb_ptrs = b_ptr + (offs_k [:, None]*stride_bk + offs_bn[None, :]*stride_bn)\\n```\\nAnd then updated in the inner loop as follows:\\n\\n```python\\na_ptrs += BLOCK_SIZE_K * stride_ak;\\nb_ptrs += BLOCK_SIZE_K * stride_bk;\\n```\\n### L2 Cache Optimizations\\n\\nAs mentioned above, each program instance computes a :code:`[BLOCK_SIZE_M, BLOCK_SIZE_N]`\\nblock of :code:`C`.\\nIt is important to remember that the order in which these blocks are computed does\\nmatter, since it affects the L2 cache hit rate of our program. and unfortunately, a\\na simple row-major ordering\\n\\n```Python\\npid = triton.program_id(0);\\ngrid_m = (M + BLOCK_SIZE_M - 1) // BLOCK_SIZE_M;\\ngrid_n = (N + BLOCK_SIZE_N - 1) // BLOCK_SIZE_N;\\npid_m = pid / grid_n;\\npid_n = pid % grid_n;\\n```\\nis just not going to cut it.\\n\\nOne possible solution is to launch blocks in an order that promotes data reuse.\\nThis can be done by 'super-grouping' blocks in groups of :code:`GROUP_M` rows before\\nswitching to the next column:\\n\\n```python\\n# Program ID\\npid = tl.program_id(axis=0)\\n# Number of program ids along the M axis\\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\\n# Number of programs ids along the N axis\\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\\n# Number of programs in group\\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\\n# Id of the group this program is in\\ngroup_id = pid // num_pid_in_group\\n# Row-id of the first program in the group\\nfirst_pid_m = group_id * GROUP_SIZE_M\\n# If `num_pid_m` isn't divisible by `GROUP_SIZE_M`, the last group is smaller\\ngroup_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\\n# *Within groups*, programs are ordered in a column-major order\\n# Row-id of the program in the *launch grid*\\npid_m = first_pid_m + (pid % group_size_m)\\n# Col-id of the program in the *launch grid*\\npid_n = (pid % num_pid_in_group) // group_size_m\\n```\\nFor example, in the following matmul where each matrix is 9 blocks by 9 blocks,\\nwe can see that if we compute the output in row-major ordering, we need to load 90\\nblocks into SRAM to compute the first 9 output blocks, but if we do it in grouped\\nordering, we only need to load 54 blocks.\\n\\n  .. image:: grouped_vs_row_major_ordering.png\\n\\nIn practice, this can improve the performance of our matrix multiplication kernel by\\nmore than 10\\\\% on some hardware architecture (e.g., 220 to 245 TFLOPS on A100).\\n\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Final Result\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n# `triton.jit`'ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:\\n#   - A list of `triton.Config` objects that define different configurations of\\n#       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try\\n#   - An auto-tuning *key* whose change in values will trigger evaluation of all the\\n#       provided configs\\n@triton.autotune(\\n    configs=[\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\\n    ],\\n    key=['M', 'N', 'K'],\\n)\\n@triton.jit\\ndef matmul_kernel(\\n    # Pointers to matrices\\n    a_ptr, b_ptr, c_ptr,\\n    # Matrix dimensions\\n    M, N, K,\\n    # The stride variables represent how much to increase the ptr by when moving by 1\\n    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\\n    # by to get the element one row down (A has M rows).\\n    stride_am, stride_ak,\\n    stride_bk, stride_bn,\\n    stride_cm, stride_cn,\\n    # Meta-parameters\\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\\n    GROUP_SIZE_M: tl.constexpr,\\n    ACTIVATION: tl.constexpr,\\n):\\n    \\\"\\\"\\\"Kernel for computing the matmul C = A x B.\\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\\n    \\\"\\\"\\\"\\n    # -----------------------------------------------------------\\n    # Map program ids `pid` to the block of C it should compute.\\n    # This is done in a grouped ordering to promote L2 data reuse.\\n    # See above `L2 Cache Optimizations` section for details.\\n    pid = tl.program_id(axis=0)\\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\\n    group_id = pid // num_pid_in_group\\n    first_pid_m = group_id * GROUP_SIZE_M\\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\\n    pid_m = first_pid_m + (pid % group_size_m)\\n    pid_n = (pid % num_pid_in_group) // group_size_m\\n\\n    # ----------------------------------------------------------\\n    # Create pointers for the first blocks of A and B.\\n    # We will advance this pointer as we move in the K direction\\n    # and accumulate\\n    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\\n    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\\n    # See above `Pointer Arithmetics` section for details\\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\\n\\n    # -----------------------------------------------------------\\n    # Iterate to compute a block of the C matrix.\\n    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\\n    # of fp32 values for higher accuracy.\\n    # `accumulator` will be converted back to fp16 after the loop.\\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\\n        # Load the next block of A and B, generate a mask by checking the K dimension.\\n        # If it is out of bounds, set it to 0.\\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\\n        # We accumulate along the K dimension.\\n        accumulator += tl.dot(a, b)\\n        # Advance the ptrs to the next K block.\\n        a_ptrs += BLOCK_SIZE_K * stride_ak\\n        b_ptrs += BLOCK_SIZE_K * stride_bk\\n    # You can fuse arbitrary activation functions here\\n    # while the accumulator is still in FP32!\\n    if ACTIVATION == \\\"leaky_relu\\\":\\n        accumulator = leaky_relu(accumulator)\\n    c = accumulator.to(tl.float16)\\n\\n    # -----------------------------------------------------------\\n    # Write back the block of the output matrix C with masks.\\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\\n    tl.store(c_ptrs, c, mask=c_mask)\\n\\n\\n# We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.\\n@triton.jit\\ndef leaky_relu(x):\\n    x = x + 1\\n    return tl.where(x >= 0, x, 0.01 * x)\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"We can now create a convenience wrapper function that only takes two input tensors,\\nand (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"def matmul(a, b, activation=\\\"\\\"):\\n    # Check constraints.\\n    assert a.shape[1] == b.shape[0], \\\"Incompatible dimensions\\\"\\n    assert a.is_contiguous(), \\\"Matrix A must be contiguous\\\"\\n    assert b.is_contiguous(), \\\"Matrix B must be contiguous\\\"\\n    M, K = a.shape\\n    K, N = b.shape\\n    # Allocates output.\\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\\n    # 1D launch kernel where each block gets its own program.\\n    grid = lambda META: (\\n        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\\n    )\\n    matmul_kernel[grid](\\n        a, b, c,\\n        M, N, K,\\n        a.stride(0), a.stride(1),\\n        b.stride(0), b.stride(1),\\n        c.stride(0), c.stride(1),\\n        ACTIVATION=activation\\n    )\\n    return c\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Unit Test\\n\\nWe can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS).\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"torch.manual_seed(0)\\na = torch.randn((512, 512), device='cuda', dtype=torch.float16)\\nb = torch.randn((512, 512), device='cuda', dtype=torch.float16)\\ntriton_output = matmul(a, b)\\ntorch_output = torch.matmul(a, b)\\nprint(f\\\"triton_output={triton_output}\\\")\\nprint(f\\\"torch_output={torch_output}\\\")\\nif torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\\n    print(\\\"\\u2705 Triton and Torch match\\\")\\nelse:\\n    print(\\\"\\u274c Triton and Torch differ\\\")\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Benchmark\\n\\n### Square Matrix Performance\\n\\nWe can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices,\\nbut feel free to arrange this script as you wish to benchmark any other matrix shape.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"@triton.testing.perf_report(\\n    triton.testing.Benchmark(\\n        x_names=['M', 'N', 'K'],  # Argument names to use as an x-axis for the plot\\n        x_vals=[\\n            128 * i for i in range(2, 33)\\n        ],  # Different possible values for `x_name`\\n        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot\\n        # Possible values for `line_arg`\\n        line_vals=['cublas', 'triton'],\\n        # Label name for the lines\\n        line_names=[\\\"cuBLAS\\\", \\\"Triton\\\"],\\n        # Line styles\\n        styles=[('green', '-'), ('blue', '-')],\\n        ylabel=\\\"TFLOPS\\\",  # Label name for the y-axis\\n        plot_name=\\\"matmul-performance\\\",  # Name for the plot, used also as a file name for saving the plot.\\n        args={},\\n    )\\n)\\ndef benchmark(M, N, K, provider):\\n    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\\n    b = torch.randn((K, N), device='cuda', dtype=torch.float16)\\n    quantiles = [0.5, 0.2, 0.8]\\n    if provider == 'cublas':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\\n    if provider == 'triton':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\\n    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\\n    return perf(ms), perf(max_ms), perf(min_ms)\\n\\n\\nbenchmark.run(show_plots=True, print_data=True)\"\n+      ]\n+    }\n+  ],\n+  \"metadata\": {\n+    \"kernelspec\": {\n+      \"display_name\": \"Python 3\",\n+      \"language\": \"python\",\n+      \"name\": \"python3\"\n+    },\n+    \"language_info\": {\n+      \"codemirror_mode\": {\n+        \"name\": \"ipython\",\n+        \"version\": 3\n+      },\n+      \"file_extension\": \".py\",\n+      \"mimetype\": \"text/x-python\",\n+      \"name\": \"python\",\n+      \"nbconvert_exporter\": \"python\",\n+      \"pygments_lexer\": \"ipython3\",\n+      \"version\": \"3.8.10\"\n+    }\n+  },\n+  \"nbformat\": 4,\n+  \"nbformat_minor\": 0\n+}\n\\ No newline at end of file"}, {"filename": "main/_downloads/bc847dec325798bdc436c4ef5ac8b78a/04-low-memory-dropout.ipynb", "status": "added", "additions": 100, "deletions": 0, "changes": 100, "file_content_changes": "@@ -0,0 +1,100 @@\n+{\n+  \"cells\": [\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"%matplotlib inline\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"\\n# Low-Memory Dropout\\n\\nIn this tutorial, you will write a memory-efficient implementation of dropout whose state\\nwill be composed of a single int32 seed. This differs from more traditional implementations of dropout,\\nwhose state is generally composed of a bit mask tensor of the same shape as the input.\\n\\nIn doing so, you will learn about:\\n\\n* The limitations of naive implementations of Dropout with PyTorch.\\n\\n* Parallel pseudo-random number generation in Triton.\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Baseline\\n\\nThe *dropout* operator was first introduced in [SRIVASTAVA2014]_ as a way to improve the performance\\nof deep neural networks in low-data regime (i.e. regularization).\\n\\nIt takes a vector as input and produces a vector of the same shape as output. Each scalar in the\\noutput has a probability $p$ of being changed to zero and otherwise it is copied from the input.\\nThis forces the network to perform well even when only $1 - p$ scalars from the input are available.\\n\\nAt evaluation time we want to use the full power of the network so we set $p=0$. Naively this would\\nincrease the norm of the output (which can be a bad thing, e.g. it can lead to artificial decrease\\nin the output softmax temperature). To prevent this we multiply the output by $\\\\frac{1}{1 - p}$, which\\nkeeps the norm consistent regardless of the dropout probability.\\n\\nLet's first take a look at the baseline implementation.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import tabulate\\nimport torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.jit\\ndef _dropout(\\n    x_ptr,  # pointer to the input\\n    x_keep_ptr,  # pointer to a mask of 0s and 1s\\n    output_ptr,  # pointer to the output\\n    n_elements,  # number of elements in the `x` tensor\\n    p,  # probability that an element of `x` is changed to zero\\n    BLOCK_SIZE: tl.constexpr,\\n):\\n    pid = tl.program_id(axis=0)\\n    block_start = pid * BLOCK_SIZE\\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\\n    mask = offsets < n_elements\\n    # Load data\\n    x = tl.load(x_ptr + offsets, mask=mask)\\n    x_keep = tl.load(x_keep_ptr + offsets, mask=mask)\\n    # The line below is the crucial part, described in the paragraph above!\\n    output = tl.where(x_keep, x / (1 - p), 0.0)\\n    # Write-back output\\n    tl.store(output_ptr + offsets, output, mask=mask)\\n\\n\\ndef dropout(x, x_keep, p):\\n    output = torch.empty_like(x)\\n    assert x.is_contiguous()\\n    n_elements = x.numel()\\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\\n    _dropout[grid](x, x_keep, output, n_elements, p, BLOCK_SIZE=1024)\\n    return output\\n\\n\\n# Input tensor\\nx = torch.randn(size=(10,)).cuda()\\n# Dropout mask\\np = 0.5\\nx_keep = (torch.rand(size=(10,)) > p).to(torch.int32).cuda()\\n#\\noutput = dropout(x, x_keep=x_keep, p=p)\\nprint(tabulate.tabulate([\\n    [\\\"input\\\"] + x.tolist(),\\n    [\\\"keep mask\\\"] + x_keep.tolist(),\\n    [\\\"output\\\"] + output.tolist()\\n]))\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Seeded dropout\\n\\nThe above implementation of dropout works fine, but it can be a bit awkward to deal with. Firstly\\nwe need to store the dropout mask for backpropagation. Secondly, dropout state management can get\\nvery tricky when using recompute/checkpointing (e.g. see all the notes about `preserve_rng_state` in\\nhttps://pytorch.org/docs/1.9.0/checkpoint.html). In this tutorial we'll describe an alternative implementation\\nthat (1) has a smaller memory footprint; (2) requires less data movement; and (3) simplifies the management\\nof persisting randomness across multiple invocations of the kernel.\\n\\nPseudo-random number generation in Triton is simple! In this tutorial we will use the\\n:code:`triton.language.rand` function which generates a block of uniformly distributed :code:`float32`\\nvalues in [0, 1), given a seed and a block of :code:`int32` offsets. But if you need it, Triton also provides\\nother `random number generation strategies <Random Number Generation>`.\\n\\n<div class=\\\"alert alert-info\\\"><h4>Note</h4><p>Triton's implementation of PRNG is based on the Philox algorithm (described on [SALMON2011]_).</p></div>\\n\\nLet's put it all together.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"@triton.jit\\ndef _seeded_dropout(\\n    x_ptr,\\n    output_ptr,\\n    n_elements,\\n    p,\\n    seed,\\n    BLOCK_SIZE: tl.constexpr,\\n):\\n    # compute memory offsets of elements handled by this instance\\n    pid = tl.program_id(axis=0)\\n    block_start = pid * BLOCK_SIZE\\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\\n    # load data from x\\n    mask = offsets < n_elements\\n    x = tl.load(x_ptr + offsets, mask=mask)\\n    # randomly prune it\\n    random = tl.rand(seed, offsets)\\n    x_keep = random > p\\n    # write-back\\n    output = tl.where(x_keep, x / (1 - p), 0.0)\\n    tl.store(output_ptr + offsets, output, mask=mask)\\n\\n\\ndef seeded_dropout(x, p, seed):\\n    output = torch.empty_like(x)\\n    assert x.is_contiguous()\\n    n_elements = x.numel()\\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\\n    _seeded_dropout[grid](x, output, n_elements, p, seed, BLOCK_SIZE=1024)\\n    return output\\n\\n\\nx = torch.randn(size=(10,)).cuda()\\n# Compare this to the baseline - dropout mask is never instantiated!\\noutput = seeded_dropout(x, p=0.5, seed=123)\\noutput2 = seeded_dropout(x, p=0.5, seed=123)\\noutput3 = seeded_dropout(x, p=0.5, seed=512)\\n\\nprint(tabulate.tabulate([\\n    [\\\"input\\\"] + x.tolist(),\\n    [\\\"output (seed = 123)\\\"] + output.tolist(),\\n    [\\\"output (seed = 123)\\\"] + output2.tolist(),\\n    [\\\"output (seed = 512)\\\"] + output3.tolist()\\n]))\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"Et Voil\\u00e0! We have a triton kernel that applies the same dropout mask provided the seed is the same!\\nIf you'd like explore further applications of pseudorandomness in GPU programming, we encourage you\\nto explore the `triton/language/random` folder!\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Exercises\\n\\n1. Extend the kernel to operate over a matrix and use a vector of seeds - one per row.\\n2. Add support for striding.\\n3. (challenge) Implement a kernel for sparse Johnson-Lindenstrauss transform which generates the projection matrix one the fly each time using a seed.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## References\\n\\n.. [SALMON2011] John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw, \\\"Parallel Random Numbers: As Easy as 1, 2, 3\\\", 2011\\n.. [SRIVASTAVA2014] Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \\\"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\\\", JMLR 2014\\n\\n\"\n+      ]\n+    }\n+  ],\n+  \"metadata\": {\n+    \"kernelspec\": {\n+      \"display_name\": \"Python 3\",\n+      \"language\": \"python\",\n+      \"name\": \"python3\"\n+    },\n+    \"language_info\": {\n+      \"codemirror_mode\": {\n+        \"name\": \"ipython\",\n+        \"version\": 3\n+      },\n+      \"file_extension\": \".py\",\n+      \"mimetype\": \"text/x-python\",\n+      \"name\": \"python\",\n+      \"nbconvert_exporter\": \"python\",\n+      \"pygments_lexer\": \"ipython3\",\n+      \"version\": \"3.8.10\"\n+    }\n+  },\n+  \"nbformat\": 4,\n+  \"nbformat_minor\": 0\n+}\n\\ No newline at end of file"}, {"filename": "main/_downloads/c9aed78977a4c05741d675a38dde3d7d/04-low-memory-dropout.py", "status": "added", "additions": 173, "deletions": 0, "changes": 173, "file_content_changes": "@@ -0,0 +1,173 @@\n+\"\"\"\n+Low-Memory Dropout\n+==================\n+\n+In this tutorial, you will write a memory-efficient implementation of dropout whose state\n+will be composed of a single int32 seed. This differs from more traditional implementations of dropout,\n+whose state is generally composed of a bit mask tensor of the same shape as the input.\n+\n+In doing so, you will learn about:\n+\n+* The limitations of naive implementations of Dropout with PyTorch.\n+\n+* Parallel pseudo-random number generation in Triton.\n+\n+\"\"\"\n+\n+# %%\n+# Baseline\n+# --------\n+#\n+# The *dropout* operator was first introduced in [SRIVASTAVA2014]_ as a way to improve the performance\n+# of deep neural networks in low-data regime (i.e. regularization).\n+#\n+# It takes a vector as input and produces a vector of the same shape as output. Each scalar in the\n+# output has a probability :math:`p` of being changed to zero and otherwise it is copied from the input.\n+# This forces the network to perform well even when only :math:`1 - p` scalars from the input are available.\n+#\n+# At evaluation time we want to use the full power of the network so we set :math:`p=0`. Naively this would\n+# increase the norm of the output (which can be a bad thing, e.g. it can lead to artificial decrease\n+# in the output softmax temperature). To prevent this we multiply the output by :math:`\\frac{1}{1 - p}`, which\n+# keeps the norm consistent regardless of the dropout probability.\n+#\n+# Let's first take a look at the baseline implementation.\n+\n+\n+import tabulate\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def _dropout(\n+    x_ptr,  # pointer to the input\n+    x_keep_ptr,  # pointer to a mask of 0s and 1s\n+    output_ptr,  # pointer to the output\n+    n_elements,  # number of elements in the `x` tensor\n+    p,  # probability that an element of `x` is changed to zero\n+    BLOCK_SIZE: tl.constexpr,\n+):\n+    pid = tl.program_id(axis=0)\n+    block_start = pid * BLOCK_SIZE\n+    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+    mask = offsets < n_elements\n+    # Load data\n+    x = tl.load(x_ptr + offsets, mask=mask)\n+    x_keep = tl.load(x_keep_ptr + offsets, mask=mask)\n+    # The line below is the crucial part, described in the paragraph above!\n+    output = tl.where(x_keep, x / (1 - p), 0.0)\n+    # Write-back output\n+    tl.store(output_ptr + offsets, output, mask=mask)\n+\n+\n+def dropout(x, x_keep, p):\n+    output = torch.empty_like(x)\n+    assert x.is_contiguous()\n+    n_elements = x.numel()\n+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+    _dropout[grid](x, x_keep, output, n_elements, p, BLOCK_SIZE=1024)\n+    return output\n+\n+\n+# Input tensor\n+x = torch.randn(size=(10,)).cuda()\n+# Dropout mask\n+p = 0.5\n+x_keep = (torch.rand(size=(10,)) > p).to(torch.int32).cuda()\n+#\n+output = dropout(x, x_keep=x_keep, p=p)\n+print(tabulate.tabulate([\n+    [\"input\"] + x.tolist(),\n+    [\"keep mask\"] + x_keep.tolist(),\n+    [\"output\"] + output.tolist()\n+]))\n+\n+# %%\n+# Seeded dropout\n+# --------------\n+#\n+# The above implementation of dropout works fine, but it can be a bit awkward to deal with. Firstly\n+# we need to store the dropout mask for backpropagation. Secondly, dropout state management can get\n+# very tricky when using recompute/checkpointing (e.g. see all the notes about `preserve_rng_state` in\n+# https://pytorch.org/docs/1.9.0/checkpoint.html). In this tutorial we'll describe an alternative implementation\n+# that (1) has a smaller memory footprint; (2) requires less data movement; and (3) simplifies the management\n+# of persisting randomness across multiple invocations of the kernel.\n+#\n+# Pseudo-random number generation in Triton is simple! In this tutorial we will use the\n+# :code:`triton.language.rand` function which generates a block of uniformly distributed :code:`float32`\n+# values in [0, 1), given a seed and a block of :code:`int32` offsets. But if you need it, Triton also provides\n+# other :ref:`random number generation strategies <Random Number Generation>`.\n+#\n+# .. note::\n+#    Triton's implementation of PRNG is based on the Philox algorithm (described on [SALMON2011]_).\n+#\n+# Let's put it all together.\n+\n+\n+@triton.jit\n+def _seeded_dropout(\n+    x_ptr,\n+    output_ptr,\n+    n_elements,\n+    p,\n+    seed,\n+    BLOCK_SIZE: tl.constexpr,\n+):\n+    # compute memory offsets of elements handled by this instance\n+    pid = tl.program_id(axis=0)\n+    block_start = pid * BLOCK_SIZE\n+    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+    # load data from x\n+    mask = offsets < n_elements\n+    x = tl.load(x_ptr + offsets, mask=mask)\n+    # randomly prune it\n+    random = tl.rand(seed, offsets)\n+    x_keep = random > p\n+    # write-back\n+    output = tl.where(x_keep, x / (1 - p), 0.0)\n+    tl.store(output_ptr + offsets, output, mask=mask)\n+\n+\n+def seeded_dropout(x, p, seed):\n+    output = torch.empty_like(x)\n+    assert x.is_contiguous()\n+    n_elements = x.numel()\n+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+    _seeded_dropout[grid](x, output, n_elements, p, seed, BLOCK_SIZE=1024)\n+    return output\n+\n+\n+x = torch.randn(size=(10,)).cuda()\n+# Compare this to the baseline - dropout mask is never instantiated!\n+output = seeded_dropout(x, p=0.5, seed=123)\n+output2 = seeded_dropout(x, p=0.5, seed=123)\n+output3 = seeded_dropout(x, p=0.5, seed=512)\n+\n+print(tabulate.tabulate([\n+    [\"input\"] + x.tolist(),\n+    [\"output (seed = 123)\"] + output.tolist(),\n+    [\"output (seed = 123)\"] + output2.tolist(),\n+    [\"output (seed = 512)\"] + output3.tolist()\n+]))\n+\n+# %%\n+# Et Voil\u00e0! We have a triton kernel that applies the same dropout mask provided the seed is the same!\n+# If you'd like explore further applications of pseudorandomness in GPU programming, we encourage you\n+# to explore the `triton/language/random` folder!\n+\n+# %%\n+# Exercises\n+# ---------\n+#\n+# 1. Extend the kernel to operate over a matrix and use a vector of seeds - one per row.\n+# 2. Add support for striding.\n+# 3. (challenge) Implement a kernel for sparse Johnson-Lindenstrauss transform which generates the projection matrix one the fly each time using a seed.\n+\n+# %%\n+# References\n+# ----------\n+#\n+# .. [SALMON2011] John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw, \"Parallel Random Numbers: As Easy as 1, 2, 3\", 2011\n+# .. [SRIVASTAVA2014] Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\", JMLR 2014"}, {"filename": "main/_downloads/d5fee5b55a64e47f1b5724ec39adf171/03-matrix-multiplication.py", "status": "added", "additions": 350, "deletions": 0, "changes": 350, "file_content_changes": "@@ -0,0 +1,350 @@\n+\"\"\"\n+Matrix Multiplication\n+=====================\n+In this tutorial, you will write a very short high-performance FP16 matrix multiplication kernel that achieves\n+performance on parallel with cuBLAS.\n+\n+You will specifically learn about:\n+\n+* Block-level matrix multiplications.\n+\n+* Multi-dimensional pointer arithmetics.\n+\n+* Program re-ordering for improved L2 cache hit rate.\n+\n+* Automatic performance tuning.\n+\n+\"\"\"\n+\n+# %%\n+# Motivations\n+# -----------\n+#\n+# Matrix multiplications are a key building block of most modern high-performance computing systems.\n+# They are notoriously hard to optimize, hence their implementation is generally done by\n+# hardware vendors themselves as part of so-called \"kernel libraries\" (e.g., cuBLAS).\n+# Unfortunately, these libraries are often proprietary and cannot be easily customized\n+# to accommodate the needs of modern deep learning workloads (e.g., fused activation functions).\n+# In this tutorial, you will learn how to implement efficient matrix multiplications by\n+# yourself with Triton, in a way that is easy to customize and extend.\n+#\n+# Roughly speaking, the kernel that we will write will implement the following blocked\n+# algorithm to multiply a (M, K) by a (K, N) matrix:\n+#\n+#  .. code-block:: python\n+#\n+#    # Do in parallel\n+#    for m in range(0, M, BLOCK_SIZE_M):\n+#      # Do in parallel\n+#      for n in range(0, N, BLOCK_SIZE_N):\n+#        acc = zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=float32)\n+#        for k in range(0, K, BLOCK_SIZE_K):\n+#          a = A[m : m+BLOCK_SIZE_M, k : k+BLOCK_SIZE_K]\n+#          b = B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]\n+#          acc += dot(a, b)\n+#        C[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N] = acc\n+#\n+# where each iteration of the doubly-nested for-loop is performed by a dedicated Triton program instance.\n+\n+# %%\n+# Compute Kernel\n+# --------------\n+#\n+# The above algorithm is, actually, fairly straightforward to implement in Triton.\n+# The main difficulty comes from the computation of the memory locations at which blocks\n+# of :code:`A` and :code:`B` must be read in the inner loop. For that, we need\n+# multi-dimensional pointer arithmetics.\n+#\n+# Pointer Arithmetics\n+# ~~~~~~~~~~~~~~~~~~~\n+#\n+# For a row-major 2D tensor :code:`X`, the memory location of :code:`X[i, j]` is given b\n+# y :code:`&X[i, j] = X + i*stride_xi + j*stride_xj`.\n+# Therefore, blocks of pointers for :code:`A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K]` and\n+# :code:`B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]` can be defined in pseudo-code as:\n+#\n+#  .. code-block:: python\n+#\n+#    &A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K] =  a_ptr + (m : m+BLOCK_SIZE_M)[:, None]*A.stride(0) + (k : k+BLOCK_SIZE_K)[None, :]*A.stride(1);\n+#    &B[k : k+BLOCK_SIZE_K, n:n+BLOCK_SIZE_N] =  b_ptr + (k : k+BLOCK_SIZE_K)[:, None]*B.stride(0) + (n : n+BLOCK_SIZE_N)[None, :]*B.stride(1);\n+#\n+# Which means that pointers for blocks of A and B can be initialized (i.e., :code:`k=0`) in Triton as the following\n+# code. Also note that we need an extra modulo to handle the case where :code:`M` is not a multiple of\n+# :code:`BLOCK_SIZE_M` or :code:`N` is not a multiple of :code:`BLOCK_SIZE_N`, in which case we can pad the data with\n+# some useless values, which will not contribute to the results. For the :code:`K` dimension, we will handle that later\n+# using masking load semantics.\n+#\n+#  .. code-block:: python\n+#\n+#    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n+#    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n+#    offs_k = tl.arange(0, BLOCK_SIZE_K)\n+#    a_ptrs = a_ptr + (offs_am[:, None]*stride_am + offs_k [None, :]*stride_ak)\n+#    b_ptrs = b_ptr + (offs_k [:, None]*stride_bk + offs_bn[None, :]*stride_bn)\n+#\n+# And then updated in the inner loop as follows:\n+#\n+#  .. code-block:: python\n+#\n+#    a_ptrs += BLOCK_SIZE_K * stride_ak;\n+#    b_ptrs += BLOCK_SIZE_K * stride_bk;\n+#\n+#\n+# L2 Cache Optimizations\n+# ~~~~~~~~~~~~~~~~~~~~~~\n+#\n+# As mentioned above, each program instance computes a :code:`[BLOCK_SIZE_M, BLOCK_SIZE_N]`\n+# block of :code:`C`.\n+# It is important to remember that the order in which these blocks are computed does\n+# matter, since it affects the L2 cache hit rate of our program. and unfortunately, a\n+# a simple row-major ordering\n+#\n+#  .. code-block:: Python\n+#\n+#    pid = triton.program_id(0);\n+#    grid_m = (M + BLOCK_SIZE_M - 1) // BLOCK_SIZE_M;\n+#    grid_n = (N + BLOCK_SIZE_N - 1) // BLOCK_SIZE_N;\n+#    pid_m = pid / grid_n;\n+#    pid_n = pid % grid_n;\n+#\n+# is just not going to cut it.\n+#\n+# One possible solution is to launch blocks in an order that promotes data reuse.\n+# This can be done by 'super-grouping' blocks in groups of :code:`GROUP_M` rows before\n+# switching to the next column:\n+#\n+#  .. code-block:: python\n+#\n+#    # Program ID\n+#    pid = tl.program_id(axis=0)\n+#    # Number of program ids along the M axis\n+#    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n+#    # Number of programs ids along the N axis\n+#    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+#    # Number of programs in group\n+#    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n+#    # Id of the group this program is in\n+#    group_id = pid // num_pid_in_group\n+#    # Row-id of the first program in the group\n+#    first_pid_m = group_id * GROUP_SIZE_M\n+#    # If `num_pid_m` isn't divisible by `GROUP_SIZE_M`, the last group is smaller\n+#    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n+#    # *Within groups*, programs are ordered in a column-major order\n+#    # Row-id of the program in the *launch grid*\n+#    pid_m = first_pid_m + (pid % group_size_m)\n+#    # Col-id of the program in the *launch grid*\n+#    pid_n = (pid % num_pid_in_group) // group_size_m\n+#\n+# For example, in the following matmul where each matrix is 9 blocks by 9 blocks,\n+# we can see that if we compute the output in row-major ordering, we need to load 90\n+# blocks into SRAM to compute the first 9 output blocks, but if we do it in grouped\n+# ordering, we only need to load 54 blocks.\n+#\n+#   .. image:: grouped_vs_row_major_ordering.png\n+#\n+# In practice, this can improve the performance of our matrix multiplication kernel by\n+# more than 10\\% on some hardware architecture (e.g., 220 to 245 TFLOPS on A100).\n+#\n+\n+# %%\n+# Final Result\n+# ------------\n+\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+# `triton.jit`'ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:\n+#   - A list of `triton.Config` objects that define different configurations of\n+#       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try\n+#   - An auto-tuning *key* whose change in values will trigger evaluation of all the\n+#       provided configs\n+@triton.autotune(\n+    configs=[\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n+        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n+    ],\n+    key=['M', 'N', 'K'],\n+)\n+@triton.jit\n+def matmul_kernel(\n+    # Pointers to matrices\n+    a_ptr, b_ptr, c_ptr,\n+    # Matrix dimensions\n+    M, N, K,\n+    # The stride variables represent how much to increase the ptr by when moving by 1\n+    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n+    # by to get the element one row down (A has M rows).\n+    stride_am, stride_ak,\n+    stride_bk, stride_bn,\n+    stride_cm, stride_cn,\n+    # Meta-parameters\n+    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+    GROUP_SIZE_M: tl.constexpr,\n+    ACTIVATION: tl.constexpr,\n+):\n+    \"\"\"Kernel for computing the matmul C = A x B.\n+    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n+    \"\"\"\n+    # -----------------------------------------------------------\n+    # Map program ids `pid` to the block of C it should compute.\n+    # This is done in a grouped ordering to promote L2 data reuse.\n+    # See above `L2 Cache Optimizations` section for details.\n+    pid = tl.program_id(axis=0)\n+    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n+    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n+    group_id = pid // num_pid_in_group\n+    first_pid_m = group_id * GROUP_SIZE_M\n+    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n+    pid_m = first_pid_m + (pid % group_size_m)\n+    pid_n = (pid % num_pid_in_group) // group_size_m\n+\n+    # ----------------------------------------------------------\n+    # Create pointers for the first blocks of A and B.\n+    # We will advance this pointer as we move in the K direction\n+    # and accumulate\n+    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n+    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n+    # See above `Pointer Arithmetics` section for details\n+    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n+    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n+    offs_k = tl.arange(0, BLOCK_SIZE_K)\n+    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n+    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n+\n+    # -----------------------------------------------------------\n+    # Iterate to compute a block of the C matrix.\n+    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n+    # of fp32 values for higher accuracy.\n+    # `accumulator` will be converted back to fp16 after the loop.\n+    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n+        # Load the next block of A and B, generate a mask by checking the K dimension.\n+        # If it is out of bounds, set it to 0.\n+        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n+        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n+        # We accumulate along the K dimension.\n+        accumulator += tl.dot(a, b)\n+        # Advance the ptrs to the next K block.\n+        a_ptrs += BLOCK_SIZE_K * stride_ak\n+        b_ptrs += BLOCK_SIZE_K * stride_bk\n+    # You can fuse arbitrary activation functions here\n+    # while the accumulator is still in FP32!\n+    if ACTIVATION == \"leaky_relu\":\n+        accumulator = leaky_relu(accumulator)\n+    c = accumulator.to(tl.float16)\n+\n+    # -----------------------------------------------------------\n+    # Write back the block of the output matrix C with masks.\n+    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n+    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n+    tl.store(c_ptrs, c, mask=c_mask)\n+\n+\n+# We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.\n+@triton.jit\n+def leaky_relu(x):\n+    x = x + 1\n+    return tl.where(x >= 0, x, 0.01 * x)\n+\n+\n+# %%\n+# We can now create a convenience wrapper function that only takes two input tensors,\n+# and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\n+\n+\n+def matmul(a, b, activation=\"\"):\n+    # Check constraints.\n+    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n+    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n+    assert b.is_contiguous(), \"Matrix B must be contiguous\"\n+    M, K = a.shape\n+    K, N = b.shape\n+    # Allocates output.\n+    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n+    # 1D launch kernel where each block gets its own program.\n+    grid = lambda META: (\n+        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n+    )\n+    matmul_kernel[grid](\n+        a, b, c,\n+        M, N, K,\n+        a.stride(0), a.stride(1),\n+        b.stride(0), b.stride(1),\n+        c.stride(0), c.stride(1),\n+        ACTIVATION=activation\n+    )\n+    return c\n+\n+\n+# %%\n+# Unit Test\n+# ---------\n+#\n+# We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS).\n+\n+torch.manual_seed(0)\n+a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n+b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n+triton_output = matmul(a, b)\n+torch_output = torch.matmul(a, b)\n+print(f\"triton_output={triton_output}\")\n+print(f\"torch_output={torch_output}\")\n+if torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\n+    print(\"\u2705 Triton and Torch match\")\n+else:\n+    print(\"\u274c Triton and Torch differ\")\n+\n+# %%\n+# Benchmark\n+# ---------\n+#\n+# Square Matrix Performance\n+# ~~~~~~~~~~~~~~~~~~~~~~~~~~\n+#\n+# We can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices,\n+# but feel free to arrange this script as you wish to benchmark any other matrix shape.\n+\n+\n+@triton.testing.perf_report(\n+    triton.testing.Benchmark(\n+        x_names=['M', 'N', 'K'],  # Argument names to use as an x-axis for the plot\n+        x_vals=[\n+            128 * i for i in range(2, 33)\n+        ],  # Different possible values for `x_name`\n+        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot\n+        # Possible values for `line_arg`\n+        line_vals=['cublas', 'triton'],\n+        # Label name for the lines\n+        line_names=[\"cuBLAS\", \"Triton\"],\n+        # Line styles\n+        styles=[('green', '-'), ('blue', '-')],\n+        ylabel=\"TFLOPS\",  # Label name for the y-axis\n+        plot_name=\"matmul-performance\",  # Name for the plot, used also as a file name for saving the plot.\n+        args={},\n+    )\n+)\n+def benchmark(M, N, K, provider):\n+    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n+    b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n+    quantiles = [0.5, 0.2, 0.8]\n+    if provider == 'cublas':\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\n+    if provider == 'triton':\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\n+    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n+    return perf(ms), perf(max_ms), perf(min_ms)\n+\n+\n+benchmark.run(show_plots=True, print_data=True)"}, {"filename": "main/_downloads/d91442ac2982c4e0cc3ab0f43534afbc/02-fused-softmax.py", "status": "added", "additions": 200, "deletions": 0, "changes": 200, "file_content_changes": "@@ -0,0 +1,200 @@\n+\"\"\"\n+Fused Softmax\n+=============\n+\n+In this tutorial, you will write a fused softmax operation that is significantly faster\n+than PyTorch's native op for a particular class of matrices: those whose rows can fit in\n+the GPU's SRAM.\n+\n+In doing so, you will learn about:\n+\n+* The benefits of kernel fusion for bandwidth-bound operations.\n+\n+* Reduction operators in Triton.\n+\n+\"\"\"\n+\n+# %%\n+# Motivations\n+# -----------\n+#\n+# Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.\n+# Let us consider instead the case of a simple (numerically stabilized) softmax operation:\n+\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@torch.jit.script\n+def naive_softmax(x):\n+    \"\"\"Compute row-wise softmax of X using native pytorch\n+\n+    We subtract the maximum element in order to avoid overflows. Softmax is invariant to\n+    this shift.\n+    \"\"\"\n+    # read  MN elements ; write M  elements\n+    x_max = x.max(dim=1)[0]\n+    # read MN + M elements ; write MN elements\n+    z = x - x_max[:, None]\n+    # read  MN elements ; write MN elements\n+    numerator = torch.exp(z)\n+    # read  MN elements ; write M  elements\n+    denominator = numerator.sum(dim=1)\n+    # read MN + M elements ; write MN elements\n+    ret = numerator / denominator[:, None]\n+    # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements\n+    return ret\n+\n+\n+# %%\n+# When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \\in R^{M \\times N}`\n+# requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.\n+# This is obviously wasteful; we'd prefer to have a custom \"fused\" kernel that only reads\n+# X once and does all the necessary computations on-chip.\n+# Doing so would require reading and writing back only :math:`MN` bytes, so we could\n+# expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).\n+# The `torch.jit.script` flags aims to perform this kind of \"kernel fusion\" automatically\n+# but, as we will see later, it is still far from ideal.\n+\n+# %%\n+# Compute Kernel\n+# --------------\n+#\n+# Our softmax kernel works as follows: each program loads a row of the input matrix X,\n+# normalizes it and writes back the result to the output Y.\n+#\n+# Note that one important limitation of Triton is that each block must have a\n+# power-of-two number of elements, so we need to internally \"pad\" each row and guard the\n+# memory operations properly if we want to handle any possible input shapes:\n+\n+\n+@triton.jit\n+def softmax_kernel(\n+    output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols,\n+    BLOCK_SIZE: tl.constexpr\n+):\n+    # The rows of the softmax are independent, so we parallelize across those\n+    row_idx = tl.program_id(0)\n+    # The stride represents how much we need to increase the pointer to advance 1 row\n+    row_start_ptr = input_ptr + row_idx * input_row_stride\n+    # The block size is the next power of two greater than n_cols, so we can fit each\n+    # row in a single block\n+    col_offsets = tl.arange(0, BLOCK_SIZE)\n+    input_ptrs = row_start_ptr + col_offsets\n+    # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n+    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n+    # Subtract maximum for numerical stability\n+    row_minus_max = row - tl.max(row, axis=0)\n+    # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n+    numerator = tl.exp(row_minus_max)\n+    denominator = tl.sum(numerator, axis=0)\n+    softmax_output = numerator / denominator\n+    # Write back output to DRAM\n+    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n+    output_ptrs = output_row_start_ptr + col_offsets\n+    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)\n+\n+\n+# %%\n+# We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.\n+\n+\n+def softmax(x):\n+    n_rows, n_cols = x.shape\n+    # The block size is the smallest power of two greater than the number of columns in `x`\n+    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n+    # Another trick we can use is to ask the compiler to use more threads per row by\n+    # increasing the number of warps (`num_warps`) over which each row is distributed.\n+    # You will see in the next tutorial how to auto-tune this value in a more natural\n+    # way so you don't have to come up with manual heuristics yourself.\n+    num_warps = 4\n+    if BLOCK_SIZE >= 2048:\n+        num_warps = 8\n+    if BLOCK_SIZE >= 4096:\n+        num_warps = 16\n+    # Allocate output\n+    y = torch.empty_like(x)\n+    # Enqueue kernel. The 1D launch grid is simple: we have one kernel instance per row o\n+    # f the input matrix\n+    softmax_kernel[(n_rows,)](\n+        y,\n+        x,\n+        x.stride(0),\n+        y.stride(0),\n+        n_cols,\n+        num_warps=num_warps,\n+        BLOCK_SIZE=BLOCK_SIZE,\n+    )\n+    return y\n+\n+\n+# %%\n+# Unit Test\n+# ---------\n+\n+# %%\n+# We make sure that we test our kernel on a matrix with an irregular number of rows and columns.\n+# This will allow us to verify that our padding mechanism works.\n+\n+torch.manual_seed(0)\n+x = torch.randn(1823, 781, device='cuda')\n+y_triton = softmax(x)\n+y_torch = torch.softmax(x, axis=1)\n+assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)\n+\n+# %%\n+# As expected, the results are identical.\n+\n+# %%\n+# Benchmark\n+# ---------\n+#\n+# Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.\n+# We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.\n+\n+\n+@triton.testing.perf_report(\n+    triton.testing.Benchmark(\n+        x_names=['N'],  # argument names to use as an x-axis for the plot\n+        x_vals=[\n+            128 * i for i in range(2, 100)\n+        ],  # different possible values for `x_name`\n+        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n+        line_vals=[\n+            'triton',\n+            'torch-native',\n+            'torch-jit',\n+        ],  # possible values for `line_arg``\n+        line_names=[\n+            \"Triton\",\n+            \"Torch (native)\",\n+            \"Torch (jit)\",\n+        ],  # label name for the lines\n+        styles=[('blue', '-'), ('green', '-'), ('green', '--')],  # line styles\n+        ylabel=\"GB/s\",  # label name for the y-axis\n+        plot_name=\"softmax-performance\",  # name for the plot. Used also as a file name for saving the plot.\n+        args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\n+    )\n+)\n+def benchmark(M, N, provider):\n+    x = torch.randn(M, N, device='cuda', dtype=torch.float32)\n+    quantiles = [0.5, 0.2, 0.8]\n+    if provider == 'torch-native':\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1), quantiles=quantiles)\n+    if provider == 'triton':\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: softmax(x), quantiles=quantiles)\n+    if provider == 'torch-jit':\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_softmax(x), quantiles=quantiles)\n+    gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\n+    return gbps(ms), gbps(max_ms), gbps(min_ms)\n+\n+\n+benchmark.run(show_plots=True, print_data=True)\n+\n+# %%\n+# In the above plot, we can see that:\n+#  - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.\n+#  - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.\n+#    Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape."}, {"filename": "main/_downloads/f191ee1e78dc52eb5f7cba88f71cef2f/01-vector-add.ipynb", "status": "added", "additions": 140, "deletions": 0, "changes": 140, "file_content_changes": "@@ -0,0 +1,140 @@\n+{\n+  \"cells\": [\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"%matplotlib inline\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"\\n# Vector Addition\\n\\nIn this tutorial, you will write a simple vector addition using Triton.\\n\\nIn doing so, you will learn about:\\n\\n* The basic programming model of Triton.\\n\\n* The `triton.jit` decorator, which is used to define Triton kernels.\\n\\n* The best practices for validating and benchmarking your custom ops against native reference implementations.\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Compute Kernel\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.jit\\ndef add_kernel(\\n    x_ptr,  # *Pointer* to first input vector.\\n    y_ptr,  # *Pointer* to second input vector.\\n    output_ptr,  # *Pointer* to output vector.\\n    n_elements,  # Size of the vector.\\n    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\\n                 # NOTE: `constexpr` so it can be used as a shape value.\\n):\\n    # There are multiple 'programs' processing different data. We identify which program\\n    # we are here:\\n    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\\n    # This program will process inputs that are offset from the initial data.\\n    # For instance, if you had a vector of length 256 and block_size of 64, the programs\\n    # would each access the elements [0:64, 64:128, 128:192, 192:256].\\n    # Note that offsets is a list of pointers:\\n    block_start = pid * BLOCK_SIZE\\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\\n    # Create a mask to guard memory operations against out-of-bounds accesses.\\n    mask = offsets < n_elements\\n    # Load x and y from DRAM, masking out any extra elements in case the input is not a\\n    # multiple of the block size.\\n    x = tl.load(x_ptr + offsets, mask=mask)\\n    y = tl.load(y_ptr + offsets, mask=mask)\\n    output = x + y\\n    # Write x + y back to DRAM.\\n    tl.store(output_ptr + offsets, output, mask=mask)\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"Let's also declare a helper function to (1) allocate the `z` tensor\\nand (2) enqueue the above kernel with appropriate grid/block sizes:\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"def add(x: torch.Tensor, y: torch.Tensor):\\n    # We need to preallocate the output.\\n    output = torch.empty_like(x)\\n    assert x.is_cuda and y.is_cuda and output.is_cuda\\n    n_elements = output.numel()\\n    # The SPMD launch grid denotes the number of kernel instances that run in parallel.\\n    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\\n    # In this case, we use a 1D grid where the size is the number of blocks:\\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\\n    # NOTE:\\n    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.\\n    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.\\n    #  - Don't forget to pass meta-parameters as keywords arguments.\\n    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\\n    # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\\n    # running asynchronously at this point.\\n    return output\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"We can now use the above function to compute the element-wise sum of two `torch.tensor` objects and test its correctness:\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"torch.manual_seed(0)\\nsize = 98432\\nx = torch.rand(size, device='cuda')\\ny = torch.rand(size, device='cuda')\\noutput_torch = x + y\\noutput_triton = add(x, y)\\nprint(output_torch)\\nprint(output_triton)\\nprint(\\n    f'The maximum difference between torch and triton is '\\n    f'{torch.max(torch.abs(output_torch - output_triton))}'\\n)\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"Seems like we're good to go!\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Benchmark\\n\\nWe can now benchmark our custom op on vectors of increasing sizes to get a sense of how it does relative to PyTorch.\\nTo make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of our custom ops.\\nfor different problem sizes.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"@triton.testing.perf_report(\\n    triton.testing.Benchmark(\\n        x_names=['size'],  # Argument names to use as an x-axis for the plot.\\n        x_vals=[\\n            2 ** i for i in range(12, 28, 1)\\n        ],  # Different possible values for `x_name`.\\n        x_log=True,  # x axis is logarithmic.\\n        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\\n        line_vals=['triton', 'torch'],  # Possible values for `line_arg`.\\n        line_names=['Triton', 'Torch'],  # Label name for the lines.\\n        styles=[('blue', '-'), ('green', '-')],  # Line styles.\\n        ylabel='GB/s',  # Label name for the y-axis.\\n        plot_name='vector-add-performance',  # Name for the plot. Used also as a file name for saving the plot.\\n        args={},  # Values for function arguments not in `x_names` and `y_name`.\\n    )\\n)\\ndef benchmark(size, provider):\\n    x = torch.rand(size, device='cuda', dtype=torch.float32)\\n    y = torch.rand(size, device='cuda', dtype=torch.float32)\\n    quantiles = [0.5, 0.2, 0.8]\\n    if provider == 'torch':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: x + y, quantiles=quantiles)\\n    if provider == 'triton':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: add(x, y), quantiles=quantiles)\\n    gbps = lambda ms: 12 * size / ms * 1e-6\\n    return gbps(ms), gbps(max_ms), gbps(min_ms)\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"We can now run the decorated function above. Pass `print_data=True` to see the performance number, `show_plots=True` to plot them, and/or\\n`save_path='/path/to/results/' to save them to disk along with raw CSV data:\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"benchmark.run(print_data=True, show_plots=True)\"\n+      ]\n+    }\n+  ],\n+  \"metadata\": {\n+    \"kernelspec\": {\n+      \"display_name\": \"Python 3\",\n+      \"language\": \"python\",\n+      \"name\": \"python3\"\n+    },\n+    \"language_info\": {\n+      \"codemirror_mode\": {\n+        \"name\": \"ipython\",\n+        \"version\": 3\n+      },\n+      \"file_extension\": \".py\",\n+      \"mimetype\": \"text/x-python\",\n+      \"name\": \"python\",\n+      \"nbconvert_exporter\": \"python\",\n+      \"pygments_lexer\": \"ipython3\",\n+      \"version\": \"3.8.10\"\n+    }\n+  },\n+  \"nbformat\": 4,\n+  \"nbformat_minor\": 0\n+}\n\\ No newline at end of file"}, {"filename": "main/_images/grouped_vs_row_major_ordering.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/parallel_reduction.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_01-vector-add_001.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_01-vector-add_thumb.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_02-fused-softmax_001.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_02-fused-softmax_thumb.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_03-matrix-multiplication_001.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_03-matrix-multiplication_thumb.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_04-low-memory-dropout_thumb.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_05-layer-norm_001.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_05-layer-norm_thumb.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_001.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_002.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_003.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_004.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_thumb.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_07-math-functions_thumb.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_08-experimental-block-pointer_thumb.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_sources/getting-started/tutorials/01-vector-add.rst.txt", "status": "added", "additions": 287, "deletions": 0, "changes": 287, "file_content_changes": "@@ -0,0 +1,287 @@\n+\n+.. DO NOT EDIT.\n+.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.\n+.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:\n+.. \"getting-started/tutorials/01-vector-add.py\"\n+.. LINE NUMBERS ARE GIVEN BELOW.\n+\n+.. only:: html\n+\n+    .. note::\n+        :class: sphx-glr-download-link-note\n+\n+        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_01-vector-add.py>`\n+        to download the full example code\n+\n+.. rst-class:: sphx-glr-example-title\n+\n+.. _sphx_glr_getting-started_tutorials_01-vector-add.py:\n+\n+\n+Vector Addition\n+===============\n+\n+In this tutorial, you will write a simple vector addition using Triton.\n+\n+In doing so, you will learn about:\n+\n+* The basic programming model of Triton.\n+\n+* The `triton.jit` decorator, which is used to define Triton kernels.\n+\n+* The best practices for validating and benchmarking your custom ops against native reference implementations.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 18-20\n+\n+Compute Kernel\n+--------------\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 20-56\n+\n+.. code-block:: default\n+\n+\n+    import torch\n+\n+    import triton\n+    import triton.language as tl\n+\n+\n+    @triton.jit\n+    def add_kernel(\n+        x_ptr,  # *Pointer* to first input vector.\n+        y_ptr,  # *Pointer* to second input vector.\n+        output_ptr,  # *Pointer* to output vector.\n+        n_elements,  # Size of the vector.\n+        BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n+                     # NOTE: `constexpr` so it can be used as a shape value.\n+    ):\n+        # There are multiple 'programs' processing different data. We identify which program\n+        # we are here:\n+        pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n+        # This program will process inputs that are offset from the initial data.\n+        # For instance, if you had a vector of length 256 and block_size of 64, the programs\n+        # would each access the elements [0:64, 64:128, 128:192, 192:256].\n+        # Note that offsets is a list of pointers:\n+        block_start = pid * BLOCK_SIZE\n+        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+        # Create a mask to guard memory operations against out-of-bounds accesses.\n+        mask = offsets < n_elements\n+        # Load x and y from DRAM, masking out any extra elements in case the input is not a\n+        # multiple of the block size.\n+        x = tl.load(x_ptr + offsets, mask=mask)\n+        y = tl.load(y_ptr + offsets, mask=mask)\n+        output = x + y\n+        # Write x + y back to DRAM.\n+        tl.store(output_ptr + offsets, output, mask=mask)\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 57-59\n+\n+Let's also declare a helper function to (1) allocate the `z` tensor\n+and (2) enqueue the above kernel with appropriate grid/block sizes:\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 59-80\n+\n+.. code-block:: default\n+\n+\n+\n+    def add(x: torch.Tensor, y: torch.Tensor):\n+        # We need to preallocate the output.\n+        output = torch.empty_like(x)\n+        assert x.is_cuda and y.is_cuda and output.is_cuda\n+        n_elements = output.numel()\n+        # The SPMD launch grid denotes the number of kernel instances that run in parallel.\n+        # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\n+        # In this case, we use a 1D grid where the size is the number of blocks:\n+        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+        # NOTE:\n+        #  - Each torch.tensor object is implicitly converted into a pointer to its first element.\n+        #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.\n+        #  - Don't forget to pass meta-parameters as keywords arguments.\n+        add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n+        # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\n+        # running asynchronously at this point.\n+        return output\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 81-82\n+\n+We can now use the above function to compute the element-wise sum of two `torch.tensor` objects and test its correctness:\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 82-96\n+\n+.. code-block:: default\n+\n+\n+    torch.manual_seed(0)\n+    size = 98432\n+    x = torch.rand(size, device='cuda')\n+    y = torch.rand(size, device='cuda')\n+    output_torch = x + y\n+    output_triton = add(x, y)\n+    print(output_torch)\n+    print(output_triton)\n+    print(\n+        f'The maximum difference between torch and triton is '\n+        f'{torch.max(torch.abs(output_torch - output_triton))}'\n+    )\n+\n+\n+\n+\n+\n+.. rst-class:: sphx-glr-script-out\n+\n+ .. code-block:: none\n+\n+    tensor([1.3713, 1.3076, 0.4940,  ..., 0.6724, 1.2141, 0.9733], device='cuda:0')\n+    tensor([1.3713, 1.3076, 0.4940,  ..., 0.6724, 1.2141, 0.9733], device='cuda:0')\n+    The maximum difference between torch and triton is 0.0\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 97-98\n+\n+Seems like we're good to go!\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 100-106\n+\n+Benchmark\n+---------\n+\n+We can now benchmark our custom op on vectors of increasing sizes to get a sense of how it does relative to PyTorch.\n+To make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of our custom ops.\n+for different problem sizes.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 106-136\n+\n+.. code-block:: default\n+\n+\n+\n+    @triton.testing.perf_report(\n+        triton.testing.Benchmark(\n+            x_names=['size'],  # Argument names to use as an x-axis for the plot.\n+            x_vals=[\n+                2 ** i for i in range(12, 28, 1)\n+            ],  # Different possible values for `x_name`.\n+            x_log=True,  # x axis is logarithmic.\n+            line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n+            line_vals=['triton', 'torch'],  # Possible values for `line_arg`.\n+            line_names=['Triton', 'Torch'],  # Label name for the lines.\n+            styles=[('blue', '-'), ('green', '-')],  # Line styles.\n+            ylabel='GB/s',  # Label name for the y-axis.\n+            plot_name='vector-add-performance',  # Name for the plot. Used also as a file name for saving the plot.\n+            args={},  # Values for function arguments not in `x_names` and `y_name`.\n+        )\n+    )\n+    def benchmark(size, provider):\n+        x = torch.rand(size, device='cuda', dtype=torch.float32)\n+        y = torch.rand(size, device='cuda', dtype=torch.float32)\n+        quantiles = [0.5, 0.2, 0.8]\n+        if provider == 'torch':\n+            ms, min_ms, max_ms = triton.testing.do_bench(lambda: x + y, quantiles=quantiles)\n+        if provider == 'triton':\n+            ms, min_ms, max_ms = triton.testing.do_bench(lambda: add(x, y), quantiles=quantiles)\n+        gbps = lambda ms: 12 * size / ms * 1e-6\n+        return gbps(ms), gbps(max_ms), gbps(min_ms)\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 137-139\n+\n+We can now run the decorated function above. Pass `print_data=True` to see the performance number, `show_plots=True` to plot them, and/or\n+`save_path='/path/to/results/' to save them to disk along with raw CSV data:\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 139-140\n+\n+.. code-block:: default\n+\n+    benchmark.run(print_data=True, show_plots=True)\n+\n+\n+\n+.. image-sg:: /getting-started/tutorials/images/sphx_glr_01-vector-add_001.png\n+   :alt: 01 vector add\n+   :srcset: /getting-started/tutorials/images/sphx_glr_01-vector-add_001.png\n+   :class: sphx-glr-single-img\n+\n+\n+.. rst-class:: sphx-glr-script-out\n+\n+ .. code-block:: none\n+\n+    vector-add-performance:\n+               size       Triton        Torch\n+    0        4096.0     9.600000     8.000000\n+    1        8192.0    19.200000    15.999999\n+    2       16384.0    31.999999    31.999999\n+    3       32768.0    63.999998    63.999998\n+    4       65536.0   127.999995   127.999995\n+    5      131072.0   219.428568   219.428568\n+    6      262144.0   384.000001   384.000001\n+    7      524288.0   614.400016   614.400016\n+    8     1048576.0   819.200021   819.200021\n+    9     2097152.0  1023.999964  1023.999964\n+    10    4194304.0  1228.800031  1228.800031\n+    11    8388608.0  1424.695621  1424.695621\n+    12   16777216.0  1560.380965  1560.380965\n+    13   33554432.0  1631.601649  1624.859540\n+    14   67108864.0  1669.706983  1662.646960\n+    15  134217728.0  1684.008546  1680.410210\n+\n+\n+\n+\n+\n+.. rst-class:: sphx-glr-timing\n+\n+   **Total running time of the script:** ( 0 minutes  5.689 seconds)\n+\n+\n+.. _sphx_glr_download_getting-started_tutorials_01-vector-add.py:\n+\n+.. only:: html\n+\n+  .. container:: sphx-glr-footer sphx-glr-footer-example\n+\n+\n+\n+\n+    .. container:: sphx-glr-download sphx-glr-download-python\n+\n+      :download:`Download Python source code: 01-vector-add.py <01-vector-add.py>`\n+\n+    .. container:: sphx-glr-download sphx-glr-download-jupyter\n+\n+      :download:`Download Jupyter notebook: 01-vector-add.ipynb <01-vector-add.ipynb>`\n+\n+\n+.. only:: html\n+\n+ .. rst-class:: sphx-glr-signature\n+\n+    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"}, {"filename": "main/_sources/getting-started/tutorials/02-fused-softmax.rst.txt", "status": "added", "additions": 341, "deletions": 0, "changes": 341, "file_content_changes": "@@ -0,0 +1,341 @@\n+\n+.. DO NOT EDIT.\n+.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.\n+.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:\n+.. \"getting-started/tutorials/02-fused-softmax.py\"\n+.. LINE NUMBERS ARE GIVEN BELOW.\n+\n+.. only:: html\n+\n+    .. note::\n+        :class: sphx-glr-download-link-note\n+\n+        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`\n+        to download the full example code\n+\n+.. rst-class:: sphx-glr-example-title\n+\n+.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:\n+\n+\n+Fused Softmax\n+=============\n+\n+In this tutorial, you will write a fused softmax operation that is significantly faster\n+than PyTorch's native op for a particular class of matrices: those whose rows can fit in\n+the GPU's SRAM.\n+\n+In doing so, you will learn about:\n+\n+* The benefits of kernel fusion for bandwidth-bound operations.\n+\n+* Reduction operators in Triton.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 18-23\n+\n+Motivations\n+-----------\n+\n+Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.\n+Let us consider instead the case of a simple (numerically stabilized) softmax operation:\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 23-51\n+\n+.. code-block:: default\n+\n+\n+    import torch\n+\n+    import triton\n+    import triton.language as tl\n+\n+\n+    @torch.jit.script\n+    def naive_softmax(x):\n+        \"\"\"Compute row-wise softmax of X using native pytorch\n+\n+        We subtract the maximum element in order to avoid overflows. Softmax is invariant to\n+        this shift.\n+        \"\"\"\n+        # read  MN elements ; write M  elements\n+        x_max = x.max(dim=1)[0]\n+        # read MN + M elements ; write MN elements\n+        z = x - x_max[:, None]\n+        # read  MN elements ; write MN elements\n+        numerator = torch.exp(z)\n+        # read  MN elements ; write M  elements\n+        denominator = numerator.sum(dim=1)\n+        # read MN + M elements ; write MN elements\n+        ret = numerator / denominator[:, None]\n+        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements\n+        return ret\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 52-60\n+\n+When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \\in R^{M \\times N}`\n+requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.\n+This is obviously wasteful; we'd prefer to have a custom \"fused\" kernel that only reads\n+X once and does all the necessary computations on-chip.\n+Doing so would require reading and writing back only :math:`MN` bytes, so we could\n+expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).\n+The `torch.jit.script` flags aims to perform this kind of \"kernel fusion\" automatically\n+but, as we will see later, it is still far from ideal.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 62-71\n+\n+Compute Kernel\n+--------------\n+\n+Our softmax kernel works as follows: each program loads a row of the input matrix X,\n+normalizes it and writes back the result to the output Y.\n+\n+Note that one important limitation of Triton is that each block must have a\n+power-of-two number of elements, so we need to internally \"pad\" each row and guard the\n+memory operations properly if we want to handle any possible input shapes:\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 71-100\n+\n+.. code-block:: default\n+\n+\n+\n+    @triton.jit\n+    def softmax_kernel(\n+        output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols,\n+        BLOCK_SIZE: tl.constexpr\n+    ):\n+        # The rows of the softmax are independent, so we parallelize across those\n+        row_idx = tl.program_id(0)\n+        # The stride represents how much we need to increase the pointer to advance 1 row\n+        row_start_ptr = input_ptr + row_idx * input_row_stride\n+        # The block size is the next power of two greater than n_cols, so we can fit each\n+        # row in a single block\n+        col_offsets = tl.arange(0, BLOCK_SIZE)\n+        input_ptrs = row_start_ptr + col_offsets\n+        # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n+        row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n+        # Subtract maximum for numerical stability\n+        row_minus_max = row - tl.max(row, axis=0)\n+        # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n+        numerator = tl.exp(row_minus_max)\n+        denominator = tl.sum(numerator, axis=0)\n+        softmax_output = numerator / denominator\n+        # Write back output to DRAM\n+        output_row_start_ptr = output_ptr + row_idx * output_row_stride\n+        output_ptrs = output_row_start_ptr + col_offsets\n+        tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 101-102\n+\n+We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 102-133\n+\n+.. code-block:: default\n+\n+\n+\n+    def softmax(x):\n+        n_rows, n_cols = x.shape\n+        # The block size is the smallest power of two greater than the number of columns in `x`\n+        BLOCK_SIZE = triton.next_power_of_2(n_cols)\n+        # Another trick we can use is to ask the compiler to use more threads per row by\n+        # increasing the number of warps (`num_warps`) over which each row is distributed.\n+        # You will see in the next tutorial how to auto-tune this value in a more natural\n+        # way so you don't have to come up with manual heuristics yourself.\n+        num_warps = 4\n+        if BLOCK_SIZE >= 2048:\n+            num_warps = 8\n+        if BLOCK_SIZE >= 4096:\n+            num_warps = 16\n+        # Allocate output\n+        y = torch.empty_like(x)\n+        # Enqueue kernel. The 1D launch grid is simple: we have one kernel instance per row o\n+        # f the input matrix\n+        softmax_kernel[(n_rows,)](\n+            y,\n+            x,\n+            x.stride(0),\n+            y.stride(0),\n+            n_cols,\n+            num_warps=num_warps,\n+            BLOCK_SIZE=BLOCK_SIZE,\n+        )\n+        return y\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 134-136\n+\n+Unit Test\n+---------\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 138-140\n+\n+We make sure that we test our kernel on a matrix with an irregular number of rows and columns.\n+This will allow us to verify that our padding mechanism works.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 140-147\n+\n+.. code-block:: default\n+\n+\n+    torch.manual_seed(0)\n+    x = torch.randn(1823, 781, device='cuda')\n+    y_triton = softmax(x)\n+    y_torch = torch.softmax(x, axis=1)\n+    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)\n+\n+\n+\n+\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 148-149\n+\n+As expected, the results are identical.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 151-156\n+\n+Benchmark\n+---------\n+\n+Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.\n+We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 156-196\n+\n+.. code-block:: default\n+\n+\n+\n+    @triton.testing.perf_report(\n+        triton.testing.Benchmark(\n+            x_names=['N'],  # argument names to use as an x-axis for the plot\n+            x_vals=[\n+                128 * i for i in range(2, 100)\n+            ],  # different possible values for `x_name`\n+            line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n+            line_vals=[\n+                'triton',\n+                'torch-native',\n+                'torch-jit',\n+            ],  # possible values for `line_arg``\n+            line_names=[\n+                \"Triton\",\n+                \"Torch (native)\",\n+                \"Torch (jit)\",\n+            ],  # label name for the lines\n+            styles=[('blue', '-'), ('green', '-'), ('green', '--')],  # line styles\n+            ylabel=\"GB/s\",  # label name for the y-axis\n+            plot_name=\"softmax-performance\",  # name for the plot. Used also as a file name for saving the plot.\n+            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\n+        )\n+    )\n+    def benchmark(M, N, provider):\n+        x = torch.randn(M, N, device='cuda', dtype=torch.float32)\n+        quantiles = [0.5, 0.2, 0.8]\n+        if provider == 'torch-native':\n+            ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1), quantiles=quantiles)\n+        if provider == 'triton':\n+            ms, min_ms, max_ms = triton.testing.do_bench(lambda: softmax(x), quantiles=quantiles)\n+        if provider == 'torch-jit':\n+            ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_softmax(x), quantiles=quantiles)\n+        gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\n+        return gbps(ms), gbps(max_ms), gbps(min_ms)\n+\n+\n+    benchmark.run(show_plots=True, print_data=True)\n+\n+\n+\n+\n+.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png\n+   :alt: 02 fused softmax\n+   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png\n+   :class: sphx-glr-single-img\n+\n+\n+.. rst-class:: sphx-glr-script-out\n+\n+ .. code-block:: none\n+\n+    softmax-performance:\n+              N       Triton  Torch (native)  Torch (jit)\n+    0     256.0   682.666643      744.727267   260.063494\n+    1     384.0   877.714274      819.200021   315.076934\n+    2     512.0   910.222190      910.222190   341.333321\n+    3     640.0  1024.000026      930.909084   372.363633\n+    4     768.0  1068.521715     1023.999964   384.000001\n+    ..      ...          ...             ...          ...\n+    93  12160.0  1594.754129     1066.082150   461.589552\n+    94  12288.0  1604.963246     1021.340281   462.063445\n+    95  12416.0  1602.064538     1031.979242   461.454135\n+    96  12544.0  1599.235121     1018.802024   460.858785\n+    97  12672.0  1596.472358     1008.716405   460.276950\n+\n+    [98 rows x 4 columns]\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 197-201\n+\n+In the above plot, we can see that:\n+ - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.\n+ - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.\n+   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.\n+\n+\n+.. rst-class:: sphx-glr-timing\n+\n+   **Total running time of the script:** ( 0 minutes  38.788 seconds)\n+\n+\n+.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:\n+\n+.. only:: html\n+\n+  .. container:: sphx-glr-footer sphx-glr-footer-example\n+\n+\n+\n+\n+    .. container:: sphx-glr-download sphx-glr-download-python\n+\n+      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`\n+\n+    .. container:: sphx-glr-download sphx-glr-download-jupyter\n+\n+      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`\n+\n+\n+.. only:: html\n+\n+ .. rst-class:: sphx-glr-signature\n+\n+    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"}, {"filename": "main/_sources/getting-started/tutorials/03-matrix-multiplication.rst.txt", "status": "added", "additions": 515, "deletions": 0, "changes": 515, "file_content_changes": "@@ -0,0 +1,515 @@\n+\n+.. DO NOT EDIT.\n+.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.\n+.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:\n+.. \"getting-started/tutorials/03-matrix-multiplication.py\"\n+.. LINE NUMBERS ARE GIVEN BELOW.\n+\n+.. only:: html\n+\n+    .. note::\n+        :class: sphx-glr-download-link-note\n+\n+        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_03-matrix-multiplication.py>`\n+        to download the full example code\n+\n+.. rst-class:: sphx-glr-example-title\n+\n+.. _sphx_glr_getting-started_tutorials_03-matrix-multiplication.py:\n+\n+\n+Matrix Multiplication\n+=====================\n+In this tutorial, you will write a very short high-performance FP16 matrix multiplication kernel that achieves\n+performance on parallel with cuBLAS.\n+\n+You will specifically learn about:\n+\n+* Block-level matrix multiplications.\n+\n+* Multi-dimensional pointer arithmetics.\n+\n+* Program re-ordering for improved L2 cache hit rate.\n+\n+* Automatic performance tuning.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 20-48\n+\n+Motivations\n+-----------\n+\n+Matrix multiplications are a key building block of most modern high-performance computing systems.\n+They are notoriously hard to optimize, hence their implementation is generally done by\n+hardware vendors themselves as part of so-called \"kernel libraries\" (e.g., cuBLAS).\n+Unfortunately, these libraries are often proprietary and cannot be easily customized\n+to accommodate the needs of modern deep learning workloads (e.g., fused activation functions).\n+In this tutorial, you will learn how to implement efficient matrix multiplications by\n+yourself with Triton, in a way that is easy to customize and extend.\n+\n+Roughly speaking, the kernel that we will write will implement the following blocked\n+algorithm to multiply a (M, K) by a (K, N) matrix:\n+\n+ .. code-block:: python\n+\n+   # Do in parallel\n+   for m in range(0, M, BLOCK_SIZE_M):\n+     # Do in parallel\n+     for n in range(0, N, BLOCK_SIZE_N):\n+       acc = zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=float32)\n+       for k in range(0, K, BLOCK_SIZE_K):\n+         a = A[m : m+BLOCK_SIZE_M, k : k+BLOCK_SIZE_K]\n+         b = B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]\n+         acc += dot(a, b)\n+       C[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N] = acc\n+\n+where each iteration of the doubly-nested for-loop is performed by a dedicated Triton program instance.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 50-148\n+\n+Compute Kernel\n+--------------\n+\n+The above algorithm is, actually, fairly straightforward to implement in Triton.\n+The main difficulty comes from the computation of the memory locations at which blocks\n+of :code:`A` and :code:`B` must be read in the inner loop. For that, we need\n+multi-dimensional pointer arithmetics.\n+\n+Pointer Arithmetics\n+~~~~~~~~~~~~~~~~~~~\n+\n+For a row-major 2D tensor :code:`X`, the memory location of :code:`X[i, j]` is given b\n+y :code:`&X[i, j] = X + i*stride_xi + j*stride_xj`.\n+Therefore, blocks of pointers for :code:`A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K]` and\n+:code:`B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]` can be defined in pseudo-code as:\n+\n+ .. code-block:: python\n+\n+   &A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K] =  a_ptr + (m : m+BLOCK_SIZE_M)[:, None]*A.stride(0) + (k : k+BLOCK_SIZE_K)[None, :]*A.stride(1);\n+   &B[k : k+BLOCK_SIZE_K, n:n+BLOCK_SIZE_N] =  b_ptr + (k : k+BLOCK_SIZE_K)[:, None]*B.stride(0) + (n : n+BLOCK_SIZE_N)[None, :]*B.stride(1);\n+\n+Which means that pointers for blocks of A and B can be initialized (i.e., :code:`k=0`) in Triton as the following\n+code. Also note that we need an extra modulo to handle the case where :code:`M` is not a multiple of\n+:code:`BLOCK_SIZE_M` or :code:`N` is not a multiple of :code:`BLOCK_SIZE_N`, in which case we can pad the data with\n+some useless values, which will not contribute to the results. For the :code:`K` dimension, we will handle that later\n+using masking load semantics.\n+\n+ .. code-block:: python\n+\n+   offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n+   offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n+   offs_k = tl.arange(0, BLOCK_SIZE_K)\n+   a_ptrs = a_ptr + (offs_am[:, None]*stride_am + offs_k [None, :]*stride_ak)\n+   b_ptrs = b_ptr + (offs_k [:, None]*stride_bk + offs_bn[None, :]*stride_bn)\n+\n+And then updated in the inner loop as follows:\n+\n+ .. code-block:: python\n+\n+   a_ptrs += BLOCK_SIZE_K * stride_ak;\n+   b_ptrs += BLOCK_SIZE_K * stride_bk;\n+\n+\n+L2 Cache Optimizations\n+~~~~~~~~~~~~~~~~~~~~~~\n+\n+As mentioned above, each program instance computes a :code:`[BLOCK_SIZE_M, BLOCK_SIZE_N]`\n+block of :code:`C`.\n+It is important to remember that the order in which these blocks are computed does\n+matter, since it affects the L2 cache hit rate of our program. and unfortunately, a\n+a simple row-major ordering\n+\n+ .. code-block:: Python\n+\n+   pid = triton.program_id(0);\n+   grid_m = (M + BLOCK_SIZE_M - 1) // BLOCK_SIZE_M;\n+   grid_n = (N + BLOCK_SIZE_N - 1) // BLOCK_SIZE_N;\n+   pid_m = pid / grid_n;\n+   pid_n = pid % grid_n;\n+\n+is just not going to cut it.\n+\n+One possible solution is to launch blocks in an order that promotes data reuse.\n+This can be done by 'super-grouping' blocks in groups of :code:`GROUP_M` rows before\n+switching to the next column:\n+\n+ .. code-block:: python\n+\n+   # Program ID\n+   pid = tl.program_id(axis=0)\n+   # Number of program ids along the M axis\n+   num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n+   # Number of programs ids along the N axis\n+   num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+   # Number of programs in group\n+   num_pid_in_group = GROUP_SIZE_M * num_pid_n\n+   # Id of the group this program is in\n+   group_id = pid // num_pid_in_group\n+   # Row-id of the first program in the group\n+   first_pid_m = group_id * GROUP_SIZE_M\n+   # If `num_pid_m` isn't divisible by `GROUP_SIZE_M`, the last group is smaller\n+   group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n+   # *Within groups*, programs are ordered in a column-major order\n+   # Row-id of the program in the *launch grid*\n+   pid_m = first_pid_m + (pid % group_size_m)\n+   # Col-id of the program in the *launch grid*\n+   pid_n = (pid % num_pid_in_group) // group_size_m\n+\n+For example, in the following matmul where each matrix is 9 blocks by 9 blocks,\n+we can see that if we compute the output in row-major ordering, we need to load 90\n+blocks into SRAM to compute the first 9 output blocks, but if we do it in grouped\n+ordering, we only need to load 54 blocks.\n+\n+  .. image:: grouped_vs_row_major_ordering.png\n+\n+In practice, this can improve the performance of our matrix multiplication kernel by\n+more than 10\\% on some hardware architecture (e.g., 220 to 245 TFLOPS on A100).\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 150-152\n+\n+Final Result\n+------------\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 152-262\n+\n+.. code-block:: default\n+\n+\n+    import torch\n+\n+    import triton\n+    import triton.language as tl\n+\n+\n+    # `triton.jit`'ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:\n+    #   - A list of `triton.Config` objects that define different configurations of\n+    #       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try\n+    #   - An auto-tuning *key* whose change in values will trigger evaluation of all the\n+    #       provided configs\n+    @triton.autotune(\n+        configs=[\n+            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n+            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n+            triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n+        ],\n+        key=['M', 'N', 'K'],\n+    )\n+    @triton.jit\n+    def matmul_kernel(\n+        # Pointers to matrices\n+        a_ptr, b_ptr, c_ptr,\n+        # Matrix dimensions\n+        M, N, K,\n+        # The stride variables represent how much to increase the ptr by when moving by 1\n+        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n+        # by to get the element one row down (A has M rows).\n+        stride_am, stride_ak,\n+        stride_bk, stride_bn,\n+        stride_cm, stride_cn,\n+        # Meta-parameters\n+        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+        GROUP_SIZE_M: tl.constexpr,\n+        ACTIVATION: tl.constexpr,\n+    ):\n+        \"\"\"Kernel for computing the matmul C = A x B.\n+        A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n+        \"\"\"\n+        # -----------------------------------------------------------\n+        # Map program ids `pid` to the block of C it should compute.\n+        # This is done in a grouped ordering to promote L2 data reuse.\n+        # See above `L2 Cache Optimizations` section for details.\n+        pid = tl.program_id(axis=0)\n+        num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n+        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+        num_pid_in_group = GROUP_SIZE_M * num_pid_n\n+        group_id = pid // num_pid_in_group\n+        first_pid_m = group_id * GROUP_SIZE_M\n+        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n+        pid_m = first_pid_m + (pid % group_size_m)\n+        pid_n = (pid % num_pid_in_group) // group_size_m\n+\n+        # ----------------------------------------------------------\n+        # Create pointers for the first blocks of A and B.\n+        # We will advance this pointer as we move in the K direction\n+        # and accumulate\n+        # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n+        # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n+        # See above `Pointer Arithmetics` section for details\n+        offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n+        offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n+        offs_k = tl.arange(0, BLOCK_SIZE_K)\n+        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n+        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n+\n+        # -----------------------------------------------------------\n+        # Iterate to compute a block of the C matrix.\n+        # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n+        # of fp32 values for higher accuracy.\n+        # `accumulator` will be converted back to fp16 after the loop.\n+        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n+            # Load the next block of A and B, generate a mask by checking the K dimension.\n+            # If it is out of bounds, set it to 0.\n+            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n+            b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n+            # We accumulate along the K dimension.\n+            accumulator += tl.dot(a, b)\n+            # Advance the ptrs to the next K block.\n+            a_ptrs += BLOCK_SIZE_K * stride_ak\n+            b_ptrs += BLOCK_SIZE_K * stride_bk\n+        # You can fuse arbitrary activation functions here\n+        # while the accumulator is still in FP32!\n+        if ACTIVATION == \"leaky_relu\":\n+            accumulator = leaky_relu(accumulator)\n+        c = accumulator.to(tl.float16)\n+\n+        # -----------------------------------------------------------\n+        # Write back the block of the output matrix C with masks.\n+        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+        c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n+        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n+        tl.store(c_ptrs, c, mask=c_mask)\n+\n+\n+    # We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.\n+    @triton.jit\n+    def leaky_relu(x):\n+        x = x + 1\n+        return tl.where(x >= 0, x, 0.01 * x)\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 263-265\n+\n+We can now create a convenience wrapper function that only takes two input tensors,\n+and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 265-291\n+\n+.. code-block:: default\n+\n+\n+\n+    def matmul(a, b, activation=\"\"):\n+        # Check constraints.\n+        assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n+        assert a.is_contiguous(), \"Matrix A must be contiguous\"\n+        assert b.is_contiguous(), \"Matrix B must be contiguous\"\n+        M, K = a.shape\n+        K, N = b.shape\n+        # Allocates output.\n+        c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n+        # 1D launch kernel where each block gets its own program.\n+        grid = lambda META: (\n+            triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n+        )\n+        matmul_kernel[grid](\n+            a, b, c,\n+            M, N, K,\n+            a.stride(0), a.stride(1),\n+            b.stride(0), b.stride(1),\n+            c.stride(0), c.stride(1),\n+            ACTIVATION=activation\n+        )\n+        return c\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 292-296\n+\n+Unit Test\n+---------\n+\n+We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS).\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 296-309\n+\n+.. code-block:: default\n+\n+\n+    torch.manual_seed(0)\n+    a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n+    b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n+    triton_output = matmul(a, b)\n+    torch_output = torch.matmul(a, b)\n+    print(f\"triton_output={triton_output}\")\n+    print(f\"torch_output={torch_output}\")\n+    if torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\n+        print(\"\u2705 Triton and Torch match\")\n+    else:\n+        print(\"\u274c Triton and Torch differ\")\n+\n+\n+\n+\n+\n+.. rst-class:: sphx-glr-script-out\n+\n+ .. code-block:: none\n+\n+    triton_output=tensor([[-10.9531,  -4.7109,  15.6953,  ..., -28.4062,   4.3320, -26.4219],\n+            [ 26.8438,  10.0469,  -5.4297,  ..., -11.2969,  -8.5312,  30.7500],\n+            [-13.2578,  15.8516,  18.0781,  ..., -21.7656,  -8.6406,  10.2031],\n+            ...,\n+            [ 40.2812,  18.6094, -25.6094,  ...,  -2.7598,  -3.2441,  41.0000],\n+            [ -6.1211, -16.8281,   4.4844,  ..., -21.0312,  24.7031,  15.0234],\n+            [-17.0938, -19.0000,  -0.3831,  ...,  21.5469, -30.2344, -13.2188]],\n+           device='cuda:0', dtype=torch.float16)\n+    torch_output=tensor([[-10.9531,  -4.7109,  15.6953,  ..., -28.4062,   4.3320, -26.4219],\n+            [ 26.8438,  10.0469,  -5.4297,  ..., -11.2969,  -8.5312,  30.7500],\n+            [-13.2578,  15.8516,  18.0781,  ..., -21.7656,  -8.6406,  10.2031],\n+            ...,\n+            [ 40.2812,  18.6094, -25.6094,  ...,  -2.7598,  -3.2441,  41.0000],\n+            [ -6.1211, -16.8281,   4.4844,  ..., -21.0312,  24.7031,  15.0234],\n+            [-17.0938, -19.0000,  -0.3831,  ...,  21.5469, -30.2344, -13.2188]],\n+           device='cuda:0', dtype=torch.float16)\n+    \u2705 Triton and Torch match\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 310-318\n+\n+Benchmark\n+---------\n+\n+Square Matrix Performance\n+~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+We can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices,\n+but feel free to arrange this script as you wish to benchmark any other matrix shape.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 318-351\n+\n+.. code-block:: default\n+\n+\n+\n+    @triton.testing.perf_report(\n+        triton.testing.Benchmark(\n+            x_names=['M', 'N', 'K'],  # Argument names to use as an x-axis for the plot\n+            x_vals=[\n+                128 * i for i in range(2, 33)\n+            ],  # Different possible values for `x_name`\n+            line_arg='provider',  # Argument name whose value corresponds to a different line in the plot\n+            # Possible values for `line_arg`\n+            line_vals=['cublas', 'triton'],\n+            # Label name for the lines\n+            line_names=[\"cuBLAS\", \"Triton\"],\n+            # Line styles\n+            styles=[('green', '-'), ('blue', '-')],\n+            ylabel=\"TFLOPS\",  # Label name for the y-axis\n+            plot_name=\"matmul-performance\",  # Name for the plot, used also as a file name for saving the plot.\n+            args={},\n+        )\n+    )\n+    def benchmark(M, N, K, provider):\n+        a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n+        b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n+        quantiles = [0.5, 0.2, 0.8]\n+        if provider == 'cublas':\n+            ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\n+        if provider == 'triton':\n+            ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\n+        perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n+        return perf(ms), perf(max_ms), perf(min_ms)\n+\n+\n+    benchmark.run(show_plots=True, print_data=True)\n+\n+\n+\n+.. image-sg:: /getting-started/tutorials/images/sphx_glr_03-matrix-multiplication_001.png\n+   :alt: 03 matrix multiplication\n+   :srcset: /getting-started/tutorials/images/sphx_glr_03-matrix-multiplication_001.png\n+   :class: sphx-glr-single-img\n+\n+\n+.. rst-class:: sphx-glr-script-out\n+\n+ .. code-block:: none\n+\n+    matmul-performance:\n+             M       N       K      cuBLAS      Triton\n+    0    256.0   256.0   256.0    4.681143    4.096000\n+    1    384.0   384.0   384.0   12.288000   12.288000\n+    2    512.0   512.0   512.0   26.214401   23.831273\n+    3    640.0   640.0   640.0   42.666665   39.384616\n+    4    768.0   768.0   768.0   63.195428   58.982401\n+    5    896.0   896.0   896.0   78.051553   82.642822\n+    6   1024.0  1024.0  1024.0  110.376426   99.864382\n+    7   1152.0  1152.0  1152.0  135.726544  129.825388\n+    8   1280.0  1280.0  1280.0  163.840004  163.840004\n+    9   1408.0  1408.0  1408.0  155.765024  132.970149\n+    10  1536.0  1536.0  1536.0  181.484314  157.286398\n+    11  1664.0  1664.0  1664.0  183.651271  179.978245\n+    12  1792.0  1792.0  1792.0  172.914215  208.137481\n+    13  1920.0  1920.0  1920.0  200.347822  168.585369\n+    14  2048.0  2048.0  2048.0  220.752852  190.650180\n+    15  2176.0  2176.0  2176.0  209.621326  207.460296\n+    16  2304.0  2304.0  2304.0  225.357284  225.357284\n+    17  2432.0  2432.0  2432.0  199.251522  200.674737\n+    18  2560.0  2560.0  2560.0  224.438347  215.578957\n+    19  2688.0  2688.0  2688.0  196.544332  198.602388\n+    20  2816.0  2816.0  2816.0  210.696652  207.686706\n+    21  2944.0  2944.0  2944.0  219.541994  223.479969\n+    22  3072.0  3072.0  3072.0  207.410628  206.653671\n+    23  3200.0  3200.0  3200.0  214.046818  216.216207\n+    24  3328.0  3328.0  3328.0  203.941342  208.670419\n+    25  3456.0  3456.0  3456.0  215.565692  216.724640\n+    26  3584.0  3584.0  3584.0  218.772251  210.082692\n+    27  3712.0  3712.0  3712.0  209.868376  213.455857\n+    28  3840.0  3840.0  3840.0  208.271176  208.271176\n+    29  3968.0  3968.0  3968.0  207.523702  216.354501\n+    30  4096.0  4096.0  4096.0  219.668951  215.437756\n+\n+\n+\n+\n+\n+.. rst-class:: sphx-glr-timing\n+\n+   **Total running time of the script:** ( 0 minutes  40.563 seconds)\n+\n+\n+.. _sphx_glr_download_getting-started_tutorials_03-matrix-multiplication.py:\n+\n+.. only:: html\n+\n+  .. container:: sphx-glr-footer sphx-glr-footer-example\n+\n+\n+\n+\n+    .. container:: sphx-glr-download sphx-glr-download-python\n+\n+      :download:`Download Python source code: 03-matrix-multiplication.py <03-matrix-multiplication.py>`\n+\n+    .. container:: sphx-glr-download sphx-glr-download-jupyter\n+\n+      :download:`Download Jupyter notebook: 03-matrix-multiplication.ipynb <03-matrix-multiplication.ipynb>`\n+\n+\n+.. only:: html\n+\n+ .. rst-class:: sphx-glr-signature\n+\n+    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"}, {"filename": "main/_sources/getting-started/tutorials/04-low-memory-dropout.rst.txt", "status": "added", "additions": 270, "deletions": 0, "changes": 270, "file_content_changes": "@@ -0,0 +1,270 @@\n+\n+.. DO NOT EDIT.\n+.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.\n+.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:\n+.. \"getting-started/tutorials/04-low-memory-dropout.py\"\n+.. LINE NUMBERS ARE GIVEN BELOW.\n+\n+.. only:: html\n+\n+    .. note::\n+        :class: sphx-glr-download-link-note\n+\n+        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_04-low-memory-dropout.py>`\n+        to download the full example code\n+\n+.. rst-class:: sphx-glr-example-title\n+\n+.. _sphx_glr_getting-started_tutorials_04-low-memory-dropout.py:\n+\n+\n+Low-Memory Dropout\n+==================\n+\n+In this tutorial, you will write a memory-efficient implementation of dropout whose state\n+will be composed of a single int32 seed. This differs from more traditional implementations of dropout,\n+whose state is generally composed of a bit mask tensor of the same shape as the input.\n+\n+In doing so, you will learn about:\n+\n+* The limitations of naive implementations of Dropout with PyTorch.\n+\n+* Parallel pseudo-random number generation in Triton.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 18-34\n+\n+Baseline\n+--------\n+\n+The *dropout* operator was first introduced in [SRIVASTAVA2014]_ as a way to improve the performance\n+of deep neural networks in low-data regime (i.e. regularization).\n+\n+It takes a vector as input and produces a vector of the same shape as output. Each scalar in the\n+output has a probability :math:`p` of being changed to zero and otherwise it is copied from the input.\n+This forces the network to perform well even when only :math:`1 - p` scalars from the input are available.\n+\n+At evaluation time we want to use the full power of the network so we set :math:`p=0`. Naively this would\n+increase the norm of the output (which can be a bad thing, e.g. it can lead to artificial decrease\n+in the output softmax temperature). To prevent this we multiply the output by :math:`\\frac{1}{1 - p}`, which\n+keeps the norm consistent regardless of the dropout probability.\n+\n+Let's first take a look at the baseline implementation.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 34-87\n+\n+.. code-block:: default\n+\n+\n+\n+    import tabulate\n+    import torch\n+\n+    import triton\n+    import triton.language as tl\n+\n+\n+    @triton.jit\n+    def _dropout(\n+        x_ptr,  # pointer to the input\n+        x_keep_ptr,  # pointer to a mask of 0s and 1s\n+        output_ptr,  # pointer to the output\n+        n_elements,  # number of elements in the `x` tensor\n+        p,  # probability that an element of `x` is changed to zero\n+        BLOCK_SIZE: tl.constexpr,\n+    ):\n+        pid = tl.program_id(axis=0)\n+        block_start = pid * BLOCK_SIZE\n+        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+        mask = offsets < n_elements\n+        # Load data\n+        x = tl.load(x_ptr + offsets, mask=mask)\n+        x_keep = tl.load(x_keep_ptr + offsets, mask=mask)\n+        # The line below is the crucial part, described in the paragraph above!\n+        output = tl.where(x_keep, x / (1 - p), 0.0)\n+        # Write-back output\n+        tl.store(output_ptr + offsets, output, mask=mask)\n+\n+\n+    def dropout(x, x_keep, p):\n+        output = torch.empty_like(x)\n+        assert x.is_contiguous()\n+        n_elements = x.numel()\n+        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+        _dropout[grid](x, x_keep, output, n_elements, p, BLOCK_SIZE=1024)\n+        return output\n+\n+\n+    # Input tensor\n+    x = torch.randn(size=(10,)).cuda()\n+    # Dropout mask\n+    p = 0.5\n+    x_keep = (torch.rand(size=(10,)) > p).to(torch.int32).cuda()\n+    #\n+    output = dropout(x, x_keep=x_keep, p=p)\n+    print(tabulate.tabulate([\n+        [\"input\"] + x.tolist(),\n+        [\"keep mask\"] + x_keep.tolist(),\n+        [\"output\"] + output.tolist()\n+    ]))\n+\n+\n+\n+\n+\n+.. rst-class:: sphx-glr-script-out\n+\n+ .. code-block:: none\n+\n+    ---------  -------  ---------  --------  --------  --------  --------  --------  --------  ---------  ---------\n+    input      1.541    -0.293429  -2.17879  0.568431  -1.08452  -1.3986   0.403347  0.838026  -0.719258  -0.403344\n+    keep mask  1         1          0        1          0         1        1         0          0          0\n+    output     3.08199  -0.586858   0        1.13686    0        -2.79719  0.806694  0          0          0\n+    ---------  -------  ---------  --------  --------  --------  --------  --------  --------  ---------  ---------\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 88-107\n+\n+Seeded dropout\n+--------------\n+\n+The above implementation of dropout works fine, but it can be a bit awkward to deal with. Firstly\n+we need to store the dropout mask for backpropagation. Secondly, dropout state management can get\n+very tricky when using recompute/checkpointing (e.g. see all the notes about `preserve_rng_state` in\n+https://pytorch.org/docs/1.9.0/checkpoint.html). In this tutorial we'll describe an alternative implementation\n+that (1) has a smaller memory footprint; (2) requires less data movement; and (3) simplifies the management\n+of persisting randomness across multiple invocations of the kernel.\n+\n+Pseudo-random number generation in Triton is simple! In this tutorial we will use the\n+:code:`triton.language.rand` function which generates a block of uniformly distributed :code:`float32`\n+values in [0, 1), given a seed and a block of :code:`int32` offsets. But if you need it, Triton also provides\n+other :ref:`random number generation strategies <Random Number Generation>`.\n+\n+.. note::\n+   Triton's implementation of PRNG is based on the Philox algorithm (described on [SALMON2011]_).\n+\n+Let's put it all together.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 107-155\n+\n+.. code-block:: default\n+\n+\n+\n+    @triton.jit\n+    def _seeded_dropout(\n+        x_ptr,\n+        output_ptr,\n+        n_elements,\n+        p,\n+        seed,\n+        BLOCK_SIZE: tl.constexpr,\n+    ):\n+        # compute memory offsets of elements handled by this instance\n+        pid = tl.program_id(axis=0)\n+        block_start = pid * BLOCK_SIZE\n+        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+        # load data from x\n+        mask = offsets < n_elements\n+        x = tl.load(x_ptr + offsets, mask=mask)\n+        # randomly prune it\n+        random = tl.rand(seed, offsets)\n+        x_keep = random > p\n+        # write-back\n+        output = tl.where(x_keep, x / (1 - p), 0.0)\n+        tl.store(output_ptr + offsets, output, mask=mask)\n+\n+\n+    def seeded_dropout(x, p, seed):\n+        output = torch.empty_like(x)\n+        assert x.is_contiguous()\n+        n_elements = x.numel()\n+        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+        _seeded_dropout[grid](x, output, n_elements, p, seed, BLOCK_SIZE=1024)\n+        return output\n+\n+\n+    x = torch.randn(size=(10,)).cuda()\n+    # Compare this to the baseline - dropout mask is never instantiated!\n+    output = seeded_dropout(x, p=0.5, seed=123)\n+    output2 = seeded_dropout(x, p=0.5, seed=123)\n+    output3 = seeded_dropout(x, p=0.5, seed=512)\n+\n+    print(tabulate.tabulate([\n+        [\"input\"] + x.tolist(),\n+        [\"output (seed = 123)\"] + output.tolist(),\n+        [\"output (seed = 123)\"] + output2.tolist(),\n+        [\"output (seed = 512)\"] + output3.tolist()\n+    ]))\n+\n+\n+\n+\n+\n+.. rst-class:: sphx-glr-script-out\n+\n+ .. code-block:: none\n+\n+    -------------------  ---------  --------  --------  -------  --------  --------  ---------  ---------  ---------  ---------\n+    input                -0.952835  0.371721  0.408716  1.42142  0.149397  -0.67086  -0.214186  -0.431969  -0.707878  -0.106434\n+    output (seed = 123)   0         0.743443  0         0        0         -1.34172   0          0         -1.41576   -0.212868\n+    output (seed = 123)   0         0.743443  0         0        0         -1.34172   0          0         -1.41576   -0.212868\n+    output (seed = 512)   0         0         0.817432  2.84284  0         -1.34172  -0.428372   0          0          0\n+    -------------------  ---------  --------  --------  -------  --------  --------  ---------  ---------  ---------  ---------\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 156-159\n+\n+Et Voil\u00e0! We have a triton kernel that applies the same dropout mask provided the seed is the same!\n+If you'd like explore further applications of pseudorandomness in GPU programming, we encourage you\n+to explore the `triton/language/random` folder!\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 161-167\n+\n+Exercises\n+---------\n+\n+1. Extend the kernel to operate over a matrix and use a vector of seeds - one per row.\n+2. Add support for striding.\n+3. (challenge) Implement a kernel for sparse Johnson-Lindenstrauss transform which generates the projection matrix one the fly each time using a seed.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 169-174\n+\n+References\n+----------\n+\n+.. [SALMON2011] John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw, \"Parallel Random Numbers: As Easy as 1, 2, 3\", 2011\n+.. [SRIVASTAVA2014] Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\", JMLR 2014\n+\n+\n+.. rst-class:: sphx-glr-timing\n+\n+   **Total running time of the script:** ( 0 minutes  0.642 seconds)\n+\n+\n+.. _sphx_glr_download_getting-started_tutorials_04-low-memory-dropout.py:\n+\n+.. only:: html\n+\n+  .. container:: sphx-glr-footer sphx-glr-footer-example\n+\n+\n+\n+\n+    .. container:: sphx-glr-download sphx-glr-download-python\n+\n+      :download:`Download Python source code: 04-low-memory-dropout.py <04-low-memory-dropout.py>`\n+\n+    .. container:: sphx-glr-download sphx-glr-download-jupyter\n+\n+      :download:`Download Jupyter notebook: 04-low-memory-dropout.ipynb <04-low-memory-dropout.ipynb>`\n+\n+\n+.. only:: html\n+\n+ .. rst-class:: sphx-glr-signature\n+\n+    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"}, {"filename": "main/_sources/getting-started/tutorials/05-layer-norm.rst.txt", "status": "added", "additions": 505, "deletions": 0, "changes": 505, "file_content_changes": "@@ -0,0 +1,505 @@\n+\n+.. DO NOT EDIT.\n+.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.\n+.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:\n+.. \"getting-started/tutorials/05-layer-norm.py\"\n+.. LINE NUMBERS ARE GIVEN BELOW.\n+\n+.. only:: html\n+\n+    .. note::\n+        :class: sphx-glr-download-link-note\n+\n+        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_05-layer-norm.py>`\n+        to download the full example code\n+\n+.. rst-class:: sphx-glr-example-title\n+\n+.. _sphx_glr_getting-started_tutorials_05-layer-norm.py:\n+\n+\n+Layer Normalization\n+====================\n+In this tutorial, you will write a high-performance layer normalization\n+kernel that runs faster than the PyTorch implementation.\n+\n+In doing so, you will learn about:\n+\n+* Implementing backward pass in Triton.\n+\n+* Implementing parallel reduction in Triton.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 16-31\n+\n+Motivations\n+-----------\n+\n+The *LayerNorm* operator was first introduced in [BA2016]_ as a way to improve the performance\n+of sequential models (e.g., Transformers) or neural networks with small batch size.\n+It takes a vector :math:`x` as input and produces a vector :math:`y` of the same shape as output.\n+The normalization is performed by subtracting the mean and dividing by the standard deviation of :math:`x`.\n+After the normalization, a learnable linear transformation with weights :math:`w` and biases :math:`b` is applied.\n+The forward pass can be expressed as follows:\n+\n+.. math::\n+   y = \\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} } * w + b\n+\n+where :math:`\\epsilon` is a small constant added to the denominator for numerical stability.\n+Let\u2019s first take a look at the forward pass implementation.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 31-96\n+\n+.. code-block:: default\n+\n+\n+    import torch\n+\n+    import triton\n+    import triton.language as tl\n+\n+    try:\n+        # This is https://github.com/NVIDIA/apex, NOT the apex on PyPi, so it\n+        # should not be added to extras_require in setup.py.\n+        import apex\n+        HAS_APEX = True\n+    except ModuleNotFoundError:\n+        HAS_APEX = False\n+\n+\n+    @triton.jit\n+    def _layer_norm_fwd_fused(\n+        X,  # pointer to the input\n+        Y,  # pointer to the output\n+        W,  # pointer to the weights\n+        B,  # pointer to the biases\n+        Mean,  # pointer to the mean\n+        Rstd,  # pointer to the 1/std\n+        stride,  # how much to increase the pointer when moving by 1 row\n+        N,  # number of columns in X\n+        eps,  # epsilon to avoid division by zero\n+        BLOCK_SIZE: tl.constexpr,\n+    ):\n+        # Map the program id to the row of X and Y it should compute.\n+        row = tl.program_id(0)\n+        Y += row * stride\n+        X += row * stride\n+        # Compute mean\n+        mean = 0\n+        _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n+        for off in range(0, N, BLOCK_SIZE):\n+            cols = off + tl.arange(0, BLOCK_SIZE)\n+            a = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n+            _mean += a\n+        mean = tl.sum(_mean, axis=0) / N\n+        # Compute variance\n+        _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n+        for off in range(0, N, BLOCK_SIZE):\n+            cols = off + tl.arange(0, BLOCK_SIZE)\n+            x = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n+            x = tl.where(cols < N, x - mean, 0.)\n+            _var += x * x\n+        var = tl.sum(_var, axis=0) / N\n+        rstd = 1 / tl.sqrt(var + eps)\n+        # Write mean / rstd\n+        tl.store(Mean + row, mean)\n+        tl.store(Rstd + row, rstd)\n+        # Normalize and apply linear transformation\n+        for off in range(0, N, BLOCK_SIZE):\n+            cols = off + tl.arange(0, BLOCK_SIZE)\n+            mask = cols < N\n+            w = tl.load(W + cols, mask=mask)\n+            b = tl.load(B + cols, mask=mask)\n+            x = tl.load(X + cols, mask=mask, other=0.).to(tl.float32)\n+            x_hat = (x - mean) * rstd\n+            y = x_hat * w + b\n+            # Write output\n+            tl.store(Y + cols, y, mask=mask)\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 97-128\n+\n+Backward pass\n+-------------\n+\n+The backward pass for the layer normalization operator is a bit more involved than the forward pass.\n+Let :math:`\\hat{x}` be the normalized inputs :math:`\\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} }` before the linear transformation,\n+the Vector-Jacobian Products (VJP) :math:`\\nabla_{x}` of :math:`x` are given by:\n+\n+.. math::\n+   \\nabla_{x} = \\frac{1}{\\sigma}\\Big( \\nabla_{y} \\odot w - \\underbrace{ \\big( \\frac{1}{N} \\hat{x} \\cdot (\\nabla_{y} \\odot w) \\big) }_{c_1} \\odot \\hat{x} - \\underbrace{ \\frac{1}{N} \\nabla_{y} \\cdot w }_{c_2} \\Big)\n+\n+where :math:`\\odot` denotes the element-wise multiplication, :math:`\\cdot` denotes the dot product, and :math:`\\sigma` is the standard deviation.\n+:math:`c_1` and :math:`c_2` are intermediate constants that improve the readability of the following implementation.\n+\n+For the weights :math:`w` and biases :math:`b`, the VJPs :math:`\\nabla_{w}` and :math:`\\nabla_{b}` are more straightforward:\n+\n+.. math::\n+   \\nabla_{w} = \\nabla_{y} \\odot \\hat{x} \\quad \\text{and} \\quad \\nabla_{b} = \\nabla_{y}\n+\n+Since the same weights :math:`w` and biases :math:`b` are used for all rows in the same batch, their gradients need to sum up.\n+To perform this step efficiently, we use a parallel reduction strategy: each kernel instance accumulates\n+partial :math:`\\nabla_{w}` and :math:`\\nabla_{b}` across certain rows into one of :math:`\\text{GROUP_SIZE_M}` independent buffers.\n+These buffers stay in the L2 cache and then are further reduced by another function to compute the actual :math:`\\nabla_{w}` and :math:`\\nabla_{b}`.\n+\n+Let the number of input rows :math:`M = 4` and :math:`\\text{GROUP_SIZE_M} = 2`,\n+here's a diagram of the parallel reduction strategy for :math:`\\nabla_{w}` (:math:`\\nabla_{b}` is omitted for brevity):\n+\n+  .. image:: parallel_reduction.png\n+\n+In Stage 1, the rows of X that have the same color share the same buffer and thus a lock is used to ensure that only one kernel instance writes to the buffer at a time.\n+In Stage 2, the buffers are further reduced to compute the final :math:`\\nabla_{w}` and :math:`\\nabla_{b}`.\n+In the following implementation, Stage 1 is implemented by the function :code:`_layer_norm_bwd_dx_fused` and Stage 2 is implemented by the function :code:`_layer_norm_bwd_dwdb`.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 128-224\n+\n+.. code-block:: default\n+\n+\n+    @triton.jit\n+    def _layer_norm_bwd_dx_fused(\n+        DX,  # pointer to the input gradient\n+        DY,  # pointer to the output gradient\n+        DW,  # pointer to the partial sum of weights gradient\n+        DB,  # pointer to the partial sum of biases gradient\n+        X,   # pointer to the input\n+        W,   # pointer to the weights\n+        B,   # pointer to the biases\n+        Mean,   # pointer to the mean\n+        Rstd,   # pointer to the 1/std\n+        Lock,  # pointer to the lock\n+        stride,  # how much to increase the pointer when moving by 1 row\n+        N,  # number of columns in X\n+        eps,  # epsilon to avoid division by zero\n+        GROUP_SIZE_M: tl.constexpr,\n+        BLOCK_SIZE_N: tl.constexpr\n+    ):\n+        # Map the program id to the elements of X, DX, and DY it should compute.\n+        row = tl.program_id(0)\n+        cols = tl.arange(0, BLOCK_SIZE_N)\n+        mask = cols < N\n+        X += row * stride\n+        DY += row * stride\n+        DX += row * stride\n+        # Offset locks and weights/biases gradient pointer for parallel reduction\n+        lock_id = row % GROUP_SIZE_M\n+        Lock += lock_id\n+        Count = Lock + GROUP_SIZE_M\n+        DW = DW + lock_id * N + cols\n+        DB = DB + lock_id * N + cols\n+        # Load data to SRAM\n+        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n+        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n+        w = tl.load(W + cols, mask=mask).to(tl.float32)\n+        mean = tl.load(Mean + row)\n+        rstd = tl.load(Rstd + row)\n+        # Compute dx\n+        xhat = (x - mean) * rstd\n+        wdy = w * dy\n+        xhat = tl.where(mask, xhat, 0.)\n+        wdy = tl.where(mask, wdy, 0.)\n+        c1 = tl.sum(xhat * wdy, axis=0) / N\n+        c2 = tl.sum(wdy, axis=0) / N\n+        dx = (wdy - (xhat * c1 + c2)) * rstd\n+        # Write dx\n+        tl.store(DX + cols, dx, mask=mask)\n+        # Accumulate partial sums for dw/db\n+        partial_dw = (dy * xhat).to(w.dtype)\n+        partial_db = (dy).to(w.dtype)\n+        while tl.atomic_cas(Lock, 0, 1) == 1:\n+            pass\n+        count = tl.load(Count)\n+        # First store doesn't accumulate\n+        if count == 0:\n+            tl.atomic_xchg(Count, 1)\n+        else:\n+            partial_dw += tl.load(DW, mask=mask)\n+            partial_db += tl.load(DB, mask=mask)\n+        tl.store(DW, partial_dw, mask=mask)\n+        tl.store(DB, partial_db, mask=mask)\n+        # Release the lock\n+        tl.atomic_xchg(Lock, 0)\n+\n+\n+    @triton.jit\n+    def _layer_norm_bwd_dwdb(\n+        DW,  # pointer to the partial sum of weights gradient\n+        DB,  # pointer to the partial sum of biases gradient\n+        FINAL_DW,  # pointer to the weights gradient\n+        FINAL_DB,  # pointer to the biases gradient\n+        M,  # GROUP_SIZE_M\n+        N,  # number of columns\n+        BLOCK_SIZE_M: tl.constexpr,\n+        BLOCK_SIZE_N: tl.constexpr\n+    ):\n+        # Map the program id to the elements of DW and DB it should compute.\n+        pid = tl.program_id(0)\n+        cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+        dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+        db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+        # Iterate through the rows of DW and DB to sum the partial sums.\n+        for i in range(0, M, BLOCK_SIZE_M):\n+            rows = i + tl.arange(0, BLOCK_SIZE_M)\n+            mask = (rows[:, None] < M) & (cols[None, :] < N)\n+            offs = rows[:, None] * N + cols[None, :]\n+            dw += tl.load(DW + offs, mask=mask, other=0.)\n+            db += tl.load(DB + offs, mask=mask, other=0.)\n+        # Write the final sum to the output.\n+        sum_dw = tl.sum(dw, axis=0)\n+        sum_db = tl.sum(db, axis=0)\n+        tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n+        tl.store(FINAL_DB + cols, sum_db, mask=cols < N)\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 225-231\n+\n+Benchmark\n+---------\n+\n+We can now compare the performance of our kernel against that of PyTorch.\n+Here we focus on inputs that have Less than 64KB per feature.\n+Specifically, one can set :code:`'mode': 'backward'` to benchmark the backward pass.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 231-372\n+\n+.. code-block:: default\n+\n+\n+\n+    class LayerNorm(torch.autograd.Function):\n+\n+        @staticmethod\n+        def forward(ctx, x, normalized_shape, weight, bias, eps):\n+            # allocate output\n+            y = torch.empty_like(x)\n+            # reshape input data into 2D tensor\n+            x_arg = x.reshape(-1, x.shape[-1])\n+            M, N = x_arg.shape\n+            mean = torch.empty((M, ), dtype=torch.float32, device='cuda')\n+            rstd = torch.empty((M, ), dtype=torch.float32, device='cuda')\n+            # Less than 64KB per feature: enqueue fused kernel\n+            MAX_FUSED_SIZE = 65536 // x.element_size()\n+            BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n+            if N > BLOCK_SIZE:\n+                raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n+            # heuristics for number of warps\n+            num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\n+            # enqueue kernel\n+            _layer_norm_fwd_fused[(M,)](x_arg, y, weight, bias, mean, rstd,\n+                                        x_arg.stride(0), N, eps,\n+                                        BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps, num_ctas=1)\n+            ctx.save_for_backward(x, weight, bias, mean, rstd)\n+            ctx.BLOCK_SIZE = BLOCK_SIZE\n+            ctx.num_warps = num_warps\n+            ctx.eps = eps\n+            return y\n+\n+        @staticmethod\n+        def backward(ctx, dy):\n+            x, w, b, m, v = ctx.saved_tensors\n+            # heuristics for amount of parallel reduction stream for DW/DB\n+            N = w.shape[0]\n+            GROUP_SIZE_M = 64\n+            if N <= 8192: GROUP_SIZE_M = 96\n+            if N <= 4096: GROUP_SIZE_M = 128\n+            if N <= 1024: GROUP_SIZE_M = 256\n+            # allocate output\n+            locks = torch.zeros(2 * GROUP_SIZE_M, dtype=torch.int32, device='cuda')\n+            _dw = torch.empty((GROUP_SIZE_M, w.shape[0]), dtype=x.dtype, device=w.device)\n+            _db = torch.empty((GROUP_SIZE_M, w.shape[0]), dtype=x.dtype, device=w.device)\n+            dw = torch.empty((w.shape[0],), dtype=w.dtype, device=w.device)\n+            db = torch.empty((w.shape[0],), dtype=w.dtype, device=w.device)\n+            dx = torch.empty_like(dy)\n+            # enqueue kernel using forward pass heuristics\n+            # also compute partial sums for DW and DB\n+            x_arg = x.reshape(-1, x.shape[-1])\n+            M, N = x_arg.shape\n+            _layer_norm_bwd_dx_fused[(M,)](dx, dy, _dw, _db, x, w, b, m, v, locks,\n+                                           x_arg.stride(0), N, ctx.eps,\n+                                           BLOCK_SIZE_N=ctx.BLOCK_SIZE,\n+                                           GROUP_SIZE_M=GROUP_SIZE_M,\n+                                           num_warps=ctx.num_warps)\n+            grid = lambda meta: [triton.cdiv(N, meta['BLOCK_SIZE_N'])]\n+            # accumulate partial sums in separate kernel\n+            _layer_norm_bwd_dwdb[grid](_dw, _db, dw, db, GROUP_SIZE_M, N,\n+                                       BLOCK_SIZE_M=32,\n+                                       BLOCK_SIZE_N=128, num_ctas=1)\n+            return dx, None, dw, db, None\n+\n+\n+    layer_norm = LayerNorm.apply\n+\n+\n+    def test_layer_norm(M, N, dtype, eps=1e-5, device='cuda'):\n+        # create data\n+        x_shape = (M, N)\n+        w_shape = (x_shape[-1], )\n+        weight = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n+        bias = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n+        x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\n+        dy = .1 * torch.randn_like(x)\n+        x.requires_grad_(True)\n+        # forward pass\n+        y_tri = layer_norm(x, w_shape, weight, bias, eps)\n+        y_ref = torch.nn.functional.layer_norm(x, w_shape, weight, bias, eps).to(dtype)\n+        # backward pass (triton)\n+        y_tri.backward(dy, retain_graph=True)\n+        dx_tri, dw_tri, db_tri = [_.grad.clone() for _ in [x, weight, bias]]\n+        x.grad, weight.grad, bias.grad = None, None, None\n+        # backward pass (torch)\n+        y_ref.backward(dy, retain_graph=True)\n+        dx_ref, dw_ref, db_ref = [_.grad.clone() for _ in [x, weight, bias]]\n+        # compare\n+        assert torch.allclose(y_tri, y_ref, atol=1e-2, rtol=0)\n+        assert torch.allclose(dx_tri, dx_ref, atol=1e-2, rtol=0)\n+        assert torch.allclose(db_tri, db_ref, atol=1e-2, rtol=0)\n+        assert torch.allclose(dw_tri, dw_ref, atol=1e-2, rtol=0)\n+\n+\n+    @triton.testing.perf_report(\n+        triton.testing.Benchmark(\n+            x_names=['N'],\n+            x_vals=[512 * i for i in range(2, 32)],\n+            line_arg='provider',\n+            line_vals=['triton', 'torch'] + (['apex'] if HAS_APEX else []),\n+            line_names=['Triton', 'Torch'] + (['Apex'] if HAS_APEX else []),\n+            styles=[('blue', '-'), ('green', '-'), ('orange', '-')],\n+            ylabel='GB/s',\n+            plot_name='layer-norm-backward',\n+            args={'M': 4096, 'dtype': torch.float16, 'mode': 'backward'}\n+        )\n+    )\n+    def bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='cuda'):\n+        # create data\n+        x_shape = (M, N)\n+        w_shape = (x_shape[-1], )\n+        weight = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n+        bias = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n+        x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\n+        dy = .1 * torch.randn_like(x)\n+        x.requires_grad_(True)\n+        quantiles = [0.5, 0.2, 0.8]\n+        # utility functions\n+        if provider == 'triton':\n+            def y_fwd(): return layer_norm(x, w_shape, weight, bias, eps)  # noqa: F811, E704\n+        if provider == 'torch':\n+            def y_fwd(): return torch.nn.functional.layer_norm(x, w_shape, weight, bias, eps)  # noqa: F811, E704\n+        if provider == 'apex':\n+            apex_layer_norm = apex.normalization.FusedLayerNorm(\n+                w_shape).to(x.device).to(x.dtype)\n+\n+            def y_fwd(): return apex_layer_norm(x)  # noqa: F811, E704\n+        # forward pass\n+        if mode == 'forward':\n+            gbps = lambda ms: 2 * x.numel() * x.element_size() / ms * 1e-6\n+            ms, min_ms, max_ms = triton.testing.do_bench(y_fwd, quantiles=quantiles, rep=500)\n+        # backward pass\n+        if mode == 'backward':\n+            def gbps(ms): return 3 * x.numel() * x.element_size() / ms * 1e-6  # noqa: F811, E704\n+            y = y_fwd()\n+            ms, min_ms, max_ms = triton.testing.do_bench(lambda: y.backward(dy, retain_graph=True),\n+                                                         quantiles=quantiles, grad_to_none=[x], rep=500)\n+        return gbps(ms), gbps(max_ms), gbps(min_ms)\n+\n+\n+    test_layer_norm(1151, 8192, torch.float16)\n+    bench_layer_norm.run(save_path='.', print_data=True)\n+\n+\n+\n+\n+.. image-sg:: /getting-started/tutorials/images/sphx_glr_05-layer-norm_001.png\n+   :alt: 05 layer norm\n+   :srcset: /getting-started/tutorials/images/sphx_glr_05-layer-norm_001.png\n+   :class: sphx-glr-single-img\n+\n+\n+.. rst-class:: sphx-glr-script-out\n+\n+ .. code-block:: none\n+\n+    layer-norm-backward:\n+              N       Triton       Torch\n+    0    1024.0   142.057809  372.363633\n+    1    1536.0   224.780491  438.857146\n+    2    2048.0   299.707322  496.484863\n+    3    2560.0   370.120486  534.260858\n+    4    3072.0   441.485015  542.117638\n+    5    3584.0   505.976473  467.478250\n+    6    4096.0   588.646687  474.898540\n+    7    4608.0   621.303364  480.834772\n+    8    5120.0   686.480466  483.779502\n+    9    5632.0   819.199976  491.520003\n+    10   6144.0   867.388239  494.818794\n+    11   6656.0   907.636357  499.200013\n+    12   7168.0   945.230752  476.542919\n+    13   7680.0   975.238103  479.999983\n+    14   8192.0  1008.246151  487.861027\n+    15   8704.0   673.858058  489.217808\n+    16   9216.0   702.171402  492.614684\n+    17   9728.0   729.600018  498.871781\n+    18  10240.0   753.865011  498.498957\n+    19  10752.0   779.601236  485.052653\n+    20  11264.0   806.973159  487.971095\n+    21  11776.0   821.581395  494.097911\n+    22  12288.0   840.205140  499.005061\n+    23  12800.0   848.618804  499.512174\n+    24  13312.0   863.481094  499.981239\n+    25  13824.0   863.999969  501.930388\n+    26  14336.0   875.480928  492.928354\n+    27  14848.0   877.714272  495.621695\n+    28  15360.0   894.757295  500.189943\n+    29  15872.0   892.103062  501.221037\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 373-377\n+\n+References\n+----------\n+\n+.. [BA2016] Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \"Layer Normalization\", Arxiv 2016\n+\n+\n+.. rst-class:: sphx-glr-timing\n+\n+   **Total running time of the script:** ( 0 minutes  29.379 seconds)\n+\n+\n+.. _sphx_glr_download_getting-started_tutorials_05-layer-norm.py:\n+\n+.. only:: html\n+\n+  .. container:: sphx-glr-footer sphx-glr-footer-example\n+\n+\n+\n+\n+    .. container:: sphx-glr-download sphx-glr-download-python\n+\n+      :download:`Download Python source code: 05-layer-norm.py <05-layer-norm.py>`\n+\n+    .. container:: sphx-glr-download sphx-glr-download-jupyter\n+\n+      :download:`Download Jupyter notebook: 05-layer-norm.ipynb <05-layer-norm.ipynb>`\n+\n+\n+.. only:: html\n+\n+ .. rst-class:: sphx-glr-signature\n+\n+    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"}, {"filename": "main/_sources/getting-started/tutorials/06-fused-attention.rst.txt", "status": "added", "additions": 539, "deletions": 0, "changes": 539, "file_content_changes": "@@ -0,0 +1,539 @@\n+\n+.. DO NOT EDIT.\n+.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.\n+.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:\n+.. \"getting-started/tutorials/06-fused-attention.py\"\n+.. LINE NUMBERS ARE GIVEN BELOW.\n+\n+.. only:: html\n+\n+    .. note::\n+        :class: sphx-glr-download-link-note\n+\n+        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_06-fused-attention.py>`\n+        to download the full example code\n+\n+.. rst-class:: sphx-glr-example-title\n+\n+.. _sphx_glr_getting-started_tutorials_06-fused-attention.py:\n+\n+\n+Fused Attention\n+===============\n+\n+This is a Triton implementation of the Flash Attention v2 algorithm from Tri Dao (https://tridao.me/publications/flash2/flash2.pdf)\n+\n+Extra Credits:\n+- Original flash attention paper (https://arxiv.org/abs/2205.14135)\n+- Rabe and Staats (https://arxiv.org/pdf/2112.05682v2.pdf)\n+- Adam P. Goucher for simplified vector math\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 13-414\n+\n+\n+\n+.. rst-class:: sphx-glr-horizontal\n+\n+\n+    *\n+\n+      .. image-sg:: /getting-started/tutorials/images/sphx_glr_06-fused-attention_001.png\n+         :alt: 06 fused attention\n+         :srcset: /getting-started/tutorials/images/sphx_glr_06-fused-attention_001.png\n+         :class: sphx-glr-multi-img\n+\n+    *\n+\n+      .. image-sg:: /getting-started/tutorials/images/sphx_glr_06-fused-attention_002.png\n+         :alt: 06 fused attention\n+         :srcset: /getting-started/tutorials/images/sphx_glr_06-fused-attention_002.png\n+         :class: sphx-glr-multi-img\n+\n+    *\n+\n+      .. image-sg:: /getting-started/tutorials/images/sphx_glr_06-fused-attention_003.png\n+         :alt: 06 fused attention\n+         :srcset: /getting-started/tutorials/images/sphx_glr_06-fused-attention_003.png\n+         :class: sphx-glr-multi-img\n+\n+    *\n+\n+      .. image-sg:: /getting-started/tutorials/images/sphx_glr_06-fused-attention_004.png\n+         :alt: 06 fused attention\n+         :srcset: /getting-started/tutorials/images/sphx_glr_06-fused-attention_004.png\n+         :class: sphx-glr-multi-img\n+\n+\n+.. rst-class:: sphx-glr-script-out\n+\n+ .. code-block:: none\n+\n+    fused-attention-batch4-head48-d64-fwd:\n+         N_CTX      Triton\n+    0   1024.0  158.916075\n+    1   2048.0  166.908312\n+    2   4096.0  172.217530\n+    3   8192.0  175.142747\n+    4  16384.0  176.264046\n+    fused-attention-batch4-head48-d64-fwd:\n+         N_CTX      Triton\n+    0   1024.0  115.317167\n+    1   2048.0  134.219279\n+    2   4096.0  145.192577\n+    3   8192.0  152.928847\n+    4  16384.0  154.156973\n+    fused-attention-batch4-head48-d64-bwd:\n+         N_CTX     Triton\n+    0   1024.0  72.171640\n+    1   2048.0  81.286714\n+    2   4096.0  86.469348\n+    3   8192.0  88.474787\n+    4  16384.0  89.965301\n+    fused-attention-batch4-head48-d64-bwd:\n+         N_CTX     Triton\n+    0   1024.0  50.636375\n+    1   2048.0  62.743374\n+    2   4096.0  70.370948\n+    3   8192.0  74.700973\n+    4  16384.0  77.223170\n+\n+\n+\n+\n+\n+\n+|\n+\n+.. code-block:: default\n+\n+\n+    import pytest\n+    import torch\n+\n+    import triton\n+    import triton.language as tl\n+\n+\n+    @triton.jit\n+    def max_fn(x, y):\n+        return tl.math.max(x, y)\n+\n+\n+    @triton.jit\n+    def _fwd_kernel(\n+        Q, K, V, sm_scale,\n+        L,\n+        Out,\n+        stride_qz, stride_qh, stride_qm, stride_qk,\n+        stride_kz, stride_kh, stride_kn, stride_kk,\n+        stride_vz, stride_vh, stride_vk, stride_vn,\n+        stride_oz, stride_oh, stride_om, stride_on,\n+        Z, H, N_CTX, P_SEQ,\n+        BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n+        BLOCK_N: tl.constexpr,\n+        IS_CAUSAL: tl.constexpr,\n+    ):\n+        start_m = tl.program_id(0)\n+        off_hz = tl.program_id(1)\n+        q_offset = off_hz * stride_qh\n+        kv_offset = off_hz * stride_kh\n+        Q_block_ptr = tl.make_block_ptr(\n+            base=Q + q_offset,\n+            shape=(N_CTX, BLOCK_DMODEL),\n+            strides=(stride_qm, stride_qk),\n+            offsets=(start_m * BLOCK_M, 0),\n+            block_shape=(BLOCK_M, BLOCK_DMODEL),\n+            order=(1, 0)\n+        )\n+        K_block_ptr = tl.make_block_ptr(\n+            base=K + kv_offset,\n+            shape=(BLOCK_DMODEL, N_CTX + P_SEQ),\n+            strides=(stride_kk, stride_kn),\n+            offsets=(0, 0),\n+            block_shape=(BLOCK_DMODEL, BLOCK_N),\n+            order=(0, 1)\n+        )\n+        V_block_ptr = tl.make_block_ptr(\n+            base=V + kv_offset,\n+            shape=(N_CTX + P_SEQ, BLOCK_DMODEL),\n+            strides=(stride_vk, stride_vn),\n+            offsets=(0, 0),\n+            block_shape=(BLOCK_N, BLOCK_DMODEL),\n+            order=(1, 0)\n+        )\n+        # initialize offsets\n+        offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+        offs_n = tl.arange(0, BLOCK_N)\n+        # initialize pointer to m and l\n+        m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+        l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n+        acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+        # scale sm_scale by log_2(e) and use\n+        # 2^x instead of exp in the loop because CSE and LICM\n+        # don't work as expected with `exp` in the loop\n+        qk_scale = sm_scale * 1.44269504\n+        # load q: it will stay in SRAM throughout\n+        q = tl.load(Q_block_ptr)\n+        q = (q * qk_scale).to(tl.float16)\n+        # loop over k, v and update accumulator\n+        lo = 0\n+        hi = P_SEQ + (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX + P_SEQ\n+        for start_n in range(lo, hi, BLOCK_N):\n+            # -- load k, v --\n+            k = tl.load(K_block_ptr)\n+            v = tl.load(V_block_ptr)\n+            # -- compute qk ---\n+            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float16)\n+            if IS_CAUSAL:\n+                qk = tl.where(P_SEQ + offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n+            qk += tl.dot(q, k, out_dtype=tl.float16)\n+            # -- compute scaling constant ---\n+            m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n+            alpha = tl.math.exp2(m_i - m_i_new)\n+            p = tl.math.exp2(qk - m_i_new[:, None])\n+            # -- scale and update acc --\n+            acc_scale = l_i * 0 + alpha  # workaround some compiler bug\n+            acc *= acc_scale[:, None]\n+            acc += tl.dot(p.to(tl.float16), v)\n+            # -- update m_i and l_i --\n+            l_i = l_i * alpha + tl.sum(p, 1)\n+            m_i = m_i_new\n+            # update pointers\n+            K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n+            V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n+        # write back l and m\n+        acc = acc / l_i[:, None]\n+        l_ptrs = L + off_hz * N_CTX + offs_m\n+        tl.store(l_ptrs, m_i + tl.math.log2(l_i))\n+        # write back O\n+        O_block_ptr = tl.make_block_ptr(\n+            base=Out + q_offset,\n+            shape=(N_CTX, BLOCK_DMODEL),\n+            strides=(stride_om, stride_on),\n+            offsets=(start_m * BLOCK_M, 0),\n+            block_shape=(BLOCK_M, BLOCK_DMODEL),\n+            order=(1, 0)\n+        )\n+        tl.store(O_block_ptr, acc.to(tl.float16))\n+\n+\n+    @triton.jit\n+    def _bwd_preprocess(\n+        Out, DO,\n+        Delta,\n+        BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\n+    ):\n+        off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n+        off_n = tl.arange(0, D_HEAD)\n+        # load\n+        o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n+        do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n+        # compute\n+        delta = tl.sum(o * do, axis=1)\n+        # write-back\n+        tl.store(Delta + off_m, delta)\n+\n+\n+    @triton.jit\n+    def _bwd_kernel(\n+        Q, K, V, sm_scale, Out, DO,\n+        DQ, DK, DV,\n+        L,\n+        D,\n+        stride_qz, stride_qh, stride_qm, stride_qk,\n+        stride_kz, stride_kh, stride_kn, stride_kk,\n+        stride_vz, stride_vh, stride_vk, stride_vn,\n+        Z, H, N_CTX, P_SEQ,\n+        num_block_q, num_block_kv,\n+        BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n+        BLOCK_N: tl.constexpr,\n+        CAUSAL: tl.constexpr,\n+    ):\n+        off_hz = tl.program_id(0)\n+        off_z = off_hz // H\n+        off_h = off_hz % H\n+        qk_scale = sm_scale * 1.44269504\n+        # offset pointers for batch/head\n+        Q += off_z * stride_qz + off_h * stride_qh\n+        K += off_z * stride_kz + off_h * stride_kh\n+        V += off_z * stride_vz + off_h * stride_vh\n+        DO += off_z * stride_qz + off_h * stride_qh\n+        DQ += off_z * stride_qz + off_h * stride_qh\n+        DK += off_z * stride_kz + off_h * stride_kh\n+        DV += off_z * stride_vz + off_h * stride_vh\n+        for start_n in range(0, num_block_kv):\n+            if CAUSAL:\n+                lo = tl.math.max(start_n * BLOCK_M - P_SEQ, 0)\n+            else:\n+                lo = 0\n+            # initialize row/col offsets\n+            offs_qm = lo + tl.arange(0, BLOCK_M)\n+            offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n+            offs_m = tl.arange(0, BLOCK_N)\n+            offs_k = tl.arange(0, BLOCK_DMODEL)\n+            # initialize pointers to value-like data\n+            q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+            k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n+            v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+            do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+            dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+            # pointer to row-wise quantities in value-like data\n+            D_ptrs = D + off_hz * N_CTX\n+            l_ptrs = L + off_hz * N_CTX\n+            # initialize dk amd dv\n+            dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+            dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+            # k and v stay in SRAM throughout\n+            k = tl.load(k_ptrs)\n+            v = tl.load(v_ptrs)\n+            # loop over rows\n+            for start_m in range(lo, num_block_q * BLOCK_M, BLOCK_M):\n+                offs_m_curr = start_m + offs_m\n+                # load q, k, v, do on-chip\n+                q = tl.load(q_ptrs)\n+                # recompute p = softmax(qk, dim=-1).T\n+                if CAUSAL:\n+                    qk = tl.where(P_SEQ + offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\"-inf\"))\n+                else:\n+                    qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n+                qk += tl.dot(q, tl.trans(k))\n+                qk *= qk_scale\n+                l_i = tl.load(l_ptrs + offs_m_curr)\n+                p = tl.math.exp2(qk - l_i[:, None])\n+                # compute dv\n+                do = tl.load(do_ptrs)\n+                dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\n+                # compute dp = dot(v, do)\n+                Di = tl.load(D_ptrs + offs_m_curr)\n+                dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n+                dp += tl.dot(do, tl.trans(v))\n+                # compute ds = p * (dp - delta[:, None])\n+                ds = p * dp * sm_scale\n+                # compute dk = dot(ds.T, q)\n+                dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)\n+                # compute dq\n+                dq = tl.load(dq_ptrs)\n+                dq += tl.dot(ds.to(Q.dtype.element_ty), k)\n+                tl.store(dq_ptrs, dq)\n+                # increment pointers\n+                dq_ptrs += BLOCK_M * stride_qm\n+                q_ptrs += BLOCK_M * stride_qm\n+                do_ptrs += BLOCK_M * stride_qm\n+            # write-back\n+            dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n+            dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+            tl.store(dk_ptrs, dk)\n+            tl.store(dv_ptrs, dv)\n+\n+\n+    empty = torch.empty(128, device=\"cuda\")\n+\n+\n+    class _attention(torch.autograd.Function):\n+\n+        @staticmethod\n+        def forward(ctx, q, k, v, causal, sm_scale):\n+            # shape constraints\n+            Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n+            assert Lq == Lk and Lk == Lv\n+            assert Lk in {16, 32, 64, 128}\n+            o = torch.empty_like(q)\n+            BLOCK_M = 128\n+            BLOCK_N = 64 if Lk <= 64 else 32\n+            num_stages = 4 if Lk <= 64 else 3\n+            num_warps = 4\n+            grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n+            L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+            P_SEQ = 0 if q.shape[-2] == k.shape[-2] else k.shape[-2] - q.shape[-2]\n+            _fwd_kernel[grid](\n+                q, k, v, sm_scale,\n+                L,\n+                o,\n+                q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+                k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+                v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+                o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n+                q.shape[0], q.shape[1], q.shape[2], P_SEQ,\n+                BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=Lk,\n+                IS_CAUSAL=causal,\n+                num_warps=num_warps,\n+                num_stages=num_stages)\n+\n+            ctx.save_for_backward(q, k, v, o, L)\n+            ctx.grid = grid\n+            ctx.sm_scale = sm_scale\n+            ctx.BLOCK_DMODEL = Lk\n+            ctx.causal = causal\n+            ctx.P_SEQ = P_SEQ\n+            return o\n+\n+        @staticmethod\n+        def backward(ctx, do):\n+            BLOCK = 128\n+            q, k, v, o, L = ctx.saved_tensors\n+            do = do.contiguous()\n+            dq = torch.zeros_like(q, dtype=torch.float32)\n+            dk = torch.empty_like(k)\n+            dv = torch.empty_like(v)\n+            delta = torch.empty_like(L)\n+            _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n+                o, do,\n+                delta,\n+                BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n+            )\n+            _bwd_kernel[(ctx.grid[1],)](\n+                q, k, v, ctx.sm_scale,\n+                o, do,\n+                dq, dk, dv,\n+                L, delta,\n+                q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+                k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+                v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+                q.shape[0], q.shape[1], q.shape[2], ctx.P_SEQ,\n+                ctx.grid[0], triton.cdiv(k.shape[2], BLOCK),\n+                BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n+                BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n+                CAUSAL=ctx.causal,\n+                num_stages=1,\n+            )\n+            return dq, dk, dv, None, None\n+\n+\n+    attention = _attention.apply\n+\n+\n+    @pytest.mark.parametrize('Z, H, N_CTX, D_HEAD, P_SEQ', [(6, 9, 1024, 64, 128)])\n+    @pytest.mark.parametrize('causal', [False, True])\n+    def test_op(Z, H, N_CTX, D_HEAD, P_SEQ, causal, dtype=torch.float16):\n+        torch.manual_seed(20)\n+        q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+        k = torch.empty((Z, H, N_CTX + P_SEQ, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+        v = torch.empty((Z, H, N_CTX + P_SEQ, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+        sm_scale = 0.5\n+        dout = torch.randn_like(q)\n+        # reference implementation\n+        M = torch.tril(torch.ones((N_CTX, N_CTX + P_SEQ), device=\"cuda\"), diagonal=P_SEQ)\n+        p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n+        if causal:\n+            p[:, :, M == 0] = float(\"-inf\")\n+        p = torch.softmax(p.float(), dim=-1).half()\n+        # p = torch.exp(p)\n+        ref_out = torch.matmul(p, v)\n+        ref_out.backward(dout)\n+        ref_dv, v.grad = v.grad.clone(), None\n+        ref_dk, k.grad = k.grad.clone(), None\n+        ref_dq, q.grad = q.grad.clone(), None\n+        # triton implementation\n+        tri_out = attention(q, k, v, causal, sm_scale).half()\n+        tri_out.backward(dout)\n+        tri_dv, v.grad = v.grad.clone(), None\n+        tri_dk, k.grad = k.grad.clone(), None\n+        tri_dq, q.grad = q.grad.clone(), None\n+        # compare\n+        assert torch.allclose(ref_out, tri_out, atol=1e-2, rtol=0)\n+        assert torch.allclose(ref_dv, tri_dv, atol=1e-2, rtol=0)\n+        assert torch.allclose(ref_dk, tri_dk, atol=1e-2, rtol=0)\n+        assert torch.allclose(ref_dq, tri_dq, atol=1e-2, rtol=0)\n+\n+\n+    try:\n+        from flash_attn.flash_attn_interface import \\\n+            flash_attn_qkvpacked_func as flash_attn_func\n+        FLASH_VER = 2\n+    except BaseException:\n+        try:\n+            from flash_attn.flash_attn_interface import flash_attn_func\n+            FLASH_VER = 1\n+        except BaseException:\n+            FLASH_VER = None\n+    HAS_FLASH = FLASH_VER is not None\n+\n+    BATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\n+    # vary seq length for fixed head and batch=4\n+    configs = [triton.testing.Benchmark(\n+        x_names=['N_CTX'],\n+        x_vals=[2**i for i in range(10, 15)],\n+        line_arg='provider',\n+        line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n+        line_names=['Triton'] + ([f'Flash-{FLASH_VER}'] if HAS_FLASH else []),\n+        styles=[('red', '-'), ('blue', '-')],\n+        ylabel='ms',\n+        plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n+        args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode, 'causal': causal}\n+    ) for mode in ['fwd', 'bwd'] for causal in [False, True]]\n+\n+\n+    @triton.testing.perf_report(configs)\n+    def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, causal, mode, provider, dtype=torch.float16, device=\"cuda\"):\n+        assert mode in ['fwd', 'bwd']\n+        warmup = 25\n+        rep = 100\n+        if provider == \"triton\":\n+            q = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+            k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+            v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+            sm_scale = 1.3\n+            fn = lambda: attention(q, k, v, causal, sm_scale)\n+            if mode == 'bwd':\n+                o = fn()\n+                do = torch.randn_like(o)\n+                fn = lambda: o.backward(do, retain_graph=True)\n+            ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n+        if provider == \"flash\":\n+            qkv = torch.randn((BATCH, N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\n+            if FLASH_VER == 1:\n+                lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n+                cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n+                cu_seqlens[1:] = lengths.cumsum(0)\n+                qkv = qkv.reshape(BATCH * N_CTX, 3, H, D_HEAD)\n+                fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=causal)\n+            elif FLASH_VER == 2:\n+                fn = lambda: flash_attn_func(qkv, causal=causal)\n+            else:\n+                raise ValueError(f'unknown {FLASH_VER = }')\n+            if mode == 'bwd':\n+                o = fn()\n+                do = torch.randn_like(o)\n+                fn = lambda: o.backward(do, retain_graph=True)\n+            ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n+        flops_per_matmul = 2. * BATCH * H * N_CTX * N_CTX * D_HEAD\n+        total_flops = 2 * flops_per_matmul\n+        if causal:\n+            total_flops *= 0.5\n+        if mode == 'bwd':\n+            total_flops *= 2.5  # 2.0(bwd) + 0.5(recompute)\n+        return total_flops / ms * 1e-9\n+\n+\n+    # only works on post-Ampere GPUs right now\n+    bench_flash_attention.run(save_path='.', print_data=True)\n+\n+\n+.. rst-class:: sphx-glr-timing\n+\n+   **Total running time of the script:** ( 0 minutes  14.839 seconds)\n+\n+\n+.. _sphx_glr_download_getting-started_tutorials_06-fused-attention.py:\n+\n+.. only:: html\n+\n+  .. container:: sphx-glr-footer sphx-glr-footer-example\n+\n+\n+\n+\n+    .. container:: sphx-glr-download sphx-glr-download-python\n+\n+      :download:`Download Python source code: 06-fused-attention.py <06-fused-attention.py>`\n+\n+    .. container:: sphx-glr-download sphx-glr-download-jupyter\n+\n+      :download:`Download Jupyter notebook: 06-fused-attention.ipynb <06-fused-attention.ipynb>`\n+\n+\n+.. only:: html\n+\n+ .. rst-class:: sphx-glr-signature\n+\n+    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"}, {"filename": "main/_sources/getting-started/tutorials/07-math-functions.rst.txt", "status": "added", "additions": 175, "deletions": 0, "changes": 175, "file_content_changes": "@@ -0,0 +1,175 @@\n+\n+.. DO NOT EDIT.\n+.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.\n+.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:\n+.. \"getting-started/tutorials/07-math-functions.py\"\n+.. LINE NUMBERS ARE GIVEN BELOW.\n+\n+.. only:: html\n+\n+    .. note::\n+        :class: sphx-glr-download-link-note\n+\n+        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_07-math-functions.py>`\n+        to download the full example code\n+\n+.. rst-class:: sphx-glr-example-title\n+\n+.. _sphx_glr_getting-started_tutorials_07-math-functions.py:\n+\n+\n+Libdevice (`tl.math`) function\n+==============================\n+Triton can invoke a custom function from an external library.\n+In this example, we will use the `libdevice` library (a.k.a `math` in triton) to apply `asin` on a tensor.\n+Please refer to https://docs.nvidia.com/cuda/libdevice-users-guide/index.html regarding the semantics of all available libdevice functions.\n+In `triton/language/math.py`, we try to aggregate functions with the same computation but different data types together.\n+For example, both `__nv_asin` and `__nvasinf` calculate the principal value of the arc sine of the input, but `__nv_asin` operates on `double` and `__nv_asinf` operates on `float`.\n+Using triton, you can simply call `tl.math.asin`.\n+Triton automatically selects the correct underlying device function to invoke based on input and output types.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 14-16\n+\n+asin Kernel\n+------------\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 16-38\n+\n+.. code-block:: default\n+\n+\n+    import torch\n+\n+    import triton\n+    import triton.language as tl\n+\n+\n+    @triton.jit\n+    def asin_kernel(\n+            x_ptr,\n+            y_ptr,\n+            n_elements,\n+            BLOCK_SIZE: tl.constexpr,\n+    ):\n+        pid = tl.program_id(axis=0)\n+        block_start = pid * BLOCK_SIZE\n+        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+        mask = offsets < n_elements\n+        x = tl.load(x_ptr + offsets, mask=mask)\n+        x = tl.math.asin(x)\n+        tl.store(y_ptr + offsets, x, mask=mask)\n+\n+\n+\n+\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 39-42\n+\n+Using the default libdevice library path\n+-----------------------------------------\n+We can use the default libdevice library path encoded in `triton/language/math.py`\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 42-60\n+\n+.. code-block:: default\n+\n+\n+\n+    torch.manual_seed(0)\n+    size = 98432\n+    x = torch.rand(size, device='cuda')\n+    output_triton = torch.zeros(size, device='cuda')\n+    output_torch = torch.asin(x)\n+    assert x.is_cuda and output_triton.is_cuda\n+    n_elements = output_torch.numel()\n+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+    asin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024)\n+    print(output_torch)\n+    print(output_triton)\n+    print(\n+        f'The maximum difference between torch and triton is '\n+        f'{torch.max(torch.abs(output_torch - output_triton))}'\n+    )\n+\n+\n+\n+\n+\n+.. rst-class:: sphx-glr-script-out\n+\n+ .. code-block:: none\n+\n+    tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device='cuda:0')\n+    tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device='cuda:0')\n+    The maximum difference between torch and triton is 2.384185791015625e-07\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 61-64\n+\n+Customize the libdevice library path\n+-------------------------------------\n+We can also customize the libdevice library path by passing the path to the `libdevice` library to the `asin` kernel.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 64-74\n+\n+.. code-block:: default\n+\n+\n+    output_triton = torch.empty_like(x)\n+    asin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024,\n+                      extern_libs={'libdevice': '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'})\n+    print(output_torch)\n+    print(output_triton)\n+    print(\n+        f'The maximum difference between torch and triton is '\n+        f'{torch.max(torch.abs(output_torch - output_triton))}'\n+    )\n+\n+\n+\n+\n+.. rst-class:: sphx-glr-script-out\n+\n+ .. code-block:: none\n+\n+    tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device='cuda:0')\n+    tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device='cuda:0')\n+    The maximum difference between torch and triton is 2.384185791015625e-07\n+\n+\n+\n+\n+\n+.. rst-class:: sphx-glr-timing\n+\n+   **Total running time of the script:** ( 0 minutes  0.224 seconds)\n+\n+\n+.. _sphx_glr_download_getting-started_tutorials_07-math-functions.py:\n+\n+.. only:: html\n+\n+  .. container:: sphx-glr-footer sphx-glr-footer-example\n+\n+\n+\n+\n+    .. container:: sphx-glr-download sphx-glr-download-python\n+\n+      :download:`Download Python source code: 07-math-functions.py <07-math-functions.py>`\n+\n+    .. container:: sphx-glr-download sphx-glr-download-jupyter\n+\n+      :download:`Download Jupyter notebook: 07-math-functions.ipynb <07-math-functions.ipynb>`\n+\n+\n+.. only:: html\n+\n+ .. rst-class:: sphx-glr-signature\n+\n+    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"}, {"filename": "main/_sources/getting-started/tutorials/08-experimental-block-pointer.rst.txt", "status": "added", "additions": 326, "deletions": 0, "changes": 326, "file_content_changes": "@@ -0,0 +1,326 @@\n+\n+.. DO NOT EDIT.\n+.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.\n+.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:\n+.. \"getting-started/tutorials/08-experimental-block-pointer.py\"\n+.. LINE NUMBERS ARE GIVEN BELOW.\n+\n+.. only:: html\n+\n+    .. note::\n+        :class: sphx-glr-download-link-note\n+\n+        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_08-experimental-block-pointer.py>`\n+        to download the full example code\n+\n+.. rst-class:: sphx-glr-example-title\n+\n+.. _sphx_glr_getting-started_tutorials_08-experimental-block-pointer.py:\n+\n+\n+Block Pointer (Experimental)\n+============================\n+This tutorial will guide you through writing a matrix multiplication algorithm that utilizes block pointer semantics.\n+These semantics are more friendly for Triton to optimize and can result in better performance on specific hardware.\n+Note that this feature is still experimental and may change in the future.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 11-29\n+\n+Motivations\n+-----------\n+In the previous matrix multiplication tutorial, we constructed blocks of values by de-referencing blocks of pointers,\n+i.e., :code:`load(block<pointer_type<element_type>>) -> block<element_type>`, which involved loading blocks of\n+elements from memory. This approach allowed for flexibility in using hardware-managed cache and implementing complex\n+data structures, such as tensors of trees or unstructured look-up tables.\n+\n+However, the drawback of this approach is that it relies heavily on complex optimization passes by the compiler to\n+optimize memory access patterns. This can result in brittle code that may suffer from performance degradation when the\n+optimizer fails to perform adequately. Additionally, as memory controllers specialize to accommodate dense spatial\n+data structures commonly used in machine learning workloads, this problem is likely to worsen.\n+\n+To address this issue, we will use block pointers :code:`pointer_type<block<element_type>>` and load them into\n+:code:`block<element_type>`, in which way gives better friendliness for the compiler to optimize memory access\n+patterns.\n+\n+Let's start with the previous matrix multiplication example and demonstrate how to rewrite it to utilize block pointer\n+semantics.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 31-61\n+\n+Make a Block Pointer\n+--------------------\n+A block pointer pointers to a block in a parent tensor and is constructed by :code:`make_block_ptr` function,\n+which takes the following information as arguments:\n+\n+* :code:`base`: the base pointer to the parent tensor;\n+\n+* :code:`shape`: the shape of the parent tensor;\n+\n+* :code:`strides`: the strides of the parent tensor, which means how much to increase the pointer by when moving by 1 element in a specific axis;\n+\n+* :code:`offsets`: the offsets of the block;\n+\n+* :code:`block_shape`: the shape of the block;\n+\n+* :code:`order`: the order of the block, which means how the block is laid out in memory.\n+\n+For example, to a block pointer to a :code:`BLOCK_SIZE_M * BLOCK_SIZE_K` block in a row-major 2D matrix A by\n+offsets :code:`(pid_m * BLOCK_SIZE_M, 0)` and strides :code:`(stride_am, stride_ak)`, we can use the following code\n+(exactly the same as the previous matrix multiplication tutorial):\n+\n+.. code-block:: python\n+\n+    a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\n+                                    offsets=(pid_m * BLOCK_SIZE_M, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K),\n+                                    order=(1, 0))\n+\n+Note that the :code:`order` argument is set to :code:`(1, 0)`, which means the second axis is the inner dimension in\n+terms of storage, and the first axis is the outer dimension. This information may sound redundant, but it is necessary\n+for some hardware backends to optimize for better performance.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 63-78\n+\n+Load/Store a Block Pointer\n+--------------------------\n+To load/store a block pointer, we can use :code:`load/store` function, which takes a block pointer as an argument,\n+de-references it, and loads/stores a block. You may mask some values in the block, here we have an extra argument\n+:code:`boundary_check` to specify whether to check the boundary of each axis for the block pointer. With check on,\n+out-of-bound values will be masked according to the :code:`padding_option` argument (load only), which can be\n+:code:`zero` or :code:`nan`. Temporarily, we do not support other values due to some hardware limitations. In this\n+mode of block pointer load/store does not support :code:`mask` or :code:`other` arguments in the legacy mode.\n+\n+So to load the block pointer of A in the previous section, we can simply write\n+:code:`a = tl.load(a_block_ptr, boundary_check=(0, 1))`. Boundary check may cost extra performance, so if you can\n+guarantee that the block pointer is always in-bound in some axis, you can turn off the check by not passing the index\n+into the :code:`boundary_check` argument. For example, if we know that :code:`M` is a multiple of\n+:code:`BLOCK_SIZE_M`, we can replace with :code:`a = tl.load(a_block_ptr, boundary_check=(1, ))`, since axis 0 is\n+always in bound.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 80-88\n+\n+Advance a Block Pointer\n+-----------------------\n+To advance a block pointer, we can use :code:`advance` function, which takes a block pointer and the increment for\n+each axis as arguments and returns a new block pointer with the same shape and strides as the original one,\n+but with the offsets advanced by the specified amount.\n+\n+For example, to advance the block pointer by :code:`BLOCK_SIZE_K` in the second axis\n+(no need to multiply with strides), we can write :code:`a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))`.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 90-92\n+\n+Final Result\n+------------\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 92-212\n+\n+.. code-block:: default\n+\n+\n+    import torch\n+\n+    import triton\n+    import triton.language as tl\n+\n+\n+    @triton.autotune(\n+        configs=[\n+            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n+            triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n+        ],\n+        key=['M', 'N', 'K'],\n+    )\n+    @triton.jit\n+    def matmul_kernel_with_block_pointers(\n+            # Pointers to matrices\n+            a_ptr, b_ptr, c_ptr,\n+            # Matrix dimensions\n+            M, N, K,\n+            # The stride variables represent how much to increase the ptr by when moving by 1\n+            # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n+            # by to get the element one row down (A has M rows).\n+            stride_am, stride_ak,\n+            stride_bk, stride_bn,\n+            stride_cm, stride_cn,\n+            # Meta-parameters\n+            BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+            GROUP_SIZE_M: tl.constexpr\n+    ):\n+        \"\"\"Kernel for computing the matmul C = A x B.\n+        A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n+        \"\"\"\n+        # -----------------------------------------------------------\n+        # Map program ids `pid` to the block of C it should compute.\n+        # This is done in a grouped ordering to promote L2 data reuse.\n+        # See the matrix multiplication tutorial for details.\n+        pid = tl.program_id(axis=0)\n+        num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n+        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+        num_pid_in_group = GROUP_SIZE_M * num_pid_n\n+        group_id = pid // num_pid_in_group\n+        first_pid_m = group_id * GROUP_SIZE_M\n+        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n+        pid_m = first_pid_m + (pid % group_size_m)\n+        pid_n = (pid % num_pid_in_group) // group_size_m\n+\n+        # ----------------------------------------------------------\n+        # Create block pointers for the first blocks of A and B.\n+        # We will advance this pointer as we move in the K direction and accumulate.\n+        # See above `Make a Block Pointer` section for details.\n+        a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\n+                                        offsets=(pid_m * BLOCK_SIZE_M, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K),\n+                                        order=(1, 0))\n+        b_block_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(stride_bk, stride_bn),\n+                                        offsets=(0, pid_n * BLOCK_SIZE_N), block_shape=(BLOCK_SIZE_K, BLOCK_SIZE_N),\n+                                        order=(1, 0))\n+\n+        # -----------------------------------------------------------\n+        # Iterate to compute a block of the C matrix.\n+        # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block.\n+        # of fp32 values for higher accuracy.\n+        # `accumulator` will be converted back to fp16 after the loop.\n+        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+        for k in range(0, K, BLOCK_SIZE_K):\n+            # Load with boundary checks, no need to calculate the mask manually.\n+            # For better performance, you may remove some axis from the boundary\n+            # check, if you can guarantee that the access is always in-bound in\n+            # that axis.\n+            # See above `Load/Store a Block Pointer` section for details.\n+            a = tl.load(a_block_ptr, boundary_check=(0, 1))\n+            b = tl.load(b_block_ptr, boundary_check=(0, 1))\n+            # We accumulate along the K dimension.\n+            accumulator += tl.dot(a, b)\n+            # Advance the block pointer to the next K block.\n+            # See above `Advance a Block Pointer` section for details.\n+            a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))\n+            b_block_ptr = tl.advance(b_block_ptr, (BLOCK_SIZE_K, 0))\n+        c = accumulator.to(tl.float16)\n+\n+        # ----------------------------------------------------------------\n+        # Write back the block of the output matrix C with boundary checks.\n+        # See above `Load/Store a Block Pointer` section for details.\n+        c_block_ptr = tl.make_block_ptr(base=c_ptr, shape=(M, N), strides=(stride_cm, stride_cn),\n+                                        offsets=(pid_m * BLOCK_SIZE_M, pid_n * BLOCK_SIZE_N),\n+                                        block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_N), order=(1, 0))\n+        tl.store(c_block_ptr, c, boundary_check=(0, 1))\n+\n+\n+    # We can now create a convenience wrapper function that only takes two input tensors,\n+    # and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\n+    def matmul(a, b):\n+        # Check constraints.\n+        assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n+        assert a.is_contiguous(), \"Matrix A must be contiguous\"\n+        assert b.is_contiguous(), \"Matrix B must be contiguous\"\n+        M, K = a.shape\n+        K, N = b.shape\n+        # Allocates output.\n+        c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n+        # 1D launch kernel where each block gets its own program.\n+        grid = lambda META: (\n+            triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n+        )\n+        matmul_kernel_with_block_pointers[grid](\n+            a, b, c,\n+            M, N, K,\n+            a.stride(0), a.stride(1),\n+            b.stride(0), b.stride(1),\n+            c.stride(0), c.stride(1),\n+        )\n+        return c\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 213-217\n+\n+Unit Test\n+---------\n+\n+Still we can test our matrix multiplication with block pointers against a native torch implementation (i.e., cuBLAS).\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 217-229\n+\n+.. code-block:: default\n+\n+\n+    torch.manual_seed(0)\n+    a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n+    b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n+    triton_output = matmul(a, b)\n+    torch_output = torch.matmul(a, b)\n+    print(f\"triton_output={triton_output}\")\n+    print(f\"torch_output={torch_output}\")\n+    if torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\n+        print(\"\u2705 Triton and Torch match\")\n+    else:\n+        print(\"\u274c Triton and Torch differ\")\n+\n+\n+\n+\n+.. rst-class:: sphx-glr-script-out\n+\n+ .. code-block:: none\n+\n+    triton_output=tensor([[-10.9531,  -4.7109,  15.6953,  ..., -28.4062,   4.3320, -26.4219],\n+            [ 26.8438,  10.0469,  -5.4297,  ..., -11.2969,  -8.5312,  30.7500],\n+            [-13.2578,  15.8516,  18.0781,  ..., -21.7656,  -8.6406,  10.2031],\n+            ...,\n+            [ 40.2812,  18.6094, -25.6094,  ...,  -2.7598,  -3.2441,  41.0000],\n+            [ -6.1211, -16.8281,   4.4844,  ..., -21.0312,  24.7031,  15.0234],\n+            [-17.0938, -19.0000,  -0.3831,  ...,  21.5469, -30.2344, -13.2188]],\n+           device='cuda:0', dtype=torch.float16)\n+    torch_output=tensor([[-10.9531,  -4.7109,  15.6953,  ..., -28.4062,   4.3320, -26.4219],\n+            [ 26.8438,  10.0469,  -5.4297,  ..., -11.2969,  -8.5312,  30.7500],\n+            [-13.2578,  15.8516,  18.0781,  ..., -21.7656,  -8.6406,  10.2031],\n+            ...,\n+            [ 40.2812,  18.6094, -25.6094,  ...,  -2.7598,  -3.2441,  41.0000],\n+            [ -6.1211, -16.8281,   4.4844,  ..., -21.0312,  24.7031,  15.0234],\n+            [-17.0938, -19.0000,  -0.3831,  ...,  21.5469, -30.2344, -13.2188]],\n+           device='cuda:0', dtype=torch.float16)\n+    \u2705 Triton and Torch match\n+\n+\n+\n+\n+\n+.. rst-class:: sphx-glr-timing\n+\n+   **Total running time of the script:** ( 0 minutes  6.243 seconds)\n+\n+\n+.. _sphx_glr_download_getting-started_tutorials_08-experimental-block-pointer.py:\n+\n+.. only:: html\n+\n+  .. container:: sphx-glr-footer sphx-glr-footer-example\n+\n+\n+\n+\n+    .. container:: sphx-glr-download sphx-glr-download-python\n+\n+      :download:`Download Python source code: 08-experimental-block-pointer.py <08-experimental-block-pointer.py>`\n+\n+    .. container:: sphx-glr-download sphx-glr-download-jupyter\n+\n+      :download:`Download Jupyter notebook: 08-experimental-block-pointer.ipynb <08-experimental-block-pointer.ipynb>`\n+\n+\n+.. only:: html\n+\n+ .. rst-class:: sphx-glr-signature\n+\n+    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"}, {"filename": "main/_sources/getting-started/tutorials/index.rst.txt", "status": "modified", "additions": 149, "deletions": 0, "changes": 149, "file_content_changes": "@@ -21,9 +21,158 @@ To install the dependencies for the tutorials:\n \n .. raw:: html\n \n+    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"In this tutorial, you will write a simple vector addition using Triton.\">\n+\n+.. only:: html\n+\n+  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_01-vector-add_thumb.png\n+    :alt:\n+\n+  :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py`\n+\n+.. raw:: html\n+\n+      <div class=\"sphx-glr-thumbnail-title\">Vector Addition</div>\n+    </div>\n+\n+\n+.. raw:: html\n+\n+    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"In this tutorial, you will write a fused softmax operation that is significantly faster than Py...\">\n+\n+.. only:: html\n+\n+  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_02-fused-softmax_thumb.png\n+    :alt:\n+\n+  :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py`\n+\n+.. raw:: html\n+\n+      <div class=\"sphx-glr-thumbnail-title\">Fused Softmax</div>\n+    </div>\n+\n+\n+.. raw:: html\n+\n+    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"You will specifically learn about:\">\n+\n+.. only:: html\n+\n+  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_03-matrix-multiplication_thumb.png\n+    :alt:\n+\n+  :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py`\n+\n+.. raw:: html\n+\n+      <div class=\"sphx-glr-thumbnail-title\">Matrix Multiplication</div>\n     </div>\n \n \n+.. raw:: html\n+\n+    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"In this tutorial, you will write a memory-efficient implementation of dropout whose state will ...\">\n+\n+.. only:: html\n+\n+  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_04-low-memory-dropout_thumb.png\n+    :alt:\n+\n+  :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py`\n+\n+.. raw:: html\n+\n+      <div class=\"sphx-glr-thumbnail-title\">Low-Memory Dropout</div>\n+    </div>\n+\n+\n+.. raw:: html\n+\n+    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"In doing so, you will learn about:\">\n+\n+.. only:: html\n+\n+  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_05-layer-norm_thumb.png\n+    :alt:\n+\n+  :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py`\n+\n+.. raw:: html\n+\n+      <div class=\"sphx-glr-thumbnail-title\">Layer Normalization</div>\n+    </div>\n+\n+\n+.. raw:: html\n+\n+    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"This is a Triton implementation of the Flash Attention v2 algorithm from Tri Dao (https://trida...\">\n+\n+.. only:: html\n+\n+  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_06-fused-attention_thumb.png\n+    :alt:\n+\n+  :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py`\n+\n+.. raw:: html\n+\n+      <div class=\"sphx-glr-thumbnail-title\">Fused Attention</div>\n+    </div>\n+\n+\n+.. raw:: html\n+\n+    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"Libdevice (`tl.math`) function\">\n+\n+.. only:: html\n+\n+  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_07-math-functions_thumb.png\n+    :alt:\n+\n+  :ref:`sphx_glr_getting-started_tutorials_07-math-functions.py`\n+\n+.. raw:: html\n+\n+      <div class=\"sphx-glr-thumbnail-title\">Libdevice (`tl.math`) function</div>\n+    </div>\n+\n+\n+.. raw:: html\n+\n+    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"Block Pointer (Experimental)\">\n+\n+.. only:: html\n+\n+  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_08-experimental-block-pointer_thumb.png\n+    :alt:\n+\n+  :ref:`sphx_glr_getting-started_tutorials_08-experimental-block-pointer.py`\n+\n+.. raw:: html\n+\n+      <div class=\"sphx-glr-thumbnail-title\">Block Pointer (Experimental)</div>\n+    </div>\n+\n+\n+.. raw:: html\n+\n+    </div>\n+\n+\n+.. toctree::\n+   :hidden:\n+\n+   /getting-started/tutorials/01-vector-add\n+   /getting-started/tutorials/02-fused-softmax\n+   /getting-started/tutorials/03-matrix-multiplication\n+   /getting-started/tutorials/04-low-memory-dropout\n+   /getting-started/tutorials/05-layer-norm\n+   /getting-started/tutorials/06-fused-attention\n+   /getting-started/tutorials/07-math-functions\n+   /getting-started/tutorials/08-experimental-block-pointer\n+\n+\n .. only:: html\n \n   .. container:: sphx-glr-footer sphx-glr-footer-gallery"}, {"filename": "main/_sources/getting-started/tutorials/sg_execution_times.rst.txt", "status": "added", "additions": 27, "deletions": 0, "changes": 27, "file_content_changes": "@@ -0,0 +1,27 @@\n+\n+:orphan:\n+\n+.. _sphx_glr_getting-started_tutorials_sg_execution_times:\n+\n+\n+Computation times\n+=================\n+**02:16.367** total execution time for **getting-started_tutorials** files:\n+\n++-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n+| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``)           | 00:40.563 | 0.0 MB |\n++-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n+| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                           | 00:38.788 | 0.0 MB |\n++-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n+| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                                 | 00:29.379 | 0.0 MB |\n++-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n+| :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py` (``06-fused-attention.py``)                       | 00:14.839 | 0.0 MB |\n++-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n+| :ref:`sphx_glr_getting-started_tutorials_08-experimental-block-pointer.py` (``08-experimental-block-pointer.py``) | 00:06.243 | 0.0 MB |\n++-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n+| :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                                 | 00:05.689 | 0.0 MB |\n++-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n+| :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)                 | 00:00.642 | 0.0 MB |\n++-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n+| :ref:`sphx_glr_getting-started_tutorials_07-math-functions.py` (``07-math-functions.py``)                         | 00:00.224 | 0.0 MB |\n++-------------------------------------------------------------------------------------------------------------------+-----------+--------+"}, {"filename": "main/genindex.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -415,20 +415,7 @@ <h2 id=\"Z\">Z</h2>\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"genindex.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/getting-started/installation.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -157,20 +157,7 @@ <h3>Python Package<a class=\"headerlink\" href=\"#python-package\" title=\"Permalink\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"installation.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/getting-started/tutorials/01-vector-add.html", "status": "added", "additions": 300, "deletions": 0, "changes": 300, "file_content_changes": "@@ -0,0 +1,300 @@\n+<!DOCTYPE html>\n+<html class=\"writer-html5\" lang=\"en\" >\n+<head>\n+  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n+\n+  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n+  <title>Vector Addition &mdash; Triton  documentation</title>\n+      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n+  <!--[if lt IE 9]>\n+    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n+  <![endif]-->\n+  \n+        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n+        <script src=\"../../_static/doctools.js\"></script>\n+        <script src=\"../../_static/sphinx_highlight.js\"></script>\n+    <script src=\"../../_static/js/theme.js\"></script>\n+    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n+    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n+    <link rel=\"next\" title=\"Fused Softmax\" href=\"02-fused-softmax.html\" />\n+    <link rel=\"prev\" title=\"Tutorials\" href=\"index.html\" /> \n+</head>\n+\n+<body class=\"wy-body-for-nav\"> \n+  <div class=\"wy-grid-for-nav\">\n+    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n+      <div class=\"wy-side-scroll\">\n+        <div class=\"wy-side-nav-search\" >\n+\n+          \n+          \n+          <a href=\"../../index.html\" class=\"icon icon-home\">\n+            Triton\n+          </a>\n+<div role=\"search\">\n+  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n+    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n+    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n+    <input type=\"hidden\" name=\"area\" value=\"default\" />\n+  </form>\n+</div>\n+        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n+              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n+<ul class=\"current\">\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../installation.html\">Installation</a></li>\n+<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"index.html\">Tutorials</a><ul class=\"current\">\n+<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Vector Addition</a><ul>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#compute-kernel\">Compute Kernel</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#benchmark\">Benchmark</a></li>\n+</ul>\n+</li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"02-fused-softmax.html\">Fused Softmax</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"08-experimental-block-pointer.html\">Block Pointer (Experimental)</a></li>\n+</ul>\n+</li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n+</ul>\n+\n+        </div>\n+      </div>\n+    </nav>\n+\n+    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n+          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n+          <a href=\"../../index.html\">Triton</a>\n+      </nav>\n+\n+      <div class=\"wy-nav-content\">\n+        <div class=\"rst-content\">\n+          <div role=\"navigation\" aria-label=\"Page navigation\">\n+  <ul class=\"wy-breadcrumbs\">\n+      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n+          <li class=\"breadcrumb-item\"><a href=\"index.html\">Tutorials</a></li>\n+      <li class=\"breadcrumb-item active\">Vector Addition</li>\n+      <li class=\"wy-breadcrumbs-aside\">\n+            <a href=\"../../_sources/getting-started/tutorials/01-vector-add.rst.txt\" rel=\"nofollow\"> View page source</a>\n+      </li>\n+  </ul>\n+  <hr/>\n+</div>\n+          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n+           <div itemprop=\"articleBody\">\n+             \n+  <div class=\"sphx-glr-download-link-note admonition note\">\n+<p class=\"admonition-title\">Note</p>\n+<p><a class=\"reference internal\" href=\"#sphx-glr-download-getting-started-tutorials-01-vector-add-py\"><span class=\"std std-ref\">Go to the end</span></a>\n+to download the full example code</p>\n+</div>\n+<section class=\"sphx-glr-example-title\" id=\"vector-addition\">\n+<span id=\"sphx-glr-getting-started-tutorials-01-vector-add-py\"></span><h1>Vector Addition<a class=\"headerlink\" href=\"#vector-addition\" title=\"Permalink to this heading\">\u00b6</a></h1>\n+<p>In this tutorial, you will write a simple vector addition using Triton.</p>\n+<p>In doing so, you will learn about:</p>\n+<ul class=\"simple\">\n+<li><p>The basic programming model of Triton.</p></li>\n+<li><p>The <cite>triton.jit</cite> decorator, which is used to define Triton kernels.</p></li>\n+<li><p>The best practices for validating and benchmarking your custom ops against native reference implementations.</p></li>\n+</ul>\n+<section id=\"compute-kernel\">\n+<h2>Compute Kernel<a class=\"headerlink\" href=\"#compute-kernel\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n+\n+<span class=\"kn\">import</span> <span class=\"nn\">triton</span>\n+<span class=\"kn\">import</span> <span class=\"nn\">triton.language</span> <span class=\"k\">as</span> <span class=\"nn\">tl</span>\n+\n+\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n+<span class=\"k\">def</span> <span class=\"nf\">add_kernel</span><span class=\"p\">(</span>\n+    <span class=\"n\">x_ptr</span><span class=\"p\">,</span>  <span class=\"c1\"># *Pointer* to first input vector.</span>\n+    <span class=\"n\">y_ptr</span><span class=\"p\">,</span>  <span class=\"c1\"># *Pointer* to second input vector.</span>\n+    <span class=\"n\">output_ptr</span><span class=\"p\">,</span>  <span class=\"c1\"># *Pointer* to output vector.</span>\n+    <span class=\"n\">n_elements</span><span class=\"p\">,</span>  <span class=\"c1\"># Size of the vector.</span>\n+    <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>  <span class=\"c1\"># Number of elements each program should process.</span>\n+                 <span class=\"c1\"># NOTE: `constexpr` so it can be used as a shape value.</span>\n+<span class=\"p\">):</span>\n+    <span class=\"c1\"># There are multiple &#39;programs&#39; processing different data. We identify which program</span>\n+    <span class=\"c1\"># we are here:</span>\n+    <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>  <span class=\"c1\"># We use a 1D launch grid so axis is 0.</span>\n+    <span class=\"c1\"># This program will process inputs that are offset from the initial data.</span>\n+    <span class=\"c1\"># For instance, if you had a vector of length 256 and block_size of 64, the programs</span>\n+    <span class=\"c1\"># would each access the elements [0:64, 64:128, 128:192, 192:256].</span>\n+    <span class=\"c1\"># Note that offsets is a list of pointers:</span>\n+    <span class=\"n\">block_start</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE</span>\n+    <span class=\"n\">offsets</span> <span class=\"o\">=</span> <span class=\"n\">block_start</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># Create a mask to guard memory operations against out-of-bounds accesses.</span>\n+    <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">offsets</span> <span class=\"o\">&lt;</span> <span class=\"n\">n_elements</span>\n+    <span class=\"c1\"># Load x and y from DRAM, masking out any extra elements in case the input is not a</span>\n+    <span class=\"c1\"># multiple of the block size.</span>\n+    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">x_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n+    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">y_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n+    <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"n\">y</span>\n+    <span class=\"c1\"># Write x + y back to DRAM.</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">output_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+<p>Let\u2019s also declare a helper function to (1) allocate the <cite>z</cite> tensor\n+and (2) enqueue the above kernel with appropriate grid/block sizes:</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">add</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">):</span>\n+    <span class=\"c1\"># We need to preallocate the output.</span>\n+    <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">is_cuda</span> <span class=\"ow\">and</span> <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">is_cuda</span> <span class=\"ow\">and</span> <span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">is_cuda</span>\n+    <span class=\"n\">n_elements</span> <span class=\"o\">=</span> <span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">numel</span><span class=\"p\">()</span>\n+    <span class=\"c1\"># The SPMD launch grid denotes the number of kernel instances that run in parallel.</span>\n+    <span class=\"c1\"># It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -&gt; Tuple[int].</span>\n+    <span class=\"c1\"># In this case, we use a 1D grid where the size is the number of blocks:</span>\n+    <span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">meta</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">meta</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">]),)</span>\n+    <span class=\"c1\"># NOTE:</span>\n+    <span class=\"c1\">#  - Each torch.tensor object is implicitly converted into a pointer to its first element.</span>\n+    <span class=\"c1\">#  - `triton.jit`&#39;ed functions can be indexed with a launch grid to obtain a callable GPU kernel.</span>\n+    <span class=\"c1\">#  - Don&#39;t forget to pass meta-parameters as keywords arguments.</span>\n+    <span class=\"n\">add_kernel</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"o\">=</span><span class=\"mi\">1024</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># We return a handle to z but, since `torch.cuda.synchronize()` hasn&#39;t been called, the kernel is still</span>\n+    <span class=\"c1\"># running asynchronously at this point.</span>\n+    <span class=\"k\">return</span> <span class=\"n\">output</span>\n+</pre></div>\n+</div>\n+<p>We can now use the above function to compute the element-wise sum of two <cite>torch.tensor</cite> objects and test its correctness:</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+<span class=\"n\">size</span> <span class=\"o\">=</span> <span class=\"mi\">98432</span>\n+<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n+<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n+<span class=\"n\">output_torch</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"n\">y</span>\n+<span class=\"n\">output_triton</span> <span class=\"o\">=</span> <span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n+<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">output_torch</span><span class=\"p\">)</span>\n+<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">output_triton</span><span class=\"p\">)</span>\n+<span class=\"nb\">print</span><span class=\"p\">(</span>\n+    <span class=\"sa\">f</span><span class=\"s1\">&#39;The maximum difference between torch and triton is &#39;</span>\n+    <span class=\"sa\">f</span><span class=\"s1\">&#39;</span><span class=\"si\">{</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">abs</span><span class=\"p\">(</span><span class=\"n\">output_torch</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"w\"> </span><span class=\"n\">output_triton</span><span class=\"p\">))</span><span class=\"si\">}</span><span class=\"s1\">&#39;</span>\n+<span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+<div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>tensor([1.3713, 1.3076, 0.4940,  ..., 0.6724, 1.2141, 0.9733], device=&#39;cuda:0&#39;)\n+tensor([1.3713, 1.3076, 0.4940,  ..., 0.6724, 1.2141, 0.9733], device=&#39;cuda:0&#39;)\n+The maximum difference between torch and triton is 0.0\n+</pre></div>\n+</div>\n+<p>Seems like we\u2019re good to go!</p>\n+</section>\n+<section id=\"benchmark\">\n+<h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>We can now benchmark our custom op on vectors of increasing sizes to get a sense of how it does relative to PyTorch.\n+To make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of our custom ops.\n+for different problem sizes.</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">perf_report</span><span class=\"p\">(</span>\n+    <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">Benchmark</span><span class=\"p\">(</span>\n+        <span class=\"n\">x_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;size&#39;</span><span class=\"p\">],</span>  <span class=\"c1\"># Argument names to use as an x-axis for the plot.</span>\n+        <span class=\"n\">x_vals</span><span class=\"o\">=</span><span class=\"p\">[</span>\n+            <span class=\"mi\">2</span> <span class=\"o\">**</span> <span class=\"n\">i</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">12</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n+        <span class=\"p\">],</span>  <span class=\"c1\"># Different possible values for `x_name`.</span>\n+        <span class=\"n\">x_log</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>  <span class=\"c1\"># x axis is logarithmic.</span>\n+        <span class=\"n\">line_arg</span><span class=\"o\">=</span><span class=\"s1\">&#39;provider&#39;</span><span class=\"p\">,</span>  <span class=\"c1\"># Argument name whose value corresponds to a different line in the plot.</span>\n+        <span class=\"n\">line_vals</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;torch&#39;</span><span class=\"p\">],</span>  <span class=\"c1\"># Possible values for `line_arg`.</span>\n+        <span class=\"n\">line_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;Triton&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;Torch&#39;</span><span class=\"p\">],</span>  <span class=\"c1\"># Label name for the lines.</span>\n+        <span class=\"n\">styles</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s1\">&#39;blue&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;green&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">)],</span>  <span class=\"c1\"># Line styles.</span>\n+        <span class=\"n\">ylabel</span><span class=\"o\">=</span><span class=\"s1\">&#39;GB/s&#39;</span><span class=\"p\">,</span>  <span class=\"c1\"># Label name for the y-axis.</span>\n+        <span class=\"n\">plot_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;vector-add-performance&#39;</span><span class=\"p\">,</span>  <span class=\"c1\"># Name for the plot. Used also as a file name for saving the plot.</span>\n+        <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{},</span>  <span class=\"c1\"># Values for function arguments not in `x_names` and `y_name`.</span>\n+    <span class=\"p\">)</span>\n+<span class=\"p\">)</span>\n+<span class=\"k\">def</span> <span class=\"nf\">benchmark</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">provider</span><span class=\"p\">):</span>\n+    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"n\">quantiles</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">]</span>\n+    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;torch&#39;</span><span class=\"p\">:</span>\n+        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">)</span>\n+    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">:</span>\n+        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">),</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">)</span>\n+    <span class=\"n\">gbps</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">ms</span><span class=\"p\">:</span> <span class=\"mi\">12</span> <span class=\"o\">*</span> <span class=\"n\">size</span> <span class=\"o\">/</span> <span class=\"n\">ms</span> <span class=\"o\">*</span> <span class=\"mf\">1e-6</span>\n+    <span class=\"k\">return</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">ms</span><span class=\"p\">),</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">max_ms</span><span class=\"p\">),</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">min_ms</span><span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+<p>We can now run the decorated function above. Pass <cite>print_data=True</cite> to see the performance number, <cite>show_plots=True</cite> to plot them, and/or\n+<a href=\"#id1\"><span class=\"problematic\" id=\"id2\">`</span></a>save_path=\u2019/path/to/results/\u2019 to save them to disk along with raw CSV data:</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">benchmark</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">print_data</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">show_plots</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+<img src=\"../../_images/sphx_glr_01-vector-add_001.png\" srcset=\"../../_images/sphx_glr_01-vector-add_001.png\" alt=\"01 vector add\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>vector-add-performance:\n+           size       Triton        Torch\n+0        4096.0     9.600000     8.000000\n+1        8192.0    19.200000    15.999999\n+2       16384.0    31.999999    31.999999\n+3       32768.0    63.999998    63.999998\n+4       65536.0   127.999995   127.999995\n+5      131072.0   219.428568   219.428568\n+6      262144.0   384.000001   384.000001\n+7      524288.0   614.400016   614.400016\n+8     1048576.0   819.200021   819.200021\n+9     2097152.0  1023.999964  1023.999964\n+10    4194304.0  1228.800031  1228.800031\n+11    8388608.0  1424.695621  1424.695621\n+12   16777216.0  1560.380965  1560.380965\n+13   33554432.0  1631.601649  1624.859540\n+14   67108864.0  1669.706983  1662.646960\n+15  134217728.0  1684.008546  1680.410210\n+</pre></div>\n+</div>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  5.689 seconds)</p>\n+<div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-01-vector-add-py\">\n+<div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/62d97d49a32414049819dd8bb8378080/01-vector-add.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">01-vector-add.py</span></code></a></p>\n+</div>\n+<div class=\"sphx-glr-download sphx-glr-download-jupyter docutils container\">\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/f191ee1e78dc52eb5f7cba88f71cef2f/01-vector-add.ipynb\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Jupyter</span> <span class=\"pre\">notebook:</span> <span class=\"pre\">01-vector-add.ipynb</span></code></a></p>\n+</div>\n+</div>\n+<p class=\"sphx-glr-signature\"><a class=\"reference external\" href=\"https://sphinx-gallery.github.io\">Gallery generated by Sphinx-Gallery</a></p>\n+</section>\n+</section>\n+\n+\n+           </div>\n+          </div>\n+          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n+        <a href=\"index.html\" class=\"btn btn-neutral float-left\" title=\"Tutorials\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"02-fused-softmax.html\" class=\"btn btn-neutral float-right\" title=\"Fused Softmax\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+    </div>\n+\n+  <hr/>\n+\n+  <div role=\"contentinfo\">\n+    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n+  </div>\n+\n+  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n+    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n+    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n+   \n+\n+</footer>\n+        </div>\n+      </div>\n+    </section>\n+  </div>\n+  <script>\n+      jQuery(function () {\n+          SphinxRtdTheme.Navigation.enable(true);\n+      });\n+  </script> \n+\n+</body>\n+</html>\n\\ No newline at end of file"}, {"filename": "main/getting-started/tutorials/02-fused-softmax.html", "status": "added", "additions": 352, "deletions": 0, "changes": 352, "file_content_changes": "@@ -0,0 +1,352 @@\n+<!DOCTYPE html>\n+<html class=\"writer-html5\" lang=\"en\" >\n+<head>\n+  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n+\n+  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n+  <title>Fused Softmax &mdash; Triton  documentation</title>\n+      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n+  <!--[if lt IE 9]>\n+    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n+  <![endif]-->\n+  \n+        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n+        <script src=\"../../_static/doctools.js\"></script>\n+        <script src=\"../../_static/sphinx_highlight.js\"></script>\n+        <script async=\"async\" src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\n+    <script src=\"../../_static/js/theme.js\"></script>\n+    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n+    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n+    <link rel=\"next\" title=\"Matrix Multiplication\" href=\"03-matrix-multiplication.html\" />\n+    <link rel=\"prev\" title=\"Vector Addition\" href=\"01-vector-add.html\" /> \n+</head>\n+\n+<body class=\"wy-body-for-nav\"> \n+  <div class=\"wy-grid-for-nav\">\n+    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n+      <div class=\"wy-side-scroll\">\n+        <div class=\"wy-side-nav-search\" >\n+\n+          \n+          \n+          <a href=\"../../index.html\" class=\"icon icon-home\">\n+            Triton\n+          </a>\n+<div role=\"search\">\n+  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n+    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n+    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n+    <input type=\"hidden\" name=\"area\" value=\"default\" />\n+  </form>\n+</div>\n+        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n+              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n+<ul class=\"current\">\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../installation.html\">Installation</a></li>\n+<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"index.html\">Tutorials</a><ul class=\"current\">\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"01-vector-add.html\">Vector Addition</a></li>\n+<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Fused Softmax</a><ul>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#motivations\">Motivations</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#compute-kernel\">Compute Kernel</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#unit-test\">Unit Test</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#benchmark\">Benchmark</a></li>\n+</ul>\n+</li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"08-experimental-block-pointer.html\">Block Pointer (Experimental)</a></li>\n+</ul>\n+</li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n+</ul>\n+\n+        </div>\n+      </div>\n+    </nav>\n+\n+    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n+          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n+          <a href=\"../../index.html\">Triton</a>\n+      </nav>\n+\n+      <div class=\"wy-nav-content\">\n+        <div class=\"rst-content\">\n+          <div role=\"navigation\" aria-label=\"Page navigation\">\n+  <ul class=\"wy-breadcrumbs\">\n+      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n+          <li class=\"breadcrumb-item\"><a href=\"index.html\">Tutorials</a></li>\n+      <li class=\"breadcrumb-item active\">Fused Softmax</li>\n+      <li class=\"wy-breadcrumbs-aside\">\n+            <a href=\"../../_sources/getting-started/tutorials/02-fused-softmax.rst.txt\" rel=\"nofollow\"> View page source</a>\n+      </li>\n+  </ul>\n+  <hr/>\n+</div>\n+          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n+           <div itemprop=\"articleBody\">\n+             \n+  <div class=\"sphx-glr-download-link-note admonition note\">\n+<p class=\"admonition-title\">Note</p>\n+<p><a class=\"reference internal\" href=\"#sphx-glr-download-getting-started-tutorials-02-fused-softmax-py\"><span class=\"std std-ref\">Go to the end</span></a>\n+to download the full example code</p>\n+</div>\n+<section class=\"sphx-glr-example-title\" id=\"fused-softmax\">\n+<span id=\"sphx-glr-getting-started-tutorials-02-fused-softmax-py\"></span><h1>Fused Softmax<a class=\"headerlink\" href=\"#fused-softmax\" title=\"Permalink to this heading\">\u00b6</a></h1>\n+<p>In this tutorial, you will write a fused softmax operation that is significantly faster\n+than PyTorch\u2019s native op for a particular class of matrices: those whose rows can fit in\n+the GPU\u2019s SRAM.</p>\n+<p>In doing so, you will learn about:</p>\n+<ul class=\"simple\">\n+<li><p>The benefits of kernel fusion for bandwidth-bound operations.</p></li>\n+<li><p>Reduction operators in Triton.</p></li>\n+</ul>\n+<section id=\"motivations\">\n+<h2>Motivations<a class=\"headerlink\" href=\"#motivations\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>Custom GPU kernels for elementwise additions are educationally valuable but won\u2019t get you very far in practice.\n+Let us consider instead the case of a simple (numerically stabilized) softmax operation:</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n+\n+<span class=\"kn\">import</span> <span class=\"nn\">triton</span>\n+<span class=\"kn\">import</span> <span class=\"nn\">triton.language</span> <span class=\"k\">as</span> <span class=\"nn\">tl</span>\n+\n+\n+<span class=\"nd\">@torch</span><span class=\"o\">.</span><span class=\"n\">jit</span><span class=\"o\">.</span><span class=\"n\">script</span>\n+<span class=\"k\">def</span> <span class=\"nf\">naive_softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n+<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Compute row-wise softmax of X using native pytorch</span>\n+\n+<span class=\"sd\">    We subtract the maximum element in order to avoid overflows. Softmax is invariant to</span>\n+<span class=\"sd\">    this shift.</span>\n+<span class=\"sd\">    &quot;&quot;&quot;</span>\n+    <span class=\"c1\"># read  MN elements ; write M  elements</span>\n+    <span class=\"n\">x_max</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n+    <span class=\"c1\"># read MN + M elements ; write MN elements</span>\n+    <span class=\"n\">z</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">x_max</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n+    <span class=\"c1\"># read  MN elements ; write MN elements</span>\n+    <span class=\"n\">numerator</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">z</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># read  MN elements ; write M  elements</span>\n+    <span class=\"n\">denominator</span> <span class=\"o\">=</span> <span class=\"n\">numerator</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># read MN + M elements ; write MN elements</span>\n+    <span class=\"n\">ret</span> <span class=\"o\">=</span> <span class=\"n\">numerator</span> <span class=\"o\">/</span> <span class=\"n\">denominator</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n+    <span class=\"c1\"># in total: read 5MN + 2M elements ; wrote 3MN + 2M elements</span>\n+    <span class=\"k\">return</span> <span class=\"n\">ret</span>\n+</pre></div>\n+</div>\n+<p>When implemented naively in PyTorch, computing <code class=\"code docutils literal notranslate\"><span class=\"pre\">y</span> <span class=\"pre\">=</span> <span class=\"pre\">naive_softmax(x)</span></code> for <span class=\"math notranslate nohighlight\">\\(x \\in R^{M \\times N}\\)</span>\n+requires reading <span class=\"math notranslate nohighlight\">\\(5MN + 2M\\)</span> elements from DRAM and writing back <span class=\"math notranslate nohighlight\">\\(3MN + 2M\\)</span> elements.\n+This is obviously wasteful; we\u2019d prefer to have a custom \u201cfused\u201d kernel that only reads\n+X once and does all the necessary computations on-chip.\n+Doing so would require reading and writing back only <span class=\"math notranslate nohighlight\">\\(MN\\)</span> bytes, so we could\n+expect a theoretical speed-up of ~4x (i.e., <span class=\"math notranslate nohighlight\">\\((8MN + 4M) / 2MN\\)</span>).\n+The <cite>torch.jit.script</cite> flags aims to perform this kind of \u201ckernel fusion\u201d automatically\n+but, as we will see later, it is still far from ideal.</p>\n+</section>\n+<section id=\"compute-kernel\">\n+<h2>Compute Kernel<a class=\"headerlink\" href=\"#compute-kernel\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>Our softmax kernel works as follows: each program loads a row of the input matrix X,\n+normalizes it and writes back the result to the output Y.</p>\n+<p>Note that one important limitation of Triton is that each block must have a\n+power-of-two number of elements, so we need to internally \u201cpad\u201d each row and guard the\n+memory operations properly if we want to handle any possible input shapes:</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n+<span class=\"k\">def</span> <span class=\"nf\">softmax_kernel</span><span class=\"p\">(</span>\n+    <span class=\"n\">output_ptr</span><span class=\"p\">,</span> <span class=\"n\">input_ptr</span><span class=\"p\">,</span> <span class=\"n\">input_row_stride</span><span class=\"p\">,</span> <span class=\"n\">output_row_stride</span><span class=\"p\">,</span> <span class=\"n\">n_cols</span><span class=\"p\">,</span>\n+    <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span>\n+<span class=\"p\">):</span>\n+    <span class=\"c1\"># The rows of the softmax are independent, so we parallelize across those</span>\n+    <span class=\"n\">row_idx</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># The stride represents how much we need to increase the pointer to advance 1 row</span>\n+    <span class=\"n\">row_start_ptr</span> <span class=\"o\">=</span> <span class=\"n\">input_ptr</span> <span class=\"o\">+</span> <span class=\"n\">row_idx</span> <span class=\"o\">*</span> <span class=\"n\">input_row_stride</span>\n+    <span class=\"c1\"># The block size is the next power of two greater than n_cols, so we can fit each</span>\n+    <span class=\"c1\"># row in a single block</span>\n+    <span class=\"n\">col_offsets</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">)</span>\n+    <span class=\"n\">input_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">row_start_ptr</span> <span class=\"o\">+</span> <span class=\"n\">col_offsets</span>\n+    <span class=\"c1\"># Load the row into SRAM, using a mask since BLOCK_SIZE may be &gt; than n_cols</span>\n+    <span class=\"n\">row</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">input_ptrs</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">col_offsets</span> <span class=\"o\">&lt;</span> <span class=\"n\">n_cols</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=-</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s1\">&#39;inf&#39;</span><span class=\"p\">))</span>\n+    <span class=\"c1\"># Subtract maximum for numerical stability</span>\n+    <span class=\"n\">row_minus_max</span> <span class=\"o\">=</span> <span class=\"n\">row</span> <span class=\"o\">-</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">row</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)</span>\n+    <span class=\"n\">numerator</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">row_minus_max</span><span class=\"p\">)</span>\n+    <span class=\"n\">denominator</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">numerator</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"n\">softmax_output</span> <span class=\"o\">=</span> <span class=\"n\">numerator</span> <span class=\"o\">/</span> <span class=\"n\">denominator</span>\n+    <span class=\"c1\"># Write back output to DRAM</span>\n+    <span class=\"n\">output_row_start_ptr</span> <span class=\"o\">=</span> <span class=\"n\">output_ptr</span> <span class=\"o\">+</span> <span class=\"n\">row_idx</span> <span class=\"o\">*</span> <span class=\"n\">output_row_stride</span>\n+    <span class=\"n\">output_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">output_row_start_ptr</span> <span class=\"o\">+</span> <span class=\"n\">col_offsets</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">output_ptrs</span><span class=\"p\">,</span> <span class=\"n\">softmax_output</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">col_offsets</span> <span class=\"o\">&lt;</span> <span class=\"n\">n_cols</span><span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+<p>We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n+    <span class=\"n\">n_rows</span><span class=\"p\">,</span> <span class=\"n\">n_cols</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n+    <span class=\"c1\"># The block size is the smallest power of two greater than the number of columns in `x`</span>\n+    <span class=\"n\">BLOCK_SIZE</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">next_power_of_2</span><span class=\"p\">(</span><span class=\"n\">n_cols</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># Another trick we can use is to ask the compiler to use more threads per row by</span>\n+    <span class=\"c1\"># increasing the number of warps (`num_warps`) over which each row is distributed.</span>\n+    <span class=\"c1\"># You will see in the next tutorial how to auto-tune this value in a more natural</span>\n+    <span class=\"c1\"># way so you don&#39;t have to come up with manual heuristics yourself.</span>\n+    <span class=\"n\">num_warps</span> <span class=\"o\">=</span> <span class=\"mi\">4</span>\n+    <span class=\"k\">if</span> <span class=\"n\">BLOCK_SIZE</span> <span class=\"o\">&gt;=</span> <span class=\"mi\">2048</span><span class=\"p\">:</span>\n+        <span class=\"n\">num_warps</span> <span class=\"o\">=</span> <span class=\"mi\">8</span>\n+    <span class=\"k\">if</span> <span class=\"n\">BLOCK_SIZE</span> <span class=\"o\">&gt;=</span> <span class=\"mi\">4096</span><span class=\"p\">:</span>\n+        <span class=\"n\">num_warps</span> <span class=\"o\">=</span> <span class=\"mi\">16</span>\n+    <span class=\"c1\"># Allocate output</span>\n+    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># Enqueue kernel. The 1D launch grid is simple: we have one kernel instance per row o</span>\n+    <span class=\"c1\"># f the input matrix</span>\n+    <span class=\"n\">softmax_kernel</span><span class=\"p\">[(</span><span class=\"n\">n_rows</span><span class=\"p\">,)](</span>\n+        <span class=\"n\">y</span><span class=\"p\">,</span>\n+        <span class=\"n\">x</span><span class=\"p\">,</span>\n+        <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span>\n+        <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span>\n+        <span class=\"n\">n_cols</span><span class=\"p\">,</span>\n+        <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"n\">num_warps</span><span class=\"p\">,</span>\n+        <span class=\"n\">BLOCK_SIZE</span><span class=\"o\">=</span><span class=\"n\">BLOCK_SIZE</span><span class=\"p\">,</span>\n+    <span class=\"p\">)</span>\n+    <span class=\"k\">return</span> <span class=\"n\">y</span>\n+</pre></div>\n+</div>\n+</section>\n+<section id=\"unit-test\">\n+<h2>Unit Test<a class=\"headerlink\" href=\"#unit-test\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>We make sure that we test our kernel on a matrix with an irregular number of rows and columns.\n+This will allow us to verify that our padding mechanism works.</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">1823</span><span class=\"p\">,</span> <span class=\"mi\">781</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n+<span class=\"n\">y_triton</span> <span class=\"o\">=</span> <span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n+<span class=\"n\">y_torch</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n+<span class=\"k\">assert</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">y_triton</span><span class=\"p\">,</span> <span class=\"n\">y_torch</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"n\">y_triton</span><span class=\"p\">,</span> <span class=\"n\">y_torch</span><span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+<p>As expected, the results are identical.</p>\n+</section>\n+<section id=\"benchmark\">\n+<h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>Here we will benchmark our operation as a function of the number of columns in the input matrix \u2013 assuming 4096 rows.\n+We will then compare its performance against (1) <code class=\"code docutils literal notranslate\"><span class=\"pre\">torch.softmax</span></code> and (2) the <code class=\"code docutils literal notranslate\"><span class=\"pre\">naive_softmax</span></code> defined above.</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">perf_report</span><span class=\"p\">(</span>\n+    <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">Benchmark</span><span class=\"p\">(</span>\n+        <span class=\"n\">x_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;N&#39;</span><span class=\"p\">],</span>  <span class=\"c1\"># argument names to use as an x-axis for the plot</span>\n+        <span class=\"n\">x_vals</span><span class=\"o\">=</span><span class=\"p\">[</span>\n+            <span class=\"mi\">128</span> <span class=\"o\">*</span> <span class=\"n\">i</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">)</span>\n+        <span class=\"p\">],</span>  <span class=\"c1\"># different possible values for `x_name`</span>\n+        <span class=\"n\">line_arg</span><span class=\"o\">=</span><span class=\"s1\">&#39;provider&#39;</span><span class=\"p\">,</span>  <span class=\"c1\"># argument name whose value corresponds to a different line in the plot</span>\n+        <span class=\"n\">line_vals</span><span class=\"o\">=</span><span class=\"p\">[</span>\n+            <span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">,</span>\n+            <span class=\"s1\">&#39;torch-native&#39;</span><span class=\"p\">,</span>\n+            <span class=\"s1\">&#39;torch-jit&#39;</span><span class=\"p\">,</span>\n+        <span class=\"p\">],</span>  <span class=\"c1\"># possible values for `line_arg``</span>\n+        <span class=\"n\">line_names</span><span class=\"o\">=</span><span class=\"p\">[</span>\n+            <span class=\"s2\">&quot;Triton&quot;</span><span class=\"p\">,</span>\n+            <span class=\"s2\">&quot;Torch (native)&quot;</span><span class=\"p\">,</span>\n+            <span class=\"s2\">&quot;Torch (jit)&quot;</span><span class=\"p\">,</span>\n+        <span class=\"p\">],</span>  <span class=\"c1\"># label name for the lines</span>\n+        <span class=\"n\">styles</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s1\">&#39;blue&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;green&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;green&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;--&#39;</span><span class=\"p\">)],</span>  <span class=\"c1\"># line styles</span>\n+        <span class=\"n\">ylabel</span><span class=\"o\">=</span><span class=\"s2\">&quot;GB/s&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># label name for the y-axis</span>\n+        <span class=\"n\">plot_name</span><span class=\"o\">=</span><span class=\"s2\">&quot;softmax-performance&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># name for the plot. Used also as a file name for saving the plot.</span>\n+        <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">4096</span><span class=\"p\">},</span>  <span class=\"c1\"># values for function arguments not in `x_names` and `y_name`</span>\n+    <span class=\"p\">)</span>\n+<span class=\"p\">)</span>\n+<span class=\"k\">def</span> <span class=\"nf\">benchmark</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">provider</span><span class=\"p\">):</span>\n+    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"n\">quantiles</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">]</span>\n+    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;torch-native&#39;</span><span class=\"p\">:</span>\n+        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">)</span>\n+    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">:</span>\n+        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">),</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">)</span>\n+    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;torch-jit&#39;</span><span class=\"p\">:</span>\n+        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">naive_softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">),</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">)</span>\n+    <span class=\"n\">gbps</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">ms</span><span class=\"p\">:</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">nelement</span><span class=\"p\">()</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">element_size</span><span class=\"p\">()</span> <span class=\"o\">*</span> <span class=\"mf\">1e-9</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">ms</span> <span class=\"o\">*</span> <span class=\"mf\">1e-3</span><span class=\"p\">)</span>\n+    <span class=\"k\">return</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">ms</span><span class=\"p\">),</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">max_ms</span><span class=\"p\">),</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">min_ms</span><span class=\"p\">)</span>\n+\n+\n+<span class=\"n\">benchmark</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">show_plots</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">print_data</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+<img src=\"../../_images/sphx_glr_02-fused-softmax_001.png\" srcset=\"../../_images/sphx_glr_02-fused-softmax_001.png\" alt=\"02 fused softmax\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>softmax-performance:\n+          N       Triton  Torch (native)  Torch (jit)\n+0     256.0   682.666643      744.727267   260.063494\n+1     384.0   877.714274      819.200021   315.076934\n+2     512.0   910.222190      910.222190   341.333321\n+3     640.0  1024.000026      930.909084   372.363633\n+4     768.0  1068.521715     1023.999964   384.000001\n+..      ...          ...             ...          ...\n+93  12160.0  1594.754129     1066.082150   461.589552\n+94  12288.0  1604.963246     1021.340281   462.063445\n+95  12416.0  1602.064538     1031.979242   461.454135\n+96  12544.0  1599.235121     1018.802024   460.858785\n+97  12672.0  1596.472358     1008.716405   460.276950\n+\n+[98 rows x 4 columns]\n+</pre></div>\n+</div>\n+<dl class=\"simple\">\n+<dt>In the above plot, we can see that:</dt><dd><ul class=\"simple\">\n+<li><p>Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.</p></li>\n+<li><p>Triton is noticeably faster than <code class=\"code docutils literal notranslate\"><span class=\"pre\">torch.softmax</span></code> \u2013 in addition to being <strong>easier to read, understand and maintain</strong>.\n+Note however that the PyTorch <cite>softmax</cite> operation is more general and will work on tensors of any shape.</p></li>\n+</ul>\n+</dd>\n+</dl>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  38.788 seconds)</p>\n+<div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-02-fused-softmax-py\">\n+<div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d91442ac2982c4e0cc3ab0f43534afbc/02-fused-softmax.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">02-fused-softmax.py</span></code></a></p>\n+</div>\n+<div class=\"sphx-glr-download sphx-glr-download-jupyter docutils container\">\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/034d953b6214fedce6ea03803c712b89/02-fused-softmax.ipynb\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Jupyter</span> <span class=\"pre\">notebook:</span> <span class=\"pre\">02-fused-softmax.ipynb</span></code></a></p>\n+</div>\n+</div>\n+<p class=\"sphx-glr-signature\"><a class=\"reference external\" href=\"https://sphinx-gallery.github.io\">Gallery generated by Sphinx-Gallery</a></p>\n+</section>\n+</section>\n+\n+\n+           </div>\n+          </div>\n+          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n+        <a href=\"01-vector-add.html\" class=\"btn btn-neutral float-left\" title=\"Vector Addition\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"03-matrix-multiplication.html\" class=\"btn btn-neutral float-right\" title=\"Matrix Multiplication\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+    </div>\n+\n+  <hr/>\n+\n+  <div role=\"contentinfo\">\n+    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n+  </div>\n+\n+  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n+    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n+    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n+   \n+\n+</footer>\n+        </div>\n+      </div>\n+    </section>\n+  </div>\n+  <script>\n+      jQuery(function () {\n+          SphinxRtdTheme.Navigation.enable(true);\n+      });\n+  </script> \n+\n+</body>\n+</html>\n\\ No newline at end of file"}, {"filename": "main/getting-started/tutorials/03-matrix-multiplication.html", "status": "added", "additions": 544, "deletions": 0, "changes": 544, "file_content_changes": "@@ -0,0 +1,544 @@\n+<!DOCTYPE html>\n+<html class=\"writer-html5\" lang=\"en\" >\n+<head>\n+  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n+\n+  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n+  <title>Matrix Multiplication &mdash; Triton  documentation</title>\n+      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n+  <!--[if lt IE 9]>\n+    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n+  <![endif]-->\n+  \n+        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n+        <script src=\"../../_static/doctools.js\"></script>\n+        <script src=\"../../_static/sphinx_highlight.js\"></script>\n+    <script src=\"../../_static/js/theme.js\"></script>\n+    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n+    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n+    <link rel=\"next\" title=\"Low-Memory Dropout\" href=\"04-low-memory-dropout.html\" />\n+    <link rel=\"prev\" title=\"Fused Softmax\" href=\"02-fused-softmax.html\" /> \n+</head>\n+\n+<body class=\"wy-body-for-nav\"> \n+  <div class=\"wy-grid-for-nav\">\n+    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n+      <div class=\"wy-side-scroll\">\n+        <div class=\"wy-side-nav-search\" >\n+\n+          \n+          \n+          <a href=\"../../index.html\" class=\"icon icon-home\">\n+            Triton\n+          </a>\n+<div role=\"search\">\n+  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n+    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n+    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n+    <input type=\"hidden\" name=\"area\" value=\"default\" />\n+  </form>\n+</div>\n+        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n+              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n+<ul class=\"current\">\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../installation.html\">Installation</a></li>\n+<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"index.html\">Tutorials</a><ul class=\"current\">\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"01-vector-add.html\">Vector Addition</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"02-fused-softmax.html\">Fused Softmax</a></li>\n+<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Matrix Multiplication</a><ul>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#motivations\">Motivations</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#compute-kernel\">Compute Kernel</a><ul>\n+<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#pointer-arithmetics\">Pointer Arithmetics</a></li>\n+<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#l2-cache-optimizations\">L2 Cache Optimizations</a></li>\n+</ul>\n+</li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#final-result\">Final Result</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#unit-test\">Unit Test</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#benchmark\">Benchmark</a><ul>\n+<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#square-matrix-performance\">Square Matrix Performance</a></li>\n+</ul>\n+</li>\n+</ul>\n+</li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"08-experimental-block-pointer.html\">Block Pointer (Experimental)</a></li>\n+</ul>\n+</li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n+</ul>\n+\n+        </div>\n+      </div>\n+    </nav>\n+\n+    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n+          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n+          <a href=\"../../index.html\">Triton</a>\n+      </nav>\n+\n+      <div class=\"wy-nav-content\">\n+        <div class=\"rst-content\">\n+          <div role=\"navigation\" aria-label=\"Page navigation\">\n+  <ul class=\"wy-breadcrumbs\">\n+      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n+          <li class=\"breadcrumb-item\"><a href=\"index.html\">Tutorials</a></li>\n+      <li class=\"breadcrumb-item active\">Matrix Multiplication</li>\n+      <li class=\"wy-breadcrumbs-aside\">\n+            <a href=\"../../_sources/getting-started/tutorials/03-matrix-multiplication.rst.txt\" rel=\"nofollow\"> View page source</a>\n+      </li>\n+  </ul>\n+  <hr/>\n+</div>\n+          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n+           <div itemprop=\"articleBody\">\n+             \n+  <div class=\"sphx-glr-download-link-note admonition note\">\n+<p class=\"admonition-title\">Note</p>\n+<p><a class=\"reference internal\" href=\"#sphx-glr-download-getting-started-tutorials-03-matrix-multiplication-py\"><span class=\"std std-ref\">Go to the end</span></a>\n+to download the full example code</p>\n+</div>\n+<section class=\"sphx-glr-example-title\" id=\"matrix-multiplication\">\n+<span id=\"sphx-glr-getting-started-tutorials-03-matrix-multiplication-py\"></span><h1>Matrix Multiplication<a class=\"headerlink\" href=\"#matrix-multiplication\" title=\"Permalink to this heading\">\u00b6</a></h1>\n+<p>In this tutorial, you will write a very short high-performance FP16 matrix multiplication kernel that achieves\n+performance on parallel with cuBLAS.</p>\n+<p>You will specifically learn about:</p>\n+<ul class=\"simple\">\n+<li><p>Block-level matrix multiplications.</p></li>\n+<li><p>Multi-dimensional pointer arithmetics.</p></li>\n+<li><p>Program re-ordering for improved L2 cache hit rate.</p></li>\n+<li><p>Automatic performance tuning.</p></li>\n+</ul>\n+<section id=\"motivations\">\n+<h2>Motivations<a class=\"headerlink\" href=\"#motivations\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>Matrix multiplications are a key building block of most modern high-performance computing systems.\n+They are notoriously hard to optimize, hence their implementation is generally done by\n+hardware vendors themselves as part of so-called \u201ckernel libraries\u201d (e.g., cuBLAS).\n+Unfortunately, these libraries are often proprietary and cannot be easily customized\n+to accommodate the needs of modern deep learning workloads (e.g., fused activation functions).\n+In this tutorial, you will learn how to implement efficient matrix multiplications by\n+yourself with Triton, in a way that is easy to customize and extend.</p>\n+<p>Roughly speaking, the kernel that we will write will implement the following blocked\n+algorithm to multiply a (M, K) by a (K, N) matrix:</p>\n+<blockquote>\n+<div><div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"c1\"># Do in parallel</span>\n+<span class=\"k\">for</span> <span class=\"n\">m</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">):</span>\n+  <span class=\"c1\"># Do in parallel</span>\n+  <span class=\"k\">for</span> <span class=\"n\">n</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">):</span>\n+    <span class=\"n\">acc</span> <span class=\"o\">=</span> <span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">):</span>\n+      <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">m</span> <span class=\"p\">:</span> <span class=\"n\">m</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">k</span> <span class=\"p\">:</span> <span class=\"n\">k</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">]</span>\n+      <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">B</span><span class=\"p\">[</span><span class=\"n\">k</span> <span class=\"p\">:</span> <span class=\"n\">k</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">,</span> <span class=\"n\">n</span> <span class=\"p\">:</span> <span class=\"n\">n</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">]</span>\n+      <span class=\"n\">acc</span> <span class=\"o\">+=</span> <span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n+    <span class=\"n\">C</span><span class=\"p\">[</span><span class=\"n\">m</span> <span class=\"p\">:</span> <span class=\"n\">m</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">n</span> <span class=\"p\">:</span> <span class=\"n\">n</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">acc</span>\n+</pre></div>\n+</div>\n+</div></blockquote>\n+<p>where each iteration of the doubly-nested for-loop is performed by a dedicated Triton program instance.</p>\n+</section>\n+<section id=\"compute-kernel\">\n+<h2>Compute Kernel<a class=\"headerlink\" href=\"#compute-kernel\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>The above algorithm is, actually, fairly straightforward to implement in Triton.\n+The main difficulty comes from the computation of the memory locations at which blocks\n+of <code class=\"code docutils literal notranslate\"><span class=\"pre\">A</span></code> and <code class=\"code docutils literal notranslate\"><span class=\"pre\">B</span></code> must be read in the inner loop. For that, we need\n+multi-dimensional pointer arithmetics.</p>\n+<section id=\"pointer-arithmetics\">\n+<h3>Pointer Arithmetics<a class=\"headerlink\" href=\"#pointer-arithmetics\" title=\"Permalink to this heading\">\u00b6</a></h3>\n+<p>For a row-major 2D tensor <code class=\"code docutils literal notranslate\"><span class=\"pre\">X</span></code>, the memory location of <code class=\"code docutils literal notranslate\"><span class=\"pre\">X[i,</span> <span class=\"pre\">j]</span></code> is given b\n+y <code class=\"code docutils literal notranslate\"><span class=\"pre\">&amp;X[i,</span> <span class=\"pre\">j]</span> <span class=\"pre\">=</span> <span class=\"pre\">X</span> <span class=\"pre\">+</span> <span class=\"pre\">i*stride_xi</span> <span class=\"pre\">+</span> <span class=\"pre\">j*stride_xj</span></code>.\n+Therefore, blocks of pointers for <code class=\"code docutils literal notranslate\"><span class=\"pre\">A[m</span> <span class=\"pre\">:</span> <span class=\"pre\">m+BLOCK_SIZE_M,</span> <span class=\"pre\">k:k+BLOCK_SIZE_K]</span></code> and\n+<code class=\"code docutils literal notranslate\"><span class=\"pre\">B[k</span> <span class=\"pre\">:</span> <span class=\"pre\">k+BLOCK_SIZE_K,</span> <span class=\"pre\">n</span> <span class=\"pre\">:</span> <span class=\"pre\">n+BLOCK_SIZE_N]</span></code> can be defined in pseudo-code as:</p>\n+<blockquote>\n+<div><div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"o\">&amp;</span><span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">m</span> <span class=\"p\">:</span> <span class=\"n\">m</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">:</span><span class=\"n\">k</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">]</span> <span class=\"o\">=</span>  <span class=\"n\">a_ptr</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">m</span> <span class=\"p\">:</span> <span class=\"n\">m</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">)[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"n\">A</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">k</span> <span class=\"p\">:</span> <span class=\"n\">k</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">)[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span><span class=\"o\">*</span><span class=\"n\">A</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">);</span>\n+<span class=\"o\">&amp;</span><span class=\"n\">B</span><span class=\"p\">[</span><span class=\"n\">k</span> <span class=\"p\">:</span> <span class=\"n\">k</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">:</span><span class=\"n\">n</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">]</span> <span class=\"o\">=</span>  <span class=\"n\">b_ptr</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">k</span> <span class=\"p\">:</span> <span class=\"n\">k</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">)[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"n\">B</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">n</span> <span class=\"p\">:</span> <span class=\"n\">n</span><span class=\"o\">+</span><span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span><span class=\"o\">*</span><span class=\"n\">B</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">);</span>\n+</pre></div>\n+</div>\n+</div></blockquote>\n+<p>Which means that pointers for blocks of A and B can be initialized (i.e., <code class=\"code docutils literal notranslate\"><span class=\"pre\">k=0</span></code>) in Triton as the following\n+code. Also note that we need an extra modulo to handle the case where <code class=\"code docutils literal notranslate\"><span class=\"pre\">M</span></code> is not a multiple of\n+<code class=\"code docutils literal notranslate\"><span class=\"pre\">BLOCK_SIZE_M</span></code> or <code class=\"code docutils literal notranslate\"><span class=\"pre\">N</span></code> is not a multiple of <code class=\"code docutils literal notranslate\"><span class=\"pre\">BLOCK_SIZE_N</span></code>, in which case we can pad the data with\n+some useless values, which will not contribute to the results. For the <code class=\"code docutils literal notranslate\"><span class=\"pre\">K</span></code> dimension, we will handle that later\n+using masking load semantics.</p>\n+<blockquote>\n+<div><div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">offs_am</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">pid_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">))</span> <span class=\"o\">%</span> <span class=\"n\">M</span>\n+<span class=\"n\">offs_bn</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">pid_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_N</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">))</span> <span class=\"o\">%</span> <span class=\"n\">N</span>\n+<span class=\"n\">offs_k</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">)</span>\n+<span class=\"n\">a_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">a_ptr</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_am</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"n\">stride_am</span> <span class=\"o\">+</span> <span class=\"n\">offs_k</span> <span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span><span class=\"o\">*</span><span class=\"n\">stride_ak</span><span class=\"p\">)</span>\n+<span class=\"n\">b_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">b_ptr</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_k</span> <span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"n\">stride_bk</span> <span class=\"o\">+</span> <span class=\"n\">offs_bn</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span><span class=\"o\">*</span><span class=\"n\">stride_bn</span><span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+</div></blockquote>\n+<p>And then updated in the inner loop as follows:</p>\n+<blockquote>\n+<div><div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">a_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_SIZE_K</span> <span class=\"o\">*</span> <span class=\"n\">stride_ak</span><span class=\"p\">;</span>\n+<span class=\"n\">b_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_SIZE_K</span> <span class=\"o\">*</span> <span class=\"n\">stride_bk</span><span class=\"p\">;</span>\n+</pre></div>\n+</div>\n+</div></blockquote>\n+</section>\n+<section id=\"l2-cache-optimizations\">\n+<h3>L2 Cache Optimizations<a class=\"headerlink\" href=\"#l2-cache-optimizations\" title=\"Permalink to this heading\">\u00b6</a></h3>\n+<p>As mentioned above, each program instance computes a <code class=\"code docutils literal notranslate\"><span class=\"pre\">[BLOCK_SIZE_M,</span> <span class=\"pre\">BLOCK_SIZE_N]</span></code>\n+block of <code class=\"code docutils literal notranslate\"><span class=\"pre\">C</span></code>.\n+It is important to remember that the order in which these blocks are computed does\n+matter, since it affects the L2 cache hit rate of our program. and unfortunately, a\n+a simple row-major ordering</p>\n+<blockquote>\n+<div><div class=\"highlight-Python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">);</span>\n+<span class=\"n\">grid_m</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">M</span> <span class=\"o\">+</span> <span class=\"n\">BLOCK_SIZE_M</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">;</span>\n+<span class=\"n\">grid_n</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">N</span> <span class=\"o\">+</span> <span class=\"n\">BLOCK_SIZE_N</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">;</span>\n+<span class=\"n\">pid_m</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">/</span> <span class=\"n\">grid_n</span><span class=\"p\">;</span>\n+<span class=\"n\">pid_n</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">%</span> <span class=\"n\">grid_n</span><span class=\"p\">;</span>\n+</pre></div>\n+</div>\n+</div></blockquote>\n+<p>is just not going to cut it.</p>\n+<p>One possible solution is to launch blocks in an order that promotes data reuse.\n+This can be done by \u2018super-grouping\u2019 blocks in groups of <code class=\"code docutils literal notranslate\"><span class=\"pre\">GROUP_M</span></code> rows before\n+switching to the next column:</p>\n+<blockquote>\n+<div><div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"c1\"># Program ID</span>\n+<span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+<span class=\"c1\"># Number of program ids along the M axis</span>\n+<span class=\"n\">num_pid_m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">)</span>\n+<span class=\"c1\"># Number of programs ids along the N axis</span>\n+<span class=\"n\">num_pid_n</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)</span>\n+<span class=\"c1\"># Number of programs in group</span>\n+<span class=\"n\">num_pid_in_group</span> <span class=\"o\">=</span> <span class=\"n\">GROUP_SIZE_M</span> <span class=\"o\">*</span> <span class=\"n\">num_pid_n</span>\n+<span class=\"c1\"># Id of the group this program is in</span>\n+<span class=\"n\">group_id</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">//</span> <span class=\"n\">num_pid_in_group</span>\n+<span class=\"c1\"># Row-id of the first program in the group</span>\n+<span class=\"n\">first_pid_m</span> <span class=\"o\">=</span> <span class=\"n\">group_id</span> <span class=\"o\">*</span> <span class=\"n\">GROUP_SIZE_M</span>\n+<span class=\"c1\"># If `num_pid_m` isn&#39;t divisible by `GROUP_SIZE_M`, the last group is smaller</span>\n+<span class=\"n\">group_size_m</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">num_pid_m</span> <span class=\"o\">-</span> <span class=\"n\">first_pid_m</span><span class=\"p\">,</span> <span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">)</span>\n+<span class=\"c1\"># *Within groups*, programs are ordered in a column-major order</span>\n+<span class=\"c1\"># Row-id of the program in the *launch grid*</span>\n+<span class=\"n\">pid_m</span> <span class=\"o\">=</span> <span class=\"n\">first_pid_m</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">pid</span> <span class=\"o\">%</span> <span class=\"n\">group_size_m</span><span class=\"p\">)</span>\n+<span class=\"c1\"># Col-id of the program in the *launch grid*</span>\n+<span class=\"n\">pid_n</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">pid</span> <span class=\"o\">%</span> <span class=\"n\">num_pid_in_group</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"n\">group_size_m</span>\n+</pre></div>\n+</div>\n+</div></blockquote>\n+<p>For example, in the following matmul where each matrix is 9 blocks by 9 blocks,\n+we can see that if we compute the output in row-major ordering, we need to load 90\n+blocks into SRAM to compute the first 9 output blocks, but if we do it in grouped\n+ordering, we only need to load 54 blocks.</p>\n+<blockquote>\n+<div><img alt=\"../../_images/grouped_vs_row_major_ordering.png\" src=\"../../_images/grouped_vs_row_major_ordering.png\" />\n+</div></blockquote>\n+<p>In practice, this can improve the performance of our matrix multiplication kernel by\n+more than 10% on some hardware architecture (e.g., 220 to 245 TFLOPS on A100).</p>\n+</section>\n+</section>\n+<section id=\"final-result\">\n+<h2>Final Result<a class=\"headerlink\" href=\"#final-result\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n+\n+<span class=\"kn\">import</span> <span class=\"nn\">triton</span>\n+<span class=\"kn\">import</span> <span class=\"nn\">triton.language</span> <span class=\"k\">as</span> <span class=\"nn\">tl</span>\n+\n+\n+<span class=\"c1\"># `triton.jit`&#39;ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:</span>\n+<span class=\"c1\">#   - A list of `triton.Config` objects that define different configurations of</span>\n+<span class=\"c1\">#       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try</span>\n+<span class=\"c1\">#   - An auto-tuning *key* whose change in values will trigger evaluation of all the</span>\n+<span class=\"c1\">#       provided configs</span>\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">autotune</span><span class=\"p\">(</span>\n+    <span class=\"n\">configs</span><span class=\"o\">=</span><span class=\"p\">[</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">),</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n+    <span class=\"p\">],</span>\n+    <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;M&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;K&#39;</span><span class=\"p\">],</span>\n+<span class=\"p\">)</span>\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n+<span class=\"k\">def</span> <span class=\"nf\">matmul_kernel</span><span class=\"p\">(</span>\n+    <span class=\"c1\"># Pointers to matrices</span>\n+    <span class=\"n\">a_ptr</span><span class=\"p\">,</span> <span class=\"n\">b_ptr</span><span class=\"p\">,</span> <span class=\"n\">c_ptr</span><span class=\"p\">,</span>\n+    <span class=\"c1\"># Matrix dimensions</span>\n+    <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span>\n+    <span class=\"c1\"># The stride variables represent how much to increase the ptr by when moving by 1</span>\n+    <span class=\"c1\"># element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`</span>\n+    <span class=\"c1\"># by to get the element one row down (A has M rows).</span>\n+    <span class=\"n\">stride_am</span><span class=\"p\">,</span> <span class=\"n\">stride_ak</span><span class=\"p\">,</span>\n+    <span class=\"n\">stride_bk</span><span class=\"p\">,</span> <span class=\"n\">stride_bn</span><span class=\"p\">,</span>\n+    <span class=\"n\">stride_cm</span><span class=\"p\">,</span> <span class=\"n\">stride_cn</span><span class=\"p\">,</span>\n+    <span class=\"c1\"># Meta-parameters</span>\n+    <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+    <span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+    <span class=\"n\">ACTIVATION</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+<span class=\"p\">):</span>\n+<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Kernel for computing the matmul C = A x B.</span>\n+<span class=\"sd\">    A has shape (M, K), B has shape (K, N) and C has shape (M, N)</span>\n+<span class=\"sd\">    &quot;&quot;&quot;</span>\n+    <span class=\"c1\"># -----------------------------------------------------------</span>\n+    <span class=\"c1\"># Map program ids `pid` to the block of C it should compute.</span>\n+    <span class=\"c1\"># This is done in a grouped ordering to promote L2 data reuse.</span>\n+    <span class=\"c1\"># See above `L2 Cache Optimizations` section for details.</span>\n+    <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"n\">num_pid_m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">)</span>\n+    <span class=\"n\">num_pid_n</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)</span>\n+    <span class=\"n\">num_pid_in_group</span> <span class=\"o\">=</span> <span class=\"n\">GROUP_SIZE_M</span> <span class=\"o\">*</span> <span class=\"n\">num_pid_n</span>\n+    <span class=\"n\">group_id</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">//</span> <span class=\"n\">num_pid_in_group</span>\n+    <span class=\"n\">first_pid_m</span> <span class=\"o\">=</span> <span class=\"n\">group_id</span> <span class=\"o\">*</span> <span class=\"n\">GROUP_SIZE_M</span>\n+    <span class=\"n\">group_size_m</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">num_pid_m</span> <span class=\"o\">-</span> <span class=\"n\">first_pid_m</span><span class=\"p\">,</span> <span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">)</span>\n+    <span class=\"n\">pid_m</span> <span class=\"o\">=</span> <span class=\"n\">first_pid_m</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">pid</span> <span class=\"o\">%</span> <span class=\"n\">group_size_m</span><span class=\"p\">)</span>\n+    <span class=\"n\">pid_n</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">pid</span> <span class=\"o\">%</span> <span class=\"n\">num_pid_in_group</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"n\">group_size_m</span>\n+\n+    <span class=\"c1\"># ----------------------------------------------------------</span>\n+    <span class=\"c1\"># Create pointers for the first blocks of A and B.</span>\n+    <span class=\"c1\"># We will advance this pointer as we move in the K direction</span>\n+    <span class=\"c1\"># and accumulate</span>\n+    <span class=\"c1\"># `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers</span>\n+    <span class=\"c1\"># `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers</span>\n+    <span class=\"c1\"># See above `Pointer Arithmetics` section for details</span>\n+    <span class=\"n\">offs_am</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">pid_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">))</span> <span class=\"o\">%</span> <span class=\"n\">M</span>\n+    <span class=\"n\">offs_bn</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">pid_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_N</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">))</span> <span class=\"o\">%</span> <span class=\"n\">N</span>\n+    <span class=\"n\">offs_k</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">)</span>\n+    <span class=\"n\">a_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">a_ptr</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_am</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_am</span> <span class=\"o\">+</span> <span class=\"n\">offs_k</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_ak</span><span class=\"p\">)</span>\n+    <span class=\"n\">b_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">b_ptr</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_k</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_bk</span> <span class=\"o\">+</span> <span class=\"n\">offs_bn</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_bn</span><span class=\"p\">)</span>\n+\n+    <span class=\"c1\"># -----------------------------------------------------------</span>\n+    <span class=\"c1\"># Iterate to compute a block of the C matrix.</span>\n+    <span class=\"c1\"># We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block</span>\n+    <span class=\"c1\"># of fp32 values for higher accuracy.</span>\n+    <span class=\"c1\"># `accumulator` will be converted back to fp16 after the loop.</span>\n+    <span class=\"n\">accumulator</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">)):</span>\n+        <span class=\"c1\"># Load the next block of A and B, generate a mask by checking the K dimension.</span>\n+        <span class=\"c1\"># If it is out of bounds, set it to 0.</span>\n+        <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">a_ptrs</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">offs_k</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">&lt;</span> <span class=\"n\">K</span> <span class=\"o\">-</span> <span class=\"n\">k</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">)</span>\n+        <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">b_ptrs</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">offs_k</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&lt;</span> <span class=\"n\">K</span> <span class=\"o\">-</span> <span class=\"n\">k</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># We accumulate along the K dimension.</span>\n+        <span class=\"n\">accumulator</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># Advance the ptrs to the next K block.</span>\n+        <span class=\"n\">a_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_SIZE_K</span> <span class=\"o\">*</span> <span class=\"n\">stride_ak</span>\n+        <span class=\"n\">b_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_SIZE_K</span> <span class=\"o\">*</span> <span class=\"n\">stride_bk</span>\n+    <span class=\"c1\"># You can fuse arbitrary activation functions here</span>\n+    <span class=\"c1\"># while the accumulator is still in FP32!</span>\n+    <span class=\"k\">if</span> <span class=\"n\">ACTIVATION</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;leaky_relu&quot;</span><span class=\"p\">:</span>\n+        <span class=\"n\">accumulator</span> <span class=\"o\">=</span> <span class=\"n\">leaky_relu</span><span class=\"p\">(</span><span class=\"n\">accumulator</span><span class=\"p\">)</span>\n+    <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">accumulator</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n+\n+    <span class=\"c1\"># -----------------------------------------------------------</span>\n+    <span class=\"c1\"># Write back the block of the output matrix C with masks.</span>\n+    <span class=\"n\">offs_cm</span> <span class=\"o\">=</span> <span class=\"n\">pid_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">)</span>\n+    <span class=\"n\">offs_cn</span> <span class=\"o\">=</span> <span class=\"n\">pid_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_N</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)</span>\n+    <span class=\"n\">c_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">c_ptr</span> <span class=\"o\">+</span> <span class=\"n\">stride_cm</span> <span class=\"o\">*</span> <span class=\"n\">offs_cm</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">stride_cn</span> <span class=\"o\">*</span> <span class=\"n\">offs_cn</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n+    <span class=\"n\">c_mask</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">offs_cm</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&lt;</span> <span class=\"n\">M</span><span class=\"p\">)</span> <span class=\"o\">&amp;</span> <span class=\"p\">(</span><span class=\"n\">offs_cn</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">c_ptrs</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">c_mask</span><span class=\"p\">)</span>\n+\n+\n+<span class=\"c1\"># We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.</span>\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n+<span class=\"k\">def</span> <span class=\"nf\">leaky_relu</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n+    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n+    <span class=\"k\">return</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">&gt;=</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"mf\">0.01</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+<p>We can now create a convenience wrapper function that only takes two input tensors,\n+and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s2\">&quot;&quot;</span><span class=\"p\">):</span>\n+    <span class=\"c1\"># Check constraints.</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"s2\">&quot;Incompatible dimensions&quot;</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">is_contiguous</span><span class=\"p\">(),</span> <span class=\"s2\">&quot;Matrix A must be contiguous&quot;</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">is_contiguous</span><span class=\"p\">(),</span> <span class=\"s2\">&quot;Matrix B must be contiguous&quot;</span>\n+    <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">K</span> <span class=\"o\">=</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n+    <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n+    <span class=\"c1\"># Allocates output.</span>\n+    <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># 1D launch kernel where each block gets its own program.</span>\n+    <span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">META</span><span class=\"p\">:</span> <span class=\"p\">(</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">META</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">META</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">]),</span>\n+    <span class=\"p\">)</span>\n+    <span class=\"n\">matmul_kernel</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span>\n+        <span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">,</span>\n+        <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span>\n+        <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n+        <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n+        <span class=\"n\">c</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">c</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n+        <span class=\"n\">ACTIVATION</span><span class=\"o\">=</span><span class=\"n\">activation</span>\n+    <span class=\"p\">)</span>\n+    <span class=\"k\">return</span> <span class=\"n\">c</span>\n+</pre></div>\n+</div>\n+</section>\n+<section id=\"unit-test\">\n+<h2>Unit Test<a class=\"headerlink\" href=\"#unit-test\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS).</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+<span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n+<span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n+<span class=\"n\">triton_output</span> <span class=\"o\">=</span> <span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n+<span class=\"n\">torch_output</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n+<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;triton_output=</span><span class=\"si\">{</span><span class=\"n\">triton_output</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n+<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;torch_output=</span><span class=\"si\">{</span><span class=\"n\">torch_output</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n+<span class=\"k\">if</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">triton_output</span><span class=\"p\">,</span> <span class=\"n\">torch_output</span><span class=\"p\">,</span> <span class=\"n\">atol</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">,</span> <span class=\"n\">rtol</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">):</span>\n+    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;\u2705 Triton and Torch match&quot;</span><span class=\"p\">)</span>\n+<span class=\"k\">else</span><span class=\"p\">:</span>\n+    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;\u274c Triton and Torch differ&quot;</span><span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+<div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>triton_output=tensor([[-10.9531,  -4.7109,  15.6953,  ..., -28.4062,   4.3320, -26.4219],\n+        [ 26.8438,  10.0469,  -5.4297,  ..., -11.2969,  -8.5312,  30.7500],\n+        [-13.2578,  15.8516,  18.0781,  ..., -21.7656,  -8.6406,  10.2031],\n+        ...,\n+        [ 40.2812,  18.6094, -25.6094,  ...,  -2.7598,  -3.2441,  41.0000],\n+        [ -6.1211, -16.8281,   4.4844,  ..., -21.0312,  24.7031,  15.0234],\n+        [-17.0938, -19.0000,  -0.3831,  ...,  21.5469, -30.2344, -13.2188]],\n+       device=&#39;cuda:0&#39;, dtype=torch.float16)\n+torch_output=tensor([[-10.9531,  -4.7109,  15.6953,  ..., -28.4062,   4.3320, -26.4219],\n+        [ 26.8438,  10.0469,  -5.4297,  ..., -11.2969,  -8.5312,  30.7500],\n+        [-13.2578,  15.8516,  18.0781,  ..., -21.7656,  -8.6406,  10.2031],\n+        ...,\n+        [ 40.2812,  18.6094, -25.6094,  ...,  -2.7598,  -3.2441,  41.0000],\n+        [ -6.1211, -16.8281,   4.4844,  ..., -21.0312,  24.7031,  15.0234],\n+        [-17.0938, -19.0000,  -0.3831,  ...,  21.5469, -30.2344, -13.2188]],\n+       device=&#39;cuda:0&#39;, dtype=torch.float16)\n+\u2705 Triton and Torch match\n+</pre></div>\n+</div>\n+</section>\n+<section id=\"benchmark\">\n+<h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<section id=\"square-matrix-performance\">\n+<h3>Square Matrix Performance<a class=\"headerlink\" href=\"#square-matrix-performance\" title=\"Permalink to this heading\">\u00b6</a></h3>\n+<p>We can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices,\n+but feel free to arrange this script as you wish to benchmark any other matrix shape.</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">perf_report</span><span class=\"p\">(</span>\n+    <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">Benchmark</span><span class=\"p\">(</span>\n+        <span class=\"n\">x_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;M&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;K&#39;</span><span class=\"p\">],</span>  <span class=\"c1\"># Argument names to use as an x-axis for the plot</span>\n+        <span class=\"n\">x_vals</span><span class=\"o\">=</span><span class=\"p\">[</span>\n+            <span class=\"mi\">128</span> <span class=\"o\">*</span> <span class=\"n\">i</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">33</span><span class=\"p\">)</span>\n+        <span class=\"p\">],</span>  <span class=\"c1\"># Different possible values for `x_name`</span>\n+        <span class=\"n\">line_arg</span><span class=\"o\">=</span><span class=\"s1\">&#39;provider&#39;</span><span class=\"p\">,</span>  <span class=\"c1\"># Argument name whose value corresponds to a different line in the plot</span>\n+        <span class=\"c1\"># Possible values for `line_arg`</span>\n+        <span class=\"n\">line_vals</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;cublas&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">],</span>\n+        <span class=\"c1\"># Label name for the lines</span>\n+        <span class=\"n\">line_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">&quot;cuBLAS&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;Triton&quot;</span><span class=\"p\">],</span>\n+        <span class=\"c1\"># Line styles</span>\n+        <span class=\"n\">styles</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s1\">&#39;green&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;blue&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">)],</span>\n+        <span class=\"n\">ylabel</span><span class=\"o\">=</span><span class=\"s2\">&quot;TFLOPS&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># Label name for the y-axis</span>\n+        <span class=\"n\">plot_name</span><span class=\"o\">=</span><span class=\"s2\">&quot;matmul-performance&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># Name for the plot, used also as a file name for saving the plot.</span>\n+        <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{},</span>\n+    <span class=\"p\">)</span>\n+<span class=\"p\">)</span>\n+<span class=\"k\">def</span> <span class=\"nf\">benchmark</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">provider</span><span class=\"p\">):</span>\n+    <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n+    <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n+    <span class=\"n\">quantiles</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">]</span>\n+    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;cublas&#39;</span><span class=\"p\">:</span>\n+        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">),</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">)</span>\n+    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">:</span>\n+        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">),</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">)</span>\n+    <span class=\"n\">perf</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">ms</span><span class=\"p\">:</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">M</span> <span class=\"o\">*</span> <span class=\"n\">N</span> <span class=\"o\">*</span> <span class=\"n\">K</span> <span class=\"o\">*</span> <span class=\"mf\">1e-12</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">ms</span> <span class=\"o\">*</span> <span class=\"mf\">1e-3</span><span class=\"p\">)</span>\n+    <span class=\"k\">return</span> <span class=\"n\">perf</span><span class=\"p\">(</span><span class=\"n\">ms</span><span class=\"p\">),</span> <span class=\"n\">perf</span><span class=\"p\">(</span><span class=\"n\">max_ms</span><span class=\"p\">),</span> <span class=\"n\">perf</span><span class=\"p\">(</span><span class=\"n\">min_ms</span><span class=\"p\">)</span>\n+\n+\n+<span class=\"n\">benchmark</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">show_plots</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">print_data</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+<img src=\"../../_images/sphx_glr_03-matrix-multiplication_001.png\" srcset=\"../../_images/sphx_glr_03-matrix-multiplication_001.png\" alt=\"03 matrix multiplication\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>matmul-performance:\n+         M       N       K      cuBLAS      Triton\n+0    256.0   256.0   256.0    4.681143    4.096000\n+1    384.0   384.0   384.0   12.288000   12.288000\n+2    512.0   512.0   512.0   26.214401   23.831273\n+3    640.0   640.0   640.0   42.666665   39.384616\n+4    768.0   768.0   768.0   63.195428   58.982401\n+5    896.0   896.0   896.0   78.051553   82.642822\n+6   1024.0  1024.0  1024.0  110.376426   99.864382\n+7   1152.0  1152.0  1152.0  135.726544  129.825388\n+8   1280.0  1280.0  1280.0  163.840004  163.840004\n+9   1408.0  1408.0  1408.0  155.765024  132.970149\n+10  1536.0  1536.0  1536.0  181.484314  157.286398\n+11  1664.0  1664.0  1664.0  183.651271  179.978245\n+12  1792.0  1792.0  1792.0  172.914215  208.137481\n+13  1920.0  1920.0  1920.0  200.347822  168.585369\n+14  2048.0  2048.0  2048.0  220.752852  190.650180\n+15  2176.0  2176.0  2176.0  209.621326  207.460296\n+16  2304.0  2304.0  2304.0  225.357284  225.357284\n+17  2432.0  2432.0  2432.0  199.251522  200.674737\n+18  2560.0  2560.0  2560.0  224.438347  215.578957\n+19  2688.0  2688.0  2688.0  196.544332  198.602388\n+20  2816.0  2816.0  2816.0  210.696652  207.686706\n+21  2944.0  2944.0  2944.0  219.541994  223.479969\n+22  3072.0  3072.0  3072.0  207.410628  206.653671\n+23  3200.0  3200.0  3200.0  214.046818  216.216207\n+24  3328.0  3328.0  3328.0  203.941342  208.670419\n+25  3456.0  3456.0  3456.0  215.565692  216.724640\n+26  3584.0  3584.0  3584.0  218.772251  210.082692\n+27  3712.0  3712.0  3712.0  209.868376  213.455857\n+28  3840.0  3840.0  3840.0  208.271176  208.271176\n+29  3968.0  3968.0  3968.0  207.523702  216.354501\n+30  4096.0  4096.0  4096.0  219.668951  215.437756\n+</pre></div>\n+</div>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  40.563 seconds)</p>\n+<div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-03-matrix-multiplication-py\">\n+<div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d5fee5b55a64e47f1b5724ec39adf171/03-matrix-multiplication.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">03-matrix-multiplication.py</span></code></a></p>\n+</div>\n+<div class=\"sphx-glr-download sphx-glr-download-jupyter docutils container\">\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/b51b68bc1c6b1a5e509f67800b6235af/03-matrix-multiplication.ipynb\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Jupyter</span> <span class=\"pre\">notebook:</span> <span class=\"pre\">03-matrix-multiplication.ipynb</span></code></a></p>\n+</div>\n+</div>\n+<p class=\"sphx-glr-signature\"><a class=\"reference external\" href=\"https://sphinx-gallery.github.io\">Gallery generated by Sphinx-Gallery</a></p>\n+</section>\n+</section>\n+</section>\n+\n+\n+           </div>\n+          </div>\n+          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n+        <a href=\"02-fused-softmax.html\" class=\"btn btn-neutral float-left\" title=\"Fused Softmax\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"04-low-memory-dropout.html\" class=\"btn btn-neutral float-right\" title=\"Low-Memory Dropout\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+    </div>\n+\n+  <hr/>\n+\n+  <div role=\"contentinfo\">\n+    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n+  </div>\n+\n+  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n+    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n+    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n+   \n+\n+</footer>\n+        </div>\n+      </div>\n+    </section>\n+  </div>\n+  <script>\n+      jQuery(function () {\n+          SphinxRtdTheme.Navigation.enable(true);\n+      });\n+  </script> \n+\n+</body>\n+</html>\n\\ No newline at end of file"}, {"filename": "main/getting-started/tutorials/04-low-memory-dropout.html", "status": "added", "additions": 334, "deletions": 0, "changes": 334, "file_content_changes": "@@ -0,0 +1,334 @@\n+<!DOCTYPE html>\n+<html class=\"writer-html5\" lang=\"en\" >\n+<head>\n+  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n+\n+  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n+  <title>Low-Memory Dropout &mdash; Triton  documentation</title>\n+      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n+  <!--[if lt IE 9]>\n+    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n+  <![endif]-->\n+  \n+        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n+        <script src=\"../../_static/doctools.js\"></script>\n+        <script src=\"../../_static/sphinx_highlight.js\"></script>\n+        <script async=\"async\" src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\n+    <script src=\"../../_static/js/theme.js\"></script>\n+    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n+    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n+    <link rel=\"next\" title=\"Layer Normalization\" href=\"05-layer-norm.html\" />\n+    <link rel=\"prev\" title=\"Matrix Multiplication\" href=\"03-matrix-multiplication.html\" /> \n+</head>\n+\n+<body class=\"wy-body-for-nav\"> \n+  <div class=\"wy-grid-for-nav\">\n+    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n+      <div class=\"wy-side-scroll\">\n+        <div class=\"wy-side-nav-search\" >\n+\n+          \n+          \n+          <a href=\"../../index.html\" class=\"icon icon-home\">\n+            Triton\n+          </a>\n+<div role=\"search\">\n+  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n+    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n+    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n+    <input type=\"hidden\" name=\"area\" value=\"default\" />\n+  </form>\n+</div>\n+        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n+              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n+<ul class=\"current\">\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../installation.html\">Installation</a></li>\n+<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"index.html\">Tutorials</a><ul class=\"current\">\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"01-vector-add.html\">Vector Addition</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"02-fused-softmax.html\">Fused Softmax</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n+<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Low-Memory Dropout</a><ul>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#baseline\">Baseline</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#seeded-dropout\">Seeded dropout</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#exercises\">Exercises</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#references\">References</a></li>\n+</ul>\n+</li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"08-experimental-block-pointer.html\">Block Pointer (Experimental)</a></li>\n+</ul>\n+</li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n+</ul>\n+\n+        </div>\n+      </div>\n+    </nav>\n+\n+    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n+          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n+          <a href=\"../../index.html\">Triton</a>\n+      </nav>\n+\n+      <div class=\"wy-nav-content\">\n+        <div class=\"rst-content\">\n+          <div role=\"navigation\" aria-label=\"Page navigation\">\n+  <ul class=\"wy-breadcrumbs\">\n+      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n+          <li class=\"breadcrumb-item\"><a href=\"index.html\">Tutorials</a></li>\n+      <li class=\"breadcrumb-item active\">Low-Memory Dropout</li>\n+      <li class=\"wy-breadcrumbs-aside\">\n+            <a href=\"../../_sources/getting-started/tutorials/04-low-memory-dropout.rst.txt\" rel=\"nofollow\"> View page source</a>\n+      </li>\n+  </ul>\n+  <hr/>\n+</div>\n+          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n+           <div itemprop=\"articleBody\">\n+             \n+  <div class=\"sphx-glr-download-link-note admonition note\">\n+<p class=\"admonition-title\">Note</p>\n+<p><a class=\"reference internal\" href=\"#sphx-glr-download-getting-started-tutorials-04-low-memory-dropout-py\"><span class=\"std std-ref\">Go to the end</span></a>\n+to download the full example code</p>\n+</div>\n+<section class=\"sphx-glr-example-title\" id=\"low-memory-dropout\">\n+<span id=\"sphx-glr-getting-started-tutorials-04-low-memory-dropout-py\"></span><h1>Low-Memory Dropout<a class=\"headerlink\" href=\"#low-memory-dropout\" title=\"Permalink to this heading\">\u00b6</a></h1>\n+<p>In this tutorial, you will write a memory-efficient implementation of dropout whose state\n+will be composed of a single int32 seed. This differs from more traditional implementations of dropout,\n+whose state is generally composed of a bit mask tensor of the same shape as the input.</p>\n+<p>In doing so, you will learn about:</p>\n+<ul class=\"simple\">\n+<li><p>The limitations of naive implementations of Dropout with PyTorch.</p></li>\n+<li><p>Parallel pseudo-random number generation in Triton.</p></li>\n+</ul>\n+<section id=\"baseline\">\n+<h2>Baseline<a class=\"headerlink\" href=\"#baseline\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>The <em>dropout</em> operator was first introduced in <a class=\"reference internal\" href=\"#srivastava2014\" id=\"id1\"><span>[SRIVASTAVA2014]</span></a> as a way to improve the performance\n+of deep neural networks in low-data regime (i.e. regularization).</p>\n+<p>It takes a vector as input and produces a vector of the same shape as output. Each scalar in the\n+output has a probability <span class=\"math notranslate nohighlight\">\\(p\\)</span> of being changed to zero and otherwise it is copied from the input.\n+This forces the network to perform well even when only <span class=\"math notranslate nohighlight\">\\(1 - p\\)</span> scalars from the input are available.</p>\n+<p>At evaluation time we want to use the full power of the network so we set <span class=\"math notranslate nohighlight\">\\(p=0\\)</span>. Naively this would\n+increase the norm of the output (which can be a bad thing, e.g. it can lead to artificial decrease\n+in the output softmax temperature). To prevent this we multiply the output by <span class=\"math notranslate nohighlight\">\\(\\frac{1}{1 - p}\\)</span>, which\n+keeps the norm consistent regardless of the dropout probability.</p>\n+<p>Let\u2019s first take a look at the baseline implementation.</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">tabulate</span>\n+<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n+\n+<span class=\"kn\">import</span> <span class=\"nn\">triton</span>\n+<span class=\"kn\">import</span> <span class=\"nn\">triton.language</span> <span class=\"k\">as</span> <span class=\"nn\">tl</span>\n+\n+\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n+<span class=\"k\">def</span> <span class=\"nf\">_dropout</span><span class=\"p\">(</span>\n+    <span class=\"n\">x_ptr</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the input</span>\n+    <span class=\"n\">x_keep_ptr</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to a mask of 0s and 1s</span>\n+    <span class=\"n\">output_ptr</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the output</span>\n+    <span class=\"n\">n_elements</span><span class=\"p\">,</span>  <span class=\"c1\"># number of elements in the `x` tensor</span>\n+    <span class=\"n\">p</span><span class=\"p\">,</span>  <span class=\"c1\"># probability that an element of `x` is changed to zero</span>\n+    <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+<span class=\"p\">):</span>\n+    <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"n\">block_start</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE</span>\n+    <span class=\"n\">offsets</span> <span class=\"o\">=</span> <span class=\"n\">block_start</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">)</span>\n+    <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">offsets</span> <span class=\"o\">&lt;</span> <span class=\"n\">n_elements</span>\n+    <span class=\"c1\"># Load data</span>\n+    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">x_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n+    <span class=\"n\">x_keep</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">x_keep_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># The line below is the crucial part, described in the paragraph above!</span>\n+    <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">x_keep</span><span class=\"p\">,</span> <span class=\"n\">x</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">p</span><span class=\"p\">),</span> <span class=\"mf\">0.0</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># Write-back output</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">output_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n+\n+\n+<span class=\"k\">def</span> <span class=\"nf\">dropout</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">x_keep</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"p\">):</span>\n+    <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">is_contiguous</span><span class=\"p\">()</span>\n+    <span class=\"n\">n_elements</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">numel</span><span class=\"p\">()</span>\n+    <span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">meta</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">meta</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">]),)</span>\n+    <span class=\"n\">_dropout</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">x_keep</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"o\">=</span><span class=\"mi\">1024</span><span class=\"p\">)</span>\n+    <span class=\"k\">return</span> <span class=\"n\">output</span>\n+\n+\n+<span class=\"c1\"># Input tensor</span>\n+<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,))</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"p\">()</span>\n+<span class=\"c1\"># Dropout mask</span>\n+<span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>\n+<span class=\"n\">x_keep</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,))</span> <span class=\"o\">&gt;</span> <span class=\"n\">p</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"p\">()</span>\n+<span class=\"c1\">#</span>\n+<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">dropout</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">x_keep</span><span class=\"o\">=</span><span class=\"n\">x_keep</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"o\">=</span><span class=\"n\">p</span><span class=\"p\">)</span>\n+<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">tabulate</span><span class=\"o\">.</span><span class=\"n\">tabulate</span><span class=\"p\">([</span>\n+    <span class=\"p\">[</span><span class=\"s2\">&quot;input&quot;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">(),</span>\n+    <span class=\"p\">[</span><span class=\"s2\">&quot;keep mask&quot;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">x_keep</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">(),</span>\n+    <span class=\"p\">[</span><span class=\"s2\">&quot;output&quot;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">()</span>\n+<span class=\"p\">]))</span>\n+</pre></div>\n+</div>\n+<div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>---------  -------  ---------  --------  --------  --------  --------  --------  --------  ---------  ---------\n+input      1.541    -0.293429  -2.17879  0.568431  -1.08452  -1.3986   0.403347  0.838026  -0.719258  -0.403344\n+keep mask  1         1          0        1          0         1        1         0          0          0\n+output     3.08199  -0.586858   0        1.13686    0        -2.79719  0.806694  0          0          0\n+---------  -------  ---------  --------  --------  --------  --------  --------  --------  ---------  ---------\n+</pre></div>\n+</div>\n+</section>\n+<section id=\"seeded-dropout\">\n+<h2>Seeded dropout<a class=\"headerlink\" href=\"#seeded-dropout\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>The above implementation of dropout works fine, but it can be a bit awkward to deal with. Firstly\n+we need to store the dropout mask for backpropagation. Secondly, dropout state management can get\n+very tricky when using recompute/checkpointing (e.g. see all the notes about <cite>preserve_rng_state</cite> in\n+<a class=\"reference external\" href=\"https://pytorch.org/docs/1.9.0/checkpoint.html\">https://pytorch.org/docs/1.9.0/checkpoint.html</a>). In this tutorial we\u2019ll describe an alternative implementation\n+that (1) has a smaller memory footprint; (2) requires less data movement; and (3) simplifies the management\n+of persisting randomness across multiple invocations of the kernel.</p>\n+<p>Pseudo-random number generation in Triton is simple! In this tutorial we will use the\n+<code class=\"code docutils literal notranslate\"><span class=\"pre\">triton.language.rand</span></code> function which generates a block of uniformly distributed <code class=\"code docutils literal notranslate\"><span class=\"pre\">float32</span></code>\n+values in [0, 1), given a seed and a block of <code class=\"code docutils literal notranslate\"><span class=\"pre\">int32</span></code> offsets. But if you need it, Triton also provides\n+other <a class=\"reference internal\" href=\"../../python-api/triton.language.html#random-number-generation\"><span class=\"std std-ref\">random number generation strategies</span></a>.</p>\n+<div class=\"admonition note\">\n+<p class=\"admonition-title\">Note</p>\n+<p>Triton\u2019s implementation of PRNG is based on the Philox algorithm (described on <a class=\"reference internal\" href=\"#salmon2011\" id=\"id2\"><span>[SALMON2011]</span></a>).</p>\n+</div>\n+<p>Let\u2019s put it all together.</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n+<span class=\"k\">def</span> <span class=\"nf\">_seeded_dropout</span><span class=\"p\">(</span>\n+    <span class=\"n\">x_ptr</span><span class=\"p\">,</span>\n+    <span class=\"n\">output_ptr</span><span class=\"p\">,</span>\n+    <span class=\"n\">n_elements</span><span class=\"p\">,</span>\n+    <span class=\"n\">p</span><span class=\"p\">,</span>\n+    <span class=\"n\">seed</span><span class=\"p\">,</span>\n+    <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+<span class=\"p\">):</span>\n+    <span class=\"c1\"># compute memory offsets of elements handled by this instance</span>\n+    <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"n\">block_start</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE</span>\n+    <span class=\"n\">offsets</span> <span class=\"o\">=</span> <span class=\"n\">block_start</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># load data from x</span>\n+    <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">offsets</span> <span class=\"o\">&lt;</span> <span class=\"n\">n_elements</span>\n+    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">x_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># randomly prune it</span>\n+    <span class=\"n\">random</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">seed</span><span class=\"p\">,</span> <span class=\"n\">offsets</span><span class=\"p\">)</span>\n+    <span class=\"n\">x_keep</span> <span class=\"o\">=</span> <span class=\"n\">random</span> <span class=\"o\">&gt;</span> <span class=\"n\">p</span>\n+    <span class=\"c1\"># write-back</span>\n+    <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">x_keep</span><span class=\"p\">,</span> <span class=\"n\">x</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">p</span><span class=\"p\">),</span> <span class=\"mf\">0.0</span><span class=\"p\">)</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">output_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n+\n+\n+<span class=\"k\">def</span> <span class=\"nf\">seeded_dropout</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"p\">):</span>\n+    <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">is_contiguous</span><span class=\"p\">()</span>\n+    <span class=\"n\">n_elements</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">numel</span><span class=\"p\">()</span>\n+    <span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">meta</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">meta</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">]),)</span>\n+    <span class=\"n\">_seeded_dropout</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"o\">=</span><span class=\"mi\">1024</span><span class=\"p\">)</span>\n+    <span class=\"k\">return</span> <span class=\"n\">output</span>\n+\n+\n+<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,))</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"p\">()</span>\n+<span class=\"c1\"># Compare this to the baseline - dropout mask is never instantiated!</span>\n+<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">seeded_dropout</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"mi\">123</span><span class=\"p\">)</span>\n+<span class=\"n\">output2</span> <span class=\"o\">=</span> <span class=\"n\">seeded_dropout</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"mi\">123</span><span class=\"p\">)</span>\n+<span class=\"n\">output3</span> <span class=\"o\">=</span> <span class=\"n\">seeded_dropout</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"mi\">512</span><span class=\"p\">)</span>\n+\n+<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">tabulate</span><span class=\"o\">.</span><span class=\"n\">tabulate</span><span class=\"p\">([</span>\n+    <span class=\"p\">[</span><span class=\"s2\">&quot;input&quot;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">(),</span>\n+    <span class=\"p\">[</span><span class=\"s2\">&quot;output (seed = 123)&quot;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">(),</span>\n+    <span class=\"p\">[</span><span class=\"s2\">&quot;output (seed = 123)&quot;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">output2</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">(),</span>\n+    <span class=\"p\">[</span><span class=\"s2\">&quot;output (seed = 512)&quot;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">output3</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">()</span>\n+<span class=\"p\">]))</span>\n+</pre></div>\n+</div>\n+<div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>-------------------  ---------  --------  --------  -------  --------  --------  ---------  ---------  ---------  ---------\n+input                -0.952835  0.371721  0.408716  1.42142  0.149397  -0.67086  -0.214186  -0.431969  -0.707878  -0.106434\n+output (seed = 123)   0         0.743443  0         0        0         -1.34172   0          0         -1.41576   -0.212868\n+output (seed = 123)   0         0.743443  0         0        0         -1.34172   0          0         -1.41576   -0.212868\n+output (seed = 512)   0         0         0.817432  2.84284  0         -1.34172  -0.428372   0          0          0\n+-------------------  ---------  --------  --------  -------  --------  --------  ---------  ---------  ---------  ---------\n+</pre></div>\n+</div>\n+<p>Et Voil\u00e0! We have a triton kernel that applies the same dropout mask provided the seed is the same!\n+If you\u2019d like explore further applications of pseudorandomness in GPU programming, we encourage you\n+to explore the <cite>triton/language/random</cite> folder!</p>\n+</section>\n+<section id=\"exercises\">\n+<h2>Exercises<a class=\"headerlink\" href=\"#exercises\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<ol class=\"arabic simple\">\n+<li><p>Extend the kernel to operate over a matrix and use a vector of seeds - one per row.</p></li>\n+<li><p>Add support for striding.</p></li>\n+<li><p>(challenge) Implement a kernel for sparse Johnson-Lindenstrauss transform which generates the projection matrix one the fly each time using a seed.</p></li>\n+</ol>\n+</section>\n+<section id=\"references\">\n+<h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<div role=\"list\" class=\"citation-list\">\n+<div class=\"citation\" id=\"salmon2011\" role=\"doc-biblioentry\">\n+<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id2\">SALMON2011</a><span class=\"fn-bracket\">]</span></span>\n+<p>John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw, \u201cParallel Random Numbers: As Easy as 1, 2, 3\u201d, 2011</p>\n+</div>\n+<div class=\"citation\" id=\"srivastava2014\" role=\"doc-biblioentry\">\n+<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id1\">SRIVASTAVA2014</a><span class=\"fn-bracket\">]</span></span>\n+<p>Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \u201cDropout: A Simple Way to Prevent Neural Networks from Overfitting\u201d, JMLR 2014</p>\n+</div>\n+</div>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.642 seconds)</p>\n+<div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-04-low-memory-dropout-py\">\n+<div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/c9aed78977a4c05741d675a38dde3d7d/04-low-memory-dropout.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">04-low-memory-dropout.py</span></code></a></p>\n+</div>\n+<div class=\"sphx-glr-download sphx-glr-download-jupyter docutils container\">\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/bc847dec325798bdc436c4ef5ac8b78a/04-low-memory-dropout.ipynb\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Jupyter</span> <span class=\"pre\">notebook:</span> <span class=\"pre\">04-low-memory-dropout.ipynb</span></code></a></p>\n+</div>\n+</div>\n+<p class=\"sphx-glr-signature\"><a class=\"reference external\" href=\"https://sphinx-gallery.github.io\">Gallery generated by Sphinx-Gallery</a></p>\n+</section>\n+</section>\n+\n+\n+           </div>\n+          </div>\n+          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n+        <a href=\"03-matrix-multiplication.html\" class=\"btn btn-neutral float-left\" title=\"Matrix Multiplication\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"05-layer-norm.html\" class=\"btn btn-neutral float-right\" title=\"Layer Normalization\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+    </div>\n+\n+  <hr/>\n+\n+  <div role=\"contentinfo\">\n+    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n+  </div>\n+\n+  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n+    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n+    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n+   \n+\n+</footer>\n+        </div>\n+      </div>\n+    </section>\n+  </div>\n+  <script>\n+      jQuery(function () {\n+          SphinxRtdTheme.Navigation.enable(true);\n+      });\n+  </script> \n+\n+</body>\n+</html>\n\\ No newline at end of file"}, {"filename": "main/getting-started/tutorials/05-layer-norm.html", "status": "added", "additions": 550, "deletions": 0, "changes": 550, "file_content_changes": "@@ -0,0 +1,550 @@\n+<!DOCTYPE html>\n+<html class=\"writer-html5\" lang=\"en\" >\n+<head>\n+  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n+\n+  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n+  <title>Layer Normalization &mdash; Triton  documentation</title>\n+      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n+  <!--[if lt IE 9]>\n+    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n+  <![endif]-->\n+  \n+        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n+        <script src=\"../../_static/doctools.js\"></script>\n+        <script src=\"../../_static/sphinx_highlight.js\"></script>\n+        <script async=\"async\" src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\n+    <script src=\"../../_static/js/theme.js\"></script>\n+    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n+    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n+    <link rel=\"next\" title=\"Fused Attention\" href=\"06-fused-attention.html\" />\n+    <link rel=\"prev\" title=\"Low-Memory Dropout\" href=\"04-low-memory-dropout.html\" /> \n+</head>\n+\n+<body class=\"wy-body-for-nav\"> \n+  <div class=\"wy-grid-for-nav\">\n+    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n+      <div class=\"wy-side-scroll\">\n+        <div class=\"wy-side-nav-search\" >\n+\n+          \n+          \n+          <a href=\"../../index.html\" class=\"icon icon-home\">\n+            Triton\n+          </a>\n+<div role=\"search\">\n+  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n+    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n+    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n+    <input type=\"hidden\" name=\"area\" value=\"default\" />\n+  </form>\n+</div>\n+        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n+              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n+<ul class=\"current\">\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../installation.html\">Installation</a></li>\n+<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"index.html\">Tutorials</a><ul class=\"current\">\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"01-vector-add.html\">Vector Addition</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"02-fused-softmax.html\">Fused Softmax</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n+<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Layer Normalization</a><ul>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#motivations\">Motivations</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#backward-pass\">Backward pass</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#benchmark\">Benchmark</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#references\">References</a></li>\n+</ul>\n+</li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"08-experimental-block-pointer.html\">Block Pointer (Experimental)</a></li>\n+</ul>\n+</li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n+</ul>\n+\n+        </div>\n+      </div>\n+    </nav>\n+\n+    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n+          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n+          <a href=\"../../index.html\">Triton</a>\n+      </nav>\n+\n+      <div class=\"wy-nav-content\">\n+        <div class=\"rst-content\">\n+          <div role=\"navigation\" aria-label=\"Page navigation\">\n+  <ul class=\"wy-breadcrumbs\">\n+      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n+          <li class=\"breadcrumb-item\"><a href=\"index.html\">Tutorials</a></li>\n+      <li class=\"breadcrumb-item active\">Layer Normalization</li>\n+      <li class=\"wy-breadcrumbs-aside\">\n+            <a href=\"../../_sources/getting-started/tutorials/05-layer-norm.rst.txt\" rel=\"nofollow\"> View page source</a>\n+      </li>\n+  </ul>\n+  <hr/>\n+</div>\n+          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n+           <div itemprop=\"articleBody\">\n+             \n+  <div class=\"sphx-glr-download-link-note admonition note\">\n+<p class=\"admonition-title\">Note</p>\n+<p><a class=\"reference internal\" href=\"#sphx-glr-download-getting-started-tutorials-05-layer-norm-py\"><span class=\"std std-ref\">Go to the end</span></a>\n+to download the full example code</p>\n+</div>\n+<section class=\"sphx-glr-example-title\" id=\"layer-normalization\">\n+<span id=\"sphx-glr-getting-started-tutorials-05-layer-norm-py\"></span><h1>Layer Normalization<a class=\"headerlink\" href=\"#layer-normalization\" title=\"Permalink to this heading\">\u00b6</a></h1>\n+<p>In this tutorial, you will write a high-performance layer normalization\n+kernel that runs faster than the PyTorch implementation.</p>\n+<p>In doing so, you will learn about:</p>\n+<ul class=\"simple\">\n+<li><p>Implementing backward pass in Triton.</p></li>\n+<li><p>Implementing parallel reduction in Triton.</p></li>\n+</ul>\n+<section id=\"motivations\">\n+<h2>Motivations<a class=\"headerlink\" href=\"#motivations\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>The <em>LayerNorm</em> operator was first introduced in <a class=\"reference internal\" href=\"#ba2016\" id=\"id1\"><span>[BA2016]</span></a> as a way to improve the performance\n+of sequential models (e.g., Transformers) or neural networks with small batch size.\n+It takes a vector <span class=\"math notranslate nohighlight\">\\(x\\)</span> as input and produces a vector <span class=\"math notranslate nohighlight\">\\(y\\)</span> of the same shape as output.\n+The normalization is performed by subtracting the mean and dividing by the standard deviation of <span class=\"math notranslate nohighlight\">\\(x\\)</span>.\n+After the normalization, a learnable linear transformation with weights <span class=\"math notranslate nohighlight\">\\(w\\)</span> and biases <span class=\"math notranslate nohighlight\">\\(b\\)</span> is applied.\n+The forward pass can be expressed as follows:</p>\n+<div class=\"math notranslate nohighlight\">\n+\\[y = \\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} } * w + b\\]</div>\n+<p>where <span class=\"math notranslate nohighlight\">\\(\\epsilon\\)</span> is a small constant added to the denominator for numerical stability.\n+Let\u2019s first take a look at the forward pass implementation.</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n+\n+<span class=\"kn\">import</span> <span class=\"nn\">triton</span>\n+<span class=\"kn\">import</span> <span class=\"nn\">triton.language</span> <span class=\"k\">as</span> <span class=\"nn\">tl</span>\n+\n+<span class=\"k\">try</span><span class=\"p\">:</span>\n+    <span class=\"c1\"># This is https://github.com/NVIDIA/apex, NOT the apex on PyPi, so it</span>\n+    <span class=\"c1\"># should not be added to extras_require in setup.py.</span>\n+    <span class=\"kn\">import</span> <span class=\"nn\">apex</span>\n+    <span class=\"n\">HAS_APEX</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>\n+<span class=\"k\">except</span> <span class=\"ne\">ModuleNotFoundError</span><span class=\"p\">:</span>\n+    <span class=\"n\">HAS_APEX</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n+\n+\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n+<span class=\"k\">def</span> <span class=\"nf\">_layer_norm_fwd_fused</span><span class=\"p\">(</span>\n+    <span class=\"n\">X</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the input</span>\n+    <span class=\"n\">Y</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the output</span>\n+    <span class=\"n\">W</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the weights</span>\n+    <span class=\"n\">B</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the biases</span>\n+    <span class=\"n\">Mean</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the mean</span>\n+    <span class=\"n\">Rstd</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the 1/std</span>\n+    <span class=\"n\">stride</span><span class=\"p\">,</span>  <span class=\"c1\"># how much to increase the pointer when moving by 1 row</span>\n+    <span class=\"n\">N</span><span class=\"p\">,</span>  <span class=\"c1\"># number of columns in X</span>\n+    <span class=\"n\">eps</span><span class=\"p\">,</span>  <span class=\"c1\"># epsilon to avoid division by zero</span>\n+    <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+<span class=\"p\">):</span>\n+    <span class=\"c1\"># Map the program id to the row of X and Y it should compute.</span>\n+    <span class=\"n\">row</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"n\">Y</span> <span class=\"o\">+=</span> <span class=\"n\">row</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n+    <span class=\"n\">X</span> <span class=\"o\">+=</span> <span class=\"n\">row</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n+    <span class=\"c1\"># Compute mean</span>\n+    <span class=\"n\">mean</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n+    <span class=\"n\">_mean</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_SIZE</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"k\">for</span> <span class=\"n\">off</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">):</span>\n+        <span class=\"n\">cols</span> <span class=\"o\">=</span> <span class=\"n\">off</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">)</span>\n+        <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">X</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">cols</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+        <span class=\"n\">_mean</span> <span class=\"o\">+=</span> <span class=\"n\">a</span>\n+    <span class=\"n\">mean</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">_mean</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">N</span>\n+    <span class=\"c1\"># Compute variance</span>\n+    <span class=\"n\">_var</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_SIZE</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"k\">for</span> <span class=\"n\">off</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">):</span>\n+        <span class=\"n\">cols</span> <span class=\"o\">=</span> <span class=\"n\">off</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">)</span>\n+        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">X</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">cols</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">cols</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">mean</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">)</span>\n+        <span class=\"n\">_var</span> <span class=\"o\">+=</span> <span class=\"n\">x</span> <span class=\"o\">*</span> <span class=\"n\">x</span>\n+    <span class=\"n\">var</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">_var</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">N</span>\n+    <span class=\"n\">rstd</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"o\">/</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">var</span> <span class=\"o\">+</span> <span class=\"n\">eps</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># Write mean / rstd</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">Mean</span> <span class=\"o\">+</span> <span class=\"n\">row</span><span class=\"p\">,</span> <span class=\"n\">mean</span><span class=\"p\">)</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">Rstd</span> <span class=\"o\">+</span> <span class=\"n\">row</span><span class=\"p\">,</span> <span class=\"n\">rstd</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># Normalize and apply linear transformation</span>\n+    <span class=\"k\">for</span> <span class=\"n\">off</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">):</span>\n+        <span class=\"n\">cols</span> <span class=\"o\">=</span> <span class=\"n\">off</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">)</span>\n+        <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">cols</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span>\n+        <span class=\"n\">w</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">W</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n+        <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">B</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n+        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">X</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+        <span class=\"n\">x_hat</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">mean</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">rstd</span>\n+        <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">x_hat</span> <span class=\"o\">*</span> <span class=\"n\">w</span> <span class=\"o\">+</span> <span class=\"n\">b</span>\n+        <span class=\"c1\"># Write output</span>\n+        <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">Y</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+</section>\n+<section id=\"backward-pass\">\n+<h2>Backward pass<a class=\"headerlink\" href=\"#backward-pass\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>The backward pass for the layer normalization operator is a bit more involved than the forward pass.\n+Let <span class=\"math notranslate nohighlight\">\\(\\hat{x}\\)</span> be the normalized inputs <span class=\"math notranslate nohighlight\">\\(\\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} }\\)</span> before the linear transformation,\n+the Vector-Jacobian Products (VJP) <span class=\"math notranslate nohighlight\">\\(\\nabla_{x}\\)</span> of <span class=\"math notranslate nohighlight\">\\(x\\)</span> are given by:</p>\n+<div class=\"math notranslate nohighlight\">\n+\\[\\nabla_{x} = \\frac{1}{\\sigma}\\Big( \\nabla_{y} \\odot w - \\underbrace{ \\big( \\frac{1}{N} \\hat{x} \\cdot (\\nabla_{y} \\odot w) \\big) }_{c_1} \\odot \\hat{x} - \\underbrace{ \\frac{1}{N} \\nabla_{y} \\cdot w }_{c_2} \\Big)\\]</div>\n+<p>where <span class=\"math notranslate nohighlight\">\\(\\odot\\)</span> denotes the element-wise multiplication, <span class=\"math notranslate nohighlight\">\\(\\cdot\\)</span> denotes the dot product, and <span class=\"math notranslate nohighlight\">\\(\\sigma\\)</span> is the standard deviation.\n+<span class=\"math notranslate nohighlight\">\\(c_1\\)</span> and <span class=\"math notranslate nohighlight\">\\(c_2\\)</span> are intermediate constants that improve the readability of the following implementation.</p>\n+<p>For the weights <span class=\"math notranslate nohighlight\">\\(w\\)</span> and biases <span class=\"math notranslate nohighlight\">\\(b\\)</span>, the VJPs <span class=\"math notranslate nohighlight\">\\(\\nabla_{w}\\)</span> and <span class=\"math notranslate nohighlight\">\\(\\nabla_{b}\\)</span> are more straightforward:</p>\n+<div class=\"math notranslate nohighlight\">\n+\\[\\nabla_{w} = \\nabla_{y} \\odot \\hat{x} \\quad \\text{and} \\quad \\nabla_{b} = \\nabla_{y}\\]</div>\n+<p>Since the same weights <span class=\"math notranslate nohighlight\">\\(w\\)</span> and biases <span class=\"math notranslate nohighlight\">\\(b\\)</span> are used for all rows in the same batch, their gradients need to sum up.\n+To perform this step efficiently, we use a parallel reduction strategy: each kernel instance accumulates\n+partial <span class=\"math notranslate nohighlight\">\\(\\nabla_{w}\\)</span> and <span class=\"math notranslate nohighlight\">\\(\\nabla_{b}\\)</span> across certain rows into one of <span class=\"math notranslate nohighlight\">\\(\\text{GROUP_SIZE_M}\\)</span> independent buffers.\n+These buffers stay in the L2 cache and then are further reduced by another function to compute the actual <span class=\"math notranslate nohighlight\">\\(\\nabla_{w}\\)</span> and <span class=\"math notranslate nohighlight\">\\(\\nabla_{b}\\)</span>.</p>\n+<p>Let the number of input rows <span class=\"math notranslate nohighlight\">\\(M = 4\\)</span> and <span class=\"math notranslate nohighlight\">\\(\\text{GROUP_SIZE_M} = 2\\)</span>,\n+here\u2019s a diagram of the parallel reduction strategy for <span class=\"math notranslate nohighlight\">\\(\\nabla_{w}\\)</span> (<span class=\"math notranslate nohighlight\">\\(\\nabla_{b}\\)</span> is omitted for brevity):</p>\n+<blockquote>\n+<div><img alt=\"../../_images/parallel_reduction.png\" src=\"../../_images/parallel_reduction.png\" />\n+</div></blockquote>\n+<p>In Stage 1, the rows of X that have the same color share the same buffer and thus a lock is used to ensure that only one kernel instance writes to the buffer at a time.\n+In Stage 2, the buffers are further reduced to compute the final <span class=\"math notranslate nohighlight\">\\(\\nabla_{w}\\)</span> and <span class=\"math notranslate nohighlight\">\\(\\nabla_{b}\\)</span>.\n+In the following implementation, Stage 1 is implemented by the function <code class=\"code docutils literal notranslate\"><span class=\"pre\">_layer_norm_bwd_dx_fused</span></code> and Stage 2 is implemented by the function <code class=\"code docutils literal notranslate\"><span class=\"pre\">_layer_norm_bwd_dwdb</span></code>.</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n+<span class=\"k\">def</span> <span class=\"nf\">_layer_norm_bwd_dx_fused</span><span class=\"p\">(</span>\n+    <span class=\"n\">DX</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the input gradient</span>\n+    <span class=\"n\">DY</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the output gradient</span>\n+    <span class=\"n\">DW</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the partial sum of weights gradient</span>\n+    <span class=\"n\">DB</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the partial sum of biases gradient</span>\n+    <span class=\"n\">X</span><span class=\"p\">,</span>   <span class=\"c1\"># pointer to the input</span>\n+    <span class=\"n\">W</span><span class=\"p\">,</span>   <span class=\"c1\"># pointer to the weights</span>\n+    <span class=\"n\">B</span><span class=\"p\">,</span>   <span class=\"c1\"># pointer to the biases</span>\n+    <span class=\"n\">Mean</span><span class=\"p\">,</span>   <span class=\"c1\"># pointer to the mean</span>\n+    <span class=\"n\">Rstd</span><span class=\"p\">,</span>   <span class=\"c1\"># pointer to the 1/std</span>\n+    <span class=\"n\">Lock</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the lock</span>\n+    <span class=\"n\">stride</span><span class=\"p\">,</span>  <span class=\"c1\"># how much to increase the pointer when moving by 1 row</span>\n+    <span class=\"n\">N</span><span class=\"p\">,</span>  <span class=\"c1\"># number of columns in X</span>\n+    <span class=\"n\">eps</span><span class=\"p\">,</span>  <span class=\"c1\"># epsilon to avoid division by zero</span>\n+    <span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+    <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span>\n+<span class=\"p\">):</span>\n+    <span class=\"c1\"># Map the program id to the elements of X, DX, and DY it should compute.</span>\n+    <span class=\"n\">row</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"n\">cols</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)</span>\n+    <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">cols</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span>\n+    <span class=\"n\">X</span> <span class=\"o\">+=</span> <span class=\"n\">row</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n+    <span class=\"n\">DY</span> <span class=\"o\">+=</span> <span class=\"n\">row</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n+    <span class=\"n\">DX</span> <span class=\"o\">+=</span> <span class=\"n\">row</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n+    <span class=\"c1\"># Offset locks and weights/biases gradient pointer for parallel reduction</span>\n+    <span class=\"n\">lock_id</span> <span class=\"o\">=</span> <span class=\"n\">row</span> <span class=\"o\">%</span> <span class=\"n\">GROUP_SIZE_M</span>\n+    <span class=\"n\">Lock</span> <span class=\"o\">+=</span> <span class=\"n\">lock_id</span>\n+    <span class=\"n\">Count</span> <span class=\"o\">=</span> <span class=\"n\">Lock</span> <span class=\"o\">+</span> <span class=\"n\">GROUP_SIZE_M</span>\n+    <span class=\"n\">DW</span> <span class=\"o\">=</span> <span class=\"n\">DW</span> <span class=\"o\">+</span> <span class=\"n\">lock_id</span> <span class=\"o\">*</span> <span class=\"n\">N</span> <span class=\"o\">+</span> <span class=\"n\">cols</span>\n+    <span class=\"n\">DB</span> <span class=\"o\">=</span> <span class=\"n\">DB</span> <span class=\"o\">+</span> <span class=\"n\">lock_id</span> <span class=\"o\">*</span> <span class=\"n\">N</span> <span class=\"o\">+</span> <span class=\"n\">cols</span>\n+    <span class=\"c1\"># Load data to SRAM</span>\n+    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">X</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"n\">dy</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">DY</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"n\">w</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">W</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"n\">mean</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">Mean</span> <span class=\"o\">+</span> <span class=\"n\">row</span><span class=\"p\">)</span>\n+    <span class=\"n\">rstd</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">Rstd</span> <span class=\"o\">+</span> <span class=\"n\">row</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># Compute dx</span>\n+    <span class=\"n\">xhat</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">mean</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">rstd</span>\n+    <span class=\"n\">wdy</span> <span class=\"o\">=</span> <span class=\"n\">w</span> <span class=\"o\">*</span> <span class=\"n\">dy</span>\n+    <span class=\"n\">xhat</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">xhat</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">)</span>\n+    <span class=\"n\">wdy</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">wdy</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">)</span>\n+    <span class=\"n\">c1</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">xhat</span> <span class=\"o\">*</span> <span class=\"n\">wdy</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">N</span>\n+    <span class=\"n\">c2</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">wdy</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">N</span>\n+    <span class=\"n\">dx</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">wdy</span> <span class=\"o\">-</span> <span class=\"p\">(</span><span class=\"n\">xhat</span> <span class=\"o\">*</span> <span class=\"n\">c1</span> <span class=\"o\">+</span> <span class=\"n\">c2</span><span class=\"p\">))</span> <span class=\"o\">*</span> <span class=\"n\">rstd</span>\n+    <span class=\"c1\"># Write dx</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">DX</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">dx</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># Accumulate partial sums for dw/db</span>\n+    <span class=\"n\">partial_dw</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">dy</span> <span class=\"o\">*</span> <span class=\"n\">xhat</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n+    <span class=\"n\">partial_db</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">dy</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n+    <span class=\"k\">while</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">atomic_cas</span><span class=\"p\">(</span><span class=\"n\">Lock</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n+        <span class=\"k\">pass</span>\n+    <span class=\"n\">count</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">Count</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># First store doesn&#39;t accumulate</span>\n+    <span class=\"k\">if</span> <span class=\"n\">count</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n+        <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">atomic_xchg</span><span class=\"p\">(</span><span class=\"n\">Count</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n+    <span class=\"k\">else</span><span class=\"p\">:</span>\n+        <span class=\"n\">partial_dw</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">DW</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n+        <span class=\"n\">partial_db</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">DB</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">DW</span><span class=\"p\">,</span> <span class=\"n\">partial_dw</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">DB</span><span class=\"p\">,</span> <span class=\"n\">partial_db</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># Release the lock</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">atomic_xchg</span><span class=\"p\">(</span><span class=\"n\">Lock</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n+\n+\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n+<span class=\"k\">def</span> <span class=\"nf\">_layer_norm_bwd_dwdb</span><span class=\"p\">(</span>\n+    <span class=\"n\">DW</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the partial sum of weights gradient</span>\n+    <span class=\"n\">DB</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the partial sum of biases gradient</span>\n+    <span class=\"n\">FINAL_DW</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the weights gradient</span>\n+    <span class=\"n\">FINAL_DB</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the biases gradient</span>\n+    <span class=\"n\">M</span><span class=\"p\">,</span>  <span class=\"c1\"># GROUP_SIZE_M</span>\n+    <span class=\"n\">N</span><span class=\"p\">,</span>  <span class=\"c1\"># number of columns</span>\n+    <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+    <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span>\n+<span class=\"p\">):</span>\n+    <span class=\"c1\"># Map the program id to the elements of DW and DB it should compute.</span>\n+    <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"n\">cols</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_N</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)</span>\n+    <span class=\"n\">dw</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># Iterate through the rows of DW and DB to sum the partial sums.</span>\n+    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">):</span>\n+        <span class=\"n\">rows</span> <span class=\"o\">=</span> <span class=\"n\">i</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">)</span>\n+        <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">rows</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&lt;</span> <span class=\"n\">M</span><span class=\"p\">)</span> <span class=\"o\">&amp;</span> <span class=\"p\">(</span><span class=\"n\">cols</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n+        <span class=\"n\">offs</span> <span class=\"o\">=</span> <span class=\"n\">rows</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">N</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n+        <span class=\"n\">dw</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">DW</span> <span class=\"o\">+</span> <span class=\"n\">offs</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">)</span>\n+        <span class=\"n\">db</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">DB</span> <span class=\"o\">+</span> <span class=\"n\">offs</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># Write the final sum to the output.</span>\n+    <span class=\"n\">sum_dw</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">dw</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"n\">sum_db</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">db</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">FINAL_DW</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">sum_dw</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">cols</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">FINAL_DB</span> <span class=\"o\">+</span> <span class=\"n\">cols</span><span class=\"p\">,</span> <span class=\"n\">sum_db</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">cols</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+</section>\n+<section id=\"benchmark\">\n+<h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>We can now compare the performance of our kernel against that of PyTorch.\n+Here we focus on inputs that have Less than 64KB per feature.\n+Specifically, one can set <code class=\"code docutils literal notranslate\"><span class=\"pre\">'mode':</span> <span class=\"pre\">'backward'</span></code> to benchmark the backward pass.</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"k\">class</span> <span class=\"nc\">LayerNorm</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">autograd</span><span class=\"o\">.</span><span class=\"n\">Function</span><span class=\"p\">):</span>\n+\n+    <span class=\"nd\">@staticmethod</span>\n+    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">normalized_shape</span><span class=\"p\">,</span> <span class=\"n\">weight</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"p\">):</span>\n+        <span class=\"c1\"># allocate output</span>\n+        <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># reshape input data into 2D tensor</span>\n+        <span class=\"n\">x_arg</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n+        <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"n\">x_arg</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n+        <span class=\"n\">mean</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n+        <span class=\"n\">rstd</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># Less than 64KB per feature: enqueue fused kernel</span>\n+        <span class=\"n\">MAX_FUSED_SIZE</span> <span class=\"o\">=</span> <span class=\"mi\">65536</span> <span class=\"o\">//</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">element_size</span><span class=\"p\">()</span>\n+        <span class=\"n\">BLOCK_SIZE</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">MAX_FUSED_SIZE</span><span class=\"p\">,</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">next_power_of_2</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">))</span>\n+        <span class=\"k\">if</span> <span class=\"n\">N</span> <span class=\"o\">&gt;</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">:</span>\n+            <span class=\"k\">raise</span> <span class=\"ne\">RuntimeError</span><span class=\"p\">(</span><span class=\"s2\">&quot;This layer norm doesn&#39;t support feature dim &gt;= 64KB.&quot;</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># heuristics for number of warps</span>\n+        <span class=\"n\">num_warps</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"nb\">max</span><span class=\"p\">(</span><span class=\"n\">BLOCK_SIZE</span> <span class=\"o\">//</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"mi\">8</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># enqueue kernel</span>\n+        <span class=\"n\">_layer_norm_fwd_fused</span><span class=\"p\">[(</span><span class=\"n\">M</span><span class=\"p\">,)](</span><span class=\"n\">x_arg</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">weight</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"p\">,</span> <span class=\"n\">mean</span><span class=\"p\">,</span> <span class=\"n\">rstd</span><span class=\"p\">,</span>\n+                                    <span class=\"n\">x_arg</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"p\">,</span>\n+                                    <span class=\"n\">BLOCK_SIZE</span><span class=\"o\">=</span><span class=\"n\">BLOCK_SIZE</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"n\">num_warps</span><span class=\"p\">,</span> <span class=\"n\">num_ctas</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n+        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">save_for_backward</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">weight</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"p\">,</span> <span class=\"n\">mean</span><span class=\"p\">,</span> <span class=\"n\">rstd</span><span class=\"p\">)</span>\n+        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">BLOCK_SIZE</span> <span class=\"o\">=</span> <span class=\"n\">BLOCK_SIZE</span>\n+        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">num_warps</span> <span class=\"o\">=</span> <span class=\"n\">num_warps</span>\n+        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">eps</span> <span class=\"o\">=</span> <span class=\"n\">eps</span>\n+        <span class=\"k\">return</span> <span class=\"n\">y</span>\n+\n+    <span class=\"nd\">@staticmethod</span>\n+    <span class=\"k\">def</span> <span class=\"nf\">backward</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">,</span> <span class=\"n\">dy</span><span class=\"p\">):</span>\n+        <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">saved_tensors</span>\n+        <span class=\"c1\"># heuristics for amount of parallel reduction stream for DW/DB</span>\n+        <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n+        <span class=\"n\">GROUP_SIZE_M</span> <span class=\"o\">=</span> <span class=\"mi\">64</span>\n+        <span class=\"k\">if</span> <span class=\"n\">N</span> <span class=\"o\">&lt;=</span> <span class=\"mi\">8192</span><span class=\"p\">:</span> <span class=\"n\">GROUP_SIZE_M</span> <span class=\"o\">=</span> <span class=\"mi\">96</span>\n+        <span class=\"k\">if</span> <span class=\"n\">N</span> <span class=\"o\">&lt;=</span> <span class=\"mi\">4096</span><span class=\"p\">:</span> <span class=\"n\">GROUP_SIZE_M</span> <span class=\"o\">=</span> <span class=\"mi\">128</span>\n+        <span class=\"k\">if</span> <span class=\"n\">N</span> <span class=\"o\">&lt;=</span> <span class=\"mi\">1024</span><span class=\"p\">:</span> <span class=\"n\">GROUP_SIZE_M</span> <span class=\"o\">=</span> <span class=\"mi\">256</span>\n+        <span class=\"c1\"># allocate output</span>\n+        <span class=\"n\">locks</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n+        <span class=\"n\">_dw</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span>\n+        <span class=\"n\">_db</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span>\n+        <span class=\"n\">dw</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span>\n+        <span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span>\n+        <span class=\"n\">dx</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">dy</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># enqueue kernel using forward pass heuristics</span>\n+        <span class=\"c1\"># also compute partial sums for DW and DB</span>\n+        <span class=\"n\">x_arg</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n+        <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"n\">x_arg</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n+        <span class=\"n\">_layer_norm_bwd_dx_fused</span><span class=\"p\">[(</span><span class=\"n\">M</span><span class=\"p\">,)](</span><span class=\"n\">dx</span><span class=\"p\">,</span> <span class=\"n\">dy</span><span class=\"p\">,</span> <span class=\"n\">_dw</span><span class=\"p\">,</span> <span class=\"n\">_db</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">locks</span><span class=\"p\">,</span>\n+                                       <span class=\"n\">x_arg</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">eps</span><span class=\"p\">,</span>\n+                                       <span class=\"n\">BLOCK_SIZE_N</span><span class=\"o\">=</span><span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">BLOCK_SIZE</span><span class=\"p\">,</span>\n+                                       <span class=\"n\">GROUP_SIZE_M</span><span class=\"o\">=</span><span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">,</span>\n+                                       <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">num_warps</span><span class=\"p\">)</span>\n+        <span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">meta</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">meta</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">])]</span>\n+        <span class=\"c1\"># accumulate partial sums in separate kernel</span>\n+        <span class=\"n\">_layer_norm_bwd_dwdb</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span><span class=\"n\">_dw</span><span class=\"p\">,</span> <span class=\"n\">_db</span><span class=\"p\">,</span> <span class=\"n\">dw</span><span class=\"p\">,</span> <span class=\"n\">db</span><span class=\"p\">,</span> <span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span>\n+                                   <span class=\"n\">BLOCK_SIZE_M</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">,</span>\n+                                   <span class=\"n\">BLOCK_SIZE_N</span><span class=\"o\">=</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"n\">num_ctas</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n+        <span class=\"k\">return</span> <span class=\"n\">dx</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">dw</span><span class=\"p\">,</span> <span class=\"n\">db</span><span class=\"p\">,</span> <span class=\"kc\">None</span>\n+\n+\n+<span class=\"n\">layer_norm</span> <span class=\"o\">=</span> <span class=\"n\">LayerNorm</span><span class=\"o\">.</span><span class=\"n\">apply</span>\n+\n+\n+<span class=\"k\">def</span> <span class=\"nf\">test_layer_norm</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-5</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">):</span>\n+    <span class=\"c1\"># create data</span>\n+    <span class=\"n\">x_shape</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n+    <span class=\"n\">w_shape</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">x_shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">)</span>\n+    <span class=\"n\">weight</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">w_shape</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+    <span class=\"n\">bias</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">w_shape</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mf\">2.3</span> <span class=\"o\">+</span> <span class=\"mf\">0.5</span> <span class=\"o\">*</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">x_shape</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n+    <span class=\"n\">dy</span> <span class=\"o\">=</span> <span class=\"mf\">.1</span> <span class=\"o\">*</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn_like</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n+    <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">requires_grad_</span><span class=\"p\">(</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># forward pass</span>\n+    <span class=\"n\">y_tri</span> <span class=\"o\">=</span> <span class=\"n\">layer_norm</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">w_shape</span><span class=\"p\">,</span> <span class=\"n\">weight</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"p\">)</span>\n+    <span class=\"n\">y_ref</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">functional</span><span class=\"o\">.</span><span class=\"n\">layer_norm</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">w_shape</span><span class=\"p\">,</span> <span class=\"n\">weight</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># backward pass (triton)</span>\n+    <span class=\"n\">y_tri</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">(</span><span class=\"n\">dy</span><span class=\"p\">,</span> <span class=\"n\">retain_graph</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+    <span class=\"n\">dx_tri</span><span class=\"p\">,</span> <span class=\"n\">dw_tri</span><span class=\"p\">,</span> <span class=\"n\">db_tri</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">_</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">clone</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">weight</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"p\">]]</span>\n+    <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"p\">,</span> <span class=\"n\">weight</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span>\n+    <span class=\"c1\"># backward pass (torch)</span>\n+    <span class=\"n\">y_ref</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">(</span><span class=\"n\">dy</span><span class=\"p\">,</span> <span class=\"n\">retain_graph</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+    <span class=\"n\">dx_ref</span><span class=\"p\">,</span> <span class=\"n\">dw_ref</span><span class=\"p\">,</span> <span class=\"n\">db_ref</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">_</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">clone</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">weight</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"p\">]]</span>\n+    <span class=\"c1\"># compare</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">y_tri</span><span class=\"p\">,</span> <span class=\"n\">y_ref</span><span class=\"p\">,</span> <span class=\"n\">atol</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">,</span> <span class=\"n\">rtol</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">dx_tri</span><span class=\"p\">,</span> <span class=\"n\">dx_ref</span><span class=\"p\">,</span> <span class=\"n\">atol</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">,</span> <span class=\"n\">rtol</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">db_tri</span><span class=\"p\">,</span> <span class=\"n\">db_ref</span><span class=\"p\">,</span> <span class=\"n\">atol</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">,</span> <span class=\"n\">rtol</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">dw_tri</span><span class=\"p\">,</span> <span class=\"n\">dw_ref</span><span class=\"p\">,</span> <span class=\"n\">atol</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">,</span> <span class=\"n\">rtol</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+\n+\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">perf_report</span><span class=\"p\">(</span>\n+    <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">Benchmark</span><span class=\"p\">(</span>\n+        <span class=\"n\">x_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;N&#39;</span><span class=\"p\">],</span>\n+        <span class=\"n\">x_vals</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">512</span> <span class=\"o\">*</span> <span class=\"n\">i</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">32</span><span class=\"p\">)],</span>\n+        <span class=\"n\">line_arg</span><span class=\"o\">=</span><span class=\"s1\">&#39;provider&#39;</span><span class=\"p\">,</span>\n+        <span class=\"n\">line_vals</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;torch&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"p\">([</span><span class=\"s1\">&#39;apex&#39;</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">HAS_APEX</span> <span class=\"k\">else</span> <span class=\"p\">[]),</span>\n+        <span class=\"n\">line_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;Triton&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;Torch&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"p\">([</span><span class=\"s1\">&#39;Apex&#39;</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">HAS_APEX</span> <span class=\"k\">else</span> <span class=\"p\">[]),</span>\n+        <span class=\"n\">styles</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s1\">&#39;blue&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;green&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;orange&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">)],</span>\n+        <span class=\"n\">ylabel</span><span class=\"o\">=</span><span class=\"s1\">&#39;GB/s&#39;</span><span class=\"p\">,</span>\n+        <span class=\"n\">plot_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;layer-norm-backward&#39;</span><span class=\"p\">,</span>\n+        <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">4096</span><span class=\"p\">,</span> <span class=\"s1\">&#39;dtype&#39;</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">,</span> <span class=\"s1\">&#39;mode&#39;</span><span class=\"p\">:</span> <span class=\"s1\">&#39;backward&#39;</span><span class=\"p\">}</span>\n+    <span class=\"p\">)</span>\n+<span class=\"p\">)</span>\n+<span class=\"k\">def</span> <span class=\"nf\">bench_layer_norm</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">provider</span><span class=\"p\">,</span> <span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;backward&#39;</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-5</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">):</span>\n+    <span class=\"c1\"># create data</span>\n+    <span class=\"n\">x_shape</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">)</span>\n+    <span class=\"n\">w_shape</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">x_shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">)</span>\n+    <span class=\"n\">weight</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">w_shape</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+    <span class=\"n\">bias</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">w_shape</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mf\">2.3</span> <span class=\"o\">+</span> <span class=\"mf\">0.5</span> <span class=\"o\">*</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">x_shape</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n+    <span class=\"n\">dy</span> <span class=\"o\">=</span> <span class=\"mf\">.1</span> <span class=\"o\">*</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn_like</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n+    <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">requires_grad_</span><span class=\"p\">(</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+    <span class=\"n\">quantiles</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">]</span>\n+    <span class=\"c1\"># utility functions</span>\n+    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">:</span>\n+        <span class=\"k\">def</span> <span class=\"nf\">y_fwd</span><span class=\"p\">():</span> <span class=\"k\">return</span> <span class=\"n\">layer_norm</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">w_shape</span><span class=\"p\">,</span> <span class=\"n\">weight</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"p\">)</span>  <span class=\"c1\"># noqa: F811, E704</span>\n+    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;torch&#39;</span><span class=\"p\">:</span>\n+        <span class=\"k\">def</span> <span class=\"nf\">y_fwd</span><span class=\"p\">():</span> <span class=\"k\">return</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">functional</span><span class=\"o\">.</span><span class=\"n\">layer_norm</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">w_shape</span><span class=\"p\">,</span> <span class=\"n\">weight</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"p\">,</span> <span class=\"n\">eps</span><span class=\"p\">)</span>  <span class=\"c1\"># noqa: F811, E704</span>\n+    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;apex&#39;</span><span class=\"p\">:</span>\n+        <span class=\"n\">apex_layer_norm</span> <span class=\"o\">=</span> <span class=\"n\">apex</span><span class=\"o\">.</span><span class=\"n\">normalization</span><span class=\"o\">.</span><span class=\"n\">FusedLayerNorm</span><span class=\"p\">(</span>\n+            <span class=\"n\">w_shape</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n+\n+        <span class=\"k\">def</span> <span class=\"nf\">y_fwd</span><span class=\"p\">():</span> <span class=\"k\">return</span> <span class=\"n\">apex_layer_norm</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>  <span class=\"c1\"># noqa: F811, E704</span>\n+    <span class=\"c1\"># forward pass</span>\n+    <span class=\"k\">if</span> <span class=\"n\">mode</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;forward&#39;</span><span class=\"p\">:</span>\n+        <span class=\"n\">gbps</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">ms</span><span class=\"p\">:</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">numel</span><span class=\"p\">()</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">element_size</span><span class=\"p\">()</span> <span class=\"o\">/</span> <span class=\"n\">ms</span> <span class=\"o\">*</span> <span class=\"mf\">1e-6</span>\n+        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"n\">y_fwd</span><span class=\"p\">,</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">,</span> <span class=\"n\">rep</span><span class=\"o\">=</span><span class=\"mi\">500</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># backward pass</span>\n+    <span class=\"k\">if</span> <span class=\"n\">mode</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;backward&#39;</span><span class=\"p\">:</span>\n+        <span class=\"k\">def</span> <span class=\"nf\">gbps</span><span class=\"p\">(</span><span class=\"n\">ms</span><span class=\"p\">):</span> <span class=\"k\">return</span> <span class=\"mi\">3</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">numel</span><span class=\"p\">()</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">element_size</span><span class=\"p\">()</span> <span class=\"o\">/</span> <span class=\"n\">ms</span> <span class=\"o\">*</span> <span class=\"mf\">1e-6</span>  <span class=\"c1\"># noqa: F811, E704</span>\n+        <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">y_fwd</span><span class=\"p\">()</span>\n+        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">(</span><span class=\"n\">dy</span><span class=\"p\">,</span> <span class=\"n\">retain_graph</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">),</span>\n+                                                     <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">,</span> <span class=\"n\">grad_to_none</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">x</span><span class=\"p\">],</span> <span class=\"n\">rep</span><span class=\"o\">=</span><span class=\"mi\">500</span><span class=\"p\">)</span>\n+    <span class=\"k\">return</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">ms</span><span class=\"p\">),</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">max_ms</span><span class=\"p\">),</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">min_ms</span><span class=\"p\">)</span>\n+\n+\n+<span class=\"n\">test_layer_norm</span><span class=\"p\">(</span><span class=\"mi\">1151</span><span class=\"p\">,</span> <span class=\"mi\">8192</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n+<span class=\"n\">bench_layer_norm</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;.&#39;</span><span class=\"p\">,</span> <span class=\"n\">print_data</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+<img src=\"../../_images/sphx_glr_05-layer-norm_001.png\" srcset=\"../../_images/sphx_glr_05-layer-norm_001.png\" alt=\"05 layer norm\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>layer-norm-backward:\n+          N       Triton       Torch\n+0    1024.0   142.057809  372.363633\n+1    1536.0   224.780491  438.857146\n+2    2048.0   299.707322  496.484863\n+3    2560.0   370.120486  534.260858\n+4    3072.0   441.485015  542.117638\n+5    3584.0   505.976473  467.478250\n+6    4096.0   588.646687  474.898540\n+7    4608.0   621.303364  480.834772\n+8    5120.0   686.480466  483.779502\n+9    5632.0   819.199976  491.520003\n+10   6144.0   867.388239  494.818794\n+11   6656.0   907.636357  499.200013\n+12   7168.0   945.230752  476.542919\n+13   7680.0   975.238103  479.999983\n+14   8192.0  1008.246151  487.861027\n+15   8704.0   673.858058  489.217808\n+16   9216.0   702.171402  492.614684\n+17   9728.0   729.600018  498.871781\n+18  10240.0   753.865011  498.498957\n+19  10752.0   779.601236  485.052653\n+20  11264.0   806.973159  487.971095\n+21  11776.0   821.581395  494.097911\n+22  12288.0   840.205140  499.005061\n+23  12800.0   848.618804  499.512174\n+24  13312.0   863.481094  499.981239\n+25  13824.0   863.999969  501.930388\n+26  14336.0   875.480928  492.928354\n+27  14848.0   877.714272  495.621695\n+28  15360.0   894.757295  500.189943\n+29  15872.0   892.103062  501.221037\n+</pre></div>\n+</div>\n+</section>\n+<section id=\"references\">\n+<h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<div role=\"list\" class=\"citation-list\">\n+<div class=\"citation\" id=\"ba2016\" role=\"doc-biblioentry\">\n+<span class=\"label\"><span class=\"fn-bracket\">[</span><a role=\"doc-backlink\" href=\"#id1\">BA2016</a><span class=\"fn-bracket\">]</span></span>\n+<p>Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \u201cLayer Normalization\u201d, Arxiv 2016</p>\n+</div>\n+</div>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  29.379 seconds)</p>\n+<div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-05-layer-norm-py\">\n+<div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/935c0dd0fbeb4b2e69588471cbb2d4b2/05-layer-norm.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">05-layer-norm.py</span></code></a></p>\n+</div>\n+<div class=\"sphx-glr-download sphx-glr-download-jupyter docutils container\">\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/ae7fff29e1b574187bc930ed94bcc353/05-layer-norm.ipynb\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Jupyter</span> <span class=\"pre\">notebook:</span> <span class=\"pre\">05-layer-norm.ipynb</span></code></a></p>\n+</div>\n+</div>\n+<p class=\"sphx-glr-signature\"><a class=\"reference external\" href=\"https://sphinx-gallery.github.io\">Gallery generated by Sphinx-Gallery</a></p>\n+</section>\n+</section>\n+\n+\n+           </div>\n+          </div>\n+          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n+        <a href=\"04-low-memory-dropout.html\" class=\"btn btn-neutral float-left\" title=\"Low-Memory Dropout\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"06-fused-attention.html\" class=\"btn btn-neutral float-right\" title=\"Fused Attention\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+    </div>\n+\n+  <hr/>\n+\n+  <div role=\"contentinfo\">\n+    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n+  </div>\n+\n+  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n+    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n+    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n+   \n+\n+</footer>\n+        </div>\n+      </div>\n+    </section>\n+  </div>\n+  <script>\n+      jQuery(function () {\n+          SphinxRtdTheme.Navigation.enable(true);\n+      });\n+  </script> \n+\n+</body>\n+</html>\n\\ No newline at end of file"}, {"filename": "main/getting-started/tutorials/06-fused-attention.html", "status": "added", "additions": 596, "deletions": 0, "changes": 596, "file_content_changes": "N/A"}, {"filename": "main/getting-started/tutorials/07-math-functions.html", "status": "added", "additions": 233, "deletions": 0, "changes": 233, "file_content_changes": "@@ -0,0 +1,233 @@\n+<!DOCTYPE html>\n+<html class=\"writer-html5\" lang=\"en\" >\n+<head>\n+  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n+\n+  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n+  <title>Libdevice (tl.math) function &mdash; Triton  documentation</title>\n+      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n+  <!--[if lt IE 9]>\n+    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n+  <![endif]-->\n+  \n+        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n+        <script src=\"../../_static/doctools.js\"></script>\n+        <script src=\"../../_static/sphinx_highlight.js\"></script>\n+    <script src=\"../../_static/js/theme.js\"></script>\n+    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n+    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n+    <link rel=\"next\" title=\"Block Pointer (Experimental)\" href=\"08-experimental-block-pointer.html\" />\n+    <link rel=\"prev\" title=\"Fused Attention\" href=\"06-fused-attention.html\" /> \n+</head>\n+\n+<body class=\"wy-body-for-nav\"> \n+  <div class=\"wy-grid-for-nav\">\n+    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n+      <div class=\"wy-side-scroll\">\n+        <div class=\"wy-side-nav-search\" >\n+\n+          \n+          \n+          <a href=\"../../index.html\" class=\"icon icon-home\">\n+            Triton\n+          </a>\n+<div role=\"search\">\n+  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n+    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n+    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n+    <input type=\"hidden\" name=\"area\" value=\"default\" />\n+  </form>\n+</div>\n+        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n+              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n+<ul class=\"current\">\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../installation.html\">Installation</a></li>\n+<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"index.html\">Tutorials</a><ul class=\"current\">\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"01-vector-add.html\">Vector Addition</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"02-fused-softmax.html\">Fused Softmax</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n+<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Libdevice (<cite>tl.math</cite>) function</a><ul>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#asin-kernel\">asin Kernel</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#using-the-default-libdevice-library-path\">Using the default libdevice library path</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#customize-the-libdevice-library-path\">Customize the libdevice library path</a></li>\n+</ul>\n+</li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"08-experimental-block-pointer.html\">Block Pointer (Experimental)</a></li>\n+</ul>\n+</li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n+</ul>\n+\n+        </div>\n+      </div>\n+    </nav>\n+\n+    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n+          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n+          <a href=\"../../index.html\">Triton</a>\n+      </nav>\n+\n+      <div class=\"wy-nav-content\">\n+        <div class=\"rst-content\">\n+          <div role=\"navigation\" aria-label=\"Page navigation\">\n+  <ul class=\"wy-breadcrumbs\">\n+      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n+          <li class=\"breadcrumb-item\"><a href=\"index.html\">Tutorials</a></li>\n+      <li class=\"breadcrumb-item active\">Libdevice (<cite>tl.math</cite>) function</li>\n+      <li class=\"wy-breadcrumbs-aside\">\n+            <a href=\"../../_sources/getting-started/tutorials/07-math-functions.rst.txt\" rel=\"nofollow\"> View page source</a>\n+      </li>\n+  </ul>\n+  <hr/>\n+</div>\n+          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n+           <div itemprop=\"articleBody\">\n+             \n+  <div class=\"sphx-glr-download-link-note admonition note\">\n+<p class=\"admonition-title\">Note</p>\n+<p><a class=\"reference internal\" href=\"#sphx-glr-download-getting-started-tutorials-07-math-functions-py\"><span class=\"std std-ref\">Go to the end</span></a>\n+to download the full example code</p>\n+</div>\n+<section class=\"sphx-glr-example-title\" id=\"libdevice-tl-math-function\">\n+<span id=\"sphx-glr-getting-started-tutorials-07-math-functions-py\"></span><h1>Libdevice (<cite>tl.math</cite>) function<a class=\"headerlink\" href=\"#libdevice-tl-math-function\" title=\"Permalink to this heading\">\u00b6</a></h1>\n+<p>Triton can invoke a custom function from an external library.\n+In this example, we will use the <cite>libdevice</cite> library (a.k.a <cite>math</cite> in triton) to apply <cite>asin</cite> on a tensor.\n+Please refer to <a class=\"reference external\" href=\"https://docs.nvidia.com/cuda/libdevice-users-guide/index.html\">https://docs.nvidia.com/cuda/libdevice-users-guide/index.html</a> regarding the semantics of all available libdevice functions.\n+In <cite>triton/language/math.py</cite>, we try to aggregate functions with the same computation but different data types together.\n+For example, both <cite>__nv_asin</cite> and <cite>__nvasinf</cite> calculate the principal value of the arc sine of the input, but <cite>__nv_asin</cite> operates on <cite>double</cite> and <cite>__nv_asinf</cite> operates on <cite>float</cite>.\n+Using triton, you can simply call <cite>tl.math.asin</cite>.\n+Triton automatically selects the correct underlying device function to invoke based on input and output types.</p>\n+<section id=\"asin-kernel\">\n+<h2>asin Kernel<a class=\"headerlink\" href=\"#asin-kernel\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n+\n+<span class=\"kn\">import</span> <span class=\"nn\">triton</span>\n+<span class=\"kn\">import</span> <span class=\"nn\">triton.language</span> <span class=\"k\">as</span> <span class=\"nn\">tl</span>\n+\n+\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n+<span class=\"k\">def</span> <span class=\"nf\">asin_kernel</span><span class=\"p\">(</span>\n+        <span class=\"n\">x_ptr</span><span class=\"p\">,</span>\n+        <span class=\"n\">y_ptr</span><span class=\"p\">,</span>\n+        <span class=\"n\">n_elements</span><span class=\"p\">,</span>\n+        <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+<span class=\"p\">):</span>\n+    <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"n\">block_start</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE</span>\n+    <span class=\"n\">offsets</span> <span class=\"o\">=</span> <span class=\"n\">block_start</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">)</span>\n+    <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">offsets</span> <span class=\"o\">&lt;</span> <span class=\"n\">n_elements</span>\n+    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">x_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n+    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">asin</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">y_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+</section>\n+<section id=\"using-the-default-libdevice-library-path\">\n+<h2>Using the default libdevice library path<a class=\"headerlink\" href=\"#using-the-default-libdevice-library-path\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>We can use the default libdevice library path encoded in <cite>triton/language/math.py</cite></p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+<span class=\"n\">size</span> <span class=\"o\">=</span> <span class=\"mi\">98432</span>\n+<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n+<span class=\"n\">output_triton</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n+<span class=\"n\">output_torch</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">asin</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n+<span class=\"k\">assert</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">is_cuda</span> <span class=\"ow\">and</span> <span class=\"n\">output_triton</span><span class=\"o\">.</span><span class=\"n\">is_cuda</span>\n+<span class=\"n\">n_elements</span> <span class=\"o\">=</span> <span class=\"n\">output_torch</span><span class=\"o\">.</span><span class=\"n\">numel</span><span class=\"p\">()</span>\n+<span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">meta</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">meta</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">]),)</span>\n+<span class=\"n\">asin_kernel</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">output_triton</span><span class=\"p\">,</span> <span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"o\">=</span><span class=\"mi\">1024</span><span class=\"p\">)</span>\n+<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">output_torch</span><span class=\"p\">)</span>\n+<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">output_triton</span><span class=\"p\">)</span>\n+<span class=\"nb\">print</span><span class=\"p\">(</span>\n+    <span class=\"sa\">f</span><span class=\"s1\">&#39;The maximum difference between torch and triton is &#39;</span>\n+    <span class=\"sa\">f</span><span class=\"s1\">&#39;</span><span class=\"si\">{</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">abs</span><span class=\"p\">(</span><span class=\"n\">output_torch</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"w\"> </span><span class=\"n\">output_triton</span><span class=\"p\">))</span><span class=\"si\">}</span><span class=\"s1\">&#39;</span>\n+<span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+<div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device=&#39;cuda:0&#39;)\n+tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device=&#39;cuda:0&#39;)\n+The maximum difference between torch and triton is 2.384185791015625e-07\n+</pre></div>\n+</div>\n+</section>\n+<section id=\"customize-the-libdevice-library-path\">\n+<h2>Customize the libdevice library path<a class=\"headerlink\" href=\"#customize-the-libdevice-library-path\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>We can also customize the libdevice library path by passing the path to the <cite>libdevice</cite> library to the <cite>asin</cite> kernel.</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">output_triton</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n+<span class=\"n\">asin_kernel</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">output_triton</span><span class=\"p\">,</span> <span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"o\">=</span><span class=\"mi\">1024</span><span class=\"p\">,</span>\n+                  <span class=\"n\">extern_libs</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;libdevice&#39;</span><span class=\"p\">:</span> <span class=\"s1\">&#39;/usr/local/cuda/nvvm/libdevice/libdevice.10.bc&#39;</span><span class=\"p\">})</span>\n+<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">output_torch</span><span class=\"p\">)</span>\n+<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">output_triton</span><span class=\"p\">)</span>\n+<span class=\"nb\">print</span><span class=\"p\">(</span>\n+    <span class=\"sa\">f</span><span class=\"s1\">&#39;The maximum difference between torch and triton is &#39;</span>\n+    <span class=\"sa\">f</span><span class=\"s1\">&#39;</span><span class=\"si\">{</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">abs</span><span class=\"p\">(</span><span class=\"n\">output_torch</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"w\"> </span><span class=\"n\">output_triton</span><span class=\"p\">))</span><span class=\"si\">}</span><span class=\"s1\">&#39;</span>\n+<span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+<div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device=&#39;cuda:0&#39;)\n+tensor([0.4105, 0.5430, 0.0249,  ..., 0.0424, 0.5351, 0.8149], device=&#39;cuda:0&#39;)\n+The maximum difference between torch and triton is 2.384185791015625e-07\n+</pre></div>\n+</div>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.224 seconds)</p>\n+<div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-07-math-functions-py\">\n+<div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/77571465f7f4bd281d3a847dc2633146/07-math-functions.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">07-math-functions.py</span></code></a></p>\n+</div>\n+<div class=\"sphx-glr-download sphx-glr-download-jupyter docutils container\">\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/302f1761fdc89516b532aad33b963ca0/07-math-functions.ipynb\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Jupyter</span> <span class=\"pre\">notebook:</span> <span class=\"pre\">07-math-functions.ipynb</span></code></a></p>\n+</div>\n+</div>\n+<p class=\"sphx-glr-signature\"><a class=\"reference external\" href=\"https://sphinx-gallery.github.io\">Gallery generated by Sphinx-Gallery</a></p>\n+</section>\n+</section>\n+\n+\n+           </div>\n+          </div>\n+          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n+        <a href=\"06-fused-attention.html\" class=\"btn btn-neutral float-left\" title=\"Fused Attention\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"08-experimental-block-pointer.html\" class=\"btn btn-neutral float-right\" title=\"Block Pointer (Experimental)\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+    </div>\n+\n+  <hr/>\n+\n+  <div role=\"contentinfo\">\n+    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n+  </div>\n+\n+  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n+    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n+    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n+   \n+\n+</footer>\n+        </div>\n+      </div>\n+    </section>\n+  </div>\n+  <script>\n+      jQuery(function () {\n+          SphinxRtdTheme.Navigation.enable(true);\n+      });\n+  </script> \n+\n+</body>\n+</html>\n\\ No newline at end of file"}, {"filename": "main/getting-started/tutorials/08-experimental-block-pointer.html", "status": "added", "additions": 382, "deletions": 0, "changes": 382, "file_content_changes": "@@ -0,0 +1,382 @@\n+<!DOCTYPE html>\n+<html class=\"writer-html5\" lang=\"en\" >\n+<head>\n+  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n+\n+  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n+  <title>Block Pointer (Experimental) &mdash; Triton  documentation</title>\n+      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n+  <!--[if lt IE 9]>\n+    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n+  <![endif]-->\n+  \n+        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n+        <script src=\"../../_static/doctools.js\"></script>\n+        <script src=\"../../_static/sphinx_highlight.js\"></script>\n+    <script src=\"../../_static/js/theme.js\"></script>\n+    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n+    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n+    <link rel=\"next\" title=\"triton\" href=\"../../python-api/triton.html\" />\n+    <link rel=\"prev\" title=\"Libdevice (tl.math) function\" href=\"07-math-functions.html\" /> \n+</head>\n+\n+<body class=\"wy-body-for-nav\"> \n+  <div class=\"wy-grid-for-nav\">\n+    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n+      <div class=\"wy-side-scroll\">\n+        <div class=\"wy-side-nav-search\" >\n+\n+          \n+          \n+          <a href=\"../../index.html\" class=\"icon icon-home\">\n+            Triton\n+          </a>\n+<div role=\"search\">\n+  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n+    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n+    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n+    <input type=\"hidden\" name=\"area\" value=\"default\" />\n+  </form>\n+</div>\n+        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n+              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n+<ul class=\"current\">\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../installation.html\">Installation</a></li>\n+<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"index.html\">Tutorials</a><ul class=\"current\">\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"01-vector-add.html\">Vector Addition</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"02-fused-softmax.html\">Fused Softmax</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n+<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Block Pointer (Experimental)</a><ul>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#motivations\">Motivations</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#make-a-block-pointer\">Make a Block Pointer</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#load-store-a-block-pointer\">Load/Store a Block Pointer</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#advance-a-block-pointer\">Advance a Block Pointer</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#final-result\">Final Result</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#unit-test\">Unit Test</a></li>\n+</ul>\n+</li>\n+</ul>\n+</li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n+</ul>\n+\n+        </div>\n+      </div>\n+    </nav>\n+\n+    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n+          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n+          <a href=\"../../index.html\">Triton</a>\n+      </nav>\n+\n+      <div class=\"wy-nav-content\">\n+        <div class=\"rst-content\">\n+          <div role=\"navigation\" aria-label=\"Page navigation\">\n+  <ul class=\"wy-breadcrumbs\">\n+      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n+          <li class=\"breadcrumb-item\"><a href=\"index.html\">Tutorials</a></li>\n+      <li class=\"breadcrumb-item active\">Block Pointer (Experimental)</li>\n+      <li class=\"wy-breadcrumbs-aside\">\n+            <a href=\"../../_sources/getting-started/tutorials/08-experimental-block-pointer.rst.txt\" rel=\"nofollow\"> View page source</a>\n+      </li>\n+  </ul>\n+  <hr/>\n+</div>\n+          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n+           <div itemprop=\"articleBody\">\n+             \n+  <div class=\"sphx-glr-download-link-note admonition note\">\n+<p class=\"admonition-title\">Note</p>\n+<p><a class=\"reference internal\" href=\"#sphx-glr-download-getting-started-tutorials-08-experimental-block-pointer-py\"><span class=\"std std-ref\">Go to the end</span></a>\n+to download the full example code</p>\n+</div>\n+<section class=\"sphx-glr-example-title\" id=\"block-pointer-experimental\">\n+<span id=\"sphx-glr-getting-started-tutorials-08-experimental-block-pointer-py\"></span><h1>Block Pointer (Experimental)<a class=\"headerlink\" href=\"#block-pointer-experimental\" title=\"Permalink to this heading\">\u00b6</a></h1>\n+<p>This tutorial will guide you through writing a matrix multiplication algorithm that utilizes block pointer semantics.\n+These semantics are more friendly for Triton to optimize and can result in better performance on specific hardware.\n+Note that this feature is still experimental and may change in the future.</p>\n+<section id=\"motivations\">\n+<h2>Motivations<a class=\"headerlink\" href=\"#motivations\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>In the previous matrix multiplication tutorial, we constructed blocks of values by de-referencing blocks of pointers,\n+i.e., <code class=\"code docutils literal notranslate\"><span class=\"pre\">load(block&lt;pointer_type&lt;element_type&gt;&gt;)</span> <span class=\"pre\">-&gt;</span> <span class=\"pre\">block&lt;element_type&gt;</span></code>, which involved loading blocks of\n+elements from memory. This approach allowed for flexibility in using hardware-managed cache and implementing complex\n+data structures, such as tensors of trees or unstructured look-up tables.</p>\n+<p>However, the drawback of this approach is that it relies heavily on complex optimization passes by the compiler to\n+optimize memory access patterns. This can result in brittle code that may suffer from performance degradation when the\n+optimizer fails to perform adequately. Additionally, as memory controllers specialize to accommodate dense spatial\n+data structures commonly used in machine learning workloads, this problem is likely to worsen.</p>\n+<p>To address this issue, we will use block pointers <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer_type&lt;block&lt;element_type&gt;&gt;</span></code> and load them into\n+<code class=\"code docutils literal notranslate\"><span class=\"pre\">block&lt;element_type&gt;</span></code>, in which way gives better friendliness for the compiler to optimize memory access\n+patterns.</p>\n+<p>Let\u2019s start with the previous matrix multiplication example and demonstrate how to rewrite it to utilize block pointer\n+semantics.</p>\n+</section>\n+<section id=\"make-a-block-pointer\">\n+<h2>Make a Block Pointer<a class=\"headerlink\" href=\"#make-a-block-pointer\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>A block pointer pointers to a block in a parent tensor and is constructed by <code class=\"code docutils literal notranslate\"><span class=\"pre\">make_block_ptr</span></code> function,\n+which takes the following information as arguments:</p>\n+<ul class=\"simple\">\n+<li><p><code class=\"code docutils literal notranslate\"><span class=\"pre\">base</span></code>: the base pointer to the parent tensor;</p></li>\n+<li><p><code class=\"code docutils literal notranslate\"><span class=\"pre\">shape</span></code>: the shape of the parent tensor;</p></li>\n+<li><p><code class=\"code docutils literal notranslate\"><span class=\"pre\">strides</span></code>: the strides of the parent tensor, which means how much to increase the pointer by when moving by 1 element in a specific axis;</p></li>\n+<li><p><code class=\"code docutils literal notranslate\"><span class=\"pre\">offsets</span></code>: the offsets of the block;</p></li>\n+<li><p><code class=\"code docutils literal notranslate\"><span class=\"pre\">block_shape</span></code>: the shape of the block;</p></li>\n+<li><p><code class=\"code docutils literal notranslate\"><span class=\"pre\">order</span></code>: the order of the block, which means how the block is laid out in memory.</p></li>\n+</ul>\n+<p>For example, to a block pointer to a <code class=\"code docutils literal notranslate\"><span class=\"pre\">BLOCK_SIZE_M</span> <span class=\"pre\">*</span> <span class=\"pre\">BLOCK_SIZE_K</span></code> block in a row-major 2D matrix A by\n+offsets <code class=\"code docutils literal notranslate\"><span class=\"pre\">(pid_m</span> <span class=\"pre\">*</span> <span class=\"pre\">BLOCK_SIZE_M,</span> <span class=\"pre\">0)</span></code> and strides <code class=\"code docutils literal notranslate\"><span class=\"pre\">(stride_am,</span> <span class=\"pre\">stride_ak)</span></code>, we can use the following code\n+(exactly the same as the previous matrix multiplication tutorial):</p>\n+<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">a_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span><span class=\"n\">base</span><span class=\"o\">=</span><span class=\"n\">a_ptr</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">),</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_am</span><span class=\"p\">,</span> <span class=\"n\">stride_ak</span><span class=\"p\">),</span>\n+                                <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">pid_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">),</span>\n+                                <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n+</pre></div>\n+</div>\n+<p>Note that the <code class=\"code docutils literal notranslate\"><span class=\"pre\">order</span></code> argument is set to <code class=\"code docutils literal notranslate\"><span class=\"pre\">(1,</span> <span class=\"pre\">0)</span></code>, which means the second axis is the inner dimension in\n+terms of storage, and the first axis is the outer dimension. This information may sound redundant, but it is necessary\n+for some hardware backends to optimize for better performance.</p>\n+</section>\n+<section id=\"load-store-a-block-pointer\">\n+<h2>Load/Store a Block Pointer<a class=\"headerlink\" href=\"#load-store-a-block-pointer\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>To load/store a block pointer, we can use <code class=\"code docutils literal notranslate\"><span class=\"pre\">load/store</span></code> function, which takes a block pointer as an argument,\n+de-references it, and loads/stores a block. You may mask some values in the block, here we have an extra argument\n+<code class=\"code docutils literal notranslate\"><span class=\"pre\">boundary_check</span></code> to specify whether to check the boundary of each axis for the block pointer. With check on,\n+out-of-bound values will be masked according to the <code class=\"code docutils literal notranslate\"><span class=\"pre\">padding_option</span></code> argument (load only), which can be\n+<code class=\"code docutils literal notranslate\"><span class=\"pre\">zero</span></code> or <code class=\"code docutils literal notranslate\"><span class=\"pre\">nan</span></code>. Temporarily, we do not support other values due to some hardware limitations. In this\n+mode of block pointer load/store does not support <code class=\"code docutils literal notranslate\"><span class=\"pre\">mask</span></code> or <code class=\"code docutils literal notranslate\"><span class=\"pre\">other</span></code> arguments in the legacy mode.</p>\n+<p>So to load the block pointer of A in the previous section, we can simply write\n+<code class=\"code docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">=</span> <span class=\"pre\">tl.load(a_block_ptr,</span> <span class=\"pre\">boundary_check=(0,</span> <span class=\"pre\">1))</span></code>. Boundary check may cost extra performance, so if you can\n+guarantee that the block pointer is always in-bound in some axis, you can turn off the check by not passing the index\n+into the <code class=\"code docutils literal notranslate\"><span class=\"pre\">boundary_check</span></code> argument. For example, if we know that <code class=\"code docutils literal notranslate\"><span class=\"pre\">M</span></code> is a multiple of\n+<code class=\"code docutils literal notranslate\"><span class=\"pre\">BLOCK_SIZE_M</span></code>, we can replace with <code class=\"code docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">=</span> <span class=\"pre\">tl.load(a_block_ptr,</span> <span class=\"pre\">boundary_check=(1,</span> <span class=\"pre\">))</span></code>, since axis 0 is\n+always in bound.</p>\n+</section>\n+<section id=\"advance-a-block-pointer\">\n+<h2>Advance a Block Pointer<a class=\"headerlink\" href=\"#advance-a-block-pointer\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>To advance a block pointer, we can use <code class=\"code docutils literal notranslate\"><span class=\"pre\">advance</span></code> function, which takes a block pointer and the increment for\n+each axis as arguments and returns a new block pointer with the same shape and strides as the original one,\n+but with the offsets advanced by the specified amount.</p>\n+<p>For example, to advance the block pointer by <code class=\"code docutils literal notranslate\"><span class=\"pre\">BLOCK_SIZE_K</span></code> in the second axis\n+(no need to multiply with strides), we can write <code class=\"code docutils literal notranslate\"><span class=\"pre\">a_block_ptr</span> <span class=\"pre\">=</span> <span class=\"pre\">tl.advance(a_block_ptr,</span> <span class=\"pre\">(0,</span> <span class=\"pre\">BLOCK_SIZE_K))</span></code>.</p>\n+</section>\n+<section id=\"final-result\">\n+<h2>Final Result<a class=\"headerlink\" href=\"#final-result\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n+\n+<span class=\"kn\">import</span> <span class=\"nn\">triton</span>\n+<span class=\"kn\">import</span> <span class=\"nn\">triton.language</span> <span class=\"k\">as</span> <span class=\"nn\">tl</span>\n+\n+\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">autotune</span><span class=\"p\">(</span>\n+    <span class=\"n\">configs</span><span class=\"o\">=</span><span class=\"p\">[</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n+    <span class=\"p\">],</span>\n+    <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;M&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;K&#39;</span><span class=\"p\">],</span>\n+<span class=\"p\">)</span>\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n+<span class=\"k\">def</span> <span class=\"nf\">matmul_kernel_with_block_pointers</span><span class=\"p\">(</span>\n+        <span class=\"c1\"># Pointers to matrices</span>\n+        <span class=\"n\">a_ptr</span><span class=\"p\">,</span> <span class=\"n\">b_ptr</span><span class=\"p\">,</span> <span class=\"n\">c_ptr</span><span class=\"p\">,</span>\n+        <span class=\"c1\"># Matrix dimensions</span>\n+        <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span>\n+        <span class=\"c1\"># The stride variables represent how much to increase the ptr by when moving by 1</span>\n+        <span class=\"c1\"># element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`</span>\n+        <span class=\"c1\"># by to get the element one row down (A has M rows).</span>\n+        <span class=\"n\">stride_am</span><span class=\"p\">,</span> <span class=\"n\">stride_ak</span><span class=\"p\">,</span>\n+        <span class=\"n\">stride_bk</span><span class=\"p\">,</span> <span class=\"n\">stride_bn</span><span class=\"p\">,</span>\n+        <span class=\"n\">stride_cm</span><span class=\"p\">,</span> <span class=\"n\">stride_cn</span><span class=\"p\">,</span>\n+        <span class=\"c1\"># Meta-parameters</span>\n+        <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+        <span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span>\n+<span class=\"p\">):</span>\n+<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Kernel for computing the matmul C = A x B.</span>\n+<span class=\"sd\">    A has shape (M, K), B has shape (K, N) and C has shape (M, N)</span>\n+<span class=\"sd\">    &quot;&quot;&quot;</span>\n+    <span class=\"c1\"># -----------------------------------------------------------</span>\n+    <span class=\"c1\"># Map program ids `pid` to the block of C it should compute.</span>\n+    <span class=\"c1\"># This is done in a grouped ordering to promote L2 data reuse.</span>\n+    <span class=\"c1\"># See the matrix multiplication tutorial for details.</span>\n+    <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"n\">num_pid_m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">)</span>\n+    <span class=\"n\">num_pid_n</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)</span>\n+    <span class=\"n\">num_pid_in_group</span> <span class=\"o\">=</span> <span class=\"n\">GROUP_SIZE_M</span> <span class=\"o\">*</span> <span class=\"n\">num_pid_n</span>\n+    <span class=\"n\">group_id</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">//</span> <span class=\"n\">num_pid_in_group</span>\n+    <span class=\"n\">first_pid_m</span> <span class=\"o\">=</span> <span class=\"n\">group_id</span> <span class=\"o\">*</span> <span class=\"n\">GROUP_SIZE_M</span>\n+    <span class=\"n\">group_size_m</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">num_pid_m</span> <span class=\"o\">-</span> <span class=\"n\">first_pid_m</span><span class=\"p\">,</span> <span class=\"n\">GROUP_SIZE_M</span><span class=\"p\">)</span>\n+    <span class=\"n\">pid_m</span> <span class=\"o\">=</span> <span class=\"n\">first_pid_m</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">pid</span> <span class=\"o\">%</span> <span class=\"n\">group_size_m</span><span class=\"p\">)</span>\n+    <span class=\"n\">pid_n</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">pid</span> <span class=\"o\">%</span> <span class=\"n\">num_pid_in_group</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"n\">group_size_m</span>\n+\n+    <span class=\"c1\"># ----------------------------------------------------------</span>\n+    <span class=\"c1\"># Create block pointers for the first blocks of A and B.</span>\n+    <span class=\"c1\"># We will advance this pointer as we move in the K direction and accumulate.</span>\n+    <span class=\"c1\"># See above `Make a Block Pointer` section for details.</span>\n+    <span class=\"n\">a_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span><span class=\"n\">base</span><span class=\"o\">=</span><span class=\"n\">a_ptr</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">),</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_am</span><span class=\"p\">,</span> <span class=\"n\">stride_ak</span><span class=\"p\">),</span>\n+                                    <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">pid_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">),</span>\n+                                    <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n+    <span class=\"n\">b_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span><span class=\"n\">base</span><span class=\"o\">=</span><span class=\"n\">b_ptr</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">),</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_bk</span><span class=\"p\">,</span> <span class=\"n\">stride_bn</span><span class=\"p\">),</span>\n+                                    <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">pid_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span> <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span>\n+                                    <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n+\n+    <span class=\"c1\"># -----------------------------------------------------------</span>\n+    <span class=\"c1\"># Iterate to compute a block of the C matrix.</span>\n+    <span class=\"c1\"># We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block.</span>\n+    <span class=\"c1\"># of fp32 values for higher accuracy.</span>\n+    <span class=\"c1\"># `accumulator` will be converted back to fp16 after the loop.</span>\n+    <span class=\"n\">accumulator</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">):</span>\n+        <span class=\"c1\"># Load with boundary checks, no need to calculate the mask manually.</span>\n+        <span class=\"c1\"># For better performance, you may remove some axis from the boundary</span>\n+        <span class=\"c1\"># check, if you can guarantee that the access is always in-bound in</span>\n+        <span class=\"c1\"># that axis.</span>\n+        <span class=\"c1\"># See above `Load/Store a Block Pointer` section for details.</span>\n+        <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">a_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n+        <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">b_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n+        <span class=\"c1\"># We accumulate along the K dimension.</span>\n+        <span class=\"n\">accumulator</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># Advance the block pointer to the next K block.</span>\n+        <span class=\"c1\"># See above `Advance a Block Pointer` section for details.</span>\n+        <span class=\"n\">a_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">advance</span><span class=\"p\">(</span><span class=\"n\">a_block_ptr</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">))</span>\n+        <span class=\"n\">b_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">advance</span><span class=\"p\">(</span><span class=\"n\">b_block_ptr</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n+    <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">accumulator</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n+\n+    <span class=\"c1\"># ----------------------------------------------------------------</span>\n+    <span class=\"c1\"># Write back the block of the output matrix C with boundary checks.</span>\n+    <span class=\"c1\"># See above `Load/Store a Block Pointer` section for details.</span>\n+    <span class=\"n\">c_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span><span class=\"n\">base</span><span class=\"o\">=</span><span class=\"n\">c_ptr</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">),</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_cm</span><span class=\"p\">,</span> <span class=\"n\">stride_cn</span><span class=\"p\">),</span>\n+                                    <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">pid_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">pid_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span>\n+                                    <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span> <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">c_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">,</span> <span class=\"n\">boundary_check</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n+\n+\n+<span class=\"c1\"># We can now create a convenience wrapper function that only takes two input tensors,</span>\n+<span class=\"c1\"># and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.</span>\n+<span class=\"k\">def</span> <span class=\"nf\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">):</span>\n+    <span class=\"c1\"># Check constraints.</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"s2\">&quot;Incompatible dimensions&quot;</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">is_contiguous</span><span class=\"p\">(),</span> <span class=\"s2\">&quot;Matrix A must be contiguous&quot;</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">is_contiguous</span><span class=\"p\">(),</span> <span class=\"s2\">&quot;Matrix B must be contiguous&quot;</span>\n+    <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">K</span> <span class=\"o\">=</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n+    <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n+    <span class=\"c1\"># Allocates output.</span>\n+    <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># 1D launch kernel where each block gets its own program.</span>\n+    <span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">META</span><span class=\"p\">:</span> <span class=\"p\">(</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">META</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">META</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">]),</span>\n+    <span class=\"p\">)</span>\n+    <span class=\"n\">matmul_kernel_with_block_pointers</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span>\n+        <span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">,</span>\n+        <span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span>\n+        <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n+        <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n+        <span class=\"n\">c</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">c</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n+    <span class=\"p\">)</span>\n+    <span class=\"k\">return</span> <span class=\"n\">c</span>\n+</pre></div>\n+</div>\n+</section>\n+<section id=\"unit-test\">\n+<h2>Unit Test<a class=\"headerlink\" href=\"#unit-test\" title=\"Permalink to this heading\">\u00b6</a></h2>\n+<p>Still we can test our matrix multiplication with block pointers against a native torch implementation (i.e., cuBLAS).</p>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+<span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n+<span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n+<span class=\"n\">triton_output</span> <span class=\"o\">=</span> <span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n+<span class=\"n\">torch_output</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n+<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;triton_output=</span><span class=\"si\">{</span><span class=\"n\">triton_output</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n+<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;torch_output=</span><span class=\"si\">{</span><span class=\"n\">torch_output</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n+<span class=\"k\">if</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">triton_output</span><span class=\"p\">,</span> <span class=\"n\">torch_output</span><span class=\"p\">,</span> <span class=\"n\">atol</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">,</span> <span class=\"n\">rtol</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">):</span>\n+    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;\u2705 Triton and Torch match&quot;</span><span class=\"p\">)</span>\n+<span class=\"k\">else</span><span class=\"p\">:</span>\n+    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;\u274c Triton and Torch differ&quot;</span><span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+<div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>triton_output=tensor([[-10.9531,  -4.7109,  15.6953,  ..., -28.4062,   4.3320, -26.4219],\n+        [ 26.8438,  10.0469,  -5.4297,  ..., -11.2969,  -8.5312,  30.7500],\n+        [-13.2578,  15.8516,  18.0781,  ..., -21.7656,  -8.6406,  10.2031],\n+        ...,\n+        [ 40.2812,  18.6094, -25.6094,  ...,  -2.7598,  -3.2441,  41.0000],\n+        [ -6.1211, -16.8281,   4.4844,  ..., -21.0312,  24.7031,  15.0234],\n+        [-17.0938, -19.0000,  -0.3831,  ...,  21.5469, -30.2344, -13.2188]],\n+       device=&#39;cuda:0&#39;, dtype=torch.float16)\n+torch_output=tensor([[-10.9531,  -4.7109,  15.6953,  ..., -28.4062,   4.3320, -26.4219],\n+        [ 26.8438,  10.0469,  -5.4297,  ..., -11.2969,  -8.5312,  30.7500],\n+        [-13.2578,  15.8516,  18.0781,  ..., -21.7656,  -8.6406,  10.2031],\n+        ...,\n+        [ 40.2812,  18.6094, -25.6094,  ...,  -2.7598,  -3.2441,  41.0000],\n+        [ -6.1211, -16.8281,   4.4844,  ..., -21.0312,  24.7031,  15.0234],\n+        [-17.0938, -19.0000,  -0.3831,  ...,  21.5469, -30.2344, -13.2188]],\n+       device=&#39;cuda:0&#39;, dtype=torch.float16)\n+\u2705 Triton and Torch match\n+</pre></div>\n+</div>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  6.243 seconds)</p>\n+<div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-08-experimental-block-pointer-py\">\n+<div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/4d6052117d61c2ca779cd4b75567fee5/08-experimental-block-pointer.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">08-experimental-block-pointer.py</span></code></a></p>\n+</div>\n+<div class=\"sphx-glr-download sphx-glr-download-jupyter docutils container\">\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/9705af6f33c6e66c6fa78f2394331536/08-experimental-block-pointer.ipynb\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Jupyter</span> <span class=\"pre\">notebook:</span> <span class=\"pre\">08-experimental-block-pointer.ipynb</span></code></a></p>\n+</div>\n+</div>\n+<p class=\"sphx-glr-signature\"><a class=\"reference external\" href=\"https://sphinx-gallery.github.io\">Gallery generated by Sphinx-Gallery</a></p>\n+</section>\n+</section>\n+\n+\n+           </div>\n+          </div>\n+          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n+        <a href=\"07-math-functions.html\" class=\"btn btn-neutral float-left\" title=\"Libdevice (tl.math) function\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"../../python-api/triton.html\" class=\"btn btn-neutral float-right\" title=\"triton\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+    </div>\n+\n+  <hr/>\n+\n+  <div role=\"contentinfo\">\n+    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n+  </div>\n+\n+  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n+    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n+    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n+   \n+\n+</footer>\n+        </div>\n+      </div>\n+    </section>\n+  </div>\n+  <script>\n+      jQuery(function () {\n+          SphinxRtdTheme.Navigation.enable(true);\n+      });\n+  </script> \n+\n+</body>\n+</html>\n\\ No newline at end of file"}, {"filename": "main/getting-started/tutorials/index.html", "status": "modified", "additions": 41, "deletions": 18, "changes": 59, "file_content_changes": "@@ -22,7 +22,7 @@\n     <script src=\"../../_static/js/theme.js\"></script>\n     <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n     <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton\" href=\"../../python-api/triton.html\" />\n+    <link rel=\"next\" title=\"Vector Addition\" href=\"01-vector-add.html\" />\n     <link rel=\"prev\" title=\"Installation\" href=\"../installation.html\" /> \n </head>\n \n@@ -48,7 +48,17 @@\n               <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n <ul class=\"current\">\n <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../installation.html\">Installation</a></li>\n-<li class=\"toctree-l1 current\"><a class=\"current reference internal\" href=\"#\">Tutorials</a></li>\n+<li class=\"toctree-l1 current\"><a class=\"current reference internal\" href=\"#\">Tutorials</a><ul>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"01-vector-add.html\">Vector Addition</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"02-fused-softmax.html\">Fused Softmax</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"08-experimental-block-pointer.html\">Block Pointer (Experimental)</a></li>\n+</ul>\n+</li>\n </ul>\n <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n <ul>\n@@ -94,7 +104,33 @@ <h1>Tutorials<a class=\"headerlink\" href=\"#tutorials\" title=\"Permalink to this he\n pip<span class=\"w\"> </span>install<span class=\"w\"> </span>-e<span class=\"w\"> </span><span class=\"s1\">&#39;./python[tutorials]&#39;</span>\n </pre></div>\n </div>\n-<div class=\"sphx-glr-thumbnails\"></div><div class=\"sphx-glr-footer sphx-glr-footer-gallery docutils container\">\n+<div class=\"sphx-glr-thumbnails\"><div class=\"sphx-glr-thumbcontainer\" tooltip=\"In this tutorial, you will write a simple vector addition using Triton.\"><img alt=\"\" src=\"../../_images/sphx_glr_01-vector-add_thumb.png\" />\n+<p><a class=\"reference internal\" href=\"01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py\"><span class=\"std std-ref\">Vector Addition</span></a></p>\n+  <div class=\"sphx-glr-thumbnail-title\">Vector Addition</div>\n+</div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"In this tutorial, you will write a fused softmax operation that is significantly faster than Py...\"><img alt=\"\" src=\"../../_images/sphx_glr_02-fused-softmax_thumb.png\" />\n+<p><a class=\"reference internal\" href=\"02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py\"><span class=\"std std-ref\">Fused Softmax</span></a></p>\n+  <div class=\"sphx-glr-thumbnail-title\">Fused Softmax</div>\n+</div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"You will specifically learn about:\"><img alt=\"\" src=\"../../_images/sphx_glr_03-matrix-multiplication_thumb.png\" />\n+<p><a class=\"reference internal\" href=\"03-matrix-multiplication.html#sphx-glr-getting-started-tutorials-03-matrix-multiplication-py\"><span class=\"std std-ref\">Matrix Multiplication</span></a></p>\n+  <div class=\"sphx-glr-thumbnail-title\">Matrix Multiplication</div>\n+</div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"In this tutorial, you will write a memory-efficient implementation of dropout whose state will ...\"><img alt=\"\" src=\"../../_images/sphx_glr_04-low-memory-dropout_thumb.png\" />\n+<p><a class=\"reference internal\" href=\"04-low-memory-dropout.html#sphx-glr-getting-started-tutorials-04-low-memory-dropout-py\"><span class=\"std std-ref\">Low-Memory Dropout</span></a></p>\n+  <div class=\"sphx-glr-thumbnail-title\">Low-Memory Dropout</div>\n+</div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"In doing so, you will learn about:\"><img alt=\"\" src=\"../../_images/sphx_glr_05-layer-norm_thumb.png\" />\n+<p><a class=\"reference internal\" href=\"05-layer-norm.html#sphx-glr-getting-started-tutorials-05-layer-norm-py\"><span class=\"std std-ref\">Layer Normalization</span></a></p>\n+  <div class=\"sphx-glr-thumbnail-title\">Layer Normalization</div>\n+</div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"This is a Triton implementation of the Flash Attention v2 algorithm from Tri Dao (https://trida...\"><img alt=\"\" src=\"../../_images/sphx_glr_06-fused-attention_thumb.png\" />\n+<p><a class=\"reference internal\" href=\"06-fused-attention.html#sphx-glr-getting-started-tutorials-06-fused-attention-py\"><span class=\"std std-ref\">Fused Attention</span></a></p>\n+  <div class=\"sphx-glr-thumbnail-title\">Fused Attention</div>\n+</div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"Libdevice (`tl.math`) function\"><img alt=\"\" src=\"../../_images/sphx_glr_07-math-functions_thumb.png\" />\n+<p><a class=\"reference internal\" href=\"07-math-functions.html#sphx-glr-getting-started-tutorials-07-math-functions-py\"><span class=\"std std-ref\">Libdevice (tl.math) function</span></a></p>\n+  <div class=\"sphx-glr-thumbnail-title\">Libdevice (`tl.math`) function</div>\n+</div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"Block Pointer (Experimental)\"><img alt=\"\" src=\"../../_images/sphx_glr_08-experimental-block-pointer_thumb.png\" />\n+<p><a class=\"reference internal\" href=\"08-experimental-block-pointer.html#sphx-glr-getting-started-tutorials-08-experimental-block-pointer-py\"><span class=\"std std-ref\">Block Pointer (Experimental)</span></a></p>\n+  <div class=\"sphx-glr-thumbnail-title\">Block Pointer (Experimental)</div>\n+</div></div><div class=\"toctree-wrapper compound\">\n+</div>\n+<div class=\"sphx-glr-footer sphx-glr-footer-gallery docutils container\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/763344228ae6bc253ed1a6cf586aa30d/tutorials_python.zip\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">all</span> <span class=\"pre\">examples</span> <span class=\"pre\">in</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">tutorials_python.zip</span></code></a></p>\n </div>\n@@ -110,7 +146,7 @@ <h1>Tutorials<a class=\"headerlink\" href=\"#tutorials\" title=\"Permalink to this he\n           </div>\n           <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n         <a href=\"../installation.html\" class=\"btn btn-neutral float-left\" title=\"Installation\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"../../python-api/triton.html\" class=\"btn btn-neutral float-right\" title=\"triton\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+        <a href=\"01-vector-add.html\" class=\"btn btn-neutral float-right\" title=\"Vector Addition\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n     </div>\n \n   <hr/>\n@@ -129,20 +165,7 @@ <h1>Tutorials<a class=\"headerlink\" href=\"#tutorials\" title=\"Permalink to this he\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"index.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/getting-started/tutorials/sg_execution_times.html", "status": "added", "additions": 156, "deletions": 0, "changes": 156, "file_content_changes": "@@ -0,0 +1,156 @@\n+<!DOCTYPE html>\n+<html class=\"writer-html5\" lang=\"en\" >\n+<head>\n+  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n+\n+  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n+  <title>Computation times &mdash; Triton  documentation</title>\n+      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n+  <!--[if lt IE 9]>\n+    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n+  <![endif]-->\n+  \n+        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n+        <script src=\"../../_static/doctools.js\"></script>\n+        <script src=\"../../_static/sphinx_highlight.js\"></script>\n+    <script src=\"../../_static/js/theme.js\"></script>\n+    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n+    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" /> \n+</head>\n+\n+<body class=\"wy-body-for-nav\"> \n+  <div class=\"wy-grid-for-nav\">\n+    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n+      <div class=\"wy-side-scroll\">\n+        <div class=\"wy-side-nav-search\" >\n+\n+          \n+          \n+          <a href=\"../../index.html\" class=\"icon icon-home\">\n+            Triton\n+          </a>\n+<div role=\"search\">\n+  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n+    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n+    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n+    <input type=\"hidden\" name=\"area\" value=\"default\" />\n+  </form>\n+</div>\n+        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n+              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../installation.html\">Installation</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"index.html\">Tutorials</a></li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n+</ul>\n+\n+        </div>\n+      </div>\n+    </nav>\n+\n+    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n+          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n+          <a href=\"../../index.html\">Triton</a>\n+      </nav>\n+\n+      <div class=\"wy-nav-content\">\n+        <div class=\"rst-content\">\n+          <div role=\"navigation\" aria-label=\"Page navigation\">\n+  <ul class=\"wy-breadcrumbs\">\n+      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n+      <li class=\"breadcrumb-item active\">Computation times</li>\n+      <li class=\"wy-breadcrumbs-aside\">\n+            <a href=\"../../_sources/getting-started/tutorials/sg_execution_times.rst.txt\" rel=\"nofollow\"> View page source</a>\n+      </li>\n+  </ul>\n+  <hr/>\n+</div>\n+          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n+           <div itemprop=\"articleBody\">\n+             \n+  <section id=\"computation-times\">\n+<span id=\"sphx-glr-getting-started-tutorials-sg-execution-times\"></span><h1>Computation times<a class=\"headerlink\" href=\"#computation-times\" title=\"Permalink to this heading\">\u00b6</a></h1>\n+<p><strong>02:16.367</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n+<table class=\"docutils align-default\">\n+<tbody>\n+<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"03-matrix-multiplication.html#sphx-glr-getting-started-tutorials-03-matrix-multiplication-py\"><span class=\"std std-ref\">Matrix Multiplication</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">03-matrix-multiplication.py</span></code>)</p></td>\n+<td><p>00:40.563</p></td>\n+<td><p>0.0 MB</p></td>\n+</tr>\n+<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py\"><span class=\"std std-ref\">Fused Softmax</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">02-fused-softmax.py</span></code>)</p></td>\n+<td><p>00:38.788</p></td>\n+<td><p>0.0 MB</p></td>\n+</tr>\n+<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"05-layer-norm.html#sphx-glr-getting-started-tutorials-05-layer-norm-py\"><span class=\"std std-ref\">Layer Normalization</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">05-layer-norm.py</span></code>)</p></td>\n+<td><p>00:29.379</p></td>\n+<td><p>0.0 MB</p></td>\n+</tr>\n+<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"06-fused-attention.html#sphx-glr-getting-started-tutorials-06-fused-attention-py\"><span class=\"std std-ref\">Fused Attention</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">06-fused-attention.py</span></code>)</p></td>\n+<td><p>00:14.839</p></td>\n+<td><p>0.0 MB</p></td>\n+</tr>\n+<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"08-experimental-block-pointer.html#sphx-glr-getting-started-tutorials-08-experimental-block-pointer-py\"><span class=\"std std-ref\">Block Pointer (Experimental)</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">08-experimental-block-pointer.py</span></code>)</p></td>\n+<td><p>00:06.243</p></td>\n+<td><p>0.0 MB</p></td>\n+</tr>\n+<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py\"><span class=\"std std-ref\">Vector Addition</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">01-vector-add.py</span></code>)</p></td>\n+<td><p>00:05.689</p></td>\n+<td><p>0.0 MB</p></td>\n+</tr>\n+<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"04-low-memory-dropout.html#sphx-glr-getting-started-tutorials-04-low-memory-dropout-py\"><span class=\"std std-ref\">Low-Memory Dropout</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">04-low-memory-dropout.py</span></code>)</p></td>\n+<td><p>00:00.642</p></td>\n+<td><p>0.0 MB</p></td>\n+</tr>\n+<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"07-math-functions.html#sphx-glr-getting-started-tutorials-07-math-functions-py\"><span class=\"std std-ref\">Libdevice (tl.math) function</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">07-math-functions.py</span></code>)</p></td>\n+<td><p>00:00.224</p></td>\n+<td><p>0.0 MB</p></td>\n+</tr>\n+</tbody>\n+</table>\n+</section>\n+\n+\n+           </div>\n+          </div>\n+          <footer>\n+\n+  <hr/>\n+\n+  <div role=\"contentinfo\">\n+    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n+  </div>\n+\n+  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n+    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n+    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n+   \n+\n+</footer>\n+        </div>\n+      </div>\n+    </section>\n+  </div>\n+  <script>\n+      jQuery(function () {\n+          SphinxRtdTheme.Navigation.enable(true);\n+      });\n+  </script> \n+\n+</body>\n+</html>\n\\ No newline at end of file"}, {"filename": "main/index.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -142,20 +142,7 @@ <h2>Going Further<a class=\"headerlink\" href=\"#going-further\" title=\"Permalink to\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"index.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/objects.inv", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/programming-guide/chapter-1/introduction.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -237,20 +237,7 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"introduction.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/programming-guide/chapter-2/related-work.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -369,20 +369,7 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"related-work.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.Config.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -161,20 +161,7 @@ <h1>triton.Config<a class=\"headerlink\" href=\"#triton-config\" title=\"Permalink to\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.Config.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.autotune.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -163,20 +163,7 @@ <h1>triton.autotune<a class=\"headerlink\" href=\"#triton-autotune\" title=\"Permalin\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.autotune.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.heuristics.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -143,20 +143,7 @@ <h1>triton.heuristics<a class=\"headerlink\" href=\"#triton-heuristics\" title=\"Perm\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.heuristics.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.jit.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -151,20 +151,7 @@ <h1>triton.jit<a class=\"headerlink\" href=\"#triton-jit\" title=\"Permalink to this\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.jit.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.abs.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -158,20 +158,7 @@ <h1>triton.language.abs<a class=\"headerlink\" href=\"#triton-language-abs\" title=\"\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.abs.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.arange.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -155,20 +155,7 @@ <h1>triton.language.arange<a class=\"headerlink\" href=\"#triton-language-arange\" t\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.arange.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.argmax.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -159,20 +159,7 @@ <h1>triton.language.argmax<a class=\"headerlink\" href=\"#triton-language-argmax\" t\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.argmax.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.argmin.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -159,20 +159,7 @@ <h1>triton.language.argmin<a class=\"headerlink\" href=\"#triton-language-argmin\" t\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.argmin.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.associative_scan.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -155,20 +155,7 @@ <h1>triton.language.associative_scan<a class=\"headerlink\" href=\"#triton-language\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.associative_scan.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.atomic_add.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -157,20 +157,7 @@ <h1>triton.language.atomic_add<a class=\"headerlink\" href=\"#triton-language-atomi\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.atomic_add.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.atomic_cas.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -166,20 +166,7 @@ <h1>triton.language.atomic_cas<a class=\"headerlink\" href=\"#triton-language-atomi\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.atomic_cas.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.atomic_max.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -157,20 +157,7 @@ <h1>triton.language.atomic_max<a class=\"headerlink\" href=\"#triton-language-atomi\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.atomic_max.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.atomic_min.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -157,20 +157,7 @@ <h1>triton.language.atomic_min<a class=\"headerlink\" href=\"#triton-language-atomi\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.atomic_min.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.atomic_xchg.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -157,20 +157,7 @@ <h1>triton.language.atomic_xchg<a class=\"headerlink\" href=\"#triton-language-atom\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.atomic_xchg.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.broadcast.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -158,20 +158,7 @@ <h1>triton.language.broadcast<a class=\"headerlink\" href=\"#triton-language-broadc\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.broadcast.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.broadcast_to.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -158,20 +158,7 @@ <h1>triton.language.broadcast_to<a class=\"headerlink\" href=\"#triton-language-bro\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.broadcast_to.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.cat.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -159,20 +159,7 @@ <h1>triton.language.cat<a class=\"headerlink\" href=\"#triton-language-cat\" title=\"\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.cat.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.cos.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -158,20 +158,7 @@ <h1>triton.language.cos<a class=\"headerlink\" href=\"#triton-language-cos\" title=\"\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.cos.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.cumprod.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -154,20 +154,7 @@ <h1>triton.language.cumprod<a class=\"headerlink\" href=\"#triton-language-cumprod\"\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.cumprod.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.cumsum.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -154,20 +154,7 @@ <h1>triton.language.cumsum<a class=\"headerlink\" href=\"#triton-language-cumsum\" t\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.cumsum.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.debug_barrier.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -147,20 +147,7 @@ <h1>triton.language.debug_barrier<a class=\"headerlink\" href=\"#triton-language-de\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.debug_barrier.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.device_assert.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -163,20 +163,7 @@ <h1>triton.language.device_assert<a class=\"headerlink\" href=\"#triton-language-de\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.device_assert.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.device_print.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -163,20 +163,7 @@ <h1>triton.language.device_print<a class=\"headerlink\" href=\"#triton-language-dev\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.device_print.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.dot.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -153,20 +153,7 @@ <h1>triton.language.dot<a class=\"headerlink\" href=\"#triton-language-dot\" title=\"\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.dot.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.exp.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -158,20 +158,7 @@ <h1>triton.language.exp<a class=\"headerlink\" href=\"#triton-language-exp\" title=\"\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.exp.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.expand_dims.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -160,20 +160,7 @@ <h1>triton.language.expand_dims<a class=\"headerlink\" href=\"#triton-language-expa\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.expand_dims.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.fdiv.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -152,20 +152,7 @@ <h1>triton.language.fdiv<a class=\"headerlink\" href=\"#triton-language-fdiv\" title\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.fdiv.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.full.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -158,20 +158,7 @@ <h1>triton.language.full<a class=\"headerlink\" href=\"#triton-language-full\" title\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.full.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.load.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -177,20 +177,7 @@ <h1>triton.language.load<a class=\"headerlink\" href=\"#triton-language-load\" title\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.load.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.log.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -158,20 +158,7 @@ <h1>triton.language.log<a class=\"headerlink\" href=\"#triton-language-log\" title=\"\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.log.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.max.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -160,20 +160,7 @@ <h1>triton.language.max<a class=\"headerlink\" href=\"#triton-language-max\" title=\"\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.max.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.max_constancy.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -149,20 +149,7 @@ <h1>triton.language.max_constancy<a class=\"headerlink\" href=\"#triton-language-ma\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.max_constancy.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.max_contiguous.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -147,20 +147,7 @@ <h1>triton.language.max_contiguous<a class=\"headerlink\" href=\"#triton-language-m\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.max_contiguous.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.maximum.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -153,20 +153,7 @@ <h1>triton.language.maximum<a class=\"headerlink\" href=\"#triton-language-maximum\"\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.maximum.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.min.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -160,20 +160,7 @@ <h1>triton.language.min<a class=\"headerlink\" href=\"#triton-language-min\" title=\"\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.min.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.minimum.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -153,20 +153,7 @@ <h1>triton.language.minimum<a class=\"headerlink\" href=\"#triton-language-minimum\"\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.minimum.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.multiple_of.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -154,20 +154,7 @@ <h1>triton.language.multiple_of<a class=\"headerlink\" href=\"#triton-language-mult\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.multiple_of.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.num_programs.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -150,20 +150,7 @@ <h1>triton.language.num_programs<a class=\"headerlink\" href=\"#triton-language-num\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.num_programs.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.program_id.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -150,20 +150,7 @@ <h1>triton.language.program_id<a class=\"headerlink\" href=\"#triton-language-progr\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.program_id.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.rand.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -157,20 +157,7 @@ <h1>triton.language.rand<a class=\"headerlink\" href=\"#triton-language-rand\" title\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.rand.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.randint.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -158,20 +158,7 @@ <h1>triton.language.randint<a class=\"headerlink\" href=\"#triton-language-randint\"\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.randint.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.randint4x.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -158,20 +158,7 @@ <h1>triton.language.randint4x<a class=\"headerlink\" href=\"#triton-language-randin\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.randint4x.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.randn.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -157,20 +157,7 @@ <h1>triton.language.randn<a class=\"headerlink\" href=\"#triton-language-randn\" tit\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.randn.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.ravel.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -155,20 +155,7 @@ <h1>triton.language.ravel<a class=\"headerlink\" href=\"#triton-language-ravel\" tit\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.ravel.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.reduce.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -159,20 +159,7 @@ <h1>triton.language.reduce<a class=\"headerlink\" href=\"#triton-language-reduce\" t\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.reduce.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.reshape.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -149,20 +149,7 @@ <h1>triton.language.reshape<a class=\"headerlink\" href=\"#triton-language-reshape\"\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.reshape.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.sigmoid.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -158,20 +158,7 @@ <h1>triton.language.sigmoid<a class=\"headerlink\" href=\"#triton-language-sigmoid\"\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.sigmoid.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.sin.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -158,20 +158,7 @@ <h1>triton.language.sin<a class=\"headerlink\" href=\"#triton-language-sin\" title=\"\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.sin.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.softmax.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -158,20 +158,7 @@ <h1>triton.language.softmax<a class=\"headerlink\" href=\"#triton-language-softmax\"\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.softmax.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.sqrt.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -158,20 +158,7 @@ <h1>triton.language.sqrt<a class=\"headerlink\" href=\"#triton-language-sqrt\" title\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.sqrt.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.static_assert.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -151,20 +151,7 @@ <h1>triton.language.static_assert<a class=\"headerlink\" href=\"#triton-language-st\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.static_assert.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.static_print.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -152,20 +152,7 @@ <h1>triton.language.static_print<a class=\"headerlink\" href=\"#triton-language-sta\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.static_print.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.static_range.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -177,20 +177,7 @@ <h1>triton.language.static_range<a class=\"headerlink\" href=\"#triton-language-sta\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.static_range.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.store.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -173,20 +173,7 @@ <h1>triton.language.store<a class=\"headerlink\" href=\"#triton-language-store\" tit\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.store.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.sum.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -158,20 +158,7 @@ <h1>triton.language.sum<a class=\"headerlink\" href=\"#triton-language-sum\" title=\"\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.sum.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.trans.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -149,20 +149,7 @@ <h1>triton.language.trans<a class=\"headerlink\" href=\"#triton-language-trans\" tit\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.trans.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.umulhi.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -152,20 +152,7 @@ <h1>triton.language.umulhi<a class=\"headerlink\" href=\"#triton-language-umulhi\" t\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.umulhi.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.view.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -159,20 +159,7 @@ <h1>triton.language.view<a class=\"headerlink\" href=\"#triton-language-view\" title\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.view.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.where.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -157,20 +157,7 @@ <h1>triton.language.where<a class=\"headerlink\" href=\"#triton-language-where\" tit\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.where.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.xor_sum.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -158,20 +158,7 @@ <h1>triton.language.xor_sum<a class=\"headerlink\" href=\"#triton-language-xor-sum\"\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.xor_sum.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.language.zeros.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -155,20 +155,7 @@ <h1>triton.language.zeros<a class=\"headerlink\" href=\"#triton-language-zeros\" tit\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.zeros.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.testing.Benchmark.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -167,20 +167,7 @@ <h1>triton.testing.Benchmark<a class=\"headerlink\" href=\"#triton-testing-benchmar\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.testing.Benchmark.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.testing.do_bench.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -128,20 +128,7 @@ <h1>triton.testing.do_bench<a class=\"headerlink\" href=\"#triton-testing-do-bench\"\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.testing.do_bench.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/generated/triton.testing.perf_report.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -134,20 +134,7 @@ <h1>triton.testing.perf_report<a class=\"headerlink\" href=\"#triton-testing-perf-r\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.testing.perf_report.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/triton.html", "status": "modified", "additions": 3, "deletions": 16, "changes": 19, "file_content_changes": "@@ -23,7 +23,7 @@\n     <link rel=\"index\" title=\"Index\" href=\"../genindex.html\" />\n     <link rel=\"search\" title=\"Search\" href=\"../search.html\" />\n     <link rel=\"next\" title=\"triton.jit\" href=\"generated/triton.jit.html\" />\n-    <link rel=\"prev\" title=\"Tutorials\" href=\"../getting-started/tutorials/index.html\" /> \n+    <link rel=\"prev\" title=\"Block Pointer (Experimental)\" href=\"../getting-started/tutorials/08-experimental-block-pointer.html\" /> \n </head>\n \n <body class=\"wy-body-for-nav\"> \n@@ -116,7 +116,7 @@ <h1>triton<a class=\"headerlink\" href=\"#triton\" title=\"Permalink to this heading\"\n            </div>\n           </div>\n           <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"../getting-started/tutorials/index.html\" class=\"btn btn-neutral float-left\" title=\"Tutorials\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"../getting-started/tutorials/08-experimental-block-pointer.html\" class=\"btn btn-neutral float-left\" title=\"Block Pointer (Experimental)\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n         <a href=\"generated/triton.jit.html\" class=\"btn btn-neutral float-right\" title=\"triton.jit\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n     </div>\n \n@@ -136,20 +136,7 @@ <h1>triton<a class=\"headerlink\" href=\"#triton\" title=\"Permalink to this heading\"\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/triton.language.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -503,20 +503,7 @@ <h2>Iterators<a class=\"headerlink\" href=\"#iterators\" title=\"Permalink to this he\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.language.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/python-api/triton.testing.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -132,20 +132,7 @@ <h1>triton.testing<a class=\"headerlink\" href=\"#triton-testing\" title=\"Permalink\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"triton.testing.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/search.html", "status": "modified", "additions": 1, "deletions": 14, "changes": 15, "file_content_changes": "@@ -118,20 +118,7 @@\n       </div>\n     </section>\n   </div>\n-  \n-<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n-    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n-        <span class=\"fa fa-book\"> Other Versions</span>\n-        v: main\n-        <span class=\"fa fa-caret-down\"></span>\n-    </span>\n-    <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Branches</dt>\n-            <dd><a href=\"search.html\">main</a></dd>\n-        </dl>\n-    </div>\n-</div><script>\n+  <script>\n       jQuery(function () {\n           SphinxRtdTheme.Navigation.enable(true);\n       });"}, {"filename": "main/searchindex.js", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "N/A"}]