[{"filename": "main/.doctrees/environment.pickle", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/01-vector-add.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/02-fused-softmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/03-matrix-multiplication.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/04-low-memory-dropout.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/05-layer-norm.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/06-fused-attention.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/08-experimental-block-pointer.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/sg_execution_times.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/662999063954282841dc90b8945f85ce/tutorials_jupyter.zip", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/763344228ae6bc253ed1a6cf586aa30d/tutorials_python.zip", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_01-vector-add_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_01-vector-add_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_02-fused-softmax_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_02-fused-softmax_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_03-matrix-multiplication_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_03-matrix-multiplication_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_05-layer-norm_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_05-layer-norm_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_002.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_003.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_004.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_sources/getting-started/tutorials/01-vector-add.rst.txt", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -237,7 +237,7 @@ We can now run the decorated function above. Pass `print_data=True` to see the p\n     vector-add-performance:\n                size       Triton        Torch\n     0        4096.0     8.000000     9.600000\n-    1        8192.0    19.200000    19.200000\n+    1        8192.0    15.999999    19.200000\n     2       16384.0    31.999999    31.999999\n     3       32768.0    63.999998    63.999998\n     4       65536.0   127.999995   127.999995\n@@ -246,20 +246,20 @@ We can now run the decorated function above. Pass `print_data=True` to see the p\n     7      524288.0   614.400016   614.400016\n     8     1048576.0   819.200021   819.200021\n     9     2097152.0  1023.999964  1023.999964\n-    10    4194304.0  1260.307736  1228.800031\n+    10    4194304.0  1228.800031  1228.800031\n     11    8388608.0  1424.695621  1424.695621\n     12   16777216.0  1560.380965  1560.380965\n-    13   33554432.0  1631.601649  1624.859540\n+    13   33554432.0  1624.859540  1624.859540\n     14   67108864.0  1669.706983  1662.646960\n-    15  134217728.0  1685.813499  1680.410210\n+    15  134217728.0  1685.813499  1679.513027\n \n \n \n \n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  4.567 seconds)\n+   **Total running time of the script:** ( 0 minutes  4.614 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_01-vector-add.py:"}, {"filename": "main/_sources/getting-started/tutorials/02-fused-softmax.rst.txt", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -286,17 +286,17 @@ We will then compare its performance against (1) :code:`torch.softmax` and (2) t\n \n     softmax-performance:\n               N       Triton  Torch (native)  Torch (jit)\n-    0     256.0   682.666643      744.727267   260.063494\n-    1     384.0   877.714274      877.714274   315.076934\n-    2     512.0   910.222190      910.222190   341.333321\n-    3     640.0  1024.000026      930.909084   372.363633\n-    4     768.0  1117.090900     1023.999964   384.000001\n+    0     256.0   682.666643      682.666643   256.000001\n+    1     384.0   877.714274      877.714274   307.200008\n+    2     512.0   910.222190      963.764689   334.367358\n+    3     640.0  1024.000026      975.238103   372.363633\n+    4     768.0  1068.521715     1023.999964   384.000001\n     ..      ...          ...             ...          ...\n-    93  12160.0  1601.316858     1069.010969   464.343688\n+    93  12160.0  1601.316858     1071.955925   464.343688\n     94  12288.0  1604.963246     1018.694301   464.245579\n-    95  12416.0  1595.630495     1034.666630   461.454135\n-    96  12544.0  1599.235121     1018.802024   462.984993\n-    97  12672.0  1596.472358     1008.716405   462.904122\n+    95  12416.0  1602.064538     1031.979242   461.990713\n+    96  12544.0  1599.235121     1018.802024   463.519622\n+    97  12672.0  1596.472358     1006.213368   462.376291\n \n     [98 rows x 4 columns]\n \n@@ -313,7 +313,7 @@ In the above plot, we can see that:\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  36.893 seconds)\n+   **Total running time of the script:** ( 0 minutes  36.937 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:"}, {"filename": "main/_sources/getting-started/tutorials/03-matrix-multiplication.rst.txt", "status": "modified", "additions": 19, "deletions": 19, "changes": 38, "file_content_changes": "@@ -450,7 +450,7 @@ but feel free to arrange this script as you wish to benchmark any other matrix s\n     matmul-performance:\n              M       N       K      cuBLAS      Triton\n     0    256.0   256.0   256.0    4.096000    4.096000\n-    1    384.0   384.0   384.0   12.288000   12.288000\n+    1    384.0   384.0   384.0   11.059200   12.288000\n     2    512.0   512.0   512.0   26.214401   23.831273\n     3    640.0   640.0   640.0   42.666665   39.384616\n     4    768.0   768.0   768.0   68.056616   58.982401\n@@ -460,34 +460,34 @@ but feel free to arrange this script as you wish to benchmark any other matrix s\n     8   1280.0  1280.0  1280.0  163.840004  163.840004\n     9   1408.0  1408.0  1408.0  155.765024  132.970149\n     10  1536.0  1536.0  1536.0  181.484314  157.286398\n-    11  1664.0  1664.0  1664.0  183.651271  179.978245\n+    11  1664.0  1664.0  1664.0  179.978245  179.978245\n     12  1792.0  1792.0  1792.0  172.914215  208.137481\n     13  1920.0  1920.0  1920.0  203.294114  168.585369\n-    14  2048.0  2048.0  2048.0  226.719125  192.841562\n+    14  2048.0  2048.0  2048.0  223.696203  192.841562\n     15  2176.0  2176.0  2176.0  211.827867  211.827867\n-    16  2304.0  2304.0  2304.0  219.154788  227.503545\n-    17  2432.0  2432.0  2432.0  203.583068  203.583068\n-    18  2560.0  2560.0  2560.0  224.438347  215.578957\n-    19  2688.0  2688.0  2688.0  197.567993  198.602388\n-    20  2816.0  2816.0  2816.0  210.696652  210.696652\n-    21  2944.0  2944.0  2944.0  221.493479  223.479969\n-    22  3072.0  3072.0  3072.0  209.715208  207.410628\n-    23  3200.0  3200.0  3200.0  214.046818  216.216207\n-    24  3328.0  3328.0  3328.0  205.689424  209.277023\n-    25  3456.0  3456.0  3456.0  216.143621  216.143621\n-    26  3584.0  3584.0  3584.0  218.772251  208.137481\n-    27  3712.0  3712.0  3712.0  210.310194  220.038206\n-    28  3840.0  3840.0  3840.0  206.328356  211.053446\n-    29  3968.0  3968.0  3968.0  210.023986  217.899880\n-    30  4096.0  4096.0  4096.0  219.310012  217.180793\n+    16  2304.0  2304.0  2304.0  229.691080  229.691080\n+    17  2432.0  2432.0  2432.0  203.583068  202.118452\n+    18  2560.0  2560.0  2560.0  225.986210  218.453323\n+    19  2688.0  2688.0  2688.0  198.602388  196.544332\n+    20  2816.0  2816.0  2816.0  209.683695  211.719459\n+    21  2944.0  2944.0  2944.0  223.479969  223.479969\n+    22  3072.0  3072.0  3072.0  207.410628  208.941345\n+    23  3200.0  3200.0  3200.0  214.046818  219.178074\n+    24  3328.0  3328.0  3328.0  204.520726  206.871539\n+    25  3456.0  3456.0  3456.0  216.143621  216.724640\n+    26  3584.0  3584.0  3584.0  219.305830  205.756041\n+    27  3712.0  3712.0  3712.0  210.089046  217.168134\n+    28  3840.0  3840.0  3840.0  208.271176  208.271176\n+    29  3968.0  3968.0  3968.0  208.231980  215.971570\n+    30  4096.0  4096.0  4096.0  219.310012  216.829933\n \n \n \n \n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  40.572 seconds)\n+   **Total running time of the script:** ( 0 minutes  40.581 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_03-matrix-multiplication.py:"}, {"filename": "main/_sources/getting-started/tutorials/04-low-memory-dropout.rst.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -242,7 +242,7 @@ References\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  0.643 seconds)\n+   **Total running time of the script:** ( 0 minutes  0.635 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_04-low-memory-dropout.py:"}, {"filename": "main/_sources/getting-started/tutorials/05-layer-norm.rst.txt", "status": "modified", "additions": 25, "deletions": 25, "changes": 50, "file_content_changes": "@@ -433,36 +433,36 @@ Specifically, one can set :code:`'mode': 'backward'` to benchmark the backward p\n \n     layer-norm-backward:\n               N       Triton       Torch\n-    0    1024.0   143.719292  372.363633\n-    1    1536.0   215.578939  438.857146\n-    2    2048.0   290.840226  496.484863\n-    3    2560.0   357.209302  534.260858\n-    4    3072.0   431.157877  542.117638\n-    5    3584.0   505.976473  467.478250\n-    6    4096.0   581.680452  474.898540\n-    7    4608.0   617.832419  480.834772\n-    8    5120.0   678.895043  483.779502\n-    9    5632.0   750.933322  491.520003\n-    10   6144.0   814.674052  496.484863\n-    11   6656.0   882.563556  500.764869\n-    12   7168.0   945.230752  476.542919\n-    13   7680.0   980.425504  481.253256\n+    0    1024.0   139.636363  372.363633\n+    1    1536.0   210.651436  433.694119\n+    2    2048.0   280.868581  496.484863\n+    3    2560.0   351.085727  529.655159\n+    4    3072.0   423.724136  538.160602\n+    5    3584.0   497.202332  470.032796\n+    6    4096.0   568.231237  474.898540\n+    7    4608.0   597.794604  480.834772\n+    8    5120.0   664.216226  483.779502\n+    9    5632.0   734.608679  489.739120\n+    10   6144.0   801.391287  494.818794\n+    11   6656.0   868.173894  499.200013\n+    12   7168.0   940.065592  476.542919\n+    13   7680.0   975.238103  479.999983\n     14   8192.0  1008.246151  487.861027\n-    15   8704.0   673.858058  489.217808\n+    15   8704.0   671.691295  488.074767\n     16   9216.0   702.171402  491.520008\n-    17   9728.0   725.068307  496.748937\n+    17   9728.0   722.823562  497.808111\n     18  10240.0   753.865011  499.512174\n-    19  10752.0   781.963613  485.052653\n-    20  11264.0   809.389194  488.853509\n-    21  11776.0   826.385931  493.235604\n-    22  12288.0   840.205140  500.699488\n-    23  12800.0   848.618804  501.141916\n-    24  13312.0   865.821169  499.981239\n+    19  10752.0   779.601236  485.052653\n+    20  11264.0   809.389194  489.739120\n+    21  11776.0   823.976695  493.235604\n+    22  12288.0   840.205140  499.850839\n+    23  12800.0   846.280994  500.325718\n+    24  13312.0   863.481094  499.981239\n     25  13824.0   859.523317  501.172223\n     26  14336.0   875.480928  493.635574\n-    27  14848.0   882.059425  496.311981\n+    27  14848.0   879.881484  497.004192\n     28  15360.0   894.757295  501.551014\n-    29  15872.0   894.197155  502.543530\n+    29  15872.0   894.197155  503.873020\n \n \n \n@@ -477,7 +477,7 @@ References\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  29.521 seconds)\n+   **Total running time of the script:** ( 0 minutes  29.371 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_05-layer-norm.py:"}, {"filename": "main/_sources/getting-started/tutorials/06-fused-attention.rst.txt", "status": "modified", "additions": 21, "deletions": 21, "changes": 42, "file_content_changes": "@@ -70,32 +70,32 @@ Extra Credits:\n \n     fused-attention-batch4-head48-d64-fwd:\n          N_CTX      Triton\n-    0   1024.0  158.542779\n-    1   2048.0  166.913790\n-    2   4096.0  174.440894\n-    3   8192.0  177.052682\n-    4  16384.0  176.251992\n+    0   1024.0  159.091886\n+    1   2048.0  168.801871\n+    2   4096.0  175.516831\n+    3   8192.0  177.132493\n+    4  16384.0  178.305654\n     fused-attention-batch4-head48-d64-fwd:\n          N_CTX      Triton\n-    0   1024.0  115.066030\n-    1   2048.0  132.931935\n-    2   4096.0  147.031047\n-    3   8192.0  154.580880\n-    4  16384.0  152.952855\n+    0   1024.0  116.224800\n+    1   2048.0  134.780294\n+    2   4096.0  149.303213\n+    3   8192.0  154.727745\n+    4  16384.0  152.478642\n     fused-attention-batch4-head48-d64-bwd:\n          N_CTX     Triton\n-    0   1024.0  72.145583\n-    1   2048.0  81.558058\n-    2   4096.0  86.495359\n-    3   8192.0  87.505720\n-    4  16384.0  90.256695\n+    0   1024.0  72.461349\n+    1   2048.0  81.487631\n+    2   4096.0  87.046972\n+    3   8192.0  88.701852\n+    4  16384.0  90.644526\n     fused-attention-batch4-head48-d64-bwd:\n          N_CTX     Triton\n-    0   1024.0  50.602755\n-    1   2048.0  62.675440\n-    2   4096.0  70.089209\n-    3   8192.0  75.064446\n-    4  16384.0  77.221686\n+    0   1024.0  50.614484\n+    1   2048.0  62.978218\n+    2   4096.0  70.237556\n+    3   8192.0  74.952661\n+    4  16384.0  77.275410\n \n \n \n@@ -511,7 +511,7 @@ Extra Credits:\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  14.835 seconds)\n+   **Total running time of the script:** ( 0 minutes  14.841 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_06-fused-attention.py:"}, {"filename": "main/_sources/getting-started/tutorials/08-experimental-block-pointer.rst.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -298,7 +298,7 @@ Still we can test our matrix multiplication with block pointers against a native\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  6.286 seconds)\n+   **Total running time of the script:** ( 0 minutes  6.297 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_08-experimental-block-pointer.py:"}, {"filename": "main/_sources/getting-started/tutorials/sg_execution_times.rst.txt", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -6,22 +6,22 @@\n \n Computation times\n =================\n-**02:13.541** total execution time for **getting-started_tutorials** files:\n+**02:13.500** total execution time for **getting-started_tutorials** files:\n \n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``)           | 00:40.572 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``)           | 00:40.581 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                           | 00:36.893 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                           | 00:36.937 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                                 | 00:29.521 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                                 | 00:29.371 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py` (``06-fused-attention.py``)                       | 00:14.835 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py` (``06-fused-attention.py``)                       | 00:14.841 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_08-experimental-block-pointer.py` (``08-experimental-block-pointer.py``) | 00:06.286 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_08-experimental-block-pointer.py` (``08-experimental-block-pointer.py``) | 00:06.297 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                                 | 00:04.567 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                                 | 00:04.614 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)                 | 00:00.643 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)                 | 00:00.635 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n | :ref:`sphx_glr_getting-started_tutorials_07-math-functions.py` (``07-math-functions.py``)                         | 00:00.223 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+"}, {"filename": "main/getting-started/tutorials/01-vector-add.html", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -236,7 +236,7 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n <img src=\"../../_images/sphx_glr_01-vector-add_001.png\" srcset=\"../../_images/sphx_glr_01-vector-add_001.png\" alt=\"01 vector add\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>vector-add-performance:\n            size       Triton        Torch\n 0        4096.0     8.000000     9.600000\n-1        8192.0    19.200000    19.200000\n+1        8192.0    15.999999    19.200000\n 2       16384.0    31.999999    31.999999\n 3       32768.0    63.999998    63.999998\n 4       65536.0   127.999995   127.999995\n@@ -245,15 +245,15 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n 7      524288.0   614.400016   614.400016\n 8     1048576.0   819.200021   819.200021\n 9     2097152.0  1023.999964  1023.999964\n-10    4194304.0  1260.307736  1228.800031\n+10    4194304.0  1228.800031  1228.800031\n 11    8388608.0  1424.695621  1424.695621\n 12   16777216.0  1560.380965  1560.380965\n-13   33554432.0  1631.601649  1624.859540\n+13   33554432.0  1624.859540  1624.859540\n 14   67108864.0  1669.706983  1662.646960\n-15  134217728.0  1685.813499  1680.410210\n+15  134217728.0  1685.813499  1679.513027\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  4.567 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  4.614 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-01-vector-add-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/62d97d49a32414049819dd8bb8378080/01-vector-add.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">01-vector-add.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/02-fused-softmax.html", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -282,17 +282,17 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n </div>\n <img src=\"../../_images/sphx_glr_02-fused-softmax_001.png\" srcset=\"../../_images/sphx_glr_02-fused-softmax_001.png\" alt=\"02 fused softmax\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>softmax-performance:\n           N       Triton  Torch (native)  Torch (jit)\n-0     256.0   682.666643      744.727267   260.063494\n-1     384.0   877.714274      877.714274   315.076934\n-2     512.0   910.222190      910.222190   341.333321\n-3     640.0  1024.000026      930.909084   372.363633\n-4     768.0  1117.090900     1023.999964   384.000001\n+0     256.0   682.666643      682.666643   256.000001\n+1     384.0   877.714274      877.714274   307.200008\n+2     512.0   910.222190      963.764689   334.367358\n+3     640.0  1024.000026      975.238103   372.363633\n+4     768.0  1068.521715     1023.999964   384.000001\n ..      ...          ...             ...          ...\n-93  12160.0  1601.316858     1069.010969   464.343688\n+93  12160.0  1601.316858     1071.955925   464.343688\n 94  12288.0  1604.963246     1018.694301   464.245579\n-95  12416.0  1595.630495     1034.666630   461.454135\n-96  12544.0  1599.235121     1018.802024   462.984993\n-97  12672.0  1596.472358     1008.716405   462.904122\n+95  12416.0  1602.064538     1031.979242   461.990713\n+96  12544.0  1599.235121     1018.802024   463.519622\n+97  12672.0  1596.472358     1006.213368   462.376291\n \n [98 rows x 4 columns]\n </pre></div>\n@@ -305,7 +305,7 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n </ul>\n </dd>\n </dl>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  36.893 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  36.937 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-02-fused-softmax-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d91442ac2982c4e0cc3ab0f43534afbc/02-fused-softmax.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">02-fused-softmax.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/03-matrix-multiplication.html", "status": "modified", "additions": 19, "deletions": 19, "changes": 38, "file_content_changes": "@@ -464,7 +464,7 @@ <h3>Square Matrix Performance<a class=\"headerlink\" href=\"#square-matrix-performa\n <img src=\"../../_images/sphx_glr_03-matrix-multiplication_001.png\" srcset=\"../../_images/sphx_glr_03-matrix-multiplication_001.png\" alt=\"03 matrix multiplication\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>matmul-performance:\n          M       N       K      cuBLAS      Triton\n 0    256.0   256.0   256.0    4.096000    4.096000\n-1    384.0   384.0   384.0   12.288000   12.288000\n+1    384.0   384.0   384.0   11.059200   12.288000\n 2    512.0   512.0   512.0   26.214401   23.831273\n 3    640.0   640.0   640.0   42.666665   39.384616\n 4    768.0   768.0   768.0   68.056616   58.982401\n@@ -474,29 +474,29 @@ <h3>Square Matrix Performance<a class=\"headerlink\" href=\"#square-matrix-performa\n 8   1280.0  1280.0  1280.0  163.840004  163.840004\n 9   1408.0  1408.0  1408.0  155.765024  132.970149\n 10  1536.0  1536.0  1536.0  181.484314  157.286398\n-11  1664.0  1664.0  1664.0  183.651271  179.978245\n+11  1664.0  1664.0  1664.0  179.978245  179.978245\n 12  1792.0  1792.0  1792.0  172.914215  208.137481\n 13  1920.0  1920.0  1920.0  203.294114  168.585369\n-14  2048.0  2048.0  2048.0  226.719125  192.841562\n+14  2048.0  2048.0  2048.0  223.696203  192.841562\n 15  2176.0  2176.0  2176.0  211.827867  211.827867\n-16  2304.0  2304.0  2304.0  219.154788  227.503545\n-17  2432.0  2432.0  2432.0  203.583068  203.583068\n-18  2560.0  2560.0  2560.0  224.438347  215.578957\n-19  2688.0  2688.0  2688.0  197.567993  198.602388\n-20  2816.0  2816.0  2816.0  210.696652  210.696652\n-21  2944.0  2944.0  2944.0  221.493479  223.479969\n-22  3072.0  3072.0  3072.0  209.715208  207.410628\n-23  3200.0  3200.0  3200.0  214.046818  216.216207\n-24  3328.0  3328.0  3328.0  205.689424  209.277023\n-25  3456.0  3456.0  3456.0  216.143621  216.143621\n-26  3584.0  3584.0  3584.0  218.772251  208.137481\n-27  3712.0  3712.0  3712.0  210.310194  220.038206\n-28  3840.0  3840.0  3840.0  206.328356  211.053446\n-29  3968.0  3968.0  3968.0  210.023986  217.899880\n-30  4096.0  4096.0  4096.0  219.310012  217.180793\n+16  2304.0  2304.0  2304.0  229.691080  229.691080\n+17  2432.0  2432.0  2432.0  203.583068  202.118452\n+18  2560.0  2560.0  2560.0  225.986210  218.453323\n+19  2688.0  2688.0  2688.0  198.602388  196.544332\n+20  2816.0  2816.0  2816.0  209.683695  211.719459\n+21  2944.0  2944.0  2944.0  223.479969  223.479969\n+22  3072.0  3072.0  3072.0  207.410628  208.941345\n+23  3200.0  3200.0  3200.0  214.046818  219.178074\n+24  3328.0  3328.0  3328.0  204.520726  206.871539\n+25  3456.0  3456.0  3456.0  216.143621  216.724640\n+26  3584.0  3584.0  3584.0  219.305830  205.756041\n+27  3712.0  3712.0  3712.0  210.089046  217.168134\n+28  3840.0  3840.0  3840.0  208.271176  208.271176\n+29  3968.0  3968.0  3968.0  208.231980  215.971570\n+30  4096.0  4096.0  4096.0  219.310012  216.829933\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  40.572 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  40.581 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-03-matrix-multiplication-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d5fee5b55a64e47f1b5724ec39adf171/03-matrix-multiplication.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">03-matrix-multiplication.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/04-low-memory-dropout.html", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -287,7 +287,7 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n <p>Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \u201cDropout: A Simple Way to Prevent Neural Networks from Overfitting\u201d, JMLR 2014</p>\n </div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.643 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.635 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-04-low-memory-dropout-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/c9aed78977a4c05741d675a38dde3d7d/04-low-memory-dropout.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">04-low-memory-dropout.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/05-layer-norm.html", "status": "modified", "additions": 25, "deletions": 25, "changes": 50, "file_content_changes": "@@ -462,36 +462,36 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n </div>\n <img src=\"../../_images/sphx_glr_05-layer-norm_001.png\" srcset=\"../../_images/sphx_glr_05-layer-norm_001.png\" alt=\"05 layer norm\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>layer-norm-backward:\n           N       Triton       Torch\n-0    1024.0   143.719292  372.363633\n-1    1536.0   215.578939  438.857146\n-2    2048.0   290.840226  496.484863\n-3    2560.0   357.209302  534.260858\n-4    3072.0   431.157877  542.117638\n-5    3584.0   505.976473  467.478250\n-6    4096.0   581.680452  474.898540\n-7    4608.0   617.832419  480.834772\n-8    5120.0   678.895043  483.779502\n-9    5632.0   750.933322  491.520003\n-10   6144.0   814.674052  496.484863\n-11   6656.0   882.563556  500.764869\n-12   7168.0   945.230752  476.542919\n-13   7680.0   980.425504  481.253256\n+0    1024.0   139.636363  372.363633\n+1    1536.0   210.651436  433.694119\n+2    2048.0   280.868581  496.484863\n+3    2560.0   351.085727  529.655159\n+4    3072.0   423.724136  538.160602\n+5    3584.0   497.202332  470.032796\n+6    4096.0   568.231237  474.898540\n+7    4608.0   597.794604  480.834772\n+8    5120.0   664.216226  483.779502\n+9    5632.0   734.608679  489.739120\n+10   6144.0   801.391287  494.818794\n+11   6656.0   868.173894  499.200013\n+12   7168.0   940.065592  476.542919\n+13   7680.0   975.238103  479.999983\n 14   8192.0  1008.246151  487.861027\n-15   8704.0   673.858058  489.217808\n+15   8704.0   671.691295  488.074767\n 16   9216.0   702.171402  491.520008\n-17   9728.0   725.068307  496.748937\n+17   9728.0   722.823562  497.808111\n 18  10240.0   753.865011  499.512174\n-19  10752.0   781.963613  485.052653\n-20  11264.0   809.389194  488.853509\n-21  11776.0   826.385931  493.235604\n-22  12288.0   840.205140  500.699488\n-23  12800.0   848.618804  501.141916\n-24  13312.0   865.821169  499.981239\n+19  10752.0   779.601236  485.052653\n+20  11264.0   809.389194  489.739120\n+21  11776.0   823.976695  493.235604\n+22  12288.0   840.205140  499.850839\n+23  12800.0   846.280994  500.325718\n+24  13312.0   863.481094  499.981239\n 25  13824.0   859.523317  501.172223\n 26  14336.0   875.480928  493.635574\n-27  14848.0   882.059425  496.311981\n+27  14848.0   879.881484  497.004192\n 28  15360.0   894.757295  501.551014\n-29  15872.0   894.197155  502.543530\n+29  15872.0   894.197155  503.873020\n </pre></div>\n </div>\n </section>\n@@ -503,7 +503,7 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n <p>Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \u201cLayer Normalization\u201d, Arxiv 2016</p>\n </div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  29.521 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  29.371 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-05-layer-norm-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/935c0dd0fbeb4b2e69588471cbb2d4b2/05-layer-norm.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">05-layer-norm.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/06-fused-attention.html", "status": "modified", "additions": 21, "deletions": 21, "changes": 42, "file_content_changes": "@@ -117,32 +117,32 @@\n </ul>\n <div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>fused-attention-batch4-head48-d64-fwd:\n      N_CTX      Triton\n-0   1024.0  158.542779\n-1   2048.0  166.913790\n-2   4096.0  174.440894\n-3   8192.0  177.052682\n-4  16384.0  176.251992\n+0   1024.0  159.091886\n+1   2048.0  168.801871\n+2   4096.0  175.516831\n+3   8192.0  177.132493\n+4  16384.0  178.305654\n fused-attention-batch4-head48-d64-fwd:\n      N_CTX      Triton\n-0   1024.0  115.066030\n-1   2048.0  132.931935\n-2   4096.0  147.031047\n-3   8192.0  154.580880\n-4  16384.0  152.952855\n+0   1024.0  116.224800\n+1   2048.0  134.780294\n+2   4096.0  149.303213\n+3   8192.0  154.727745\n+4  16384.0  152.478642\n fused-attention-batch4-head48-d64-bwd:\n      N_CTX     Triton\n-0   1024.0  72.145583\n-1   2048.0  81.558058\n-2   4096.0  86.495359\n-3   8192.0  87.505720\n-4  16384.0  90.256695\n+0   1024.0  72.461349\n+1   2048.0  81.487631\n+2   4096.0  87.046972\n+3   8192.0  88.701852\n+4  16384.0  90.644526\n fused-attention-batch4-head48-d64-bwd:\n      N_CTX     Triton\n-0   1024.0  50.602755\n-1   2048.0  62.675440\n-2   4096.0  70.089209\n-3   8192.0  75.064446\n-4  16384.0  77.221686\n+0   1024.0  50.614484\n+1   2048.0  62.978218\n+2   4096.0  70.237556\n+3   8192.0  74.952661\n+4  16384.0  77.275410\n </pre></div>\n </div>\n <div class=\"line-block\">\n@@ -550,7 +550,7 @@\n <span class=\"n\">bench_flash_attention</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;.&#39;</span><span class=\"p\">,</span> <span class=\"n\">print_data</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  14.835 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  14.841 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-06-fused-attention-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/54a35f6ec55f9746935b9566fb6bb1df/06-fused-attention.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">06-fused-attention.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/08-experimental-block-pointer.html", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -335,7 +335,7 @@ <h2>Unit Test<a class=\"headerlink\" href=\"#unit-test\" title=\"Permalink to this he\n \u2705 Triton and Torch match\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  6.286 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  6.297 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-08-experimental-block-pointer-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/4d6052117d61c2ca779cd4b75567fee5/08-experimental-block-pointer.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">08-experimental-block-pointer.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/sg_execution_times.html", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -86,35 +86,35 @@\n              \n   <section id=\"computation-times\">\n <span id=\"sphx-glr-getting-started-tutorials-sg-execution-times\"></span><h1>Computation times<a class=\"headerlink\" href=\"#computation-times\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<p><strong>02:13.541</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n+<p><strong>02:13.500</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n <table class=\"docutils align-default\">\n <tbody>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"03-matrix-multiplication.html#sphx-glr-getting-started-tutorials-03-matrix-multiplication-py\"><span class=\"std std-ref\">Matrix Multiplication</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">03-matrix-multiplication.py</span></code>)</p></td>\n-<td><p>00:40.572</p></td>\n+<td><p>00:40.581</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py\"><span class=\"std std-ref\">Fused Softmax</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">02-fused-softmax.py</span></code>)</p></td>\n-<td><p>00:36.893</p></td>\n+<td><p>00:36.937</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"05-layer-norm.html#sphx-glr-getting-started-tutorials-05-layer-norm-py\"><span class=\"std std-ref\">Layer Normalization</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">05-layer-norm.py</span></code>)</p></td>\n-<td><p>00:29.521</p></td>\n+<td><p>00:29.371</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"06-fused-attention.html#sphx-glr-getting-started-tutorials-06-fused-attention-py\"><span class=\"std std-ref\">Fused Attention</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">06-fused-attention.py</span></code>)</p></td>\n-<td><p>00:14.835</p></td>\n+<td><p>00:14.841</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"08-experimental-block-pointer.html#sphx-glr-getting-started-tutorials-08-experimental-block-pointer-py\"><span class=\"std std-ref\">Block Pointer (Experimental)</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">08-experimental-block-pointer.py</span></code>)</p></td>\n-<td><p>00:06.286</p></td>\n+<td><p>00:06.297</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py\"><span class=\"std std-ref\">Vector Addition</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">01-vector-add.py</span></code>)</p></td>\n-<td><p>00:04.567</p></td>\n+<td><p>00:04.614</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"04-low-memory-dropout.html#sphx-glr-getting-started-tutorials-04-low-memory-dropout-py\"><span class=\"std std-ref\">Low-Memory Dropout</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">04-low-memory-dropout.py</span></code>)</p></td>\n-<td><p>00:00.643</p></td>\n+<td><p>00:00.635</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"07-math-functions.html#sphx-glr-getting-started-tutorials-07-math-functions-py\"><span class=\"std std-ref\">Libdevice (tl.math) function</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">07-math-functions.py</span></code>)</p></td>"}, {"filename": "main/searchindex.js", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "N/A"}]