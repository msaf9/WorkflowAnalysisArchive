[{"filename": "main/.buildinfo", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n # Sphinx build info version 1\n # This file hashes the configuration used when building these files. When it is not found, a full rebuild will be done.\n-config: 15fb25c783fc7e6a2dcf0b7819426d6e\n+config: 34d61eda4ce385a8e509cc2744e59d68\n tags: 645f666f9bcd5a90fca523b33c5a78b7"}, {"filename": "main/.doctrees/environment.pickle", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/installation.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/01-vector-add.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/02-fused-softmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/03-matrix-multiplication.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/04-low-memory-dropout.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/05-layer-norm.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/06-fused-attention.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/07-math-functions.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/08-experimental-block-pointer.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/index.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/sg_execution_times.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/index.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/programming-guide/chapter-1/introduction.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/programming-guide/chapter-2/related-work.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.Config.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.autotune.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.heuristics.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.jit.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.abs.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.arange.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.argmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.argmin.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_add.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_cas.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_max.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_min.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_xchg.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.broadcast_to.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cos.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.dot.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.exp.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.expand_dims.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.load.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.log.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.max.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.maximum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.min.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.minimum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.multiple_of.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.num_programs.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.program_id.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.rand.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randint.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randint4x.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randn.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.ravel.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.reduce.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.reshape.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sigmoid.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sin.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.softmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sqrt.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.store.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.where.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.xor_sum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.zeros.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.Benchmark.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.do_bench.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.perf_report.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.language.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.testing.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/662999063954282841dc90b8945f85ce/tutorials_jupyter.zip", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/763344228ae6bc253ed1a6cf586aa30d/tutorials_python.zip", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_01-vector-add_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_01-vector-add_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_02-fused-softmax_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_02-fused-softmax_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_03-matrix-multiplication_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_03-matrix-multiplication_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_05-layer-norm_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_05-layer-norm_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_002.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_sources/getting-started/tutorials/01-vector-add.rst.txt", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -236,9 +236,9 @@ We can now run the decorated function above. Pass `print_data=True` to see the p\n \n     vector-add-performance:\n                size       Triton        Torch\n-    0        4096.0     8.000000     8.000000\n+    0        4096.0     8.000000     9.600000\n     1        8192.0    15.999999    15.999999\n-    2       16384.0    31.999999    38.400001\n+    2       16384.0    31.999999    31.999999\n     3       32768.0    63.999998    63.999998\n     4       65536.0   127.999995   127.999995\n     5      131072.0   219.428568   219.428568\n@@ -249,7 +249,7 @@ We can now run the decorated function above. Pass `print_data=True` to see the p\n     10    4194304.0  1260.307736  1228.800031\n     11    8388608.0  1424.695621  1424.695621\n     12   16777216.0  1560.380965  1560.380965\n-    13   33554432.0  1624.859540  1624.859540\n+    13   33554432.0  1631.601649  1624.859540\n     14   67108864.0  1669.706983  1662.646960\n     15  134217728.0  1684.008546  1680.410210\n \n@@ -259,7 +259,7 @@ We can now run the decorated function above. Pass `print_data=True` to see the p\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  27.151 seconds)\n+   **Total running time of the script:** ( 0 minutes  25.628 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_01-vector-add.py:"}, {"filename": "main/_sources/getting-started/tutorials/02-fused-softmax.rst.txt", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -286,16 +286,16 @@ We will then compare its performance against (1) :code:`torch.softmax` and (2) t\n \n     softmax-performance:\n               N       Triton  Torch (native)  Torch (jit)\n-    0     256.0   682.666643      744.727267   273.066674\n-    1     384.0   877.714274      819.200021   332.108094\n-    2     512.0   910.222190      963.764689   372.363633\n-    3     640.0   975.238103      930.909084   409.600010\n-    4     768.0  1068.521715     1023.999964   438.857137\n+    0     256.0   682.666643      682.666643   273.066674\n+    1     384.0   877.714274      877.714274   323.368435\n+    2     512.0   910.222190      910.222190   372.363633\n+    3     640.0   975.238103      975.238103   409.600010\n+    4     768.0  1068.521715     1023.999964   431.157886\n     ..      ...          ...             ...          ...\n-    93  12160.0  1588.244879     1069.010969   589.575740\n-    94  12288.0  1598.438956     1016.061996   589.529222\n-    95  12416.0  1576.634933     1029.305700   586.871514\n-    96  12544.0  1574.149071     1018.802024   588.574769\n+    93  12160.0  1588.244879     1066.082150   589.575740\n+    94  12288.0  1591.967682     1016.061996   590.414408\n+    95  12416.0  1576.634933     1031.979242   588.610355\n+    96  12544.0  1574.149071     1016.222759   588.574769\n     97  12672.0  1577.836533     1008.716405   588.539906\n \n     [98 rows x 4 columns]\n@@ -313,7 +313,7 @@ In the above plot, we can see that:\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 1 minutes  18.148 seconds)\n+   **Total running time of the script:** ( 1 minutes  19.770 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:"}, {"filename": "main/_sources/getting-started/tutorials/03-matrix-multiplication.rst.txt", "status": "modified", "additions": 23, "deletions": 23, "changes": 46, "file_content_changes": "@@ -449,45 +449,45 @@ but feel free to arrange this script as you wish to benchmark any other matrix s\n \n     matmul-performance:\n              M      cuBLAS      Triton\n-    0    256.0    4.681143    4.096000\n-    1    384.0   12.288000   12.288000\n+    0    256.0    4.096000    4.096000\n+    1    384.0   11.059200   12.288000\n     2    512.0   26.214401   23.831273\n     3    640.0   42.666665   39.384616\n-    4    768.0   63.195428   58.982401\n+    4    768.0   68.056616   58.982401\n     5    896.0   78.051553   82.642822\n     6   1024.0  110.376426   99.864382\n     7   1152.0  135.726544  129.825388\n     8   1280.0  163.840004  163.840004\n-    9   1408.0  155.765024  136.294403\n+    9   1408.0  155.765024  132.970149\n     10  1536.0  181.484314  157.286398\n-    11  1664.0  179.978245  179.978245\n+    11  1664.0  183.651271  179.978245\n     12  1792.0  172.914215  208.137481\n     13  1920.0  203.294114  168.585369\n-    14  2048.0  197.379013  190.650180\n-    15  2176.0  189.845737  209.621326\n-    16  2304.0  229.691080  229.691080\n-    17  2432.0  202.118452  200.674737\n-    18  2560.0  222.911566  217.006622\n-    19  2688.0  196.544332  196.544332\n-    20  2816.0  208.680416  206.702402\n-    21  2944.0  217.624596  218.579083\n-    22  3072.0  205.902197  205.528506\n-    23  3200.0  212.624590  214.765101\n-    24  3328.0  203.941342  206.278780\n-    25  3456.0  210.500174  213.284573\n-    26  3584.0  218.772251  207.178329\n-    27  3712.0  207.686788  214.371984\n-    28  3840.0  208.271176  210.250955\n-    29  3968.0  207.877238  215.589992\n-    30  4096.0  219.310012  210.702863\n+    14  2048.0  197.379013  192.841562\n+    15  2176.0  191.653792  209.621326\n+    16  2304.0  229.691080  231.921091\n+    17  2432.0  203.583068  200.674737\n+    18  2560.0  224.438347  215.578957\n+    19  2688.0  195.531224  195.531224\n+    20  2816.0  210.696652  207.686706\n+    21  2944.0  219.541994  223.479969\n+    22  3072.0  206.653671  206.653671\n+    23  3200.0  214.405358  215.488222\n+    24  3328.0  203.941342  202.792385\n+    25  3456.0  214.419058  213.850319\n+    26  3584.0  216.663602  207.178329\n+    27  3712.0  206.399476  216.228019\n+    28  3840.0  206.714019  207.489687\n+    29  3968.0  208.945088  215.589992\n+    30  4096.0  219.310012  209.388034\n \n \n \n \n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 1 minutes  45.796 seconds)\n+   **Total running time of the script:** ( 1 minutes  46.991 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_03-matrix-multiplication.py:"}, {"filename": "main/_sources/getting-started/tutorials/04-low-memory-dropout.rst.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -242,7 +242,7 @@ References\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  0.641 seconds)\n+   **Total running time of the script:** ( 0 minutes  0.633 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_04-low-memory-dropout.py:"}, {"filename": "main/_sources/getting-started/tutorials/05-layer-norm.rst.txt", "status": "modified", "additions": 17, "deletions": 17, "changes": 34, "file_content_changes": "@@ -431,35 +431,35 @@ Specifically, one can set :code:`'mode': 'backward'` to benchmark the backward p\n \n     layer-norm-backward:\n               N      Triton       Torch\n-    0    1024.0  215.578943  378.092307\n+    0    1024.0  221.405403  378.092307\n     1    1536.0  354.461542  438.857146\n-    2    2048.0  446.836360  496.484863\n+    2    2048.0  455.111110  496.484863\n     3    2560.0  538.947358  534.260858\n     4    3072.0  614.400016  542.117638\n-    5    3584.0  677.291303  475.226530\n-    6    4096.0  744.727267  474.898540\n-    7    4608.0  699.949388  480.834772\n+    5    3584.0  677.291303  470.032796\n+    6    4096.0  750.412251  474.898540\n+    7    4608.0  699.949388  478.753251\n     8    5120.0  753.865011  483.779502\n     9    5632.0  795.105885  489.739120\n     10   6144.0  837.818175  494.818794\n-    11   6656.0  877.714269  499.200013\n-    12   7168.0  915.063803  476.542919\n-    13   7680.0  945.230767  478.753257\n-    14   8192.0  978.149241  486.653476\n+    11   6656.0  877.714269  500.764869\n+    12   7168.0  915.063803  477.866659\n+    13   7680.0  945.230767  479.999983\n+    14   8192.0  983.040025  486.653476\n     15   8704.0  661.063311  485.804668\n     16   9216.0  689.046730  489.345125\n-    17   9728.0  709.641333  492.556962\n-    18  10240.0  735.808359  494.486921\n+    17   9728.0  709.641333  494.644053\n+    18  10240.0  738.018010  494.486921\n     19  10752.0  763.455593  485.052653\n-    20  11264.0  790.456108  486.215841\n+    20  11264.0  790.456108  487.971095\n     21  11776.0  807.497172  490.666649\n     22  12288.0  819.199988  495.650428\n-    23  12800.0  830.270283  497.893044\n+    23  12800.0  828.032341  497.893044\n     24  13312.0  845.206356  499.200013\n     25  13824.0  844.213752  497.415281\n-    26  14336.0  855.880586  491.520018\n-    27  14848.0  858.679544  494.933326\n-    28  15360.0  871.489381  500.189943\n+    26  14336.0  860.160022  491.520018\n+    27  14848.0  860.753604  494.933326\n+    28  15360.0  873.554478  500.189943\n     29  15872.0  869.698654  501.881412\n \n \n@@ -475,7 +475,7 @@ References\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  37.366 seconds)\n+   **Total running time of the script:** ( 0 minutes  37.482 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_05-layer-norm.py:"}, {"filename": "main/_sources/getting-started/tutorials/06-fused-attention.rst.txt", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "@@ -52,16 +52,16 @@ This is a Triton implementation of the Flash Attention algorithm\n \n     fused-attention-batch4-head48-d64-fwd:\n         N_CTX     Triton\n-    0  1024.0   0.291230\n-    1  2048.0   1.002344\n-    2  4096.0   3.685035\n-    3  8192.0  14.033628\n+    0  1024.0   0.291369\n+    1  2048.0   1.008098\n+    2  4096.0   3.677829\n+    3  8192.0  14.033481\n     fused-attention-batch4-head48-d64-bwd:\n         N_CTX     Triton\n-    0  1024.0   1.185223\n-    1  2048.0   3.788524\n-    2  4096.0  13.335991\n-    3  8192.0  49.772034\n+    0  1024.0   1.183997\n+    1  2048.0   3.789391\n+    2  4096.0  13.282157\n+    3  8192.0  49.604095\n \n \n \n@@ -430,7 +430,7 @@ This is a Triton implementation of the Flash Attention algorithm\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  4.707 seconds)\n+   **Total running time of the script:** ( 0 minutes  4.745 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_06-fused-attention.py:"}, {"filename": "main/_sources/getting-started/tutorials/07-math-functions.rst.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -147,7 +147,7 @@ We can also customize the libdevice library path by passing the path to the `lib\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  0.237 seconds)\n+   **Total running time of the script:** ( 0 minutes  0.238 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_07-math-functions.py:"}, {"filename": "main/_sources/getting-started/tutorials/08-experimental-block-pointer.rst.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -298,7 +298,7 @@ Still we can test our matrix multiplication with block pointers against a native\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  9.521 seconds)\n+   **Total running time of the script:** ( 0 minutes  9.504 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_08-experimental-block-pointer.py:"}, {"filename": "main/_sources/getting-started/tutorials/sg_execution_times.rst.txt", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "@@ -6,22 +6,22 @@\n \n Computation times\n =================\n-**04:23.567** total execution time for **getting-started_tutorials** files:\n+**04:24.992** total execution time for **getting-started_tutorials** files:\n \n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``)           | 01:45.796 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``)           | 01:46.991 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                           | 01:18.148 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                           | 01:19.770 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                                 | 00:37.366 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                                 | 00:37.482 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                                 | 00:27.151 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                                 | 00:25.628 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_08-experimental-block-pointer.py` (``08-experimental-block-pointer.py``) | 00:09.521 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_08-experimental-block-pointer.py` (``08-experimental-block-pointer.py``) | 00:09.504 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py` (``06-fused-attention.py``)                       | 00:04.707 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py` (``06-fused-attention.py``)                       | 00:04.745 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)                 | 00:00.641 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)                 | 00:00.633 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_07-math-functions.py` (``07-math-functions.py``)                         | 00:00.237 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_07-math-functions.py` (``07-math-functions.py``)                         | 00:00.238 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+"}, {"filename": "main/getting-started/tutorials/01-vector-add.html", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -235,9 +235,9 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n </div>\n <img src=\"../../_images/sphx_glr_01-vector-add_001.png\" srcset=\"../../_images/sphx_glr_01-vector-add_001.png\" alt=\"01 vector add\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>vector-add-performance:\n            size       Triton        Torch\n-0        4096.0     8.000000     8.000000\n+0        4096.0     8.000000     9.600000\n 1        8192.0    15.999999    15.999999\n-2       16384.0    31.999999    38.400001\n+2       16384.0    31.999999    31.999999\n 3       32768.0    63.999998    63.999998\n 4       65536.0   127.999995   127.999995\n 5      131072.0   219.428568   219.428568\n@@ -248,12 +248,12 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n 10    4194304.0  1260.307736  1228.800031\n 11    8388608.0  1424.695621  1424.695621\n 12   16777216.0  1560.380965  1560.380965\n-13   33554432.0  1624.859540  1624.859540\n+13   33554432.0  1631.601649  1624.859540\n 14   67108864.0  1669.706983  1662.646960\n 15  134217728.0  1684.008546  1680.410210\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  27.151 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  25.628 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-01-vector-add-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/62d97d49a32414049819dd8bb8378080/01-vector-add.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">01-vector-add.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/02-fused-softmax.html", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -282,16 +282,16 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n </div>\n <img src=\"../../_images/sphx_glr_02-fused-softmax_001.png\" srcset=\"../../_images/sphx_glr_02-fused-softmax_001.png\" alt=\"02 fused softmax\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>softmax-performance:\n           N       Triton  Torch (native)  Torch (jit)\n-0     256.0   682.666643      744.727267   273.066674\n-1     384.0   877.714274      819.200021   332.108094\n-2     512.0   910.222190      963.764689   372.363633\n-3     640.0   975.238103      930.909084   409.600010\n-4     768.0  1068.521715     1023.999964   438.857137\n+0     256.0   682.666643      682.666643   273.066674\n+1     384.0   877.714274      877.714274   323.368435\n+2     512.0   910.222190      910.222190   372.363633\n+3     640.0   975.238103      975.238103   409.600010\n+4     768.0  1068.521715     1023.999964   431.157886\n ..      ...          ...             ...          ...\n-93  12160.0  1588.244879     1069.010969   589.575740\n-94  12288.0  1598.438956     1016.061996   589.529222\n-95  12416.0  1576.634933     1029.305700   586.871514\n-96  12544.0  1574.149071     1018.802024   588.574769\n+93  12160.0  1588.244879     1066.082150   589.575740\n+94  12288.0  1591.967682     1016.061996   590.414408\n+95  12416.0  1576.634933     1031.979242   588.610355\n+96  12544.0  1574.149071     1016.222759   588.574769\n 97  12672.0  1577.836533     1008.716405   588.539906\n \n [98 rows x 4 columns]\n@@ -305,7 +305,7 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n </ul>\n </dd>\n </dl>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  18.148 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  19.770 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-02-fused-softmax-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d91442ac2982c4e0cc3ab0f43534afbc/02-fused-softmax.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">02-fused-softmax.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/03-matrix-multiplication.html", "status": "modified", "additions": 23, "deletions": 23, "changes": 46, "file_content_changes": "@@ -463,40 +463,40 @@ <h3>Square Matrix Performance<a class=\"headerlink\" href=\"#square-matrix-performa\n </div>\n <img src=\"../../_images/sphx_glr_03-matrix-multiplication_001.png\" srcset=\"../../_images/sphx_glr_03-matrix-multiplication_001.png\" alt=\"03 matrix multiplication\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>matmul-performance:\n          M      cuBLAS      Triton\n-0    256.0    4.681143    4.096000\n-1    384.0   12.288000   12.288000\n+0    256.0    4.096000    4.096000\n+1    384.0   11.059200   12.288000\n 2    512.0   26.214401   23.831273\n 3    640.0   42.666665   39.384616\n-4    768.0   63.195428   58.982401\n+4    768.0   68.056616   58.982401\n 5    896.0   78.051553   82.642822\n 6   1024.0  110.376426   99.864382\n 7   1152.0  135.726544  129.825388\n 8   1280.0  163.840004  163.840004\n-9   1408.0  155.765024  136.294403\n+9   1408.0  155.765024  132.970149\n 10  1536.0  181.484314  157.286398\n-11  1664.0  179.978245  179.978245\n+11  1664.0  183.651271  179.978245\n 12  1792.0  172.914215  208.137481\n 13  1920.0  203.294114  168.585369\n-14  2048.0  197.379013  190.650180\n-15  2176.0  189.845737  209.621326\n-16  2304.0  229.691080  229.691080\n-17  2432.0  202.118452  200.674737\n-18  2560.0  222.911566  217.006622\n-19  2688.0  196.544332  196.544332\n-20  2816.0  208.680416  206.702402\n-21  2944.0  217.624596  218.579083\n-22  3072.0  205.902197  205.528506\n-23  3200.0  212.624590  214.765101\n-24  3328.0  203.941342  206.278780\n-25  3456.0  210.500174  213.284573\n-26  3584.0  218.772251  207.178329\n-27  3712.0  207.686788  214.371984\n-28  3840.0  208.271176  210.250955\n-29  3968.0  207.877238  215.589992\n-30  4096.0  219.310012  210.702863\n+14  2048.0  197.379013  192.841562\n+15  2176.0  191.653792  209.621326\n+16  2304.0  229.691080  231.921091\n+17  2432.0  203.583068  200.674737\n+18  2560.0  224.438347  215.578957\n+19  2688.0  195.531224  195.531224\n+20  2816.0  210.696652  207.686706\n+21  2944.0  219.541994  223.479969\n+22  3072.0  206.653671  206.653671\n+23  3200.0  214.405358  215.488222\n+24  3328.0  203.941342  202.792385\n+25  3456.0  214.419058  213.850319\n+26  3584.0  216.663602  207.178329\n+27  3712.0  206.399476  216.228019\n+28  3840.0  206.714019  207.489687\n+29  3968.0  208.945088  215.589992\n+30  4096.0  219.310012  209.388034\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  45.796 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  46.991 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-03-matrix-multiplication-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d5fee5b55a64e47f1b5724ec39adf171/03-matrix-multiplication.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">03-matrix-multiplication.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/04-low-memory-dropout.html", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -287,7 +287,7 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n <p>Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \u201cDropout: A Simple Way to Prevent Neural Networks from Overfitting\u201d, JMLR 2014</p>\n </div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.641 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.633 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-04-low-memory-dropout-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/c9aed78977a4c05741d675a38dde3d7d/04-low-memory-dropout.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">04-low-memory-dropout.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/05-layer-norm.html", "status": "modified", "additions": 17, "deletions": 17, "changes": 34, "file_content_changes": "@@ -460,35 +460,35 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n </div>\n <img src=\"../../_images/sphx_glr_05-layer-norm_001.png\" srcset=\"../../_images/sphx_glr_05-layer-norm_001.png\" alt=\"05 layer norm\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>layer-norm-backward:\n           N      Triton       Torch\n-0    1024.0  215.578943  378.092307\n+0    1024.0  221.405403  378.092307\n 1    1536.0  354.461542  438.857146\n-2    2048.0  446.836360  496.484863\n+2    2048.0  455.111110  496.484863\n 3    2560.0  538.947358  534.260858\n 4    3072.0  614.400016  542.117638\n-5    3584.0  677.291303  475.226530\n-6    4096.0  744.727267  474.898540\n-7    4608.0  699.949388  480.834772\n+5    3584.0  677.291303  470.032796\n+6    4096.0  750.412251  474.898540\n+7    4608.0  699.949388  478.753251\n 8    5120.0  753.865011  483.779502\n 9    5632.0  795.105885  489.739120\n 10   6144.0  837.818175  494.818794\n-11   6656.0  877.714269  499.200013\n-12   7168.0  915.063803  476.542919\n-13   7680.0  945.230767  478.753257\n-14   8192.0  978.149241  486.653476\n+11   6656.0  877.714269  500.764869\n+12   7168.0  915.063803  477.866659\n+13   7680.0  945.230767  479.999983\n+14   8192.0  983.040025  486.653476\n 15   8704.0  661.063311  485.804668\n 16   9216.0  689.046730  489.345125\n-17   9728.0  709.641333  492.556962\n-18  10240.0  735.808359  494.486921\n+17   9728.0  709.641333  494.644053\n+18  10240.0  738.018010  494.486921\n 19  10752.0  763.455593  485.052653\n-20  11264.0  790.456108  486.215841\n+20  11264.0  790.456108  487.971095\n 21  11776.0  807.497172  490.666649\n 22  12288.0  819.199988  495.650428\n-23  12800.0  830.270283  497.893044\n+23  12800.0  828.032341  497.893044\n 24  13312.0  845.206356  499.200013\n 25  13824.0  844.213752  497.415281\n-26  14336.0  855.880586  491.520018\n-27  14848.0  858.679544  494.933326\n-28  15360.0  871.489381  500.189943\n+26  14336.0  860.160022  491.520018\n+27  14848.0  860.753604  494.933326\n+28  15360.0  873.554478  500.189943\n 29  15872.0  869.698654  501.881412\n </pre></div>\n </div>\n@@ -501,7 +501,7 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n <p>Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \u201cLayer Normalization\u201d, Arxiv 2016</p>\n </div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  37.366 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  37.482 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-05-layer-norm-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/935c0dd0fbeb4b2e69588471cbb2d4b2/05-layer-norm.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">05-layer-norm.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/06-fused-attention.html", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "@@ -112,16 +112,16 @@\n </ul>\n <div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>fused-attention-batch4-head48-d64-fwd:\n     N_CTX     Triton\n-0  1024.0   0.291230\n-1  2048.0   1.002344\n-2  4096.0   3.685035\n-3  8192.0  14.033628\n+0  1024.0   0.291369\n+1  2048.0   1.008098\n+2  4096.0   3.677829\n+3  8192.0  14.033481\n fused-attention-batch4-head48-d64-bwd:\n     N_CTX     Triton\n-0  1024.0   1.185223\n-1  2048.0   3.788524\n-2  4096.0  13.335991\n-3  8192.0  49.772034\n+0  1024.0   1.183997\n+1  2048.0   3.789391\n+2  4096.0  13.282157\n+3  8192.0  49.604095\n </pre></div>\n </div>\n <div class=\"line-block\">\n@@ -482,7 +482,7 @@\n <span class=\"n\">bench_flash_attention</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;.&#39;</span><span class=\"p\">,</span> <span class=\"n\">print_data</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  4.707 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  4.745 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-06-fused-attention-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/54a35f6ec55f9746935b9566fb6bb1df/06-fused-attention.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">06-fused-attention.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/07-math-functions.html", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -186,7 +186,7 @@ <h2>Customize the libdevice library path<a class=\"headerlink\" href=\"#customize-t\n The maximum difference between torch and triton is 2.384185791015625e-07\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.237 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.238 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-07-math-functions-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/77571465f7f4bd281d3a847dc2633146/07-math-functions.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">07-math-functions.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/08-experimental-block-pointer.html", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -335,7 +335,7 @@ <h2>Unit Test<a class=\"headerlink\" href=\"#unit-test\" title=\"Permalink to this he\n \u2705 Triton and Torch match\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  9.521 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  9.504 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-08-experimental-block-pointer-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/4d6052117d61c2ca779cd4b75567fee5/08-experimental-block-pointer.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">08-experimental-block-pointer.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/sg_execution_times.html", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "@@ -86,39 +86,39 @@\n              \n   <section id=\"computation-times\">\n <span id=\"sphx-glr-getting-started-tutorials-sg-execution-times\"></span><h1>Computation times<a class=\"headerlink\" href=\"#computation-times\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<p><strong>04:23.567</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n+<p><strong>04:24.992</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n <table class=\"docutils align-default\">\n <tbody>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"03-matrix-multiplication.html#sphx-glr-getting-started-tutorials-03-matrix-multiplication-py\"><span class=\"std std-ref\">Matrix Multiplication</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">03-matrix-multiplication.py</span></code>)</p></td>\n-<td><p>01:45.796</p></td>\n+<td><p>01:46.991</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py\"><span class=\"std std-ref\">Fused Softmax</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">02-fused-softmax.py</span></code>)</p></td>\n-<td><p>01:18.148</p></td>\n+<td><p>01:19.770</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"05-layer-norm.html#sphx-glr-getting-started-tutorials-05-layer-norm-py\"><span class=\"std std-ref\">Layer Normalization</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">05-layer-norm.py</span></code>)</p></td>\n-<td><p>00:37.366</p></td>\n+<td><p>00:37.482</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py\"><span class=\"std std-ref\">Vector Addition</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">01-vector-add.py</span></code>)</p></td>\n-<td><p>00:27.151</p></td>\n+<td><p>00:25.628</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"08-experimental-block-pointer.html#sphx-glr-getting-started-tutorials-08-experimental-block-pointer-py\"><span class=\"std std-ref\">Block Pointer (Experimental)</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">08-experimental-block-pointer.py</span></code>)</p></td>\n-<td><p>00:09.521</p></td>\n+<td><p>00:09.504</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"06-fused-attention.html#sphx-glr-getting-started-tutorials-06-fused-attention-py\"><span class=\"std std-ref\">Fused Attention</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">06-fused-attention.py</span></code>)</p></td>\n-<td><p>00:04.707</p></td>\n+<td><p>00:04.745</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"04-low-memory-dropout.html#sphx-glr-getting-started-tutorials-04-low-memory-dropout-py\"><span class=\"std std-ref\">Low-Memory Dropout</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">04-low-memory-dropout.py</span></code>)</p></td>\n-<td><p>00:00.641</p></td>\n+<td><p>00:00.633</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"07-math-functions.html#sphx-glr-getting-started-tutorials-07-math-functions-py\"><span class=\"std std-ref\">Libdevice (tl.math) function</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">07-math-functions.py</span></code>)</p></td>\n-<td><p>00:00.237</p></td>\n+<td><p>00:00.238</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n </tbody>"}, {"filename": "main/python-api/generated/triton.Config.html", "status": "modified", "additions": 5, "deletions": 8, "changes": 13, "file_content_changes": "@@ -104,17 +104,14 @@ <h1>triton.Config<a class=\"headerlink\" href=\"#triton-config\" title=\"Permalink to\n <dl class=\"py class\">\n <dt class=\"sig sig-object py\" id=\"triton.Config\">\n <em class=\"property\"><span class=\"pre\">class</span><span class=\"w\"> </span></em><span class=\"sig-prename descclassname\"><span class=\"pre\">triton.</span></span><span class=\"sig-name descname\"><span class=\"pre\">Config</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">self</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">kwargs</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">num_warps</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">4</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">num_stages</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">2</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">pre_hook</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.Config\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>An object that represents a possible kernel configuration for the auto-tuner to try.\n-:ivar meta: a dictionary of meta-parameters to pass to the kernel as keyword arguments.\n-:type meta: dict[Str, Any]\n-:ivar num_warps: the number of warps to use for the kernel when compiled for GPUs. For example, if</p>\n-<blockquote>\n-<div><p><cite>num_warps=8</cite>, then each kernel instance will be automatically parallelized to\n-cooperatively execute using <cite>8 * 32 = 256</cite> threads.</p>\n-</div></blockquote>\n+<dd><p>An object that represents a possible kernel configuration for the auto-tuner to try.</p>\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Variables<span class=\"colon\">:</span></dt>\n <dd class=\"field-odd\"><ul class=\"simple\">\n+<li><p><strong>meta</strong> \u2013 a dictionary of meta-parameters to pass to the kernel as keyword arguments.</p></li>\n+<li><p><strong>num_warps</strong> \u2013 the number of warps to use for the kernel when compiled for GPUs. For example, if\n+<cite>num_warps=8</cite>, then each kernel instance will be automatically parallelized to\n+cooperatively execute using <cite>8 * 32 = 256</cite> threads.</p></li>\n <li><p><strong>num_stages</strong> \u2013 the number of stages that the compiler should use when software-pipelining loops.\n Mostly useful for matrix multiplication workloads on SM80+ GPUs.</p></li>\n <li><p><strong>pre_hook</strong> \u2013 a function that will be called before the kernel is called. Parameters of this"}, {"filename": "main/python-api/generated/triton.autotune.html", "status": "modified", "additions": 13, "deletions": 23, "changes": 36, "file_content_changes": "@@ -101,29 +101,19 @@ <h1>triton.autotune<a class=\"headerlink\" href=\"#triton-autotune\" title=\"Permalin\n <dl class=\"py function\">\n <dt class=\"sig sig-object py\" id=\"triton.autotune\">\n <span class=\"sig-prename descclassname\"><span class=\"pre\">triton.</span></span><span class=\"sig-name descname\"><span class=\"pre\">autotune</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">configs</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">key</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prune_configs_by</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">reset_to_zero</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.autotune\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Decorator for auto-tuning a <code class=\"code docutils literal notranslate\"><span class=\"pre\">triton.jit</span></code>\u2019d function.\n-.. highlight:: python\n-.. code-block:: python</p>\n-<blockquote>\n-<div><dl>\n-<dt>&#64;triton.autotune(configs=[</dt><dd><blockquote>\n-<div><p>triton.Config(meta={\u2018BLOCK_SIZE\u2019: 128}, num_warps=4),\n-triton.Config(meta={\u2018BLOCK_SIZE\u2019: 1024}, num_warps=8),</p>\n-</div></blockquote>\n-<p>],\n-key=[\u2018x_size\u2019] # the two above configs will be evaluated anytime</p>\n-<blockquote>\n-<div><p># the value of x_size changes</p>\n-</div></blockquote>\n-</dd>\n-</dl>\n-<p>)\n-&#64;triton.jit\n-def kernel(x_ptr, x_size, <a href=\"#id1\"><span class=\"problematic\" id=\"id2\">**</span></a>META):</p>\n-<blockquote>\n-<div><p>BLOCK_SIZE = META[\u2018BLOCK_SIZE\u2019]</p>\n-</div></blockquote>\n-</div></blockquote>\n+<dd><p>Decorator for auto-tuning a <code class=\"code docutils literal notranslate\"><span class=\"pre\">triton.jit</span></code>\u2019d function.</p>\n+<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">autotune</span><span class=\"p\">(</span><span class=\"n\">configs</span><span class=\"o\">=</span><span class=\"p\">[</span>\n+    <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">(</span><span class=\"n\">meta</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">},</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n+    <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">(</span><span class=\"n\">meta</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">:</span> <span class=\"mi\">1024</span><span class=\"p\">},</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">),</span>\n+  <span class=\"p\">],</span>\n+  <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;x_size&#39;</span><span class=\"p\">]</span> <span class=\"c1\"># the two above configs will be evaluated anytime</span>\n+                 <span class=\"c1\"># the value of x_size changes</span>\n+<span class=\"p\">)</span>\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n+<span class=\"k\">def</span> <span class=\"nf\">kernel</span><span class=\"p\">(</span><span class=\"n\">x_ptr</span><span class=\"p\">,</span> <span class=\"n\">x_size</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">META</span><span class=\"p\">):</span>\n+    <span class=\"n\">BLOCK_SIZE</span> <span class=\"o\">=</span> <span class=\"n\">META</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">]</span>\n+</pre></div>\n+</div>\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Note<span class=\"colon\">:</span></dt>\n <dd class=\"field-odd\"><p>When all the configurations are evaluated, the kernel will run multiple times."}, {"filename": "main/python-api/generated/triton.heuristics.html", "status": "modified", "additions": 7, "deletions": 11, "changes": 18, "file_content_changes": "@@ -102,17 +102,13 @@ <h1>triton.heuristics<a class=\"headerlink\" href=\"#triton-heuristics\" title=\"Perm\n <dt class=\"sig sig-object py\" id=\"triton.heuristics\">\n <span class=\"sig-prename descclassname\"><span class=\"pre\">triton.</span></span><span class=\"sig-name descname\"><span class=\"pre\">heuristics</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">values</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.heuristics\" title=\"Permalink to this definition\">\u00b6</a></dt>\n <dd><p>Decorator for specifying how the values of certain meta-parameters may be computed.\n-This is useful for cases where auto-tuning is prohibitevely expensive, or just not applicable.\n-.. highlight:: python\n-.. code-block:: python</p>\n-<blockquote>\n-<div><p>&#64;triton.heuristics(values={\u2018BLOCK_SIZE\u2019: lambda args: 2 ** int(math.ceil(math.log2(args[1])))})\n-&#64;triton.jit\n-def kernel(x_ptr, x_size, <a href=\"#id1\"><span class=\"problematic\" id=\"id2\">**</span></a>META):</p>\n-<blockquote>\n-<div><p>BLOCK_SIZE = META[\u2018BLOCK_SIZE\u2019] # smallest power-of-two &gt;= x_size</p>\n-</div></blockquote>\n-</div></blockquote>\n+This is useful for cases where auto-tuning is prohibitevely expensive, or just not applicable.</p>\n+<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">heuristics</span><span class=\"p\">(</span><span class=\"n\">values</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">:</span> <span class=\"k\">lambda</span> <span class=\"n\">args</span><span class=\"p\">:</span> <span class=\"mi\">2</span> <span class=\"o\">**</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">ceil</span><span class=\"p\">(</span><span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">log2</span><span class=\"p\">(</span><span class=\"n\">args</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])))})</span>\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n+<span class=\"k\">def</span> <span class=\"nf\">kernel</span><span class=\"p\">(</span><span class=\"n\">x_ptr</span><span class=\"p\">,</span> <span class=\"n\">x_size</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">META</span><span class=\"p\">):</span>\n+    <span class=\"n\">BLOCK_SIZE</span> <span class=\"o\">=</span> <span class=\"n\">META</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">]</span> <span class=\"c1\"># smallest power-of-two &gt;= x_size</span>\n+</pre></div>\n+</div>\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Parameters<span class=\"colon\">:</span></dt>\n <dd class=\"field-odd\"><p><strong>values</strong> (<em>dict</em><em>[</em><em>str</em><em>, </em><em>Callable</em><em>[</em><em>[</em><em>list</em><em>[</em><em>Any</em><em>]</em><em>]</em><em>, </em><em>Any</em><em>]</em><em>]</em>) \u2013 a dictionary of meta-parameter names and functions that compute the value of the meta-parameter."}, {"filename": "main/python-api/triton.html", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -100,13 +100,13 @@ <h1>triton<a class=\"headerlink\" href=\"#triton\" title=\"Permalink to this heading\"\n <td><p>Decorator for JIT-compiling a function using the Triton compiler.</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"generated/triton.autotune.html#triton.autotune\" title=\"triton.autotune\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">autotune</span></code></a></p></td>\n-<td><p>Decorator for auto-tuning a <code class=\"code docutils literal notranslate\"><span class=\"pre\">triton.jit</span></code>'d function. .. highlight:: python .. code-block:: python     &#64;triton.autotune(configs=[         triton.Config(meta={'BLOCK_SIZE': 128}, num_warps=4),         triton.Config(meta={'BLOCK_SIZE': 1024}, num_warps=8),       ],       key=['x_size'] # the two above configs will be evaluated anytime                      # the value of x_size changes     )     &#64;triton.jit     def kernel(x_ptr, x_size, <a href=\"#id1\"><span class=\"problematic\" id=\"id2\">**</span></a>META):         BLOCK_SIZE = META['BLOCK_SIZE'] :note: When all the configurations are evaluated, the kernel will run multiple times. This means that whatever value the kernel updates will be updated multiple times. To avoid this undesired behavior, you can use the <cite>reset_to_zero</cite> argument, which        resets the value of the provided tensor to <cite>zero</cite> before running any configuration. :param configs: a list of <code class=\"code docutils literal notranslate\"><span class=\"pre\">triton.Config</span></code> objects :type configs: list[triton.Config] :param key: a list of argument names whose change in value will trigger the evaluation of all provided configs. :type key: list[str] :param prune_configs_by: a dict of functions that are used to prune configs, fields:     'perf_model': performance model used to predicate running time with different configs, returns running time     'top_k': number of configs to bench     'early_config_prune'(optional): a function used to do early prune (eg, num_stages). It takes configs:List[Config] as its input, and returns pruned configs. :param reset_to_zero: a list of argument names whose value will be reset to zero before evaluating any configs. :type reset_to_zero: list[str].</p></td>\n+<td><p>Decorator for auto-tuning a <code class=\"code docutils literal notranslate\"><span class=\"pre\">triton.jit</span></code>'d function.</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.heuristics.html#triton.heuristics\" title=\"triton.heuristics\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">heuristics</span></code></a></p></td>\n-<td><p>Decorator for specifying how the values of certain meta-parameters may be computed. This is useful for cases where auto-tuning is prohibitevely expensive, or just not applicable. .. highlight:: python .. code-block:: python     &#64;triton.heuristics(values={'BLOCK_SIZE': lambda args: 2 ** int(math.ceil(math.log2(args[1])))})     &#64;triton.jit     def kernel(x_ptr, x_size, <a href=\"#id3\"><span class=\"problematic\" id=\"id4\">**</span></a>META):         BLOCK_SIZE = META['BLOCK_SIZE'] # smallest power-of-two &gt;= x_size :param values: a dictionary of meta-parameter names and functions that compute the value of the meta-parameter. each such function takes a list of positional arguments as input. :type values: dict[str, Callable[[list[Any]], Any]].</p></td>\n+<td><p>Decorator for specifying how the values of certain meta-parameters may be computed.</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"generated/triton.Config.html#triton.Config\" title=\"triton.Config\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">Config</span></code></a></p></td>\n-<td><p>An object that represents a possible kernel configuration for the auto-tuner to try. :ivar meta: a dictionary of meta-parameters to pass to the kernel as keyword arguments. :type meta: dict[Str, Any] :ivar num_warps: the number of warps to use for the kernel when compiled for GPUs. For example, if                   <cite>num_warps=8</cite>, then each kernel instance will be automatically parallelized to                   cooperatively execute using <cite>8 * 32 = 256</cite> threads. :type num_warps: int :ivar num_stages: the number of stages that the compiler should use when software-pipelining loops. Mostly useful for matrix multiplication workloads on SM80+ GPUs. :type num_stages: int :ivar pre_hook: a function that will be called before the kernel is called. Parameters of this                 function are args.</p></td>\n+<td><p>An object that represents a possible kernel configuration for the auto-tuner to try.</p></td>\n </tr>\n </tbody>\n </table>"}, {"filename": "main/searchindex.js", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "N/A"}]