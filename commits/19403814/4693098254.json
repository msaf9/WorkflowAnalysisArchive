[{"filename": "keren/fix-doc/.buildinfo", "status": "renamed", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n # Sphinx build info version 1\n # This file hashes the configuration used when building these files. When it is not found, a full rebuild will be done.\n-config: 14b85de4b7e3e36a7ea480dc51f7fc75\n+config: 18884fdea4a683f530ac09d12c990ffe\n tags: 645f666f9bcd5a90fca523b33c5a78b7"}, {"filename": "keren/fix-doc/.doctrees/environment.pickle", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/.doctrees/getting-started/installation.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/getting-started/installation.doctree"}, {"filename": "keren/fix-doc/.doctrees/getting-started/tutorials/01-vector-add.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/.doctrees/getting-started/tutorials/02-fused-softmax.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/getting-started/tutorials/02-fused-softmax.doctree"}, {"filename": "keren/fix-doc/.doctrees/getting-started/tutorials/03-matrix-multiplication.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/getting-started/tutorials/03-matrix-multiplication.doctree"}, {"filename": "keren/fix-doc/.doctrees/getting-started/tutorials/04-low-memory-dropout.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/getting-started/tutorials/04-low-memory-dropout.doctree"}, {"filename": "keren/fix-doc/.doctrees/getting-started/tutorials/05-layer-norm.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/.doctrees/getting-started/tutorials/07-math-functions.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/.doctrees/getting-started/tutorials/index.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/getting-started/tutorials/index.doctree"}, {"filename": "keren/fix-doc/.doctrees/getting-started/tutorials/sg_execution_times.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "v1.1.2/.doctrees/getting-started/tutorials/sg_execution_times.doctree"}, {"filename": "keren/fix-doc/.doctrees/index.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "v1.1.2/.doctrees/index.doctree"}, {"filename": "keren/fix-doc/.doctrees/programming-guide/chapter-1/introduction.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/.doctrees/programming-guide/chapter-2/related-work.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/programming-guide/chapter-2/related-work.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.Config.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.autotune.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.heuristics.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.jit.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.abs.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.arange.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/python-api/generated/triton.language.arange.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.atomic_add.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "v1.1.2/.doctrees/python-api/generated/triton.language.atomic_add.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.atomic_cas.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/python-api/generated/triton.language.atomic_cas.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.atomic_max.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "v1.1.2/.doctrees/python-api/generated/triton.language.atomic_max.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.atomic_min.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "v1.1.2/.doctrees/python-api/generated/triton.language.atomic_min.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.atomic_xchg.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "v1.1.2/.doctrees/python-api/generated/triton.language.atomic_xchg.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.broadcast_to.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/python-api/generated/triton.language.broadcast_to.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.cos.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "v1.1.2/.doctrees/python-api/generated/triton.language.cos.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.dot.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.exp.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "v1.1.2/.doctrees/python-api/generated/triton.language.exp.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.load.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.log.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "v1.1.2/.doctrees/python-api/generated/triton.language.log.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.max.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/python-api/generated/triton.language.max.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.maximum.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/python-api/generated/triton.language.maximum.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.min.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/python-api/generated/triton.language.min.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.minimum.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/python-api/generated/triton.language.minimum.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.multiple_of.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/python-api/generated/triton.language.multiple_of.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.num_programs.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/python-api/generated/triton.language.num_programs.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.program_id.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/python-api/generated/triton.language.program_id.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.rand.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/python-api/generated/triton.language.rand.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.randint.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/python-api/generated/triton.language.randint.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.randint4x.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/python-api/generated/triton.language.randint4x.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.randn.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/python-api/generated/triton.language.randn.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.ravel.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/python-api/generated/triton.language.ravel.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.reshape.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.sigmoid.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/python-api/generated/triton.language.sigmoid.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.sin.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "v1.1.2/.doctrees/python-api/generated/triton.language.sin.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.softmax.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.sqrt.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/python-api/generated/triton.language.sqrt.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.store.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.sum.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/python-api/generated/triton.language.sum.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.where.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/python-api/generated/triton.language.where.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.language.zeros.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/python-api/generated/triton.language.zeros.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.testing.Benchmark.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/python-api/generated/triton.testing.Benchmark.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.testing.do_bench.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/.doctrees/python-api/generated/triton.testing.perf_report.doctree", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/.doctrees/python-api/generated/triton.testing.perf_report.doctree"}, {"filename": "keren/fix-doc/.doctrees/python-api/triton.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/.doctrees/python-api/triton.language.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/.doctrees/python-api/triton.testing.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/_downloads/034d953b6214fedce6ea03803c712b89/02-fused-softmax.ipynb", "status": "renamed", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -15,14 +15,14 @@\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"\\n# Fused Softmax\\nIn this tutorial, you will write a fused softmax operation that is significantly faster\\nthan PyTorch's native op for a particular class of matrices: those whose rows can fit in\\nthe GPU's SRAM.\\nYou will learn about:\\n\\n- The benefits of kernel fusion for bandwidth-bound operations.\\n- Reduction operators in Triton.\\n\"\n+        \"\\n# Fused Softmax\\n\\nIn this tutorial, you will write a fused softmax operation that is significantly faster\\nthan PyTorch's native op for a particular class of matrices: those whose rows can fit in\\nthe GPU's SRAM.\\n\\nIn doing so, you will learn about:\\n- The benefits of kernel fusion for bandwidth-bound operations.\\n- Reduction operators in Triton.\\n\"\n       ]\n     },\n     {\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"## Motivations\\nCustom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.\\nLet us consider instead the case of a simple (numerically stabilized) softmax operation:\\n\\n\"\n+        \"## Motivations\\n\\nCustom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.\\nLet us consider instead the case of a simple (numerically stabilized) softmax operation:\\n\\n\"\n       ]\n     },\n     {\n@@ -47,7 +47,7 @@\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"## Compute Kernel\\nOur softmax kernel works as follows: each program loads a row of the input matrix X,\\nnormalizes it and writes back the result to the output Y.\\nNote that one important limitation of Triton is that each block must have a\\npower-of-two number of elements, so we need to internally \\\"pad\\\" each row and guard the\\nmemory operations properly if we want to handle any possible input shapes:\\n\\n\"\n+        \"## Compute Kernel\\n\\nOur softmax kernel works as follows: each program loads a row of the input matrix X,\\nnormalizes it and writes back the result to the output Y.\\n\\nNote that one important limitation of Triton is that each block must have a\\npower-of-two number of elements, so we need to internally \\\"pad\\\" each row and guard the\\nmemory operations properly if we want to handle any possible input shapes:\\n\\n\"\n       ]\n     },\n     {\n@@ -58,7 +58,7 @@\n       },\n       \"outputs\": [],\n       \"source\": [\n-        \"@triton.jit\\ndef softmax_kernel(\\n    output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols,\\n    BLOCK_SIZE: tl.constexpr\\n):\\n    # The rows of the softmax are independent, so we parallelize across those\\n    row_idx = tl.program_id(0)\\n    # The stride represents how much we need to increase the pointer to advance 1 row\\n    row_start_ptr = input_ptr + row_idx * input_row_stride\\n    # The block size is the next power of two greater than n_cols, so we can fit each\\n    # row in a single block\\n    col_offsets = tl.arange(0, BLOCK_SIZE)\\n    input_ptrs = row_start_ptr + col_offsets\\n    # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\\n    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\\n    # Substract maximum for numerical stability\\n    row_minus_max = row - tl.max(row, axis=0)\\n    # Note that exponentials in Triton are fast but approximate (i.e., think __expf in CUDA)\\n    numerator = tl.exp(row_minus_max)\\n    denominator = tl.sum(numerator, axis=0)\\n    softmax_output = numerator / denominator\\n    # Write back output to DRAM\\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\\n    output_ptrs = output_row_start_ptr + col_offsets\\n    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)\"\n+        \"@triton.jit\\ndef softmax_kernel(\\n    output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols,\\n    BLOCK_SIZE: tl.constexpr\\n):\\n    # The rows of the softmax are independent, so we parallelize across those\\n    row_idx = tl.program_id(0)\\n    # The stride represents how much we need to increase the pointer to advance 1 row\\n    row_start_ptr = input_ptr + row_idx * input_row_stride\\n    # The block size is the next power of two greater than n_cols, so we can fit each\\n    # row in a single block\\n    col_offsets = tl.arange(0, BLOCK_SIZE)\\n    input_ptrs = row_start_ptr + col_offsets\\n    # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\\n    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\\n    # Subtract maximum for numerical stability\\n    row_minus_max = row - tl.max(row, axis=0)\\n    # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\\n    numerator = tl.exp(row_minus_max)\\n    denominator = tl.sum(numerator, axis=0)\\n    softmax_output = numerator / denominator\\n    # Write back output to DRAM\\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\\n    output_ptrs = output_row_start_ptr + col_offsets\\n    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)\"\n       ]\n     },\n     {\n@@ -115,7 +115,7 @@\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"## Benchmark\\nHere we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.\\nWe will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.\\n\\n\"\n+        \"## Benchmark\\n\\nHere we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.\\nWe will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.\\n\\n\"\n       ]\n     },\n     {\n@@ -126,14 +126,14 @@\n       },\n       \"outputs\": [],\n       \"source\": [\n-        \"@triton.testing.perf_report(\\n    triton.testing.Benchmark(\\n        x_names=['N'],  # argument names to use as an x-axis for the plot\\n        x_vals=[\\n            128 * i for i in range(2, 100)\\n        ],  # different possible values for `x_name`\\n        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\\n        line_vals=[\\n            'triton',\\n            'torch-native',\\n            'torch-jit',\\n        ],  # possible values for `line_arg``\\n        line_names=[\\n            \\\"Triton\\\",\\n            \\\"Torch (native)\\\",\\n            \\\"Torch (jit)\\\",\\n        ],  # label name for the lines\\n        styles=[('blue', '-'), ('green', '-'), ('green', '--')],  # line styles\\n        ylabel=\\\"GB/s\\\",  # label name for the y-axis\\n        plot_name=\\\"softmax-performance\\\",  # name for the plot. Used also as a file name for saving the plot.\\n        args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\\n    )\\n)\\ndef benchmark(M, N, provider):\\n    x = torch.randn(M, N, device='cuda', dtype=torch.float32)\\n    if provider == 'torch-native':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))\\n    if provider == 'triton':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: softmax(x))\\n    if provider == 'torch-jit':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_softmax(x))\\n    gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\\n    return gbps(ms), gbps(max_ms), gbps(min_ms)\\n\\n\\nbenchmark.run(show_plots=True, print_data=True)\"\n+        \"@triton.testing.perf_report(\\n    triton.testing.Benchmark(\\n        x_names=['N'],  # argument names to use as an x-axis for the plot\\n        x_vals=[\\n            128 * i for i in range(2, 100)\\n        ],  # different possible values for `x_name`\\n        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\\n        line_vals=[\\n            'triton',\\n            'torch-native',\\n            'torch-jit',\\n        ],  # possible values for `line_arg``\\n        line_names=[\\n            \\\"Triton\\\",\\n            \\\"Torch (native)\\\",\\n            \\\"Torch (jit)\\\",\\n        ],  # label name for the lines\\n        styles=[('blue', '-'), ('green', '-'), ('green', '--')],  # line styles\\n        ylabel=\\\"GB/s\\\",  # label name for the y-axis\\n        plot_name=\\\"softmax-performance\\\",  # name for the plot. Used also as a file name for saving the plot.\\n        args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\\n    )\\n)\\ndef benchmark(M, N, provider):\\n    x = torch.randn(M, N, device='cuda', dtype=torch.float32)\\n    quantiles = [0.5, 0.2, 0.8]\\n    if provider == 'torch-native':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1), quantiles=quantiles)\\n    if provider == 'triton':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: softmax(x), quantiles=quantiles)\\n    if provider == 'torch-jit':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_softmax(x), quantiles=quantiles)\\n    gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\\n    return gbps(ms), gbps(max_ms), gbps(min_ms)\\n\\n\\nbenchmark.run(show_plots=True, print_data=True)\"\n       ]\n     },\n     {\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"In the above plot, we can see that:\\n\\n - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.\\n - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.\\n   Note however that the PyTorch `softmax` operation is more general and will works on tensors of any shape.\\n\\n\"\n+        \"In the above plot, we can see that:\\n - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.\\n - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.\\n   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.\\n\\n\"\n       ]\n     }\n   ],"}, {"filename": "keren/fix-doc/_downloads/302f1761fdc89516b532aad33b963ca0/07-math-functions.ipynb", "status": "renamed", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -15,7 +15,7 @@\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"\\n# Libdevice function\\nTriton can invoke a custom function from an external library.\\nIn this example, we will use the `libdevice` library to apply `asin` on a tensor.\\nPlease refer to https://docs.nvidia.com/cuda/libdevice-users-guide/index.html regarding the semantics of all available libdevice functions.\\n\\nIn `trition/language/libdevice.py`, we try to aggregate functions with the same computation but different data types together.\\nFor example, both `__nv_asin` and `__nvasinf` calculate the principal value of the arc sine of the input, but `__nv_asin` operates on `double` and `__nv_asinf` operates on `float`.\\nUsing triton, you can simply call `tl.libdevice.asinf`.\\ntriton automatically selects the correct underlying device function to invoke based on input and output types.\\n\"\n+        \"\\n# Libdevice (`tl.math`) function\\nTriton can invoke a custom function from an external library.\\nIn this example, we will use the `libdevice` library (a.k.a `math` in triton) to apply `asin` on a tensor.\\nPlease refer to https://docs.nvidia.com/cuda/libdevice-users-guide/index.html regarding the semantics of all available libdevice functions.\\nIn `triton/language/math.py`, we try to aggregate functions with the same computation but different data types together.\\nFor example, both `__nv_asin` and `__nvasinf` calculate the principal value of the arc sine of the input, but `__nv_asin` operates on `double` and `__nv_asinf` operates on `float`.\\nUsing triton, you can simply call `tl.math.asin`.\\nTriton automatically selects the correct underlying device function to invoke based on input and output types.\\n\"\n       ]\n     },\n     {\n@@ -33,14 +33,14 @@\n       },\n       \"outputs\": [],\n       \"source\": [\n-        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.jit\\ndef asin_kernel(\\n    x_ptr,\\n    y_ptr,\\n    n_elements,\\n    BLOCK_SIZE: tl.constexpr,\\n):\\n    pid = tl.program_id(axis=0)\\n    block_start = pid * BLOCK_SIZE\\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\\n    mask = offsets < n_elements\\n    x = tl.load(x_ptr + offsets, mask=mask)\\n    x = tl.libdevice.asin(x)\\n    tl.store(y_ptr + offsets, x, mask=mask)\"\n+        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.jit\\ndef asin_kernel(\\n        x_ptr,\\n        y_ptr,\\n        n_elements,\\n        BLOCK_SIZE: tl.constexpr,\\n):\\n    pid = tl.program_id(axis=0)\\n    block_start = pid * BLOCK_SIZE\\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\\n    mask = offsets < n_elements\\n    x = tl.load(x_ptr + offsets, mask=mask)\\n    x = tl.math.asin(x)\\n    tl.store(y_ptr + offsets, x, mask=mask)\"\n       ]\n     },\n     {\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"## Using the default libdevice library path\\nWe can use the default libdevice library path encoded in `triton/language/libdevice.py`\\n\\n\"\n+        \"## Using the default libdevice library path\\nWe can use the default libdevice library path encoded in `triton/language/math.py`\\n\\n\"\n       ]\n     },\n     {"}, {"filename": "keren/fix-doc/_downloads/62d97d49a32414049819dd8bb8378080/01-vector-add.py", "status": "renamed", "additions": 45, "deletions": 41, "changes": 86, "file_content_changes": "@@ -1,16 +1,18 @@\n \"\"\"\n Vector Addition\n-=================\n-In this tutorial, you will write a simple vector addition using Triton and learn about:\n+===============\n \n-- The basic programming model of Triton\n+In this tutorial, you will write a simple vector addition using Triton.\n+\n+In doing so, you will learn about:\n+- The basic programming model of Triton.\n - The `triton.jit` decorator, which is used to define Triton kernels.\n-- The best practices for validating and benchmarking your custom ops against native reference implementations\n+- The best practices for validating and benchmarking your custom ops against native reference implementations.\n \"\"\"\n \n # %%\n # Compute Kernel\n-# --------------------------\n+# --------------\n \n import torch\n \n@@ -20,51 +22,51 @@\n \n @triton.jit\n def add_kernel(\n-    x_ptr,  # *Pointer* to first input vector\n-    y_ptr,  # *Pointer* to second input vector\n-    output_ptr,  # *Pointer* to output vector\n-    n_elements,  # Size of the vector\n-    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process\n-                 # NOTE: `constexpr` so it can be used as a shape value\n+    x_ptr,  # *Pointer* to first input vector.\n+    y_ptr,  # *Pointer* to second input vector.\n+    output_ptr,  # *Pointer* to output vector.\n+    n_elements,  # Size of the vector.\n+    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n+                 # NOTE: `constexpr` so it can be used as a shape value.\n ):\n-    # There are multiple 'program's processing different data. We identify which program\n-    # we are here\n-    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0\n+    # There are multiple 'programs' processing different data. We identify which program\n+    # we are here:\n+    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n     # This program will process inputs that are offset from the initial data.\n-    # for instance, if you had a vector of length 256 and block_size of 64, the programs\n+    # For instance, if you had a vector of length 256 and block_size of 64, the programs\n     # would each access the elements [0:64, 64:128, 128:192, 192:256].\n-    # Note that offsets is a list of pointers\n+    # Note that offsets is a list of pointers:\n     block_start = pid * BLOCK_SIZE\n     offsets = block_start + tl.arange(0, BLOCK_SIZE)\n-    # Create a mask to guard memory operations against out-of-bounds accesses\n+    # Create a mask to guard memory operations against out-of-bounds accesses.\n     mask = offsets < n_elements\n     # Load x and y from DRAM, masking out any extra elements in case the input is not a\n-    # multiple of the block size\n+    # multiple of the block size.\n     x = tl.load(x_ptr + offsets, mask=mask)\n     y = tl.load(y_ptr + offsets, mask=mask)\n     output = x + y\n-    # Write x + y back to DRAM\n+    # Write x + y back to DRAM.\n     tl.store(output_ptr + offsets, output, mask=mask)\n \n \n # %%\n # Let's also declare a helper function to (1) allocate the `z` tensor\n-# and (2) enqueue the above kernel with appropriate grid/block sizes.\n+# and (2) enqueue the above kernel with appropriate grid/block sizes:\n \n \n def add(x: torch.Tensor, y: torch.Tensor):\n-    # We need to preallocate the output\n+    # We need to preallocate the output.\n     output = torch.empty_like(x)\n     assert x.is_cuda and y.is_cuda and output.is_cuda\n     n_elements = output.numel()\n     # The SPMD launch grid denotes the number of kernel instances that run in parallel.\n-    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int]\n-    # In this case, we use a 1D grid where the size is the number of blocks\n+    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\n+    # In this case, we use a 1D grid where the size is the number of blocks:\n     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n     # NOTE:\n-    #  - each torch.tensor object is implicitly converted into a pointer to its first element.\n-    #  - `triton.jit`'ed functions can be index with a launch grid to obtain a callable GPU kernel\n-    #  - don't forget to pass meta-parameters as keywords arguments\n+    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.\n+    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.\n+    #  - Don't forget to pass meta-parameters as keywords arguments.\n     add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n     # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\n     # running asynchronously at this point.\n@@ -92,40 +94,42 @@ def add(x: torch.Tensor, y: torch.Tensor):\n \n # %%\n # Benchmark\n-# -----------\n+# ---------\n+#\n # We can now benchmark our custom op on vectors of increasing sizes to get a sense of how it does relative to PyTorch.\n-# To make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of your custom ops\n+# To make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of our custom ops.\n # for different problem sizes.\n \n \n @triton.testing.perf_report(\n     triton.testing.Benchmark(\n-        x_names=['size'],  # argument names to use as an x-axis for the plot\n+        x_names=['size'],  # Argument names to use as an x-axis for the plot.\n         x_vals=[\n             2 ** i for i in range(12, 28, 1)\n-        ],  # different possible values for `x_name`\n-        x_log=True,  # x axis is logarithmic\n-        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n-        line_vals=['triton', 'torch'],  # possible values for `line_arg`\n-        line_names=['Triton', 'Torch'],  # label name for the lines\n-        styles=[('blue', '-'), ('green', '-')],  # line styles\n-        ylabel='GB/s',  # label name for the y-axis\n-        plot_name='vector-add-performance',  # name for the plot. Used also as a file name for saving the plot.\n-        args={},  # values for function arguments not in `x_names` and `y_name`\n+        ],  # Different possible values for `x_name`.\n+        x_log=True,  # x axis is logarithmic.\n+        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n+        line_vals=['triton', 'torch'],  # Possible values for `line_arg`.\n+        line_names=['Triton', 'Torch'],  # Label name for the lines.\n+        styles=[('blue', '-'), ('green', '-')],  # Line styles.\n+        ylabel='GB/s',  # Label name for the y-axis.\n+        plot_name='vector-add-performance',  # Name for the plot. Used also as a file name for saving the plot.\n+        args={},  # Values for function arguments not in `x_names` and `y_name`.\n     )\n )\n def benchmark(size, provider):\n     x = torch.rand(size, device='cuda', dtype=torch.float32)\n     y = torch.rand(size, device='cuda', dtype=torch.float32)\n+    quantiles = [0.5, 0.2, 0.8]\n     if provider == 'torch':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: x + y)\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: x + y, quantiles=quantiles)\n     if provider == 'triton':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: add(x, y))\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: add(x, y), quantiles=quantiles)\n     gbps = lambda ms: 12 * size / ms * 1e-6\n     return gbps(ms), gbps(max_ms), gbps(min_ms)\n \n \n # %%\n # We can now run the decorated function above. Pass `print_data=True` to see the performance number, `show_plots=True` to plot them, and/or\n-# `save_path='/path/to/results/' to save them to disk along with raw CSV data\n+# `save_path='/path/to/results/' to save them to disk along with raw CSV data:\n benchmark.run(print_data=True, show_plots=True)"}, {"filename": "keren/fix-doc/_downloads/662999063954282841dc90b8945f85ce/tutorials_jupyter.zip", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/_downloads/763344228ae6bc253ed1a6cf586aa30d/tutorials_python.zip", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "v1.1.2/_downloads/763344228ae6bc253ed1a6cf586aa30d/tutorials_python.zip"}, {"filename": "keren/fix-doc/_downloads/77571465f7f4bd281d3a847dc2633146/07-math-functions.py", "status": "renamed", "additions": 11, "deletions": 12, "changes": 23, "file_content_changes": "@@ -1,14 +1,13 @@\n \"\"\"\n-Libdevice function\n+Libdevice (`tl.math`) function\n ===============\n Triton can invoke a custom function from an external library.\n-In this example, we will use the `libdevice` library to apply `asin` on a tensor.\n+In this example, we will use the `libdevice` library (a.k.a `math` in triton) to apply `asin` on a tensor.\n Please refer to https://docs.nvidia.com/cuda/libdevice-users-guide/index.html regarding the semantics of all available libdevice functions.\n-\n-In `trition/language/libdevice.py`, we try to aggregate functions with the same computation but different data types together.\n+In `triton/language/math.py`, we try to aggregate functions with the same computation but different data types together.\n For example, both `__nv_asin` and `__nvasinf` calculate the principal value of the arc sine of the input, but `__nv_asin` operates on `double` and `__nv_asinf` operates on `float`.\n-Using triton, you can simply call `tl.libdevice.asinf`.\n-triton automatically selects the correct underlying device function to invoke based on input and output types.\n+Using triton, you can simply call `tl.math.asin`.\n+Triton automatically selects the correct underlying device function to invoke based on input and output types.\n \"\"\"\n \n # %%\n@@ -23,23 +22,23 @@\n \n @triton.jit\n def asin_kernel(\n-    x_ptr,\n-    y_ptr,\n-    n_elements,\n-    BLOCK_SIZE: tl.constexpr,\n+        x_ptr,\n+        y_ptr,\n+        n_elements,\n+        BLOCK_SIZE: tl.constexpr,\n ):\n     pid = tl.program_id(axis=0)\n     block_start = pid * BLOCK_SIZE\n     offsets = block_start + tl.arange(0, BLOCK_SIZE)\n     mask = offsets < n_elements\n     x = tl.load(x_ptr + offsets, mask=mask)\n-    x = tl.libdevice.asin(x)\n+    x = tl.math.asin(x)\n     tl.store(y_ptr + offsets, x, mask=mask)\n \n # %%\n #  Using the default libdevice library path\n # --------------------------\n-# We can use the default libdevice library path encoded in `triton/language/libdevice.py`\n+# We can use the default libdevice library path encoded in `triton/language/math.py`\n \n \n torch.manual_seed(0)"}, {"filename": "keren/fix-doc/_downloads/935c0dd0fbeb4b2e69588471cbb2d4b2/05-layer-norm.py", "status": "added", "additions": 371, "deletions": 0, "changes": 371, "file_content_changes": "@@ -0,0 +1,371 @@\n+\"\"\"\n+Layer Normalization\n+====================\n+In this tutorial, you will write a high-performance layer normalization\n+kernel that runs faster than the PyTorch implementation.\n+\n+In doing so, you will learn about:\n+- Implementing backward pass in Triton\n+- Implementing parallel reduction in Triton\n+\"\"\"\n+\n+# %%\n+# Motivations\n+# -----------\n+#\n+# The *LayerNorm* operator was first introduced in [BA2016]_ as a way to improve the performance\n+# of sequential models (e.g., Transformers) or neural networks with small batch size.\n+# It takes a vector :math:`x` as input and produces a vector :math:`y` of the same shape as output.\n+# The normalization is performed by subtracting the mean and dividing by the standard deviation of :math:`x`.\n+# After the normalization, a learnable linear transformation with weights :math:`w` and biases :math:`b` is applied.\n+# The forward pass can be expressed as follows:\n+#\n+# .. math::\n+#    y = \\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} } * w + b\n+#\n+# where :math:`\\epsilon` is a small constant added to the denominator for numerical stability.\n+# Let\u2019s first take a look at the forward pass implementation.\n+\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+try:\n+    # This is https://github.com/NVIDIA/apex, NOT the apex on PyPi, so it\n+    # should not be added to extras_require in setup.py.\n+    import apex\n+    HAS_APEX = True\n+except ModuleNotFoundError:\n+    HAS_APEX = False\n+\n+\n+@triton.jit\n+def _layer_norm_fwd_fused(\n+    X,  # pointer to the input\n+    Y,  # pointer to the output\n+    W,  # pointer to the weights\n+    B,  # pointer to the biases\n+    Mean,  # pointer to the mean\n+    Rstd,  # pointer to the 1/std\n+    stride,  # how much to increase the pointer when moving by 1 row\n+    N,  # number of columns in X\n+    eps,  # epsilon to avoid division by zero\n+    BLOCK_SIZE: tl.constexpr,\n+):\n+    # Map the program id to the row of X and Y it should compute.\n+    row = tl.program_id(0)\n+    Y += row * stride\n+    X += row * stride\n+    # Compute mean\n+    mean = 0\n+    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n+    for off in range(0, N, BLOCK_SIZE):\n+        cols = off + tl.arange(0, BLOCK_SIZE)\n+        a = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n+        _mean += a\n+    mean = tl.sum(_mean, axis=0) / N\n+    # Compute variance\n+    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n+    for off in range(0, N, BLOCK_SIZE):\n+        cols = off + tl.arange(0, BLOCK_SIZE)\n+        x = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n+        x = tl.where(cols < N, x - mean, 0.)\n+        _var += x * x\n+    var = tl.sum(_var, axis=0) / N\n+    rstd = 1 / tl.sqrt(var + eps)\n+    # Write mean / rstd\n+    tl.store(Mean + row, mean)\n+    tl.store(Rstd + row, rstd)\n+    # Normalize and apply linear transformation\n+    for off in range(0, N, BLOCK_SIZE):\n+        cols = off + tl.arange(0, BLOCK_SIZE)\n+        mask = cols < N\n+        w = tl.load(W + cols, mask=mask)\n+        b = tl.load(B + cols, mask=mask)\n+        x = tl.load(X + cols, mask=mask, other=0.).to(tl.float32)\n+        x_hat = (x - mean) * rstd\n+        y = x_hat * w + b\n+        # Write output\n+        tl.store(Y + cols, y, mask=mask)\n+\n+\n+# %%\n+# Backward pass\n+# -------------\n+#\n+# The backward pass for the layer normalization operator is a bit more involved than the forward pass.\n+# Let :math:`\\hat{x}` be the normalized inputs :math:`\\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} }` before the linear transformation,\n+# the Vector-Jacobian Products (VJP) :math:`\\nabla_{x}` of :math:`x` are given by:\n+#\n+# .. math::\n+#    \\nabla_{x} = \\frac{1}{\\sigma}\\Big( \\nabla_{y} \\odot w - \\underbrace{ \\big( \\frac{1}{N} \\hat{x} \\cdot (\\nabla_{y} \\odot w) \\big) }_{c_1} \\odot \\hat{x} - \\underbrace{ \\frac{1}{N} \\nabla_{y} \\cdot w }_{c_2} \\Big)\n+#\n+# where :math:`\\odot` denotes the element-wise multiplication, :math:`\\cdot` denotes the dot product, and :math:`\\sigma` is the standard deviation.\n+# :math:`c_1` and :math:`c_2` are intermediate constants that improve the readability of the following implementation.\n+#\n+# For the weights :math:`w` and biases :math:`b`, the VJPs :math:`\\nabla_{w}` and :math:`\\nabla_{b}` are more straightforward:\n+#\n+# .. math::\n+#    \\nabla_{w} = \\nabla_{y} \\odot \\hat{x} \\quad \\text{and} \\quad \\nabla_{b} = \\nabla_{y}\n+#\n+# Since the same weights :math:`w` and biases :math:`b` are used for all rows in the same batch, their gradients need to sum up.\n+# To perform this step efficiently, we use a parallel reduction strategy: each kernel instance accumulates\n+# partial :math:`\\nabla_{w}` and :math:`\\nabla_{b}` across certain rows into one of :math:`\\text{GROUP_SIZE_M}` independent buffers.\n+# These buffers stay in the L2 cache and then are further reduced by another function to compute the actual :math:`\\nabla_{w}` and :math:`\\nabla_{b}`.\n+#\n+# Let the number of input rows :math:`M = 4` and :math:`\\text{GROUP_SIZE_M} = 2`,\n+# here's a diagram of the parallel reduction strategy for :math:`\\nabla_{w}` (:math:`\\nabla_{b}` is omitted for brevity):\n+#\n+#   .. image:: parallel_reduction.png\n+#\n+# In Stage 1, the rows of X that have the same color share the same buffer and thus a lock is used to ensure that only one kernel instance writes to the buffer at a time.\n+# In Stage 2, the buffers are further reduced to compute the final :math:`\\nabla_{w}` and :math:`\\nabla_{b}`.\n+# In the following implementation, Stage 1 is implemented by the function :code:`_layer_norm_bwd_dx_fused` and Stage 2 is implemented by the function :code:`_layer_norm_bwd_dwdb`.\n+\n+@triton.jit\n+def _layer_norm_bwd_dx_fused(\n+    DX,  # pointer to the input gradient\n+    DY,  # pointer to the output gradient\n+    DW,  # pointer to the partial sum of weights gradient\n+    DB,  # pointer to the partial sum of biases gradient\n+    X,   # pointer to the input\n+    W,   # pointer to the weights\n+    B,   # pointer to the biases\n+    Mean,   # pointer to the mean\n+    Rstd,   # pointer to the 1/std\n+    Lock,  # pointer to the lock\n+    stride,  # how much to increase the pointer when moving by 1 row\n+    N,  # number of columns in X\n+    eps,  # epsilon to avoid division by zero\n+    GROUP_SIZE_M: tl.constexpr,\n+    BLOCK_SIZE_N: tl.constexpr\n+):\n+    # Map the program id to the elements of X, DX, and DY it should compute.\n+    row = tl.program_id(0)\n+    cols = tl.arange(0, BLOCK_SIZE_N)\n+    mask = cols < N\n+    X += row * stride\n+    DY += row * stride\n+    DX += row * stride\n+    # Offset locks and weights/biases gradient pointer for parallel reduction\n+    lock_id = row % GROUP_SIZE_M\n+    Lock += lock_id\n+    Count = Lock + GROUP_SIZE_M\n+    DW = DW + lock_id * N + cols\n+    DB = DB + lock_id * N + cols\n+    # Load data to SRAM\n+    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n+    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n+    w = tl.load(W + cols, mask=mask).to(tl.float32)\n+    mean = tl.load(Mean + row)\n+    rstd = tl.load(Rstd + row)\n+    # Compute dx\n+    xhat = (x - mean) * rstd\n+    wdy = w * dy\n+    xhat = tl.where(mask, xhat, 0.)\n+    wdy = tl.where(mask, wdy, 0.)\n+    c1 = tl.sum(xhat * wdy, axis=0) / N\n+    c2 = tl.sum(wdy, axis=0) / N\n+    dx = (wdy - (xhat * c1 + c2)) * rstd\n+    # Write dx\n+    tl.store(DX + cols, dx, mask=mask)\n+    # Accumulate partial sums for dw/db\n+    partial_dw = (dy * xhat).to(w.dtype)\n+    partial_db = (dy).to(w.dtype)\n+    while tl.atomic_cas(Lock, 0, 1) == 1:\n+        pass\n+    count = tl.load(Count)\n+    # First store doesn't accumulate\n+    if count == 0:\n+        tl.atomic_xchg(Count, 1)\n+    else:\n+        partial_dw += tl.load(DW, mask=mask)\n+        partial_db += tl.load(DB, mask=mask)\n+    tl.store(DW, partial_dw, mask=mask)\n+    tl.store(DB, partial_db, mask=mask)\n+    # Release the lock\n+    tl.atomic_xchg(Lock, 0)\n+\n+\n+@triton.jit\n+def _layer_norm_bwd_dwdb(\n+    DW,  # pointer to the partial sum of weights gradient\n+    DB,  # pointer to the partial sum of biases gradient\n+    FINAL_DW,  # pointer to the weights gradient\n+    FINAL_DB,  # pointer to the biases gradient\n+    M,  # GROUP_SIZE_M\n+    N,  # number of columns\n+    BLOCK_SIZE_M: tl.constexpr,\n+    BLOCK_SIZE_N: tl.constexpr\n+):\n+    # Map the program id to the elements of DW and DB it should compute.\n+    pid = tl.program_id(0)\n+    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+    db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+    # Iterate through the rows of DW and DB to sum the partial sums.\n+    for i in range(0, M, BLOCK_SIZE_M):\n+        rows = i + tl.arange(0, BLOCK_SIZE_M)\n+        mask = (rows[:, None] < M) & (cols[None, :] < N)\n+        offs = rows[:, None] * N + cols[None, :]\n+        dw += tl.load(DW + offs, mask=mask, other=0.)\n+        db += tl.load(DB + offs, mask=mask, other=0.)\n+    # Write the final sum to the output.\n+    sum_dw = tl.sum(dw, axis=0)\n+    sum_db = tl.sum(db, axis=0)\n+    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n+    tl.store(FINAL_DB + cols, sum_db, mask=cols < N)\n+\n+\n+# %%\n+# Benchmark\n+# ---------\n+#\n+# We can now compare the performance of our kernel against that of PyTorch.\n+# Here we focus on inputs that have Less than 64KB per feature.\n+# Specifically, one can set :code:`'mode': 'backward'` to benchmark the backward pass.\n+\n+\n+class LayerNorm(torch.autograd.Function):\n+\n+    @staticmethod\n+    def forward(ctx, x, normalized_shape, weight, bias, eps):\n+        # allocate output\n+        y = torch.empty_like(x)\n+        # reshape input data into 2D tensor\n+        x_arg = x.reshape(-1, x.shape[-1])\n+        M, N = x_arg.shape\n+        mean = torch.empty((M, ), dtype=torch.float32, device='cuda')\n+        rstd = torch.empty((M, ), dtype=torch.float32, device='cuda')\n+        # Less than 64KB per feature: enqueue fused kernel\n+        MAX_FUSED_SIZE = 65536 // x.element_size()\n+        BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n+        if N > BLOCK_SIZE:\n+            raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n+        # heuristics for number of warps\n+        num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\n+        # enqueue kernel\n+        _layer_norm_fwd_fused[(M,)](x_arg, y, weight, bias, mean, rstd,\n+                                    x_arg.stride(0), N, eps,\n+                                    BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)\n+        ctx.save_for_backward(x, weight, bias, mean, rstd)\n+        ctx.BLOCK_SIZE = BLOCK_SIZE\n+        ctx.num_warps = num_warps\n+        ctx.eps = eps\n+        return y\n+\n+    @staticmethod\n+    def backward(ctx, dy):\n+        x, w, b, m, v = ctx.saved_tensors\n+        # heuristics for amount of parallel reduction stream for DW/DB\n+        N = w.shape[0]\n+        GROUP_SIZE_M = 64\n+        if N <= 8192: GROUP_SIZE_M = 96\n+        if N <= 4096: GROUP_SIZE_M = 128\n+        if N <= 1024: GROUP_SIZE_M = 256\n+        # allocate output\n+        locks = torch.zeros(2 * GROUP_SIZE_M, dtype=torch.int32, device='cuda')\n+        _dw = torch.empty((GROUP_SIZE_M, w.shape[0]), dtype=x.dtype, device=w.device)\n+        _db = torch.empty((GROUP_SIZE_M, w.shape[0]), dtype=x.dtype, device=w.device)\n+        dw = torch.empty((w.shape[0],), dtype=w.dtype, device=w.device)\n+        db = torch.empty((w.shape[0],), dtype=w.dtype, device=w.device)\n+        dx = torch.empty_like(dy)\n+        # enqueue kernel using forward pass heuristics\n+        # also compute partial sums for DW and DB\n+        x_arg = x.reshape(-1, x.shape[-1])\n+        M, N = x_arg.shape\n+        _layer_norm_bwd_dx_fused[(M,)](dx, dy, _dw, _db, x, w, b, m, v, locks,\n+                                       x_arg.stride(0), N, ctx.eps,\n+                                       BLOCK_SIZE_N=ctx.BLOCK_SIZE,\n+                                       GROUP_SIZE_M=GROUP_SIZE_M,\n+                                       num_warps=ctx.num_warps)\n+        grid = lambda meta: [triton.cdiv(N, meta['BLOCK_SIZE_N'])]\n+        # accumulate partial sums in separate kernel\n+        _layer_norm_bwd_dwdb[grid](_dw, _db, dw, db, GROUP_SIZE_M, N,\n+                                   BLOCK_SIZE_M=32,\n+                                   BLOCK_SIZE_N=128)\n+        return dx, None, dw, db, None\n+\n+\n+layer_norm = LayerNorm.apply\n+\n+\n+def test_layer_norm(M, N, dtype, eps=1e-5, device='cuda'):\n+    # create data\n+    x_shape = (M, N)\n+    w_shape = (x_shape[-1], )\n+    weight = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n+    bias = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n+    x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\n+    dy = .1 * torch.randn_like(x)\n+    x.requires_grad_(True)\n+    # forward pass\n+    y_tri = layer_norm(x, w_shape, weight, bias, eps)\n+    y_ref = torch.nn.functional.layer_norm(x, w_shape, weight, bias, eps).to(dtype)\n+    # backward pass (triton)\n+    y_tri.backward(dy, retain_graph=True)\n+    dx_tri, dw_tri, db_tri = [_.grad.clone() for _ in [x, weight, bias]]\n+    x.grad, weight.grad, bias.grad = None, None, None\n+    # backward pass (torch)\n+    y_ref.backward(dy, retain_graph=True)\n+    dx_ref, dw_ref, db_ref = [_.grad.clone() for _ in [x, weight, bias]]\n+    # compare\n+    assert torch.allclose(y_tri, y_ref, atol=1e-2, rtol=0)\n+    assert torch.allclose(dx_tri, dx_ref, atol=1e-2, rtol=0)\n+    assert torch.allclose(db_tri, db_ref, atol=1e-2, rtol=0)\n+    assert torch.allclose(dw_tri, dw_ref, atol=1e-2, rtol=0)\n+\n+\n+@triton.testing.perf_report(\n+    triton.testing.Benchmark(\n+        x_names=['N'],\n+        x_vals=[512 * i for i in range(2, 32)],\n+        line_arg='provider',\n+        line_vals=['triton', 'torch'] + (['apex'] if HAS_APEX else []),\n+        line_names=['Triton', 'Torch'] + (['Apex'] if HAS_APEX else []),\n+        styles=[('blue', '-'), ('green', '-'), ('orange', '-')],\n+        ylabel='GB/s',\n+        plot_name='layer-norm-backward',\n+        args={'M': 4096, 'dtype': torch.float16, 'mode': 'backward'}\n+    )\n+)\n+def bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='cuda'):\n+    # create data\n+    x_shape = (M, N)\n+    w_shape = (x_shape[-1], )\n+    weight = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n+    bias = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n+    x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\n+    dy = .1 * torch.randn_like(x)\n+    x.requires_grad_(True)\n+    quantiles = [0.5, 0.2, 0.8]\n+    # utility functions\n+    if provider == 'triton':\n+        y_fwd = lambda: layer_norm(x, w_shape, weight, bias, eps)\n+    if provider == 'torch':\n+        y_fwd = lambda: torch.nn.functional.layer_norm(x, w_shape, weight, bias, eps)\n+    if provider == 'apex':\n+        apex_layer_norm = apex.normalization.FusedLayerNorm(w_shape).to(x.device).to(x.dtype)\n+        y_fwd = lambda: apex_layer_norm(x)\n+    # forward pass\n+    if mode == 'forward':\n+        gbps = lambda ms: 2 * x.numel() * x.element_size() / ms * 1e-6\n+        ms, min_ms, max_ms = triton.testing.do_bench(y_fwd, quantiles=quantiles, rep=500)\n+    # backward pass\n+    if mode == 'backward':\n+        gbps = lambda ms: 3 * x.numel() * x.element_size() / ms * 1e-6\n+        y = y_fwd()\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: y.backward(dy, retain_graph=True),\n+                                                     quantiles=quantiles, grad_to_none=[x], rep=500)\n+    return gbps(ms), gbps(max_ms), gbps(min_ms)\n+\n+\n+test_layer_norm(1151, 8192, torch.float16)\n+bench_layer_norm.run(save_path='.', print_data=True)\n+\n+# %%\n+# References\n+# ----------\n+#\n+# .. [BA2016] Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \"Layer Normalization\", Arxiv 2016"}, {"filename": "keren/fix-doc/_downloads/ae7fff29e1b574187bc930ed94bcc353/05-layer-norm.ipynb", "status": "added", "additions": 104, "deletions": 0, "changes": 104, "file_content_changes": "@@ -0,0 +1,104 @@\n+{\n+  \"cells\": [\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"%matplotlib inline\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"\\n# Layer Normalization\\nIn this tutorial, you will write a high-performance layer normalization\\nkernel that runs faster than the PyTorch implementation.\\n\\nIn doing so, you will learn about:\\n- Implementing backward pass in Triton\\n- Implementing parallel reduction in Triton\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Motivations\\n\\nThe *LayerNorm* operator was first introduced in [BA2016]_ as a way to improve the performance\\nof sequential models (e.g., Transformers) or neural networks with small batch size.\\nIt takes a vector $x$ as input and produces a vector $y$ of the same shape as output.\\nThe normalization is performed by subtracting the mean and dividing by the standard deviation of $x$.\\nAfter the normalization, a learnable linear transformation with weights $w$ and biases $b$ is applied.\\nThe forward pass can be expressed as follows:\\n\\n\\\\begin{align}y = \\\\frac{ x - \\\\text{E}[x] }{ \\\\sqrt{\\\\text{Var}(x) + \\\\epsilon} } * w + b\\\\end{align}\\n\\nwhere $\\\\epsilon$ is a small constant added to the denominator for numerical stability.\\nLet\\u2019s first take a look at the forward pass implementation.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\ntry:\\n    # This is https://github.com/NVIDIA/apex, NOT the apex on PyPi, so it\\n    # should not be added to extras_require in setup.py.\\n    import apex\\n    HAS_APEX = True\\nexcept ModuleNotFoundError:\\n    HAS_APEX = False\\n\\n\\n@triton.jit\\ndef _layer_norm_fwd_fused(\\n    X,  # pointer to the input\\n    Y,  # pointer to the output\\n    W,  # pointer to the weights\\n    B,  # pointer to the biases\\n    Mean,  # pointer to the mean\\n    Rstd,  # pointer to the 1/std\\n    stride,  # how much to increase the pointer when moving by 1 row\\n    N,  # number of columns in X\\n    eps,  # epsilon to avoid division by zero\\n    BLOCK_SIZE: tl.constexpr,\\n):\\n    # Map the program id to the row of X and Y it should compute.\\n    row = tl.program_id(0)\\n    Y += row * stride\\n    X += row * stride\\n    # Compute mean\\n    mean = 0\\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\\n    for off in range(0, N, BLOCK_SIZE):\\n        cols = off + tl.arange(0, BLOCK_SIZE)\\n        a = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\\n        _mean += a\\n    mean = tl.sum(_mean, axis=0) / N\\n    # Compute variance\\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\\n    for off in range(0, N, BLOCK_SIZE):\\n        cols = off + tl.arange(0, BLOCK_SIZE)\\n        x = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\\n        x = tl.where(cols < N, x - mean, 0.)\\n        _var += x * x\\n    var = tl.sum(_var, axis=0) / N\\n    rstd = 1 / tl.sqrt(var + eps)\\n    # Write mean / rstd\\n    tl.store(Mean + row, mean)\\n    tl.store(Rstd + row, rstd)\\n    # Normalize and apply linear transformation\\n    for off in range(0, N, BLOCK_SIZE):\\n        cols = off + tl.arange(0, BLOCK_SIZE)\\n        mask = cols < N\\n        w = tl.load(W + cols, mask=mask)\\n        b = tl.load(B + cols, mask=mask)\\n        x = tl.load(X + cols, mask=mask, other=0.).to(tl.float32)\\n        x_hat = (x - mean) * rstd\\n        y = x_hat * w + b\\n        # Write output\\n        tl.store(Y + cols, y, mask=mask)\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Backward pass\\n\\nThe backward pass for the layer normalization operator is a bit more involved than the forward pass.\\nLet $\\\\hat{x}$ be the normalized inputs $\\\\frac{ x - \\\\text{E}[x] }{ \\\\sqrt{\\\\text{Var}(x) + \\\\epsilon} }$ before the linear transformation,\\nthe Vector-Jacobian Products (VJP) $\\\\nabla_{x}$ of $x$ are given by:\\n\\n\\\\begin{align}\\\\nabla_{x} = \\\\frac{1}{\\\\sigma}\\\\Big( \\\\nabla_{y} \\\\odot w - \\\\underbrace{ \\\\big( \\\\frac{1}{N} \\\\hat{x} \\\\cdot (\\\\nabla_{y} \\\\odot w) \\\\big) }_{c_1} \\\\odot \\\\hat{x} - \\\\underbrace{ \\\\frac{1}{N} \\\\nabla_{y} \\\\cdot w }_{c_2} \\\\Big)\\\\end{align}\\n\\nwhere $\\\\odot$ denotes the element-wise multiplication, $\\\\cdot$ denotes the dot product, and $\\\\sigma$ is the standard deviation.\\n$c_1$ and $c_2$ are intermediate constants that improve the readability of the following implementation.\\n\\nFor the weights $w$ and biases $b$, the VJPs $\\\\nabla_{w}$ and $\\\\nabla_{b}$ are more straightforward:\\n\\n\\\\begin{align}\\\\nabla_{w} = \\\\nabla_{y} \\\\odot \\\\hat{x} \\\\quad \\\\text{and} \\\\quad \\\\nabla_{b} = \\\\nabla_{y}\\\\end{align}\\n\\nSince the same weights $w$ and biases $b$ are used for all rows in the same batch, their gradients need to sum up.\\nTo perform this step efficiently, we use a parallel reduction strategy: each kernel instance accumulates\\npartial $\\\\nabla_{w}$ and $\\\\nabla_{b}$ across certain rows into one of $\\\\text{GROUP_SIZE_M}$ independent buffers.\\nThese buffers stay in the L2 cache and then are further reduced by another function to compute the actual $\\\\nabla_{w}$ and $\\\\nabla_{b}$.\\n\\nLet the number of input rows $M = 4$ and $\\\\text{GROUP_SIZE_M} = 2$,\\nhere's a diagram of the parallel reduction strategy for $\\\\nabla_{w}$ ($\\\\nabla_{b}$ is omitted for brevity):\\n\\n  .. image:: parallel_reduction.png\\n\\nIn Stage 1, the rows of X that have the same color share the same buffer and thus a lock is used to ensure that only one kernel instance writes to the buffer at a time.\\nIn Stage 2, the buffers are further reduced to compute the final $\\\\nabla_{w}$ and $\\\\nabla_{b}$.\\nIn the following implementation, Stage 1 is implemented by the function :code:`_layer_norm_bwd_dx_fused` and Stage 2 is implemented by the function :code:`_layer_norm_bwd_dwdb`.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"@triton.jit\\ndef _layer_norm_bwd_dx_fused(\\n    DX,  # pointer to the input gradient\\n    DY,  # pointer to the output gradient\\n    DW,  # pointer to the partial sum of weights gradient\\n    DB,  # pointer to the partial sum of biases gradient\\n    X,   # pointer to the input\\n    W,   # pointer to the weights\\n    B,   # pointer to the biases\\n    Mean,   # pointer to the mean\\n    Rstd,   # pointer to the 1/std\\n    Lock,  # pointer to the lock\\n    stride,  # how much to increase the pointer when moving by 1 row\\n    N,  # number of columns in X\\n    eps,  # epsilon to avoid division by zero\\n    GROUP_SIZE_M: tl.constexpr,\\n    BLOCK_SIZE_N: tl.constexpr\\n):\\n    # Map the program id to the elements of X, DX, and DY it should compute.\\n    row = tl.program_id(0)\\n    cols = tl.arange(0, BLOCK_SIZE_N)\\n    mask = cols < N\\n    X += row * stride\\n    DY += row * stride\\n    DX += row * stride\\n    # Offset locks and weights/biases gradient pointer for parallel reduction\\n    lock_id = row % GROUP_SIZE_M\\n    Lock += lock_id\\n    Count = Lock + GROUP_SIZE_M\\n    DW = DW + lock_id * N + cols\\n    DB = DB + lock_id * N + cols\\n    # Load data to SRAM\\n    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\\n    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\\n    mean = tl.load(Mean + row)\\n    rstd = tl.load(Rstd + row)\\n    # Compute dx\\n    xhat = (x - mean) * rstd\\n    wdy = w * dy\\n    xhat = tl.where(mask, xhat, 0.)\\n    wdy = tl.where(mask, wdy, 0.)\\n    c1 = tl.sum(xhat * wdy, axis=0) / N\\n    c2 = tl.sum(wdy, axis=0) / N\\n    dx = (wdy - (xhat * c1 + c2)) * rstd\\n    # Write dx\\n    tl.store(DX + cols, dx, mask=mask)\\n    # Accumulate partial sums for dw/db\\n    partial_dw = (dy * xhat).to(w.dtype)\\n    partial_db = (dy).to(w.dtype)\\n    while tl.atomic_cas(Lock, 0, 1) == 1:\\n        pass\\n    count = tl.load(Count)\\n    # First store doesn't accumulate\\n    if count == 0:\\n        tl.atomic_xchg(Count, 1)\\n    else:\\n        partial_dw += tl.load(DW, mask=mask)\\n        partial_db += tl.load(DB, mask=mask)\\n    tl.store(DW, partial_dw, mask=mask)\\n    tl.store(DB, partial_db, mask=mask)\\n    # Release the lock\\n    tl.atomic_xchg(Lock, 0)\\n\\n\\n@triton.jit\\ndef _layer_norm_bwd_dwdb(\\n    DW,  # pointer to the partial sum of weights gradient\\n    DB,  # pointer to the partial sum of biases gradient\\n    FINAL_DW,  # pointer to the weights gradient\\n    FINAL_DB,  # pointer to the biases gradient\\n    M,  # GROUP_SIZE_M\\n    N,  # number of columns\\n    BLOCK_SIZE_M: tl.constexpr,\\n    BLOCK_SIZE_N: tl.constexpr\\n):\\n    # Map the program id to the elements of DW and DB it should compute.\\n    pid = tl.program_id(0)\\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\\n    db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\\n    # Iterate through the rows of DW and DB to sum the partial sums.\\n    for i in range(0, M, BLOCK_SIZE_M):\\n        rows = i + tl.arange(0, BLOCK_SIZE_M)\\n        mask = (rows[:, None] < M) & (cols[None, :] < N)\\n        offs = rows[:, None] * N + cols[None, :]\\n        dw += tl.load(DW + offs, mask=mask, other=0.)\\n        db += tl.load(DB + offs, mask=mask, other=0.)\\n    # Write the final sum to the output.\\n    sum_dw = tl.sum(dw, axis=0)\\n    sum_db = tl.sum(db, axis=0)\\n    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\\n    tl.store(FINAL_DB + cols, sum_db, mask=cols < N)\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Benchmark\\n\\nWe can now compare the performance of our kernel against that of PyTorch.\\nHere we focus on inputs that have Less than 64KB per feature.\\nSpecifically, one can set :code:`'mode': 'backward'` to benchmark the backward pass.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"class LayerNorm(torch.autograd.Function):\\n\\n    @staticmethod\\n    def forward(ctx, x, normalized_shape, weight, bias, eps):\\n        # allocate output\\n        y = torch.empty_like(x)\\n        # reshape input data into 2D tensor\\n        x_arg = x.reshape(-1, x.shape[-1])\\n        M, N = x_arg.shape\\n        mean = torch.empty((M, ), dtype=torch.float32, device='cuda')\\n        rstd = torch.empty((M, ), dtype=torch.float32, device='cuda')\\n        # Less than 64KB per feature: enqueue fused kernel\\n        MAX_FUSED_SIZE = 65536 // x.element_size()\\n        BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\\n        if N > BLOCK_SIZE:\\n            raise RuntimeError(\\\"This layer norm doesn't support feature dim >= 64KB.\\\")\\n        # heuristics for number of warps\\n        num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\\n        # enqueue kernel\\n        _layer_norm_fwd_fused[(M,)](x_arg, y, weight, bias, mean, rstd,\\n                                    x_arg.stride(0), N, eps,\\n                                    BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)\\n        ctx.save_for_backward(x, weight, bias, mean, rstd)\\n        ctx.BLOCK_SIZE = BLOCK_SIZE\\n        ctx.num_warps = num_warps\\n        ctx.eps = eps\\n        return y\\n\\n    @staticmethod\\n    def backward(ctx, dy):\\n        x, w, b, m, v = ctx.saved_tensors\\n        # heuristics for amount of parallel reduction stream for DW/DB\\n        N = w.shape[0]\\n        GROUP_SIZE_M = 64\\n        if N <= 8192: GROUP_SIZE_M = 96\\n        if N <= 4096: GROUP_SIZE_M = 128\\n        if N <= 1024: GROUP_SIZE_M = 256\\n        # allocate output\\n        locks = torch.zeros(2 * GROUP_SIZE_M, dtype=torch.int32, device='cuda')\\n        _dw = torch.empty((GROUP_SIZE_M, w.shape[0]), dtype=x.dtype, device=w.device)\\n        _db = torch.empty((GROUP_SIZE_M, w.shape[0]), dtype=x.dtype, device=w.device)\\n        dw = torch.empty((w.shape[0],), dtype=w.dtype, device=w.device)\\n        db = torch.empty((w.shape[0],), dtype=w.dtype, device=w.device)\\n        dx = torch.empty_like(dy)\\n        # enqueue kernel using forward pass heuristics\\n        # also compute partial sums for DW and DB\\n        x_arg = x.reshape(-1, x.shape[-1])\\n        M, N = x_arg.shape\\n        _layer_norm_bwd_dx_fused[(M,)](dx, dy, _dw, _db, x, w, b, m, v, locks,\\n                                       x_arg.stride(0), N, ctx.eps,\\n                                       BLOCK_SIZE_N=ctx.BLOCK_SIZE,\\n                                       GROUP_SIZE_M=GROUP_SIZE_M,\\n                                       num_warps=ctx.num_warps)\\n        grid = lambda meta: [triton.cdiv(N, meta['BLOCK_SIZE_N'])]\\n        # accumulate partial sums in separate kernel\\n        _layer_norm_bwd_dwdb[grid](_dw, _db, dw, db, GROUP_SIZE_M, N,\\n                                   BLOCK_SIZE_M=32,\\n                                   BLOCK_SIZE_N=128)\\n        return dx, None, dw, db, None\\n\\n\\nlayer_norm = LayerNorm.apply\\n\\n\\ndef test_layer_norm(M, N, dtype, eps=1e-5, device='cuda'):\\n    # create data\\n    x_shape = (M, N)\\n    w_shape = (x_shape[-1], )\\n    weight = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\\n    bias = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\\n    x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\\n    dy = .1 * torch.randn_like(x)\\n    x.requires_grad_(True)\\n    # forward pass\\n    y_tri = layer_norm(x, w_shape, weight, bias, eps)\\n    y_ref = torch.nn.functional.layer_norm(x, w_shape, weight, bias, eps).to(dtype)\\n    # backward pass (triton)\\n    y_tri.backward(dy, retain_graph=True)\\n    dx_tri, dw_tri, db_tri = [_.grad.clone() for _ in [x, weight, bias]]\\n    x.grad, weight.grad, bias.grad = None, None, None\\n    # backward pass (torch)\\n    y_ref.backward(dy, retain_graph=True)\\n    dx_ref, dw_ref, db_ref = [_.grad.clone() for _ in [x, weight, bias]]\\n    # compare\\n    assert torch.allclose(y_tri, y_ref, atol=1e-2, rtol=0)\\n    assert torch.allclose(dx_tri, dx_ref, atol=1e-2, rtol=0)\\n    assert torch.allclose(db_tri, db_ref, atol=1e-2, rtol=0)\\n    assert torch.allclose(dw_tri, dw_ref, atol=1e-2, rtol=0)\\n\\n\\n@triton.testing.perf_report(\\n    triton.testing.Benchmark(\\n        x_names=['N'],\\n        x_vals=[512 * i for i in range(2, 32)],\\n        line_arg='provider',\\n        line_vals=['triton', 'torch'] + (['apex'] if HAS_APEX else []),\\n        line_names=['Triton', 'Torch'] + (['Apex'] if HAS_APEX else []),\\n        styles=[('blue', '-'), ('green', '-'), ('orange', '-')],\\n        ylabel='GB/s',\\n        plot_name='layer-norm-backward',\\n        args={'M': 4096, 'dtype': torch.float16, 'mode': 'backward'}\\n    )\\n)\\ndef bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='cuda'):\\n    # create data\\n    x_shape = (M, N)\\n    w_shape = (x_shape[-1], )\\n    weight = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\\n    bias = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\\n    x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\\n    dy = .1 * torch.randn_like(x)\\n    x.requires_grad_(True)\\n    quantiles = [0.5, 0.2, 0.8]\\n    # utility functions\\n    if provider == 'triton':\\n        y_fwd = lambda: layer_norm(x, w_shape, weight, bias, eps)\\n    if provider == 'torch':\\n        y_fwd = lambda: torch.nn.functional.layer_norm(x, w_shape, weight, bias, eps)\\n    if provider == 'apex':\\n        apex_layer_norm = apex.normalization.FusedLayerNorm(w_shape).to(x.device).to(x.dtype)\\n        y_fwd = lambda: apex_layer_norm(x)\\n    # forward pass\\n    if mode == 'forward':\\n        gbps = lambda ms: 2 * x.numel() * x.element_size() / ms * 1e-6\\n        ms, min_ms, max_ms = triton.testing.do_bench(y_fwd, quantiles=quantiles, rep=500)\\n    # backward pass\\n    if mode == 'backward':\\n        gbps = lambda ms: 3 * x.numel() * x.element_size() / ms * 1e-6\\n        y = y_fwd()\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: y.backward(dy, retain_graph=True),\\n                                                     quantiles=quantiles, grad_to_none=[x], rep=500)\\n    return gbps(ms), gbps(max_ms), gbps(min_ms)\\n\\n\\ntest_layer_norm(1151, 8192, torch.float16)\\nbench_layer_norm.run(save_path='.', print_data=True)\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## References\\n\\n.. [BA2016] Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \\\"Layer Normalization\\\", Arxiv 2016\\n\\n\"\n+      ]\n+    }\n+  ],\n+  \"metadata\": {\n+    \"kernelspec\": {\n+      \"display_name\": \"Python 3\",\n+      \"language\": \"python\",\n+      \"name\": \"python3\"\n+    },\n+    \"language_info\": {\n+      \"codemirror_mode\": {\n+        \"name\": \"ipython\",\n+        \"version\": 3\n+      },\n+      \"file_extension\": \".py\",\n+      \"mimetype\": \"text/x-python\",\n+      \"name\": \"python\",\n+      \"nbconvert_exporter\": \"python\",\n+      \"pygments_lexer\": \"ipython3\",\n+      \"version\": \"3.8.10\"\n+    }\n+  },\n+  \"nbformat\": 4,\n+  \"nbformat_minor\": 0\n+}\n\\ No newline at end of file"}, {"filename": "keren/fix-doc/_downloads/b51b68bc1c6b1a5e509f67800b6235af/03-matrix-multiplication.ipynb", "status": "added", "additions": 129, "deletions": 0, "changes": 129, "file_content_changes": "@@ -0,0 +1,129 @@\n+{\n+  \"cells\": [\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"%matplotlib inline\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"\\n# Matrix Multiplication\\nIn this tutorial, you will write a 25-lines high-performance FP16 matrix multiplication\\nkernel that achieves performance on par with cuBLAS.\\n\\nIn doing so, you will learn about:\\n- Block-level matrix multiplications\\n- Multi-dimensional pointer arithmetic\\n- Program re-ordering for improved L2 cache hit rate\\n- Automatic performance tuning\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Motivations\\n\\nMatrix multiplications are a key building block of most modern high-performance computing systems.\\nThey are notoriously hard to optimize, hence their implementation is generally done by\\nhardware vendors themselves as part of so-called \\\"kernel libraries\\\" (e.g., cuBLAS).\\nUnfortunately, these libraries are often proprietary and cannot be easily customized\\nto accommodate the needs of modern deep learning workloads (e.g., fused activation functions).\\nIn this tutorial, you will learn how to implement efficient matrix multiplications by\\nyourself with Triton, in a way that is easy to customize and extend.\\n\\nRoughly speaking, the kernel that we will write will implement the following blocked\\nalgorithm to multiply a (M, K) by a (K, N) matrix:\\n\\n .. code-block:: python\\n\\n   # do in parallel\\n   for m in range(0, M, BLOCK_SIZE_M):\\n     # do in parallel\\n     for n in range(0, N, BLOCK_SIZE_N):\\n       acc = zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=float32)\\n       for k in range(0, K, BLOCK_SIZE_K):\\n         a = A[m : m+BLOCK_SIZE_M, k : k+BLOCK_SIZE_K]\\n         b = B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]\\n         acc += dot(a, b)\\n       C[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N] = acc;\\n\\nwhere each iteration of the doubly-nested for-loop is performed by a dedicated Triton program instance.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Compute Kernel\\n\\nThe above algorithm is, actually, fairly straightforward to implement in Triton.\\nThe main difficulty comes from the computation of the memory locations at which blocks\\nof :code:`A` and :code:`B` must be read in the inner loop. For that, we need\\nmulti-dimensional pointer arithmetic.\\n\\n\\n### Pointer Arithmetic\\n\\nFor a row-major 2D tensor :code:`X`, the memory location of :code:`X[i, j]` is given b\\ny :code:`&X[i, j] = X + i*stride_xi + j*stride_xj`.\\nTherefore, blocks of pointers for :code:`A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K]` and\\n:code:`B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]` can be defined in pseudo-code as:\\n\\n .. code-block:: python\\n\\n   &A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K] =  a_ptr + (m : m+BLOCK_SIZE_M)[:, None]*A.stride(0) + (k : k+BLOCK_SIZE_K)[None, :]*A.stride(1);\\n   &B[k : k+BLOCK_SIZE_K, n:n+BLOCK_SIZE_N] =  b_ptr + (k : k+BLOCK_SIZE_K)[:, None]*B.stride(0) + (n : n+BLOCK_SIZE_N)[None, :]*B.stride(1);\\n\\nWhich means that pointers for blocks of A and B can be initialized (i.e., :code:`k=0`) in Triton as:\\n\\n .. code-block:: python\\n\\n   offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\\n   offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\\n   offs_k = tl.arange(0, BLOCK_SIZE_K)\\n   a_ptrs = a_ptr + (offs_am[:, None]*stride_am + offs_k [None, :]*stride_ak)\\n   b_ptrs = b_ptr + (offs_k [:, None]*stride_bk + offs_bn[None, :]*stride_bn)\\n\\nAnd then updated in the inner loop as follows:\\n\\n .. code-block:: python\\n\\n   a_ptrs += BLOCK_SIZE_K * stride_ak;\\n   b_ptrs += BLOCK_SIZE_K * stride_bk;\\n\\n\\n### L2 Cache Optimizations\\n\\nAs mentioned above, each program instance computes a :code:`[BLOCK_SIZE_M, BLOCK_SIZE_N]`\\nblock of :code:`C`.\\nIt is important to remember that the order in which these blocks are computed does\\nmatter, since it affects the L2 cache hit rate of our program. and unfortunately, a\\na simple row-major ordering\\n\\n .. code-block:: Python\\n\\n   pid = triton.program_id(0);\\n   grid_m = (M + BLOCK_SIZE_M - 1) // BLOCK_SIZE_M;\\n   grid_n = (N + BLOCK_SIZE_N - 1) // BLOCK_SIZE_N;\\n   pid_m = pid / grid_n;\\n   pid_n = pid % grid_n;\\n\\nis just not going to cut it.\\n\\nOne possible solution is to launch blocks in an order that promotes data reuse.\\nThis can be done by 'super-grouping' blocks in groups of :code:`GROUP_M` rows before\\nswitching to the next column:\\n\\n .. code-block:: python\\n\\n   # program ID\\n   pid = tl.program_id(axis=0)\\n   # number of program ids along the M axis\\n   num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\\n   # number of programs ids along the N axis\\n   num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\\n   # number of programs in group\\n   num_pid_in_group = GROUP_SIZE_M * num_pid_n\\n   # id of the group this program is in\\n   group_id = pid // num_pid_in_group\\n   # row-id of the first program in the group\\n   first_pid_m = group_id * GROUP_SIZE_M\\n   # if `num_pid_m` isn't divisible by `GROUP_SIZE_M`, the last group is smaller\\n   group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\\n   # *within groups*, programs are ordered in a column-major order\\n   # row-id of the program in the *launch grid*\\n   pid_m = first_pid_m + (pid % group_size_m)\\n   # col-id of the program in the *launch grid*\\n   pid_n = (pid % num_pid_in_group) // group_size_m\\n\\nFor example, in the following matmul where each matrix is 9 blocks by 9 blocks,\\nwe can see that if we compute the output in row-major ordering, we need to load 90\\nblocks into SRAM to compute the first 9 output blocks, but if we do it in grouped\\nordering, we only need to load 54 blocks.\\n  .. image:: grouped_vs_row_major_ordering.png\\n\\nIn practice, this can improve the performance of our matrix multiplication kernel by\\nmore than 10\\\\% on some hardware architecture (e.g., 220 to 245 TFLOPS on A100).\\n\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Final Result\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n# %\\n# :code:`triton.jit`'ed functions can be auto-tuned by using the `triton.autotune`\\n# decorator, which consumes:\\n#   - A list of :code:`triton.Config` objects that define different configurations of\\n#       meta-parameters (e.g., BLOCK_SIZE_M) and compilation options (e.g., num_warps) to try\\n#   - An autotuning *key* whose change in values will trigger evaluation of all the\\n#       provided configs\\n\\n\\n@triton.autotune(\\n    configs=[\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\\n    ],\\n    key=['M', 'N', 'K'],\\n)\\n@triton.jit\\ndef matmul_kernel(\\n    # Pointers to matrices\\n    a_ptr, b_ptr, c_ptr,\\n    # Matrix dimensions\\n    M, N, K,\\n    # The stride variables represent how much to increase the ptr by when moving by 1\\n    # element in a particular dimension. E.g. stride_am is how much to increase a_ptr\\n    # by to get the element one row down (A has M rows)\\n    stride_am, stride_ak,\\n    stride_bk, stride_bn,\\n    stride_cm, stride_cn,\\n    # Meta-parameters\\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\\n    GROUP_SIZE_M: tl.constexpr,\\n    ACTIVATION: tl.constexpr,\\n):\\n    \\\"\\\"\\\"Kernel for computing the matmul C = A x B.\\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\\n    \\\"\\\"\\\"\\n    # -----------------------------------------------------------\\n    # Map program ids `pid` to the block of C it should compute.\\n    # This is done in a grouped ordering to promote L2 data reuse\\n    # See above `L2 Cache Optimizations` section for details\\n    pid = tl.program_id(axis=0)\\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\\n    group_id = pid // num_pid_in_group\\n    first_pid_m = group_id * GROUP_SIZE_M\\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\\n    pid_m = first_pid_m + (pid % group_size_m)\\n    pid_n = (pid % num_pid_in_group) // group_size_m\\n\\n    # ----------------------------------------------------------\\n    # Create pointers for the first blocks of A and B.\\n    # We will advance this pointer as we move in the K direction\\n    # and accumulate\\n    # a_ptrs is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\\n    # b_ptrs is a block of [BLOCK_SIZE_K, BLOCK_SIZE_n] pointers\\n    # see above `Pointer Arithmetic` section for details\\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\\n\\n    # -----------------------------------------------------------\\n    # Iterate to compute a block of the C matrix\\n    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\\n    # of fp32 values for higher accuracy.\\n    # `accumulator` will be converted back to fp16 after the loop\\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\\n    for k in range(0, num_pid_k):\\n        # Note that for simplicity, we don't apply a mask here.\\n        # This means that if K is not a multiple of BLOCK_SIZE_K,\\n        # this will access out-of-bounds memory and produce an\\n        # error or (worse!) incorrect results.\\n        a = tl.load(a_ptrs)\\n        b = tl.load(b_ptrs)\\n        # We accumulate along the K dimension\\n        accumulator += tl.dot(a, b)\\n        # Advance the ptrs to the next K block\\n        a_ptrs += BLOCK_SIZE_K * stride_ak\\n        b_ptrs += BLOCK_SIZE_K * stride_bk\\n    # you can fuse arbitrary activation functions here\\n    # while the accumulator is still in FP32!\\n    if ACTIVATION:\\n        accumulator = ACTIVATION(accumulator)\\n    c = accumulator.to(tl.float16)\\n\\n    # -----------------------------------------------------------\\n    # Write back the block of the output matrix C\\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\\n    tl.store(c_ptrs, c, mask=c_mask)\\n\\n\\n# we can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`\\n@triton.jit\\ndef leaky_relu(x):\\n    return tl.where(x >= 0, x, 0.01 * x)\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"We can now create a convenience wrapper function that only takes two input tensors\\nand (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"def matmul(a, b, activation=None):\\n    # checks constraints\\n    assert a.shape[1] == b.shape[0], \\\"incompatible dimensions\\\"\\n    assert a.is_contiguous(), \\\"matrix A must be contiguous\\\"\\n    assert b.is_contiguous(), \\\"matrix B must be contiguous\\\"\\n    M, K = a.shape\\n    K, N = b.shape\\n    assert (\\n        K % 32 == 0\\n    ), \\\"We don't check memory-out-of-bounds with K so K must be divisible by BLOCK_SIZE_K\\\"\\n    # allocates output\\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\\n    # 1D launch kernel where each block gets its own program.\\n    grid = lambda META: (\\n        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\\n    )\\n    matmul_kernel[grid](\\n        a, b, c,\\n        M, N, K,\\n        a.stride(0), a.stride(1),\\n        b.stride(0), b.stride(1),\\n        c.stride(0), c.stride(1),\\n        ACTIVATION=activation,\\n    )\\n    return c\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Unit Test\\n\\nWe can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS)\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"torch.manual_seed(0)\\na = torch.randn((512, 512), device='cuda', dtype=torch.float16)\\nb = torch.randn((512, 512), device='cuda', dtype=torch.float16)\\ntriton_output = matmul(a, b, activation=None)\\ntorch_output = torch.matmul(a, b)\\nprint(f\\\"triton_output={triton_output}\\\")\\nprint(f\\\"torch_output={torch_output}\\\")\\nif torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\\n    print(\\\"\\u2705 Triton and Torch match\\\")\\nelse:\\n    print(\\\"\\u274c Triton and Torch differ\\\")\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Benchmark\\n\\n### Square Matrix Performance\\n\\nWe can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices, but feel free to arrange this script as you wish to benchmark any other matrix shape.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"@triton.testing.perf_report(\\n    triton.testing.Benchmark(\\n        x_names=['M', 'N', 'K'],  # argument names to use as an x-axis for the plot\\n        x_vals=[\\n            128 * i for i in range(2, 33)\\n        ],  # different possible values for `x_name`\\n        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\\n        # possible values for `line_arg``\\n        line_vals=['cublas', 'triton'],\\n        # label name for the lines\\n        line_names=[\\\"cuBLAS\\\", \\\"Triton\\\"],\\n        # line styles\\n        styles=[('green', '-'), ('blue', '-')],\\n        ylabel=\\\"TFLOPS\\\",  # label name for the y-axis\\n        plot_name=\\\"matmul-performance\\\",  # name for the plot. Used also as a file name for saving the plot.\\n        args={},\\n    )\\n)\\ndef benchmark(M, N, K, provider):\\n    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\\n    b = torch.randn((K, N), device='cuda', dtype=torch.float16)\\n    quantiles = [0.5, 0.2, 0.8]\\n    if provider == 'cublas':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\\n    if provider == 'triton':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\\n    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\\n    return perf(ms), perf(max_ms), perf(min_ms)\\n\\n\\nbenchmark.run(show_plots=True, print_data=True)\"\n+      ]\n+    }\n+  ],\n+  \"metadata\": {\n+    \"kernelspec\": {\n+      \"display_name\": \"Python 3\",\n+      \"language\": \"python\",\n+      \"name\": \"python3\"\n+    },\n+    \"language_info\": {\n+      \"codemirror_mode\": {\n+        \"name\": \"ipython\",\n+        \"version\": 3\n+      },\n+      \"file_extension\": \".py\",\n+      \"mimetype\": \"text/x-python\",\n+      \"name\": \"python\",\n+      \"nbconvert_exporter\": \"python\",\n+      \"pygments_lexer\": \"ipython3\",\n+      \"version\": \"3.8.10\"\n+    }\n+  },\n+  \"nbformat\": 4,\n+  \"nbformat_minor\": 0\n+}\n\\ No newline at end of file"}, {"filename": "keren/fix-doc/_downloads/bc847dec325798bdc436c4ef5ac8b78a/04-low-memory-dropout.ipynb", "status": "added", "additions": 100, "deletions": 0, "changes": 100, "file_content_changes": "@@ -0,0 +1,100 @@\n+{\n+  \"cells\": [\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"%matplotlib inline\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"\\n# Low-Memory Dropout\\n\\nIn this tutorial, you will write a memory-efficient implementation of dropout whose state\\nwill be composed of a single int32 seed. This differs from more traditional implementations of dropout,\\nwhose state is generally composed of a bit mask tensor of the same shape as the input.\\n\\nIn doing so, you will learn about:\\n- The limitations of naive implementations of Dropout with PyTorch\\n- Parallel pseudo-random number generation in Triton\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Baseline\\n\\nThe *dropout* operator was first introduced in [SRIVASTAVA2014]_ as a way to improve the performance\\nof deep neural networks in low-data regime (i.e. regularization).\\n\\nIt takes a vector as input and produces a vector of the same shape as output. Each scalar in the\\noutput has a probability $p$ of being changed to zero and otherwise it is copied from the input.\\nThis forces the network to perform well even when only $1 - p$ scalars from the input are available.\\n\\nAt evaluation time we want to use the full power of the network so we set $p=0$. Naively this would\\nincrease the norm of the output (which can be a bad thing, e.g. it can lead to artificial decrease\\nin the output softmax temperature). To prevent this we multiply the output by $\\\\frac{1}{1 - p}$, which\\nkeeps the norm consistent regardless of the dropout probability.\\n\\nLet's first take a look at the baseline implementation.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import tabulate\\nimport torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.jit\\ndef _dropout(\\n    x_ptr,  # pointer to the input\\n    x_keep_ptr,  # pointer to a mask of 0s and 1s\\n    output_ptr,  # pointer to the output\\n    n_elements,  # number of elements in the `x` tensor\\n    p,  # probability that an element of `x` is changed to zero\\n    BLOCK_SIZE: tl.constexpr,\\n):\\n    pid = tl.program_id(axis=0)\\n    block_start = pid * BLOCK_SIZE\\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\\n    mask = offsets < n_elements\\n    # Load data\\n    x = tl.load(x_ptr + offsets, mask=mask)\\n    x_keep = tl.load(x_keep_ptr + offsets, mask=mask)\\n    # The line below is the crucial part, described in the paragraph above!\\n    output = tl.where(x_keep, x / (1 - p), 0.0)\\n    # Write-back output\\n    tl.store(output_ptr + offsets, output, mask=mask)\\n\\n\\ndef dropout(x, x_keep, p):\\n    output = torch.empty_like(x)\\n    assert x.is_contiguous()\\n    n_elements = x.numel()\\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\\n    _dropout[grid](x, x_keep, output, n_elements, p, BLOCK_SIZE=1024)\\n    return output\\n\\n\\n# Input tensor\\nx = torch.randn(size=(10,)).cuda()\\n# Dropout mask\\np = 0.5\\nx_keep = (torch.rand(size=(10,)) > p).to(torch.int32).cuda()\\n#\\noutput = dropout(x, x_keep=x_keep, p=p)\\nprint(tabulate.tabulate([\\n    [\\\"input\\\"] + x.tolist(),\\n    [\\\"keep mask\\\"] + x_keep.tolist(),\\n    [\\\"output\\\"] + output.tolist()\\n]))\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Seeded dropout\\n\\nThe above implementation of dropout works fine, but it can be a bit awkward to deal with. Firstly\\nwe need to store the dropout mask for backpropagation. Secondly, dropout state management can get\\nvery tricky when using recompute/checkpointing (e.g. see all the notes about `preserve_rng_state` in\\nhttps://pytorch.org/docs/1.9.0/checkpoint.html). In this tutorial we'll describe an alternative implementation\\nthat (1) has a smaller memory footprint; (2) requires less data movement; and (3) simplifies the management\\nof persisting randomness across multiple invocations of the kernel.\\n\\nPseudo-random number generation in Triton is simple! In this tutorial we will use the\\n:code:`triton.language.rand` function which generates a block of uniformly distributed :code:`float32`\\nvalues in [0, 1), given a seed and a block of :code:`int32` offsets. But if you need it, Triton also provides\\nother `random number generation strategies <Random Number Generation>`.\\n\\n<div class=\\\"alert alert-info\\\"><h4>Note</h4><p>Triton's implementation of PRNG is based on the Philox algorithm (described on [SALMON2011]_).</p></div>\\n\\nLet's put it all together.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"@triton.jit\\ndef _seeded_dropout(\\n    x_ptr,\\n    output_ptr,\\n    n_elements,\\n    p,\\n    seed,\\n    BLOCK_SIZE: tl.constexpr,\\n):\\n    # compute memory offsets of elements handled by this instance\\n    pid = tl.program_id(axis=0)\\n    block_start = pid * BLOCK_SIZE\\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\\n    # load data from x\\n    mask = offsets < n_elements\\n    x = tl.load(x_ptr + offsets, mask=mask)\\n    # randomly prune it\\n    random = tl.rand(seed, offsets)\\n    x_keep = random > p\\n    # write-back\\n    output = tl.where(x_keep, x / (1 - p), 0.0)\\n    tl.store(output_ptr + offsets, output, mask=mask)\\n\\n\\ndef seeded_dropout(x, p, seed):\\n    output = torch.empty_like(x)\\n    assert x.is_contiguous()\\n    n_elements = x.numel()\\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\\n    _seeded_dropout[grid](x, output, n_elements, p, seed, BLOCK_SIZE=1024)\\n    return output\\n\\n\\nx = torch.randn(size=(10,)).cuda()\\n# Compare this to the baseline - dropout mask is never instantiated!\\noutput = seeded_dropout(x, p=0.5, seed=123)\\noutput2 = seeded_dropout(x, p=0.5, seed=123)\\noutput3 = seeded_dropout(x, p=0.5, seed=512)\\n\\nprint(tabulate.tabulate([\\n    [\\\"input\\\"] + x.tolist(),\\n    [\\\"output (seed = 123)\\\"] + output.tolist(),\\n    [\\\"output (seed = 123)\\\"] + output2.tolist(),\\n    [\\\"output (seed = 512)\\\"] + output3.tolist()\\n]))\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"Et Voil\\u00e0! We have a triton kernel that applies the same dropout mask provided the seed is the same!\\nIf you'd like explore further applications of pseudorandomness in GPU programming, we encourage you\\nto explore the `triton/language/random` folder!\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Exercises\\n\\n1. Extend the kernel to operate over a matrix and use a vector of seeds - one per row.\\n2. Add support for striding.\\n3. (challenge) Implement a kernel for sparse Johnson-Lindenstrauss transform which generates the projection matrix one the fly each time using a seed.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## References\\n\\n.. [SALMON2011] John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw, \\\"Parallel Random Numbers: As Easy as 1, 2, 3\\\", 2011\\n.. [SRIVASTAVA2014] Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \\\"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\\\", JMLR 2014\\n\\n\"\n+      ]\n+    }\n+  ],\n+  \"metadata\": {\n+    \"kernelspec\": {\n+      \"display_name\": \"Python 3\",\n+      \"language\": \"python\",\n+      \"name\": \"python3\"\n+    },\n+    \"language_info\": {\n+      \"codemirror_mode\": {\n+        \"name\": \"ipython\",\n+        \"version\": 3\n+      },\n+      \"file_extension\": \".py\",\n+      \"mimetype\": \"text/x-python\",\n+      \"name\": \"python\",\n+      \"nbconvert_exporter\": \"python\",\n+      \"pygments_lexer\": \"ipython3\",\n+      \"version\": \"3.8.10\"\n+    }\n+  },\n+  \"nbformat\": 4,\n+  \"nbformat_minor\": 0\n+}\n\\ No newline at end of file"}, {"filename": "keren/fix-doc/_downloads/c9aed78977a4c05741d675a38dde3d7d/04-low-memory-dropout.py", "status": "renamed", "additions": 24, "deletions": 20, "changes": 44, "file_content_changes": "@@ -1,18 +1,20 @@\n \"\"\"\n Low-Memory Dropout\n-=================\n+==================\n \n In this tutorial, you will write a memory-efficient implementation of dropout whose state\n will be composed of a single int32 seed. This differs from more traditional implementations of dropout,\n-whose state is generally composed of a bit mask tensor of the same shape as the input. You will learn about:\n+whose state is generally composed of a bit mask tensor of the same shape as the input.\n \n+In doing so, you will learn about:\n - The limitations of naive implementations of Dropout with PyTorch\n - Parallel pseudo-random number generation in Triton\n \"\"\"\n \n # %%\n # Baseline\n-# -------------\n+# --------\n+#\n # The *dropout* operator was first introduced in [SRIVASTAVA2014]_ as a way to improve the performance\n # of deep neural networks in low-data regime (i.e. regularization).\n #\n@@ -37,12 +39,12 @@\n \n @triton.jit\n def _dropout(\n-        x_ptr,  # pointer to the input\n-        x_keep_ptr,  # pointer to a mask of 0s and 1s\n-        output_ptr,  # pointer to the output\n-        n_elements,  # number of elements in the `x` tensor\n-        p,  # probability that an element of `x` is changed to zero\n-        BLOCK_SIZE: tl.constexpr,\n+    x_ptr,  # pointer to the input\n+    x_keep_ptr,  # pointer to a mask of 0s and 1s\n+    output_ptr,  # pointer to the output\n+    n_elements,  # number of elements in the `x` tensor\n+    p,  # probability that an element of `x` is changed to zero\n+    BLOCK_SIZE: tl.constexpr,\n ):\n     pid = tl.program_id(axis=0)\n     block_start = pid * BLOCK_SIZE\n@@ -81,15 +83,16 @@ def dropout(x, x_keep, p):\n \n # %%\n # Seeded dropout\n-# -------------\n-# Above implementation of dropout works fine, but it can be a bit awkward to deal with. Firstly\n+# --------------\n+#\n+# The above implementation of dropout works fine, but it can be a bit awkward to deal with. Firstly\n # we need to store the dropout mask for backpropagation. Secondly, dropout state management can get\n # very tricky when using recompute/checkpointing (e.g. see all the notes about `preserve_rng_state` in\n # https://pytorch.org/docs/1.9.0/checkpoint.html). In this tutorial we'll describe an alternative implementation\n # that (1) has a smaller memory footprint; (2) requires less data movement; and (3) simplifies the management\n # of persisting randomness across multiple invocations of the kernel.\n #\n-# Pseudorandom number generation in Triton is simple! In this tutorial we will use the\n+# Pseudo-random number generation in Triton is simple! In this tutorial we will use the\n # :code:`triton.language.rand` function which generates a block of uniformly distributed :code:`float32`\n # values in [0, 1), given a seed and a block of :code:`int32` offsets. But if you need it, Triton also provides\n # other :ref:`random number generation strategies <Random Number Generation>`.\n@@ -102,12 +105,12 @@ def dropout(x, x_keep, p):\n \n @triton.jit\n def _seeded_dropout(\n-        x_ptr,\n-        output_ptr,\n-        n_elements,\n-        p,\n-        seed,\n-        BLOCK_SIZE: tl.constexpr,\n+    x_ptr,\n+    output_ptr,\n+    n_elements,\n+    p,\n+    seed,\n+    BLOCK_SIZE: tl.constexpr,\n ):\n     # compute memory offsets of elements handled by this instance\n     pid = tl.program_id(axis=0)\n@@ -153,14 +156,15 @@ def seeded_dropout(x, p, seed):\n \n # %%\n # Exercises\n-# -------------\n+# ---------\n+#\n # 1. Extend the kernel to operate over a matrix and use a vector of seeds - one per row.\n # 2. Add support for striding.\n # 3. (challenge) Implement a kernel for sparse Johnson-Lindenstrauss transform which generates the projection matrix one the fly each time using a seed.\n \n # %%\n # References\n-# --------------\n+# ----------\n #\n # .. [SALMON2011] John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw, \"Parallel Random Numbers: As Easy as 1, 2, 3\", 2011\n # .. [SRIVASTAVA2014] Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\", JMLR 2014"}, {"filename": "keren/fix-doc/_downloads/d5fee5b55a64e47f1b5724ec39adf171/03-matrix-multiplication.py", "status": "renamed", "additions": 32, "deletions": 40, "changes": 72, "file_content_changes": "@@ -1,10 +1,10 @@\n \"\"\"\n Matrix Multiplication\n-======================\n+=====================\n In this tutorial, you will write a 25-lines high-performance FP16 matrix multiplication\n kernel that achieves performance on par with cuBLAS.\n-You will specifically learn about:\n \n+In doing so, you will learn about:\n - Block-level matrix multiplications\n - Multi-dimensional pointer arithmetic\n - Program re-ordering for improved L2 cache hit rate\n@@ -13,12 +13,13 @@\n \n # %%\n # Motivations\n-# -------------\n+# -----------\n+#\n # Matrix multiplications are a key building block of most modern high-performance computing systems.\n # They are notoriously hard to optimize, hence their implementation is generally done by\n # hardware vendors themselves as part of so-called \"kernel libraries\" (e.g., cuBLAS).\n # Unfortunately, these libraries are often proprietary and cannot be easily customized\n-# to accomodate the needs of modern deep learning workloads (e.g., fused activation functions).\n+# to accommodate the needs of modern deep learning workloads (e.g., fused activation functions).\n # In this tutorial, you will learn how to implement efficient matrix multiplications by\n # yourself with Triton, in a way that is easy to customize and extend.\n #\n@@ -42,15 +43,16 @@\n \n # %%\n # Compute Kernel\n-# ----------------\n+# --------------\n #\n # The above algorithm is, actually, fairly straightforward to implement in Triton.\n # The main difficulty comes from the computation of the memory locations at which blocks\n # of :code:`A` and :code:`B` must be read in the inner loop. For that, we need\n-# multi-dimensional pointer arithmetics.\n+# multi-dimensional pointer arithmetic.\n+#\n #\n-# Pointer Arithmetics\n-# ~~~~~~~~~~~~~~~~~~~~\n+# Pointer Arithmetic\n+# ~~~~~~~~~~~~~~~~~~\n #\n # For a row-major 2D tensor :code:`X`, the memory location of :code:`X[i, j]` is given b\n # y :code:`&X[i, j] = X + i*stride_xi + j*stride_xj`.\n@@ -76,12 +78,12 @@\n #\n #  .. code-block:: python\n #\n-#    pa += BLOCK_SIZE_K * stride_ak;\n-#    pb += BLOCK_SIZE_K * stride_bk;\n+#    a_ptrs += BLOCK_SIZE_K * stride_ak;\n+#    b_ptrs += BLOCK_SIZE_K * stride_bk;\n #\n #\n # L2 Cache Optimizations\n-# ~~~~~~~~~~~~~~~~~~~~~~~~\n+# ~~~~~~~~~~~~~~~~~~~~~~\n #\n # As mentioned above, each program instance computes a :code:`[BLOCK_SIZE_M, BLOCK_SIZE_N]`\n # block of :code:`C`.\n@@ -137,8 +139,7 @@\n \n # %%\n # Final Result\n-# -------------\n-#\n+# ------------\n \n import torch\n \n@@ -156,9 +157,7 @@\n \n @triton.autotune(\n     configs=[\n-        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n-        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n-        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n         triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n         triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n         triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n@@ -196,6 +195,7 @@ def matmul_kernel(\n     pid = tl.program_id(axis=0)\n     num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n     num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n     num_pid_in_group = GROUP_SIZE_M * num_pid_n\n     group_id = pid // num_pid_in_group\n     first_pid_m = group_id * GROUP_SIZE_M\n@@ -209,7 +209,7 @@ def matmul_kernel(\n     # and accumulate\n     # a_ptrs is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n     # b_ptrs is a block of [BLOCK_SIZE_K, BLOCK_SIZE_n] pointers\n-    # see above `Pointer Arithmetics` section for details\n+    # see above `Pointer Arithmetic` section for details\n     offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n     offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n     offs_k = tl.arange(0, BLOCK_SIZE_K)\n@@ -222,7 +222,7 @@ def matmul_kernel(\n     # of fp32 values for higher accuracy.\n     # `accumulator` will be converted back to fp16 after the loop\n     accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-    for k in range(0, K, BLOCK_SIZE_K):\n+    for k in range(0, num_pid_k):\n         # Note that for simplicity, we don't apply a mask here.\n         # This means that if K is not a multiple of BLOCK_SIZE_K,\n         # this will access out-of-bounds memory and produce an\n@@ -236,8 +236,8 @@ def matmul_kernel(\n         b_ptrs += BLOCK_SIZE_K * stride_bk\n     # you can fuse arbitrary activation functions here\n     # while the accumulator is still in FP32!\n-    if ACTIVATION == \"leaky_relu\":\n-        accumulator = leaky_relu(accumulator)\n+    if ACTIVATION:\n+        accumulator = ACTIVATION(accumulator)\n     c = accumulator.to(tl.float16)\n \n     # -----------------------------------------------------------\n@@ -252,7 +252,6 @@ def matmul_kernel(\n # we can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`\n @triton.jit\n def leaky_relu(x):\n-    x = x + 1\n     return tl.where(x >= 0, x, 0.01 * x)\n \n \n@@ -261,7 +260,7 @@ def leaky_relu(x):\n # and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel\n \n \n-def matmul(a, b, activation=\"\"):\n+def matmul(a, b, activation=None):\n     # checks constraints\n     assert a.shape[1] == b.shape[0], \"incompatible dimensions\"\n     assert a.is_contiguous(), \"matrix A must be contiguous\"\n@@ -290,28 +289,29 @@ def matmul(a, b, activation=\"\"):\n \n # %%\n # Unit Test\n-# -----------\n+# ---------\n #\n # We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS)\n \n torch.manual_seed(0)\n a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n-triton_output = matmul(a, b)\n+triton_output = matmul(a, b, activation=None)\n torch_output = torch.matmul(a, b)\n print(f\"triton_output={triton_output}\")\n print(f\"torch_output={torch_output}\")\n-if triton.testing.allclose(triton_output, torch_output):\n+if torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\n     print(\"\u2705 Triton and Torch match\")\n else:\n     print(\"\u274c Triton and Torch differ\")\n \n # %%\n # Benchmark\n-# --------------\n+# ---------\n #\n # Square Matrix Performance\n # ~~~~~~~~~~~~~~~~~~~~~~~~~~\n+#\n # We can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices, but feel free to arrange this script as you wish to benchmark any other matrix shape.\n \n \n@@ -323,11 +323,11 @@ def matmul(a, b, activation=\"\"):\n         ],  # different possible values for `x_name`\n         line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n         # possible values for `line_arg``\n-        line_vals=['cublas', 'cublas + relu', 'triton', 'triton + relu'],\n+        line_vals=['cublas', 'triton'],\n         # label name for the lines\n-        line_names=[\"cuBLAS\", \"cuBLAS (+ torch.nn.LeakyReLU)\", \"Triton\", \"Triton (+ LeakyReLU)\"],\n+        line_names=[\"cuBLAS\", \"Triton\"],\n         # line styles\n-        styles=[('green', '-'), ('green', '--'), ('blue', '-'), ('blue', '--')],\n+        styles=[('green', '-'), ('blue', '-')],\n         ylabel=\"TFLOPS\",  # label name for the y-axis\n         plot_name=\"matmul-performance\",  # name for the plot. Used also as a file name for saving the plot.\n         args={},\n@@ -336,19 +336,11 @@ def matmul(a, b, activation=\"\"):\n def benchmark(M, N, K, provider):\n     a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n     b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n+    quantiles = [0.5, 0.2, 0.8]\n     if provider == 'cublas':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b))\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\n     if provider == 'triton':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b))\n-    if provider == 'cublas + relu':\n-        torch_relu = torch.nn.ReLU(inplace=True)\n-        ms, min_ms, max_ms = triton.testing.do_bench(\n-            lambda: torch_relu(torch.matmul(a, b))\n-        )\n-    if provider == 'triton + relu':\n-        ms, min_ms, max_ms = triton.testing.do_bench(\n-            lambda: matmul(a, b, activation=\"leaky_relu\")\n-        )\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\n     perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n     return perf(ms), perf(max_ms), perf(min_ms)\n "}, {"filename": "keren/fix-doc/_downloads/d91442ac2982c4e0cc3ab0f43534afbc/02-fused-softmax.py", "status": "renamed", "additions": 19, "deletions": 13, "changes": 32, "file_content_changes": "@@ -1,18 +1,20 @@\n \"\"\"\n Fused Softmax\n-=================\n+=============\n+\n In this tutorial, you will write a fused softmax operation that is significantly faster\n than PyTorch's native op for a particular class of matrices: those whose rows can fit in\n the GPU's SRAM.\n-You will learn about:\n \n+In doing so, you will learn about:\n - The benefits of kernel fusion for bandwidth-bound operations.\n - Reduction operators in Triton.\n \"\"\"\n \n # %%\n # Motivations\n-# ------------\n+# -----------\n+#\n # Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.\n # Let us consider instead the case of a simple (numerically stabilized) softmax operation:\n \n@@ -55,9 +57,11 @@ def naive_softmax(x):\n \n # %%\n # Compute Kernel\n-# ----------------\n+# --------------\n+#\n # Our softmax kernel works as follows: each program loads a row of the input matrix X,\n # normalizes it and writes back the result to the output Y.\n+#\n # Note that one important limitation of Triton is that each block must have a\n # power-of-two number of elements, so we need to internally \"pad\" each row and guard the\n # memory operations properly if we want to handle any possible input shapes:\n@@ -78,9 +82,9 @@ def softmax_kernel(\n     input_ptrs = row_start_ptr + col_offsets\n     # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n     row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n-    # Substract maximum for numerical stability\n+    # Subtract maximum for numerical stability\n     row_minus_max = row - tl.max(row, axis=0)\n-    # Note that exponentials in Triton are fast but approximate (i.e., think __expf in CUDA)\n+    # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n     numerator = tl.exp(row_minus_max)\n     denominator = tl.sum(numerator, axis=0)\n     softmax_output = numerator / denominator\n@@ -93,6 +97,7 @@ def softmax_kernel(\n # %%\n # We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.\n \n+\n def softmax(x):\n     n_rows, n_cols = x.shape\n     # The block size is the smallest power of two greater than the number of columns in `x`\n@@ -124,7 +129,7 @@ def softmax(x):\n \n # %%\n # Unit Test\n-# ----------\n+# ---------\n \n # %%\n # We make sure that we test our kernel on a matrix with an irregular number of rows and columns.\n@@ -141,7 +146,8 @@ def softmax(x):\n \n # %%\n # Benchmark\n-# -------------\n+# ---------\n+#\n # Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.\n # We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.\n \n@@ -171,12 +177,13 @@ def softmax(x):\n )\n def benchmark(M, N, provider):\n     x = torch.randn(M, N, device='cuda', dtype=torch.float32)\n+    quantiles = [0.5, 0.2, 0.8]\n     if provider == 'torch-native':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1), quantiles=quantiles)\n     if provider == 'triton':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: softmax(x))\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: softmax(x), quantiles=quantiles)\n     if provider == 'torch-jit':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_softmax(x))\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_softmax(x), quantiles=quantiles)\n     gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\n     return gbps(ms), gbps(max_ms), gbps(min_ms)\n \n@@ -185,7 +192,6 @@ def benchmark(M, N, provider):\n \n # %%\n # In the above plot, we can see that:\n-#\n #  - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.\n #  - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.\n-#    Note however that the PyTorch `softmax` operation is more general and will works on tensors of any shape.\n+#    Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape."}, {"filename": "keren/fix-doc/_downloads/f191ee1e78dc52eb5f7cba88f71cef2f/01-vector-add.ipynb", "status": "added", "additions": 140, "deletions": 0, "changes": 140, "file_content_changes": "@@ -0,0 +1,140 @@\n+{\n+  \"cells\": [\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"%matplotlib inline\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"\\n# Vector Addition\\n\\nIn this tutorial, you will write a simple vector addition using Triton.\\n\\nIn doing so, you will learn about:\\n- The basic programming model of Triton.\\n- The `triton.jit` decorator, which is used to define Triton kernels.\\n- The best practices for validating and benchmarking your custom ops against native reference implementations.\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Compute Kernel\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.jit\\ndef add_kernel(\\n    x_ptr,  # *Pointer* to first input vector.\\n    y_ptr,  # *Pointer* to second input vector.\\n    output_ptr,  # *Pointer* to output vector.\\n    n_elements,  # Size of the vector.\\n    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\\n                 # NOTE: `constexpr` so it can be used as a shape value.\\n):\\n    # There are multiple 'programs' processing different data. We identify which program\\n    # we are here:\\n    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\\n    # This program will process inputs that are offset from the initial data.\\n    # For instance, if you had a vector of length 256 and block_size of 64, the programs\\n    # would each access the elements [0:64, 64:128, 128:192, 192:256].\\n    # Note that offsets is a list of pointers:\\n    block_start = pid * BLOCK_SIZE\\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\\n    # Create a mask to guard memory operations against out-of-bounds accesses.\\n    mask = offsets < n_elements\\n    # Load x and y from DRAM, masking out any extra elements in case the input is not a\\n    # multiple of the block size.\\n    x = tl.load(x_ptr + offsets, mask=mask)\\n    y = tl.load(y_ptr + offsets, mask=mask)\\n    output = x + y\\n    # Write x + y back to DRAM.\\n    tl.store(output_ptr + offsets, output, mask=mask)\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"Let's also declare a helper function to (1) allocate the `z` tensor\\nand (2) enqueue the above kernel with appropriate grid/block sizes:\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"def add(x: torch.Tensor, y: torch.Tensor):\\n    # We need to preallocate the output.\\n    output = torch.empty_like(x)\\n    assert x.is_cuda and y.is_cuda and output.is_cuda\\n    n_elements = output.numel()\\n    # The SPMD launch grid denotes the number of kernel instances that run in parallel.\\n    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\\n    # In this case, we use a 1D grid where the size is the number of blocks:\\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\\n    # NOTE:\\n    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.\\n    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.\\n    #  - Don't forget to pass meta-parameters as keywords arguments.\\n    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\\n    # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\\n    # running asynchronously at this point.\\n    return output\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"We can now use the above function to compute the element-wise sum of two `torch.tensor` objects and test its correctness:\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"torch.manual_seed(0)\\nsize = 98432\\nx = torch.rand(size, device='cuda')\\ny = torch.rand(size, device='cuda')\\noutput_torch = x + y\\noutput_triton = add(x, y)\\nprint(output_torch)\\nprint(output_triton)\\nprint(\\n    f'The maximum difference between torch and triton is '\\n    f'{torch.max(torch.abs(output_torch - output_triton))}'\\n)\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"Seems like we're good to go!\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"## Benchmark\\n\\nWe can now benchmark our custom op on vectors of increasing sizes to get a sense of how it does relative to PyTorch.\\nTo make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of our custom ops.\\nfor different problem sizes.\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"@triton.testing.perf_report(\\n    triton.testing.Benchmark(\\n        x_names=['size'],  # Argument names to use as an x-axis for the plot.\\n        x_vals=[\\n            2 ** i for i in range(12, 28, 1)\\n        ],  # Different possible values for `x_name`.\\n        x_log=True,  # x axis is logarithmic.\\n        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\\n        line_vals=['triton', 'torch'],  # Possible values for `line_arg`.\\n        line_names=['Triton', 'Torch'],  # Label name for the lines.\\n        styles=[('blue', '-'), ('green', '-')],  # Line styles.\\n        ylabel='GB/s',  # Label name for the y-axis.\\n        plot_name='vector-add-performance',  # Name for the plot. Used also as a file name for saving the plot.\\n        args={},  # Values for function arguments not in `x_names` and `y_name`.\\n    )\\n)\\ndef benchmark(size, provider):\\n    x = torch.rand(size, device='cuda', dtype=torch.float32)\\n    y = torch.rand(size, device='cuda', dtype=torch.float32)\\n    quantiles = [0.5, 0.2, 0.8]\\n    if provider == 'torch':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: x + y, quantiles=quantiles)\\n    if provider == 'triton':\\n        ms, min_ms, max_ms = triton.testing.do_bench(lambda: add(x, y), quantiles=quantiles)\\n    gbps = lambda ms: 12 * size / ms * 1e-6\\n    return gbps(ms), gbps(max_ms), gbps(min_ms)\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"We can now run the decorated function above. Pass `print_data=True` to see the performance number, `show_plots=True` to plot them, and/or\\n`save_path='/path/to/results/' to save them to disk along with raw CSV data:\\n\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"benchmark.run(print_data=True, show_plots=True)\"\n+      ]\n+    }\n+  ],\n+  \"metadata\": {\n+    \"kernelspec\": {\n+      \"display_name\": \"Python 3\",\n+      \"language\": \"python\",\n+      \"name\": \"python3\"\n+    },\n+    \"language_info\": {\n+      \"codemirror_mode\": {\n+        \"name\": \"ipython\",\n+        \"version\": 3\n+      },\n+      \"file_extension\": \".py\",\n+      \"mimetype\": \"text/x-python\",\n+      \"name\": \"python\",\n+      \"nbconvert_exporter\": \"python\",\n+      \"pygments_lexer\": \"ipython3\",\n+      \"version\": \"3.8.10\"\n+    }\n+  },\n+  \"nbformat\": 4,\n+  \"nbformat_minor\": 0\n+}\n\\ No newline at end of file"}, {"filename": "keren/fix-doc/_images/cuda-parallel-matmul.png", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_images/cuda-parallel-matmul.png"}, {"filename": "keren/fix-doc/_images/grouped_vs_row_major_ordering.png", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_images/grouped_vs_row_major_ordering.png"}, {"filename": "keren/fix-doc/_images/halide-iteration.png", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_images/halide-iteration.png"}, {"filename": "keren/fix-doc/_images/parallel_reduction.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/_images/polyhedral-iteration.png", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_images/polyhedral-iteration.png"}, {"filename": "keren/fix-doc/_images/sphx_glr_01-vector-add_001.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/_images/sphx_glr_01-vector-add_thumb.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/_images/sphx_glr_02-fused-softmax_001.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/_images/sphx_glr_02-fused-softmax_thumb.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/_images/sphx_glr_03-matrix-multiplication_001.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/_images/sphx_glr_03-matrix-multiplication_thumb.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/_images/sphx_glr_04-low-memory-dropout_thumb.png", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_images/sphx_glr_04-low-memory-dropout_thumb.png"}, {"filename": "keren/fix-doc/_images/sphx_glr_05-layer-norm_001.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/_images/sphx_glr_05-layer-norm_thumb.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/_images/sphx_glr_07-math-functions_thumb.png", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_images/sphx_glr_06-fused-attention_thumb.png"}, {"filename": "keren/fix-doc/_images/triton-parallel-matmul.png", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_images/triton-parallel-matmul.png"}, {"filename": "keren/fix-doc/_sources/getting-started/installation.rst.txt", "status": "renamed", "additions": 12, "deletions": 12, "changes": 24, "file_content_changes": "@@ -1,10 +1,10 @@\n-==============\n+============\n Installation\n-==============\n+============\n \n----------------------\n+--------------------\n Binary Distributions\n----------------------\n+--------------------\n \n You can install the latest stable release of Triton from pip:\n \n@@ -17,25 +17,25 @@ Binary wheels are available for CPython 3.6-3.9 and PyPy 3.6-3.7.\n And the latest nightly release:\n \n .. code-block:: bash\n-  \n+\n       pip install -U --pre triton\n \n \n---------------\n+-----------\n From Source\n---------------\n+-----------\n \n-+++++++++++++++\n+++++++++++++++\n Python Package\n-+++++++++++++++\n+++++++++++++++\n \n You can install the Python package from source by running the following commands:\n \n .. code-block:: bash\n \n       git clone https://github.com/openai/triton.git;\n       cd triton/python;\n-      pip install cmake; # build time dependency\n+      pip install cmake; # build-time dependency\n       pip install -e .\n \n Note that, if llvm-11 is not present on your system, the setup.py script will download the official LLVM11 static libraries link against that.\n@@ -50,6 +50,6 @@ You can then test your installation by running the unit tests:\n and the benchmarks\n \n .. code-block:: bash\n-      \n-      cd bench/\n+\n+      cd bench\n       python -m run --with-plots --result-dir /tmp/triton-bench"}, {"filename": "keren/fix-doc/_sources/getting-started/tutorials/01-vector-add.rst.txt", "status": "renamed", "additions": 62, "deletions": 58, "changes": 120, "file_content_changes": "@@ -19,19 +19,21 @@\n \n \n Vector Addition\n-=================\n-In this tutorial, you will write a simple vector addition using Triton and learn about:\n+===============\n \n-- The basic programming model of Triton\n+In this tutorial, you will write a simple vector addition using Triton.\n+\n+In doing so, you will learn about:\n+- The basic programming model of Triton.\n - The `triton.jit` decorator, which is used to define Triton kernels.\n-- The best practices for validating and benchmarking your custom ops against native reference implementations\n+- The best practices for validating and benchmarking your custom ops against native reference implementations.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 12-14\n+.. GENERATED FROM PYTHON SOURCE LINES 14-16\n \n Compute Kernel\n---------------------------\n+--------------\n \n-.. GENERATED FROM PYTHON SOURCE LINES 14-50\n+.. GENERATED FROM PYTHON SOURCE LINES 16-52\n \n .. code-block:: default\n \n@@ -44,30 +46,30 @@ Compute Kernel\n \n     @triton.jit\n     def add_kernel(\n-        x_ptr,  # *Pointer* to first input vector\n-        y_ptr,  # *Pointer* to second input vector\n-        output_ptr,  # *Pointer* to output vector\n-        n_elements,  # Size of the vector\n-        BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process\n-                     # NOTE: `constexpr` so it can be used as a shape value\n+        x_ptr,  # *Pointer* to first input vector.\n+        y_ptr,  # *Pointer* to second input vector.\n+        output_ptr,  # *Pointer* to output vector.\n+        n_elements,  # Size of the vector.\n+        BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n+                     # NOTE: `constexpr` so it can be used as a shape value.\n     ):\n-        # There are multiple 'program's processing different data. We identify which program\n-        # we are here\n-        pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0\n+        # There are multiple 'programs' processing different data. We identify which program\n+        # we are here:\n+        pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n         # This program will process inputs that are offset from the initial data.\n-        # for instance, if you had a vector of length 256 and block_size of 64, the programs\n+        # For instance, if you had a vector of length 256 and block_size of 64, the programs\n         # would each access the elements [0:64, 64:128, 128:192, 192:256].\n-        # Note that offsets is a list of pointers\n+        # Note that offsets is a list of pointers:\n         block_start = pid * BLOCK_SIZE\n         offsets = block_start + tl.arange(0, BLOCK_SIZE)\n-        # Create a mask to guard memory operations against out-of-bounds accesses\n+        # Create a mask to guard memory operations against out-of-bounds accesses.\n         mask = offsets < n_elements\n         # Load x and y from DRAM, masking out any extra elements in case the input is not a\n-        # multiple of the block size\n+        # multiple of the block size.\n         x = tl.load(x_ptr + offsets, mask=mask)\n         y = tl.load(y_ptr + offsets, mask=mask)\n         output = x + y\n-        # Write x + y back to DRAM\n+        # Write x + y back to DRAM.\n         tl.store(output_ptr + offsets, output, mask=mask)\n \n \n@@ -78,30 +80,30 @@ Compute Kernel\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 51-53\n+.. GENERATED FROM PYTHON SOURCE LINES 53-55\n \n Let's also declare a helper function to (1) allocate the `z` tensor\n-and (2) enqueue the above kernel with appropriate grid/block sizes.\n+and (2) enqueue the above kernel with appropriate grid/block sizes:\n \n-.. GENERATED FROM PYTHON SOURCE LINES 53-74\n+.. GENERATED FROM PYTHON SOURCE LINES 55-76\n \n .. code-block:: default\n \n \n \n     def add(x: torch.Tensor, y: torch.Tensor):\n-        # We need to preallocate the output\n+        # We need to preallocate the output.\n         output = torch.empty_like(x)\n         assert x.is_cuda and y.is_cuda and output.is_cuda\n         n_elements = output.numel()\n         # The SPMD launch grid denotes the number of kernel instances that run in parallel.\n-        # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int]\n-        # In this case, we use a 1D grid where the size is the number of blocks\n+        # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\n+        # In this case, we use a 1D grid where the size is the number of blocks:\n         grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n         # NOTE:\n-        #  - each torch.tensor object is implicitly converted into a pointer to its first element.\n-        #  - `triton.jit`'ed functions can be index with a launch grid to obtain a callable GPU kernel\n-        #  - don't forget to pass meta-parameters as keywords arguments\n+        #  - Each torch.tensor object is implicitly converted into a pointer to its first element.\n+        #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.\n+        #  - Don't forget to pass meta-parameters as keywords arguments.\n         add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n         # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\n         # running asynchronously at this point.\n@@ -115,11 +117,11 @@ and (2) enqueue the above kernel with appropriate grid/block sizes.\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 75-76\n+.. GENERATED FROM PYTHON SOURCE LINES 77-78\n \n We can now use the above function to compute the element-wise sum of two `torch.tensor` objects and test its correctness:\n \n-.. GENERATED FROM PYTHON SOURCE LINES 76-90\n+.. GENERATED FROM PYTHON SOURCE LINES 78-92\n \n .. code-block:: default\n \n@@ -154,47 +156,49 @@ We can now use the above function to compute the element-wise sum of two `torch.\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 91-92\n+.. GENERATED FROM PYTHON SOURCE LINES 93-94\n \n Seems like we're good to go!\n \n-.. GENERATED FROM PYTHON SOURCE LINES 94-99\n+.. GENERATED FROM PYTHON SOURCE LINES 96-102\n \n Benchmark\n------------\n+---------\n+\n We can now benchmark our custom op on vectors of increasing sizes to get a sense of how it does relative to PyTorch.\n-To make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of your custom ops\n+To make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of our custom ops.\n for different problem sizes.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 99-128\n+.. GENERATED FROM PYTHON SOURCE LINES 102-132\n \n .. code-block:: default\n \n \n \n     @triton.testing.perf_report(\n         triton.testing.Benchmark(\n-            x_names=['size'],  # argument names to use as an x-axis for the plot\n+            x_names=['size'],  # Argument names to use as an x-axis for the plot.\n             x_vals=[\n                 2 ** i for i in range(12, 28, 1)\n-            ],  # different possible values for `x_name`\n-            x_log=True,  # x axis is logarithmic\n-            line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n-            line_vals=['triton', 'torch'],  # possible values for `line_arg`\n-            line_names=['Triton', 'Torch'],  # label name for the lines\n-            styles=[('blue', '-'), ('green', '-')],  # line styles\n-            ylabel='GB/s',  # label name for the y-axis\n-            plot_name='vector-add-performance',  # name for the plot. Used also as a file name for saving the plot.\n-            args={},  # values for function arguments not in `x_names` and `y_name`\n+            ],  # Different possible values for `x_name`.\n+            x_log=True,  # x axis is logarithmic.\n+            line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n+            line_vals=['triton', 'torch'],  # Possible values for `line_arg`.\n+            line_names=['Triton', 'Torch'],  # Label name for the lines.\n+            styles=[('blue', '-'), ('green', '-')],  # Line styles.\n+            ylabel='GB/s',  # Label name for the y-axis.\n+            plot_name='vector-add-performance',  # Name for the plot. Used also as a file name for saving the plot.\n+            args={},  # Values for function arguments not in `x_names` and `y_name`.\n         )\n     )\n     def benchmark(size, provider):\n         x = torch.rand(size, device='cuda', dtype=torch.float32)\n         y = torch.rand(size, device='cuda', dtype=torch.float32)\n+        quantiles = [0.5, 0.2, 0.8]\n         if provider == 'torch':\n-            ms, min_ms, max_ms = triton.testing.do_bench(lambda: x + y)\n+            ms, min_ms, max_ms = triton.testing.do_bench(lambda: x + y, quantiles=quantiles)\n         if provider == 'triton':\n-            ms, min_ms, max_ms = triton.testing.do_bench(lambda: add(x, y))\n+            ms, min_ms, max_ms = triton.testing.do_bench(lambda: add(x, y), quantiles=quantiles)\n         gbps = lambda ms: 12 * size / ms * 1e-6\n         return gbps(ms), gbps(max_ms), gbps(min_ms)\n \n@@ -206,12 +210,12 @@ for different problem sizes.\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 129-131\n+.. GENERATED FROM PYTHON SOURCE LINES 133-135\n \n We can now run the decorated function above. Pass `print_data=True` to see the performance number, `show_plots=True` to plot them, and/or\n-`save_path='/path/to/results/' to save them to disk along with raw CSV data\n+`save_path='/path/to/results/' to save them to disk along with raw CSV data:\n \n-.. GENERATED FROM PYTHON SOURCE LINES 131-132\n+.. GENERATED FROM PYTHON SOURCE LINES 135-136\n \n .. code-block:: default\n \n@@ -233,29 +237,29 @@ We can now run the decorated function above. Pass `print_data=True` to see the p\n     vector-add-performance:\n                size      Triton       Torch\n     0        4096.0    9.600000    9.600000\n-    1        8192.0   19.200000   19.200000\n+    1        8192.0   15.999999   15.999999\n     2       16384.0   38.400001   38.400001\n     3       32768.0   76.800002   76.800002\n     4       65536.0  127.999995  127.999995\n     5      131072.0  219.428568  219.428568\n-    6      262144.0  384.000001  384.000001\n+    6      262144.0  341.333321  341.333321\n     7      524288.0  472.615390  472.615390\n     8     1048576.0  614.400016  614.400016\n-    9     2097152.0  722.823517  702.171410\n+    9     2097152.0  722.823517  722.823517\n     10    4194304.0  780.190482  780.190482\n     11    8388608.0  812.429770  812.429770\n     12   16777216.0  833.084721  833.084721\n-    13   33554432.0  842.004273  842.004273\n+    13   33554432.0  842.004273  843.811163\n     14   67108864.0  847.448255  848.362445\n-    15  134217728.0  849.737435  850.656574\n+    15  134217728.0  850.196756  851.116890\n \n \n \n \n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 1 minutes  46.359 seconds)\n+   **Total running time of the script:** ( 0 minutes  29.843 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_01-vector-add.py:"}, {"filename": "keren/fix-doc/_sources/getting-started/tutorials/02-fused-softmax.rst.txt", "status": "renamed", "additions": 44, "deletions": 38, "changes": 82, "file_content_changes": "@@ -19,23 +19,25 @@\n \n \n Fused Softmax\n-=================\n+=============\n+\n In this tutorial, you will write a fused softmax operation that is significantly faster\n than PyTorch's native op for a particular class of matrices: those whose rows can fit in\n the GPU's SRAM.\n-You will learn about:\n \n+In doing so, you will learn about:\n - The benefits of kernel fusion for bandwidth-bound operations.\n - Reduction operators in Triton.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 14-18\n+.. GENERATED FROM PYTHON SOURCE LINES 15-20\n \n Motivations\n-------------\n+-----------\n+\n Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.\n Let us consider instead the case of a simple (numerically stabilized) softmax operation:\n \n-.. GENERATED FROM PYTHON SOURCE LINES 18-46\n+.. GENERATED FROM PYTHON SOURCE LINES 20-48\n \n .. code-block:: default\n \n@@ -74,7 +76,7 @@ Let us consider instead the case of a simple (numerically stabilized) softmax op\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 47-55\n+.. GENERATED FROM PYTHON SOURCE LINES 49-57\n \n When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \\in R^{M \\times N}`\n requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.\n@@ -85,17 +87,19 @@ expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).\n The `torch.jit.script` flags aims to perform this kind of \"kernel fusion\" automatically\n but, as we will see later, it is still far from ideal.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 57-64\n+.. GENERATED FROM PYTHON SOURCE LINES 59-68\n \n Compute Kernel\n-----------------\n+--------------\n+\n Our softmax kernel works as follows: each program loads a row of the input matrix X,\n normalizes it and writes back the result to the output Y.\n+\n Note that one important limitation of Triton is that each block must have a\n power-of-two number of elements, so we need to internally \"pad\" each row and guard the\n memory operations properly if we want to handle any possible input shapes:\n \n-.. GENERATED FROM PYTHON SOURCE LINES 64-93\n+.. GENERATED FROM PYTHON SOURCE LINES 68-97\n \n .. code-block:: default\n \n@@ -116,9 +120,9 @@ memory operations properly if we want to handle any possible input shapes:\n         input_ptrs = row_start_ptr + col_offsets\n         # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n         row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n-        # Substract maximum for numerical stability\n+        # Subtract maximum for numerical stability\n         row_minus_max = row - tl.max(row, axis=0)\n-        # Note that exponentials in Triton are fast but approximate (i.e., think __expf in CUDA)\n+        # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n         numerator = tl.exp(row_minus_max)\n         denominator = tl.sum(numerator, axis=0)\n         softmax_output = numerator / denominator\n@@ -135,15 +139,16 @@ memory operations properly if we want to handle any possible input shapes:\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 94-95\n+.. GENERATED FROM PYTHON SOURCE LINES 98-99\n \n We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 95-125\n+.. GENERATED FROM PYTHON SOURCE LINES 99-130\n \n .. code-block:: default\n \n \n+\n     def softmax(x):\n         n_rows, n_cols = x.shape\n         # The block size is the smallest power of two greater than the number of columns in `x`\n@@ -180,17 +185,17 @@ We can create a helper function that enqueues the kernel and its (meta-)argument\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 126-128\n+.. GENERATED FROM PYTHON SOURCE LINES 131-133\n \n Unit Test\n-----------\n+---------\n \n-.. GENERATED FROM PYTHON SOURCE LINES 130-132\n+.. GENERATED FROM PYTHON SOURCE LINES 135-137\n \n We make sure that we test our kernel on a matrix with an irregular number of rows and columns.\n This will allow us to verify that our padding mechanism works.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 132-139\n+.. GENERATED FROM PYTHON SOURCE LINES 137-144\n \n .. code-block:: default\n \n@@ -208,18 +213,19 @@ This will allow us to verify that our padding mechanism works.\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 140-141\n+.. GENERATED FROM PYTHON SOURCE LINES 145-146\n \n As expected, the results are identical.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 143-147\n+.. GENERATED FROM PYTHON SOURCE LINES 148-153\n \n Benchmark\n--------------\n+---------\n+\n Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.\n We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 147-186\n+.. GENERATED FROM PYTHON SOURCE LINES 153-193\n \n .. code-block:: default\n \n@@ -250,12 +256,13 @@ We will then compare its performance against (1) :code:`torch.softmax` and (2) t\n     )\n     def benchmark(M, N, provider):\n         x = torch.randn(M, N, device='cuda', dtype=torch.float32)\n+        quantiles = [0.5, 0.2, 0.8]\n         if provider == 'torch-native':\n-            ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))\n+            ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1), quantiles=quantiles)\n         if provider == 'triton':\n-            ms, min_ms, max_ms = triton.testing.do_bench(lambda: softmax(x))\n+            ms, min_ms, max_ms = triton.testing.do_bench(lambda: softmax(x), quantiles=quantiles)\n         if provider == 'torch-jit':\n-            ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_softmax(x))\n+            ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_softmax(x), quantiles=quantiles)\n         gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\n         return gbps(ms), gbps(max_ms), gbps(min_ms)\n \n@@ -278,35 +285,34 @@ We will then compare its performance against (1) :code:`torch.softmax` and (2) t\n \n     softmax-performance:\n               N      Triton  Torch (native)  Torch (jit)\n-    0     256.0  512.000001      546.133347   186.181817\n-    1     384.0  614.400016      585.142862   153.600004\n-    2     512.0  655.360017      585.142849   154.566038\n-    3     640.0  706.206879      640.000002   160.000000\n-    4     768.0  722.823517      664.216187   162.754967\n+    0     256.0  546.133347      546.133347   204.800005\n+    1     384.0  614.400016      646.736871   236.307695\n+    2     512.0  655.360017      630.153853   248.242422\n+    3     640.0  706.206879      620.606056   259.240514\n+    4     768.0  722.823517      646.736871   261.446801\n     ..      ...         ...             ...          ...\n-    93  12160.0  812.359066      406.179533   198.834951\n-    94  12288.0  812.429770      415.222812   199.197579\n-    95  12416.0  812.498981      412.149375   198.655991\n-    96  12544.0  810.925276      412.546756   199.012395\n-    97  12672.0  811.007961      412.097543   199.069228\n+    93  12160.0  815.765209      409.599993   324.537101\n+    94  12288.0  815.800825      416.984084   324.703544\n+    95  12416.0  815.835709      414.730681   324.071772\n+    96  12544.0  814.214963      415.536223   324.501215\n+    97  12672.0  814.265046      415.050158   324.143890\n \n     [98 rows x 4 columns]\n \n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 187-192\n+.. GENERATED FROM PYTHON SOURCE LINES 194-198\n \n In the above plot, we can see that:\n-\n  - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.\n  - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.\n-   Note however that the PyTorch `softmax` operation is more general and will works on tensors of any shape.\n+   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.\n \n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 3 minutes  30.997 seconds)\n+   **Total running time of the script:** ( 1 minutes  19.698 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:"}, {"filename": "keren/fix-doc/_sources/getting-started/tutorials/03-matrix-multiplication.rst.txt", "status": "renamed", "additions": 76, "deletions": 86, "changes": 162, "file_content_changes": "@@ -19,25 +19,26 @@\n \n \n Matrix Multiplication\n-======================\n+=====================\n In this tutorial, you will write a 25-lines high-performance FP16 matrix multiplication\n kernel that achieves performance on par with cuBLAS.\n-You will specifically learn about:\n \n+In doing so, you will learn about:\n - Block-level matrix multiplications\n - Multi-dimensional pointer arithmetic\n - Program re-ordering for improved L2 cache hit rate\n - Automatic performance tuning\n \n-.. GENERATED FROM PYTHON SOURCE LINES 15-42\n+.. GENERATED FROM PYTHON SOURCE LINES 15-43\n \n Motivations\n--------------\n+-----------\n+\n Matrix multiplications are a key building block of most modern high-performance computing systems.\n They are notoriously hard to optimize, hence their implementation is generally done by\n hardware vendors themselves as part of so-called \"kernel libraries\" (e.g., cuBLAS).\n Unfortunately, these libraries are often proprietary and cannot be easily customized\n-to accomodate the needs of modern deep learning workloads (e.g., fused activation functions).\n+to accommodate the needs of modern deep learning workloads (e.g., fused activation functions).\n In this tutorial, you will learn how to implement efficient matrix multiplications by\n yourself with Triton, in a way that is easy to customize and extend.\n \n@@ -59,18 +60,19 @@ algorithm to multiply a (M, K) by a (K, N) matrix:\n \n where each iteration of the doubly-nested for-loop is performed by a dedicated Triton program instance.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 44-137\n+.. GENERATED FROM PYTHON SOURCE LINES 45-139\n \n Compute Kernel\n-----------------\n+--------------\n \n The above algorithm is, actually, fairly straightforward to implement in Triton.\n The main difficulty comes from the computation of the memory locations at which blocks\n of :code:`A` and :code:`B` must be read in the inner loop. For that, we need\n-multi-dimensional pointer arithmetics.\n+multi-dimensional pointer arithmetic.\n+\n \n-Pointer Arithmetics\n-~~~~~~~~~~~~~~~~~~~~\n+Pointer Arithmetic\n+~~~~~~~~~~~~~~~~~~\n \n For a row-major 2D tensor :code:`X`, the memory location of :code:`X[i, j]` is given b\n y :code:`&X[i, j] = X + i*stride_xi + j*stride_xj`.\n@@ -96,12 +98,12 @@ And then updated in the inner loop as follows:\n \n  .. code-block:: python\n \n-   pa += BLOCK_SIZE_K * stride_ak;\n-   pb += BLOCK_SIZE_K * stride_bk;\n+   a_ptrs += BLOCK_SIZE_K * stride_ak;\n+   b_ptrs += BLOCK_SIZE_K * stride_bk;\n \n \n L2 Cache Optimizations\n-~~~~~~~~~~~~~~~~~~~~~~~~\n+~~~~~~~~~~~~~~~~~~~~~~\n \n As mentioned above, each program instance computes a :code:`[BLOCK_SIZE_M, BLOCK_SIZE_N]`\n block of :code:`C`.\n@@ -155,13 +157,12 @@ In practice, this can improve the performance of our matrix multiplication kerne\n more than 10\\% on some hardware architecture (e.g., 220 to 245 TFLOPS on A100).\n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 139-142\n+.. GENERATED FROM PYTHON SOURCE LINES 141-143\n \n Final Result\n--------------\n+------------\n \n-\n-.. GENERATED FROM PYTHON SOURCE LINES 142-259\n+.. GENERATED FROM PYTHON SOURCE LINES 143-258\n \n .. code-block:: default\n \n@@ -182,9 +183,7 @@ Final Result\n \n     @triton.autotune(\n         configs=[\n-            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n-            triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n-            triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n             triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n             triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n             triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n@@ -222,6 +221,7 @@ Final Result\n         pid = tl.program_id(axis=0)\n         num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n         num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+        num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n         num_pid_in_group = GROUP_SIZE_M * num_pid_n\n         group_id = pid // num_pid_in_group\n         first_pid_m = group_id * GROUP_SIZE_M\n@@ -235,7 +235,7 @@ Final Result\n         # and accumulate\n         # a_ptrs is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n         # b_ptrs is a block of [BLOCK_SIZE_K, BLOCK_SIZE_n] pointers\n-        # see above `Pointer Arithmetics` section for details\n+        # see above `Pointer Arithmetic` section for details\n         offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n         offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n         offs_k = tl.arange(0, BLOCK_SIZE_K)\n@@ -248,7 +248,7 @@ Final Result\n         # of fp32 values for higher accuracy.\n         # `accumulator` will be converted back to fp16 after the loop\n         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-        for k in range(0, K, BLOCK_SIZE_K):\n+        for k in range(0, num_pid_k):\n             # Note that for simplicity, we don't apply a mask here.\n             # This means that if K is not a multiple of BLOCK_SIZE_K,\n             # this will access out-of-bounds memory and produce an\n@@ -262,8 +262,8 @@ Final Result\n             b_ptrs += BLOCK_SIZE_K * stride_bk\n         # you can fuse arbitrary activation functions here\n         # while the accumulator is still in FP32!\n-        if ACTIVATION == \"leaky_relu\":\n-            accumulator = leaky_relu(accumulator)\n+        if ACTIVATION:\n+            accumulator = ACTIVATION(accumulator)\n         c = accumulator.to(tl.float16)\n \n         # -----------------------------------------------------------\n@@ -278,7 +278,6 @@ Final Result\n     # we can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`\n     @triton.jit\n     def leaky_relu(x):\n-        x = x + 1\n         return tl.where(x >= 0, x, 0.01 * x)\n \n \n@@ -289,18 +288,18 @@ Final Result\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 260-262\n+.. GENERATED FROM PYTHON SOURCE LINES 259-261\n \n We can now create a convenience wrapper function that only takes two input tensors\n and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel\n \n-.. GENERATED FROM PYTHON SOURCE LINES 262-291\n+.. GENERATED FROM PYTHON SOURCE LINES 261-290\n \n .. code-block:: default\n \n \n \n-    def matmul(a, b, activation=\"\"):\n+    def matmul(a, b, activation=None):\n         # checks constraints\n         assert a.shape[1] == b.shape[0], \"incompatible dimensions\"\n         assert a.is_contiguous(), \"matrix A must be contiguous\"\n@@ -334,26 +333,26 @@ and (1) checks any shape constraint; (2) allocates the output; (3) launches the\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 292-296\n+.. GENERATED FROM PYTHON SOURCE LINES 291-295\n \n Unit Test\n------------\n+---------\n \n We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS)\n \n-.. GENERATED FROM PYTHON SOURCE LINES 296-309\n+.. GENERATED FROM PYTHON SOURCE LINES 295-308\n \n .. code-block:: default\n \n \n     torch.manual_seed(0)\n     a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n     b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n-    triton_output = matmul(a, b)\n+    triton_output = matmul(a, b, activation=None)\n     torch_output = torch.matmul(a, b)\n     print(f\"triton_output={triton_output}\")\n     print(f\"torch_output={torch_output}\")\n-    if triton.testing.allclose(triton_output, torch_output):\n+    if torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\n         print(\"\u2705 Triton and Torch match\")\n     else:\n         print(\"\u274c Triton and Torch differ\")\n@@ -384,21 +383,22 @@ We can test our custom matrix multiplication operation against a native torch im\n             [ 25.5000,  24.3438,  -8.4609,  ..., -18.9375,  32.5312, -29.9219],\n             [ -5.3477,   4.9805,  11.8828,  ...,   5.5859,   6.4023, -17.3125]],\n            device='cuda:0', dtype=torch.float16)\n-    \u2705 Triton and Torch match\n+    \u274c Triton and Torch differ\n \n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 310-316\n+.. GENERATED FROM PYTHON SOURCE LINES 309-316\n \n Benchmark\n---------------\n+---------\n \n Square Matrix Performance\n ~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n We can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices, but feel free to arrange this script as you wish to benchmark any other matrix shape.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 316-357\n+.. GENERATED FROM PYTHON SOURCE LINES 316-349\n \n .. code-block:: default\n \n@@ -412,11 +412,11 @@ We can now compare the performance of our kernel against that of cuBLAS. Here we\n             ],  # different possible values for `x_name`\n             line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n             # possible values for `line_arg``\n-            line_vals=['cublas', 'cublas + relu', 'triton', 'triton + relu'],\n+            line_vals=['cublas', 'triton'],\n             # label name for the lines\n-            line_names=[\"cuBLAS\", \"cuBLAS (+ torch.nn.LeakyReLU)\", \"Triton\", \"Triton (+ LeakyReLU)\"],\n+            line_names=[\"cuBLAS\", \"Triton\"],\n             # line styles\n-            styles=[('green', '-'), ('green', '--'), ('blue', '-'), ('blue', '--')],\n+            styles=[('green', '-'), ('blue', '-')],\n             ylabel=\"TFLOPS\",  # label name for the y-axis\n             plot_name=\"matmul-performance\",  # name for the plot. Used also as a file name for saving the plot.\n             args={},\n@@ -425,19 +425,11 @@ We can now compare the performance of our kernel against that of cuBLAS. Here we\n     def benchmark(M, N, K, provider):\n         a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n         b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n+        quantiles = [0.5, 0.2, 0.8]\n         if provider == 'cublas':\n-            ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b))\n+            ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\n         if provider == 'triton':\n-            ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b))\n-        if provider == 'cublas + relu':\n-            torch_relu = torch.nn.ReLU(inplace=True)\n-            ms, min_ms, max_ms = triton.testing.do_bench(\n-                lambda: torch_relu(torch.matmul(a, b))\n-            )\n-        if provider == 'triton + relu':\n-            ms, min_ms, max_ms = triton.testing.do_bench(\n-                lambda: matmul(a, b, activation=\"leaky_relu\")\n-            )\n+            ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\n         perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n         return perf(ms), perf(max_ms), perf(min_ms)\n \n@@ -458,48 +450,46 @@ We can now compare the performance of our kernel against that of cuBLAS. Here we\n  .. code-block:: none\n \n     matmul-performance:\n-             M     cuBLAS  ...     Triton  Triton (+ LeakyReLU)\n-    0    256.0   2.978909  ...   2.978909              3.276800\n-    1    384.0   7.372800  ...   8.507077              8.507077\n-    2    512.0  14.563555  ...  15.420235             16.384000\n-    3    640.0  22.260869  ...  24.380953             24.380953\n-    4    768.0  32.768000  ...  34.028308             34.028308\n-    5    896.0  39.025776  ...  40.140799             39.025776\n-    6   1024.0  49.932191  ...  53.773130             52.428801\n-    7   1152.0  45.242181  ...  48.161033             47.396572\n-    8   1280.0  51.200001  ...  57.690139             57.690139\n-    9   1408.0  64.138541  ...  69.009825             67.305878\n-    10  1536.0  80.430545  ...  81.355034             79.526831\n-    11  1664.0  62.929456  ...  63.372618             62.492442\n-    12  1792.0  72.512412  ...  73.460287             59.467852\n-    13  1920.0  69.120002  ...  71.257735             71.257735\n-    14  2048.0  73.584279  ...  78.033565             77.314362\n-    15  2176.0  83.500614  ...  87.115360             85.998493\n-    16  2304.0  68.446623  ...  78.064941             77.057651\n-    17  2432.0  71.305746  ...  86.711310             75.320281\n-    18  2560.0  78.019048  ...  82.539044             81.310171\n-    19  2688.0  83.737433  ...  90.532356             89.254248\n-    20  2816.0  79.587973  ...  83.392363             83.392363\n-    21  2944.0  81.298583  ...  83.060049             82.373605\n-    22  3072.0  81.825298  ...  89.310890             87.112467\n-    23  3200.0  81.424937  ...  96.385543             95.380032\n-    24  3328.0  83.034941  ...  86.424125             85.096096\n-    25  3456.0  79.430113  ...  90.180725             89.281913\n-    26  3584.0  87.381330  ...  99.134944             90.370072\n-    27  3712.0  81.482335  ...  89.114488             84.159518\n-    28  3840.0  84.874902  ...  92.934455             85.663823\n-    29  3968.0  92.652949  ...  89.198780             91.954739\n-    30  4096.0  86.256445  ...  89.003801             91.304576\n-\n-    [31 rows x 5 columns]\n+             M     cuBLAS     Triton\n+    0    256.0   3.276800   2.520615\n+    1    384.0   6.505412   6.912000\n+    2    512.0  13.797053  12.483048\n+    3    640.0  21.333333  19.692308\n+    4    768.0  31.597714  25.278171\n+    5    896.0  37.971025  27.017846\n+    6   1024.0  49.932191  35.544948\n+    7   1152.0  44.566925  28.437943\n+    8   1280.0  49.951220  29.257142\n+    9   1408.0  62.664092  35.866949\n+    10  1536.0  77.778988  42.896290\n+    11  1664.0  60.803457  34.879506\n+    12  1792.0  70.246402  40.140799\n+    13  1920.0  67.106797  45.473686\n+    14  2048.0  72.315584  40.721399\n+    15  2176.0  81.472263  45.120288\n+    16  2304.0  66.725901  41.185987\n+    17  2432.0  69.886725  45.682053\n+    18  2560.0  76.382283  49.648483\n+    19  2688.0  79.691296  46.830933\n+    20  2816.0  79.154642  44.054757\n+    21  2944.0  77.626218  48.197325\n+    22  3072.0  78.100834  46.298532\n+    23  3200.0  77.385224  49.921998\n+    24  3328.0  78.679014  48.381246\n+    25  3456.0  77.371945  47.119561\n+    26  3584.0  80.786517  50.485901\n+    27  3712.0  78.688372  49.186282\n+    28  3840.0  78.155164  47.627907\n+    29  3968.0  81.240968  47.946536\n+    30  4096.0  82.040176  51.861566\n \n \n \n \n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 7 minutes  19.249 seconds)\n+   **Total running time of the script:** ( 1 minutes  39.119 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_03-matrix-multiplication.py:"}, {"filename": "keren/fix-doc/_sources/getting-started/tutorials/04-low-memory-dropout.rst.txt", "status": "renamed", "additions": 32, "deletions": 28, "changes": 60, "file_content_changes": "@@ -19,19 +19,21 @@\n \n \n Low-Memory Dropout\n-=================\n+==================\n \n In this tutorial, you will write a memory-efficient implementation of dropout whose state\n will be composed of a single int32 seed. This differs from more traditional implementations of dropout,\n-whose state is generally composed of a bit mask tensor of the same shape as the input. You will learn about:\n+whose state is generally composed of a bit mask tensor of the same shape as the input.\n \n+In doing so, you will learn about:\n - The limitations of naive implementations of Dropout with PyTorch\n - Parallel pseudo-random number generation in Triton\n \n-.. GENERATED FROM PYTHON SOURCE LINES 14-29\n+.. GENERATED FROM PYTHON SOURCE LINES 15-31\n \n Baseline\n--------------\n+--------\n+\n The *dropout* operator was first introduced in [SRIVASTAVA2014]_ as a way to improve the performance\n of deep neural networks in low-data regime (i.e. regularization).\n \n@@ -46,7 +48,7 @@ keeps the norm consistent regardless of the dropout probability.\n \n Let's first take a look at the baseline implementation.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 29-82\n+.. GENERATED FROM PYTHON SOURCE LINES 31-84\n \n .. code-block:: default\n \n@@ -61,12 +63,12 @@ Let's first take a look at the baseline implementation.\n \n     @triton.jit\n     def _dropout(\n-            x_ptr,  # pointer to the input\n-            x_keep_ptr,  # pointer to a mask of 0s and 1s\n-            output_ptr,  # pointer to the output\n-            n_elements,  # number of elements in the `x` tensor\n-            p,  # probability that an element of `x` is changed to zero\n-            BLOCK_SIZE: tl.constexpr,\n+        x_ptr,  # pointer to the input\n+        x_keep_ptr,  # pointer to a mask of 0s and 1s\n+        output_ptr,  # pointer to the output\n+        n_elements,  # number of elements in the `x` tensor\n+        p,  # probability that an element of `x` is changed to zero\n+        BLOCK_SIZE: tl.constexpr,\n     ):\n         pid = tl.program_id(axis=0)\n         block_start = pid * BLOCK_SIZE\n@@ -122,18 +124,19 @@ Let's first take a look at the baseline implementation.\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 83-101\n+.. GENERATED FROM PYTHON SOURCE LINES 85-104\n \n Seeded dropout\n--------------\n-Above implementation of dropout works fine, but it can be a bit awkward to deal with. Firstly\n+--------------\n+\n+The above implementation of dropout works fine, but it can be a bit awkward to deal with. Firstly\n we need to store the dropout mask for backpropagation. Secondly, dropout state management can get\n very tricky when using recompute/checkpointing (e.g. see all the notes about `preserve_rng_state` in\n https://pytorch.org/docs/1.9.0/checkpoint.html). In this tutorial we'll describe an alternative implementation\n that (1) has a smaller memory footprint; (2) requires less data movement; and (3) simplifies the management\n of persisting randomness across multiple invocations of the kernel.\n \n-Pseudorandom number generation in Triton is simple! In this tutorial we will use the\n+Pseudo-random number generation in Triton is simple! In this tutorial we will use the\n :code:`triton.language.rand` function which generates a block of uniformly distributed :code:`float32`\n values in [0, 1), given a seed and a block of :code:`int32` offsets. But if you need it, Triton also provides\n other :ref:`random number generation strategies <Random Number Generation>`.\n@@ -143,20 +146,20 @@ other :ref:`random number generation strategies <Random Number Generation>`.\n \n Let's put it all together.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 101-149\n+.. GENERATED FROM PYTHON SOURCE LINES 104-152\n \n .. code-block:: default\n \n \n \n     @triton.jit\n     def _seeded_dropout(\n-            x_ptr,\n-            output_ptr,\n-            n_elements,\n-            p,\n-            seed,\n-            BLOCK_SIZE: tl.constexpr,\n+        x_ptr,\n+        output_ptr,\n+        n_elements,\n+        p,\n+        seed,\n+        BLOCK_SIZE: tl.constexpr,\n     ):\n         # compute memory offsets of elements handled by this instance\n         pid = tl.program_id(axis=0)\n@@ -215,32 +218,33 @@ Let's put it all together.\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 150-153\n+.. GENERATED FROM PYTHON SOURCE LINES 153-156\n \n Et Voil\u00e0! We have a triton kernel that applies the same dropout mask provided the seed is the same!\n If you'd like explore further applications of pseudorandomness in GPU programming, we encourage you\n to explore the `triton/language/random` folder!\n \n-.. GENERATED FROM PYTHON SOURCE LINES 155-160\n+.. GENERATED FROM PYTHON SOURCE LINES 158-164\n \n Exercises\n--------------\n+---------\n+\n 1. Extend the kernel to operate over a matrix and use a vector of seeds - one per row.\n 2. Add support for striding.\n 3. (challenge) Implement a kernel for sparse Johnson-Lindenstrauss transform which generates the projection matrix one the fly each time using a seed.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 162-167\n+.. GENERATED FROM PYTHON SOURCE LINES 166-171\n \n References\n---------------\n+----------\n \n .. [SALMON2011] John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw, \"Parallel Random Numbers: As Easy as 1, 2, 3\", 2011\n .. [SRIVASTAVA2014] Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\", JMLR 2014\n \n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  0.287 seconds)\n+   **Total running time of the script:** ( 0 minutes  1.261 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_04-low-memory-dropout.py:"}, {"filename": "keren/fix-doc/_sources/getting-started/tutorials/05-layer-norm.rst.txt", "status": "added", "additions": 505, "deletions": 0, "changes": 505, "file_content_changes": "@@ -0,0 +1,505 @@\n+\n+.. DO NOT EDIT.\n+.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.\n+.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:\n+.. \"getting-started/tutorials/05-layer-norm.py\"\n+.. LINE NUMBERS ARE GIVEN BELOW.\n+\n+.. only:: html\n+\n+    .. note::\n+        :class: sphx-glr-download-link-note\n+\n+        Click :ref:`here <sphx_glr_download_getting-started_tutorials_05-layer-norm.py>`\n+        to download the full example code\n+\n+.. rst-class:: sphx-glr-example-title\n+\n+.. _sphx_glr_getting-started_tutorials_05-layer-norm.py:\n+\n+\n+Layer Normalization\n+====================\n+In this tutorial, you will write a high-performance layer normalization\n+kernel that runs faster than the PyTorch implementation.\n+\n+In doing so, you will learn about:\n+- Implementing backward pass in Triton\n+- Implementing parallel reduction in Triton\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 13-28\n+\n+Motivations\n+-----------\n+\n+The *LayerNorm* operator was first introduced in [BA2016]_ as a way to improve the performance\n+of sequential models (e.g., Transformers) or neural networks with small batch size.\n+It takes a vector :math:`x` as input and produces a vector :math:`y` of the same shape as output.\n+The normalization is performed by subtracting the mean and dividing by the standard deviation of :math:`x`.\n+After the normalization, a learnable linear transformation with weights :math:`w` and biases :math:`b` is applied.\n+The forward pass can be expressed as follows:\n+\n+.. math::\n+   y = \\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} } * w + b\n+\n+where :math:`\\epsilon` is a small constant added to the denominator for numerical stability.\n+Let\u2019s first take a look at the forward pass implementation.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 28-93\n+\n+.. code-block:: default\n+\n+\n+    import torch\n+\n+    import triton\n+    import triton.language as tl\n+\n+    try:\n+        # This is https://github.com/NVIDIA/apex, NOT the apex on PyPi, so it\n+        # should not be added to extras_require in setup.py.\n+        import apex\n+        HAS_APEX = True\n+    except ModuleNotFoundError:\n+        HAS_APEX = False\n+\n+\n+    @triton.jit\n+    def _layer_norm_fwd_fused(\n+        X,  # pointer to the input\n+        Y,  # pointer to the output\n+        W,  # pointer to the weights\n+        B,  # pointer to the biases\n+        Mean,  # pointer to the mean\n+        Rstd,  # pointer to the 1/std\n+        stride,  # how much to increase the pointer when moving by 1 row\n+        N,  # number of columns in X\n+        eps,  # epsilon to avoid division by zero\n+        BLOCK_SIZE: tl.constexpr,\n+    ):\n+        # Map the program id to the row of X and Y it should compute.\n+        row = tl.program_id(0)\n+        Y += row * stride\n+        X += row * stride\n+        # Compute mean\n+        mean = 0\n+        _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n+        for off in range(0, N, BLOCK_SIZE):\n+            cols = off + tl.arange(0, BLOCK_SIZE)\n+            a = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n+            _mean += a\n+        mean = tl.sum(_mean, axis=0) / N\n+        # Compute variance\n+        _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n+        for off in range(0, N, BLOCK_SIZE):\n+            cols = off + tl.arange(0, BLOCK_SIZE)\n+            x = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n+            x = tl.where(cols < N, x - mean, 0.)\n+            _var += x * x\n+        var = tl.sum(_var, axis=0) / N\n+        rstd = 1 / tl.sqrt(var + eps)\n+        # Write mean / rstd\n+        tl.store(Mean + row, mean)\n+        tl.store(Rstd + row, rstd)\n+        # Normalize and apply linear transformation\n+        for off in range(0, N, BLOCK_SIZE):\n+            cols = off + tl.arange(0, BLOCK_SIZE)\n+            mask = cols < N\n+            w = tl.load(W + cols, mask=mask)\n+            b = tl.load(B + cols, mask=mask)\n+            x = tl.load(X + cols, mask=mask, other=0.).to(tl.float32)\n+            x_hat = (x - mean) * rstd\n+            y = x_hat * w + b\n+            # Write output\n+            tl.store(Y + cols, y, mask=mask)\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 94-125\n+\n+Backward pass\n+-------------\n+\n+The backward pass for the layer normalization operator is a bit more involved than the forward pass.\n+Let :math:`\\hat{x}` be the normalized inputs :math:`\\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} }` before the linear transformation,\n+the Vector-Jacobian Products (VJP) :math:`\\nabla_{x}` of :math:`x` are given by:\n+\n+.. math::\n+   \\nabla_{x} = \\frac{1}{\\sigma}\\Big( \\nabla_{y} \\odot w - \\underbrace{ \\big( \\frac{1}{N} \\hat{x} \\cdot (\\nabla_{y} \\odot w) \\big) }_{c_1} \\odot \\hat{x} - \\underbrace{ \\frac{1}{N} \\nabla_{y} \\cdot w }_{c_2} \\Big)\n+\n+where :math:`\\odot` denotes the element-wise multiplication, :math:`\\cdot` denotes the dot product, and :math:`\\sigma` is the standard deviation.\n+:math:`c_1` and :math:`c_2` are intermediate constants that improve the readability of the following implementation.\n+\n+For the weights :math:`w` and biases :math:`b`, the VJPs :math:`\\nabla_{w}` and :math:`\\nabla_{b}` are more straightforward:\n+\n+.. math::\n+   \\nabla_{w} = \\nabla_{y} \\odot \\hat{x} \\quad \\text{and} \\quad \\nabla_{b} = \\nabla_{y}\n+\n+Since the same weights :math:`w` and biases :math:`b` are used for all rows in the same batch, their gradients need to sum up.\n+To perform this step efficiently, we use a parallel reduction strategy: each kernel instance accumulates\n+partial :math:`\\nabla_{w}` and :math:`\\nabla_{b}` across certain rows into one of :math:`\\text{GROUP_SIZE_M}` independent buffers.\n+These buffers stay in the L2 cache and then are further reduced by another function to compute the actual :math:`\\nabla_{w}` and :math:`\\nabla_{b}`.\n+\n+Let the number of input rows :math:`M = 4` and :math:`\\text{GROUP_SIZE_M} = 2`,\n+here's a diagram of the parallel reduction strategy for :math:`\\nabla_{w}` (:math:`\\nabla_{b}` is omitted for brevity):\n+\n+  .. image:: parallel_reduction.png\n+\n+In Stage 1, the rows of X that have the same color share the same buffer and thus a lock is used to ensure that only one kernel instance writes to the buffer at a time.\n+In Stage 2, the buffers are further reduced to compute the final :math:`\\nabla_{w}` and :math:`\\nabla_{b}`.\n+In the following implementation, Stage 1 is implemented by the function :code:`_layer_norm_bwd_dx_fused` and Stage 2 is implemented by the function :code:`_layer_norm_bwd_dwdb`.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 125-221\n+\n+.. code-block:: default\n+\n+\n+    @triton.jit\n+    def _layer_norm_bwd_dx_fused(\n+        DX,  # pointer to the input gradient\n+        DY,  # pointer to the output gradient\n+        DW,  # pointer to the partial sum of weights gradient\n+        DB,  # pointer to the partial sum of biases gradient\n+        X,   # pointer to the input\n+        W,   # pointer to the weights\n+        B,   # pointer to the biases\n+        Mean,   # pointer to the mean\n+        Rstd,   # pointer to the 1/std\n+        Lock,  # pointer to the lock\n+        stride,  # how much to increase the pointer when moving by 1 row\n+        N,  # number of columns in X\n+        eps,  # epsilon to avoid division by zero\n+        GROUP_SIZE_M: tl.constexpr,\n+        BLOCK_SIZE_N: tl.constexpr\n+    ):\n+        # Map the program id to the elements of X, DX, and DY it should compute.\n+        row = tl.program_id(0)\n+        cols = tl.arange(0, BLOCK_SIZE_N)\n+        mask = cols < N\n+        X += row * stride\n+        DY += row * stride\n+        DX += row * stride\n+        # Offset locks and weights/biases gradient pointer for parallel reduction\n+        lock_id = row % GROUP_SIZE_M\n+        Lock += lock_id\n+        Count = Lock + GROUP_SIZE_M\n+        DW = DW + lock_id * N + cols\n+        DB = DB + lock_id * N + cols\n+        # Load data to SRAM\n+        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n+        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n+        w = tl.load(W + cols, mask=mask).to(tl.float32)\n+        mean = tl.load(Mean + row)\n+        rstd = tl.load(Rstd + row)\n+        # Compute dx\n+        xhat = (x - mean) * rstd\n+        wdy = w * dy\n+        xhat = tl.where(mask, xhat, 0.)\n+        wdy = tl.where(mask, wdy, 0.)\n+        c1 = tl.sum(xhat * wdy, axis=0) / N\n+        c2 = tl.sum(wdy, axis=0) / N\n+        dx = (wdy - (xhat * c1 + c2)) * rstd\n+        # Write dx\n+        tl.store(DX + cols, dx, mask=mask)\n+        # Accumulate partial sums for dw/db\n+        partial_dw = (dy * xhat).to(w.dtype)\n+        partial_db = (dy).to(w.dtype)\n+        while tl.atomic_cas(Lock, 0, 1) == 1:\n+            pass\n+        count = tl.load(Count)\n+        # First store doesn't accumulate\n+        if count == 0:\n+            tl.atomic_xchg(Count, 1)\n+        else:\n+            partial_dw += tl.load(DW, mask=mask)\n+            partial_db += tl.load(DB, mask=mask)\n+        tl.store(DW, partial_dw, mask=mask)\n+        tl.store(DB, partial_db, mask=mask)\n+        # Release the lock\n+        tl.atomic_xchg(Lock, 0)\n+\n+\n+    @triton.jit\n+    def _layer_norm_bwd_dwdb(\n+        DW,  # pointer to the partial sum of weights gradient\n+        DB,  # pointer to the partial sum of biases gradient\n+        FINAL_DW,  # pointer to the weights gradient\n+        FINAL_DB,  # pointer to the biases gradient\n+        M,  # GROUP_SIZE_M\n+        N,  # number of columns\n+        BLOCK_SIZE_M: tl.constexpr,\n+        BLOCK_SIZE_N: tl.constexpr\n+    ):\n+        # Map the program id to the elements of DW and DB it should compute.\n+        pid = tl.program_id(0)\n+        cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+        dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+        db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+        # Iterate through the rows of DW and DB to sum the partial sums.\n+        for i in range(0, M, BLOCK_SIZE_M):\n+            rows = i + tl.arange(0, BLOCK_SIZE_M)\n+            mask = (rows[:, None] < M) & (cols[None, :] < N)\n+            offs = rows[:, None] * N + cols[None, :]\n+            dw += tl.load(DW + offs, mask=mask, other=0.)\n+            db += tl.load(DB + offs, mask=mask, other=0.)\n+        # Write the final sum to the output.\n+        sum_dw = tl.sum(dw, axis=0)\n+        sum_db = tl.sum(db, axis=0)\n+        tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n+        tl.store(FINAL_DB + cols, sum_db, mask=cols < N)\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 222-228\n+\n+Benchmark\n+---------\n+\n+We can now compare the performance of our kernel against that of PyTorch.\n+Here we focus on inputs that have Less than 64KB per feature.\n+Specifically, one can set :code:`'mode': 'backward'` to benchmark the backward pass.\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 228-367\n+\n+.. code-block:: default\n+\n+\n+\n+    class LayerNorm(torch.autograd.Function):\n+\n+        @staticmethod\n+        def forward(ctx, x, normalized_shape, weight, bias, eps):\n+            # allocate output\n+            y = torch.empty_like(x)\n+            # reshape input data into 2D tensor\n+            x_arg = x.reshape(-1, x.shape[-1])\n+            M, N = x_arg.shape\n+            mean = torch.empty((M, ), dtype=torch.float32, device='cuda')\n+            rstd = torch.empty((M, ), dtype=torch.float32, device='cuda')\n+            # Less than 64KB per feature: enqueue fused kernel\n+            MAX_FUSED_SIZE = 65536 // x.element_size()\n+            BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n+            if N > BLOCK_SIZE:\n+                raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n+            # heuristics for number of warps\n+            num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\n+            # enqueue kernel\n+            _layer_norm_fwd_fused[(M,)](x_arg, y, weight, bias, mean, rstd,\n+                                        x_arg.stride(0), N, eps,\n+                                        BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)\n+            ctx.save_for_backward(x, weight, bias, mean, rstd)\n+            ctx.BLOCK_SIZE = BLOCK_SIZE\n+            ctx.num_warps = num_warps\n+            ctx.eps = eps\n+            return y\n+\n+        @staticmethod\n+        def backward(ctx, dy):\n+            x, w, b, m, v = ctx.saved_tensors\n+            # heuristics for amount of parallel reduction stream for DW/DB\n+            N = w.shape[0]\n+            GROUP_SIZE_M = 64\n+            if N <= 8192: GROUP_SIZE_M = 96\n+            if N <= 4096: GROUP_SIZE_M = 128\n+            if N <= 1024: GROUP_SIZE_M = 256\n+            # allocate output\n+            locks = torch.zeros(2 * GROUP_SIZE_M, dtype=torch.int32, device='cuda')\n+            _dw = torch.empty((GROUP_SIZE_M, w.shape[0]), dtype=x.dtype, device=w.device)\n+            _db = torch.empty((GROUP_SIZE_M, w.shape[0]), dtype=x.dtype, device=w.device)\n+            dw = torch.empty((w.shape[0],), dtype=w.dtype, device=w.device)\n+            db = torch.empty((w.shape[0],), dtype=w.dtype, device=w.device)\n+            dx = torch.empty_like(dy)\n+            # enqueue kernel using forward pass heuristics\n+            # also compute partial sums for DW and DB\n+            x_arg = x.reshape(-1, x.shape[-1])\n+            M, N = x_arg.shape\n+            _layer_norm_bwd_dx_fused[(M,)](dx, dy, _dw, _db, x, w, b, m, v, locks,\n+                                           x_arg.stride(0), N, ctx.eps,\n+                                           BLOCK_SIZE_N=ctx.BLOCK_SIZE,\n+                                           GROUP_SIZE_M=GROUP_SIZE_M,\n+                                           num_warps=ctx.num_warps)\n+            grid = lambda meta: [triton.cdiv(N, meta['BLOCK_SIZE_N'])]\n+            # accumulate partial sums in separate kernel\n+            _layer_norm_bwd_dwdb[grid](_dw, _db, dw, db, GROUP_SIZE_M, N,\n+                                       BLOCK_SIZE_M=32,\n+                                       BLOCK_SIZE_N=128)\n+            return dx, None, dw, db, None\n+\n+\n+    layer_norm = LayerNorm.apply\n+\n+\n+    def test_layer_norm(M, N, dtype, eps=1e-5, device='cuda'):\n+        # create data\n+        x_shape = (M, N)\n+        w_shape = (x_shape[-1], )\n+        weight = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n+        bias = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n+        x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\n+        dy = .1 * torch.randn_like(x)\n+        x.requires_grad_(True)\n+        # forward pass\n+        y_tri = layer_norm(x, w_shape, weight, bias, eps)\n+        y_ref = torch.nn.functional.layer_norm(x, w_shape, weight, bias, eps).to(dtype)\n+        # backward pass (triton)\n+        y_tri.backward(dy, retain_graph=True)\n+        dx_tri, dw_tri, db_tri = [_.grad.clone() for _ in [x, weight, bias]]\n+        x.grad, weight.grad, bias.grad = None, None, None\n+        # backward pass (torch)\n+        y_ref.backward(dy, retain_graph=True)\n+        dx_ref, dw_ref, db_ref = [_.grad.clone() for _ in [x, weight, bias]]\n+        # compare\n+        assert torch.allclose(y_tri, y_ref, atol=1e-2, rtol=0)\n+        assert torch.allclose(dx_tri, dx_ref, atol=1e-2, rtol=0)\n+        assert torch.allclose(db_tri, db_ref, atol=1e-2, rtol=0)\n+        assert torch.allclose(dw_tri, dw_ref, atol=1e-2, rtol=0)\n+\n+\n+    @triton.testing.perf_report(\n+        triton.testing.Benchmark(\n+            x_names=['N'],\n+            x_vals=[512 * i for i in range(2, 32)],\n+            line_arg='provider',\n+            line_vals=['triton', 'torch'] + (['apex'] if HAS_APEX else []),\n+            line_names=['Triton', 'Torch'] + (['Apex'] if HAS_APEX else []),\n+            styles=[('blue', '-'), ('green', '-'), ('orange', '-')],\n+            ylabel='GB/s',\n+            plot_name='layer-norm-backward',\n+            args={'M': 4096, 'dtype': torch.float16, 'mode': 'backward'}\n+        )\n+    )\n+    def bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='cuda'):\n+        # create data\n+        x_shape = (M, N)\n+        w_shape = (x_shape[-1], )\n+        weight = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n+        bias = torch.rand(w_shape, dtype=dtype, device='cuda', requires_grad=True)\n+        x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\n+        dy = .1 * torch.randn_like(x)\n+        x.requires_grad_(True)\n+        quantiles = [0.5, 0.2, 0.8]\n+        # utility functions\n+        if provider == 'triton':\n+            y_fwd = lambda: layer_norm(x, w_shape, weight, bias, eps)\n+        if provider == 'torch':\n+            y_fwd = lambda: torch.nn.functional.layer_norm(x, w_shape, weight, bias, eps)\n+        if provider == 'apex':\n+            apex_layer_norm = apex.normalization.FusedLayerNorm(w_shape).to(x.device).to(x.dtype)\n+            y_fwd = lambda: apex_layer_norm(x)\n+        # forward pass\n+        if mode == 'forward':\n+            gbps = lambda ms: 2 * x.numel() * x.element_size() / ms * 1e-6\n+            ms, min_ms, max_ms = triton.testing.do_bench(y_fwd, quantiles=quantiles, rep=500)\n+        # backward pass\n+        if mode == 'backward':\n+            gbps = lambda ms: 3 * x.numel() * x.element_size() / ms * 1e-6\n+            y = y_fwd()\n+            ms, min_ms, max_ms = triton.testing.do_bench(lambda: y.backward(dy, retain_graph=True),\n+                                                         quantiles=quantiles, grad_to_none=[x], rep=500)\n+        return gbps(ms), gbps(max_ms), gbps(min_ms)\n+\n+\n+    test_layer_norm(1151, 8192, torch.float16)\n+    bench_layer_norm.run(save_path='.', print_data=True)\n+\n+\n+\n+\n+.. image:: /getting-started/tutorials/images/sphx_glr_05-layer-norm_001.png\n+    :alt: 05 layer norm\n+    :class: sphx-glr-single-img\n+\n+\n+.. rst-class:: sphx-glr-script-out\n+\n+ Out:\n+\n+ .. code-block:: none\n+\n+    layer-norm-backward:\n+              N      Triton       Torch        Apex\n+    0    1024.0  299.707322  289.129413  311.088617\n+    1    1536.0  344.523365  290.267701  338.201833\n+    2    2048.0  413.042029  303.407414  321.254900\n+    3    2560.0  461.954908  299.707311  326.808501\n+    4    3072.0  511.999982  282.482750  317.793096\n+    5    3584.0  551.384634  277.470965  308.301075\n+    6    4096.0  564.965515  296.096389  296.096389\n+    7    4608.0  336.145894  282.122458  291.799469\n+    8    5120.0  360.351899  279.272725  287.775181\n+    9    5632.0  380.754933  261.953503  287.591490\n+    10   6144.0  397.455524  271.558010  286.322318\n+    11   6656.0  412.775186  272.136292  285.767438\n+    12   7168.0  417.553404  271.772517  283.881181\n+    13   7680.0  405.991195  275.928134  282.266452\n+    14   8192.0  392.431159  289.982309  275.747539\n+    15   8704.0  405.623284  267.815384  279.646592\n+    16   9216.0  417.328287  274.762727  286.137125\n+    17   9728.0  426.043810  274.028160  289.308559\n+    18  10240.0  436.518666  265.686486  289.469963\n+    19  10752.0  441.862996  268.520296  289.291486\n+    20  11264.0  443.901492  260.942092  286.980888\n+    21  11776.0  438.176729  262.661699  288.097854\n+    22  12288.0  429.900884  267.372616  295.207195\n+    23  12800.0  405.812417  264.827590  288.993430\n+    24  13312.0  404.415180  263.821630  290.179836\n+    25  13824.0  392.170222  261.756215  291.543045\n+    26  14336.0  386.588753  256.000002  285.293536\n+    27  14848.0  373.143457  262.023530  288.544136\n+    28  15360.0  367.537405  266.550966  288.676598\n+    29  15872.0  362.788584  259.841749  290.120338\n+\n+\n+\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 368-372\n+\n+References\n+----------\n+\n+.. [BA2016] Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \"Layer Normalization\", Arxiv 2016\n+\n+\n+.. rst-class:: sphx-glr-timing\n+\n+   **Total running time of the script:** ( 1 minutes  0.853 seconds)\n+\n+\n+.. _sphx_glr_download_getting-started_tutorials_05-layer-norm.py:\n+\n+\n+.. only :: html\n+\n+ .. container:: sphx-glr-footer\n+    :class: sphx-glr-footer-example\n+\n+\n+\n+  .. container:: sphx-glr-download sphx-glr-download-python\n+\n+     :download:`Download Python source code: 05-layer-norm.py <05-layer-norm.py>`\n+\n+\n+\n+  .. container:: sphx-glr-download sphx-glr-download-jupyter\n+\n+     :download:`Download Jupyter notebook: 05-layer-norm.ipynb <05-layer-norm.ipynb>`\n+\n+\n+.. only:: html\n+\n+ .. rst-class:: sphx-glr-signature\n+\n+    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"}, {"filename": "keren/fix-doc/_sources/getting-started/tutorials/07-math-functions.rst.txt", "status": "renamed", "additions": 24, "deletions": 25, "changes": 49, "file_content_changes": "@@ -2,39 +2,38 @@\n .. DO NOT EDIT.\n .. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.\n .. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:\n-.. \"getting-started/tutorials/07-libdevice-function.py\"\n+.. \"getting-started/tutorials/07-math-functions.py\"\n .. LINE NUMBERS ARE GIVEN BELOW.\n \n .. only:: html\n \n     .. note::\n         :class: sphx-glr-download-link-note\n \n-        Click :ref:`here <sphx_glr_download_getting-started_tutorials_07-libdevice-function.py>`\n+        Click :ref:`here <sphx_glr_download_getting-started_tutorials_07-math-functions.py>`\n         to download the full example code\n \n .. rst-class:: sphx-glr-example-title\n \n-.. _sphx_glr_getting-started_tutorials_07-libdevice-function.py:\n+.. _sphx_glr_getting-started_tutorials_07-math-functions.py:\n \n \n-Libdevice function\n+Libdevice (`tl.math`) function\n ===============\n Triton can invoke a custom function from an external library.\n-In this example, we will use the `libdevice` library to apply `asin` on a tensor.\n+In this example, we will use the `libdevice` library (a.k.a `math` in triton) to apply `asin` on a tensor.\n Please refer to https://docs.nvidia.com/cuda/libdevice-users-guide/index.html regarding the semantics of all available libdevice functions.\n-\n-In `trition/language/libdevice.py`, we try to aggregate functions with the same computation but different data types together.\n+In `triton/language/math.py`, we try to aggregate functions with the same computation but different data types together.\n For example, both `__nv_asin` and `__nvasinf` calculate the principal value of the arc sine of the input, but `__nv_asin` operates on `double` and `__nv_asinf` operates on `float`.\n-Using triton, you can simply call `tl.libdevice.asinf`.\n-triton automatically selects the correct underlying device function to invoke based on input and output types.\n+Using triton, you can simply call `tl.math.asin`.\n+Triton automatically selects the correct underlying device function to invoke based on input and output types.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 15-17\n+.. GENERATED FROM PYTHON SOURCE LINES 14-16\n \n asin Kernel\n --------------------------\n \n-.. GENERATED FROM PYTHON SOURCE LINES 17-39\n+.. GENERATED FROM PYTHON SOURCE LINES 16-38\n \n .. code-block:: default\n \n@@ -47,17 +46,17 @@ asin Kernel\n \n     @triton.jit\n     def asin_kernel(\n-        x_ptr,\n-        y_ptr,\n-        n_elements,\n-        BLOCK_SIZE: tl.constexpr,\n+            x_ptr,\n+            y_ptr,\n+            n_elements,\n+            BLOCK_SIZE: tl.constexpr,\n     ):\n         pid = tl.program_id(axis=0)\n         block_start = pid * BLOCK_SIZE\n         offsets = block_start + tl.arange(0, BLOCK_SIZE)\n         mask = offsets < n_elements\n         x = tl.load(x_ptr + offsets, mask=mask)\n-        x = tl.libdevice.asin(x)\n+        x = tl.math.asin(x)\n         tl.store(y_ptr + offsets, x, mask=mask)\n \n \n@@ -67,13 +66,13 @@ asin Kernel\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 40-43\n+.. GENERATED FROM PYTHON SOURCE LINES 39-42\n \n Using the default libdevice library path\n --------------------------\n-We can use the default libdevice library path encoded in `triton/language/libdevice.py`\n+We can use the default libdevice library path encoded in `triton/language/math.py`\n \n-.. GENERATED FROM PYTHON SOURCE LINES 43-61\n+.. GENERATED FROM PYTHON SOURCE LINES 42-60\n \n .. code-block:: default\n \n@@ -112,13 +111,13 @@ We can use the default libdevice library path encoded in `triton/language/libdev\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 62-65\n+.. GENERATED FROM PYTHON SOURCE LINES 61-64\n \n Customize the libdevice library path\n --------------------------\n We can also customize the libdevice library path by passing the path to the `libdevice` library to the `asin` kernel.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 65-75\n+.. GENERATED FROM PYTHON SOURCE LINES 64-74\n \n .. code-block:: default\n \n@@ -152,10 +151,10 @@ We can also customize the libdevice library path by passing the path to the `lib\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  0.258 seconds)\n+   **Total running time of the script:** ( 0 minutes  0.444 seconds)\n \n \n-.. _sphx_glr_download_getting-started_tutorials_07-libdevice-function.py:\n+.. _sphx_glr_download_getting-started_tutorials_07-math-functions.py:\n \n \n .. only :: html\n@@ -167,13 +166,13 @@ We can also customize the libdevice library path by passing the path to the `lib\n \n   .. container:: sphx-glr-download sphx-glr-download-python\n \n-     :download:`Download Python source code: 07-libdevice-function.py <07-libdevice-function.py>`\n+     :download:`Download Python source code: 07-math-functions.py <07-math-functions.py>`\n \n \n \n   .. container:: sphx-glr-download sphx-glr-download-jupyter\n \n-     :download:`Download Jupyter notebook: 07-libdevice-function.ipynb <07-libdevice-function.ipynb>`\n+     :download:`Download Jupyter notebook: 07-math-functions.ipynb <07-math-functions.ipynb>`\n \n \n .. only:: html"}, {"filename": "keren/fix-doc/_sources/getting-started/tutorials/index.rst.txt", "status": "renamed", "additions": 10, "deletions": 31, "changes": 41, "file_content_changes": "@@ -5,7 +5,7 @@\n .. _sphx_glr_getting-started_tutorials:\n \n Tutorials\n-==================\n+=========\n \n Below is a gallery of tutorials for writing various basic operations with Triton. It is recommended that you read through the tutorials in order, starting with the simplest one.\n \n@@ -20,7 +20,7 @@ To install the dependencies for the tutorials:\n \n .. raw:: html\n \n-    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"- The basic programming model of Triton - The triton.jit decorator, which is used to define Tri...\">\n+    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"In this tutorial, you will write a simple vector addition using Triton.\">\n \n .. only:: html\n \n@@ -41,7 +41,7 @@ To install the dependencies for the tutorials:\n \n .. raw:: html\n \n-    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"- The benefits of kernel fusion for bandwidth-bound operations. - Reduction operators in Triton...\">\n+    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"In this tutorial, you will write a fused softmax operation that is significantly faster than Py...\">\n \n .. only:: html\n \n@@ -62,7 +62,7 @@ To install the dependencies for the tutorials:\n \n .. raw:: html\n \n-    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"- Block-level matrix multiplications - Multi-dimensional pointer arithmetic - Program re-orderi...\">\n+    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"In doing so, you will learn about: - Block-level matrix multiplications - Multi-dimensional poi...\">\n \n .. only:: html\n \n@@ -104,7 +104,7 @@ To install the dependencies for the tutorials:\n \n .. raw:: html\n \n-    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"Layer Normalization\">\n+    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"In doing so, you will learn about: - Implementing backward pass in Triton - Implementing parall...\">\n \n .. only:: html\n \n@@ -125,14 +125,14 @@ To install the dependencies for the tutorials:\n \n .. raw:: html\n \n-    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"Fused Attention\">\n+    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"Libdevice (`tl.math`) function\">\n \n .. only:: html\n \n- .. figure:: /getting-started/tutorials/images/thumb/sphx_glr_06-fused-attention_thumb.png\n-     :alt: Fused Attention\n+ .. figure:: /getting-started/tutorials/images/thumb/sphx_glr_07-math-functions_thumb.png\n+     :alt: Libdevice (`tl.math`) function\n \n-     :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py`\n+     :ref:`sphx_glr_getting-started_tutorials_07-math-functions.py`\n \n .. raw:: html\n \n@@ -142,28 +142,7 @@ To install the dependencies for the tutorials:\n .. toctree::\n    :hidden:\n \n-   /getting-started/tutorials/06-fused-attention\n-\n-.. raw:: html\n-\n-    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"In trition/language/libdevice.py, we try to aggregate functions with the same computation but d...\">\n-\n-.. only:: html\n-\n- .. figure:: /getting-started/tutorials/images/thumb/sphx_glr_07-libdevice-function_thumb.png\n-     :alt: Libdevice function\n-\n-     :ref:`sphx_glr_getting-started_tutorials_07-libdevice-function.py`\n-\n-.. raw:: html\n-\n-    </div>\n-\n-\n-.. toctree::\n-   :hidden:\n-\n-   /getting-started/tutorials/07-libdevice-function\n+   /getting-started/tutorials/07-math-functions\n .. raw:: html\n \n     <div class=\"sphx-glr-clear\"></div>"}, {"filename": "keren/fix-doc/_sources/getting-started/tutorials/sg_execution_times.rst.txt", "status": "renamed", "additions": 8, "deletions": 6, "changes": 14, "file_content_changes": "@@ -5,16 +5,18 @@\n \n Computation times\n =================\n-**12:39.242** total execution time for **getting-started_tutorials** files:\n+**04:31.219** total execution time for **getting-started_tutorials** files:\n \n +---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``) | 05:26.818 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``) | 01:39.119 | 0.0 MB |\n +---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                 | 03:20.891 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                 | 01:19.698 | 0.0 MB |\n +---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                       | 02:07.366 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                       | 01:00.853 | 0.0 MB |\n +---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                       | 01:44.156 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                       | 00:29.843 | 0.0 MB |\n +---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)       | 00:00.011 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)       | 00:01.261 | 0.0 MB |\n++---------------------------------------------------------------------------------------------------------+-----------+--------+\n+| :ref:`sphx_glr_getting-started_tutorials_07-math-functions.py` (``07-math-functions.py``)               | 00:00.444 | 0.0 MB |\n +---------------------------------------------------------------------------------------------------------+-----------+--------+"}, {"filename": "keren/fix-doc/_sources/index.rst.txt", "status": "renamed", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -3,6 +3,7 @@ Welcome to Triton's documentation!\n \n Triton is a language and compiler for parallel programming. It aims to provide a Python-based programming environment for productively writing custom DNN compute kernels capable of running at maximal throughput on modern GPU hardware.\n \n+\n Getting Started\n ---------------\n \n@@ -17,8 +18,9 @@ Getting Started\n    getting-started/installation\n    getting-started/tutorials/index\n \n+\n Python API\n--------------------\n+----------\n \n - :doc:`triton <python-api/triton>`\n - :doc:`triton.language <python-api/triton.language>`\n@@ -34,9 +36,9 @@ Python API\n    python-api/triton.language\n    python-api/triton.testing\n \n-   \n+\n Going Further\n-------------------\n+-------------\n \n Check out the following documents to learn more about Triton and how it compares against other DSLs for DNNs:\n "}, {"filename": "keren/fix-doc/_sources/programming-guide/chapter-1/introduction.rst.txt", "status": "renamed", "additions": 19, "deletions": 17, "changes": 36, "file_content_changes": "@@ -1,18 +1,18 @@\n-==============\n+============\n Introduction\n-==============\n+============\n \n---------------\n+-----------\n Motivations\n---------------\n+-----------\n \n-Over the past decade, Deep Neural Networks (DNNs) have emerged as an important class of Machine Learning (ML) models, capable of  achieving state-of-the-art performance across many domains ranging from natural language processing [SUTSKEVER2014]_ to computer vision [REDMON2016]_ to computational neuroscience [LEE2017]_. The strength of these models lies in their hierarchical structure, composed of a sequence of parametric (e.g., convolutional) and non-parametric (e.g., rectified linearity) *layers*. This pattern, though notoriously computationally expensive, also generates a large amount of highly parallelizable work particularly well suited for multi- and many- core processors.\n+Over the past decade, Deep Neural Networks (DNNs) have emerged as an important class of Machine Learning (ML) models, capable of achieving state-of-the-art performance across many domains ranging from natural language processing [SUTSKEVER2014]_ to computer vision [REDMON2016]_ to computational neuroscience [LEE2017]_. The strength of these models lies in their hierarchical structure, composed of a sequence of parametric (e.g., convolutional) and non-parametric (e.g., rectified linearity) *layers*. This pattern, though notoriously computationally expensive, also generates a large amount of highly parallelizable work particularly well suited for multi- and many- core processors.\n \n As a consequence, Graphics Processing Units (GPUs) have become a cheap and accessible resource for exploring and/or deploying novel research ideas in the field. This trend has been accelerated by the release of several frameworks for General-Purpose GPU (GPGPU) computing, such as CUDA and OpenCL, which have made the development of high-performance programs easier. Yet, GPUs remain incredibly challenging to optimize for locality and parallelism, especially for computations that cannot be efficiently implemented using a combination of pre-existing optimized primitives. To make matters worse, GPU architectures are also rapidly evolving and specializing, as evidenced by the addition of tensor cores to NVIDIA (and more recently AMD) micro-architectures.\n \n-This tension between the computational opportunities offered by DNNs and the practical difficulty of GPU programming has created substantial academic and industrial interest for Domain-Specific Languages (DSLs) and compilers. Regrettably, these systems -- whether they be based on  polyhedral machinery (*e.g.*, Tiramisu [BAGHDADI2021]_, Tensor Comprehensions [VASILACHE2018]_) or scheduling languages (*e.g.*, Halide [JRK2013]_, TVM [CHEN2018]_) -- remain less flexible and (for the same algorithm) markedly slower than the best handwritten compute kernels available in libraries like `cuBLAS <https://docs.nvidia.com/cuda/cublas/index.html>`_, `cuDNN <https://docs.nvidia.com/deeplearning/cudnn/api/index.html>`_ or `TensorRT <https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html>`_.\n+This tension between the computational opportunities offered by DNNs and the practical difficulty of GPU programming has created substantial academic and industrial interest for Domain-Specific Languages (DSLs) and compilers. Regrettably, these systems -- whether they be based on polyhedral machinery (e.g., Tiramisu [BAGHDADI2021]_, Tensor Comprehensions [VASILACHE2018]_) or scheduling languages (e.g., Halide [JRK2013]_, TVM [CHEN2018]_) -- remain less flexible and (for the same algorithm) markedly slower than the best handwritten compute kernels available in libraries like `cuBLAS <https://docs.nvidia.com/cuda/cublas/index.html>`_, `cuDNN <https://docs.nvidia.com/deeplearning/cudnn/api/index.html>`_ or `TensorRT <https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html>`_.\n \n-The main premise of this project is the following: programming paradigms based on blocked algorithms [LAM1991]_ can facilitate the construction of high-performance compute kernels for neural networks.  We specifically revisit traditional \"Single Program, Multiple Data\" (SPMD [AUGUIN1983]_) execution models for GPUs, and propose a variant in which programs -- rather than threads -- are blocked. For example, in the case of matrix multiplication, CUDA and Triton differ as follows:\n+The main premise of this project is the following: programming paradigms based on blocked algorithms [LAM1991]_ can facilitate the construction of high-performance compute kernels for neural networks. We specifically revisit traditional \"Single Program, Multiple Data\" (SPMD [AUGUIN1983]_) execution models for GPUs, and propose a variant in which programs -- rather than threads -- are blocked. For example, in the case of matrix multiplication, CUDA and Triton differ as follows:\n \n .. table::\n     :widths: 50 50\n@@ -27,14 +27,14 @@ The main premise of this project is the following: programming paradigms based o\n     |                                                     |   :force:                                           |\n     |                                                     |                                                     |\n     |   #pragma parallel                                  |   #pragma parallel                                  |\n-    |   for(int m = 0; i < M; m++)                        |   for(int m = 0; m < M; m += MB)                    |\n+    |   for(int m = 0; m < M; m++)                        |   for(int m = 0; m < M; m += MB)                    |\n     |   #pragma parallel                                  |   #pragma parallel                                  |\n-    |   for(int n = 0; j < N; n++){                       |   for(int n = 0; n < N; n += NB){                   |\n+    |   for(int n = 0; n < N; n++){                       |   for(int n = 0; n < N; n += NB){                   |\n     |     float acc = 0;                                  |     float acc[MB, NB] = 0;                          |\n-    |     for(int k = 0; k < K;k ++)                      |     for(int k = 0; k < K; k += KB)                  |\n-    |       acc += A[i, k]* B[k, j];                      |       acc +=  A[m:m+MB, k:k+KB]                     |\n+    |     for(int k = 0; k < K; k++)                      |     for(int k = 0; k < K; k += KB)                  |\n+    |       acc += A[m, k] * B[k, n];                     |       acc +=  A[m:m+MB, k:k+KB]                     |\n     |                                                     |             @ B[k:k+KB, n:n+NB];                    |\n-    |     C[i, j] = acc;                                  |     C[m:m+MB, n:n+NB] = acc;                        |\n+    |     C[m, n] = acc;                                  |     C[m:m+MB, n:n+NB] = acc;                        |\n     |   }                                                 |   }                                                 |\n     |                                                     |                                                     |\n     +-----------------------------------------------------+-----------------------------------------------------+\n@@ -48,15 +48,17 @@ The main premise of this project is the following: programming paradigms based o\n \n A key benefit of this approach is that it leads to block-structured iteration spaces that offer programmers more flexibility than existing DSLs when implementing sparse operations, all while allowing compilers to aggressively optimize programs for data locality and parallelism.\n \n---------------\n+\n+----------\n Challenges\n---------------\n+----------\n \n The main challenge posed by our proposed paradigm is that of work scheduling, i.e., how the work done by each program instance should be partitioned for efficient execution on modern GPUs. To address this issue, the Triton compiler makes heavy use of *block-level data-flow analysis*, a technique for scheduling iteration blocks statically based on the control- and data-flow structure of the target program. The resulting system actually works surprisingly well: our compiler manages to apply a broad range of interesting optimization automatically (e.g., automatic coalescing, thread swizzling, pre-fetching, automatic vectorization, tensor core-aware instruction selection, shared memory allocation/synchronization, asynchronous copy scheduling). Of course doing all this is not trivial; one of the purposes of this guide is to give you a sense of how it works.\n \n---------------\n+\n+----------\n References\n---------------\n+----------\n \n .. [SUTSKEVER2014] I. Sutskever et al., \"Sequence to Sequence Learning with Neural Networks\", NIPS 2014\n .. [REDMON2016] J. Redmon et al., \"You Only Look Once: Unified, Real-Time Object Detection\", CVPR 2016\n@@ -66,4 +68,4 @@ References\n .. [JRK2013] J. Ragan-Kelley et al., \"Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines\", PLDI 2013\n .. [CHEN2018] T. Chen et al., \"TVM: An Automated End-to-End Optimizing Compiler for Deep Learning\", OSDI 2018\n .. [LAM1991] M. Lam et al., \"The Cache Performance and Optimizations of Blocked Algorithms\", ASPLOS 1991\n-.. [AUGUIN1983] M. Auguin et al., \"Opsila: an advanced SIMD for numerical analysis and signal processing\", EUROMICRO 1983\n\\ No newline at end of file\n+.. [AUGUIN1983] M. Auguin et al., \"Opsila: an advanced SIMD for numerical analysis and signal processing\", EUROMICRO 1983"}, {"filename": "keren/fix-doc/_sources/programming-guide/chapter-2/related-work.rst.txt", "status": "renamed", "additions": 38, "deletions": 35, "changes": 73, "file_content_changes": "@@ -1,20 +1,21 @@\n-==============\n+============\n Related Work\n-==============\n+============\n \n At first sight, Triton may seem like just yet another DSL for DNNs. The purpose of this section is to contextualize Triton and highlight its differences with the two leading approaches in this domain: polyhedral compilation and scheduling languages.\n \n------------------------\n+\n+----------------------\n Polyhedral Compilation\n------------------------\n+----------------------\n \n Traditional compilers typically rely on intermediate representations, such as LLVM-IR [LATTNER2004]_, that encode control flow information using (un)conditional branches. This relatively low-level format makes it difficult to statically analyze the runtime behavior (e.g., cache misses) of input programs, and to  automatically optimize loops accordingly through the use of tiling [WOLFE1989]_, fusion [DARTE1999]_ and interchange [ALLEN1984]_. To solve this issue, polyhedral compilers [ANCOURT1991]_ rely on program representations that have statically predictable control flow, thereby enabling aggressive compile-time program transformations for data locality and parallelism. Though this strategy has been adopted by many languages and compilers for DNNs such as Tiramisu [BAGHDADI2021]_, Tensor Comprehensions [VASILACHE2018]_, Diesel [ELANGO2018]_ and the Affine dialect in MLIR [LATTNER2019]_, it also comes with a number of limitations that will be described later in this section.\n \n-+++++++++++++++++++++++\n+++++++++++++++++++++++\n Program Representation\n-+++++++++++++++++++++++\n+++++++++++++++++++++++\n \n-Polyhedral compilation is a vast area of research. In this section we only outline the most basic aspects of this topic, but readers interested in the solid mathematical foundations underneath may refer to the ample litterature on linear and integer programming.\n+Polyhedral compilation is a vast area of research. In this section we only outline the most basic aspects of this topic, but readers interested in the solid mathematical foundations underneath may refer to the ample literature on linear and integer programming.\n \n .. table::\n     :widths: 50 50\n@@ -105,19 +106,19 @@ Where :math:`\\Theta_S(\\mathbf{x})` is a p-dimensional vector representing the sl\n \n where :math:`i` and :math:`j` are respectively the slowest and fastest growing loop indices in the nest. If :math:`T_S` is a vector (resp. tensor), then :math:`\\Theta_S` is a said to be one-dimensional (resp. multi-dimensional).\n \n-+++++++++++\n+++++++++++\n Advantages\n-+++++++++++\n+++++++++++\n \n Programs amenable to polyhedral compilation can be aggressively transformed and optimized. Most of these transformations actually boil down to the production of  schedules and iteration domains that enable loop transformations promoting parallelism and spatial/temporal data locality (e.g., fusion, interchange, tiling, parallelization).\n \n Polyhedral compilers can also automatically go through complex verification processes to ensure that the semantics of their input program is preserved throughout this optimization phase. Note that polyhedral optimizers are not incompatible with more standard optimization techniques. In fact, it is not uncommon for these systems to be implemented as a set of LLVM passes that can be run ahead of more traditional compilation techniques [GROSSER2012]_.\n \n-All in all, polyhedral machinery is extremely powerful, when applicable. It has been shown to support most common loop transformations, and has indeed achieved performance comparable to state-of-the-art GPU libraries for dense matrix multiplication [ELANGO2018]_. Additionally, it is also fully automatic and doesn't require any hint from programmers apart from source-code in a C-like format. \n+All in all, polyhedral machinery is extremely powerful, when applicable. It has been shown to support most common loop transformations, and has indeed achieved performance comparable to state-of-the-art GPU libraries for dense matrix multiplication [ELANGO2018]_. Additionally, it is also fully automatic and doesn't require any hint from programmers apart from source-code in a C-like format.\n \n-++++++++++++\n++++++++++++\n Limitations\n-++++++++++++\n++++++++++++\n \n Unfortunately, polyhedral compilers suffer from two major limitations that have prevented its adoption as a universal method for code generation in neural networks.\n \n@@ -127,48 +128,49 @@ Second, the polyhedral framework is not very generally applicable; SCoPs are rel\n \n On the other hand, blocked program representations advocated by this dissertation are less restricted in scope and can achieve close to peak performance using standard dataflow analysis.\n \n------------------------\n+\n+--------------------\n Scheduling Languages\n------------------------\n+--------------------\n \n-Separation of concerns [DIJKSTRA82]_ is a well-known design principle in computer science: programs should be decomposed into modular layers of abstraction that separate the semantics of their algorithms from the details of their implementation. Systems like Halide and TVM push this philosophy one step further, and enforce this separation at the grammatical level through the use of a  **scheduling language**. The benefits of this methodology are particularly visible in the case of matrix multiplication, where, as one can see below, the definition of the algorithm (Line 1-7) is completely disjoint from its implementation (Line 8-16), meaning that both can be maintained, optimized and distributed independently. \n+Separation of concerns [DIJKSTRA82]_ is a well-known design principle in computer science: programs should be decomposed into modular layers of abstraction that separate the semantics of their algorithms from the details of their implementation. Systems like Halide and TVM push this philosophy one step further, and enforce this separation at the grammatical level through the use of a  **scheduling language**. The benefits of this methodology are particularly visible in the case of matrix multiplication, where, as one can see below, the definition of the algorithm (Line 1-7) is completely disjoint from its implementation (Line 8-16), meaning that both can be maintained, optimized and distributed independently.\n \n .. code-block:: python\n   :linenos:\n \n   // algorithm\n   Var x(\"x\"), y(\"y\");\n-  Func matmul(\"matmul\"); \n-  RDom k(0, matrix_size); \n-  RVar ki; \n-  matmul(x, y) = 0.0f; \n-  matmul(x, y) += A(k, y) * B(x, k); \n+  Func matmul(\"matmul\");\n+  RDom k(0, matrix_size);\n+  RVar ki;\n+  matmul(x, y) = 0.0f;\n+  matmul(x, y) += A(k, y) * B(x, k);\n   // schedule\n-  Var xi(\"xi\"), xo(\"xo\"), yo(\"yo\"), yi(\"yo\"), yii(\"yii\"), xii(\"xii\"); \n-  matmul.vectorize(x, 8); \n-  matmul.update(0) \n-      .split(x, x, xi, block_size).split(xi, xi, xii, 8) \n-      .split(y, y, yi, block_size).split(yi, yi, yii, 4) \n-      .split(k, k, ki, block_size) \n-      .reorder(xii, yii, xi, ki, yi, k, x, y) \n+  Var xi(\"xi\"), xo(\"xo\"), yo(\"yo\"), yi(\"yo\"), yii(\"yii\"), xii(\"xii\");\n+  matmul.vectorize(x, 8);\n+  matmul.update(0)\n+      .split(x, x, xi, block_size).split(xi, xi, xii, 8)\n+      .split(y, y, yi, block_size).split(yi, yi, yii, 4)\n+      .split(k, k, ki, block_size)\n+      .reorder(xii, yii, xi, ki, yi, k, x, y)\n       .parallel(y).vectorize(xii).unroll(xi).unroll(yii);\n \n \n The resulting code may however not be completely portable, as schedules can sometimes rely on execution models (e.g., SPMD) or hardware intrinsics (e.g., matrix-multiply-accumulate) that are not widely available. This issue can be mitigated by auto-scheduling mechanisms [MULLAPUDI2016]_.\n \n-+++++++++++\n+++++++++++\n Advantages\n-+++++++++++\n+++++++++++\n \n The main advantage of this approach is that it allows programmers to write an algorithm *only once*, and focus on performance optimization separately. It makes it possible to manually specify optimizations that a polyhedral compiler wouldn't be able to figure out automatically using static data-flow analysis.\n \n Scheduling languages are, without a doubt, one of the most popular approaches for neural network code generation. The most popular system for this purpose is probably TVM, which provides good performance across a wide range of platforms as well as built-in automatic scheduling mechanisms.\n \n-++++++++++++\n++++++++++++\n Limitations\n-++++++++++++\n++++++++++++\n \n-This ease-of-development comes at a cost. First of all, existing systems that follow this paradigm tend to be noticeably slower than Triton on modern hardware when applicable (e.g., V100/A100 tensor cores w/ equal tile sizes). I do believe that this is not a fundamental issue of scheduling languages -- in the sense that it could probably be solved with more efforts -- but it could mean that these systems are harder to engineer. More importantly, existing scheduling languages generate loops whose bounds and increments cannot depend on surrounding loop indice without at least imposing severe constraints on possible schedules -- if not breaking the system entirely. This is problematic for sparse computations, whose iteration spaces may be irregular.\n+This ease-of-development comes at a cost. First of all, existing systems that follow this paradigm tend to be noticeably slower than Triton on modern hardware when applicable (e.g., V100/A100 tensor cores w/ equal tile sizes). I do believe that this is not a fundamental issue of scheduling languages -- in the sense that it could probably be solved with more efforts -- but it could mean that these systems are harder to engineer. More importantly, existing scheduling languages generate loops whose bounds and increments cannot depend on surrounding loop indices without at least imposing severe constraints on possible schedules -- if not breaking the system entirely. This is problematic for sparse computations, whose iteration spaces may be irregular.\n \n .. table::\n     :widths: 50 50\n@@ -181,7 +183,7 @@ This ease-of-development comes at a cost. First of all, existing systems that fo\n     |   for(int j = 0; j < 4; j++)                        |                                                     |\n     |     float acc = 0;                                  |                                                     |\n     |     for(int k = 0; k < K[i]; k++)                   |                                                     |\n-    |       acc += A[i][col[i,k]]*B[k][j]                 |                                                     |\n+    |       acc += A[i][col[i, k]] * B[k][j]              |                                                     |\n     |     C[i][j] = acc;                                  |                                                     |\n     +-----------------------------------------------------+-----------------------------------------------------+\n \n@@ -190,9 +192,10 @@ This ease-of-development comes at a cost. First of all, existing systems that fo\n \n On the other hand, the block-based program representation that we advocate for through this work allows for block-structured iteration spaces and allows programmers to manually handle load-balancing as they wish.\n \n---------------\n+\n+----------\n References\n---------------\n+----------\n \n .. [LATTNER2004] C. Lattner et al., \"LLVM: a compilation framework for lifelong program analysis transformation\", CGO 2004\n .. [WOLFE1989] M. Wolfe, \"More Iteration Space Tiling\", SC 1989"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.Config.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.Config.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.autotune.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.autotune.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.heuristics.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.heuristics.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.jit.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.jit.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.abs.rst.txt", "status": "renamed", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,6 +1,6 @@\n-\ufefftriton.language.cos\n+\ufefftriton.language.abs\n ===================\n \n .. currentmodule:: triton.language\n \n-.. autofunction:: cos\n\\ No newline at end of file\n+.. autofunction:: abs\n\\ No newline at end of file"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.arange.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.arange.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.atomic_add.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.atomic_add.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.atomic_cas.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.atomic_cas.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.atomic_max.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.atomic_max.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.atomic_min.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.atomic_min.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.atomic_xchg.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.atomic_xchg.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.broadcast_to.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.broadcast_to.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.cos.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.cos.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.dot.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.dot.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.exp.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.exp.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.load.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.load.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.log.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.log.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.max.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.max.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.maximum.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.maximum.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.min.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.min.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.minimum.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.minimum.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.multiple_of.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.multiple_of.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.num_programs.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.num_programs.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.program_id.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.program_id.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.rand.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.rand.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.randint.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.randint.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.randint4x.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.randint4x.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.randn.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.randn.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.ravel.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.ravel.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.reshape.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.reshape.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.sigmoid.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.sigmoid.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.sin.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.sin.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.softmax.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.softmax.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.sqrt.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.sqrt.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.store.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.store.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.sum.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.sum.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.where.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.where.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.language.zeros.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.language.zeros.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.testing.Benchmark.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.testing.Benchmark.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.testing.do_bench.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.testing.do_bench.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/generated/triton.testing.perf_report.rst.txt", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_sources/python-api/generated/triton.testing.perf_report.rst.txt"}, {"filename": "keren/fix-doc/_sources/python-api/triton.language.rst.txt", "status": "renamed", "additions": 18, "deletions": 14, "changes": 32, "file_content_changes": "@@ -1,11 +1,11 @@\n triton.language\n-================\n+===============\n \n .. currentmodule:: triton.language\n \n \n Programming Model\n--------------------\n+-----------------\n \n .. autosummary::\n     :toctree: generated\n@@ -16,7 +16,7 @@ Programming Model\n \n \n Creation Ops\n--------------\n+------------\n \n .. autosummary::\n     :toctree: generated\n@@ -27,7 +27,7 @@ Creation Ops\n \n \n Shape Manipulation Ops\n------------------------\n+----------------------\n \n .. autosummary::\n     :toctree: generated\n@@ -40,16 +40,17 @@ Shape Manipulation Ops\n \n \n Linear Algebra Ops\n--------------------\n+------------------\n \n .. autosummary::\n     :toctree: generated\n     :nosignatures:\n \n     dot\n \n+\n Memory Ops\n---------------------\n+----------\n \n .. autosummary::\n     :toctree: generated\n@@ -62,7 +63,7 @@ Memory Ops\n \n \n Indexing Ops\n---------------\n+------------\n \n .. autosummary::\n     :toctree: generated\n@@ -72,12 +73,13 @@ Indexing Ops\n \n \n Math Ops\n-----------\n+--------\n \n .. autosummary::\n     :toctree: generated\n     :nosignatures:\n \n+    abs\n     exp\n     log\n     cos\n@@ -88,7 +90,7 @@ Math Ops\n \n \n Reduction Ops\n----------------\n+-------------\n \n .. autosummary::\n     :toctree: generated\n@@ -98,8 +100,9 @@ Reduction Ops\n     min\n     sum\n \n+\n Atomic Ops\n----------------\n+----------\n \n .. autosummary::\n     :toctree: generated\n@@ -112,7 +115,7 @@ Atomic Ops\n \n \n Comparison ops\n----------------\n+--------------\n \n .. autosummary::\n     :toctree: generated\n@@ -124,7 +127,7 @@ Comparison ops\n .. _Random Number Generation:\n \n Random Number Generation\n--------------------------\n+------------------------\n \n .. autosummary::\n     :toctree: generated\n@@ -135,11 +138,12 @@ Random Number Generation\n     rand\n     randn\n \n+\n Compiler Hint Ops\n--------------------\n+-----------------\n \n .. autosummary::\n     :toctree: generated\n     :nosignatures:\n \n-    multiple_of\n\\ No newline at end of file\n+    multiple_of"}, {"filename": "keren/fix-doc/_sources/python-api/triton.rst.txt", "status": "renamed", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,5 +1,5 @@\n triton\n-========\n+======\n \n .. currentmodule:: triton\n \n@@ -10,4 +10,4 @@ triton\n     jit\n     autotune\n     heuristics\n-    Config\n\\ No newline at end of file\n+    Config"}, {"filename": "keren/fix-doc/_sources/python-api/triton.testing.rst.txt", "status": "renamed", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,5 +1,5 @@\n triton.testing\n-================\n+==============\n \n .. currentmodule:: triton.testing\n \n@@ -9,4 +9,4 @@ triton.testing\n \n     do_bench\n     Benchmark\n-    perf_report\n\\ No newline at end of file\n+    perf_report"}, {"filename": "keren/fix-doc/_static/basic.css", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/basic.css"}, {"filename": "keren/fix-doc/_static/binder_badge_logo.svg", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/binder_badge_logo.svg"}, {"filename": "keren/fix-doc/_static/broken_example.png", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/broken_example.png"}, {"filename": "keren/fix-doc/_static/css/badge_only.css", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/css/badge_only.css"}, {"filename": "keren/fix-doc/_static/css/fonts/Roboto-Slab-Bold.woff", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/css/fonts/Roboto-Slab-Bold.woff"}, {"filename": "keren/fix-doc/_static/css/fonts/Roboto-Slab-Bold.woff2", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/css/fonts/Roboto-Slab-Bold.woff2"}, {"filename": "keren/fix-doc/_static/css/fonts/Roboto-Slab-Regular.woff", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/css/fonts/Roboto-Slab-Regular.woff"}, {"filename": "keren/fix-doc/_static/css/fonts/Roboto-Slab-Regular.woff2", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/css/fonts/Roboto-Slab-Regular.woff2"}, {"filename": "keren/fix-doc/_static/css/fonts/fontawesome-webfont.eot", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/css/fonts/fontawesome-webfont.eot"}, {"filename": "keren/fix-doc/_static/css/fonts/fontawesome-webfont.svg", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/css/fonts/fontawesome-webfont.svg"}, {"filename": "keren/fix-doc/_static/css/fonts/fontawesome-webfont.ttf", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/css/fonts/fontawesome-webfont.ttf"}, {"filename": "keren/fix-doc/_static/css/fonts/fontawesome-webfont.woff", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/css/fonts/fontawesome-webfont.woff"}, {"filename": "keren/fix-doc/_static/css/fonts/fontawesome-webfont.woff2", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/css/fonts/fontawesome-webfont.woff2"}, {"filename": "keren/fix-doc/_static/css/fonts/lato-bold-italic.woff", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/css/fonts/lato-bold-italic.woff"}, {"filename": "keren/fix-doc/_static/css/fonts/lato-bold-italic.woff2", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/css/fonts/lato-bold-italic.woff2"}, {"filename": "keren/fix-doc/_static/css/fonts/lato-bold.woff", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/css/fonts/lato-bold.woff"}, {"filename": "keren/fix-doc/_static/css/fonts/lato-bold.woff2", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/css/fonts/lato-bold.woff2"}, {"filename": "keren/fix-doc/_static/css/fonts/lato-normal-italic.woff", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/css/fonts/lato-normal-italic.woff"}, {"filename": "keren/fix-doc/_static/css/fonts/lato-normal-italic.woff2", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/css/fonts/lato-normal-italic.woff2"}, {"filename": "keren/fix-doc/_static/css/fonts/lato-normal.woff", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/css/fonts/lato-normal.woff"}, {"filename": "keren/fix-doc/_static/css/fonts/lato-normal.woff2", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/css/fonts/lato-normal.woff2"}, {"filename": "keren/fix-doc/_static/css/theme.css", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/css/theme.css"}, {"filename": "keren/fix-doc/_static/doctools.js", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/doctools.js"}, {"filename": "keren/fix-doc/_static/documentation_options.js", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/documentation_options.js"}, {"filename": "keren/fix-doc/_static/file.png", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/file.png"}, {"filename": "keren/fix-doc/_static/fonts/Inconsolata-Bold.ttf", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/Inconsolata-Bold.ttf"}, {"filename": "keren/fix-doc/_static/fonts/Inconsolata-Regular.ttf", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/Inconsolata-Regular.ttf"}, {"filename": "keren/fix-doc/_static/fonts/Inconsolata.ttf", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/Inconsolata.ttf"}, {"filename": "keren/fix-doc/_static/fonts/Lato-Bold.ttf", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/Lato-Bold.ttf"}, {"filename": "keren/fix-doc/_static/fonts/Lato-Regular.ttf", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/Lato-Regular.ttf"}, {"filename": "keren/fix-doc/_static/fonts/Lato/lato-bold.eot", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/Lato/lato-bold.eot"}, {"filename": "keren/fix-doc/_static/fonts/Lato/lato-bold.ttf", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/Lato/lato-bold.ttf"}, {"filename": "keren/fix-doc/_static/fonts/Lato/lato-bold.woff", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/Lato/lato-bold.woff"}, {"filename": "keren/fix-doc/_static/fonts/Lato/lato-bold.woff2", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/Lato/lato-bold.woff2"}, {"filename": "keren/fix-doc/_static/fonts/Lato/lato-bolditalic.eot", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/Lato/lato-bolditalic.eot"}, {"filename": "keren/fix-doc/_static/fonts/Lato/lato-bolditalic.ttf", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/Lato/lato-bolditalic.ttf"}, {"filename": "keren/fix-doc/_static/fonts/Lato/lato-bolditalic.woff", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/Lato/lato-bolditalic.woff"}, {"filename": "keren/fix-doc/_static/fonts/Lato/lato-bolditalic.woff2", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/Lato/lato-bolditalic.woff2"}, {"filename": "keren/fix-doc/_static/fonts/Lato/lato-italic.eot", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/Lato/lato-italic.eot"}, {"filename": "keren/fix-doc/_static/fonts/Lato/lato-italic.ttf", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/Lato/lato-italic.ttf"}, {"filename": "keren/fix-doc/_static/fonts/Lato/lato-italic.woff", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/Lato/lato-italic.woff"}, {"filename": "keren/fix-doc/_static/fonts/Lato/lato-italic.woff2", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/Lato/lato-italic.woff2"}, {"filename": "keren/fix-doc/_static/fonts/Lato/lato-regular.eot", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/Lato/lato-regular.eot"}, {"filename": "keren/fix-doc/_static/fonts/Lato/lato-regular.ttf", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/Lato/lato-regular.ttf"}, {"filename": "keren/fix-doc/_static/fonts/Lato/lato-regular.woff", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/Lato/lato-regular.woff"}, {"filename": "keren/fix-doc/_static/fonts/Lato/lato-regular.woff2", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/Lato/lato-regular.woff2"}, {"filename": "keren/fix-doc/_static/fonts/RobotoSlab-Bold.ttf", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/RobotoSlab-Bold.ttf"}, {"filename": "keren/fix-doc/_static/fonts/RobotoSlab-Regular.ttf", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/RobotoSlab-Regular.ttf"}, {"filename": "keren/fix-doc/_static/fonts/RobotoSlab/roboto-slab-v7-bold.eot", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/RobotoSlab/roboto-slab-v7-bold.eot"}, {"filename": "keren/fix-doc/_static/fonts/RobotoSlab/roboto-slab-v7-bold.ttf", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/RobotoSlab/roboto-slab-v7-bold.ttf"}, {"filename": "keren/fix-doc/_static/fonts/RobotoSlab/roboto-slab-v7-bold.woff", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/RobotoSlab/roboto-slab-v7-bold.woff"}, {"filename": "keren/fix-doc/_static/fonts/RobotoSlab/roboto-slab-v7-bold.woff2", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/RobotoSlab/roboto-slab-v7-bold.woff2"}, {"filename": "keren/fix-doc/_static/fonts/RobotoSlab/roboto-slab-v7-regular.eot", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/RobotoSlab/roboto-slab-v7-regular.eot"}, {"filename": "keren/fix-doc/_static/fonts/RobotoSlab/roboto-slab-v7-regular.ttf", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/RobotoSlab/roboto-slab-v7-regular.ttf"}, {"filename": "keren/fix-doc/_static/fonts/RobotoSlab/roboto-slab-v7-regular.woff", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/RobotoSlab/roboto-slab-v7-regular.woff"}, {"filename": "keren/fix-doc/_static/fonts/RobotoSlab/roboto-slab-v7-regular.woff2", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/RobotoSlab/roboto-slab-v7-regular.woff2"}, {"filename": "keren/fix-doc/_static/fonts/fontawesome-webfont.eot", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/fontawesome-webfont.eot"}, {"filename": "keren/fix-doc/_static/fonts/fontawesome-webfont.svg", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/fontawesome-webfont.svg"}, {"filename": "keren/fix-doc/_static/fonts/fontawesome-webfont.ttf", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/fontawesome-webfont.ttf"}, {"filename": "keren/fix-doc/_static/fonts/fontawesome-webfont.woff", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/fontawesome-webfont.woff"}, {"filename": "keren/fix-doc/_static/fonts/fontawesome-webfont.woff2", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/fonts/fontawesome-webfont.woff2"}, {"filename": "keren/fix-doc/_static/gallery-binder.css", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/gallery-binder.css"}, {"filename": "keren/fix-doc/_static/gallery-dataframe.css", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/gallery-dataframe.css"}, {"filename": "keren/fix-doc/_static/gallery-rendered-html.css", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/gallery-rendered-html.css"}, {"filename": "keren/fix-doc/_static/gallery.css", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/gallery.css"}, {"filename": "keren/fix-doc/_static/jquery-3.5.1.js", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/jquery-3.5.1.js"}, {"filename": "keren/fix-doc/_static/jquery.js", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/jquery.js"}, {"filename": "keren/fix-doc/_static/js/badge_only.js", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/js/badge_only.js"}, {"filename": "keren/fix-doc/_static/js/html5shiv-printshiv.min.js", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/js/html5shiv-printshiv.min.js"}, {"filename": "keren/fix-doc/_static/js/html5shiv.min.js", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/js/html5shiv.min.js"}, {"filename": "keren/fix-doc/_static/js/modernizr.min.js", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/js/modernizr.min.js"}, {"filename": "keren/fix-doc/_static/js/theme.js", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/js/theme.js"}, {"filename": "keren/fix-doc/_static/language_data.js", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/language_data.js"}, {"filename": "keren/fix-doc/_static/minus.png", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/minus.png"}, {"filename": "keren/fix-doc/_static/no_image.png", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/no_image.png"}, {"filename": "keren/fix-doc/_static/plus.png", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/plus.png"}, {"filename": "keren/fix-doc/_static/pygments.css", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/pygments.css"}, {"filename": "keren/fix-doc/_static/searchtools.js", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/searchtools.js"}, {"filename": "keren/fix-doc/_static/underscore-1.13.1.js", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/underscore-1.13.1.js"}, {"filename": "keren/fix-doc/_static/underscore.js", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "master/_static/underscore.js"}, {"filename": "keren/fix-doc/genindex.html", "status": "renamed", "additions": 4, "deletions": 6, "changes": 10, "file_content_changes": "@@ -207,6 +207,8 @@ <h2 id=\"_\">_</h2>\n <h2 id=\"A\">A</h2>\n <table style=\"width: 100%\" class=\"indextable genindextable\"><tr>\n   <td style=\"width: 33%; vertical-align: top;\"><ul>\n+      <li><a href=\"python-api/generated/triton.language.abs.html#triton.language.abs\">abs() (in module triton.language)</a>\n+</li>\n       <li><a href=\"python-api/generated/triton.language.arange.html#triton.language.arange\">arange() (in module triton.language)</a>\n </li>\n       <li><a href=\"python-api/generated/triton.language.atomic_add.html#triton.language.atomic_add\">atomic_add() (in module triton.language)</a>\n@@ -427,17 +429,13 @@ <h2 id=\"Z\">Z</h2>\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: v1.1.2\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"genindex.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"../master/index.html\">master</a></dd>\n+            <dd><a href=\"genindex.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/getting-started/installation.html", "status": "renamed", "additions": 4, "deletions": 8, "changes": 12, "file_content_changes": "@@ -202,7 +202,7 @@ <h3>Python Package<a class=\"headerlink\" href=\"#python-package\" title=\"Permalink\n <p>You can install the Python package from source by running the following commands:</p>\n <div class=\"highlight-bash notranslate\"><div class=\"highlight\"><pre><span></span>git clone https://github.com/openai/triton.git<span class=\"p\">;</span>\n <span class=\"nb\">cd</span> triton/python<span class=\"p\">;</span>\n-pip install cmake<span class=\"p\">;</span> <span class=\"c1\"># build time dependency</span>\n+pip install cmake<span class=\"p\">;</span> <span class=\"c1\"># build-time dependency</span>\n pip install -e .\n </pre></div>\n </div>\n@@ -213,7 +213,7 @@ <h3>Python Package<a class=\"headerlink\" href=\"#python-package\" title=\"Permalink\n </pre></div>\n </div>\n <p>and the benchmarks</p>\n-<div class=\"highlight-bash notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nb\">cd</span> bench/\n+<div class=\"highlight-bash notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nb\">cd</span> bench\n python -m run --with-plots --result-dir /tmp/triton-bench\n </pre></div>\n </div>\n@@ -259,17 +259,13 @@ <h3>Python Package<a class=\"headerlink\" href=\"#python-package\" title=\"Permalink\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../v1.1.2/getting-started/installation.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"installation.html\">master</a></dd>\n+            <dd><a href=\"installation.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/getting-started/tutorials/01-vector-add.html", "status": "renamed", "additions": 50, "deletions": 55, "changes": 105, "file_content_changes": "@@ -105,8 +105,7 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-libdevice-function.html\">Libdevice function</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n </ul>\n </li>\n </ul>\n@@ -196,12 +195,11 @@\n </div>\n <div class=\"sphx-glr-example-title section\" id=\"vector-addition\">\n <span id=\"sphx-glr-getting-started-tutorials-01-vector-add-py\"></span><h1>Vector Addition<a class=\"headerlink\" href=\"#vector-addition\" title=\"Permalink to this headline\">\u00b6</a></h1>\n-<p>In this tutorial, you will write a simple vector addition using Triton and learn about:</p>\n-<ul class=\"simple\">\n-<li><p>The basic programming model of Triton</p></li>\n-<li><p>The <cite>triton.jit</cite> decorator, which is used to define Triton kernels.</p></li>\n-<li><p>The best practices for validating and benchmarking your custom ops against native reference implementations</p></li>\n-</ul>\n+<p>In this tutorial, you will write a simple vector addition using Triton.</p>\n+<p>In doing so, you will learn about:\n+- The basic programming model of Triton.\n+- The <cite>triton.jit</cite> decorator, which is used to define Triton kernels.\n+- The best practices for validating and benchmarking your custom ops against native reference implementations.</p>\n <div class=\"section\" id=\"compute-kernel\">\n <h2>Compute Kernel<a class=\"headerlink\" href=\"#compute-kernel\" title=\"Permalink to this headline\">\u00b6</a></h2>\n <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n@@ -212,48 +210,48 @@ <h2>Compute Kernel<a class=\"headerlink\" href=\"#compute-kernel\" title=\"Permalink\n \n <span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n <span class=\"k\">def</span> <span class=\"nf\">add_kernel</span><span class=\"p\">(</span>\n-    <span class=\"n\">x_ptr</span><span class=\"p\">,</span>  <span class=\"c1\"># *Pointer* to first input vector</span>\n-    <span class=\"n\">y_ptr</span><span class=\"p\">,</span>  <span class=\"c1\"># *Pointer* to second input vector</span>\n-    <span class=\"n\">output_ptr</span><span class=\"p\">,</span>  <span class=\"c1\"># *Pointer* to output vector</span>\n-    <span class=\"n\">n_elements</span><span class=\"p\">,</span>  <span class=\"c1\"># Size of the vector</span>\n-    <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>  <span class=\"c1\"># Number of elements each program should process</span>\n-                 <span class=\"c1\"># NOTE: `constexpr` so it can be used as a shape value</span>\n+    <span class=\"n\">x_ptr</span><span class=\"p\">,</span>  <span class=\"c1\"># *Pointer* to first input vector.</span>\n+    <span class=\"n\">y_ptr</span><span class=\"p\">,</span>  <span class=\"c1\"># *Pointer* to second input vector.</span>\n+    <span class=\"n\">output_ptr</span><span class=\"p\">,</span>  <span class=\"c1\"># *Pointer* to output vector.</span>\n+    <span class=\"n\">n_elements</span><span class=\"p\">,</span>  <span class=\"c1\"># Size of the vector.</span>\n+    <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>  <span class=\"c1\"># Number of elements each program should process.</span>\n+                 <span class=\"c1\"># NOTE: `constexpr` so it can be used as a shape value.</span>\n <span class=\"p\">):</span>\n-    <span class=\"c1\"># There are multiple &#39;program&#39;s processing different data. We identify which program</span>\n-    <span class=\"c1\"># we are here</span>\n-    <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>  <span class=\"c1\"># We use a 1D launch grid so axis is 0</span>\n+    <span class=\"c1\"># There are multiple &#39;programs&#39; processing different data. We identify which program</span>\n+    <span class=\"c1\"># we are here:</span>\n+    <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>  <span class=\"c1\"># We use a 1D launch grid so axis is 0.</span>\n     <span class=\"c1\"># This program will process inputs that are offset from the initial data.</span>\n-    <span class=\"c1\"># for instance, if you had a vector of length 256 and block_size of 64, the programs</span>\n+    <span class=\"c1\"># For instance, if you had a vector of length 256 and block_size of 64, the programs</span>\n     <span class=\"c1\"># would each access the elements [0:64, 64:128, 128:192, 192:256].</span>\n-    <span class=\"c1\"># Note that offsets is a list of pointers</span>\n+    <span class=\"c1\"># Note that offsets is a list of pointers:</span>\n     <span class=\"n\">block_start</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE</span>\n     <span class=\"n\">offsets</span> <span class=\"o\">=</span> <span class=\"n\">block_start</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># Create a mask to guard memory operations against out-of-bounds accesses</span>\n+    <span class=\"c1\"># Create a mask to guard memory operations against out-of-bounds accesses.</span>\n     <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">offsets</span> <span class=\"o\">&lt;</span> <span class=\"n\">n_elements</span>\n     <span class=\"c1\"># Load x and y from DRAM, masking out any extra elements in case the input is not a</span>\n-    <span class=\"c1\"># multiple of the block size</span>\n+    <span class=\"c1\"># multiple of the block size.</span>\n     <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">x_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n     <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">y_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n     <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"n\">y</span>\n-    <span class=\"c1\"># Write x + y back to DRAM</span>\n+    <span class=\"c1\"># Write x + y back to DRAM.</span>\n     <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">output_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n </pre></div>\n </div>\n <p>Let\u2019s also declare a helper function to (1) allocate the <cite>z</cite> tensor\n-and (2) enqueue the above kernel with appropriate grid/block sizes.</p>\n+and (2) enqueue the above kernel with appropriate grid/block sizes:</p>\n <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">add</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">):</span>\n-    <span class=\"c1\"># We need to preallocate the output</span>\n+    <span class=\"c1\"># We need to preallocate the output.</span>\n     <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n     <span class=\"k\">assert</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">is_cuda</span> <span class=\"ow\">and</span> <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">is_cuda</span> <span class=\"ow\">and</span> <span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">is_cuda</span>\n     <span class=\"n\">n_elements</span> <span class=\"o\">=</span> <span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">numel</span><span class=\"p\">()</span>\n     <span class=\"c1\"># The SPMD launch grid denotes the number of kernel instances that run in parallel.</span>\n-    <span class=\"c1\"># It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -&gt; Tuple[int]</span>\n-    <span class=\"c1\"># In this case, we use a 1D grid where the size is the number of blocks</span>\n+    <span class=\"c1\"># It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -&gt; Tuple[int].</span>\n+    <span class=\"c1\"># In this case, we use a 1D grid where the size is the number of blocks:</span>\n     <span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">meta</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">meta</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">]),)</span>\n     <span class=\"c1\"># NOTE:</span>\n-    <span class=\"c1\">#  - each torch.tensor object is implicitly converted into a pointer to its first element.</span>\n-    <span class=\"c1\">#  - `triton.jit`&#39;ed functions can be index with a launch grid to obtain a callable GPU kernel</span>\n-    <span class=\"c1\">#  - don&#39;t forget to pass meta-parameters as keywords arguments</span>\n+    <span class=\"c1\">#  - Each torch.tensor object is implicitly converted into a pointer to its first element.</span>\n+    <span class=\"c1\">#  - `triton.jit`&#39;ed functions can be indexed with a launch grid to obtain a callable GPU kernel.</span>\n+    <span class=\"c1\">#  - Don&#39;t forget to pass meta-parameters as keywords arguments.</span>\n     <span class=\"n\">add_kernel</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"o\">=</span><span class=\"mi\">1024</span><span class=\"p\">)</span>\n     <span class=\"c1\"># We return a handle to z but, since `torch.cuda.synchronize()` hasn&#39;t been called, the kernel is still</span>\n     <span class=\"c1\"># running asynchronously at this point.</span>\n@@ -286,37 +284,38 @@ <h2>Compute Kernel<a class=\"headerlink\" href=\"#compute-kernel\" title=\"Permalink\n <div class=\"section\" id=\"benchmark\">\n <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this headline\">\u00b6</a></h2>\n <p>We can now benchmark our custom op on vectors of increasing sizes to get a sense of how it does relative to PyTorch.\n-To make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of your custom ops\n+To make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of our custom ops.\n for different problem sizes.</p>\n <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">perf_report</span><span class=\"p\">(</span>\n     <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">Benchmark</span><span class=\"p\">(</span>\n-        <span class=\"n\">x_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;size&#39;</span><span class=\"p\">],</span>  <span class=\"c1\"># argument names to use as an x-axis for the plot</span>\n+        <span class=\"n\">x_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;size&#39;</span><span class=\"p\">],</span>  <span class=\"c1\"># Argument names to use as an x-axis for the plot.</span>\n         <span class=\"n\">x_vals</span><span class=\"o\">=</span><span class=\"p\">[</span>\n             <span class=\"mi\">2</span> <span class=\"o\">**</span> <span class=\"n\">i</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">12</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n-        <span class=\"p\">],</span>  <span class=\"c1\"># different possible values for `x_name`</span>\n-        <span class=\"n\">x_log</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>  <span class=\"c1\"># x axis is logarithmic</span>\n-        <span class=\"n\">line_arg</span><span class=\"o\">=</span><span class=\"s1\">&#39;provider&#39;</span><span class=\"p\">,</span>  <span class=\"c1\"># argument name whose value corresponds to a different line in the plot</span>\n-        <span class=\"n\">line_vals</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;torch&#39;</span><span class=\"p\">],</span>  <span class=\"c1\"># possible values for `line_arg`</span>\n-        <span class=\"n\">line_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;Triton&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;Torch&#39;</span><span class=\"p\">],</span>  <span class=\"c1\"># label name for the lines</span>\n-        <span class=\"n\">styles</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s1\">&#39;blue&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;green&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">)],</span>  <span class=\"c1\"># line styles</span>\n-        <span class=\"n\">ylabel</span><span class=\"o\">=</span><span class=\"s1\">&#39;GB/s&#39;</span><span class=\"p\">,</span>  <span class=\"c1\"># label name for the y-axis</span>\n-        <span class=\"n\">plot_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;vector-add-performance&#39;</span><span class=\"p\">,</span>  <span class=\"c1\"># name for the plot. Used also as a file name for saving the plot.</span>\n-        <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{},</span>  <span class=\"c1\"># values for function arguments not in `x_names` and `y_name`</span>\n+        <span class=\"p\">],</span>  <span class=\"c1\"># Different possible values for `x_name`.</span>\n+        <span class=\"n\">x_log</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>  <span class=\"c1\"># x axis is logarithmic.</span>\n+        <span class=\"n\">line_arg</span><span class=\"o\">=</span><span class=\"s1\">&#39;provider&#39;</span><span class=\"p\">,</span>  <span class=\"c1\"># Argument name whose value corresponds to a different line in the plot.</span>\n+        <span class=\"n\">line_vals</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;torch&#39;</span><span class=\"p\">],</span>  <span class=\"c1\"># Possible values for `line_arg`.</span>\n+        <span class=\"n\">line_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;Triton&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;Torch&#39;</span><span class=\"p\">],</span>  <span class=\"c1\"># Label name for the lines.</span>\n+        <span class=\"n\">styles</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s1\">&#39;blue&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;green&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">)],</span>  <span class=\"c1\"># Line styles.</span>\n+        <span class=\"n\">ylabel</span><span class=\"o\">=</span><span class=\"s1\">&#39;GB/s&#39;</span><span class=\"p\">,</span>  <span class=\"c1\"># Label name for the y-axis.</span>\n+        <span class=\"n\">plot_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;vector-add-performance&#39;</span><span class=\"p\">,</span>  <span class=\"c1\"># Name for the plot. Used also as a file name for saving the plot.</span>\n+        <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{},</span>  <span class=\"c1\"># Values for function arguments not in `x_names` and `y_name`.</span>\n     <span class=\"p\">)</span>\n <span class=\"p\">)</span>\n <span class=\"k\">def</span> <span class=\"nf\">benchmark</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">provider</span><span class=\"p\">):</span>\n     <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n     <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"n\">quantiles</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">]</span>\n     <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;torch&#39;</span><span class=\"p\">:</span>\n-        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n+        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">)</span>\n     <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">:</span>\n-        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">))</span>\n+        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">),</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">)</span>\n     <span class=\"n\">gbps</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">ms</span><span class=\"p\">:</span> <span class=\"mi\">12</span> <span class=\"o\">*</span> <span class=\"n\">size</span> <span class=\"o\">/</span> <span class=\"n\">ms</span> <span class=\"o\">*</span> <span class=\"mf\">1e-6</span>\n     <span class=\"k\">return</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">ms</span><span class=\"p\">),</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">max_ms</span><span class=\"p\">),</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">min_ms</span><span class=\"p\">)</span>\n </pre></div>\n </div>\n <p>We can now run the decorated function above. Pass <cite>print_data=True</cite> to see the performance number, <cite>show_plots=True</cite> to plot them, and/or\n-<a href=\"#id1\"><span class=\"problematic\" id=\"id2\">`</span></a>save_path=\u2019/path/to/results/\u2019 to save them to disk along with raw CSV data</p>\n+<a href=\"#id1\"><span class=\"problematic\" id=\"id2\">`</span></a>save_path=\u2019/path/to/results/\u2019 to save them to disk along with raw CSV data:</p>\n <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">benchmark</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">print_data</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">show_plots</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n </pre></div>\n </div>\n@@ -325,24 +324,24 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n <div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>vector-add-performance:\n            size      Triton       Torch\n 0        4096.0    9.600000    9.600000\n-1        8192.0   19.200000   19.200000\n+1        8192.0   15.999999   15.999999\n 2       16384.0   38.400001   38.400001\n 3       32768.0   76.800002   76.800002\n 4       65536.0  127.999995  127.999995\n 5      131072.0  219.428568  219.428568\n-6      262144.0  384.000001  384.000001\n+6      262144.0  341.333321  341.333321\n 7      524288.0  472.615390  472.615390\n 8     1048576.0  614.400016  614.400016\n-9     2097152.0  722.823517  702.171410\n+9     2097152.0  722.823517  722.823517\n 10    4194304.0  780.190482  780.190482\n 11    8388608.0  812.429770  812.429770\n 12   16777216.0  833.084721  833.084721\n-13   33554432.0  842.004273  842.004273\n+13   33554432.0  842.004273  843.811163\n 14   67108864.0  847.448255  848.362445\n-15  134217728.0  849.737435  850.656574\n+15  134217728.0  850.196756  851.116890\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  46.359 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  29.843 seconds)</p>\n <div class=\"sphx-glr-footer class sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-01-vector-add-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/62d97d49a32414049819dd8bb8378080/01-vector-add.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">01-vector-add.py</span></code></a></p>\n@@ -393,17 +392,13 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"01-vector-add.html\">master</a></dd>\n+            <dd><a href=\"01-vector-add.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/getting-started/tutorials/02-fused-softmax.html", "status": "renamed", "additions": 31, "deletions": 37, "changes": 68, "file_content_changes": "@@ -108,8 +108,7 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-libdevice-function.html\">Libdevice function</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n </ul>\n </li>\n </ul>\n@@ -201,12 +200,10 @@\n <span id=\"sphx-glr-getting-started-tutorials-02-fused-softmax-py\"></span><h1>Fused Softmax<a class=\"headerlink\" href=\"#fused-softmax\" title=\"Permalink to this headline\">\u00b6</a></h1>\n <p>In this tutorial, you will write a fused softmax operation that is significantly faster\n than PyTorch\u2019s native op for a particular class of matrices: those whose rows can fit in\n-the GPU\u2019s SRAM.\n-You will learn about:</p>\n-<ul class=\"simple\">\n-<li><p>The benefits of kernel fusion for bandwidth-bound operations.</p></li>\n-<li><p>Reduction operators in Triton.</p></li>\n-</ul>\n+the GPU\u2019s SRAM.</p>\n+<p>In doing so, you will learn about:\n+- The benefits of kernel fusion for bandwidth-bound operations.\n+- Reduction operators in Triton.</p>\n <div class=\"section\" id=\"motivations\">\n <h2>Motivations<a class=\"headerlink\" href=\"#motivations\" title=\"Permalink to this headline\">\u00b6</a></h2>\n <p>Custom GPU kernels for elementwise additions are educationally valuable but won\u2019t get you very far in practice.\n@@ -250,8 +247,8 @@ <h2>Motivations<a class=\"headerlink\" href=\"#motivations\" title=\"Permalink to thi\n <div class=\"section\" id=\"compute-kernel\">\n <h2>Compute Kernel<a class=\"headerlink\" href=\"#compute-kernel\" title=\"Permalink to this headline\">\u00b6</a></h2>\n <p>Our softmax kernel works as follows: each program loads a row of the input matrix X,\n-normalizes it and writes back the result to the output Y.\n-Note that one important limitation of Triton is that each block must have a\n+normalizes it and writes back the result to the output Y.</p>\n+<p>Note that one important limitation of Triton is that each block must have a\n power-of-two number of elements, so we need to internally \u201cpad\u201d each row and guard the\n memory operations properly if we want to handle any possible input shapes:</p>\n <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n@@ -269,9 +266,9 @@ <h2>Compute Kernel<a class=\"headerlink\" href=\"#compute-kernel\" title=\"Permalink\n     <span class=\"n\">input_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">row_start_ptr</span> <span class=\"o\">+</span> <span class=\"n\">col_offsets</span>\n     <span class=\"c1\"># Load the row into SRAM, using a mask since BLOCK_SIZE may be &gt; than n_cols</span>\n     <span class=\"n\">row</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">input_ptrs</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">col_offsets</span> <span class=\"o\">&lt;</span> <span class=\"n\">n_cols</span><span class=\"p\">,</span> <span class=\"n\">other</span><span class=\"o\">=-</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s1\">&#39;inf&#39;</span><span class=\"p\">))</span>\n-    <span class=\"c1\"># Substract maximum for numerical stability</span>\n+    <span class=\"c1\"># Subtract maximum for numerical stability</span>\n     <span class=\"n\">row_minus_max</span> <span class=\"o\">=</span> <span class=\"n\">row</span> <span class=\"o\">-</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">row</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># Note that exponentials in Triton are fast but approximate (i.e., think __expf in CUDA)</span>\n+    <span class=\"c1\"># Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)</span>\n     <span class=\"n\">numerator</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">row_minus_max</span><span class=\"p\">)</span>\n     <span class=\"n\">denominator</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">numerator</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n     <span class=\"n\">softmax_output</span> <span class=\"o\">=</span> <span class=\"n\">numerator</span> <span class=\"o\">/</span> <span class=\"n\">denominator</span>\n@@ -354,12 +351,13 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n <span class=\"p\">)</span>\n <span class=\"k\">def</span> <span class=\"nf\">benchmark</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">provider</span><span class=\"p\">):</span>\n     <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"n\">quantiles</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">]</span>\n     <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;torch-native&#39;</span><span class=\"p\">:</span>\n-        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n+        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">)</span>\n     <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">:</span>\n-        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n+        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">),</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">)</span>\n     <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;torch-jit&#39;</span><span class=\"p\">:</span>\n-        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">naive_softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n+        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">naive_softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">),</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">)</span>\n     <span class=\"n\">gbps</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">ms</span><span class=\"p\">:</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">nelement</span><span class=\"p\">()</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">element_size</span><span class=\"p\">()</span> <span class=\"o\">*</span> <span class=\"mf\">1e-9</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">ms</span> <span class=\"o\">*</span> <span class=\"mf\">1e-3</span><span class=\"p\">)</span>\n     <span class=\"k\">return</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">ms</span><span class=\"p\">),</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">max_ms</span><span class=\"p\">),</span> <span class=\"n\">gbps</span><span class=\"p\">(</span><span class=\"n\">min_ms</span><span class=\"p\">)</span>\n \n@@ -371,30 +369,30 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n <p class=\"sphx-glr-script-out\">Out:</p>\n <div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>softmax-performance:\n           N      Triton  Torch (native)  Torch (jit)\n-0     256.0  512.000001      546.133347   186.181817\n-1     384.0  614.400016      585.142862   153.600004\n-2     512.0  655.360017      585.142849   154.566038\n-3     640.0  706.206879      640.000002   160.000000\n-4     768.0  722.823517      664.216187   162.754967\n+0     256.0  546.133347      546.133347   204.800005\n+1     384.0  614.400016      646.736871   236.307695\n+2     512.0  655.360017      630.153853   248.242422\n+3     640.0  706.206879      620.606056   259.240514\n+4     768.0  722.823517      646.736871   261.446801\n ..      ...         ...             ...          ...\n-93  12160.0  812.359066      406.179533   198.834951\n-94  12288.0  812.429770      415.222812   199.197579\n-95  12416.0  812.498981      412.149375   198.655991\n-96  12544.0  810.925276      412.546756   199.012395\n-97  12672.0  811.007961      412.097543   199.069228\n+93  12160.0  815.765209      409.599993   324.537101\n+94  12288.0  815.800825      416.984084   324.703544\n+95  12416.0  815.835709      414.730681   324.071772\n+96  12544.0  814.214963      415.536223   324.501215\n+97  12672.0  814.265046      415.050158   324.143890\n \n [98 rows x 4 columns]\n </pre></div>\n </div>\n-<p>In the above plot, we can see that:</p>\n-<blockquote>\n-<div><ul class=\"simple\">\n+<dl class=\"simple\">\n+<dt>In the above plot, we can see that:</dt><dd><ul class=\"simple\">\n <li><p>Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.</p></li>\n <li><p>Triton is noticeably faster than <code class=\"code docutils literal notranslate\"><span class=\"pre\">torch.softmax</span></code> \u2013 in addition to being <strong>easier to read, understand and maintain</strong>.\n-Note however that the PyTorch <cite>softmax</cite> operation is more general and will works on tensors of any shape.</p></li>\n+Note however that the PyTorch <cite>softmax</cite> operation is more general and will work on tensors of any shape.</p></li>\n </ul>\n-</div></blockquote>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 3 minutes  30.997 seconds)</p>\n+</dd>\n+</dl>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  19.698 seconds)</p>\n <div class=\"sphx-glr-footer class sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-02-fused-softmax-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d91442ac2982c4e0cc3ab0f43534afbc/02-fused-softmax.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">02-fused-softmax.py</span></code></a></p>\n@@ -445,17 +443,13 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"02-fused-softmax.html\">master</a></dd>\n+            <dd><a href=\"02-fused-softmax.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/getting-started/tutorials/03-matrix-multiplication.html", "status": "renamed", "additions": 65, "deletions": 84, "changes": 149, "file_content_changes": "@@ -101,7 +101,7 @@\n <li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Matrix Multiplication</a><ul>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#motivations\">Motivations</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#compute-kernel\">Compute Kernel</a><ul>\n-<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#pointer-arithmetics\">Pointer Arithmetics</a></li>\n+<li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#pointer-arithmetic\">Pointer Arithmetic</a></li>\n <li class=\"toctree-l4\"><a class=\"reference internal\" href=\"#l2-cache-optimizations\">L2 Cache Optimizations</a></li>\n </ul>\n </li>\n@@ -115,8 +115,7 @@\n </li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-libdevice-function.html\">Libdevice function</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n </ul>\n </li>\n </ul>\n@@ -207,21 +206,19 @@\n <div class=\"sphx-glr-example-title section\" id=\"matrix-multiplication\">\n <span id=\"sphx-glr-getting-started-tutorials-03-matrix-multiplication-py\"></span><h1>Matrix Multiplication<a class=\"headerlink\" href=\"#matrix-multiplication\" title=\"Permalink to this headline\">\u00b6</a></h1>\n <p>In this tutorial, you will write a 25-lines high-performance FP16 matrix multiplication\n-kernel that achieves performance on par with cuBLAS.\n-You will specifically learn about:</p>\n-<ul class=\"simple\">\n-<li><p>Block-level matrix multiplications</p></li>\n-<li><p>Multi-dimensional pointer arithmetic</p></li>\n-<li><p>Program re-ordering for improved L2 cache hit rate</p></li>\n-<li><p>Automatic performance tuning</p></li>\n-</ul>\n+kernel that achieves performance on par with cuBLAS.</p>\n+<p>In doing so, you will learn about:\n+- Block-level matrix multiplications\n+- Multi-dimensional pointer arithmetic\n+- Program re-ordering for improved L2 cache hit rate\n+- Automatic performance tuning</p>\n <div class=\"section\" id=\"motivations\">\n <h2>Motivations<a class=\"headerlink\" href=\"#motivations\" title=\"Permalink to this headline\">\u00b6</a></h2>\n <p>Matrix multiplications are a key building block of most modern high-performance computing systems.\n They are notoriously hard to optimize, hence their implementation is generally done by\n hardware vendors themselves as part of so-called \u201ckernel libraries\u201d (e.g., cuBLAS).\n Unfortunately, these libraries are often proprietary and cannot be easily customized\n-to accomodate the needs of modern deep learning workloads (e.g., fused activation functions).\n+to accommodate the needs of modern deep learning workloads (e.g., fused activation functions).\n In this tutorial, you will learn how to implement efficient matrix multiplications by\n yourself with Triton, in a way that is easy to customize and extend.</p>\n <p>Roughly speaking, the kernel that we will write will implement the following blocked\n@@ -247,9 +244,9 @@ <h2>Compute Kernel<a class=\"headerlink\" href=\"#compute-kernel\" title=\"Permalink\n <p>The above algorithm is, actually, fairly straightforward to implement in Triton.\n The main difficulty comes from the computation of the memory locations at which blocks\n of <code class=\"code docutils literal notranslate\"><span class=\"pre\">A</span></code> and <code class=\"code docutils literal notranslate\"><span class=\"pre\">B</span></code> must be read in the inner loop. For that, we need\n-multi-dimensional pointer arithmetics.</p>\n-<div class=\"section\" id=\"pointer-arithmetics\">\n-<h3>Pointer Arithmetics<a class=\"headerlink\" href=\"#pointer-arithmetics\" title=\"Permalink to this headline\">\u00b6</a></h3>\n+multi-dimensional pointer arithmetic.</p>\n+<div class=\"section\" id=\"pointer-arithmetic\">\n+<h3>Pointer Arithmetic<a class=\"headerlink\" href=\"#pointer-arithmetic\" title=\"Permalink to this headline\">\u00b6</a></h3>\n <p>For a row-major 2D tensor <code class=\"code docutils literal notranslate\"><span class=\"pre\">X</span></code>, the memory location of <code class=\"code docutils literal notranslate\"><span class=\"pre\">X[i,</span> <span class=\"pre\">j]</span></code> is given b\n y <code class=\"code docutils literal notranslate\"><span class=\"pre\">&amp;X[i,</span> <span class=\"pre\">j]</span> <span class=\"pre\">=</span> <span class=\"pre\">X</span> <span class=\"pre\">+</span> <span class=\"pre\">i*stride_xi</span> <span class=\"pre\">+</span> <span class=\"pre\">j*stride_xj</span></code>.\n Therefore, blocks of pointers for <code class=\"code docutils literal notranslate\"><span class=\"pre\">A[m</span> <span class=\"pre\">:</span> <span class=\"pre\">m+BLOCK_SIZE_M,</span> <span class=\"pre\">k:k+BLOCK_SIZE_K]</span></code> and\n@@ -272,8 +269,8 @@ <h3>Pointer Arithmetics<a class=\"headerlink\" href=\"#pointer-arithmetics\" title=\"\n </div></blockquote>\n <p>And then updated in the inner loop as follows:</p>\n <blockquote>\n-<div><div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">pa</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_SIZE_K</span> <span class=\"o\">*</span> <span class=\"n\">stride_ak</span><span class=\"p\">;</span>\n-<span class=\"n\">pb</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_SIZE_K</span> <span class=\"o\">*</span> <span class=\"n\">stride_bk</span><span class=\"p\">;</span>\n+<div><div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">a_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_SIZE_K</span> <span class=\"o\">*</span> <span class=\"n\">stride_ak</span><span class=\"p\">;</span>\n+<span class=\"n\">b_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_SIZE_K</span> <span class=\"o\">*</span> <span class=\"n\">stride_bk</span><span class=\"p\">;</span>\n </pre></div>\n </div>\n </div></blockquote>\n@@ -350,9 +347,7 @@ <h2>Final Result<a class=\"headerlink\" href=\"#final-result\" title=\"Permalink to t\n \n <span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">autotune</span><span class=\"p\">(</span>\n     <span class=\"n\">configs</span><span class=\"o\">=</span><span class=\"p\">[</span>\n-        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">),</span>\n-        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">),</span>\n-        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n+        <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n         <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n         <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n         <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">({</span><span class=\"s1\">&#39;BLOCK_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_N&#39;</span><span class=\"p\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BLOCK_SIZE_K&#39;</span><span class=\"p\">:</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"s1\">&#39;GROUP_SIZE_M&#39;</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">},</span> <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n@@ -390,6 +385,7 @@ <h2>Final Result<a class=\"headerlink\" href=\"#final-result\" title=\"Permalink to t\n     <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n     <span class=\"n\">num_pid_m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">)</span>\n     <span class=\"n\">num_pid_n</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)</span>\n+    <span class=\"n\">num_pid_k</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">)</span>\n     <span class=\"n\">num_pid_in_group</span> <span class=\"o\">=</span> <span class=\"n\">GROUP_SIZE_M</span> <span class=\"o\">*</span> <span class=\"n\">num_pid_n</span>\n     <span class=\"n\">group_id</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">//</span> <span class=\"n\">num_pid_in_group</span>\n     <span class=\"n\">first_pid_m</span> <span class=\"o\">=</span> <span class=\"n\">group_id</span> <span class=\"o\">*</span> <span class=\"n\">GROUP_SIZE_M</span>\n@@ -403,7 +399,7 @@ <h2>Final Result<a class=\"headerlink\" href=\"#final-result\" title=\"Permalink to t\n     <span class=\"c1\"># and accumulate</span>\n     <span class=\"c1\"># a_ptrs is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers</span>\n     <span class=\"c1\"># b_ptrs is a block of [BLOCK_SIZE_K, BLOCK_SIZE_n] pointers</span>\n-    <span class=\"c1\"># see above `Pointer Arithmetics` section for details</span>\n+    <span class=\"c1\"># see above `Pointer Arithmetic` section for details</span>\n     <span class=\"n\">offs_am</span> <span class=\"o\">=</span> <span class=\"n\">pid_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">)</span>\n     <span class=\"n\">offs_bn</span> <span class=\"o\">=</span> <span class=\"n\">pid_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE_N</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">)</span>\n     <span class=\"n\">offs_k</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">)</span>\n@@ -416,7 +412,7 @@ <h2>Final Result<a class=\"headerlink\" href=\"#final-result\" title=\"Permalink to t\n     <span class=\"c1\"># of fp32 values for higher accuracy.</span>\n     <span class=\"c1\"># `accumulator` will be converted back to fp16 after the loop</span>\n     <span class=\"n\">accumulator</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">BLOCK_SIZE_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_N</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n-    <span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE_K</span><span class=\"p\">):</span>\n+    <span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">num_pid_k</span><span class=\"p\">):</span>\n         <span class=\"c1\"># Note that for simplicity, we don&#39;t apply a mask here.</span>\n         <span class=\"c1\"># This means that if K is not a multiple of BLOCK_SIZE_K,</span>\n         <span class=\"c1\"># this will access out-of-bounds memory and produce an</span>\n@@ -430,8 +426,8 @@ <h2>Final Result<a class=\"headerlink\" href=\"#final-result\" title=\"Permalink to t\n         <span class=\"n\">b_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_SIZE_K</span> <span class=\"o\">*</span> <span class=\"n\">stride_bk</span>\n     <span class=\"c1\"># you can fuse arbitrary activation functions here</span>\n     <span class=\"c1\"># while the accumulator is still in FP32!</span>\n-    <span class=\"k\">if</span> <span class=\"n\">ACTIVATION</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;leaky_relu&quot;</span><span class=\"p\">:</span>\n-        <span class=\"n\">accumulator</span> <span class=\"o\">=</span> <span class=\"n\">leaky_relu</span><span class=\"p\">(</span><span class=\"n\">accumulator</span><span class=\"p\">)</span>\n+    <span class=\"k\">if</span> <span class=\"n\">ACTIVATION</span><span class=\"p\">:</span>\n+        <span class=\"n\">accumulator</span> <span class=\"o\">=</span> <span class=\"n\">ACTIVATION</span><span class=\"p\">(</span><span class=\"n\">accumulator</span><span class=\"p\">)</span>\n     <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">accumulator</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n \n     <span class=\"c1\"># -----------------------------------------------------------</span>\n@@ -446,13 +442,12 @@ <h2>Final Result<a class=\"headerlink\" href=\"#final-result\" title=\"Permalink to t\n <span class=\"c1\"># we can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`</span>\n <span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n <span class=\"k\">def</span> <span class=\"nf\">leaky_relu</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n-    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n     <span class=\"k\">return</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">&gt;=</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"mf\">0.01</span> <span class=\"o\">*</span> <span class=\"n\">x</span><span class=\"p\">)</span>\n </pre></div>\n </div>\n <p>We can now create a convenience wrapper function that only takes two input tensors\n and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel</p>\n-<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s2\">&quot;&quot;</span><span class=\"p\">):</span>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">):</span>\n     <span class=\"c1\"># checks constraints</span>\n     <span class=\"k\">assert</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"s2\">&quot;incompatible dimensions&quot;</span>\n     <span class=\"k\">assert</span> <span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">is_contiguous</span><span class=\"p\">(),</span> <span class=\"s2\">&quot;matrix A must be contiguous&quot;</span>\n@@ -486,11 +481,11 @@ <h2>Unit Test<a class=\"headerlink\" href=\"#unit-test\" title=\"Permalink to this he\n <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">512</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n-<span class=\"n\">triton_output</span> <span class=\"o\">=</span> <span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n+<span class=\"n\">triton_output</span> <span class=\"o\">=</span> <span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">)</span>\n <span class=\"n\">torch_output</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;triton_output=</span><span class=\"si\">{</span><span class=\"n\">triton_output</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;torch_output=</span><span class=\"si\">{</span><span class=\"n\">torch_output</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n-<span class=\"k\">if</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">triton_output</span><span class=\"p\">,</span> <span class=\"n\">torch_output</span><span class=\"p\">):</span>\n+<span class=\"k\">if</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">triton_output</span><span class=\"p\">,</span> <span class=\"n\">torch_output</span><span class=\"p\">,</span> <span class=\"n\">atol</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">,</span> <span class=\"n\">rtol</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">):</span>\n     <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;\u2705 Triton and Torch match&quot;</span><span class=\"p\">)</span>\n <span class=\"k\">else</span><span class=\"p\">:</span>\n     <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;\u274c Triton and Torch differ&quot;</span><span class=\"p\">)</span>\n@@ -513,7 +508,7 @@ <h2>Unit Test<a class=\"headerlink\" href=\"#unit-test\" title=\"Permalink to this he\n         [ 25.5000,  24.3438,  -8.4609,  ..., -18.9375,  32.5312, -29.9219],\n         [ -5.3477,   4.9805,  11.8828,  ...,   5.5859,   6.4023, -17.3125]],\n        device=&#39;cuda:0&#39;, dtype=torch.float16)\n-\u2705 Triton and Torch match\n+\u274c Triton and Torch differ\n </pre></div>\n </div>\n </div>\n@@ -530,11 +525,11 @@ <h3>Square Matrix Performance<a class=\"headerlink\" href=\"#square-matrix-performa\n         <span class=\"p\">],</span>  <span class=\"c1\"># different possible values for `x_name`</span>\n         <span class=\"n\">line_arg</span><span class=\"o\">=</span><span class=\"s1\">&#39;provider&#39;</span><span class=\"p\">,</span>  <span class=\"c1\"># argument name whose value corresponds to a different line in the plot</span>\n         <span class=\"c1\"># possible values for `line_arg``</span>\n-        <span class=\"n\">line_vals</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;cublas&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;cublas + relu&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;triton + relu&#39;</span><span class=\"p\">],</span>\n+        <span class=\"n\">line_vals</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;cublas&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">],</span>\n         <span class=\"c1\"># label name for the lines</span>\n-        <span class=\"n\">line_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">&quot;cuBLAS&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;cuBLAS (+ torch.nn.LeakyReLU)&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;Triton&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;Triton (+ LeakyReLU)&quot;</span><span class=\"p\">],</span>\n+        <span class=\"n\">line_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">&quot;cuBLAS&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;Triton&quot;</span><span class=\"p\">],</span>\n         <span class=\"c1\"># line styles</span>\n-        <span class=\"n\">styles</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s1\">&#39;green&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;green&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;--&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;blue&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;blue&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;--&#39;</span><span class=\"p\">)],</span>\n+        <span class=\"n\">styles</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s1\">&#39;green&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;blue&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">)],</span>\n         <span class=\"n\">ylabel</span><span class=\"o\">=</span><span class=\"s2\">&quot;TFLOPS&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># label name for the y-axis</span>\n         <span class=\"n\">plot_name</span><span class=\"o\">=</span><span class=\"s2\">&quot;matmul-performance&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># name for the plot. Used also as a file name for saving the plot.</span>\n         <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{},</span>\n@@ -543,19 +538,11 @@ <h3>Square Matrix Performance<a class=\"headerlink\" href=\"#square-matrix-performa\n <span class=\"k\">def</span> <span class=\"nf\">benchmark</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">provider</span><span class=\"p\">):</span>\n     <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n     <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n+    <span class=\"n\">quantiles</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">]</span>\n     <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;cublas&#39;</span><span class=\"p\">:</span>\n-        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">))</span>\n+        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">),</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">)</span>\n     <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">:</span>\n-        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">))</span>\n-    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;cublas + relu&#39;</span><span class=\"p\">:</span>\n-        <span class=\"n\">torch_relu</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(</span><span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n-        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span>\n-            <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">torch_relu</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">))</span>\n-        <span class=\"p\">)</span>\n-    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;triton + relu&#39;</span><span class=\"p\">:</span>\n-        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span>\n-            <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s2\">&quot;leaky_relu&quot;</span><span class=\"p\">)</span>\n-        <span class=\"p\">)</span>\n+        <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">min_ms</span><span class=\"p\">,</span> <span class=\"n\">max_ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">),</span> <span class=\"n\">quantiles</span><span class=\"o\">=</span><span class=\"n\">quantiles</span><span class=\"p\">)</span>\n     <span class=\"n\">perf</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">ms</span><span class=\"p\">:</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">M</span> <span class=\"o\">*</span> <span class=\"n\">N</span> <span class=\"o\">*</span> <span class=\"n\">K</span> <span class=\"o\">*</span> <span class=\"mf\">1e-12</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">ms</span> <span class=\"o\">*</span> <span class=\"mf\">1e-3</span><span class=\"p\">)</span>\n     <span class=\"k\">return</span> <span class=\"n\">perf</span><span class=\"p\">(</span><span class=\"n\">ms</span><span class=\"p\">),</span> <span class=\"n\">perf</span><span class=\"p\">(</span><span class=\"n\">max_ms</span><span class=\"p\">),</span> <span class=\"n\">perf</span><span class=\"p\">(</span><span class=\"n\">min_ms</span><span class=\"p\">)</span>\n \n@@ -566,43 +553,41 @@ <h3>Square Matrix Performance<a class=\"headerlink\" href=\"#square-matrix-performa\n <img alt=\"03 matrix multiplication\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_03-matrix-multiplication_001.png\" />\n <p class=\"sphx-glr-script-out\">Out:</p>\n <div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>matmul-performance:\n-         M     cuBLAS  ...     Triton  Triton (+ LeakyReLU)\n-0    256.0   2.978909  ...   2.978909              3.276800\n-1    384.0   7.372800  ...   8.507077              8.507077\n-2    512.0  14.563555  ...  15.420235             16.384000\n-3    640.0  22.260869  ...  24.380953             24.380953\n-4    768.0  32.768000  ...  34.028308             34.028308\n-5    896.0  39.025776  ...  40.140799             39.025776\n-6   1024.0  49.932191  ...  53.773130             52.428801\n-7   1152.0  45.242181  ...  48.161033             47.396572\n-8   1280.0  51.200001  ...  57.690139             57.690139\n-9   1408.0  64.138541  ...  69.009825             67.305878\n-10  1536.0  80.430545  ...  81.355034             79.526831\n-11  1664.0  62.929456  ...  63.372618             62.492442\n-12  1792.0  72.512412  ...  73.460287             59.467852\n-13  1920.0  69.120002  ...  71.257735             71.257735\n-14  2048.0  73.584279  ...  78.033565             77.314362\n-15  2176.0  83.500614  ...  87.115360             85.998493\n-16  2304.0  68.446623  ...  78.064941             77.057651\n-17  2432.0  71.305746  ...  86.711310             75.320281\n-18  2560.0  78.019048  ...  82.539044             81.310171\n-19  2688.0  83.737433  ...  90.532356             89.254248\n-20  2816.0  79.587973  ...  83.392363             83.392363\n-21  2944.0  81.298583  ...  83.060049             82.373605\n-22  3072.0  81.825298  ...  89.310890             87.112467\n-23  3200.0  81.424937  ...  96.385543             95.380032\n-24  3328.0  83.034941  ...  86.424125             85.096096\n-25  3456.0  79.430113  ...  90.180725             89.281913\n-26  3584.0  87.381330  ...  99.134944             90.370072\n-27  3712.0  81.482335  ...  89.114488             84.159518\n-28  3840.0  84.874902  ...  92.934455             85.663823\n-29  3968.0  92.652949  ...  89.198780             91.954739\n-30  4096.0  86.256445  ...  89.003801             91.304576\n-\n-[31 rows x 5 columns]\n+         M     cuBLAS     Triton\n+0    256.0   3.276800   2.520615\n+1    384.0   6.505412   6.912000\n+2    512.0  13.797053  12.483048\n+3    640.0  21.333333  19.692308\n+4    768.0  31.597714  25.278171\n+5    896.0  37.971025  27.017846\n+6   1024.0  49.932191  35.544948\n+7   1152.0  44.566925  28.437943\n+8   1280.0  49.951220  29.257142\n+9   1408.0  62.664092  35.866949\n+10  1536.0  77.778988  42.896290\n+11  1664.0  60.803457  34.879506\n+12  1792.0  70.246402  40.140799\n+13  1920.0  67.106797  45.473686\n+14  2048.0  72.315584  40.721399\n+15  2176.0  81.472263  45.120288\n+16  2304.0  66.725901  41.185987\n+17  2432.0  69.886725  45.682053\n+18  2560.0  76.382283  49.648483\n+19  2688.0  79.691296  46.830933\n+20  2816.0  79.154642  44.054757\n+21  2944.0  77.626218  48.197325\n+22  3072.0  78.100834  46.298532\n+23  3200.0  77.385224  49.921998\n+24  3328.0  78.679014  48.381246\n+25  3456.0  77.371945  47.119561\n+26  3584.0  80.786517  50.485901\n+27  3712.0  78.688372  49.186282\n+28  3840.0  78.155164  47.627907\n+29  3968.0  81.240968  47.946536\n+30  4096.0  82.040176  51.861566\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 7 minutes  19.249 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  39.119 seconds)</p>\n <div class=\"sphx-glr-footer class sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-03-matrix-multiplication-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d5fee5b55a64e47f1b5724ec39adf171/03-matrix-multiplication.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">03-matrix-multiplication.py</span></code></a></p>\n@@ -654,17 +639,13 @@ <h3>Square Matrix Performance<a class=\"headerlink\" href=\"#square-matrix-performa\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"03-matrix-multiplication.html\">master</a></dd>\n+            <dd><a href=\"03-matrix-multiplication.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/getting-started/tutorials/04-low-memory-dropout.html", "status": "renamed", "additions": 25, "deletions": 28, "changes": 53, "file_content_changes": "@@ -108,6 +108,7 @@\n </ul>\n </li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n </ul>\n </li>\n </ul>\n@@ -199,11 +200,10 @@\n <span id=\"sphx-glr-getting-started-tutorials-04-low-memory-dropout-py\"></span><h1>Low-Memory Dropout<a class=\"headerlink\" href=\"#low-memory-dropout\" title=\"Permalink to this headline\">\u00b6</a></h1>\n <p>In this tutorial, you will write a memory-efficient implementation of dropout whose state\n will be composed of a single int32 seed. This differs from more traditional implementations of dropout,\n-whose state is generally composed of a bit mask tensor of the same shape as the input. You will learn about:</p>\n-<ul class=\"simple\">\n-<li><p>The limitations of naive implementations of Dropout with PyTorch</p></li>\n-<li><p>Parallel pseudo-random number generation in Triton</p></li>\n-</ul>\n+whose state is generally composed of a bit mask tensor of the same shape as the input.</p>\n+<p>In doing so, you will learn about:\n+- The limitations of naive implementations of Dropout with PyTorch\n+- Parallel pseudo-random number generation in Triton</p>\n <div class=\"section\" id=\"baseline\">\n <h2>Baseline<a class=\"headerlink\" href=\"#baseline\" title=\"Permalink to this headline\">\u00b6</a></h2>\n <p>The <em>dropout</em> operator was first introduced in <a class=\"reference internal\" href=\"#srivastava2014\" id=\"id1\"><span>[SRIVASTAVA2014]</span></a> as a way to improve the performance\n@@ -218,19 +218,20 @@ <h2>Baseline<a class=\"headerlink\" href=\"#baseline\" title=\"Permalink to this head\n <p>Let\u2019s first take a look at the baseline implementation.</p>\n <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">tabulate</span>\n <span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n+\n <span class=\"kn\">import</span> <span class=\"nn\">triton</span>\n <span class=\"kn\">import</span> <span class=\"nn\">triton.language</span> <span class=\"k\">as</span> <span class=\"nn\">tl</span>\n \n+\n <span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n <span class=\"k\">def</span> <span class=\"nf\">_dropout</span><span class=\"p\">(</span>\n-        <span class=\"n\">x_ptr</span><span class=\"p\">,</span> <span class=\"c1\"># pointer to the input</span>\n-        <span class=\"n\">x_keep_ptr</span><span class=\"p\">,</span> <span class=\"c1\"># pointer to a mask of 0s and 1s</span>\n-        <span class=\"n\">output_ptr</span><span class=\"p\">,</span> <span class=\"c1\"># pointer to the output</span>\n-        <span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"c1\"># number of elements in the `x` tensor</span>\n-        <span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"c1\"># probability that an element of `x` is changed to zero</span>\n-        <span class=\"o\">**</span><span class=\"n\">meta</span><span class=\"p\">,</span>\n+    <span class=\"n\">x_ptr</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the input</span>\n+    <span class=\"n\">x_keep_ptr</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to a mask of 0s and 1s</span>\n+    <span class=\"n\">output_ptr</span><span class=\"p\">,</span>  <span class=\"c1\"># pointer to the output</span>\n+    <span class=\"n\">n_elements</span><span class=\"p\">,</span>  <span class=\"c1\"># number of elements in the `x` tensor</span>\n+    <span class=\"n\">p</span><span class=\"p\">,</span>  <span class=\"c1\"># probability that an element of `x` is changed to zero</span>\n+    <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n <span class=\"p\">):</span>\n-    <span class=\"n\">BLOCK_SIZE</span> <span class=\"o\">=</span> <span class=\"n\">meta</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">]</span>\n     <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n     <span class=\"n\">block_start</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE</span>\n     <span class=\"n\">offsets</span> <span class=\"o\">=</span> <span class=\"n\">block_start</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">)</span>\n@@ -252,6 +253,7 @@ <h2>Baseline<a class=\"headerlink\" href=\"#baseline\" title=\"Permalink to this head\n     <span class=\"n\">_dropout</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">x_keep</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">n_elements</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"o\">=</span><span class=\"mi\">1024</span><span class=\"p\">)</span>\n     <span class=\"k\">return</span> <span class=\"n\">output</span>\n \n+\n <span class=\"c1\"># Input tensor</span>\n <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,))</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"p\">()</span>\n <span class=\"c1\"># Dropout mask</span>\n@@ -277,13 +279,13 @@ <h2>Baseline<a class=\"headerlink\" href=\"#baseline\" title=\"Permalink to this head\n </div>\n <div class=\"section\" id=\"seeded-dropout\">\n <h2>Seeded dropout<a class=\"headerlink\" href=\"#seeded-dropout\" title=\"Permalink to this headline\">\u00b6</a></h2>\n-<p>Above implementation of dropout works fine, but it can be a bit awkward to deal with. Firstly\n+<p>The above implementation of dropout works fine, but it can be a bit awkward to deal with. Firstly\n we need to store the dropout mask for backpropagation. Secondly, dropout state management can get\n very tricky when using recompute/checkpointing (e.g. see all the notes about <cite>preserve_rng_state</cite> in\n <a class=\"reference external\" href=\"https://pytorch.org/docs/1.9.0/checkpoint.html\">https://pytorch.org/docs/1.9.0/checkpoint.html</a>). In this tutorial we\u2019ll describe an alternative implementation\n that (1) has a smaller memory footprint; (2) requires less data movement; and (3) simplifies the management\n of persisting randomness across multiple invocations of the kernel.</p>\n-<p>Pseudorandom number generation in Triton is simple! In this tutorial we will use the\n+<p>Pseudo-random number generation in Triton is simple! In this tutorial we will use the\n <code class=\"code docutils literal notranslate\"><span class=\"pre\">triton.language.rand</span></code> function which generates a block of uniformly distributed <code class=\"code docutils literal notranslate\"><span class=\"pre\">float32</span></code>\n values in [0, 1), given a seed and a block of <code class=\"code docutils literal notranslate\"><span class=\"pre\">int32</span></code> offsets. But if you need it, Triton also provides\n other <a class=\"reference internal\" href=\"../../python-api/triton.language.html#random-number-generation\"><span class=\"std std-ref\">random number generation strategies</span></a>.</p>\n@@ -294,15 +296,14 @@ <h2>Seeded dropout<a class=\"headerlink\" href=\"#seeded-dropout\" title=\"Permalink\n <p>Let\u2019s put it all together.</p>\n <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n <span class=\"k\">def</span> <span class=\"nf\">_seeded_dropout</span><span class=\"p\">(</span>\n-        <span class=\"n\">x_ptr</span><span class=\"p\">,</span>\n-        <span class=\"n\">output_ptr</span><span class=\"p\">,</span>\n-        <span class=\"n\">n_elements</span><span class=\"p\">,</span>\n-        <span class=\"n\">p</span><span class=\"p\">,</span>\n-        <span class=\"n\">seed</span><span class=\"p\">,</span>\n-        <span class=\"o\">**</span><span class=\"n\">meta</span><span class=\"p\">,</span>\n+    <span class=\"n\">x_ptr</span><span class=\"p\">,</span>\n+    <span class=\"n\">output_ptr</span><span class=\"p\">,</span>\n+    <span class=\"n\">n_elements</span><span class=\"p\">,</span>\n+    <span class=\"n\">p</span><span class=\"p\">,</span>\n+    <span class=\"n\">seed</span><span class=\"p\">,</span>\n+    <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n <span class=\"p\">):</span>\n     <span class=\"c1\"># compute memory offsets of elements handled by this instance</span>\n-    <span class=\"n\">BLOCK_SIZE</span> <span class=\"o\">=</span> <span class=\"n\">meta</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">]</span>\n     <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n     <span class=\"n\">block_start</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE</span>\n     <span class=\"n\">offsets</span> <span class=\"o\">=</span> <span class=\"n\">block_start</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">)</span>\n@@ -371,7 +372,7 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n <dd><p>Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \u201cDropout: A Simple Way to Prevent Neural Networks from Overfitting\u201d, JMLR 2014</p>\n </dd>\n </dl>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.011 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  1.261 seconds)</p>\n <div class=\"sphx-glr-footer class sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-04-low-memory-dropout-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/c9aed78977a4c05741d675a38dde3d7d/04-low-memory-dropout.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">04-low-memory-dropout.py</span></code></a></p>\n@@ -422,17 +423,13 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: v1.1.2\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"04-low-memory-dropout.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"../../../master/index.html\">master</a></dd>\n+            <dd><a href=\"04-low-memory-dropout.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/getting-started/tutorials/05-layer-norm.html", "status": "renamed", "additions": 267, "deletions": 160, "changes": 427, "file_content_changes": "v1.1.2/getting-started/tutorials/05-layer-norm.html"}, {"filename": "keren/fix-doc/getting-started/tutorials/07-math-functions.html", "status": "renamed", "additions": 26, "deletions": 31, "changes": 57, "file_content_changes": "@@ -7,7 +7,7 @@\n   \n   <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n   \n-  <title>Libdevice function &mdash; Triton  documentation</title>\n+  <title>Libdevice (tl.math) function &mdash; Triton  documentation</title>\n   \n \n   \n@@ -47,7 +47,7 @@\n     <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n     <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n     <link rel=\"next\" title=\"triton\" href=\"../../python-api/triton.html\" />\n-    <link rel=\"prev\" title=\"Fused Attention\" href=\"06-fused-attention.html\" /> \n+    <link rel=\"prev\" title=\"Layer Normalization\" href=\"05-layer-norm.html\" /> \n </head>\n \n <body class=\"wy-body-for-nav\">\n@@ -101,8 +101,7 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n-<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n-<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Libdevice function</a><ul>\n+<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Libdevice (<cite>tl.math</cite>) function</a><ul>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#asin-kernel\">asin Kernel</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#using-the-default-libdevice-library-path\">Using the default libdevice library path</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#customize-the-libdevice-library-path\">Customize the libdevice library path</a></li>\n@@ -171,13 +170,13 @@\n         \n           <li><a href=\"index.html\">Tutorials</a> &raquo;</li>\n         \n-      <li>Libdevice function</li>\n+      <li>Libdevice (<cite>tl.math</cite>) function</li>\n     \n     \n       <li class=\"wy-breadcrumbs-aside\">\n         \n           \n-            <a href=\"../../_sources/getting-started/tutorials/07-libdevice-function.rst.txt\" rel=\"nofollow\"> View page source</a>\n+            <a href=\"../../_sources/getting-started/tutorials/07-math-functions.rst.txt\" rel=\"nofollow\"> View page source</a>\n           \n         \n       </li>\n@@ -192,18 +191,18 @@\n             \n   <div class=\"sphx-glr-download-link-note admonition note\">\n <p class=\"admonition-title\">Note</p>\n-<p>Click <a class=\"reference internal\" href=\"#sphx-glr-download-getting-started-tutorials-07-libdevice-function-py\"><span class=\"std std-ref\">here</span></a>\n+<p>Click <a class=\"reference internal\" href=\"#sphx-glr-download-getting-started-tutorials-07-math-functions-py\"><span class=\"std std-ref\">here</span></a>\n to download the full example code</p>\n </div>\n-<div class=\"sphx-glr-example-title section\" id=\"libdevice-function\">\n-<span id=\"sphx-glr-getting-started-tutorials-07-libdevice-function-py\"></span><h1>Libdevice function<a class=\"headerlink\" href=\"#libdevice-function\" title=\"Permalink to this headline\">\u00b6</a></h1>\n+<div class=\"sphx-glr-example-title section\" id=\"libdevice-tl-math-function\">\n+<span id=\"sphx-glr-getting-started-tutorials-07-math-functions-py\"></span><h1>Libdevice (<cite>tl.math</cite>) function<a class=\"headerlink\" href=\"#libdevice-tl-math-function\" title=\"Permalink to this headline\">\u00b6</a></h1>\n <p>Triton can invoke a custom function from an external library.\n-In this example, we will use the <cite>libdevice</cite> library to apply <cite>asin</cite> on a tensor.\n-Please refer to <a class=\"reference external\" href=\"https://docs.nvidia.com/cuda/libdevice-users-guide/index.html\">https://docs.nvidia.com/cuda/libdevice-users-guide/index.html</a> regarding the semantics of all available libdevice functions.</p>\n-<p>In <cite>trition/language/libdevice.py</cite>, we try to aggregate functions with the same computation but different data types together.\n+In this example, we will use the <cite>libdevice</cite> library (a.k.a <cite>math</cite> in triton) to apply <cite>asin</cite> on a tensor.\n+Please refer to <a class=\"reference external\" href=\"https://docs.nvidia.com/cuda/libdevice-users-guide/index.html\">https://docs.nvidia.com/cuda/libdevice-users-guide/index.html</a> regarding the semantics of all available libdevice functions.\n+In <cite>triton/language/math.py</cite>, we try to aggregate functions with the same computation but different data types together.\n For example, both <cite>__nv_asin</cite> and <cite>__nvasinf</cite> calculate the principal value of the arc sine of the input, but <cite>__nv_asin</cite> operates on <cite>double</cite> and <cite>__nv_asinf</cite> operates on <cite>float</cite>.\n-Using triton, you can simply call <cite>tl.libdevice.asinf</cite>.\n-triton automatically selects the correct underlying device function to invoke based on input and output types.</p>\n+Using triton, you can simply call <cite>tl.math.asin</cite>.\n+Triton automatically selects the correct underlying device function to invoke based on input and output types.</p>\n <div class=\"section\" id=\"asin-kernel\">\n <h2>asin Kernel<a class=\"headerlink\" href=\"#asin-kernel\" title=\"Permalink to this headline\">\u00b6</a></h2>\n <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n@@ -214,24 +213,24 @@ <h2>asin Kernel<a class=\"headerlink\" href=\"#asin-kernel\" title=\"Permalink to thi\n \n <span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n <span class=\"k\">def</span> <span class=\"nf\">asin_kernel</span><span class=\"p\">(</span>\n-    <span class=\"n\">x_ptr</span><span class=\"p\">,</span>\n-    <span class=\"n\">y_ptr</span><span class=\"p\">,</span>\n-    <span class=\"n\">n_elements</span><span class=\"p\">,</span>\n-    <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+        <span class=\"n\">x_ptr</span><span class=\"p\">,</span>\n+        <span class=\"n\">y_ptr</span><span class=\"p\">,</span>\n+        <span class=\"n\">n_elements</span><span class=\"p\">,</span>\n+        <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n <span class=\"p\">):</span>\n     <span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n     <span class=\"n\">block_start</span> <span class=\"o\">=</span> <span class=\"n\">pid</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_SIZE</span>\n     <span class=\"n\">offsets</span> <span class=\"o\">=</span> <span class=\"n\">block_start</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_SIZE</span><span class=\"p\">)</span>\n     <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">offsets</span> <span class=\"o\">&lt;</span> <span class=\"n\">n_elements</span>\n     <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">x_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n-    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">libdevice</span><span class=\"o\">.</span><span class=\"n\">asin</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n+    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">asin</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n     <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">y_ptr</span> <span class=\"o\">+</span> <span class=\"n\">offsets</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"o\">=</span><span class=\"n\">mask</span><span class=\"p\">)</span>\n </pre></div>\n </div>\n </div>\n <div class=\"section\" id=\"using-the-default-libdevice-library-path\">\n <h2>Using the default libdevice library path<a class=\"headerlink\" href=\"#using-the-default-libdevice-library-path\" title=\"Permalink to this headline\">\u00b6</a></h2>\n-<p>We can use the default libdevice library path encoded in <cite>triton/language/libdevice.py</cite></p>\n+<p>We can use the default libdevice library path encoded in <cite>triton/language/math.py</cite></p>\n <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n <span class=\"n\">size</span> <span class=\"o\">=</span> <span class=\"mi\">98432</span>\n <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s1\">&#39;cuda&#39;</span><span class=\"p\">)</span>\n@@ -276,13 +275,13 @@ <h2>Customize the libdevice library path<a class=\"headerlink\" href=\"#customize-t\n The maximum difference between torch and triton is 2.384185791015625e-07\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.258 seconds)</p>\n-<div class=\"sphx-glr-footer class sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-07-libdevice-function-py\">\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.444 seconds)</p>\n+<div class=\"sphx-glr-footer class sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-07-math-functions-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n-<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/3ff29f967ace7985da24aab10352fc76/07-libdevice-function.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">07-libdevice-function.py</span></code></a></p>\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/77571465f7f4bd281d3a847dc2633146/07-math-functions.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">07-math-functions.py</span></code></a></p>\n </div>\n <div class=\"sphx-glr-download sphx-glr-download-jupyter docutils container\">\n-<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/1bc2e471d2fb0ec017c4d1d0890db4e2/07-libdevice-function.ipynb\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Jupyter</span> <span class=\"pre\">notebook:</span> <span class=\"pre\">07-libdevice-function.ipynb</span></code></a></p>\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/302f1761fdc89516b532aad33b963ca0/07-math-functions.ipynb\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Jupyter</span> <span class=\"pre\">notebook:</span> <span class=\"pre\">07-math-functions.ipynb</span></code></a></p>\n </div>\n </div>\n <p class=\"sphx-glr-signature\"><a class=\"reference external\" href=\"https://sphinx-gallery.github.io\">Gallery generated by Sphinx-Gallery</a></p>\n@@ -296,7 +295,7 @@ <h2>Customize the libdevice library path<a class=\"headerlink\" href=\"#customize-t\n           <footer>\n     <div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"footer navigation\">\n         <a href=\"../../python-api/triton.html\" class=\"btn btn-neutral float-right\" title=\"triton\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-        <a href=\"06-fused-attention.html\" class=\"btn btn-neutral float-left\" title=\"Fused Attention\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"05-layer-norm.html\" class=\"btn btn-neutral float-left\" title=\"Layer Normalization\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n     </div>\n \n   <hr/>\n@@ -327,17 +326,13 @@ <h2>Customize the libdevice library path<a class=\"headerlink\" href=\"#customize-t\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"07-libdevice-function.html\">master</a></dd>\n+            <dd><a href=\"07-math-functions.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/getting-started/tutorials/index.html", "status": "renamed", "additions": 18, "deletions": 10, "changes": 28, "file_content_changes": "@@ -101,6 +101,7 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n </ul>\n </li>\n </ul>\n@@ -184,19 +185,24 @@\n   <div class=\"section\" id=\"tutorials\">\n <span id=\"sphx-glr-getting-started-tutorials\"></span><h1>Tutorials<a class=\"headerlink\" href=\"#tutorials\" title=\"Permalink to this headline\">\u00b6</a></h1>\n <p>Below is a gallery of tutorials for writing various basic operations with Triton. It is recommended that you read through the tutorials in order, starting with the simplest one.</p>\n-<div class=\"sphx-glr-thumbcontainer\" tooltip=\"- The basic programming model of Triton - The triton.jit decorator, which is used to define Tri...\"><div class=\"figure align-default\" id=\"id1\">\n+<p>To install the dependencies for the tutorials:</p>\n+<div class=\"highlight-bash notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nb\">cd</span> triton\n+pip install -e <span class=\"s1\">&#39;./python[tutorials]&#39;</span>\n+</pre></div>\n+</div>\n+<div class=\"sphx-glr-thumbcontainer\" tooltip=\"In this tutorial, you will write a simple vector addition using Triton.\"><div class=\"figure align-default\" id=\"id1\">\n <img alt=\"Vector Addition\" src=\"../../_images/sphx_glr_01-vector-add_thumb.png\" />\n <p class=\"caption\"><span class=\"caption-text\"><a class=\"reference internal\" href=\"01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py\"><span class=\"std std-ref\">Vector Addition</span></a></span><a class=\"headerlink\" href=\"#id1\" title=\"Permalink to this image\">\u00b6</a></p>\n </div>\n </div><div class=\"toctree-wrapper compound\">\n </div>\n-<div class=\"sphx-glr-thumbcontainer\" tooltip=\"- The benefits of kernel fusion for bandwidth-bound operations. - Reduction operators in Triton...\"><div class=\"figure align-default\" id=\"id2\">\n+<div class=\"sphx-glr-thumbcontainer\" tooltip=\"In this tutorial, you will write a fused softmax operation that is significantly faster than Py...\"><div class=\"figure align-default\" id=\"id2\">\n <img alt=\"Fused Softmax\" src=\"../../_images/sphx_glr_02-fused-softmax_thumb.png\" />\n <p class=\"caption\"><span class=\"caption-text\"><a class=\"reference internal\" href=\"02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py\"><span class=\"std std-ref\">Fused Softmax</span></a></span><a class=\"headerlink\" href=\"#id2\" title=\"Permalink to this image\">\u00b6</a></p>\n </div>\n </div><div class=\"toctree-wrapper compound\">\n </div>\n-<div class=\"sphx-glr-thumbcontainer\" tooltip=\"- Block-level matrix multiplications - Multi-dimensional pointer arithmetic - Program re-orderi...\"><div class=\"figure align-default\" id=\"id3\">\n+<div class=\"sphx-glr-thumbcontainer\" tooltip=\"In doing so, you will learn about: - Block-level matrix multiplications - Multi-dimensional poi...\"><div class=\"figure align-default\" id=\"id3\">\n <img alt=\"Matrix Multiplication\" src=\"../../_images/sphx_glr_03-matrix-multiplication_thumb.png\" />\n <p class=\"caption\"><span class=\"caption-text\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html#sphx-glr-getting-started-tutorials-03-matrix-multiplication-py\"><span class=\"std std-ref\">Matrix Multiplication</span></a></span><a class=\"headerlink\" href=\"#id3\" title=\"Permalink to this image\">\u00b6</a></p>\n </div>\n@@ -208,12 +214,18 @@\n </div>\n </div><div class=\"toctree-wrapper compound\">\n </div>\n-<div class=\"sphx-glr-thumbcontainer\" tooltip=\"Layer Normalization\"><div class=\"figure align-default\" id=\"id5\">\n+<div class=\"sphx-glr-thumbcontainer\" tooltip=\"In doing so, you will learn about: - Implementing backward pass in Triton - Implementing parall...\"><div class=\"figure align-default\" id=\"id5\">\n <img alt=\"Layer Normalization\" src=\"../../_images/sphx_glr_05-layer-norm_thumb.png\" />\n <p class=\"caption\"><span class=\"caption-text\"><a class=\"reference internal\" href=\"05-layer-norm.html#sphx-glr-getting-started-tutorials-05-layer-norm-py\"><span class=\"std std-ref\">Layer Normalization</span></a></span><a class=\"headerlink\" href=\"#id5\" title=\"Permalink to this image\">\u00b6</a></p>\n </div>\n </div><div class=\"toctree-wrapper compound\">\n </div>\n+<div class=\"sphx-glr-thumbcontainer\" tooltip=\"Libdevice (`tl.math`) function\"><div class=\"figure align-default\" id=\"id6\">\n+<img alt=\"Libdevice (`tl.math`) function\" src=\"../../_images/sphx_glr_07-math-functions_thumb.png\" />\n+<p class=\"caption\"><span class=\"caption-text\"><a class=\"reference internal\" href=\"07-math-functions.html#sphx-glr-getting-started-tutorials-07-math-functions-py\"><span class=\"std std-ref\">Libdevice (tl.math) function</span></a></span><a class=\"headerlink\" href=\"#id6\" title=\"Permalink to this image\">\u00b6</a></p>\n+</div>\n+</div><div class=\"toctree-wrapper compound\">\n+</div>\n <div class=\"sphx-glr-clear\"></div><div class=\"sphx-glr-footer class sphx-glr-footer-gallery docutils container\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/763344228ae6bc253ed1a6cf586aa30d/tutorials_python.zip\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">all</span> <span class=\"pre\">examples</span> <span class=\"pre\">in</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">tutorials_python.zip</span></code></a></p>\n@@ -263,17 +275,13 @@\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: v1.1.2\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"../../../master/index.html\">master</a></dd>\n+            <dd><a href=\"index.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/getting-started/tutorials/sg_execution_times.html", "status": "renamed", "additions": 12, "deletions": 12, "changes": 24, "file_content_changes": "@@ -174,7 +174,7 @@\n             \n   <div class=\"section\" id=\"computation-times\">\n <span id=\"sphx-glr-getting-started-tutorials-sg-execution-times\"></span><h1>Computation times<a class=\"headerlink\" href=\"#computation-times\" title=\"Permalink to this headline\">\u00b6</a></h1>\n-<p><strong>12:39.242</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n+<p><strong>04:31.219</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n <table class=\"docutils align-default\">\n <colgroup>\n <col style=\"width: 85%\" />\n@@ -183,23 +183,27 @@\n </colgroup>\n <tbody>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"03-matrix-multiplication.html#sphx-glr-getting-started-tutorials-03-matrix-multiplication-py\"><span class=\"std std-ref\">Matrix Multiplication</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">03-matrix-multiplication.py</span></code>)</p></td>\n-<td><p>05:26.818</p></td>\n+<td><p>01:39.119</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py\"><span class=\"std std-ref\">Fused Softmax</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">02-fused-softmax.py</span></code>)</p></td>\n-<td><p>03:20.891</p></td>\n+<td><p>01:19.698</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"05-layer-norm.html#sphx-glr-getting-started-tutorials-05-layer-norm-py\"><span class=\"std std-ref\">Layer Normalization</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">05-layer-norm.py</span></code>)</p></td>\n-<td><p>02:07.366</p></td>\n+<td><p>01:00.853</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py\"><span class=\"std std-ref\">Vector Addition</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">01-vector-add.py</span></code>)</p></td>\n-<td><p>01:44.156</p></td>\n+<td><p>00:29.843</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"04-low-memory-dropout.html#sphx-glr-getting-started-tutorials-04-low-memory-dropout-py\"><span class=\"std std-ref\">Low-Memory Dropout</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">04-low-memory-dropout.py</span></code>)</p></td>\n-<td><p>00:00.011</p></td>\n+<td><p>00:01.261</p></td>\n+<td><p>0.0 MB</p></td>\n+</tr>\n+<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"07-math-functions.html#sphx-glr-getting-started-tutorials-07-math-functions-py\"><span class=\"std std-ref\">Libdevice (tl.math) function</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">07-math-functions.py</span></code>)</p></td>\n+<td><p>00:00.444</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n </tbody>\n@@ -240,17 +244,13 @@\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: v1.1.2\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"sg_execution_times.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"../../../master/index.html\">master</a></dd>\n+            <dd><a href=\"sg_execution_times.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/index.html", "status": "renamed", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -244,17 +244,13 @@ <h2>Going Further<a class=\"headerlink\" href=\"#going-further\" title=\"Permalink to\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"index.html\">master</a></dd>\n+            <dd><a href=\"index.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/objects.inv", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "keren/fix-doc/programming-guide/chapter-1/introduction.html", "status": "renamed", "additions": 10, "deletions": 14, "changes": 24, "file_content_changes": "@@ -183,10 +183,10 @@\n <h1>Introduction<a class=\"headerlink\" href=\"#introduction\" title=\"Permalink to this headline\">\u00b6</a></h1>\n <div class=\"section\" id=\"motivations\">\n <h2>Motivations<a class=\"headerlink\" href=\"#motivations\" title=\"Permalink to this headline\">\u00b6</a></h2>\n-<p>Over the past decade, Deep Neural Networks (DNNs) have emerged as an important class of Machine Learning (ML) models, capable of  achieving state-of-the-art performance across many domains ranging from natural language processing <a class=\"reference internal\" href=\"#sutskever2014\" id=\"id1\"><span>[SUTSKEVER2014]</span></a> to computer vision <a class=\"reference internal\" href=\"#redmon2016\" id=\"id2\"><span>[REDMON2016]</span></a> to computational neuroscience <a class=\"reference internal\" href=\"#lee2017\" id=\"id3\"><span>[LEE2017]</span></a>. The strength of these models lies in their hierarchical structure, composed of a sequence of parametric (e.g., convolutional) and non-parametric (e.g., rectified linearity) <em>layers</em>. This pattern, though notoriously computationally expensive, also generates a large amount of highly parallelizable work particularly well suited for multi- and many- core processors.</p>\n+<p>Over the past decade, Deep Neural Networks (DNNs) have emerged as an important class of Machine Learning (ML) models, capable of achieving state-of-the-art performance across many domains ranging from natural language processing <a class=\"reference internal\" href=\"#sutskever2014\" id=\"id1\"><span>[SUTSKEVER2014]</span></a> to computer vision <a class=\"reference internal\" href=\"#redmon2016\" id=\"id2\"><span>[REDMON2016]</span></a> to computational neuroscience <a class=\"reference internal\" href=\"#lee2017\" id=\"id3\"><span>[LEE2017]</span></a>. The strength of these models lies in their hierarchical structure, composed of a sequence of parametric (e.g., convolutional) and non-parametric (e.g., rectified linearity) <em>layers</em>. This pattern, though notoriously computationally expensive, also generates a large amount of highly parallelizable work particularly well suited for multi- and many- core processors.</p>\n <p>As a consequence, Graphics Processing Units (GPUs) have become a cheap and accessible resource for exploring and/or deploying novel research ideas in the field. This trend has been accelerated by the release of several frameworks for General-Purpose GPU (GPGPU) computing, such as CUDA and OpenCL, which have made the development of high-performance programs easier. Yet, GPUs remain incredibly challenging to optimize for locality and parallelism, especially for computations that cannot be efficiently implemented using a combination of pre-existing optimized primitives. To make matters worse, GPU architectures are also rapidly evolving and specializing, as evidenced by the addition of tensor cores to NVIDIA (and more recently AMD) micro-architectures.</p>\n-<p>This tension between the computational opportunities offered by DNNs and the practical difficulty of GPU programming has created substantial academic and industrial interest for Domain-Specific Languages (DSLs) and compilers. Regrettably, these systems \u2013 whether they be based on  polyhedral machinery (<em>e.g.</em>, Tiramisu <a class=\"reference internal\" href=\"../chapter-2/related-work.html#baghdadi2021\" id=\"id4\"><span>[BAGHDADI2021]</span></a>, Tensor Comprehensions <a class=\"reference internal\" href=\"../chapter-2/related-work.html#vasilache2018\" id=\"id5\"><span>[VASILACHE2018]</span></a>) or scheduling languages (<em>e.g.</em>, Halide <a class=\"reference internal\" href=\"#jrk2013\" id=\"id6\"><span>[JRK2013]</span></a>, TVM <a class=\"reference internal\" href=\"#chen2018\" id=\"id7\"><span>[CHEN2018]</span></a>) \u2013 remain less flexible and (for the same algorithm) markedly slower than the best handwritten compute kernels available in libraries like <a class=\"reference external\" href=\"https://docs.nvidia.com/cuda/cublas/index.html\">cuBLAS</a>, <a class=\"reference external\" href=\"https://docs.nvidia.com/deeplearning/cudnn/api/index.html\">cuDNN</a> or <a class=\"reference external\" href=\"https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html\">TensorRT</a>.</p>\n-<p>The main premise of this project is the following: programming paradigms based on blocked algorithms <a class=\"reference internal\" href=\"#lam1991\" id=\"id8\"><span>[LAM1991]</span></a> can facilitate the construction of high-performance compute kernels for neural networks.  We specifically revisit traditional \u201cSingle Program, Multiple Data\u201d (SPMD <a class=\"reference internal\" href=\"#auguin1983\" id=\"id9\"><span>[AUGUIN1983]</span></a>) execution models for GPUs, and propose a variant in which programs \u2013 rather than threads \u2013 are blocked. For example, in the case of matrix multiplication, CUDA and Triton differ as follows:</p>\n+<p>This tension between the computational opportunities offered by DNNs and the practical difficulty of GPU programming has created substantial academic and industrial interest for Domain-Specific Languages (DSLs) and compilers. Regrettably, these systems \u2013 whether they be based on polyhedral machinery (e.g., Tiramisu <a class=\"reference internal\" href=\"../chapter-2/related-work.html#baghdadi2021\" id=\"id4\"><span>[BAGHDADI2021]</span></a>, Tensor Comprehensions <a class=\"reference internal\" href=\"../chapter-2/related-work.html#vasilache2018\" id=\"id5\"><span>[VASILACHE2018]</span></a>) or scheduling languages (e.g., Halide <a class=\"reference internal\" href=\"#jrk2013\" id=\"id6\"><span>[JRK2013]</span></a>, TVM <a class=\"reference internal\" href=\"#chen2018\" id=\"id7\"><span>[CHEN2018]</span></a>) \u2013 remain less flexible and (for the same algorithm) markedly slower than the best handwritten compute kernels available in libraries like <a class=\"reference external\" href=\"https://docs.nvidia.com/cuda/cublas/index.html\">cuBLAS</a>, <a class=\"reference external\" href=\"https://docs.nvidia.com/deeplearning/cudnn/api/index.html\">cuDNN</a> or <a class=\"reference external\" href=\"https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html\">TensorRT</a>.</p>\n+<p>The main premise of this project is the following: programming paradigms based on blocked algorithms <a class=\"reference internal\" href=\"#lam1991\" id=\"id8\"><span>[LAM1991]</span></a> can facilitate the construction of high-performance compute kernels for neural networks. We specifically revisit traditional \u201cSingle Program, Multiple Data\u201d (SPMD <a class=\"reference internal\" href=\"#auguin1983\" id=\"id9\"><span>[AUGUIN1983]</span></a>) execution models for GPUs, and propose a variant in which programs \u2013 rather than threads \u2013 are blocked. For example, in the case of matrix multiplication, CUDA and Triton differ as follows:</p>\n <table class=\"colwidths-given docutils align-default\">\n <colgroup>\n <col style=\"width: 50%\" />\n@@ -203,14 +203,14 @@ <h2>Motivations<a class=\"headerlink\" href=\"#motivations\" title=\"Permalink to thi\n </thead>\n <tbody>\n <tr class=\"row-even\"><td><div class=\"highlight-C notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"cp\">#pragma parallel</span>\n-<span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kt\">int</span> <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"n\">i</span> <span class=\"o\">&lt;</span> <span class=\"n\">M</span><span class=\"p\">;</span> <span class=\"n\">m</span><span class=\"o\">++</span><span class=\"p\">)</span>\n+<span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kt\">int</span> <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"n\">m</span> <span class=\"o\">&lt;</span> <span class=\"n\">M</span><span class=\"p\">;</span> <span class=\"n\">m</span><span class=\"o\">++</span><span class=\"p\">)</span>\n <span class=\"cp\">#pragma parallel</span>\n-<span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kt\">int</span> <span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"n\">j</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span><span class=\"p\">;</span> <span class=\"n\">n</span><span class=\"o\">++</span><span class=\"p\">){</span>\n+<span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kt\">int</span> <span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"n\">n</span> <span class=\"o\">&lt;</span> <span class=\"n\">N</span><span class=\"p\">;</span> <span class=\"n\">n</span><span class=\"o\">++</span><span class=\"p\">){</span>\n   <span class=\"kt\">float</span> <span class=\"n\">acc</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n-  <span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kt\">int</span> <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"n\">k</span> <span class=\"o\">&lt;</span> <span class=\"n\">K</span><span class=\"p\">;</span><span class=\"n\">k</span> <span class=\"o\">++</span><span class=\"p\">)</span>\n-    <span class=\"n\">acc</span> <span class=\"o\">+=</span> <span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">]</span><span class=\"o\">*</span> <span class=\"n\">B</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">j</span><span class=\"p\">];</span>\n+  <span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kt\">int</span> <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"n\">k</span> <span class=\"o\">&lt;</span> <span class=\"n\">K</span><span class=\"p\">;</span> <span class=\"n\">k</span><span class=\"o\">++</span><span class=\"p\">)</span>\n+    <span class=\"n\">acc</span> <span class=\"o\">+=</span> <span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">B</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">];</span>\n \n-  <span class=\"n\">C</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">j</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">acc</span><span class=\"p\">;</span>\n+  <span class=\"n\">C</span><span class=\"p\">[</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">acc</span><span class=\"p\">;</span>\n <span class=\"p\">}</span>\n </pre></div>\n </div>\n@@ -330,17 +330,13 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/programming-guide/chapter-1/introduction.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"introduction.html\">master</a></dd>\n+            <dd><a href=\"introduction.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/programming-guide/chapter-2/related-work.html", "status": "renamed", "additions": 5, "deletions": 9, "changes": 14, "file_content_changes": "@@ -196,7 +196,7 @@ <h2>Polyhedral Compilation<a class=\"headerlink\" href=\"#polyhedral-compilation\" t\n <p>Traditional compilers typically rely on intermediate representations, such as LLVM-IR <a class=\"reference internal\" href=\"#lattner2004\" id=\"id1\"><span>[LATTNER2004]</span></a>, that encode control flow information using (un)conditional branches. This relatively low-level format makes it difficult to statically analyze the runtime behavior (e.g., cache misses) of input programs, and to  automatically optimize loops accordingly through the use of tiling <a class=\"reference internal\" href=\"#wolfe1989\" id=\"id2\"><span>[WOLFE1989]</span></a>, fusion <a class=\"reference internal\" href=\"#darte1999\" id=\"id3\"><span>[DARTE1999]</span></a> and interchange <a class=\"reference internal\" href=\"#allen1984\" id=\"id4\"><span>[ALLEN1984]</span></a>. To solve this issue, polyhedral compilers <a class=\"reference internal\" href=\"#ancourt1991\" id=\"id5\"><span>[ANCOURT1991]</span></a> rely on program representations that have statically predictable control flow, thereby enabling aggressive compile-time program transformations for data locality and parallelism. Though this strategy has been adopted by many languages and compilers for DNNs such as Tiramisu <a class=\"reference internal\" href=\"#baghdadi2021\" id=\"id6\"><span>[BAGHDADI2021]</span></a>, Tensor Comprehensions <a class=\"reference internal\" href=\"#vasilache2018\" id=\"id7\"><span>[VASILACHE2018]</span></a>, Diesel <a class=\"reference internal\" href=\"#elango2018\" id=\"id8\"><span>[ELANGO2018]</span></a> and the Affine dialect in MLIR <a class=\"reference internal\" href=\"#lattner2019\" id=\"id9\"><span>[LATTNER2019]</span></a>, it also comes with a number of limitations that will be described later in this section.</p>\n <div class=\"section\" id=\"program-representation\">\n <h3>Program Representation<a class=\"headerlink\" href=\"#program-representation\" title=\"Permalink to this headline\">\u00b6</a></h3>\n-<p>Polyhedral compilation is a vast area of research. In this section we only outline the most basic aspects of this topic, but readers interested in the solid mathematical foundations underneath may refer to the ample litterature on linear and integer programming.</p>\n+<p>Polyhedral compilation is a vast area of research. In this section we only outline the most basic aspects of this topic, but readers interested in the solid mathematical foundations underneath may refer to the ample literature on linear and integer programming.</p>\n <table class=\"colwidths-given docutils align-default\">\n <colgroup>\n <col style=\"width: 50%\" />\n@@ -316,7 +316,7 @@ <h3>Advantages<a class=\"headerlink\" href=\"#id16\" title=\"Permalink to this headli\n </div>\n <div class=\"section\" id=\"id17\">\n <h3>Limitations<a class=\"headerlink\" href=\"#id17\" title=\"Permalink to this headline\">\u00b6</a></h3>\n-<p>This ease-of-development comes at a cost. First of all, existing systems that follow this paradigm tend to be noticeably slower than Triton on modern hardware when applicable (e.g., V100/A100 tensor cores w/ equal tile sizes). I do believe that this is not a fundamental issue of scheduling languages \u2013 in the sense that it could probably be solved with more efforts \u2013 but it could mean that these systems are harder to engineer. More importantly, existing scheduling languages generate loops whose bounds and increments cannot depend on surrounding loop indice without at least imposing severe constraints on possible schedules \u2013 if not breaking the system entirely. This is problematic for sparse computations, whose iteration spaces may be irregular.</p>\n+<p>This ease-of-development comes at a cost. First of all, existing systems that follow this paradigm tend to be noticeably slower than Triton on modern hardware when applicable (e.g., V100/A100 tensor cores w/ equal tile sizes). I do believe that this is not a fundamental issue of scheduling languages \u2013 in the sense that it could probably be solved with more efforts \u2013 but it could mean that these systems are harder to engineer. More importantly, existing scheduling languages generate loops whose bounds and increments cannot depend on surrounding loop indices without at least imposing severe constraints on possible schedules \u2013 if not breaking the system entirely. This is problematic for sparse computations, whose iteration spaces may be irregular.</p>\n <table class=\"colwidths-given docutils align-default\">\n <colgroup>\n <col style=\"width: 50%\" />\n@@ -327,7 +327,7 @@ <h3>Limitations<a class=\"headerlink\" href=\"#id17\" title=\"Permalink to this headl\n <span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kt\">int</span> <span class=\"n\">j</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"n\">j</span> <span class=\"o\">&lt;</span> <span class=\"mi\">4</span><span class=\"p\">;</span> <span class=\"n\">j</span><span class=\"o\">++</span><span class=\"p\">)</span>\n   <span class=\"kt\">float</span> <span class=\"n\">acc</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n   <span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kt\">int</span> <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"n\">k</span> <span class=\"o\">&lt;</span> <span class=\"n\">K</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">];</span> <span class=\"n\">k</span><span class=\"o\">++</span><span class=\"p\">)</span>\n-    <span class=\"n\">acc</span> <span class=\"o\">+=</span> <span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">][</span><span class=\"n\">col</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span><span class=\"n\">k</span><span class=\"p\">]]</span><span class=\"o\">*</span><span class=\"n\">B</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"p\">][</span><span class=\"n\">j</span><span class=\"p\">]</span>\n+    <span class=\"n\">acc</span> <span class=\"o\">+=</span> <span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">][</span><span class=\"n\">col</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">]]</span> <span class=\"o\">*</span> <span class=\"n\">B</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"p\">][</span><span class=\"n\">j</span><span class=\"p\">]</span>\n   <span class=\"n\">C</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">][</span><span class=\"n\">j</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">acc</span><span class=\"p\">;</span>\n </pre></div>\n </div>\n@@ -456,17 +456,13 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/programming-guide/chapter-2/related-work.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"related-work.html\">master</a></dd>\n+            <dd><a href=\"related-work.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.Config.html", "status": "renamed", "additions": 10, "deletions": 11, "changes": 21, "file_content_changes": "@@ -187,14 +187,17 @@ <h1>triton.Config<a class=\"headerlink\" href=\"#triton-config\" title=\"Permalink to\n <dl class=\"py class\">\n <dt class=\"sig sig-object py\" id=\"triton.Config\">\n <em class=\"property\"><span class=\"pre\">class</span> </em><span class=\"sig-prename descclassname\"><span class=\"pre\">triton.</span></span><span class=\"sig-name descname\"><span class=\"pre\">Config</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">self</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">kwargs</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">num_warps</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">4</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">num_stages</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">2</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">pre_hook</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.Config\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>An object that represents a possible kernel configuration for the auto-tuner to try.</p>\n+<dd><p>An object that represents a possible kernel configuration for the auto-tuner to try.\n+:ivar meta: a dictionary of meta-parameters to pass to the kernel as keyword arguments.\n+:type meta: dict[Str, Any]\n+:ivar num_warps: the number of warps to use for the kernel when compiled for GPUs. For example, if</p>\n+<blockquote>\n+<div><p><cite>num_warps=8</cite>, then each kernel instance will be automatically parallelized to\n+cooperatively execute using <cite>8 * 32 = 256</cite> threads.</p>\n+</div></blockquote>\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Variables</dt>\n <dd class=\"field-odd\"><ul class=\"simple\">\n-<li><p><strong>meta</strong> \u2013 a dictionary of meta-parameters to pass to the kernel as keyword arguments.</p></li>\n-<li><p><strong>num_warps</strong> \u2013 the number of warps to use for the kernel when compiled for GPUs. For example, if\n-<cite>num_warps=8</cite>, then each kernel instance will be automatically parallelized to\n-cooperatively execute using <cite>8 * 32 = 256</cite> threads.</p></li>\n <li><p><strong>num_stages</strong> \u2013 the number of stages that the compiler should use when software-pipelining loops.\n Mostly useful for matrix multiplication workloads on SM80+ GPUs.</p></li>\n <li><p><strong>pre_hook</strong> \u2013 a function that will be called before the kernel is called. Parameters of this\n@@ -261,17 +264,13 @@ <h1>triton.Config<a class=\"headerlink\" href=\"#triton-config\" title=\"Permalink to\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.Config.html\">master</a></dd>\n+            <dd><a href=\"triton.Config.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.autotune.html", "status": "renamed", "additions": 28, "deletions": 22, "changes": 50, "file_content_changes": "@@ -187,25 +187,35 @@ <h1>triton.autotune<a class=\"headerlink\" href=\"#triton-autotune\" title=\"Permalin\n <dl class=\"py function\">\n <dt class=\"sig sig-object py\" id=\"triton.autotune\">\n <span class=\"sig-prename descclassname\"><span class=\"pre\">triton.</span></span><span class=\"sig-name descname\"><span class=\"pre\">autotune</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">configs</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">key</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">prune_configs_by</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">reset_to_zero</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.autotune\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Decorator for auto-tuning a <code class=\"code docutils literal notranslate\"><span class=\"pre\">triton.jit</span></code>\u2019d function.</p>\n-<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">autotune</span><span class=\"p\">(</span><span class=\"n\">configs</span><span class=\"o\">=</span><span class=\"p\">[</span>\n-    <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">(</span><span class=\"n\">meta</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">:</span> <span class=\"mi\">128</span><span class=\"p\">},</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n-    <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">Config</span><span class=\"p\">(</span><span class=\"n\">meta</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">:</span> <span class=\"mi\">1024</span><span class=\"p\">},</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">),</span>\n-  <span class=\"p\">],</span>\n-  <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;x_size&#39;</span><span class=\"p\">]</span> <span class=\"c1\"># the two above configs will be evaluated anytime</span>\n-                 <span class=\"c1\"># the value of x_size changes</span>\n-<span class=\"p\">)</span>\n-<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n-<span class=\"k\">def</span> <span class=\"nf\">kernel</span><span class=\"p\">(</span><span class=\"n\">x_ptr</span><span class=\"p\">,</span> <span class=\"n\">x_size</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">META</span><span class=\"p\">):</span>\n-    <span class=\"n\">BLOCK_SIZE</span> <span class=\"o\">=</span> <span class=\"n\">META</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">]</span>\n-</pre></div>\n-</div>\n+<dd><p>Decorator for auto-tuning a <code class=\"code docutils literal notranslate\"><span class=\"pre\">triton.jit</span></code>\u2019d function.\n+.. highlight:: python\n+.. code-block:: python</p>\n+<blockquote>\n+<div><dl>\n+<dt>&#64;triton.autotune(configs=[</dt><dd><blockquote>\n+<div><p>triton.Config(meta={\u2018BLOCK_SIZE\u2019: 128}, num_warps=4),\n+triton.Config(meta={\u2018BLOCK_SIZE\u2019: 1024}, num_warps=8),</p>\n+</div></blockquote>\n+<p>],\n+key=[\u2018x_size\u2019] # the two above configs will be evaluated anytime</p>\n+<blockquote>\n+<div><p># the value of x_size changes</p>\n+</div></blockquote>\n+</dd>\n+</dl>\n+<p>)\n+&#64;triton.jit\n+def kernel(x_ptr, x_size, <a href=\"#id1\"><span class=\"problematic\" id=\"id2\">**</span></a>META):</p>\n+<blockquote>\n+<div><p>BLOCK_SIZE = META[\u2018BLOCK_SIZE\u2019]</p>\n+</div></blockquote>\n+</div></blockquote>\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Note</dt>\n-<dd class=\"field-odd\"><p>When all the configurations are evaluated, the kernel will run multiple time.\n+<dd class=\"field-odd\"><p>When all the configurations are evaluated, the kernel will run multiple times.\n This means that whatever value the kernel updates will be updated multiple times.\n To avoid this undesired behavior, you can use the <cite>reset_to_zero</cite> argument, which\n-reset the value of the provided tensor to <cite>zero</cite> before running any configuration.</p>\n+resets the value of the provided tensor to <cite>zero</cite> before running any configuration.</p>\n </dd>\n <dt class=\"field-even\">Parameters</dt>\n <dd class=\"field-even\"><ul class=\"simple\">\n@@ -214,7 +224,7 @@ <h1>triton.autotune<a class=\"headerlink\" href=\"#triton-autotune\" title=\"Permalin\n <li><p><strong>prune_configs_by</strong> \u2013 a dict of functions that are used to prune configs, fields:\n \u2018perf_model\u2019: performance model used to predicate running time with different configs, returns running time\n \u2018top_k\u2019: number of configs to bench\n-\u2018early_config_prune\u2019(optional): a function used to do early prune (eg, num_stages). It take configs:List[Config] as its input, and returns pruned configs.</p></li>\n+\u2018early_config_prune\u2019(optional): a function used to do early prune (eg, num_stages). It takes configs:List[Config] as its input, and returns pruned configs.</p></li>\n <li><p><strong>reset_to_zero</strong> (<em>list</em><em>[</em><em>str</em><em>]</em>) \u2013 a list of argument names whose value will be reset to zero before evaluating any configs.</p></li>\n </ul>\n </dd>\n@@ -261,17 +271,13 @@ <h1>triton.autotune<a class=\"headerlink\" href=\"#triton-autotune\" title=\"Permalin\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.autotune.html\">master</a></dd>\n+            <dd><a href=\"triton.autotune.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.heuristics.html", "status": "renamed", "additions": 17, "deletions": 16, "changes": 33, "file_content_changes": "@@ -188,18 +188,23 @@ <h1>triton.heuristics<a class=\"headerlink\" href=\"#triton-heuristics\" title=\"Perm\n <dt class=\"sig sig-object py\" id=\"triton.heuristics\">\n <span class=\"sig-prename descclassname\"><span class=\"pre\">triton.</span></span><span class=\"sig-name descname\"><span class=\"pre\">heuristics</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">values</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.heuristics\" title=\"Permalink to this definition\">\u00b6</a></dt>\n <dd><p>Decorator for specifying how the values of certain meta-parameters may be computed.\n-This is useful for cases where auto-tuning is prohibitevely expensive, or just not applicable.</p>\n-<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">heuristics</span><span class=\"p\">(</span><span class=\"n\">values</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">:</span> <span class=\"k\">lambda</span> <span class=\"n\">args</span><span class=\"p\">:</span> <span class=\"mi\">2</span> <span class=\"o\">**</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">ceil</span><span class=\"p\">(</span><span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">log2</span><span class=\"p\">(</span><span class=\"n\">args</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])))})</span>\n-<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n-<span class=\"k\">def</span> <span class=\"nf\">kernel</span><span class=\"p\">(</span><span class=\"n\">x_ptr</span><span class=\"p\">,</span> <span class=\"n\">x_size</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">META</span><span class=\"p\">):</span>\n-    <span class=\"n\">BLOCK_SIZE</span> <span class=\"o\">=</span> <span class=\"n\">META</span><span class=\"p\">[</span><span class=\"s1\">&#39;BLOCK_SIZE&#39;</span><span class=\"p\">]</span> <span class=\"c1\"># smallest power-of-two &gt;= x_size</span>\n-</pre></div>\n-</div>\n-<dl class=\"simple\">\n-<dt>.param values: a dictionary of meta-parameter names and functions that compute the value of the meta-parameter.</dt><dd><p>each such function takes a list of positional arguments as input.</p>\n+This is useful for cases where auto-tuning is prohibitevely expensive, or just not applicable.\n+.. highlight:: python\n+.. code-block:: python</p>\n+<blockquote>\n+<div><p>&#64;triton.heuristics(values={\u2018BLOCK_SIZE\u2019: lambda args: 2 ** int(math.ceil(math.log2(args[1])))})\n+&#64;triton.jit\n+def kernel(x_ptr, x_size, <a href=\"#id1\"><span class=\"problematic\" id=\"id2\">**</span></a>META):</p>\n+<blockquote>\n+<div><p>BLOCK_SIZE = META[\u2018BLOCK_SIZE\u2019] # smallest power-of-two &gt;= x_size</p>\n+</div></blockquote>\n+</div></blockquote>\n+<dl class=\"field-list simple\">\n+<dt class=\"field-odd\">Parameters</dt>\n+<dd class=\"field-odd\"><p><strong>values</strong> (<em>dict</em><em>[</em><em>str</em><em>, </em><em>Callable</em><em>[</em><em>[</em><em>list</em><em>[</em><em>Any</em><em>]</em><em>]</em><em>, </em><em>Any</em><em>]</em><em>]</em>) \u2013 a dictionary of meta-parameter names and functions that compute the value of the meta-parameter.\n+each such function takes a list of positional arguments as input.</p>\n </dd>\n </dl>\n-<p>.type values: dict[str, Callable[[list[Any]], Any]]</p>\n </dd></dl>\n \n </div>\n@@ -242,17 +247,13 @@ <h1>triton.heuristics<a class=\"headerlink\" href=\"#triton-heuristics\" title=\"Perm\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.heuristics.html\">master</a></dd>\n+            <dd><a href=\"triton.heuristics.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.jit.html", "status": "renamed", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "@@ -186,17 +186,21 @@\n <h1>triton.jit<a class=\"headerlink\" href=\"#triton-jit\" title=\"Permalink to this headline\">\u00b6</a></h1>\n <dl class=\"py function\">\n <dt class=\"sig sig-object py\" id=\"triton.jit\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.</span></span><span class=\"sig-name descname\"><span class=\"pre\">jit</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span><span class=\"n\"><span class=\"pre\">args</span></span></em>, <em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">**</span></span><span class=\"n\"><span class=\"pre\">kwargs</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.jit\" title=\"Permalink to this definition\">\u00b6</a></dt>\n+<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.</span></span><span class=\"sig-name descname\"><span class=\"pre\">jit</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">fn</span></span><span class=\"p\"><span class=\"pre\">:</span></span> <span class=\"n\"><span class=\"pre\">triton.runtime.jit.T</span></span></em><span class=\"sig-paren\">)</span> <span class=\"sig-return\"><span class=\"sig-return-icon\">&#x2192;</span> <span class=\"sig-return-typehint\"><span class=\"pre\">triton.runtime.jit.JITFunction</span><span class=\"p\"><span class=\"pre\">[</span></span><span class=\"pre\">triton.runtime.jit.T</span><span class=\"p\"><span class=\"pre\">]</span></span></span></span><a class=\"headerlink\" href=\"#triton.jit\" title=\"Permalink to this definition\">\u00b6</a></dt>\n+<dt class=\"sig sig-object py\">\n+<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.</span></span><span class=\"sig-name descname\"><span class=\"pre\">jit</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"o\"><span class=\"pre\">*</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">version</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">'None'</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">do_not_specialize</span></span><span class=\"p\"><span class=\"pre\">:</span></span> <span class=\"n\"><span class=\"pre\">Optional</span><span class=\"p\"><span class=\"pre\">[</span></span><span class=\"pre\">Iterable</span><span class=\"p\"><span class=\"pre\">[</span></span><span class=\"pre\">int</span><span class=\"p\"><span class=\"pre\">]</span></span><span class=\"p\"><span class=\"pre\">]</span></span></span> <span class=\"o\"><span class=\"pre\">=</span></span> <span class=\"default_value\"><span class=\"pre\">'None'</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">debug</span></span><span class=\"p\"><span class=\"pre\">:</span></span> <span class=\"n\"><span class=\"pre\">Optional</span><span class=\"p\"><span class=\"pre\">[</span></span><span class=\"pre\">bool</span><span class=\"p\"><span class=\"pre\">]</span></span></span> <span class=\"o\"><span class=\"pre\">=</span></span> <span class=\"default_value\"><span class=\"pre\">'None'</span></span></em><span class=\"sig-paren\">)</span> <span class=\"sig-return\"><span class=\"sig-return-icon\">&#x2192;</span> <span class=\"sig-return-typehint\"><span class=\"pre\">Callable</span><span class=\"p\"><span class=\"pre\">[</span></span><span class=\"p\"><span class=\"pre\">[</span></span><span class=\"pre\">triton.runtime.jit.T</span><span class=\"p\"><span class=\"pre\">]</span></span><span class=\"p\"><span class=\"pre\">,</span> </span><span class=\"pre\">triton.runtime.jit.JITFunction</span><span class=\"p\"><span class=\"pre\">[</span></span><span class=\"pre\">triton.runtime.jit.T</span><span class=\"p\"><span class=\"pre\">]</span></span><span class=\"p\"><span class=\"pre\">]</span></span></span></span></dt>\n <dd><p>Decorator for JIT-compiling a function using the Triton compiler.</p>\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Note</dt>\n-<dd class=\"field-odd\"><p>When a jit\u2019d function is called, <code class=\"code docutils literal notranslate\"><span class=\"pre\">torch.tensor</span></code> arguments are implicitly converted to pointers using the <code class=\"code docutils literal notranslate\"><span class=\"pre\">.data_ptr()</span></code> method.</p>\n+<dd class=\"field-odd\"><p>When a jit\u2019d function is called, arguments are\n+implicitly converted to pointers if they have a <code class=\"code docutils literal notranslate\"><span class=\"pre\">.data_ptr()</span></code> method\n+and a <cite>.dtype</cite> attribute.</p>\n </dd>\n <dt class=\"field-even\">Note</dt>\n <dd class=\"field-even\"><p>This function will be compiled and run on the GPU. It will only have access to:</p>\n <ul class=\"simple\">\n <li><p>python primitives,</p></li>\n-<li><p>objects within the triton.language package,</p></li>\n+<li><p>builtins within the triton package,</p></li>\n <li><p>arguments to this function,</p></li>\n <li><p>other jit\u2019d functions</p></li>\n </ul>\n@@ -247,17 +251,13 @@ <h1>triton.jit<a class=\"headerlink\" href=\"#triton-jit\" title=\"Permalink to this\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: v1.1.2\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"triton.jit.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"../../../master/index.html\">master</a></dd>\n+            <dd><a href=\"triton.jit.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.abs.html", "status": "renamed", "additions": 14, "deletions": 17, "changes": 31, "file_content_changes": "@@ -7,7 +7,7 @@\n   \n   <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n   \n-  <title>triton.language.exp &mdash; Triton  documentation</title>\n+  <title>triton.language.abs &mdash; Triton  documentation</title>\n   \n \n   \n@@ -46,7 +46,7 @@\n     \n     <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n     <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton.language.log\" href=\"triton.language.log.html\" />\n+    <link rel=\"next\" title=\"triton.language.exp\" href=\"triton.language.exp.html\" />\n     <link rel=\"prev\" title=\"triton.language.where\" href=\"triton.language.where.html\" /> \n </head>\n \n@@ -108,7 +108,8 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n <li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a><ul class=\"current\">\n-<li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.exp</a></li>\n+<li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.abs</a></li>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.exp.html\">triton.language.exp</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.log.html\">triton.language.log</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.cos.html\">triton.language.cos</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.sin.html\">triton.language.sin</a></li>\n@@ -180,13 +181,13 @@\n         \n           <li><a href=\"../triton.language.html\">triton.language</a> &raquo;</li>\n         \n-      <li>triton.language.exp</li>\n+      <li>triton.language.abs</li>\n     \n     \n       <li class=\"wy-breadcrumbs-aside\">\n         \n           \n-            <a href=\"../../_sources/python-api/generated/triton.language.exp.rst.txt\" rel=\"nofollow\"> View page source</a>\n+            <a href=\"../../_sources/python-api/generated/triton.language.abs.rst.txt\" rel=\"nofollow\"> View page source</a>\n           \n         \n       </li>\n@@ -199,12 +200,12 @@\n           <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n            <div itemprop=\"articleBody\">\n             \n-  <div class=\"section\" id=\"triton-language-exp\">\n-<h1>triton.language.exp<a class=\"headerlink\" href=\"#triton-language-exp\" title=\"Permalink to this headline\">\u00b6</a></h1>\n+  <div class=\"section\" id=\"triton-language-abs\">\n+<h1>triton.language.abs<a class=\"headerlink\" href=\"#triton-language-abs\" title=\"Permalink to this headline\">\u00b6</a></h1>\n <dl class=\"py function\">\n-<dt class=\"sig sig-object py\" id=\"triton.language.exp\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">exp</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">x</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.exp\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Computes the element-wise exponential of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code></p>\n+<dt class=\"sig sig-object py\" id=\"triton.language.abs\">\n+<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">abs</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">x</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.abs\" title=\"Permalink to this definition\">\u00b6</a></dt>\n+<dd><p>Computes the element-wise absolute value of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code>.</p>\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Parameters</dt>\n <dd class=\"field-odd\"><p><strong>x</strong> (<em>Block</em>) \u2013 the input values</p>\n@@ -220,7 +221,7 @@ <h1>triton.language.exp<a class=\"headerlink\" href=\"#triton-language-exp\" title=\"\n           </div>\n           <footer>\n     <div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"footer navigation\">\n-        <a href=\"triton.language.log.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.log\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+        <a href=\"triton.language.exp.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.exp\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n         <a href=\"triton.language.where.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.where\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n     </div>\n \n@@ -252,17 +253,13 @@ <h1>triton.language.exp<a class=\"headerlink\" href=\"#triton-language-exp\" title=\"\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.language.exp.html\">master</a></dd>\n+            <dd><a href=\"triton.language.abs.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.arange.html", "status": "renamed", "additions": 5, "deletions": 9, "changes": 14, "file_content_changes": "@@ -199,12 +199,12 @@ <h1>triton.language.arange<a class=\"headerlink\" href=\"#triton-language-arange\" t\n <dl class=\"py function\">\n <dt class=\"sig sig-object py\" id=\"triton.language.arange\">\n <span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">arange</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">start</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">end</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.arange\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Returns contiguous values within the open interval [<code class=\"code docutils literal notranslate\"><span class=\"pre\">start</span></code>, <code class=\"code docutils literal notranslate\"><span class=\"pre\">end</span></code>).</p>\n+<dd><p>Returns contiguous values within the left-closed and right-open interval [<code class=\"code docutils literal notranslate\"><span class=\"pre\">start</span></code>, <code class=\"code docutils literal notranslate\"><span class=\"pre\">end</span></code>).     End - Start must be less than or equal to TRITON_MAX_TENSOR_NUMEL = 131072</p>\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Parameters</dt>\n <dd class=\"field-odd\"><ul class=\"simple\">\n-<li><p><strong>start</strong> (<em>int</em>) \u2013 Start of the interval. Must be a power of two.</p></li>\n-<li><p><strong>stop</strong> (<em>int</em>) \u2013 End of the interval. Must be a power of two &gt;= start.</p></li>\n+<li><p><strong>start</strong> (<em>int32</em>) \u2013 Start of the interval. Must be a power of two.</p></li>\n+<li><p><strong>end</strong> (<em>int32</em>) \u2013 End of the interval. Must be a power of two &gt; start.</p></li>\n </ul>\n </dd>\n </dl>\n@@ -250,17 +250,13 @@ <h1>triton.language.arange<a class=\"headerlink\" href=\"#triton-language-arange\" t\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: v1.1.2\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"triton.language.arange.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"../../../master/index.html\">master</a></dd>\n+            <dd><a href=\"triton.language.arange.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.atomic_add.html", "status": "renamed", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -254,17 +254,13 @@ <h1>triton.language.atomic_add<a class=\"headerlink\" href=\"#triton-language-atomi\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: v1.1.2\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"triton.language.atomic_add.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"../../../master/index.html\">master</a></dd>\n+            <dd><a href=\"triton.language.atomic_add.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.atomic_cas.html", "status": "renamed", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -260,17 +260,13 @@ <h1>triton.language.atomic_cas<a class=\"headerlink\" href=\"#triton-language-atomi\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: v1.1.2\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"triton.language.atomic_cas.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"../../../master/index.html\">master</a></dd>\n+            <dd><a href=\"triton.language.atomic_cas.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.atomic_max.html", "status": "renamed", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -254,17 +254,13 @@ <h1>triton.language.atomic_max<a class=\"headerlink\" href=\"#triton-language-atomi\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: v1.1.2\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"triton.language.atomic_max.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"../../../master/index.html\">master</a></dd>\n+            <dd><a href=\"triton.language.atomic_max.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.atomic_min.html", "status": "renamed", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -254,17 +254,13 @@ <h1>triton.language.atomic_min<a class=\"headerlink\" href=\"#triton-language-atomi\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: v1.1.2\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"triton.language.atomic_min.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"../../../master/index.html\">master</a></dd>\n+            <dd><a href=\"triton.language.atomic_min.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.atomic_xchg.html", "status": "renamed", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -254,17 +254,13 @@ <h1>triton.language.atomic_xchg<a class=\"headerlink\" href=\"#triton-language-atom\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: v1.1.2\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"triton.language.atomic_xchg.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"../../../master/index.html\">master</a></dd>\n+            <dd><a href=\"triton.language.atomic_xchg.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.broadcast_to.html", "status": "renamed", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -251,17 +251,13 @@ <h1>triton.language.broadcast_to<a class=\"headerlink\" href=\"#triton-language-bro\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.language.broadcast_to.html\">master</a></dd>\n+            <dd><a href=\"triton.language.broadcast_to.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.cos.html", "status": "renamed", "additions": 4, "deletions": 7, "changes": 11, "file_content_changes": "@@ -108,6 +108,7 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n <li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a><ul class=\"current\">\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.abs.html\">triton.language.abs</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.exp.html\">triton.language.exp</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.log.html\">triton.language.log</a></li>\n <li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.cos</a></li>\n@@ -204,7 +205,7 @@ <h1>triton.language.cos<a class=\"headerlink\" href=\"#triton-language-cos\" title=\"\n <dl class=\"py function\">\n <dt class=\"sig sig-object py\" id=\"triton.language.cos\">\n <span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">cos</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">x</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.cos\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Computes the element-wise cosine of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code></p>\n+<dd><p>Computes the element-wise cosine of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code>.</p>\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Parameters</dt>\n <dd class=\"field-odd\"><p><strong>x</strong> (<em>Block</em>) \u2013 the input values</p>\n@@ -252,17 +253,13 @@ <h1>triton.language.cos<a class=\"headerlink\" href=\"#triton-language-cos\" title=\"\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: v1.1.2\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"triton.language.cos.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"../../../master/index.html\">master</a></dd>\n+            <dd><a href=\"triton.language.cos.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.dot.html", "status": "renamed", "additions": 4, "deletions": 8, "changes": 12, "file_content_changes": "@@ -197,9 +197,9 @@\n <h1>triton.language.dot<a class=\"headerlink\" href=\"#triton-language-dot\" title=\"Permalink to this headline\">\u00b6</a></h1>\n <dl class=\"py function\">\n <dt class=\"sig sig-object py\" id=\"triton.language.dot\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">dot</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">input</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">other</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">trans_a</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">trans_b</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">allow_tf32</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.dot\" title=\"Permalink to this definition\">\u00b6</a></dt>\n+<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">dot</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">input</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">other</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">allow_tf32</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">out_dtype</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">triton.language.fp32</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.dot\" title=\"Permalink to this definition\">\u00b6</a></dt>\n <dd><p>Returns the matrix product of two blocks.</p>\n-<p>The two blocks must be two dimensionals and have compatible inner dimensions.</p>\n+<p>The two blocks must be two-dimensional and have compatible inner dimensions.</p>\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Parameters</dt>\n <dd class=\"field-odd\"><ul class=\"simple\">\n@@ -250,17 +250,13 @@ <h1>triton.language.dot<a class=\"headerlink\" href=\"#triton-language-dot\" title=\"\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.language.dot.html\">master</a></dd>\n+            <dd><a href=\"triton.language.dot.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.exp.html", "status": "renamed", "additions": 6, "deletions": 9, "changes": 15, "file_content_changes": "@@ -47,7 +47,7 @@\n     <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n     <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n     <link rel=\"next\" title=\"triton.language.log\" href=\"triton.language.log.html\" />\n-    <link rel=\"prev\" title=\"triton.language.where\" href=\"triton.language.where.html\" /> \n+    <link rel=\"prev\" title=\"triton.language.abs\" href=\"triton.language.abs.html\" /> \n </head>\n \n <body class=\"wy-body-for-nav\">\n@@ -108,6 +108,7 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n <li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a><ul class=\"current\">\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.abs.html\">triton.language.abs</a></li>\n <li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.exp</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.log.html\">triton.language.log</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.cos.html\">triton.language.cos</a></li>\n@@ -204,7 +205,7 @@ <h1>triton.language.exp<a class=\"headerlink\" href=\"#triton-language-exp\" title=\"\n <dl class=\"py function\">\n <dt class=\"sig sig-object py\" id=\"triton.language.exp\">\n <span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">exp</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">x</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.exp\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Computes the element-wise exponential of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code></p>\n+<dd><p>Computes the element-wise exponential of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code>.</p>\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Parameters</dt>\n <dd class=\"field-odd\"><p><strong>x</strong> (<em>Block</em>) \u2013 the input values</p>\n@@ -221,7 +222,7 @@ <h1>triton.language.exp<a class=\"headerlink\" href=\"#triton-language-exp\" title=\"\n           <footer>\n     <div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"footer navigation\">\n         <a href=\"triton.language.log.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.log\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-        <a href=\"triton.language.where.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.where\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"triton.language.abs.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.abs\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n     </div>\n \n   <hr/>\n@@ -252,17 +253,13 @@ <h1>triton.language.exp<a class=\"headerlink\" href=\"#triton-language-exp\" title=\"\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: v1.1.2\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"triton.language.exp.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"../../../master/index.html\">master</a></dd>\n+            <dd><a href=\"triton.language.exp.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.load.html", "status": "renamed", "additions": 28, "deletions": 15, "changes": 43, "file_content_changes": "@@ -200,21 +200,38 @@\n <h1>triton.language.load<a class=\"headerlink\" href=\"#triton-language-load\" title=\"Permalink to this headline\">\u00b6</a></h1>\n <dl class=\"py function\">\n <dt class=\"sig sig-object py\" id=\"triton.language.load\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">load</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">pointer</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">mask</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">other</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">cache_modifier</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">''</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">eviction_policy</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">''</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">volatile</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.load\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Return a tensor of data whose values are, elementwise, loaded from memory at location defined by <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer</span></code>.</p>\n-<p><code class=\"code docutils literal notranslate\"><span class=\"pre\">mask</span></code> and <code class=\"code docutils literal notranslate\"><span class=\"pre\">other</span></code> are implicitly broadcast to <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer.shape</span></code>.</p>\n-<p><code class=\"code docutils literal notranslate\"><span class=\"pre\">other</span></code> is implicitly typecast to <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer.dtype.element_ty</span></code>.</p>\n+<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">load</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">pointer</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">mask</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">other</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">boundary_check</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">()</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">padding_option</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">''</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">cache_modifier</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">''</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">eviction_policy</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">''</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">volatile</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.load\" title=\"Permalink to this definition\">\u00b6</a></dt>\n+<dd><dl class=\"simple\">\n+<dt>Return a tensor of data whose values are loaded from memory at location defined by <cite>pointer</cite>:</dt><dd><ol class=\"arabic simple\">\n+<li><p><cite>pointer</cite> could be a single element pointer, then a scalar will be loaded\n+- <cite>mask</cite> and <cite>other</cite> must be scalar too\n+- <cite>other</cite> is implicitly typecast to <cite>pointer.dtype.element_ty</cite>\n+- <cite>boundary_check</cite> and <cite>padding_option</cite> must be empty</p></li>\n+<li><p><cite>pointer</cite> could be element-wise tensor of pointers, in which case:\n+- <cite>mask</cite> and <cite>other</cite> are implicitly broadcast to <cite>pointer.shape</cite>\n+- <cite>other</cite> is implicitly typecast to <cite>pointer.dtype.element_ty</cite>\n+- <cite>boundary_check</cite> and <cite>padding_option</cite> must be empty</p></li>\n+<li><p><cite>pointer</cite> could be a block pointer defined by <cite>make_block_ptr</cite>, in which case:\n+- <cite>mask</cite> and <cite>other</cite> must be None\n+- <cite>boundary_check</cite> and <cite>padding_option</cite> can be specified to control the behavior of out-of-bound access</p></li>\n+</ol>\n+</dd>\n+</dl>\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Parameters</dt>\n <dd class=\"field-odd\"><ul class=\"simple\">\n-<li><p><strong>pointer</strong> (<em>Block of dtype=triton.PointerDType</em>) \u2013 Pointers to the data to be loaded.</p></li>\n-<li><p><strong>mask</strong> (<em>Block of triton.int1</em><em>, </em><em>optional</em>) \u2013 if mask[idx] is false, do not load the data at address <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer[idx]</span></code>.</p></li>\n-<li><p><strong>other</strong> (<em>Block</em><em>, </em><em>optional</em>) \u2013 if mask[idx] is false, return other[idx]</p></li>\n-<li><p><strong>cache_modifier</strong> \u2013 changes cache option in nvidia ptx</p></li>\n+<li><p><strong>pointer</strong> (<cite>triton.PointerType</cite>, or block of <cite>dtype=triton.PointerType</cite>) \u2013 Pointer to the data to be loaded</p></li>\n+<li><p><strong>mask</strong> (Block of <cite>triton.int1</cite>, optional) \u2013 if <cite>mask[idx]</cite> is false, do not load the data at address <cite>pointer[idx]</cite>\n+(must be <cite>None</cite> with block pointers)</p></li>\n+<li><p><strong>other</strong> (<em>Block</em><em>, </em><em>optional</em>) \u2013 if <cite>mask[idx]</cite> is false, return <cite>other[idx]</cite></p></li>\n+<li><p><strong>boundary_check</strong> (<em>tuple of ints</em><em>, </em><em>optional</em>) \u2013 tuple of integers, indicating the dimensions which should do the boundary check</p></li>\n+<li><p><strong>padding_option</strong> \u2013 should be one of {\u201c\u201d, \u201czero\u201d, \u201cnan\u201d}, do padding while out of bound</p></li>\n+<li><p><strong>cache_modifier</strong> (<em>str</em><em>, </em><em>optional</em>) \u2013 changes cache option in NVIDIA PTX</p></li>\n+<li><p><strong>eviction_policy</strong> (<em>str</em><em>, </em><em>optional</em>) \u2013 changes eviction policy in NVIDIA PTX</p></li>\n+<li><p><strong>volatile</strong> (<em>bool</em><em>, </em><em>optional</em>) \u2013 changes volatile option in NVIDIA PTX</p></li>\n </ul>\n </dd>\n </dl>\n-<p>\u2018type cache_modifier: str, optional</p>\n </dd></dl>\n \n </div>\n@@ -257,17 +274,13 @@ <h1>triton.language.load<a class=\"headerlink\" href=\"#triton-language-load\" title\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.language.load.html\">master</a></dd>\n+            <dd><a href=\"triton.language.load.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.log.html", "status": "renamed", "additions": 4, "deletions": 7, "changes": 11, "file_content_changes": "@@ -108,6 +108,7 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n <li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a><ul class=\"current\">\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.abs.html\">triton.language.abs</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.exp.html\">triton.language.exp</a></li>\n <li class=\"toctree-l3 current\"><a class=\"current reference internal\" href=\"#\">triton.language.log</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.cos.html\">triton.language.cos</a></li>\n@@ -204,7 +205,7 @@ <h1>triton.language.log<a class=\"headerlink\" href=\"#triton-language-log\" title=\"\n <dl class=\"py function\">\n <dt class=\"sig sig-object py\" id=\"triton.language.log\">\n <span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">log</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">x</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.log\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Computes the element-wise natural logarithm of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code></p>\n+<dd><p>Computes the element-wise natural logarithm of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code>.</p>\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Parameters</dt>\n <dd class=\"field-odd\"><p><strong>x</strong> (<em>Block</em>) \u2013 the input values</p>\n@@ -252,17 +253,13 @@ <h1>triton.language.log<a class=\"headerlink\" href=\"#triton-language-log\" title=\"\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: v1.1.2\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"triton.language.log.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"../../../master/index.html\">master</a></dd>\n+            <dd><a href=\"triton.language.log.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.max.html", "status": "renamed", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -251,17 +251,13 @@ <h1>triton.language.max<a class=\"headerlink\" href=\"#triton-language-max\" title=\"\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.language.max.html\">master</a></dd>\n+            <dd><a href=\"triton.language.max.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.maximum.html", "status": "renamed", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -250,17 +250,13 @@ <h1>triton.language.maximum<a class=\"headerlink\" href=\"#triton-language-maximum\"\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.language.maximum.html\">master</a></dd>\n+            <dd><a href=\"triton.language.maximum.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.min.html", "status": "renamed", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -251,17 +251,13 @@ <h1>triton.language.min<a class=\"headerlink\" href=\"#triton-language-min\" title=\"\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.language.min.html\">master</a></dd>\n+            <dd><a href=\"triton.language.min.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.minimum.html", "status": "renamed", "additions": 4, "deletions": 8, "changes": 12, "file_content_changes": "@@ -203,8 +203,8 @@ <h1>triton.language.minimum<a class=\"headerlink\" href=\"#triton-language-minimum\"\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Parameters</dt>\n <dd class=\"field-odd\"><ul class=\"simple\">\n-<li><p><strong>input</strong> (<em>Block</em>) \u2013 the first input block</p></li>\n-<li><p><strong>other</strong> (<em>Block</em>) \u2013 the second input block</p></li>\n+<li><p><strong>input</strong> (<em>Block</em>) \u2013 the first input tensor</p></li>\n+<li><p><strong>other</strong> (<em>Block</em>) \u2013 the second input tensor</p></li>\n </ul>\n </dd>\n </dl>\n@@ -250,17 +250,13 @@ <h1>triton.language.minimum<a class=\"headerlink\" href=\"#triton-language-minimum\"\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: v1.1.2\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"triton.language.minimum.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"../../../master/index.html\">master</a></dd>\n+            <dd><a href=\"triton.language.minimum.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.multiple_of.html", "status": "renamed", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -241,17 +241,13 @@ <h1>triton.language.multiple_of<a class=\"headerlink\" href=\"#triton-language-mult\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.language.multiple_of.html\">master</a></dd>\n+            <dd><a href=\"triton.language.multiple_of.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.num_programs.html", "status": "renamed", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -247,17 +247,13 @@ <h1>triton.language.num_programs<a class=\"headerlink\" href=\"#triton-language-num\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.language.num_programs.html\">master</a></dd>\n+            <dd><a href=\"triton.language.num_programs.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.program_id.html", "status": "renamed", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -247,17 +247,13 @@ <h1>triton.language.program_id<a class=\"headerlink\" href=\"#triton-language-progr\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.language.program_id.html\">master</a></dd>\n+            <dd><a href=\"triton.language.program_id.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.rand.html", "status": "renamed", "additions": 3, "deletions": 7, "changes": 10, "file_content_changes": "@@ -203,7 +203,7 @@ <h1>triton.language.rand<a class=\"headerlink\" href=\"#triton-language-rand\" title\n <dt class=\"sig sig-object py\" id=\"triton.language.rand\">\n <span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">rand</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">seed</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">offset</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">n_rounds</span></span><span class=\"p\"><span class=\"pre\">:</span></span> <span class=\"n\"><span class=\"pre\">triton.language.core.constexpr</span></span> <span class=\"o\"><span class=\"pre\">=</span></span> <span class=\"default_value\"><span class=\"pre\">10</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.rand\" title=\"Permalink to this definition\">\u00b6</a></dt>\n <dd><p>Given a <code class=\"code docutils literal notranslate\"><span class=\"pre\">seed</span></code> scalar and an <code class=\"code docutils literal notranslate\"><span class=\"pre\">offset</span></code> block,\n-returns a block of random <code class=\"code docutils literal notranslate\"><span class=\"pre\">float32</span></code> in <span class=\"math notranslate nohighlight\">\\(U(0, 1)\\)</span></p>\n+returns a block of random <code class=\"code docutils literal notranslate\"><span class=\"pre\">float32</span></code> in <span class=\"math notranslate nohighlight\">\\(U(0, 1)\\)</span>.</p>\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Parameters</dt>\n <dd class=\"field-odd\"><ul class=\"simple\">\n@@ -254,17 +254,13 @@ <h1>triton.language.rand<a class=\"headerlink\" href=\"#triton-language-rand\" title\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.language.rand.html\">master</a></dd>\n+            <dd><a href=\"triton.language.rand.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.randint.html", "status": "renamed", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -255,17 +255,13 @@ <h1>triton.language.randint<a class=\"headerlink\" href=\"#triton-language-randint\"\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.language.randint.html\">master</a></dd>\n+            <dd><a href=\"triton.language.randint.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.randint4x.html", "status": "renamed", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -255,17 +255,13 @@ <h1>triton.language.randint4x<a class=\"headerlink\" href=\"#triton-language-randin\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.language.randint4x.html\">master</a></dd>\n+            <dd><a href=\"triton.language.randint4x.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.randn.html", "status": "renamed", "additions": 3, "deletions": 7, "changes": 10, "file_content_changes": "@@ -203,7 +203,7 @@ <h1>triton.language.randn<a class=\"headerlink\" href=\"#triton-language-randn\" tit\n <dt class=\"sig sig-object py\" id=\"triton.language.randn\">\n <span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">randn</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">seed</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">offset</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">n_rounds</span></span><span class=\"p\"><span class=\"pre\">:</span></span> <span class=\"n\"><span class=\"pre\">triton.language.core.constexpr</span></span> <span class=\"o\"><span class=\"pre\">=</span></span> <span class=\"default_value\"><span class=\"pre\">10</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.randn\" title=\"Permalink to this definition\">\u00b6</a></dt>\n <dd><p>Given a <code class=\"code docutils literal notranslate\"><span class=\"pre\">seed</span></code> scalar and an <code class=\"code docutils literal notranslate\"><span class=\"pre\">offset</span></code> block,\n-returns a block of random <code class=\"code docutils literal notranslate\"><span class=\"pre\">float32</span></code> in <span class=\"math notranslate nohighlight\">\\(\\mathcal{N}(0, 1)\\)</span></p>\n+returns a block of random <code class=\"code docutils literal notranslate\"><span class=\"pre\">float32</span></code> in <span class=\"math notranslate nohighlight\">\\(\\mathcal{N}(0, 1)\\)</span>.</p>\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Parameters</dt>\n <dd class=\"field-odd\"><ul class=\"simple\">\n@@ -254,17 +254,13 @@ <h1>triton.language.randn<a class=\"headerlink\" href=\"#triton-language-randn\" tit\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.language.randn.html\">master</a></dd>\n+            <dd><a href=\"triton.language.randn.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.ravel.html", "status": "renamed", "additions": 3, "deletions": 7, "changes": 10, "file_content_changes": "@@ -200,7 +200,7 @@ <h1>triton.language.ravel<a class=\"headerlink\" href=\"#triton-language-ravel\" tit\n <dl class=\"py function\">\n <dt class=\"sig sig-object py\" id=\"triton.language.ravel\">\n <span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">ravel</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">x</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.ravel\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Returns a contiguous flattened view of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code></p>\n+<dd><p>Returns a contiguous flattened view of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code>.</p>\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Parameters</dt>\n <dd class=\"field-odd\"><p><strong>x</strong> (<em>Block</em>) \u2013 the input tensor</p>\n@@ -248,17 +248,13 @@ <h1>triton.language.ravel<a class=\"headerlink\" href=\"#triton-language-ravel\" tit\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.language.ravel.html\">master</a></dd>\n+            <dd><a href=\"triton.language.ravel.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.reshape.html", "status": "renamed", "additions": 3, "deletions": 16, "changes": 19, "file_content_changes": "@@ -200,16 +200,7 @@ <h1>triton.language.reshape<a class=\"headerlink\" href=\"#triton-language-reshape\"\n <dl class=\"py function\">\n <dt class=\"sig sig-object py\" id=\"triton.language.reshape\">\n <span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">reshape</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">input</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">shape</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.reshape\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Tries to reshape the given block to a new shape.</p>\n-<dl class=\"field-list simple\">\n-<dt class=\"field-odd\">Parameters</dt>\n-<dd class=\"field-odd\"><ul class=\"simple\">\n-<li><p><strong>input</strong> \u2013 The input block.</p></li>\n-<li><p><strong>shape</strong> (<em>Tuple</em><em>[</em><em>int</em><em>]</em>) \u2013 The desired shape.</p></li>\n-</ul>\n-</dd>\n-</dl>\n-</dd></dl>\n+<dd></dd></dl>\n \n </div>\n \n@@ -251,17 +242,13 @@ <h1>triton.language.reshape<a class=\"headerlink\" href=\"#triton-language-reshape\"\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: v1.1.2\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"triton.language.reshape.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"../../../master/index.html\">master</a></dd>\n+            <dd><a href=\"triton.language.reshape.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.sigmoid.html", "status": "renamed", "additions": 4, "deletions": 7, "changes": 11, "file_content_changes": "@@ -108,6 +108,7 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n <li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a><ul class=\"current\">\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.abs.html\">triton.language.abs</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.exp.html\">triton.language.exp</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.log.html\">triton.language.log</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.cos.html\">triton.language.cos</a></li>\n@@ -204,7 +205,7 @@ <h1>triton.language.sigmoid<a class=\"headerlink\" href=\"#triton-language-sigmoid\"\n <dl class=\"py function\">\n <dt class=\"sig sig-object py\" id=\"triton.language.sigmoid\">\n <span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">sigmoid</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">x</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.sigmoid\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Computes the element-wise sigmoid of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code></p>\n+<dd><p>Computes the element-wise sigmoid of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code>.</p>\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Parameters</dt>\n <dd class=\"field-odd\"><p><strong>x</strong> (<em>Block</em>) \u2013 the input values</p>\n@@ -252,17 +253,13 @@ <h1>triton.language.sigmoid<a class=\"headerlink\" href=\"#triton-language-sigmoid\"\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: v1.1.2\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"triton.language.sigmoid.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"../../../master/index.html\">master</a></dd>\n+            <dd><a href=\"triton.language.sigmoid.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.sin.html", "status": "renamed", "additions": 4, "deletions": 7, "changes": 11, "file_content_changes": "@@ -108,6 +108,7 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n <li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a><ul class=\"current\">\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.abs.html\">triton.language.abs</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.exp.html\">triton.language.exp</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.log.html\">triton.language.log</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.cos.html\">triton.language.cos</a></li>\n@@ -204,7 +205,7 @@ <h1>triton.language.sin<a class=\"headerlink\" href=\"#triton-language-sin\" title=\"\n <dl class=\"py function\">\n <dt class=\"sig sig-object py\" id=\"triton.language.sin\">\n <span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">sin</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">x</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.sin\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Computes the element-wise sine of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code></p>\n+<dd><p>Computes the element-wise sine of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code>.</p>\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Parameters</dt>\n <dd class=\"field-odd\"><p><strong>x</strong> (<em>Block</em>) \u2013 the input values</p>\n@@ -252,17 +253,13 @@ <h1>triton.language.sin<a class=\"headerlink\" href=\"#triton-language-sin\" title=\"\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.language.sin.html\">master</a></dd>\n+            <dd><a href=\"triton.language.sin.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.softmax.html", "status": "renamed", "additions": 5, "deletions": 8, "changes": 13, "file_content_changes": "@@ -108,6 +108,7 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n <li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a><ul class=\"current\">\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.abs.html\">triton.language.abs</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.exp.html\">triton.language.exp</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.log.html\">triton.language.log</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.cos.html\">triton.language.cos</a></li>\n@@ -203,8 +204,8 @@\n <h1>triton.language.softmax<a class=\"headerlink\" href=\"#triton-language-softmax\" title=\"Permalink to this headline\">\u00b6</a></h1>\n <dl class=\"py function\">\n <dt class=\"sig sig-object py\" id=\"triton.language.softmax\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">softmax</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">x</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.softmax\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Computes the element-wise softmax of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code></p>\n+<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">softmax</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">x</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">ieee_rounding</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.softmax\" title=\"Permalink to this definition\">\u00b6</a></dt>\n+<dd><p>Computes the element-wise softmax of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code>.</p>\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Parameters</dt>\n <dd class=\"field-odd\"><p><strong>x</strong> (<em>Block</em>) \u2013 the input values</p>\n@@ -252,17 +253,13 @@ <h1>triton.language.softmax<a class=\"headerlink\" href=\"#triton-language-softmax\"\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: v1.1.2\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"triton.language.softmax.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"../../../master/index.html\">master</a></dd>\n+            <dd><a href=\"triton.language.softmax.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.sqrt.html", "status": "renamed", "additions": 4, "deletions": 7, "changes": 11, "file_content_changes": "@@ -108,6 +108,7 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#memory-ops\">Memory Ops</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"../triton.language.html#indexing-ops\">Indexing Ops</a></li>\n <li class=\"toctree-l2 current\"><a class=\"reference internal\" href=\"../triton.language.html#math-ops\">Math Ops</a><ul class=\"current\">\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.abs.html\">triton.language.abs</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.exp.html\">triton.language.exp</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.log.html\">triton.language.log</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"triton.language.cos.html\">triton.language.cos</a></li>\n@@ -204,7 +205,7 @@ <h1>triton.language.sqrt<a class=\"headerlink\" href=\"#triton-language-sqrt\" title\n <dl class=\"py function\">\n <dt class=\"sig sig-object py\" id=\"triton.language.sqrt\">\n <span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">sqrt</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">x</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.sqrt\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Computes the element-wise square root of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code></p>\n+<dd><p>Computes the element-wise square root of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code>.</p>\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Parameters</dt>\n <dd class=\"field-odd\"><p><strong>x</strong> (<em>Block</em>) \u2013 the input values</p>\n@@ -252,17 +253,13 @@ <h1>triton.language.sqrt<a class=\"headerlink\" href=\"#triton-language-sqrt\" title\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.language.sqrt.html\">master</a></dd>\n+            <dd><a href=\"triton.language.sqrt.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.store.html", "status": "renamed", "additions": 24, "deletions": 12, "changes": 36, "file_content_changes": "@@ -200,15 +200,31 @@\n <h1>triton.language.store<a class=\"headerlink\" href=\"#triton-language-store\" title=\"Permalink to this headline\">\u00b6</a></h1>\n <dl class=\"py function\">\n <dt class=\"sig sig-object py\" id=\"triton.language.store\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">store</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">pointer</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">value</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">mask</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">eviction_policy</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">''</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.store\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Stores <code class=\"code docutils literal notranslate\"><span class=\"pre\">value</span></code> tensor of elements in memory, element-wise, at the memory locations specified by <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer</span></code>.</p>\n-<p><code class=\"code docutils literal notranslate\"><span class=\"pre\">value</span></code> is implicitly broadcast to <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer.shape</span></code> and typecast to <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer.dtype.element_ty</span></code>.</p>\n+<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">store</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">pointer</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">value</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">mask</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">boundary_check</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">()</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">cache_modifier</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">''</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">eviction_policy</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">''</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.store\" title=\"Permalink to this definition\">\u00b6</a></dt>\n+<dd><dl class=\"simple\">\n+<dt>Store a tensor of data into memory locations defined by <cite>pointer</cite>:</dt><dd><ol class=\"arabic simple\">\n+<li><p><cite>pointer</cite> could be a single element pointer, then a scalar will be stored\n+- <cite>mask</cite> must be scalar too\n+- <cite>boundary_check</cite> and <cite>padding_option</cite> must be empty</p></li>\n+<li><p><cite>pointer</cite> could be element-wise tensor of pointers, in which case:\n+- <cite>mask</cite> is implicitly broadcast to <cite>pointer.shape</cite>\n+- <cite>boundary_check</cite> must be empty</p></li>\n+<li><p>or <cite>pointer</cite> could be a block pointer defined by <cite>make_block_ptr</cite>, in which case:\n+- <cite>mask</cite> must be None\n+- <cite>boundary_check</cite> can be specified to control the behavior of out-of-bound access</p></li>\n+</ol>\n+</dd>\n+</dl>\n+<p><cite>value</cite> is implicitly broadcast to <cite>pointer.shape</cite> and typecast to <cite>pointer.dtype.element_ty</cite>.</p>\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Parameters</dt>\n <dd class=\"field-odd\"><ul class=\"simple\">\n-<li><p><strong>pointer</strong> (<em>Block of dtype=triton.PointerDType</em>) \u2013 The memory locations where the elements of <code class=\"code docutils literal notranslate\"><span class=\"pre\">value</span></code> are stored.</p></li>\n-<li><p><strong>value</strong> (<em>Block</em>) \u2013 The tensor of elements to be stored.</p></li>\n-<li><p><strong>mask</strong> (<em>Block of triton.int1</em><em>, </em><em>optional</em>) \u2013 If mask[idx] is false, do not store <code class=\"code docutils literal notranslate\"><span class=\"pre\">value[idx]</span></code> at <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer[idx]</span></code>.</p></li>\n+<li><p><strong>pointer</strong> (<cite>triton.PointerType</cite>, or block of <cite>dtype=triton.PointerType</cite>) \u2013 The memory location where the elements of <cite>value</cite> are stored</p></li>\n+<li><p><strong>value</strong> (<em>Block</em>) \u2013 The tensor of elements to be stored</p></li>\n+<li><p><strong>mask</strong> (<em>Block of triton.int1</em><em>, </em><em>optional</em>) \u2013 If <cite>mask[idx]</cite> is false, do not store <cite>value[idx]</cite> at <cite>pointer[idx]</cite></p></li>\n+<li><p><strong>boundary_check</strong> (<em>tuple of ints</em><em>, </em><em>optional</em>) \u2013 tuple of integers, indicating the dimensions which should do the boundary check</p></li>\n+<li><p><strong>cache_modifier</strong> (<em>str</em><em>, </em><em>optional</em>) \u2013 changes cache option in NVIDIA PTX</p></li>\n+<li><p><strong>eviction_policy</strong> (<em>str</em><em>, </em><em>optional</em>) \u2013 changes eviction policy in NVIDIA PTX</p></li>\n </ul>\n </dd>\n </dl>\n@@ -254,17 +270,13 @@ <h1>triton.language.store<a class=\"headerlink\" href=\"#triton-language-store\" tit\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.language.store.html\">master</a></dd>\n+            <dd><a href=\"triton.language.store.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.sum.html", "status": "renamed", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -251,17 +251,13 @@ <h1>triton.language.sum<a class=\"headerlink\" href=\"#triton-language-sum\" title=\"\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.language.sum.html\">master</a></dd>\n+            <dd><a href=\"triton.language.sum.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.where.html", "status": "renamed", "additions": 6, "deletions": 10, "changes": 16, "file_content_changes": "@@ -46,7 +46,7 @@\n     \n     <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n     <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"triton.language.exp\" href=\"triton.language.exp.html\" />\n+    <link rel=\"next\" title=\"triton.language.abs\" href=\"triton.language.abs.html\" />\n     <link rel=\"prev\" title=\"triton.language.atomic_xchg\" href=\"triton.language.atomic_xchg.html\" /> \n </head>\n \n@@ -200,9 +200,9 @@ <h1>triton.language.where<a class=\"headerlink\" href=\"#triton-language-where\" tit\n <span class=\"sig-prename descclassname\"><span class=\"pre\">triton.language.</span></span><span class=\"sig-name descname\"><span class=\"pre\">where</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">condition</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">x</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">y</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.language.where\" title=\"Permalink to this definition\">\u00b6</a></dt>\n <dd><p>Returns a tensor of elements from either <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code> or <code class=\"code docutils literal notranslate\"><span class=\"pre\">y</span></code>, depending on <code class=\"code docutils literal notranslate\"><span class=\"pre\">condition</span></code>.</p>\n <p>Note that <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code> and <code class=\"code docutils literal notranslate\"><span class=\"pre\">y</span></code> are always evaluated regardless of the value of <code class=\"code docutils literal notranslate\"><span class=\"pre\">condition</span></code>.</p>\n-<p>If you want to avoid unintented memory operations, use the <code class=\"code docutils literal notranslate\"><span class=\"pre\">mask</span></code> arguments in <cite>triton.load</cite> and <cite>triton.store</cite> instead.</p>\n+<p>If you want to avoid unintended memory operations, use the <code class=\"code docutils literal notranslate\"><span class=\"pre\">mask</span></code> arguments in <cite>triton.load</cite> and <cite>triton.store</cite> instead.</p>\n <p>The shape of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code> and <code class=\"code docutils literal notranslate\"><span class=\"pre\">y</span></code> are both broadcast to the shape of <code class=\"code docutils literal notranslate\"><span class=\"pre\">condition</span></code>.\n-<code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code> and <code class=\"code docutils literal notranslate\"><span class=\"pre\">y</span></code> must have the data type.</p>\n+<code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code> and <code class=\"code docutils literal notranslate\"><span class=\"pre\">y</span></code> must have the same data type.</p>\n <dl class=\"field-list simple\">\n <dt class=\"field-odd\">Parameters</dt>\n <dd class=\"field-odd\"><ul class=\"simple\">\n@@ -222,7 +222,7 @@ <h1>triton.language.where<a class=\"headerlink\" href=\"#triton-language-where\" tit\n           </div>\n           <footer>\n     <div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"footer navigation\">\n-        <a href=\"triton.language.exp.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.exp\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+        <a href=\"triton.language.abs.html\" class=\"btn btn-neutral float-right\" title=\"triton.language.abs\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n         <a href=\"triton.language.atomic_xchg.html\" class=\"btn btn-neutral float-left\" title=\"triton.language.atomic_xchg\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n     </div>\n \n@@ -254,17 +254,13 @@ <h1>triton.language.where<a class=\"headerlink\" href=\"#triton-language-where\" tit\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.language.where.html\">master</a></dd>\n+            <dd><a href=\"triton.language.where.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.language.zeros.html", "status": "renamed", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -250,17 +250,13 @@ <h1>triton.language.zeros<a class=\"headerlink\" href=\"#triton-language-zeros\" tit\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.language.zeros.html\">master</a></dd>\n+            <dd><a href=\"triton.language.zeros.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.testing.Benchmark.html", "status": "renamed", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -264,17 +264,13 @@ <h1>triton.testing.Benchmark<a class=\"headerlink\" href=\"#triton-testing-benchmar\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.testing.Benchmark.html\">master</a></dd>\n+            <dd><a href=\"triton.testing.Benchmark.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.testing.do_bench.html", "status": "renamed", "additions": 4, "deletions": 21, "changes": 25, "file_content_changes": "@@ -185,21 +185,8 @@\n <h1>triton.testing.do_bench<a class=\"headerlink\" href=\"#triton-testing-do-bench\" title=\"Permalink to this headline\">\u00b6</a></h1>\n <dl class=\"py function\">\n <dt class=\"sig sig-object py\" id=\"triton.testing.do_bench\">\n-<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.testing.</span></span><span class=\"sig-name descname\"><span class=\"pre\">do_bench</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">fn</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">warmup</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">25</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">rep</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">100</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">grad_to_none</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">percentiles</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">[0.5,</span> <span class=\"pre\">0.2,</span> <span class=\"pre\">0.8]</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">record_clocks</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">False</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.testing.do_bench\" title=\"Permalink to this definition\">\u00b6</a></dt>\n-<dd><p>Benchmark the runtime of the provided function. By default, return the median runtime of <code class=\"code docutils literal notranslate\"><span class=\"pre\">fn</span></code> along with\n-the 20-th and 80-th performance percentile.</p>\n-<dl class=\"field-list simple\">\n-<dt class=\"field-odd\">Parameters</dt>\n-<dd class=\"field-odd\"><ul class=\"simple\">\n-<li><p><strong>fn</strong> (<em>Callable</em>) \u2013 Function to benchmark</p></li>\n-<li><p><strong>warmup</strong> (<em>int</em>) \u2013 Warmup time (in ms)</p></li>\n-<li><p><strong>rep</strong> (<em>int</em>) \u2013 Repetition time (in ms)</p></li>\n-<li><p><strong>grad_to_none</strong> (<em>torch.tensor</em><em>, </em><em>optional</em>) \u2013 Reset the gradient of the provided tensor to None</p></li>\n-<li><p><strong>percentiles</strong> (<em>list</em><em>[</em><em>float</em><em>]</em>) \u2013 Performance percentile to return in addition to the median.</p></li>\n-</ul>\n-</dd>\n-</dl>\n-</dd></dl>\n+<span class=\"sig-prename descclassname\"><span class=\"pre\">triton.testing.</span></span><span class=\"sig-name descname\"><span class=\"pre\">do_bench</span></span><span class=\"sig-paren\">(</span><em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">fn</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">warmup</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">25</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">rep</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">100</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">grad_to_none</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">quantiles</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">None</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">fast_flush</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">True</span></span></em>, <em class=\"sig-param\"><span class=\"n\"><span class=\"pre\">return_mode</span></span><span class=\"o\"><span class=\"pre\">=</span></span><span class=\"default_value\"><span class=\"pre\">'mean'</span></span></em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#triton.testing.do_bench\" title=\"Permalink to this definition\">\u00b6</a></dt>\n+<dd></dd></dl>\n \n </div>\n \n@@ -241,17 +228,13 @@ <h1>triton.testing.do_bench<a class=\"headerlink\" href=\"#triton-testing-do-bench\"\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.testing.do_bench.html\">master</a></dd>\n+            <dd><a href=\"triton.testing.do_bench.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/generated/triton.testing.perf_report.html", "status": "renamed", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -234,17 +234,13 @@ <h1>triton.testing.perf_report<a class=\"headerlink\" href=\"#triton-testing-perf-r\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.testing.perf_report.html\">master</a></dd>\n+            <dd><a href=\"triton.testing.perf_report.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/triton.html", "status": "renamed", "additions": 7, "deletions": 11, "changes": 18, "file_content_changes": "@@ -47,7 +47,7 @@\n     <link rel=\"index\" title=\"Index\" href=\"../genindex.html\" />\n     <link rel=\"search\" title=\"Search\" href=\"../search.html\" />\n     <link rel=\"next\" title=\"triton.jit\" href=\"generated/triton.jit.html\" />\n-    <link rel=\"prev\" title=\"Layer Normalization\" href=\"../getting-started/tutorials/05-layer-norm.html\" /> \n+    <link rel=\"prev\" title=\"Libdevice (tl.math) function\" href=\"../getting-started/tutorials/07-math-functions.html\" /> \n </head>\n \n <body class=\"wy-body-for-nav\">\n@@ -192,13 +192,13 @@ <h1>triton<a class=\"headerlink\" href=\"#triton\" title=\"Permalink to this headline\n <td><p>Decorator for JIT-compiling a function using the Triton compiler.</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"generated/triton.autotune.html#triton.autotune\" title=\"triton.autotune\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">autotune</span></code></a></p></td>\n-<td><p>Decorator for auto-tuning a <code class=\"code docutils literal notranslate\"><span class=\"pre\">triton.jit</span></code>\u2019d function.</p></td>\n+<td><p>Decorator for auto-tuning a <code class=\"code docutils literal notranslate\"><span class=\"pre\">triton.jit</span></code>\u2019d function. .. highlight:: python .. code-block:: python     &#64;triton.autotune(configs=[         triton.Config(meta={\u2018BLOCK_SIZE\u2019: 128}, num_warps=4),         triton.Config(meta={\u2018BLOCK_SIZE\u2019: 1024}, num_warps=8),       ],       key=[\u2018x_size\u2019] # the two above configs will be evaluated anytime                      # the value of x_size changes     )     &#64;triton.jit     def kernel(x_ptr, x_size, <a href=\"#id1\"><span class=\"problematic\" id=\"id2\">**</span></a>META):         BLOCK_SIZE = META[\u2018BLOCK_SIZE\u2019] :note: When all the configurations are evaluated, the kernel will run multiple times. This means that whatever value the kernel updates will be updated multiple times. To avoid this undesired behavior, you can use the <cite>reset_to_zero</cite> argument, which        resets the value of the provided tensor to <cite>zero</cite> before running any configuration. :param configs: a list of <code class=\"code docutils literal notranslate\"><span class=\"pre\">triton.Config</span></code> objects :type configs: list[triton.Config] :param key: a list of argument names whose change in value will trigger the evaluation of all provided configs. :type key: list[str] :param prune_configs_by: a dict of functions that are used to prune configs, fields:     \u2018perf_model\u2019: performance model used to predicate running time with different configs, returns running time     \u2018top_k\u2019: number of configs to bench     \u2018early_config_prune\u2019(optional): a function used to do early prune (eg, num_stages). It takes configs:List[Config] as its input, and returns pruned configs. :param reset_to_zero: a list of argument names whose value will be reset to zero before evaluating any configs. :type reset_to_zero: list[str].</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.heuristics.html#triton.heuristics\" title=\"triton.heuristics\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">heuristics</span></code></a></p></td>\n-<td><p>Decorator for specifying how the values of certain meta-parameters may be computed.</p></td>\n+<td><p>Decorator for specifying how the values of certain meta-parameters may be computed. This is useful for cases where auto-tuning is prohibitevely expensive, or just not applicable. .. highlight:: python .. code-block:: python     &#64;triton.heuristics(values={\u2018BLOCK_SIZE\u2019: lambda args: 2 ** int(math.ceil(math.log2(args[1])))})     &#64;triton.jit     def kernel(x_ptr, x_size, <a href=\"#id3\"><span class=\"problematic\" id=\"id4\">**</span></a>META):         BLOCK_SIZE = META[\u2018BLOCK_SIZE\u2019] # smallest power-of-two &gt;= x_size :param values: a dictionary of meta-parameter names and functions that compute the value of the meta-parameter. each such function takes a list of positional arguments as input. :type values: dict[str, Callable[[list[Any]], Any]].</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"generated/triton.Config.html#triton.Config\" title=\"triton.Config\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">Config</span></code></a></p></td>\n-<td><p>An object that represents a possible kernel configuration for the auto-tuner to try.</p></td>\n+<td><p>An object that represents a possible kernel configuration for the auto-tuner to try. :ivar meta: a dictionary of meta-parameters to pass to the kernel as keyword arguments. :type meta: dict[Str, Any] :ivar num_warps: the number of warps to use for the kernel when compiled for GPUs. For example, if                   <cite>num_warps=8</cite>, then each kernel instance will be automatically parallelized to                   cooperatively execute using <cite>8 * 32 = 256</cite> threads. :type num_warps: int :ivar num_stages: the number of stages that the compiler should use when software-pipelining loops. Mostly useful for matrix multiplication workloads on SM80+ GPUs. :type num_stages: int :ivar pre_hook: a function that will be called before the kernel is called. Parameters of this                 function are args.</p></td>\n </tr>\n </tbody>\n </table>\n@@ -211,7 +211,7 @@ <h1>triton<a class=\"headerlink\" href=\"#triton\" title=\"Permalink to this headline\n           <footer>\n     <div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"footer navigation\">\n         <a href=\"generated/triton.jit.html\" class=\"btn btn-neutral float-right\" title=\"triton.jit\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n-        <a href=\"../getting-started/tutorials/05-layer-norm.html\" class=\"btn btn-neutral float-left\" title=\"Layer Normalization\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"../getting-started/tutorials/07-math-functions.html\" class=\"btn btn-neutral float-left\" title=\"Libdevice (tl.math) function\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n     </div>\n \n   <hr/>\n@@ -242,17 +242,13 @@ <h1>triton<a class=\"headerlink\" href=\"#triton\" title=\"Permalink to this headline\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: v1.1.2\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"triton.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"../../master/python-api/triton.html\">master</a></dd>\n+            <dd><a href=\"triton.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/triton.language.html", "status": "renamed", "additions": 33, "deletions": 33, "changes": 66, "file_content_changes": "@@ -134,6 +134,7 @@\n </ul>\n </li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"#math-ops\">Math Ops</a><ul>\n+<li class=\"toctree-l3\"><a class=\"reference internal\" href=\"generated/triton.language.abs.html\">triton.language.abs</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"generated/triton.language.exp.html\">triton.language.exp</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"generated/triton.language.log.html\">triton.language.log</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"generated/triton.language.cos.html\">triton.language.cos</a></li>\n@@ -275,10 +276,10 @@ <h2>Creation Ops<a class=\"headerlink\" href=\"#creation-ops\" title=\"Permalink to t\n </colgroup>\n <tbody>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.arange.html#triton.language.arange\" title=\"triton.language.arange\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">arange</span></code></a></p></td>\n-<td><p>Returns contiguous values within the open interval [<code class=\"code docutils literal notranslate\"><span class=\"pre\">start</span></code>, <code class=\"code docutils literal notranslate\"><span class=\"pre\">end</span></code>).</p></td>\n+<td><p>Returns contiguous values within the left-closed and right-open interval [<code class=\"code docutils literal notranslate\"><span class=\"pre\">start</span></code>, <code class=\"code docutils literal notranslate\"><span class=\"pre\">end</span></code>).</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.zeros.html#triton.language.zeros\" title=\"triton.language.zeros\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">zeros</span></code></a></p></td>\n-<td><p>Returns a block filled with the scalar value 0 for the given <code class=\"code docutils literal notranslate\"><span class=\"pre\">shape</span></code> and <code class=\"code docutils literal notranslate\"><span class=\"pre\">dtype</span></code>.</p></td>\n+<td><p>Returns a tensor filled with the scalar value 0 for the given <code class=\"code docutils literal notranslate\"><span class=\"pre\">shape</span></code> and <code class=\"code docutils literal notranslate\"><span class=\"pre\">dtype</span></code>.</p></td>\n </tr>\n </tbody>\n </table>\n@@ -292,13 +293,13 @@ <h2>Shape Manipulation Ops<a class=\"headerlink\" href=\"#shape-manipulation-ops\" t\n </colgroup>\n <tbody>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.broadcast_to.html#triton.language.broadcast_to\" title=\"triton.language.broadcast_to\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">broadcast_to</span></code></a></p></td>\n-<td><p>Tries to broadcast the given block to a new <code class=\"code docutils literal notranslate\"><span class=\"pre\">shape</span></code>.</p></td>\n+<td><p>Tries to broadcast the given tensor to a new <code class=\"code docutils literal notranslate\"><span class=\"pre\">shape</span></code>.</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.reshape.html#triton.language.reshape\" title=\"triton.language.reshape\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">reshape</span></code></a></p></td>\n-<td><p>Tries to reshape the given block to a new shape.</p></td>\n+<td><p></p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.ravel.html#triton.language.ravel\" title=\"triton.language.ravel\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">ravel</span></code></a></p></td>\n-<td><p>Returns a contiguous flattened view of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code></p></td>\n+<td><p>Returns a contiguous flattened view of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code>.</p></td>\n </tr>\n </tbody>\n </table>\n@@ -326,10 +327,10 @@ <h2>Memory Ops<a class=\"headerlink\" href=\"#memory-ops\" title=\"Permalink to this\n </colgroup>\n <tbody>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.load.html#triton.language.load\" title=\"triton.language.load\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">load</span></code></a></p></td>\n-<td><p>Return a block of data whose values are, elementwise, loaded from memory at location defined by <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer</span></code>.</p></td>\n+<td><p>Return a tensor of data whose values are loaded from memory at location defined by <cite>pointer</cite>:</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.store.html#triton.language.store\" title=\"triton.language.store\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">store</span></code></a></p></td>\n-<td><p>Stores <code class=\"code docutils literal notranslate\"><span class=\"pre\">value</span></code> block of elements in memory, element-wise, at the memory locations specified by <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer</span></code>.</p></td>\n+<td><p>Store a tensor of data into memory locations defined by <cite>pointer</cite>:</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.atomic_cas.html#triton.language.atomic_cas\" title=\"triton.language.atomic_cas\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">atomic_cas</span></code></a></p></td>\n <td><p>Performs an atomic compare-and-swap at the memory location specified by <code class=\"code docutils literal notranslate\"><span class=\"pre\">pointer</span></code>.</p></td>\n@@ -349,7 +350,7 @@ <h2>Indexing Ops<a class=\"headerlink\" href=\"#indexing-ops\" title=\"Permalink to t\n </colgroup>\n <tbody>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.where.html#triton.language.where\" title=\"triton.language.where\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">where</span></code></a></p></td>\n-<td><p>Returns a block of elements from either <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code> or <code class=\"code docutils literal notranslate\"><span class=\"pre\">y</span></code>, depending on <code class=\"code docutils literal notranslate\"><span class=\"pre\">condition</span></code>.</p></td>\n+<td><p>Returns a tensor of elements from either <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code> or <code class=\"code docutils literal notranslate\"><span class=\"pre\">y</span></code>, depending on <code class=\"code docutils literal notranslate\"><span class=\"pre\">condition</span></code>.</p></td>\n </tr>\n </tbody>\n </table>\n@@ -362,26 +363,29 @@ <h2>Math Ops<a class=\"headerlink\" href=\"#math-ops\" title=\"Permalink to this head\n <col style=\"width: 90%\" />\n </colgroup>\n <tbody>\n-<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.exp.html#triton.language.exp\" title=\"triton.language.exp\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">exp</span></code></a></p></td>\n-<td><p>Computes the element-wise exponential of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code></p></td>\n+<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.abs.html#triton.language.abs\" title=\"triton.language.abs\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">abs</span></code></a></p></td>\n+<td><p>Computes the element-wise absolute value of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code>.</p></td>\n </tr>\n-<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.log.html#triton.language.log\" title=\"triton.language.log\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">log</span></code></a></p></td>\n-<td><p>Computes the element-wise natural logarithm of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code></p></td>\n+<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.exp.html#triton.language.exp\" title=\"triton.language.exp\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">exp</span></code></a></p></td>\n+<td><p>Computes the element-wise exponential of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code>.</p></td>\n </tr>\n-<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.cos.html#triton.language.cos\" title=\"triton.language.cos\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">cos</span></code></a></p></td>\n-<td><p>Computes the element-wise cosine of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code></p></td>\n+<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.log.html#triton.language.log\" title=\"triton.language.log\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">log</span></code></a></p></td>\n+<td><p>Computes the element-wise natural logarithm of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code>.</p></td>\n </tr>\n-<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.sin.html#triton.language.sin\" title=\"triton.language.sin\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">sin</span></code></a></p></td>\n-<td><p>Computes the element-wise sine of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code></p></td>\n+<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.cos.html#triton.language.cos\" title=\"triton.language.cos\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">cos</span></code></a></p></td>\n+<td><p>Computes the element-wise cosine of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code>.</p></td>\n </tr>\n-<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.sqrt.html#triton.language.sqrt\" title=\"triton.language.sqrt\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">sqrt</span></code></a></p></td>\n-<td><p>Computes the element-wise square root of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code></p></td>\n+<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.sin.html#triton.language.sin\" title=\"triton.language.sin\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">sin</span></code></a></p></td>\n+<td><p>Computes the element-wise sine of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code>.</p></td>\n </tr>\n-<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.sigmoid.html#triton.language.sigmoid\" title=\"triton.language.sigmoid\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">sigmoid</span></code></a></p></td>\n-<td><p>Computes the element-wise sigmoid of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code></p></td>\n+<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.sqrt.html#triton.language.sqrt\" title=\"triton.language.sqrt\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">sqrt</span></code></a></p></td>\n+<td><p>Computes the element-wise square root of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code>.</p></td>\n </tr>\n-<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.softmax.html#triton.language.softmax\" title=\"triton.language.softmax\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">softmax</span></code></a></p></td>\n-<td><p>Computes the element-wise softmax of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code></p></td>\n+<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.sigmoid.html#triton.language.sigmoid\" title=\"triton.language.sigmoid\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">sigmoid</span></code></a></p></td>\n+<td><p>Computes the element-wise sigmoid of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code>.</p></td>\n+</tr>\n+<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.softmax.html#triton.language.softmax\" title=\"triton.language.softmax\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">softmax</span></code></a></p></td>\n+<td><p>Computes the element-wise softmax of <code class=\"code docutils literal notranslate\"><span class=\"pre\">x</span></code>.</p></td>\n </tr>\n </tbody>\n </table>\n@@ -395,13 +399,13 @@ <h2>Reduction Ops<a class=\"headerlink\" href=\"#reduction-ops\" title=\"Permalink to\n </colgroup>\n <tbody>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.max.html#triton.language.max\" title=\"triton.language.max\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">max</span></code></a></p></td>\n-<td><p>Returns the maximum of all elements in the <code class=\"code docutils literal notranslate\"><span class=\"pre\">input</span></code> block along the provided <code class=\"code docutils literal notranslate\"><span class=\"pre\">axis</span></code></p></td>\n+<td><p>Returns the maximum of all elements in the <code class=\"code docutils literal notranslate\"><span class=\"pre\">input</span></code> tensor along the provided <code class=\"code docutils literal notranslate\"><span class=\"pre\">axis</span></code></p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.min.html#triton.language.min\" title=\"triton.language.min\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">min</span></code></a></p></td>\n-<td><p>Returns the minimum of all elements in the <code class=\"code docutils literal notranslate\"><span class=\"pre\">input</span></code> block along the provided <code class=\"code docutils literal notranslate\"><span class=\"pre\">axis</span></code></p></td>\n+<td><p>Returns the minimum of all elements in the <code class=\"code docutils literal notranslate\"><span class=\"pre\">input</span></code> tensor along the provided <code class=\"code docutils literal notranslate\"><span class=\"pre\">axis</span></code></p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.sum.html#triton.language.sum\" title=\"triton.language.sum\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">sum</span></code></a></p></td>\n-<td><p>Returns the sum of all elements in the <code class=\"code docutils literal notranslate\"><span class=\"pre\">input</span></code> block along the provided <code class=\"code docutils literal notranslate\"><span class=\"pre\">axis</span></code></p></td>\n+<td><p>Returns the sum of all elements in the <code class=\"code docutils literal notranslate\"><span class=\"pre\">input</span></code> tensor along the provided <code class=\"code docutils literal notranslate\"><span class=\"pre\">axis</span></code></p></td>\n </tr>\n </tbody>\n </table>\n@@ -461,10 +465,10 @@ <h2>Comparison ops<a class=\"headerlink\" href=\"#comparison-ops\" title=\"Permalink\n <td><p>Given a <code class=\"code docutils literal notranslate\"><span class=\"pre\">seed</span></code> scalar and an <code class=\"code docutils literal notranslate\"><span class=\"pre\">offset</span></code> block, returns a single block of random <code class=\"code docutils literal notranslate\"><span class=\"pre\">int32</span></code>.</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.rand.html#triton.language.rand\" title=\"triton.language.rand\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">rand</span></code></a></p></td>\n-<td><p>Given a <code class=\"code docutils literal notranslate\"><span class=\"pre\">seed</span></code> scalar and an <code class=\"code docutils literal notranslate\"><span class=\"pre\">offset</span></code> block, returns a block of random <code class=\"code docutils literal notranslate\"><span class=\"pre\">float32</span></code> in <span class=\"math notranslate nohighlight\">\\(U(0, 1)\\)</span></p></td>\n+<td><p>Given a <code class=\"code docutils literal notranslate\"><span class=\"pre\">seed</span></code> scalar and an <code class=\"code docutils literal notranslate\"><span class=\"pre\">offset</span></code> block, returns a block of random <code class=\"code docutils literal notranslate\"><span class=\"pre\">float32</span></code> in <span class=\"math notranslate nohighlight\">\\(U(0, 1)\\)</span>.</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"generated/triton.language.randn.html#triton.language.randn\" title=\"triton.language.randn\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">randn</span></code></a></p></td>\n-<td><p>Given a <code class=\"code docutils literal notranslate\"><span class=\"pre\">seed</span></code> scalar and an <code class=\"code docutils literal notranslate\"><span class=\"pre\">offset</span></code> block, returns a block of random <code class=\"code docutils literal notranslate\"><span class=\"pre\">float32</span></code> in <span class=\"math notranslate nohighlight\">\\(\\mathcal{N}(0, 1)\\)</span></p></td>\n+<td><p>Given a <code class=\"code docutils literal notranslate\"><span class=\"pre\">seed</span></code> scalar and an <code class=\"code docutils literal notranslate\"><span class=\"pre\">offset</span></code> block, returns a block of random <code class=\"code docutils literal notranslate\"><span class=\"pre\">float32</span></code> in <span class=\"math notranslate nohighlight\">\\(\\mathcal{N}(0, 1)\\)</span>.</p></td>\n </tr>\n </tbody>\n </table>\n@@ -523,17 +527,13 @@ <h2>Compiler Hint Ops<a class=\"headerlink\" href=\"#compiler-hint-ops\" title=\"Perm\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: v1.1.2\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"triton.language.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"../../master/python-api/triton.language.html\">master</a></dd>\n+            <dd><a href=\"triton.language.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/python-api/triton.testing.html", "status": "renamed", "additions": 3, "deletions": 7, "changes": 10, "file_content_changes": "@@ -188,7 +188,7 @@ <h1>triton.testing<a class=\"headerlink\" href=\"#triton-testing\" title=\"Permalink\n </colgroup>\n <tbody>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"generated/triton.testing.do_bench.html#triton.testing.do_bench\" title=\"triton.testing.do_bench\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">do_bench</span></code></a></p></td>\n-<td><p>Benchmark the runtime of the provided function.</p></td>\n+<td><p></p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"generated/triton.testing.Benchmark.html#triton.testing.Benchmark\" title=\"triton.testing.Benchmark\"><code class=\"xref py py-obj docutils literal notranslate\"><span class=\"pre\">Benchmark</span></code></a></p></td>\n <td><p>This class is used by the <code class=\"code docutils literal notranslate\"><span class=\"pre\">perf_report</span></code> function to generate line plots with a concise API.</p></td>\n@@ -238,17 +238,13 @@ <h1>triton.testing<a class=\"headerlink\" href=\"#triton-testing\" title=\"Permalink\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../../v1.1.2/python-api/triton.testing.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"triton.testing.html\">master</a></dd>\n+            <dd><a href=\"triton.testing.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/search.html", "status": "renamed", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -217,17 +217,13 @@\n <div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n     <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n         <span class=\"fa fa-book\"> Other Versions</span>\n-        v: master\n+        v: keren/fix-doc\n         <span class=\"fa fa-caret-down\"></span>\n     </span>\n     <div class=\"rst-other-versions\">\n-        <dl>\n-            <dt>Tags</dt>\n-            <dd><a href=\"../v1.1.2/index.html\">v1.1.2</a></dd>\n-        </dl>\n         <dl>\n             <dt>Branches</dt>\n-            <dd><a href=\"search.html\">master</a></dd>\n+            <dd><a href=\"search.html\">keren/fix-doc</a></dd>\n         </dl>\n     </div>\n </div>"}, {"filename": "keren/fix-doc/searchindex.js", "status": "added", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -0,0 +1 @@\n+Search.setIndex({docnames:[\"getting-started/installation\",\"getting-started/tutorials/01-vector-add\",\"getting-started/tutorials/02-fused-softmax\",\"getting-started/tutorials/03-matrix-multiplication\",\"getting-started/tutorials/04-low-memory-dropout\",\"getting-started/tutorials/05-layer-norm\",\"getting-started/tutorials/07-math-functions\",\"getting-started/tutorials/index\",\"getting-started/tutorials/sg_execution_times\",\"index\",\"programming-guide/chapter-1/introduction\",\"programming-guide/chapter-2/related-work\",\"python-api/generated/triton.Config\",\"python-api/generated/triton.autotune\",\"python-api/generated/triton.heuristics\",\"python-api/generated/triton.jit\",\"python-api/generated/triton.language.abs\",\"python-api/generated/triton.language.arange\",\"python-api/generated/triton.language.atomic_add\",\"python-api/generated/triton.language.atomic_cas\",\"python-api/generated/triton.language.atomic_max\",\"python-api/generated/triton.language.atomic_min\",\"python-api/generated/triton.language.atomic_xchg\",\"python-api/generated/triton.language.broadcast_to\",\"python-api/generated/triton.language.cos\",\"python-api/generated/triton.language.dot\",\"python-api/generated/triton.language.exp\",\"python-api/generated/triton.language.load\",\"python-api/generated/triton.language.log\",\"python-api/generated/triton.language.max\",\"python-api/generated/triton.language.maximum\",\"python-api/generated/triton.language.min\",\"python-api/generated/triton.language.minimum\",\"python-api/generated/triton.language.multiple_of\",\"python-api/generated/triton.language.num_programs\",\"python-api/generated/triton.language.program_id\",\"python-api/generated/triton.language.rand\",\"python-api/generated/triton.language.randint\",\"python-api/generated/triton.language.randint4x\",\"python-api/generated/triton.language.randn\",\"python-api/generated/triton.language.ravel\",\"python-api/generated/triton.language.reshape\",\"python-api/generated/triton.language.sigmoid\",\"python-api/generated/triton.language.sin\",\"python-api/generated/triton.language.softmax\",\"python-api/generated/triton.language.sqrt\",\"python-api/generated/triton.language.store\",\"python-api/generated/triton.language.sum\",\"python-api/generated/triton.language.where\",\"python-api/generated/triton.language.zeros\",\"python-api/generated/triton.testing.Benchmark\",\"python-api/generated/triton.testing.do_bench\",\"python-api/generated/triton.testing.perf_report\",\"python-api/triton\",\"python-api/triton.language\",\"python-api/triton.testing\"],envversion:{\"sphinx.domains.c\":2,\"sphinx.domains.changeset\":1,\"sphinx.domains.citation\":1,\"sphinx.domains.cpp\":4,\"sphinx.domains.index\":1,\"sphinx.domains.javascript\":2,\"sphinx.domains.math\":2,\"sphinx.domains.python\":3,\"sphinx.domains.rst\":2,\"sphinx.domains.std\":2,\"sphinx.ext.intersphinx\":1,sphinx:56},filenames:[\"getting-started/installation.rst\",\"getting-started/tutorials/01-vector-add.rst\",\"getting-started/tutorials/02-fused-softmax.rst\",\"getting-started/tutorials/03-matrix-multiplication.rst\",\"getting-started/tutorials/04-low-memory-dropout.rst\",\"getting-started/tutorials/05-layer-norm.rst\",\"getting-started/tutorials/07-math-functions.rst\",\"getting-started/tutorials/index.rst\",\"getting-started/tutorials/sg_execution_times.rst\",\"index.rst\",\"programming-guide/chapter-1/introduction.rst\",\"programming-guide/chapter-2/related-work.rst\",\"python-api/generated/triton.Config.rst\",\"python-api/generated/triton.autotune.rst\",\"python-api/generated/triton.heuristics.rst\",\"python-api/generated/triton.jit.rst\",\"python-api/generated/triton.language.abs.rst\",\"python-api/generated/triton.language.arange.rst\",\"python-api/generated/triton.language.atomic_add.rst\",\"python-api/generated/triton.language.atomic_cas.rst\",\"python-api/generated/triton.language.atomic_max.rst\",\"python-api/generated/triton.language.atomic_min.rst\",\"python-api/generated/triton.language.atomic_xchg.rst\",\"python-api/generated/triton.language.broadcast_to.rst\",\"python-api/generated/triton.language.cos.rst\",\"python-api/generated/triton.language.dot.rst\",\"python-api/generated/triton.language.exp.rst\",\"python-api/generated/triton.language.load.rst\",\"python-api/generated/triton.language.log.rst\",\"python-api/generated/triton.language.max.rst\",\"python-api/generated/triton.language.maximum.rst\",\"python-api/generated/triton.language.min.rst\",\"python-api/generated/triton.language.minimum.rst\",\"python-api/generated/triton.language.multiple_of.rst\",\"python-api/generated/triton.language.num_programs.rst\",\"python-api/generated/triton.language.program_id.rst\",\"python-api/generated/triton.language.rand.rst\",\"python-api/generated/triton.language.randint.rst\",\"python-api/generated/triton.language.randint4x.rst\",\"python-api/generated/triton.language.randn.rst\",\"python-api/generated/triton.language.ravel.rst\",\"python-api/generated/triton.language.reshape.rst\",\"python-api/generated/triton.language.sigmoid.rst\",\"python-api/generated/triton.language.sin.rst\",\"python-api/generated/triton.language.softmax.rst\",\"python-api/generated/triton.language.sqrt.rst\",\"python-api/generated/triton.language.store.rst\",\"python-api/generated/triton.language.sum.rst\",\"python-api/generated/triton.language.where.rst\",\"python-api/generated/triton.language.zeros.rst\",\"python-api/generated/triton.testing.Benchmark.rst\",\"python-api/generated/triton.testing.do_bench.rst\",\"python-api/generated/triton.testing.perf_report.rst\",\"python-api/triton.rst\",\"python-api/triton.language.rst\",\"python-api/triton.testing.rst\"],objects:{\"triton.Config\":{__init__:[12,1,1,\"\"]},\"triton.language\":{abs:[16,2,1,\"\"],arange:[17,2,1,\"\"],atomic_add:[18,2,1,\"\"],atomic_cas:[19,2,1,\"\"],atomic_max:[20,2,1,\"\"],atomic_min:[21,2,1,\"\"],atomic_xchg:[22,2,1,\"\"],broadcast_to:[23,2,1,\"\"],cos:[24,2,1,\"\"],dot:[25,2,1,\"\"],exp:[26,2,1,\"\"],load:[27,2,1,\"\"],log:[28,2,1,\"\"],max:[29,2,1,\"\"],maximum:[30,2,1,\"\"],min:[31,2,1,\"\"],minimum:[32,2,1,\"\"],multiple_of:[33,2,1,\"\"],num_programs:[34,2,1,\"\"],program_id:[35,2,1,\"\"],rand:[36,2,1,\"\"],randint4x:[38,2,1,\"\"],randint:[37,2,1,\"\"],randn:[39,2,1,\"\"],ravel:[40,2,1,\"\"],reshape:[41,2,1,\"\"],sigmoid:[42,2,1,\"\"],sin:[43,2,1,\"\"],softmax:[44,2,1,\"\"],sqrt:[45,2,1,\"\"],store:[46,2,1,\"\"],sum:[47,2,1,\"\"],where:[48,2,1,\"\"],zeros:[49,2,1,\"\"]},\"triton.testing\":{Benchmark:[50,0,1,\"\"],do_bench:[51,2,1,\"\"],perf_report:[52,2,1,\"\"]},\"triton.testing.Benchmark\":{__init__:[50,1,1,\"\"]},triton:{Config:[12,0,1,\"\"],autotune:[13,2,1,\"\"],heuristics:[14,2,1,\"\"],jit:[15,2,1,\"\"]}},objnames:{\"0\":[\"py\",\"class\",\"Python class\"],\"1\":[\"py\",\"method\",\"Python method\"],\"2\":[\"py\",\"function\",\"Python function\"]},objtypes:{\"0\":\"py:class\",\"1\":\"py:method\",\"2\":\"py:function\"},terms:{\"0\":[1,2,3,4,5,6,8,10,11,34,35,36,39,49],\"00\":8,\"0000\":3,\"000002\":5,\"004273\":1,\"01\":[1,3,8],\"017846\":3,\"02\":[2,8],\"023530\":5,\"0249\":6,\"028160\":5,\"03\":[3,8],\"04\":[4,8],\"040176\":3,\"042029\":5,\"0424\":6,\"043810\":5,\"05\":[5,8],\"050158\":2,\"054757\":3,\"0625\":3,\"07\":[6,8],\"071772\":2,\"08199\":4,\"08452\":4,\"084721\":1,\"088617\":5,\"0938\":3,\"096389\":5,\"097854\":5,\"0f\":11,\"0s\":4,\"1\":[1,2,3,4,5,9,11,14,34,35,36,39],\"10\":[1,3,4,5,6,36,37,38,39],\"100\":[2,51],\"100834\":3,\"1024\":[1,3,4,5,6,13],\"10240\":5,\"1045\":3,\"1048576\":1,\"106434\":4,\"106797\":3,\"10752\":5,\"11\":[0,1,3,5],\"11264\":5,\"1151\":5,\"1152\":3,\"116890\":1,\"11776\":5,\"119\":[3,8],\"119561\":3,\"12\":[1,3,5],\"120288\":3,\"120338\":5,\"12160\":2,\"122458\":5,\"12288\":[2,5],\"123\":4,\"12416\":2,\"12544\":2,\"12672\":2,\"127\":1,\"128\":[1,2,3,5,13],\"1280\":3,\"12800\":5,\"129413\":5,\"13\":[1,3,5],\"131072\":[1,17],\"1328\":3,\"13312\":5,\"133347\":2,\"134217728\":1,\"136292\":5,\"13686\":4,\"137125\":5,\"13824\":5,\"14\":[1,3,5],\"140799\":3,\"1408\":3,\"14336\":5,\"143457\":5,\"143890\":2,\"145894\":5,\"14848\":5,\"149397\":4,\"15\":[1,3,5],\"1536\":[3,5],\"15360\":5,\"153853\":2,\"154642\":3,\"155164\":3,\"15872\":5,\"16\":[2,3,5,11,49],\"16384\":1,\"1664\":3,\"16777216\":1,\"17\":[3,5],\"170222\":5,\"176729\":5,\"17879\":4,\"1792\":3,\"179836\":5,\"18\":[3,5],\"1823\":2,\"185987\":3,\"186282\":3,\"19\":[2,3,5,8],\"190482\":1,\"192\":1,\"1920\":3,\"196756\":1,\"197325\":3,\"1982\":11,\"1983\":10,\"1984\":11,\"1989\":11,\"1991\":[10,11],\"1999\":11,\"1d\":[1,2,3],\"1e\":[1,2,3,5],\"1s\":4,\"2\":[1,2,3,4,5,6,9,11,12,14,34,35],\"20\":[3,5],\"2004\":11,\"2006\":11,\"2011\":4,\"2012\":11,\"2013\":10,\"2014\":[4,10],\"2016\":[5,10,11],\"2017\":10,\"2018\":[10,11],\"201833\":5,\"2019\":11,\"2021\":[10,11],\"204\":2,\"2048\":[2,3,5],\"206879\":2,\"207195\":5,\"2097152\":1,\"21\":[3,5],\"212868\":4,\"2141\":1,\"214186\":4,\"214963\":2,\"2176\":3,\"219\":[1,8],\"22\":[3,5],\"220\":3,\"23\":[3,5],\"2304\":3,\"236\":2,\"24\":[3,5],\"240514\":2,\"240968\":3,\"242422\":2,\"2432\":3,\"245\":3,\"246402\":3,\"248\":2,\"25\":[3,5,51],\"254900\":5,\"256\":[1,2,3,5,12],\"2560\":[3,5],\"257142\":3,\"259\":[2,5],\"26\":[3,5],\"260\":5,\"261\":[2,4,5,8],\"262\":5,\"262144\":1,\"263\":5,\"264\":5,\"265\":5,\"265046\":2,\"2656\":3,\"266\":5,\"266452\":5,\"267\":5,\"267701\":5,\"268\":5,\"2688\":3,\"27\":[3,5],\"271\":5,\"272\":5,\"272725\":5,\"274\":5,\"275\":5,\"276800\":3,\"277\":5,\"278171\":3,\"279\":5,\"28\":[1,3,5],\"2812\":3,\"2816\":3,\"282\":5,\"283\":5,\"285\":5,\"286\":5,\"287\":5,\"288\":5,\"289\":5,\"2891\":3,\"29\":[1,3,5,8],\"290\":5,\"291\":5,\"291486\":5,\"293429\":4,\"293536\":5,\"2944\":3,\"295\":5,\"296\":5,\"298532\":3,\"299\":5,\"2d\":[3,5,25],\"2m\":2,\"2mn\":2,\"3\":[0,1,2,3,4,5,11],\"30\":3,\"301075\":5,\"303\":5,\"3072\":[3,5],\"3076\":1,\"307695\":2,\"308\":5,\"308559\":5,\"31\":[3,8],\"311\":5,\"3125\":3,\"315584\":3,\"317\":5,\"32\":[3,5,12],\"3200\":3,\"321\":5,\"322318\":5,\"324\":2,\"326\":5,\"32768\":1,\"3281\":3,\"328287\":5,\"33\":3,\"3328\":3,\"333321\":1,\"333333\":3,\"33554432\":1,\"336\":5,\"338\":5,\"34\":3,\"341\":1,\"34172\":4,\"3438\":3,\"344\":5,\"3456\":3,\"3477\":3,\"35\":3,\"3516\":3,\"351899\":5,\"3555\":3,\"3584\":[3,5],\"36\":3,\"360\":5,\"360017\":2,\"362\":5,\"362445\":1,\"367\":5,\"37\":3,\"3712\":3,\"3713\":1,\"371721\":4,\"371945\":3,\"372616\":5,\"373\":5,\"38\":1,\"380\":5,\"381246\":3,\"382283\":3,\"384\":[2,3],\"3840\":3,\"384185791015625e\":6,\"384634\":5,\"385224\":3,\"386\":5,\"39\":[3,8],\"3906\":3,\"392\":5,\"3968\":3,\"397\":5,\"3984\":3,\"3986\":4,\"3d\":[34,35],\"3mn\":2,\"4\":[1,2,3,5,11,12,13,37],\"40\":3,\"400001\":1,\"400016\":[1,2],\"4023\":3,\"403344\":4,\"403347\":4,\"404\":5,\"405\":5,\"4062\":3,\"407414\":5,\"408716\":4,\"409\":2,\"4096\":[1,2,3,5],\"41\":3,\"4105\":6,\"412\":5,\"413\":5,\"414\":2,\"415\":2,\"415180\":5,\"41576\":4,\"416\":2,\"417\":5,\"4194304\":1,\"42\":3,\"42142\":4,\"426\":5,\"428372\":4,\"428568\":1,\"429\":5,\"429770\":1,\"431159\":5,\"431969\":4,\"436\":5,\"437943\":3,\"438\":5,\"44\":3,\"441\":5,\"443\":5,\"444\":[6,8],\"446801\":2,\"448255\":1,\"4492\":3,\"45\":3,\"4531\":3,\"455524\":5,\"46\":3,\"4608\":5,\"4609\":3,\"461\":5,\"4688\":3,\"469963\":5,\"47\":3,\"470965\":5,\"472\":1,\"472263\":3,\"473686\":3,\"48\":3,\"482750\":5,\"483048\":3,\"485901\":3,\"49\":3,\"4940\":1,\"4m\":2,\"4x\":2,\"5\":[1,2,3,4,5,11],\"50\":3,\"500\":5,\"5000\":3,\"501215\":2,\"505412\":3,\"51\":3,\"511\":5,\"512\":[2,3,4,5],\"5120\":5,\"518666\":5,\"520296\":5,\"520615\":3,\"523365\":5,\"524288\":1,\"5312\":3,\"5351\":6,\"536223\":2,\"537101\":2,\"537405\":5,\"54\":3,\"541\":4,\"5430\":6,\"543045\":5,\"544136\":5,\"544948\":3,\"546\":2,\"550966\":5,\"551\":5,\"553404\":5,\"558010\":5,\"5632\":5,\"564\":5,\"566925\":3,\"568431\":4,\"5859\":3,\"586858\":4,\"588753\":5,\"5898\":3,\"591490\":5,\"597714\":3,\"599993\":2,\"5mn\":2,\"6\":[0,1,3,5],\"60\":3,\"600000\":1,\"606056\":2,\"6094\":3,\"614\":[1,2],\"6144\":5,\"615390\":1,\"62\":3,\"620\":2,\"623284\":5,\"626218\":3,\"627907\":3,\"630\":2,\"64\":[1,3,5],\"640\":[2,3],\"646\":2,\"646592\":5,\"648483\":3,\"64kb\":5,\"655\":2,\"65536\":[1,5],\"66\":3,\"661699\":5,\"664092\":3,\"6656\":5,\"67\":3,\"67086\":4,\"67108864\":1,\"6724\":1,\"676598\":5,\"679014\":3,\"682053\":3,\"686486\":5,\"688372\":3,\"69\":3,\"691296\":3,\"692308\":3,\"6953\":3,\"698\":[2,8],\"7\":[0,1,3,5,11],\"70\":3,\"7031\":3,\"703544\":2,\"706\":2,\"7070\":3,\"707311\":5,\"707322\":5,\"707878\":4,\"7168\":5,\"719258\":4,\"72\":3,\"721399\":3,\"722\":[1,2],\"725901\":3,\"730681\":2,\"736871\":2,\"743443\":4,\"747539\":5,\"7500\":3,\"754933\":5,\"756215\":5,\"76\":[1,3],\"762727\":5,\"765209\":2,\"767438\":5,\"768\":[2,3],\"7680\":5,\"77\":3,\"772517\":5,\"775181\":5,\"775186\":5,\"778988\":3,\"78\":3,\"780\":1,\"781\":2,\"786517\":3,\"788584\":5,\"79\":3,\"793096\":5,\"797053\":3,\"79719\":4,\"799469\":5,\"8\":[1,2,3,5,11,12,13,49],\"80\":3,\"800002\":1,\"800005\":2,\"800825\":2,\"803457\":3,\"806694\":4,\"808501\":5,\"81\":3,\"811163\":1,\"812\":1,\"812417\":5,\"814\":2,\"8149\":6,\"815\":2,\"815384\":5,\"817432\":4,\"8192\":[1,5],\"82\":3,\"821630\":5,\"823517\":[1,2],\"827590\":5,\"830933\":3,\"833\":1,\"835709\":2,\"838026\":4,\"8388608\":1,\"841749\":5,\"842\":1,\"84284\":4,\"843\":[1,8],\"847\":1,\"848\":1,\"850\":1,\"851\":1,\"853\":[5,8],\"861566\":3,\"862996\":5,\"866949\":3,\"8704\":5,\"879506\":3,\"881181\":5,\"8828\":3,\"8867\":3,\"886725\":3,\"8906\":3,\"8945\":3,\"896\":3,\"896290\":3,\"8mn\":2,\"9\":[0,1,2,3,4,5],\"90\":3,\"900884\":5,\"901492\":5,\"912000\":3,\"9216\":5,\"9219\":3,\"921998\":3,\"928134\":5,\"93\":2,\"932191\":3,\"9375\":3,\"94\":2,\"942092\":5,\"946536\":3,\"9492\":3,\"95\":2,\"951220\":3,\"952835\":4,\"9531\":3,\"953503\":5,\"954908\":5,\"96\":[2,5],\"965515\":5,\"9688\":3,\"97\":2,\"971025\":3,\"9728\":5,\"9733\":1,\"98\":2,\"9805\":3,\"980888\":5,\"982309\":5,\"984084\":2,\"98432\":[1,6],\"9844\":3,\"991195\":5,\"993430\":5,\"999982\":5,\"999995\":1,\"999999\":1,\"abstract\":[10,11],\"break\":11,\"byte\":2,\"case\":[1,2,10,11,14,18,19,20,21,22,27,46],\"class\":[2,5,10,11,12,50],\"do\":[1,2,3,4,5,10,11,13,27,46],\"final\":5,\"float\":[2,6,10,11],\"function\":[1,2,3,4,5,7,8,11,12,13,14,15,50,52],\"import\":[1,2,3,4,5,6,10,11],\"int\":[1,10,11,14,15,23,27,34,35,46,49],\"new\":[23,49],\"return\":[1,2,3,4,5,13,17,18,19,20,21,22,25,27,29,31,34,35,36,37,38,39,40,47,48,49,52],\"static\":[0,10,11],\"super\":3,\"switch\":3,\"true\":[1,2,3,5,25,48,51],\"try\":[3,5,6,12],\"var\":[5,11],\"voil\\u00e0\":4,\"while\":[3,5,10,27],A:[3,4,10,11],And:[0,3],As:[2,3,4,10,11],At:[4,11],But:4,For:[1,3,5,6,10,11,12],If:[4,11,37,46,48,50],In:[1,2,3,4,5,6,11],It:[1,3,4,5,7,9,11,13,15],NOT:5,Of:10,On:11,One:3,The:[1,2,3,4,5,6,10,11,18,19,20,21,22,23,25,34,35,36,37,38,39,46,48,52],There:1,These:[5,11],To:[1,4,5,7,10,11,13],_:5,__expf:2,__init__:[12,50],__nv_asin:6,__nv_asinf:6,__nvasinf:6,_db:5,_dropout:4,_dw:5,_layer_norm_bwd_dwdb:5,_layer_norm_bwd_dx_fus:5,_layer_norm_fwd_fus:5,_matmul:3,_mean:5,_seeded_dropout:4,_var:5,a100:[3,11],a_ptr:3,ab:[1,6],abl:11,about:[1,2,3,4,5,9],abov:[1,2,3,4,11,13],absolut:16,academ:10,acc:[3,10,11],acceler:10,access:[1,3,10,11,15,27,46],accommod:3,accordingli:11,account:11,accumul:[3,5,11],accuraci:[3,10],achiev:[3,10,11],across:[2,4,5,10,11],activ:3,actual:[3,5,10,11],ad:5,add:[1,4,8,18],add_kernel:1,addit:[2,7,8,10],addition:11,address:[10,27],adopt:11,advanc:[2,3,10],advoc:11,affect:3,affin:11,after:[3,5],against:[0,1,2,3,5,9],aggreg:6,aggress:[10,11],agnost:[10,11],ahead:11,aim:[2,9],al:[10,11],alex:4,algebra:11,algorithm:[3,4,10,11],alia:11,all:[2,3,4,5,6,7,10,11,13,29,31,33,47,50],allclos:[2,3,5],allen1984:11,allen:11,alloc:[1,2,3,5,10],allow:[1,2,10,11],allow_tf32:25,along:[1,3,29,31,34,35,47],also:[1,2,3,4,5,6,10,11],altern:4,alwai:[11,48],amd:10,amen:11,amount:[5,10],ampl:11,an:[1,2,3,4,6,10,11,12,18,19,20,21,22,36,37,38,39],analog:1,analysi:[10,11],analyz:11,ancourt1991:11,ancourt:11,ani:[1,2,3,11,12,13,14,50],anoth:[2,5,11],anytim:13,apart:11,apex:5,apex_layer_norm:5,api:50,appear:50,appli:[3,4,5,6,10,11],applic:[4,11,14],approach:[10,11],appropri:1,approxim:2,ar:[0,1,2,3,4,5,10,11,12,13,15,27,33,46,48,50],arang:[1,2,3,4,5,6],arbitrari:3,arc:6,architectur:[3,10],area:11,arg:[1,2,3,5,12,14,50],argument:[1,2,3,12,13,14,15,48,50],arrai:[11,49],arrang:3,art:[10,11],artifici:4,arxiv:[5,10,11],asin_kernel:6,ask:2,aspect:11,asplo:10,assert:[1,2,3,4,5,6],assum:[2,50],asynchron:[1,10],atol:[3,5],atom:[18,19,20,21,22],atomic_ca:5,atomic_xchg:5,attribut:15,auguin1983:10,auguin:10,auto:[2,3,11,12,13,14],autograd:5,autom:10,automat:[2,3,6,10,11,12],autotun:[3,11],avail:[0,4,6,10,11],avoid:[2,5,13,48],awar:10,awkward:4,axi:[1,2,3,4,5,6,29,31,34,35,47,50],b:[3,5,10,11],b_ptr:3,ba2016:5,ba:5,back:[1,2,3,4],backpropag:4,bad:4,baghdadi2021:[10,11],baghdadi:[10,11],balanc:11,bandwidth:2,base:[4,6,9,10,11],basic:[1,7,11],batch:5,bc:6,becom:10,been:[1,10,11],befor:[3,5,12,13,18,19,20,21,22],begin:11,behavior:[11,13,27,46],being:[2,4],believ:11,below:[4,7,11],bench:[0,13],bench_layer_norm:5,benchmark:[0,52],benefit:[2,10,11],best:[1,10],between:[1,6,10],bfloat16:25,bia:5,bias:5,big:5,bit:[4,5],block:[1,2,3,4,10,11,13,14,16,18,19,20,21,22,23,24,25,26,27,28,30,32,36,37,38,39,40,42,43,44,45,46,48],block_siz:[1,2,4,5,6,11,13,14],block_size_k:3,block_size_m:[3,5],block_size_n:[3,5],block_start:[1,4,6],blue:[1,2,3,5],boil:11,bool:[15,27,48,50],both:[6,11,48],bound:[1,2,3,11,27,46],boundari:[27,46],boundary_check:[27,46],branch:11,breviti:5,broad:10,broadcast:[23,27,46,48],buffer:5,build:[0,3],built:[1,11],builtin:15,c1:5,c2:5,c:[3,10,11],c_1:5,c_2:5,c_mask:3,c_ptr:3,cach:[5,10,11,27,46],cache_modifi:[27,46],calcul:6,call:[1,3,6,11,12,15,37],callabl:[1,14,15],can:[0,1,2,3,4,5,6,10,11,13,27,46,52],cannot:[3,10,11],capabl:[9,10],cd:[0,7],cdiv:[1,3,4,5,6],cdot:5,ceil:14,certain:[5,14],cgo:[10,11],challeng:4,chang:[3,4,13,27,46],chapter:9,characterist:11,cheap:10,check:[3,9,27,46],checkpoint:4,chen2018:10,chen:10,chip:2,choic:9,click:[1,2,3,4,5,6],clone:[0,5],close:[11,17],cmake:0,cmp:[18,19,20,21,22],coalesc:10,code:[1,2,3,4,5,6,7,10,11,13,14],col:[3,5,11],col_offset:2,color:[5,50],column:[2,3,5],com:[0,5,6],combin:10,come:[2,3,11],command:0,common:11,commonli:11,compar:[2,3,4,5,9,11,18,19,20,21,22],compat:25,compil:[2,3,9,10,12,15,33],complet:11,complex:11,compos:[4,10],composit:11,comprehens:[10,11],comput:[4,5,6,9,10,11,14,16,24,26,28,30,32,42,43,44,45],computation:[10,11],concern:11,concis:[1,50],condit:[11,48],config:[3,13],configur:[3,12,13,52],confirm:2,connectom:10,consecut:11,consequ:10,consid:2,consist:4,constant:5,constexpr:[1,2,3,4,5,6,36,37,38,39],constraint:[3,11],construct:10,constructor:50,consum:3,contain:[11,18,19,20,21,22,50],contextu:11,contigu:[3,17,40],control:[10,11,27,46],conveni:3,convert:[1,3,15],convolut:10,cooper:12,copi:[4,10,18,19,20,21,22],core:[10,11,36,37,38,39],correct:[1,6],correspond:[1,2,3,50],cosin:24,cost:11,could:[2,11,27,46],count:5,cours:10,cpython:0,creat:[1,2,3,5,10],crucial:4,csv:1,ctx:5,cubla:[3,10],cuda:[1,2,3,4,5,6,10],cudnn:10,current:35,custom:[1,2,3,9],cut:3,cvpr:10,d:[2,4,13,15],dart:11,darte1999:11,data:[1,3,4,5,6,10,11,18,19,20,21,22,27,46,48,49],data_ptr:15,dataflow:11,david:4,db:5,db_ref:5,db_tri:5,deal:4,debug:15,decad:10,declar:1,decompos:11,decor:[1,3,13,14,15],decreas:4,dedic:3,deep:[3,4,10,11],def:[1,2,3,4,5,6,13,14],defin:[1,2,3,11,27,46],definit:11,denomin:[2,5],denot:[1,5],dens:11,depend:[0,7,11,48],deploi:10,describ:[4,11],design:11,desir:23,detail:[3,11],detect:10,develop:[10,11],deviat:5,devic:[1,2,3,5,6],diagram:5,dialect:11,dict:[12,13,14],dictionari:[12,14],diesel:11,differ:[1,2,3,4,6,10,11,13,50],difficult:11,difficulti:[3,10],dijkstra82:11,dijkstra:11,dim:[2,5,11],dimens:[3,25,27,29,31,46,47],dimension:[3,11,25],dir:0,direct:3,disjoint:11,disk:1,dissert:11,distribut:[2,4,11],divid:5,divis:[3,5],dnn:[9,10,11],do_bench:[1,2,3,5],do_not_speci:15,doc:[4,6],doe:[1,2,3,11],doesn:[5,11],domain:[10,11],don:[1,2,3],done:[3,10,29,31,47],dot:[3,5],doubl:6,doubli:3,doubt:11,down:[3,11],download:[0,1,2,3,4,5,6,7],dram:[1,2],dropout:[7,8],dror:4,dsl:[9,10,11],dtype:[1,2,3,5,15,18,19,20,21,22,27,46,49],dw:5,dw_ref:5,dw_tri:5,dx:5,dx_ref:5,dx_tri:5,dy:5,e:[0,2,3,4,5,7,10,11,49],each:[1,2,3,4,5,10,11,12,14],earli:13,early_config_prun:13,eas:11,easi:[3,4],easier:[1,2,10],easili:3,ed:[1,3],education:2,effect:11,effici:[3,4,5,10,38],effort:11,eg:13,either:[1,34,35,48],elango2018:11,elango:11,element:[1,2,3,4,5,16,24,26,27,28,29,30,31,32,42,43,44,45,46,47,48,50],element_s:[2,5],element_ti:[18,19,20,21,22,27,46],elementwis:2,els:[3,5],emerg:10,empti:[3,5,27,46],empty_lik:[1,2,4,5,6],enabl:11,encod:[6,11],encourag:4,end:[10,11,17],enforc:11,engin:11,enqueu:[1,2,5],ensur:[5,11],entir:11,entri:38,environ:9,ep:5,epsilon:5,equal:[11,17],error:3,especi:10,et:[4,10,11],euromicro:10,evalu:[3,4,13,48],even:[4,11],evict:[27,46],eviction_polici:[27,46],evidenc:10,evolv:10,exampl:[1,2,3,4,5,6,7,10,11,12],except:5,exchang:22,execut:[8,10,11,12,52],exist:[10,11],exp:2,expect:[2,18,19,20,21,22],expens:[10,11,14],explor:[4,10],exponenti:[2,26],express:[5,10,11],extend:[3,4],extern:6,extern_lib:6,extra:1,extras_requir:5,extrem:11,f:[1,2,3,6,11],facilit:[10,11],fact:11,fairli:3,fals:[5,27,44,46,48,50],far:2,fast:[2,10,11],fast_flush:51,faster:[2,5,37],fastest:11,featur:5,feel:3,fetch:10,few:11,field:[10,13],figur:11,file:[1,2,3,8],fill:49,final_db:5,final_dw:5,fine:4,first:[1,3,4,5,9,11,25,30,32],first_pid_m:3,firstli:4,fit:2,fix:50,flag:2,flatten:40,flexibl:10,float16:[3,5,25,49],float32:[1,2,3,4,5,25,36,39],flow:[10,11],fly:4,fn:[15,51],focu:[3,5,11],folder:4,follow:[0,2,3,5,9,10,11],footprint:4,forc:4,forget:1,formal:11,format:11,forward:5,found:[18,19,20,21,22],foundat:11,four:38,fp16:3,fp32:[3,25],frac:[4,5],framework:[10,11],free:3,from:[1,2,3,4,6,10,11,27,48],full:[1,2,3,4,5,6],fulli:11,func:11,fundament:11,further:[4,5,11],fuse:[3,5,7,8],fusedlayernorm:5,fusion:[2,11],g:[3,4,5,10,11,49],galleri:[1,2,3,4,5,6,7],gb:[1,2,5],gbp:[1,2,5],gener:[1,2,3,4,5,6,7,10,11,36,37,38,39,50],geoffrei:[4,5],geq:11,get:[1,2,3,4,8],girbal2006:11,girbal:11,git:0,github:[0,5],give:10,given:[2,3,4,5,23,34,35,36,37,38,39,49],global:11,go:[1,3,11],good:[1,11],gpgpu:10,gpu:[1,2,4,9,10,11,12,15],grad:5,grad_to_non:[5,51],gradient:5,grammat:11,graphic:10,greater:2,green:[1,2,3,5],grid:[1,2,3,4,5,6,34,35],grid_m:3,grid_n:3,grosser2012:11,grosser:11,group:3,group_id:3,group_m:3,group_size_m:[3,5],grow:11,guard:[1,2],guid:[6,10],ha:[1,3,4,10,11,34,35],had:1,halid:[10,11],hand:11,handl:[1,2,4,11],handwritten:10,hard:3,harder:11,hardwar:[3,9,11],has_apex:5,hasn:1,hat:5,have:[2,4,5,10,11,15,25,48,50],heavi:10,helper:[1,2],henc:3,here:[1,2,3,4,5,6],heurist:[2,5],hierarch:10,hierarchi:11,high:[3,5,10,11],higher:3,highli:10,highlight:[11,13,14],hint:11,hinton:[4,5],hit:3,how:[1,2,3,5,9,10,14],howev:[2,11],html:[4,6],http:[0,4,5,6],i:[1,2,3,4,5,10,11],id:[3,5,35],idea:10,ideal:2,ident:2,identifi:1,idx:[27,46],ieee_round:44,ilya:4,imag:[10,11],implement:[1,2,3,4,5,10,11],implicitli:[1,15,27,46],importantli:11,impos:11,improv:[3,4,5],incompat:[3,11],incorrect:3,increas:[1,2,3,4,5],incred:10,increment:11,inde:11,independ:[2,5,11],index:[1,6],indic:[11,27,46,48],induc:11,industri:10,inequ:11,inf:2,inform:11,infrastructur:11,initi:[1,3],inner:[3,25],input:[1,2,3,4,5,6,11,13,14,16,23,24,25,26,28,29,30,31,32,33,40,41,42,43,44,45,47],input_ptr:2,input_row_strid:2,instal:[7,9],instanc:[1,2,3,4,5,10,12,34,35],instanti:4,instead:[2,48],instruct:[9,10],int1:[27,46],int32:[4,5,17,37,38],integ:[11,27,46],interchang:11,interest:[10,11],intermedi:[5,11],intern:[2,11],interv:17,intrins:11,introduc:[4,5],introduct:9,invari:[2,11],invoc:4,invok:6,involv:5,ipynb:[1,2,3,4,5,6],ir:11,irregular:[2,11],is_contigu:[3,4],is_cuda:[1,6],isn:3,issu:[10,11],iter:[3,5,10,11,15],its:[1,2,3,11,13],ivar:12,j:[3,11],jacobian:5,jami:5,jimmi:5,jit:[1,2,3,4,5,6,13,14],jitfunct:15,jmlr:4,john:4,johnson:4,journal:11,jrk2013:10,jupyt:[1,2,3,4,5,6,7],just:[3,11,14],k:[3,4,6,10,11],kb:10,keep:4,kei:[3,10,13],kellei:10,kernel:[4,5,9,10,12,13,14],keyword:[1,12],ki:11,kind:2,kiro:5,know:33,known:11,krizhevski:4,kwarg:12,l2:5,label:[1,2,3,50],lam1991:10,lam:10,lambda:[1,2,3,4,5,6,14],languag:[1,2,3,4,5,6,9,10],larg:[10,11],last:3,later:[2,11],latest:0,lattner2004:11,lattner2019:11,lattner:11,launch:[1,2,3,34,35],law:11,layer:[7,8,10,11],layer_norm:5,layernorm:5,lead:[4,10,11],leaky_relu:3,learn:[1,2,3,4,5,9,10,11],learnabl:5,least:11,lee2017:10,lee:10,left:[11,17],legal:11,lei:5,length:1,less:[4,5,10,11,17],let:[1,2,4,5,33],letter:11,level:[3,10,11],li:10,libdevic:[7,8],librari:[0,3,10,11],lifelong:11,like:[1,4,10,11,37],limit:[2,4],lindenstrauss:4,line:[1,2,3,4,11,50],line_arg:[1,2,3,5,50],line_nam:[1,2,3,5,50],line_v:[1,2,3,5,50],linear:[5,10,11],link:0,list:[1,3,13,14,50,52],literatur:11,ll:4,llvm11:0,llvm:[0,11],load:[1,2,3,4,5,6,11,48],local:[6,10,11],locat:[3,18,19,20,21,22,27,46],lock:5,lock_id:5,log2:14,log:50,logarithm:[1,28],look:[4,5,9,10],loop:[3,11,12],low:[7,8,11],m:[0,2,3,5,10],machin:[10,11],machineri:[10,11],made:10,mai:[2,11,14],main:[3,10,11],maintain:[2,11],major:[3,11],make:[1,2,10,11],make_block_ptr:[27,46],manag:[4,10],mani:[10,11],manual:[2,11],manual_se:[1,2,3,6],map:[3,5],mapl:11,mark:[4,52],markedli:10,mask:[1,2,3,4,5,6,18,20,21,22,27,46,48],match:[3,18,19,20,21,22],math:[7,8,14],mathbb:11,mathbf:11,mathcal:[11,39],mathemat:11,matmul:[3,11],matmul_kernel:3,matric:[2,3],matrix:[2,4,7,8,10,11,12,25],matrix_s:11,matter:[3,10,11],max:[1,2,5,6,20],max_fused_s:5,max_m:[1,2,3,5],maxim:[9,11,38],maximum:[1,2,6,29],mb:[8,10],mean:[3,5,11,13,51],mechan:[2,11],memori:[1,2,3,7,8,10,11,18,19,20,21,22,27,46,48],mention:3,meta:[1,2,3,4,5,6,12,13,14],metaparamet:1,method:[11,12,15,50,52],methodolog:11,micro:10,min:[3,5,21],min_m:[1,2,3,5],minimum:31,minut:[1,2,3,4,5,6],miss:11,mitig:11,ml:10,mlir:11,mn:2,mode:5,model:[1,5,10,11,13],modern:[3,9,10,11],modular:11,modulenotfounderror:5,moor:11,mora:4,more:[2,3,4,5,9,10,11,50],most:[3,11],mostli:12,move:[3,5],movement:4,ms:[1,2,3,5],much:[2,3,5],mullapudi2016:11,mullapudi:11,multi:[3,10,11],multipl:[1,4,5,7,8,10,11,12,13,33,37],multipli:[3,4,11,25],must:[2,3,17,25,27,46,48],n:[2,3,5,10,39],n_col:2,n_element:[1,4,6],n_round:[36,37,38,39],n_row:2,nabla_:5,naiv:[2,4],naive_softmax:2,name:[1,2,3,13,14,50],nan:27,nativ:[1,2,3],natur:[2,10,28],nb:10,necessari:2,need:[1,2,3,4,5,37],nelement:2,nest:[3,11],net:11,network:[4,5,10,11],neural:[4,5,10,11],neurosci:10,never:4,next:[2,3],next_power_of_2:[2,5],nightli:0,nip:10,nitish:4,nn:5,non:10,none:[2,3,5,12,13,15,18,20,21,22,27,46,50,51],nonzero:48,norm:[4,5,8],normal:[2,7,8],normalized_shap:5,note:[0,1,2,3,4,11,13,15,48],notebook:[1,2,3,4,5,6,7],notic:[2,11],notori:[3,10],novel:10,now:[1,3,5],num_pid_in_group:3,num_pid_k:3,num_pid_m:3,num_pid_n:3,num_stag:[3,12,13],num_warp:[2,3,5,12,13],number:[1,2,3,4,5,11,12,13,34,36,37,38,39],numel:[1,4,5,6],numer:[2,5,10],nvidia:[5,6,10,27,46],nvvm:6,o:[2,4],object:[1,3,10,12,13,18,19,20,21,22],obtain:1,obvious:2,occur:11,odot:5,off:5,offer:10,offici:0,offs_am:3,offs_bn:3,offs_cm:3,offs_cn:3,offs_k:3,offset:[1,4,5,6,36,37,38,39],often:3,omega:11,omit:5,onc:[2,10,11],one:[2,3,4,5,7,10,11,27,50],onli:[2,3,4,5,10,11,15],op:[1,2],open:17,openai:0,opencl:10,oper:[1,2,3,4,5,6,7,10,18,19,20,21,22,48],opportun:10,opsila:10,optim:[10,11],option:[3,13,15,27,46,50],orang:5,order:[2,3,7,11],org:4,origin:11,osdi:10,other:[2,3,4,5,9,11,15,25,27,30,32],otherwis:[4,48],our:[1,2,3,5,10],out:[1,2,3,4,5,6,9,11,27,46],out_dtyp:25,outlin:11,output2:4,output3:4,output:[1,2,3,4,5,6],output_ptr:[1,2,4],output_row_start_ptr:2,output_row_strid:2,output_torch:[1,6],output_triton:[1,6],over:[2,4,10,11],overfit:4,overflow:2,own:3,p:[4,11],packag:15,pact:11,pad:[2,27],padding_opt:[27,46],par:3,paradigm:[10,11],paragraph:4,parallel:[1,2,3,4,5,9,10,11,12],paralleliz:10,paramet:[1,3,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,34,35,36,37,38,39,40,42,43,44,45,46,47,48,49,50,52],parametr:10,part:[3,4,11],partial:5,partial_db:5,partial_dw:5,particular:[2,3],particularli:[10,11],partit:10,pass:[1,6,11,12],past:[10,11],path:1,pattern:10,peak:11,per:[2,4,5],perf:3,perf_model:13,perf_report:[1,2,3,5,50],perform:[1,2,4,5,10,11,13,18,19,20,21,22],persist:4,person:11,perspect:11,phase:11,philosophi:11,philox:[4,38],pid:[1,3,4,5,6],pid_m:3,pid_n:3,pip:[0,7],pipelin:[10,11,12],platform:[9,11],pldi:10,pleas:6,plot:[0,1,2,3,50],plot_nam:[1,2,3,5,50],pmatrix:11,point:[1,11,38],pointer:[1,2,4,5,15,18,19,20,21,22,27,46],pointerdtyp:[18,19,20,21,22],pointertyp:[27,46],polici:[27,46],polli:11,polyhedr:10,polyhedra:11,popular:11,portabl:[10,11],pose:10,posit:14,possibl:[1,2,3,11,12],power:[2,4,11,14,17],ppopp:11,practic:[1,2,3,10],pragma:10,pre:[0,10],pre_hook:12,prealloc:1,predic:13,predict:11,prefer:2,premis:10,present:0,preserv:11,preserve_rng_st:4,prevent:[4,11],primer:11,primit:[10,15],princip:6,principl:11,print:[1,3,4,6],print_data:[1,2,3,5],prng:4,probabl:[4,11],problem:1,problemat:11,procedur:11,process:[1,10,11],processor:10,produc:[3,4,5],product:[5,9,11,25],program:[1,2,3,4,5,9,10,34,35],program_id:[1,2,3,4,5,6],programm:[10,11],prohibitev:14,project:[4,10],promot:[3,11],properli:2,properti:11,propos:10,proprietari:3,provid:[1,2,3,4,5,9,11,13,29,31,47],prune:[4,13],prune_configs_bi:13,pseudo:[3,4,38],pseudorandom:4,ptr:3,ptx:[27,46],purpos:[10,11],push:11,put:4,py:[0,1,2,3,4,5,6,8],pypi:[0,5],pytest:0,python:[1,2,3,4,5,6,7,13,14,15],pytorch:[1,2,4,5],qquad:11,quad:5,quantil:[1,2,3,5,51],r:2,ragan:10,rais:5,rand:[1,4,5,6],randint4x:37,randn:[2,3,4,5],randn_lik:5,random:[4,36,37,38,39],randomli:4,rang:[1,2,3,5,10,11],rapidli:[10,11],rate:3,rather:10,raw:1,rdom:11,re:[1,3],read:[2,3,7],readabl:5,reader:11,real:10,reason:11,recent:10,recommend:7,recomput:[4,10],rectifi:10,redmon2016:10,redmon:10,reduc:5,reduct:[2,5,29,31,47],refer:[1,6],regard:6,regardless:[4,48],regim:4,regrett:10,regular:[4,11],rel:[1,11],relat:9,releas:[0,5,10],reli:11,remain:[10,50],rememb:3,reorder:11,rep:[5,51],repres:[2,3,11,12],requir:[2,4,11],requires_grad:5,requires_grad_:5,research:[10,11],reset:13,reset_to_zero:13,reshap:5,resolut:11,resourc:10,resp:11,respect:11,restrict:11,result:[0,1,2,10,11],ret:2,retain_graph:5,retriev:11,return_mod:51,reus:3,revisit:10,right:[11,17],rise:11,role:11,ron:4,root:45,roughli:3,row:[2,3,4,5],row_idx:2,row_minus_max:2,row_start_ptr:2,rstd:5,rtol:[3,5],run:[0,1,2,3,4,5,6,9,11,13,15,52],runtim:[11,15],runtimeerror:5,ruslan:4,rvar:11,ryan:5,s:[1,2,4,5,11,38],said:11,salakhutdinov:4,salmon2011:4,salmon:4,same:[4,5,6,10,48,50],sato2019:11,sato:11,save:[1,2,3],save_for_backward:5,save_path:[1,5],saved_tensor:5,sc:11,scalabl:11,scalar:[4,10,25,27,36,37,38,39,46,49],scale:50,scan:11,schedul:10,scienc:11,scientif:11,scop:11,scope:11,script:[0,1,2,3,4,5,6],second:[1,2,3,4,5,6,11,25,30,32],secondli:4,section:[3,11],see:[1,2,3,4,11],seed:[36,37,38,39],seeded_dropout:4,seem:[1,11],select:[6,10,11,48],self:[12,50],semant:[6,11],semi:11,sens:[1,10,11],separ:[5,11],sequenc:10,sequenti:5,set:[1,4,5,11],setup:[0,5],sever:[10,11],shall:11,shape:[1,2,3,4,5,11,23,27,41,46,48,49],share:[5,10],shaw:4,shift:2,should:[1,3,5,10,11,12,27,29,31,46,47,50],show_plot:[1,2,3],shown:11,side:11,sight:11,sigma:5,signal:10,significantli:2,sigplan:11,simd:10,simpl:[1,2,3,4],simplest:7,simpli:[6,11],simplic:3,simplifi:4,sinc:[1,2,3,5],sine:[6,43],singl:[2,4,10,27,37,46],size:[1,2,4,5,6,11],slower:[10,11],slowest:11,sm80:12,sm:11,small:5,smaller:[3,4],smallest:[2,14],snemi3d:10,so:[1,2,3,4,5,11],softmax:[4,7,8],softmax_kernel:2,softmax_output:2,softwar:12,solid:11,solut:3,solv:11,some:3,sometim:11,sourc:[1,2,3,4,5,6,7,11],space:[10,11],spars:[4,10,11],spatial:11,speak:3,special:10,specif:[5,10],specifi:[11,14,18,19,20,21,22,27,46],speed:2,sphinx:[1,2,3,4,5,6,7],split:11,spmd:[1,10,11],sqrt:5,squar:45,sram:[2,3,5],srivastava2014:4,srivastava:4,stabil:[2,5],stabl:0,stage:[5,12],stai:5,standard:[5,11],start:[7,17],started_tutori:8,state:[4,10,11],statement:11,staticmethod:5,std:5,step:[5,11],still:[1,2,3,11],store:[1,2,3,4,5,6,18,19,20,21,22,48],str:[12,13,14,27,46,50],straightforward:[3,5],strategi:[4,5,11],stream:[5,37],strength:10,stride:[2,3,4,5],stride_ak:3,stride_am:3,stride_bk:3,stride_bn:3,stride_cm:3,stride_cn:3,stride_xi:3,stride_xj:3,structur:[10,11],style:[1,2,3,5,50],subscript:11,substanti:10,subtract:[2,5],successfulli:11,suffer:11,suit:10,sum:[1,2,5],sum_db:5,sum_dw:5,superhuman:10,support:[4,5,11],sure:2,surprisingli:10,surround:11,suspicion:2,sutskev:[4,10],sutskever2014:10,swap:[18,19,20,21,22],swizzl:10,synchron:[1,10],system:[0,3,10,11],t:[1,2,3,5,11,15],t_:11,tabul:4,taco:11,take:[3,4,5,9,13,14],taken:11,target:10,techniqu:[10,11],temperatur:4,tempor:11,tend:11,tension:10,tensor:[1,2,3,4,5,6,10,11,13,23,25,27,29,30,31,32,40,46,47,48,49],tensorrt:10,test:[0,1,5,9],test_layer_norm:5,text:[5,11],tflop:3,than:[2,3,5,10,11,17,37,50],thei:[3,10,11,15],them:1,themselv:3,theoret:2,therebi:11,therefor:3,theta:11,theta_:11,thi:[1,2,3,4,5,6,10,11,12,13,14,15,38,50],thing:[1,4],think:2,those:2,though:[10,11],thought:11,thread:[2,10,12],through:[5,7,11],throughout:[11,50],throughput:9,thu:5,tile:11,time:[0,1,2,3,4,5,6,10,11,13,37],tiramisu:[10,11],tl:[1,2,3,4,5,7,8,49],tmp:0,tog:11,togeth:[4,6],tolist:4,too:[27,46],top_k:13,topic:11,torch:[1,2,3,4,5,6],torch_output:3,total:[1,2,3,4,5,6,8],tradit:[4,10,11],transform:[4,5,11],travers:11,trend:10,tri:23,trick:2,tricki:4,trigger:[3,13],triton:[0,1,2,3,4,5,6,7,10,11],triton_max_tensor_numel:17,triton_output:3,trivial:10,tune:[2,3,11,13,14],tuner:12,tupl:[1,23,27,46,49],tutori:[1,2,3,4,5,9],tutorials_jupyt:7,tutorials_python:7,tvm:[10,11],two:[1,2,3,11,13,14,17,25],type:[6,12,25,48,49],typecast:[27,46],typic:11,u:[0,36],un:11,uncommon:11,underbrac:5,underli:6,underneath:11,understand:2,undesir:13,unfortun:[3,11],unifi:10,uniformli:4,unintend:48,unit:[0,10],univers:11,unrol:11,up:[2,5],updat:[3,11,13],us:[1,2,3,4,5,10,11,12,13,14,15,37,48,50,52],user:6,usr:6,util:[1,5],v100:11,v:5,val:[18,19,20,21,22],valid:1,valu:[1,2,3,4,6,13,14,16,17,18,19,20,21,22,24,26,27,28,29,31,33,42,43,44,45,46,47,48,49,50,52],valuabl:2,variabl:[3,12],varianc:5,variant:10,variou:7,vasilach:[10,11],vasilache2018:[10,11],vast:11,vec:11,vector:[4,5,7,8,10,11],vendor:3,veri:[2,4,11],verif:11,verifi:[2,11],version:15,via:11,view:40,visibl:11,vision:10,vjp:5,volatil:27,vs:0,w:[5,11],w_shape:5,wa:[4,5],wai:[2,3,4,5],want:[2,4,48],warmup:51,warp:[2,5,12],wast:2,wdy:5,we:[1,2,3,4,5,6,10,11],weight:5,well:[4,10,11],whatev:13,wheel:0,when:[2,3,4,5,10,11,12,13,15,48],where:[1,3,4,5,11,14,46],whether:[10,50],which:[1,2,3,4,10,11,13,27,29,31,46,47,50],whose:[1,2,3,4,11,13,27],wide:11,wise:[1,2,5,16,24,26,27,28,30,32,42,43,44,45,46],wish:[3,11],within:[3,15,17],without:11,wolf:11,wolfe1989:11,won:2,word:11,work:[2,4,9,10],workload:[3,12],wors:[3,10,11],would:[1,2,4],wouldn:11,wrapper:3,write:[1,2,3,4,5,7,9,11],wrote:2,x:[1,2,3,4,5,6,11,16,24,26,28,30,32,40,42,43,44,45,48,50],x_arg:5,x_hat:5,x_keep:4,x_keep_ptr:4,x_log:[1,50],x_max:2,x_name:[1,2,3,5,50],x_ptr:[1,4,6,13,14],x_shape:5,x_size:[13,14],x_val:[1,2,3,5,50],xhat:5,xi:11,xii:11,xlabel:50,xo:11,y:[1,2,3,5,11,30,32,48,50],y_fwd:5,y_log:50,y_name:[1,2],y_ptr:[1,6],y_ref:5,y_torch:2,y_tri:5,y_triton:2,year:11,yet:[10,11],yi:11,yield:48,yii:11,ylabel:[1,2,3,5,50],yo:11,you:[0,1,2,3,4,5,6,7,10,13,37,48],your:[0,1,9],yourself:[2,3],z:[1,2,11],zero:[3,4,5,6,13,27],zip:7},titles:[\"Installation\",\"Vector Addition\",\"Fused Softmax\",\"Matrix Multiplication\",\"Low-Memory Dropout\",\"Layer Normalization\",\"Libdevice (<cite>tl.math</cite>) function\",\"Tutorials\",\"Computation times\",\"Welcome to Triton\\u2019s documentation!\",\"Introduction\",\"Related Work\",\"triton.Config\",\"triton.autotune\",\"triton.heuristics\",\"triton.jit\",\"triton.language.abs\",\"triton.language.arange\",\"triton.language.atomic_add\",\"triton.language.atomic_cas\",\"triton.language.atomic_max\",\"triton.language.atomic_min\",\"triton.language.atomic_xchg\",\"triton.language.broadcast_to\",\"triton.language.cos\",\"triton.language.dot\",\"triton.language.exp\",\"triton.language.load\",\"triton.language.log\",\"triton.language.max\",\"triton.language.maximum\",\"triton.language.min\",\"triton.language.minimum\",\"triton.language.multiple_of\",\"triton.language.num_programs\",\"triton.language.program_id\",\"triton.language.rand\",\"triton.language.randint\",\"triton.language.randint4x\",\"triton.language.randn\",\"triton.language.ravel\",\"triton.language.reshape\",\"triton.language.sigmoid\",\"triton.language.sin\",\"triton.language.softmax\",\"triton.language.sqrt\",\"triton.language.store\",\"triton.language.sum\",\"triton.language.where\",\"triton.language.zeros\",\"triton.testing.Benchmark\",\"triton.testing.do_bench\",\"triton.testing.perf_report\",\"triton\",\"triton.language\",\"triton.testing\"],titleterms:{\"default\":6,\"final\":3,\"function\":6,ab:16,addit:1,advantag:11,algebra:54,api:9,arang:17,arithmet:3,asin:6,atom:54,atomic_add:18,atomic_ca:19,atomic_max:20,atomic_min:21,atomic_xchg:22,autotun:13,backward:5,baselin:4,benchmark:[1,2,3,5,50],binari:0,broadcast_to:23,cach:3,challeng:10,co:24,comparison:54,compil:[11,54],comput:[1,2,3,8],config:12,creation:54,custom:6,distribut:0,do_bench:51,document:9,dot:25,dropout:4,exercis:4,exp:26,from:0,further:9,fuse:2,gener:54,get:9,go:9,heurist:14,hint:54,index:54,instal:0,introduct:10,jit:15,kernel:[1,2,3,6],l2:3,languag:[11,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,54],layer:5,libdevic:6,librari:6,limit:11,linear:54,load:27,log:28,low:4,manipul:54,math:[6,54],matrix:3,max:29,maximum:30,memori:[4,54],min:31,minimum:32,model:54,motiv:[2,3,5,10],multipl:3,multiple_of:33,normal:5,num_program:34,number:54,op:54,optim:3,packag:0,pass:5,path:6,perf_report:52,perform:3,pointer:3,polyhedr:11,program:[11,54],program_id:35,python:[0,9],rand:36,randint4x:38,randint:37,randn:39,random:54,ravel:40,reduct:54,refer:[4,5,10,11],relat:11,represent:11,reshap:41,result:3,s:9,schedul:11,seed:4,shape:54,sigmoid:42,sin:43,softmax:[2,44],sourc:0,sqrt:45,squar:3,start:9,store:46,sum:47,test:[2,3,50,51,52,55],time:8,tl:6,triton:[9,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55],tutori:7,unit:[2,3],us:6,vector:1,welcom:9,where:48,work:11,zero:49}})\n\\ No newline at end of file"}, {"filename": "master/.doctrees/environment.pickle", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/getting-started/tutorials/01-vector-add.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/getting-started/tutorials/05-layer-norm.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/getting-started/tutorials/06-fused-attention.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/getting-started/tutorials/07-libdevice-function.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/getting-started/tutorials/sg_execution_times.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/index.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/programming-guide/chapter-1/introduction.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.Config.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.autotune.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.heuristics.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.jit.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.atomic_add.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.atomic_and.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.atomic_max.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "master/.doctrees/python-api/generated/triton.language.atomic_min.doctree", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}]