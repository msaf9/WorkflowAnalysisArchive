[{"filename": "main/.buildinfo", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n # Sphinx build info version 1\n # This file hashes the configuration used when building these files. When it is not found, a full rebuild will be done.\n-config: 0b8002dbdbea4298b5958ac7a13f2ce6\n+config: b07568a9faad04df86d02144741a0e8e\n tags: 645f666f9bcd5a90fca523b33c5a78b7"}, {"filename": "main/.doctrees/environment.pickle", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/installation.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/01-vector-add.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/02-fused-softmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/03-matrix-multiplication.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/04-low-memory-dropout.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/05-layer-norm.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/06-fused-attention.doctree", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/07-math-functions.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/index.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/sg_execution_times.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/index.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/programming-guide/chapter-1/introduction.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/programming-guide/chapter-2/related-work.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.Config.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.autotune.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.heuristics.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.jit.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.abs.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.arange.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_add.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_cas.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_max.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_min.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_xchg.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.broadcast_to.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cos.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.dot.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.exp.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.load.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.log.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.max.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.maximum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.min.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.minimum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.multiple_of.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.num_programs.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.program_id.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.rand.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randint.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randint4x.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randn.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.ravel.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.reshape.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sigmoid.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sin.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.softmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sqrt.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.store.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.where.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.zeros.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.Benchmark.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.do_bench.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.perf_report.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.language.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.testing.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/3176accb6c7288b0e45f41d94eebacb9/06-fused-attention.ipynb", "status": "added", "additions": 54, "deletions": 0, "changes": 54, "file_content_changes": "@@ -0,0 +1,54 @@\n+{\n+  \"cells\": [\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"%matplotlib inline\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"markdown\",\n+      \"metadata\": {},\n+      \"source\": [\n+        \"\\n# Fused Attention\\n\\nThis is a Triton implementation of the Flash Attention algorithm\\n(see: Dao et al., https://arxiv.org/pdf/2205.14135v2.pdf; Rabe and Staats https://arxiv.org/pdf/2112.05682v2.pdf)\\n\"\n+      ]\n+    },\n+    {\n+      \"cell_type\": \"code\",\n+      \"execution_count\": null,\n+      \"metadata\": {\n+        \"collapsed\": false\n+      },\n+      \"outputs\": [],\n+      \"source\": [\n+        \"import pytest\\nimport torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.jit\\ndef _fwd_kernel(\\n    Q, K, V, sm_scale,\\n    L, M,\\n    Out,\\n    stride_qz, stride_qh, stride_qm, stride_qk,\\n    stride_kz, stride_kh, stride_kn, stride_kk,\\n    stride_vz, stride_vh, stride_vk, stride_vn,\\n    stride_oz, stride_oh, stride_om, stride_on,\\n    Z, H, N_CTX,\\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\\n    BLOCK_N: tl.constexpr,\\n):\\n    start_m = tl.program_id(0)\\n    off_hz = tl.program_id(1)\\n    # initialize offsets\\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\\n    offs_n = tl.arange(0, BLOCK_N)\\n    offs_d = tl.arange(0, BLOCK_DMODEL)\\n    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\\n    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\\n    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\\n    # Initialize pointers to Q, K, V\\n    q_ptrs = Q + off_q\\n    k_ptrs = K + off_k\\n    v_ptrs = V + off_v\\n    # initialize pointer to m and l\\n    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\\\"inf\\\")\\n    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n    # load q: it will stay in SRAM throughout\\n    q = tl.load(q_ptrs)\\n    # loop over k, v and update accumulator\\n    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\\n        # -- compute qk ----\\n        k = tl.load(k_ptrs)\\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\\n        qk += tl.dot(q, k)\\n        qk *= sm_scale\\n        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\\\"-inf\\\"))\\n        # compute new m\\n        m_curr = tl.maximum(tl.max(qk, 1), m_prev)\\n        # correct old l\\n        l_prev *= tl.exp(m_prev - m_curr)\\n        # attention weights\\n        p = tl.exp(qk - m_curr[:, None])\\n        l_curr = tl.sum(p, 1) + l_prev\\n        # rescale operands of matmuls\\n        l_rcp = 1. / l_curr\\n        p *= l_rcp[:, None]\\n        acc *= (l_prev * l_rcp)[:, None]\\n        # update acc\\n        p = p.to(Q.dtype.element_ty)\\n        v = tl.load(v_ptrs)\\n        acc += tl.dot(p, v)\\n        # update m_i and l_i\\n        l_prev = l_curr\\n        m_prev = m_curr\\n        # update pointers\\n        k_ptrs += BLOCK_N * stride_kn\\n        v_ptrs += BLOCK_N * stride_vk\\n    # rematerialize offsets to save registers\\n    start_m = tl.program_id(0)\\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\\n    # write back l and m\\n    l_ptrs = L + off_hz * N_CTX + offs_m\\n    m_ptrs = M + off_hz * N_CTX + offs_m\\n    tl.store(l_ptrs, l_prev)\\n    tl.store(m_ptrs, m_prev)\\n    # initialize pointers to output\\n    offs_n = tl.arange(0, BLOCK_DMODEL)\\n    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\\n    out_ptrs = Out + off_o\\n    tl.store(out_ptrs, acc)\\n\\n\\n@triton.jit\\ndef _bwd_preprocess(\\n    Out, DO, L,\\n    NewDO, Delta,\\n    BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\\n):\\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\\n    off_n = tl.arange(0, D_HEAD)\\n    # load\\n    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\\n    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\\n    denom = tl.load(L + off_m).to(tl.float32)\\n    # compute\\n    do = do / denom[:, None]\\n    delta = tl.sum(o * do, axis=1)\\n    # write-back\\n    tl.store(NewDO + off_m[:, None] * D_HEAD + off_n[None, :], do)\\n    tl.store(Delta + off_m, delta)\\n\\n\\n@triton.jit\\ndef _bwd_kernel(\\n    Q, K, V, sm_scale, Out, DO,\\n    DQ, DK, DV,\\n    L, M,\\n    D,\\n    stride_qz, stride_qh, stride_qm, stride_qk,\\n    stride_kz, stride_kh, stride_kn, stride_kk,\\n    stride_vz, stride_vh, stride_vk, stride_vn,\\n    Z, H, N_CTX,\\n    num_block,\\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\\n    BLOCK_N: tl.constexpr,\\n):\\n    off_hz = tl.program_id(0)\\n    off_z = off_hz // H\\n    off_h = off_hz % H\\n    # offset pointers for batch/head\\n    Q += off_z * stride_qz + off_h * stride_qh\\n    K += off_z * stride_qz + off_h * stride_qh\\n    V += off_z * stride_qz + off_h * stride_qh\\n    DO += off_z * stride_qz + off_h * stride_qh\\n    DQ += off_z * stride_qz + off_h * stride_qh\\n    DK += off_z * stride_qz + off_h * stride_qh\\n    DV += off_z * stride_qz + off_h * stride_qh\\n    for start_n in range(0, num_block):\\n        lo = start_n * BLOCK_M\\n        # initialize row/col offsets\\n        offs_qm = lo + tl.arange(0, BLOCK_M)\\n        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\\n        offs_m = tl.arange(0, BLOCK_N)\\n        offs_k = tl.arange(0, BLOCK_DMODEL)\\n        # initialize pointers to value-like data\\n        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\\n        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        # pointer to row-wise quantities in value-like data\\n        D_ptrs = D + off_hz * N_CTX\\n        m_ptrs = M + off_hz * N_CTX\\n        # initialize dv amd dk\\n        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n        # k and v stay in SRAM throughout\\n        k = tl.load(k_ptrs)\\n        v = tl.load(v_ptrs)\\n        # loop over rows\\n        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\\n            offs_m_curr = start_m + offs_m\\n            # load q, k, v, do on-chip\\n            q = tl.load(q_ptrs)\\n            # recompute p = softmax(qk, dim=-1).T\\n            # NOTE: `do` is pre-divided by `l`; no normalization here\\n            qk = tl.dot(q, tl.trans(k))\\n            qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\\\"-inf\\\"))\\n            m = tl.load(m_ptrs + offs_m_curr)\\n            p = tl.exp(qk * sm_scale - m[:, None])\\n            # compute dv\\n            do = tl.load(do_ptrs)\\n            dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\\n            # compute dp = dot(v, do)\\n            Di = tl.load(D_ptrs + offs_m_curr)\\n            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\\n            dp += tl.dot(do, tl.trans(v))\\n            # compute ds = p * (dp - delta[:, None])\\n            ds = p * dp * sm_scale\\n            # compute dk = dot(ds.T, q)\\n            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)\\n            # compute dq\\n            dq = tl.load(dq_ptrs)\\n            dq += tl.dot(ds.to(Q.dtype.element_ty), k)\\n            tl.store(dq_ptrs, dq)\\n            # increment pointers\\n            dq_ptrs += BLOCK_M * stride_qm\\n            q_ptrs += BLOCK_M * stride_qm\\n            do_ptrs += BLOCK_M * stride_qm\\n        # write-back\\n        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\\n        tl.store(dv_ptrs, dv)\\n        tl.store(dk_ptrs, dk)\\n\\n\\nempty = torch.empty(128, device=\\\"cuda\\\")\\n\\n\\nclass _attention(torch.autograd.Function):\\n\\n    @staticmethod\\n    def forward(ctx, q, k, v, sm_scale):\\n        BLOCK = 128\\n        # shape constraints\\n        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\\n        assert Lq == Lk and Lk == Lv\\n        assert Lk in {16, 32, 64, 128}\\n        o = torch.empty_like(q)\\n        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\\n        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\\n        m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\\n        num_warps = 4 if Lk <= 64 else 8\\n\\n        _fwd_kernel[grid](\\n            q, k, v, sm_scale,\\n            L, m,\\n            o,\\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\\n            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\\n            q.shape[0], q.shape[1], q.shape[2],\\n            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\\n            BLOCK_DMODEL=Lk, num_warps=num_warps,\\n            num_stages=2,\\n        )\\n        # print(h.asm[\\\"ttgir\\\"])\\n\\n        ctx.save_for_backward(q, k, v, o, L, m)\\n        ctx.grid = grid\\n        ctx.sm_scale = sm_scale\\n        ctx.BLOCK_DMODEL = Lk\\n        return o\\n\\n    @staticmethod\\n    def backward(ctx, do):\\n        BLOCK = 128\\n        q, k, v, o, l, m = ctx.saved_tensors\\n        do = do.contiguous()\\n        dq = torch.zeros_like(q, dtype=torch.float32)\\n        dk = torch.empty_like(k)\\n        dv = torch.empty_like(v)\\n        do_scaled = torch.empty_like(do)\\n        delta = torch.empty_like(l)\\n        _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\\n            o, do, l,\\n            do_scaled, delta,\\n            BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\\n        )\\n        _bwd_kernel[(ctx.grid[1],)](\\n            q, k, v, ctx.sm_scale,\\n            o, do_scaled,\\n            dq, dk, dv,\\n            l, m,\\n            delta,\\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\\n            q.shape[0], q.shape[1], q.shape[2],\\n            ctx.grid[0],\\n            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\\n            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\\n            num_stages=1,\\n        )\\n        # print(h.asm[\\\"ttgir\\\"])\\n        return dq, dk, dv, None\\n\\n\\nattention = _attention.apply\\n\\n\\n@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\\ndef test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\\n    torch.manual_seed(20)\\n    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0.1, std=0.2).requires_grad_()\\n    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0.4, std=0.2).requires_grad_()\\n    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0.3, std=0.2).requires_grad_()\\n    sm_scale = 0.2\\n    dout = torch.randn_like(q)\\n    # reference implementation\\n    M = torch.tril(torch.ones((N_CTX, N_CTX), device=\\\"cuda\\\"))\\n    p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\\n    for z in range(Z):\\n        for h in range(H):\\n            p[:, :, M == 0] = float(\\\"-inf\\\")\\n    p = torch.softmax(p.float(), dim=-1).half()\\n    # p = torch.exp(p)\\n    ref_out = torch.matmul(p, v)\\n    ref_out.backward(dout)\\n    ref_dv, v.grad = v.grad.clone(), None\\n    ref_dk, k.grad = k.grad.clone(), None\\n    ref_dq, q.grad = q.grad.clone(), None\\n    # # triton implementation\\n    tri_out = attention(q, k, v, sm_scale)\\n    # print(ref_out)\\n    # print(tri_out)\\n    tri_out.backward(dout)\\n    tri_dv, v.grad = v.grad.clone(), None\\n    tri_dk, k.grad = k.grad.clone(), None\\n    tri_dq, q.grad = q.grad.clone(), None\\n    # compare\\n    assert torch.allclose(ref_out, tri_out, atol=1e-2, rtol=0)\\n    assert torch.allclose(ref_dv, tri_dv, atol=1e-2, rtol=0)\\n    assert torch.allclose(ref_dk, tri_dk, atol=1e-2, rtol=0)\\n    assert torch.allclose(ref_dq, tri_dq, atol=1e-2, rtol=0)\\n\\n\\ntry:\\n    from flash_attn.flash_attn_interface import flash_attn_func\\n    HAS_FLASH = True\\nexcept BaseException:\\n    HAS_FLASH = False\\n\\nBATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\\n# vary seq length for fixed head and batch=4\\nconfigs = [triton.testing.Benchmark(\\n    x_names=['N_CTX'],\\n    x_vals=[2**i for i in range(10, 14)],\\n    line_arg='provider',\\n    line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\\n    line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\\n    styles=[('red', '-'), ('blue', '-')],\\n    ylabel='ms',\\n    plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\\n    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode}\\n) for mode in ['fwd', 'bwd']]\\n\\n\\n@triton.testing.perf_report(configs)\\ndef bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.float16, device=\\\"cuda\\\"):\\n    assert mode in ['fwd', 'bwd']\\n    warmup = 25\\n    rep = 100\\n    if provider == \\\"triton\\\":\\n        q = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        sm_scale = 1.3\\n        fn = lambda: attention(q, k, v, sm_scale)\\n        if mode == 'bwd':\\n            o = fn()\\n            do = torch.randn_like(o)\\n            fn = lambda: o.backward(do, retain_graph=True)\\n        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\\n        return ms\\n    if provider == \\\"flash\\\":\\n        lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\\n        cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\\n        cu_seqlens[1:] = lengths.cumsum(0)\\n        qkv = torch.randn((BATCH * N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\\n        fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=True)\\n        if mode == 'bwd':\\n            o = fn()\\n            do = torch.randn_like(o)\\n            fn = lambda: o.backward(do, retain_graph=True)\\n        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\\n        return ms\\n\\n\\n# only works on post-Ampere GPUs right now\\nbench_flash_attention.run(save_path='.', print_data=True)\"\n+      ]\n+    }\n+  ],\n+  \"metadata\": {\n+    \"kernelspec\": {\n+      \"display_name\": \"Python 3\",\n+      \"language\": \"python\",\n+      \"name\": \"python3\"\n+    },\n+    \"language_info\": {\n+      \"codemirror_mode\": {\n+        \"name\": \"ipython\",\n+        \"version\": 3\n+      },\n+      \"file_extension\": \".py\",\n+      \"mimetype\": \"text/x-python\",\n+      \"name\": \"python\",\n+      \"nbconvert_exporter\": \"python\",\n+      \"pygments_lexer\": \"ipython3\",\n+      \"version\": \"3.8.10\"\n+    }\n+  },\n+  \"nbformat\": 4,\n+  \"nbformat_minor\": 0\n+}\n\\ No newline at end of file"}, {"filename": "main/_downloads/54a35f6ec55f9746935b9566fb6bb1df/06-fused-attention.py", "status": "added", "additions": 361, "deletions": 0, "changes": 361, "file_content_changes": "@@ -0,0 +1,361 @@\n+\"\"\"\n+Fused Attention\n+===============\n+\n+This is a Triton implementation of the Flash Attention algorithm\n+(see: Dao et al., https://arxiv.org/pdf/2205.14135v2.pdf; Rabe and Staats https://arxiv.org/pdf/2112.05682v2.pdf)\n+\"\"\"\n+\n+import pytest\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def _fwd_kernel(\n+    Q, K, V, sm_scale,\n+    L, M,\n+    Out,\n+    stride_qz, stride_qh, stride_qm, stride_qk,\n+    stride_kz, stride_kh, stride_kn, stride_kk,\n+    stride_vz, stride_vh, stride_vk, stride_vn,\n+    stride_oz, stride_oh, stride_om, stride_on,\n+    Z, H, N_CTX,\n+    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n+    BLOCK_N: tl.constexpr,\n+):\n+    start_m = tl.program_id(0)\n+    off_hz = tl.program_id(1)\n+    # initialize offsets\n+    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+    offs_n = tl.arange(0, BLOCK_N)\n+    offs_d = tl.arange(0, BLOCK_DMODEL)\n+    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n+    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n+    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n+    # Initialize pointers to Q, K, V\n+    q_ptrs = Q + off_q\n+    k_ptrs = K + off_k\n+    v_ptrs = V + off_v\n+    # initialize pointer to m and l\n+    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n+    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+    # load q: it will stay in SRAM throughout\n+    q = tl.load(q_ptrs)\n+    # loop over k, v and update accumulator\n+    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n+        # -- compute qk ----\n+        k = tl.load(k_ptrs)\n+        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n+        qk += tl.dot(q, k)\n+        qk *= sm_scale\n+        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n+        # compute new m\n+        m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n+        # correct old l\n+        l_prev *= tl.exp(m_prev - m_curr)\n+        # attention weights\n+        p = tl.exp(qk - m_curr[:, None])\n+        l_curr = tl.sum(p, 1) + l_prev\n+        # rescale operands of matmuls\n+        l_rcp = 1. / l_curr\n+        p *= l_rcp[:, None]\n+        acc *= (l_prev * l_rcp)[:, None]\n+        # update acc\n+        p = p.to(Q.dtype.element_ty)\n+        v = tl.load(v_ptrs)\n+        acc += tl.dot(p, v)\n+        # update m_i and l_i\n+        l_prev = l_curr\n+        m_prev = m_curr\n+        # update pointers\n+        k_ptrs += BLOCK_N * stride_kn\n+        v_ptrs += BLOCK_N * stride_vk\n+    # rematerialize offsets to save registers\n+    start_m = tl.program_id(0)\n+    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+    # write back l and m\n+    l_ptrs = L + off_hz * N_CTX + offs_m\n+    m_ptrs = M + off_hz * N_CTX + offs_m\n+    tl.store(l_ptrs, l_prev)\n+    tl.store(m_ptrs, m_prev)\n+    # initialize pointers to output\n+    offs_n = tl.arange(0, BLOCK_DMODEL)\n+    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n+    out_ptrs = Out + off_o\n+    tl.store(out_ptrs, acc)\n+\n+\n+@triton.jit\n+def _bwd_preprocess(\n+    Out, DO, L,\n+    NewDO, Delta,\n+    BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\n+):\n+    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n+    off_n = tl.arange(0, D_HEAD)\n+    # load\n+    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n+    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n+    denom = tl.load(L + off_m).to(tl.float32)\n+    # compute\n+    do = do / denom[:, None]\n+    delta = tl.sum(o * do, axis=1)\n+    # write-back\n+    tl.store(NewDO + off_m[:, None] * D_HEAD + off_n[None, :], do)\n+    tl.store(Delta + off_m, delta)\n+\n+\n+@triton.jit\n+def _bwd_kernel(\n+    Q, K, V, sm_scale, Out, DO,\n+    DQ, DK, DV,\n+    L, M,\n+    D,\n+    stride_qz, stride_qh, stride_qm, stride_qk,\n+    stride_kz, stride_kh, stride_kn, stride_kk,\n+    stride_vz, stride_vh, stride_vk, stride_vn,\n+    Z, H, N_CTX,\n+    num_block,\n+    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n+    BLOCK_N: tl.constexpr,\n+):\n+    off_hz = tl.program_id(0)\n+    off_z = off_hz // H\n+    off_h = off_hz % H\n+    # offset pointers for batch/head\n+    Q += off_z * stride_qz + off_h * stride_qh\n+    K += off_z * stride_qz + off_h * stride_qh\n+    V += off_z * stride_qz + off_h * stride_qh\n+    DO += off_z * stride_qz + off_h * stride_qh\n+    DQ += off_z * stride_qz + off_h * stride_qh\n+    DK += off_z * stride_qz + off_h * stride_qh\n+    DV += off_z * stride_qz + off_h * stride_qh\n+    for start_n in range(0, num_block):\n+        lo = start_n * BLOCK_M\n+        # initialize row/col offsets\n+        offs_qm = lo + tl.arange(0, BLOCK_M)\n+        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n+        offs_m = tl.arange(0, BLOCK_N)\n+        offs_k = tl.arange(0, BLOCK_DMODEL)\n+        # initialize pointers to value-like data\n+        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n+        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        # pointer to row-wise quantities in value-like data\n+        D_ptrs = D + off_hz * N_CTX\n+        m_ptrs = M + off_hz * N_CTX\n+        # initialize dv amd dk\n+        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+        # k and v stay in SRAM throughout\n+        k = tl.load(k_ptrs)\n+        v = tl.load(v_ptrs)\n+        # loop over rows\n+        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n+            offs_m_curr = start_m + offs_m\n+            # load q, k, v, do on-chip\n+            q = tl.load(q_ptrs)\n+            # recompute p = softmax(qk, dim=-1).T\n+            # NOTE: `do` is pre-divided by `l`; no normalization here\n+            qk = tl.dot(q, tl.trans(k))\n+            qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\"-inf\"))\n+            m = tl.load(m_ptrs + offs_m_curr)\n+            p = tl.exp(qk * sm_scale - m[:, None])\n+            # compute dv\n+            do = tl.load(do_ptrs)\n+            dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\n+            # compute dp = dot(v, do)\n+            Di = tl.load(D_ptrs + offs_m_curr)\n+            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n+            dp += tl.dot(do, tl.trans(v))\n+            # compute ds = p * (dp - delta[:, None])\n+            ds = p * dp * sm_scale\n+            # compute dk = dot(ds.T, q)\n+            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)\n+            # compute dq\n+            dq = tl.load(dq_ptrs)\n+            dq += tl.dot(ds.to(Q.dtype.element_ty), k)\n+            tl.store(dq_ptrs, dq)\n+            # increment pointers\n+            dq_ptrs += BLOCK_M * stride_qm\n+            q_ptrs += BLOCK_M * stride_qm\n+            do_ptrs += BLOCK_M * stride_qm\n+        # write-back\n+        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n+        tl.store(dv_ptrs, dv)\n+        tl.store(dk_ptrs, dk)\n+\n+\n+empty = torch.empty(128, device=\"cuda\")\n+\n+\n+class _attention(torch.autograd.Function):\n+\n+    @staticmethod\n+    def forward(ctx, q, k, v, sm_scale):\n+        BLOCK = 128\n+        # shape constraints\n+        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n+        assert Lq == Lk and Lk == Lv\n+        assert Lk in {16, 32, 64, 128}\n+        o = torch.empty_like(q)\n+        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n+        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        num_warps = 4 if Lk <= 64 else 8\n+\n+        _fwd_kernel[grid](\n+            q, k, v, sm_scale,\n+            L, m,\n+            o,\n+            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n+            q.shape[0], q.shape[1], q.shape[2],\n+            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n+            BLOCK_DMODEL=Lk, num_warps=num_warps,\n+            num_stages=2,\n+        )\n+        # print(h.asm[\"ttgir\"])\n+\n+        ctx.save_for_backward(q, k, v, o, L, m)\n+        ctx.grid = grid\n+        ctx.sm_scale = sm_scale\n+        ctx.BLOCK_DMODEL = Lk\n+        return o\n+\n+    @staticmethod\n+    def backward(ctx, do):\n+        BLOCK = 128\n+        q, k, v, o, l, m = ctx.saved_tensors\n+        do = do.contiguous()\n+        dq = torch.zeros_like(q, dtype=torch.float32)\n+        dk = torch.empty_like(k)\n+        dv = torch.empty_like(v)\n+        do_scaled = torch.empty_like(do)\n+        delta = torch.empty_like(l)\n+        _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n+            o, do, l,\n+            do_scaled, delta,\n+            BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n+        )\n+        _bwd_kernel[(ctx.grid[1],)](\n+            q, k, v, ctx.sm_scale,\n+            o, do_scaled,\n+            dq, dk, dv,\n+            l, m,\n+            delta,\n+            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+            q.shape[0], q.shape[1], q.shape[2],\n+            ctx.grid[0],\n+            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n+            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n+            num_stages=1,\n+        )\n+        # print(h.asm[\"ttgir\"])\n+        return dq, dk, dv, None\n+\n+\n+attention = _attention.apply\n+\n+\n+@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n+def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n+    torch.manual_seed(20)\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n+    sm_scale = 0.2\n+    dout = torch.randn_like(q)\n+    # reference implementation\n+    M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n+    p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n+    for z in range(Z):\n+        for h in range(H):\n+            p[:, :, M == 0] = float(\"-inf\")\n+    p = torch.softmax(p.float(), dim=-1).half()\n+    # p = torch.exp(p)\n+    ref_out = torch.matmul(p, v)\n+    ref_out.backward(dout)\n+    ref_dv, v.grad = v.grad.clone(), None\n+    ref_dk, k.grad = k.grad.clone(), None\n+    ref_dq, q.grad = q.grad.clone(), None\n+    # # triton implementation\n+    tri_out = attention(q, k, v, sm_scale)\n+    # print(ref_out)\n+    # print(tri_out)\n+    tri_out.backward(dout)\n+    tri_dv, v.grad = v.grad.clone(), None\n+    tri_dk, k.grad = k.grad.clone(), None\n+    tri_dq, q.grad = q.grad.clone(), None\n+    # compare\n+    assert torch.allclose(ref_out, tri_out, atol=1e-2, rtol=0)\n+    assert torch.allclose(ref_dv, tri_dv, atol=1e-2, rtol=0)\n+    assert torch.allclose(ref_dk, tri_dk, atol=1e-2, rtol=0)\n+    assert torch.allclose(ref_dq, tri_dq, atol=1e-2, rtol=0)\n+\n+\n+try:\n+    from flash_attn.flash_attn_interface import flash_attn_func\n+    HAS_FLASH = True\n+except BaseException:\n+    HAS_FLASH = False\n+\n+BATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\n+# vary seq length for fixed head and batch=4\n+configs = [triton.testing.Benchmark(\n+    x_names=['N_CTX'],\n+    x_vals=[2**i for i in range(10, 14)],\n+    line_arg='provider',\n+    line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n+    line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\n+    styles=[('red', '-'), ('blue', '-')],\n+    ylabel='ms',\n+    plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n+    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode}\n+) for mode in ['fwd', 'bwd']]\n+\n+\n+@triton.testing.perf_report(configs)\n+def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.float16, device=\"cuda\"):\n+    assert mode in ['fwd', 'bwd']\n+    warmup = 25\n+    rep = 100\n+    if provider == \"triton\":\n+        q = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+        k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+        v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+        sm_scale = 1.3\n+        fn = lambda: attention(q, k, v, sm_scale)\n+        if mode == 'bwd':\n+            o = fn()\n+            do = torch.randn_like(o)\n+            fn = lambda: o.backward(do, retain_graph=True)\n+        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n+        return ms\n+    if provider == \"flash\":\n+        lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n+        cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n+        cu_seqlens[1:] = lengths.cumsum(0)\n+        qkv = torch.randn((BATCH * N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\n+        fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=True)\n+        if mode == 'bwd':\n+            o = fn()\n+            do = torch.randn_like(o)\n+            fn = lambda: o.backward(do, retain_graph=True)\n+        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n+        return ms\n+\n+\n+# only works on post-Ampere GPUs right now\n+bench_flash_attention.run(save_path='.', print_data=True)"}, {"filename": "main/_downloads/662999063954282841dc90b8945f85ce/tutorials_jupyter.zip", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/763344228ae6bc253ed1a6cf586aa30d/tutorials_python.zip", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_01-vector-add_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_01-vector-add_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_02-fused-softmax_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_02-fused-softmax_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_03-matrix-multiplication_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_03-matrix-multiplication_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_05-layer-norm_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_05-layer-norm_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_001.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_002.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_thumb.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_sources/getting-started/tutorials/01-vector-add.rst.txt", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -244,19 +244,19 @@ We can now run the decorated function above. Pass `print_data=True` to see the p\n     8     1048576.0   819.200021   819.200021\n     9     2097152.0  1023.999964  1023.999964\n     10    4194304.0  1228.800031  1228.800031\n-    11    8388608.0  1424.695621  1404.342820\n+    11    8388608.0  1424.695621  1424.695621\n     12   16777216.0  1560.380965  1560.380965\n-    13   33554432.0  1631.601649  1624.859540\n+    13   33554432.0  1624.859540  1624.859540\n     14   67108864.0  1669.706983  1662.646960\n-    15  134217728.0  1685.813499  1678.616907\n+    15  134217728.0  1684.008546  1678.616907\n \n \n \n \n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  24.542 seconds)\n+   **Total running time of the script:** ( 0 minutes  25.429 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_01-vector-add.py:"}, {"filename": "main/_sources/getting-started/tutorials/02-fused-softmax.rst.txt", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -305,17 +305,17 @@ We will then compare its performance against (1) :code:`torch.softmax` and (2) t\n     [16384] 0\n     softmax-performance:\n               N       Triton  Torch (native)  Torch (jit)\n-    0     256.0   682.666643      682.666643   256.000001\n-    1     384.0   877.714274      819.200021   323.368435\n+    0     256.0   682.666643      682.666643   264.258068\n+    1     384.0   877.714274      819.200021   332.108094\n     2     512.0   910.222190      910.222190   372.363633\n-    3     640.0   975.238103      930.909084   409.600010\n-    4     768.0  1068.521715     1023.999964   423.724127\n+    3     640.0   975.238103      930.909084   401.568635\n+    4     768.0  1068.521715     1023.999964   431.157886\n     ..      ...          ...             ...          ...\n-    93  12160.0  1594.754129     1069.010969   590.470399\n-    94  12288.0  1598.438956     1016.061996   589.971483\n+    93  12160.0  1594.754129     1069.010969   591.367777\n+    94  12288.0  1598.438956     1018.694301   591.302256\n     95  12416.0  1582.916395     1029.305700   587.739623\n     96  12544.0  1580.346374     1013.656595   589.439056\n-    97  12672.0  1584.000004     1006.213368   589.395349\n+    97  12672.0  1584.000004     1008.716405   590.253282\n \n     [98 rows x 4 columns]\n \n@@ -332,7 +332,7 @@ In the above plot, we can see that:\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 1 minutes  17.871 seconds)\n+   **Total running time of the script:** ( 1 minutes  18.602 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:"}, {"filename": "main/_sources/getting-started/tutorials/03-matrix-multiplication.rst.txt", "status": "modified", "additions": 19, "deletions": 19, "changes": 38, "file_content_changes": "@@ -456,37 +456,37 @@ We can now compare the performance of our kernel against that of cuBLAS. Here we\n     5    896.0   78.051553   82.642822\n     6   1024.0  104.857603   99.864382\n     7   1152.0  135.726544  129.825388\n-    8   1280.0  157.538463  163.840004\n+    8   1280.0  163.840004  163.840004\n     9   1408.0  155.765024  132.970149\n-    10  1536.0  181.484314  157.286398\n+    10  1536.0  176.947204  157.286398\n     11  1664.0  179.978245  179.978245\n     12  1792.0  172.914215  208.137481\n-    13  1920.0  203.294114  168.585369\n-    14  2048.0  197.379013  192.841562\n+    13  1920.0  200.347822  168.585369\n+    14  2048.0  197.379013  190.650180\n     15  2176.0  189.845737  209.621326\n-    16  2304.0  227.503545  229.691080\n-    17  2432.0  202.848105  200.674737\n-    18  2560.0  224.438347  218.453323\n+    16  2304.0  225.357284  227.503545\n+    17  2432.0  202.118452  200.674737\n+    18  2560.0  222.911566  217.006622\n     19  2688.0  196.544332  197.567993\n-    20  2816.0  209.683695  209.683695\n-    21  2944.0  218.579083  224.486628\n-    22  3072.0  205.902197  208.173173\n-    23  3200.0  212.624590  219.931269\n-    24  3328.0  205.103410  206.871539\n-    25  3456.0  213.850319  216.724640\n-    26  3584.0  218.241246  211.068989\n-    27  3712.0  207.686788  215.295995\n-    28  3840.0  207.879708  211.659322\n-    29  3968.0  209.663117  216.738793\n-    30  4096.0  218.952244  214.748370\n+    20  2816.0  208.680416  210.696652\n+    21  2944.0  221.493479  224.486628\n+    22  3072.0  207.410628  208.173173\n+    23  3200.0  214.046818  219.178074\n+    24  3328.0  205.103410  208.670419\n+    25  3456.0  215.565692  216.724640\n+    26  3584.0  215.624440  211.068989\n+    27  3712.0  207.686788  217.404445\n+    28  3840.0  208.271176  206.328356\n+    29  3968.0  207.877238  216.738793\n+    30  4096.0  221.847481  214.748370\n \n \n \n \n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 1 minutes  36.561 seconds)\n+   **Total running time of the script:** ( 1 minutes  37.087 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_03-matrix-multiplication.py:"}, {"filename": "main/_sources/getting-started/tutorials/04-low-memory-dropout.rst.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -240,7 +240,7 @@ References\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  1.029 seconds)\n+   **Total running time of the script:** ( 0 minutes  1.060 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_04-low-memory-dropout.py:"}, {"filename": "main/_sources/getting-started/tutorials/05-layer-norm.rst.txt", "status": "modified", "additions": 17, "deletions": 17, "changes": 34, "file_content_changes": "@@ -440,35 +440,35 @@ Specifically, one can set :code:`'mode': 'backward'` to benchmark the backward p\n     [16384] 0\n     layer-norm-backward:\n               N      Triton       Torch\n-    0    1024.0  245.760006  356.173905\n+    0    1024.0  236.307695  356.173905\n     1    1536.0  361.411771  400.695643\n     2    2048.0  455.111110  438.857137\n     3    2560.0  534.260858  469.007657\n     4    3072.0  619.563043  501.551024\n     5    3584.0  688.127967  443.381459\n     6    4096.0  744.727267  436.906674\n     7    4608.0  704.407633  437.122520\n-    8    5120.0  758.518534  443.610086\n-    9    5632.0  804.571435  459.755106\n-    10   6144.0  842.605744  466.632925\n-    11   6656.0  882.563556  471.221251\n-    12   7168.0  919.957230  452.715775\n+    8    5120.0  763.229797  443.610086\n+    9    5632.0  809.389194  459.755106\n+    10   6144.0  842.605744  465.160886\n+    11   6656.0  882.563556  472.615367\n+    12   7168.0  917.503992  452.715775\n     13   7680.0  950.103127  443.076928\n-    14   8192.0  987.979890  464.794337\n+    14   8192.0  983.040025  464.794337\n     15   8704.0  663.161879  459.112087\n     16   9216.0  686.906817  466.632911\n-    17   9728.0  713.981680  472.615406\n+    17   9728.0  711.804890  472.615406\n     18  10240.0  731.428577  468.114302\n-    19  10752.0  758.964709  467.478250\n+    19  10752.0  761.203560  467.478250\n     20  11264.0  781.317950  470.149555\n-    21  11776.0  802.909085  472.615374\n-    22  12288.0  826.084006  471.859211\n-    23  12800.0  830.270283  472.615383\n-    24  13312.0  847.448300  478.275433\n-    25  13824.0  852.894616  478.063409\n+    21  11776.0  802.909085  471.826361\n+    22  12288.0  828.404485  471.105436\n+    23  12800.0  832.520355  471.889394\n+    24  13312.0  847.448300  477.560558\n+    25  13824.0  848.531950  478.753261\n     26  14336.0  851.643583  477.203871\n-    27  14848.0  854.561145  477.044187\n-    28  15360.0  863.325544  476.279061\n+    27  14848.0  852.516725  477.683661\n+    28  15360.0  865.352086  476.279061\n     29  15872.0  867.717548  479.758191\n \n \n@@ -484,7 +484,7 @@ References\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  38.887 seconds)\n+   **Total running time of the script:** ( 0 minutes  39.183 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_05-layer-norm.py:"}, {"filename": "main/_sources/getting-started/tutorials/06-fused-attention.rst.txt", "status": "added", "additions": 461, "deletions": 0, "changes": 461, "file_content_changes": "@@ -0,0 +1,461 @@\n+\n+.. DO NOT EDIT.\n+.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.\n+.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:\n+.. \"getting-started/tutorials/06-fused-attention.py\"\n+.. LINE NUMBERS ARE GIVEN BELOW.\n+\n+.. only:: html\n+\n+    .. note::\n+        :class: sphx-glr-download-link-note\n+\n+        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_06-fused-attention.py>`\n+        to download the full example code\n+\n+.. rst-class:: sphx-glr-example-title\n+\n+.. _sphx_glr_getting-started_tutorials_06-fused-attention.py:\n+\n+\n+Fused Attention\n+===============\n+\n+This is a Triton implementation of the Flash Attention algorithm\n+(see: Dao et al., https://arxiv.org/pdf/2205.14135v2.pdf; Rabe and Staats https://arxiv.org/pdf/2112.05682v2.pdf)\n+\n+.. GENERATED FROM PYTHON SOURCE LINES 8-362\n+\n+\n+\n+.. rst-class:: sphx-glr-horizontal\n+\n+\n+    *\n+\n+      .. image-sg:: /getting-started/tutorials/images/sphx_glr_06-fused-attention_001.png\n+         :alt: 06 fused attention\n+         :srcset: /getting-started/tutorials/images/sphx_glr_06-fused-attention_001.png\n+         :class: sphx-glr-multi-img\n+\n+    *\n+\n+      .. image-sg:: /getting-started/tutorials/images/sphx_glr_06-fused-attention_002.png\n+         :alt: 06 fused attention\n+         :srcset: /getting-started/tutorials/images/sphx_glr_06-fused-attention_002.png\n+         :class: sphx-glr-multi-img\n+\n+\n+.. rst-class:: sphx-glr-script-out\n+\n+ .. code-block:: none\n+\n+    [128, 128] 1\n+    [128, 128] 1\n+    fused-attention-batch4-head48-d64-fwd:\n+        N_CTX     Triton\n+    0  1024.0   0.323929\n+    1  2048.0   1.086871\n+    2  4096.0   3.919503\n+    3  8192.0  14.976001\n+    [128, 64] 1\n+    fused-attention-batch4-head48-d64-bwd:\n+        N_CTX     Triton\n+    0  1024.0   1.184640\n+    1  2048.0   3.757174\n+    2  4096.0  13.251730\n+    3  8192.0  49.541634\n+\n+\n+\n+\n+\n+\n+|\n+\n+.. code-block:: default\n+\n+\n+    import pytest\n+    import torch\n+\n+    import triton\n+    import triton.language as tl\n+\n+\n+    @triton.jit\n+    def _fwd_kernel(\n+        Q, K, V, sm_scale,\n+        L, M,\n+        Out,\n+        stride_qz, stride_qh, stride_qm, stride_qk,\n+        stride_kz, stride_kh, stride_kn, stride_kk,\n+        stride_vz, stride_vh, stride_vk, stride_vn,\n+        stride_oz, stride_oh, stride_om, stride_on,\n+        Z, H, N_CTX,\n+        BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n+        BLOCK_N: tl.constexpr,\n+    ):\n+        start_m = tl.program_id(0)\n+        off_hz = tl.program_id(1)\n+        # initialize offsets\n+        offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+        offs_n = tl.arange(0, BLOCK_N)\n+        offs_d = tl.arange(0, BLOCK_DMODEL)\n+        off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n+        off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n+        off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n+        # Initialize pointers to Q, K, V\n+        q_ptrs = Q + off_q\n+        k_ptrs = K + off_k\n+        v_ptrs = V + off_v\n+        # initialize pointer to m and l\n+        m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+        l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n+        acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+        # load q: it will stay in SRAM throughout\n+        q = tl.load(q_ptrs)\n+        # loop over k, v and update accumulator\n+        for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n+            # -- compute qk ----\n+            k = tl.load(k_ptrs)\n+            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n+            qk += tl.dot(q, k)\n+            qk *= sm_scale\n+            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n+            # compute new m\n+            m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n+            # correct old l\n+            l_prev *= tl.exp(m_prev - m_curr)\n+            # attention weights\n+            p = tl.exp(qk - m_curr[:, None])\n+            l_curr = tl.sum(p, 1) + l_prev\n+            # rescale operands of matmuls\n+            l_rcp = 1. / l_curr\n+            p *= l_rcp[:, None]\n+            acc *= (l_prev * l_rcp)[:, None]\n+            # update acc\n+            p = p.to(Q.dtype.element_ty)\n+            v = tl.load(v_ptrs)\n+            acc += tl.dot(p, v)\n+            # update m_i and l_i\n+            l_prev = l_curr\n+            m_prev = m_curr\n+            # update pointers\n+            k_ptrs += BLOCK_N * stride_kn\n+            v_ptrs += BLOCK_N * stride_vk\n+        # rematerialize offsets to save registers\n+        start_m = tl.program_id(0)\n+        offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+        # write back l and m\n+        l_ptrs = L + off_hz * N_CTX + offs_m\n+        m_ptrs = M + off_hz * N_CTX + offs_m\n+        tl.store(l_ptrs, l_prev)\n+        tl.store(m_ptrs, m_prev)\n+        # initialize pointers to output\n+        offs_n = tl.arange(0, BLOCK_DMODEL)\n+        off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n+        out_ptrs = Out + off_o\n+        tl.store(out_ptrs, acc)\n+\n+\n+    @triton.jit\n+    def _bwd_preprocess(\n+        Out, DO, L,\n+        NewDO, Delta,\n+        BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\n+    ):\n+        off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n+        off_n = tl.arange(0, D_HEAD)\n+        # load\n+        o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n+        do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n+        denom = tl.load(L + off_m).to(tl.float32)\n+        # compute\n+        do = do / denom[:, None]\n+        delta = tl.sum(o * do, axis=1)\n+        # write-back\n+        tl.store(NewDO + off_m[:, None] * D_HEAD + off_n[None, :], do)\n+        tl.store(Delta + off_m, delta)\n+\n+\n+    @triton.jit\n+    def _bwd_kernel(\n+        Q, K, V, sm_scale, Out, DO,\n+        DQ, DK, DV,\n+        L, M,\n+        D,\n+        stride_qz, stride_qh, stride_qm, stride_qk,\n+        stride_kz, stride_kh, stride_kn, stride_kk,\n+        stride_vz, stride_vh, stride_vk, stride_vn,\n+        Z, H, N_CTX,\n+        num_block,\n+        BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n+        BLOCK_N: tl.constexpr,\n+    ):\n+        off_hz = tl.program_id(0)\n+        off_z = off_hz // H\n+        off_h = off_hz % H\n+        # offset pointers for batch/head\n+        Q += off_z * stride_qz + off_h * stride_qh\n+        K += off_z * stride_qz + off_h * stride_qh\n+        V += off_z * stride_qz + off_h * stride_qh\n+        DO += off_z * stride_qz + off_h * stride_qh\n+        DQ += off_z * stride_qz + off_h * stride_qh\n+        DK += off_z * stride_qz + off_h * stride_qh\n+        DV += off_z * stride_qz + off_h * stride_qh\n+        for start_n in range(0, num_block):\n+            lo = start_n * BLOCK_M\n+            # initialize row/col offsets\n+            offs_qm = lo + tl.arange(0, BLOCK_M)\n+            offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n+            offs_m = tl.arange(0, BLOCK_N)\n+            offs_k = tl.arange(0, BLOCK_DMODEL)\n+            # initialize pointers to value-like data\n+            q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+            k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n+            v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+            do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+            dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+            # pointer to row-wise quantities in value-like data\n+            D_ptrs = D + off_hz * N_CTX\n+            m_ptrs = M + off_hz * N_CTX\n+            # initialize dv amd dk\n+            dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+            dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+            # k and v stay in SRAM throughout\n+            k = tl.load(k_ptrs)\n+            v = tl.load(v_ptrs)\n+            # loop over rows\n+            for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n+                offs_m_curr = start_m + offs_m\n+                # load q, k, v, do on-chip\n+                q = tl.load(q_ptrs)\n+                # recompute p = softmax(qk, dim=-1).T\n+                # NOTE: `do` is pre-divided by `l`; no normalization here\n+                qk = tl.dot(q, tl.trans(k))\n+                qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\"-inf\"))\n+                m = tl.load(m_ptrs + offs_m_curr)\n+                p = tl.exp(qk * sm_scale - m[:, None])\n+                # compute dv\n+                do = tl.load(do_ptrs)\n+                dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\n+                # compute dp = dot(v, do)\n+                Di = tl.load(D_ptrs + offs_m_curr)\n+                dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n+                dp += tl.dot(do, tl.trans(v))\n+                # compute ds = p * (dp - delta[:, None])\n+                ds = p * dp * sm_scale\n+                # compute dk = dot(ds.T, q)\n+                dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)\n+                # compute dq\n+                dq = tl.load(dq_ptrs)\n+                dq += tl.dot(ds.to(Q.dtype.element_ty), k)\n+                tl.store(dq_ptrs, dq)\n+                # increment pointers\n+                dq_ptrs += BLOCK_M * stride_qm\n+                q_ptrs += BLOCK_M * stride_qm\n+                do_ptrs += BLOCK_M * stride_qm\n+            # write-back\n+            dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+            dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n+            tl.store(dv_ptrs, dv)\n+            tl.store(dk_ptrs, dk)\n+\n+\n+    empty = torch.empty(128, device=\"cuda\")\n+\n+\n+    class _attention(torch.autograd.Function):\n+\n+        @staticmethod\n+        def forward(ctx, q, k, v, sm_scale):\n+            BLOCK = 128\n+            # shape constraints\n+            Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n+            assert Lq == Lk and Lk == Lv\n+            assert Lk in {16, 32, 64, 128}\n+            o = torch.empty_like(q)\n+            grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n+            L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+            m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+            num_warps = 4 if Lk <= 64 else 8\n+\n+            _fwd_kernel[grid](\n+                q, k, v, sm_scale,\n+                L, m,\n+                o,\n+                q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+                k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+                v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+                o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n+                q.shape[0], q.shape[1], q.shape[2],\n+                BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n+                BLOCK_DMODEL=Lk, num_warps=num_warps,\n+                num_stages=2,\n+            )\n+            # print(h.asm[\"ttgir\"])\n+\n+            ctx.save_for_backward(q, k, v, o, L, m)\n+            ctx.grid = grid\n+            ctx.sm_scale = sm_scale\n+            ctx.BLOCK_DMODEL = Lk\n+            return o\n+\n+        @staticmethod\n+        def backward(ctx, do):\n+            BLOCK = 128\n+            q, k, v, o, l, m = ctx.saved_tensors\n+            do = do.contiguous()\n+            dq = torch.zeros_like(q, dtype=torch.float32)\n+            dk = torch.empty_like(k)\n+            dv = torch.empty_like(v)\n+            do_scaled = torch.empty_like(do)\n+            delta = torch.empty_like(l)\n+            _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n+                o, do, l,\n+                do_scaled, delta,\n+                BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n+            )\n+            _bwd_kernel[(ctx.grid[1],)](\n+                q, k, v, ctx.sm_scale,\n+                o, do_scaled,\n+                dq, dk, dv,\n+                l, m,\n+                delta,\n+                q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+                k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+                v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+                q.shape[0], q.shape[1], q.shape[2],\n+                ctx.grid[0],\n+                BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n+                BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n+                num_stages=1,\n+            )\n+            # print(h.asm[\"ttgir\"])\n+            return dq, dk, dv, None\n+\n+\n+    attention = _attention.apply\n+\n+\n+    @pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n+    def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n+        torch.manual_seed(20)\n+        q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n+        k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n+        v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n+        sm_scale = 0.2\n+        dout = torch.randn_like(q)\n+        # reference implementation\n+        M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n+        p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n+        for z in range(Z):\n+            for h in range(H):\n+                p[:, :, M == 0] = float(\"-inf\")\n+        p = torch.softmax(p.float(), dim=-1).half()\n+        # p = torch.exp(p)\n+        ref_out = torch.matmul(p, v)\n+        ref_out.backward(dout)\n+        ref_dv, v.grad = v.grad.clone(), None\n+        ref_dk, k.grad = k.grad.clone(), None\n+        ref_dq, q.grad = q.grad.clone(), None\n+        # # triton implementation\n+        tri_out = attention(q, k, v, sm_scale)\n+        # print(ref_out)\n+        # print(tri_out)\n+        tri_out.backward(dout)\n+        tri_dv, v.grad = v.grad.clone(), None\n+        tri_dk, k.grad = k.grad.clone(), None\n+        tri_dq, q.grad = q.grad.clone(), None\n+        # compare\n+        assert torch.allclose(ref_out, tri_out, atol=1e-2, rtol=0)\n+        assert torch.allclose(ref_dv, tri_dv, atol=1e-2, rtol=0)\n+        assert torch.allclose(ref_dk, tri_dk, atol=1e-2, rtol=0)\n+        assert torch.allclose(ref_dq, tri_dq, atol=1e-2, rtol=0)\n+\n+\n+    try:\n+        from flash_attn.flash_attn_interface import flash_attn_func\n+        HAS_FLASH = True\n+    except BaseException:\n+        HAS_FLASH = False\n+\n+    BATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\n+    # vary seq length for fixed head and batch=4\n+    configs = [triton.testing.Benchmark(\n+        x_names=['N_CTX'],\n+        x_vals=[2**i for i in range(10, 14)],\n+        line_arg='provider',\n+        line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n+        line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\n+        styles=[('red', '-'), ('blue', '-')],\n+        ylabel='ms',\n+        plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n+        args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode}\n+    ) for mode in ['fwd', 'bwd']]\n+\n+\n+    @triton.testing.perf_report(configs)\n+    def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.float16, device=\"cuda\"):\n+        assert mode in ['fwd', 'bwd']\n+        warmup = 25\n+        rep = 100\n+        if provider == \"triton\":\n+            q = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+            k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+            v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+            sm_scale = 1.3\n+            fn = lambda: attention(q, k, v, sm_scale)\n+            if mode == 'bwd':\n+                o = fn()\n+                do = torch.randn_like(o)\n+                fn = lambda: o.backward(do, retain_graph=True)\n+            ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n+            return ms\n+        if provider == \"flash\":\n+            lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n+            cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n+            cu_seqlens[1:] = lengths.cumsum(0)\n+            qkv = torch.randn((BATCH * N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\n+            fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=True)\n+            if mode == 'bwd':\n+                o = fn()\n+                do = torch.randn_like(o)\n+                fn = lambda: o.backward(do, retain_graph=True)\n+            ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n+            return ms\n+\n+\n+    # only works on post-Ampere GPUs right now\n+    bench_flash_attention.run(save_path='.', print_data=True)\n+\n+\n+.. rst-class:: sphx-glr-timing\n+\n+   **Total running time of the script:** ( 0 minutes  5.605 seconds)\n+\n+\n+.. _sphx_glr_download_getting-started_tutorials_06-fused-attention.py:\n+\n+.. only:: html\n+\n+  .. container:: sphx-glr-footer sphx-glr-footer-example\n+\n+\n+\n+\n+    .. container:: sphx-glr-download sphx-glr-download-python\n+\n+      :download:`Download Python source code: 06-fused-attention.py <06-fused-attention.py>`\n+\n+    .. container:: sphx-glr-download sphx-glr-download-jupyter\n+\n+      :download:`Download Jupyter notebook: 06-fused-attention.ipynb <06-fused-attention.ipynb>`\n+\n+\n+.. only:: html\n+\n+ .. rst-class:: sphx-glr-signature\n+\n+    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"}, {"filename": "main/_sources/getting-started/tutorials/07-math-functions.rst.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -147,7 +147,7 @@ We can also customize the libdevice library path by passing the path to the `lib\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  0.373 seconds)\n+   **Total running time of the script:** ( 0 minutes  0.390 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_07-math-functions.py:"}, {"filename": "main/_sources/getting-started/tutorials/index.rst.txt", "status": "modified", "additions": 18, "deletions": 0, "changes": 18, "file_content_changes": "@@ -104,6 +104,23 @@ To install the dependencies for the tutorials:\n     </div>\n \n \n+.. raw:: html\n+\n+    <div class=\"sphx-glr-thumbcontainer\" tooltip=\"This is a Triton implementation of the Flash Attention algorithm (see: Dao et al., https://arxi...\">\n+\n+.. only:: html\n+\n+  .. image:: /getting-started/tutorials/images/thumb/sphx_glr_06-fused-attention_thumb.png\n+    :alt:\n+\n+  :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py`\n+\n+.. raw:: html\n+\n+      <div class=\"sphx-glr-thumbnail-title\">Fused Attention</div>\n+    </div>\n+\n+\n .. raw:: html\n \n     <div class=\"sphx-glr-thumbcontainer\" tooltip=\"Libdevice (`tl.math`) function\">\n@@ -134,6 +151,7 @@ To install the dependencies for the tutorials:\n    /getting-started/tutorials/03-matrix-multiplication\n    /getting-started/tutorials/04-low-memory-dropout\n    /getting-started/tutorials/05-layer-norm\n+   /getting-started/tutorials/06-fused-attention\n    /getting-started/tutorials/07-math-functions\n \n "}, {"filename": "main/_sources/getting-started/tutorials/sg_execution_times.rst.txt", "status": "modified", "additions": 9, "deletions": 7, "changes": 16, "file_content_changes": "@@ -6,18 +6,20 @@\n \n Computation times\n =================\n-**03:59.262** total execution time for **getting-started_tutorials** files:\n+**04:07.355** total execution time for **getting-started_tutorials** files:\n \n +---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``) | 01:36.561 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``) | 01:37.087 | 0.0 MB |\n +---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                 | 01:17.871 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                 | 01:18.602 | 0.0 MB |\n +---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                       | 00:38.887 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                       | 00:39.183 | 0.0 MB |\n +---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                       | 00:24.542 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                       | 00:25.429 | 0.0 MB |\n +---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)       | 00:01.029 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py` (``06-fused-attention.py``)             | 00:05.605 | 0.0 MB |\n +---------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_07-math-functions.py` (``07-math-functions.py``)               | 00:00.373 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)       | 00:01.060 | 0.0 MB |\n++---------------------------------------------------------------------------------------------------------+-----------+--------+\n+| :ref:`sphx_glr_getting-started_tutorials_07-math-functions.py` (``07-math-functions.py``)               | 00:00.390 | 0.0 MB |\n +---------------------------------------------------------------------------------------------------------+-----------+--------+"}, {"filename": "main/getting-started/tutorials/01-vector-add.html", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -58,6 +58,7 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n </ul>\n </li>\n@@ -242,14 +243,14 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n 8     1048576.0   819.200021   819.200021\n 9     2097152.0  1023.999964  1023.999964\n 10    4194304.0  1228.800031  1228.800031\n-11    8388608.0  1424.695621  1404.342820\n+11    8388608.0  1424.695621  1424.695621\n 12   16777216.0  1560.380965  1560.380965\n-13   33554432.0  1631.601649  1624.859540\n+13   33554432.0  1624.859540  1624.859540\n 14   67108864.0  1669.706983  1662.646960\n-15  134217728.0  1685.813499  1678.616907\n+15  134217728.0  1684.008546  1678.616907\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  24.542 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  25.429 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-01-vector-add-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/62d97d49a32414049819dd8bb8378080/01-vector-add.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">01-vector-add.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/02-fused-softmax.html", "status": "modified", "additions": 9, "deletions": 8, "changes": 17, "file_content_changes": "@@ -61,6 +61,7 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n </ul>\n </li>\n@@ -296,17 +297,17 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n [16384] 0\n softmax-performance:\n           N       Triton  Torch (native)  Torch (jit)\n-0     256.0   682.666643      682.666643   256.000001\n-1     384.0   877.714274      819.200021   323.368435\n+0     256.0   682.666643      682.666643   264.258068\n+1     384.0   877.714274      819.200021   332.108094\n 2     512.0   910.222190      910.222190   372.363633\n-3     640.0   975.238103      930.909084   409.600010\n-4     768.0  1068.521715     1023.999964   423.724127\n+3     640.0   975.238103      930.909084   401.568635\n+4     768.0  1068.521715     1023.999964   431.157886\n ..      ...          ...             ...          ...\n-93  12160.0  1594.754129     1069.010969   590.470399\n-94  12288.0  1598.438956     1016.061996   589.971483\n+93  12160.0  1594.754129     1069.010969   591.367777\n+94  12288.0  1598.438956     1018.694301   591.302256\n 95  12416.0  1582.916395     1029.305700   587.739623\n 96  12544.0  1580.346374     1013.656595   589.439056\n-97  12672.0  1584.000004     1006.213368   589.395349\n+97  12672.0  1584.000004     1008.716405   590.253282\n \n [98 rows x 4 columns]\n </pre></div>\n@@ -319,7 +320,7 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n </ul>\n </dd>\n </dl>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  17.871 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  18.602 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-02-fused-softmax-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d91442ac2982c4e0cc3ab0f43534afbc/02-fused-softmax.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">02-fused-softmax.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/03-matrix-multiplication.html", "status": "modified", "additions": 20, "deletions": 19, "changes": 39, "file_content_changes": "@@ -68,6 +68,7 @@\n </li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n </ul>\n </li>\n@@ -470,32 +471,32 @@ <h3>Square Matrix Performance<a class=\"headerlink\" href=\"#square-matrix-performa\n 5    896.0   78.051553   82.642822\n 6   1024.0  104.857603   99.864382\n 7   1152.0  135.726544  129.825388\n-8   1280.0  157.538463  163.840004\n+8   1280.0  163.840004  163.840004\n 9   1408.0  155.765024  132.970149\n-10  1536.0  181.484314  157.286398\n+10  1536.0  176.947204  157.286398\n 11  1664.0  179.978245  179.978245\n 12  1792.0  172.914215  208.137481\n-13  1920.0  203.294114  168.585369\n-14  2048.0  197.379013  192.841562\n+13  1920.0  200.347822  168.585369\n+14  2048.0  197.379013  190.650180\n 15  2176.0  189.845737  209.621326\n-16  2304.0  227.503545  229.691080\n-17  2432.0  202.848105  200.674737\n-18  2560.0  224.438347  218.453323\n+16  2304.0  225.357284  227.503545\n+17  2432.0  202.118452  200.674737\n+18  2560.0  222.911566  217.006622\n 19  2688.0  196.544332  197.567993\n-20  2816.0  209.683695  209.683695\n-21  2944.0  218.579083  224.486628\n-22  3072.0  205.902197  208.173173\n-23  3200.0  212.624590  219.931269\n-24  3328.0  205.103410  206.871539\n-25  3456.0  213.850319  216.724640\n-26  3584.0  218.241246  211.068989\n-27  3712.0  207.686788  215.295995\n-28  3840.0  207.879708  211.659322\n-29  3968.0  209.663117  216.738793\n-30  4096.0  218.952244  214.748370\n+20  2816.0  208.680416  210.696652\n+21  2944.0  221.493479  224.486628\n+22  3072.0  207.410628  208.173173\n+23  3200.0  214.046818  219.178074\n+24  3328.0  205.103410  208.670419\n+25  3456.0  215.565692  216.724640\n+26  3584.0  215.624440  211.068989\n+27  3712.0  207.686788  217.404445\n+28  3840.0  208.271176  206.328356\n+29  3968.0  207.877238  216.738793\n+30  4096.0  221.847481  214.748370\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  36.561 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  37.087 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-03-matrix-multiplication-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d5fee5b55a64e47f1b5724ec39adf171/03-matrix-multiplication.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">03-matrix-multiplication.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/04-low-memory-dropout.html", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -61,6 +61,7 @@\n </ul>\n </li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n </ul>\n </li>\n@@ -283,7 +284,7 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n <p>Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \u201cDropout: A Simple Way to Prevent Neural Networks from Overfitting\u201d, JMLR 2014</p>\n </div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  1.029 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  1.060 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-04-low-memory-dropout-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/c9aed78977a4c05741d675a38dde3d7d/04-low-memory-dropout.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">04-low-memory-dropout.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/05-layer-norm.html", "status": "modified", "additions": 20, "deletions": 19, "changes": 39, "file_content_changes": "@@ -23,7 +23,7 @@\n     <script src=\"../../_static/js/theme.js\"></script>\n     <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n     <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n-    <link rel=\"next\" title=\"Libdevice (tl.math) function\" href=\"07-math-functions.html\" />\n+    <link rel=\"next\" title=\"Fused Attention\" href=\"06-fused-attention.html\" />\n     <link rel=\"prev\" title=\"Low-Memory Dropout\" href=\"04-low-memory-dropout.html\" /> \n </head>\n \n@@ -61,6 +61,7 @@\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#references\">References</a></li>\n </ul>\n </li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n </ul>\n </li>\n@@ -467,35 +468,35 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n [16384] 0\n layer-norm-backward:\n           N      Triton       Torch\n-0    1024.0  245.760006  356.173905\n+0    1024.0  236.307695  356.173905\n 1    1536.0  361.411771  400.695643\n 2    2048.0  455.111110  438.857137\n 3    2560.0  534.260858  469.007657\n 4    3072.0  619.563043  501.551024\n 5    3584.0  688.127967  443.381459\n 6    4096.0  744.727267  436.906674\n 7    4608.0  704.407633  437.122520\n-8    5120.0  758.518534  443.610086\n-9    5632.0  804.571435  459.755106\n-10   6144.0  842.605744  466.632925\n-11   6656.0  882.563556  471.221251\n-12   7168.0  919.957230  452.715775\n+8    5120.0  763.229797  443.610086\n+9    5632.0  809.389194  459.755106\n+10   6144.0  842.605744  465.160886\n+11   6656.0  882.563556  472.615367\n+12   7168.0  917.503992  452.715775\n 13   7680.0  950.103127  443.076928\n-14   8192.0  987.979890  464.794337\n+14   8192.0  983.040025  464.794337\n 15   8704.0  663.161879  459.112087\n 16   9216.0  686.906817  466.632911\n-17   9728.0  713.981680  472.615406\n+17   9728.0  711.804890  472.615406\n 18  10240.0  731.428577  468.114302\n-19  10752.0  758.964709  467.478250\n+19  10752.0  761.203560  467.478250\n 20  11264.0  781.317950  470.149555\n-21  11776.0  802.909085  472.615374\n-22  12288.0  826.084006  471.859211\n-23  12800.0  830.270283  472.615383\n-24  13312.0  847.448300  478.275433\n-25  13824.0  852.894616  478.063409\n+21  11776.0  802.909085  471.826361\n+22  12288.0  828.404485  471.105436\n+23  12800.0  832.520355  471.889394\n+24  13312.0  847.448300  477.560558\n+25  13824.0  848.531950  478.753261\n 26  14336.0  851.643583  477.203871\n-27  14848.0  854.561145  477.044187\n-28  15360.0  863.325544  476.279061\n+27  14848.0  852.516725  477.683661\n+28  15360.0  865.352086  476.279061\n 29  15872.0  867.717548  479.758191\n </pre></div>\n </div>\n@@ -508,7 +509,7 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n <p>Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \u201cLayer Normalization\u201d, Arxiv 2016</p>\n </div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  38.887 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  39.183 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-05-layer-norm-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/935c0dd0fbeb4b2e69588471cbb2d4b2/05-layer-norm.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">05-layer-norm.py</span></code></a></p>\n@@ -526,7 +527,7 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n           </div>\n           <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n         <a href=\"04-low-memory-dropout.html\" class=\"btn btn-neutral float-left\" title=\"Low-Memory Dropout\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n-        <a href=\"07-math-functions.html\" class=\"btn btn-neutral float-right\" title=\"Libdevice (tl.math) function\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+        <a href=\"06-fused-attention.html\" class=\"btn btn-neutral float-right\" title=\"Fused Attention\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n     </div>\n \n   <hr/>"}, {"filename": "main/getting-started/tutorials/06-fused-attention.html", "status": "added", "additions": 543, "deletions": 0, "changes": 543, "file_content_changes": "@@ -0,0 +1,543 @@\n+<!DOCTYPE html>\n+<html class=\"writer-html5\" lang=\"en\" >\n+<head>\n+  <meta charset=\"utf-8\" /><meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n+\n+  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n+  <title>Fused Attention &mdash; Triton  documentation</title>\n+      <link rel=\"stylesheet\" href=\"../../_static/pygments.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/theme.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-binder.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-dataframe.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/sg_gallery-rendered-html.css\" type=\"text/css\" />\n+      <link rel=\"stylesheet\" href=\"../../_static/css/custom.css\" type=\"text/css\" />\n+  <!--[if lt IE 9]>\n+    <script src=\"../../_static/js/html5shiv.min.js\"></script>\n+  <![endif]-->\n+  \n+        <script data-url_root=\"../../\" id=\"documentation_options\" src=\"../../_static/documentation_options.js\"></script>\n+        <script src=\"../../_static/doctools.js\"></script>\n+        <script src=\"../../_static/sphinx_highlight.js\"></script>\n+    <script src=\"../../_static/js/theme.js\"></script>\n+    <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n+    <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n+    <link rel=\"next\" title=\"Libdevice (tl.math) function\" href=\"07-math-functions.html\" />\n+    <link rel=\"prev\" title=\"Layer Normalization\" href=\"05-layer-norm.html\" /> \n+</head>\n+\n+<body class=\"wy-body-for-nav\"> \n+  <div class=\"wy-grid-for-nav\">\n+    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\n+      <div class=\"wy-side-scroll\">\n+        <div class=\"wy-side-nav-search\" >\n+\n+          \n+          \n+          <a href=\"../../index.html\" class=\"icon icon-home\">\n+            Triton\n+          </a>\n+<div role=\"search\">\n+  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"../../search.html\" method=\"get\">\n+    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" aria-label=\"Search docs\" />\n+    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n+    <input type=\"hidden\" name=\"area\" value=\"default\" />\n+  </form>\n+</div>\n+        </div><div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"Navigation menu\">\n+              <p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n+<ul class=\"current\">\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../installation.html\">Installation</a></li>\n+<li class=\"toctree-l1 current\"><a class=\"reference internal\" href=\"index.html\">Tutorials</a><ul class=\"current\">\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"01-vector-add.html\">Vector Addition</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"02-fused-softmax.html\">Fused Softmax</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Fused Attention</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n+</ul>\n+</li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Python API</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.html\">triton</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.language.html\">triton.language</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../python-api/triton.testing.html\">triton.testing</a></li>\n+</ul>\n+<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Programming Guide</span></p>\n+<ul>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-1/introduction.html\">Introduction</a></li>\n+<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"../../programming-guide/chapter-2/related-work.html\">Related Work</a></li>\n+</ul>\n+\n+        </div>\n+      </div>\n+    </nav>\n+\n+    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\"><nav class=\"wy-nav-top\" aria-label=\"Mobile navigation menu\" >\n+          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\n+          <a href=\"../../index.html\">Triton</a>\n+      </nav>\n+\n+      <div class=\"wy-nav-content\">\n+        <div class=\"rst-content\">\n+          <div role=\"navigation\" aria-label=\"Page navigation\">\n+  <ul class=\"wy-breadcrumbs\">\n+      <li><a href=\"../../index.html\" class=\"icon icon-home\" aria-label=\"Home\"></a></li>\n+          <li class=\"breadcrumb-item\"><a href=\"index.html\">Tutorials</a></li>\n+      <li class=\"breadcrumb-item active\">Fused Attention</li>\n+      <li class=\"wy-breadcrumbs-aside\">\n+            <a href=\"../../_sources/getting-started/tutorials/06-fused-attention.rst.txt\" rel=\"nofollow\"> View page source</a>\n+      </li>\n+  </ul>\n+  <hr/>\n+</div>\n+          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n+           <div itemprop=\"articleBody\">\n+             \n+  <div class=\"sphx-glr-download-link-note admonition note\">\n+<p class=\"admonition-title\">Note</p>\n+<p><a class=\"reference internal\" href=\"#sphx-glr-download-getting-started-tutorials-06-fused-attention-py\"><span class=\"std std-ref\">Go to the end</span></a>\n+to download the full example code</p>\n+</div>\n+<section class=\"sphx-glr-example-title\" id=\"fused-attention\">\n+<span id=\"sphx-glr-getting-started-tutorials-06-fused-attention-py\"></span><h1>Fused Attention<a class=\"headerlink\" href=\"#fused-attention\" title=\"Permalink to this heading\">\u00b6</a></h1>\n+<p>This is a Triton implementation of the Flash Attention algorithm\n+(see: Dao et al., <a class=\"reference external\" href=\"https://arxiv.org/pdf/2205.14135v2.pdf\">https://arxiv.org/pdf/2205.14135v2.pdf</a>; Rabe and Staats <a class=\"reference external\" href=\"https://arxiv.org/pdf/2112.05682v2.pdf\">https://arxiv.org/pdf/2112.05682v2.pdf</a>)</p>\n+<ul class=\"sphx-glr-horizontal\">\n+<li><img src=\"../../_images/sphx_glr_06-fused-attention_001.png\" srcset=\"../../_images/sphx_glr_06-fused-attention_001.png\" alt=\"06 fused attention\" class = \"sphx-glr-multi-img\"/></li>\n+<li><img src=\"../../_images/sphx_glr_06-fused-attention_002.png\" srcset=\"../../_images/sphx_glr_06-fused-attention_002.png\" alt=\"06 fused attention\" class = \"sphx-glr-multi-img\"/></li>\n+</ul>\n+<div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>[128, 128] 1\n+[128, 128] 1\n+fused-attention-batch4-head48-d64-fwd:\n+    N_CTX     Triton\n+0  1024.0   0.323929\n+1  2048.0   1.086871\n+2  4096.0   3.919503\n+3  8192.0  14.976001\n+[128, 64] 1\n+fused-attention-batch4-head48-d64-bwd:\n+    N_CTX     Triton\n+0  1024.0   1.184640\n+1  2048.0   3.757174\n+2  4096.0  13.251730\n+3  8192.0  49.541634\n+</pre></div>\n+</div>\n+<div class=\"line-block\">\n+<div class=\"line\"><br /></div>\n+</div>\n+<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">pytest</span>\n+<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n+\n+<span class=\"kn\">import</span> <span class=\"nn\">triton</span>\n+<span class=\"kn\">import</span> <span class=\"nn\">triton.language</span> <span class=\"k\">as</span> <span class=\"nn\">tl</span>\n+\n+\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n+<span class=\"k\">def</span> <span class=\"nf\">_fwd_kernel</span><span class=\"p\">(</span>\n+    <span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"n\">sm_scale</span><span class=\"p\">,</span>\n+    <span class=\"n\">L</span><span class=\"p\">,</span> <span class=\"n\">M</span><span class=\"p\">,</span>\n+    <span class=\"n\">Out</span><span class=\"p\">,</span>\n+    <span class=\"n\">stride_qz</span><span class=\"p\">,</span> <span class=\"n\">stride_qh</span><span class=\"p\">,</span> <span class=\"n\">stride_qm</span><span class=\"p\">,</span> <span class=\"n\">stride_qk</span><span class=\"p\">,</span>\n+    <span class=\"n\">stride_kz</span><span class=\"p\">,</span> <span class=\"n\">stride_kh</span><span class=\"p\">,</span> <span class=\"n\">stride_kn</span><span class=\"p\">,</span> <span class=\"n\">stride_kk</span><span class=\"p\">,</span>\n+    <span class=\"n\">stride_vz</span><span class=\"p\">,</span> <span class=\"n\">stride_vh</span><span class=\"p\">,</span> <span class=\"n\">stride_vk</span><span class=\"p\">,</span> <span class=\"n\">stride_vn</span><span class=\"p\">,</span>\n+    <span class=\"n\">stride_oz</span><span class=\"p\">,</span> <span class=\"n\">stride_oh</span><span class=\"p\">,</span> <span class=\"n\">stride_om</span><span class=\"p\">,</span> <span class=\"n\">stride_on</span><span class=\"p\">,</span>\n+    <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span>\n+    <span class=\"n\">BLOCK_M</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+    <span class=\"n\">BLOCK_N</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+<span class=\"p\">):</span>\n+    <span class=\"n\">start_m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"n\">off_hz</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># initialize offsets</span>\n+    <span class=\"n\">offs_m</span> <span class=\"o\">=</span> <span class=\"n\">start_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">)</span>\n+    <span class=\"n\">offs_n</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"p\">)</span>\n+    <span class=\"n\">offs_d</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">)</span>\n+    <span class=\"n\">off_q</span> <span class=\"o\">=</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span> <span class=\"o\">+</span> <span class=\"n\">offs_m</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qm</span> <span class=\"o\">+</span> <span class=\"n\">offs_d</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qk</span>\n+    <span class=\"n\">off_k</span> <span class=\"o\">=</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span> <span class=\"o\">+</span> <span class=\"n\">offs_n</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_kn</span> <span class=\"o\">+</span> <span class=\"n\">offs_d</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_kk</span>\n+    <span class=\"n\">off_v</span> <span class=\"o\">=</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span> <span class=\"o\">+</span> <span class=\"n\">offs_n</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qm</span> <span class=\"o\">+</span> <span class=\"n\">offs_d</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qk</span>\n+    <span class=\"c1\"># Initialize pointers to Q, K, V</span>\n+    <span class=\"n\">q_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">Q</span> <span class=\"o\">+</span> <span class=\"n\">off_q</span>\n+    <span class=\"n\">k_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">K</span> <span class=\"o\">+</span> <span class=\"n\">off_k</span>\n+    <span class=\"n\">v_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">V</span> <span class=\"o\">+</span> <span class=\"n\">off_v</span>\n+    <span class=\"c1\"># initialize pointer to m and l</span>\n+    <span class=\"n\">m_prev</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_M</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s2\">&quot;inf&quot;</span><span class=\"p\">)</span>\n+    <span class=\"n\">l_prev</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_M</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"n\">acc</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># load q: it will stay in SRAM throughout</span>\n+    <span class=\"n\">q</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">q_ptrs</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># loop over k, v and update accumulator</span>\n+    <span class=\"k\">for</span> <span class=\"n\">start_n</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">start_m</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"p\">):</span>\n+        <span class=\"c1\"># -- compute qk ----</span>\n+        <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">k_ptrs</span><span class=\"p\">)</span>\n+        <span class=\"n\">qk</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+        <span class=\"n\">qk</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">)</span>\n+        <span class=\"n\">qk</span> <span class=\"o\">*=</span> <span class=\"n\">sm_scale</span>\n+        <span class=\"n\">qk</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">offs_m</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&gt;=</span> <span class=\"p\">(</span><span class=\"n\">start_n</span> <span class=\"o\">+</span> <span class=\"n\">offs_n</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]),</span> <span class=\"n\">qk</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s2\">&quot;-inf&quot;</span><span class=\"p\">))</span>\n+        <span class=\"c1\"># compute new m</span>\n+        <span class=\"n\">m_curr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">maximum</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">qk</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">m_prev</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># correct old l</span>\n+        <span class=\"n\">l_prev</span> <span class=\"o\">*=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">m_prev</span> <span class=\"o\">-</span> <span class=\"n\">m_curr</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># attention weights</span>\n+        <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">qk</span> <span class=\"o\">-</span> <span class=\"n\">m_curr</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">])</span>\n+        <span class=\"n\">l_curr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">l_prev</span>\n+        <span class=\"c1\"># rescale operands of matmuls</span>\n+        <span class=\"n\">l_rcp</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span> <span class=\"o\">/</span> <span class=\"n\">l_curr</span>\n+        <span class=\"n\">p</span> <span class=\"o\">*=</span> <span class=\"n\">l_rcp</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n+        <span class=\"n\">acc</span> <span class=\"o\">*=</span> <span class=\"p\">(</span><span class=\"n\">l_prev</span> <span class=\"o\">*</span> <span class=\"n\">l_rcp</span><span class=\"p\">)[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n+        <span class=\"c1\"># update acc</span>\n+        <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"o\">.</span><span class=\"n\">element_ty</span><span class=\"p\">)</span>\n+        <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">v_ptrs</span><span class=\"p\">)</span>\n+        <span class=\"n\">acc</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># update m_i and l_i</span>\n+        <span class=\"n\">l_prev</span> <span class=\"o\">=</span> <span class=\"n\">l_curr</span>\n+        <span class=\"n\">m_prev</span> <span class=\"o\">=</span> <span class=\"n\">m_curr</span>\n+        <span class=\"c1\"># update pointers</span>\n+        <span class=\"n\">k_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_N</span> <span class=\"o\">*</span> <span class=\"n\">stride_kn</span>\n+        <span class=\"n\">v_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_N</span> <span class=\"o\">*</span> <span class=\"n\">stride_vk</span>\n+    <span class=\"c1\"># rematerialize offsets to save registers</span>\n+    <span class=\"n\">start_m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"n\">offs_m</span> <span class=\"o\">=</span> <span class=\"n\">start_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># write back l and m</span>\n+    <span class=\"n\">l_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">L</span> <span class=\"o\">+</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">N_CTX</span> <span class=\"o\">+</span> <span class=\"n\">offs_m</span>\n+    <span class=\"n\">m_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">M</span> <span class=\"o\">+</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">N_CTX</span> <span class=\"o\">+</span> <span class=\"n\">offs_m</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">l_ptrs</span><span class=\"p\">,</span> <span class=\"n\">l_prev</span><span class=\"p\">)</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">m_ptrs</span><span class=\"p\">,</span> <span class=\"n\">m_prev</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># initialize pointers to output</span>\n+    <span class=\"n\">offs_n</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">)</span>\n+    <span class=\"n\">off_o</span> <span class=\"o\">=</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">stride_oh</span> <span class=\"o\">+</span> <span class=\"n\">offs_m</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_om</span> <span class=\"o\">+</span> <span class=\"n\">offs_n</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_on</span>\n+    <span class=\"n\">out_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">Out</span> <span class=\"o\">+</span> <span class=\"n\">off_o</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">out_ptrs</span><span class=\"p\">,</span> <span class=\"n\">acc</span><span class=\"p\">)</span>\n+\n+\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n+<span class=\"k\">def</span> <span class=\"nf\">_bwd_preprocess</span><span class=\"p\">(</span>\n+    <span class=\"n\">Out</span><span class=\"p\">,</span> <span class=\"n\">DO</span><span class=\"p\">,</span> <span class=\"n\">L</span><span class=\"p\">,</span>\n+    <span class=\"n\">NewDO</span><span class=\"p\">,</span> <span class=\"n\">Delta</span><span class=\"p\">,</span>\n+    <span class=\"n\">BLOCK_M</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+<span class=\"p\">):</span>\n+    <span class=\"n\">off_m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">)</span>\n+    <span class=\"n\">off_n</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># load</span>\n+    <span class=\"n\">o</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">Out</span> <span class=\"o\">+</span> <span class=\"n\">off_m</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">D_HEAD</span> <span class=\"o\">+</span> <span class=\"n\">off_n</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:])</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"n\">do</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">DO</span> <span class=\"o\">+</span> <span class=\"n\">off_m</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">D_HEAD</span> <span class=\"o\">+</span> <span class=\"n\">off_n</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:])</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"n\">denom</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">L</span> <span class=\"o\">+</span> <span class=\"n\">off_m</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># compute</span>\n+    <span class=\"n\">do</span> <span class=\"o\">=</span> <span class=\"n\">do</span> <span class=\"o\">/</span> <span class=\"n\">denom</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n+    <span class=\"n\">delta</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">o</span> <span class=\"o\">*</span> <span class=\"n\">do</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># write-back</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">NewDO</span> <span class=\"o\">+</span> <span class=\"n\">off_m</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">D_HEAD</span> <span class=\"o\">+</span> <span class=\"n\">off_n</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:],</span> <span class=\"n\">do</span><span class=\"p\">)</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">Delta</span> <span class=\"o\">+</span> <span class=\"n\">off_m</span><span class=\"p\">,</span> <span class=\"n\">delta</span><span class=\"p\">)</span>\n+\n+\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n+<span class=\"k\">def</span> <span class=\"nf\">_bwd_kernel</span><span class=\"p\">(</span>\n+    <span class=\"n\">Q</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">V</span><span class=\"p\">,</span> <span class=\"n\">sm_scale</span><span class=\"p\">,</span> <span class=\"n\">Out</span><span class=\"p\">,</span> <span class=\"n\">DO</span><span class=\"p\">,</span>\n+    <span class=\"n\">DQ</span><span class=\"p\">,</span> <span class=\"n\">DK</span><span class=\"p\">,</span> <span class=\"n\">DV</span><span class=\"p\">,</span>\n+    <span class=\"n\">L</span><span class=\"p\">,</span> <span class=\"n\">M</span><span class=\"p\">,</span>\n+    <span class=\"n\">D</span><span class=\"p\">,</span>\n+    <span class=\"n\">stride_qz</span><span class=\"p\">,</span> <span class=\"n\">stride_qh</span><span class=\"p\">,</span> <span class=\"n\">stride_qm</span><span class=\"p\">,</span> <span class=\"n\">stride_qk</span><span class=\"p\">,</span>\n+    <span class=\"n\">stride_kz</span><span class=\"p\">,</span> <span class=\"n\">stride_kh</span><span class=\"p\">,</span> <span class=\"n\">stride_kn</span><span class=\"p\">,</span> <span class=\"n\">stride_kk</span><span class=\"p\">,</span>\n+    <span class=\"n\">stride_vz</span><span class=\"p\">,</span> <span class=\"n\">stride_vh</span><span class=\"p\">,</span> <span class=\"n\">stride_vk</span><span class=\"p\">,</span> <span class=\"n\">stride_vn</span><span class=\"p\">,</span>\n+    <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span>\n+    <span class=\"n\">num_block</span><span class=\"p\">,</span>\n+    <span class=\"n\">BLOCK_M</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+    <span class=\"n\">BLOCK_N</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+<span class=\"p\">):</span>\n+    <span class=\"n\">off_hz</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"n\">off_z</span> <span class=\"o\">=</span> <span class=\"n\">off_hz</span> <span class=\"o\">//</span> <span class=\"n\">H</span>\n+    <span class=\"n\">off_h</span> <span class=\"o\">=</span> <span class=\"n\">off_hz</span> <span class=\"o\">%</span> <span class=\"n\">H</span>\n+    <span class=\"c1\"># offset pointers for batch/head</span>\n+    <span class=\"n\">Q</span> <span class=\"o\">+=</span> <span class=\"n\">off_z</span> <span class=\"o\">*</span> <span class=\"n\">stride_qz</span> <span class=\"o\">+</span> <span class=\"n\">off_h</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span>\n+    <span class=\"n\">K</span> <span class=\"o\">+=</span> <span class=\"n\">off_z</span> <span class=\"o\">*</span> <span class=\"n\">stride_qz</span> <span class=\"o\">+</span> <span class=\"n\">off_h</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span>\n+    <span class=\"n\">V</span> <span class=\"o\">+=</span> <span class=\"n\">off_z</span> <span class=\"o\">*</span> <span class=\"n\">stride_qz</span> <span class=\"o\">+</span> <span class=\"n\">off_h</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span>\n+    <span class=\"n\">DO</span> <span class=\"o\">+=</span> <span class=\"n\">off_z</span> <span class=\"o\">*</span> <span class=\"n\">stride_qz</span> <span class=\"o\">+</span> <span class=\"n\">off_h</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span>\n+    <span class=\"n\">DQ</span> <span class=\"o\">+=</span> <span class=\"n\">off_z</span> <span class=\"o\">*</span> <span class=\"n\">stride_qz</span> <span class=\"o\">+</span> <span class=\"n\">off_h</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span>\n+    <span class=\"n\">DK</span> <span class=\"o\">+=</span> <span class=\"n\">off_z</span> <span class=\"o\">*</span> <span class=\"n\">stride_qz</span> <span class=\"o\">+</span> <span class=\"n\">off_h</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span>\n+    <span class=\"n\">DV</span> <span class=\"o\">+=</span> <span class=\"n\">off_z</span> <span class=\"o\">*</span> <span class=\"n\">stride_qz</span> <span class=\"o\">+</span> <span class=\"n\">off_h</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span>\n+    <span class=\"k\">for</span> <span class=\"n\">start_n</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">num_block</span><span class=\"p\">):</span>\n+        <span class=\"n\">lo</span> <span class=\"o\">=</span> <span class=\"n\">start_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span>\n+        <span class=\"c1\"># initialize row/col offsets</span>\n+        <span class=\"n\">offs_qm</span> <span class=\"o\">=</span> <span class=\"n\">lo</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">)</span>\n+        <span class=\"n\">offs_n</span> <span class=\"o\">=</span> <span class=\"n\">start_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">)</span>\n+        <span class=\"n\">offs_m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"p\">)</span>\n+        <span class=\"n\">offs_k</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># initialize pointers to value-like data</span>\n+        <span class=\"n\">q_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">Q</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_qm</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qm</span> <span class=\"o\">+</span> <span class=\"n\">offs_k</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qk</span><span class=\"p\">)</span>\n+        <span class=\"n\">k_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">K</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_n</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_kn</span> <span class=\"o\">+</span> <span class=\"n\">offs_k</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_kk</span><span class=\"p\">)</span>\n+        <span class=\"n\">v_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">V</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_n</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qm</span> <span class=\"o\">+</span> <span class=\"n\">offs_k</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qk</span><span class=\"p\">)</span>\n+        <span class=\"n\">do_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">DO</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_qm</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qm</span> <span class=\"o\">+</span> <span class=\"n\">offs_k</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qk</span><span class=\"p\">)</span>\n+        <span class=\"n\">dq_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">DQ</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_qm</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qm</span> <span class=\"o\">+</span> <span class=\"n\">offs_k</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qk</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># pointer to row-wise quantities in value-like data</span>\n+        <span class=\"n\">D_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">D</span> <span class=\"o\">+</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">N_CTX</span>\n+        <span class=\"n\">m_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">M</span> <span class=\"o\">+</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">N_CTX</span>\n+        <span class=\"c1\"># initialize dv amd dk</span>\n+        <span class=\"n\">dv</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+        <span class=\"n\">dk</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># k and v stay in SRAM throughout</span>\n+        <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">k_ptrs</span><span class=\"p\">)</span>\n+        <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">v_ptrs</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># loop over rows</span>\n+        <span class=\"k\">for</span> <span class=\"n\">start_m</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">lo</span><span class=\"p\">,</span> <span class=\"n\">num_block</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">):</span>\n+            <span class=\"n\">offs_m_curr</span> <span class=\"o\">=</span> <span class=\"n\">start_m</span> <span class=\"o\">+</span> <span class=\"n\">offs_m</span>\n+            <span class=\"c1\"># load q, k, v, do on-chip</span>\n+            <span class=\"n\">q</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">q_ptrs</span><span class=\"p\">)</span>\n+            <span class=\"c1\"># recompute p = softmax(qk, dim=-1).T</span>\n+            <span class=\"c1\"># NOTE: `do` is pre-divided by `l`; no normalization here</span>\n+            <span class=\"n\">qk</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">trans</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">))</span>\n+            <span class=\"n\">qk</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">offs_m_curr</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&gt;=</span> <span class=\"p\">(</span><span class=\"n\">offs_n</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]),</span> <span class=\"n\">qk</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s2\">&quot;-inf&quot;</span><span class=\"p\">))</span>\n+            <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">m_ptrs</span> <span class=\"o\">+</span> <span class=\"n\">offs_m_curr</span><span class=\"p\">)</span>\n+            <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">qk</span> <span class=\"o\">*</span> <span class=\"n\">sm_scale</span> <span class=\"o\">-</span> <span class=\"n\">m</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">])</span>\n+            <span class=\"c1\"># compute dv</span>\n+            <span class=\"n\">do</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">do_ptrs</span><span class=\"p\">)</span>\n+            <span class=\"n\">dv</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">trans</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"o\">.</span><span class=\"n\">element_ty</span><span class=\"p\">)),</span> <span class=\"n\">do</span><span class=\"p\">)</span>\n+            <span class=\"c1\"># compute dp = dot(v, do)</span>\n+            <span class=\"n\">Di</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">D_ptrs</span> <span class=\"o\">+</span> <span class=\"n\">offs_m_curr</span><span class=\"p\">)</span>\n+            <span class=\"n\">dp</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">Di</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n+            <span class=\"n\">dp</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">do</span><span class=\"p\">,</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">trans</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">))</span>\n+            <span class=\"c1\"># compute ds = p * (dp - delta[:, None])</span>\n+            <span class=\"n\">ds</span> <span class=\"o\">=</span> <span class=\"n\">p</span> <span class=\"o\">*</span> <span class=\"n\">dp</span> <span class=\"o\">*</span> <span class=\"n\">sm_scale</span>\n+            <span class=\"c1\"># compute dk = dot(ds.T, q)</span>\n+            <span class=\"n\">dk</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">trans</span><span class=\"p\">(</span><span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"o\">.</span><span class=\"n\">element_ty</span><span class=\"p\">)),</span> <span class=\"n\">q</span><span class=\"p\">)</span>\n+            <span class=\"c1\"># compute dq</span>\n+            <span class=\"n\">dq</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">dq_ptrs</span><span class=\"p\">)</span>\n+            <span class=\"n\">dq</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">ds</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"o\">.</span><span class=\"n\">element_ty</span><span class=\"p\">),</span> <span class=\"n\">k</span><span class=\"p\">)</span>\n+            <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">dq_ptrs</span><span class=\"p\">,</span> <span class=\"n\">dq</span><span class=\"p\">)</span>\n+            <span class=\"c1\"># increment pointers</span>\n+            <span class=\"n\">dq_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_M</span> <span class=\"o\">*</span> <span class=\"n\">stride_qm</span>\n+            <span class=\"n\">q_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_M</span> <span class=\"o\">*</span> <span class=\"n\">stride_qm</span>\n+            <span class=\"n\">do_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_M</span> <span class=\"o\">*</span> <span class=\"n\">stride_qm</span>\n+        <span class=\"c1\"># write-back</span>\n+        <span class=\"n\">dv_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">DV</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_n</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qm</span> <span class=\"o\">+</span> <span class=\"n\">offs_k</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qk</span><span class=\"p\">)</span>\n+        <span class=\"n\">dk_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">DK</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">offs_n</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_kn</span> <span class=\"o\">+</span> <span class=\"n\">offs_k</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_kk</span><span class=\"p\">)</span>\n+        <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">dv_ptrs</span><span class=\"p\">,</span> <span class=\"n\">dv</span><span class=\"p\">)</span>\n+        <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">dk_ptrs</span><span class=\"p\">,</span> <span class=\"n\">dk</span><span class=\"p\">)</span>\n+\n+\n+<span class=\"n\">empty</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">)</span>\n+\n+\n+<span class=\"k\">class</span> <span class=\"nc\">_attention</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">autograd</span><span class=\"o\">.</span><span class=\"n\">Function</span><span class=\"p\">):</span>\n+\n+    <span class=\"nd\">@staticmethod</span>\n+    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">,</span> <span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">sm_scale</span><span class=\"p\">):</span>\n+        <span class=\"n\">BLOCK</span> <span class=\"o\">=</span> <span class=\"mi\">128</span>\n+        <span class=\"c1\"># shape constraints</span>\n+        <span class=\"n\">Lq</span><span class=\"p\">,</span> <span class=\"n\">Lk</span><span class=\"p\">,</span> <span class=\"n\">Lv</span> <span class=\"o\">=</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n+        <span class=\"k\">assert</span> <span class=\"n\">Lq</span> <span class=\"o\">==</span> <span class=\"n\">Lk</span> <span class=\"ow\">and</span> <span class=\"n\">Lk</span> <span class=\"o\">==</span> <span class=\"n\">Lv</span>\n+        <span class=\"k\">assert</span> <span class=\"n\">Lk</span> <span class=\"ow\">in</span> <span class=\"p\">{</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">}</span>\n+        <span class=\"n\">o</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">)</span>\n+        <span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"n\">BLOCK</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n+        <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+        <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+        <span class=\"n\">num_warps</span> <span class=\"o\">=</span> <span class=\"mi\">4</span> <span class=\"k\">if</span> <span class=\"n\">Lk</span> <span class=\"o\">&lt;=</span> <span class=\"mi\">64</span> <span class=\"k\">else</span> <span class=\"mi\">8</span>\n+\n+        <span class=\"n\">_fwd_kernel</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span>\n+            <span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">sm_scale</span><span class=\"p\">,</span>\n+            <span class=\"n\">L</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">,</span>\n+            <span class=\"n\">o</span><span class=\"p\">,</span>\n+            <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n+            <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n+            <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n+            <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n+            <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">],</span>\n+            <span class=\"n\">BLOCK_M</span><span class=\"o\">=</span><span class=\"n\">BLOCK</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"o\">=</span><span class=\"n\">BLOCK</span><span class=\"p\">,</span>\n+            <span class=\"n\">BLOCK_DMODEL</span><span class=\"o\">=</span><span class=\"n\">Lk</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"n\">num_warps</span><span class=\"p\">,</span>\n+            <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n+        <span class=\"p\">)</span>\n+        <span class=\"c1\"># print(h.asm[&quot;ttgir&quot;])</span>\n+\n+        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">save_for_backward</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">o</span><span class=\"p\">,</span> <span class=\"n\">L</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">)</span>\n+        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"n\">grid</span>\n+        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">sm_scale</span> <span class=\"o\">=</span> <span class=\"n\">sm_scale</span>\n+        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">BLOCK_DMODEL</span> <span class=\"o\">=</span> <span class=\"n\">Lk</span>\n+        <span class=\"k\">return</span> <span class=\"n\">o</span>\n+\n+    <span class=\"nd\">@staticmethod</span>\n+    <span class=\"k\">def</span> <span class=\"nf\">backward</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">,</span> <span class=\"n\">do</span><span class=\"p\">):</span>\n+        <span class=\"n\">BLOCK</span> <span class=\"o\">=</span> <span class=\"mi\">128</span>\n+        <span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">o</span><span class=\"p\">,</span> <span class=\"n\">l</span><span class=\"p\">,</span> <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">saved_tensors</span>\n+        <span class=\"n\">do</span> <span class=\"o\">=</span> <span class=\"n\">do</span><span class=\"o\">.</span><span class=\"n\">contiguous</span><span class=\"p\">()</span>\n+        <span class=\"n\">dq</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros_like</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+        <span class=\"n\">dk</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">)</span>\n+        <span class=\"n\">dv</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">)</span>\n+        <span class=\"n\">do_scaled</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">do</span><span class=\"p\">)</span>\n+        <span class=\"n\">delta</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)</span>\n+        <span class=\"n\">_bwd_preprocess</span><span class=\"p\">[(</span><span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">)](</span>\n+            <span class=\"n\">o</span><span class=\"p\">,</span> <span class=\"n\">do</span><span class=\"p\">,</span> <span class=\"n\">l</span><span class=\"p\">,</span>\n+            <span class=\"n\">do_scaled</span><span class=\"p\">,</span> <span class=\"n\">delta</span><span class=\"p\">,</span>\n+            <span class=\"n\">BLOCK_M</span><span class=\"o\">=</span><span class=\"n\">BLOCK</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"o\">=</span><span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">,</span>\n+        <span class=\"p\">)</span>\n+        <span class=\"n\">_bwd_kernel</span><span class=\"p\">[(</span><span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],)](</span>\n+            <span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">sm_scale</span><span class=\"p\">,</span>\n+            <span class=\"n\">o</span><span class=\"p\">,</span> <span class=\"n\">do_scaled</span><span class=\"p\">,</span>\n+            <span class=\"n\">dq</span><span class=\"p\">,</span> <span class=\"n\">dk</span><span class=\"p\">,</span> <span class=\"n\">dv</span><span class=\"p\">,</span>\n+            <span class=\"n\">l</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">,</span>\n+            <span class=\"n\">delta</span><span class=\"p\">,</span>\n+            <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n+            <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n+            <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n+            <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">],</span>\n+            <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span>\n+            <span class=\"n\">BLOCK_M</span><span class=\"o\">=</span><span class=\"n\">BLOCK</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"o\">=</span><span class=\"n\">BLOCK</span><span class=\"p\">,</span>\n+            <span class=\"n\">BLOCK_DMODEL</span><span class=\"o\">=</span><span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">,</span>\n+            <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n+        <span class=\"p\">)</span>\n+        <span class=\"c1\"># print(h.asm[&quot;ttgir&quot;])</span>\n+        <span class=\"k\">return</span> <span class=\"n\">dq</span><span class=\"p\">,</span> <span class=\"n\">dk</span><span class=\"p\">,</span> <span class=\"n\">dv</span><span class=\"p\">,</span> <span class=\"kc\">None</span>\n+\n+\n+<span class=\"n\">attention</span> <span class=\"o\">=</span> <span class=\"n\">_attention</span><span class=\"o\">.</span><span class=\"n\">apply</span>\n+\n+\n+<span class=\"nd\">@pytest</span><span class=\"o\">.</span><span class=\"n\">mark</span><span class=\"o\">.</span><span class=\"n\">parametrize</span><span class=\"p\">(</span><span class=\"s1\">&#39;Z, H, N_CTX, D_HEAD&#39;</span><span class=\"p\">,</span> <span class=\"p\">[(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">48</span><span class=\"p\">,</span> <span class=\"mi\">1024</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">)])</span>\n+<span class=\"k\">def</span> <span class=\"nf\">test_op</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">):</span>\n+    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">)</span>\n+    <span class=\"n\">q</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">normal_</span><span class=\"p\">(</span><span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">std</span><span class=\"o\">=</span><span class=\"mf\">0.2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">requires_grad_</span><span class=\"p\">()</span>\n+    <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">normal_</span><span class=\"p\">(</span><span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"mf\">0.4</span><span class=\"p\">,</span> <span class=\"n\">std</span><span class=\"o\">=</span><span class=\"mf\">0.2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">requires_grad_</span><span class=\"p\">()</span>\n+    <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">normal_</span><span class=\"p\">(</span><span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"mf\">0.3</span><span class=\"p\">,</span> <span class=\"n\">std</span><span class=\"o\">=</span><span class=\"mf\">0.2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">requires_grad_</span><span class=\"p\">()</span>\n+    <span class=\"n\">sm_scale</span> <span class=\"o\">=</span> <span class=\"mf\">0.2</span>\n+    <span class=\"n\">dout</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn_like</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># reference implementation</span>\n+    <span class=\"n\">M</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tril</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">))</span>\n+    <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">))</span> <span class=\"o\">*</span> <span class=\"n\">sm_scale</span>\n+    <span class=\"k\">for</span> <span class=\"n\">z</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"p\">):</span>\n+        <span class=\"k\">for</span> <span class=\"n\">h</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">H</span><span class=\"p\">):</span>\n+            <span class=\"n\">p</span><span class=\"p\">[:,</span> <span class=\"p\">:,</span> <span class=\"n\">M</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s2\">&quot;-inf&quot;</span><span class=\"p\">)</span>\n+    <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">float</span><span class=\"p\">(),</span> <span class=\"n\">dim</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">half</span><span class=\"p\">()</span>\n+    <span class=\"c1\"># p = torch.exp(p)</span>\n+    <span class=\"n\">ref_out</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">)</span>\n+    <span class=\"n\">ref_out</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">(</span><span class=\"n\">dout</span><span class=\"p\">)</span>\n+    <span class=\"n\">ref_dv</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">clone</span><span class=\"p\">(),</span> <span class=\"kc\">None</span>\n+    <span class=\"n\">ref_dk</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">clone</span><span class=\"p\">(),</span> <span class=\"kc\">None</span>\n+    <span class=\"n\">ref_dq</span><span class=\"p\">,</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">clone</span><span class=\"p\">(),</span> <span class=\"kc\">None</span>\n+    <span class=\"c1\"># # triton implementation</span>\n+    <span class=\"n\">tri_out</span> <span class=\"o\">=</span> <span class=\"n\">attention</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">sm_scale</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># print(ref_out)</span>\n+    <span class=\"c1\"># print(tri_out)</span>\n+    <span class=\"n\">tri_out</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">(</span><span class=\"n\">dout</span><span class=\"p\">)</span>\n+    <span class=\"n\">tri_dv</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">clone</span><span class=\"p\">(),</span> <span class=\"kc\">None</span>\n+    <span class=\"n\">tri_dk</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">clone</span><span class=\"p\">(),</span> <span class=\"kc\">None</span>\n+    <span class=\"n\">tri_dq</span><span class=\"p\">,</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">clone</span><span class=\"p\">(),</span> <span class=\"kc\">None</span>\n+    <span class=\"c1\"># compare</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">ref_out</span><span class=\"p\">,</span> <span class=\"n\">tri_out</span><span class=\"p\">,</span> <span class=\"n\">atol</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">,</span> <span class=\"n\">rtol</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">ref_dv</span><span class=\"p\">,</span> <span class=\"n\">tri_dv</span><span class=\"p\">,</span> <span class=\"n\">atol</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">,</span> <span class=\"n\">rtol</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">ref_dk</span><span class=\"p\">,</span> <span class=\"n\">tri_dk</span><span class=\"p\">,</span> <span class=\"n\">atol</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">,</span> <span class=\"n\">rtol</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">ref_dq</span><span class=\"p\">,</span> <span class=\"n\">tri_dq</span><span class=\"p\">,</span> <span class=\"n\">atol</span><span class=\"o\">=</span><span class=\"mf\">1e-2</span><span class=\"p\">,</span> <span class=\"n\">rtol</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+\n+\n+<span class=\"k\">try</span><span class=\"p\">:</span>\n+    <span class=\"kn\">from</span> <span class=\"nn\">flash_attn.flash_attn_interface</span> <span class=\"kn\">import</span> <span class=\"n\">flash_attn_func</span>\n+    <span class=\"n\">HAS_FLASH</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>\n+<span class=\"k\">except</span> <span class=\"ne\">BaseException</span><span class=\"p\">:</span>\n+    <span class=\"n\">HAS_FLASH</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n+\n+<span class=\"n\">BATCH</span><span class=\"p\">,</span> <span class=\"n\">N_HEADS</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span> <span class=\"o\">=</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">48</span><span class=\"p\">,</span> <span class=\"mi\">4096</span><span class=\"p\">,</span> <span class=\"mi\">64</span>\n+<span class=\"c1\"># vary seq length for fixed head and batch=4</span>\n+<span class=\"n\">configs</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">Benchmark</span><span class=\"p\">(</span>\n+    <span class=\"n\">x_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;N_CTX&#39;</span><span class=\"p\">],</span>\n+    <span class=\"n\">x_vals</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"o\">**</span><span class=\"n\">i</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">14</span><span class=\"p\">)],</span>\n+    <span class=\"n\">line_arg</span><span class=\"o\">=</span><span class=\"s1\">&#39;provider&#39;</span><span class=\"p\">,</span>\n+    <span class=\"n\">line_vals</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"p\">([</span><span class=\"s1\">&#39;flash&#39;</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">HAS_FLASH</span> <span class=\"k\">else</span> <span class=\"p\">[]),</span>\n+    <span class=\"n\">line_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;Triton&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"p\">([</span><span class=\"s1\">&#39;Flash&#39;</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">HAS_FLASH</span> <span class=\"k\">else</span> <span class=\"p\">[]),</span>\n+    <span class=\"n\">styles</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s1\">&#39;red&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;blue&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">)],</span>\n+    <span class=\"n\">ylabel</span><span class=\"o\">=</span><span class=\"s1\">&#39;ms&#39;</span><span class=\"p\">,</span>\n+    <span class=\"n\">plot_name</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"s1\">&#39;fused-attention-batch</span><span class=\"si\">{</span><span class=\"n\">BATCH</span><span class=\"si\">}</span><span class=\"s1\">-head</span><span class=\"si\">{</span><span class=\"n\">N_HEADS</span><span class=\"si\">}</span><span class=\"s1\">-d</span><span class=\"si\">{</span><span class=\"n\">D_HEAD</span><span class=\"si\">}</span><span class=\"s1\">-</span><span class=\"si\">{</span><span class=\"n\">mode</span><span class=\"si\">}</span><span class=\"s1\">&#39;</span><span class=\"p\">,</span>\n+    <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;H&#39;</span><span class=\"p\">:</span> <span class=\"n\">N_HEADS</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BATCH&#39;</span><span class=\"p\">:</span> <span class=\"n\">BATCH</span><span class=\"p\">,</span> <span class=\"s1\">&#39;D_HEAD&#39;</span><span class=\"p\">:</span> <span class=\"n\">D_HEAD</span><span class=\"p\">,</span> <span class=\"s1\">&#39;dtype&#39;</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">,</span> <span class=\"s1\">&#39;mode&#39;</span><span class=\"p\">:</span> <span class=\"n\">mode</span><span class=\"p\">}</span>\n+<span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">mode</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"s1\">&#39;fwd&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;bwd&#39;</span><span class=\"p\">]]</span>\n+\n+\n+<span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">perf_report</span><span class=\"p\">(</span><span class=\"n\">configs</span><span class=\"p\">)</span>\n+<span class=\"k\">def</span> <span class=\"nf\">bench_flash_attention</span><span class=\"p\">(</span><span class=\"n\">BATCH</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">,</span> <span class=\"n\">mode</span><span class=\"p\">,</span> <span class=\"n\">provider</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">):</span>\n+    <span class=\"k\">assert</span> <span class=\"n\">mode</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"s1\">&#39;fwd&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;bwd&#39;</span><span class=\"p\">]</span>\n+    <span class=\"n\">warmup</span> <span class=\"o\">=</span> <span class=\"mi\">25</span>\n+    <span class=\"n\">rep</span> <span class=\"o\">=</span> <span class=\"mi\">100</span>\n+    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;triton&quot;</span><span class=\"p\">:</span>\n+        <span class=\"n\">q</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"n\">BATCH</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+        <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"n\">BATCH</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+        <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"n\">BATCH</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+        <span class=\"n\">sm_scale</span> <span class=\"o\">=</span> <span class=\"mf\">1.3</span>\n+        <span class=\"n\">fn</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">attention</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">sm_scale</span><span class=\"p\">)</span>\n+        <span class=\"k\">if</span> <span class=\"n\">mode</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;bwd&#39;</span><span class=\"p\">:</span>\n+            <span class=\"n\">o</span> <span class=\"o\">=</span> <span class=\"n\">fn</span><span class=\"p\">()</span>\n+            <span class=\"n\">do</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn_like</span><span class=\"p\">(</span><span class=\"n\">o</span><span class=\"p\">)</span>\n+            <span class=\"n\">fn</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">(</span><span class=\"n\">do</span><span class=\"p\">,</span> <span class=\"n\">retain_graph</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+        <span class=\"n\">ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"n\">fn</span><span class=\"p\">,</span> <span class=\"n\">warmup</span><span class=\"o\">=</span><span class=\"n\">warmup</span><span class=\"p\">,</span> <span class=\"n\">rep</span><span class=\"o\">=</span><span class=\"n\">rep</span><span class=\"p\">)</span>\n+        <span class=\"k\">return</span> <span class=\"n\">ms</span>\n+    <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;flash&quot;</span><span class=\"p\">:</span>\n+        <span class=\"n\">lengths</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"n\">BATCH</span><span class=\"p\">,),</span> <span class=\"n\">fill_value</span><span class=\"o\">=</span><span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">)</span>\n+        <span class=\"n\">cu_seqlens</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">BATCH</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">,),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">)</span>\n+        <span class=\"n\">cu_seqlens</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:]</span> <span class=\"o\">=</span> <span class=\"n\">lengths</span><span class=\"o\">.</span><span class=\"n\">cumsum</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n+        <span class=\"n\">qkv</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"n\">BATCH</span> <span class=\"o\">*</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+        <span class=\"n\">fn</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">flash_attn_func</span><span class=\"p\">(</span><span class=\"n\">qkv</span><span class=\"p\">,</span> <span class=\"n\">cu_seqlens</span><span class=\"p\">,</span> <span class=\"mf\">0.</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">causal</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+        <span class=\"k\">if</span> <span class=\"n\">mode</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;bwd&#39;</span><span class=\"p\">:</span>\n+            <span class=\"n\">o</span> <span class=\"o\">=</span> <span class=\"n\">fn</span><span class=\"p\">()</span>\n+            <span class=\"n\">do</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn_like</span><span class=\"p\">(</span><span class=\"n\">o</span><span class=\"p\">)</span>\n+            <span class=\"n\">fn</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">(</span><span class=\"n\">do</span><span class=\"p\">,</span> <span class=\"n\">retain_graph</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+        <span class=\"n\">ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"n\">fn</span><span class=\"p\">,</span> <span class=\"n\">warmup</span><span class=\"o\">=</span><span class=\"n\">warmup</span><span class=\"p\">,</span> <span class=\"n\">rep</span><span class=\"o\">=</span><span class=\"n\">rep</span><span class=\"p\">)</span>\n+        <span class=\"k\">return</span> <span class=\"n\">ms</span>\n+\n+\n+<span class=\"c1\"># only works on post-Ampere GPUs right now</span>\n+<span class=\"n\">bench_flash_attention</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;.&#39;</span><span class=\"p\">,</span> <span class=\"n\">print_data</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n+</pre></div>\n+</div>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  5.605 seconds)</p>\n+<div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-06-fused-attention-py\">\n+<div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/54a35f6ec55f9746935b9566fb6bb1df/06-fused-attention.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">06-fused-attention.py</span></code></a></p>\n+</div>\n+<div class=\"sphx-glr-download sphx-glr-download-jupyter docutils container\">\n+<p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/3176accb6c7288b0e45f41d94eebacb9/06-fused-attention.ipynb\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Jupyter</span> <span class=\"pre\">notebook:</span> <span class=\"pre\">06-fused-attention.ipynb</span></code></a></p>\n+</div>\n+</div>\n+<p class=\"sphx-glr-signature\"><a class=\"reference external\" href=\"https://sphinx-gallery.github.io\">Gallery generated by Sphinx-Gallery</a></p>\n+</section>\n+\n+\n+           </div>\n+          </div>\n+          <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n+        <a href=\"05-layer-norm.html\" class=\"btn btn-neutral float-left\" title=\"Layer Normalization\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"07-math-functions.html\" class=\"btn btn-neutral float-right\" title=\"Libdevice (tl.math) function\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n+    </div>\n+\n+  <hr/>\n+\n+  <div role=\"contentinfo\">\n+    <p>&#169; Copyright 2020, Philippe Tillet.</p>\n+  </div>\n+\n+  Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\n+    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\n+    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>.\n+   \n+\n+</footer>\n+        </div>\n+      </div>\n+    </section>\n+  </div>\n+  \n+<div class=\"rst-versions\" data-toggle=\"rst-versions\" role=\"note\" aria-label=\"versions\">\n+    <span class=\"rst-current-version\" data-toggle=\"rst-current-version\">\n+        <span class=\"fa fa-book\"> Other Versions</span>\n+        v: main\n+        <span class=\"fa fa-caret-down\"></span>\n+    </span>\n+    <div class=\"rst-other-versions\">\n+        <dl>\n+            <dt>Branches</dt>\n+            <dd><a href=\"06-fused-attention.html\">main</a></dd>\n+        </dl>\n+    </div>\n+</div><script>\n+      jQuery(function () {\n+          SphinxRtdTheme.Navigation.enable(true);\n+      });\n+  </script> \n+\n+</body>\n+</html>\n\\ No newline at end of file"}, {"filename": "main/getting-started/tutorials/07-math-functions.html", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -23,7 +23,7 @@\n     <link rel=\"index\" title=\"Index\" href=\"../../genindex.html\" />\n     <link rel=\"search\" title=\"Search\" href=\"../../search.html\" />\n     <link rel=\"next\" title=\"triton\" href=\"../../python-api/triton.html\" />\n-    <link rel=\"prev\" title=\"Layer Normalization\" href=\"05-layer-norm.html\" /> \n+    <link rel=\"prev\" title=\"Fused Attention\" href=\"06-fused-attention.html\" /> \n </head>\n \n <body class=\"wy-body-for-nav\"> \n@@ -54,6 +54,7 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n <li class=\"toctree-l2 current\"><a class=\"current reference internal\" href=\"#\">Libdevice (<cite>tl.math</cite>) function</a><ul>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#asin-kernel\">asin Kernel</a></li>\n <li class=\"toctree-l3\"><a class=\"reference internal\" href=\"#using-the-default-libdevice-library-path\">Using the default libdevice library path</a></li>\n@@ -184,7 +185,7 @@ <h2>Customize the libdevice library path<a class=\"headerlink\" href=\"#customize-t\n The maximum difference between torch and triton is 2.384185791015625e-07\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.373 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.390 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-07-math-functions-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/77571465f7f4bd281d3a847dc2633146/07-math-functions.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">07-math-functions.py</span></code></a></p>\n@@ -201,7 +202,7 @@ <h2>Customize the libdevice library path<a class=\"headerlink\" href=\"#customize-t\n            </div>\n           </div>\n           <footer><div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"Footer\">\n-        <a href=\"05-layer-norm.html\" class=\"btn btn-neutral float-left\" title=\"Layer Normalization\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n+        <a href=\"06-fused-attention.html\" class=\"btn btn-neutral float-left\" title=\"Fused Attention\" accesskey=\"p\" rel=\"prev\"><span class=\"fa fa-arrow-circle-left\" aria-hidden=\"true\"></span> Previous</a>\n         <a href=\"../../python-api/triton.html\" class=\"btn btn-neutral float-right\" title=\"triton\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></span></a>\n     </div>\n "}, {"filename": "main/getting-started/tutorials/index.html", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -54,6 +54,7 @@\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"03-matrix-multiplication.html\">Matrix Multiplication</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"04-low-memory-dropout.html\">Low-Memory Dropout</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"05-layer-norm.html\">Layer Normalization</a></li>\n+<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"06-fused-attention.html\">Fused Attention</a></li>\n <li class=\"toctree-l2\"><a class=\"reference internal\" href=\"07-math-functions.html\">Libdevice (<cite>tl.math</cite>) function</a></li>\n </ul>\n </li>\n@@ -117,6 +118,9 @@ <h1>Tutorials<a class=\"headerlink\" href=\"#tutorials\" title=\"Permalink to this he\n </div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"In doing so, you will learn about: - Implementing backward pass in Triton - Implementing parall...\"><img alt=\"\" src=\"../../_images/sphx_glr_05-layer-norm_thumb.png\" />\n <p><a class=\"reference internal\" href=\"05-layer-norm.html#sphx-glr-getting-started-tutorials-05-layer-norm-py\"><span class=\"std std-ref\">Layer Normalization</span></a></p>\n   <div class=\"sphx-glr-thumbnail-title\">Layer Normalization</div>\n+</div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"This is a Triton implementation of the Flash Attention algorithm (see: Dao et al., https://arxi...\"><img alt=\"\" src=\"../../_images/sphx_glr_06-fused-attention_thumb.png\" />\n+<p><a class=\"reference internal\" href=\"06-fused-attention.html#sphx-glr-getting-started-tutorials-06-fused-attention-py\"><span class=\"std std-ref\">Fused Attention</span></a></p>\n+  <div class=\"sphx-glr-thumbnail-title\">Fused Attention</div>\n </div><div class=\"sphx-glr-thumbcontainer\" tooltip=\"Libdevice (`tl.math`) function\"><img alt=\"\" src=\"../../_images/sphx_glr_07-math-functions_thumb.png\" />\n <p><a class=\"reference internal\" href=\"07-math-functions.html#sphx-glr-getting-started-tutorials-07-math-functions-py\"><span class=\"std std-ref\">Libdevice (tl.math) function</span></a></p>\n   <div class=\"sphx-glr-thumbnail-title\">Libdevice (`tl.math`) function</div>"}, {"filename": "main/getting-started/tutorials/sg_execution_times.html", "status": "modified", "additions": 13, "deletions": 9, "changes": 22, "file_content_changes": "@@ -86,31 +86,35 @@\n              \n   <section id=\"computation-times\">\n <span id=\"sphx-glr-getting-started-tutorials-sg-execution-times\"></span><h1>Computation times<a class=\"headerlink\" href=\"#computation-times\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<p><strong>03:59.262</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n+<p><strong>04:07.355</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n <table class=\"docutils align-default\">\n <tbody>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"03-matrix-multiplication.html#sphx-glr-getting-started-tutorials-03-matrix-multiplication-py\"><span class=\"std std-ref\">Matrix Multiplication</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">03-matrix-multiplication.py</span></code>)</p></td>\n-<td><p>01:36.561</p></td>\n+<td><p>01:37.087</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py\"><span class=\"std std-ref\">Fused Softmax</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">02-fused-softmax.py</span></code>)</p></td>\n-<td><p>01:17.871</p></td>\n+<td><p>01:18.602</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"05-layer-norm.html#sphx-glr-getting-started-tutorials-05-layer-norm-py\"><span class=\"std std-ref\">Layer Normalization</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">05-layer-norm.py</span></code>)</p></td>\n-<td><p>00:38.887</p></td>\n+<td><p>00:39.183</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py\"><span class=\"std std-ref\">Vector Addition</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">01-vector-add.py</span></code>)</p></td>\n-<td><p>00:24.542</p></td>\n+<td><p>00:25.429</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n-<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"04-low-memory-dropout.html#sphx-glr-getting-started-tutorials-04-low-memory-dropout-py\"><span class=\"std std-ref\">Low-Memory Dropout</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">04-low-memory-dropout.py</span></code>)</p></td>\n-<td><p>00:01.029</p></td>\n+<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"06-fused-attention.html#sphx-glr-getting-started-tutorials-06-fused-attention-py\"><span class=\"std std-ref\">Fused Attention</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">06-fused-attention.py</span></code>)</p></td>\n+<td><p>00:05.605</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n-<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"07-math-functions.html#sphx-glr-getting-started-tutorials-07-math-functions-py\"><span class=\"std std-ref\">Libdevice (tl.math) function</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">07-math-functions.py</span></code>)</p></td>\n-<td><p>00:00.373</p></td>\n+<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"04-low-memory-dropout.html#sphx-glr-getting-started-tutorials-04-low-memory-dropout-py\"><span class=\"std std-ref\">Low-Memory Dropout</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">04-low-memory-dropout.py</span></code>)</p></td>\n+<td><p>00:01.060</p></td>\n+<td><p>0.0 MB</p></td>\n+</tr>\n+<tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"07-math-functions.html#sphx-glr-getting-started-tutorials-07-math-functions-py\"><span class=\"std std-ref\">Libdevice (tl.math) function</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">07-math-functions.py</span></code>)</p></td>\n+<td><p>00:00.390</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n </tbody>"}, {"filename": "main/objects.inv", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/searchindex.js", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "N/A"}]