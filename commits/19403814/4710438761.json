[{"filename": "main/.buildinfo", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n # Sphinx build info version 1\n # This file hashes the configuration used when building these files. When it is not found, a full rebuild will be done.\n-config: c0246c2755c9369676fe2eee0eabd07d\n+config: 6d9df7aa5618d52e98a901a0795811e2\n tags: 645f666f9bcd5a90fca523b33c5a78b7"}, {"filename": "main/.doctrees/environment.pickle", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/installation.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/01-vector-add.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/02-fused-softmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/03-matrix-multiplication.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/04-low-memory-dropout.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/05-layer-norm.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/06-fused-attention.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/07-math-functions.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/08-experimental-block-pointer.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/index.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/sg_execution_times.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/index.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/programming-guide/chapter-1/introduction.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/programming-guide/chapter-2/related-work.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.Config.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.autotune.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.heuristics.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.jit.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.abs.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.arange.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.argmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.argmin.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_add.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_cas.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_max.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_min.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_xchg.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.broadcast_to.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cos.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.dot.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.exp.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.load.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.log.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.max.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.maximum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.min.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.minimum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.multiple_of.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.num_programs.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.program_id.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.rand.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randint.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randint4x.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randn.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.ravel.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.reduce.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.reshape.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sigmoid.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sin.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.softmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sqrt.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.store.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.where.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.xor_sum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.zeros.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.Benchmark.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.do_bench.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.perf_report.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.language.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.testing.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/4d6052117d61c2ca779cd4b75567fee5/08-experimental-block-pointer.py", "status": "modified", "additions": 22, "deletions": 14, "changes": 36, "file_content_changes": "@@ -32,14 +32,20 @@\n # --------------------\n # A block pointer pointers to a block in a parent tensor and is constructed by :code:`make_block_ptr` function,\n # which takes the following information as arguments:\n-# - :code:`base`: the base pointer to the parent tensor;\n-# - :code:`shape`: the shape of the parent tensor;\n-# - :code:`strides`: the strides of the parent tensor, which means how much to increase the pointer by when moving by 1 element in a specific axis;\n-# - :code:`offsets`: the offsets of the block;\n-# - :code:`block_shape`: the shape of the block;\n-# - :code:`order`: the order of the block, which means how the block is laid out in memory.\n-#\n-# For example, to a block pointer to a :code:`BLOCK_SIZE_M`x:code:`BLOCK_SIZE_K` block in a row-major 2D matrix A by\n+#\n+# * :code:`base`: the base pointer to the parent tensor;\n+#\n+# * :code:`shape`: the shape of the parent tensor;\n+#\n+# * :code:`strides`: the strides of the parent tensor, which means how much to increase the pointer by when moving by 1 element in a specific axis;\n+#\n+# * :code:`offsets`: the offsets of the block;\n+#\n+# * :code:`block_shape`: the shape of the block;\n+#\n+# * :code:`order`: the order of the block, which means how the block is laid out in memory.\n+#\n+# For example, to a block pointer to a :code:`BLOCK_SIZE_M * BLOCK_SIZE_K` block in a row-major 2D matrix A by\n # offsets :code:`(pid_m * BLOCK_SIZE_M, 0)` and strides :code:`(stride_am, stride_ak)`, we can use the following code\n # (exactly the same as the previous matrix multiplication tutorial):\n #\n@@ -58,15 +64,17 @@\n # --------------------------\n # To load/store a block pointer, we can use :code:`load/store` function, which takes a block pointer as an argument,\n # de-references it, and loads/stores a block. You may mask some values in the block, here we have an extra argument\n-# :code:`boundary_check` to specify whether to check the boundary of each axis for the block pointer. With check on and\n+# :code:`boundary_check` to specify whether to check the boundary of each axis for the block pointer. With check on,\n # out-of-bound values will be masked according to the :code:`padding_option` argument (load only), which can be\n # :code:`zero` or :code:`nan`. Temporarily, we do not support other values due to some hardware limitations. In this\n # mode of block pointer load/store does not support :code:`mask` or :code:`other` arguments in the legacy mode.\n #\n-# So to load a block in A, we can simply write :code:`a = tl.load(a_block_ptr, boundary_check=(0, 1))`. Boundary check\n-# may cost extra performance, so if you can guarantee that the block pointer is always in-bound in some axis, you can\n-# turn off the check by not passing the index into the :code:`boundary_check` argument. For example, if we know that\n-# :code:`M` is a multiple of :code:`BLOCK_SIZE_M`, we can write :code:`a = tl.load(a_block_ptr, boundary_check=(1, ))`.\n+# So to load the block pointer of A in the previous section, we can simply write\n+# :code:`a = tl.load(a_block_ptr, boundary_check=(0, 1))`. Boundary check may cost extra performance, so if you can\n+# guarantee that the block pointer is always in-bound in some axis, you can turn off the check by not passing the index\n+# into the :code:`boundary_check` argument. For example, if we know that :code:`M` is a multiple of\n+# :code:`BLOCK_SIZE_M`, we can replace with :code:`a = tl.load(a_block_ptr, boundary_check=(1, ))`, since axis 0 is\n+# always in bound.\n \n # %%\n # Advance a Block Pointer\n@@ -76,7 +84,7 @@\n # but with the offsets advanced by the specified amount.\n #\n # For example, to advance the block pointer by :code:`BLOCK_SIZE_K` in the second axis\n-# (no need to multiply with stride), we can write :code:`a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))`.\n+# (no need to multiply with strides), we can write :code:`a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))`.\n \n # %%\n # Final Result"}, {"filename": "main/_downloads/662999063954282841dc90b8945f85ce/tutorials_jupyter.zip", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/763344228ae6bc253ed1a6cf586aa30d/tutorials_python.zip", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/9705af6f33c6e66c6fa78f2394331536/08-experimental-block-pointer.ipynb", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -29,21 +29,21 @@\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"## Make a Block Pointer\\nA block pointer pointers to a block in a parent tensor and is constructed by :code:`make_block_ptr` function,\\nwhich takes the following information as arguments:\\n- :code:`base`: the base pointer to the parent tensor;\\n- :code:`shape`: the shape of the parent tensor;\\n- :code:`strides`: the strides of the parent tensor, which means how much to increase the pointer by when moving by 1 element in a specific axis;\\n- :code:`offsets`: the offsets of the block;\\n- :code:`block_shape`: the shape of the block;\\n- :code:`order`: the order of the block, which means how the block is laid out in memory.\\n\\nFor example, to a block pointer to a :code:`BLOCK_SIZE_M`x:code:`BLOCK_SIZE_K` block in a row-major 2D matrix A by\\noffsets :code:`(pid_m * BLOCK_SIZE_M, 0)` and strides :code:`(stride_am, stride_ak)`, we can use the following code\\n(exactly the same as the previous matrix multiplication tutorial):\\n\\n```python\\na_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\\n                                offsets=(pid_m * BLOCK_SIZE_M, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K),\\n                                order=(1, 0))\\n```\\nNote that the :code:`order` argument is set to :code:`(1, 0)`, which means the second axis is the inner dimension in\\nterms of storage, and the first axis is the outer dimension. This information may sound redundant, but it is necessary\\nfor some hardware backends to optimize for better performance.\\n\\n\"\n+        \"## Make a Block Pointer\\nA block pointer pointers to a block in a parent tensor and is constructed by :code:`make_block_ptr` function,\\nwhich takes the following information as arguments:\\n\\n* :code:`base`: the base pointer to the parent tensor;\\n\\n* :code:`shape`: the shape of the parent tensor;\\n\\n* :code:`strides`: the strides of the parent tensor, which means how much to increase the pointer by when moving by 1 element in a specific axis;\\n\\n* :code:`offsets`: the offsets of the block;\\n\\n* :code:`block_shape`: the shape of the block;\\n\\n* :code:`order`: the order of the block, which means how the block is laid out in memory.\\n\\nFor example, to a block pointer to a :code:`BLOCK_SIZE_M * BLOCK_SIZE_K` block in a row-major 2D matrix A by\\noffsets :code:`(pid_m * BLOCK_SIZE_M, 0)` and strides :code:`(stride_am, stride_ak)`, we can use the following code\\n(exactly the same as the previous matrix multiplication tutorial):\\n\\n```python\\na_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\\n                                offsets=(pid_m * BLOCK_SIZE_M, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K),\\n                                order=(1, 0))\\n```\\nNote that the :code:`order` argument is set to :code:`(1, 0)`, which means the second axis is the inner dimension in\\nterms of storage, and the first axis is the outer dimension. This information may sound redundant, but it is necessary\\nfor some hardware backends to optimize for better performance.\\n\\n\"\n       ]\n     },\n     {\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"## Load/Store a Block Pointer\\nTo load/store a block pointer, we can use :code:`load/store` function, which takes a block pointer as an argument,\\nde-references it, and loads/stores a block. You may mask some values in the block, here we have an extra argument\\n:code:`boundary_check` to specify whether to check the boundary of each axis for the block pointer. With check on and\\nout-of-bound values will be masked according to the :code:`padding_option` argument (load only), which can be\\n:code:`zero` or :code:`nan`. Temporarily, we do not support other values due to some hardware limitations. In this\\nmode of block pointer load/store does not support :code:`mask` or :code:`other` arguments in the legacy mode.\\n\\nSo to load a block in A, we can simply write :code:`a = tl.load(a_block_ptr, boundary_check=(0, 1))`. Boundary check\\nmay cost extra performance, so if you can guarantee that the block pointer is always in-bound in some axis, you can\\nturn off the check by not passing the index into the :code:`boundary_check` argument. For example, if we know that\\n:code:`M` is a multiple of :code:`BLOCK_SIZE_M`, we can write :code:`a = tl.load(a_block_ptr, boundary_check=(1, ))`.\\n\\n\"\n+        \"## Load/Store a Block Pointer\\nTo load/store a block pointer, we can use :code:`load/store` function, which takes a block pointer as an argument,\\nde-references it, and loads/stores a block. You may mask some values in the block, here we have an extra argument\\n:code:`boundary_check` to specify whether to check the boundary of each axis for the block pointer. With check on,\\nout-of-bound values will be masked according to the :code:`padding_option` argument (load only), which can be\\n:code:`zero` or :code:`nan`. Temporarily, we do not support other values due to some hardware limitations. In this\\nmode of block pointer load/store does not support :code:`mask` or :code:`other` arguments in the legacy mode.\\n\\nSo to load the block pointer of A in the previous section, we can simply write\\n:code:`a = tl.load(a_block_ptr, boundary_check=(0, 1))`. Boundary check may cost extra performance, so if you can\\nguarantee that the block pointer is always in-bound in some axis, you can turn off the check by not passing the index\\ninto the :code:`boundary_check` argument. For example, if we know that :code:`M` is a multiple of\\n:code:`BLOCK_SIZE_M`, we can replace with :code:`a = tl.load(a_block_ptr, boundary_check=(1, ))`, since axis 0 is\\nalways in bound.\\n\\n\"\n       ]\n     },\n     {\n       \"cell_type\": \"markdown\",\n       \"metadata\": {},\n       \"source\": [\n-        \"## Advance a Block Pointer\\nTo advance a block pointer, we can use :code:`advance` function, which takes a block pointer and the increment for\\neach axis as arguments and returns a new block pointer with the same shape and strides as the original one,\\nbut with the offsets advanced by the specified amount.\\n\\nFor example, to advance the block pointer by :code:`BLOCK_SIZE_K` in the second axis\\n(no need to multiply with stride), we can write :code:`a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))`.\\n\\n\"\n+        \"## Advance a Block Pointer\\nTo advance a block pointer, we can use :code:`advance` function, which takes a block pointer and the increment for\\neach axis as arguments and returns a new block pointer with the same shape and strides as the original one,\\nbut with the offsets advanced by the specified amount.\\n\\nFor example, to advance the block pointer by :code:`BLOCK_SIZE_K` in the second axis\\n(no need to multiply with strides), we can write :code:`a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))`.\\n\\n\"\n       ]\n     },\n     {"}, {"filename": "main/_images/sphx_glr_01-vector-add_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_01-vector-add_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_02-fused-softmax_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_02-fused-softmax_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_03-matrix-multiplication_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_03-matrix-multiplication_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_05-layer-norm_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_05-layer-norm_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_002.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_sources/getting-started/tutorials/01-vector-add.rst.txt", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -246,20 +246,20 @@ We can now run the decorated function above. Pass `print_data=True` to see the p\n     7      524288.0   614.400016   614.400016\n     8     1048576.0   819.200021   819.200021\n     9     2097152.0  1023.999964  1023.999964\n-    10    4194304.0  1260.307736  1228.800031\n+    10    4194304.0  1228.800031  1228.800031\n     11    8388608.0  1424.695621  1404.342820\n     12   16777216.0  1560.380965  1560.380965\n     13   33554432.0  1631.601649  1624.859540\n     14   67108864.0  1669.706983  1662.646960\n-    15  134217728.0  1684.008546  1678.616907\n+    15  134217728.0  1684.910539  1678.616907\n \n \n \n \n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  24.232 seconds)\n+   **Total running time of the script:** ( 0 minutes  23.862 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_01-vector-add.py:"}, {"filename": "main/_sources/getting-started/tutorials/02-fused-softmax.rst.txt", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -307,17 +307,17 @@ We will then compare its performance against (1) :code:`torch.softmax` and (2) t\n     [16384] 0\n     softmax-performance:\n               N       Triton  Torch (native)  Torch (jit)\n-    0     256.0   682.666643      744.727267   273.066674\n+    0     256.0   682.666643      682.666643   264.258068\n     1     384.0   877.714274      877.714274   332.108094\n     2     512.0   910.222190      910.222190   372.363633\n-    3     640.0   975.238103      930.909084   409.600010\n+    3     640.0   975.238103      975.238103   409.600010\n     4     768.0  1068.521715     1023.999964   431.157886\n     ..      ...          ...             ...          ...\n-    93  12160.0  1588.244879     1069.010969   590.022730\n-    94  12288.0  1591.967682     1018.694301   590.414408\n-    95  12416.0  1582.916395     1031.979242   586.871514\n+    93  12160.0  1594.754129     1066.082150   589.575740\n+    94  12288.0  1591.967682     1018.694301   589.529222\n+    95  12416.0  1582.916395     1029.305700   586.871514\n     96  12544.0  1580.346374     1013.656595   588.574769\n-    97  12672.0  1584.000004     1006.213368   588.539906\n+    97  12672.0  1584.000004     1008.716405   587.686943\n \n     [98 rows x 4 columns]\n \n@@ -334,7 +334,7 @@ In the above plot, we can see that:\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 1 minutes  17.617 seconds)\n+   **Total running time of the script:** ( 1 minutes  17.826 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:"}, {"filename": "main/_sources/getting-started/tutorials/03-matrix-multiplication.rst.txt", "status": "modified", "additions": 18, "deletions": 18, "changes": 36, "file_content_changes": "@@ -457,37 +457,37 @@ but feel free to arrange this script as you wish to benchmark any other matrix s\n     5    896.0   78.051553   82.642822\n     6   1024.0  110.376426   99.864382\n     7   1152.0  135.726544  129.825388\n-    8   1280.0  157.538463  163.840004\n+    8   1280.0  163.840004  163.840004\n     9   1408.0  155.765024  132.970149\n     10  1536.0  181.484314  157.286398\n     11  1664.0  179.978245  179.978245\n-    12  1792.0  172.914215  208.137481\n-    13  1920.0  200.347822  168.585369\n-    14  2048.0  197.379013  190.650180\n+    12  1792.0  172.914215  204.353162\n+    13  1920.0  197.485709  168.585369\n+    14  2048.0  195.083907  190.650180\n     15  2176.0  189.845737  209.621326\n     16  2304.0  225.357284  227.503545\n     17  2432.0  200.674737  200.674737\n-    18  2560.0  222.911566  217.006622\n-    19  2688.0  196.544332  196.544332\n-    20  2816.0  208.680416  208.680416\n-    21  2944.0  215.740400  222.482283\n-    22  3072.0  207.410628  207.410628\n-    23  3200.0  215.488222  217.687077\n-    24  3328.0  202.792385  205.689424\n-    25  3456.0  216.143621  217.896133\n-    26  3584.0  217.186932  205.756041\n-    27  3712.0  208.553950  214.371984\n-    28  3840.0  205.944129  206.328356\n-    29  3968.0  207.171367  213.702171\n-    30  4096.0  219.310012  214.405318\n+    18  2560.0  222.911566  212.779229\n+    19  2688.0  194.528492  196.544332\n+    20  2816.0  207.686706  210.696652\n+    21  2944.0  220.513412  222.482283\n+    22  3072.0  205.156169  205.902197\n+    23  3200.0  211.221117  217.687077\n+    24  3328.0  205.689424  203.941342\n+    25  3456.0  214.990846  215.565692\n+    26  3584.0  217.712813  207.656790\n+    27  3712.0  207.686788  214.371984\n+    28  3840.0  205.944129  205.944129\n+    29  3968.0  207.171367  215.209760\n+    30  4096.0  218.595642  211.699888\n \n \n \n \n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 1 minutes  36.499 seconds)\n+   **Total running time of the script:** ( 1 minutes  35.307 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_03-matrix-multiplication.py:"}, {"filename": "main/_sources/getting-started/tutorials/04-low-memory-dropout.rst.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -242,7 +242,7 @@ References\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  0.707 seconds)\n+   **Total running time of the script:** ( 0 minutes  0.706 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_04-low-memory-dropout.py:"}, {"filename": "main/_sources/getting-started/tutorials/05-layer-norm.rst.txt", "status": "modified", "additions": 20, "deletions": 20, "changes": 40, "file_content_changes": "@@ -442,36 +442,36 @@ Specifically, one can set :code:`'mode': 'backward'` to benchmark the backward p\n     [16384] 0\n     layer-norm-backward:\n               N      Triton       Torch\n-    0    1024.0  240.941181  356.173905\n-    1    1536.0  361.411771  400.695643\n-    2    2048.0  450.935778  438.857137\n-    3    2560.0  538.947358  469.007657\n+    0    1024.0  234.057145  356.173905\n+    1    1536.0  357.902918  400.695643\n+    2    2048.0  455.111110  438.857137\n+    3    2560.0  534.260858  469.007657\n     4    3072.0  619.563043  501.551024\n-    5    3584.0  688.127967  445.678757\n-    6    4096.0  744.727267  436.906674\n-    7    4608.0  708.923101  437.122520\n-    8    5120.0  763.229797  443.610086\n+    5    3584.0  688.127967  443.381459\n+    6    4096.0  750.412251  436.906674\n+    7    4608.0  704.407633  437.122520\n+    8    5120.0  758.518534  443.610086\n     9    5632.0  804.571435  459.755106\n     10   6144.0  842.605744  465.160886\n     11   6656.0  882.563556  471.221251\n     12   7168.0  915.063803  451.527570\n-    13   7680.0  950.103127  442.014385\n+    13   7680.0  945.230767  442.014385\n     14   8192.0  978.149241  464.794337\n-    15   8704.0  663.161879  457.102857\n+    15   8704.0  661.063311  457.102857\n     16   9216.0  686.906817  466.632911\n-    17   9728.0  711.804890  470.709684\n+    17   9728.0  713.981680  470.709684\n     18  10240.0  731.428577  466.337764\n-    19  10752.0  758.964709  466.632895\n+    19  10752.0  761.203560  465.790591\n     20  11264.0  781.317950  469.333317\n-    21  11776.0  802.909085  471.826361\n-    22  12288.0  821.481899  471.105436\n+    21  11776.0  800.634537  471.826361\n+    22  12288.0  819.199988  471.105436\n     23  12800.0  823.592520  471.889394\n     24  13312.0  840.757868  477.560558\n-    25  13824.0  844.213752  478.753261\n-    26  14336.0  849.540743  476.542919\n-    27  14848.0  852.516725  476.406423\n-    28  15360.0  863.325544  476.279061\n-    29  15872.0  865.745448  479.154717\n+    25  13824.0  844.213752  478.063409\n+    26  14336.0  851.643583  475.883834\n+    27  14848.0  852.516725  476.725092\n+    28  15360.0  861.308412  476.279061\n+    29  15872.0  863.782291  479.154717\n \n \n \n@@ -486,7 +486,7 @@ References\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  38.028 seconds)\n+   **Total running time of the script:** ( 0 minutes  37.674 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_05-layer-norm.py:"}, {"filename": "main/_sources/getting-started/tutorials/06-fused-attention.rst.txt", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "@@ -54,17 +54,17 @@ This is a Triton implementation of the Flash Attention algorithm\n     [128, 128] 1\n     fused-attention-batch4-head48-d64-fwd:\n         N_CTX     Triton\n-    0  1024.0   0.323017\n-    1  2048.0   1.094384\n-    2  4096.0   3.962921\n-    3  8192.0  14.962859\n+    0  1024.0   0.323442\n+    1  2048.0   1.100183\n+    2  4096.0   3.973653\n+    3  8192.0  14.964223\n     [128, 64] 1\n     fused-attention-batch4-head48-d64-bwd:\n         N_CTX     Triton\n-    0  1024.0   1.186626\n-    1  2048.0   3.760393\n-    2  4096.0  13.224523\n-    3  8192.0  49.601536\n+    0  1024.0   1.186240\n+    1  2048.0   3.763357\n+    2  4096.0  13.250707\n+    3  8192.0  49.626114\n \n \n \n@@ -433,7 +433,7 @@ This is a Triton implementation of the Flash Attention algorithm\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  5.120 seconds)\n+   **Total running time of the script:** ( 0 minutes  5.118 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_06-fused-attention.py:"}, {"filename": "main/_sources/getting-started/tutorials/08-experimental-block-pointer.rst.txt", "status": "modified", "additions": 30, "deletions": 22, "changes": 52, "file_content_changes": "@@ -45,20 +45,26 @@ patterns.\n Let's start with the previous matrix multiplication example and demonstrate how to rewrite it to utilize block pointer\n semantics.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 31-55\n+.. GENERATED FROM PYTHON SOURCE LINES 31-61\n \n Make a Block Pointer\n --------------------\n A block pointer pointers to a block in a parent tensor and is constructed by :code:`make_block_ptr` function,\n which takes the following information as arguments:\n-- :code:`base`: the base pointer to the parent tensor;\n-- :code:`shape`: the shape of the parent tensor;\n-- :code:`strides`: the strides of the parent tensor, which means how much to increase the pointer by when moving by 1 element in a specific axis;\n-- :code:`offsets`: the offsets of the block;\n-- :code:`block_shape`: the shape of the block;\n-- :code:`order`: the order of the block, which means how the block is laid out in memory.\n-\n-For example, to a block pointer to a :code:`BLOCK_SIZE_M`x:code:`BLOCK_SIZE_K` block in a row-major 2D matrix A by\n+\n+* :code:`base`: the base pointer to the parent tensor;\n+\n+* :code:`shape`: the shape of the parent tensor;\n+\n+* :code:`strides`: the strides of the parent tensor, which means how much to increase the pointer by when moving by 1 element in a specific axis;\n+\n+* :code:`offsets`: the offsets of the block;\n+\n+* :code:`block_shape`: the shape of the block;\n+\n+* :code:`order`: the order of the block, which means how the block is laid out in memory.\n+\n+For example, to a block pointer to a :code:`BLOCK_SIZE_M * BLOCK_SIZE_K` block in a row-major 2D matrix A by\n offsets :code:`(pid_m * BLOCK_SIZE_M, 0)` and strides :code:`(stride_am, stride_ak)`, we can use the following code\n (exactly the same as the previous matrix multiplication tutorial):\n \n@@ -72,23 +78,25 @@ Note that the :code:`order` argument is set to :code:`(1, 0)`, which means the s\n terms of storage, and the first axis is the outer dimension. This information may sound redundant, but it is necessary\n for some hardware backends to optimize for better performance.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 57-70\n+.. GENERATED FROM PYTHON SOURCE LINES 63-78\n \n Load/Store a Block Pointer\n --------------------------\n To load/store a block pointer, we can use :code:`load/store` function, which takes a block pointer as an argument,\n de-references it, and loads/stores a block. You may mask some values in the block, here we have an extra argument\n-:code:`boundary_check` to specify whether to check the boundary of each axis for the block pointer. With check on and\n+:code:`boundary_check` to specify whether to check the boundary of each axis for the block pointer. With check on,\n out-of-bound values will be masked according to the :code:`padding_option` argument (load only), which can be\n :code:`zero` or :code:`nan`. Temporarily, we do not support other values due to some hardware limitations. In this\n mode of block pointer load/store does not support :code:`mask` or :code:`other` arguments in the legacy mode.\n \n-So to load a block in A, we can simply write :code:`a = tl.load(a_block_ptr, boundary_check=(0, 1))`. Boundary check\n-may cost extra performance, so if you can guarantee that the block pointer is always in-bound in some axis, you can\n-turn off the check by not passing the index into the :code:`boundary_check` argument. For example, if we know that\n-:code:`M` is a multiple of :code:`BLOCK_SIZE_M`, we can write :code:`a = tl.load(a_block_ptr, boundary_check=(1, ))`.\n+So to load the block pointer of A in the previous section, we can simply write\n+:code:`a = tl.load(a_block_ptr, boundary_check=(0, 1))`. Boundary check may cost extra performance, so if you can\n+guarantee that the block pointer is always in-bound in some axis, you can turn off the check by not passing the index\n+into the :code:`boundary_check` argument. For example, if we know that :code:`M` is a multiple of\n+:code:`BLOCK_SIZE_M`, we can replace with :code:`a = tl.load(a_block_ptr, boundary_check=(1, ))`, since axis 0 is\n+always in bound.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 72-80\n+.. GENERATED FROM PYTHON SOURCE LINES 80-88\n \n Advance a Block Pointer\n -----------------------\n@@ -97,14 +105,14 @@ each axis as arguments and returns a new block pointer with the same shape and s\n but with the offsets advanced by the specified amount.\n \n For example, to advance the block pointer by :code:`BLOCK_SIZE_K` in the second axis\n-(no need to multiply with stride), we can write :code:`a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))`.\n+(no need to multiply with strides), we can write :code:`a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_SIZE_K))`.\n \n-.. GENERATED FROM PYTHON SOURCE LINES 82-84\n+.. GENERATED FROM PYTHON SOURCE LINES 90-92\n \n Final Result\n ------------\n \n-.. GENERATED FROM PYTHON SOURCE LINES 84-204\n+.. GENERATED FROM PYTHON SOURCE LINES 92-212\n \n .. code-block:: default\n \n@@ -235,14 +243,14 @@ Final Result\n \n \n \n-.. GENERATED FROM PYTHON SOURCE LINES 205-209\n+.. GENERATED FROM PYTHON SOURCE LINES 213-217\n \n Unit Test\n ---------\n \n Still we can test our matrix multiplication with block pointers against a native torch implementation (i.e., cuBLAS).\n \n-.. GENERATED FROM PYTHON SOURCE LINES 209-221\n+.. GENERATED FROM PYTHON SOURCE LINES 217-229\n \n .. code-block:: default\n \n@@ -290,7 +298,7 @@ Still we can test our matrix multiplication with block pointers against a native\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  9.969 seconds)\n+   **Total running time of the script:** ( 0 minutes  10.049 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_08-experimental-block-pointer.py:"}, {"filename": "main/_sources/getting-started/tutorials/sg_execution_times.rst.txt", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -6,22 +6,22 @@\n \n Computation times\n =================\n-**04:12.435** total execution time for **getting-started_tutorials** files:\n+**04:10.806** total execution time for **getting-started_tutorials** files:\n \n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``)           | 01:36.499 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``)           | 01:35.307 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                           | 01:17.617 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                           | 01:17.826 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                                 | 00:38.028 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                                 | 00:37.674 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                                 | 00:24.232 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                                 | 00:23.862 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_08-experimental-block-pointer.py` (``08-experimental-block-pointer.py``) | 00:09.969 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_08-experimental-block-pointer.py` (``08-experimental-block-pointer.py``) | 00:10.049 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py` (``06-fused-attention.py``)                       | 00:05.120 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py` (``06-fused-attention.py``)                       | 00:05.118 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)                 | 00:00.707 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)                 | 00:00.706 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n | :ref:`sphx_glr_getting-started_tutorials_07-math-functions.py` (``07-math-functions.py``)                         | 00:00.264 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+"}, {"filename": "main/getting-started/tutorials/01-vector-add.html", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -245,15 +245,15 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n 7      524288.0   614.400016   614.400016\n 8     1048576.0   819.200021   819.200021\n 9     2097152.0  1023.999964  1023.999964\n-10    4194304.0  1260.307736  1228.800031\n+10    4194304.0  1228.800031  1228.800031\n 11    8388608.0  1424.695621  1404.342820\n 12   16777216.0  1560.380965  1560.380965\n 13   33554432.0  1631.601649  1624.859540\n 14   67108864.0  1669.706983  1662.646960\n-15  134217728.0  1684.008546  1678.616907\n+15  134217728.0  1684.910539  1678.616907\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  24.232 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  23.862 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-01-vector-add-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/62d97d49a32414049819dd8bb8378080/01-vector-add.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">01-vector-add.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/02-fused-softmax.html", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -300,17 +300,17 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n [16384] 0\n softmax-performance:\n           N       Triton  Torch (native)  Torch (jit)\n-0     256.0   682.666643      744.727267   273.066674\n+0     256.0   682.666643      682.666643   264.258068\n 1     384.0   877.714274      877.714274   332.108094\n 2     512.0   910.222190      910.222190   372.363633\n-3     640.0   975.238103      930.909084   409.600010\n+3     640.0   975.238103      975.238103   409.600010\n 4     768.0  1068.521715     1023.999964   431.157886\n ..      ...          ...             ...          ...\n-93  12160.0  1588.244879     1069.010969   590.022730\n-94  12288.0  1591.967682     1018.694301   590.414408\n-95  12416.0  1582.916395     1031.979242   586.871514\n+93  12160.0  1594.754129     1066.082150   589.575740\n+94  12288.0  1591.967682     1018.694301   589.529222\n+95  12416.0  1582.916395     1029.305700   586.871514\n 96  12544.0  1580.346374     1013.656595   588.574769\n-97  12672.0  1584.000004     1006.213368   588.539906\n+97  12672.0  1584.000004     1008.716405   587.686943\n \n [98 rows x 4 columns]\n </pre></div>\n@@ -323,7 +323,7 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n </ul>\n </dd>\n </dl>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  17.617 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  17.826 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-02-fused-softmax-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d91442ac2982c4e0cc3ab0f43534afbc/02-fused-softmax.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">02-fused-softmax.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/03-matrix-multiplication.html", "status": "modified", "additions": 18, "deletions": 18, "changes": 36, "file_content_changes": "@@ -471,32 +471,32 @@ <h3>Square Matrix Performance<a class=\"headerlink\" href=\"#square-matrix-performa\n 5    896.0   78.051553   82.642822\n 6   1024.0  110.376426   99.864382\n 7   1152.0  135.726544  129.825388\n-8   1280.0  157.538463  163.840004\n+8   1280.0  163.840004  163.840004\n 9   1408.0  155.765024  132.970149\n 10  1536.0  181.484314  157.286398\n 11  1664.0  179.978245  179.978245\n-12  1792.0  172.914215  208.137481\n-13  1920.0  200.347822  168.585369\n-14  2048.0  197.379013  190.650180\n+12  1792.0  172.914215  204.353162\n+13  1920.0  197.485709  168.585369\n+14  2048.0  195.083907  190.650180\n 15  2176.0  189.845737  209.621326\n 16  2304.0  225.357284  227.503545\n 17  2432.0  200.674737  200.674737\n-18  2560.0  222.911566  217.006622\n-19  2688.0  196.544332  196.544332\n-20  2816.0  208.680416  208.680416\n-21  2944.0  215.740400  222.482283\n-22  3072.0  207.410628  207.410628\n-23  3200.0  215.488222  217.687077\n-24  3328.0  202.792385  205.689424\n-25  3456.0  216.143621  217.896133\n-26  3584.0  217.186932  205.756041\n-27  3712.0  208.553950  214.371984\n-28  3840.0  205.944129  206.328356\n-29  3968.0  207.171367  213.702171\n-30  4096.0  219.310012  214.405318\n+18  2560.0  222.911566  212.779229\n+19  2688.0  194.528492  196.544332\n+20  2816.0  207.686706  210.696652\n+21  2944.0  220.513412  222.482283\n+22  3072.0  205.156169  205.902197\n+23  3200.0  211.221117  217.687077\n+24  3328.0  205.689424  203.941342\n+25  3456.0  214.990846  215.565692\n+26  3584.0  217.712813  207.656790\n+27  3712.0  207.686788  214.371984\n+28  3840.0  205.944129  205.944129\n+29  3968.0  207.171367  215.209760\n+30  4096.0  218.595642  211.699888\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  36.499 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 1 minutes  35.307 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-03-matrix-multiplication-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d5fee5b55a64e47f1b5724ec39adf171/03-matrix-multiplication.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">03-matrix-multiplication.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/04-low-memory-dropout.html", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -287,7 +287,7 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n <p>Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \u201cDropout: A Simple Way to Prevent Neural Networks from Overfitting\u201d, JMLR 2014</p>\n </div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.707 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.706 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-04-low-memory-dropout-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/c9aed78977a4c05741d675a38dde3d7d/04-low-memory-dropout.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">04-low-memory-dropout.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/05-layer-norm.html", "status": "modified", "additions": 20, "deletions": 20, "changes": 40, "file_content_changes": "@@ -471,36 +471,36 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n [16384] 0\n layer-norm-backward:\n           N      Triton       Torch\n-0    1024.0  240.941181  356.173905\n-1    1536.0  361.411771  400.695643\n-2    2048.0  450.935778  438.857137\n-3    2560.0  538.947358  469.007657\n+0    1024.0  234.057145  356.173905\n+1    1536.0  357.902918  400.695643\n+2    2048.0  455.111110  438.857137\n+3    2560.0  534.260858  469.007657\n 4    3072.0  619.563043  501.551024\n-5    3584.0  688.127967  445.678757\n-6    4096.0  744.727267  436.906674\n-7    4608.0  708.923101  437.122520\n-8    5120.0  763.229797  443.610086\n+5    3584.0  688.127967  443.381459\n+6    4096.0  750.412251  436.906674\n+7    4608.0  704.407633  437.122520\n+8    5120.0  758.518534  443.610086\n 9    5632.0  804.571435  459.755106\n 10   6144.0  842.605744  465.160886\n 11   6656.0  882.563556  471.221251\n 12   7168.0  915.063803  451.527570\n-13   7680.0  950.103127  442.014385\n+13   7680.0  945.230767  442.014385\n 14   8192.0  978.149241  464.794337\n-15   8704.0  663.161879  457.102857\n+15   8704.0  661.063311  457.102857\n 16   9216.0  686.906817  466.632911\n-17   9728.0  711.804890  470.709684\n+17   9728.0  713.981680  470.709684\n 18  10240.0  731.428577  466.337764\n-19  10752.0  758.964709  466.632895\n+19  10752.0  761.203560  465.790591\n 20  11264.0  781.317950  469.333317\n-21  11776.0  802.909085  471.826361\n-22  12288.0  821.481899  471.105436\n+21  11776.0  800.634537  471.826361\n+22  12288.0  819.199988  471.105436\n 23  12800.0  823.592520  471.889394\n 24  13312.0  840.757868  477.560558\n-25  13824.0  844.213752  478.753261\n-26  14336.0  849.540743  476.542919\n-27  14848.0  852.516725  476.406423\n-28  15360.0  863.325544  476.279061\n-29  15872.0  865.745448  479.154717\n+25  13824.0  844.213752  478.063409\n+26  14336.0  851.643583  475.883834\n+27  14848.0  852.516725  476.725092\n+28  15360.0  861.308412  476.279061\n+29  15872.0  863.782291  479.154717\n </pre></div>\n </div>\n </section>\n@@ -512,7 +512,7 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n <p>Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \u201cLayer Normalization\u201d, Arxiv 2016</p>\n </div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  38.028 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  37.674 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-05-layer-norm-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/935c0dd0fbeb4b2e69588471cbb2d4b2/05-layer-norm.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">05-layer-norm.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/06-fused-attention.html", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "@@ -114,17 +114,17 @@\n [128, 128] 1\n fused-attention-batch4-head48-d64-fwd:\n     N_CTX     Triton\n-0  1024.0   0.323017\n-1  2048.0   1.094384\n-2  4096.0   3.962921\n-3  8192.0  14.962859\n+0  1024.0   0.323442\n+1  2048.0   1.100183\n+2  4096.0   3.973653\n+3  8192.0  14.964223\n [128, 64] 1\n fused-attention-batch4-head48-d64-bwd:\n     N_CTX     Triton\n-0  1024.0   1.186626\n-1  2048.0   3.760393\n-2  4096.0  13.224523\n-3  8192.0  49.601536\n+0  1024.0   1.186240\n+1  2048.0   3.763357\n+2  4096.0  13.250707\n+3  8192.0  49.626114\n </pre></div>\n </div>\n <div class=\"line-block\">\n@@ -485,7 +485,7 @@\n <span class=\"n\">bench_flash_attention</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;.&#39;</span><span class=\"p\">,</span> <span class=\"n\">print_data</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  5.120 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  5.118 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-06-fused-attention-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/54a35f6ec55f9746935b9566fb6bb1df/06-fused-attention.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">06-fused-attention.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/08-experimental-block-pointer.html", "status": "modified", "additions": 19, "deletions": 15, "changes": 34, "file_content_changes": "@@ -134,14 +134,16 @@ <h2>Motivations<a class=\"headerlink\" href=\"#motivations\" title=\"Permalink to thi\n <section id=\"make-a-block-pointer\">\n <h2>Make a Block Pointer<a class=\"headerlink\" href=\"#make-a-block-pointer\" title=\"Permalink to this heading\">\u00b6</a></h2>\n <p>A block pointer pointers to a block in a parent tensor and is constructed by <code class=\"code docutils literal notranslate\"><span class=\"pre\">make_block_ptr</span></code> function,\n-which takes the following information as arguments:\n-- <code class=\"code docutils literal notranslate\"><span class=\"pre\">base</span></code>: the base pointer to the parent tensor;\n-- <code class=\"code docutils literal notranslate\"><span class=\"pre\">shape</span></code>: the shape of the parent tensor;\n-- <code class=\"code docutils literal notranslate\"><span class=\"pre\">strides</span></code>: the strides of the parent tensor, which means how much to increase the pointer by when moving by 1 element in a specific axis;\n-- <code class=\"code docutils literal notranslate\"><span class=\"pre\">offsets</span></code>: the offsets of the block;\n-- <code class=\"code docutils literal notranslate\"><span class=\"pre\">block_shape</span></code>: the shape of the block;\n-- <code class=\"code docutils literal notranslate\"><span class=\"pre\">order</span></code>: the order of the block, which means how the block is laid out in memory.</p>\n-<p>For example, to a block pointer to a <code class=\"code docutils literal notranslate\"><span class=\"pre\">BLOCK_SIZE_M`x:code:`BLOCK_SIZE_K</span></code> block in a row-major 2D matrix A by\n+which takes the following information as arguments:</p>\n+<ul class=\"simple\">\n+<li><p><code class=\"code docutils literal notranslate\"><span class=\"pre\">base</span></code>: the base pointer to the parent tensor;</p></li>\n+<li><p><code class=\"code docutils literal notranslate\"><span class=\"pre\">shape</span></code>: the shape of the parent tensor;</p></li>\n+<li><p><code class=\"code docutils literal notranslate\"><span class=\"pre\">strides</span></code>: the strides of the parent tensor, which means how much to increase the pointer by when moving by 1 element in a specific axis;</p></li>\n+<li><p><code class=\"code docutils literal notranslate\"><span class=\"pre\">offsets</span></code>: the offsets of the block;</p></li>\n+<li><p><code class=\"code docutils literal notranslate\"><span class=\"pre\">block_shape</span></code>: the shape of the block;</p></li>\n+<li><p><code class=\"code docutils literal notranslate\"><span class=\"pre\">order</span></code>: the order of the block, which means how the block is laid out in memory.</p></li>\n+</ul>\n+<p>For example, to a block pointer to a <code class=\"code docutils literal notranslate\"><span class=\"pre\">BLOCK_SIZE_M</span> <span class=\"pre\">*</span> <span class=\"pre\">BLOCK_SIZE_K</span></code> block in a row-major 2D matrix A by\n offsets <code class=\"code docutils literal notranslate\"><span class=\"pre\">(pid_m</span> <span class=\"pre\">*</span> <span class=\"pre\">BLOCK_SIZE_M,</span> <span class=\"pre\">0)</span></code> and strides <code class=\"code docutils literal notranslate\"><span class=\"pre\">(stride_am,</span> <span class=\"pre\">stride_ak)</span></code>, we can use the following code\n (exactly the same as the previous matrix multiplication tutorial):</p>\n <div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">a_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span><span class=\"n\">base</span><span class=\"o\">=</span><span class=\"n\">a_ptr</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">M</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"p\">),</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_am</span><span class=\"p\">,</span> <span class=\"n\">stride_ak</span><span class=\"p\">),</span>\n@@ -157,22 +159,24 @@ <h2>Make a Block Pointer<a class=\"headerlink\" href=\"#make-a-block-pointer\" title\n <h2>Load/Store a Block Pointer<a class=\"headerlink\" href=\"#load-store-a-block-pointer\" title=\"Permalink to this heading\">\u00b6</a></h2>\n <p>To load/store a block pointer, we can use <code class=\"code docutils literal notranslate\"><span class=\"pre\">load/store</span></code> function, which takes a block pointer as an argument,\n de-references it, and loads/stores a block. You may mask some values in the block, here we have an extra argument\n-<code class=\"code docutils literal notranslate\"><span class=\"pre\">boundary_check</span></code> to specify whether to check the boundary of each axis for the block pointer. With check on and\n+<code class=\"code docutils literal notranslate\"><span class=\"pre\">boundary_check</span></code> to specify whether to check the boundary of each axis for the block pointer. With check on,\n out-of-bound values will be masked according to the <code class=\"code docutils literal notranslate\"><span class=\"pre\">padding_option</span></code> argument (load only), which can be\n <code class=\"code docutils literal notranslate\"><span class=\"pre\">zero</span></code> or <code class=\"code docutils literal notranslate\"><span class=\"pre\">nan</span></code>. Temporarily, we do not support other values due to some hardware limitations. In this\n mode of block pointer load/store does not support <code class=\"code docutils literal notranslate\"><span class=\"pre\">mask</span></code> or <code class=\"code docutils literal notranslate\"><span class=\"pre\">other</span></code> arguments in the legacy mode.</p>\n-<p>So to load a block in A, we can simply write <code class=\"code docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">=</span> <span class=\"pre\">tl.load(a_block_ptr,</span> <span class=\"pre\">boundary_check=(0,</span> <span class=\"pre\">1))</span></code>. Boundary check\n-may cost extra performance, so if you can guarantee that the block pointer is always in-bound in some axis, you can\n-turn off the check by not passing the index into the <code class=\"code docutils literal notranslate\"><span class=\"pre\">boundary_check</span></code> argument. For example, if we know that\n-<code class=\"code docutils literal notranslate\"><span class=\"pre\">M</span></code> is a multiple of <code class=\"code docutils literal notranslate\"><span class=\"pre\">BLOCK_SIZE_M</span></code>, we can write <code class=\"code docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">=</span> <span class=\"pre\">tl.load(a_block_ptr,</span> <span class=\"pre\">boundary_check=(1,</span> <span class=\"pre\">))</span></code>.</p>\n+<p>So to load the block pointer of A in the previous section, we can simply write\n+<code class=\"code docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">=</span> <span class=\"pre\">tl.load(a_block_ptr,</span> <span class=\"pre\">boundary_check=(0,</span> <span class=\"pre\">1))</span></code>. Boundary check may cost extra performance, so if you can\n+guarantee that the block pointer is always in-bound in some axis, you can turn off the check by not passing the index\n+into the <code class=\"code docutils literal notranslate\"><span class=\"pre\">boundary_check</span></code> argument. For example, if we know that <code class=\"code docutils literal notranslate\"><span class=\"pre\">M</span></code> is a multiple of\n+<code class=\"code docutils literal notranslate\"><span class=\"pre\">BLOCK_SIZE_M</span></code>, we can replace with <code class=\"code docutils literal notranslate\"><span class=\"pre\">a</span> <span class=\"pre\">=</span> <span class=\"pre\">tl.load(a_block_ptr,</span> <span class=\"pre\">boundary_check=(1,</span> <span class=\"pre\">))</span></code>, since axis 0 is\n+always in bound.</p>\n </section>\n <section id=\"advance-a-block-pointer\">\n <h2>Advance a Block Pointer<a class=\"headerlink\" href=\"#advance-a-block-pointer\" title=\"Permalink to this heading\">\u00b6</a></h2>\n <p>To advance a block pointer, we can use <code class=\"code docutils literal notranslate\"><span class=\"pre\">advance</span></code> function, which takes a block pointer and the increment for\n each axis as arguments and returns a new block pointer with the same shape and strides as the original one,\n but with the offsets advanced by the specified amount.</p>\n <p>For example, to advance the block pointer by <code class=\"code docutils literal notranslate\"><span class=\"pre\">BLOCK_SIZE_K</span></code> in the second axis\n-(no need to multiply with stride), we can write <code class=\"code docutils literal notranslate\"><span class=\"pre\">a_block_ptr</span> <span class=\"pre\">=</span> <span class=\"pre\">tl.advance(a_block_ptr,</span> <span class=\"pre\">(0,</span> <span class=\"pre\">BLOCK_SIZE_K))</span></code>.</p>\n+(no need to multiply with strides), we can write <code class=\"code docutils literal notranslate\"><span class=\"pre\">a_block_ptr</span> <span class=\"pre\">=</span> <span class=\"pre\">tl.advance(a_block_ptr,</span> <span class=\"pre\">(0,</span> <span class=\"pre\">BLOCK_SIZE_K))</span></code>.</p>\n </section>\n <section id=\"final-result\">\n <h2>Final Result<a class=\"headerlink\" href=\"#final-result\" title=\"Permalink to this heading\">\u00b6</a></h2>\n@@ -331,7 +335,7 @@ <h2>Unit Test<a class=\"headerlink\" href=\"#unit-test\" title=\"Permalink to this he\n \u2705 Triton and Torch match\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  9.969 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  10.049 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-08-experimental-block-pointer-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/4d6052117d61c2ca779cd4b75567fee5/08-experimental-block-pointer.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">08-experimental-block-pointer.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/sg_execution_times.html", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -86,35 +86,35 @@\n              \n   <section id=\"computation-times\">\n <span id=\"sphx-glr-getting-started-tutorials-sg-execution-times\"></span><h1>Computation times<a class=\"headerlink\" href=\"#computation-times\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<p><strong>04:12.435</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n+<p><strong>04:10.806</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n <table class=\"docutils align-default\">\n <tbody>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"03-matrix-multiplication.html#sphx-glr-getting-started-tutorials-03-matrix-multiplication-py\"><span class=\"std std-ref\">Matrix Multiplication</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">03-matrix-multiplication.py</span></code>)</p></td>\n-<td><p>01:36.499</p></td>\n+<td><p>01:35.307</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py\"><span class=\"std std-ref\">Fused Softmax</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">02-fused-softmax.py</span></code>)</p></td>\n-<td><p>01:17.617</p></td>\n+<td><p>01:17.826</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"05-layer-norm.html#sphx-glr-getting-started-tutorials-05-layer-norm-py\"><span class=\"std std-ref\">Layer Normalization</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">05-layer-norm.py</span></code>)</p></td>\n-<td><p>00:38.028</p></td>\n+<td><p>00:37.674</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py\"><span class=\"std std-ref\">Vector Addition</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">01-vector-add.py</span></code>)</p></td>\n-<td><p>00:24.232</p></td>\n+<td><p>00:23.862</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"08-experimental-block-pointer.html#sphx-glr-getting-started-tutorials-08-experimental-block-pointer-py\"><span class=\"std std-ref\">Block Pointer (Experimental)</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">08-experimental-block-pointer.py</span></code>)</p></td>\n-<td><p>00:09.969</p></td>\n+<td><p>00:10.049</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"06-fused-attention.html#sphx-glr-getting-started-tutorials-06-fused-attention-py\"><span class=\"std std-ref\">Fused Attention</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">06-fused-attention.py</span></code>)</p></td>\n-<td><p>00:05.120</p></td>\n+<td><p>00:05.118</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"04-low-memory-dropout.html#sphx-glr-getting-started-tutorials-04-low-memory-dropout-py\"><span class=\"std std-ref\">Low-Memory Dropout</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">04-low-memory-dropout.py</span></code>)</p></td>\n-<td><p>00:00.707</p></td>\n+<td><p>00:00.706</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"07-math-functions.html#sphx-glr-getting-started-tutorials-07-math-functions-py\"><span class=\"std std-ref\">Libdevice (tl.math) function</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">07-math-functions.py</span></code>)</p></td>"}, {"filename": "main/searchindex.js", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "N/A"}]