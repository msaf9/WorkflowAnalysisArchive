[{"filename": "main/.buildinfo", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n # Sphinx build info version 1\n # This file hashes the configuration used when building these files. When it is not found, a full rebuild will be done.\n-config: bb1dfdf075d4149d0ce8f8f0e3d7dcb7\n+config: 54c4fcf2d013156f3c2a8fe3759a2c96\n tags: 645f666f9bcd5a90fca523b33c5a78b7"}, {"filename": "main/.doctrees/environment.pickle", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/installation.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/01-vector-add.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/02-fused-softmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/03-matrix-multiplication.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/04-low-memory-dropout.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/05-layer-norm.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/06-fused-attention.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/07-math-functions.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/08-experimental-block-pointer.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/index.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/getting-started/tutorials/sg_execution_times.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/index.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/programming-guide/chapter-1/introduction.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/programming-guide/chapter-2/related-work.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.Config.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.autotune.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.heuristics.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.jit.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.abs.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.arange.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.argmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.argmin.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.associative_scan.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_add.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_cas.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_max.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_min.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.atomic_xchg.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.broadcast.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.broadcast_to.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cat.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cos.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cumprod.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.cumsum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.debug_barrier.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.device_assert.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.device_print.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.dot.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.exp.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.expand_dims.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.fdiv.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.full.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.load.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.log.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.max.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.max_contiguous.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.maximum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.min.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.minimum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.multiple_of.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.num_programs.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.program_id.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.rand.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randint.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randint4x.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.randn.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.ravel.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.reduce.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.reshape.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sigmoid.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sin.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.softmax.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sqrt.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.static_assert.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.static_print.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.static_range.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.store.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.sum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.trans.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.umulhi.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.view.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.where.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.xor_sum.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.language.zeros.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.Benchmark.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.do_bench.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/generated/triton.testing.perf_report.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.language.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/.doctrees/python-api/triton.testing.doctree", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/3176accb6c7288b0e45f41d94eebacb9/06-fused-attention.ipynb", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -26,7 +26,7 @@\n       },\n       \"outputs\": [],\n       \"source\": [\n-        \"import pytest\\nimport torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.jit\\ndef _fwd_kernel(\\n    Q, K, V, sm_scale,\\n    L, M,\\n    Out,\\n    stride_qz, stride_qh, stride_qm, stride_qk,\\n    stride_kz, stride_kh, stride_kn, stride_kk,\\n    stride_vz, stride_vh, stride_vk, stride_vn,\\n    stride_oz, stride_oh, stride_om, stride_on,\\n    Z, H, N_CTX,\\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\\n    BLOCK_N: tl.constexpr,\\n):\\n    start_m = tl.program_id(0)\\n    off_hz = tl.program_id(1)\\n    # initialize offsets\\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\\n    offs_n = tl.arange(0, BLOCK_N)\\n    offs_d = tl.arange(0, BLOCK_DMODEL)\\n    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\\n    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\\n    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\\n    # Initialize pointers to Q, K, V\\n    q_ptrs = Q + off_q\\n    k_ptrs = K + off_k\\n    v_ptrs = V + off_v\\n    # initialize pointer to m and l\\n    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\\\"inf\\\")\\n    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n    # load q: it will stay in SRAM throughout\\n    q = tl.load(q_ptrs)\\n    # loop over k, v and update accumulator\\n    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\\n        # -- compute qk ----\\n        k = tl.load(k_ptrs)\\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\\n        qk += tl.dot(q, k)\\n        qk *= sm_scale\\n        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\\\"-inf\\\"))\\n        # compute new m\\n        m_curr = tl.maximum(tl.max(qk, 1), m_prev)\\n        # correct old l\\n        l_prev *= tl.exp(m_prev - m_curr)\\n        # attention weights\\n        p = tl.exp(qk - m_curr[:, None])\\n        l_curr = tl.sum(p, 1) + l_prev\\n        # rescale operands of matmuls\\n        l_rcp = 1. / l_curr\\n        p *= l_rcp[:, None]\\n        acc *= (l_prev * l_rcp)[:, None]\\n        # update acc\\n        p = p.to(Q.dtype.element_ty)\\n        v = tl.load(v_ptrs)\\n        acc += tl.dot(p, v)\\n        # update m_i and l_i\\n        l_prev = l_curr\\n        m_prev = m_curr\\n        # update pointers\\n        k_ptrs += BLOCK_N * stride_kn\\n        v_ptrs += BLOCK_N * stride_vk\\n    # rematerialize offsets to save registers\\n    start_m = tl.program_id(0)\\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\\n    # write back l and m\\n    l_ptrs = L + off_hz * N_CTX + offs_m\\n    m_ptrs = M + off_hz * N_CTX + offs_m\\n    tl.store(l_ptrs, l_prev)\\n    tl.store(m_ptrs, m_prev)\\n    # initialize pointers to output\\n    offs_n = tl.arange(0, BLOCK_DMODEL)\\n    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\\n    out_ptrs = Out + off_o\\n    tl.store(out_ptrs, acc)\\n\\n\\n@triton.jit\\ndef _bwd_preprocess(\\n    Out, DO, L,\\n    NewDO, Delta,\\n    BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\\n):\\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\\n    off_n = tl.arange(0, D_HEAD)\\n    # load\\n    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\\n    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\\n    denom = tl.load(L + off_m).to(tl.float32)\\n    # compute\\n    do = do / denom[:, None]\\n    delta = tl.sum(o * do, axis=1)\\n    # write-back\\n    tl.store(NewDO + off_m[:, None] * D_HEAD + off_n[None, :], do)\\n    tl.store(Delta + off_m, delta)\\n\\n\\n@triton.jit\\ndef _bwd_kernel(\\n    Q, K, V, sm_scale, Out, DO,\\n    DQ, DK, DV,\\n    L, M,\\n    D,\\n    stride_qz, stride_qh, stride_qm, stride_qk,\\n    stride_kz, stride_kh, stride_kn, stride_kk,\\n    stride_vz, stride_vh, stride_vk, stride_vn,\\n    Z, H, N_CTX,\\n    num_block,\\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\\n    BLOCK_N: tl.constexpr,\\n):\\n    off_hz = tl.program_id(0)\\n    off_z = off_hz // H\\n    off_h = off_hz % H\\n    # offset pointers for batch/head\\n    Q += off_z * stride_qz + off_h * stride_qh\\n    K += off_z * stride_qz + off_h * stride_qh\\n    V += off_z * stride_qz + off_h * stride_qh\\n    DO += off_z * stride_qz + off_h * stride_qh\\n    DQ += off_z * stride_qz + off_h * stride_qh\\n    DK += off_z * stride_qz + off_h * stride_qh\\n    DV += off_z * stride_qz + off_h * stride_qh\\n    for start_n in range(0, num_block):\\n        lo = start_n * BLOCK_M\\n        # initialize row/col offsets\\n        offs_qm = lo + tl.arange(0, BLOCK_M)\\n        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\\n        offs_m = tl.arange(0, BLOCK_N)\\n        offs_k = tl.arange(0, BLOCK_DMODEL)\\n        # initialize pointers to value-like data\\n        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\\n        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        # pointer to row-wise quantities in value-like data\\n        D_ptrs = D + off_hz * N_CTX\\n        m_ptrs = M + off_hz * N_CTX\\n        # initialize dv amd dk\\n        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n        # k and v stay in SRAM throughout\\n        k = tl.load(k_ptrs)\\n        v = tl.load(v_ptrs)\\n        # loop over rows\\n        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\\n            offs_m_curr = start_m + offs_m\\n            # load q, k, v, do on-chip\\n            q = tl.load(q_ptrs)\\n            # recompute p = softmax(qk, dim=-1).T\\n            # NOTE: `do` is pre-divided by `l`; no normalization here\\n            qk = tl.dot(q, tl.trans(k))\\n            qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\\\"-inf\\\"))\\n            m = tl.load(m_ptrs + offs_m_curr)\\n            p = tl.exp(qk * sm_scale - m[:, None])\\n            # compute dv\\n            do = tl.load(do_ptrs)\\n            dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\\n            # compute dp = dot(v, do)\\n            Di = tl.load(D_ptrs + offs_m_curr)\\n            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\\n            dp += tl.dot(do, tl.trans(v))\\n            # compute ds = p * (dp - delta[:, None])\\n            ds = p * dp * sm_scale\\n            # compute dk = dot(ds.T, q)\\n            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)\\n            # compute dq\\n            dq = tl.load(dq_ptrs)\\n            dq += tl.dot(ds.to(Q.dtype.element_ty), k)\\n            tl.store(dq_ptrs, dq)\\n            # increment pointers\\n            dq_ptrs += BLOCK_M * stride_qm\\n            q_ptrs += BLOCK_M * stride_qm\\n            do_ptrs += BLOCK_M * stride_qm\\n        # write-back\\n        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\\n        tl.store(dv_ptrs, dv)\\n        tl.store(dk_ptrs, dk)\\n\\n\\nempty = torch.empty(128, device=\\\"cuda\\\")\\n\\n\\nclass _attention(torch.autograd.Function):\\n\\n    @staticmethod\\n    def forward(ctx, q, k, v, sm_scale):\\n        BLOCK = 128\\n        # shape constraints\\n        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\\n        assert Lq == Lk and Lk == Lv\\n        assert Lk in {16, 32, 64, 128}\\n        o = torch.empty_like(q)\\n        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\\n        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\\n        m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\\n        num_warps = 4 if Lk <= 64 else 8\\n\\n        _fwd_kernel[grid](\\n            q, k, v, sm_scale,\\n            L, m,\\n            o,\\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\\n            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\\n            q.shape[0], q.shape[1], q.shape[2],\\n            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\\n            BLOCK_DMODEL=Lk, num_warps=num_warps,\\n            num_stages=2,\\n        )\\n        # print(h.asm[\\\"ttgir\\\"])\\n\\n        ctx.save_for_backward(q, k, v, o, L, m)\\n        ctx.grid = grid\\n        ctx.sm_scale = sm_scale\\n        ctx.BLOCK_DMODEL = Lk\\n        return o\\n\\n    @staticmethod\\n    def backward(ctx, do):\\n        BLOCK = 128\\n        q, k, v, o, l, m = ctx.saved_tensors\\n        do = do.contiguous()\\n        dq = torch.zeros_like(q, dtype=torch.float32)\\n        dk = torch.empty_like(k)\\n        dv = torch.empty_like(v)\\n        do_scaled = torch.empty_like(do)\\n        delta = torch.empty_like(l)\\n        _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\\n            o, do, l,\\n            do_scaled, delta,\\n            BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\\n        )\\n        _bwd_kernel[(ctx.grid[1],)](\\n            q, k, v, ctx.sm_scale,\\n            o, do_scaled,\\n            dq, dk, dv,\\n            l, m,\\n            delta,\\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\\n            q.shape[0], q.shape[1], q.shape[2],\\n            ctx.grid[0],\\n            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\\n            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\\n            num_stages=1,\\n        )\\n        # print(h.asm[\\\"ttgir\\\"])\\n        return dq, dk, dv, None\\n\\n\\nattention = _attention.apply\\n\\n\\n@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\\ndef test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\\n    torch.manual_seed(20)\\n    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0.1, std=0.2).requires_grad_()\\n    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0.4, std=0.2).requires_grad_()\\n    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0.3, std=0.2).requires_grad_()\\n    sm_scale = 0.2\\n    dout = torch.randn_like(q)\\n    # reference implementation\\n    M = torch.tril(torch.ones((N_CTX, N_CTX), device=\\\"cuda\\\"))\\n    p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\\n    for z in range(Z):\\n        for h in range(H):\\n            p[:, :, M == 0] = float(\\\"-inf\\\")\\n    p = torch.softmax(p.float(), dim=-1).half()\\n    # p = torch.exp(p)\\n    ref_out = torch.matmul(p, v)\\n    ref_out.backward(dout)\\n    ref_dv, v.grad = v.grad.clone(), None\\n    ref_dk, k.grad = k.grad.clone(), None\\n    ref_dq, q.grad = q.grad.clone(), None\\n    # # triton implementation\\n    tri_out = attention(q, k, v, sm_scale)\\n    # print(ref_out)\\n    # print(tri_out)\\n    tri_out.backward(dout)\\n    tri_dv, v.grad = v.grad.clone(), None\\n    tri_dk, k.grad = k.grad.clone(), None\\n    tri_dq, q.grad = q.grad.clone(), None\\n    # compare\\n    assert torch.allclose(ref_out, tri_out, atol=1e-2, rtol=0)\\n    assert torch.allclose(ref_dv, tri_dv, atol=1e-2, rtol=0)\\n    assert torch.allclose(ref_dk, tri_dk, atol=1e-2, rtol=0)\\n    assert torch.allclose(ref_dq, tri_dq, atol=1e-2, rtol=0)\\n\\n\\ntry:\\n    from flash_attn.flash_attn_interface import flash_attn_func\\n    HAS_FLASH = True\\nexcept BaseException:\\n    HAS_FLASH = False\\n\\nBATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\\n# vary seq length for fixed head and batch=4\\nconfigs = [triton.testing.Benchmark(\\n    x_names=['N_CTX'],\\n    x_vals=[2**i for i in range(10, 14)],\\n    line_arg='provider',\\n    line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\\n    line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\\n    styles=[('red', '-'), ('blue', '-')],\\n    ylabel='ms',\\n    plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\\n    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode}\\n) for mode in ['fwd', 'bwd']]\\n\\n\\n@triton.testing.perf_report(configs)\\ndef bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.float16, device=\\\"cuda\\\"):\\n    assert mode in ['fwd', 'bwd']\\n    warmup = 25\\n    rep = 100\\n    if provider == \\\"triton\\\":\\n        q = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        sm_scale = 1.3\\n        fn = lambda: attention(q, k, v, sm_scale)\\n        if mode == 'bwd':\\n            o = fn()\\n            do = torch.randn_like(o)\\n            fn = lambda: o.backward(do, retain_graph=True)\\n        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\\n        return ms\\n    if provider == \\\"flash\\\":\\n        lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\\n        cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\\n        cu_seqlens[1:] = lengths.cumsum(0)\\n        qkv = torch.randn((BATCH * N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\\n        fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=True)\\n        if mode == 'bwd':\\n            o = fn()\\n            do = torch.randn_like(o)\\n            fn = lambda: o.backward(do, retain_graph=True)\\n        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\\n        return ms\\n\\n\\n# only works on post-Ampere GPUs right now\\nbench_flash_attention.run(save_path='.', print_data=True)\"\n+        \"import pytest\\nimport torch\\n\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.jit\\ndef _fwd_kernel(\\n    Q, K, V, sm_scale,\\n    L, M,\\n    Out,\\n    stride_qz, stride_qh, stride_qm, stride_qk,\\n    stride_kz, stride_kh, stride_kn, stride_kk,\\n    stride_vz, stride_vh, stride_vk, stride_vn,\\n    stride_oz, stride_oh, stride_om, stride_on,\\n    Z, H, N_CTX,\\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\\n    BLOCK_N: tl.constexpr,\\n    MODE: tl.constexpr,\\n):\\n    start_m = tl.program_id(0)\\n    off_hz = tl.program_id(1)\\n    qvk_offset = off_hz * stride_qh\\n    Q_block_ptr = tl.make_block_ptr(\\n        base=Q + qvk_offset,\\n        shape=(N_CTX, BLOCK_DMODEL),\\n        strides=(stride_qm, stride_qk),\\n        offsets=(start_m * BLOCK_M, 0),\\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\\n        order=(1, 0)\\n    )\\n    K_block_ptr = tl.make_block_ptr(\\n        base=K + qvk_offset,\\n        shape=(BLOCK_DMODEL, N_CTX),\\n        strides=(stride_kk, stride_kn),\\n        offsets=(0, 0),\\n        block_shape=(BLOCK_DMODEL, BLOCK_N),\\n        order=(0, 1)\\n    )\\n    V_block_ptr = tl.make_block_ptr(\\n        base=V + qvk_offset,\\n        shape=(N_CTX, BLOCK_DMODEL),\\n        strides=(stride_vk, stride_vn),\\n        offsets=(0, 0),\\n        block_shape=(BLOCK_N, BLOCK_DMODEL),\\n        order=(1, 0)\\n    )\\n    O_block_ptr = tl.make_block_ptr(\\n        base=Out + qvk_offset,\\n        shape=(N_CTX, BLOCK_DMODEL),\\n        strides=(stride_om, stride_on),\\n        offsets=(start_m * BLOCK_M, 0),\\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\\n        order=(1, 0)\\n    )\\n    # initialize offsets\\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\\n    offs_n = tl.arange(0, BLOCK_N)\\n    # initialize pointer to m and l\\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\\\"inf\\\")\\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n    # causal check on every loop iteration can be expensive\\n    # and peeling the last iteration of the loop does not work well with ptxas\\n    # so we have a mode to do the causal check in a separate kernel entirely\\n    if MODE == 0:  # entire non-causal attention\\n        lo, hi = 0, N_CTX\\n    if MODE == 1:  # entire causal attention\\n        lo, hi = 0, (start_m + 1) * BLOCK_M\\n    if MODE == 2:  # off band-diagonal\\n        lo, hi = 0, start_m * BLOCK_M\\n    if MODE == 3:  # on band-diagonal\\n        l_ptrs = L + off_hz * N_CTX + offs_m\\n        m_ptrs = M + off_hz * N_CTX + offs_m\\n        m_i = tl.load(m_ptrs)\\n        l_i = tl.load(l_ptrs)\\n        acc += tl.load(O_block_ptr).to(tl.float32)\\n        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\\n    # credits to: Adam P. Goucher (https://github.com/apgoucher):\\n    # scale sm_scale by 1/log_2(e) and use\\n    # 2^x instead of exp in the loop because CSE and LICM\\n    # don't work as expected with `exp` in the loop\\n    qk_scale = sm_scale * 1.44269504\\n    # load q: it will stay in SRAM throughout\\n    q = tl.load(Q_block_ptr)\\n    q = (q * qk_scale).to(tl.float16)\\n    # loop over k, v and update accumulator\\n    for start_n in range(lo, hi, BLOCK_N):\\n        start_n = tl.multiple_of(start_n, BLOCK_N)\\n        # -- compute qk ----\\n        k = tl.load(tl.advance(K_block_ptr, (0, start_n)))\\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\\n        qk += tl.dot(q, k)\\n        if MODE == 1 or MODE == 3:\\n            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\\\"-inf\\\"))\\n        # -- compute m_ij, p, l_ij\\n        m_ij = tl.max(qk, 1)\\n        p = tl.math.exp2(qk - m_ij[:, None])\\n        l_ij = tl.sum(p, 1)\\n        # -- update m_i and l_i\\n        m_i_new = tl.maximum(m_i, m_ij)\\n        alpha = tl.math.exp2(m_i - m_i_new)\\n        beta = tl.math.exp2(m_ij - m_i_new)\\n        l_i *= alpha\\n        l_i_new = l_i + beta * l_ij\\n        # scale p\\n        p_scale = beta / l_i_new\\n        p = p * p_scale[:, None]\\n        # scale acc\\n        acc_scale = l_i / l_i_new\\n        acc = acc * acc_scale[:, None]\\n        # update acc\\n        v = tl.load(tl.advance(V_block_ptr, (start_n, 0)))\\n        p = p.to(tl.float16)\\n        acc += tl.dot(p, v)\\n        # update m_i and l_i\\n        l_i = l_i_new\\n        m_i = m_i_new\\n    # write back l and m\\n    l_ptrs = L + off_hz * N_CTX + offs_m\\n    m_ptrs = M + off_hz * N_CTX + offs_m\\n    tl.store(l_ptrs, l_i)\\n    tl.store(m_ptrs, m_i)\\n    # write back O\\n    tl.store(O_block_ptr, acc.to(tl.float16))\\n\\n\\n@triton.jit\\ndef _bwd_preprocess(\\n    Out, DO, L,\\n    NewDO, Delta,\\n    BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\\n):\\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\\n    off_n = tl.arange(0, D_HEAD)\\n    # load\\n    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\\n    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\\n    denom = tl.load(L + off_m).to(tl.float32)\\n    # compute\\n    do = do / denom[:, None]\\n    delta = tl.sum(o * do, axis=1)\\n    # write-back\\n    tl.store(NewDO + off_m[:, None] * D_HEAD + off_n[None, :], do)\\n    tl.store(Delta + off_m, delta)\\n\\n\\n@triton.jit\\ndef _bwd_kernel(\\n    Q, K, V, sm_scale, Out, DO,\\n    DQ, DK, DV,\\n    L, M,\\n    D,\\n    stride_qz, stride_qh, stride_qm, stride_qk,\\n    stride_kz, stride_kh, stride_kn, stride_kk,\\n    stride_vz, stride_vh, stride_vk, stride_vn,\\n    Z, H, N_CTX,\\n    num_block,\\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\\n    BLOCK_N: tl.constexpr,\\n    MODE: tl.constexpr,\\n):\\n    off_hz = tl.program_id(0)\\n    off_z = off_hz // H\\n    off_h = off_hz % H\\n    qk_scale = sm_scale * 1.44269504\\n    # offset pointers for batch/head\\n    Q += off_z * stride_qz + off_h * stride_qh\\n    K += off_z * stride_qz + off_h * stride_qh\\n    V += off_z * stride_qz + off_h * stride_qh\\n    DO += off_z * stride_qz + off_h * stride_qh\\n    DQ += off_z * stride_qz + off_h * stride_qh\\n    DK += off_z * stride_qz + off_h * stride_qh\\n    DV += off_z * stride_qz + off_h * stride_qh\\n    for start_n in range(0, num_block):\\n        if MODE == 0:\\n            lo = 0\\n        else:\\n            lo = start_n * BLOCK_M\\n        # initialize row/col offsets\\n        offs_qm = lo + tl.arange(0, BLOCK_M)\\n        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\\n        offs_m = tl.arange(0, BLOCK_N)\\n        offs_k = tl.arange(0, BLOCK_DMODEL)\\n        # initialize pointers to value-like data\\n        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\\n        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        # pointer to row-wise quantities in value-like data\\n        D_ptrs = D + off_hz * N_CTX\\n        m_ptrs = M + off_hz * N_CTX\\n        # initialize dv amd dk\\n        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\\n        # k and v stay in SRAM throughout\\n        k = tl.load(k_ptrs)\\n        v = tl.load(v_ptrs)\\n        # loop over rows\\n        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\\n            offs_m_curr = start_m + offs_m\\n            # load q, k, v, do on-chip\\n            q = tl.load(q_ptrs)\\n            # recompute p = softmax(qk, dim=-1).T\\n            # NOTE: `do` is pre-divided by `l`; no normalization here\\n            # if MODE == 1:\\n            if MODE == 1:\\n                qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\\\"-inf\\\"))\\n            else:\\n                qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\\n            qk += tl.dot(q, tl.trans(k))\\n            qk *= qk_scale\\n            m = tl.load(m_ptrs + offs_m_curr)\\n            p = tl.math.exp2(qk - m[:, None])\\n            # compute dv\\n            do = tl.load(do_ptrs)\\n            dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\\n            # compute dp = dot(v, do)\\n            Di = tl.load(D_ptrs + offs_m_curr)\\n            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\\n            dp += tl.dot(do, tl.trans(v))\\n            # compute ds = p * (dp - delta[:, None])\\n            ds = p * dp * sm_scale\\n            # compute dk = dot(ds.T, q)\\n            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)\\n            # compute dq\\n            dq = tl.load(dq_ptrs)\\n            dq += tl.dot(ds.to(Q.dtype.element_ty), k)\\n            tl.store(dq_ptrs, dq)\\n            # increment pointers\\n            dq_ptrs += BLOCK_M * stride_qm\\n            q_ptrs += BLOCK_M * stride_qm\\n            do_ptrs += BLOCK_M * stride_qm\\n        # write-back\\n        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\\n        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\\n        tl.store(dv_ptrs, dv)\\n        tl.store(dk_ptrs, dk)\\n\\n\\nempty = torch.empty(128, device=\\\"cuda\\\")\\n\\n\\nclass _attention(torch.autograd.Function):\\n\\n    @staticmethod\\n    def forward(ctx, q, k, v, causal, sm_scale):\\n        BLOCK = 128\\n        # shape constraints\\n        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\\n        assert Lq == Lk and Lk == Lv\\n        assert Lk in {16, 32, 64, 128}\\n        o = torch.empty_like(q)\\n        grid = (triton.cdiv(q.shape[2], 128), q.shape[0] * q.shape[1], 1)\\n        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\\n        m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\\n\\n        num_warps = 4 if Lk <= 64 else 8\\n        if causal:\\n            modes = [1] if q.shape[2] <= 2048 else [2, 3]\\n        else:\\n            modes = [0]\\n        for mode in modes:\\n            _fwd_kernel[grid](\\n                q, k, v, sm_scale,\\n                L, m,\\n                o,\\n                q.stride(0), q.stride(1), q.stride(2), q.stride(3),\\n                k.stride(0), k.stride(1), k.stride(2), k.stride(3),\\n                v.stride(0), v.stride(1), v.stride(2), v.stride(3),\\n                o.stride(0), o.stride(1), o.stride(2), o.stride(3),\\n                q.shape[0], q.shape[1], q.shape[2],\\n                BLOCK_M=128, BLOCK_N=BLOCK, BLOCK_DMODEL=Lk,\\n                MODE=mode,\\n                num_warps=num_warps,\\n                num_stages=2)\\n\\n        ctx.save_for_backward(q, k, v, o, L, m)\\n        ctx.grid = grid\\n        ctx.sm_scale = sm_scale\\n        ctx.BLOCK_DMODEL = Lk\\n        ctx.causal = causal\\n        return o\\n\\n    @staticmethod\\n    def backward(ctx, do):\\n        BLOCK = 128\\n        q, k, v, o, l, m = ctx.saved_tensors\\n        do = do.contiguous()\\n        dq = torch.zeros_like(q, dtype=torch.float32)\\n        dk = torch.empty_like(k)\\n        dv = torch.empty_like(v)\\n        do_scaled = torch.empty_like(do)\\n        delta = torch.empty_like(l)\\n        if ctx.causal:\\n            mode = 1\\n        else:\\n            mode = 0\\n        _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\\n            o, do, l,\\n            do_scaled, delta,\\n            BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\\n        )\\n        _bwd_kernel[(ctx.grid[1],)](\\n            q, k, v, ctx.sm_scale,\\n            o, do_scaled,\\n            dq, dk, dv,\\n            l, m,\\n            delta,\\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\\n            q.shape[0], q.shape[1], q.shape[2],\\n            ctx.grid[0],\\n            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\\n            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\\n            MODE=mode,\\n            num_stages=1,\\n        )\\n        return dq, dk, dv, None, None\\n\\n\\nattention = _attention.apply\\n\\n\\n@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(6, 9, 1024, 64)])\\n@pytest.mark.parametrize('causal', [False, True])\\ndef test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\\n    torch.manual_seed(20)\\n    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0., std=0.5).requires_grad_()\\n    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0., std=0.5).requires_grad_()\\n    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\").normal_(mean=0., std=0.5).requires_grad_()\\n    sm_scale = 0.5\\n    dout = torch.randn_like(q)\\n    # reference implementation\\n    M = torch.tril(torch.ones((N_CTX, N_CTX), device=\\\"cuda\\\"))\\n    p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\\n    if causal:\\n        for z in range(Z):\\n            for h in range(H):\\n                p[:, :, M == 0] = float(\\\"-inf\\\")\\n    p = torch.softmax(p.float(), dim=-1).half()\\n    # p = torch.exp(p)\\n    ref_out = torch.matmul(p, v)\\n    ref_out.backward(dout)\\n    ref_dv, v.grad = v.grad.clone(), None\\n    ref_dk, k.grad = k.grad.clone(), None\\n    ref_dq, q.grad = q.grad.clone(), None\\n    # triton implementation\\n    tri_out = attention(q, k, v, causal, sm_scale).half()\\n    tri_out.backward(dout)\\n    tri_dv, v.grad = v.grad.clone(), None\\n    tri_dk, k.grad = k.grad.clone(), None\\n    tri_dq, q.grad = q.grad.clone(), None\\n    # compare\\n    assert torch.allclose(ref_out, tri_out, atol=1e-2, rtol=0)\\n    assert torch.allclose(ref_dv, tri_dv, atol=1e-2, rtol=0)\\n    assert torch.allclose(ref_dk, tri_dk, atol=1e-2, rtol=0)\\n    assert torch.allclose(ref_dq, tri_dq, atol=1e-2, rtol=0)\\n\\n\\ntry:\\n    from flash_attn.flash_attn_interface import flash_attn_func\\n    HAS_FLASH = True\\nexcept BaseException:\\n    HAS_FLASH = False\\n\\nBATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\\n# vary seq length for fixed head and batch=4\\nconfigs = [triton.testing.Benchmark(\\n    x_names=['N_CTX'],\\n    x_vals=[2**i for i in range(10, 15)],\\n    line_arg='provider',\\n    line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\\n    line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\\n    styles=[('red', '-'), ('blue', '-')],\\n    ylabel='ms',\\n    plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\\n    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode, 'causal': causal}\\n) for mode in ['fwd', 'bwd'] for causal in [False, True]]\\n\\n\\n@triton.testing.perf_report(configs)\\ndef bench_flash_attention(BATCH, H, N_CTX, D_HEAD, causal, mode, provider, dtype=torch.float16, device=\\\"cuda\\\"):\\n    assert mode in ['fwd', 'bwd']\\n    warmup = 25\\n    rep = 100\\n    if provider == \\\"triton\\\":\\n        q = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\\\"cuda\\\", requires_grad=True)\\n        sm_scale = 1.3\\n        fn = lambda: attention(q, k, v, causal, sm_scale)\\n        if mode == 'bwd':\\n            o = fn()\\n            do = torch.randn_like(o)\\n            fn = lambda: o.backward(do, retain_graph=True)\\n        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\\n    if provider == \\\"flash\\\":\\n        lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\\n        cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\\n        cu_seqlens[1:] = lengths.cumsum(0)\\n        qkv = torch.randn((BATCH * N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\\n        fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=True)\\n        if mode == 'bwd':\\n            o = fn()\\n            do = torch.randn_like(o)\\n            fn = lambda: o.backward(do, retain_graph=True)\\n        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\\n    flops_per_matmul = 2. * BATCH * H * N_CTX * N_CTX * D_HEAD\\n    total_flops = 2 * flops_per_matmul\\n    if causal:\\n        total_flops *= 0.5\\n    if mode == 'bwd':\\n        total_flops *= 2.5  # 2.0(bwd) + 0.5(recompute)\\n    return total_flops / ms * 1e-9\\n\\n\\n# only works on post-Ampere GPUs right now\\nbench_flash_attention.run(save_path='.', print_data=True)\"\n       ]\n     }\n   ],"}, {"filename": "main/_downloads/54a35f6ec55f9746935b9566fb6bb1df/06-fused-attention.py", "status": "modified", "additions": 155, "deletions": 86, "changes": 241, "file_content_changes": "@@ -25,68 +25,113 @@ def _fwd_kernel(\n     Z, H, N_CTX,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n+    MODE: tl.constexpr,\n ):\n     start_m = tl.program_id(0)\n     off_hz = tl.program_id(1)\n+    qvk_offset = off_hz * stride_qh\n+    Q_block_ptr = tl.make_block_ptr(\n+        base=Q + qvk_offset,\n+        shape=(N_CTX, BLOCK_DMODEL),\n+        strides=(stride_qm, stride_qk),\n+        offsets=(start_m * BLOCK_M, 0),\n+        block_shape=(BLOCK_M, BLOCK_DMODEL),\n+        order=(1, 0)\n+    )\n+    K_block_ptr = tl.make_block_ptr(\n+        base=K + qvk_offset,\n+        shape=(BLOCK_DMODEL, N_CTX),\n+        strides=(stride_kk, stride_kn),\n+        offsets=(0, 0),\n+        block_shape=(BLOCK_DMODEL, BLOCK_N),\n+        order=(0, 1)\n+    )\n+    V_block_ptr = tl.make_block_ptr(\n+        base=V + qvk_offset,\n+        shape=(N_CTX, BLOCK_DMODEL),\n+        strides=(stride_vk, stride_vn),\n+        offsets=(0, 0),\n+        block_shape=(BLOCK_N, BLOCK_DMODEL),\n+        order=(1, 0)\n+    )\n+    O_block_ptr = tl.make_block_ptr(\n+        base=Out + qvk_offset,\n+        shape=(N_CTX, BLOCK_DMODEL),\n+        strides=(stride_om, stride_on),\n+        offsets=(start_m * BLOCK_M, 0),\n+        block_shape=(BLOCK_M, BLOCK_DMODEL),\n+        order=(1, 0)\n+    )\n     # initialize offsets\n     offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n     offs_n = tl.arange(0, BLOCK_N)\n-    offs_d = tl.arange(0, BLOCK_DMODEL)\n-    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n-    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    # Initialize pointers to Q, K, V\n-    q_ptrs = Q + off_q\n-    k_ptrs = K + off_k\n-    v_ptrs = V + off_v\n     # initialize pointer to m and l\n-    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n-    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n+    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n     acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+    # causal check on every loop iteration can be expensive\n+    # and peeling the last iteration of the loop does not work well with ptxas\n+    # so we have a mode to do the causal check in a separate kernel entirely\n+    if MODE == 0:  # entire non-causal attention\n+        lo, hi = 0, N_CTX\n+    if MODE == 1:  # entire causal attention\n+        lo, hi = 0, (start_m + 1) * BLOCK_M\n+    if MODE == 2:  # off band-diagonal\n+        lo, hi = 0, start_m * BLOCK_M\n+    if MODE == 3:  # on band-diagonal\n+        l_ptrs = L + off_hz * N_CTX + offs_m\n+        m_ptrs = M + off_hz * N_CTX + offs_m\n+        m_i = tl.load(m_ptrs)\n+        l_i = tl.load(l_ptrs)\n+        acc += tl.load(O_block_ptr).to(tl.float32)\n+        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n+    # credits to: Adam P. Goucher (https://github.com/apgoucher):\n+    # scale sm_scale by 1/log_2(e) and use\n+    # 2^x instead of exp in the loop because CSE and LICM\n+    # don't work as expected with `exp` in the loop\n+    qk_scale = sm_scale * 1.44269504\n     # load q: it will stay in SRAM throughout\n-    q = tl.load(q_ptrs)\n+    q = tl.load(Q_block_ptr)\n+    q = (q * qk_scale).to(tl.float16)\n     # loop over k, v and update accumulator\n-    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n+    for start_n in range(lo, hi, BLOCK_N):\n+        start_n = tl.multiple_of(start_n, BLOCK_N)\n         # -- compute qk ----\n-        k = tl.load(k_ptrs)\n+        k = tl.load(tl.advance(K_block_ptr, (0, start_n)))\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n         qk += tl.dot(q, k)\n-        qk *= sm_scale\n-        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n-        # compute new m\n-        m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n-        # correct old l\n-        l_prev *= tl.exp(m_prev - m_curr)\n-        # attention weights\n-        p = tl.exp(qk - m_curr[:, None])\n-        l_curr = tl.sum(p, 1) + l_prev\n-        # rescale operands of matmuls\n-        l_rcp = 1. / l_curr\n-        p *= l_rcp[:, None]\n-        acc *= (l_prev * l_rcp)[:, None]\n+        if MODE == 1 or MODE == 3:\n+            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n+        # -- compute m_ij, p, l_ij\n+        m_ij = tl.max(qk, 1)\n+        p = tl.math.exp2(qk - m_ij[:, None])\n+        l_ij = tl.sum(p, 1)\n+        # -- update m_i and l_i\n+        m_i_new = tl.maximum(m_i, m_ij)\n+        alpha = tl.math.exp2(m_i - m_i_new)\n+        beta = tl.math.exp2(m_ij - m_i_new)\n+        l_i *= alpha\n+        l_i_new = l_i + beta * l_ij\n+        # scale p\n+        p_scale = beta / l_i_new\n+        p = p * p_scale[:, None]\n+        # scale acc\n+        acc_scale = l_i / l_i_new\n+        acc = acc * acc_scale[:, None]\n         # update acc\n-        p = p.to(Q.dtype.element_ty)\n-        v = tl.load(v_ptrs)\n+        v = tl.load(tl.advance(V_block_ptr, (start_n, 0)))\n+        p = p.to(tl.float16)\n         acc += tl.dot(p, v)\n         # update m_i and l_i\n-        l_prev = l_curr\n-        m_prev = m_curr\n-        # update pointers\n-        k_ptrs += BLOCK_N * stride_kn\n-        v_ptrs += BLOCK_N * stride_vk\n-    # rematerialize offsets to save registers\n-    start_m = tl.program_id(0)\n-    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+        l_i = l_i_new\n+        m_i = m_i_new\n     # write back l and m\n     l_ptrs = L + off_hz * N_CTX + offs_m\n     m_ptrs = M + off_hz * N_CTX + offs_m\n-    tl.store(l_ptrs, l_prev)\n-    tl.store(m_ptrs, m_prev)\n-    # initialize pointers to output\n-    offs_n = tl.arange(0, BLOCK_DMODEL)\n-    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n-    out_ptrs = Out + off_o\n-    tl.store(out_ptrs, acc)\n+    tl.store(l_ptrs, l_i)\n+    tl.store(m_ptrs, m_i)\n+    # write back O\n+    tl.store(O_block_ptr, acc.to(tl.float16))\n \n \n @triton.jit\n@@ -122,10 +167,12 @@ def _bwd_kernel(\n     num_block,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n+    MODE: tl.constexpr,\n ):\n     off_hz = tl.program_id(0)\n     off_z = off_hz // H\n     off_h = off_hz % H\n+    qk_scale = sm_scale * 1.44269504\n     # offset pointers for batch/head\n     Q += off_z * stride_qz + off_h * stride_qh\n     K += off_z * stride_qz + off_h * stride_qh\n@@ -135,7 +182,10 @@ def _bwd_kernel(\n     DK += off_z * stride_qz + off_h * stride_qh\n     DV += off_z * stride_qz + off_h * stride_qh\n     for start_n in range(0, num_block):\n-        lo = start_n * BLOCK_M\n+        if MODE == 0:\n+            lo = 0\n+        else:\n+            lo = start_n * BLOCK_M\n         # initialize row/col offsets\n         offs_qm = lo + tl.arange(0, BLOCK_M)\n         offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n@@ -163,10 +213,15 @@ def _bwd_kernel(\n             q = tl.load(q_ptrs)\n             # recompute p = softmax(qk, dim=-1).T\n             # NOTE: `do` is pre-divided by `l`; no normalization here\n-            qk = tl.dot(q, tl.trans(k))\n-            qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\"-inf\"))\n+            # if MODE == 1:\n+            if MODE == 1:\n+                qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\"-inf\"))\n+            else:\n+                qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n+            qk += tl.dot(q, tl.trans(k))\n+            qk *= qk_scale\n             m = tl.load(m_ptrs + offs_m_curr)\n-            p = tl.exp(qk * sm_scale - m[:, None])\n+            p = tl.math.exp2(qk - m[:, None])\n             # compute dv\n             do = tl.load(do_ptrs)\n             dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\n@@ -199,37 +254,42 @@ def _bwd_kernel(\n class _attention(torch.autograd.Function):\n \n     @staticmethod\n-    def forward(ctx, q, k, v, sm_scale):\n+    def forward(ctx, q, k, v, causal, sm_scale):\n         BLOCK = 128\n         # shape constraints\n         Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n         assert Lq == Lk and Lk == Lv\n         assert Lk in {16, 32, 64, 128}\n         o = torch.empty_like(q)\n-        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n+        grid = (triton.cdiv(q.shape[2], 128), q.shape[0] * q.shape[1], 1)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n-        num_warps = 4 if Lk <= 64 else 8\n \n-        _fwd_kernel[grid](\n-            q, k, v, sm_scale,\n-            L, m,\n-            o,\n-            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n-            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n-            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n-            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n-            q.shape[0], q.shape[1], q.shape[2],\n-            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n-            BLOCK_DMODEL=Lk, num_warps=num_warps,\n-            num_stages=2,\n-        )\n-        # print(h.asm[\"ttgir\"])\n+        num_warps = 4 if Lk <= 64 else 8\n+        if causal:\n+            modes = [1] if q.shape[2] <= 2048 else [2, 3]\n+        else:\n+            modes = [0]\n+        for mode in modes:\n+            _fwd_kernel[grid](\n+                q, k, v, sm_scale,\n+                L, m,\n+                o,\n+                q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+                k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+                v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+                o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n+                q.shape[0], q.shape[1], q.shape[2],\n+                BLOCK_M=128, BLOCK_N=BLOCK, BLOCK_DMODEL=Lk,\n+                MODE=mode,\n+                num_warps=num_warps,\n+                num_stages=2)\n \n         ctx.save_for_backward(q, k, v, o, L, m)\n         ctx.grid = grid\n         ctx.sm_scale = sm_scale\n         ctx.BLOCK_DMODEL = Lk\n+        ctx.causal = causal\n         return o\n \n     @staticmethod\n@@ -242,6 +302,10 @@ def backward(ctx, do):\n         dv = torch.empty_like(v)\n         do_scaled = torch.empty_like(do)\n         delta = torch.empty_like(l)\n+        if ctx.causal:\n+            mode = 1\n+        else:\n+            mode = 0\n         _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n             o, do, l,\n             do_scaled, delta,\n@@ -260,40 +324,40 @@ def backward(ctx, do):\n             ctx.grid[0],\n             BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n             BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n+            MODE=mode,\n             num_stages=1,\n         )\n-        # print(h.asm[\"ttgir\"])\n-        return dq, dk, dv, None\n+        return dq, dk, dv, None, None\n \n \n attention = _attention.apply\n \n \n-@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n-def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n+@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(6, 9, 1024, 64)])\n+@pytest.mark.parametrize('causal', [False, True])\n+def test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\n     torch.manual_seed(20)\n-    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n-    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n-    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n-    sm_scale = 0.2\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    sm_scale = 0.5\n     dout = torch.randn_like(q)\n     # reference implementation\n     M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n     p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n-    for z in range(Z):\n-        for h in range(H):\n-            p[:, :, M == 0] = float(\"-inf\")\n+    if causal:\n+        for z in range(Z):\n+            for h in range(H):\n+                p[:, :, M == 0] = float(\"-inf\")\n     p = torch.softmax(p.float(), dim=-1).half()\n     # p = torch.exp(p)\n     ref_out = torch.matmul(p, v)\n     ref_out.backward(dout)\n     ref_dv, v.grad = v.grad.clone(), None\n     ref_dk, k.grad = k.grad.clone(), None\n     ref_dq, q.grad = q.grad.clone(), None\n-    # # triton implementation\n-    tri_out = attention(q, k, v, sm_scale)\n-    # print(ref_out)\n-    # print(tri_out)\n+    # triton implementation\n+    tri_out = attention(q, k, v, causal, sm_scale).half()\n     tri_out.backward(dout)\n     tri_dv, v.grad = v.grad.clone(), None\n     tri_dk, k.grad = k.grad.clone(), None\n@@ -315,19 +379,19 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n # vary seq length for fixed head and batch=4\n configs = [triton.testing.Benchmark(\n     x_names=['N_CTX'],\n-    x_vals=[2**i for i in range(10, 14)],\n+    x_vals=[2**i for i in range(10, 15)],\n     line_arg='provider',\n     line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n     line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\n     styles=[('red', '-'), ('blue', '-')],\n     ylabel='ms',\n     plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n-    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode}\n-) for mode in ['fwd', 'bwd']]\n+    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode, 'causal': causal}\n+) for mode in ['fwd', 'bwd'] for causal in [False, True]]\n \n \n @triton.testing.perf_report(configs)\n-def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.float16, device=\"cuda\"):\n+def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, causal, mode, provider, dtype=torch.float16, device=\"cuda\"):\n     assert mode in ['fwd', 'bwd']\n     warmup = 25\n     rep = 100\n@@ -336,13 +400,12 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.f\n         k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n         v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n         sm_scale = 1.3\n-        fn = lambda: attention(q, k, v, sm_scale)\n+        fn = lambda: attention(q, k, v, causal, sm_scale)\n         if mode == 'bwd':\n             o = fn()\n             do = torch.randn_like(o)\n             fn = lambda: o.backward(do, retain_graph=True)\n         ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n-        return ms\n     if provider == \"flash\":\n         lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n         cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n@@ -354,7 +417,13 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.f\n             do = torch.randn_like(o)\n             fn = lambda: o.backward(do, retain_graph=True)\n         ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n-        return ms\n+    flops_per_matmul = 2. * BATCH * H * N_CTX * N_CTX * D_HEAD\n+    total_flops = 2 * flops_per_matmul\n+    if causal:\n+        total_flops *= 0.5\n+    if mode == 'bwd':\n+        total_flops *= 2.5  # 2.0(bwd) + 0.5(recompute)\n+    return total_flops / ms * 1e-9\n \n \n # only works on post-Ampere GPUs right now"}, {"filename": "main/_downloads/662999063954282841dc90b8945f85ce/tutorials_jupyter.zip", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_downloads/763344228ae6bc253ed1a6cf586aa30d/tutorials_python.zip", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_01-vector-add_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_01-vector-add_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_02-fused-softmax_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_02-fused-softmax_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_03-matrix-multiplication_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_03-matrix-multiplication_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_05-layer-norm_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_05-layer-norm_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_001.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_002.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_003.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_004.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_images/sphx_glr_06-fused-attention_thumb.png", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "main/_sources/getting-started/tutorials/01-vector-add.rst.txt", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "@@ -236,30 +236,30 @@ We can now run the decorated function above. Pass `print_data=True` to see the p\n \n     vector-add-performance:\n                size       Triton        Torch\n-    0        4096.0     8.000000     8.000000\n-    1        8192.0    15.999999    15.999999\n+    0        4096.0     9.600000     8.000000\n+    1        8192.0    15.999999    19.200000\n     2       16384.0    31.999999    31.999999\n     3       32768.0    63.999998    63.999998\n-    4       65536.0   127.999995   109.714284\n+    4       65536.0   127.999995   127.999995\n     5      131072.0   219.428568   219.428568\n     6      262144.0   384.000001   384.000001\n     7      524288.0   614.400016   614.400016\n     8     1048576.0   819.200021   819.200021\n     9     2097152.0  1023.999964  1023.999964\n-    10    4194304.0  1228.800031  1228.800031\n-    11    8388608.0  1424.695621  1404.342820\n+    10    4194304.0  1260.307736  1228.800031\n+    11    8388608.0  1424.695621  1424.695621\n     12   16777216.0  1560.380965  1560.380965\n-    13   33554432.0  1631.601649  1631.601649\n-    14   67108864.0  1676.827323  1669.706983\n-    15  134217728.0  1691.251649  1685.813499\n+    13   33554432.0  1631.601649  1624.859540\n+    14   67108864.0  1669.706983  1666.169441\n+    15  134217728.0  1684.008546  1680.410210\n \n \n \n \n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  6.769 seconds)\n+   **Total running time of the script:** ( 0 minutes  5.717 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_01-vector-add.py:"}, {"filename": "main/_sources/getting-started/tutorials/02-fused-softmax.rst.txt", "status": "modified", "additions": 11, "deletions": 11, "changes": 22, "file_content_changes": "@@ -286,17 +286,17 @@ We will then compare its performance against (1) :code:`torch.softmax` and (2) t\n \n     softmax-performance:\n               N       Triton  Torch (native)  Torch (jit)\n-    0     256.0   682.666643      682.666643   248.242422\n-    1     384.0   819.200021      819.200021   323.368435\n-    2     512.0   910.222190      910.222190   364.088883\n-    3     640.0   975.238103      930.909084   401.568635\n-    4     768.0  1068.521715      983.040025   416.542360\n+    0     256.0   682.666643      744.727267   264.258068\n+    1     384.0   877.714274      877.714274   332.108094\n+    2     512.0   910.222190      910.222190   381.023256\n+    3     640.0   975.238103      975.238103   409.600010\n+    4     768.0  1068.521715     1023.999964   438.857137\n     ..      ...          ...             ...          ...\n-    93  12160.0  1588.244879     1069.010969   587.794583\n-    94  12288.0  1598.438956     1018.694301   587.766841\n-    95  12416.0  1576.634933     1034.666630   585.142866\n-    96  12544.0  1574.149071     1016.222759   586.853777\n-    97  12672.0  1577.836533     1011.231926   586.836449\n+    93  12160.0  1594.754129     1066.082150   593.170741\n+    94  12288.0  1591.967682     1018.694301   592.192778\n+    95  12416.0  1576.634933     1029.305700   589.483671\n+    96  12544.0  1580.346374     1016.222759   592.047213\n+    97  12672.0  1580.912261     1006.213368   591.976662\n \n     [98 rows x 4 columns]\n \n@@ -313,7 +313,7 @@ In the above plot, we can see that:\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  39.007 seconds)\n+   **Total running time of the script:** ( 0 minutes  37.989 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:"}, {"filename": "main/_sources/getting-started/tutorials/03-matrix-multiplication.rst.txt", "status": "modified", "additions": 30, "deletions": 30, "changes": 60, "file_content_changes": "@@ -450,44 +450,44 @@ but feel free to arrange this script as you wish to benchmark any other matrix s\n     matmul-performance:\n              M      cuBLAS      Triton\n     0    256.0    4.096000    4.096000\n-    1    384.0   11.059200   12.288000\n+    1    384.0   12.288000   12.288000\n     2    512.0   26.214401   23.831273\n-    3    640.0   39.384616   36.571428\n-    4    768.0   63.195428   55.296000\n-    5    896.0   78.051553   78.051553\n-    6   1024.0  104.857603   95.325090\n-    7   1152.0  129.825388  124.415996\n-    8   1280.0  157.538463  157.538463\n-    9   1408.0  147.345291  129.804192\n-    10  1536.0  172.631417  150.593357\n-    11  1664.0  173.056002  173.056002\n-    12  1792.0  167.752595  204.353162\n-    13  1920.0  197.485709  164.571430\n-    14  2048.0  192.841562  188.508043\n-    15  2176.0  191.653792  214.081356\n-    16  2304.0  227.503545  229.691080\n-    17  2432.0  203.583068  200.674737\n-    18  2560.0  224.438347  214.169933\n-    19  2688.0  195.531224  199.647657\n-    20  2816.0  210.696652  211.719459\n-    21  2944.0  223.479969  222.482283\n-    22  3072.0  209.715208  206.653671\n-    23  3200.0  214.046818  213.333323\n-    24  3328.0  205.689424  208.670419\n-    25  3456.0  215.565692  215.565692\n-    26  3584.0  218.772251  207.656790\n-    27  3712.0  210.310194  217.641271\n-    28  3840.0  210.651436  208.664143\n-    29  3968.0  207.877238  219.467517\n-    30  4096.0  222.214781  211.699888\n+    3    640.0   42.666665   39.384616\n+    4    768.0   68.056616   58.982401\n+    5    896.0   78.051553   82.642822\n+    6   1024.0  110.376426   99.864382\n+    7   1152.0  135.726544  129.825388\n+    8   1280.0  163.840004  163.840004\n+    9   1408.0  155.765024  136.294403\n+    10  1536.0  181.484314  157.286398\n+    11  1664.0  183.651271  179.978245\n+    12  1792.0  172.914215  208.137481\n+    13  1920.0  203.294114  168.585369\n+    14  2048.0  199.728763  188.508043\n+    15  2176.0  191.653792  207.460296\n+    16  2304.0  229.691080  229.691080\n+    17  2432.0  202.118452  203.583068\n+    18  2560.0  225.986210  214.169933\n+    19  2688.0  197.567993  198.602388\n+    20  2816.0  210.696652  209.683695\n+    21  2944.0  223.479969  223.479969\n+    22  3072.0  205.902197  205.902197\n+    23  3200.0  214.765101  218.430042\n+    24  3328.0  205.689424  206.278780\n+    25  3456.0  213.850319  217.896133\n+    26  3584.0  221.466479  208.137481\n+    27  3712.0  209.868376  216.228019\n+    28  3840.0  207.101127  208.664143\n+    29  3968.0  208.945088  216.738793\n+    30  4096.0  222.214781  214.405318\n \n \n \n \n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  40.861 seconds)\n+   **Total running time of the script:** ( 0 minutes  40.844 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_03-matrix-multiplication.py:"}, {"filename": "main/_sources/getting-started/tutorials/04-low-memory-dropout.rst.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -242,7 +242,7 @@ References\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  0.643 seconds)\n+   **Total running time of the script:** ( 0 minutes  0.630 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_04-low-memory-dropout.py:"}, {"filename": "main/_sources/getting-started/tutorials/05-layer-norm.rst.txt", "status": "modified", "additions": 32, "deletions": 32, "changes": 64, "file_content_changes": "@@ -430,37 +430,37 @@ Specifically, one can set :code:`'mode': 'backward'` to benchmark the backward p\n  .. code-block:: none\n \n     layer-norm-backward:\n-              N      Triton       Torch\n-    0    1024.0  195.047621  366.805965\n-    1    1536.0  302.163940  428.651163\n-    2    2048.0  396.387102  486.653476\n-    3    2560.0  487.619051  525.128191\n-    4    3072.0  571.534916  534.260858\n-    5    3584.0  641.910440  467.478250\n-    6    4096.0  753.287332  470.354055\n-    7    4608.0  691.200017  474.643779\n-    8    5120.0  744.727250  478.132283\n-    9    5632.0  795.105885  484.473119\n-    10   6144.0  833.084772  489.887055\n-    11   6656.0  872.918050  494.563489\n-    12   7168.0  910.222229  472.615376\n-    13   7680.0  940.408194  475.051564\n-    14   8192.0  978.149241  483.066322\n-    15   8704.0  648.745327  483.555555\n-    16   9216.0  678.478510  486.118680\n-    17   9728.0  699.017941  490.487409\n-    18  10240.0  716.501474  491.519977\n-    19  10752.0  743.654192  482.332711\n-    20  11264.0  770.188045  484.473119\n-    21  11776.0  789.452536  488.968877\n-    22  12288.0  810.197787  494.818794\n-    23  12800.0  814.854134  498.701318\n-    24  13312.0  834.172311  499.200013\n-    25  13824.0  829.440021  498.911279\n-    26  14336.0  837.138714  490.119665\n-    27  14848.0  842.439735  493.562309\n-    28  15360.0  857.302354  498.836278\n-    29  15872.0  856.017954  500.562435\n+              N       Triton       Torch\n+    0    1024.0   189.046153  378.092307\n+    1    1536.0   285.767458  438.857146\n+    2    2048.0   381.023277  501.551037\n+    3    2560.0   472.615383  534.260858\n+    4    3072.0   571.534916  538.160602\n+    5    3584.0   641.910440  470.032796\n+    6    4096.0   707.223041  472.615390\n+    7    4608.0   722.823525  478.753251\n+    8    5120.0   772.830175  483.779502\n+    9    5632.0   824.195135  489.739120\n+    10   6144.0   862.315754  491.519977\n+    11   6656.0   907.636357  500.764869\n+    12   7168.0   945.230752  476.542919\n+    13   7680.0   975.238103  479.999983\n+    14   8192.0  1013.443336  489.074621\n+    15   8704.0   678.233793  490.366182\n+    16   9216.0   706.658154  495.928261\n+    17   9728.0   727.327104  496.748937\n+    18  10240.0   746.990876  496.484863\n+    19  10752.0   772.598776  487.803392\n+    20  11264.0   799.810621  487.971095\n+    21  11776.0   821.581395  491.519989\n+    22  12288.0   842.605744  499.850839\n+    23  12800.0   846.280994  501.141916\n+    24  13312.0   870.539508  499.981239\n+    25  13824.0   866.255861  502.690894\n+    26  14336.0   873.258878  492.928354\n+    27  14848.0   873.411781  496.311981\n+    28  15360.0   888.289183  500.869554\n+    29  15872.0   890.018693  501.221037\n \n \n \n@@ -475,7 +475,7 @@ References\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  29.168 seconds)\n+   **Total running time of the script:** ( 0 minutes  29.195 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_05-layer-norm.py:"}, {"filename": "main/_sources/getting-started/tutorials/06-fused-attention.rst.txt", "status": "modified", "additions": 197, "deletions": 98, "changes": 295, "file_content_changes": "@@ -24,7 +24,7 @@ Fused Attention\n This is a Triton implementation of the Flash Attention algorithm\n (see: Dao et al., https://arxiv.org/pdf/2205.14135v2.pdf; Rabe and Staats https://arxiv.org/pdf/2112.05682v2.pdf)\n \n-.. GENERATED FROM PYTHON SOURCE LINES 8-362\n+.. GENERATED FROM PYTHON SOURCE LINES 8-431\n \n \n \n@@ -45,23 +45,53 @@ This is a Triton implementation of the Flash Attention algorithm\n          :srcset: /getting-started/tutorials/images/sphx_glr_06-fused-attention_002.png\n          :class: sphx-glr-multi-img\n \n+    *\n+\n+      .. image-sg:: /getting-started/tutorials/images/sphx_glr_06-fused-attention_003.png\n+         :alt: 06 fused attention\n+         :srcset: /getting-started/tutorials/images/sphx_glr_06-fused-attention_003.png\n+         :class: sphx-glr-multi-img\n+\n+    *\n+\n+      .. image-sg:: /getting-started/tutorials/images/sphx_glr_06-fused-attention_004.png\n+         :alt: 06 fused attention\n+         :srcset: /getting-started/tutorials/images/sphx_glr_06-fused-attention_004.png\n+         :class: sphx-glr-multi-img\n+\n \n .. rst-class:: sphx-glr-script-out\n \n  .. code-block:: none\n \n     fused-attention-batch4-head48-d64-fwd:\n-        N_CTX     Triton\n-    0  1024.0   0.303429\n-    1  2048.0   1.003967\n-    2  4096.0   3.674860\n-    3  8192.0  14.002908\n+         N_CTX      Triton\n+    0   1024.0  126.157204\n+    1   2048.0  134.355754\n+    2   4096.0  140.594263\n+    3   8192.0  142.562069\n+    4  16384.0  143.578766\n+    fused-attention-batch4-head48-d64-fwd:\n+         N_CTX      Triton\n+    0   1024.0   90.947476\n+    1   2048.0  108.466002\n+    2   4096.0  124.015402\n+    3   8192.0  133.523689\n+    4  16384.0  140.291167\n+    fused-attention-batch4-head48-d64-bwd:\n+         N_CTX     Triton\n+    0   1024.0  77.483639\n+    1   2048.0  87.607698\n+    2   4096.0  92.718487\n+    3   8192.0  96.406932\n+    4  16384.0  97.984645\n     fused-attention-batch4-head48-d64-bwd:\n-        N_CTX     Triton\n-    0  1024.0   1.245852\n-    1  2048.0   3.964074\n-    2  4096.0  13.915574\n-    3  8192.0  51.827713\n+         N_CTX     Triton\n+    0   1024.0  54.121347\n+    1   2048.0  67.973852\n+    2   4096.0  76.892519\n+    3   8192.0  82.593830\n+    4  16384.0  85.563482\n \n \n \n@@ -92,68 +122,113 @@ This is a Triton implementation of the Flash Attention algorithm\n         Z, H, N_CTX,\n         BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n         BLOCK_N: tl.constexpr,\n+        MODE: tl.constexpr,\n     ):\n         start_m = tl.program_id(0)\n         off_hz = tl.program_id(1)\n+        qvk_offset = off_hz * stride_qh\n+        Q_block_ptr = tl.make_block_ptr(\n+            base=Q + qvk_offset,\n+            shape=(N_CTX, BLOCK_DMODEL),\n+            strides=(stride_qm, stride_qk),\n+            offsets=(start_m * BLOCK_M, 0),\n+            block_shape=(BLOCK_M, BLOCK_DMODEL),\n+            order=(1, 0)\n+        )\n+        K_block_ptr = tl.make_block_ptr(\n+            base=K + qvk_offset,\n+            shape=(BLOCK_DMODEL, N_CTX),\n+            strides=(stride_kk, stride_kn),\n+            offsets=(0, 0),\n+            block_shape=(BLOCK_DMODEL, BLOCK_N),\n+            order=(0, 1)\n+        )\n+        V_block_ptr = tl.make_block_ptr(\n+            base=V + qvk_offset,\n+            shape=(N_CTX, BLOCK_DMODEL),\n+            strides=(stride_vk, stride_vn),\n+            offsets=(0, 0),\n+            block_shape=(BLOCK_N, BLOCK_DMODEL),\n+            order=(1, 0)\n+        )\n+        O_block_ptr = tl.make_block_ptr(\n+            base=Out + qvk_offset,\n+            shape=(N_CTX, BLOCK_DMODEL),\n+            strides=(stride_om, stride_on),\n+            offsets=(start_m * BLOCK_M, 0),\n+            block_shape=(BLOCK_M, BLOCK_DMODEL),\n+            order=(1, 0)\n+        )\n         # initialize offsets\n         offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n         offs_n = tl.arange(0, BLOCK_N)\n-        offs_d = tl.arange(0, BLOCK_DMODEL)\n-        off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-        off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n-        off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-        # Initialize pointers to Q, K, V\n-        q_ptrs = Q + off_q\n-        k_ptrs = K + off_k\n-        v_ptrs = V + off_v\n         # initialize pointer to m and l\n-        m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n-        l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n+        m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+        l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n         acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+        # causal check on every loop iteration can be expensive\n+        # and peeling the last iteration of the loop does not work well with ptxas\n+        # so we have a mode to do the causal check in a separate kernel entirely\n+        if MODE == 0:  # entire non-causal attention\n+            lo, hi = 0, N_CTX\n+        if MODE == 1:  # entire causal attention\n+            lo, hi = 0, (start_m + 1) * BLOCK_M\n+        if MODE == 2:  # off band-diagonal\n+            lo, hi = 0, start_m * BLOCK_M\n+        if MODE == 3:  # on band-diagonal\n+            l_ptrs = L + off_hz * N_CTX + offs_m\n+            m_ptrs = M + off_hz * N_CTX + offs_m\n+            m_i = tl.load(m_ptrs)\n+            l_i = tl.load(l_ptrs)\n+            acc += tl.load(O_block_ptr).to(tl.float32)\n+            lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n+        # credits to: Adam P. Goucher (https://github.com/apgoucher):\n+        # scale sm_scale by 1/log_2(e) and use\n+        # 2^x instead of exp in the loop because CSE and LICM\n+        # don't work as expected with `exp` in the loop\n+        qk_scale = sm_scale * 1.44269504\n         # load q: it will stay in SRAM throughout\n-        q = tl.load(q_ptrs)\n+        q = tl.load(Q_block_ptr)\n+        q = (q * qk_scale).to(tl.float16)\n         # loop over k, v and update accumulator\n-        for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n+        for start_n in range(lo, hi, BLOCK_N):\n+            start_n = tl.multiple_of(start_n, BLOCK_N)\n             # -- compute qk ----\n-            k = tl.load(k_ptrs)\n+            k = tl.load(tl.advance(K_block_ptr, (0, start_n)))\n             qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n             qk += tl.dot(q, k)\n-            qk *= sm_scale\n-            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n-            # compute new m\n-            m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n-            # correct old l\n-            l_prev *= tl.exp(m_prev - m_curr)\n-            # attention weights\n-            p = tl.exp(qk - m_curr[:, None])\n-            l_curr = tl.sum(p, 1) + l_prev\n-            # rescale operands of matmuls\n-            l_rcp = 1. / l_curr\n-            p *= l_rcp[:, None]\n-            acc *= (l_prev * l_rcp)[:, None]\n+            if MODE == 1 or MODE == 3:\n+                qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n+            # -- compute m_ij, p, l_ij\n+            m_ij = tl.max(qk, 1)\n+            p = tl.math.exp2(qk - m_ij[:, None])\n+            l_ij = tl.sum(p, 1)\n+            # -- update m_i and l_i\n+            m_i_new = tl.maximum(m_i, m_ij)\n+            alpha = tl.math.exp2(m_i - m_i_new)\n+            beta = tl.math.exp2(m_ij - m_i_new)\n+            l_i *= alpha\n+            l_i_new = l_i + beta * l_ij\n+            # scale p\n+            p_scale = beta / l_i_new\n+            p = p * p_scale[:, None]\n+            # scale acc\n+            acc_scale = l_i / l_i_new\n+            acc = acc * acc_scale[:, None]\n             # update acc\n-            p = p.to(Q.dtype.element_ty)\n-            v = tl.load(v_ptrs)\n+            v = tl.load(tl.advance(V_block_ptr, (start_n, 0)))\n+            p = p.to(tl.float16)\n             acc += tl.dot(p, v)\n             # update m_i and l_i\n-            l_prev = l_curr\n-            m_prev = m_curr\n-            # update pointers\n-            k_ptrs += BLOCK_N * stride_kn\n-            v_ptrs += BLOCK_N * stride_vk\n-        # rematerialize offsets to save registers\n-        start_m = tl.program_id(0)\n-        offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+            l_i = l_i_new\n+            m_i = m_i_new\n         # write back l and m\n         l_ptrs = L + off_hz * N_CTX + offs_m\n         m_ptrs = M + off_hz * N_CTX + offs_m\n-        tl.store(l_ptrs, l_prev)\n-        tl.store(m_ptrs, m_prev)\n-        # initialize pointers to output\n-        offs_n = tl.arange(0, BLOCK_DMODEL)\n-        off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n-        out_ptrs = Out + off_o\n-        tl.store(out_ptrs, acc)\n+        tl.store(l_ptrs, l_i)\n+        tl.store(m_ptrs, m_i)\n+        # write back O\n+        tl.store(O_block_ptr, acc.to(tl.float16))\n \n \n     @triton.jit\n@@ -189,10 +264,12 @@ This is a Triton implementation of the Flash Attention algorithm\n         num_block,\n         BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n         BLOCK_N: tl.constexpr,\n+        MODE: tl.constexpr,\n     ):\n         off_hz = tl.program_id(0)\n         off_z = off_hz // H\n         off_h = off_hz % H\n+        qk_scale = sm_scale * 1.44269504\n         # offset pointers for batch/head\n         Q += off_z * stride_qz + off_h * stride_qh\n         K += off_z * stride_qz + off_h * stride_qh\n@@ -202,7 +279,10 @@ This is a Triton implementation of the Flash Attention algorithm\n         DK += off_z * stride_qz + off_h * stride_qh\n         DV += off_z * stride_qz + off_h * stride_qh\n         for start_n in range(0, num_block):\n-            lo = start_n * BLOCK_M\n+            if MODE == 0:\n+                lo = 0\n+            else:\n+                lo = start_n * BLOCK_M\n             # initialize row/col offsets\n             offs_qm = lo + tl.arange(0, BLOCK_M)\n             offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n@@ -230,10 +310,15 @@ This is a Triton implementation of the Flash Attention algorithm\n                 q = tl.load(q_ptrs)\n                 # recompute p = softmax(qk, dim=-1).T\n                 # NOTE: `do` is pre-divided by `l`; no normalization here\n-                qk = tl.dot(q, tl.trans(k))\n-                qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\"-inf\"))\n+                # if MODE == 1:\n+                if MODE == 1:\n+                    qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\"-inf\"))\n+                else:\n+                    qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n+                qk += tl.dot(q, tl.trans(k))\n+                qk *= qk_scale\n                 m = tl.load(m_ptrs + offs_m_curr)\n-                p = tl.exp(qk * sm_scale - m[:, None])\n+                p = tl.math.exp2(qk - m[:, None])\n                 # compute dv\n                 do = tl.load(do_ptrs)\n                 dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\n@@ -266,37 +351,42 @@ This is a Triton implementation of the Flash Attention algorithm\n     class _attention(torch.autograd.Function):\n \n         @staticmethod\n-        def forward(ctx, q, k, v, sm_scale):\n+        def forward(ctx, q, k, v, causal, sm_scale):\n             BLOCK = 128\n             # shape constraints\n             Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n             assert Lq == Lk and Lk == Lv\n             assert Lk in {16, 32, 64, 128}\n             o = torch.empty_like(q)\n-            grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n+            grid = (triton.cdiv(q.shape[2], 128), q.shape[0] * q.shape[1], 1)\n             L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n             m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n-            num_warps = 4 if Lk <= 64 else 8\n \n-            _fwd_kernel[grid](\n-                q, k, v, sm_scale,\n-                L, m,\n-                o,\n-                q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n-                k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n-                v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n-                o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n-                q.shape[0], q.shape[1], q.shape[2],\n-                BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n-                BLOCK_DMODEL=Lk, num_warps=num_warps,\n-                num_stages=2,\n-            )\n-            # print(h.asm[\"ttgir\"])\n+            num_warps = 4 if Lk <= 64 else 8\n+            if causal:\n+                modes = [1] if q.shape[2] <= 2048 else [2, 3]\n+            else:\n+                modes = [0]\n+            for mode in modes:\n+                _fwd_kernel[grid](\n+                    q, k, v, sm_scale,\n+                    L, m,\n+                    o,\n+                    q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+                    k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+                    v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+                    o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n+                    q.shape[0], q.shape[1], q.shape[2],\n+                    BLOCK_M=128, BLOCK_N=BLOCK, BLOCK_DMODEL=Lk,\n+                    MODE=mode,\n+                    num_warps=num_warps,\n+                    num_stages=2)\n \n             ctx.save_for_backward(q, k, v, o, L, m)\n             ctx.grid = grid\n             ctx.sm_scale = sm_scale\n             ctx.BLOCK_DMODEL = Lk\n+            ctx.causal = causal\n             return o\n \n         @staticmethod\n@@ -309,6 +399,10 @@ This is a Triton implementation of the Flash Attention algorithm\n             dv = torch.empty_like(v)\n             do_scaled = torch.empty_like(do)\n             delta = torch.empty_like(l)\n+            if ctx.causal:\n+                mode = 1\n+            else:\n+                mode = 0\n             _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n                 o, do, l,\n                 do_scaled, delta,\n@@ -327,40 +421,40 @@ This is a Triton implementation of the Flash Attention algorithm\n                 ctx.grid[0],\n                 BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n                 BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n+                MODE=mode,\n                 num_stages=1,\n             )\n-            # print(h.asm[\"ttgir\"])\n-            return dq, dk, dv, None\n+            return dq, dk, dv, None, None\n \n \n     attention = _attention.apply\n \n \n-    @pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n-    def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n+    @pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(6, 9, 1024, 64)])\n+    @pytest.mark.parametrize('causal', [False, True])\n+    def test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\n         torch.manual_seed(20)\n-        q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n-        k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n-        v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n-        sm_scale = 0.2\n+        q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+        k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+        v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+        sm_scale = 0.5\n         dout = torch.randn_like(q)\n         # reference implementation\n         M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n         p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n-        for z in range(Z):\n-            for h in range(H):\n-                p[:, :, M == 0] = float(\"-inf\")\n+        if causal:\n+            for z in range(Z):\n+                for h in range(H):\n+                    p[:, :, M == 0] = float(\"-inf\")\n         p = torch.softmax(p.float(), dim=-1).half()\n         # p = torch.exp(p)\n         ref_out = torch.matmul(p, v)\n         ref_out.backward(dout)\n         ref_dv, v.grad = v.grad.clone(), None\n         ref_dk, k.grad = k.grad.clone(), None\n         ref_dq, q.grad = q.grad.clone(), None\n-        # # triton implementation\n-        tri_out = attention(q, k, v, sm_scale)\n-        # print(ref_out)\n-        # print(tri_out)\n+        # triton implementation\n+        tri_out = attention(q, k, v, causal, sm_scale).half()\n         tri_out.backward(dout)\n         tri_dv, v.grad = v.grad.clone(), None\n         tri_dk, k.grad = k.grad.clone(), None\n@@ -382,19 +476,19 @@ This is a Triton implementation of the Flash Attention algorithm\n     # vary seq length for fixed head and batch=4\n     configs = [triton.testing.Benchmark(\n         x_names=['N_CTX'],\n-        x_vals=[2**i for i in range(10, 14)],\n+        x_vals=[2**i for i in range(10, 15)],\n         line_arg='provider',\n         line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n         line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\n         styles=[('red', '-'), ('blue', '-')],\n         ylabel='ms',\n         plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n-        args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode}\n-    ) for mode in ['fwd', 'bwd']]\n+        args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode, 'causal': causal}\n+    ) for mode in ['fwd', 'bwd'] for causal in [False, True]]\n \n \n     @triton.testing.perf_report(configs)\n-    def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.float16, device=\"cuda\"):\n+    def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, causal, mode, provider, dtype=torch.float16, device=\"cuda\"):\n         assert mode in ['fwd', 'bwd']\n         warmup = 25\n         rep = 100\n@@ -403,13 +497,12 @@ This is a Triton implementation of the Flash Attention algorithm\n             k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n             v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n             sm_scale = 1.3\n-            fn = lambda: attention(q, k, v, sm_scale)\n+            fn = lambda: attention(q, k, v, causal, sm_scale)\n             if mode == 'bwd':\n                 o = fn()\n                 do = torch.randn_like(o)\n                 fn = lambda: o.backward(do, retain_graph=True)\n             ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n-            return ms\n         if provider == \"flash\":\n             lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n             cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n@@ -421,7 +514,13 @@ This is a Triton implementation of the Flash Attention algorithm\n                 do = torch.randn_like(o)\n                 fn = lambda: o.backward(do, retain_graph=True)\n             ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n-            return ms\n+        flops_per_matmul = 2. * BATCH * H * N_CTX * N_CTX * D_HEAD\n+        total_flops = 2 * flops_per_matmul\n+        if causal:\n+            total_flops *= 0.5\n+        if mode == 'bwd':\n+            total_flops *= 2.5  # 2.0(bwd) + 0.5(recompute)\n+        return total_flops / ms * 1e-9\n \n \n     # only works on post-Ampere GPUs right now\n@@ -430,7 +529,7 @@ This is a Triton implementation of the Flash Attention algorithm\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  4.671 seconds)\n+   **Total running time of the script:** ( 0 minutes  16.492 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_06-fused-attention.py:"}, {"filename": "main/_sources/getting-started/tutorials/07-math-functions.rst.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -147,7 +147,7 @@ We can also customize the libdevice library path by passing the path to the `lib\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  0.217 seconds)\n+   **Total running time of the script:** ( 0 minutes  0.219 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_07-math-functions.py:"}, {"filename": "main/_sources/getting-started/tutorials/08-experimental-block-pointer.rst.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -298,7 +298,7 @@ Still we can test our matrix multiplication with block pointers against a native\n \n .. rst-class:: sphx-glr-timing\n \n-   **Total running time of the script:** ( 0 minutes  6.374 seconds)\n+   **Total running time of the script:** ( 0 minutes  6.438 seconds)\n \n \n .. _sphx_glr_download_getting-started_tutorials_08-experimental-block-pointer.py:"}, {"filename": "main/_sources/getting-started/tutorials/sg_execution_times.rst.txt", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "@@ -6,22 +6,22 @@\n \n Computation times\n =================\n-**02:07.710** total execution time for **getting-started_tutorials** files:\n+**02:17.525** total execution time for **getting-started_tutorials** files:\n \n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``)           | 00:40.861 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_03-matrix-multiplication.py` (``03-matrix-multiplication.py``)           | 00:40.844 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                           | 00:39.007 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_02-fused-softmax.py` (``02-fused-softmax.py``)                           | 00:37.989 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                                 | 00:29.168 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_05-layer-norm.py` (``05-layer-norm.py``)                                 | 00:29.195 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                                 | 00:06.769 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py` (``06-fused-attention.py``)                       | 00:16.492 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_08-experimental-block-pointer.py` (``08-experimental-block-pointer.py``) | 00:06.374 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_08-experimental-block-pointer.py` (``08-experimental-block-pointer.py``) | 00:06.438 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_06-fused-attention.py` (``06-fused-attention.py``)                       | 00:04.671 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_01-vector-add.py` (``01-vector-add.py``)                                 | 00:05.717 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)                 | 00:00.643 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_04-low-memory-dropout.py` (``04-low-memory-dropout.py``)                 | 00:00.630 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+\n-| :ref:`sphx_glr_getting-started_tutorials_07-math-functions.py` (``07-math-functions.py``)                         | 00:00.217 | 0.0 MB |\n+| :ref:`sphx_glr_getting-started_tutorials_07-math-functions.py` (``07-math-functions.py``)                         | 00:00.219 | 0.0 MB |\n +-------------------------------------------------------------------------------------------------------------------+-----------+--------+"}, {"filename": "main/getting-started/tutorials/01-vector-add.html", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "@@ -235,25 +235,25 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n </div>\n <img src=\"../../_images/sphx_glr_01-vector-add_001.png\" srcset=\"../../_images/sphx_glr_01-vector-add_001.png\" alt=\"01 vector add\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>vector-add-performance:\n            size       Triton        Torch\n-0        4096.0     8.000000     8.000000\n-1        8192.0    15.999999    15.999999\n+0        4096.0     9.600000     8.000000\n+1        8192.0    15.999999    19.200000\n 2       16384.0    31.999999    31.999999\n 3       32768.0    63.999998    63.999998\n-4       65536.0   127.999995   109.714284\n+4       65536.0   127.999995   127.999995\n 5      131072.0   219.428568   219.428568\n 6      262144.0   384.000001   384.000001\n 7      524288.0   614.400016   614.400016\n 8     1048576.0   819.200021   819.200021\n 9     2097152.0  1023.999964  1023.999964\n-10    4194304.0  1228.800031  1228.800031\n-11    8388608.0  1424.695621  1404.342820\n+10    4194304.0  1260.307736  1228.800031\n+11    8388608.0  1424.695621  1424.695621\n 12   16777216.0  1560.380965  1560.380965\n-13   33554432.0  1631.601649  1631.601649\n-14   67108864.0  1676.827323  1669.706983\n-15  134217728.0  1691.251649  1685.813499\n+13   33554432.0  1631.601649  1624.859540\n+14   67108864.0  1669.706983  1666.169441\n+15  134217728.0  1684.008546  1680.410210\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  6.769 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  5.717 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-01-vector-add-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/62d97d49a32414049819dd8bb8378080/01-vector-add.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">01-vector-add.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/02-fused-softmax.html", "status": "modified", "additions": 11, "deletions": 11, "changes": 22, "file_content_changes": "@@ -282,17 +282,17 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n </div>\n <img src=\"../../_images/sphx_glr_02-fused-softmax_001.png\" srcset=\"../../_images/sphx_glr_02-fused-softmax_001.png\" alt=\"02 fused softmax\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>softmax-performance:\n           N       Triton  Torch (native)  Torch (jit)\n-0     256.0   682.666643      682.666643   248.242422\n-1     384.0   819.200021      819.200021   323.368435\n-2     512.0   910.222190      910.222190   364.088883\n-3     640.0   975.238103      930.909084   401.568635\n-4     768.0  1068.521715      983.040025   416.542360\n+0     256.0   682.666643      744.727267   264.258068\n+1     384.0   877.714274      877.714274   332.108094\n+2     512.0   910.222190      910.222190   381.023256\n+3     640.0   975.238103      975.238103   409.600010\n+4     768.0  1068.521715     1023.999964   438.857137\n ..      ...          ...             ...          ...\n-93  12160.0  1588.244879     1069.010969   587.794583\n-94  12288.0  1598.438956     1018.694301   587.766841\n-95  12416.0  1576.634933     1034.666630   585.142866\n-96  12544.0  1574.149071     1016.222759   586.853777\n-97  12672.0  1577.836533     1011.231926   586.836449\n+93  12160.0  1594.754129     1066.082150   593.170741\n+94  12288.0  1591.967682     1018.694301   592.192778\n+95  12416.0  1576.634933     1029.305700   589.483671\n+96  12544.0  1580.346374     1016.222759   592.047213\n+97  12672.0  1580.912261     1006.213368   591.976662\n \n [98 rows x 4 columns]\n </pre></div>\n@@ -305,7 +305,7 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n </ul>\n </dd>\n </dl>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  39.007 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  37.989 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-02-fused-softmax-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d91442ac2982c4e0cc3ab0f43534afbc/02-fused-softmax.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">02-fused-softmax.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/03-matrix-multiplication.html", "status": "modified", "additions": 30, "deletions": 30, "changes": 60, "file_content_changes": "@@ -464,39 +464,39 @@ <h3>Square Matrix Performance<a class=\"headerlink\" href=\"#square-matrix-performa\n <img src=\"../../_images/sphx_glr_03-matrix-multiplication_001.png\" srcset=\"../../_images/sphx_glr_03-matrix-multiplication_001.png\" alt=\"03 matrix multiplication\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>matmul-performance:\n          M      cuBLAS      Triton\n 0    256.0    4.096000    4.096000\n-1    384.0   11.059200   12.288000\n+1    384.0   12.288000   12.288000\n 2    512.0   26.214401   23.831273\n-3    640.0   39.384616   36.571428\n-4    768.0   63.195428   55.296000\n-5    896.0   78.051553   78.051553\n-6   1024.0  104.857603   95.325090\n-7   1152.0  129.825388  124.415996\n-8   1280.0  157.538463  157.538463\n-9   1408.0  147.345291  129.804192\n-10  1536.0  172.631417  150.593357\n-11  1664.0  173.056002  173.056002\n-12  1792.0  167.752595  204.353162\n-13  1920.0  197.485709  164.571430\n-14  2048.0  192.841562  188.508043\n-15  2176.0  191.653792  214.081356\n-16  2304.0  227.503545  229.691080\n-17  2432.0  203.583068  200.674737\n-18  2560.0  224.438347  214.169933\n-19  2688.0  195.531224  199.647657\n-20  2816.0  210.696652  211.719459\n-21  2944.0  223.479969  222.482283\n-22  3072.0  209.715208  206.653671\n-23  3200.0  214.046818  213.333323\n-24  3328.0  205.689424  208.670419\n-25  3456.0  215.565692  215.565692\n-26  3584.0  218.772251  207.656790\n-27  3712.0  210.310194  217.641271\n-28  3840.0  210.651436  208.664143\n-29  3968.0  207.877238  219.467517\n-30  4096.0  222.214781  211.699888\n+3    640.0   42.666665   39.384616\n+4    768.0   68.056616   58.982401\n+5    896.0   78.051553   82.642822\n+6   1024.0  110.376426   99.864382\n+7   1152.0  135.726544  129.825388\n+8   1280.0  163.840004  163.840004\n+9   1408.0  155.765024  136.294403\n+10  1536.0  181.484314  157.286398\n+11  1664.0  183.651271  179.978245\n+12  1792.0  172.914215  208.137481\n+13  1920.0  203.294114  168.585369\n+14  2048.0  199.728763  188.508043\n+15  2176.0  191.653792  207.460296\n+16  2304.0  229.691080  229.691080\n+17  2432.0  202.118452  203.583068\n+18  2560.0  225.986210  214.169933\n+19  2688.0  197.567993  198.602388\n+20  2816.0  210.696652  209.683695\n+21  2944.0  223.479969  223.479969\n+22  3072.0  205.902197  205.902197\n+23  3200.0  214.765101  218.430042\n+24  3328.0  205.689424  206.278780\n+25  3456.0  213.850319  217.896133\n+26  3584.0  221.466479  208.137481\n+27  3712.0  209.868376  216.228019\n+28  3840.0  207.101127  208.664143\n+29  3968.0  208.945088  216.738793\n+30  4096.0  222.214781  214.405318\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  40.861 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  40.844 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-03-matrix-multiplication-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/d5fee5b55a64e47f1b5724ec39adf171/03-matrix-multiplication.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">03-matrix-multiplication.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/04-low-memory-dropout.html", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -287,7 +287,7 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n <p>Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \u201cDropout: A Simple Way to Prevent Neural Networks from Overfitting\u201d, JMLR 2014</p>\n </div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.643 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.630 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-04-low-memory-dropout-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/c9aed78977a4c05741d675a38dde3d7d/04-low-memory-dropout.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">04-low-memory-dropout.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/05-layer-norm.html", "status": "modified", "additions": 32, "deletions": 32, "changes": 64, "file_content_changes": "@@ -459,37 +459,37 @@ <h2>Benchmark<a class=\"headerlink\" href=\"#benchmark\" title=\"Permalink to this he\n </pre></div>\n </div>\n <img src=\"../../_images/sphx_glr_05-layer-norm_001.png\" srcset=\"../../_images/sphx_glr_05-layer-norm_001.png\" alt=\"05 layer norm\" class = \"sphx-glr-single-img\"/><div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>layer-norm-backward:\n-          N      Triton       Torch\n-0    1024.0  195.047621  366.805965\n-1    1536.0  302.163940  428.651163\n-2    2048.0  396.387102  486.653476\n-3    2560.0  487.619051  525.128191\n-4    3072.0  571.534916  534.260858\n-5    3584.0  641.910440  467.478250\n-6    4096.0  753.287332  470.354055\n-7    4608.0  691.200017  474.643779\n-8    5120.0  744.727250  478.132283\n-9    5632.0  795.105885  484.473119\n-10   6144.0  833.084772  489.887055\n-11   6656.0  872.918050  494.563489\n-12   7168.0  910.222229  472.615376\n-13   7680.0  940.408194  475.051564\n-14   8192.0  978.149241  483.066322\n-15   8704.0  648.745327  483.555555\n-16   9216.0  678.478510  486.118680\n-17   9728.0  699.017941  490.487409\n-18  10240.0  716.501474  491.519977\n-19  10752.0  743.654192  482.332711\n-20  11264.0  770.188045  484.473119\n-21  11776.0  789.452536  488.968877\n-22  12288.0  810.197787  494.818794\n-23  12800.0  814.854134  498.701318\n-24  13312.0  834.172311  499.200013\n-25  13824.0  829.440021  498.911279\n-26  14336.0  837.138714  490.119665\n-27  14848.0  842.439735  493.562309\n-28  15360.0  857.302354  498.836278\n-29  15872.0  856.017954  500.562435\n+          N       Triton       Torch\n+0    1024.0   189.046153  378.092307\n+1    1536.0   285.767458  438.857146\n+2    2048.0   381.023277  501.551037\n+3    2560.0   472.615383  534.260858\n+4    3072.0   571.534916  538.160602\n+5    3584.0   641.910440  470.032796\n+6    4096.0   707.223041  472.615390\n+7    4608.0   722.823525  478.753251\n+8    5120.0   772.830175  483.779502\n+9    5632.0   824.195135  489.739120\n+10   6144.0   862.315754  491.519977\n+11   6656.0   907.636357  500.764869\n+12   7168.0   945.230752  476.542919\n+13   7680.0   975.238103  479.999983\n+14   8192.0  1013.443336  489.074621\n+15   8704.0   678.233793  490.366182\n+16   9216.0   706.658154  495.928261\n+17   9728.0   727.327104  496.748937\n+18  10240.0   746.990876  496.484863\n+19  10752.0   772.598776  487.803392\n+20  11264.0   799.810621  487.971095\n+21  11776.0   821.581395  491.519989\n+22  12288.0   842.605744  499.850839\n+23  12800.0   846.280994  501.141916\n+24  13312.0   870.539508  499.981239\n+25  13824.0   866.255861  502.690894\n+26  14336.0   873.258878  492.928354\n+27  14848.0   873.411781  496.311981\n+28  15360.0   888.289183  500.869554\n+29  15872.0   890.018693  501.221037\n </pre></div>\n </div>\n </section>\n@@ -501,7 +501,7 @@ <h2>References<a class=\"headerlink\" href=\"#references\" title=\"Permalink to this\n <p>Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \u201cLayer Normalization\u201d, Arxiv 2016</p>\n </div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  29.168 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  29.195 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-05-layer-norm-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/935c0dd0fbeb4b2e69588471cbb2d4b2/05-layer-norm.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">05-layer-norm.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/06-fused-attention.html", "status": "modified", "additions": 184, "deletions": 97, "changes": 281, "file_content_changes": "@@ -109,19 +109,37 @@\n <ul class=\"sphx-glr-horizontal\">\n <li><img src=\"../../_images/sphx_glr_06-fused-attention_001.png\" srcset=\"../../_images/sphx_glr_06-fused-attention_001.png\" alt=\"06 fused attention\" class = \"sphx-glr-multi-img\"/></li>\n <li><img src=\"../../_images/sphx_glr_06-fused-attention_002.png\" srcset=\"../../_images/sphx_glr_06-fused-attention_002.png\" alt=\"06 fused attention\" class = \"sphx-glr-multi-img\"/></li>\n+<li><img src=\"../../_images/sphx_glr_06-fused-attention_003.png\" srcset=\"../../_images/sphx_glr_06-fused-attention_003.png\" alt=\"06 fused attention\" class = \"sphx-glr-multi-img\"/></li>\n+<li><img src=\"../../_images/sphx_glr_06-fused-attention_004.png\" srcset=\"../../_images/sphx_glr_06-fused-attention_004.png\" alt=\"06 fused attention\" class = \"sphx-glr-multi-img\"/></li>\n </ul>\n <div class=\"sphx-glr-script-out highlight-none notranslate\"><div class=\"highlight\"><pre><span></span>fused-attention-batch4-head48-d64-fwd:\n-    N_CTX     Triton\n-0  1024.0   0.303429\n-1  2048.0   1.003967\n-2  4096.0   3.674860\n-3  8192.0  14.002908\n+     N_CTX      Triton\n+0   1024.0  126.157204\n+1   2048.0  134.355754\n+2   4096.0  140.594263\n+3   8192.0  142.562069\n+4  16384.0  143.578766\n+fused-attention-batch4-head48-d64-fwd:\n+     N_CTX      Triton\n+0   1024.0   90.947476\n+1   2048.0  108.466002\n+2   4096.0  124.015402\n+3   8192.0  133.523689\n+4  16384.0  140.291167\n fused-attention-batch4-head48-d64-bwd:\n-    N_CTX     Triton\n-0  1024.0   1.245852\n-1  2048.0   3.964074\n-2  4096.0  13.915574\n-3  8192.0  51.827713\n+     N_CTX     Triton\n+0   1024.0  77.483639\n+1   2048.0  87.607698\n+2   4096.0  92.718487\n+3   8192.0  96.406932\n+4  16384.0  97.984645\n+fused-attention-batch4-head48-d64-bwd:\n+     N_CTX     Triton\n+0   1024.0  54.121347\n+1   2048.0  67.973852\n+2   4096.0  76.892519\n+3   8192.0  82.593830\n+4  16384.0  85.563482\n </pre></div>\n </div>\n <div class=\"line-block\">\n@@ -146,68 +164,113 @@\n     <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span>\n     <span class=\"n\">BLOCK_M</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n     <span class=\"n\">BLOCK_N</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+    <span class=\"n\">MODE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n <span class=\"p\">):</span>\n     <span class=\"n\">start_m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n     <span class=\"n\">off_hz</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n+    <span class=\"n\">qvk_offset</span> <span class=\"o\">=</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span>\n+    <span class=\"n\">Q_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span>\n+        <span class=\"n\">base</span><span class=\"o\">=</span><span class=\"n\">Q</span> <span class=\"o\">+</span> <span class=\"n\">qvk_offset</span><span class=\"p\">,</span>\n+        <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">),</span>\n+        <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_qm</span><span class=\"p\">,</span> <span class=\"n\">stride_qk</span><span class=\"p\">),</span>\n+        <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">start_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span>\n+        <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">),</span>\n+        <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"p\">)</span>\n+    <span class=\"n\">K_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span>\n+        <span class=\"n\">base</span><span class=\"o\">=</span><span class=\"n\">K</span> <span class=\"o\">+</span> <span class=\"n\">qvk_offset</span><span class=\"p\">,</span>\n+        <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">),</span>\n+        <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_kk</span><span class=\"p\">,</span> <span class=\"n\">stride_kn</span><span class=\"p\">),</span>\n+        <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span>\n+        <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"p\">),</span>\n+        <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n+    <span class=\"p\">)</span>\n+    <span class=\"n\">V_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span>\n+        <span class=\"n\">base</span><span class=\"o\">=</span><span class=\"n\">V</span> <span class=\"o\">+</span> <span class=\"n\">qvk_offset</span><span class=\"p\">,</span>\n+        <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">),</span>\n+        <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_vk</span><span class=\"p\">,</span> <span class=\"n\">stride_vn</span><span class=\"p\">),</span>\n+        <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span>\n+        <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">BLOCK_N</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">),</span>\n+        <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"p\">)</span>\n+    <span class=\"n\">O_block_ptr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">make_block_ptr</span><span class=\"p\">(</span>\n+        <span class=\"n\">base</span><span class=\"o\">=</span><span class=\"n\">Out</span> <span class=\"o\">+</span> <span class=\"n\">qvk_offset</span><span class=\"p\">,</span>\n+        <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">),</span>\n+        <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">stride_om</span><span class=\"p\">,</span> <span class=\"n\">stride_on</span><span class=\"p\">),</span>\n+        <span class=\"n\">offsets</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">start_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span>\n+        <span class=\"n\">block_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">),</span>\n+        <span class=\"n\">order</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n+    <span class=\"p\">)</span>\n     <span class=\"c1\"># initialize offsets</span>\n     <span class=\"n\">offs_m</span> <span class=\"o\">=</span> <span class=\"n\">start_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">)</span>\n     <span class=\"n\">offs_n</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"p\">)</span>\n-    <span class=\"n\">offs_d</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">)</span>\n-    <span class=\"n\">off_q</span> <span class=\"o\">=</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span> <span class=\"o\">+</span> <span class=\"n\">offs_m</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qm</span> <span class=\"o\">+</span> <span class=\"n\">offs_d</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qk</span>\n-    <span class=\"n\">off_k</span> <span class=\"o\">=</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span> <span class=\"o\">+</span> <span class=\"n\">offs_n</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_kn</span> <span class=\"o\">+</span> <span class=\"n\">offs_d</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_kk</span>\n-    <span class=\"n\">off_v</span> <span class=\"o\">=</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span> <span class=\"o\">+</span> <span class=\"n\">offs_n</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qm</span> <span class=\"o\">+</span> <span class=\"n\">offs_d</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_qk</span>\n-    <span class=\"c1\"># Initialize pointers to Q, K, V</span>\n-    <span class=\"n\">q_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">Q</span> <span class=\"o\">+</span> <span class=\"n\">off_q</span>\n-    <span class=\"n\">k_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">K</span> <span class=\"o\">+</span> <span class=\"n\">off_k</span>\n-    <span class=\"n\">v_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">V</span> <span class=\"o\">+</span> <span class=\"n\">off_v</span>\n     <span class=\"c1\"># initialize pointer to m and l</span>\n-    <span class=\"n\">m_prev</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_M</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s2\">&quot;inf&quot;</span><span class=\"p\">)</span>\n-    <span class=\"n\">l_prev</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_M</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"n\">m_i</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_M</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s2\">&quot;inf&quot;</span><span class=\"p\">)</span>\n+    <span class=\"n\">l_i</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_M</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n     <span class=\"n\">acc</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># causal check on every loop iteration can be expensive</span>\n+    <span class=\"c1\"># and peeling the last iteration of the loop does not work well with ptxas</span>\n+    <span class=\"c1\"># so we have a mode to do the causal check in a separate kernel entirely</span>\n+    <span class=\"k\">if</span> <span class=\"n\">MODE</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>  <span class=\"c1\"># entire non-causal attention</span>\n+        <span class=\"n\">lo</span><span class=\"p\">,</span> <span class=\"n\">hi</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span>\n+    <span class=\"k\">if</span> <span class=\"n\">MODE</span> <span class=\"o\">==</span> <span class=\"mi\">1</span><span class=\"p\">:</span>  <span class=\"c1\"># entire causal attention</span>\n+        <span class=\"n\">lo</span><span class=\"p\">,</span> <span class=\"n\">hi</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">start_m</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span>\n+    <span class=\"k\">if</span> <span class=\"n\">MODE</span> <span class=\"o\">==</span> <span class=\"mi\">2</span><span class=\"p\">:</span>  <span class=\"c1\"># off band-diagonal</span>\n+        <span class=\"n\">lo</span><span class=\"p\">,</span> <span class=\"n\">hi</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">start_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span>\n+    <span class=\"k\">if</span> <span class=\"n\">MODE</span> <span class=\"o\">==</span> <span class=\"mi\">3</span><span class=\"p\">:</span>  <span class=\"c1\"># on band-diagonal</span>\n+        <span class=\"n\">l_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">L</span> <span class=\"o\">+</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">N_CTX</span> <span class=\"o\">+</span> <span class=\"n\">offs_m</span>\n+        <span class=\"n\">m_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">M</span> <span class=\"o\">+</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">N_CTX</span> <span class=\"o\">+</span> <span class=\"n\">offs_m</span>\n+        <span class=\"n\">m_i</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">m_ptrs</span><span class=\"p\">)</span>\n+        <span class=\"n\">l_i</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">l_ptrs</span><span class=\"p\">)</span>\n+        <span class=\"n\">acc</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">O_block_ptr</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+        <span class=\"n\">lo</span><span class=\"p\">,</span> <span class=\"n\">hi</span> <span class=\"o\">=</span> <span class=\"n\">start_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">start_m</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span>\n+    <span class=\"c1\"># credits to: Adam P. Goucher (https://github.com/apgoucher):</span>\n+    <span class=\"c1\"># scale sm_scale by 1/log_2(e) and use</span>\n+    <span class=\"c1\"># 2^x instead of exp in the loop because CSE and LICM</span>\n+    <span class=\"c1\"># don&#39;t work as expected with `exp` in the loop</span>\n+    <span class=\"n\">qk_scale</span> <span class=\"o\">=</span> <span class=\"n\">sm_scale</span> <span class=\"o\">*</span> <span class=\"mf\">1.44269504</span>\n     <span class=\"c1\"># load q: it will stay in SRAM throughout</span>\n-    <span class=\"n\">q</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">q_ptrs</span><span class=\"p\">)</span>\n+    <span class=\"n\">q</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">Q_block_ptr</span><span class=\"p\">)</span>\n+    <span class=\"n\">q</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">q</span> <span class=\"o\">*</span> <span class=\"n\">qk_scale</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n     <span class=\"c1\"># loop over k, v and update accumulator</span>\n-    <span class=\"k\">for</span> <span class=\"n\">start_n</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">start_m</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"p\">):</span>\n+    <span class=\"k\">for</span> <span class=\"n\">start_n</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">lo</span><span class=\"p\">,</span> <span class=\"n\">hi</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"p\">):</span>\n+        <span class=\"n\">start_n</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">multiple_of</span><span class=\"p\">(</span><span class=\"n\">start_n</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"p\">)</span>\n         <span class=\"c1\"># -- compute qk ----</span>\n-        <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">k_ptrs</span><span class=\"p\">)</span>\n+        <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">advance</span><span class=\"p\">(</span><span class=\"n\">K_block_ptr</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">start_n</span><span class=\"p\">)))</span>\n         <span class=\"n\">qk</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n         <span class=\"n\">qk</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">)</span>\n-        <span class=\"n\">qk</span> <span class=\"o\">*=</span> <span class=\"n\">sm_scale</span>\n-        <span class=\"n\">qk</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">offs_m</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&gt;=</span> <span class=\"p\">(</span><span class=\"n\">start_n</span> <span class=\"o\">+</span> <span class=\"n\">offs_n</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]),</span> <span class=\"n\">qk</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s2\">&quot;-inf&quot;</span><span class=\"p\">))</span>\n-        <span class=\"c1\"># compute new m</span>\n-        <span class=\"n\">m_curr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">maximum</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">qk</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">m_prev</span><span class=\"p\">)</span>\n-        <span class=\"c1\"># correct old l</span>\n-        <span class=\"n\">l_prev</span> <span class=\"o\">*=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">m_prev</span> <span class=\"o\">-</span> <span class=\"n\">m_curr</span><span class=\"p\">)</span>\n-        <span class=\"c1\"># attention weights</span>\n-        <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">qk</span> <span class=\"o\">-</span> <span class=\"n\">m_curr</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">])</span>\n-        <span class=\"n\">l_curr</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">l_prev</span>\n-        <span class=\"c1\"># rescale operands of matmuls</span>\n-        <span class=\"n\">l_rcp</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span> <span class=\"o\">/</span> <span class=\"n\">l_curr</span>\n-        <span class=\"n\">p</span> <span class=\"o\">*=</span> <span class=\"n\">l_rcp</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n-        <span class=\"n\">acc</span> <span class=\"o\">*=</span> <span class=\"p\">(</span><span class=\"n\">l_prev</span> <span class=\"o\">*</span> <span class=\"n\">l_rcp</span><span class=\"p\">)[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n+        <span class=\"k\">if</span> <span class=\"n\">MODE</span> <span class=\"o\">==</span> <span class=\"mi\">1</span> <span class=\"ow\">or</span> <span class=\"n\">MODE</span> <span class=\"o\">==</span> <span class=\"mi\">3</span><span class=\"p\">:</span>\n+            <span class=\"n\">qk</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">offs_m</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&gt;=</span> <span class=\"p\">(</span><span class=\"n\">start_n</span> <span class=\"o\">+</span> <span class=\"n\">offs_n</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]),</span> <span class=\"n\">qk</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s2\">&quot;-inf&quot;</span><span class=\"p\">))</span>\n+        <span class=\"c1\"># -- compute m_ij, p, l_ij</span>\n+        <span class=\"n\">m_ij</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">qk</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n+        <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">exp2</span><span class=\"p\">(</span><span class=\"n\">qk</span> <span class=\"o\">-</span> <span class=\"n\">m_ij</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">])</span>\n+        <span class=\"n\">l_ij</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n+        <span class=\"c1\"># -- update m_i and l_i</span>\n+        <span class=\"n\">m_i_new</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">maximum</span><span class=\"p\">(</span><span class=\"n\">m_i</span><span class=\"p\">,</span> <span class=\"n\">m_ij</span><span class=\"p\">)</span>\n+        <span class=\"n\">alpha</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">exp2</span><span class=\"p\">(</span><span class=\"n\">m_i</span> <span class=\"o\">-</span> <span class=\"n\">m_i_new</span><span class=\"p\">)</span>\n+        <span class=\"n\">beta</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">exp2</span><span class=\"p\">(</span><span class=\"n\">m_ij</span> <span class=\"o\">-</span> <span class=\"n\">m_i_new</span><span class=\"p\">)</span>\n+        <span class=\"n\">l_i</span> <span class=\"o\">*=</span> <span class=\"n\">alpha</span>\n+        <span class=\"n\">l_i_new</span> <span class=\"o\">=</span> <span class=\"n\">l_i</span> <span class=\"o\">+</span> <span class=\"n\">beta</span> <span class=\"o\">*</span> <span class=\"n\">l_ij</span>\n+        <span class=\"c1\"># scale p</span>\n+        <span class=\"n\">p_scale</span> <span class=\"o\">=</span> <span class=\"n\">beta</span> <span class=\"o\">/</span> <span class=\"n\">l_i_new</span>\n+        <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">p</span> <span class=\"o\">*</span> <span class=\"n\">p_scale</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n+        <span class=\"c1\"># scale acc</span>\n+        <span class=\"n\">acc_scale</span> <span class=\"o\">=</span> <span class=\"n\">l_i</span> <span class=\"o\">/</span> <span class=\"n\">l_i_new</span>\n+        <span class=\"n\">acc</span> <span class=\"o\">=</span> <span class=\"n\">acc</span> <span class=\"o\">*</span> <span class=\"n\">acc_scale</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n         <span class=\"c1\"># update acc</span>\n-        <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"o\">.</span><span class=\"n\">element_ty</span><span class=\"p\">)</span>\n-        <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">v_ptrs</span><span class=\"p\">)</span>\n+        <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">advance</span><span class=\"p\">(</span><span class=\"n\">V_block_ptr</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">start_n</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">)))</span>\n+        <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">)</span>\n         <span class=\"n\">acc</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">)</span>\n         <span class=\"c1\"># update m_i and l_i</span>\n-        <span class=\"n\">l_prev</span> <span class=\"o\">=</span> <span class=\"n\">l_curr</span>\n-        <span class=\"n\">m_prev</span> <span class=\"o\">=</span> <span class=\"n\">m_curr</span>\n-        <span class=\"c1\"># update pointers</span>\n-        <span class=\"n\">k_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_N</span> <span class=\"o\">*</span> <span class=\"n\">stride_kn</span>\n-        <span class=\"n\">v_ptrs</span> <span class=\"o\">+=</span> <span class=\"n\">BLOCK_N</span> <span class=\"o\">*</span> <span class=\"n\">stride_vk</span>\n-    <span class=\"c1\"># rematerialize offsets to save registers</span>\n-    <span class=\"n\">start_m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n-    <span class=\"n\">offs_m</span> <span class=\"o\">=</span> <span class=\"n\">start_m</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">)</span>\n+        <span class=\"n\">l_i</span> <span class=\"o\">=</span> <span class=\"n\">l_i_new</span>\n+        <span class=\"n\">m_i</span> <span class=\"o\">=</span> <span class=\"n\">m_i_new</span>\n     <span class=\"c1\"># write back l and m</span>\n     <span class=\"n\">l_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">L</span> <span class=\"o\">+</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">N_CTX</span> <span class=\"o\">+</span> <span class=\"n\">offs_m</span>\n     <span class=\"n\">m_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">M</span> <span class=\"o\">+</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">N_CTX</span> <span class=\"o\">+</span> <span class=\"n\">offs_m</span>\n-    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">l_ptrs</span><span class=\"p\">,</span> <span class=\"n\">l_prev</span><span class=\"p\">)</span>\n-    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">m_ptrs</span><span class=\"p\">,</span> <span class=\"n\">m_prev</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># initialize pointers to output</span>\n-    <span class=\"n\">offs_n</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">)</span>\n-    <span class=\"n\">off_o</span> <span class=\"o\">=</span> <span class=\"n\">off_hz</span> <span class=\"o\">*</span> <span class=\"n\">stride_oh</span> <span class=\"o\">+</span> <span class=\"n\">offs_m</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">stride_om</span> <span class=\"o\">+</span> <span class=\"n\">offs_n</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">*</span> <span class=\"n\">stride_on</span>\n-    <span class=\"n\">out_ptrs</span> <span class=\"o\">=</span> <span class=\"n\">Out</span> <span class=\"o\">+</span> <span class=\"n\">off_o</span>\n-    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">out_ptrs</span><span class=\"p\">,</span> <span class=\"n\">acc</span><span class=\"p\">)</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">l_ptrs</span><span class=\"p\">,</span> <span class=\"n\">l_i</span><span class=\"p\">)</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">m_ptrs</span><span class=\"p\">,</span> <span class=\"n\">m_i</span><span class=\"p\">)</span>\n+    <span class=\"c1\"># write back O</span>\n+    <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">store</span><span class=\"p\">(</span><span class=\"n\">O_block_ptr</span><span class=\"p\">,</span> <span class=\"n\">acc</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">))</span>\n \n \n <span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">jit</span>\n@@ -243,10 +306,12 @@\n     <span class=\"n\">num_block</span><span class=\"p\">,</span>\n     <span class=\"n\">BLOCK_M</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n     <span class=\"n\">BLOCK_N</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n+    <span class=\"n\">MODE</span><span class=\"p\">:</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">constexpr</span><span class=\"p\">,</span>\n <span class=\"p\">):</span>\n     <span class=\"n\">off_hz</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">program_id</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n     <span class=\"n\">off_z</span> <span class=\"o\">=</span> <span class=\"n\">off_hz</span> <span class=\"o\">//</span> <span class=\"n\">H</span>\n     <span class=\"n\">off_h</span> <span class=\"o\">=</span> <span class=\"n\">off_hz</span> <span class=\"o\">%</span> <span class=\"n\">H</span>\n+    <span class=\"n\">qk_scale</span> <span class=\"o\">=</span> <span class=\"n\">sm_scale</span> <span class=\"o\">*</span> <span class=\"mf\">1.44269504</span>\n     <span class=\"c1\"># offset pointers for batch/head</span>\n     <span class=\"n\">Q</span> <span class=\"o\">+=</span> <span class=\"n\">off_z</span> <span class=\"o\">*</span> <span class=\"n\">stride_qz</span> <span class=\"o\">+</span> <span class=\"n\">off_h</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span>\n     <span class=\"n\">K</span> <span class=\"o\">+=</span> <span class=\"n\">off_z</span> <span class=\"o\">*</span> <span class=\"n\">stride_qz</span> <span class=\"o\">+</span> <span class=\"n\">off_h</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span>\n@@ -256,7 +321,10 @@\n     <span class=\"n\">DK</span> <span class=\"o\">+=</span> <span class=\"n\">off_z</span> <span class=\"o\">*</span> <span class=\"n\">stride_qz</span> <span class=\"o\">+</span> <span class=\"n\">off_h</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span>\n     <span class=\"n\">DV</span> <span class=\"o\">+=</span> <span class=\"n\">off_z</span> <span class=\"o\">*</span> <span class=\"n\">stride_qz</span> <span class=\"o\">+</span> <span class=\"n\">off_h</span> <span class=\"o\">*</span> <span class=\"n\">stride_qh</span>\n     <span class=\"k\">for</span> <span class=\"n\">start_n</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">num_block</span><span class=\"p\">):</span>\n-        <span class=\"n\">lo</span> <span class=\"o\">=</span> <span class=\"n\">start_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span>\n+        <span class=\"k\">if</span> <span class=\"n\">MODE</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n+            <span class=\"n\">lo</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n+        <span class=\"k\">else</span><span class=\"p\">:</span>\n+            <span class=\"n\">lo</span> <span class=\"o\">=</span> <span class=\"n\">start_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span>\n         <span class=\"c1\"># initialize row/col offsets</span>\n         <span class=\"n\">offs_qm</span> <span class=\"o\">=</span> <span class=\"n\">lo</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">)</span>\n         <span class=\"n\">offs_n</span> <span class=\"o\">=</span> <span class=\"n\">start_n</span> <span class=\"o\">*</span> <span class=\"n\">BLOCK_M</span> <span class=\"o\">+</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_M</span><span class=\"p\">)</span>\n@@ -284,10 +352,15 @@\n             <span class=\"n\">q</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">q_ptrs</span><span class=\"p\">)</span>\n             <span class=\"c1\"># recompute p = softmax(qk, dim=-1).T</span>\n             <span class=\"c1\"># NOTE: `do` is pre-divided by `l`; no normalization here</span>\n-            <span class=\"n\">qk</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">trans</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">))</span>\n-            <span class=\"n\">qk</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">offs_m_curr</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&gt;=</span> <span class=\"p\">(</span><span class=\"n\">offs_n</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]),</span> <span class=\"n\">qk</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s2\">&quot;-inf&quot;</span><span class=\"p\">))</span>\n+            <span class=\"c1\"># if MODE == 1:</span>\n+            <span class=\"k\">if</span> <span class=\"n\">MODE</span> <span class=\"o\">==</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n+                <span class=\"n\">qk</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">offs_m_curr</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">&gt;=</span> <span class=\"p\">(</span><span class=\"n\">offs_n</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">:]),</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"mf\">0.</span><span class=\"p\">),</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s2\">&quot;-inf&quot;</span><span class=\"p\">))</span>\n+            <span class=\"k\">else</span><span class=\"p\">:</span>\n+                <span class=\"n\">qk</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">BLOCK_M</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n+            <span class=\"n\">qk</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">trans</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">))</span>\n+            <span class=\"n\">qk</span> <span class=\"o\">*=</span> <span class=\"n\">qk_scale</span>\n             <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">m_ptrs</span> <span class=\"o\">+</span> <span class=\"n\">offs_m_curr</span><span class=\"p\">)</span>\n-            <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">qk</span> <span class=\"o\">*</span> <span class=\"n\">sm_scale</span> <span class=\"o\">-</span> <span class=\"n\">m</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">])</span>\n+            <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">exp2</span><span class=\"p\">(</span><span class=\"n\">qk</span> <span class=\"o\">-</span> <span class=\"n\">m</span><span class=\"p\">[:,</span> <span class=\"kc\">None</span><span class=\"p\">])</span>\n             <span class=\"c1\"># compute dv</span>\n             <span class=\"n\">do</span> <span class=\"o\">=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">do_ptrs</span><span class=\"p\">)</span>\n             <span class=\"n\">dv</span> <span class=\"o\">+=</span> <span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">tl</span><span class=\"o\">.</span><span class=\"n\">trans</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"o\">.</span><span class=\"n\">element_ty</span><span class=\"p\">)),</span> <span class=\"n\">do</span><span class=\"p\">)</span>\n@@ -320,37 +393,42 @@\n <span class=\"k\">class</span> <span class=\"nc\">_attention</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">autograd</span><span class=\"o\">.</span><span class=\"n\">Function</span><span class=\"p\">):</span>\n \n     <span class=\"nd\">@staticmethod</span>\n-    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">,</span> <span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">sm_scale</span><span class=\"p\">):</span>\n+    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">,</span> <span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">causal</span><span class=\"p\">,</span> <span class=\"n\">sm_scale</span><span class=\"p\">):</span>\n         <span class=\"n\">BLOCK</span> <span class=\"o\">=</span> <span class=\"mi\">128</span>\n         <span class=\"c1\"># shape constraints</span>\n         <span class=\"n\">Lq</span><span class=\"p\">,</span> <span class=\"n\">Lk</span><span class=\"p\">,</span> <span class=\"n\">Lv</span> <span class=\"o\">=</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n         <span class=\"k\">assert</span> <span class=\"n\">Lq</span> <span class=\"o\">==</span> <span class=\"n\">Lk</span> <span class=\"ow\">and</span> <span class=\"n\">Lk</span> <span class=\"o\">==</span> <span class=\"n\">Lv</span>\n         <span class=\"k\">assert</span> <span class=\"n\">Lk</span> <span class=\"ow\">in</span> <span class=\"p\">{</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">}</span>\n         <span class=\"n\">o</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">)</span>\n-        <span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"n\">BLOCK</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n+        <span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">cdiv</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"mi\">128</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n         <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n         <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n-        <span class=\"n\">num_warps</span> <span class=\"o\">=</span> <span class=\"mi\">4</span> <span class=\"k\">if</span> <span class=\"n\">Lk</span> <span class=\"o\">&lt;=</span> <span class=\"mi\">64</span> <span class=\"k\">else</span> <span class=\"mi\">8</span>\n \n-        <span class=\"n\">_fwd_kernel</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span>\n-            <span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">sm_scale</span><span class=\"p\">,</span>\n-            <span class=\"n\">L</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">,</span>\n-            <span class=\"n\">o</span><span class=\"p\">,</span>\n-            <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n-            <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n-            <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n-            <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n-            <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">],</span>\n-            <span class=\"n\">BLOCK_M</span><span class=\"o\">=</span><span class=\"n\">BLOCK</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"o\">=</span><span class=\"n\">BLOCK</span><span class=\"p\">,</span>\n-            <span class=\"n\">BLOCK_DMODEL</span><span class=\"o\">=</span><span class=\"n\">Lk</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"n\">num_warps</span><span class=\"p\">,</span>\n-            <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n-        <span class=\"p\">)</span>\n-        <span class=\"c1\"># print(h.asm[&quot;ttgir&quot;])</span>\n+        <span class=\"n\">num_warps</span> <span class=\"o\">=</span> <span class=\"mi\">4</span> <span class=\"k\">if</span> <span class=\"n\">Lk</span> <span class=\"o\">&lt;=</span> <span class=\"mi\">64</span> <span class=\"k\">else</span> <span class=\"mi\">8</span>\n+        <span class=\"k\">if</span> <span class=\"n\">causal</span><span class=\"p\">:</span>\n+            <span class=\"n\">modes</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]</span> <span class=\"o\">&lt;=</span> <span class=\"mi\">2048</span> <span class=\"k\">else</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">]</span>\n+        <span class=\"k\">else</span><span class=\"p\">:</span>\n+            <span class=\"n\">modes</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n+        <span class=\"k\">for</span> <span class=\"n\">mode</span> <span class=\"ow\">in</span> <span class=\"n\">modes</span><span class=\"p\">:</span>\n+            <span class=\"n\">_fwd_kernel</span><span class=\"p\">[</span><span class=\"n\">grid</span><span class=\"p\">](</span>\n+                <span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">sm_scale</span><span class=\"p\">,</span>\n+                <span class=\"n\">L</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">,</span>\n+                <span class=\"n\">o</span><span class=\"p\">,</span>\n+                <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n+                <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n+                <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n+                <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">stride</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n+                <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">],</span>\n+                <span class=\"n\">BLOCK_M</span><span class=\"o\">=</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"o\">=</span><span class=\"n\">BLOCK</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_DMODEL</span><span class=\"o\">=</span><span class=\"n\">Lk</span><span class=\"p\">,</span>\n+                <span class=\"n\">MODE</span><span class=\"o\">=</span><span class=\"n\">mode</span><span class=\"p\">,</span>\n+                <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"n\">num_warps</span><span class=\"p\">,</span>\n+                <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n \n         <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">save_for_backward</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">o</span><span class=\"p\">,</span> <span class=\"n\">L</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">)</span>\n         <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"n\">grid</span>\n         <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">sm_scale</span> <span class=\"o\">=</span> <span class=\"n\">sm_scale</span>\n         <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">BLOCK_DMODEL</span> <span class=\"o\">=</span> <span class=\"n\">Lk</span>\n+        <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">causal</span> <span class=\"o\">=</span> <span class=\"n\">causal</span>\n         <span class=\"k\">return</span> <span class=\"n\">o</span>\n \n     <span class=\"nd\">@staticmethod</span>\n@@ -363,6 +441,10 @@\n         <span class=\"n\">dv</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">)</span>\n         <span class=\"n\">do_scaled</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">do</span><span class=\"p\">)</span>\n         <span class=\"n\">delta</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty_like</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)</span>\n+        <span class=\"k\">if</span> <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">causal</span><span class=\"p\">:</span>\n+            <span class=\"n\">mode</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n+        <span class=\"k\">else</span><span class=\"p\">:</span>\n+            <span class=\"n\">mode</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n         <span class=\"n\">_bwd_preprocess</span><span class=\"p\">[(</span><span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">)](</span>\n             <span class=\"n\">o</span><span class=\"p\">,</span> <span class=\"n\">do</span><span class=\"p\">,</span> <span class=\"n\">l</span><span class=\"p\">,</span>\n             <span class=\"n\">do_scaled</span><span class=\"p\">,</span> <span class=\"n\">delta</span><span class=\"p\">,</span>\n@@ -381,40 +463,40 @@\n             <span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span>\n             <span class=\"n\">BLOCK_M</span><span class=\"o\">=</span><span class=\"n\">BLOCK</span><span class=\"p\">,</span> <span class=\"n\">BLOCK_N</span><span class=\"o\">=</span><span class=\"n\">BLOCK</span><span class=\"p\">,</span>\n             <span class=\"n\">BLOCK_DMODEL</span><span class=\"o\">=</span><span class=\"n\">ctx</span><span class=\"o\">.</span><span class=\"n\">BLOCK_DMODEL</span><span class=\"p\">,</span> <span class=\"n\">num_warps</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">,</span>\n+            <span class=\"n\">MODE</span><span class=\"o\">=</span><span class=\"n\">mode</span><span class=\"p\">,</span>\n             <span class=\"n\">num_stages</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n         <span class=\"p\">)</span>\n-        <span class=\"c1\"># print(h.asm[&quot;ttgir&quot;])</span>\n-        <span class=\"k\">return</span> <span class=\"n\">dq</span><span class=\"p\">,</span> <span class=\"n\">dk</span><span class=\"p\">,</span> <span class=\"n\">dv</span><span class=\"p\">,</span> <span class=\"kc\">None</span>\n+        <span class=\"k\">return</span> <span class=\"n\">dq</span><span class=\"p\">,</span> <span class=\"n\">dk</span><span class=\"p\">,</span> <span class=\"n\">dv</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span>\n \n \n <span class=\"n\">attention</span> <span class=\"o\">=</span> <span class=\"n\">_attention</span><span class=\"o\">.</span><span class=\"n\">apply</span>\n \n \n-<span class=\"nd\">@pytest</span><span class=\"o\">.</span><span class=\"n\">mark</span><span class=\"o\">.</span><span class=\"n\">parametrize</span><span class=\"p\">(</span><span class=\"s1\">&#39;Z, H, N_CTX, D_HEAD&#39;</span><span class=\"p\">,</span> <span class=\"p\">[(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">48</span><span class=\"p\">,</span> <span class=\"mi\">1024</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">)])</span>\n-<span class=\"k\">def</span> <span class=\"nf\">test_op</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">):</span>\n+<span class=\"nd\">@pytest</span><span class=\"o\">.</span><span class=\"n\">mark</span><span class=\"o\">.</span><span class=\"n\">parametrize</span><span class=\"p\">(</span><span class=\"s1\">&#39;Z, H, N_CTX, D_HEAD&#39;</span><span class=\"p\">,</span> <span class=\"p\">[(</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">9</span><span class=\"p\">,</span> <span class=\"mi\">1024</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">)])</span>\n+<span class=\"nd\">@pytest</span><span class=\"o\">.</span><span class=\"n\">mark</span><span class=\"o\">.</span><span class=\"n\">parametrize</span><span class=\"p\">(</span><span class=\"s1\">&#39;causal&#39;</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">])</span>\n+<span class=\"k\">def</span> <span class=\"nf\">test_op</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">,</span> <span class=\"n\">causal</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">):</span>\n     <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">)</span>\n-    <span class=\"n\">q</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">normal_</span><span class=\"p\">(</span><span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">std</span><span class=\"o\">=</span><span class=\"mf\">0.2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">requires_grad_</span><span class=\"p\">()</span>\n-    <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">normal_</span><span class=\"p\">(</span><span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"mf\">0.4</span><span class=\"p\">,</span> <span class=\"n\">std</span><span class=\"o\">=</span><span class=\"mf\">0.2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">requires_grad_</span><span class=\"p\">()</span>\n-    <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">normal_</span><span class=\"p\">(</span><span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"mf\">0.3</span><span class=\"p\">,</span> <span class=\"n\">std</span><span class=\"o\">=</span><span class=\"mf\">0.2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">requires_grad_</span><span class=\"p\">()</span>\n-    <span class=\"n\">sm_scale</span> <span class=\"o\">=</span> <span class=\"mf\">0.2</span>\n+    <span class=\"n\">q</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">normal_</span><span class=\"p\">(</span><span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">,</span> <span class=\"n\">std</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">requires_grad_</span><span class=\"p\">()</span>\n+    <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">normal_</span><span class=\"p\">(</span><span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">,</span> <span class=\"n\">std</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">requires_grad_</span><span class=\"p\">()</span>\n+    <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">normal_</span><span class=\"p\">(</span><span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">,</span> <span class=\"n\">std</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">requires_grad_</span><span class=\"p\">()</span>\n+    <span class=\"n\">sm_scale</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>\n     <span class=\"n\">dout</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn_like</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">)</span>\n     <span class=\"c1\"># reference implementation</span>\n     <span class=\"n\">M</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tril</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">))</span>\n     <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">))</span> <span class=\"o\">*</span> <span class=\"n\">sm_scale</span>\n-    <span class=\"k\">for</span> <span class=\"n\">z</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"p\">):</span>\n-        <span class=\"k\">for</span> <span class=\"n\">h</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">H</span><span class=\"p\">):</span>\n-            <span class=\"n\">p</span><span class=\"p\">[:,</span> <span class=\"p\">:,</span> <span class=\"n\">M</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s2\">&quot;-inf&quot;</span><span class=\"p\">)</span>\n+    <span class=\"k\">if</span> <span class=\"n\">causal</span><span class=\"p\">:</span>\n+        <span class=\"k\">for</span> <span class=\"n\">z</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"p\">):</span>\n+            <span class=\"k\">for</span> <span class=\"n\">h</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">H</span><span class=\"p\">):</span>\n+                <span class=\"n\">p</span><span class=\"p\">[:,</span> <span class=\"p\">:,</span> <span class=\"n\">M</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s2\">&quot;-inf&quot;</span><span class=\"p\">)</span>\n     <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">float</span><span class=\"p\">(),</span> <span class=\"n\">dim</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">half</span><span class=\"p\">()</span>\n     <span class=\"c1\"># p = torch.exp(p)</span>\n     <span class=\"n\">ref_out</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">)</span>\n     <span class=\"n\">ref_out</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">(</span><span class=\"n\">dout</span><span class=\"p\">)</span>\n     <span class=\"n\">ref_dv</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">clone</span><span class=\"p\">(),</span> <span class=\"kc\">None</span>\n     <span class=\"n\">ref_dk</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">clone</span><span class=\"p\">(),</span> <span class=\"kc\">None</span>\n     <span class=\"n\">ref_dq</span><span class=\"p\">,</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">clone</span><span class=\"p\">(),</span> <span class=\"kc\">None</span>\n-    <span class=\"c1\"># # triton implementation</span>\n-    <span class=\"n\">tri_out</span> <span class=\"o\">=</span> <span class=\"n\">attention</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">sm_scale</span><span class=\"p\">)</span>\n-    <span class=\"c1\"># print(ref_out)</span>\n-    <span class=\"c1\"># print(tri_out)</span>\n+    <span class=\"c1\"># triton implementation</span>\n+    <span class=\"n\">tri_out</span> <span class=\"o\">=</span> <span class=\"n\">attention</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">causal</span><span class=\"p\">,</span> <span class=\"n\">sm_scale</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">half</span><span class=\"p\">()</span>\n     <span class=\"n\">tri_out</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">(</span><span class=\"n\">dout</span><span class=\"p\">)</span>\n     <span class=\"n\">tri_dv</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">clone</span><span class=\"p\">(),</span> <span class=\"kc\">None</span>\n     <span class=\"n\">tri_dk</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">k</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">clone</span><span class=\"p\">(),</span> <span class=\"kc\">None</span>\n@@ -436,19 +518,19 @@\n <span class=\"c1\"># vary seq length for fixed head and batch=4</span>\n <span class=\"n\">configs</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">Benchmark</span><span class=\"p\">(</span>\n     <span class=\"n\">x_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;N_CTX&#39;</span><span class=\"p\">],</span>\n-    <span class=\"n\">x_vals</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"o\">**</span><span class=\"n\">i</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">14</span><span class=\"p\">)],</span>\n+    <span class=\"n\">x_vals</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"o\">**</span><span class=\"n\">i</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">15</span><span class=\"p\">)],</span>\n     <span class=\"n\">line_arg</span><span class=\"o\">=</span><span class=\"s1\">&#39;provider&#39;</span><span class=\"p\">,</span>\n     <span class=\"n\">line_vals</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;triton&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"p\">([</span><span class=\"s1\">&#39;flash&#39;</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">HAS_FLASH</span> <span class=\"k\">else</span> <span class=\"p\">[]),</span>\n     <span class=\"n\">line_names</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;Triton&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"p\">([</span><span class=\"s1\">&#39;Flash&#39;</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">HAS_FLASH</span> <span class=\"k\">else</span> <span class=\"p\">[]),</span>\n     <span class=\"n\">styles</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s1\">&#39;red&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s1\">&#39;blue&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;-&#39;</span><span class=\"p\">)],</span>\n     <span class=\"n\">ylabel</span><span class=\"o\">=</span><span class=\"s1\">&#39;ms&#39;</span><span class=\"p\">,</span>\n     <span class=\"n\">plot_name</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"s1\">&#39;fused-attention-batch</span><span class=\"si\">{</span><span class=\"n\">BATCH</span><span class=\"si\">}</span><span class=\"s1\">-head</span><span class=\"si\">{</span><span class=\"n\">N_HEADS</span><span class=\"si\">}</span><span class=\"s1\">-d</span><span class=\"si\">{</span><span class=\"n\">D_HEAD</span><span class=\"si\">}</span><span class=\"s1\">-</span><span class=\"si\">{</span><span class=\"n\">mode</span><span class=\"si\">}</span><span class=\"s1\">&#39;</span><span class=\"p\">,</span>\n-    <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;H&#39;</span><span class=\"p\">:</span> <span class=\"n\">N_HEADS</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BATCH&#39;</span><span class=\"p\">:</span> <span class=\"n\">BATCH</span><span class=\"p\">,</span> <span class=\"s1\">&#39;D_HEAD&#39;</span><span class=\"p\">:</span> <span class=\"n\">D_HEAD</span><span class=\"p\">,</span> <span class=\"s1\">&#39;dtype&#39;</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">,</span> <span class=\"s1\">&#39;mode&#39;</span><span class=\"p\">:</span> <span class=\"n\">mode</span><span class=\"p\">}</span>\n-<span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">mode</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"s1\">&#39;fwd&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;bwd&#39;</span><span class=\"p\">]]</span>\n+    <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s1\">&#39;H&#39;</span><span class=\"p\">:</span> <span class=\"n\">N_HEADS</span><span class=\"p\">,</span> <span class=\"s1\">&#39;BATCH&#39;</span><span class=\"p\">:</span> <span class=\"n\">BATCH</span><span class=\"p\">,</span> <span class=\"s1\">&#39;D_HEAD&#39;</span><span class=\"p\">:</span> <span class=\"n\">D_HEAD</span><span class=\"p\">,</span> <span class=\"s1\">&#39;dtype&#39;</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">,</span> <span class=\"s1\">&#39;mode&#39;</span><span class=\"p\">:</span> <span class=\"n\">mode</span><span class=\"p\">,</span> <span class=\"s1\">&#39;causal&#39;</span><span class=\"p\">:</span> <span class=\"n\">causal</span><span class=\"p\">}</span>\n+<span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">mode</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"s1\">&#39;fwd&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;bwd&#39;</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">causal</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">]]</span>\n \n \n <span class=\"nd\">@triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">perf_report</span><span class=\"p\">(</span><span class=\"n\">configs</span><span class=\"p\">)</span>\n-<span class=\"k\">def</span> <span class=\"nf\">bench_flash_attention</span><span class=\"p\">(</span><span class=\"n\">BATCH</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">,</span> <span class=\"n\">mode</span><span class=\"p\">,</span> <span class=\"n\">provider</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">):</span>\n+<span class=\"k\">def</span> <span class=\"nf\">bench_flash_attention</span><span class=\"p\">(</span><span class=\"n\">BATCH</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">,</span> <span class=\"n\">causal</span><span class=\"p\">,</span> <span class=\"n\">mode</span><span class=\"p\">,</span> <span class=\"n\">provider</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float16</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">):</span>\n     <span class=\"k\">assert</span> <span class=\"n\">mode</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"s1\">&#39;fwd&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;bwd&#39;</span><span class=\"p\">]</span>\n     <span class=\"n\">warmup</span> <span class=\"o\">=</span> <span class=\"mi\">25</span>\n     <span class=\"n\">rep</span> <span class=\"o\">=</span> <span class=\"mi\">100</span>\n@@ -457,13 +539,12 @@\n         <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"n\">BATCH</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n         <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">((</span><span class=\"n\">BATCH</span><span class=\"p\">,</span> <span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">D_HEAD</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n         <span class=\"n\">sm_scale</span> <span class=\"o\">=</span> <span class=\"mf\">1.3</span>\n-        <span class=\"n\">fn</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">attention</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">sm_scale</span><span class=\"p\">)</span>\n+        <span class=\"n\">fn</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">attention</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">causal</span><span class=\"p\">,</span> <span class=\"n\">sm_scale</span><span class=\"p\">)</span>\n         <span class=\"k\">if</span> <span class=\"n\">mode</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;bwd&#39;</span><span class=\"p\">:</span>\n             <span class=\"n\">o</span> <span class=\"o\">=</span> <span class=\"n\">fn</span><span class=\"p\">()</span>\n             <span class=\"n\">do</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn_like</span><span class=\"p\">(</span><span class=\"n\">o</span><span class=\"p\">)</span>\n             <span class=\"n\">fn</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">(</span><span class=\"n\">do</span><span class=\"p\">,</span> <span class=\"n\">retain_graph</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n         <span class=\"n\">ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"n\">fn</span><span class=\"p\">,</span> <span class=\"n\">warmup</span><span class=\"o\">=</span><span class=\"n\">warmup</span><span class=\"p\">,</span> <span class=\"n\">rep</span><span class=\"o\">=</span><span class=\"n\">rep</span><span class=\"p\">)</span>\n-        <span class=\"k\">return</span> <span class=\"n\">ms</span>\n     <span class=\"k\">if</span> <span class=\"n\">provider</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;flash&quot;</span><span class=\"p\">:</span>\n         <span class=\"n\">lengths</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"n\">BATCH</span><span class=\"p\">,),</span> <span class=\"n\">fill_value</span><span class=\"o\">=</span><span class=\"n\">N_CTX</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">)</span>\n         <span class=\"n\">cu_seqlens</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">BATCH</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">,),</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">)</span>\n@@ -475,14 +556,20 @@\n             <span class=\"n\">do</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn_like</span><span class=\"p\">(</span><span class=\"n\">o</span><span class=\"p\">)</span>\n             <span class=\"n\">fn</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">(</span><span class=\"n\">do</span><span class=\"p\">,</span> <span class=\"n\">retain_graph</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n         <span class=\"n\">ms</span> <span class=\"o\">=</span> <span class=\"n\">triton</span><span class=\"o\">.</span><span class=\"n\">testing</span><span class=\"o\">.</span><span class=\"n\">do_bench</span><span class=\"p\">(</span><span class=\"n\">fn</span><span class=\"p\">,</span> <span class=\"n\">warmup</span><span class=\"o\">=</span><span class=\"n\">warmup</span><span class=\"p\">,</span> <span class=\"n\">rep</span><span class=\"o\">=</span><span class=\"n\">rep</span><span class=\"p\">)</span>\n-        <span class=\"k\">return</span> <span class=\"n\">ms</span>\n+    <span class=\"n\">flops_per_matmul</span> <span class=\"o\">=</span> <span class=\"mf\">2.</span> <span class=\"o\">*</span> <span class=\"n\">BATCH</span> <span class=\"o\">*</span> <span class=\"n\">H</span> <span class=\"o\">*</span> <span class=\"n\">N_CTX</span> <span class=\"o\">*</span> <span class=\"n\">N_CTX</span> <span class=\"o\">*</span> <span class=\"n\">D_HEAD</span>\n+    <span class=\"n\">total_flops</span> <span class=\"o\">=</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">flops_per_matmul</span>\n+    <span class=\"k\">if</span> <span class=\"n\">causal</span><span class=\"p\">:</span>\n+        <span class=\"n\">total_flops</span> <span class=\"o\">*=</span> <span class=\"mf\">0.5</span>\n+    <span class=\"k\">if</span> <span class=\"n\">mode</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;bwd&#39;</span><span class=\"p\">:</span>\n+        <span class=\"n\">total_flops</span> <span class=\"o\">*=</span> <span class=\"mf\">2.5</span>  <span class=\"c1\"># 2.0(bwd) + 0.5(recompute)</span>\n+    <span class=\"k\">return</span> <span class=\"n\">total_flops</span> <span class=\"o\">/</span> <span class=\"n\">ms</span> <span class=\"o\">*</span> <span class=\"mf\">1e-9</span>\n \n \n <span class=\"c1\"># only works on post-Ampere GPUs right now</span>\n <span class=\"n\">bench_flash_attention</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;.&#39;</span><span class=\"p\">,</span> <span class=\"n\">print_data</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  4.671 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  16.492 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-06-fused-attention-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/54a35f6ec55f9746935b9566fb6bb1df/06-fused-attention.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">06-fused-attention.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/07-math-functions.html", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -186,7 +186,7 @@ <h2>Customize the libdevice library path<a class=\"headerlink\" href=\"#customize-t\n The maximum difference between torch and triton is 2.384185791015625e-07\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.217 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  0.219 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-07-math-functions-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/77571465f7f4bd281d3a847dc2633146/07-math-functions.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">07-math-functions.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/08-experimental-block-pointer.html", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -335,7 +335,7 @@ <h2>Unit Test<a class=\"headerlink\" href=\"#unit-test\" title=\"Permalink to this he\n \u2705 Triton and Torch match\n </pre></div>\n </div>\n-<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  6.374 seconds)</p>\n+<p class=\"sphx-glr-timing\"><strong>Total running time of the script:</strong> ( 0 minutes  6.438 seconds)</p>\n <div class=\"sphx-glr-footer sphx-glr-footer-example docutils container\" id=\"sphx-glr-download-getting-started-tutorials-08-experimental-block-pointer-py\">\n <div class=\"sphx-glr-download sphx-glr-download-python docutils container\">\n <p><a class=\"reference download internal\" download=\"\" href=\"../../_downloads/4d6052117d61c2ca779cd4b75567fee5/08-experimental-block-pointer.py\"><code class=\"xref download docutils literal notranslate\"><span class=\"pre\">Download</span> <span class=\"pre\">Python</span> <span class=\"pre\">source</span> <span class=\"pre\">code:</span> <span class=\"pre\">08-experimental-block-pointer.py</span></code></a></p>"}, {"filename": "main/getting-started/tutorials/sg_execution_times.html", "status": "modified", "additions": 11, "deletions": 11, "changes": 22, "file_content_changes": "@@ -86,39 +86,39 @@\n              \n   <section id=\"computation-times\">\n <span id=\"sphx-glr-getting-started-tutorials-sg-execution-times\"></span><h1>Computation times<a class=\"headerlink\" href=\"#computation-times\" title=\"Permalink to this heading\">\u00b6</a></h1>\n-<p><strong>02:07.710</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n+<p><strong>02:17.525</strong> total execution time for <strong>getting-started_tutorials</strong> files:</p>\n <table class=\"docutils align-default\">\n <tbody>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"03-matrix-multiplication.html#sphx-glr-getting-started-tutorials-03-matrix-multiplication-py\"><span class=\"std std-ref\">Matrix Multiplication</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">03-matrix-multiplication.py</span></code>)</p></td>\n-<td><p>00:40.861</p></td>\n+<td><p>00:40.844</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py\"><span class=\"std std-ref\">Fused Softmax</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">02-fused-softmax.py</span></code>)</p></td>\n-<td><p>00:39.007</p></td>\n+<td><p>00:37.989</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"05-layer-norm.html#sphx-glr-getting-started-tutorials-05-layer-norm-py\"><span class=\"std std-ref\">Layer Normalization</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">05-layer-norm.py</span></code>)</p></td>\n-<td><p>00:29.168</p></td>\n+<td><p>00:29.195</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n-<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py\"><span class=\"std std-ref\">Vector Addition</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">01-vector-add.py</span></code>)</p></td>\n-<td><p>00:06.769</p></td>\n+<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"06-fused-attention.html#sphx-glr-getting-started-tutorials-06-fused-attention-py\"><span class=\"std std-ref\">Fused Attention</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">06-fused-attention.py</span></code>)</p></td>\n+<td><p>00:16.492</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"08-experimental-block-pointer.html#sphx-glr-getting-started-tutorials-08-experimental-block-pointer-py\"><span class=\"std std-ref\">Block Pointer (Experimental)</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">08-experimental-block-pointer.py</span></code>)</p></td>\n-<td><p>00:06.374</p></td>\n+<td><p>00:06.438</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n-<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"06-fused-attention.html#sphx-glr-getting-started-tutorials-06-fused-attention-py\"><span class=\"std std-ref\">Fused Attention</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">06-fused-attention.py</span></code>)</p></td>\n-<td><p>00:04.671</p></td>\n+<tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py\"><span class=\"std std-ref\">Vector Addition</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">01-vector-add.py</span></code>)</p></td>\n+<td><p>00:05.717</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-odd\"><td><p><a class=\"reference internal\" href=\"04-low-memory-dropout.html#sphx-glr-getting-started-tutorials-04-low-memory-dropout-py\"><span class=\"std std-ref\">Low-Memory Dropout</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">04-low-memory-dropout.py</span></code>)</p></td>\n-<td><p>00:00.643</p></td>\n+<td><p>00:00.630</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n <tr class=\"row-even\"><td><p><a class=\"reference internal\" href=\"07-math-functions.html#sphx-glr-getting-started-tutorials-07-math-functions-py\"><span class=\"std std-ref\">Libdevice (tl.math) function</span></a> (<code class=\"docutils literal notranslate\"><span class=\"pre\">07-math-functions.py</span></code>)</p></td>\n-<td><p>00:00.217</p></td>\n+<td><p>00:00.219</p></td>\n <td><p>0.0 MB</p></td>\n </tr>\n </tbody>"}, {"filename": "main/searchindex.js", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "N/A"}]