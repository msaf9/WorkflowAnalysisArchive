[{"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 20, "deletions": 9, "changes": 29, "file_content_changes": "@@ -338,10 +338,15 @@ def test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\n \n \n try:\n-    from flash_attn.flash_attn_interface import flash_attn_func\n-    HAS_FLASH = True\n+    from flash_attn.flash_attn_interface import flash_attn_qkvpacked_func as flash_attn_func\n+    FLASH_VER = 2\n except BaseException:\n-    HAS_FLASH = False\n+    try:\n+        from flash_attn.flash_attn_interface import flash_attn_func\n+        FLASH_VER = 1\n+    except BaseException:\n+        FLASH_VER = None\n+HAS_FLASH = FLASH_VER is not None\n \n BATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\n # vary seq length for fixed head and batch=4\n@@ -350,7 +355,7 @@ def test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\n     x_vals=[2**i for i in range(10, 15)],\n     line_arg='provider',\n     line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n-    line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\n+    line_names=['Triton'] + ([f'Flash-{FLASH_VER}'] if HAS_FLASH else []),\n     styles=[('red', '-'), ('blue', '-')],\n     ylabel='ms',\n     plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n@@ -375,11 +380,17 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, causal, mode, provider, dtype\n             fn = lambda: o.backward(do, retain_graph=True)\n         ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n     if provider == \"flash\":\n-        lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n-        cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n-        cu_seqlens[1:] = lengths.cumsum(0)\n-        qkv = torch.randn((BATCH * N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\n-        fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=causal)\n+        qkv = torch.randn((BATCH, N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\n+        if FLASH_VER == 1:\n+            lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n+            cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n+            cu_seqlens[1:] = lengths.cumsum(0)\n+            qkv = qkv.reshape(BATCH * N_CTX, 3, H, D_HEAD)\n+            fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=causal)\n+        elif FLASH_VER == 2:\n+            fn = lambda: flash_attn_func(qkv, causal=causal)\n+        else:\n+            raise ValueError(f'unknown {FLASH_VER = }')\n         if mode == 'bwd':\n             o = fn()\n             do = torch.randn_like(o)"}]