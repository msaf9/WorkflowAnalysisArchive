[{"filename": "include/triton/Dialect/TritonGPU/IR/Attributes.h", "status": "added", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -0,0 +1,7 @@\n+#ifndef TRITON_DIALECT_TRITONGPU_IR_ATTRIBUTES_H_\n+#define TRITON_DIALECT_TRITONGPU_IR_ATTRIBUTES_H_\n+\n+#define GET_ATTRDEF_CLASSES\n+#include \"triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.h.inc\"\n+\n+#endif // TRITON_DIALECT_TRITONGPU_IR_ATTRIBUTES_H_"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "file_content_changes": "@@ -8,12 +8,10 @@\n \n // TritonGPU depends on Triton\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Attributes.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h.inc\"\n #include \"triton/Dialect/TritonGPU/IR/Traits.h\"\n \n-#define GET_ATTRDEF_CLASSES\n-#include \"triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.h.inc\"\n-\n #define GET_OP_CLASSES\n #include \"triton/Dialect/TritonGPU/IR/Ops.h.inc\"\n \n@@ -82,9 +80,10 @@ bool isaDistributedLayout(Attribute layout);\n \n bool isSharedEncoding(Value value);\n \n+bool isExpensiveCat(CatOp cat, Attribute &targetEncoding);\n+\n } // namespace gpu\n } // namespace triton\n-\n } // namespace mlir\n \n #endif // TRITON_DIALECT_TRITONGPU_IR_DIALECT_H_"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -18,8 +18,6 @@ LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n \n bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding);\n \n-bool isExpensiveCat(triton::CatOp cat, Attribute &targetEncoding);\n-\n bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding);\n \n // skipInit is True when we only consider the operands of the initOp but"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -6,7 +6,7 @@\n #include \"mlir/IR/OperationSupport.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/Triton/IR/Types.h\"\n-#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Attributes.h\"\n \n namespace mlir {\n namespace triton {"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 13, "deletions": 2, "changes": 15, "file_content_changes": "@@ -7,7 +7,6 @@\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.cpp.inc\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n-#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include \"llvm/ADT/TypeSwitch.h\"\n \n using namespace mlir;\n@@ -368,9 +367,21 @@ bool isSharedEncoding(Value value) {\n   return false;\n }\n \n+bool isExpensiveCat(CatOp cat, Attribute &targetEncoding) {\n+  // If the new elements per thread is less than the old one, we will need to do\n+  // convert encoding that goes through shared memory anyway. So we consider it\n+  // as expensive.\n+  auto tensorTy = cat.getResult().getType().cast<RankedTensorType>();\n+  auto totalElemsPerThread = gpu::getTotalElemsPerThread(tensorTy);\n+  auto shape = tensorTy.getShape();\n+  auto elemTy = tensorTy.getElementType();\n+  auto newTotalElemsPerThread =\n+      gpu::getTotalElemsPerThread(targetEncoding, shape, elemTy);\n+  return newTotalElemsPerThread < totalElemsPerThread;\n+}\n+\n } // namespace gpu\n } // namespace triton\n-\n } // namespace mlir\n \n static LogicalResult parseIntAttrValue(AsmParser &parser, Attribute attr,"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 3, "deletions": 15, "changes": 18, "file_content_changes": "@@ -104,26 +104,13 @@ bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n   return true;\n }\n \n-bool isExpensiveCat(triton::CatOp cat, Attribute &targetEncoding) {\n-  // If the new elements per thread is less than the old one, we will need to do\n-  // convert encoding that goes through shared memory anyway. So we consider it\n-  // as expensive.\n-  auto tensorTy = cat.getResult().getType().cast<RankedTensorType>();\n-  auto totalElemsPerThread = triton::gpu::getTotalElemsPerThread(tensorTy);\n-  auto shape = tensorTy.getShape();\n-  auto elemTy = tensorTy.getElementType();\n-  auto newTotalElemsPerThread =\n-      triton::gpu::getTotalElemsPerThread(targetEncoding, shape, elemTy);\n-  return newTotalElemsPerThread < totalElemsPerThread;\n-}\n-\n bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding) {\n   if (!op)\n     return true;\n   if (isa<triton::LoadOp, triton::StoreOp>(op))\n     return isExpensiveLoadOrStore(op, targetEncoding);\n   if (isa<triton::CatOp>(op))\n-    return isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n+    return triton::gpu::isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n   if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n           triton::gpu::InsertSliceAsyncOp, triton::AtomicRMWOp,\n           triton::AtomicCASOp, triton::DotOp>(op))\n@@ -136,7 +123,8 @@ bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding) {\n \n bool canFoldConversion(Operation *op, Attribute &targetEncoding) {\n   if (isa<triton::CatOp>(op))\n-    return !isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n+    return !triton::gpu::isExpensiveCat(cast<triton::CatOp>(op),\n+                                        targetEncoding);\n   return isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n              triton::MakeRangeOp, triton::SplatOp, triton::ViewOp>(op);\n }"}]