[{"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -44,7 +44,6 @@ def nvsmi(attrs):\n         (512, 512, 512): {'float16': 0.061, 'float32': 0.097, 'int8': 0.05},\n         (1024, 1024, 1024): {'float16': 0.283, 'float32': 0.313, 'int8': 0.169},\n         (2048, 2048, 2048): {'float16': 0.618, 'float32': 0.532, 'int8': 0.34},\n-        (4096, 4096, 4096): {'float16': 0.751, 'float32': 0.726, 'int8': 0.46},\n         (8192, 8192, 8192): {'float16': 0.786, 'float32': 0.754, 'int8': 0.51},\n         # tall-skinny\n         (16, 1024, 1024): {'float16': 0.006, 'float32': 0.009, 'int8': 0.005},\n@@ -56,6 +55,8 @@ def nvsmi(attrs):\n         (1024, 64, 1024): {'float16': 0.029, 'float32': 0.046, 'int8': 0.017},\n         (4096, 64, 4096): {'float16': 0.179, 'float32': 0.214, 'int8': 0.102},\n         (8192, 64, 8192): {'float16': 0.278, 'float32': 0.000, 'int8': 0.177},\n+        # test EVEN_K==False\n+        (8192, 8192, 8176): {'float16': 0.786, 'float32': 0.696, 'int8': 0.51},\n     }\n }\n \n@@ -115,7 +116,6 @@ def _add(x_ptr, y_ptr, output_ptr, n_elements,\n         1024 * 64: {'float16': 0.013, 'float32': 0.026},\n         1024 * 256: {'float16': 0.053, 'float32': 0.105},\n         1024 * 1024: {'float16': 0.212, 'float32': 0.420},\n-        1024 * 4096: {'float16': 0.791, 'float32': 0.668},\n         1024 * 16384: {'float16': 0.762, 'float32': 0.812},\n         1024 * 65536: {'float16': 0.846, 'float32': 0.869},\n         # Non pow 2\n@@ -162,7 +162,7 @@ def test_elementwise(N, dtype_str):\n         (4, 48, 4096, 64, True, True, 'backward', 'bfloat16'): 0.202,\n         (4, 48, 1024, 16, True, True, 'backward', 'float32'): 0.089,\n         (4, 48, 4096, 64, True, False, 'forward', 'float16'): 0.242,\n-        (4, 48, 4096, 64, True, False, 'forward', 'bfloat16'): 0.248,\n+        (4, 48, 4096, 64, True, False, 'forward', 'bfloat16'): 0.220,\n         (4, 48, 1024, 16, True, False, 'forward', 'float32'): 0.069,\n         (4, 48, 4096, 64, True, False, 'backward', 'float16'): 0.136,\n         (4, 48, 4096, 64, True, False, 'backward', 'bfloat16'): 0.135,\n@@ -173,8 +173,8 @@ def test_elementwise(N, dtype_str):\n         (4, 48, 4096, 64, False, True, 'backward', 'float16'): 0.265,\n         (4, 48, 4096, 64, False, True, 'backward', 'bfloat16'): 0.257,\n         (4, 48, 1024, 16, False, True, 'backward', 'float32'): 0.128,\n-        (4, 48, 4096, 64, False, False, 'forward', 'float16'): 0.242,\n-        (4, 48, 4096, 64, False, False, 'forward', 'bfloat16'): 0.248,\n+        (4, 48, 4096, 64, False, False, 'forward', 'float16'): 0.251,\n+        (4, 48, 4096, 64, False, False, 'forward', 'bfloat16'): 0.220,\n         (4, 48, 1024, 16, False, False, 'forward', 'float32'): 0.069,\n         (4, 48, 4096, 64, False, False, 'backward', 'float16'): 0.159,\n         (4, 48, 4096, 64, False, False, 'backward', 'bfloat16'): 0.138,"}, {"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "file_content_changes": "@@ -94,11 +94,14 @@ def _fwd_kernel(\n     # load q: it will stay in SRAM throughout\n     q = tl.load(Q_block_ptr)\n     q = (q * qk_scale).to(K.dtype.element_ty)\n+    # advance block pointers to first iteration of the loop\n+    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n+    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n     # loop over k, v and update accumulator\n     for start_n in range(lo, hi, BLOCK_N):\n         start_n = tl.multiple_of(start_n, BLOCK_N)\n         # -- compute qk ----\n-        k = tl.load(tl.advance(K_block_ptr, (0, start_n)))\n+        k = tl.load(K_block_ptr)\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n         qk += tl.dot(q, k, allow_tf32=True)\n         if MODE == 1 or MODE == 3:\n@@ -120,12 +123,15 @@ def _fwd_kernel(\n         acc_scale = l_i / l_i_new\n         acc = acc * acc_scale[:, None]\n         # update acc\n-        v = tl.load(tl.advance(V_block_ptr, (start_n, 0)))\n+        v = tl.load(V_block_ptr)\n         p = p.to(V.dtype.element_ty)\n         acc += tl.dot(p, v, allow_tf32=True)\n         # update m_i and l_i\n         l_i = l_i_new\n         m_i = m_i_new\n+        # update pointers\n+        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n+        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n     # write back l and m\n     l_ptrs = L + off_hz * N_CTX + offs_m\n     m_ptrs = M + off_hz * N_CTX + offs_m"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "file_content_changes": "@@ -93,11 +93,14 @@ def _fwd_kernel(\n     # load q: it will stay in SRAM throughout\n     q = tl.load(Q_block_ptr)\n     q = (q * qk_scale).to(tl.float16)\n+    # advance block pointers to first iteration of the loop\n+    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n+    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n     # loop over k, v and update accumulator\n     for start_n in range(lo, hi, BLOCK_N):\n         start_n = tl.multiple_of(start_n, BLOCK_N)\n         # -- compute qk ----\n-        k = tl.load(tl.advance(K_block_ptr, (0, start_n)))\n+        k = tl.load(K_block_ptr)\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n         qk += tl.dot(q, k)\n         if MODE == 1 or MODE == 3:\n@@ -119,12 +122,15 @@ def _fwd_kernel(\n         acc_scale = l_i / l_i_new\n         acc = acc * acc_scale[:, None]\n         # update acc\n-        v = tl.load(tl.advance(V_block_ptr, (start_n, 0)))\n+        v = tl.load(V_block_ptr)\n         p = p.to(tl.float16)\n         acc += tl.dot(p, v)\n         # update m_i and l_i\n         l_i = l_i_new\n         m_i = m_i_new\n+        # update pointers\n+        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n+        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n     # write back l and m\n     l_ptrs = L + off_hz * N_CTX + offs_m\n     m_ptrs = M + off_hz * N_CTX + offs_m"}]