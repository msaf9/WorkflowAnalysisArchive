[{"filename": "include/triton/codegen/selection/generator.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -148,6 +148,8 @@ class generator: public ir::visitor, public analysis::layout_visitor {\n   std::tuple<Value*, Value*, Value*, Value*> fp32x4_to_fp8x4(Value *in0, Value *in1, Value *in2, Value *in3);\n   std::tuple<Value*, Value*, Value*, Value*> fp8x4_to_fp16x4(Value *in0, Value *in1, Value *in2, Value *in3);\n   std::tuple<Value*, Value*, Value*, Value*> fp16x4_to_fp8x4(Value *in0, Value *in1, Value *in2, Value *in3);\n+  std::tuple<Value*, Value*, Value*, Value*> fp8x4_to_bf16x4(Value *in0, Value *in1, Value *in2, Value *in3);\n+  std::tuple<Value*, Value*, Value*, Value*> bf16x4_to_fp8x4(Value *in0, Value *in1, Value *in2, Value *in3);\n   Value* bf16_to_fp32(Value *in0);\n   Value* fp32_to_bf16(Value *in0);\n "}, {"filename": "lib/codegen/selection/generator.cc", "status": "modified", "additions": 113, "deletions": 4, "changes": 117, "file_content_changes": "@@ -569,10 +569,10 @@ std::tuple<Value*, Value*, Value*, Value*> generator::fp8x4_to_fp16x4(Value *in0\n   \"prmt.b32 a1, 0, $2, 0x7060;            \\n\\t\" // If input is 0xdcba set a1 to 0xd0c0\n   \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\\t\" // b0 = a0 & 0x7fff7fff (strip sign)\n   \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\\t\" // b1 = a1 & 0x7fff7fff (strip sign)\n-  \"shr.b32  b0, b0, 1;                    \\n\\t\" // b0 <<= 1 (shift into fp16 poistion)\n-  \"shr.b32  b1, b1, 1;                    \\n\\t\" // b1 <<= 1 (shift into fp16 position)\n-  \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\\t\" // out0 = b0 | (0x80008000 | a0) (restore sign)\n-  \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\\t\" // out1 = b1 | (0x80008000 | a1) (restore sign)\n+  \"shr.b32  b0, b0, 1;                    \\n\\t\" // b0 >>= 1 (shift into fp16 poistion)\n+  \"shr.b32  b1, b1, 1;                    \\n\\t\" // b1 >>= 1 (shift into fp16 position)\n+  \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\\t\" // out0 = b0 | (0x80008000 & a0) (restore sign)\n+  \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\\t\" // out1 = b1 | (0x80008000 & a1) (restore sign)\n   \"}\", \"=r,=r,r\", false);\n   Value *packed_in = UndefValue::get(vec_ty(i8_ty, 4));\n   packed_in = insert_elt(packed_in, in0, (uint64_t)0);\n@@ -635,6 +635,110 @@ std::tuple<Value*, Value*, Value*, Value*> generator::fp16x4_to_fp8x4(Value *in0\n   return std::make_tuple(ret0, ret1, ret2, ret3);\n }\n \n+std::tuple<Value*, Value*, Value*, Value*> generator::fp8x4_to_bf16x4(Value *in0, Value *in1, Value *in2, Value *in3) {\n+  // current exp offset: 15\n+  // Add 112 (127-15) to compensate the difference in exponent bias\n+  // bf16 = (nosign >> (8-4) + 112 << 7) | sign;\n+  // bf16 = (nosign >> 4 + 0x3800) | sign;\n+  Type *ret_ty = StructType::get(*ctx_, {vec_ty(bf16_ty, 2), vec_ty(bf16_ty, 2)});\n+  InlineAsm *ptx = InlineAsm::get(FunctionType::get(ret_ty, {i32_ty}, false),\n+  \"{\"\n+  \".reg .b32 a<2>, sign<2>, nosign<2>, b<2>; \\n\\t\"\n+  \"prmt.b32 a0, 0, $2, 0x5040; \\n\\t\" // 0xdcba => 0xb0a0\n+  \"prmt.b32 a1, 0, $2, 0x7060; \\n\\t\" // 0xdcba => 0xd0c0\n+  \"and.b32 sign0, a0, 0x80008000; \\n\\t\"\n+  \"and.b32 sign1, a1, 0x80008000; \\n\\t\"\n+  \"and.b32 nosign0, a0, 0x7fff7fff; \\n\\t\"\n+  \"and.b32 nosign1, a1, 0x7fff7fff; \\n\\t\"\n+  \"shr.b32 nosign0, nosign0, 4; \\n\\t\"\n+  \"shr.b32 nosign1, nosign1, 4; \\n\\t\"\n+  \"add.u32 nosign0, nosign0, 0x38003800; \\n\\t\"\n+  \"add.u32 nosign1, nosign1, 0x38003800; \\n\\t\"\n+  \"or.b32 $0, sign0, nosign0; \\n\\t\"\n+  \"or.b32 $1, sign1, nosign1; \\n\\t\"\n+  \"}\", \"=r,=r,r\", false);\n+  Value *packed_in = UndefValue::get(vec_ty(i8_ty, 4));\n+  packed_in = insert_elt(packed_in, in0, (uint64_t)0);\n+  packed_in = insert_elt(packed_in, in1, (uint64_t)1);\n+  packed_in = insert_elt(packed_in, in2, (uint64_t)2);\n+  packed_in = insert_elt(packed_in, in3, (uint64_t)3);\n+  Value *in = bit_cast(packed_in, i32_ty);\n+  Value *ret = call(ptx, {in});\n+  Value *packed_ret0 = extract_val(ret, {0});\n+  Value *packed_ret1 = extract_val(ret, {1});\n+  Value *ret0 = extract_elt(packed_ret0, (uint64_t)0);\n+  Value *ret1 = extract_elt(packed_ret0, (uint64_t)1);\n+  Value *ret2 = extract_elt(packed_ret1, (uint64_t)0);\n+  Value *ret3 = extract_elt(packed_ret1, (uint64_t)1);\n+  return std::make_tuple(ret0, ret1, ret2, ret3);\n+}\n+\n+std::tuple<Value*, Value*, Value*, Value*> generator::bf16x4_to_fp8x4(Value *in0, Value *in1, Value *in2, Value *in3) {\n+  /* Assuming fp8 exponent offset is 16. bf16 exponent offset is 127.\n+     Max value in fp8: 0b01111111 (0x7f),\n+                  bf16: 3ff0\n+     Min value in fp8: 0b00000000 (0x00)\n+                  bf16: 0x3c00\n+     // @note: +0x8 is for \"rounding to nearest zero\"\n+     fp8 = (nosign(bf16) - (112 << 7) + 0x8) << 4;\n+     return fp8 | sign;  // also permute bytes\n+  */\n+  InlineAsm *ptx = InlineAsm::get(FunctionType::get({vec_ty(i8_ty, 4)}, {i32_ty, i32_ty}, false),\n+  \"{\\n\\t\"\n+  \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\\t\"\n+  \".reg .u32 fp8_min, fp8_max, rn_, zero; \\n\\t\"\n+  \"mov.u32 fp8_min, 0x38003800; \\n\\t\"\n+  \"mov.u32 fp8_max, 0x3ff03ff0; \\n\\t\"\n+  \"mov.u32 rn_, 0x80008; \\n\\t\"\n+  \"mov.u32 zero, 0; \\n\\t\"\n+  \"and.b32 sign0, $1, 0x80008000;  \\n\\t\"\n+  \"and.b32 sign1, $2, 0x80008000;  \\n\\t\"\n+  \"prmt.b32 sign, sign0, sign1, 0x7531; \\n\\t\"\n+  \"and.b32 nosign0, $1, 0x7fff7fff; \\n\\t\"\n+  \"and.b32 nosign1, $2, 0x7fff7fff; \\n\\t\"\n+\n+  \".reg .u32 nosign_0_<2>, nosign_1_<2>; \\n\\t\"  // nosign = clamp(nosign, min, max)\n+  \"and.b32 nosign_0_0, nosign0, 0xffff0000; \\n\\t\"\n+  \"max.u32 nosign_0_0, nosign_0_0, 0x38000000; \\n\\t\"\n+  \"min.u32 nosign_0_0, nosign_0_0, 0x3ff00000; \\n\\t\"\n+  \"and.b32 nosign_0_1, nosign0, 0x0000ffff; \\n\\t\"\n+  \"max.u32 nosign_0_1, nosign_0_1, 0x3800; \\n\\t\"\n+  \"min.u32 nosign_0_1, nosign_0_1, 0x3ff0; \\n\\t\"\n+  \"or.b32 nosign0, nosign_0_0, nosign_0_1; \\n\\t\"\n+  \"and.b32 nosign_1_0, nosign1, 0xffff0000; \\n\\t\"\n+  \"max.u32 nosign_1_0, nosign_1_0, 0x38000000; \\n\\t\"\n+  \"min.u32 nosign_1_0, nosign_1_0, 0x3ff00000; \\n\\t\"\n+  \"and.b32 nosign_1_1, nosign1, 0x0000ffff; \\n\\t\"\n+  \"max.u32 nosign_1_1, nosign_1_1, 0x3800; \\n\\t\"\n+  \"min.u32 nosign_1_1, nosign_1_1, 0x3ff0; \\n\\t\"\n+  \"or.b32 nosign1, nosign_1_0, nosign_1_1; \\n\\t\"\n+\n+  \"add.u32 nosign0, nosign0, rn_; \\n\\t\"  // round to nearest zero\n+  \"add.u32 nosign1, nosign1, rn_; \\n\\t\"\n+  \"sub.u32 nosign0, nosign0, 0x38003800; \\n\\t\"  // compensate offset\n+  \"sub.u32 nosign1, nosign1, 0x38003800; \\n\\t\"\n+  \"shr.u32 nosign0, nosign0, 4; \\n\\t\"\n+  \"shr.u32 nosign1, nosign1, 4; \\n\\t\"\n+  \"prmt.b32 nosign, nosign0, nosign1, 0x6420; \\n\\t\"\n+  \"or.b32 $0, nosign, sign; \\n\\t\"\n+  \"\"\n+  \"}\", \"=r,r,r\", false);\n+  Value *packed_in0 = UndefValue::get(vec_ty(bf16_ty, 2));\n+  Value *packed_in1 = UndefValue::get(vec_ty(bf16_ty, 2));\n+  packed_in0 = insert_elt(packed_in0, in0, (int)0);\n+  packed_in0 = insert_elt(packed_in0, in1, (int)1);\n+  packed_in1 = insert_elt(packed_in1, in2, (int)0);\n+  packed_in1 = insert_elt(packed_in1, in3, (int)1);\n+  Value *in_arg0 = bit_cast(packed_in0, i32_ty);\n+  Value *in_arg1 = bit_cast(packed_in1, i32_ty);\n+  Value *ret = call(ptx, {in_arg0, in_arg1});\n+  Value *ret0 = extract_elt(ret, (int)0);\n+  Value *ret1 = extract_elt(ret, (int)1);\n+  Value *ret2 = extract_elt(ret, (int)2);\n+  Value *ret3 = extract_elt(ret, (int)3);\n+  return std::make_tuple(ret0, ret1, ret2, ret3);\n+}\n+\n Value* generator::bf16_to_fp32(Value *in0){\n   if (tgt_->as_nvidia()->sm() >= 80) {\n     InlineAsm *ptx = InlineAsm::get(FunctionType::get(f32_ty, {bf16_ty}, false),\n@@ -685,6 +789,11 @@ void generator::visit_cast_inst(ir::cast_inst* x) {\n         return fp8x4_to_fp16x4(a, b, c, d);\n       if(op_sca_ty->is_fp8_ty() && ret_sca_ty->is_fp32_ty())\n         return fp8x4_to_fp32x4(a, b, c, d);\n+      // fp8 <> bf16\n+      if(op_sca_ty->is_fp8_ty() && ret_sca_ty->is_bf16_ty())\n+        return fp8x4_to_bf16x4(a, b, c, d);\n+      if (op_sca_ty->is_bf16_ty() && ret_sca_ty->is_fp8_ty())\n+        return bf16x4_to_fp8x4(a, b, c, d);\n       throw std::runtime_error(\"unsupported conversion\");\n     };\n     for(size_t i = 0; i < x_idxs.size(); i+=4){"}, {"filename": "lib/codegen/transform/membar.cc", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -36,6 +36,9 @@ int membar::group_of(ir::value* v, std::vector<ir::value*> &async_write) {\n   else{\n     if(layouts_->has_tmp(v))\n       return async_write.size() - 1;\n+    // // Ignore copy_to_shared. It won't modify async behavior.\n+    // if(dynamic_cast<ir::copy_to_shared_inst*>(v))\n+    //   return 0;\n     auto it = std::find(async_write.begin(), async_write.end(), v);\n     return std::distance(async_write.begin(), it);\n   }"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 8, "deletions": 6, "changes": 14, "file_content_changes": "@@ -719,8 +719,11 @@ def kernel(X, Z, BITCAST: tl.constexpr):\n         assert to_numpy(z_tri) == z_ref\n \n \n-def test_f8_f16_roundtrip():\n+@pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16])\n+def test_f8_xf16_roundtrip(dtype):\n     \"\"\"Tests that converting an f8 to f16 and back to f8 doesn't change its value\"\"\"\n+    check_type_supported(dtype)\n+\n     @triton.jit\n     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n@@ -732,21 +735,20 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device='cuda')\n     f8 = triton.reinterpret(f8_tensor, tl.float8)\n     n_elements = f8_tensor.numel()\n-    f16 = torch.empty_like(f8_tensor, dtype=torch.float16)\n+    xf16 = torch.empty_like(f8_tensor, dtype=dtype)\n     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n-    copy_kernel[grid](f8, f16, n_elements, BLOCK_SIZE=1024)\n+    copy_kernel[grid](f8, xf16, n_elements, BLOCK_SIZE=1024)\n \n-    f8_output_tensor = torch.empty_like(f16, dtype=torch.int8)\n+    f8_output_tensor = torch.empty_like(xf16, dtype=torch.int8)\n     f8_output = triton.reinterpret(f8_output_tensor, tl.float8)\n-    copy_kernel[grid](f16, f8_output, n_elements, BLOCK_SIZE=1024)\n+    copy_kernel[grid](xf16, f8_output, n_elements, BLOCK_SIZE=1024)\n \n     assert torch.all(f8_tensor == f8_output_tensor)\n \n \n def test_f16_to_f8_rounding():\n     \"\"\"Takes all float16s, converts them to float8 and back to float16. Checks that the absolute\n     error is the minimum over all float8.\n-\n     Or the same explanation a bit mathier:\n     for all f16 |f16 - fromf8(tof8(f16))| == min over all f8 |f16 - fromf8(f8)|\"\"\"\n     @triton.jit"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -581,6 +581,13 @@ def cast(input: tl.tensor,\n         return input\n     src_sca_ty = src_ty.scalar\n     dst_sca_ty = dst_ty.scalar\n+    # fp8 <=> bf16/fp16\n+    if (src_sca_ty.is_bf16() or src_sca_ty.is_fp16()) and dst_sca_ty.is_fp8():\n+        return tl.tensor(builder.create_fp_trunc(input.handle, dst_ty.to_ir(builder)),\n+                         dst_ty)\n+    if src_sca_ty.is_fp8() and (dst_sca_ty.is_bf16() or dst_sca_ty.is_fp16()):\n+        return tl.tensor(builder.create_fp_ext(input.handle, dst_ty.to_ir(builder)),\n+                         dst_ty)\n     # bf16 <=> (not fp32)\n     if (src_sca_ty.is_bf16() and not dst_sca_ty.is_fp32()) or \\\n        (dst_sca_ty.is_bf16() and not src_sca_ty.is_fp32()):"}]