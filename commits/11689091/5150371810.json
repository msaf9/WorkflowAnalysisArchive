[{"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 481, "deletions": 379, "changes": 860, "file_content_changes": "@@ -11,10 +11,68 @@\n #include \"llvm/ADT/MapVector.h\"\n \n //===----------------------------------------------------------------------===//\n+// This file implements software pipelining for loops. The implementation here\n+// is inspired by the pipeline pass in Triton (version 2.0) and SCF's\n+// LoopPipelining.\n //\n-// This file implements loop software pipelining\n-// The implementation here is inspired by the pipeline pass in Triton (-v2.0)\n-// and SCF's LoopPipelining.\n+// We divide the loop body into the following phases:\n+// a. Pre-load operations: for instance, index computation.\n+// b. Load operations: loading from global memory to shared memory.\n+// c. Compute operations: for instance, Triton dot.\n+// d. Post-load operations: for instance, index computation.\n+//\n+// To pipeline the loop, we need to:\n+// - Hoist the pipelinable load operations for the first numStages-1 iterations\n+// to the loop pre-header\n+// - Find all the dependencies of the load operations.\n+// - Rematerialize the dependencies for their values at the first numStage-1\n+// iterations\n+// - Assemble the loop body (numStage) and prefetch (numStage + 1).\n+//\n+// In the prologue, the sequence of operations is the same as the original loop\n+// body, following the (a) -> (b) -> (c) -> (d) order. In the loop body,\n+// however, we first execute the compute operations, then pre-load operations,\n+// post-load operations, and eventually the asynchronous load operations - in\n+// the (c) -> (a) -> (d) -> (b) order. This is used to better hide the latency\n+// of the load operations. Because of this, if post-load operations have direct\n+// dependencies on the load operations, we could repeat the post-load\n+// operations. More specifically, this occurs when:\n+// 1. Any load operand has an immediate dependency argument used at numStage-1.\n+// 2. The argument is first defined at numStage-2.\n+// To avoid the repeat, we peeled off post-load operations in the prologue that\n+// satisfy the above two conditions. See the example below for the definition of\n+// immediate and non-immediate dependencies.\n+// If we have a load that immediately depends on a block argument in the\n+// current iteration, it is an immediate dependency. Otherwise, it is a\n+// non-immediate dependency, which means the load depends on a block argument\n+// in the previous iterations.\n+// For example:\n+// scf.for (%arg0, %arg1, %arg2) {\n+//   %0 = load %arg0  <--- immediate dep, this address is initialized before\n+//   numStages-1.\n+//   %1 = load %arg1\n+//   %2 = add %1, %arg2\n+//   %3 = load %2  <--- non-immediate dep, %arg1 must be an\n+//   update-to-date value.\n+// }\n+//\n+// Our pipelining pass share some common characteristics with SCF's\n+// LoopPipelining. However, it is also noteworthy that our pipelining pass has\n+// the following characteristics different from SCF's LoopPipelining:\n+// 1. It can handle loop-carried dependencies of distance greater than 1.\n+// 2. It does not have a complicated epilogue but instead uses masking to handle\n+// boundary conditions.\n+// 3. Each operation/loop-carried argument cannot provide values to both\n+// immediate and non-immediate dependencies. Otherwise, we have to rematerialize\n+// the operation and arguments, which would likely increase register pressure.\n+// For example:\n+// scf.for (%arg0, %arg1, %arg2) {\n+//  %0 = load %arg0\n+//  %1 = load %arg1, %0  <--- %0 is both a post-load op at numStages-2 and a\n+//  pre-load op at numStages-1, so that we need two versions of %0.\n+//  %2 = add %0, %arg2\n+//  scf.yield %arg0, %2, %arg2\n+//  }\n //\n //===----------------------------------------------------------------------===//\n \n@@ -25,8 +83,12 @@ namespace ttg = triton::gpu;\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n \n-// pass named attrs (e.g., tt.contiguity) from Triton to Triton\n-static void addNamedAttrs(Operation *op, DictionaryAttr dictAttrs) {\n+#define int_attr(num) builder.getI64IntegerAttr(num)\n+\n+namespace {\n+\n+// Pass named attrs (e.g., tt.contiguity) from Triton to Triton\n+void addNamedAttrs(Operation *op, DictionaryAttr dictAttrs) {\n   NamedAttrList attrs = op->getDiscardableAttrs();\n   // Collect the attributes to propagate: the ones in dictAttrs and not yet on\n   // the operation.\n@@ -42,19 +104,13 @@ static void addNamedAttrs(Operation *op, DictionaryAttr dictAttrs) {\n   }\n }\n \n-#define int_attr(num) builder.getI64IntegerAttr(num)\n-\n-namespace {\n-\n class LoopPipeliner {\n-  /// Cache forOp we are working on\n+  /// Cache of ForOp and YieldOp related to this pipeliner.\n   scf::ForOp forOp;\n-\n-  /// Cache YieldOp for this forOp\n   scf::YieldOp yieldOp;\n \n   /// Loads to be pipelined\n-  SetVector<Value> loads;\n+  SetVector<Value> validLoads;\n   /// Smallest data-type for each load (used to optimize swizzle and\n   /// (create DotOpEncoding layout)\n   DenseMap<Value, Type> loadsSmallestType;\n@@ -68,74 +124,111 @@ class LoopPipeliner {\n   DenseMap<Value, SmallVector<Value>> loadStageBuffer;\n   /// load => after extract\n   DenseMap<Value, Value> loadsExtract;\n-  ///\n+\n+  /// Iterator values\n   Value pipelineIterIdx;\n-  ///\n   Value loopIterIdx;\n+  Value nextIV;\n \n-  /// Comments on numStages:\n-  ///   [0, numStages-1) are in the prologue\n-  ///   numStages-1 is appended after the loop body\n+  /// Yield values\n+  SmallVector<Value> nextBuffers;\n+  SmallVector<Value> extractSlices;\n+  SmallVector<Value> yieldValues;\n+\n+  /// The number of stages in the pipeline.\n+  /// Stages in the range of [0, numStages-1) are in the prologue.\n+  /// numStages-1 is appended after the loop body.\n   int numStages;\n \n+  /// Arg indicies\n+  size_t bufferIdx, loadIdx, depArgsBeginIdx, ivIndex;\n+  DenseMap<BlockArgument, size_t> depArgsIdx;\n+\n   /// value (in loop) => value at stage N\n   DenseMap<Value, SmallVector<Value>> valueMapping;\n+  /// loop iter arg => value\n+  DenseMap<BlockArgument, Value> depArgsMapping;\n+  /// forOp value => newForOp value\n+  IRMapping mapping;\n+  /// forOp value => prefetch value\n+  IRMapping nextMapping;\n \n-  /// For each argument, we need to record at which stage it is defined.\n-  /// If we have a load that immediately depends on a block argument in the\n-  /// current iteration, it is an immediate dependency. Otherwise, it is a\n-  /// non-immediate dependency, which means the load depends on a block argument\n-  /// in the previous iterations.\n-  /// For example:\n-  /// scf.for (%arg0, %arg1, %arg2) {\n-  ///   %0 = load %arg0  <--- immediate dep, this address is initialized before\n-  ///   numStages-1\n-  ///   %1 = load %arg1\n-  ///   %2 = add %1, %arg2\n-  ///   %3 = load %2  <--- non-immediate dep, %arg1 must be an update-to-date\n-  ///   value\n-  /// }\n-  /// Collect values that v depends on and are defined inside the loop\n-  LogicalResult collectDeps(Value v, int stage,\n-                            MapVector<Value, int> &depStage);\n-\n-  /// Associate each variable with a unique stage. If a variable is defined\n-  /// at multiple stages, we don't pipeline it.\n-  LogicalResult addDep(Value v, int stage, MapVector<Value, int> &depStage);\n-\n-  int getArgDefStage(Value v, int stage);\n-\n-  /// Block arguments that loads depend on\n-  MapVector<BlockArgument, int> depArgUseStage;\n-\n-  /// Block arguments that loads depend on (defined in the loop body)\n-  MapVector<BlockArgument, int> depArgDefStage;\n-\n-  /// Operations (inside the loop body) that loads depend on\n-  MapVector<Operation *, int> depOpDefStage;\n-\n-  /// Operations (inside the loop body) that loads depend on (defined in the\n-  /// loop body)\n-  SetVector<BlockArgument> immediateDepArgs;\n-\n-  /// Operations (inside the loop body) that loads depend on (defined in the\n-  /// previous iterations)\n-  SetVector<BlockArgument> nonImmediateDepArgs;\n+  /// Dependency ops by program order\n+  SmallVector<Operation *> orderedDeps;\n+\n+  /// arg => source operand defined stages\n+  DenseMap<BlockArgument, DenseSet<int>> immediateArgStages;\n+\n+  /// block arguments that loads depend on\n+  SetVector<BlockArgument> depArgs;\n+\n+  /// operation => source operand defined stages\n+  DenseMap<Operation *, DenseSet<int>> immediateOpStages;\n+\n+  /// operations that loads depend on\n+  SetVector<Operation *> depOps;\n+\n+  /// Collect all pipelinable ops\n+  LogicalResult collectOps(SetVector<Operation *> &ops);\n+\n+  /// Collect values that `v` depends on and are defined inside the loop\n+  void collectValueDep(Value v, int stage, SetVector<Value> &opDeps);\n+\n+  /// Collect all op dependencies\n+  void collectDeps(SetVector<Operation *> &ops,\n+                   MapVector<Operation *, SetVector<Value>> &opDeps);\n \n+  /// Check if none of the ops has valid uses\n+  LogicalResult checkOpUses(SetVector<Operation *> &ops);\n+\n+  /// Check if ops have dependencies that are not pipelinable\n+  void checkOpDeps(SetVector<Operation *> &ops);\n+\n+  void createBufferTypes();\n+\n+  void createOrderedDeps();\n+\n+  /// Return the stage at which `v` is defined prior to `stage`\n+  int getValueDefStage(Value v, int stage);\n+\n+  /// Map `origin` to `newValue` at `stage`\n   void setValueMapping(Value origin, Value newValue, int stage);\n \n+  /// Map `origin` to `newValue` at `stage` according to the association between\n+  /// yieldOp and forOp\n+  void setValueMappingYield(Value origin, Value newValue, int stage);\n+\n+  /// Map `origin` to `newValue` at the next stage according to the association\n+  /// between yieldOp and forOp\n+  void setValueMappingYield(scf::ForOp newForOp, Value origin, Value newValue);\n+\n+  /// Return the value mapped to `origin` at `stage`, if it exists.\n   Value lookupOrDefault(Value origin, int stage);\n \n+  /// Get the load mask for `loadOp`, given the mapped mask `mappedMask` (if\n+  /// exists) and the current iteration's `loopCond`.\n   Value getLoadMask(triton::LoadOp loadOp, Value mappedMask, Value loopCond,\n                     OpBuilder &builder);\n \n-  /// Returns a empty buffer of size <numStages, ...>\n-  ttg::AllocTensorOp allocateEmptyBuffer(Operation *op, OpBuilder &builder);\n+  /// Return an empty buffer of size <numStages, ...>\n+  ttg::AllocTensorOp allocateEmptyBuffer(triton::LoadOp loadOp,\n+                                         OpBuilder &builder);\n+\n+  /// Collect all args of the new loop\n+  SmallVector<Value> collectNewLoopArgs();\n+\n+  /// Clone the forOp and return the new forOp\n+  scf::ForOp cloneForOp(ArrayRef<Value> newLoopArgs, OpBuilder &builder);\n+\n+  /// Prefetch the next iteration for `newForOp`\n+  void prefetchNextIteration(scf::ForOp newForOp, OpBuilder &builder);\n+\n+  /// Assemble `newForOp`'s yield op\n+  void finalizeYield(scf::ForOp newForOp, OpBuilder &builder);\n \n public:\n   LoopPipeliner(scf::ForOp forOp, int numStages)\n       : forOp(forOp), numStages(numStages) {\n-    // cache yieldOp\n     yieldOp = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n   }\n \n@@ -154,100 +247,14 @@ class LoopPipeliner {\n   friend struct PipelinePass;\n };\n \n-// helpers\n-void LoopPipeliner::setValueMapping(Value origin, Value newValue, int stage) {\n-  if (valueMapping.find(origin) == valueMapping.end())\n-    valueMapping[origin] = SmallVector<Value>(numStages);\n-  valueMapping[origin][stage] = newValue;\n-}\n-\n-Value LoopPipeliner::lookupOrDefault(Value origin, int stage) {\n-  if (valueMapping.find(origin) == valueMapping.end())\n-    return origin;\n-  return valueMapping[origin][stage];\n-}\n-\n-LogicalResult LoopPipeliner::addDep(Value v, int stage,\n-                                    MapVector<Value, int> &depStage) {\n-  if (!depStage.contains(v)) {\n-    depStage.insert(std::make_pair(v, stage));\n-  } else if (depStage[v] != stage)\n-    return failure();\n-  return success();\n-}\n-\n-LogicalResult LoopPipeliner::collectDeps(Value v, int stage,\n-                                         MapVector<Value, int> &depStage) {\n-  // Loop-invariant value, skip\n-  if (v.getParentRegion() != &forOp.getLoopBody())\n-    return success();\n-\n-  // Since we only need to peel the loop numStages-1 times, don't worry about\n-  // depends that are too far away\n-  if (stage < 0)\n-    return success();\n-\n-  if (auto arg = v.dyn_cast<BlockArgument>()) {\n-    // Skip the first arg (loop induction variable)\n-    // Otherwise the op idx is arg.getArgNumber()-1\n-    if (arg.getArgNumber() > 0) {\n-      // If we've found the first definition of this arg, we're done, don't\n-      // recurse\n-      if (addDep(v, stage, depStage).succeeded())\n-        if (collectDeps(yieldOp->getOperand(arg.getArgNumber() - 1), stage - 1,\n-                        depStage)\n-                .failed())\n-          return failure();\n-    }\n-  } else { // value\n-    // An operation cannot be dependent on different stages\n-    if (addDep(v, stage, depStage).failed())\n-      return failure();\n-    for (Value op : v.getDefiningOp()->getOperands())\n-      if (collectDeps(op, stage, depStage).failed())\n-        return failure();\n-  }\n-  return success();\n-}\n-\n-int LoopPipeliner::getArgDefStage(Value v, int stage) {\n-  if (stage < 0)\n-    return -1;\n-  if (auto arg = v.dyn_cast<BlockArgument>()) {\n-    if (arg.getArgNumber() > 0) {\n-      return getArgDefStage(yieldOp->getOperand(arg.getArgNumber() - 1),\n-                            stage - 1);\n-    }\n-    llvm_unreachable(\"Loop induction variable should not be a dependency\");\n-  } else\n-    return stage;\n-}\n-\n-ttg::AllocTensorOp LoopPipeliner::allocateEmptyBuffer(Operation *op,\n-                                                      OpBuilder &builder) {\n-  // Allocate a buffer for each pipelined tensor\n-  // shape: e.g. (numStages==4), <32x64xbf16> -> <4x32x64xbf16>\n-  Value convertLayout = loadsMapping[op->getResult(0)];\n-  if (auto tensorType = convertLayout.getType().dyn_cast<RankedTensorType>()) {\n-    return builder.create<ttg::AllocTensorOp>(\n-        convertLayout.getLoc(), loadsBufferType[op->getResult(0)]);\n-  }\n-  llvm_unreachable(\"Async copy's return should be of RankedTensorType\");\n-}\n-\n-/// A load instruction can be pipelined if:\n-///   - the load doesn't depend on any other loads (after loop peeling)\n-///   - (?) this load is not a loop-invariant value (we should run LICM before\n-///                                                  this pass?)\n-LogicalResult LoopPipeliner::initialize() {\n-  Block *loop = forOp.getBody();\n+/// Collect loads to pipeline. Return success if we can pipeline this loop\n+LogicalResult LoopPipeliner::collectOps(SetVector<Operation *> &ops) {\n   ModuleOp moduleOp = forOp->getParentOfType<ModuleOp>();\n   ModuleAxisInfoAnalysis axisInfoAnalysis(moduleOp);\n \n   // We cannot use forOp.walk(...) here because we only want to visit the\n   // operations in the loop body block. Nested blocks are handled separately.\n-  SmallVector<triton::LoadOp, 2> validLoads;\n-  for (Operation &op : *loop)\n+  for (Operation &op : forOp)\n     if (auto loadOp = dyn_cast<triton::LoadOp>(&op)) {\n       auto ptr = loadOp.getPtr();\n       unsigned vec = axisInfoAnalysis.getPtrContiguity(ptr);\n@@ -264,97 +271,222 @@ LogicalResult LoopPipeliner::initialize() {\n       unsigned width = vec * ty.getIntOrFloatBitWidth();\n       // We do not pipeline all loads for the following reasons:\n       // 1. On nvidia GPUs, cp.async's cp-size can only be 4, 8 and 16.\n-      // 2. It's likely that pipling small load won't offer much performance\n+      // 2. It's likely that pipling small loads won't offer much performance\n       //    improvement and may even hurt performance by increasing register\n       //    pressure.\n       if (width >= 32)\n-        validLoads.push_back(loadOp);\n+        ops.insert(loadOp);\n+    }\n+\n+  if (ops.empty())\n+    return failure();\n+  else\n+    return success();\n+}\n+\n+void LoopPipeliner::collectValueDep(Value v, int stage,\n+                                    SetVector<Value> &deps) {\n+  // Loop-invariant value, skip\n+  if (v.getParentRegion() != &forOp.getLoopBody())\n+    return;\n+\n+  // Since we only need to peel the loop numStages-1 times, don't worry\n+  // about depends that are too far away\n+  if (stage < 0)\n+    return;\n+\n+  if (auto arg = v.dyn_cast<BlockArgument>()) {\n+    if (arg.getArgNumber() > 0) {\n+      deps.insert(v);\n+      collectValueDep(yieldOp->getOperand(arg.getArgNumber() - 1), stage - 1,\n+                      deps);\n+    }\n+  } else { // value\n+    deps.insert(v);\n+    for (Value op : v.getDefiningOp()->getOperands())\n+      collectValueDep(op, stage, deps);\n+  }\n+}\n+\n+void LoopPipeliner::collectDeps(\n+    SetVector<Operation *> &ops,\n+    MapVector<Operation *, SetVector<Value>> &valueDeps) {\n+  for (auto op : ops) {\n+    for (Value v : op->getOperands()) {\n+      SetVector<Value> deps;\n+      collectValueDep(v, numStages - 1, deps);\n+      valueDeps[op] = deps;\n+    }\n+  }\n+}\n+\n+LogicalResult LoopPipeliner::checkOpUses(SetVector<Operation *> &ops) {\n+  DenseSet<Operation *> invalidOps;\n+  // Collect all ops' dependencies\n+  MapVector<Operation *, SetVector<Value>> opDeps;\n+  collectDeps(ops, opDeps);\n+\n+  for (Operation *op : ops) {\n+    if (auto loadOp = dyn_cast<triton::LoadOp>(op)) {\n+      // Don't pipeline valid loads that depend on other valid loads\n+      // (Because if a valid load depends on another valid load, this load needs\n+      // to wait on the other load in the prologue, which is against the point\n+      // of the pipeline pass)\n+      bool isCandidate = true;\n+      for (Operation *other : ops)\n+        if (isa<triton::LoadOp>(other))\n+          if (opDeps[op].contains(other->getResult(0))) {\n+            isCandidate = false;\n+            break;\n+          }\n+      // We only pipeline loads that have one covert_layout (to dot_op) use\n+      // TODO: lift this constraint in the future\n+      if (isCandidate && loadOp.getResult().hasOneUse()) {\n+        isCandidate = false;\n+        Operation *use = *loadOp.getResult().getUsers().begin();\n+\n+        // Advance to the first conversion as long as the use resides in shared\n+        // memory and it has a single use itself\n+        while (use) {\n+          if (use->getNumResults() != 1 || !use->getResult(0).hasOneUse())\n+            break;\n+          auto tensorType =\n+              use->getResult(0).getType().dyn_cast<RankedTensorType>();\n+          if (!tensorType.getEncoding().isa<ttg::SharedEncodingAttr>())\n+            break;\n+          use = *use->getResult(0).getUsers().begin();\n+        }\n+\n+        if (auto convertLayout = llvm::dyn_cast<ttg::ConvertLayoutOp>(use))\n+          if (auto tensorType = convertLayout.getResult()\n+                                    .getType()\n+                                    .dyn_cast<RankedTensorType>())\n+            if (auto dotOpEnc = tensorType.getEncoding()\n+                                    .dyn_cast<ttg::DotOperandEncodingAttr>()) {\n+              isCandidate = true;\n+              loadsMapping[loadOp] = convertLayout;\n+            }\n+      } else\n+        isCandidate = false;\n+\n+      if (!isCandidate)\n+        invalidOps.insert(loadOp);\n+      else\n+        validLoads.insert(loadOp);\n     }\n+  }\n \n-  // Early stop: no need to continue if there is no load in the loop.\n-  if (validLoads.empty())\n+  for (Operation *op : invalidOps)\n+    ops.remove(op);\n+\n+  if (ops.empty())\n     return failure();\n+  else\n+    return success();\n+}\n \n-  // load => values that it depends on\n-  // Don't pipeline if any load's operands\n-  DenseMap<Value, SetVector<Value>> loadDeps;\n-  MapVector<Value, int> depStage;\n-  for (triton::LoadOp loadOp : validLoads) {\n-    for (Value op : loadOp->getOperands()) {\n-      MapVector<Value, int> operandDepStage;\n-      if (collectDeps(op, numStages - 1, operandDepStage).failed())\n-        return failure();\n-      for (auto [v, stage] : operandDepStage) {\n-        auto immedidate = operandDepStage.front().first.isa<BlockArgument>();\n-        if (v.isa<BlockArgument>()) {\n-          auto arg = v.cast<BlockArgument>();\n-          if (immedidate)\n-            immediateDepArgs.insert(arg);\n+void LoopPipeliner::checkOpDeps(SetVector<Operation *> &ops) {\n+  SetVector<BlockArgument> nonImmediateDepArgs;\n+  SetVector<Operation *> nonImmediateOps;\n+  for (Operation *op : ops) {\n+    for (Value v : op->getOperands()) {\n+      SetVector<Value> deps;\n+      collectValueDep(v, numStages - 1, deps);\n+      int defStage = getValueDefStage(v, numStages - 1);\n+      assert(defStage >= 0 &&\n+             \"newLoopArgs has null args without a define op. Consider either \"\n+             \"rewrite the loop to reduce cross iteration dependencies or \"\n+             \"increase the num_stages value.\");\n+      for (auto dep : deps) {\n+        auto immediate = deps.front().isa<BlockArgument>();\n+        if (auto arg = dyn_cast<BlockArgument>(dep)) {\n+          depArgs.insert(arg);\n+          if (immediate)\n+            immediateArgStages[arg].insert(defStage);\n           else\n             nonImmediateDepArgs.insert(arg);\n+        } else {\n+          depOps.insert(dep.getDefiningOp());\n+          if (immediate)\n+            immediateOpStages[dep.getDefiningOp()].insert(defStage);\n+          else\n+            nonImmediateOps.insert(dep.getDefiningOp());\n         }\n-        loadDeps[loadOp].insert(v);\n-        if (addDep(v, stage, depStage).failed())\n-          return failure();\n       }\n     }\n   }\n \n-  // Don't pipeline valid loads that depend on other valid loads\n-  // (Because if a valid load depends on another valid load, this load needs to\n-  // wait on the other load in the prologue, which is against the point of the\n-  // pipeline pass)\n-  for (triton::LoadOp loadOp : validLoads) {\n-    bool isCandidate = true;\n-    for (triton::LoadOp other : validLoads) {\n-      if (loadDeps[loadOp].contains(other)) {\n-        isCandidate = false;\n-        break;\n-      }\n-    }\n+  // XXX: We could remove the following constraints if we can rematerialize in\n+  // the loop.\n+  // Check if immediateDepArgs and nonImmediateDepArgs are disjoint.\n+  for (auto &[arg, stages] : immediateArgStages) {\n+    assert(stages.size() == 1 &&\n+           \"Triton doesn't support an argument provides values for \"\n+           \"immediate operands of loads from multiple stages. Consider \"\n+           \"removing post load instructions dependency on this argument.\");\n+    assert(!(nonImmediateDepArgs.contains(arg) &&\n+             stages.contains(numStages - 2)) &&\n+           \"Loop-carried arguments provide values for both immediate and \"\n+           \"non-immediate operands of loads. Please consider removing \"\n+           \"pre/post load instructions dependency on this argument.\");\n+  }\n \n-    // We only pipeline loads that have one covert_layout (to dot_op) use\n-    // TODO: lift this constraint in the future\n-    if (isCandidate && loadOp.getResult().hasOneUse()) {\n-      isCandidate = false;\n-      Operation *use = *loadOp.getResult().getUsers().begin();\n-\n-      // advance to the first conversion as long\n-      // as the use resides in shared memory and it has\n-      // a single use itself\n-      while (use) {\n-        if (use->getNumResults() != 1 || !use->getResult(0).hasOneUse())\n-          break;\n-        auto tensorType =\n-            use->getResult(0).getType().dyn_cast<RankedTensorType>();\n-        if (!tensorType.getEncoding().isa<ttg::SharedEncodingAttr>())\n-          break;\n-        use = *use->getResult(0).getUsers().begin();\n-      }\n+  // Check if immediateOps and nonImmediateOps are disjoint.\n+  for (auto &[op, stages] : immediateOpStages) {\n+    assert(stages.size() == 1 &&\n+           \"Triton doesn't support an operation provides values for \"\n+           \"immediate operands of loads from multiple stages. Consider \"\n+           \"removing post load instructions dependency on this argument.\");\n+    assert(!(nonImmediateOps.contains(op) && stages.contains(numStages - 2)) &&\n+           \"Operations provide values for both immediate and \"\n+           \"non-immediate operands of loads.  Please consider \"\n+           \"removing pre/post load instructions dependency on this \"\n+           \"operation.\");\n+  }\n+}\n \n-      auto convertLayout = llvm::dyn_cast<ttg::ConvertLayoutOp>(use);\n-      if (!convertLayout)\n-        continue;\n-      auto tensorType =\n-          convertLayout.getResult().getType().dyn_cast<RankedTensorType>();\n-      if (!tensorType)\n-        continue;\n-      auto dotOpEnc =\n-          tensorType.getEncoding().dyn_cast<ttg::DotOperandEncodingAttr>();\n-      if (!dotOpEnc)\n-        continue;\n-      isCandidate = true;\n-      loadsMapping[loadOp] = convertLayout;\n-    } else\n-      isCandidate = false;\n+// helpers\n+void LoopPipeliner::setValueMapping(Value origin, Value newValue, int stage) {\n+  if (valueMapping.find(origin) == valueMapping.end())\n+    valueMapping[origin] = SmallVector<Value>(numStages);\n+  valueMapping[origin][stage] = newValue;\n+}\n \n-    if (isCandidate)\n-      loads.insert(loadOp);\n+void LoopPipeliner::setValueMappingYield(Value origin, Value newValue,\n+                                         int stage) {\n+  for (OpOperand &operand : origin.getUses()) {\n+    if (operand.getOwner() == yieldOp) {\n+      auto yieldIdx = operand.getOperandNumber();\n+      auto value = forOp.getRegionIterArgs()[yieldIdx];\n+      setValueMapping(value, newValue, stage);\n+    }\n   }\n+}\n+\n+void LoopPipeliner::setValueMappingYield(scf::ForOp newForOp, Value origin,\n+                                         Value newValue) {\n+  for (OpOperand &operand : origin.getUses()) {\n+    if (operand.getOwner() == yieldOp) {\n+      auto yieldIdx = operand.getOperandNumber();\n+      auto depYieldIdx = depArgsIdx[forOp.getRegionIterArgs()[yieldIdx]];\n+      auto originArg = forOp.getRegionIterArgs()[yieldIdx];\n+      nextMapping.map(originArg, newValue);\n+      auto newArg = newForOp.getRegionIterArgs()[depYieldIdx];\n+      if (!depArgsMapping.contains(newArg))\n+        depArgsMapping[newArg] = newValue;\n+    }\n+  }\n+}\n \n-  // we need to find the smallest ocmmon dtype\n-  // since this determines the layout of `mma.sync` operands\n-  // in mixed-precision mode\n+Value LoopPipeliner::lookupOrDefault(Value origin, int stage) {\n+  if (valueMapping.find(origin) == valueMapping.end())\n+    return origin;\n+  return valueMapping[origin][stage];\n+}\n+\n+void LoopPipeliner::createBufferTypes() {\n+  // We need to find the smallest common dtype since this determines the layout\n+  // of `mma.sync` operands in mixed-precision mode\n   Type smallestType;\n   for (auto loadCvt : loadsMapping) {\n     auto loadOp = loadCvt.first;\n@@ -385,27 +517,59 @@ LogicalResult LoopPipeliner::initialize() {\n     loadsBufferType[loadOp] =\n         RankedTensorType::get(bufferShape, ty.getElementType(), sharedEnc);\n   }\n+}\n \n-  // We have some loads to pipeline\n-  if (!loads.empty()) {\n-    // Update depArgs & depOps\n-    for (auto [dep, stage] : depStage) {\n-      if (auto arg = dep.dyn_cast<BlockArgument>())\n-        depArgUseStage.insert({arg, stage});\n-      else\n-        depOpDefStage.insert({dep.getDefiningOp(), stage});\n-    }\n-    return success();\n+void LoopPipeliner::createOrderedDeps() {\n+  for (Operation &op : forOp.getLoopBody().front()) {\n+    if (depOps.contains(&op))\n+      orderedDeps.push_back(&op);\n+    else if (op.getNumResults() > 0 && validLoads.contains(op.getResult(0)))\n+      orderedDeps.push_back(&op);\n   }\n+  assert(depOps.size() + validLoads.size() == orderedDeps.size() &&\n+         \"depOps contains invalid values\");\n+}\n \n-  // Check if immedidateDepArgs and nonImmedidateDepArgs are disjoint\n-  // If yes, we cannot pipeline the loop for now\n-  for (BlockArgument arg : immediateDepArgs)\n-    if (nonImmediateDepArgs.contains(arg)) {\n-      return failure();\n-    }\n+int LoopPipeliner::getValueDefStage(Value v, int stage) {\n+  if (stage < 0)\n+    return -1;\n+  if (auto arg = v.dyn_cast<BlockArgument>()) {\n+    if (arg.getArgNumber() > 0)\n+      return getValueDefStage(yieldOp->getOperand(arg.getArgNumber() - 1),\n+                              stage - 1);\n+    llvm_unreachable(\"Loop induction variable should not be a dependency\");\n+  } else\n+    return stage;\n+}\n+\n+ttg::AllocTensorOp LoopPipeliner::allocateEmptyBuffer(triton::LoadOp loadOp,\n+                                                      OpBuilder &builder) {\n+  // Allocate a buffer for each pipelined tensor\n+  // shape: e.g. (numStages==4), <32x64xbf16> -> <4x32x64xbf16>\n+  Value convertLayout = loadsMapping[loadOp];\n+  if (auto tensorType = convertLayout.getType().dyn_cast<RankedTensorType>())\n+    return builder.create<ttg::AllocTensorOp>(convertLayout.getLoc(),\n+                                              loadsBufferType[loadOp]);\n+  llvm_unreachable(\"Async copy's return should be of RankedTensorType\");\n+}\n+\n+LogicalResult LoopPipeliner::initialize() {\n+  // All ops that maybe pipelined\n+  SetVector<Operation *> ops;\n+\n+  if (collectOps(ops).failed())\n+    return failure();\n+\n+  if (checkOpUses(ops).failed())\n+    return failure();\n+\n+  checkOpDeps(ops);\n+\n+  createBufferTypes();\n+\n+  createOrderedDeps();\n \n-  return failure();\n+  return success();\n }\n \n Value LoopPipeliner::getLoadMask(triton::LoadOp loadOp, Value mappedMask,\n@@ -450,24 +614,14 @@ void LoopPipeliner::emitPrologue() {\n     // Special handling for loop condition as there is no condition in ForOp\n     Value loopCond = builder.create<arith::CmpIOp>(\n         iv.getLoc(), arith::CmpIPredicate::slt, iv, forOp.getUpperBound());\n-\n-    // Rematerialize peeled values\n-    SmallVector<Operation *> orderedDeps;\n-    for (Operation &op : forOp.getLoopBody().front()) {\n-      if (depOpDefStage.contains(&op))\n-        orderedDeps.push_back(&op);\n-      else if (op.getNumResults() > 0 && loads.contains(op.getResult(0)))\n-        orderedDeps.push_back(&op);\n-    }\n-    assert(depOpDefStage.size() + loads.size() == orderedDeps.size() &&\n-           \"depOps contains invalid values\");\n     for (Operation *op : orderedDeps) {\n       Operation *newOp = nullptr;\n-      if (loads.contains(op->getResult(0))) {\n+      if (validLoads.contains(op->getResult(0))) {\n+        auto load = cast<triton::LoadOp>(op);\n         // Allocate empty buffer\n         if (stage == 0) {\n-          loadsBuffer[op->getResult(0)] = allocateEmptyBuffer(op, builder);\n-          loadStageBuffer[op->getResult(0)] = {loadsBuffer[op->getResult(0)]};\n+          loadsBuffer[load] = allocateEmptyBuffer(load, builder);\n+          loadStageBuffer[load] = {loadsBuffer[load]};\n         }\n         // load => copy async\n         if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(op)) {\n@@ -503,53 +657,37 @@ void LoopPipeliner::emitPrologue() {\n           auto it = valueMapping.find(op->getOperand(opIdx));\n           if (it != valueMapping.end()) {\n             Value v = it->second[stage];\n-            assert(v);\n+            assert(v && \"Value not found in valueMapping\");\n             newOp->setOperand(opIdx, v);\n           } // else, op at opIdx is a loop-invariant value\n         }\n       }\n \n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n         Value originResult = op->getResult(dstIdx);\n-        // copy_async will update the value of its only use\n-        if (loads.contains(originResult))\n+        if (validLoads.contains(originResult))\n           break;\n         setValueMapping(originResult, newOp->getResult(dstIdx), stage);\n-        // update mapping for loop-carried values (args)\n-        for (OpOperand &operand : yieldOp->getOpOperands()) {\n-          if (operand.get() == op->getResult(dstIdx)) {\n-            auto yieldIdx = operand.getOperandNumber();\n-            auto value = forOp.getRegionIterArgs()[yieldIdx];\n-            setValueMapping(value, newOp->getResult(dstIdx), stage + 1);\n-          }\n-        }\n+        // Update mapping for loop-carried values (args)\n+        setValueMappingYield(op->getResult(dstIdx), newOp->getResult(dstIdx),\n+                             stage + 1);\n       }\n     } // for (Operation *op : orderedDeps)\n \n     // Update pipeline index\n     pipelineIterIdx = builder.create<arith::AddIOp>(\n         iv.getLoc(), pipelineIterIdx,\n         builder.create<arith::ConstantIntOp>(iv.getLoc(), 1, 32));\n-\n     // Some values have not been used by any ops in the loop body\n-    for (BlockArgument arg : forOp.getRegionIterArgs()) {\n-      // Check if arg has a yieldOp use\n-      for (OpOperand &operand : arg.getUses()) {\n-        if (operand.getOwner() == yieldOp) {\n-          auto yieldIdx = operand.getOperandNumber();\n-          auto value = forOp.getRegionIterArgs()[yieldIdx];\n-          if (!valueMapping[value][stage + 1])\n-            setValueMapping(value, valueMapping[arg][stage], stage + 1);\n-        }\n-      }\n-    }\n+    for (BlockArgument arg : forOp.getRegionIterArgs())\n+      setValueMappingYield(arg, valueMapping[arg][stage], stage + 1);\n   } // for (int stage = 0; stage < numStages - 1; ++stage)\n \n   // async.wait & extract_slice\n-  builder.create<ttg::AsyncWaitOp>(loads[0].getLoc(),\n-                                   loads.size() * (numStages - 2));\n+  builder.create<ttg::AsyncWaitOp>(validLoads.front().getLoc(),\n+                                   validLoads.size() * (numStages - 2));\n   loopIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n-  for (Value loadOp : loads) {\n+  for (Value loadOp : validLoads) {\n     auto bufferType = loadStageBuffer[loadOp][numStages - 1]\n                           .getType()\n                           .cast<RankedTensorType>();\n@@ -568,7 +706,7 @@ void LoopPipeliner::emitPrologue() {\n     loadsExtract[loadOp] = extractSlice;\n   }\n   // Bump up loopIterIdx, this is used for getting the correct slice for the\n-  // *next* iteration\n+  // `next` iteration\n   loopIterIdx = builder.create<arith::AddIOp>(\n       loopIterIdx.getLoc(), loopIterIdx,\n       builder.create<arith::ConstantIntOp>(loopIterIdx.getLoc(), 1, 32));\n@@ -582,78 +720,74 @@ void LoopPipeliner::emitEpilogue() {\n   builder.create<ttg::AsyncWaitOp>(forOp.getLoc(), 0);\n }\n \n-scf::ForOp LoopPipeliner::createNewForOp() {\n-  OpBuilder builder(forOp);\n-\n+SmallVector<Value> LoopPipeliner::collectNewLoopArgs() {\n   // Order of new args:\n   //   (original args)\n   //   (insertSliceAsync buffer at stage numStages - 1) for each load\n   //   (extracted tensor) for each load\n-  //   (depArgs at stage numStages - 1):\n-  //   for each dep arg that is not an immediate block argument\n-  //   (depArgs at stage numStages - 2):\n-  //   for each dep arg that is an immediate block argument\n+  //   (depArgs at stage numStages - 1)\n+  //   (depArgs at stage numStages - 2)\n+  //   ...\n   //   (iv at stage numStages - 2)\n   //   (pipeline iteration index)\n   //   (loop iteration index)\n-  SmallVector<Value> newLoopArgs;\n+\n   // We need this to update operands for yield\n   // original block arg => new arg's idx\n-  DenseMap<BlockArgument, size_t> depArgsIdx;\n+  SmallVector<Value> newLoopArgs;\n   for (auto v : forOp.getIterOperands())\n     newLoopArgs.push_back(v);\n \n-  size_t bufferIdx = newLoopArgs.size();\n-  for (Value loadOp : loads)\n+  bufferIdx = newLoopArgs.size();\n+  for (auto loadOp : validLoads)\n     newLoopArgs.push_back(loadStageBuffer[loadOp].back());\n-  size_t loadIdx = newLoopArgs.size();\n-  for (Value loadOp : loads)\n+\n+  loadIdx = newLoopArgs.size();\n+  for (auto loadOp : validLoads)\n     newLoopArgs.push_back(loadsExtract[loadOp]);\n \n-  size_t depArgsBeginIdx = newLoopArgs.size();\n-  for (auto [depArg, useStage] : depArgUseStage) {\n+  depArgsBeginIdx = newLoopArgs.size();\n+  for (auto depArg : depArgs) {\n     depArgsIdx[depArg] = newLoopArgs.size();\n-    auto defStage = getArgDefStage(depArg, useStage);\n-    assert(defStage >= 0 &&\n-           \"newLoopArgs has null args without a define op. Consider either \"\n-           \"rewrite the loop to reduce cross iteration dependencies or \"\n-           \"increase the num_stages value.\");\n-    if (immediateDepArgs.contains(depArg) && defStage == numStages - 2) {\n+    if (immediateArgStages[depArg].contains(numStages - 2))\n+      // Peel off post load ops in numStage-1\n       newLoopArgs.push_back(valueMapping[depArg][numStages - 2]);\n-    } else\n+    else\n       newLoopArgs.push_back(valueMapping[depArg][numStages - 1]);\n   }\n \n-  size_t ivIndex = newLoopArgs.size();\n+  ivIndex = newLoopArgs.size();\n   newLoopArgs.push_back(valueMapping[forOp.getInductionVar()][numStages - 2]);\n   newLoopArgs.push_back(pipelineIterIdx);\n   newLoopArgs.push_back(loopIterIdx);\n+  return newLoopArgs;\n+}\n \n-  // 1. signature of the new ForOp\n+scf::ForOp LoopPipeliner::cloneForOp(ArrayRef<Value> newLoopArgs,\n+                                     OpBuilder &builder) {\n+  // Clone the original ForOp\n   auto newForOp = builder.create<scf::ForOp>(\n       forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n       forOp.getStep(), newLoopArgs);\n \n-  // 2. body of the new ForOp\n+  // Set mapping on body of the new ForOp\n   builder.setInsertionPointToStart(newForOp.getBody());\n-  IRMapping mapping;\n   for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n     mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n   mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n \n-  // 3. clone the loop body, replace original args with args of the new ForOp\n+  // Clone the loop body, replace original args with args of the new ForOp\n   // Insert async wait if necessary.\n-  DenseSet<Value> isModified;\n   for (Operation &op : forOp.getBody()->without_terminator()) {\n     // is modified\n-    auto it = std::find(loads.begin(), loads.end(), op.getOperand(0));\n-    if (it == loads.end()) {\n+    auto it = std::find(validLoads.begin(), validLoads.end(), op.getOperand(0));\n+    if (it == validLoads.end()) {\n       Operation *newOp = cloneWithInferType(builder, &op, mapping);\n       continue;\n     }\n \n     // we replace the use new load use with a convert layout\n-    size_t i = std::distance(loads.begin(), it);\n+    size_t i = std::distance(validLoads.begin(), it);\n     auto cvtDstTy = op.getResult(0).getType().cast<RankedTensorType>();\n     auto cvtDstEnc =\n         cvtDstTy.getEncoding().dyn_cast<ttg::DotOperandEncodingAttr>();\n@@ -670,23 +804,16 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n         op.getResult(0).getLoc(), newDstTy,\n         newForOp.getRegionIterArgs()[loadIdx + i]);\n     mapping.map(op.getResult(0), cvt.getResult());\n-    isModified.insert(op.getResult(0));\n   }\n \n-  // 4. prefetch the next iteration\n-  SmallVector<Operation *> orderedDeps;\n-  for (Operation &op : forOp.getLoopBody().front()) {\n-    if (depOpDefStage.contains(&op))\n-      orderedDeps.push_back(&op);\n-    else if (op.getNumResults() > 0 && loads.contains(op.getResult(0)))\n-      orderedDeps.push_back(&op);\n-  }\n-  assert(depOpDefStage.size() + loads.size() == orderedDeps.size() &&\n-         \"depOps contains invalid values\");\n-  IRMapping nextMapping;\n-  DenseMap<BlockArgument, Value> depArgsMapping;\n+  return newForOp;\n+}\n+\n+void LoopPipeliner::prefetchNextIteration(scf::ForOp newForOp,\n+                                          OpBuilder &builder) {\n+  // Map the dep args of the next iteration to the dep args of the current\n   size_t argIdx = 0;\n-  for (auto [depArg, useStage] : depArgUseStage) {\n+  for (auto depArg : depArgs) {\n     BlockArgument nextArg =\n         newForOp.getRegionIterArgs()[argIdx + depArgsBeginIdx];\n     nextMapping.map(depArg, nextArg);\n@@ -695,16 +822,12 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n \n   // Special handling for iv & loop condition\n   Value curIV = newForOp.getRegionIterArgs()[ivIndex];\n-  Value nextIV = builder.create<arith::AddIOp>(\n-      newForOp.getInductionVar().getLoc(), curIV, newForOp.getStep());\n+  nextIV = builder.create<arith::AddIOp>(newForOp.getInductionVar().getLoc(),\n+                                         curIV, newForOp.getStep());\n   Value nextLoopCond =\n       builder.create<arith::CmpIOp>(nextIV.getLoc(), arith::CmpIPredicate::slt,\n                                     nextIV, newForOp.getUpperBound());\n \n-  // Slice index\n-  SmallVector<Value> nextBuffers;\n-  SmallVector<Value> extractSlices;\n-\n   pipelineIterIdx = newForOp.getRegionIterArgs()[ivIndex + 1];\n   Value insertSliceIndex = builder.create<arith::RemSIOp>(\n       nextIV.getLoc(), pipelineIterIdx,\n@@ -716,8 +839,9 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n \n   // Prefetch load deps\n   for (Operation *op : orderedDeps)\n-    if (!loads.contains(op->getResult(0))) {\n-      if (depOpDefStage[op] == numStages - 2)\n+    if (!validLoads.contains(op->getResult(0))) {\n+      if (immediateOpStages[op].contains(numStages - 2))\n+        // A post load op that provides values for numStage - 2\n         nextMapping.map(forOp.getInductionVar(), curIV);\n       else\n         nextMapping.map(forOp.getInductionVar(), nextIV);\n@@ -734,30 +858,19 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n             loadOp.getCache(), loadOp.getEvict(), loadOp.getIsVolatile());\n         addNamedAttrs(nextOp, op->getDiscardableAttrDictionary());\n         nextMapping.map(loadOp.getResult(), nextOp->getResult(0));\n-      } else {\n+      } else\n         nextOp = builder.clone(*op, nextMapping);\n-      }\n \n-      for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n-        for (OpOperand &operand : yieldOp->getOpOperands()) {\n-          if (operand.get() == op->getResult(dstIdx)) {\n-            size_t yieldIdx = operand.getOperandNumber();\n-            size_t depYieldIdx =\n-                depArgsIdx[forOp.getRegionIterArgs()[yieldIdx]];\n-            BlockArgument newArg = newForOp.getRegionIterArgs()[depYieldIdx];\n-            nextMapping.map(forOp.getRegionIterArgs()[yieldIdx],\n-                            nextOp->getResult(dstIdx));\n-            depArgsMapping[newArg] = nextOp->getResult(dstIdx);\n-          }\n-        }\n-      }\n+      for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults()))\n+        setValueMappingYield(newForOp, op->getResult(dstIdx),\n+                             nextOp->getResult(dstIdx));\n     }\n \n   // loads -> async loads\n   for (Operation *op : orderedDeps) {\n     Operation *nextOp = nullptr;\n     // Update loading mask\n-    if (loads.contains(op->getResult(0))) {\n+    if (validLoads.contains(op->getResult(0))) {\n       auto loadOp = llvm::cast<triton::LoadOp>(op);\n       auto mask = loadOp.getMask();\n       auto newMask =\n@@ -779,7 +892,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n           loadOp.getEvict(), loadOp.getIsVolatile(), /*axis*/ 0);\n       builder.create<ttg::AsyncCommitGroupOp>(op->getLoc());\n       nextBuffers.push_back(insertAsyncOp);\n-      // ExtractSlice\n+      // Extract slice\n       auto bufferType = insertAsyncOp.getType().cast<RankedTensorType>();\n       auto bufferShape = bufferType.getShape();\n       auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n@@ -798,40 +911,21 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n       extractSlices.push_back(nextOp->getResult(0));\n \n       // Update mapping of results\n-      for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n-        nextMapping.map(op->getResult(dstIdx), nextOp->getResult(dstIdx));\n+      for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults()))\n         // If this is a loop-carried value, update the mapping for yield\n-        for (OpOperand &operand : yieldOp->getOpOperands()) {\n-          if (operand.get() == op->getResult(dstIdx)) {\n-            auto yieldIdx = operand.getOperandNumber();\n-            auto depYieldIdx = depArgsIdx[forOp.getRegionIterArgs()[yieldIdx]];\n-            auto newArg = newForOp.getRegionIterArgs()[depYieldIdx];\n-            depArgsMapping[newArg] = nextOp->getResult(dstIdx);\n-          }\n-        }\n-      }\n+        setValueMappingYield(newForOp, op->getResult(dstIdx),\n+                             nextOp->getResult(dstIdx));\n     }\n   }\n \n   // Some values have not been used by any ops in the loop body\n-  for (BlockArgument arg : forOp.getRegionIterArgs()) {\n-    // Check if arg has a yieldOp use\n-    for (OpOperand &operand : arg.getUses()) {\n-      if (operand.getOwner() == yieldOp) {\n-        auto yieldIdx = operand.getOperandNumber();\n-        auto depYieldIdx = depArgsIdx[forOp.getRegionIterArgs()[yieldIdx]];\n-        auto newArg = newForOp.getRegionIterArgs()[depYieldIdx];\n-        if (!depArgsMapping.contains(newArg)) {\n-          auto argIdx = depArgsIdx[arg];\n-          depArgsMapping[newArg] = newForOp.getRegionIterArgs()[argIdx];\n-        }\n-      }\n-    }\n-  }\n+  for (BlockArgument arg : forOp.getRegionIterArgs())\n+    setValueMappingYield(newForOp, arg,\n+                         newForOp.getRegionIterArgs()[depArgsIdx[arg]]);\n \n   // async.wait & extract_slice\n   Operation *asyncWait = builder.create<ttg::AsyncWaitOp>(\n-      loads[0].getLoc(), loads.size() * (numStages - 2));\n+      validLoads[0].getLoc(), validLoads.size() * (numStages - 2));\n   for (auto it = extractSlices.rbegin(); it != extractSlices.rend(); ++it) {\n     // move extract_slice after asyncWait\n     it->getDefiningOp()->moveAfter(asyncWait);\n@@ -844,8 +938,9 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   loopIterIdx = builder.create<arith::AddIOp>(\n       nextIV.getLoc(), loopIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), 1, 32));\n+}\n \n-  // Finally, the YieldOp, need to sync with the order of newLoopArgs\n+void LoopPipeliner::finalizeYield(scf::ForOp newForOp, OpBuilder &builder) {\n   SmallVector<Value> yieldValues;\n   for (Value v : yieldOp->getOperands())\n     yieldValues.push_back(mapping.lookup(v));\n@@ -865,6 +960,14 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n \n   builder.setInsertionPointToEnd(newForOp.getBody());\n   builder.create<scf::YieldOp>(yieldOp->getLoc(), yieldValues);\n+}\n+\n+scf::ForOp LoopPipeliner::createNewForOp() {\n+  OpBuilder builder(forOp);\n+  auto newLoopArgs = collectNewLoopArgs();\n+  auto newForOp = cloneForOp(newLoopArgs, builder);\n+  prefetchNextIteration(newForOp, builder);\n+  finalizeYield(newForOp, builder);\n   return newForOp;\n }\n \n@@ -897,11 +1000,10 @@ struct PipelinePass : public TritonGPUPipelineBase<PipelinePass> {\n         return;\n \n       pipeliner.emitPrologue();\n-\n       scf::ForOp newForOp = pipeliner.createNewForOp();\n       pipeliner.emitEpilogue();\n \n-      // replace the original loop\n+      // Replace the original loop\n       for (unsigned i = 0; i < forOp->getNumResults(); ++i)\n         forOp->getResult(i).replaceAllUsesWith(newForOp->getResult(i));\n       forOp->erase();"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 119, "deletions": 0, "changes": 119, "file_content_changes": "@@ -313,3 +313,122 @@ tt.func @lut_bmm_vector(%77: tensor<16x16xi64, #BL> {tt.divisibility=16: i32, tt\n   }\n   tt.return %79#0 : tensor<16x16xf32, #C>\n }\n+\n+// CHECK: tt.func @post_load_inv\n+// CHECK: scf.for\n+// CHECK: arith.index_cast\n+// CHECK-DAG: %[[IV:.*]] = arith.index_cast\n+// CHECK: %[[NEXT_IV:.*]] = arith.addi %[[IV]], %c1_i32 : i32\n+// CHECK-NOT: arith.addi %[[NEXT_IV]]\n+tt.func @post_load_inv(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                       %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                       %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                       %arg3: i32 {tt.divisibility = 16 : i32},\n+                       %arg4: i32 {tt.divisibility = 16 : i32},\n+                       %arg5: i32 {tt.divisibility = 16 : i32},\n+                       %arg6: i32 {tt.divisibility = 16 : i32},\n+                       %arg7: i32 {tt.divisibility = 16 : i32},\n+                       %arg8: i32 {tt.divisibility = 16 : i32}) -> tensor<32x32xf32, #C> {\n+  %c0_index = arith.constant 0 : index\n+  %c1_index = arith.constant 1 : index\n+  %c1_i32 = arith.constant 1 : i32\n+  %c32_i32 = arith.constant 32 : i32\n+  %84 = arith.constant 900 : index\n+  %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #C>\n+  %cst_0 = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #AL>\n+  %50 = tt.splat %arg3 : (i32) -> tensor<1x32xi32, #AL>\n+  %59 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %81 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %66 = tt.splat %arg4 : (i32) -> tensor<32x1xi32, #AL>\n+  %60 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %82 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %85:3 = scf.for %arg9 = %c0_index to %84 step %c1_index iter_args(%arg10 = %cst, %arg11 = %59, %arg12 = %81) -> (tensor<32x32xf32, #C>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>)  {\n+    %130 = arith.index_cast %arg9 : index to i32\n+    %107 = arith.muli %130, %c32_i32 : i32\n+    %108 = arith.subi %arg5, %107 : i32\n+    %109 = tt.splat %108 : (i32) -> tensor<1x32xi32, #AL>\n+    %110 = \"triton_gpu.cmpi\"(%50, %109) <{predicate = 2 : i64}> : (tensor<1x32xi32, #AL>, tensor<1x32xi32, #AL>) -> tensor<1x32xi1, #AL>\n+    %111 = tt.broadcast %110 : (tensor<1x32xi1, #AL>) -> tensor<32x32xi1, #AL>\n+    %112 = tt.load %arg11, %111, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #AL>\n+    %113 = tt.splat %108 : (i32) -> tensor<32x1xi32, #AL>\n+    %114 = \"triton_gpu.cmpi\"(%66, %113) <{predicate = 2 : i64}> : (tensor<32x1xi32, #AL>, tensor<32x1xi32, #AL>) -> tensor<32x1xi1, #AL>\n+    %115 = tt.broadcast %114 : (tensor<32x1xi1, #AL>) -> tensor<32x32xi1, #AL>\n+    %116 = tt.load %arg12, %115, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #AL>\n+    %117 = triton_gpu.convert_layout %112 : (tensor<32x32xf32, #AL>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth = 1}>>\n+    %118 = triton_gpu.convert_layout %116 : (tensor<32x32xf32, #AL>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth = 1}>>\n+    %119 = tt.dot %117, %118, %arg10 {allowTF32 = true} : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth = 1}>> * tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth = 1}>> -> tensor<32x32xf32, #C>\n+    %131 = arith.index_cast %arg9 : index to i32\n+    %120 = arith.addi %131, %c1_i32 : i32\n+    %121 = arith.muli %120, %c32_i32 : i32\n+    %122 = tt.splat %121 : (i32) -> tensor<32x32xi32, #AL>\n+    %123 = tt.addptr %60, %122 : tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32xi32, #AL>\n+    %124 = arith.muli %121, %arg7 : i32\n+    %125 = tt.splat %124 : (i32) -> tensor<32x32xi32, #AL>\n+    %126 = tt.addptr %82, %125 : tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32xi32, #AL>\n+    scf.yield %119, %123, %126 : tensor<32x32xf32, #C>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>\n+  }\n+  tt.return %85#0 : tensor<32x32xf32, #C>\n+}\n+\n+// CHECK: tt.func @cross_iter_dep\n+// CHECK: triton_gpu.async_commit_group\n+// CHECK: triton_gpu.async_commit_group\n+// CHECK: triton_gpu.async_commit_group\n+// CHECK: triton_gpu.async_commit_group\n+// CHECK: %[[PTR0:.*]] = tt.addptr\n+// CHECK: %[[PTR1:.*]] = tt.addptr\n+// CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[BUF0:.*]] = %[[PTR0]], {{.*}}, %[[BUF1:.*]] = %[[PTR1]]\n+// CHECK: scf.yield\n+// CHECK-SAME: %[[BUF0]]\n+// CHECK-SAME: %[[BUF1]]\n+tt.func @cross_iter_dep(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                        %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                        %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                        %arg3: i32 {tt.divisibility = 16 : i32},\n+                        %arg4: i32 {tt.divisibility = 16 : i32},\n+                        %arg5: i32 {tt.divisibility = 16 : i32},\n+                        %arg6: i32 {tt.divisibility = 16 : i32},\n+                        %arg7: i32 {tt.divisibility = 16 : i32},\n+                        %arg8: i32 {tt.divisibility = 16 : i32}) -> tensor<32x32xf32, #C> {\n+  %c0_i32 = arith.constant 0 : index\n+  %118 = arith.constant 32 : index\n+  %c1_i32 = arith.constant 1 : index\n+  %c2_i32 = arith.constant 2 : i32\n+  %c32_i32 = arith.constant 32 : i32\n+  %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #C>\n+  %cst_1 = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #AL>\n+  %78 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %110 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %112 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %113 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %116 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %65 = tt.splat %arg3 : (i32) -> tensor<1x32xi32, #AL>\n+  %88 = tt.splat %arg4 : (i32) -> tensor<32x1xi32, #AL>\n+  %80 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %119:5 = scf.for %arg9 = %c0_i32 to %118 step %c1_i32 iter_args(%arg10 = %cst, %arg11 = %78, %arg12 = %110, %arg13 = %113, %arg14 = %116) -> (tensor<32x32xf32, #C>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>)  {\n+    %161 = arith.index_cast %arg9 : index to i32\n+    %141 = arith.muli %161, %c32_i32 : i32\n+    %142 = arith.subi %arg5, %141 : i32\n+    %143 = tt.splat %142 : (i32) -> tensor<1x32xi32, #AL>\n+    %144 = \"triton_gpu.cmpi\"(%65, %143) <{predicate = 2 : i64}> : (tensor<1x32xi32, #AL>, tensor<1x32xi32, #AL>) -> tensor<1x32xi1, #AL>\n+    %145 = tt.broadcast %144 : (tensor<1x32xi1, #AL>) -> tensor<32x32xi1, #AL>\n+    %146 = tt.load %arg11, %145, %cst_1 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #AL>\n+    %147 = tt.splat %142 : (i32) -> tensor<32x1xi32, #AL>\n+    %148 = \"triton_gpu.cmpi\"(%88, %147) <{predicate = 2 : i64}> : (tensor<32x1xi32, #AL>, tensor<32x1xi32, #AL>) -> tensor<32x1xi1, #AL>\n+    %149 = tt.broadcast %148 : (tensor<32x1xi1, #AL>) -> tensor<32x32xi1, #AL>\n+    %150 = tt.load %arg12, %149, %cst_1 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #AL>\n+    %151 = triton_gpu.convert_layout %146 : (tensor<32x32xf32, #AL>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth = 1}>>\n+    %152 = triton_gpu.convert_layout %150 : (tensor<32x32xf32, #AL>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth = 1}>>\n+    %153 = tt.dot %151, %152, %arg10 {allowTF32 = true} : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth = 1}>> * tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth = 1}>> -> tensor<32x32xf32, #C>\n+    %162 = arith.index_cast %arg9 : index to i32\n+    %154 = arith.addi %162, %c2_i32 : i32\n+    %155 = arith.muli %154, %c32_i32 : i32\n+    %156 = tt.splat %155 : (i32) -> tensor<32x32xi32, #AL>\n+    %157 = tt.addptr %80, %156 : tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32xi32, #AL>\n+    %158 = arith.muli %155, %arg7 : i32\n+    %159 = tt.splat %158 : (i32) -> tensor<32x32xi32, #AL>\n+    %160 = tt.addptr %112, %159 : tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32xi32, #AL>\n+    scf.yield %153, %arg13, %arg14, %157, %160 : tensor<32x32xf32, #C>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>\n+  }\n+  tt.return %119#0 : tensor<32x32xf32, #C>\n+}"}]