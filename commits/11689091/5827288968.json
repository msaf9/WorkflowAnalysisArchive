[{"filename": "bin/RegisterTritonDialects.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -7,6 +7,7 @@\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n \n+#include \"triton/Conversion/NVGPUToLLVM/Passes.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/Passes.h\"\n #include \"triton/Conversion/TritonToTritonGPU/Passes.h\"\n \n@@ -32,6 +33,7 @@ inline void registerTritonDialects(mlir::DialectRegistry &registry) {\n   mlir::test::registerTestMembarPass();\n   mlir::triton::registerConvertTritonToTritonGPUPass();\n   mlir::triton::registerConvertTritonGPUToLLVMPass();\n+  mlir::triton::registerConvertNVGPUToLLVMPass();\n \n   // TODO: register Triton & TritonGPU passes\n   registry.insert<mlir::triton::TritonDialect, mlir::cf::ControlFlowDialect,"}, {"filename": "include/triton/Conversion/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1,2 +1,3 @@\n add_subdirectory(TritonToTritonGPU)\n add_subdirectory(TritonGPUToLLVM)\n+add_subdirectory(NVGPUToLLVM)"}, {"filename": "include/triton/Conversion/NVGPUToLLVM/CMakeLists.txt", "status": "added", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -0,0 +1,3 @@\n+set(LLVM_TARGET_DEFINITIONS Passes.td)\n+mlir_tablegen(Passes.h.inc -gen-pass-decls --name NVGPUToLLVM)\n+add_public_tablegen_target(NVGPUConversionPassIncGen)"}, {"filename": "include/triton/Conversion/NVGPUToLLVM/NVGPUToLLVMPass.h", "status": "added", "additions": 19, "deletions": 0, "changes": 19, "file_content_changes": "@@ -0,0 +1,19 @@\n+#ifndef TRITON_CONVERSION_NVGPU_TO_LLVM_PASS_H\n+#define TRITON_CONVERSION_NVGPU_TO_LLVM_PASS_H\n+\n+#include <memory>\n+\n+namespace mlir {\n+\n+class ModuleOp;\n+template <typename T> class OperationPass;\n+\n+namespace triton {\n+\n+std::unique_ptr<OperationPass<ModuleOp>> createConvertNVGPUToLLVMPass();\n+\n+} // namespace triton\n+\n+} // namespace mlir\n+\n+#endif"}, {"filename": "include/triton/Conversion/NVGPUToLLVM/Passes.h", "status": "added", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -0,0 +1,16 @@\n+#ifndef NVGPU_CONVERSION_PASSES_H\n+#define NVGPU_CONVERSION_PASSES_H\n+\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"triton/Conversion/NVGPUToLLVM/NVGPUToLLVMPass.h\"\n+\n+namespace mlir {\n+namespace triton {\n+\n+#define GEN_PASS_REGISTRATION\n+#include \"triton/Conversion/NVGPUToLLVM/Passes.h.inc\"\n+\n+} // namespace triton\n+} // namespace mlir\n+\n+#endif"}, {"filename": "include/triton/Conversion/NVGPUToLLVM/Passes.td", "status": "added", "additions": 20, "deletions": 0, "changes": 20, "file_content_changes": "@@ -0,0 +1,20 @@\n+#ifndef NVGPU_CONVERSION_PASSES\n+#define NVGPU_CONVERSION_PASSES\n+\n+include \"mlir/Pass/PassBase.td\"\n+\n+\n+def ConvertNVGPUToLLVM : Pass<\"convert-nv-gpu-to-llvm\", \"mlir::ModuleOp\"> {\n+    let summary = \"Convert NVGPU to LLVM\";\n+    let description = [{\n+\n+    }];\n+    let constructor = \"mlir::triton::createConvertNVGPUToLLVMPass()\";\n+\n+    let dependentDialects = [\"mlir::arith::ArithDialect\",\n+                             \"mlir::LLVM::LLVMDialect\",\n+                             \"mlir::NVVM::NVVMDialect\",\n+                             \"mlir::triton::nvgpu::NVGPUDialect\"];\n+}\n+\n+#endif"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -151,6 +151,12 @@ struct PTXBuilder {\n   // aggressive optimizations that may lead to incorrect results.\n   Operand *newOperand(StringRef constraint, bool init = false);\n \n+  // Create a new operand that is tied to a previous operand. In this case the\n+  // asm would be permitted to write to an input register. Instead of providing\n+  // constraint code for this operand, the constraint code of the tied operand\n+  // is used.\n+  Operand *newOperand(unsigned operandIndex);\n+\n   // Create a constant integer operand.\n   Operand *newConstantOperand(int64_t v);\n   // Create a constant operand with explicit code specified."}, {"filename": "include/triton/Dialect/NVGPU/IR/NVGPUOps.td", "status": "modified", "additions": 1, "deletions": 124, "changes": 125, "file_content_changes": "@@ -36,38 +36,22 @@ class NVGPU_Op<string mnemonic, list<Trait> traits = []> :\n     LLVM_OpBase<NVGPU_Dialect, mnemonic, traits>;\n \n def NVGPU_WGMMAFenceOp : NVGPU_Op<\"wgmma_fence\", []> {\n-  string llvmBuilder = [{\n-      createWGMMAFence(builder);\n-  }];\n   let assemblyFormat = \"attr-dict\";\n }\n \n \n def NVGPU_WGMMACommitGroupOp : NVGPU_Op<\"wgmma_commit_group\", []> {\n   let assemblyFormat = \"attr-dict\";\n-  string llvmBuilder = [{\n-      createWGMMACommitGroup(builder);\n-  }];\n }\n \n-def NVGPU_WGMMAWaitOp : NVGPU_Op<\"wgmma_wait_group\", []> {\n+def NVGPU_WGMMAWaitGroupOp : NVGPU_Op<\"wgmma_wait_group\", []> {\n   let arguments = (ins I32Attr:$pendings);\n   let assemblyFormat = \"attr-dict\";\n-  string llvmBuilder = [{\n-      createWGMMAWaitGroup(builder, $pendings);\n-  }];\n }\n \n def NVGPU_MBarrierInitOp : NVGPU_Op<\"mbarrier_init\", [MemoryEffects<[MemWrite]>]> {\n   let arguments = (ins I64Ptr_shared:$mbarrier, I1:$pred, I32Attr:$count);\n   let assemblyFormat = \"$mbarrier `,` $pred attr-dict `:` type($mbarrier)\";\n-  string llvmBuilder = [{\n-      auto *i32Ty = builder.getInt32Ty();\n-      auto *arriveCnt = builder.getInt32($count);\n-        createExternalCall(builder, \"__nv_mbarrier_init\", {builder.CreatePtrToInt($mbarrier, i32Ty),\n-        arriveCnt,\n-        builder.CreateIntCast($pred, i32Ty, false)});\n-  }];\n }\n \n def MBarrier_ArriveTypeAttr : I32EnumAttr<\"MBarriveType\",\n@@ -84,39 +68,21 @@ def MBarrier_ArriveTypeAttr : I32EnumAttr<\"MBarriveType\",\n def NVGPU_MBarrierArriveOp : NVGPU_Op<\"mbarrier_arrive\", []> {\n   let arguments = (ins I64Ptr_shared:$mbarrier, I1:$pred, Optional<I32>:$ctaId, MBarrier_ArriveTypeAttr:$arriveType, DefaultValuedAttr<I32Attr, \"0\">:$txCount);\n   let assemblyFormat = \"$mbarrier `,` $pred (`,` $ctaId^)? attr-dict `:` type($mbarrier)\";\n-  string llvmBuilder = [{\n-      auto *i32Ty = builder.getInt32Ty();\n-      createMBarrierArrive(builder, $arriveType, builder.CreatePtrToInt($mbarrier, i32Ty),\n-        builder.CreateIntCast($pred, i32Ty, false), $ctaId,\n-        $txCount);\n-  }];\n }\n \n def NVGPU_MBarrierWaitOp : NVGPU_Op<\"mbarrier_wait\", []> {\n   let arguments = (ins I64Ptr_shared:$mbarrier, I1:$phase);\n   let assemblyFormat = \"$mbarrier `,` $phase attr-dict `:` type(operands)\";\n-\n-  string llvmBuilder = [{\n-      auto *i32Ty = builder.getInt32Ty();\n-      createExternalCall(builder, \"__nv_mbarrier_wait\", {builder.CreatePtrToInt($mbarrier, i32Ty),\n-        builder.CreateIntCast($phase, i32Ty, false)});\n-  }];\n }\n \n def NVGPU_NamedBarrierArriveOp : NVGPU_Op<\"bar_arrive\", []> {\n   let arguments = (ins I32:$bar, I32:$numThreads);\n   let assemblyFormat = \"$bar `,` $numThreads attr-dict `:` type(operands)\";\n-  string llvmBuilder = [{\n-      createExternalCall(builder, \"__nv_bar_arrive\", {$bar, $numThreads});\n-  }];\n }\n \n def NVGPU_NamedBarrierWaitOp : NVGPU_Op<\"bar_wait\", []> {\n   let arguments = (ins I32:$bar, I32:$numThreads);\n   let assemblyFormat = \"$bar `,` $numThreads attr-dict `:` type(operands)\";\n-  string llvmBuilder = [{\n-      createExternalCall(builder, \"__nv_bar_wait\", {$bar, $numThreads});\n-  }];\n }\n \n def WGMMADesc_ModeAttr : I32EnumAttr<\"WGMMADescMode\",\n@@ -134,38 +100,17 @@ def NVGPU_WGMMADescCreateOp : NVGPU_Op<\"wgmma_desc_create\", []> {\n   let arguments = (ins LLVM_AnyPointer:$buffer, I32:$height, WGMMADesc_ModeAttr:$mode);\n   let results = (outs I64:$res);\n   let assemblyFormat = \"$buffer `,` $height attr-dict `:` functional-type(operands, results)\";\n-  string llvmBuilder = [{\n-    $res = createWGMMADesc(builder, builder.CreatePtrToInt($buffer, builder.getInt32Ty()), $mode, $height);\n-  }];\n }\n \n def NVGPU_TMALoadTiledOp : NVGPU_Op<\"tma_load_tiled\", [AttrSizedOperandSegments]> {\n   let arguments = (ins I8Ptr_shared:$dst, I64Ptr_shared:$mbarrier, I8Ptr_global:$tmaDesc, I64:$l2Desc,\n                        I1:$pred, Variadic<I32>:$coords, Optional<I16>:$mcastMask);\n   let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n-  string llvmBuilder = [{\n-    auto *i32Ty = builder.getInt32Ty();\n-    auto *i64Ty = builder.getInt64Ty();\n-    createTMALoadTiled(builder,\n-      builder.CreatePtrToInt($dst, i32Ty),\n-      builder.CreatePtrToInt($mbarrier, i32Ty),\n-      builder.CreatePtrToInt($tmaDesc, i64Ty),\n-      $l2Desc, $mcastMask, builder.CreateIntCast($pred, builder.getInt32Ty(), false), $coords);\n-  }];\n }\n \n def NVGPU_TMALoadIm2colOp : NVGPU_Op<\"tma_load_im2col\", []> {\n   let arguments = (ins I8Ptr_shared:$dst, I64Ptr_shared:$mbarrier, I8Ptr_global:$tmaDesc, I64:$l2Desc, LLVM_AnyStruct:$im2colOffsets, I1:$pred, Variadic<I32>:$coords, I16Attr:$mcastMask);\n   let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n-  string llvmBuilder = [{\n-    auto *i32Ty = builder.getInt32Ty();\n-    auto *i64Ty = builder.getInt64Ty();\n-    createTMALoadIm2col(builder,\n-    builder.CreatePtrToInt($dst, i32Ty),\n-    builder.CreatePtrToInt($mbarrier, i32Ty),\n-    builder.CreatePtrToInt($tmaDesc, i64Ty),\n-    $l2Desc, $mcastMask, $im2colOffsets, builder.CreateIntCast($pred, builder.getInt32Ty(), false), $coords);\n-  }];\n }\n \n def WGMMA_LayoutAttr : I32EnumAttr<\"WGMMALayout\",\n@@ -201,30 +146,18 @@ def NVGPU_WGMMAOp : NVGPU_Op<\"wgmma\", []> {\n                    WGMMA_LayoutAttr:$layoutA, WGMMA_LayoutAttr:$layoutB);\n   let results = (outs LLVM_AnyStruct:$res);\n   let assemblyFormat = \"$opA `,` $opB `,` $opC attr-dict `:` functional-type(operands, $res)\";\n-  string llvmBuilder = [{\n-    $res = createWGMMA(builder, $m, $n, $k, $eltTypeC, $eltTypeA, $eltTypeB, $layoutA, $layoutB, $opA, $opB, $opC);\n-  }];\n }\n \n def NVGPU_CGABarrierSyncOp : NVGPU_Op<\"cga_barrier_sync\", []> {\n   let assemblyFormat = \"attr-dict\";\n-  string llvmBuilder = [{\n-      createExternalCall(builder, \"__nv_cga_barrier_sync\");\n-  }];\n }\n \n def NVGPU_CGABarrierArriveOp : NVGPU_Op<\"cga_barrier_arrive\", []> {\n   let assemblyFormat = \"attr-dict\";\n-  string llvmBuilder = [{\n-      createExternalCall(builder, \"__nv_cga_barrier_arrive\");\n-  }];\n }\n \n def NVGPU_CGABarrierWaitOp : NVGPU_Op<\"cga_barrier_wait\", []> {\n   let assemblyFormat = \"attr-dict\";\n-  string llvmBuilder = [{\n-      createExternalCall(builder, \"__nv_cga_barrier_wait\");\n-  }];\n }\n \n def NVGPU_LoadDSmemOp : NVGPU_Op<\"load_dsmem\", [MemoryEffects<[MemRead]>]> {\n@@ -236,9 +169,6 @@ def NVGPU_LoadDSmemOp : NVGPU_Op<\"load_dsmem\", [MemoryEffects<[MemRead]>]> {\n   ];\n   let results = (outs LLVM_LoadableType:$result);\n   let assemblyFormat = \"operands attr-dict `:` functional-type(operands, results)\";\n-  string llvmBuilder = [{\n-      $result = createLoadSharedCluster(builder, $addr, $ctaId, $bitwidth, $vec);\n-  }];\n }\n \n def NVGPU_StoreDSmemOp : NVGPU_Op<\"store_dsmem\", [MemoryEffects<[MemWrite]>]> {\n@@ -248,9 +178,6 @@ def NVGPU_StoreDSmemOp : NVGPU_Op<\"store_dsmem\", [MemoryEffects<[MemWrite]>]> {\n       OpBuilder<(ins \"Value\":$addr, \"Value\":$ctaId, \"Value\":$value, \"Value\":$pred)>,\n   ];\n   let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n-  string llvmBuilder = [{\n-      createStoreSharedCluster(builder, $addr, $ctaId, $values, $pred, op.getBitwidth(), op.getVec());\n-  }];\n   let extraClassDeclaration = [{\n       unsigned getBitwidth();\n       unsigned getVec();\n@@ -259,113 +186,63 @@ def NVGPU_StoreDSmemOp : NVGPU_Op<\"store_dsmem\", [MemoryEffects<[MemWrite]>]> {\n \n def NVGPU_FenceAsyncSharedOp : NVGPU_Op<\"fence_async_shared\", []> {\n   let arguments = (ins BoolAttr:$bCluster);\n-  string llvmBuilder = [{\n-    if ($bCluster)\n-      createExternalCall(builder, \"__nv_fence_async_shared_cluster\", {});\n-    else\n-      createExternalCall(builder, \"__nv_fence_async_shared_cta\", {});\n-  }];\n   let assemblyFormat = \"attr-dict\";\n }\n \n def NVGPU_FenceMBarrierInitOp : NVGPU_Op<\"fence_mbarrier_init\", []> {\n-  string llvmBuilder = [{\n-      createExternalCall(builder, \"__nv_fence_mbarrier_init\", {});\n-  }];\n   let assemblyFormat = \"attr-dict\";\n }\n \n def NVGPU_ClusterArriveOp : NVGPU_Op<\"cluster_arrive\", []> {\n   let arguments = (ins I1Attr:$relaxed);\n \n-  string llvmBuilder = [{\n-    if ($relaxed)\n-      createExternalCall(builder, \"__nv_cluster_arrive_relaxed\", {});\n-    else\n-      createExternalCall(builder, \"__nv_cluster_arrive\", {});\n-  }];\n   let assemblyFormat = \"attr-dict\";\n }\n \n def NVGPU_ClusterWaitOp : NVGPU_Op<\"cluster_wait\", []> {\n-  string llvmBuilder = [{\n-      createExternalCall(builder, \"__nv_cluster_wait\", {});\n-  }];\n   let assemblyFormat = \"attr-dict\";\n }\n \n def NVGPU_TMAStoreTiledOp : NVGPU_Op<\"tma_store_tiled\", [MemoryEffects<[MemWrite]>]> {\n   let arguments = (ins I8Ptr_global:$tmaDesc, I8Ptr_shared:$src, I1:$pred, Variadic<I32>:$coords);\n   let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n-  string llvmBuilder = [{\n-    auto *i32Ty = builder.getInt32Ty();\n-    auto *i64Ty = builder.getInt64Ty();\n-    createTMAStoreTiled(builder,\n-      builder.CreatePtrToInt($tmaDesc, i64Ty),\n-      builder.CreatePtrToInt($src, i32Ty),\n-      builder.CreateIntCast($pred, i32Ty, false), $coords);\n-  }];\n }\n \n def NVGPU_StoreMatrixOp : NVGPU_Op<\"stmatrix\", [MemoryEffects<[MemWrite]>]> {\n   let arguments = (ins I8Ptr_shared:$addr, Variadic<I32>:$datas);\n   let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n-  string llvmBuilder = [{\n-    auto *i32Ty = builder.getInt32Ty();\n-    createStoreMatrix(builder,\n-      builder.CreatePtrToInt($addr, i32Ty),\n-      $datas);\n-  }];\n }\n \n def NVGPU_OffsetOfStmatrixV4Op : NVGPU_Op<\"offset_of_stmatrix_v4\", []> {\n   let arguments = (ins I32:$threadId, I32:$rowOfWarp, I32:$elemIdx, I32Attr:$leadingDimOffset, I32Attr:$rowStride, I1Attr:$swizzleEnabled);\n   let results = (outs I32:$offset);\n   let assemblyFormat = \"operands attr-dict `:` type(operands) `->` type($offset)\";\n-  string llvmBuilder = [{\n-    $offset = createOffsetOfStmatrixV4(builder, $threadId, $rowOfWarp, $elemIdx, $leadingDimOffset, $rowStride, $swizzleEnabled);\n-  }];\n }\n \n def NVGPU_OffsetOfSts64Op : NVGPU_Op<\"offset_of_sts64\", []> {\n   let arguments = (ins I32:$threadId, I32:$rowOfWarp, I32:$elemIdx, I32Attr:$leadingDimOffset, I32Attr:$rowStride, I1Attr:$swizzleEnabled);\n   let results = (outs I32:$offset);\n   let assemblyFormat = \"operands attr-dict `:` type(operands) `->` type($offset)\";\n-  string llvmBuilder = [{\n-    $offset = createOffsetOfSts64(builder, $threadId, $rowOfWarp, $elemIdx, $leadingDimOffset, $rowStride, $swizzleEnabled);\n-  }];\n }\n \n def NVGPU_Sts64Op : NVGPU_Op<\"sts64\", [MemoryEffects<[MemWrite]>]> {\n   let arguments = (ins I32:$offset, AnyTypeOf<[F32, I32]>:$d0, AnyTypeOf<[F32, I32]>:$d1);\n   let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n-  string llvmBuilder = [{\n-    createSts64(builder, $offset, $d0, $d1);\n-  }];\n }\n \n def NVGPU_ClusterCTAIdOp : NVGPU_Op<\"cluster_id\", [Pure]> {\n   let results = (outs I32:$result);\n   let assemblyFormat = \"attr-dict\";\n-  string llvmBuilder = [{\n-      $result = createClusterId(builder);\n-      }];\n }\n \n def NVGPU_RegAllocOp : NVGPU_Op<\"reg_alloc\", []> {\n   let arguments = (ins I32Attr: $regCount);\n   let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n-  string llvmBuilder = [{\n-      createRegAlloc(builder, $regCount);\n-  }];\n }\n \n def NVGPU_RegDeallocOp : NVGPU_Op<\"reg_dealloc\", []> {\n   let arguments = (ins I32Attr: $regCount);\n   let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n-  string llvmBuilder = [{\n-      createRegDealloc(builder, $regCount);\n-  }];\n }\n \n #endif"}, {"filename": "include/triton/Dialect/NVGPU/ToLLVMIR/NVGPUToLLVMIR.h", "status": "removed", "additions": 0, "deletions": 41, "changes": 41, "file_content_changes": "@@ -1,41 +0,0 @@\n-/*\n- * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n- *\n- * Permission is hereby granted, free of charge, to any person obtaining\n- * a copy of this software and associated documentation files\n- * (the \"Software\"), to deal in the Software without restriction,\n- * including without limitation the rights to use, copy, modify, merge,\n- * publish, distribute, sublicense, and/or sell copies of the Software,\n- * and to permit persons to whom the Software is furnished to do so,\n- * subject to the following conditions:\n- *\n- * The above copyright notice and this permission notice shall be\n- * included in all copies or substantial portions of the Software.\n- *\n- * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n- * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n- * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n- * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n- * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n- */\n-\n-#ifndef TRITON_DIALECT_NVGPU_NVGPUTOLLVMIRTRANSLATION_H\n-#define TRITON_DIALECT_NVGPU_NVGPUTOLLVMIRTRANSLATION_H\n-\n-namespace mlir {\n-\n-class DialectRegistry;\n-class MLIRContext;\n-\n-/// Register the nvgpu dialect and the translation from it to the LLVM IR in the\n-/// given registry;\n-void registerNVGPUDialectTranslation(DialectRegistry &registry);\n-\n-/// Register the nvgpu dialect and the translation from it in the registry\n-/// associated with the given context.\n-void registerNVGPUDialectTranslation(MLIRContext &context);\n-} // namespace mlir\n-\n-#endif // TRITON_DIALECT_NVGPU_NVGPUTOLLVMIRTRANSLATION_H"}, {"filename": "lib/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -3,4 +3,3 @@ add_subdirectory(Analysis)\n add_subdirectory(Conversion)\n add_subdirectory(Dialect)\n add_subdirectory(Target)\n-add_subdirectory(Hopper)"}, {"filename": "lib/Conversion/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1,2 +1,3 @@\n add_subdirectory(TritonToTritonGPU)\n add_subdirectory(TritonGPUToLLVM)\n+add_subdirectory(NVGPUToLLVM)"}, {"filename": "lib/Conversion/NVGPUToLLVM/CMakeLists.txt", "status": "added", "additions": 27, "deletions": 0, "changes": 27, "file_content_changes": "@@ -0,0 +1,27 @@\n+add_mlir_conversion_library(NVGPUToLLVM\n+    NVGPUToLLVMPass.cpp\n+\n+    ADDITIONAL_HEADER_DIRS\n+    ${PROJECT_SOURCE_DIR}/include/triton/Conversion/NVGPUToLLVM\n+    ${PROJECT_BINARY_DIR}/include/triton/Conversion/NVGPUToLLVM\n+\n+    DEPENDS\n+    NVGPUConversionPassIncGen\n+\n+    LINK_COMPONENTS\n+    Core\n+\n+    LINK_LIBS PUBLIC\n+    MLIRIR\n+    MLIRPass\n+    MLIRGPUOps\n+    MLIRGPUToNVVMTransforms\n+    MLIRGPUToROCDLTransforms\n+    MLIRGPUTransforms\n+    TritonAnalysis\n+    TritonIR\n+    TritonGPUIR\n+    TritonGPUTransforms\n+    TritonNvidiaGPUTransforms\n+    NVGPUIR\n+)"}, {"filename": "lib/Conversion/NVGPUToLLVM/NVGPUToLLVMPass.cpp", "status": "added", "additions": 1202, "deletions": 0, "changes": 1202, "file_content_changes": "@@ -0,0 +1,1202 @@\n+#include \"triton/Conversion/NVGPUToLLVM/NVGPUToLLVMPass.h\"\n+\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LogicalResult.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+\n+#include \"triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h\"\n+\n+#include \"../lib/Conversion/TritonGPUToLLVM/Utility.h\"\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Conversion/NVGPUToLLVM/Passes.h.inc\"\n+\n+namespace ttn = mlir::triton::nvgpu;\n+using ::mlir::LLVM::getSRegValue;\n+\n+namespace {\n+\n+template <typename SourceOp, typename ConcreteT>\n+class NVGPUOpPatternBase : public mlir::RewritePattern {\n+public:\n+  explicit NVGPUOpPatternBase(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(SourceOp::getOperationName(), 1, context) {}\n+\n+  LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto loc = op->getLoc();\n+    auto sourceOp = llvm::dyn_cast<SourceOp>(op);\n+    if (!sourceOp)\n+      return mlir::failure();\n+    auto ptxAsm = static_cast<const ConcreteT *>(this)->getPtxAsm(sourceOp);\n+    auto hasSideEffects = !isMemoryEffectFree(sourceOp);\n+    PTXBuilder ptxBuilder;\n+    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+    ptxInstr({}, /*onlyAttachMLIRArgs=*/true);\n+    auto asmReturnTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, asmReturnTy,\n+                      /*hasSideEffects*/ hasSideEffects);\n+    rewriter.eraseOp(op);\n+    return mlir::success();\n+  }\n+};\n+\n+class CGABarrierSyncOpPattern\n+    : public NVGPUOpPatternBase<ttn::CGABarrierSyncOp,\n+                                CGABarrierSyncOpPattern> {\n+public:\n+  using Base =\n+      NVGPUOpPatternBase<ttn::CGABarrierSyncOp, CGABarrierSyncOpPattern>;\n+  using Base::Base;\n+\n+  std::string getPtxAsm(ttn::CGABarrierSyncOp op) const {\n+    return \"barrier.cluster.sync.aligned;\";\n+  }\n+};\n+\n+class FenceAsyncSharedOpPattern\n+    : public NVGPUOpPatternBase<ttn::FenceAsyncSharedOp,\n+                                FenceAsyncSharedOpPattern> {\n+public:\n+  using Base =\n+      NVGPUOpPatternBase<ttn::FenceAsyncSharedOp, FenceAsyncSharedOpPattern>;\n+  using Base::Base;\n+\n+  std::string getPtxAsm(ttn::FenceAsyncSharedOp op) const {\n+    auto bCluster = op.getBCluster();\n+    if (bCluster)\n+      return \"fence.proxy.async.shared::cluster;\";\n+    else\n+      return \"fence.proxy.async.shared::cta;\";\n+  }\n+};\n+\n+class WGMMAFenceOpPattern\n+    : public NVGPUOpPatternBase<ttn::WGMMAFenceOp, WGMMAFenceOpPattern> {\n+public:\n+  using Base = NVGPUOpPatternBase<ttn::WGMMAFenceOp, WGMMAFenceOpPattern>;\n+  using Base::Base;\n+\n+  std::string getPtxAsm(ttn::WGMMAFenceOp op) const {\n+    return \"wgmma.fence.sync.aligned;\";\n+  }\n+};\n+\n+class WGMMACommitGroupOpPattern\n+    : public NVGPUOpPatternBase<ttn::WGMMACommitGroupOp,\n+                                WGMMACommitGroupOpPattern> {\n+public:\n+  using Base =\n+      NVGPUOpPatternBase<ttn::WGMMACommitGroupOp, WGMMACommitGroupOpPattern>;\n+  using Base::Base;\n+\n+  std::string getPtxAsm(ttn::WGMMACommitGroupOp op) const {\n+    return \"wgmma.commit_group.sync.aligned;\";\n+  }\n+};\n+\n+class WGMMAWaitGroupOpPattern\n+    : public NVGPUOpPatternBase<ttn::WGMMAWaitGroupOp,\n+                                WGMMAWaitGroupOpPattern> {\n+public:\n+  using Base =\n+      NVGPUOpPatternBase<ttn::WGMMAWaitGroupOp, WGMMAWaitGroupOpPattern>;\n+  using Base::Base;\n+\n+  std::string getPtxAsm(ttn::WGMMAWaitGroupOp op) const {\n+    auto pendings = op.getPendings();\n+    return \"wgmma.wait_group.sync.aligned \" + std::to_string(pendings) + \";\";\n+  }\n+};\n+\n+class StoreMatrixOpPattern : public mlir::RewritePattern {\n+public:\n+  StoreMatrixOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::StoreMatrixOp::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto storeMatrixOp = llvm::dyn_cast<ttn::StoreMatrixOp>(op);\n+    if (!storeMatrixOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto addr = storeMatrixOp.getAddr();\n+    auto datas = storeMatrixOp.getDatas();\n+\n+    assert(datas.size() == 1 || datas.size() == 2 ||\n+           datas.size() == 4 && \"Invalid size for StoreMatrixOp\");\n+    PTXBuilder ptxBuilder;\n+    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(\n+        \"stmatrix.sync.aligned.m8n8.x\" + std::to_string(datas.size()) +\n+        \".shared.b16\");\n+    auto *addrOpr = ptxBuilder.newAddrOperand(ptrtoint(i32_ty, addr), \"r\");\n+\n+    SmallVector<std::pair<Value, std::string>> args;\n+    for (unsigned i = 0; i < datas.size(); ++i) {\n+      args.push_back({datas[i], \"r\"});\n+    }\n+    auto *operands = ptxBuilder.newListOperand(args);\n+\n+    ptxInstr(addrOpr, operands);\n+\n+    auto asmReturnTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, asmReturnTy);\n+    rewriter.eraseOp(op);\n+    return mlir::success();\n+  }\n+};\n+\n+class MBarrierInitOpPattern : public mlir::RewritePattern {\n+public:\n+  MBarrierInitOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::MBarrierInitOp::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto mBarrierInitOp = llvm::dyn_cast<ttn::MBarrierInitOp>(op);\n+    if (!mBarrierInitOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    Value mbarrier = mBarrierInitOp.getMbarrier();\n+    Value pred = mBarrierInitOp.getPred();\n+    uint32_t count = mBarrierInitOp.getCount();\n+    PTXBuilder ptxBuilder;\n+\n+    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(\"mbarrier.init.shared.b64\");\n+    auto *barOpr =\n+        ptxBuilder.newAddrOperand(ptrtoint(i32_ty, mbarrier), \"r\", 0);\n+    auto *expectedOpr = ptxBuilder.newConstantOperand(count);\n+\n+    ptxInstr(barOpr, expectedOpr).predicate(pred, \"b\");\n+\n+    auto asmReturnTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, asmReturnTy);\n+    rewriter.eraseOp(op);\n+    return mlir::success();\n+  }\n+};\n+\n+class MBarrierArriveOpPattern : public mlir::RewritePattern {\n+public:\n+  MBarrierArriveOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::MBarrierArriveOp::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto mbarrierArriveOp = llvm::dyn_cast<ttn::MBarrierArriveOp>(op);\n+    if (!mbarrierArriveOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    Value mbarrier = mbarrierArriveOp.getMbarrier();\n+    Value pred = mbarrierArriveOp.getPred();\n+    Value ctaId = mbarrierArriveOp.getCtaId();\n+    auto arriveType = mbarrierArriveOp.getArriveType();\n+    uint32_t txCount = mbarrierArriveOp.getTxCount();\n+\n+    PTXBuilder ptxBuilder;\n+    if (arriveType == ttn::MBarriveType::normal) {\n+      auto &ptxInstr =\n+          *ptxBuilder.create<PTXInstr>(\"mbarrier.arrive.shared.b64 _,\");\n+      auto *barOpr =\n+          ptxBuilder.newAddrOperand(ptrtoint(i32_ty, mbarrier), \"r\", 0);\n+\n+      ptxInstr(barOpr).predicate(pred, \"b\");\n+    } else if (arriveType == ttn::MBarriveType::cp_async) {\n+      auto &ptxInstr = *ptxBuilder.create<PTXInstr>(\n+          \"cp.async.mbarrier.arrive.noinc.shared.b64\");\n+      auto *barOpr =\n+          ptxBuilder.newAddrOperand(ptrtoint(i32_ty, mbarrier), \"r\", 0);\n+\n+      ptxInstr(barOpr).predicate(pred, \"b\");\n+    } else if (arriveType == ttn::MBarriveType::expect_tx) {\n+      assert(txCount > 0 && \"txCount should be valid\");\n+      auto &ptxInstr = *ptxBuilder.create<PTXInstr>(\n+          \"mbarrier.arrive.expect_tx.shared.b64 _,\");\n+      auto *barOpr =\n+          ptxBuilder.newAddrOperand(ptrtoint(i32_ty, mbarrier), \"r\", 0);\n+      auto *expectedOpr = ptxBuilder.newConstantOperand(txCount);\n+\n+      ptxInstr(barOpr, expectedOpr).predicate(pred, \"b\");\n+    } else if (arriveType == ttn::MBarriveType::remote) {\n+      assert(ctaId && \"ctaId should have a valid value\");\n+      auto ptxAsm =\n+          \" { .reg .b32 remAddr32;                                       \\n\"\n+          \"  @$2 mapa.shared::cluster.u32  remAddr32, $0, $1;            \\n\"\n+          \"  @$2 mbarrier.arrive.shared::cluster.b64  _, [remAddr32]; }  \\n\";\n+      auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+      auto *barOpr =\n+          ptxBuilder.newAddrOperand(ptrtoint(i32_ty, mbarrier), \"r\", 0);\n+      auto *ctaIdOpr = ptxBuilder.newOperand(ctaId, \"r\");\n+      auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n+\n+      ptxInstr({barOpr, ctaIdOpr, predOpr}, /*onlyAttachMLIRArgs=*/true);\n+    } else {\n+      assert(false &&\n+             \"Unsupported mbarrier arrive type\"); // TODO: is this the right way\n+                                                  // to assert in LLVM pass ?\n+    }\n+    auto asmReturnTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, asmReturnTy);\n+    rewriter.eraseOp(op);\n+    return mlir::success();\n+  }\n+};\n+class MBarrierWaitOpPattern : public mlir::RewritePattern {\n+public:\n+  MBarrierWaitOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::MBarrierWaitOp::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto mBarrierWaitOp = llvm::dyn_cast<ttn::MBarrierWaitOp>(op);\n+    if (!mBarrierWaitOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    Value mbarrier = mBarrierWaitOp.getMbarrier();\n+    Value phase = mBarrierWaitOp.getPhase();\n+    PTXBuilder ptxBuilder;\n+\n+    auto ptxAsm =\n+        \"{\\n\"\n+        \".reg .pred                P1; \\n\"\n+        \"LAB_WAIT: \\n\"\n+        \"mbarrier.try_wait.parity.shared.b64 P1, [$0], $1, 0x989680; \\n\"\n+        \"@P1                       bra.uni DONE; \\n\"\n+        \"bra.uni                   LAB_WAIT; \\n\"\n+        \"DONE: \\n\"\n+        \"}\";\n+    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+    auto *barOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, mbarrier), \"r\");\n+    auto *phaseOpr = ptxBuilder.newOperand(zext(i32_ty, phase), \"r\");\n+\n+    ptxInstr({barOpr, phaseOpr},\n+             /*onlyAttachMLIRArgs=*/true);\n+\n+    auto asmReturnTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, asmReturnTy);\n+    rewriter.eraseOp(op);\n+    return mlir::success();\n+  }\n+};\n+\n+class ClusterArriveOpPattern\n+    : public NVGPUOpPatternBase<ttn::ClusterArriveOp, ClusterArriveOpPattern> {\n+public:\n+  using Base = NVGPUOpPatternBase<ttn::ClusterArriveOp, ClusterArriveOpPattern>;\n+  using Base::Base;\n+\n+  std::string getPtxAsm(ttn::ClusterArriveOp op) const {\n+    auto relaxed = op.getRelaxed();\n+    if (relaxed)\n+      return \"barrier.cluster.arrive.relaxed.aligned;\";\n+    else\n+      return \"barrier.cluster.arrive.aligned;\";\n+  }\n+};\n+\n+class ClusterWaitOpPattern\n+    : public NVGPUOpPatternBase<ttn::ClusterWaitOp, ClusterWaitOpPattern> {\n+public:\n+  using Base = NVGPUOpPatternBase<ttn::ClusterWaitOp, ClusterWaitOpPattern>;\n+  using Base::Base;\n+  std::string getPtxAsm(ttn::ClusterWaitOp op) const {\n+    return \"barrier.cluster.wait.aligned;\";\n+  }\n+};\n+\n+class TMALoadTiledOpPattern : public mlir::RewritePattern {\n+public:\n+  TMALoadTiledOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::TMALoadTiledOp::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto tmaLoadTiledOp = llvm::dyn_cast<ttn::TMALoadTiledOp>(op);\n+    if (!tmaLoadTiledOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto dst = tmaLoadTiledOp.getDst();\n+    auto mbarrier = tmaLoadTiledOp.getMbarrier();\n+    auto tmaDesc = tmaLoadTiledOp.getTmaDesc();\n+    auto l2Desc = tmaLoadTiledOp.getL2Desc();\n+    auto pred = tmaLoadTiledOp.getPred();\n+    auto coords = tmaLoadTiledOp.getCoords();\n+    auto mcastMask = tmaLoadTiledOp.getMcastMask();\n+\n+    auto dimSize = coords.size();\n+\n+    PTXBuilder ptxBuilder;\n+    if (dimSize == 2) {\n+      if (mcastMask == nullptr) {\n+        auto ptxAsm =\n+            \"@$6 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier:\"\n+            \":complete_tx\"\n+            \"::bytes.L2::cache_hint [$0], [$1, {$2, $3}], [$4], $5;\";\n+        auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+        auto *dstOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, dst), \"r\");\n+        auto *descOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, tmaDesc), \"l\");\n+        auto *c0Opr = ptxBuilder.newOperand(coords[0], \"r\");\n+        auto *c1Opr = ptxBuilder.newOperand(coords[1], \"r\");\n+        auto *barOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, mbarrier), \"r\");\n+        auto *l2DescOpr = ptxBuilder.newOperand(l2Desc, \"l\");\n+        auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n+\n+        ptxInstr({dstOpr, descOpr, c0Opr, c1Opr, barOpr, l2DescOpr, predOpr},\n+                 /*onlyAttachMLIRArgs=*/true);\n+      } else {\n+        auto ptxAsm =\n+            \"@$7 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::\"\n+            \"complete_tx::bytes.multicast::cluster.L2::cache_hint\"\n+            \" [$0], [$1, {$2, $3}], [$4], $5, $6;\";\n+        auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+        auto *dstOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, dst), \"r\");\n+        auto *descOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, tmaDesc), \"l\");\n+        auto *c0Opr = ptxBuilder.newOperand(coords[0], \"r\");\n+        auto *c1Opr = ptxBuilder.newOperand(coords[1], \"r\");\n+        auto *barOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, mbarrier), \"r\");\n+        auto *maskOpr = ptxBuilder.newOperand(mcastMask, \"h\");\n+        auto *l2DescOpr = ptxBuilder.newOperand(l2Desc, \"l\");\n+        auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n+        ptxInstr({dstOpr, descOpr, c0Opr, c1Opr, barOpr, maskOpr, l2DescOpr,\n+                  predOpr},\n+                 /*onlyAttachMLIRArgs=*/true);\n+      }\n+    } else if (dimSize == 4) {\n+      assert(mcastMask == nullptr && \"Does not support multicast\");\n+      auto ptxAsm = \"@$8 \"\n+                    \"cp.async.bulk.tensor.4d.shared::cluster.global.mbarrier:\"\n+                    \":complete_tx\"\n+                    \"::bytes.L2::cache_hint [$0], [$1, {$2, $3, $4, $5}], \"\n+                    \"[$6], $7;\";\n+      auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+      auto *dstOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, dst), \"r\");\n+      auto *descOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, tmaDesc), \"l\");\n+      auto *c0Opr = ptxBuilder.newOperand(coords[0], \"r\");\n+      auto *c1Opr = ptxBuilder.newOperand(coords[1], \"r\");\n+      auto *c2Opr = ptxBuilder.newOperand(coords[2], \"r\");\n+      auto *c3Opr = ptxBuilder.newOperand(coords[3], \"r\");\n+      auto *barOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, mbarrier), \"r\");\n+      auto *l2DescOpr = ptxBuilder.newOperand(l2Desc, \"l\");\n+      auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n+      ptxInstr({dstOpr, descOpr, c0Opr, c1Opr, c2Opr, c3Opr, barOpr, l2DescOpr,\n+                predOpr},\n+               /*onlyAttachMLIRArgs=*/true);\n+    } else {\n+      assert(false && \"invalid dim size\");\n+    }\n+\n+    auto asmReturnTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, asmReturnTy, /*hasSideEffect*/ true);\n+    rewriter.eraseOp(op);\n+    return mlir::success();\n+  }\n+};\n+\n+class TMAStoreTiledOpPattern : public mlir::RewritePattern {\n+public:\n+  TMAStoreTiledOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::TMAStoreTiledOp::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto tmaStoreTiledOp = llvm::dyn_cast<ttn::TMAStoreTiledOp>(op);\n+    if (!tmaStoreTiledOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto src = tmaStoreTiledOp.getSrc();\n+    auto tmaDesc = tmaStoreTiledOp.getTmaDesc();\n+    auto pred = tmaStoreTiledOp.getPred();\n+    auto coords = tmaStoreTiledOp.getCoords();\n+\n+    auto dimSize = coords.size();\n+\n+    PTXBuilder ptxBuilder;\n+    if (dimSize == 2) {\n+      auto ptxAsm = \"cp.async.bulk.tensor.2d.global.shared::cta.bulk_group\"\n+                    \"[$0, {$2, $3}], [$1];\";\n+      auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+\n+      auto *descOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, tmaDesc), \"l\");\n+      auto *srcOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, src), \"r\");\n+      auto *c0Opr = ptxBuilder.newOperand(coords[0], \"r\");\n+      auto *c1Opr = ptxBuilder.newOperand(coords[1], \"r\");\n+      auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n+      ptxInstr({descOpr, srcOpr, c0Opr, c1Opr, predOpr},\n+               /*onlyAttachMLIRArgs=*/true);\n+    } else if (dimSize == 3) {\n+      auto ptxAsm = \"@$5 cp.async.bulk.tensor.3d.global.shared::cta.bulk_group\"\n+                    \"[$0, {$2, $3, $4}], [$1];\";\n+      auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+\n+      auto *descOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, tmaDesc), \"l\");\n+      auto *srcOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, src), \"r\");\n+      auto *c0Opr = ptxBuilder.newOperand(coords[0], \"r\");\n+      auto *c1Opr = ptxBuilder.newOperand(coords[1], \"r\");\n+      auto *c2Opr = ptxBuilder.newOperand(coords[2], \"r\");\n+      auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n+      ptxInstr({descOpr, srcOpr, c0Opr, c1Opr, c2Opr, predOpr},\n+               /*onlyAttachMLIRArgs=*/true);\n+    } else if (dimSize == 4) {\n+      auto ptxAsm = \"@$6 cp.async.bulk.tensor.4d.global.shared::cta.bulk_group\"\n+                    \"[$0, {$2, $3, $4, $5}], [$1];\";\n+      auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+      auto *descOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, tmaDesc), \"l\");\n+      auto *srcOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, src), \"r\");\n+      auto *c0Opr = ptxBuilder.newOperand(coords[0], \"r\");\n+      auto *c1Opr = ptxBuilder.newOperand(coords[1], \"r\");\n+      auto *c2Opr = ptxBuilder.newOperand(coords[2], \"r\");\n+      auto *c3Opr = ptxBuilder.newOperand(coords[3], \"r\");\n+      auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n+      ptxInstr({descOpr, srcOpr, c0Opr, c1Opr, c2Opr, c3Opr, predOpr},\n+               /*onlyAttachMLIRArgs=*/true);\n+    } else {\n+      assert(false && \"invalid dim size\");\n+    }\n+\n+    auto asmReturnTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, asmReturnTy, /*hasSideEffect*/ true);\n+    rewriter.eraseOp(op);\n+    return mlir::success();\n+  }\n+};\n+\n+class LoadDSmemOpPattern : public mlir::RewritePattern {\n+public:\n+  LoadDSmemOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::LoadDSmemOp::getOperationName(), 1, context) {\n+  }\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto loadDSmemOp = llvm::dyn_cast<ttn::LoadDSmemOp>(op);\n+    if (!loadDSmemOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto addr = loadDSmemOp.getAddr();\n+    auto ctaId = loadDSmemOp.getCtaId();\n+    auto bitwidth = loadDSmemOp.getBitwidth();\n+    auto vec = loadDSmemOp.getVec();\n+\n+    assert(\n+        (bitwidth == 8 || bitwidth == 16 || bitwidth == 32 || bitwidth == 64) &&\n+        \"invalid bitwidth\");\n+    assert((vec == 1 || vec == 2 || vec == 4) && \"invalid vec size\");\n+    PTXBuilder ptxBuilder;\n+\n+    std::string o1 = vec > 1 ? \".v.u\" : \".u\";\n+    std::string vecStr = vec == 1   ? \"$0\"\n+                         : vec == 2 ? \"{$0, $1}\"\n+                                    : \"{$0, $1, $2, $3}\";\n+    unsigned argNum = vec == 1 ? 1 : vec == 2 ? 2 : 4;\n+    auto ptxAsm = \"{\\n\"\n+                  \".reg .u32 remoteAddr;\\n\"\n+                  \"mapa.shared::cluster.u32 remoteAddr, $\" +\n+                  std::to_string(argNum) + \" , $\" + std::to_string(argNum + 1) +\n+                  \" ; \\n\"\n+                  \"ld.shared::cluster\" +\n+                  o1 + std::to_string(bitwidth) + \" \" + vecStr +\n+                  \", [remoteAddr];\\n\"\n+                  \"}\\n\";\n+\n+    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+    std::string c = bitwidth == 16 ? \"=h\" : (bitwidth == 32 ? \"=r\" : \"=l\");\n+    SmallVector<PTXBuilder::Operand *> oprs;\n+    for (unsigned i = 0; i < vec; ++i) {\n+      auto *ret = ptxBuilder.newOperand(c);\n+      oprs.push_back(ret);\n+    }\n+    auto *addrOpr = ptxBuilder.newOperand(addr, \"r\");\n+    auto *ctaIdOpr = ptxBuilder.newOperand(ctaId, \"r\");\n+    oprs.push_back(addrOpr);\n+    oprs.push_back(ctaIdOpr);\n+\n+    Type retTy = IntegerType::get(rewriter.getContext(), bitwidth);\n+    SmallVector<Type> retTys(vec, retTy);\n+    if (vec > 1)\n+      retTy = struct_ty(retTys);\n+\n+    ptxInstr(oprs,\n+             /*onlyAttachMLIRArgs=*/true);\n+\n+    auto res = ptxBuilder.launch(rewriter, loc, retTy);\n+    rewriter.replaceOp(op, {res});\n+    return mlir::success();\n+  }\n+};\n+\n+class WGMMAOpPattern : public mlir::RewritePattern {\n+public:\n+  WGMMAOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::WGMMAOp::getOperationName(), 1, context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    using namespace ttn;\n+    auto ctx = rewriter.getContext();\n+    auto wgmmaOp = llvm::dyn_cast<ttn::WGMMAOp>(op);\n+    if (!wgmmaOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto opA = wgmmaOp.getOpA();\n+    auto opB = wgmmaOp.getOpB();\n+    auto opC = wgmmaOp.getOpC();\n+    auto m = wgmmaOp.getM();\n+    auto n = wgmmaOp.getN();\n+    auto k = wgmmaOp.getK();\n+    auto eltTypeC = wgmmaOp.getEltTypeC();\n+    auto eltTypeA = wgmmaOp.getEltTypeA();\n+    auto eltTypeB = wgmmaOp.getEltTypeB();\n+    auto layoutA = wgmmaOp.getLayoutA();\n+    auto layoutB = wgmmaOp.getLayoutB();\n+\n+    // Register checks\n+    auto typeA = opA.getType();\n+    auto typeB = opB.getType();\n+    auto typeC = opC.getType();\n+    auto structTypeA = typeA.dyn_cast<LLVM::LLVMStructType>();\n+    auto structTypeB = typeB.dyn_cast<LLVM::LLVMStructType>();\n+    auto structTypeC = typeC.dyn_cast<LLVM::LLVMStructType>();\n+    assert(!structTypeB && \"Operand B can not be registers\");\n+    assert(structTypeC && \"Operand C must be registers\");\n+\n+    // Element type, MNK shape and transposing support check\n+    // Reference:\n+    // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-instructions-wgmma-mma\n+    bool transA = layoutA == WGMMALayout::col;\n+    bool transB = layoutB == WGMMALayout::row;\n+    bool supported = false, needTransArgs = false, floatTypeWGMMA = false;\n+    assert(m % 8 == 0 && n % 8 == 0 && k % 8 == 0);\n+    // Below instructions do support transposing, must pass `trans` arguments\n+    supported |=\n+        (eltTypeA == WGMMAEltType::f16) && (eltTypeB == WGMMAEltType::f16) &&\n+        (eltTypeC == WGMMAEltType::f16 || eltTypeC == WGMMAEltType::f32) &&\n+        (m == 64 && 8 <= n && n <= 256 && k == 16);\n+    supported |= (eltTypeA == WGMMAEltType::bf16) &&\n+                 (eltTypeB == WGMMAEltType::bf16) &&\n+                 (eltTypeC == WGMMAEltType::f32) &&\n+                 (m == 64 && 8 <= n && n <= 256 && k == 16);\n+    needTransArgs = supported;\n+    floatTypeWGMMA = supported;\n+    // Below instructions do not support transposing\n+    if (!supported && !transA && !transB) {\n+      supported |= (eltTypeA == WGMMAEltType::tf32) &&\n+                   (eltTypeB == WGMMAEltType::tf32) &&\n+                   (eltTypeC == WGMMAEltType::f32) &&\n+                   (m == 64 && 8 <= n && n <= 256 && k == 8);\n+      supported |=\n+          (eltTypeA == WGMMAEltType::e4m3 || eltTypeA == WGMMAEltType::e5m2) &&\n+          (eltTypeB == WGMMAEltType::e4m3 || eltTypeB == WGMMAEltType::e5m2) &&\n+          (eltTypeC == WGMMAEltType::f16 || eltTypeC == WGMMAEltType::f32) &&\n+          (m == 64 && 8 <= n && n <= 256 && k == 32);\n+      floatTypeWGMMA = supported;\n+      // Below instructions are integer-based\n+      supported |= (eltTypeA == WGMMAEltType::s8) &&\n+                   (eltTypeB == WGMMAEltType::s8) &&\n+                   (eltTypeC == WGMMAEltType::s32) &&\n+                   (m == 64 && 8 <= n && n <= 224 && k == 32);\n+    }\n+    assert(supported && \"WGMMA type or shape is not supported\");\n+    PTXBuilder ptxBuilder;\n+    SmallVector<PTXBuilder::Operand *> oprs;\n+\n+    // Operands\n+    uint32_t asmOpIdx = 0;\n+\n+    // Operand C\n+    uint32_t numCRegs = structTypeC.getBody().size();\n+\n+    std::string args = \"\";\n+    args += \"{\";\n+    for (uint32_t i = 0; i < numCRegs; ++i) {\n+      args += \"$\" + std::to_string(asmOpIdx++) + (i == numCRegs - 1 ? \"\" : \",\");\n+      // LLVM does not support `+` semantic, we must repeat the arguments for\n+      // both input and outputs\n+      PTXBuilder::Operand *opr;\n+      if (structTypeC.getBody().front().isF32())\n+        opr = ptxBuilder.newOperand(\n+            extract_val(structTypeC.getBody()[i], opC, i), \"=f\");\n+      else\n+        opr = ptxBuilder.newOperand(\n+            extract_val(structTypeC.getBody()[i], opC, i), \"=r\");\n+      oprs.push_back(opr);\n+    }\n+    args += \"}, \";\n+\n+    for (uint32_t i = asmOpIdx - numCRegs; i < asmOpIdx; ++i) {\n+      auto *opr = ptxBuilder.newOperand(i);\n+      oprs.push_back(opr);\n+    }\n+\n+    // Note that LLVM will not skip the indexed repeating placeholders\n+    asmOpIdx += numCRegs;\n+    // Operand A\n+    if (structTypeA) {\n+      uint32_t numARegs = m * k / 128;\n+      assert(numARegs == structTypeA.getBody().size());\n+      args += \"{\";\n+      for (uint32_t i = 0; i < numARegs; ++i) {\n+        args +=\n+            \"$\" + std::to_string(asmOpIdx++) + (i == numARegs - 1 ? \"\" : \",\");\n+        auto *opr = ptxBuilder.newOperand(\n+            extract_val(structTypeA.getBody()[i], opA, i), \"f\");\n+        oprs.push_back(opr);\n+      }\n+      args += \"}, \";\n+    } else {\n+      args += \"$\" + std::to_string(asmOpIdx++) + \", \";\n+      auto *opr = ptxBuilder.newOperand(opA, \"l\");\n+      oprs.push_back(opr);\n+    }\n+\n+    // Operand B (must be `desc`)\n+    args += \"$\" + std::to_string(asmOpIdx++) + \", \";\n+    auto *opr = ptxBuilder.newOperand(opB, \"l\");\n+    oprs.push_back(opr);\n+\n+    // `scale-d` is 1 by default\n+    args += \"1\";\n+\n+    // `imm-scale-a`, and `imm-scale-b` are 1 by default only for float-based\n+    // WGMMA\n+    if (floatTypeWGMMA)\n+      args += \", 1, 1\";\n+\n+    // Push `trans-a` and `trans-b` args if needed (determined as constant)\n+    if (needTransArgs)\n+      args += \", \" + std::to_string(transA) + \", \" + std::to_string(transB);\n+\n+    auto ptxAsm = \"wgmma.mma_async.sync.aligned\"\n+                  \".m\" +\n+                  std::to_string(m) + \"n\" + std::to_string(n) + \"k\" +\n+                  std::to_string(k) + \".\" + stringifyEnum(eltTypeC).str() +\n+                  \".\" + stringifyEnum(eltTypeA).str() + \".\" +\n+                  stringifyEnum(eltTypeB).str() + \" \" + args + \";\";\n+\n+    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+    ptxInstr(oprs,\n+             /*onlyAttachMLIRArgs=*/true);\n+\n+    auto res =\n+        ptxBuilder.launch(rewriter, loc, structTypeC, /*hasSideEffect*/ true);\n+    rewriter.replaceOp(op, {res});\n+    return mlir::success();\n+  }\n+};\n+\n+class FenceMBarrierInitOpPattern\n+    : public NVGPUOpPatternBase<ttn::FenceMBarrierInitOp,\n+                                FenceMBarrierInitOpPattern> {\n+public:\n+  using Base =\n+      NVGPUOpPatternBase<ttn::FenceMBarrierInitOp, FenceMBarrierInitOpPattern>;\n+  using Base::Base;\n+\n+  std::string getPtxAsm(ttn::FenceMBarrierInitOp op) const {\n+    return \"fence.mbarrier_init.release.cluster;\";\n+  }\n+};\n+\n+class NamedBarrierArriveOpPattern : public mlir::RewritePattern {\n+public:\n+  NamedBarrierArriveOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::NamedBarrierArriveOp::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto namedBarrierArriveOp = llvm::dyn_cast<ttn::NamedBarrierArriveOp>(op);\n+    if (!namedBarrierArriveOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto bar = namedBarrierArriveOp.getBar();\n+    auto numThreads = namedBarrierArriveOp.getNumThreads();\n+    PTXBuilder ptxBuilder;\n+\n+    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(\"bar.arrive $0, $1;\");\n+    auto *barOpr = ptxBuilder.newOperand(bar, \"r\");\n+    auto *numThreadsOpr = ptxBuilder.newOperand(numThreads, \"r\");\n+    ptxInstr({barOpr, numThreadsOpr}, /*onlyAttachMLIRArgs=*/true);\n+\n+    auto asmReturnTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, asmReturnTy);\n+    rewriter.eraseOp(op);\n+    return mlir::success();\n+  }\n+};\n+\n+class NamedBarrierWaitOpPattern : public mlir::RewritePattern {\n+public:\n+  NamedBarrierWaitOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::NamedBarrierWaitOp::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto namedBarrierWaitOp = llvm::dyn_cast<ttn::NamedBarrierWaitOp>(op);\n+    if (!namedBarrierWaitOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto bar = namedBarrierWaitOp.getBar();\n+    auto numThreads = namedBarrierWaitOp.getNumThreads();\n+    PTXBuilder ptxBuilder;\n+\n+    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(\"bar.sync $0, $1;\");\n+    auto *barOpr = ptxBuilder.newOperand(bar, \"r\");\n+    auto *numThreadsOpr = ptxBuilder.newOperand(numThreads, \"r\");\n+    ptxInstr({barOpr, numThreadsOpr}, /*onlyAttachMLIRArgs=*/true);\n+\n+    auto asmReturnTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, asmReturnTy);\n+    rewriter.eraseOp(op);\n+    return mlir::success();\n+  }\n+};\n+\n+class CGABarrierArriveOpPattern\n+    : public NVGPUOpPatternBase<ttn::CGABarrierArriveOp,\n+                                CGABarrierArriveOpPattern> {\n+public:\n+  using Base =\n+      NVGPUOpPatternBase<ttn::CGABarrierArriveOp, CGABarrierArriveOpPattern>;\n+  using Base::Base;\n+  std::string getPtxAsm(ttn::CGABarrierArriveOp op) const {\n+    return \"barrier.cluster.arrive;\";\n+  }\n+};\n+\n+class CGABarrierWaitOpPattern\n+    : public NVGPUOpPatternBase<ttn::CGABarrierWaitOp,\n+                                CGABarrierWaitOpPattern> {\n+public:\n+  using Base =\n+      NVGPUOpPatternBase<ttn::CGABarrierWaitOp, CGABarrierWaitOpPattern>;\n+  using Base::Base;\n+  std::string getPtxAsm(ttn::CGABarrierWaitOp op) const {\n+    return \"barrier.cluster.wait;\";\n+  }\n+};\n+\n+class StoreDSmemOpPattern : public mlir::RewritePattern {\n+public:\n+  StoreDSmemOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::StoreDSmemOp::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto storeDSmemOp = llvm::dyn_cast<ttn::StoreDSmemOp>(op);\n+    if (!storeDSmemOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto addr = storeDSmemOp.getAddr();\n+    auto ctaId = storeDSmemOp.getCtaId();\n+    auto values = storeDSmemOp.getValues();\n+    auto pred = storeDSmemOp.getPred();\n+\n+    auto bitwidth = storeDSmemOp.getBitwidth();\n+    auto vec = storeDSmemOp.getVec();\n+    assert(\n+        (bitwidth == 8 || bitwidth == 16 || bitwidth == 32 || bitwidth == 64) &&\n+        \"invalid bitwidth\");\n+    assert((vec == 1 || vec == 2 || vec == 4) && vec == values.size() &&\n+           \"invalid vec size\");\n+\n+    PTXBuilder ptxBuilder;\n+\n+    std::string ptxAsm = \"{\\n\\t\"\n+                         \".reg .u32 remoteAddr;\\n\\t\"\n+                         \"mapa.shared::cluster.u32 remoteAddr, $0, $1;\\n\\t\"\n+                         \".reg .pred p;\\n\\t\"\n+                         \"mov.pred p, $2;\\n\\t\"\n+                         \"@p st.shared::cluster\";\n+    if (vec > 1)\n+      ptxAsm += \".v\" + std::to_string(vec);\n+    ptxAsm += \".u\" + std::to_string(bitwidth) + \" [remoteAddr], \";\n+    if (vec == 1)\n+      ptxAsm += \"$3\";\n+    else if (vec == 2)\n+      ptxAsm += \"{$3, $4}\";\n+    else if (vec == 4)\n+      ptxAsm += \"{$3, $4, $5, $6}\";\n+    ptxAsm += \";\\n\\t\";\n+    ptxAsm += \"}\\n\";\n+    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+\n+    std::string c = bitwidth == 16 ? \"h\" : (bitwidth == 32 ? \"r\" : \"l\");\n+    SmallVector<PTXBuilder::Operand *> oprs;\n+    auto *addrOpr = ptxBuilder.newOperand(addr, \"r\");\n+    oprs.push_back(addrOpr);\n+    auto *ctaIdOpr = ptxBuilder.newOperand(ctaId, \"r\");\n+    oprs.push_back(ctaIdOpr);\n+    auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n+    oprs.push_back(predOpr);\n+    for (unsigned i = 0; i < values.size(); i++) {\n+      auto *valueOpr = ptxBuilder.newOperand(values[i], c);\n+      oprs.push_back(valueOpr);\n+    }\n+    ptxInstr(oprs,\n+             /*onlyAttachMLIRArgs=*/true);\n+\n+    auto asmReturnTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, asmReturnTy, /*hasSideEffect*/ true);\n+    rewriter.eraseOp(op);\n+    return mlir::success();\n+  }\n+};\n+\n+class Sts64OpPattern : public mlir::RewritePattern {\n+public:\n+  Sts64OpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::Sts64Op::getOperationName(), 1, context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto sts64Op = llvm::dyn_cast<ttn::Sts64Op>(op);\n+    if (!sts64Op)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto offset = sts64Op.getOffset();\n+    auto d0 = sts64Op.getD0();\n+    auto d1 = sts64Op.getD1();\n+\n+    PTXBuilder ptxBuilder;\n+\n+    std::string ptxAsm = \"st.shared.v2.b32 [$0], {$1, $2}\";\n+    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+\n+    SmallVector<PTXBuilder::Operand *> oprs;\n+    auto *addrOpr = ptxBuilder.newOperand(offset, \"r\");\n+    auto *d0Opr = ptxBuilder.newOperand(d0, \"r\");\n+    auto *d1Opr = ptxBuilder.newOperand(d1, \"r\");\n+\n+    ptxInstr({addrOpr, d0Opr, d1Opr},\n+             /*onlyAttachMLIRArgs=*/true);\n+\n+    auto asmReturnTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, asmReturnTy);\n+    rewriter.eraseOp(op);\n+    return mlir::success();\n+  }\n+};\n+\n+class RegAllocOpPattern\n+    : public NVGPUOpPatternBase<ttn::RegAllocOp, RegAllocOpPattern> {\n+public:\n+  using Base = NVGPUOpPatternBase<ttn::RegAllocOp, RegAllocOpPattern>;\n+  using Base::Base;\n+\n+  std::string getPtxAsm(ttn::RegAllocOp op) const {\n+    auto regCount = op.getRegCount();\n+    return \"setmaxnreg.inc.sync.aligned.u32 \" + std::to_string(regCount) + \";\";\n+  }\n+};\n+\n+class RegDeallocOpPattern\n+    : public NVGPUOpPatternBase<ttn::RegDeallocOp, RegDeallocOpPattern> {\n+public:\n+  using Base = NVGPUOpPatternBase<ttn::RegDeallocOp, RegDeallocOpPattern>;\n+  using Base::Base;\n+\n+  std::string getPtxAsm(ttn::RegDeallocOp op) const {\n+    auto regCount = op.getRegCount();\n+    return \"setmaxnreg.dec.sync.aligned.u32 \" + std::to_string(regCount) + \";\";\n+  }\n+};\n+\n+class ClusterCTAIdOpPattern : public mlir::RewritePattern {\n+public:\n+  ClusterCTAIdOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::ClusterCTAIdOp::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto clusterCTAIdOp = llvm::dyn_cast<ttn::ClusterCTAIdOp>(op);\n+    if (!clusterCTAIdOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+\n+    auto x = getSRegValue(rewriter, loc, \"%cluster_ctaid.x\");\n+    auto y = getSRegValue(rewriter, loc, \"%cluster_ctaid.y\");\n+    auto z = getSRegValue(rewriter, loc, \"%cluster_ctaid.z\");\n+    auto nx = getSRegValue(rewriter, loc, \"%cluster_nctaid.x\");\n+    auto ny = getSRegValue(rewriter, loc, \"%cluster_nctaid.y\");\n+    auto res = add(x, mul(add(y, mul(z, ny)), nx));\n+    rewriter.replaceOp(op, {res});\n+    return mlir::success();\n+  }\n+};\n+\n+class WGMMADescCreateOpPattern : public mlir::RewritePattern {\n+public:\n+  WGMMADescCreateOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::WGMMADescCreateOp::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto wgmmaDescCreateOp = llvm::dyn_cast<ttn::WGMMADescCreateOp>(op);\n+    if (!wgmmaDescCreateOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto buffer = wgmmaDescCreateOp.getBuffer();\n+    auto height = wgmmaDescCreateOp.getHeight();\n+    uint32_t mode = static_cast<uint32_t>(wgmmaDescCreateOp.getMode());\n+\n+    auto smem_nvvm_pointer = ptrtoint(i64_ty, buffer);\n+\n+    Value desc = int_val(64, 0);\n+    uint64_t swizzling = (mode == 1 ? 128 : mode == 2 ? 64 : 32);\n+    Value swizzling_ = int_val(64, swizzling);\n+    Value smem_address_bit = smem_nvvm_pointer;\n+\n+    Value strideDimension =\n+        lshr(shl(swizzling_, int_val(64, 3)), int_val(64, 4));\n+    Value height64 = zext(i64_ty, height);\n+    Value leadingDimension = lshr(mul(height64, swizzling_), int_val(64, 4));\n+\n+    // Value baseOffset = int_val(64, 0);\n+    Value startAddr =\n+        lshr(shl(smem_address_bit, int_val(64, 46)), int_val(64, 50));\n+\n+    Value mode_ = int_val(64, mode);\n+    desc = or_(desc, shl(mode_, int_val(64, 62)));\n+    desc = or_(desc, shl(strideDimension, int_val(64, 32)));\n+    desc = or_(desc, shl(leadingDimension, int_val(64, 16)));\n+    // desc = or_(desc, shl(baseOffset, int_val(64, 49)));\n+    desc = or_(desc, startAddr);\n+\n+    rewriter.replaceOp(op, {desc});\n+    return mlir::success();\n+  }\n+};\n+\n+class OffsetOfSts64OpPattern : public mlir::RewritePattern {\n+public:\n+  OffsetOfSts64OpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::OffsetOfSts64Op::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto offsetOfSts64Op = llvm::dyn_cast<ttn::OffsetOfSts64Op>(op);\n+    if (!offsetOfSts64Op)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto threadId = offsetOfSts64Op.getThreadId();\n+    auto rowOfWarp = offsetOfSts64Op.getRowOfWarp();\n+    auto elemIdx = offsetOfSts64Op.getElemIdx();\n+    auto leadingDimOffset = offsetOfSts64Op.getLeadingDimOffset();\n+    auto rowStride = offsetOfSts64Op.getRowStride();\n+    auto swizzleEnabled = offsetOfSts64Op.getSwizzleEnabled();\n+\n+    if (swizzleEnabled) {\n+      assert((rowStride == 32 || rowStride == 64 || rowStride == 128) &&\n+             \"wrong rowString for swizzleEnabled\");\n+    }\n+\n+    uint32_t perPhase = 0;\n+    uint32_t maxPhase = 0;\n+    if (rowStride == 128) {\n+      perPhase = 1;\n+      maxPhase = 8;\n+    } else if (rowStride == 64) {\n+      perPhase = 2;\n+      maxPhase = 4;\n+    } else if (rowStride == 32) {\n+      perPhase = 4;\n+      maxPhase = 2;\n+    }\n+\n+    auto laneId = and_(threadId, i32_val(0x1f));\n+    auto myRow =\n+        add(mul(and_(lshr(elemIdx, i32_val(1)), i32_val(0x1)), i32_val(8)),\n+            udiv(laneId, i32_val(4)));\n+    auto myCol = add(mul(udiv(elemIdx, i32_val(4)), i32_val(8)),\n+                     mul(urem(laneId, i32_val(4)), i32_val(2)));\n+    myRow = add(myRow, rowOfWarp);\n+    auto phase = urem(udiv(myRow, i32_val(perPhase)), i32_val(maxPhase));\n+    auto lineOffset =\n+        add(mul(urem(myRow, i32_val(perPhase)), i32_val(rowStride)),\n+            mul(myCol, i32_val(4)));\n+    auto colOffset =\n+        add(mul(xor_(udiv(lineOffset, i32_val(16)), phase), i32_val(16)),\n+            urem(lineOffset, i32_val(16)));\n+    auto offset =\n+        add(mul(udiv(myRow, i32_val(perPhase)), i32_val(128)), colOffset);\n+\n+    rewriter.replaceOp(op, {offset});\n+    return mlir::success();\n+  }\n+};\n+\n+class OffsetOfStmatrixV4OpPattern : public mlir::RewritePattern {\n+public:\n+  OffsetOfStmatrixV4OpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::OffsetOfStmatrixV4Op::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto offsetOfStmatrixV4Op = llvm::dyn_cast<ttn::OffsetOfStmatrixV4Op>(op);\n+    if (!offsetOfStmatrixV4Op)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto threadId = offsetOfStmatrixV4Op.getThreadId();\n+    auto rowOfWarp = offsetOfStmatrixV4Op.getRowOfWarp();\n+    auto elemIdx = offsetOfStmatrixV4Op.getElemIdx();\n+    auto leadingDimOffset = offsetOfStmatrixV4Op.getLeadingDimOffset();\n+    auto rowStride = offsetOfStmatrixV4Op.getRowStride();\n+    auto swizzleEnabled = offsetOfStmatrixV4Op.getSwizzleEnabled();\n+\n+    if (swizzleEnabled) {\n+      uint32_t perPhase = 0;\n+      uint32_t maxPhase = 0;\n+      if (rowStride == 64) {\n+        perPhase = 1;\n+        maxPhase = 8;\n+      } else if (rowStride == 32) {\n+        perPhase = 2;\n+        maxPhase = 4;\n+      } else if (rowStride == 16) {\n+        perPhase = 4;\n+        maxPhase = 2;\n+      }\n+\n+      Value iterOfCol = udiv(elemIdx, i32_val(8));\n+      Value myRow = add(rowOfWarp, and_(threadId, i32_val(0xf)));\n+      Value myCol =\n+          mul(and_(lshr(threadId, i32_val(4)), i32_val(0x1)), i32_val(8));\n+      myCol = add(myCol, mul(iterOfCol, i32_val(16)));\n+\n+      Value offset0 =\n+          mul(udiv(myCol, i32_val(rowStride)), i32_val(leadingDimOffset));\n+      myCol = urem(myCol, i32_val(rowStride));\n+\n+      Value phase = urem(udiv(myRow, i32_val(perPhase)), i32_val(maxPhase));\n+\n+      Value lineOffset =\n+          add(mul(urem(myRow, i32_val(perPhase)), i32_val(rowStride)), myCol);\n+      Value colOffset =\n+          add(mul(xor_(udiv(lineOffset, i32_val(8)), phase), i32_val(8)),\n+              urem(lineOffset, i32_val(8)));\n+      Value offset1 =\n+          add(mul(udiv(myRow, i32_val(perPhase)), i32_val(64)), colOffset);\n+\n+      Value res = add(offset1, offset0);\n+\n+      rewriter.replaceOp(op, {res});\n+    } else {\n+      Value iterOfCol = udiv(elemIdx, i32_val(4));\n+      Value myRow = add(rowOfWarp, and_(threadId, i32_val(0xf)));\n+      Value myCol =\n+          mul(and_(lshr(threadId, i32_val(4)), i32_val(0x1)), i32_val(8));\n+      myCol = add(myCol, mul(iterOfCol, i32_val(16)));\n+\n+      Value offset =\n+          add(mul(myRow, i32_val(rowStride)), mul(myCol, i32_val(2)));\n+      rewriter.replaceOp(op, {offset});\n+    }\n+    return mlir::success();\n+  }\n+};\n+\n+class ConvertNVGPUToLLVM : public ConvertNVGPUToLLVMBase<ConvertNVGPUToLLVM> {\n+\n+public:\n+  explicit ConvertNVGPUToLLVM() {}\n+\n+  void runOnOperation() override {\n+    MLIRContext *context = &getContext();\n+    ModuleOp mod = getOperation();\n+    RewritePatternSet patterns(context);\n+\n+    patterns.add<CGABarrierSyncOpPattern>(context);\n+    patterns.add<FenceAsyncSharedOpPattern>(context);\n+    patterns.add<WGMMAFenceOpPattern>(context);\n+    patterns.add<WGMMACommitGroupOpPattern>(context);\n+    patterns.add<WGMMAWaitGroupOpPattern>(context);\n+    patterns.add<StoreMatrixOpPattern>(context);\n+    patterns.add<OffsetOfStmatrixV4OpPattern>(context);\n+    patterns.add<WGMMADescCreateOpPattern>(context);\n+    patterns.add<MBarrierInitOpPattern>(context);\n+    patterns.add<MBarrierArriveOpPattern>(context);\n+    patterns.add<MBarrierWaitOpPattern>(context);\n+    patterns.add<ClusterArriveOpPattern>(context);\n+    patterns.add<ClusterWaitOpPattern>(context);\n+    patterns.add<TMALoadTiledOpPattern>(context);\n+    patterns.add<TMAStoreTiledOpPattern>(context);\n+    patterns.add<LoadDSmemOpPattern>(context);\n+    patterns.add<ClusterCTAIdOpPattern>(context);\n+    patterns.add<RegAllocOpPattern>(context);\n+    patterns.add<RegDeallocOpPattern>(context);\n+    patterns.add<WGMMAOpPattern>(context);\n+    patterns.add<NamedBarrierWaitOpPattern>(context);\n+    patterns.add<NamedBarrierArriveOpPattern>(context);\n+\n+    patterns.add<FenceMBarrierInitOpPattern>(context);\n+    patterns.add<StoreDSmemOpPattern>(context);\n+    patterns.add<Sts64OpPattern>(context);\n+    patterns.add<OffsetOfSts64OpPattern>(context);\n+    patterns.add<CGABarrierWaitOpPattern>(context);\n+    patterns.add<CGABarrierArriveOpPattern>(context);\n+    if (applyPatternsAndFoldGreedily(mod, std::move(patterns)).failed())\n+      signalPassFailure();\n+  }\n+};\n+\n+} // anonymous namespace\n+\n+namespace mlir {\n+namespace triton {\n+\n+std::unique_ptr<OperationPass<ModuleOp>> createConvertNVGPUToLLVMPass() {\n+  return std::make_unique<::ConvertNVGPUToLLVM>();\n+}\n+\n+} // namespace triton\n+} // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -14,7 +14,6 @@ add_mlir_conversion_library(TritonGPUToLLVM\n     TritonGPUToLLVM.cpp\n     GCNAsmFormat.cpp\n     PTXAsmFormat.cpp\n-    TritonGPUToLLVMPass.cpp\n     ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp\n     ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp\n     ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -125,7 +125,7 @@ struct DotWaitOpConversion\n   matchAndRewrite(triton::nvidia_gpu::DotWaitOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     auto pendings = op.getPendings();\n-    rewriter.create<triton::nvgpu::WGMMAWaitOp>(op.getLoc(), pendings);\n+    rewriter.create<triton::nvgpu::WGMMAWaitGroupOp>(op.getLoc(), pendings);\n \n     // Safe to remove the op since it doesn't have any return value.\n     rewriter.eraseOp(op);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/WGMMA.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -340,7 +340,7 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n   rewriter.create<triton::nvgpu::WGMMACommitGroupOp>(loc);\n \n   if (sync)\n-    rewriter.create<triton::nvgpu::WGMMAWaitOp>(loc, 0);\n+    rewriter.create<triton::nvgpu::WGMMAWaitGroupOp>(loc, 0);\n \n   SmallVector<Value> results =\n       unpackAccumulator(rewriter, loc, mmaResults, dTensorTy);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/PTXAsmFormat.cpp", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -50,6 +50,14 @@ PTXBuilder::Operand *PTXBuilder::newOperand(StringRef constraint, bool init) {\n   return opr;\n }\n \n+PTXBuilder::Operand *PTXBuilder::newOperand(unsigned operandIndex) {\n+  assert(operandIndex < oprCounter && \"operand index out of range\");\n+  auto *opr = newOperand();\n+  opr->idx = oprCounter++;\n+  opr->constraint = std::to_string(operandIndex);\n+  return opr;\n+}\n+\n PTXBuilder::Operand *PTXBuilder::newConstantOperand(const std::string &v) {\n   argArchive.emplace_back(std::make_unique<Operand>());\n   argArchive.back()->repr = [v](int idx) { return v; };"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -6,6 +6,7 @@ using namespace mlir;\n using namespace mlir::triton;\n \n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n+using ::mlir::LLVM::getSRegValue;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 0, "deletions": 14, "changes": 14, "file_content_changes": "@@ -260,20 +260,6 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return tid;\n   }\n \n-  static Value getSRegValue(OpBuilder &b, Location loc,\n-                            const std::string &sRegStr) {\n-    PTXBuilder builder;\n-    auto &mov = builder.create(\"mov\")->o(\"u32\");\n-    auto *destOpr = builder.newOperand(\"=r\");\n-    auto *sRegOpr = builder.newConstantOperand(sRegStr);\n-    mov(destOpr, sRegOpr);\n-    Value val = builder.launch(b, loc, b.getIntegerType(32), false);\n-\n-    auto cast = b.create<UnrealizedConversionCastOp>(\n-        loc, TypeRange{b.getIntegerType(32)}, ValueRange{val});\n-    return cast.getResult(0);\n-  }\n-\n   Value getClusterCTAId(ConversionPatternRewriter &rewriter,\n                         Location loc) const {\n     return rewriter.create<triton::nvgpu::ClusterCTAIdOp>("}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.cpp", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -288,6 +288,18 @@ Value shflUpSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n                  int i) {\n   return commonShflSync(loc, rewriter, val, i, \"up\", \"0x0\");\n }\n+Value getSRegValue(OpBuilder &b, Location loc, const std::string &sRegStr) {\n+  PTXBuilder builder;\n+  auto &mov = builder.create(\"mov\")->o(\"u32\");\n+  auto *destOpr = builder.newOperand(\"=r\");\n+  auto *sRegOpr = builder.newConstantOperand(sRegStr);\n+  mov(destOpr, sRegOpr);\n+  Value val = builder.launch(b, loc, b.getIntegerType(32), false);\n+\n+  auto cast = b.create<UnrealizedConversionCastOp>(\n+      loc, TypeRange{b.getIntegerType(32)}, ValueRange{val});\n+  return cast.getResult(0);\n+}\n \n Value addStringToModule(Location loc, ConversionPatternRewriter &rewriter,\n                         StringRef key, StringRef content) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -29,6 +29,7 @@\n #define umin(...) rewriter.create<LLVM::UMinOp>(loc, __VA_ARGS__)\n #define fmin(...) rewriter.create<LLVM::MinNumOp>(loc, __VA_ARGS__)\n #define shl(...) rewriter.create<LLVM::ShlOp>(loc, __VA_ARGS__)\n+#define lshr(...) rewriter.create<LLVM::LShrOp>(loc, __VA_ARGS__)\n #define and_(...) rewriter.create<LLVM::AndOp>(loc, __VA_ARGS__)\n #define xor_(...) rewriter.create<LLVM::XOrOp>(loc, __VA_ARGS__)\n #define or_(...) rewriter.create<LLVM::OrOp>(loc, __VA_ARGS__)\n@@ -323,7 +324,7 @@ Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n                int i);\n Value shflUpSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n                  int i);\n-\n+Value getSRegValue(OpBuilder &b, Location loc, const std::string &sRegStr);\n Value addStringToModule(Location loc, ConversionPatternRewriter &rewriter,\n                         StringRef key, StringRef content);\n "}, {"filename": "lib/Dialect/NVGPU/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,2 +1 @@\n add_subdirectory(IR)\n-add_subdirectory(ToLLVMIR)"}, {"filename": "lib/Dialect/NVGPU/ToLLVMIR/CMakeLists.txt", "status": "removed", "additions": 0, "deletions": 17, "changes": 17, "file_content_changes": "@@ -1,17 +0,0 @@\n-add_mlir_translation_library(NVGPUToLLVMIR\n-  NVGPUToLLVMIR.cpp\n-\n-  DEPENDS\n-  NVGPUTableGen\n-\n-  LINK_COMPONENTS\n-  Core\n-\n-  LINK_LIBS PUBLIC\n-  MLIRIR\n-  MLIRLLVMDialect\n-  MLIRNVVMDialect\n-  MLIRSupport\n-  MLIRTargetLLVMIRExport\n-  NVGPUIR\n-  )"}, {"filename": "lib/Dialect/NVGPU/ToLLVMIR/NVGPUToLLVMIR.cpp", "status": "removed", "additions": 0, "deletions": 782, "changes": 782, "file_content_changes": "@@ -1,782 +0,0 @@\n-/*\n- * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n- *\n- * Permission is hereby granted, free of charge, to any person obtaining\n- * a copy of this software and associated documentation files\n- * (the \"Software\"), to deal in the Software without restriction,\n- * including without limitation the rights to use, copy, modify, merge,\n- * publish, distribute, sublicense, and/or sell copies of the Software,\n- * and to permit persons to whom the Software is furnished to do so,\n- * subject to the following conditions:\n- *\n- * The above copyright notice and this permission notice shall be\n- * included in all copies or substantial portions of the Software.\n- *\n- * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n- * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n- * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n- * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n- * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n- */\n-\n-#include \"triton/Dialect/NVGPU/ToLLVMIR/NVGPUToLLVMIR.h\"\n-#include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n-#include \"mlir/IR/Operation.h\"\n-#include \"mlir/Target/LLVMIR/ModuleTranslation.h\"\n-#include \"triton/Dialect/NVGPU/IR/Dialect.h\"\n-\n-#include \"llvm/IR/IRBuilder.h\"\n-#include \"llvm/IR/InlineAsm.h\"\n-#include \"llvm/IR/IntrinsicsNVPTX.h\"\n-\n-using namespace mlir;\n-using namespace mlir::LLVM;\n-\n-namespace {\n-static llvm::FunctionCallee\n-getExternalFuncOP(llvm::Module *module, llvm::StringRef funcName,\n-                  llvm::Type *retTy, ArrayRef<llvm::Type *> argTys = {}) {\n-  return module->getOrInsertFunction(\n-      funcName, llvm::FunctionType::get(retTy, argTys, false),\n-      llvm::AttributeList{});\n-}\n-\n-llvm::Value *createExternalCall(llvm::IRBuilderBase &builder,\n-                                llvm::StringRef funcName,\n-                                ArrayRef<llvm::Value *> args = {},\n-                                ArrayRef<llvm::Type *> tys = {}) {\n-  auto *module = builder.GetInsertBlock()->getModule();\n-  auto *func = module->getFunction(funcName);\n-\n-  if (func == nullptr) {\n-    llvm::SmallVector<llvm::Type *> argTys;\n-    for (auto *arg : args) {\n-      argTys.push_back(arg->getType());\n-    }\n-\n-    llvm::Type *retTy;\n-    if (tys.empty())\n-      retTy = builder.getVoidTy();\n-    else\n-      retTy = tys[0];\n-\n-    func = dyn_cast<llvm::Function>(\n-        getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n-  }\n-\n-  return builder.CreateCall(func, args);\n-}\n-\n-void createMBarrierArrive(llvm::IRBuilderBase &builder,\n-                          mlir::triton::nvgpu::MBarriveType arriveType,\n-                          llvm::Value *barrier, llvm::Value *pred,\n-                          llvm::Value *ctaId, uint32_t txCount) {\n-  auto *module = builder.GetInsertBlock()->getModule();\n-\n-  llvm::SmallVector<llvm::Type *> argTys;\n-  argTys.push_back(barrier->getType());\n-  llvm::Type *retTy = builder.getVoidTy();\n-\n-  if (arriveType == mlir::triton::nvgpu::MBarriveType::normal) {\n-    argTys.push_back(pred->getType());\n-    auto *func = dyn_cast<llvm::Function>(\n-        getExternalFuncOP(module, \"__nv_mbarrier_arrive_normal\", retTy, argTys)\n-            .getCallee());\n-    builder.CreateCall(func, {barrier, pred});\n-  } else if (arriveType == mlir::triton::nvgpu::MBarriveType::cp_async) {\n-    argTys.push_back(pred->getType());\n-    auto *func = dyn_cast<llvm::Function>(\n-        getExternalFuncOP(module, \"__nv_mbarrier_arrive_cp_async\", retTy,\n-                          argTys)\n-            .getCallee());\n-    builder.CreateCall(func, {barrier, pred});\n-  } else if (arriveType == mlir::triton::nvgpu::MBarriveType::expect_tx) {\n-    assert(txCount > 0 && \"txCount should be valid\");\n-    argTys.push_back(builder.getInt32Ty());\n-    argTys.push_back(pred->getType());\n-\n-    auto *func = dyn_cast<llvm::Function>(\n-        getExternalFuncOP(module, \"__nv_mbarrier_arrive_expect_tx\", retTy,\n-                          argTys)\n-            .getCallee());\n-    builder.CreateCall(func, {barrier, builder.getInt32(txCount), pred});\n-  } else if (arriveType == mlir::triton::nvgpu::MBarriveType::remote) {\n-    assert(ctaId && \"ctaId should have a valid value\");\n-    argTys.push_back(ctaId->getType());\n-    argTys.push_back(pred->getType());\n-\n-    auto *func = dyn_cast<llvm::Function>(\n-        getExternalFuncOP(module, \"__nv_mbarrier_arrive_remote\", retTy, argTys)\n-            .getCallee());\n-    builder.CreateCall(func, {barrier, ctaId, pred});\n-  }\n-\n-  return;\n-}\n-\n-llvm::Value *createWGMMADesc(llvm::IRBuilderBase &builder, llvm::Value *buffer,\n-                             mlir::triton::nvgpu::WGMMADescMode mode,\n-                             llvm::Value *height) {\n-  llvm::SmallVector<llvm::Type *> argTys;\n-  argTys.push_back(buffer->getType());\n-  argTys.push_back(builder.getInt32Ty());\n-  argTys.push_back(height->getType());\n-  llvm::Type *retTy = builder.getInt64Ty();\n-\n-  llvm::Value *mode_ = builder.getInt32((uint32_t)mode);\n-  auto *module = builder.GetInsertBlock()->getModule();\n-  auto *func = dyn_cast<llvm::Function>(\n-      getExternalFuncOP(module, \"__nv_get_wgmma_desc\", retTy, argTys)\n-          .getCallee());\n-  return builder.CreateCall(func, {buffer, mode_, height});\n-}\n-\n-static std::string getTMALoadFuncName(bool tiled, bool mcast,\n-                                      uint32_t dimSize) {\n-  std::string funcName;\n-  llvm::raw_string_ostream os(funcName);\n-  os << \"__nv_tma_load\";\n-  if (tiled)\n-    os << \"_tiled\";\n-  else\n-    os << \"_im2col\";\n-\n-  if (mcast)\n-    os << \"_mcast\";\n-\n-  os << \"_\" << dimSize << \"d\";\n-\n-  return funcName;\n-}\n-\n-void createTMALoadTiled(llvm::IRBuilderBase &builder, llvm::Value *dst,\n-                        llvm::Value *mbarrier, llvm::Value *tmaDesc,\n-                        llvm::Value *l2Desc, llvm::Value *mcastMask,\n-                        llvm::Value *pred,\n-                        llvm::SmallVector<llvm::Value *> coords) {\n-  assert(coords.size() >= 2 && coords.size() <= 5 && \"invalid coords.size()\");\n-  auto funcName = getTMALoadFuncName(true, mcastMask != 0, coords.size());\n-  llvm::Type *retTy = builder.getVoidTy();\n-  llvm::SmallVector<llvm::Value *> args;\n-  llvm::SmallVector<llvm::Type *> argTys;\n-\n-  argTys.push_back(tmaDesc->getType());\n-  args.push_back(tmaDesc);\n-\n-  argTys.push_back(dst->getType());\n-  args.push_back(dst);\n-\n-  argTys.push_back(mbarrier->getType());\n-  args.push_back(mbarrier);\n-  for (auto *c : coords) {\n-    argTys.push_back(c->getType());\n-    args.push_back(c);\n-  }\n-  argTys.push_back(l2Desc->getType());\n-  args.push_back(l2Desc);\n-\n-  if (mcastMask != nullptr) {\n-    argTys.push_back(builder.getInt16Ty());\n-    args.push_back(mcastMask);\n-  }\n-\n-  argTys.push_back(pred->getType());\n-  args.push_back(pred);\n-\n-  auto *module = builder.GetInsertBlock()->getModule();\n-  auto *func = dyn_cast<llvm::Function>(\n-      getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n-  builder.CreateCall(func, args);\n-\n-  return;\n-}\n-\n-void createTMALoadIm2col(llvm::IRBuilderBase &builder, llvm::Value *dst,\n-                         llvm::Value *mbarrier, llvm::Value *tmaDesc,\n-                         llvm::Value *l2Desc, uint16_t mcastMask,\n-                         llvm::Value *im2colOffsets, llvm::Value *pred,\n-                         llvm::SmallVector<llvm::Value *> coords) {\n-  assert(coords.size() >= 3 && coords.size() <= 5 &&\n-         \"invalid coords.size() for im2col\");\n-  auto funcName = getTMALoadFuncName(false, mcastMask != 0, coords.size());\n-  llvm::Type *retTy = builder.getVoidTy();\n-  llvm::SmallVector<llvm::Value *> args;\n-  llvm::SmallVector<llvm::Type *> argTys;\n-\n-  argTys.push_back(tmaDesc->getType());\n-  args.push_back(tmaDesc);\n-\n-  argTys.push_back(dst->getType());\n-  args.push_back(dst);\n-\n-  argTys.push_back(mbarrier->getType());\n-  args.push_back(mbarrier);\n-  for (auto *c : coords) {\n-    argTys.push_back(c->getType());\n-    args.push_back(c);\n-  }\n-\n-  {\n-    auto offsetsType = dyn_cast<llvm::StructType>(im2colOffsets->getType());\n-    auto subTypes = offsetsType->elements();\n-    assert((coords.size() - subTypes.size() == 2) && \"wrong imcolOffsets\");\n-    unsigned idx = 0;\n-    for (auto subType : subTypes) {\n-      argTys.push_back(subType);\n-      args.push_back(builder.CreateExtractValue(im2colOffsets, {idx}));\n-      idx++;\n-    }\n-  }\n-\n-  argTys.push_back(l2Desc->getType());\n-  args.push_back(l2Desc);\n-\n-  if (mcastMask != 0) {\n-    argTys.push_back(builder.getInt16Ty());\n-    llvm::Value *mcastMask_ = builder.getInt16(mcastMask);\n-    args.push_back(mcastMask_);\n-  }\n-\n-  argTys.push_back(pred->getType());\n-  args.push_back(pred);\n-\n-  auto *module = builder.GetInsertBlock()->getModule();\n-  auto *func = dyn_cast<llvm::Function>(\n-      getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n-  builder.CreateCall(func, args);\n-\n-  return;\n-}\n-\n-llvm::Value *createWGMMA(llvm::IRBuilderBase &builder, uint32_t m, uint32_t n,\n-                         uint32_t k, mlir::triton::nvgpu::WGMMAEltType eltTypeC,\n-                         mlir::triton::nvgpu::WGMMAEltType eltTypeA,\n-                         mlir::triton::nvgpu::WGMMAEltType eltTypeB,\n-                         mlir::triton::nvgpu::WGMMALayout layoutA,\n-                         mlir::triton::nvgpu::WGMMALayout layoutB,\n-                         llvm::Value *opA, llvm::Value *opB, llvm::Value *opC) {\n-  // Simplify enum namespace\n-  using namespace mlir::triton::nvgpu;\n-\n-  // Register checks\n-  auto typeA = opA->getType();\n-  auto typeB = opB->getType();\n-  auto typeC = opC->getType();\n-  auto structTypeA = dyn_cast<llvm::StructType>(typeA);\n-  auto structTypeB = dyn_cast<llvm::StructType>(typeB);\n-  auto structTypeC = dyn_cast<llvm::StructType>(typeC);\n-  assert(!structTypeB && \"Operand B can not be registers\");\n-  assert(structTypeC && \"Operand C must be registers\");\n-\n-  // Element type, MNK shape and transposing support check\n-  // Reference:\n-  // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-instructions-wgmma-mma\n-  bool transA = layoutA == WGMMALayout::col;\n-  bool transB = layoutB == WGMMALayout::row;\n-  bool supported = false, needTransArgs = false, floatTypeWGMMA = false;\n-  assert(m % 8 == 0 && n % 8 == 0 && k % 8 == 0);\n-  // Below instructions do support transposing, must pass `trans` arguments\n-  supported |=\n-      (eltTypeA == WGMMAEltType::f16) && (eltTypeB == WGMMAEltType::f16) &&\n-      (eltTypeC == WGMMAEltType::f16 || eltTypeC == WGMMAEltType::f32) &&\n-      (m == 64 && 8 <= n && n <= 256 && k == 16);\n-  supported |= (eltTypeA == WGMMAEltType::bf16) &&\n-               (eltTypeB == WGMMAEltType::bf16) &&\n-               (eltTypeC == WGMMAEltType::f32) &&\n-               (m == 64 && 8 <= n && n <= 256 && k == 16);\n-  needTransArgs = supported;\n-  floatTypeWGMMA = supported;\n-  // Below instructions do not support transposing\n-  if (!supported && !transA && !transB) {\n-    supported |= (eltTypeA == WGMMAEltType::tf32) &&\n-                 (eltTypeB == WGMMAEltType::tf32) &&\n-                 (eltTypeC == WGMMAEltType::f32) &&\n-                 (m == 64 && 8 <= n && n <= 256 && k == 8);\n-    supported |=\n-        (eltTypeA == WGMMAEltType::e4m3 || eltTypeA == WGMMAEltType::e5m2) &&\n-        (eltTypeB == WGMMAEltType::e4m3 || eltTypeB == WGMMAEltType::e5m2) &&\n-        (eltTypeC == WGMMAEltType::f16 || eltTypeC == WGMMAEltType::f32) &&\n-        (m == 64 && 8 <= n && n <= 256 && k == 32);\n-    floatTypeWGMMA = supported;\n-    // Below instructions are integer-based\n-    supported |= (eltTypeA == WGMMAEltType::s8) &&\n-                 (eltTypeB == WGMMAEltType::s8) &&\n-                 (eltTypeC == WGMMAEltType::s32) &&\n-                 (m == 64 && 8 <= n && n <= 224 && k == 32);\n-  }\n-  assert(supported && \"WGMMA type or shape is not supported\");\n-\n-  // Build PTX asm\n-  std::string ptxAsm;\n-  std::string constraints;\n-  llvm::raw_string_ostream asmOs(ptxAsm);\n-  llvm::raw_string_ostream conOs(constraints);\n-  llvm::SmallVector<llvm::Type *> argTypes;\n-  llvm::SmallVector<llvm::Value *> args;\n-\n-  // MMA instruction\n-  asmOs << \"wgmma.mma_async.sync.aligned\"\n-        << \".m\" << m << \"n\" << n << \"k\" << k << \".\" << stringifyEnum(eltTypeC)\n-        << \".\" << stringifyEnum(eltTypeA) << \".\" << stringifyEnum(eltTypeB)\n-        << \" \";\n-\n-  // Operands\n-  uint32_t asmOpIdx = 0;\n-\n-  // Operand C\n-  uint32_t numCRegs = structTypeC->getStructNumElements();\n-  asmOs << \"{\";\n-  for (uint32_t i = 0; i < numCRegs; ++i) {\n-    argTypes.push_back(structTypeC->getElementType(i));\n-    args.push_back(builder.CreateExtractValue(opC, {i}));\n-    asmOs << \"$\" << asmOpIdx++ << (i == numCRegs - 1 ? \"\" : \",\");\n-    // LLVM does not support `+` semantic, we must repeat the arguments for both\n-    // input and outputs\n-    if (structTypeC->getElementType(i)->isFloatTy())\n-      conOs << \"=f,\";\n-    else\n-      conOs << \"=r,\";\n-  }\n-  asmOs << \"}, \";\n-  for (uint32_t i = asmOpIdx - numCRegs; i < asmOpIdx; ++i)\n-    conOs << i << \",\";\n-  // Note that LLVM will not skip the indexed repeating placeholders\n-  asmOpIdx += numCRegs;\n-\n-  // Operand A\n-  if (structTypeA) {\n-    uint32_t numARegs = m * k / 128;\n-    assert(numARegs == structTypeA->getNumElements());\n-    asmOs << \"{\";\n-    for (uint32_t i = 0; i < numARegs; ++i) {\n-      argTypes.push_back(structTypeA->getElementType(i));\n-      args.push_back(builder.CreateExtractValue(opA, {i}));\n-      asmOs << \"$\" << asmOpIdx++ << (i == numARegs - 1 ? \"\" : \",\");\n-      conOs << \"f,\";\n-    }\n-    asmOs << \"}, \";\n-  } else {\n-    argTypes.push_back(typeA);\n-    args.push_back(opA);\n-    asmOs << \"$\" << asmOpIdx++ << \", \";\n-    conOs << \"l,\";\n-  }\n-\n-  // Operand B (must be `desc`)\n-  argTypes.push_back(typeB);\n-  args.push_back(opB);\n-  asmOs << \"$\" << asmOpIdx++ << \", \";\n-  conOs << \"l\";\n-\n-  // `scale-d` is 1 by default\n-  asmOs << \"1\";\n-\n-  // `imm-scale-a`, and `imm-scale-b` are 1 by default only for float-based\n-  // WGMMA\n-  if (floatTypeWGMMA)\n-    asmOs << \", 1, 1\";\n-\n-  // Push `trans-a` and `trans-b` args if needed (determined as constant)\n-  if (needTransArgs)\n-    asmOs << \", \" << transA << \", \" << transB;\n-  asmOs << \";\";\n-\n-  // Finally build `llvm::InlineAsm` and call it\n-  auto inlineAsm = llvm::InlineAsm::get(\n-      llvm::FunctionType::get(structTypeC, argTypes, false), ptxAsm,\n-      constraints, true);\n-  return builder.CreateCall(inlineAsm, args);\n-}\n-\n-void createWGMMAFence(llvm::IRBuilderBase &builder) {\n-  std::string asmStr = \"wgmma.fence.sync.aligned;\";\n-  llvm::InlineAsm *iasm =\n-      llvm::InlineAsm::get(llvm::FunctionType::get(builder.getVoidTy(), {}),\n-                           asmStr, \"\", /*hasSideEffect*/ true);\n-  builder.CreateCall(iasm, {});\n-}\n-\n-void createWGMMACommitGroup(llvm::IRBuilderBase &builder) {\n-  std::string asmStr = \"wgmma.commit_group.sync.aligned;\";\n-  llvm::InlineAsm *iasm =\n-      llvm::InlineAsm::get(llvm::FunctionType::get(builder.getVoidTy(), {}),\n-                           asmStr, \"\", /*hasSideEffect*/ true);\n-  builder.CreateCall(iasm, {});\n-}\n-\n-void createWGMMAWaitGroup(llvm::IRBuilderBase &builder, uint32_t pendings) {\n-  std::string asmStr = (llvm::Twine(\"wgmma.wait_group.sync.aligned \") +\n-                        llvm::Twine(pendings) + \";\")\n-                           .str();\n-  llvm::InlineAsm *iasm =\n-      llvm::InlineAsm::get(llvm::FunctionType::get(builder.getVoidTy(), {}),\n-                           asmStr, \"\", /*hasSideEffect*/ true);\n-  builder.CreateCall(iasm, {});\n-}\n-\n-llvm::Value *createLoadSharedCluster(llvm::IRBuilderBase &builder,\n-                                     llvm::Value *addr, llvm::Value *ctaId,\n-                                     unsigned bitwidth, unsigned vec) {\n-  assert(\n-      (bitwidth == 8 || bitwidth == 16 || bitwidth == 32 || bitwidth == 64) &&\n-      \"invalid bitwidth\");\n-  assert((vec == 1 || vec == 2 || vec == 4) && \"invalid vec size\");\n-\n-  // PTX string\n-  std::string ptxStr;\n-  llvm::raw_string_ostream asmOs(ptxStr);\n-  unsigned addrArgId = vec, ctaIdArgId = vec + 1;\n-  asmOs << \"{\\n\\t\"\n-        << \".reg .u32 remoteAddr;\\n\\t\"\n-        << \"mapa.shared::cluster.u32 remoteAddr, $\" << addrArgId << \", $\"\n-        << ctaIdArgId << \";\\n\\t\";\n-  asmOs << \"ld.shared::cluster\";\n-  if (vec > 1)\n-    asmOs << \".v\" << vec;\n-  asmOs << \".u\" << bitwidth << \" \";\n-  if (vec == 1)\n-    asmOs << \"$0\";\n-  else if (vec == 2)\n-    asmOs << \"{$0, $1}\";\n-  else\n-    asmOs << \"{$0, $1, $2, $3}\";\n-  asmOs << \", [remoteAddr];\\n\\t\"\n-        << \"}\\n\";\n-\n-  // Constraints\n-  std::string constraints;\n-  llvm::raw_string_ostream conOs(constraints);\n-  std::string c = bitwidth == 16 ? \"h\" : (bitwidth == 32 ? \"r\" : \"l\");\n-  for (unsigned i = 0; i < vec; ++i)\n-    conOs << \"=\" << c << \",\";\n-  conOs << \"r,r\";\n-\n-  // Arguments\n-  llvm::SmallVector<llvm::Type *> argTypes;\n-  llvm::SmallVector<llvm::Value *> args;\n-  argTypes.push_back(addr->getType());\n-  args.push_back(addr);\n-  argTypes.push_back(ctaId->getType());\n-  args.push_back(ctaId);\n-\n-  // Return type\n-  llvm::Type *retTy = builder.getIntNTy(bitwidth);\n-  llvm::SmallVector<llvm::Type *> retTys(vec, retTy);\n-  if (vec > 1)\n-    retTy = llvm::StructType::get(builder.getContext(), retTys);\n-\n-  // Call InlineAsm\n-  llvm::InlineAsm *iasm =\n-      llvm::InlineAsm::get(llvm::FunctionType::get(retTy, argTypes, false),\n-                           ptxStr, constraints, /*hasSideEffect*/ false);\n-  return builder.CreateCall(iasm, args);\n-}\n-\n-void createStoreSharedCluster(llvm::IRBuilderBase &builder, llvm::Value *addr,\n-                              llvm::Value *ctaId,\n-                              llvm::SmallVector<llvm::Value *> values,\n-                              llvm::Value *pred, unsigned bitwidth,\n-                              unsigned vec) {\n-  assert(\n-      (bitwidth == 8 || bitwidth == 16 || bitwidth == 32 || bitwidth == 64) &&\n-      \"invalid bitwidth\");\n-  assert((vec == 1 || vec == 2 || vec == 4) && vec == values.size() &&\n-         \"invalid vec size\");\n-\n-  // PTX string\n-  std::string ptxStr;\n-  llvm::raw_string_ostream asmOs(ptxStr);\n-  asmOs << \"{\\n\\t\"\n-        << \".reg .u32 remoteAddr;\\n\\t\"\n-        << \"mapa.shared::cluster.u32 remoteAddr, $0, $1;\\n\\t\"\n-        << \".reg .pred p;\\n\\t\"\n-        << \"mov.pred p, $2;\\n\\t\";\n-  asmOs << \"@p st.shared::cluster\";\n-  if (vec > 1)\n-    asmOs << \".v\" << vec;\n-  asmOs << \".u\" << bitwidth << \" [remoteAddr], \";\n-  if (vec == 1)\n-    asmOs << \"$3\";\n-  else if (vec == 2)\n-    asmOs << \"{$3, $4}\";\n-  else if (vec == 4)\n-    asmOs << \"{$3, $4, $5, $6}\";\n-  asmOs << \";\\n\\t\"\n-        << \"}\\n\";\n-\n-  // Constraints\n-  std::string constraints;\n-  llvm::raw_string_ostream conOs(constraints);\n-  std::string c = bitwidth == 16 ? \"h\" : (bitwidth == 32 ? \"r\" : \"l\");\n-  conOs << \"r,r,b\";\n-  for (unsigned i = 0; i < vec; ++i)\n-    conOs << \",\" << c;\n-\n-  // Arguments\n-  llvm::SmallVector<llvm::Type *> argTypes;\n-  llvm::SmallVector<llvm::Value *> args;\n-  argTypes.push_back(addr->getType());\n-  args.push_back(addr);\n-  argTypes.push_back(ctaId->getType());\n-  args.push_back(ctaId);\n-  argTypes.push_back(pred->getType());\n-  args.push_back(pred);\n-  for (llvm::Value *value : values) {\n-    argTypes.push_back(value->getType());\n-    args.push_back(value);\n-  }\n-\n-  // Call InlineAsm\n-  llvm::InlineAsm *iasm = llvm::InlineAsm::get(\n-      llvm::FunctionType::get(builder.getVoidTy(), argTypes, false), ptxStr,\n-      constraints, /*hasSideEffect*/ true);\n-  builder.CreateCall(iasm, args);\n-}\n-\n-static std::string getTMAStoreFuncName(bool tiled, uint32_t dimSize) {\n-  std::string funcName;\n-  llvm::raw_string_ostream os(funcName);\n-  os << \"__nv_tma_store\";\n-  if (tiled)\n-    os << \"_tiled\";\n-  else\n-    os << \"_im2col\";\n-\n-  os << \"_\" << dimSize << \"d\";\n-\n-  return funcName;\n-}\n-\n-void createTMAStoreTiled(llvm::IRBuilderBase &builder, llvm::Value *tmaDesc,\n-                         llvm::Value *src, llvm::Value *pred,\n-                         llvm::SmallVector<llvm::Value *> coords) {\n-  assert(coords.size() >= 2 && coords.size() <= 5 && \"invalid coords.size()\");\n-  auto funcName = getTMAStoreFuncName(true, coords.size());\n-  llvm::Type *retTy = builder.getVoidTy();\n-  llvm::SmallVector<llvm::Value *> args;\n-  llvm::SmallVector<llvm::Type *> argTys;\n-\n-  argTys.push_back(tmaDesc->getType());\n-  args.push_back(tmaDesc);\n-\n-  argTys.push_back(src->getType());\n-  args.push_back(src);\n-\n-  for (auto *c : coords) {\n-    argTys.push_back(c->getType());\n-    args.push_back(c);\n-  }\n-  argTys.push_back(pred->getType());\n-  args.push_back(pred);\n-\n-  auto *module = builder.GetInsertBlock()->getModule();\n-  auto *func = dyn_cast<llvm::Function>(\n-      getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n-  builder.CreateCall(func, args);\n-\n-  return;\n-}\n-\n-void createStoreMatrix(llvm::IRBuilderBase &builder, llvm::Value *addr,\n-                       llvm::SmallVector<llvm::Value *> datas) {\n-  auto size = datas.size();\n-  assert((size == 1 || size == 2 || size == 4) &&\n-         \"not support size with stmatrix\");\n-\n-  std::string funcName;\n-  llvm::raw_string_ostream os(funcName);\n-  os << \"__nv_stmatrix_x\" << size;\n-\n-  llvm::Type *retTy = builder.getVoidTy();\n-  llvm::SmallVector<llvm::Value *> args;\n-  llvm::SmallVector<llvm::Type *> argTys;\n-\n-  argTys.push_back(addr->getType());\n-  args.push_back(addr);\n-\n-  for (size_t i = 0; i < datas.size(); ++i) {\n-    argTys.push_back(datas[i]->getType());\n-    args.push_back(datas[i]);\n-  }\n-\n-  auto *module = builder.GetInsertBlock()->getModule();\n-  auto *func = dyn_cast<llvm::Function>(\n-      getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n-  builder.CreateCall(func, args);\n-}\n-\n-llvm::Value *createOffsetOfStmatrixV4(llvm::IRBuilderBase &builder,\n-                                      llvm::Value *threadId,\n-                                      llvm::Value *rowOfWarp,\n-                                      llvm::Value *elemIdx,\n-                                      uint32_t leadingDimOffset,\n-                                      uint32_t rowStride, bool swizzleEnabled) {\n-  if (swizzleEnabled) {\n-    assert((rowStride == 16 || rowStride == 32 || rowStride == 64) &&\n-           \"wrong rowString for swizzleEnabled\");\n-  }\n-  llvm::Type *retTy = builder.getInt32Ty();\n-  llvm::SmallVector<llvm::Value *> args;\n-  llvm::SmallVector<llvm::Type *> argTys;\n-\n-  argTys.push_back(threadId->getType());\n-  args.push_back(threadId);\n-\n-  argTys.push_back(rowOfWarp->getType());\n-  args.push_back(rowOfWarp);\n-\n-  argTys.push_back(elemIdx->getType());\n-  args.push_back(elemIdx);\n-\n-  argTys.push_back(builder.getInt32Ty());\n-  args.push_back(builder.getInt32(leadingDimOffset));\n-\n-  argTys.push_back(builder.getInt32Ty());\n-  args.push_back(builder.getInt32(rowStride));\n-\n-  std::string funcName(\"__nv_offset_of_stmatrix_v4\");\n-  if (!swizzleEnabled)\n-    funcName = \"__nv_offset_of_stmatrix_v4_no_swizzle\";\n-  auto *module = builder.GetInsertBlock()->getModule();\n-  auto *func = dyn_cast<llvm::Function>(\n-      getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n-  return builder.CreateCall(func, args);\n-}\n-\n-llvm::Value *createOffsetOfSts64(llvm::IRBuilderBase &builder,\n-                                 llvm::Value *threadId, llvm::Value *rowOfWarp,\n-                                 llvm::Value *elemIdx,\n-                                 uint32_t leadingDimOffset, uint32_t rowStride,\n-                                 bool swizzleEnabled) {\n-  if (swizzleEnabled) {\n-    assert((rowStride == 32 || rowStride == 64 || rowStride == 128) &&\n-           \"wrong rowString for swizzleEnabled\");\n-  }\n-  llvm::Type *retTy = builder.getInt32Ty();\n-  llvm::SmallVector<llvm::Value *> args;\n-  llvm::SmallVector<llvm::Type *> argTys;\n-\n-  argTys.push_back(threadId->getType());\n-  args.push_back(threadId);\n-\n-  argTys.push_back(rowOfWarp->getType());\n-  args.push_back(rowOfWarp);\n-\n-  argTys.push_back(elemIdx->getType());\n-  args.push_back(elemIdx);\n-\n-  argTys.push_back(builder.getInt32Ty());\n-  args.push_back(builder.getInt32(leadingDimOffset));\n-\n-  argTys.push_back(builder.getInt32Ty());\n-  args.push_back(builder.getInt32(rowStride));\n-\n-  std::string funcName(\"__nv_offset_of_sts64\");\n-  auto *module = builder.GetInsertBlock()->getModule();\n-  auto *func = dyn_cast<llvm::Function>(\n-      getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n-  return builder.CreateCall(func, args);\n-}\n-\n-void createSts64(llvm::IRBuilderBase &builder, llvm::Value *offset,\n-                 llvm::Value *d0, llvm::Value *d1) {\n-  std::string funcName(\"__nv_sts64\");\n-\n-  llvm::Type *retTy = builder.getVoidTy();\n-  llvm::SmallVector<llvm::Value *> args;\n-  llvm::SmallVector<llvm::Type *> argTys;\n-  auto i32Ty = builder.getInt32Ty();\n-  argTys.push_back(i32Ty);\n-  args.push_back(offset);\n-\n-  argTys.push_back(i32Ty);\n-  args.push_back(builder.CreateBitCast(d0, i32Ty));\n-\n-  argTys.push_back(i32Ty);\n-  args.push_back(builder.CreateBitCast(d1, i32Ty));\n-\n-  auto *module = builder.GetInsertBlock()->getModule();\n-  auto *func = dyn_cast<llvm::Function>(\n-      getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n-  builder.CreateCall(func, args);\n-\n-  return;\n-}\n-\n-static llvm::Value *getSRegValue(llvm::IRBuilderBase &builder,\n-                                 llvm::StringRef name) {\n-  std::string ptxStr;\n-  llvm::raw_string_ostream asmOs(ptxStr);\n-  asmOs << \"mov.u32 $0, \" << name << \";\";\n-  std::string constraints = \"=r\";\n-  llvm::InlineAsm *inlineAsm =\n-      llvm::InlineAsm::get(llvm::FunctionType::get(builder.getInt32Ty(), false),\n-                           ptxStr, constraints, /*hasSideEffect*/ false);\n-  return builder.CreateCall(inlineAsm);\n-}\n-\n-static llvm::Value *createClusterId(llvm::IRBuilderBase &builder) {\n-  llvm::Value *x = getSRegValue(builder, \"%cluster_ctaid.x\");\n-  llvm::Value *y = getSRegValue(builder, \"%cluster_ctaid.y\");\n-  llvm::Value *z = getSRegValue(builder, \"%cluster_ctaid.z\");\n-  llvm::Value *nx = getSRegValue(builder, \"%cluster_nctaid.x\");\n-  llvm::Value *ny = getSRegValue(builder, \"%cluster_nctaid.y\");\n-  llvm::Value *clusterCTAId = builder.CreateAdd(\n-      x, builder.CreateMul(builder.CreateAdd(y, builder.CreateMul(z, ny)), nx));\n-  return clusterCTAId;\n-}\n-void createRegAlloc(llvm::IRBuilderBase &builder, const uint32_t regCount) {\n-  std::string ptxStr;\n-  llvm::raw_string_ostream asmOs(ptxStr);\n-  asmOs << \"setmaxnreg.inc.sync.aligned.u32 \" << regCount << \";\\n\";\n-  // Call InlineAsm\n-  llvm::InlineAsm *iasm =\n-      llvm::InlineAsm::get(llvm::FunctionType::get(builder.getVoidTy(), {}),\n-                           ptxStr, \"\", /*hasSideEffect*/ true);\n-  builder.CreateCall(iasm, {});\n-}\n-\n-void createRegDealloc(llvm::IRBuilderBase &builder, const uint32_t regCount) {\n-  std::string ptxStr;\n-  llvm::raw_string_ostream asmOs(ptxStr);\n-  asmOs << \"setmaxnreg.dec.sync.aligned.u32 \" << regCount << \";\\n\";\n-  // Call InlineAsm\n-  llvm::InlineAsm *iasm =\n-      llvm::InlineAsm::get(llvm::FunctionType::get(builder.getVoidTy(), {}),\n-                           ptxStr, \"\", /*hasSideEffect*/ true);\n-  builder.CreateCall(iasm, {});\n-}\n-\n-class NVGPUDialectLLVMIRTranslationInterface\n-    : public LLVMTranslationDialectInterface {\n-public:\n-  using LLVMTranslationDialectInterface::LLVMTranslationDialectInterface;\n-\n-  /// Translates the given operation to LLVM IR using the provided IR builder\n-  /// and saving the state in `moduleTranslation`.\n-  LogicalResult\n-  convertOperation(Operation *op, llvm::IRBuilderBase &builder,\n-                   LLVM::ModuleTranslation &moduleTranslation) const final {\n-    Operation &opInst = *op;\n-#include \"triton/Dialect/NVGPU/IR/OpsConversions.inc\"\n-\n-    return failure();\n-  }\n-};\n-} // namespace\n-\n-void mlir::registerNVGPUDialectTranslation(DialectRegistry &registry) {\n-  registry.insert<mlir::triton::nvgpu::NVGPUDialect>();\n-  registry.addExtension(\n-      +[](MLIRContext *ctx, mlir::triton::nvgpu::NVGPUDialect *dialect) {\n-        dialect->addInterfaces<NVGPUDialectLLVMIRTranslationInterface>();\n-      });\n-}\n-\n-void mlir::registerNVGPUDialectTranslation(MLIRContext &context) {\n-  DialectRegistry registry;\n-  registerNVGPUDialectTranslation(registry);\n-  context.appendDialectRegistry(registry);\n-}"}, {"filename": "lib/Hopper/CMakeLists.txt", "status": "removed", "additions": 0, "deletions": 24, "changes": 24, "file_content_changes": "@@ -1,24 +0,0 @@\n-# TODO: re-enable generation of the hopper helper.\n-# For now we just commit a pre-built bc file.\n-#cmake_path(GET LLVM_LIBRARY_DIR PARENT_PATH LLVM_DIR)\n-#set(CUDA_PATH \"/usr/local/cuda\")\n-#if(DEFINED ENV{CUDA_PATH})\n-#        set(CUDA_PATH $ENV{CUDA_PATH})\n-#endif()\n-#\n-#set(outfile \"libhopper_helpers.bc\")\n-#add_custom_target(HopperHelpers ALL\n-#        COMMAND ${LLVM_DIR}/bin/clang -c\n-#        -O3 -emit-llvm\n-#        -fno-unwind-tables -fno-exceptions\n-#        --cuda-gpu-arch=sm_90a\n-#        --cuda-device-only\n-#        -D__CUDACC__ -D__CUDABE__ -D__CUDA_LIBDEVICE__\n-#        -I${CUDA_PATH}/include\n-#        -target nvptx64-nvidia-gpulibs\n-#        ${CMAKE_CURRENT_SOURCE_DIR}/HopperHelpers.c -o ${outfile}\n-#        DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/HopperHelpers.c\n-#        BYPRODUCTS ${outfile}\n-#        COMMENT \"Building LLVM bitcode ${outfile}\"\n-#        VERBATIM\n-#)"}, {"filename": "lib/Hopper/HopperHelpers.c", "status": "removed", "additions": 0, "deletions": 471, "changes": 471, "file_content_changes": "@@ -1,471 +0,0 @@\n-/*\n- * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n- *\n- * Permission is hereby granted, free of charge, to any person obtaining\n- * a copy of this software and associated documentation files\n- * (the \"Software\"), to deal in the Software without restriction,\n- * including without limitation the rights to use, copy, modify, merge,\n- * publish, distribute, sublicense, and/or sell copies of the Software,\n- * and to permit persons to whom the Software is furnished to do so,\n- * subject to the following conditions:\n- *\n- * The above copyright notice and this permission notice shall be\n- * included in all copies or substantial portions of the Software.\n- *\n- * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n- * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n- * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n- * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n- * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n- */\n-\n-#include \"device_launch_parameters.h\"\n-#include <builtin_types.h>\n-#include <cuda_device_runtime_api.h>\n-#include <stdarg.h>\n-#include <stdint.h>\n-#include <stdio.h>\n-\n-#ifdef __NVCC__\n-#define __DEVICE__ __device__ inline\n-#else\n-#define __DEVICE__\n-#endif\n-\n-#ifndef BARRIER_RANDOM_DELAY\n-#define BARRIER_RANDOM_DELAY 0\n-#endif\n-\n-#if BARRIER_RANDOM_DELAY\n-__DEVICE__ uint64_t random_avalanche(uint64_t r) {\n-  r ^= r >> 33;\n-  r *= 0xff51afd7ed558ccd;\n-  r ^= r >> 33;\n-  r *= 0xc4ceb9fe1a85ec53;\n-  r ^= r >> 33;\n-  return r;\n-}\n-\n-__DEVICE__ uint64_t random_stateless_uint64() {\n-  uint64_t r = blockIdx.z << 27 ^ blockIdx.y << 22 ^ blockIdx.x;\n-  r = r << 32 ^ threadIdx.z << 20 ^ threadIdx.y << 10 ^ threadIdx.x;\n-\n-  uint64_t timer;\n-  asm volatile(\"mov.u64 %0, %%globaltimer;\\n\\t\" : \"=l\"(timer) : : \"memory\");\n-\n-  r ^= timer;\n-  return random_avalanche(r);\n-}\n-#endif\n-\n-__DEVICE__ void random_stateless_delay() {\n-#if BARRIER_RANDOM_DELAY\n-  uint64_t r = random_stateless_uint64();\n-\n-  // About 2 milliseconds.\n-  uint32_t ns = r >> (64 - 21);\n-  asm volatile(\"nanosleep.u32 %0;\\n\\t\" : : \"r\"(ns));\n-#endif\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) uint64_t\n-__nv_get_wgmma_desc(uint32_t smem_nvvm_pointer, uint32_t mode,\n-                    uint32_t height) {\n-  uint64_t desc = 0;\n-  uint64_t swizzling = (mode == 1 ? 128 : mode == 2 ? 64 : 32);\n-  uint64_t smem_address_bit = smem_nvvm_pointer;\n-\n-  uint64_t stride_dimension = swizzling << 3 >> 4;\n-  uint64_t leading_dimension = height * swizzling >> 4;\n-  // [benzh] from cutlass\n-  uint64_t base_offset = 0; //(smem_address_bit >> 7) % (swizzling >> 4);\n-  uint64_t start_addr = (smem_address_bit << 46) >> 50;\n-\n-  desc |= ((uint64_t)mode) << 62;\n-  desc |= stride_dimension << 32;\n-  desc |= leading_dimension << 16;\n-  desc |= base_offset << 49;\n-  desc |= start_addr;\n-\n-  return desc;\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_wgmma_fence() {\n-  asm volatile(\"wgmma.fence.sync.aligned;\\n\");\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_wgmma_commit_group() {\n-  asm volatile(\"wgmma.commit_group.sync.aligned;\\n\");\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_wgmma_wait_group() {\n-  asm volatile(\"wgmma.wait_group.sync.aligned 0;\\n\");\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_mbarrier_init(uint32_t bar, uint32_t expected, uint32_t pred) {\n-  if (pred) {\n-    asm volatile(\"{\\n\\t\"\n-                 \"mbarrier.init.shared.b64 [%0], %1;\\n\\t\"\n-                 \"}\"\n-                 :\n-                 : \"r\"(bar), \"r\"(expected));\n-  }\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_mbarrier_wait(uint32_t bar, uint32_t phase) {\n-  uint32_t large_val = 0x989680;\n-  asm volatile(\"{\\n\\t\"\n-               \".reg .pred                P1; \\n\\t\"\n-               \"LAB_WAIT: \\n\\t\"\n-               \"mbarrier.try_wait.parity.shared.b64 P1, [%0], %1, %2; \\n\\t\"\n-               \"@P1                       bra.uni DONE; \\n\\t\"\n-               \"bra.uni                   LAB_WAIT; \\n\\t\"\n-               \"DONE: \\n\\t\"\n-               \"}\"\n-               :\n-               : \"r\"(bar), \"r\"(phase), \"r\"(large_val));\n-  random_stateless_delay();\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) int\n-__nv_mbarrier_peek(uint32_t bar, uint32_t phase) {\n-  random_stateless_delay();\n-  int ready = 0;\n-  asm volatile(\"{\\n\\t\"\n-               \".reg .pred p;\\n\\t\"\n-               \"mbarrier.try_wait.shared.b64 p, [%1], %2;\\n\\t\"\n-               \"selp.b32 %0, 1, 0, p;\\n\\t\"\n-               \"}\"\n-               : \"=r\"(ready)\n-               : \"r\"(bar), \"l\"((unsigned long long)phase)\n-               : \"memory\");\n-  return ready;\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_mbarrier_arrive_normal(uint32_t bar, uint32_t pred) {\n-  random_stateless_delay();\n-  if (pred) {\n-    asm volatile(\"{\\n\\t\"\n-                 \"mbarrier.arrive.shared.b64 _, [%0];\\n\\t\"\n-                 \"}\"\n-                 :\n-                 : \"r\"(bar));\n-  }\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_mbarrier_arrive_cp_async(uint32_t bar, uint32_t pred) {\n-  random_stateless_delay();\n-  if (pred) {\n-    asm volatile(\"cp.async.mbarrier.arrive.noinc.shared.b64 [%0];\"\n-                 :\n-                 : \"r\"(bar));\n-  }\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_mbarrier_arrive_expect_tx(uint32_t bar, uint32_t txCount, uint32_t pred) {\n-  random_stateless_delay();\n-  if (pred) {\n-    asm volatile(\"{\\n\\t\"\n-                 \"mbarrier.arrive.expect_tx.shared.b64 _, [%0], %1;\\n\\t\"\n-                 \"}\"\n-                 :\n-                 : \"r\"(bar), \"r\"(txCount));\n-  }\n-}\n-\n-// for warp special empty barrier.\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_mbarrier_arrive_remote(uint32_t bar, uint32_t ctaId, uint32_t pred) {\n-  random_stateless_delay();\n-  if (pred) {\n-    asm volatile(\"{\\n\\t\"\n-                 \".reg .b32 remAddr32;\\n\\t\"\n-                 \"mapa.shared::cluster.u32  remAddr32, %0, %1;\\n\\t\"\n-                 \"mbarrier.arrive.shared::cluster.b64  _, [remAddr32];\\n\\t\"\n-                 \"}\"\n-                 :\n-                 : \"r\"(bar), \"r\"(ctaId));\n-  }\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_fence_mbarrier_init() {\n-  asm volatile(\"{\\n\\t\"\n-               \"fence.mbarrier_init.release.cluster; \\n\"\n-               \"}\" ::\n-                   : \"memory\");\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_fence_async_shared_cta() {\n-  asm volatile(\"fence.proxy.async.shared::cta;\\n\");\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_fence_async_shared_cluster() {\n-  asm volatile(\"fence.proxy.async.shared::cluster;\\n\");\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_cp_async_bulk(char *gmem_ptr, unsigned smem_ptr, unsigned barrier,\n-                   int bytes, uint32_t pred) {\n-  if (pred) {\n-    asm volatile(\"cp.async.bulk.shared::cluster.global.mbarrier::complete_tx::\"\n-                 \"bytes [%0], [%1], %2, [%3];\\n\"\n-                 :\n-                 : \"r\"(smem_ptr), \"l\"(gmem_ptr), \"r\"(bytes), \"r\"(barrier)\n-                 : \"memory\");\n-  }\n-}\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_tma_load_tiled_2d(const uint64_t p_tma_desc, uint32_t dst_smem,\n-                       uint32_t barrier, int32_t c0, int32_t c1,\n-                       unsigned long long mem_desc, uint32_t pred) {\n-  if (pred) {\n-    asm volatile(\n-        \"cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx\"\n-        \"::bytes.L2::cache_hint [%0], [%1, {%2, %3}], \"\n-        \"[%4], %5;\\n\"\n-        :\n-        : \"r\"(dst_smem), \"l\"(p_tma_desc), \"r\"(c0), \"r\"(c1), \"r\"(barrier),\n-          \"l\"(mem_desc)\n-        : \"memory\");\n-  }\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_tma_load_tiled_mcast_2d(\n-    const uint64_t p_tma_desc, uint32_t dst_smem, uint32_t barrier, int32_t c0,\n-    int32_t c1, unsigned long long mem_desc, uint16_t mcast, uint32_t pred) {\n-  if (pred) {\n-    asm volatile(\"cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::\"\n-                 \"complete_tx::bytes.multicast::cluster.L2::cache_hint\"\n-                 \" [%0], [%1, {%2, %3}], [%4], %5, %6;\"\n-                 :\n-                 : \"r\"(dst_smem), \"l\"(p_tma_desc), \"r\"(c0), \"r\"(c1),\n-                   \"r\"(barrier), \"h\"(mcast), \"l\"(mem_desc)\n-                 : \"memory\");\n-  }\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_tma_load_tiled_4d(const uint64_t p_tma_desc, uint32_t dst_smem,\n-                       uint32_t barrier, int32_t c0, int32_t c1, int32_t c2,\n-                       int32_t c3, unsigned long long mem_desc, uint32_t pred) {\n-  if (pred) {\n-    asm volatile(\n-        \"cp.async.bulk.tensor.4d.shared::cluster.global.mbarrier::complete_tx\"\n-        \"::bytes.L2::cache_hint [%0], [%1, {%2, %3, %4, %5}], \"\n-        \"[%6], %7;\\n\"\n-        :\n-        : \"r\"(dst_smem), \"l\"(p_tma_desc), \"r\"(c0), \"r\"(c1), \"r\"(c2), \"r\"(c3),\n-          \"r\"(barrier), \"l\"(mem_desc)\n-        : \"memory\");\n-  }\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_stmatrix_x1(uint32_t ptr, const uint32_t d0) {\n-  asm volatile(\n-      \"stmatrix.sync.aligned.m8n8.x1.shared.b16 [%0], {%1};\\n\" ::\"r\"(ptr),\n-      \"r\"(d0));\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_stmatrix_x2(uint32_t ptr, const uint32_t d0, const uint32_t d1) {\n-  asm volatile(\n-      \"stmatrix.sync.aligned.m8n8.x2.shared.b16 [%0], {%1, %2};\\n\" ::\"r\"(ptr),\n-      \"r\"(d0), \"r\"(d1));\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_stmatrix_x4(uint32_t ptr, const uint32_t d0, const uint32_t d1,\n-                 const uint32_t d2, const uint32_t d3) {\n-  asm volatile(\n-      \"stmatrix.sync.aligned.m8n8.x4.shared.b16 [%0], {%1, %2, %3, %4};\\n\" ::\n-          \"r\"(ptr),\n-      \"r\"(d0), \"r\"(d1), \"r\"(d2), \"r\"(d3));\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_async_group_commit() {\n-  asm volatile(\"cp.async.bulk.commit_group;\");\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_async_group_wait0() {\n-  asm volatile(\"cp.async.bulk.wait_group %0;\" : : \"n\"(0) : \"memory\");\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) uint32_t\n-__nv_dsmem_addr(uint32_t buffer_ptr, uint32_t ctaid) {\n-  uint32_t buffer_ptr_;\n-  asm volatile(\"{\\n\\t\"\n-               \"mapa.shared::cluster.u32 %0, %1, %2;\\n\\t\"\n-               \"}\"\n-               : \"=r\"(buffer_ptr_)\n-               : \"r\"(buffer_ptr), \"r\"(ctaid));\n-  return buffer_ptr_;\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_bar_arrive(uint32_t bar, uint32_t numThreads) {\n-  random_stateless_delay();\n-  asm volatile(\"bar.arrive %0, %1;\\n\" ::\"r\"(bar), \"r\"(numThreads));\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_bar_wait(uint32_t bar, uint32_t numThreads) {\n-  asm volatile(\"bar.sync %0, %1;\\n\" ::\"r\"(bar), \"r\"(numThreads));\n-  random_stateless_delay();\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_cga_barrier_sync() {\n-  asm volatile(\"barrier.cluster.sync.aligned;\\n\" ::);\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_cga_barrier_arrive() {\n-  asm volatile(\"barrier.cluster.arrive;\\n\" ::);\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_cga_barrier_wait() {\n-  asm volatile(\"barrier.cluster.wait;\\n\" ::);\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_cluster_arrive_relaxed() {\n-  asm volatile(\"barrier.cluster.arrive.relaxed.aligned;\\n\" : :);\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_cluster_arrive() {\n-  asm volatile(\"barrier.cluster.arrive.aligned;\\n\" : :);\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_cluster_wait() {\n-  asm volatile(\"barrier.cluster.wait.aligned;\\n\" : :);\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_tma_store_tiled_2d(const uint64_t p_tma_desc, int32_t src_smem, int32_t c0,\n-                        int32_t c1, uint32_t pred) {\n-  if (pred) {\n-    asm volatile(\"cp.async.bulk.tensor.2d.global.shared::cta.bulk_group\"\n-                 \"[%0, {%2, %3}], [%1];\\n\"\n-                 :\n-                 : \"l\"(p_tma_desc), \"r\"(src_smem), \"r\"(c0), \"r\"(c1)\n-                 : \"memory\");\n-  }\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_tma_store_tiled_3d(const uint64_t p_tma_desc, uint32_t src_smem,\n-                        int32_t c0, int32_t c1, int32_t c2, uint32_t pred) {\n-  if (pred) {\n-    asm volatile(\"cp.async.bulk.tensor.3d.global.shared::cta.bulk_group\"\n-                 \"[%0, {%2, %3, %4}], [%1];\\n\"\n-                 :\n-                 : \"l\"(p_tma_desc), \"r\"(src_smem), \"r\"(c0), \"r\"(c1), \"r\"(c2)\n-                 : \"memory\");\n-  }\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_tma_store_tiled_4d(const uint64_t p_tma_desc, uint32_t src_smem,\n-                        int32_t c0, int32_t c1, int32_t c2, int32_t c3,\n-                        uint32_t pred) {\n-  if (pred) {\n-    asm volatile(\"cp.async.bulk.tensor.4d.global.shared::cta.bulk_group\"\n-                 \"[%0, {%2, %3, %4, %5}], [%1];\\n\"\n-                 :\n-                 : \"l\"(p_tma_desc), \"r\"(src_smem), \"r\"(c0), \"r\"(c1), \"r\"(c2),\n-                   \"r\"(c3)\n-                 : \"memory\");\n-  }\n-}\n-__DEVICE__ __attribute__((__always_inline__)) uint32_t\n-__nv_offset_of_stmatrix_v4(uint32_t threadIdx, uint32_t rowOfWarp,\n-                           uint32_t elemIdx, uint32_t leadingDimOffset,\n-                           uint32_t rowStride) {\n-  uint32_t perPhase = 0;\n-  uint32_t maxPhase = 0;\n-  if (rowStride == 64) {\n-    perPhase = 1;\n-    maxPhase = 8;\n-  } else if (rowStride == 32) {\n-    perPhase = 2;\n-    maxPhase = 4;\n-  } else if (rowStride == 16) {\n-    perPhase = 4;\n-    maxPhase = 2;\n-  }\n-\n-  uint32_t iterOfCol = elemIdx / 8;\n-\n-  uint32_t myRow = rowOfWarp + (threadIdx & 0xf);\n-  uint32_t myCol = ((threadIdx >> 4) & 0x1) * 8;\n-  myCol = myCol + iterOfCol * 16;\n-\n-  uint32_t offset0 = (myCol / rowStride) * leadingDimOffset;\n-  myCol = myCol % rowStride;\n-\n-  uint32_t phase = (myRow / perPhase) % maxPhase;\n-\n-  uint32_t lineOffset = (myRow % perPhase) * rowStride + myCol;\n-  uint32_t colOffset = ((lineOffset / 8) ^ phase) * 8 + lineOffset % 8;\n-  uint32_t offset1 = (myRow / perPhase) * 64 + colOffset;\n-\n-  return offset1 + offset0;\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) uint32_t\n-__nv_offset_of_stmatrix_v4_no_swizzle(uint32_t threadIdx, uint32_t rowOfWarp,\n-                                      uint32_t elemIdx, uint32_t rowStride) {\n-  uint32_t iterOfCol = elemIdx / 4;\n-  uint32_t myRow = rowOfWarp + (threadIdx & 0xf);\n-  uint32_t myCol = ((threadIdx >> 4) & 0x1) * 8;\n-\n-  myCol = myCol + iterOfCol * 16;\n-  uint32_t offset = myRow * rowStride + myCol * 2;\n-  return offset;\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_sts64(uint32_t ptr, uint32_t d0, uint32_t d1) {\n-  asm volatile(\"st.shared.v2.b32 [%0], {%1, %2};\\n\"\n-               :\n-               : \"r\"(ptr), \"r\"(d0), \"r\"(d1));\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) uint32_t\n-__nv_offset_of_sts64(uint32_t threadIdx, uint32_t rowOfWarp, int32_t elemIdx,\n-                     uint32_t rowStride) {\n-  uint32_t perPhase = 0;\n-  uint32_t maxPhase = 0;\n-  if (rowStride == 128) {\n-    perPhase = 1;\n-    maxPhase = 8;\n-  } else if (rowStride == 64) {\n-    perPhase = 2;\n-    maxPhase = 4;\n-  } else if (rowStride == 32) {\n-    perPhase = 4;\n-    maxPhase = 2;\n-  }\n-\n-  uint32_t laneId = threadIdx & 0x1f;\n-\n-  uint32_t myRow = ((elemIdx >> 1) & 0x1) * 8 + laneId / 4;\n-  uint32_t myCol = (elemIdx / 4) * 8 + (laneId % 4) * 2;\n-  myRow += rowOfWarp;\n-\n-  uint32_t phase = (myRow / perPhase) % maxPhase;\n-\n-  uint32_t lineOffset = (myRow % perPhase) * rowStride + myCol * 4;\n-  uint32_t colOffset = ((lineOffset / 16) ^ phase) * 16 + lineOffset % 16;\n-  uint32_t offset = (myRow / perPhase) * 128 + colOffset;\n-\n-  return offset;\n-}"}, {"filename": "lib/Target/LLVMIR/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -23,7 +23,6 @@ add_mlir_translation_library(TritonLLVMIR\n         MLIRSCFToControlFlow\n         MLIRSupport\n         MLIRTargetLLVMIRExport\n-        NVGPUToLLVMIR\n         TritonGPUToLLVM\n         )\n "}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -17,8 +17,8 @@\n #include \"mlir/Target/LLVMIR/Export.h\"\n #include \"mlir/Target/LLVMIR/LLVMTranslationInterface.h\"\n #include \"mlir/Transforms/Passes.h\"\n+#include \"triton/Conversion/NVGPUToLLVM/NVGPUToLLVMPass.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n-#include \"triton/Dialect/NVGPU/ToLLVMIR/NVGPUToLLVMIR.h\"\n #include \"triton/Target/LLVMIR/Passes.h\"\n #include \"triton/Target/PTX/TmaMetadata.h\"\n #include \"triton/Tools/Sys/GetEnv.hpp\"\n@@ -280,7 +280,6 @@ translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module,\n   mlir::registerLLVMDialectTranslation(registry);\n   mlir::registerROCDLDialectTranslation(registry);\n   mlir::registerNVVMDialectTranslation(registry);\n-  mlir::registerNVGPUDialectTranslation(registry);\n \n   module->getContext()->appendDialectRegistry(registry);\n \n@@ -353,6 +352,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n   pm.addPass(mlir::createConvertIndexToLLVMPass());\n   pm.addPass(\n       createConvertTritonGPUToLLVMPass(computeCapability, &tmaInfos, isROCM));\n+  pm.addPass(createConvertNVGPUToLLVMPass());\n   pm.addPass(mlir::createArithToLLVMConversionPass());\n   pm.addPass(mlir::createCanonicalizerPass());\n   // Simplify the IR"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -23,6 +23,7 @@\n #include \"mlir/Dialect/Index/IR/IndexOps.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"triton/Analysis/Allocation.h\"\n+#include \"triton/Conversion/NVGPUToLLVM/NVGPUToLLVMPass.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n #include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h\"\n #include \"triton/Dialect/NVGPU/IR/Dialect.h\"\n@@ -1690,6 +1691,10 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::PassManager &self) {\n              self.addPass(mlir::triton::createConvertTritonGPUToLLVMPass());\n            })\n+      .def(\"add_nv_gpu_to_llvm\",\n+           [](mlir::PassManager &self) {\n+             self.addPass(mlir::triton::createConvertNVGPUToLLVMPass());\n+           })\n       .def(\"add_scf_to_cfg\", [](mlir::PassManager &self) {\n         self.addPass(mlir::createConvertSCFToCFPass());\n       });"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -172,7 +172,7 @@ def _bwd_kernel(\n             lo = 0\n         # initialize row/col offsets\n         offs_qm = lo + tl.arange(0, BLOCK_M)\n-        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M) \n+        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n         offs_m = tl.arange(0, BLOCK_N)\n         offs_k = tl.arange(0, BLOCK_DMODEL)\n         # initialize pointers to value-like data"}, {"filename": "test/NVGPU/test_cga.mlir", "status": "modified", "additions": 13, "deletions": 13, "changes": 26, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-translate %s | FileCheck %s\n+// RUN: triton-opt %s -split-input-file --convert-nv-gpu-to-llvm | FileCheck %s\n #SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 2 : i32} {\n   tt.func @test_mbarrier() {\n@@ -7,24 +7,24 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 2 :\n     %pred = arith.constant 1 : i1\n     %id0 = arith.constant 0 : i32\n     %id1 = arith.constant 1 : i32\n-    // CHECK: call void @__nv_cga_barrier_sync()\n-    // CHECK: call void @__nv_cga_barrier_arrive()\n-    // CHECK: call void @__nv_cga_barrier_wait()\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n     nvgpu.cga_barrier_sync\n     nvgpu.cga_barrier_arrive\n     nvgpu.cga_barrier_wait\n \n     %ptr = llvm.mlir.null : !llvm.ptr<i32, 3>\n \n-    // CHECK: %[[X:.+]] = tail call i32 asm \"mov.u32 $0, %cluster_ctaid.x;\", \"=r\"()\n-    // CHECK: %[[Y:.+]] = tail call i32 asm \"mov.u32 $0, %cluster_ctaid.y;\", \"=r\"()\n-    // CHECK: %[[Z:.+]] = tail call i32 asm \"mov.u32 $0, %cluster_ctaid.z;\", \"=r\"()\n-    // CHECK: %[[NX:.+]] = tail call i32 asm \"mov.u32 $0, %cluster_nctaid.x;\", \"=r\"()\n-    // CHECK: %[[NY:.+]] = tail call i32 asm \"mov.u32 $0, %cluster_nctaid.y;\", \"=r\"()\n-    // CHECK: %[[A0:.+]] = mul i32 %[[NY]], %[[Z]]\n-    // CHECK: %[[A1:.+]] = add i32 %[[A0]], %[[Y]]\n-    // CHECK: %[[A2:.+]] = mul i32 %[[A1]], %[[NX]]\n-    // CHECK: add i32 %[[A2]], %[[X]]\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.mul\n+    // CHECK: llvm.add\n+    // CHECK: llvm.mul\n+    // CHECK: llvm.add\n     %v = nvgpu.cluster_id\n     llvm.store %v, %ptr : !llvm.ptr<i32, 3>\n "}, {"filename": "test/NVGPU/test_mbarrier.mlir", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -1,18 +1,18 @@\n-// RUN: triton-translate %s | FileCheck %s\n+// RUN: triton-opt %s -split-input-file --convert-nv-gpu-to-llvm | FileCheck %s\n #SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32,  \"triton_gpu.num-ctas\" = 2 : i32} {\n   tt.func @test_mbarrier() {\n     %mbarrier = llvm.mlir.null : !llvm.ptr<i64, 3>\n     %pred = arith.constant 1 : i1\n-    // CHECK: call void @__nv_mbarrier_init\n+    // CHECK: llvm.inline_asm\n     nvgpu.mbarrier_init %mbarrier, %pred { count = 32 : i32 } : !llvm.ptr<i64, 3>\n-    // CHECK: call void @__nv_mbarrier_arrive_cp_async\n+    // CHECK: llvm.inline_asm\n     nvgpu.mbarrier_arrive %mbarrier, %pred {arriveType = 1 : i32}: !llvm.ptr<i64, 3>\n-    // CHECK: call void @__nv_mbarrier_arrive_normal\n+    // CHECK: llvm.inline_asm\n     nvgpu.mbarrier_arrive %mbarrier, %pred {arriveType = 0 : i32}: !llvm.ptr<i64, 3>\n-    // CHECK: call void @__nv_mbarrier_arrive_expect_tx\n+    // CHECK: llvm.inline_asm\n     nvgpu.mbarrier_arrive %mbarrier, %pred {arriveType = 2 : i32, txCount = 128 : i32}: !llvm.ptr<i64, 3>\n-    // CHECK: call void @__nv_mbarrier_wait\n+    // CHECK: llvm.inline_asm\n     nvgpu.mbarrier_wait %mbarrier, %pred : !llvm.ptr<i64, 3>, i1\n     tt.return\n   }"}, {"filename": "test/NVGPU/test_tma.mlir", "status": "modified", "additions": 6, "deletions": 24, "changes": 30, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-translate %s | FileCheck %s\n+// RUN: triton-opt %s -split-input-file --convert-nv-gpu-to-llvm | FileCheck %s\n #SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32,  \"triton_gpu.num-ctas\" = 2 : i32} {\n   tt.func @test_tma(%im2colOffsets0 : !llvm.struct<(i16, i16)>, %im2colOffsets1 : !llvm.struct<(i16, i16, i16)>) {\n@@ -14,33 +14,15 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32,  \"triton_gpu.num-ctas\" = 2\n     %pred = arith.constant 1 : i1\n     %mask = arith.constant 15 : i16\n \n-    // CHECK: void @__nv_tma_load_tiled_2d\n-    // CHECK: void @__nv_tma_load_tiled_3d\n-    // CHECK: void @__nv_tma_load_tiled_4d\n-    // CHECK: void @__nv_tma_load_tiled_5d\n+    // CHECK: llvm.inline_asm {{.*}} cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint\n+    // CHECK: llvm.inline_asm {{.*}} cp.async.bulk.tensor.4d.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint\n     nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1 {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 2, 0>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32\n-    nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %c2 {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 3, 0>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i32\n     nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %c2, %c3 {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 4, 0>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i32, i32\n-    nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %c2, %c3, %c4 {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 5, 0>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i32, i32, i32\n \n-    // CHECK: void @__nv_tma_load_tiled_mcast_2d\n-    // CHECK: void @__nv_tma_load_tiled_mcast_3d\n-    // CHECK: void @__nv_tma_load_tiled_mcast_4d\n-    // CHECK: void @__nv_tma_load_tiled_mcast_5d\n+    // CHECK: llvm.inline_asm {{.*}} cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster.L2::cache_hint\n+    // CHECK: llvm.inline_asm {{.*}} cp.async.bulk.tensor.4d.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint\n     nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %mask {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 2, 1>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i16\n-    nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %c2, %mask {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 3, 1>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i32, i16\n-    nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %c2, %c3, %mask {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 4, 1>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i32, i32, i16\n-    nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %c2, %c3, %c4, %mask {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 5, 1>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i32, i32, i32, i16\n-\n-    // CHECK: tail call void @__nv_tma_load_im2col_4d\n-    // CHECK: tail call void @__nv_tma_load_im2col_5d\n-    // CHECK: tail call void @__nv_tma_load_im2col_mcast_4d\n-    // CHECK: tail call void @__nv_tma_load_im2col_mcast_5d\n-    nvgpu.tma_load_im2col %dst, %mbarrier, %tmaDesc, %l2desc, %im2colOffsets0, %pred, %c0, %c1, %c2, %c3 {mcastMask = 0 : i16}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, !llvm.struct<(i16, i16)>, i1, i32, i32, i32, i32\n-    nvgpu.tma_load_im2col %dst, %mbarrier, %tmaDesc, %l2desc, %im2colOffsets1, %pred, %c0, %c1, %c2, %c3, %c4 {mcastMask = 0 : i16}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, !llvm.struct<(i16, i16, i16)>, i1, i32, i32, i32, i32, i32\n-\n-    nvgpu.tma_load_im2col %dst, %mbarrier, %tmaDesc, %l2desc, %im2colOffsets0, %pred, %c0, %c1, %c2, %c3 {mcastMask = 1 : i16}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, !llvm.struct<(i16, i16)>, i1, i32, i32, i32, i32\n-    nvgpu.tma_load_im2col %dst, %mbarrier, %tmaDesc, %l2desc, %im2colOffsets1, %pred, %c0, %c1, %c2, %c3, %c4 {mcastMask = 1 : i16}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, !llvm.struct<(i16, i16, i16)>, i1, i32, i32, i32, i32, i32\n+    nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %c2, %c3 {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 4, 0>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i32, i32\n \n     tt.return\n   }"}, {"filename": "test/NVGPU/test_wgmma.mlir", "status": "modified", "additions": 34, "deletions": 5, "changes": 39, "file_content_changes": "@@ -1,16 +1,45 @@\n-// RUN: triton-translate %s | FileCheck %s\n+// RUN: triton-opt %s -split-input-file --convert-nv-gpu-to-llvm | FileCheck %s\n #SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32,  \"triton_gpu.num-ctas\" = 2 : i32} {\n   tt.func @test_tma(%opC : !llvm.struct<(f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32)>) {\n     %buffer = llvm.mlir.null : !llvm.ptr<i64, 3>\n     %height = arith.constant 16 : i32\n-    // CHECK: call i64 @__nv_get_wgmma_desc\n+    // CHECK: llvm.ptrtoint\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.lshr\n+    // CHECK: llvm.zext\n+    // CHECK: llvm.mul\n+    // CHECK: llvm.lshr\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.lshr\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.or\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.or\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.or\n+    // CHECK: llvm.or\n     %descA = nvgpu.wgmma_desc_create %buffer, %height {mode = 2 : i32}: (!llvm.ptr<i64, 3>, i32) -> (i64)\n-    // CHECK: call i64 @__nv_get_wgmma_desc\n+    // CHECK: llvm.ptrtoint\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.lshr\n+    // CHECK: llvm.zext\n+    // CHECK: llvm.mul\n+    // CHECK: llvm.lshr\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.lshr\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.or\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.or\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.or\n+    // CHECK: llvm.or\n     %descB = nvgpu.wgmma_desc_create %buffer, %height {mode = 2 : i32}: (!llvm.ptr<i64, 3>, i32) -> (i64)\n \n-    // CHECK: wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16\n-    %acc0 = nvgpu.wgmma %descA, %descA, %opC {m=64:i32, n=64:i32, k=16:i32, eltTypeC=7:i32, eltTypeA=4:i32, eltTypeB=4:i32, layoutA=0:i32, layoutB=0:i32} : (i64, i64, !llvm.struct<(f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32)>) -> (!llvm.struct<(f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32)>)\n+    // CHECK-COUNT-32: llvm.extractvalue\n+    // CHECK: llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 {$0,$1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,$18,$19,$20,$21,$22,$23,$24,$25,$26,$27,$28,$29,$30,$31}, $64, $65, 1, 1, 1, 0, 1;\"\n+    %acc0 = nvgpu.wgmma %descA, %descB, %opC {m=64:i32, n=64:i32, k=16:i32, eltTypeC=7:i32, eltTypeA=4:i32, eltTypeB=4:i32, layoutA=0:i32, layoutB=0:i32} : (i64, i64, !llvm.struct<(f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32)>) -> (!llvm.struct<(f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32)>)\n     tt.return\n   }\n } // end module"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/DumpLayout.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -78,7 +78,8 @@ class IndexEmitter {\n \n private:\n   Value getMockSmemBase() {\n-    Value mockSmemBase = base.getSRegValue(rewriter, loc, \"%mock_smem_base\");\n+    Value mockSmemBase =\n+        mlir::LLVM::getSRegValue(rewriter, loc, \"%mock_smem_base\");\n     auto llPtrTy = LLVM::LLVMPointerType::get(\n         typeConverter.convertType(rewriter.getI8Type()), 3);\n     auto cast = rewriter.create<UnrealizedConversionCastOp>("}]