[{"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 13, "deletions": 1, "changes": 14, "file_content_changes": "@@ -501,10 +501,22 @@ section 9.7.13.4.1 for more details.\n   let parameters = (\n     ins\n     \"unsigned\":$opIdx,\n-    \"Attribute\":$parent\n+    \"Attribute\":$parent,\n+    \"unsigned\":$MMAv2kWidth\n   );\n \n   let builders = [\n+        // Specially for MMAV1(Volta)\n+    AttrBuilder<(ins \"unsigned\":$opIdx,\n+                     \"Attribute\":$parent,\n+                     \"Type\":$eltTy), [{\n+      MmaEncodingAttr parentAttr = parent.dyn_cast<MmaEncodingAttr>();\n+      if (!parentAttr || !parentAttr.isAmpere())\n+        return $_get(context, opIdx, parent, 0);\n+      unsigned bitwidth = eltTy.getIntOrFloatBitWidth();\n+      unsigned MMAv2kWidth = 32 / bitwidth;\n+      return $_get(context, opIdx, parent, MMAv2kWidth);\n+    }]>\n   ];\n \n   let hasCustomAssemblyFormat = 1;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -325,7 +325,8 @@ struct ConvertLayoutOpConversion\n \n       if (needTrans) {\n         // do transpose\n-        auto aEncoding = DotOperandEncodingAttr::get(mma.getContext(), 0, mma);\n+        auto aEncoding =\n+            DotOperandEncodingAttr::get(mma.getContext(), 0, mma, 0);\n         int numM = aEncoding.getMMAv1NumOuter(shape);\n         int numN = accumSizePerThread / numM;\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -358,11 +358,11 @@ SmallVector<CoordTy> getMNCoords(Value thread,\n   Value _fpw1 = i32_val(fpw[1]);\n \n   // A info\n-  auto aEncoding = DotOperandEncodingAttr::get(ctx, 0, mmaLayout);\n+  auto aEncoding = DotOperandEncodingAttr::get(ctx, 0, mmaLayout, 0);\n   auto aRep = aEncoding.getMMAv1Rep();\n   auto aSpw = aEncoding.getMMAv1ShapePerWarp();\n   // B info\n-  auto bEncoding = DotOperandEncodingAttr::get(ctx, 1, mmaLayout);\n+  auto bEncoding = DotOperandEncodingAttr::get(ctx, 1, mmaLayout, 0);\n   auto bSpw = bEncoding.getMMAv1ShapePerWarp();\n   auto bRep = bEncoding.getMMAv1Rep();\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -714,11 +714,11 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     Value _fpw1 = i32_val(fpw[1]);\n \n     // A info\n-    auto aEncoding = DotOperandEncodingAttr::get(ctx, 0, mmaLayout);\n+    auto aEncoding = DotOperandEncodingAttr::get(ctx, 0, mmaLayout, 0);\n     auto aRep = aEncoding.getMMAv1Rep();\n     auto aSpw = aEncoding.getMMAv1ShapePerWarp();\n     // B info\n-    auto bEncoding = DotOperandEncodingAttr::get(ctx, 1, mmaLayout);\n+    auto bEncoding = DotOperandEncodingAttr::get(ctx, 1, mmaLayout, 0);\n     auto bSpw = bEncoding.getMMAv1ShapePerWarp();\n     auto bRep = bEncoding.getMMAv1Rep();\n \n@@ -775,12 +775,12 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     // TODO: seems like the apttern below to get `rep`/`spw` appears quite often\n     // A info\n     auto aEncoding =\n-        DotOperandEncodingAttr::get(type.getContext(), 0, mmaLayout);\n+        DotOperandEncodingAttr::get(type.getContext(), 0, mmaLayout, 0);\n     auto aRep = aEncoding.getMMAv1Rep();\n     auto aSpw = aEncoding.getMMAv1ShapePerWarp();\n     // B info\n     auto bEncoding =\n-        DotOperandEncodingAttr::get(type.getContext(), 1, mmaLayout);\n+        DotOperandEncodingAttr::get(type.getContext(), 1, mmaLayout, 0);\n     auto bSpw = bEncoding.getMMAv1ShapePerWarp();\n     auto bRep = bEncoding.getMMAv1Rep();\n "}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 10, "deletions": 8, "changes": 18, "file_content_changes": "@@ -268,6 +268,8 @@ struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n     // a & b must be of smem layout\n     auto aType = adaptor.getA().getType().cast<RankedTensorType>();\n     auto bType = adaptor.getB().getType().cast<RankedTensorType>();\n+    Type aEltType = aType.getElementType();\n+    Type bEltType = bType.getElementType();\n     Attribute aEncoding = aType.getEncoding();\n     Attribute bEncoding = bType.getEncoding();\n     if (!aEncoding || !bEncoding)\n@@ -276,17 +278,17 @@ struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n     Value b = adaptor.getB();\n     Value c = adaptor.getC();\n     if (!aEncoding.isa<triton::gpu::DotOperandEncodingAttr>()) {\n-      Attribute encoding =\n-          triton::gpu::DotOperandEncodingAttr::get(getContext(), 0, dEncoding);\n-      auto dstType = RankedTensorType::get(aType.getShape(),\n-                                           aType.getElementType(), encoding);\n+      Attribute encoding = triton::gpu::DotOperandEncodingAttr::get(\n+          getContext(), 0, dEncoding, aEltType);\n+      auto dstType =\n+          RankedTensorType::get(aType.getShape(), aEltType, encoding);\n       a = rewriter.create<triton::gpu::ConvertLayoutOp>(a.getLoc(), dstType, a);\n     }\n     if (!bEncoding.isa<triton::gpu::DotOperandEncodingAttr>()) {\n-      Attribute encoding =\n-          triton::gpu::DotOperandEncodingAttr::get(getContext(), 1, dEncoding);\n-      auto dstType = RankedTensorType::get(bType.getShape(),\n-                                           bType.getElementType(), encoding);\n+      Attribute encoding = triton::gpu::DotOperandEncodingAttr::get(\n+          getContext(), 1, dEncoding, bEltType);\n+      auto dstType =\n+          RankedTensorType::get(bType.getShape(), bEltType, encoding);\n       b = rewriter.create<triton::gpu::ConvertLayoutOp>(b.getLoc(), dstType, b);\n     }\n     c = rewriter.create<triton::gpu::ConvertLayoutOp>(c.getLoc(), retType, c);"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 16, "deletions": 3, "changes": 19, "file_content_changes": "@@ -774,14 +774,27 @@ Attribute DotOperandEncodingAttr::parse(AsmParser &parser, Type type) {\n     return {};\n   unsigned opIdx = attrs.get(\"opIdx\").cast<IntegerAttr>().getInt();\n   Attribute parent = attrs.get(\"parent\");\n+  auto mmaParent = parent.dyn_cast<MmaEncodingAttr>();\n+  unsigned kWidth = 0;\n+  Attribute _kWidth = attrs.get(\"kWidth\");\n+  if (_kWidth) {\n+    if (!mmaParent || mmaParent.isVolta()) {\n+      auto loc = parser.getNameLoc();\n+      parser.emitError(loc, \"kWidth only supported for MMAv2+ parent\");\n+      return Attribute();\n+    }\n+    kWidth = _kWidth.cast<IntegerAttr>().getInt();\n+  }\n   return parser.getChecked<DotOperandEncodingAttr>(parser.getContext(), opIdx,\n-                                                   parent);\n+                                                   parent, kWidth);\n }\n \n void DotOperandEncodingAttr::print(mlir::AsmPrinter &printer) const {\n+  auto mmaParent = getParent().dyn_cast<MmaEncodingAttr>();\n   printer << \"<{\"\n-          << \"opIdx = \" << getOpIdx() << \", \"\n-          << \"parent = \" << getParent();\n+          << \"opIdx = \" << getOpIdx() << \", parent = \" << getParent();\n+  if (mmaParent && mmaParent.isAmpere())\n+    printer << \", kWidth = \" << getMMAv2kWidth();\n   printer << \"}>\";\n }\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 9, "deletions": 6, "changes": 15, "file_content_changes": "@@ -170,14 +170,17 @@ class BlockedToMMA : public mlir::RewritePattern {\n                          .cast<triton::gpu::BlockedEncodingAttr>()\n                          .getOrder();\n \n+    auto newAEncoding = triton::gpu::DotOperandEncodingAttr::get(\n+        oldAType.getContext(), 0, newRetType.getEncoding(),\n+        oldAType.getElementType());\n+    auto newBEncoding = triton::gpu::DotOperandEncodingAttr::get(\n+        oldBType.getContext(), 1, newRetType.getEncoding(),\n+        oldBType.getElementType());\n+\n     auto newAType = RankedTensorType::get(\n-        oldAType.getShape(), oldAType.getElementType(),\n-        triton::gpu::DotOperandEncodingAttr::get(oldAType.getContext(), 0,\n-                                                 newRetType.getEncoding()));\n+        oldAType.getShape(), oldAType.getElementType(), newAEncoding);\n     auto newBType = RankedTensorType::get(\n-        oldBType.getShape(), oldBType.getElementType(),\n-        triton::gpu::DotOperandEncodingAttr::get(oldBType.getContext(), 1,\n-                                                 newRetType.getEncoding()));\n+        oldBType.getShape(), oldBType.getElementType(), newBEncoding);\n \n     a = rewriter.create<triton::gpu::ConvertLayoutOp>(a.getLoc(), newAType, a);\n     b = rewriter.create<triton::gpu::ConvertLayoutOp>(b.getLoc(), newBType, b);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 62, "deletions": 22, "changes": 84, "file_content_changes": "@@ -1,3 +1,4 @@\n+#include \"Utility.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/IRMapping.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n@@ -42,6 +43,9 @@ class LoopPipeliner {\n \n   /// Loads to be pipelined\n   SetVector<Value> loads;\n+  /// Smallest data-type for each load (used to optimize swizzle and\n+  /// (create DotOpEncoding layout)\n+  DenseMap<Value, Type> loadsSmallestType;\n   /// The value that each load will be mapped to (after layout conversion)\n   DenseMap<Value, Value> loadsMapping;\n   /// load => buffer\n@@ -256,33 +260,62 @@ LogicalResult LoopPipeliner::initialize() {\n         use = *use->getResult(0).getUsers().begin();\n       }\n \n-      if (auto convertLayout = llvm::dyn_cast<ttg::ConvertLayoutOp>(use)) {\n-        if (auto tensorType = convertLayout.getResult()\n-                                  .getType()\n-                                  .dyn_cast<RankedTensorType>()) {\n-          if (auto dotOpEnc = tensorType.getEncoding()\n-                                  .dyn_cast<ttg::DotOperandEncodingAttr>()) {\n-            isCandidate = true;\n-            loadsMapping[loadOp] = convertLayout;\n-            auto ty = loadOp.getType().cast<RankedTensorType>();\n-            SmallVector<int64_t> bufferShape(ty.getShape().begin(),\n-                                             ty.getShape().end());\n-            bufferShape.insert(bufferShape.begin(), numStages);\n-            auto sharedEnc = ttg::SharedEncodingAttr::get(\n-                ty.getContext(), dotOpEnc, ty.getShape(),\n-                triton::gpu::getOrder(ty.getEncoding()), ty.getElementType());\n-            loadsBufferType[loadOp] = RankedTensorType::get(\n-                bufferShape, ty.getElementType(), sharedEnc);\n-          }\n-        }\n-      }\n-    } else\n+      auto convertLayout = llvm::dyn_cast<ttg::ConvertLayoutOp>(use);\n+      if (!convertLayout)\n+        continue;\n+      auto tensorType =\n+          convertLayout.getResult().getType().dyn_cast<RankedTensorType>();\n+      if (!tensorType)\n+        continue;\n+      auto dotOpEnc =\n+          tensorType.getEncoding().dyn_cast<ttg::DotOperandEncodingAttr>();\n+      if (!dotOpEnc)\n+        continue;\n+      isCandidate = true;\n+      loadsMapping[loadOp] = convertLayout;\n+    }\n+\n+    else\n       isCandidate = false;\n \n     if (isCandidate)\n       loads.insert(loadOp);\n   }\n \n+  // we need to find the smallest ocmmon dtype\n+  // since this determines the layout of `mma.sync` operands\n+  // in mixed-precision mode\n+  Type smallestType;\n+  for (auto loadCvt : loadsMapping) {\n+    auto loadOp = loadCvt.first;\n+    auto ty = loadOp.getType().cast<RankedTensorType>();\n+    Type eltTy = ty.getElementType();\n+    if (!smallestType ||\n+        (eltTy.getIntOrFloatBitWidth() < smallestType.getIntOrFloatBitWidth()))\n+      smallestType = eltTy;\n+  }\n+\n+  for (auto loadCvt : loadsMapping)\n+    loadsSmallestType[loadCvt.first] = smallestType;\n+\n+  for (auto loadCvt : loadsMapping) {\n+    auto loadOp = loadCvt.first;\n+    Value cvt = loadCvt.second;\n+    auto dotOpEnc = cvt.getType()\n+                        .cast<RankedTensorType>()\n+                        .getEncoding()\n+                        .cast<ttg::DotOperandEncodingAttr>();\n+    auto ty = loadOp.getType().cast<RankedTensorType>();\n+    SmallVector<int64_t> bufferShape(ty.getShape().begin(),\n+                                     ty.getShape().end());\n+    bufferShape.insert(bufferShape.begin(), numStages);\n+    auto sharedEnc = ttg::SharedEncodingAttr::get(\n+        ty.getContext(), dotOpEnc, ty.getShape(),\n+        triton::gpu::getOrder(ty.getEncoding()), loadsSmallestType[loadOp]);\n+    loadsBufferType[loadOp] =\n+        RankedTensorType::get(bufferShape, ty.getElementType(), sharedEnc);\n+  }\n+\n   // We have some loads to pipeline\n   if (!loads.empty()) {\n     // Update depArgs & depOps\n@@ -551,8 +584,15 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n     }\n     // we replace the use new load use with a convert layout\n     size_t i = std::distance(loads.begin(), it);\n+    auto cvtDstTy = op.getResult(0).getType().cast<RankedTensorType>();\n+    auto cvtDstEnc = cvtDstTy.getEncoding().cast<ttg::DotOperandEncodingAttr>();\n+    auto newDstTy = RankedTensorType::get(\n+        cvtDstTy.getShape(), cvtDstTy.getElementType(),\n+        ttg::DotOperandEncodingAttr::get(\n+            cvtDstEnc.getContext(), cvtDstEnc.getOpIdx(), cvtDstEnc.getParent(),\n+            loadsSmallestType[op.getOperand(0)]));\n     auto cvt = builder.create<ttg::ConvertLayoutOp>(\n-        op.getLoc(), op.getResult(0).getType(),\n+        op.getResult(0).getLoc(), newDstTy,\n         newForOp.getRegionIterArgs()[loadIdx + i]);\n     mapping.map(op.getResult(0), cvt.getResult());\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 15, "deletions": 5, "changes": 20, "file_content_changes": "@@ -110,7 +110,7 @@ Value Prefetcher::generatePrefetch(Value v, unsigned opIdx, bool isPrologue,\n       SmallVector<OpFoldResult>{intAttr(1), intAttr(1)});\n \n   auto dotOperandEnc = triton::gpu::DotOperandEncodingAttr::get(\n-      builder.getContext(), opIdx, dotEncoding);\n+      builder.getContext(), opIdx, dotEncoding, prefetchWidth / 8);\n   Value prefetchSlice = builder.create<triton::gpu::ConvertLayoutOp>(\n       v.getLoc(), RankedTensorType::get(shape, elementType, dotOperandEnc),\n       newSmem);\n@@ -156,12 +156,22 @@ LogicalResult Prefetcher::initialize() {\n   };\n \n   for (triton::DotOp dot : dotsInFor) {\n-    auto kSize = dot.getA().getType().cast<RankedTensorType>().getShape()[1];\n+    auto aType = dot.getA().getType().cast<RankedTensorType>();\n+    auto bType = dot.getB().getType().cast<RankedTensorType>();\n+    auto aEnc = aType.getEncoding().cast<triton::gpu::DotOperandEncodingAttr>();\n+    auto bEnc = bType.getEncoding().cast<triton::gpu::DotOperandEncodingAttr>();\n+    int aKWidth = aEnc.getMMAv2kWidth();\n+    int bKWidth = bEnc.getMMAv2kWidth();\n+    assert(aKWidth == bKWidth);\n+\n+    auto kSize = aType.getShape()[1];\n \n     // works better with nvidia tensor cores\n-    unsigned elementWidth =\n-        dot.getA().getType().cast<RankedTensorType>().getElementTypeBitWidth();\n-    prefetchWidth = 256 / elementWidth;\n+    unsigned elementWidth = aType.getElementTypeBitWidth();\n+    if (aKWidth == 0)\n+      prefetchWidth = 256 / elementWidth;\n+    else\n+      prefetchWidth = 8 * aKWidth;\n \n     // Skip prefetching if kSize is less than prefetchWidth\n     if (kSize < prefetchWidth)"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -341,7 +341,10 @@ class RematerializeForward : public mlir::RewritePattern {\n         cvt.getOperand().getType().cast<RankedTensorType>().getEncoding();\n     auto dstEncoding =\n         cvt.getResult().getType().cast<RankedTensorType>().getEncoding();\n-    // XXX: why is this needed?\n+    if (srcEncoding.isa<triton::gpu::SharedEncodingAttr>() ||\n+        dstEncoding.isa<triton::gpu::SharedEncodingAttr>())\n+      return failure();\n+    // heuristics for flash attention\n     if (srcEncoding.isa<triton::gpu::SliceEncodingAttr>())\n       return failure();\n     SetVector<Operation *> cvtSlices;"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "file_content_changes": "@@ -206,11 +206,15 @@ int simulateBackwardRematerialization(\n \n //\n \n-Operation *cloneWithInferType(mlir::PatternRewriter &rewriter, Operation *op,\n+Operation *cloneWithInferType(mlir::OpBuilder &rewriter, Operation *op,\n                               IRMapping &mapping) {\n   Operation *newOp = rewriter.clone(*op, mapping);\n-  auto origType = op->getResult(0).getType().cast<RankedTensorType>();\n-  auto argType = newOp->getOperand(0).getType().cast<RankedTensorType>();\n+  if (newOp->getNumResults() == 0)\n+    return newOp;\n+  auto origType = op->getResult(0).getType().dyn_cast<RankedTensorType>();\n+  auto argType = newOp->getOperand(0).getType().dyn_cast<RankedTensorType>();\n+  if (!origType || !argType)\n+    return newOp;\n   auto newType = RankedTensorType::get(\n       origType.getShape(), origType.getElementType(), argType.getEncoding());\n   newOp->getResult(0).setType(newType);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -21,7 +21,7 @@ int simulateBackwardRematerialization(\n     SetVector<Attribute> &layout, llvm::MapVector<Value, Attribute> &toConvert,\n     Attribute targetEncoding);\n \n-Operation *cloneWithInferType(mlir::PatternRewriter &rewriter, Operation *op,\n+Operation *cloneWithInferType(mlir::OpBuilder &rewriter, Operation *op,\n                               IRMapping &mapping);\n \n void rematerializeConversionChain("}, {"filename": "python/triton/third_party/cuda/bin/ptxas", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -6,8 +6,8 @@\n #A_SHARED_T = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [0, 1]}>\n #B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n-#A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n-#B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n+#A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth=2}>\n+#B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth=2}>\n \n // CHECK-LABEL: matmul_loop\n // There shouldn't be any aliasing with the dot op encoding."}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -7,8 +7,8 @@\n #A_SHARED_T = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [0, 1]}>\n #B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n-#A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n-#B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n+#A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth=2}>\n+#B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth=2}>\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n "}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -7,8 +7,8 @@\n #A_SHARED_T = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [0, 1]}>\n #B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n-#A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n-#B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n+#A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth=2}>\n+#B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth=2}>\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n "}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -755,8 +755,8 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n #shared0 = #triton_gpu.shared<{vec = 1, perPhase=2, maxPhase=8 ,order = [1, 0]}>\n #mma0 = #triton_gpu.mma<{versionMajor=2, warpsPerCTA=[1,1]}>\n-#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0}>\n-#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, kWidth=2}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, kWidth=2}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_dot\n   tt.func @convert_dot(%A: tensor<16x16xf16, #blocked0>, %B: tensor<16x16xf16, #blocked0>) {\n@@ -897,8 +897,8 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n #shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n #mma = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [2, 2]}>\n-#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n-#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma, kWidth=2}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma, kWidth=2}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   tt.func @matmul_kernel_dot_operand_layout(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %a:tensor<128x32xf16, #shared>, %b:tensor<32x256xf16, #shared>) {\n@@ -969,8 +969,8 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #mma = #triton_gpu.mma<{versionMajor=2, warpsPerCTA=[2, 2]}>\n #shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n-#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n-#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma, kWidth=1}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma, kWidth=1}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: matmul_tf32dot\n   tt.func @matmul_tf32dot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -8,8 +8,8 @@\n #BLs0 = #triton_gpu.slice<{parent=#BL, dim=0}>\n #BLs1 = #triton_gpu.slice<{parent=#BL, dim=1}>\n #C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n-#A = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n-#B = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n+#A = #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth=2}>\n+#B = #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth=2}>\n \n // CHECK: tt.func @matmul_loop\n // CHECK-DAG: %[[CONSTANT_0:.*]] = arith.constant 0 : i32"}, {"filename": "test/TritonGPU/prefetch.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -7,8 +7,8 @@\n #A = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #B = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #C = #triton_gpu.mma<{version = 2, warpsPerCTA = [4, 1]}>\n-#A_OP = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n-#B_OP = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n+#A_OP = #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth = 2}>\n+#B_OP = #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth = 2}>\n \n \n // CHECK: tt.func @matmul_loop"}, {"filename": "unittest/Dialect/TritonGPU/SwizzleTest.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -30,7 +30,7 @@ TEST_P(SwizzleDotOperandTestFixture, DotOperands) {\n   // create encoding\n   auto parent = triton::gpu::MmaEncodingAttr::get(&ctx, 2, 0, {1, 1});\n   auto encoding =\n-      triton::gpu::DotOperandEncodingAttr::get(&ctx, params.opIdx, parent);\n+      triton::gpu::DotOperandEncodingAttr::get(&ctx, params.opIdx, parent, 0);\n \n   // create element type\n   Type eltType = IntegerType::get(&ctx, params.typeWidth);"}]