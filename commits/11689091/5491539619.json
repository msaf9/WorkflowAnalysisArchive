[{"filename": "python/test/unit/operators/test_cross_entropy.py", "status": "modified", "additions": 4, "deletions": 5, "changes": 9, "file_content_changes": "@@ -29,13 +29,12 @@ def test_op(M, N, dtype, mode):\n     # backward pass\n     elif mode == 'backward':\n         dy = torch.randn_like(tt_y)\n-        # run torch first to make sure cuda context is initialized\n-        # torch backward\n-        th_y.backward(dy)\n-        th_dx = x.grad.clone()\n         # triton backward\n-        x.grad.zero_()\n         tt_y.backward(dy)\n         tt_dx = x.grad.clone()\n+        # torch backward\n+        x.grad = None\n+        th_y.backward(dy)\n+        th_dx = x.grad.clone()\n \n         torch.testing.assert_allclose(th_dx, tt_dx)"}, {"filename": "python/triton/runtime/backends/cuda.c", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -69,6 +69,13 @@ static PyObject *loadBinary(PyObject *self, PyObject *args) {\n   int32_t n_regs = 0;\n   int32_t n_spills = 0;\n   // create driver handles\n+  CUcontext pctx = 0;\n+  CUDA_CHECK(cuCtxGetCurrent(&pctx));\n+  if (!pctx) {\n+    CUDA_CHECK(cuDevicePrimaryCtxRetain(&pctx, device));\n+    CUDA_CHECK(cuCtxSetCurrent(pctx));\n+  }\n+\n   CUDA_CHECK(cuModuleLoadData(&mod, data));\n   CUDA_CHECK(cuModuleGetFunction(&fun, mod, name));\n   // get allocated registers and spilled registers from the function"}]