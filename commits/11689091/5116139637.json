[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "file_content_changes": "@@ -1379,6 +1379,9 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n     for op in ['min', 'max', 'sum', 'argmin', 'argmax']\n     for shape in reduce2d_shapes\n     for axis in [0, 1]\n+] + [\n+    (op, 'float32', [16, 32], None)\n+    for op in ['min', 'max', 'sum']\n ]\n \n \n@@ -1393,7 +1396,9 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n         range_n = tl.arange(0, BLOCK_N)\n         x = tl.load(X + range_m[:, None] * BLOCK_N + range_n[None, :])\n         z = GENERATE_TEST_HERE\n-        if AXIS == 1:\n+        if AXIS is None:\n+            tl.store(Z, z)\n+        elif AXIS == 1:\n             tl.store(Z + range_m, z)\n         else:\n             tl.store(Z + range_n, z)\n@@ -1418,7 +1423,8 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n     else:\n         z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n     # triton result\n-    z_tri = to_triton(numpy_random((shape[1 - axis],), dtype_str=z_dtype_str, rs=rs),\n+    ret_numel = 1 if axis is None else shape[1 - axis]\n+    z_tri = to_triton(numpy_random((ret_numel,), dtype_str=z_dtype_str, rs=rs),\n                       device=device, dst_type=z_tri_dtype_str)\n     kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis)\n     z_tri = to_numpy(z_tri)"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -388,7 +388,8 @@ def visit_Assign(self, node):\n         for name, value in zip(names, values):\n             # by default, constexpr are assigned into python variable\n             value = _unwrap_if_constexpr(value)\n-            if not _is_triton_tensor(value) and \\\n+            if value is not None and \\\n+               not _is_triton_tensor(value) and \\\n                not isinstance(value, native_nontensor_types):\n                 value = language.core._to_tensor(value, self.builder)\n             self.set_value(name, value)"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -1297,8 +1297,8 @@ def make_combine_region(reduce_op):\n             else:\n                 handles = [r.handle for r in results]\n             _builder.create_reduce_ret(*handles)\n-\n-    axis = _constexpr_to_value(axis)\n+    if axis is not None:\n+        axis = _constexpr_to_value(axis)\n     return semantic.reduction(input, axis, make_combine_region, _builder)\n \n \n@@ -1369,7 +1369,7 @@ def _max_combine(a, b):\n \n @triton.jit\n @_add_reduction_docstr(\"maximum\")\n-def max(input, axis):\n+def max(input, axis=None):\n     input = _promote_reduction_input(input)\n     return reduce(input, axis, _max_combine)\n \n@@ -1399,7 +1399,7 @@ def _min_combine(a, b):\n \n @triton.jit\n @_add_reduction_docstr(\"minimum\")\n-def min(input, axis):\n+def min(input, axis=None):\n     input = _promote_reduction_input(input)\n     return reduce(input, axis, _min_combine)\n \n@@ -1428,7 +1428,7 @@ def _sum_combine(a, b):\n \n @triton.jit\n @_add_reduction_docstr(\"sum\")\n-def sum(input, axis):\n+def sum(input, axis=None):\n     input = _promote_reduction_input(input)\n     return reduce(input, axis, _sum_combine)\n \n@@ -1440,7 +1440,7 @@ def _xor_combine(a, b):\n \n @builtin\n @_add_reduction_docstr(\"xor sum\")\n-def xor_sum(input, axis, _builder=None, _generator=None):\n+def xor_sum(input, axis=None, _builder=None, _generator=None):\n     scalar_ty = input.type.scalar\n     if not scalar_ty.is_int():\n         raise ValueError(\"xor_sum only supported for integers\")"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -1236,6 +1236,13 @@ def where(condition: tl.tensor,\n def reduction(\n     inputs: Sequence[tl.tensor], axis: int, region_builder_fn, builder: ir.builder\n ) -> Tuple[tl.tensor, ...]:\n+    if axis is None:\n+        new_inputs = []\n+        for i in range(len(inputs)):\n+            new_shape = [inputs[i].numel.value]\n+            new_inputs.append(view(inputs[i], new_shape, builder))\n+        inputs = tuple(new_inputs)\n+        axis = 0\n     # get result shape\n     shape = inputs[0].type.shape\n     ret_shape = [s for i, s in enumerate(shape) if i != axis]"}]