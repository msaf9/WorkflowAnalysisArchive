[{"filename": "include/triton/Dialect/Triton/IR/TritonTypes.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -14,7 +14,7 @@ class TritonTypeDef<string name, string _mnemonic>\n }\n \n // Floating-point Type\n-def TT_Float : AnyTypeOf<[F8E4M3FNUZ, F8E4M3B11FNUZ, F8E5M2, F16, BF16, F32, F64], \"floating-point\">;\n+def TT_Float : AnyTypeOf<[F8E4M3FNUZ, F8E4M3FN, F8E4M3B11FNUZ, F8E5M2, F16, BF16, F32, F64], \"floating-point\">;\n def TT_FloatTensor : TensorOf<[TT_Float]>;\n def TT_FloatLike : AnyTypeOf<[TT_Float, TT_FloatTensor]>;\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -576,7 +576,8 @@ struct ConvertLayoutOpConversion\n     unsigned inVec = 0;\n     unsigned outVec = 0;\n     auto paddedRepShape = getScratchConfigForCvtLayout(op, inVec, outVec);\n-    if (getElementTypeOrSelf(op.getType()).isa<mlir::Float8E4M3B11FNUZType>()) {\n+    if (getElementTypeOrSelf(op.getType())\n+            .isa<mlir::Float8E4M3B11FNUZType, mlir::Float8E4M3FNType>()) {\n       assert(inVec % 4 == 0 && \"conversion not supported for FP8E4M3B15\");\n       assert(outVec % 4 == 0 && \"conversion not supported for FP8E4M3B15\");\n     }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -503,7 +503,7 @@ std::function<void(int, int)> getLoadMatrixFn(\n   if (tensor.getType()\n           .cast<RankedTensorType>()\n           .getElementType()\n-          .isa<mlir::Float8E4M3B11FNUZType>()) {\n+          .isa<mlir::Float8E4M3B11FNUZType, mlir::Float8E4M3FNType>()) {\n     bool noTrans = (isA ^ (order[0] == 0));\n     assert(noTrans && \"float8e4b15 must have row-col layout\");\n   }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -627,17 +627,20 @@ struct FpToFpOpConversion\n     auto F8E4M3B15TyID = TypeID::get<mlir::Float8E4M3B11FNUZType>();\n     auto F8E4M3TyID = TypeID::get<mlir::Float8E4M3FNUZType>();\n     auto F8E5M2TyID = TypeID::get<mlir::Float8E5M2Type>();\n+    auto F8E4M3FNTyID = TypeID::get<mlir::Float8E4M3FNType>();\n     auto F16TyID = TypeID::get<mlir::Float16Type>();\n     auto BF16TyID = TypeID::get<mlir::BFloat16Type>();\n     auto F32TyID = TypeID::get<mlir::Float32Type>();\n     auto F64TyID = TypeID::get<mlir::Float64Type>();\n     static DenseMap<std::pair<TypeID, TypeID>, std::string> srcMap = {\n         // F8 -> F16\n         {{F8E4M3B15TyID, F16TyID}, Fp8E4M3B15_to_Fp16},\n+        {{F8E4M3FNTyID, F16TyID}, Fp8E4M3B15x4_to_Fp16},\n         {{F8E4M3TyID, F16TyID}, Fp8E4M3_to_Fp16},\n         {{F8E5M2TyID, F16TyID}, Fp8E5M2_to_Fp16},\n         // F16 -> F8\n         {{F16TyID, F8E4M3B15TyID}, Fp16_to_Fp8E4M3B15},\n+        {{F16TyID, F8E4M3FNTyID}, Fp16_to_Fp8E4M3B15x4},\n         {{F16TyID, F8E4M3TyID}, Fp16_to_Fp8E4M3},\n         {{F16TyID, F8E5M2TyID}, Fp16_to_Fp8E5M2},\n         // F8 -> BF16"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -610,7 +610,8 @@ class ConvertTritonGPUToLLVM\n   void decomposeFp8e4b15Convert(ModuleOp mod) const {\n     mod.walk([&](triton::gpu::ConvertLayoutOp cvtOp) -> void {\n       OpBuilder builder(cvtOp);\n-      if (!getElementTypeOrSelf(cvtOp).isa<mlir::Float8E4M3B11FNUZType>())\n+      if (!getElementTypeOrSelf(cvtOp)\n+               .isa<mlir::Float8E4M3B11FNUZType, mlir::Float8E4M3FNType>())\n         return;\n       auto shape = cvtOp.getType().cast<RankedTensorType>().getShape();\n       auto argEncoding ="}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.cpp", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -27,6 +27,9 @@ TritonGPUToLLVMTypeConverter::TritonGPUToLLVMTypeConverter(\n   addConversion([&](mlir::Float8E4M3B11FNUZType type) -> std::optional<Type> {\n     return IntegerType::get(type.getContext(), 8);\n   });\n+  addConversion([&](mlir::Float8E4M3FNType type) -> std::optional<Type> {\n+    return IntegerType::get(type.getContext(), 8);\n+  });\n   addConversion([&](mlir::Float8E4M3FNUZType type) -> std::optional<Type> {\n     return IntegerType::get(type.getContext(), 8);\n   });"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -765,6 +765,12 @@ void init_triton_ir(py::module &&m) {\n              // have a float-like type compatible with float only native ops\n              return self.getBuilder().getType<mlir::Float8E4M3B11FNUZType>();\n            })\n+      .def(\"get_fp8e4b15x4_ty\",\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             // TODO: upstream FP8E4B15 into MLIR, or find a way to externally\n+             // have a float-like type compatible with float only native ops\n+             return self.getBuilder().getType<mlir::Float8E4M3FNType>();\n+           })\n       .def(\"get_fp8e5_ty\",\n            [](TritonOpBuilder &self) -> mlir::Type {\n              return self.getBuilder().getType<mlir::Float8E5M2Type>();"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 26, "deletions": 30, "changes": 56, "file_content_changes": "@@ -1384,43 +1384,39 @@ def test_convert_float16_to_float32(in_dtype, device):\n \n \n def serialize_fp8(np_data, in_dtype):\n-    return np_data\n-# def serialize_fp8(np_data, in_dtype):\n-#     if in_dtype == tl.float8e4b15:\n-#         # triton's f8e4b15 format is optimized for software emulation\n-#         # as a result, each pack of 4xfp8 values:\n-#         # s0b0s1b1s2b2s3b3 (for s, b sign and bits respectively)\n-#         # is actually internally stored as\n-#         # s0s2b0b2s1s3b1b3\n-#         # we apply the conversion here\n-#         f8x4 = np_data.view(np.uint32)\n-#         s = [(f8x4 & (0x80000000 >> i)) << i for i in range(0, 32, 8)]\n-#         b = [(f8x4 & (0x7f000000 >> i)) << i for i in range(0, 32, 8)]\n-#         signs = (s[0] >> 0) | (s[1] >> 16) | (s[2] >> 1) | (s[3] >> 17)\n-#         bits = (b[0] >> 1) | (b[1] >> 17) | (b[2] >> 8) | (b[3] >> 24)\n-#         # tensor of triton fp8 data\n-#         return (signs | bits).view(np.int8)\n-#     else:\n-#         return np_data\n+    if in_dtype == tl.float8e4b15x4:\n+        # triton's f8e4b15 format is optimized for software emulation\n+        # as a result, each pack of 4xfp8 values:\n+        # s0b0s1b1s2b2s3b3 (for s, b sign and bits respectively)\n+        # is actually internally stored as\n+        # s0s2b0b2s1s3b1b3\n+        # we apply the conversion here\n+        f8x4 = np_data.view(np.uint32)\n+        s = [(f8x4 & (0x80000000 >> i)) << i for i in range(0, 32, 8)]\n+        b = [(f8x4 & (0x7f000000 >> i)) << i for i in range(0, 32, 8)]\n+        signs = (s[0] >> 0) | (s[1] >> 16) | (s[2] >> 1) | (s[3] >> 17)\n+        bits = (b[0] >> 1) | (b[1] >> 17) | (b[2] >> 8) | (b[3] >> 24)\n+        # tensor of triton fp8 data\n+        return (signs | bits).view(np.int8)\n+    else:\n+        return np_data\n \n # inverse of `serialize_fp8`\n \n \n def deserialize_fp8(np_data, in_dtype):\n-    return np_data\n-# def deserialize_fp8(np_data, in_dtype):\n-#     if in_dtype == tl.float8e4b15:\n-#         f8x4 = np_data.view(np.uint32)\n-#         s = [(f8x4 & (0x80000000 >> i)) << i for i in [0, 16, 1, 17]]\n-#         b = [(f8x4 & (0x7f000000 >> i)) << i for i in [1, 17, 8, 24]]\n-#         signs = (s[0] >> 0) | (s[1] >> 8) | (s[2] >> 16) | (s[3] >> 24)\n-#         bits = (b[0] >> 0) | (b[1] >> 8) | (b[2] >> 16) | (b[3] >> 24)\n-#         return (signs | bits).view(np.int8)\n-#     else:\n-#         return np_data\n+    if in_dtype == tl.float8e4b15x4:\n+        f8x4 = np_data.view(np.uint32)\n+        s = [(f8x4 & (0x80000000 >> i)) << i for i in [0, 16, 1, 17]]\n+        b = [(f8x4 & (0x7f000000 >> i)) << i for i in [1, 17, 8, 24]]\n+        signs = (s[0] >> 0) | (s[1] >> 8) | (s[2] >> 16) | (s[3] >> 24)\n+        bits = (b[0] >> 0) | (b[1] >> 8) | (b[2] >> 16) | (b[3] >> 24)\n+        return (signs | bits).view(np.int8)\n+    else:\n+        return np_data\n \n \n-@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4, tl.float8e5])\n+@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4b15x4, tl.float8e4, tl.float8e5])\n @pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.float32])\n def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n     \"\"\""}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1065,6 +1065,7 @@ def str_to_ty(name):\n         \"fp8e4\": language.float8e4,\n         \"fp8e5\": language.float8e5,\n         \"fp8e4b15\": language.float8e4b15,\n+        \"fp8e4b15x4\": language.float8e4b15x4,\n         \"fp16\": language.float16,\n         \"bf16\": language.bfloat16,\n         \"fp32\": language.float32,"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -50,6 +50,7 @@\n     float32,\n     float64,\n     float8e4b15,\n+    float8e4b15x4,\n     float8e4,\n     float8e5,\n     function_type,\n@@ -148,6 +149,7 @@\n     \"float32\",\n     \"float64\",\n     \"float8e4b15\",\n+    \"float8e4b15x4\",\n     \"float8e4\",\n     \"float8e5\",\n     \"full\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 11, "deletions": 1, "changes": 12, "file_content_changes": "@@ -76,7 +76,7 @@ def _to_tensor(x, builder):\n class dtype:\n     SINT_TYPES = ['int8', 'int16', 'int32', 'int64']\n     UINT_TYPES = ['int1', 'uint8', 'uint16', 'uint32', 'uint64']\n-    FP_TYPES = ['fp8e4b15', 'fp8e4', 'fp8e5', 'fp16', 'bf16', 'fp32', 'fp64']\n+    FP_TYPES = ['fp8e4b15', 'fp8e4b15x4', 'fp8e4', 'fp8e5', 'fp16', 'bf16', 'fp32', 'fp64']\n     STANDARD_FP_TYPES = ['fp16', 'bf16', 'fp32', 'fp64']\n     OTHER_TYPES = ['void']\n \n@@ -100,6 +100,10 @@ def __init__(self, name):\n                 self.fp_mantissa_width = 3\n                 self.primitive_bitwidth = 8\n                 self.exponent_bias = 15\n+            elif name == 'fp8e4b15x4':\n+                self.fp_mantissa_width = 3\n+                self.primitive_bitwidth = 8\n+                self.exponent_bias = 15\n             elif name == 'fp8e4':\n                 self.fp_mantissa_width = 3\n                 self.primitive_bitwidth = 8\n@@ -138,6 +142,9 @@ def is_fp8e4(self):\n     def is_fp8e4b15(self):\n         return self.name == 'fp8e4b15'\n \n+    def is_fp8e4b15x4(self):\n+        return self.name == 'fp8e4b15x4'\n+\n     def is_fp16(self):\n         return self.name == 'fp16'\n \n@@ -241,6 +248,8 @@ def to_ir(self, builder: ir.builder) -> ir.type:\n             return builder.get_fp8e4_ty()\n         elif self.name == 'fp8e4b15':\n             return builder.get_fp8e4b15_ty()\n+        elif self.name == 'fp8e4b15x4':\n+            return builder.get_fp8e4b15x4_ty()\n         elif self.name == 'fp16':\n             return builder.get_half_ty()\n         elif self.name == 'bf16':\n@@ -375,6 +384,7 @@ def to_ir(self, builder: ir.builder):\n float8e5 = dtype('fp8e5')\n float8e4 = dtype('fp8e4')\n float8e4b15 = dtype('fp8e4b15')\n+float8e4b15x4 = dtype('fp8e4b15x4')\n float16 = dtype('fp16')\n bfloat16 = dtype('bf16')\n float32 = dtype('fp32')"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -248,6 +248,7 @@ def _type_of(key):\n             \"float8e4\": \"fp8e4\",\n             \"float8e5\": \"fp8e5\",\n             \"float8e4b15\": \"fp8e4b15\",\n+            \"float8e4b15x4\": \"fp8e4b15x4\",\n             \"float16\": \"fp16\",\n             \"bfloat16\": \"bf16\",\n             \"float32\": \"fp32\","}]