[{"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 34, "deletions": 3, "changes": 37, "file_content_changes": "@@ -90,13 +90,44 @@ const std::string Bf16_to_Fp8E5M2 =\n //    - has infinities\n //    - has multiple nans (when all exponent bits are 1)\n //    - has an exponent bias of 15 (vs. 7 for fp8e4m3)\n+const std::string Fp8E4M3B15_to_Fp16 =\n+    \"{                                      \\n\"\n+    \".reg .b32 a<2>, b<2>;                  \\n\"\n+    \"prmt.b32 a0, 0, $2, 0x5040;            \\n\"\n+    \"prmt.b32 a1, 0, $2, 0x7060;            \\n\"\n+    \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n+    \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n+    \"shr.b32  b0, b0, 1;                    \\n\"\n+    \"shr.b32  b1, b1, 1;                    \\n\"\n+    \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\"\n+    \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\"\n+    \"}                                      \\n\";\n+\n+const std::string Fp16_to_Fp8E4M3B15 =\n+    \"{                                      \\n\"\n+    \".reg .b32 a<2>, b<2>;                  \\n\"\n+    \"shl.b32 a0, $1, 1;                     \\n\"\n+    \"shl.b32 a1, $2, 1;                     \\n\"\n+    \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n+    \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n+    \"add.u32 a0, a0, 0x00800080;            \\n\"\n+    \"add.u32 a1, a1, 0x00800080;            \\n\"\n+    \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\"\n+    \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\"\n+    \"prmt.b32 $0, b0, b1, 0x7531;           \\n\"\n+    \"}\";\n+\n+/* ----- FP8E4M3B15X4 ------ */\n+// NOTE: NOT USED RIGHT NOW\n+// Packed variant of FP8E4M3B15\n+// A little bit more efficient but elements need are not\n+// serialized as you expect when 4 are packed into int32.\n \n-// Fp8E4M3B15 -> Fp16 (packed)\n // fast conversion code provided by Scott Gray @ OpenAI\n // $0 = (($2 << 1) & 0x80008000u) | (($2 << 7) & 0x3f803f80u);\n // $1 = (($2 << 0) & 0x80008000u) | (($2 << 0) & 0x3f803f80u);\n // WARN: subnormal (0bs0000xxx) are not handled\n-const std::string Fp8E4M3B15_to_Fp16 =\n+const std::string Fp8E4M3B15x4_to_Fp16 =\n     \"{                                      \\n\"\n     \".reg .b32 a<2>;                        \\n\"\n     \"shl.b32 a0, $2, 1;                     \\n\"\n@@ -114,7 +145,7 @@ const std::string Fp8E4M3B15_to_Fp16 =\n //       ((e4.y >> 0) & (0x80008000u >> 0)) |\n //       ((e4.y >> 0) & (0x3f803f80u >> 0)) ;\n // WARN: subnormal (0bs0000xxx) are not handled\n-const std::string Fp16_to_Fp8E4M3B15 =\n+const std::string Fp16_to_Fp8E4M3B15x4 =\n     \"{                                       \\n\"\n     \".reg .b32 a<2>;                         \\n\"\n     \"shr.b32  a0, $1, 1;                     \\n\""}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 29, "deletions": 25, "changes": 54, "file_content_changes": "@@ -1264,36 +1264,40 @@ def test_convert_float16_to_float32(in_dtype, device):\n \n \n def serialize_fp8(np_data, in_dtype):\n-    if in_dtype == tl.float8e4b15:\n-        # triton's f8e4b15 format is optimized for software emulation\n-        # as a result, each pack of 4xfp8 values:\n-        # s0b0s1b1s2b2s3b3 (for s, b sign and bits respectively)\n-        # is actually internally stored as\n-        # s0s2b0b2s1s3b1b3\n-        # we apply the conversion here\n-        f8x4 = np_data.view(np.uint32)\n-        s = [(f8x4 & (0x80000000 >> i)) << i for i in range(0, 32, 8)]\n-        b = [(f8x4 & (0x7f000000 >> i)) << i for i in range(0, 32, 8)]\n-        signs = (s[0] >> 0) | (s[1] >> 16) | (s[2] >> 1) | (s[3] >> 17)\n-        bits = (b[0] >> 1) | (b[1] >> 17) | (b[2] >> 8) | (b[3] >> 24)\n-        # tensor of triton fp8 data\n-        return (signs | bits).view(np.int8)\n-    else:\n-        return np_data\n+    return np_data\n+# def serialize_fp8(np_data, in_dtype):\n+#     if in_dtype == tl.float8e4b15:\n+#         # triton's f8e4b15 format is optimized for software emulation\n+#         # as a result, each pack of 4xfp8 values:\n+#         # s0b0s1b1s2b2s3b3 (for s, b sign and bits respectively)\n+#         # is actually internally stored as\n+#         # s0s2b0b2s1s3b1b3\n+#         # we apply the conversion here\n+#         f8x4 = np_data.view(np.uint32)\n+#         s = [(f8x4 & (0x80000000 >> i)) << i for i in range(0, 32, 8)]\n+#         b = [(f8x4 & (0x7f000000 >> i)) << i for i in range(0, 32, 8)]\n+#         signs = (s[0] >> 0) | (s[1] >> 16) | (s[2] >> 1) | (s[3] >> 17)\n+#         bits = (b[0] >> 1) | (b[1] >> 17) | (b[2] >> 8) | (b[3] >> 24)\n+#         # tensor of triton fp8 data\n+#         return (signs | bits).view(np.int8)\n+#     else:\n+#         return np_data\n \n # inverse of `serialize_fp8`\n \n \n def deserialize_fp8(np_data, in_dtype):\n-    if in_dtype == tl.float8e4b15:\n-        f8x4 = np_data.view(np.uint32)\n-        s = [(f8x4 & (0x80000000 >> i)) << i for i in [0, 16, 1, 17]]\n-        b = [(f8x4 & (0x7f000000 >> i)) << i for i in [1, 17, 8, 24]]\n-        signs = (s[0] >> 0) | (s[1] >> 8) | (s[2] >> 16) | (s[3] >> 24)\n-        bits = (b[0] >> 0) | (b[1] >> 8) | (b[2] >> 16) | (b[3] >> 24)\n-        return (signs | bits).view(np.int8)\n-    else:\n-        return np_data\n+    return np_data\n+# def deserialize_fp8(np_data, in_dtype):\n+#     if in_dtype == tl.float8e4b15:\n+#         f8x4 = np_data.view(np.uint32)\n+#         s = [(f8x4 & (0x80000000 >> i)) << i for i in [0, 16, 1, 17]]\n+#         b = [(f8x4 & (0x7f000000 >> i)) << i for i in [1, 17, 8, 24]]\n+#         signs = (s[0] >> 0) | (s[1] >> 8) | (s[2] >> 16) | (s[3] >> 24)\n+#         bits = (b[0] >> 0) | (b[1] >> 8) | (b[2] >> 16) | (b[3] >> 24)\n+#         return (signs | bits).view(np.int8)\n+#     else:\n+#         return np_data\n \n \n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4, tl.float8e5])"}]