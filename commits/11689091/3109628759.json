[{"filename": "python/src/triton.cc", "status": "modified", "additions": 77, "deletions": 137, "changes": 214, "file_content_changes": "@@ -430,150 +430,90 @@ void init_triton_runtime(py::module &&m) {\n /*****************************************************************************/\n typedef std::map<std::string, py::object> asm_map_t;\n \n-// --------------------------------------- \n-// Load provided assembly code into driver\n-// --------------------------------------- \n-\n-// CUDA\n-std::tuple<uint64_t, uint64_t, uint64_t, uint64_t> cu_load_binary(const std::string& name, asm_map_t &asm_map, size_t n_shared_bytes, uint64_t dev){\n-  // load assembly\n-  std::string assembly;\n-  if(asm_map.find(\"cubin\") != asm_map.end())\n-    assembly = py::cast<std::string>(asm_map[\"cubin\"]);\n-  else\n-    assembly = py::cast<std::string>(asm_map[\"ptx\"]);\n-  // create driver handles\n-  CUfunction fun;\n-  CUmodule mod;\n-  drv::dispatch::cuModuleLoadData(&mod, assembly.c_str());\n-  drv::dispatch::cuModuleGetFunction(&fun, mod, name.c_str());\n-  // get allocated registers and spilled registers from the function\n-  int n_regs = 0;\n-  int n_spills = 0;\n-  drv::dispatch::cuFuncGetAttribute(&n_regs, CU_FUNC_ATTRIBUTE_NUM_REGS, fun);\n-  drv::dispatch::cuFuncGetAttribute(&n_spills, CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES, fun);\n-  n_spills /= 4;\n-  // set dynamic shared memory if necessary\n-  int shared_optin;\n-  drv::dispatch::cuDeviceGetAttribute(&shared_optin, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN, dev);\n-  if(n_shared_bytes > 49152 && shared_optin > 49152){\n-    drv::dispatch::cuFuncSetCacheConfig(fun, CU_FUNC_CACHE_PREFER_SHARED);\n-    int shared_total, shared_static;\n-    drv::dispatch::cuDeviceGetAttribute(&shared_total, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR, dev);\n-    drv::dispatch::cuFuncGetAttribute(&shared_static, CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES, fun);\n-    drv::dispatch::cuFuncSetAttribute(fun, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES, shared_optin - shared_static);\n-  }\n-  return std::make_tuple((uint64_t)mod, (uint64_t)fun, (uint64_t)n_regs, (uint64_t)n_spills);\n-}\n-\n-// ROCM\n-std::tuple<uint64_t, uint64_t, uint64_t, uint64_t> hip_load_binary(const std::string& name, asm_map_t &asm_map, size_t n_shared_bytes, uint64_t dev){\n-  py::bytes _assembly = asm_map[\"hsaco\"];\n-  std::string assembly = py::cast<std::string>(_assembly);\n-  // HSA-CO -> hipModule\n-  hipModule_t mod = drv::amdgpu_to_hipmodule(assembly);\n-  // Handle to the kernel\n-  hipFunction_t fun;\n-  drv::dispatch::hipModuleGetFunction(&fun, mod, name.c_str());\n-  // record asm\n-  return std::make_tuple((uint64_t)mod, (uint64_t)fun, 0, 0);\n-}\n-\n // --------------------------------------- \n // Compile Triton-IR to assembly\n // --------------------------------------- \n \n-// CUDA\n-std::tuple<std::string, asm_map_t, int> cu_compile_ttir(\n-    const std::string &name, ir::module &ir, uint64_t device, int num_warps,\n-    int num_stages, asm_map_t &asm_map,\n-    const triton::codegen::ExternLibMap &extern_lib_map) {\n-  py::gil_scoped_release allow_threads;\n-  llvm::LLVMContext ctx;\n-  // device properties\n-  CUdevice dev = (CUdevice)device;\n-  size_t major = cuGetInfo<CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR>(dev);\n-  size_t minor = cuGetInfo<CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR>(dev);\n-  size_t cc = major*10 + minor;\n-  int version;\n-  std::string ptxas_path = drv::path_to_ptxas(version);\n-  // Triton-IR -> NVPTX LLVM-IR\n-  triton::codegen::nvidia_cu_target target(cc);\n-  int n_shared_bytes;\n-  auto llvm = triton::codegen::add_passes_to_emit_bin(\n-      ir, ctx, &target, num_warps, num_stages, n_shared_bytes, extern_lib_map);\n-  std::string tmp;\n-  llvm::raw_string_ostream llir(tmp);\n-  llir << *llvm;\n-  llir.flush();\n-  asm_map[\"llir\"] = py::cast(tmp);\n-  // LLVM-IR -> PTX\n-  std::string ptx = drv::llir_to_ptx(llvm.get(), cc, version);\n-  asm_map[\"ptx\"] = py::cast(ptx);\n-  // PTX -> Binary\n-  std::string cubin = drv::ptx_to_cubin(ptx, ptxas_path, cc);\n-  if(!cubin.empty()){\n-    py::bytes bytes(cubin);\n-    asm_map[\"cubin\"] = bytes;\n-  }\n-  return std::make_tuple(name, asm_map, n_shared_bytes);\n-}\n-\n-// HIP\n-std::tuple<std::string, asm_map_t, int> hip_compile_ttir(\n-    const std::string &name, ir::module &ir, uint64_t device, int num_warps,\n-    int num_stages, asm_map_t &asm_map,\n-    const triton::codegen::ExternLibMap &extern_lib_map) {\n-  llvm::LLVMContext ctx;\n-  // Triton-IR -> NVPTX LLVM-IR\n-  triton::codegen::amd_cl_target target;\n-  int n_shared_bytes;\n-  auto llvm = triton::codegen::add_passes_to_emit_bin(\n-      ir, ctx, &target, num_warps, num_stages, n_shared_bytes, extern_lib_map);\n-  std::string tmp;\n-  llvm::raw_string_ostream llir(tmp);\n-  llir << *llvm;\n-  llir.flush();\n-  asm_map[\"llir\"] = py::cast(tmp);\n-  // LLVM-IR -> HSA-CO\n-  std::string path = drv::llir_to_amdgpu(llvm.get(), \"gfx908\");\n-  asm_map[\"hsaco\"] = py::cast(path);\n-  return std::make_tuple(name, asm_map, n_shared_bytes);\n-}\n-\n void init_triton_codegen(py::module &&m) {\n-  m.def(\n-      \"compile_ttir\",\n-      [](backend_t backend, ir::module &ir, uint64_t device, int num_warps,\n-         int num_stages, py::dict& extern_libs) {\n-        std::string name = ir.get_function_list()[0]->get_name();\n-        // record asm as we generate\n-        asm_map_t asm_map;\n-        std::ostringstream ttir;\n-        ir.print(ttir);\n-        asm_map[\"ttir\"] = py::cast(ttir.str());\n-        llvm::LLVMContext ctx;\n-        // construct extern lib map\n-        triton::codegen::ExternLibMap extern_lib_map;\n-        for (auto item : extern_libs) {\n-          auto name = item.first.cast<std::string>();\n-          auto path = item.second.cast<std::string>();\n-          extern_lib_map.emplace(\n-              name, triton::codegen::create_extern_lib(name, path));\n-        }\n-        if(backend == CUDA)\n-          return cu_compile_ttir(name, ir, device, num_warps, num_stages, asm_map, extern_lib_map);\n-        assert(backend == ROCM);\n-        return hip_compile_ttir(name, ir, device, num_warps, num_stages, asm_map, extern_lib_map);\n+  m.def(\"compile_ttir\",\n+      [](backend_t backend, ir::module &ir, uint64_t device, int num_warps, int num_stages, py::dict& extern_libs) {\n+          py::gil_scoped_release allow_threads;\n+          std::string name = ir.get_function_list()[0]->get_name();\n+          // record asm as we generate\n+          asm_map_t asm_map;\n+          std::ostringstream ttir;\n+          ir.print(ttir);\n+          asm_map[\"ttir\"] = py::cast(ttir.str());\n+          llvm::LLVMContext ctx;\n+          // construct extern lib map\n+          triton::codegen::ExternLibMap extern_lib_map;\n+          for (auto item : extern_libs) {\n+            auto name = item.first.cast<std::string>();\n+            auto path = item.second.cast<std::string>();\n+            extern_lib_map.emplace(\n+                name, triton::codegen::create_extern_lib(name, path));\n+          }\n+          // device properties\n+          CUdevice dev = (CUdevice)device;\n+          size_t major = cuGetInfo<CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR>(dev);\n+          size_t minor = cuGetInfo<CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR>(dev);\n+          size_t cc = major*10 + minor;\n+          int version;\n+          std::string ptxas_path = drv::path_to_ptxas(version);\n+          // Triton-IR -> NVPTX LLVM-IR\n+          triton::codegen::nvidia_cu_target target(cc);\n+          int n_shared_bytes;\n+          auto llvm = triton::codegen::add_passes_to_emit_bin(\n+              ir, ctx, &target, num_warps, num_stages, n_shared_bytes, extern_lib_map);\n+          std::string tmp;\n+          llvm::raw_string_ostream llir(tmp);\n+          llir << *llvm;\n+          llir.flush();\n+          asm_map[\"llir\"] = py::cast(tmp);\n+          // LLVM-IR -> PTX\n+          std::string ptx = drv::llir_to_ptx(llvm.get(), cc, version);\n+          asm_map[\"ptx\"] = py::cast(ptx);\n+          // PTX -> Binary\n+          std::string cubin = drv::ptx_to_cubin(ptx, ptxas_path, cc);\n+          if(!cubin.empty()){\n+            py::bytes bytes(cubin);\n+            asm_map[\"cubin\"] = bytes;\n+          }\n+          return std::make_tuple(name, asm_map, n_shared_bytes);\n       },\n       py::return_value_policy::take_ownership);\n-  m.def(\"load_binary\", [](backend_t backend, const std::string& name, asm_map_t &asm_map, size_t n_shared_bytes, uint64_t dev){\n-\tpy::gil_scoped_release allow_threads;\n-        if(backend == CUDA)\n-          return cu_load_binary(name, asm_map, n_shared_bytes, dev);\n-        assert(backend == ROCM);\n-        return hip_load_binary(name, asm_map, n_shared_bytes, dev);\n-      }, py::return_value_policy::take_ownership);\n+  \n+\n+  // --------------------------------------- \n+  // Load provided assembly code into driver\n+  // --------------------------------------- \n+  m.def(\"load_binary\", [](const std::string& name, const std::string& data, size_t n_shared_bytes, uint64_t device){\n+\t      py::gil_scoped_release allow_threads;\n+        // create driver handles\n+        CUfunction fun;\n+        CUmodule mod;\n+        drv::dispatch::cuModuleLoadData(&mod, data.c_str());\n+        drv::dispatch::cuModuleGetFunction(&fun, mod, name.c_str());\n+        // get allocated registers and spilled registers from the function\n+        int n_regs = 0;\n+        int n_spills = 0;\n+        drv::dispatch::cuFuncGetAttribute(&n_regs, CU_FUNC_ATTRIBUTE_NUM_REGS, fun);\n+        drv::dispatch::cuFuncGetAttribute(&n_spills, CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES, fun);\n+        n_spills /= 4;\n+        // set dynamic shared memory if necessary\n+        int shared_optin;\n+        drv::dispatch::cuDeviceGetAttribute(&shared_optin, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN, device);\n+        if(n_shared_bytes > 49152 && shared_optin > 49152){\n+          drv::dispatch::cuFuncSetCacheConfig(fun, CU_FUNC_CACHE_PREFER_SHARED);\n+          int shared_total, shared_static;\n+          drv::dispatch::cuDeviceGetAttribute(&shared_total, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR, device);\n+          drv::dispatch::cuFuncGetAttribute(&shared_static, CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES, fun);\n+          drv::dispatch::cuFuncSetAttribute(fun, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES, shared_optin - shared_static);\n+        }\n+        return std::make_tuple((uint64_t)mod, (uint64_t)fun, (uint64_t)n_regs, (uint64_t)n_spills);\n+      }, \n+      py::return_value_policy::take_ownership\n+  );\n   \n \n   struct InstanceDescriptor"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 109, "deletions": 134, "changes": 243, "file_content_changes": "@@ -5,6 +5,7 @@\n import functools\n import hashlib\n import io\n+import json\n import os\n import shutil\n import subprocess\n@@ -926,23 +927,7 @@ def binary_name_to_header_name(name):\n     return f\"{name}.h\"\n \n \n-def generate_torch_glue(kernel_name, constants, signature, num_warps, binaries, tmpdir):\n-    headers = dict()\n-\n-    # write all cubins to header files\n-    assert len(binaries) == 1, \"AoT compilation not yet supported\"\n-\n-    for bin, shmem_size, name in binaries:\n-        assert len(name) < 1024\n-        initializer = f\"\"\"\n-const char* {name}_ptx = R\"({bin[\"ptx\"]})\";\n-unsigned char {name}_bin[] = {{ {','.join(map(hex, bin[\"cubin\"]))} }};\n-unsigned int {name}_shmem = {shmem_size};\"\"\"\n-        headers[name] = os.path.join(tmpdir, binary_name_to_header_name(name))\n-        with open(headers[name], \"w\") as f:\n-            f.write(initializer)\n-\n-    func_init = '\\n  '.join(f\"init_function(\\\"{name}\\\", {name}_bin, {name}_shmem, device);\" for _, _, name in binaries)\n+def generate_launcher(identifier, constants, signature):\n     arg_decls = ', '.join(f\"{ty_to_cpp(ty)} arg{i}\" for i, ty in signature.items())\n \n     def _extracted_type(ty):\n@@ -970,13 +955,10 @@ def format_of(ty):\n             \"int64_t\": \"L\",\n         }[ty]\n \n-    format = \"iiiK\" + ''.join([format_of(_extracted_type(ty)) for ty in signature.values()])\n+    format = \"iiiiiKK\" + ''.join([format_of(_extracted_type(ty)) for ty in signature.values()])\n \n     # generate glue code\n-    src = \"\"\n-    for bin, shmem_size, name in binaries:\n-        src += f\"#include \\\"{headers[name]}\\\"\\n\"\n-    src += f\"\"\"\n+    src = f\"\"\"\n #include \\\"cuda.h\\\"\n #include <Python.h>\n \n@@ -995,50 +977,16 @@ def format_of(ty):\n }}\n #define CUDA_CHECK(ans) {{ gpuAssert((ans), __FILE__, __LINE__); }}\n \n-static CUmodule module = 0;\n-static CUfunction function = 0;\n-\n-static inline void init_function(const char* name, const unsigned char* src, size_t n_shared_bytes, int64_t device){{\n-  CUmodule mod;\n-  CUfunction fun;\n-  CUDA_CHECK(cuModuleLoadData(&mod, src));\n-  CUDA_CHECK(cuModuleGetFunction(&fun, mod, name));\n-  // set dynamic shared memory if necessary\n-  int shared_optin;\n-  CUDA_CHECK(cuDeviceGetAttribute(&shared_optin, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN, device));\n-  if (n_shared_bytes > 49152 && shared_optin > 49152) {{\n-    CUDA_CHECK(cuFuncSetCacheConfig(fun, CU_FUNC_CACHE_PREFER_SHARED));\n-    int shared_total, shared_static;\n-    int n_spills, n_reg;\n-    CUDA_CHECK(cuDeviceGetAttribute(&shared_total, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR, device));\n-    CUDA_CHECK(cuFuncGetAttribute(&shared_static, CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES, fun));\n-    CUDA_CHECK(cuFuncGetAttribute(&n_spills, CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES, fun));\n-    CUDA_CHECK(cuFuncGetAttribute(&n_reg, CU_FUNC_ATTRIBUTE_NUM_REGS, fun));\n-    CUDA_CHECK(cuFuncSetAttribute(fun, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES, shared_optin - shared_static));\n-  }}\n-  module = mod;\n-  function = fun;\n-}}\n \n-static inline void init_module(CUdevice device) {{\n-  {func_init}\n-}}\n-\n-\n-void _{kernel_name}(int gridX, int gridY, int gridZ, CUstream stream, {arg_decls}) {{\n-  // TODO: machine may have heterogeneous devices\n-  if(function == 0){{\n-    CUdevice device;\n-    CUDA_CHECK(cuCtxGetDevice(&device));\n-    init_module(device);\n-  }}\n+void _launch(int gridX, int gridY, int gridZ, int num_warps, int shared_memory, CUstream stream, CUfunction function, {arg_decls}) {{\n   void *params[] = {{ {', '.join(f\"&arg{i}\" for i in signature.keys() if i not in constants)} }};\n   if(gridX*gridY*gridZ > 0){{\n-    CUDA_CHECK(cuLaunchKernel(function, gridX, gridY, gridZ, 32*{num_warps}, 1, 1, {name}_shmem, stream, params, 0));\n+    CUDA_CHECK(cuLaunchKernel(function, gridX, gridY, gridZ, 32*num_warps, 1, 1, shared_memory, stream, params, 0));\n   }}\n }}\n \n-CUdeviceptr getPointer(PyObject *obj, int idx) {{\n+\n+static inline CUdeviceptr getPointer(PyObject *obj, int idx) {{\n   if (PyLong_Check(obj)) {{\n     return (CUdeviceptr)PyLong_AsUnsignedLongLong(obj);\n   }}\n@@ -1061,15 +1009,18 @@ def format_of(ty):\n }}\n \n \n-static PyObject* {kernel_name}(PyObject* self, PyObject* args) {{\n+static PyObject* launch(PyObject* self, PyObject* args) {{\n   int gridX, gridY, gridZ;\n-  uint64_t stream;\n+  uint64_t _stream;\n+  uint64_t _function;\n+  int num_warps;\n+  int shared_memory;\n   {' '.join([f\"{_extracted_type(ty)} _arg{i}; \" for i, ty in signature.items()])}\n-  if(!PyArg_ParseTuple(args, \\\"{format}\\\", &gridX, &gridY, &gridZ, &stream, {', '.join(f\"&_arg{i}\" for i, ty in signature.items())})) {{\n+  if(!PyArg_ParseTuple(args, \\\"{format}\\\", &gridX, &gridY, &gridZ, &num_warps, &shared_memory, &_stream, &_function, {', '.join(f\"&_arg{i}\" for i, ty in signature.items())})) {{\n     return NULL;\n   }}\n \n-  _{kernel_name}(gridX, gridY, gridZ, (CUstream)stream, {', '.join(f\"getPointer(_arg{i},{i})\" if ty[0]==\"*\" else f\"_arg{i}\"for i, ty in signature.items())});\n+  _launch(gridX, gridY, gridZ, num_warps, shared_memory, (CUstream)_stream, (CUfunction)_function, {', '.join(f\"getPointer(_arg{i},{i})\" if ty[0]==\"*\" else f\"_arg{i}\"for i, ty in signature.items())});\n \n \n   if(PyErr_Occurred()) {{\n@@ -1081,38 +1032,26 @@ def format_of(ty):\n }}\n \n static PyMethodDef ModuleMethods[] = {{\n-  {{\"{kernel_name}\", {kernel_name}, METH_VARARGS, \"Call {kernel_name} kernel\"}},\n+  {{\"launch\", launch, METH_VARARGS, \"Entry point for all kernels with this signature\"}},\n   {{NULL, NULL, 0, NULL}} // sentinel\n }};\n \n static struct PyModuleDef ModuleDef = {{\n   PyModuleDef_HEAD_INIT,\n-  \\\"{kernel_name}\\\",\n+  \\\"launcher\\\",\n   NULL, //documentation\n   -1, //size\n   ModuleMethods\n }};\n \n-PyMODINIT_FUNC PyInit_{kernel_name}(void) {{\n+PyMODINIT_FUNC PyInit_launcher(void) {{\n   PyObject *m = PyModule_Create(&ModuleDef);\n   if(m == NULL) {{\n     return NULL;\n   }}\n   PyModule_AddFunctions(m, ModuleMethods);\n-  PyObject *ptx = PyDict_New();\n-\"\"\"\n-\n-    for _, _, name in binaries:\n-        src += f\"\"\"\n-  PyObject *py_{name}_ptx = PyUnicode_FromString({name}_ptx);\n-  PyDict_SetItemString(ptx, \"{name}\", py_{name}_ptx);\n-  Py_DECREF(py_{name}_ptx);\n-\"\"\"\n-\n-    src += \"\"\"\n-  PyModule_AddObject(m, \"ptx\", ptx);\n   return m;\n-}\n+}}\n \"\"\"\n \n     return src\n@@ -1126,35 +1065,34 @@ class CacheManager:\n \n     def __init__(self, key):\n         self.key = key\n-        self.bin_path = None\n         self.lock_path = None\n-        # if caching is enabled, get the lock and bin path\n+        # create cache directory if it doesn't exist\n         self.cache_dir = os.environ.get('TRITON_CACHE_DIR', default_cache_dir())\n         if self.cache_dir:\n+            self.cache_dir = os.path.join(self.cache_dir, self.key)\n+            self.lock_path = os.path.join(self.cache_dir, \"lock\")\n             os.makedirs(self.cache_dir, exist_ok=True)\n-        if self.cache_dir:\n-            self.bin_path = os.path.join(self.cache_dir, self.key + \".so\")\n-            self.lock_path = self.bin_path + \".lock\"\n \n-    def has_file(self):\n-        return self.bin_path and os.path.exists(self.bin_path)\n+    def _make_path(self, filename):\n+        return os.path.join(self.cache_dir, filename)\n \n-    def put(self, binary):\n-        if self.bin_path:\n-            assert self.lock_path is not None\n-            with FileLock(self.lock_path):\n-                with open(self.bin_path + \".tmp\", \"wb\") as f:\n-                    f.write(binary)\n-                os.rename(self.bin_path + \".tmp\", self.bin_path)\n+    def has_file(self, filename):\n+        if not self.cache_dir:\n+            return False\n+        return os.path.exists(self._make_path(filename))\n \n+    def put(self, data, filename, binary=True):\n+        if not self.cache_dir:\n+            return\n+        assert self.lock_path is not None\n+        filepath = self._make_path(filename)\n+        with FileLock(self.lock_path):\n+            # use tempfile to be robust against program interruptions\n+            mode = \"wb\" if binary else \"w\"\n+            with open(filepath + \".tmp\", mode) as f:\n+                f.write(data)\n+            os.rename(filepath + \".tmp\", filepath)\n \n-def make_cache_key(fn, signature, configs, constants, num_warps, num_stages):\n-    # Get unique key for the compiled code\n-    get_conf_key = lambda conf: (sorted(conf.divisible_by_16), sorted(conf.equal_to_1))\n-    configs_key = [get_conf_key(conf) for conf in configs]\n-    key = f\"{fn.cache_key}-{''.join(signature.values())}-{configs_key}-{constants}-{num_warps}-{num_stages}\"\n-    key = hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n-    return key\n \n # utilties for generating and compiling C wrappers\n \n@@ -1224,54 +1162,91 @@ def _build(name, src, srcdir):\n     return so\n \n \n+def make_so_cache_key(signature, constants):\n+    # Get unique key for the compiled code\n+    signature = {k: 'ptr' if v[0] == '*' else v for k, v in signature.items()}\n+    key = f\"{''.join(signature.values())}{constants}\"\n+    key = hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n+    return key\n+\n+\n+def make_fn_cache_key(fn_hash, signature, configs, constants, num_warps, num_stages):\n+    # Get unique key for the compiled code\n+    get_conf_key = lambda conf: (sorted(conf.divisible_by_16), sorted(conf.equal_to_1))\n+    configs_key = [get_conf_key(conf) for conf in configs]\n+    key = f\"{fn_hash}-{''.join(signature.values())}-{configs_key}-{constants}-{num_warps}-{num_stages}\"\n+    key = hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n+    return key\n+\n+\n def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: int = 4, num_stages: int = 3, extern_libs=None, configs=None):\n     # we get the kernel, i.e. the first function generated in the module\n-    if configs is None:\n-        assert False, \"automatic specialization is not supported yet\"\n-        ref, _ = make_triton_ir(fn, signature, _triton.code_gen.instance_descriptor(), constants)\n-        fns = ref.get_functions()\n-        configs = _triton.infer_specialization_configs(fns[0])\n     assert len(configs) == 1\n     # cache manager\n-    cache_key = make_cache_key(fn, signature, configs, constants, num_warps, num_stages)\n-    cache_manager = CacheManager(cache_key)\n+    name = fn.__name__\n+    # name of files that are cached\n+    so_cache_key = make_so_cache_key(signature, constants)\n+    so_cache_manager = CacheManager(so_cache_key)\n+    so_name = f\"{name}.so\"\n+    # retrieve stub from cache if it exists\n+    if not so_cache_manager.has_file(so_name):\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            src = generate_launcher(name, constants, signature)\n+            src_path = os.path.join(tmpdir, \"main.c\")\n+            with open(src_path, \"w\") as f:\n+                f.write(src)\n+            so = _build(fn.__name__, src_path, tmpdir)\n+            with open(so, \"rb\") as f:\n+                so_cache_manager.put(f.read(), so_name, binary=True)\n+\n     # retrieve cached shared object if it exists\n-    if cache_manager.has_file():\n-        return CompiledKernel(fn.__name__, cache_manager.bin_path)\n-    # compile all the configs\n-    binaries = []\n-    for config in configs:\n-        binaries.append(_compile(fn, signature, device, constants, config, num_warps, num_stages, extern_libs, \"cubin\"))\n-    # generate and compile glue code into shared object\n-    with tempfile.TemporaryDirectory() as tmpdir:\n-        all_constants = set(constants.keys())\n-        all_constants.update(configs[0].equal_to_1)\n-        src = generate_torch_glue(fn.__name__, constants, signature, num_warps, binaries, tmpdir)\n-        src_path = os.path.join(tmpdir, \"main.c\")\n-        with open(src_path, \"w\") as f:\n-            f.write(src)\n-        so = _build(fn.__name__, src_path, tmpdir)\n-        with open(so, \"rb\") as f:\n-            cache_manager.put(f.read())\n-\n-    return CompiledKernel(fn.__name__, cache_manager.bin_path)\n+    fn_cache_key = make_fn_cache_key(fn.cache_key, signature, configs, constants, num_warps, num_stages)\n+    fn_cache_manager = CacheManager(fn_cache_key)\n+    ptx_name = f\"{name}.ptx\"\n+    cubin_name = f\"{name}.cubin\"\n+    data_name = f\"{name}.json\"\n+    if not fn_cache_manager.has_file(cubin_name) or \\\n+       not fn_cache_manager.has_file(data_name) or \\\n+       not fn_cache_manager.has_file(ptx_name):\n+        asm, shared, kernel_name = _compile(fn, signature, device, constants, configs[0], num_warps, num_stages, extern_libs, \"cubin\")\n+        metadata = {\"name\": kernel_name, \"shared\": shared, \"num_warps\": num_warps, \"num_stages\": num_stages}\n+        fn_cache_manager.put(asm[\"cubin\"], cubin_name)\n+        fn_cache_manager.put(asm[\"ptx\"], ptx_name, binary=False)\n+        fn_cache_manager.put(json.dumps(metadata), data_name, binary=False)\n+\n+    return CompiledKernel(name, so_cache_manager._make_path(so_name), fn_cache_manager.cache_dir)\n \n \n class CompiledKernel:\n \n-    def __init__(self, fn_name, data_path):\n+    def __init__(self, fn_name, so_path, cache_dir):\n+        # initialize launcher\n         import importlib.util\n-        spec = importlib.util.spec_from_file_location(fn_name, data_path)\n+        spec = importlib.util.spec_from_file_location(\"launcher\", so_path)\n         mod = importlib.util.module_from_spec(spec)\n         spec.loader.exec_module(mod)\n-        self.c_wrapper = getattr(mod, fn_name)\n-        ptx = getattr(mod, \"ptx\")\n-        if len(ptx) == 1:\n-            self.asm = {\"ptx\": list(ptx.values())[0]}\n+        self.c_wrapper = getattr(mod, \"launch\")\n+        # initialize metadata\n+        with open(os.path.join(cache_dir, f\"{fn_name}.json\")) as f:\n+            metadata = json.load(f)\n+        self.shared = metadata[\"shared\"]\n+        self.num_warps = metadata[\"num_warps\"]\n+        self.num_stages = metadata[\"num_stages\"]\n+        # initialize asm dict\n+        self.asm = dict()\n+        with open(os.path.join(cache_dir, f\"{fn_name}.cubin\"), \"rb\") as f:\n+            self.asm[\"cubin\"] = f.read()\n+        with open(os.path.join(cache_dir, f\"{fn_name}.ptx\"), \"r\") as f:\n+            self.asm[\"ptx\"] = f.read()\n+\n+        device = torch.cuda.current_device()\n+        mod, func, n_regs, n_spills = _triton.code_gen.load_binary(metadata[\"name\"], self.asm[\"cubin\"], self.shared, device)\n+        self.cu_module = mod\n+        self.cu_function = func\n \n     def __getitem__(self, grid):\n         def runner(*args, stream=None):\n             if stream is None:\n                 stream = torch.cuda.current_stream().cuda_stream\n-            self.c_wrapper(grid[0], grid[1], grid[2], stream, *args)\n+            self.c_wrapper(grid[0], grid[1], grid[2], self.num_warps, self.shared, stream, self.cu_function, *args)\n         return runner"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -253,7 +253,7 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n     try:\n       bin = cache[key]\n       if not warmup:\n-          bin.c_wrapper(grid_0, grid_1, grid_2, stream, {args})\n+          bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.shared, stream, bin.cu_function, {args})\n       return bin\n     # kernel not cached -- compile\n     except KeyError:\n@@ -274,7 +274,7 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n       if not self._call_hook(key, signature, device, constants, num_warps, num_stages, extern_libs, configs):\n         bin = triton.compile(self, signature, device, constants, num_warps, num_stages, extern_libs=extern_libs, configs=configs)\n         if not warmup:\n-            bin.c_wrapper(grid_0, grid_1, grid_2, stream, *args)\n+            bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.shared, stream, bin.cu_function, *args)\n         self.cache[key] = bin\n         return bin\n       return None"}]