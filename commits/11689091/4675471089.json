[{"filename": ".github/workflows/documentation.yml", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -8,7 +8,8 @@ jobs:\n \n   Build-Documentation:\n \n-    runs-on: [self-hosted, V100]\n+    # TODO: It's temporarily built on A100 only because tutorials fail on V100\n+    runs-on: [self-hosted, A100]\n \n     steps:\n       - name: Checkout branch"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.h", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -25,8 +25,6 @@ std::unique_ptr<Pass> createTritonGPUVerifier();\n \n std::unique_ptr<Pass> createTritonGPUOptimizeDotOperandsPass();\n \n-std::unique_ptr<Pass> createTritonGPUUpdateMmaForVoltaPass();\n-\n /// Generate the code for registering passes.\n #define GEN_PASS_REGISTRATION\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\""}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 10, "deletions": 3, "changes": 13, "file_content_changes": "@@ -157,7 +157,14 @@\n \n @triton.autotune(\n     configs=[\n-        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n+        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n     ],\n     key=['M', 'N', 'K'],\n )\n@@ -312,15 +319,15 @@ def matmul(a, b, activation=None):\n     triton.testing.Benchmark(\n         x_names=['M', 'N', 'K'],  # argument names to use as an x-axis for the plot\n         x_vals=[\n-            8192\n+            128 * i for i in range(2, 33)\n         ],  # different possible values for `x_name`\n         line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n         # possible values for `line_arg``\n         line_vals=['cublas', 'triton'],\n         # label name for the lines\n         line_names=[\"cuBLAS\", \"Triton\"],\n         # line styles\n-        styles=[('green', '-'), ('green', '--'), ('blue', '-'), ('blue', '--')],\n+        styles=[('green', '-'), ('blue', '-')],\n         ylabel=\"TFLOPS\",  # label name for the y-axis\n         plot_name=\"matmul-performance\",  # name for the plot. Used also as a file name for saving the plot.\n         args={},"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -33,7 +33,7 @@ def _fwd_kernel(\n     offs_n = tl.arange(0, BLOCK_N)\n     offs_d = tl.arange(0, BLOCK_DMODEL)\n     off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :] * stride_kk\n+    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n     off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n     # Initialize pointers to Q, K, V\n     q_ptrs = Q + off_q\n@@ -50,7 +50,7 @@ def _fwd_kernel(\n         # -- compute qk ----\n         k = tl.load(k_ptrs)\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-        qk += tl.dot(q, tl.trans(k))\n+        qk += tl.dot(q, k)\n         qk *= sm_scale\n         qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n         # compute new m\n@@ -62,10 +62,10 @@ def _fwd_kernel(\n         l_curr = tl.sum(p, 1) + l_prev\n         # rescale operands of matmuls\n         l_rcp = 1. / l_curr\n-        p *= l_rcp\n+        p *= l_rcp[:, None]\n         acc *= (l_prev * l_rcp)[:, None]\n         # update acc\n-        p = p.to(tl.float16)\n+        p = p.to(Q.dtype.element_ty)\n         v = tl.load(v_ptrs)\n         acc += tl.dot(p, v)\n         # update m_i and l_i\n@@ -169,18 +169,18 @@ def _bwd_kernel(\n             p = tl.exp(qk * sm_scale - m[:, None])\n             # compute dv\n             do = tl.load(do_ptrs)\n-            dv += tl.dot(tl.trans(p.to(tl.float16)), do)\n+            dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\n             # compute dp = dot(v, do)\n             Di = tl.load(D_ptrs + offs_m_curr)\n             dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n             dp += tl.dot(do, tl.trans(v))\n             # compute ds = p * (dp - delta[:, None])\n             ds = p * dp * sm_scale\n             # compute dk = dot(ds.T, q)\n-            dk += tl.dot(tl.trans(ds.to(tl.float16)), q)\n+            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)\n             # compute dq\n             dq = tl.load(dq_ptrs)\n-            dq += tl.dot(ds.to(tl.float16), k)\n+            dq += tl.dot(ds.to(Q.dtype.element_ty), k)\n             tl.store(dq_ptrs, dq)\n             # increment pointers\n             dq_ptrs += BLOCK_M * stride_qm"}]