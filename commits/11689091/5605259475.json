[{"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -439,12 +439,15 @@ struct ConvertLayoutOpConversion\n     }\n     // Potentially we need to store for multiple CTAs in this replication\n     auto accumNumReplicates = product<unsigned>(numReplicates);\n-    // unsigned elems = getTotalElemsPerThread(srcTy);\n     auto vals = getTypeConverter()->unpackLLElements(loc, adaptor.getSrc(),\n                                                      rewriter, srcTy);\n     unsigned inVec = 0;\n     unsigned outVec = 0;\n     auto paddedRepShape = getScratchConfigForCvtLayout(op, inVec, outVec);\n+    if (getElementTypeOrSelf(op.getType()).isa<mlir::Float8E4M3B11FNUZType>()) {\n+      assert(inVec % 4 == 0 && \"conversion not supported for FP8E4M3B15\");\n+      assert(outVec % 4 == 0 && \"conversion not supported for FP8E4M3B15\");\n+    }\n \n     unsigned outElems = getTotalElemsPerThread(dstTy);\n     auto outOrd = getOrder(dstLayout);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 28, "deletions": 0, "changes": 28, "file_content_changes": "@@ -313,6 +313,7 @@ class ConvertTritonGPUToLLVM\n     int threadsPerWarp = triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n \n     // Preprocess\n+    decomposeFp8e4b15Convert(mod);\n     decomposeMmaToDotOperand(mod, numWarps, threadsPerWarp);\n     decomposeBlockedToDotOperand(mod);\n     decomposeInsertSliceAsyncOp(mod);\n@@ -442,6 +443,33 @@ class ConvertTritonGPUToLLVM\n                                         allocation.getSharedMemorySize()));\n   }\n \n+  void decomposeFp8e4b15Convert(ModuleOp mod) const {\n+    mod.walk([&](triton::gpu::ConvertLayoutOp cvtOp) -> void {\n+      OpBuilder builder(cvtOp);\n+      if (!getElementTypeOrSelf(cvtOp).isa<mlir::Float8E4M3B11FNUZType>())\n+        return;\n+      auto shape = cvtOp.getType().cast<RankedTensorType>().getShape();\n+      auto argEncoding =\n+          cvtOp.getOperand().getType().cast<RankedTensorType>().getEncoding();\n+      auto cvtEncoding = cvtOp.getType().cast<RankedTensorType>().getEncoding();\n+      if (argEncoding.isa<triton::gpu::DotOperandEncodingAttr>() ||\n+          cvtEncoding.isa<triton::gpu::DotOperandEncodingAttr>())\n+        return;\n+      auto F16Ty = builder.getF16Type();\n+\n+      auto newArgType = RankedTensorType::get(shape, F16Ty, argEncoding);\n+      auto newCvtType = RankedTensorType::get(shape, F16Ty, cvtEncoding);\n+      auto newArg = builder.create<mlir::triton::FpToFpOp>(\n+          cvtOp.getLoc(), newArgType, cvtOp.getOperand());\n+      auto newCvt = builder.create<mlir::triton::gpu::ConvertLayoutOp>(\n+          cvtOp.getLoc(), newCvtType, newArg);\n+      auto newRet = builder.create<mlir::triton::FpToFpOp>(\n+          cvtOp.getLoc(), cvtOp.getType(), newCvt.getResult());\n+      cvtOp.replaceAllUsesWith(newRet.getResult());\n+      cvtOp.erase();\n+    });\n+  }\n+\n   void decomposeMmaToDotOperand(ModuleOp mod, int numWarps,\n                                 int threadsPerWarp) const {\n     // Replace `mma -> dot_op` with `mma -> blocked -> dot_op`"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 27, "deletions": 2, "changes": 29, "file_content_changes": "@@ -44,6 +44,9 @@ def numpy_random(shape, dtype_str, rs: Optional[RandomState] = None, low=None, h\n         x = rs.randint(low, high, shape, dtype=dtype)\n         x[x == 0] = 1  # Hack. Never return zero so tests of division don't error out.\n         return x\n+    elif dtype_str and 'float8' in dtype_str:\n+        x = rs.randint(20, 40, shape, dtype=np.int8)\n+        return x\n     elif dtype_str in float_dtypes:\n         return rs.normal(0, 1, shape).astype(dtype_str)\n     elif dtype_str == 'bfloat16':\n@@ -67,6 +70,8 @@ def to_triton(x: np.ndarray, device='cuda', dst_type=None) -> Union[TensorWrappe\n         x_signed = x.astype(getattr(np, signed_type_name))\n         return reinterpret(torch.tensor(x_signed, device=device), getattr(tl, t))\n     else:\n+        if dst_type and 'float8' in dst_type:\n+            return reinterpret(torch.tensor(x, device=device), getattr(tl, dst_type))\n         if t == 'float32' and dst_type == 'bfloat16':\n             return torch.tensor(x, device=device).bfloat16()\n         return torch.tensor(x, device=device)\n@@ -1276,6 +1281,20 @@ def serialize_fp8(np_data, in_dtype):\n     else:\n         return np_data\n \n+# inverse of `serialize_fp8`\n+\n+\n+def deserialize_fp8(np_data, in_dtype):\n+    if in_dtype == tl.float8e4b15:\n+        f8x4 = np_data.view(np.uint32)\n+        s = [(f8x4 & (0x80000000 >> i)) << i for i in [0, 16, 1, 17]]\n+        b = [(f8x4 & (0x7f000000 >> i)) << i for i in [1, 17, 8, 24]]\n+        signs = (s[0] >> 0) | (s[1] >> 8) | (s[2] >> 16) | (s[3] >> 24)\n+        bits = (b[0] >> 0) | (b[1] >> 8) | (b[2] >> 16) | (b[3] >> 24)\n+        return (signs | bits).view(np.int8)\n+    else:\n+        return np_data\n+\n \n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4, tl.float8e5])\n @pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16, torch.float32])\n@@ -1901,7 +1920,7 @@ def var_mean_kernel(X, out_mean, out_var, BLOCK: tl.constexpr):\n @pytest.mark.parametrize(\"dtype_str, shape, perm\",\n                          [(dtype, shape, perm)\n                           # TODO: bfloat16\n-                          for dtype in ['float16', 'float32']\n+                          for dtype in ['float8e4b15', 'float16', 'float32']\n                              for shape in [(64, 64), (128, 128)]\n                              for perm in [(1, 0)]])\n def test_permute(dtype_str, shape, perm, device):\n@@ -1930,7 +1949,13 @@ def kernel(X, stride_xm, stride_xn,\n                                     z_tri_contiguous, z_tri_contiguous.stride(0), z_tri_contiguous.stride(1),\n                                     BLOCK_M=shape[0], BLOCK_N=shape[1])\n     # numpy result\n-    z_ref = x.transpose(*perm)\n+    if dtype_str == 'float8e4b15':\n+        ty = tl.float8e4b15\n+        z_ref = serialize_fp8(deserialize_fp8(x, ty).T.copy(), ty)\n+        z_tri = z_tri.base\n+        z_tri_contiguous = z_tri_contiguous.base\n+    else:\n+        z_ref = x.transpose(*perm)\n     # compare\n     np.testing.assert_allclose(to_numpy(z_tri), z_ref)\n     np.testing.assert_allclose(to_numpy(z_tri_contiguous), z_ref)"}]