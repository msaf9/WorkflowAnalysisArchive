[{"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 15, "deletions": 6, "changes": 21, "file_content_changes": "@@ -420,22 +420,31 @@ def TT_ReduceReturnOp: TT_Op<\"reduce.return\",\n //\n // External Elementwise op\n //\n-def TT_ExtElemwiseOp : TT_Op<\"ext_elemwise\", [Pure, Elementwise, SameOperandsAndResultShape,\n-                                              SameOperandsAndResultEncoding,\n-                                              SameVariadicOperandSize]> {\n-    let summary = \"ext_elemwise\";\n+class TT_ExternElementwiseOpBase<string mnemonic, list<Trait> traits = []> :\n+    TT_Op<mnemonic,\n+         traits # [SameOperandsAndResultEncoding,\n+                   SameVariadicOperandSize]> {\n \n     let description = [{\n         call an external function $symbol implemented in $libpath/$libname with $args\n-\n         return $libpath/$libname:$symbol($args...)\n     }];\n \n     let arguments = (ins Variadic<TT_Type>:$args, StrAttr:$libname, StrAttr:$libpath, StrAttr:$symbol);\n \n     let results = (outs TT_Type:$result);\n \n-    let assemblyFormat = \"operands attr-dict `:` type(operands) `->` type($result)\";\n+    let assemblyFormat = \"operands attr-dict `:` functional-type(operands, $result)\";\n+}\n+\n+\n+def TT_PureExternElementwiseOp : TT_ExternElementwiseOpBase<\"pure_extern_elementwise\", [Pure, Elementwise]> {\n+    let summary = \"FFI for pure element-wise extern LLVM bitcode functions\";\n+}\n+\n+def TT_ImpureExternElementwiseOp : TT_ExternElementwiseOpBase<\"impure_extern_elementwise\", [MemoryEffects<[MemRead]>,\n+                                                                                            MemoryEffects<[MemWrite]>]> {\n+    let summary = \"FFI for impure element-wise extern LLVM bitcode functions\";\n }\n \n //"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 15, "deletions": 11, "changes": 26, "file_content_changes": "@@ -734,20 +734,20 @@ struct CmpFOpConversion\n   }\n };\n \n-struct ExtElemwiseOpConversion\n-    : public ElementwiseOpConversionBase<triton::ExtElemwiseOp,\n-                                         ExtElemwiseOpConversion> {\n-  using Base = ElementwiseOpConversionBase<triton::ExtElemwiseOp,\n-                                           ExtElemwiseOpConversion>;\n+template <class T>\n+struct ExternElementwiseOpConversion\n+    : public ElementwiseOpConversionBase<T, ExternElementwiseOpConversion<T>> {\n+  using Base = ElementwiseOpConversionBase<T, ExternElementwiseOpConversion<T>>;\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n+  typedef typename Base::OpAdaptor OpAdaptor;\n \n-  Value createDestOp(triton::ExtElemwiseOp op, OpAdaptor adaptor,\n+  Value createDestOp(T op, OpAdaptor adaptor,\n                      ConversionPatternRewriter &rewriter, Type elemTy,\n                      ValueRange operands, Location loc) const {\n     StringRef funcName = op.getSymbol();\n     if (funcName.empty())\n-      llvm::errs() << \"ExtElemwiseOpConversion\";\n+      llvm::errs() << \"ExternElementwiseOpConversion\";\n \n     Type funcType = getFunctionType(elemTy, operands);\n     LLVM::LLVMFuncOp funcOp =\n@@ -761,8 +761,7 @@ struct ExtElemwiseOpConversion\n     return LLVM::LLVMFunctionType::get(resultType, operandTypes);\n   }\n \n-  LLVM::LLVMFuncOp appendOrGetFuncOp(ConversionPatternRewriter &rewriter,\n-                                     triton::ExtElemwiseOp op,\n+  LLVM::LLVMFuncOp appendOrGetFuncOp(ConversionPatternRewriter &rewriter, T op,\n                                      StringRef funcName, Type funcType) const {\n     using LLVM::LLVMFuncOp;\n \n@@ -771,7 +770,8 @@ struct ExtElemwiseOpConversion\n     if (funcOp)\n       return cast<LLVMFuncOp>(*funcOp);\n \n-    mlir::OpBuilder b(op->getParentOfType<LLVMFuncOp>());\n+    auto parent = ((Operation *)op)->getParentOfType<mlir::LLVM::LLVMFuncOp>();\n+    mlir::OpBuilder b(parent);\n     auto ret = b.create<LLVMFuncOp>(op->getLoc(), funcName, funcType);\n     ret.getOperation()->setAttr(\n         \"libname\", StringAttr::get(op->getContext(), op.getLibname()));\n@@ -1117,7 +1117,11 @@ void populateElementwiseOpToLLVMPatterns(\n \n   patterns.add<FpToFpOpConversion>(typeConverter, benefit);\n \n-  patterns.add<ExtElemwiseOpConversion>(typeConverter, benefit);\n+  patterns.add<ExternElementwiseOpConversion<triton::PureExternElementwiseOp>>(\n+      typeConverter, benefit);\n+  patterns\n+      .add<ExternElementwiseOpConversion<triton::ImpureExternElementwiseOp>>(\n+          typeConverter, benefit);\n   // ExpOpConversionApprox will try using ex2.approx if the input type is\n   // FP32. For other input types, ExpOpConversionApprox will return failure and\n   // ElementwiseOpConversion<math::ExpOp, math::ExpOp> defined below will call"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 10, "deletions": 6, "changes": 16, "file_content_changes": "@@ -411,14 +411,16 @@ struct TritonAtomicRMWPattern\n   }\n };\n \n-struct TritonExtElemwisePattern\n-    : public OpConversionPattern<triton::ExtElemwiseOp> {\n-  using OpConversionPattern<triton::ExtElemwiseOp>::OpConversionPattern;\n+template <class T>\n+struct TritonExternElementwisePattern : public OpConversionPattern<T> {\n+  using OpConversionPattern<T>::OpConversionPattern;\n+  using OpConversionPattern<T>::typeConverter;\n+  typedef typename OpConversionPattern<T>::OpAdaptor OpAdaptor;\n \n   LogicalResult\n-  matchAndRewrite(triton::ExtElemwiseOp op, OpAdaptor adaptor,\n+  matchAndRewrite(T op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::ExtElemwiseOp>(\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<T>(\n                       op, typeConverter->convertType(op.getType()),\n                       adaptor.getArgs(), adaptor.getLibname(),\n                       adaptor.getLibpath(), adaptor.getSymbol()),\n@@ -539,7 +541,9 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n           TritonGenericPattern<triton::AddPtrOp>, TritonCatPattern,\n           TritonReducePattern, TritonReduceReturnPattern, TritonTransPattern,\n           TritonExpandDimsPattern, TritonMakeRangePattern, TritonDotPattern,\n-          TritonLoadPattern, TritonStorePattern, TritonExtElemwisePattern,\n+          TritonLoadPattern, TritonStorePattern,\n+          TritonExternElementwisePattern<triton::PureExternElementwiseOp>,\n+          TritonExternElementwisePattern<triton::ImpureExternElementwiseOp>,\n           TritonPrintPattern, TritonAssertPattern, TritonAtomicRMWPattern>(\n           typeConverter, context);\n }"}, {"filename": "lib/Dialect/Triton/IR/Traits.cpp", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -40,6 +40,9 @@ OpTrait::impl::verifySameOperandsEncoding(Operation *op,\n \n LogicalResult OpTrait::impl::verifySameOperandsAndResultEncoding(\n     Operation *op, bool allowTensorPointerType) {\n+  if (op->getNumOperands() == 0)\n+    return success();\n+\n   if (failed(verifyAtLeastNOperands(op, 1)) ||\n       failed(verifyAtLeastNResults(op, 1)))\n     return failure();"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -234,9 +234,10 @@ static bool linkExternLib(llvm::Module &module, llvm::StringRef name,\n   if (!isROCM) {\n     if (name == \"libdevice\") {\n       linkLibdevice(module);\n-    } else {\n-      assert(false && \"unknown extern lib: \");\n     }\n+    // else {\n+    //   assert(false && \"unknown extern lib: \");\n+    // }\n   }\n \n   return false;"}, {"filename": "python/MANIFEST.in", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1,3 +1,4 @@\n graft src\n graft triton/third_party\n graft triton/runtime/backends/\n+graft triton/language/extra"}, {"filename": "python/setup.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -224,6 +224,7 @@ def build_extension(self, ext):\n         \"triton/common\",\n         \"triton/compiler\",\n         \"triton/language\",\n+        \"triton/language/extra\",\n         \"triton/ops\",\n         \"triton/ops/blocksparse\",\n         \"triton/runtime\","}, {"filename": "python/src/extra/cuda.ll", "status": "added", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -0,0 +1,12 @@\n+; ~/.triton/llvm/llvm+mlir-17.0.0-x86_64-linux-gnu-ubuntu-18.04-release/bin/llvm-as ./src/extra/cuda.ll -o ./triton/language/extra/cuda.bc\n+\n+target datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128\"\n+target triple = \"nvptx64-nvidia-cuda\"\n+\n+\n+define i64 @globaltimer() #0 {\n+  %1 = call i64 asm sideeffect \"mov.u64 $0, %globaltimer;\", \"=l\"() nounwind\n+  ret i64 %1\n+}\n+\n+attributes #0 = { alwaysinline nounwind }"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 9, "deletions": 5, "changes": 14, "file_content_changes": "@@ -1267,14 +1267,18 @@ void init_triton_ir(py::module &&m) {\n                                                            ptr, val, mask);\n            })\n       // External\n-      .def(\"create_external_elementwise\",\n+      .def(\"create_extern_elementwise\",\n            [](mlir::OpBuilder &self, const std::string &libName,\n               const std::string &libPath, const std::string &symbol,\n-              std::vector<mlir::Value> &argList,\n-              mlir::Type retType) -> mlir::Value {\n+              std::vector<mlir::Value> &argList, mlir::Type retType,\n+              bool isPure) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::ExtElemwiseOp>(\n-                 loc, retType, argList, libName, libPath, symbol);\n+             if (isPure)\n+               return self.create<mlir::triton::PureExternElementwiseOp>(\n+                   loc, retType, argList, libName, libPath, symbol);\n+             else\n+               return self.create<mlir::triton::ImpureExternElementwiseOp>(\n+                   loc, retType, argList, libName, libPath, symbol);\n            })\n       // Built-in instruction\n       .def(\"create_get_program_id\","}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 23, "deletions": 0, "changes": 23, "file_content_changes": "@@ -2312,12 +2312,35 @@ def kernel(InitI, Bound, CutOff, OutI, OutJ):\n #     print(m[0])\n #     print(n[0])\n \n+# -----------------------\n+# test extra\n+# -----------------------\n+\n+\n+def test_globaltimer():\n+\n+    @triton.jit\n+    def kernel(Out1, Out2):\n+        start = tl.extra.cuda.globaltimer()\n+        off = tl.arange(0, 128)\n+        for i in range(100):\n+            tl.store(Out1 + off, tl.load(Out1 + off) + 1)\n+        end = tl.extra.cuda.globaltimer()\n+        tl.store(Out2, end - start)\n+\n+    out1 = to_triton(np.zeros((128,), dtype=np.int64), device='cuda')\n+    out2 = to_triton(np.zeros((1,), dtype=np.int64), device='cuda')\n+    h = kernel[(1,)](out1, out2)\n+    assert out2[0] > 0\n+    # 2 inlined globaltimers + one extra in the wrapper extern function\n+    assert h.asm[\"ptx\"].count(\"%globaltimer\") == 3\n \n # -----------------------\n # test layout conversions\n # -----------------------\n # TODO: backend should be tested separately\n \n+\n layouts = [\n     # MmaLayout(version=1, warps_per_cta=[1, 4]),\n     MmaLayout(version=(2, 0), warps_per_cta=[1, 4]),"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 11, "deletions": 7, "changes": 18, "file_content_changes": "@@ -2,6 +2,16 @@\n # Import order is significant here.\n \n from . import math\n+from . import extra\n+from .standard import (\n+    cdiv,\n+    sigmoid,\n+    softmax,\n+    ravel,\n+    swizzle2d,\n+    zeros,\n+    zeros_like,\n+)\n from .core import (\n     abs,\n     advance,\n@@ -21,7 +31,6 @@\n     broadcast,\n     broadcast_to,\n     cat,\n-    cdiv,\n     constexpr,\n     cos,\n     debug_barrier,\n@@ -56,18 +65,14 @@\n     pi32_t,\n     pointer_type,\n     program_id,\n-    ravel,\n     reduce,\n     reshape,\n-    sigmoid,\n     sin,\n-    softmax,\n     sqrt,\n     static_assert,\n     static_print,\n     store,\n     sum,\n-    swizzle2d,\n     static_range,\n     tensor,\n     trans,\n@@ -81,8 +86,6 @@\n     void,\n     where,\n     xor_sum,\n-    zeros,\n-    zeros_like,\n )\n from .random import (\n     pair_uniform_to_normal,\n@@ -127,6 +130,7 @@\n     \"dot\",\n     \"dtype\",\n     \"exp\",\n+    \"extra\",\n     \"fdiv\",\n     \"float16\",\n     \"float32\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 135, "deletions": 148, "changes": 283, "file_content_changes": "@@ -994,10 +994,36 @@ def store(pointer, value, mask=None, boundary_check=(), cache_modifier=\"\", evict\n     return semantic.store(pointer, value, mask, boundary_check, cache_modifier, eviction_policy, _builder)\n \n \n+@builtin\n+def make_block_ptr(base: tensor, shape, strides, offsets, block_shape, order, _builder=None):\n+    \"\"\"\n+    Returns a pointer to a block in a parent tensor\n+\n+    :param base: The base pointer to the parent tensor\n+    :param shape: The shape of the parent tensor\n+    :param strides: The strides of the parent tensor\n+    :param offsets: The offsets to the block\n+    :param block_shape: The shape of the block\n+    :param order: The order of the original data format\n+    \"\"\"\n+    return semantic.make_block_ptr(base, shape, strides, offsets, block_shape, order, _builder)\n+\n+\n+@builtin\n+def advance(base: tensor, offsets, _builder=None):\n+    \"\"\"\n+    Advance a block pointer\n+\n+    :param base: the block pointer to advance\n+    :param offsets: the offsets to advance, a tuple by dimension\n+    \"\"\"\n+    return semantic.advance(base, offsets, _builder)\n+\n # -----------------------\n # Atomic Memory Operations\n # -----------------------\n \n+\n def _add_atomic_docstr(name: str) -> Callable[[T], T]:\n \n     def _decorator(func: T) -> T:\n@@ -1080,7 +1106,6 @@ def atomic_xor(pointer, val, mask=None, _builder=None):\n # Conditioning\n # -----------------------\n \n-\n @builtin\n def where(condition, x, y, _builder=None):\n     \"\"\"\n@@ -1266,6 +1291,32 @@ def _argreduce(input, axis, combine_fn, _builder=None, _generator=None):\n     return rindices\n \n \n+@triton.jit\n+def minimum(x, y):\n+    \"\"\"\n+    Computes the element-wise minimum of :code:`x` and :code:`y`.\n+\n+    :param input: the first input tensor\n+    :type input: Block\n+    :param other: the second input tensor\n+    :type other: Block\n+    \"\"\"\n+    return where(x < y, x, y)\n+\n+\n+@triton.jit\n+def maximum(x, y):\n+    \"\"\"\n+    Computes the element-wise maximum of :code:`x` and :code:`y`.\n+\n+    :param input: the first input tensor\n+    :type input: Block\n+    :param other: the second input tensor\n+    :type other: Block\n+    \"\"\"\n+    return where(x > y, x, y)\n+\n+\n @triton.jit\n def _max_combine(a, b):\n     return maximum(a, b)\n@@ -1395,127 +1446,6 @@ def max_contiguous(input, values, _builder=None):\n     values = [x.value for x in values]\n     return semantic.max_contiguous(input, values)\n \n-\n-# -----------------------\n-# Standard library\n-# -----------------------\n-\n-\n-@triton.jit\n-def cdiv(x, div):\n-    \"\"\"\n-    Computes the ceiling division of :code:`x` by :code:`div`\n-\n-    :param x: the input number\n-    :type input: Block\n-    :param div: the divisor\n-    :param div: Block\n-    \"\"\"\n-    return (x + div - 1) // div\n-\n-\n-@triton.jit\n-def minimum(x, y):\n-    \"\"\"\n-    Computes the element-wise minimum of :code:`x` and :code:`y`.\n-\n-    :param input: the first input tensor\n-    :type input: Block\n-    :param other: the second input tensor\n-    :type other: Block\n-    \"\"\"\n-    return triton.language.where(x < y, x, y)\n-\n-\n-@triton.jit\n-def maximum(x, y):\n-    \"\"\"\n-    Computes the element-wise maximum of :code:`x` and :code:`y`.\n-\n-    :param input: the first input tensor\n-    :type input: Block\n-    :param other: the second input tensor\n-    :type other: Block\n-    \"\"\"\n-    return triton.language.where(x > y, x, y)\n-\n-\n-@triton.jit\n-@_add_math_1arg_docstr(\"sigmoid\")\n-def sigmoid(x):\n-    return 1 / (1 + triton.language.exp(-x))\n-\n-\n-@triton.jit\n-@_add_math_1arg_docstr(\"softmax\")\n-def softmax(x, ieee_rounding=False):\n-    z = x - triton.language.max(x, 0)\n-    num = triton.language.exp(z)\n-    den = triton.language.sum(num, 0)\n-    return fdiv(num, den, ieee_rounding)\n-\n-\n-@triton.jit\n-def ravel(x):\n-    \"\"\"\n-    Returns a contiguous flattened view of :code:`x`.\n-\n-    :param x: the input tensor\n-    :type x: Block\n-    \"\"\"\n-    return triton.language.view(x, [x.numel])\n-\n-\n-@triton.jit\n-def swizzle2d(i, j, size_i, size_j, size_g):\n-    \"\"\"\n-    Transforms indices of a row-major size_i*size_j matrix into those\n-    of one where indices are row major for each group of size_j rows.\n-    For example, for size_i = size_j = 4 and size_g = 2, it will transform\n-    [[0 , 1 , 2 , 3 ],\n-     [4 , 5 , 6 , 7 ],\n-     [8 , 9 , 10, 11],\n-     [12, 13, 14, 15]]\n-    into\n-    [[0, 2,  4 , 6 ],\n-     [1, 3,  5 , 7 ],\n-     [8, 10, 12, 14],\n-     [9, 11, 13, 15]]\n-    \"\"\"\n-    # \"unrolled index in array\"\n-    ij = i * size_j + j\n-    # number of elements in `size_g` groups\n-    # of `size_j` columns\n-    size_gj = size_g * size_j\n-    # index of the group in which (i,j) is\n-    group_id = ij // size_gj\n-    # row-index of the first element of this group\n-    off_i = group_id * size_g\n-    # last group may have fewer rows\n-    size_g = minimum(size_i - off_i, size_g)\n-    # new row and column indices\n-    new_i = off_i + (ij % size_g)\n-    new_j = (ij % size_gj) // size_g\n-    return new_i, new_j\n-\n-\n-@triton.jit\n-def zeros(shape, dtype):\n-    \"\"\"\n-    Returns a tensor filled with the scalar value 0 for the given :code:`shape` and :code:`dtype`.\n-\n-    :param shape: Shape of the new array, e.g., (8, 16) or (8, )\n-    :type shape: tuple of ints\n-    :param dtype: Data-type of the new array, e.g., :code:`tl.float16`\n-    :type dtype: DType\n-    \"\"\"\n-    return full(shape, 0, dtype)\n-\n-\n-@triton.jit\n-def zeros_like(input):\n-    return zeros(input.shape, input.dtype)\n-\n # -----------------------\n # Debugging functions\n # -----------------------\n@@ -1568,32 +1498,6 @@ def device_assert(cond, msg=\"\", _builder=None):\n     return semantic.device_assert(_to_tensor(cond, _builder), msg, file_name, func_name, lineno, _builder)\n \n \n-@builtin\n-def make_block_ptr(base: tensor, shape, strides, offsets, block_shape, order, _builder=None):\n-    \"\"\"\n-    Returns a pointer to a block in a parent tensor\n-\n-    :param base: The base pointer to the parent tensor\n-    :param shape: The shape of the parent tensor\n-    :param strides: The strides of the parent tensor\n-    :param offsets: The offsets to the block\n-    :param block_shape: The shape of the block\n-    :param order: The order of the original data format\n-    \"\"\"\n-    return semantic.make_block_ptr(base, shape, strides, offsets, block_shape, order, _builder)\n-\n-\n-@builtin\n-def advance(base: tensor, offsets, _builder=None):\n-    \"\"\"\n-    Advance a block pointer\n-\n-    :param base: the block pointer to advance\n-    :param offsets: the offsets to advance, a tuple by dimension\n-    \"\"\"\n-    return semantic.advance(base, offsets, _builder)\n-\n-\n # -----------------------\n # Iterators\n # -----------------------\n@@ -1623,3 +1527,86 @@ def __iter__(self):\n \n     def __next__(self):\n         raise RuntimeError(\"static_range can only be used in @triton.jit'd functions\")\n+\n+\n+# -----------------------\n+# Extern functions\n+# -----------------------\n+\n+def dispatch(func, lib_name: str, lib_path: str, args: list, arg_type_symbol_dict: dict, ret_shape: tuple, is_pure: bool, _builder=None):\n+    '''\n+        Dispatch a function to a library\n+        :param func: the function to dispatch\n+        :param lib_name: the name of the library\n+        :param lib_path: the path of the library\n+        :param args: the arguments of the function\n+        :param arg_type_symbol_dict: the type of the arguments\n+        :param ret_shape: the shape of the return value\n+        :param _builder: the builder\n+        :return: the return value of the function\n+    '''\n+    if len(arg_type_symbol_dict) == 0:\n+        raise ValueError(\"arg_type_symbol_dict is empty\")\n+\n+    num_args = len(list(arg_type_symbol_dict.keys())[0])\n+    if len(args) != num_args:\n+        raise ValueError(f\"length of input args does not match.\"\n+                         f\"Expect {len(args)}, got {num_args}\")\n+\n+    arg_types = []\n+    arg_list = []\n+    for arg in args:\n+        if isinstance(arg, tensor):\n+            arg_types.append(arg.dtype)\n+            arg_list.append(arg.handle)\n+        else:\n+            arg_types.append(type(arg))\n+            arg_list.append(arg)\n+    arg_types = tuple(arg_types)\n+\n+    if arg_types not in arg_type_symbol_dict:\n+        raise ValueError(f\"input arg type does not match.\"\n+                         f\"Expect one of {arg_type_symbol_dict.keys()}, got {arg_types}\")\n+    else:\n+        symbol = arg_type_symbol_dict[arg_types][0]\n+        ret_type = arg_type_symbol_dict[arg_types][1]\n+        if ret_shape:\n+            ret_type = block_type(ret_type, ret_shape)\n+        return tensor(func(lib_name, lib_path, symbol, arg_list, ret_type.to_ir(_builder), is_pure), ret_type)\n+\n+\n+def extern_elementwise(lib_name: str, lib_path: str, args: list, arg_type_symbol_dict: dict, is_pure: bool, _builder=None):\n+    '''\n+        Dispatch an elementwise function to a library\n+        :param lib_name: the name of the library\n+        :param lib_path: the path of the library\n+        :param args: the arguments of the function\n+        :param arg_type_symbol_dict: the type of the arguments\n+        :param _builder: the builder\n+        :return: the return value of the function\n+    '''\n+    dispatch_args = args.copy()\n+    all_scalar = True\n+    ret_shape = None\n+    for i in range(len(dispatch_args)):\n+        dispatch_args[i] = _to_tensor(dispatch_args[i], _builder)\n+        if dispatch_args[i].type.is_block():\n+            all_scalar = False\n+    if not all_scalar:\n+        broadcast_arg = dispatch_args[0]\n+        # Get the broadcast shape over all the arguments\n+        for i, item in enumerate(dispatch_args):\n+            _, broadcast_arg = semantic.binary_op_type_checking_impl(\n+                item, broadcast_arg, _builder)\n+        # Change the shape of each argument based on the broadcast shape\n+        for i in range(len(dispatch_args)):\n+            dispatch_args[i], _ = semantic.binary_op_type_checking_impl(\n+                dispatch_args[i], broadcast_arg, _builder)\n+        ret_shape = broadcast_arg.shape\n+    func = getattr(_builder, \"create_extern_elementwise\")\n+    return dispatch(func, lib_name, lib_path, dispatch_args, arg_type_symbol_dict, ret_shape, is_pure, _builder)\n+\n+\n+def extern(fn):\n+    \"\"\"A decorator for external functions.\"\"\"\n+    return builtin(fn)"}, {"filename": "python/triton/language/extern.py", "status": "removed", "additions": 0, "deletions": 82, "changes": 82, "file_content_changes": "@@ -1,82 +0,0 @@\n-from __future__ import annotations  # remove after python 3.11\n-\n-from . import core, semantic\n-\n-\n-def dispatch(func, lib_name: str, lib_path: str, args: list, arg_type_symbol_dict: dict, ret_shape: tuple, _builder=None):\n-    '''\n-        Dispatch a function to a library\n-        :param func: the function to dispatch\n-        :param lib_name: the name of the library\n-        :param lib_path: the path of the library\n-        :param args: the arguments of the function\n-        :param arg_type_symbol_dict: the type of the arguments\n-        :param ret_shape: the shape of the return value\n-        :param _builder: the builder\n-        :return: the return value of the function\n-    '''\n-    if len(arg_type_symbol_dict) == 0:\n-        raise ValueError(\"arg_type_symbol_dict is empty\")\n-\n-    num_args = len(list(arg_type_symbol_dict.keys())[0])\n-    if len(args) != num_args:\n-        raise ValueError(f\"length of input args does not match.\"\n-                         f\"Expect {len(args)}, got {num_args}\")\n-\n-    arg_types = []\n-    arg_list = []\n-    for arg in args:\n-        if isinstance(arg, core.tensor):\n-            arg_types.append(arg.dtype)\n-            arg_list.append(arg.handle)\n-        else:\n-            arg_types.append(type(arg))\n-            arg_list.append(arg)\n-    arg_types = tuple(arg_types)\n-\n-    if arg_types not in arg_type_symbol_dict:\n-        raise ValueError(f\"input arg type does not match.\"\n-                         f\"Expect one of {arg_type_symbol_dict.keys()}, got {arg_types}\")\n-    else:\n-        symbol = arg_type_symbol_dict[arg_types][0]\n-        ret_type = arg_type_symbol_dict[arg_types][1]\n-        if ret_shape:\n-            ret_type = core.block_type(ret_type, ret_shape)\n-        return core.tensor(func(lib_name, lib_path, symbol, arg_list, ret_type.to_ir(_builder)), ret_type)\n-\n-\n-def extern(fn):\n-    \"\"\"A decorator for external functions.\"\"\"\n-    return core.builtin(fn)\n-\n-\n-def elementwise(lib_name: str, lib_path: str, args: list, arg_type_symbol_dict: dict, _builder=None):\n-    '''\n-        Dispatch an elementwise function to a library\n-        :param lib_name: the name of the library\n-        :param lib_path: the path of the library\n-        :param args: the arguments of the function\n-        :param arg_type_symbol_dict: the type of the arguments\n-        :param _builder: the builder\n-        :return: the return value of the function\n-    '''\n-    dispatch_args = args.copy()\n-    all_scalar = True\n-    ret_shape = None\n-    for i in range(len(dispatch_args)):\n-        dispatch_args[i] = core._to_tensor(dispatch_args[i], _builder)\n-        if dispatch_args[i].type.is_block():\n-            all_scalar = False\n-    if not all_scalar:\n-        broadcast_arg = dispatch_args[0]\n-        # Get the broadcast shape over all the arguments\n-        for i, item in enumerate(dispatch_args):\n-            _, broadcast_arg = semantic.binary_op_type_checking_impl(\n-                item, broadcast_arg, _builder)\n-        # Change the shape of each argument based on the broadcast shape\n-        for i in range(len(dispatch_args)):\n-            dispatch_args[i], _ = semantic.binary_op_type_checking_impl(\n-                dispatch_args[i], broadcast_arg, _builder)\n-        ret_shape = broadcast_arg.shape\n-    func = getattr(_builder, \"create_external_elementwise\")\n-    return dispatch(func, lib_name, lib_path, dispatch_args, arg_type_symbol_dict, ret_shape, _builder)"}, {"filename": "python/triton/language/extra/__init__.py", "status": "added", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -0,0 +1,3 @@\n+from . import cuda\n+\n+__all__ = ['cuda']"}, {"filename": "python/triton/language/extra/cuda.bc", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/triton/language/extra/cuda.py", "status": "added", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -0,0 +1,12 @@\n+import os\n+\n+from .. import core\n+\n+__path__ = os.path.dirname(os.path.abspath(__file__))\n+\n+\n+@core.extern\n+def globaltimer(_builder=None):\n+    return core.extern_elementwise(\"cuda\", os.path.join(__path__, \"cuda.bc\"), [],\n+                                   {tuple(): (\"globaltimer\", core.dtype(\"int64\")),\n+                                    }, is_pure=False, _builder=_builder)"}, {"filename": "python/triton/language/math.py", "status": "modified", "additions": 923, "deletions": 923, "changes": 1846, "file_content_changes": "N/A"}, {"filename": "python/triton/language/standard.py", "status": "added", "additions": 98, "deletions": 0, "changes": 98, "file_content_changes": "@@ -0,0 +1,98 @@\n+from __future__ import annotations\n+\n+from ..runtime.jit import jit\n+from . import core\n+\n+# -----------------------\n+# Standard library\n+# -----------------------\n+\n+\n+@jit\n+def cdiv(x, div):\n+    \"\"\"\n+    Computes the ceiling division of :code:`x` by :code:`div`\n+\n+    :param x: the input number\n+    :type input: Block\n+    :param div: the divisor\n+    :param div: Block\n+    \"\"\"\n+    return (x + div - 1) // div\n+\n+\n+@jit\n+@core._add_math_1arg_docstr(\"sigmoid\")\n+def sigmoid(x):\n+    return 1 / (1 + core.exp(-x))\n+\n+\n+@jit\n+@core._add_math_1arg_docstr(\"softmax\")\n+def softmax(x, ieee_rounding=False):\n+    z = x - core.max(x, 0)\n+    num = core.exp(z)\n+    den = core.sum(num, 0)\n+    return core.fdiv(num, den, ieee_rounding)\n+\n+\n+@jit\n+def ravel(x):\n+    \"\"\"\n+    Returns a contiguous flattened view of :code:`x`.\n+\n+    :param x: the input tensor\n+    :type x: Block\n+    \"\"\"\n+    return core.view(x, [x.numel])\n+\n+\n+@jit\n+def swizzle2d(i, j, size_i, size_j, size_g):\n+    \"\"\"\n+    Transforms indices of a row-major size_i*size_j matrix into those\n+    of one where indices are row major for each group of size_j rows.\n+    For example, for size_i = size_j = 4 and size_g = 2, it will transform\n+    [[0 , 1 , 2 , 3 ],\n+     [4 , 5 , 6 , 7 ],\n+     [8 , 9 , 10, 11],\n+     [12, 13, 14, 15]]\n+    into\n+    [[0, 2,  4 , 6 ],\n+     [1, 3,  5 , 7 ],\n+     [8, 10, 12, 14],\n+     [9, 11, 13, 15]]\n+    \"\"\"\n+    # \"unrolled index in array\"\n+    ij = i * size_j + j\n+    # number of elements in `size_g` groups\n+    # of `size_j` columns\n+    size_gj = size_g * size_j\n+    # index of the group in which (i,j) is\n+    group_id = ij // size_gj\n+    # row-index of the first element of this group\n+    off_i = group_id * size_g\n+    # last group may have fewer rows\n+    size_g = core.minimum(size_i - off_i, size_g)\n+    # new row and column indices\n+    new_i = off_i + (ij % size_g)\n+    new_j = (ij % size_gj) // size_g\n+    return new_i, new_j\n+\n+\n+@jit\n+def zeros(shape, dtype):\n+    \"\"\"\n+    Returns a tensor filled with the scalar value 0 for the given :code:`shape` and :code:`dtype`.\n+\n+    :param shape: Shape of the new array, e.g., (8, 16) or (8, )\n+    :type shape: tuple of ints\n+    :param dtype: Data-type of the new array, e.g., :code:`tl.float16`\n+    :type dtype: DType\n+    \"\"\"\n+    return core.full(shape, 0, dtype)\n+\n+\n+@jit\n+def zeros_like(input):\n+    return zeros(input.shape, input.dtype)"}, {"filename": "python/triton/runtime/driver.py", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -61,13 +61,17 @@ def __new__(cls):\n             cls.instance = super(CudaDriver, cls).__new__(cls)\n         return cls.instance\n \n+    def get_extern_path(self):\n+        return os.path.join(self.third_party_dir(), \"cuda\", \"lib\")\n+\n     def get_libdevice_path(self):\n         return os.path.join(self.third_party_dir(), \"cuda\", \"lib\", \"libdevice.10.bc\")\n \n     def __init__(self):\n         self.utils = CudaUtils()\n         self.backend = self.CUDA\n         self.libdevice_path = self.get_libdevice_path()\n+        self.extern_path = self.get_extern_path()\n \n # -----------------------------\n # HIP"}, {"filename": "python/triton/runtime/errors.py", "status": "added", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "@@ -0,0 +1,15 @@\n+\n+class OutOfResources(Exception):\n+    def __init__(self, required, limit, name):\n+        self.message = f'out of resource: {name}, '\\\n+                       f'Required: {required}, '\\\n+                       f'Hardware limit: {limit}'\n+        self.message += '. Reducing block sizes or `num_stages` may help.'\n+        self.required = required\n+        self.limit = limit\n+        self.name = name\n+        super().__init__(self.message)\n+\n+    def __reduce__(self):\n+        # this is necessary to make CompilationError picklable\n+        return (type(self), (self.required, self.limit, self.name))"}, {"filename": "python/triton/tools/build_extern.py", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -156,6 +156,7 @@ def __init__(self, path) -> None:\n         '''\n         super().__init__(\"libdevice\", path)\n         self._symbol_groups = {}\n+        self.is_pure = True\n \n     @staticmethod\n     def _extract_symbol(line) -> Optional[Symbol]:\n@@ -287,21 +288,21 @@ def _output_stubs(self) -> str:\n         # def <op_name>(<args>, _builder=None):\n         #   arg_type_symbol_dict = {[arg_type]: {(symbol, ret_type)}}\n         #   return extern.dispatch(\"libdevice\", <path>, <args>, <arg_type_symbol_dict>, _builder)\n-        import_str = \"from . import core, extern\\n\"\n+        import_str = \"from . import core\\n\"\n         import_str += \"from ..runtime import driver\\n\"\n         import_str += \"import os\\n\"\n \n         header_str = \"\"\n         header_str += \"LIBDEVICE_PATH = os.getenv(\\\"TRITON_LIBDEVICE_PATH\\\", driver.libdevice_path)\\n\"\n         func_str = \"\"\n         for symbols in self._symbol_groups.values():\n-            func_str += \"@extern.extern\\n\"\n+            func_str += \"@core.extern\\n\"\n             func_name_str = f\"def {symbols[0].op_name}(\"\n             for arg_name in symbols[0].arg_names:\n                 func_name_str += f\"{arg_name}, \"\n             func_name_str += \"_builder=None):\\n\"\n \n-            return_str = f\"\\treturn extern.elementwise(\\\"{self._name}\\\", LIBDEVICE_PATH, [\"\n+            return_str = f\"\\treturn core.extern_elementwise(\\\"{self._name}\\\", LIBDEVICE_PATH, [\"\n             for arg_name in symbols[0].arg_names:\n                 return_str += f\"{arg_name}, \"\n             return_str += \"], \\n\"\n@@ -316,7 +317,8 @@ def _output_stubs(self) -> str:\n             arg_type_symbol_dict_str += \"}\"\n \n             return_str += arg_type_symbol_dict_str\n-            return_str += \", _builder)\\n\"\n+            return_str += f\", is_pure={self.is_pure}\"\n+            return_str += \", _builder=_builder)\\n\"\n \n             func_str += func_name_str + return_str + \"\\n\"\n         file_str = import_str + header_str + func_str"}]