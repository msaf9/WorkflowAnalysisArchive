[{"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 37, "deletions": 0, "changes": 37, "file_content_changes": "@@ -63,6 +63,43 @@ class ReduceOpHelper {\n   int axis;\n };\n \n+class ScanLoweringHelper {\n+public:\n+  explicit ScanLoweringHelper(triton::ScanOp op) : scanOp(op) {\n+    auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n+    srcEncoding = type.getEncoding();\n+  }\n+  // Return true if the lowering of the scan op is supported.\n+  bool isSupported();\n+  // Return the number of elements per thread along axis dim.\n+  unsigned getAxisNumElementsPerThreads();\n+  // Return the number of elements per thread along non-axis dims.\n+  unsigned getNonAxisNumElementsPerThread();\n+  // Return the number of threads per warp along non-axis dims.\n+  unsigned getNonAxisNumThreadsPerWarp();\n+  // Return the flat numbers of threads computing independent scan results.\n+  unsigned getNonAxisNumThreadsPerCTA();\n+  // Return the number of warps per CTA along axis dim.\n+  unsigned getAxisNumWarps();\n+  // Return the number of threads per warp along axis dim.\n+  unsigned getAxisNumThreadsPerWarp();\n+  // Return the number of blocks along axis dim.\n+  unsigned getAxisNumBlocks();\n+  // Return the number of blocks along non axis dim.\n+  unsigned getNonAxisNumBlocks();\n+  // Return the size of the scratch space needed for scan lowering.\n+  unsigned getScratchSizeInBytes();\n+\n+  Location getLoc() { return scanOp.getLoc(); }\n+  unsigned getAxis() { return scanOp.getAxis(); }\n+  triton::gpu::BlockedEncodingAttr getEncoding();\n+  Region &getCombineOp();\n+\n+private:\n+  triton::ScanOp scanOp;\n+  Attribute srcEncoding;\n+};\n+\n bool maybeSharedAllocationOp(Operation *op);\n \n bool maybeAliasOp(Operation *op);"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 26, "deletions": 0, "changes": 26, "file_content_changes": "@@ -426,6 +426,32 @@ def TT_ReduceReturnOp: TT_Op<\"reduce.return\",\n     let assemblyFormat = \"$result attr-dict `:` type($result)\";\n }\n \n+//\n+// Scan Op\n+//\n+def TT_ScanOp: TT_Op<\"scan\",\n+                       [Pure,\n+                        SameOperandsAndResultEncoding,\n+                        SameOperandsAndResultElementType,\n+                        SingleBlock,\n+                        DeclareOpInterfaceMethods<InferTypeOpInterface>]> {\n+    let summary = \"Reduction using generic combination algorithm\";\n+    let arguments = (ins Variadic<TT_Tensor>:$operands, I32Attr:$axis);\n+    let results = (outs Variadic<TT_Tensor>:$result);\n+    let regions = (region SizedRegion<1>:$combineOp);\n+    let builders = [\n+        OpBuilder<(ins \"ValueRange\":$operands, \"int\":$axis)>,\n+    ];\n+    let hasVerifier = 1;\n+}\n+\n+def TT_ScanReturnOp: TT_Op<\"scan.return\",\n+                             [HasParent<\"ScanOp\">, Pure, Terminator, ReturnLike]> {\n+    let summary = \"terminator for scan operator\";\n+    let arguments = (ins Variadic<AnyType>:$result);\n+    let assemblyFormat = \"$result attr-dict `:` type($result)\";\n+}\n+\n \n //\n // External Elementwise op"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -168,6 +168,10 @@ class AllocationAnalysis {\n       ReduceOpHelper helper(reduceOp);\n       unsigned bytes = helper.getScratchSizeInBytes();\n       allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+    } else if (auto scanOp = dyn_cast<triton::ScanOp>(op)) {\n+      ScanLoweringHelper helper(scanOp);\n+      unsigned bytes = helper.getScratchSizeInBytes();\n+      allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     } else if (auto cvtLayout = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n       auto srcTy = cvtLayout.getSrc().getType().cast<RankedTensorType>();\n       auto dstTy = cvtLayout.getResult().getType().cast<RankedTensorType>();"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -86,7 +86,8 @@ void MembarAnalysis::visitTerminator(Operation *op,\n     return;\n   }\n   // Otherwise, it could be a return op\n-  if (isa<triton::ReduceReturnOp>(op) || isa<triton::ReturnOp>(op)) {\n+  if (isa<triton::ReduceReturnOp>(op) || isa<triton::ScanReturnOp>(op) ||\n+      isa<triton::ReturnOp>(op)) {\n     return;\n   }\n   llvm_unreachable(\"Unknown terminator encountered in membar analysis\");"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 87, "deletions": 0, "changes": 87, "file_content_changes": "@@ -117,6 +117,93 @@ bool ReduceOpHelper::isSupportedLayout() {\n   return false;\n }\n \n+unsigned ScanLoweringHelper::getAxisNumElementsPerThreads() {\n+  return getEncoding().getSizePerThread()[getAxis()];\n+}\n+\n+unsigned ScanLoweringHelper::getNonAxisNumElementsPerThread() {\n+  SmallVector<unsigned> sizePerThreads(getEncoding().getSizePerThread().begin(),\n+                                       getEncoding().getSizePerThread().end());\n+  sizePerThreads[getAxis()] = 1;\n+  return product<unsigned>(sizePerThreads);\n+}\n+\n+Region &ScanLoweringHelper::getCombineOp() { return scanOp.getCombineOp(); }\n+\n+unsigned ScanLoweringHelper::getAxisNumThreadsPerWarp() {\n+  return triton::gpu::getThreadsPerWarp(getEncoding())[getAxis()];\n+}\n+\n+unsigned ScanLoweringHelper::getNonAxisNumThreadsPerWarp() {\n+  auto threadsPerWarp = triton::gpu::getThreadsPerWarp(getEncoding());\n+  threadsPerWarp[getAxis()] = 1;\n+  return product<unsigned>(threadsPerWarp);\n+}\n+\n+// Return the flat numbers of threads computing independent scan results.\n+unsigned ScanLoweringHelper::getNonAxisNumThreadsPerCTA() {\n+  unsigned numParallelThreadsPerWarp = getNonAxisNumThreadsPerWarp();\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(getEncoding());\n+  warpsPerCTA[getAxis()] = 1;\n+  unsigned numParallelWarpsPerCTA = product<unsigned>(warpsPerCTA);\n+  return numParallelThreadsPerWarp * numParallelWarpsPerCTA;\n+}\n+unsigned ScanLoweringHelper::getAxisNumWarps() {\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n+  return warpsPerCTA[getAxis()];\n+}\n+\n+unsigned ScanLoweringHelper::getAxisNumBlocks() {\n+  auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n+  auto sizePerThreads = triton::gpu::getSizePerThread(srcEncoding);\n+  auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcEncoding);\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n+  unsigned axis = getAxis();\n+  return type.getShape()[axis] /\n+         (sizePerThreads[axis] * threadsPerWarp[axis] * warpsPerCTA[axis]);\n+}\n+\n+unsigned ScanLoweringHelper::getNonAxisNumBlocks() {\n+  auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n+  auto sizePerThreads = triton::gpu::getSizePerThread(srcEncoding);\n+  auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcEncoding);\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n+  unsigned axis = getAxis();\n+  unsigned numBlocks = 1;\n+  for (unsigned i = 0; i < sizePerThreads.size(); i++) {\n+    if (i == axis)\n+      continue;\n+    numBlocks *= type.getShape()[i] /\n+                 (sizePerThreads[i] * threadsPerWarp[i] * warpsPerCTA[i]);\n+  }\n+  return numBlocks;\n+}\n+\n+bool ScanLoweringHelper::isSupported() {\n+  // TODO: Support the following cases:\n+  // 1. Scan on the non-fast changing dimension\n+  // 2. Scan on non-blocking encodings\n+  // 3. Scan with multiple operands\n+  if (getAxis() != triton::gpu::getOrder(srcEncoding)[0] ||\n+      !isa<triton::gpu::BlockedEncodingAttr>(srcEncoding))\n+    return false;\n+  if (scanOp.getNumOperands() != 1)\n+    return false;\n+  return true;\n+}\n+\n+unsigned ScanLoweringHelper::getScratchSizeInBytes() {\n+  auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n+  unsigned numElement =\n+      type.getNumElements() * type.getElementTypeBitWidth() / 8;\n+  return numElement /\n+         (getAxisNumElementsPerThreads() * getAxisNumThreadsPerWarp());\n+}\n+\n+triton::gpu::BlockedEncodingAttr ScanLoweringHelper::getEncoding() {\n+  return srcEncoding.cast<triton::gpu::BlockedEncodingAttr>();\n+}\n+\n bool maybeSharedAllocationOp(Operation *op) {\n   // TODO(Keren): This function can be replaced by adding\n   // MemoryEffectOpInterface. We can then use the MemoryEffectOpInterface to"}, {"filename": "lib/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -17,6 +17,7 @@ add_mlir_conversion_library(TritonGPUToLLVM\n     TritonGPUToLLVMPass.cpp\n     PTXAsmFormat.cpp\n     ReduceOpToLLVM.cpp\n+    ScanOpToLLVM.cpp\n     Utility.cpp\n     TypeConverter.cpp\n     ViewOpToLLVM.cpp"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ScanOpToLLVM.cpp", "status": "added", "additions": 307, "deletions": 0, "changes": 307, "file_content_changes": "@@ -0,0 +1,307 @@\n+#include \"ScanOpToLLVM.h\"\n+#include \"TritonGPUToLLVMBase.h\"\n+#include \"triton/Analysis/Utility.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::LLVM::delinearize;\n+using ::mlir::LLVM::linearize;\n+using ::mlir::LLVM::shflUpSync;\n+using ::mlir::LLVM::storeShared;\n+\n+// Apply the region of the scan op to the acc and cur values and update acc\n+// inplace with the result.\n+static void accumulate(ConversionPatternRewriter &rewriter, Region &combineOp,\n+                       Value &acc, Value cur) {\n+  if (!acc) {\n+    acc = cur;\n+    return;\n+  }\n+  // Create a new copy of the reduce block, and inline it\n+  Block *currentBlock = rewriter.getBlock();\n+  Region &parent = *currentBlock->getParent();\n+  rewriter.cloneRegionBefore(combineOp, &parent.front());\n+  auto &newScan = parent.front();\n+  auto returnOp = dyn_cast<triton::ScanReturnOp>(newScan.getTerminator());\n+  llvm::SmallVector<Value> combineArgs = {acc, cur};\n+  rewriter.inlineBlockBefore(&newScan, &*rewriter.getInsertionPoint(),\n+                             combineArgs);\n+  auto results = returnOp.getResult();\n+  acc = results[0];\n+  // Delete the terminator, which is no longer used\n+  rewriter.eraseOp(returnOp);\n+}\n+\n+// Scan a contiguous elements within a thread and update `srcValues` in place.\n+static void scanThreadContiguousElements(SmallVector<Value> &srcValues,\n+                                         ConversionPatternRewriter &rewriter,\n+                                         ScanLoweringHelper &helper) {\n+  // TODO: this assumes that axis is the fastest moving dimension. We should\n+  // relax that.\n+  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThreads();\n+  // Loop through the blocks of contiguous elements.\n+  for (unsigned j = 0; j < srcValues.size(); j += scanElementsPerThreads) {\n+    // Reset the accumulator at the beginning of each block of contiguous\n+    // elements.\n+    Value acc;\n+    // Loop through the contiguous elements.\n+    for (unsigned i = 0; i < scanElementsPerThreads; ++i) {\n+      accumulate(rewriter, helper.getCombineOp(), acc, srcValues[i + j]);\n+      srcValues[i + j] = acc;\n+    }\n+  }\n+}\n+\n+// Apply a scan across threads of the warp for the last element of each\n+// contiguous group of elements.\n+static void warpScan(SmallVector<Value> &srcValues,\n+                     ConversionPatternRewriter &rewriter,\n+                     ScanLoweringHelper &helper, Value laneId) {\n+  Location loc = helper.getLoc();\n+  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThreads();\n+  for (unsigned j = scanElementsPerThreads - 1; j < srcValues.size();\n+       j += scanElementsPerThreads) {\n+    Value acc = srcValues[j];\n+    unsigned scanDim = helper.getAxisNumThreadsPerWarp();\n+    // Reduce within warps.\n+    for (unsigned i = 1; i <= scanDim / 2; i = i << 1) {\n+      Value shfl = shflUpSync(loc, rewriter, acc, i);\n+      Value tempAcc = acc;\n+      accumulate(rewriter, helper.getCombineOp(), tempAcc, shfl);\n+      Value mask = icmp_slt(laneId, i32_val(i));\n+      acc = select(mask, acc, tempAcc);\n+    }\n+    srcValues[j] = acc;\n+  }\n+}\n+\n+// For each set of contiguous elements within a thread we store the partial\n+// reduction into shared memory. Each parallel scan and each warp will store its\n+// own partial reductions. The shared memory is organized as follow:\n+//          -----------------------------------------------------------------\n+// chunk 0: | acc[0] warp 0 | acc[1] warp 0 | acc[0] warp 1 | acc[1] warp 1 |\n+// chunk 1: | acc[0] warp 0 | acc[1] warp 0 | acc[0] warp 1 | acc[1] warp 1 |\n+static void storeWarpAccumulator(SmallVector<Value> &srcValues,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 ScanLoweringHelper &helper, Value laneId,\n+                                 Value warpId, Value baseSharedMemPtr,\n+                                 Value parallelLaneId) {\n+  Location loc = helper.getLoc();\n+  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThreads();\n+  unsigned scanDim = helper.getAxisNumThreadsPerWarp();\n+  unsigned numParallelLane = helper.getNonAxisNumThreadsPerCTA();\n+  unsigned numWarps = helper.getAxisNumWarps();\n+  unsigned chunkId = 0;\n+  for (unsigned j = scanElementsPerThreads - 1; j < srcValues.size();\n+       j += scanElementsPerThreads, ++chunkId) {\n+    Value lastElement = srcValues[j];\n+    Value mask = icmp_eq(laneId, i32_val(scanDim - 1));\n+    Value index = add(parallelLaneId, mul(warpId, i32_val(numParallelLane)));\n+    index = add(index, i32_val(chunkId * numParallelLane * numWarps));\n+    Value writePtr = gep(baseSharedMemPtr.getType(), baseSharedMemPtr, index);\n+    storeShared(rewriter, loc, writePtr, lastElement, mask);\n+  }\n+}\n+\n+// Read the partial reductions from shared memory from each chunk of contiguous\n+// elements for each warp and parallel scan. Then combine the partial reduction\n+// with the right elements. Within a given contiguous element chunk we update\n+// all the elements by accumulating the value from the last element of the\n+// reduced value from the previous lane.\n+static void AddPartialReduce(SmallVector<Value> &srcValues,\n+                             ConversionPatternRewriter &rewriter,\n+                             ScanLoweringHelper &helper, Value sharedMemoryPtr,\n+                             Value warpId, Value laneId, Value parallelLaneId) {\n+  Location loc = helper.getLoc();\n+  unsigned numParallelLane = helper.getNonAxisNumThreadsPerCTA();\n+  unsigned numWarps = helper.getAxisNumWarps();\n+  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThreads();\n+  unsigned parallelElementsPerThread = helper.getNonAxisNumElementsPerThread();\n+  Value maskFirstWarp = icmp_eq(warpId, i32_val(0));\n+  Value maskFirstLane = icmp_eq(laneId, i32_val(0));\n+  Value maskFirstThread = and_(maskFirstWarp, maskFirstLane);\n+  struct Accumulator {\n+    Value acc;\n+    Value maskedAcc;\n+  };\n+  unsigned numScanBlocks = helper.getAxisNumBlocks();\n+  unsigned numParallelBlocks = helper.getNonAxisNumBlocks();\n+  assert(numScanBlocks * numParallelBlocks * parallelElementsPerThread *\n+             scanElementsPerThreads ==\n+         srcValues.size());\n+  SmallVector<Accumulator> accumulators(numParallelBlocks *\n+                                        parallelElementsPerThread);\n+  unsigned chunkId = 0;\n+  for (unsigned parallelBlockId = 0; parallelBlockId < numParallelBlocks;\n+       ++parallelBlockId) {\n+    for (unsigned scanBlockId = 0; scanBlockId < numScanBlocks; ++scanBlockId) {\n+      for (unsigned parallelElementId = 0;\n+           parallelElementId < parallelElementsPerThread; ++parallelElementId) {\n+        unsigned accumulatorIndex =\n+            parallelElementId + parallelBlockId * parallelElementsPerThread;\n+        Accumulator &accumulator = accumulators[accumulatorIndex];\n+        for (unsigned i = 0; i < numWarps; ++i) {\n+          Value index = add(parallelLaneId, i32_val(numParallelLane *\n+                                                    (i + chunkId * numWarps)));\n+          Value ptr = gep(sharedMemoryPtr.getType(), sharedMemoryPtr, index);\n+          Value partialReduce = load(ptr);\n+          if (!accumulator.acc) {\n+            accumulator.acc = partialReduce;\n+            accumulator.maskedAcc = partialReduce;\n+            continue;\n+          }\n+          accumulate(rewriter, helper.getCombineOp(), accumulator.acc,\n+                     partialReduce);\n+          Value mask = icmp_slt(warpId, i32_val(i + 1));\n+          accumulator.maskedAcc =\n+              select(mask, accumulator.maskedAcc, accumulator.acc);\n+        }\n+        unsigned lastElementIndex =\n+            chunkId * scanElementsPerThreads + scanElementsPerThreads - 1;\n+        Value temp = srcValues[lastElementIndex];\n+        accumulate(rewriter, helper.getCombineOp(), temp,\n+                   accumulator.maskedAcc);\n+        if (scanBlockId == 0) {\n+          // For the first warp and first chunk we don't have anything to\n+          // accumulate.\n+          temp = select(maskFirstWarp, srcValues[lastElementIndex], temp);\n+        }\n+        srcValues[lastElementIndex] = temp;\n+\n+        // Update the rest of the contiguous elements.\n+        Value lastElement =\n+            shflUpSync(loc, rewriter, srcValues[lastElementIndex], 1);\n+        lastElement = select(maskFirstLane, accumulator.maskedAcc, lastElement);\n+        for (unsigned i = 1; i < scanElementsPerThreads; ++i) {\n+          Value laneValue = srcValues[lastElementIndex - i];\n+          accumulate(rewriter, helper.getCombineOp(), laneValue, lastElement);\n+          if (scanBlockId == 0) {\n+            // For the first warp and first chunk we don't have anything to\n+            // accumulate.\n+            laneValue = select(maskFirstThread, srcValues[lastElementIndex - i],\n+                               laneValue);\n+          }\n+          srcValues[lastElementIndex - i] = laneValue;\n+        }\n+        // For the next chunk start back from the value containing the\n+        // accumulated value of all the warps.\n+        accumulator.maskedAcc = accumulator.acc;\n+        chunkId++;\n+      }\n+    }\n+  }\n+}\n+\n+namespace {\n+struct ScanOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::ScanOp> {\n+public:\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::ScanOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::ScanOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    if (succeeded(emitFastScan(op, adaptor, rewriter)))\n+      return success();\n+    return failure();\n+  }\n+\n+private:\n+  std::tuple<Value, Value, Value>\n+  getDelinearizedIds(ConversionPatternRewriter &rewriter,\n+                     ScanLoweringHelper &helper) const;\n+  LogicalResult emitFastScan(triton::ScanOp op, triton::ScanOpAdaptor adaptor,\n+                             ConversionPatternRewriter &rewriter) const;\n+};\n+\n+// Break up the threadId into lane and warp id along the scan dimension and\n+// compute a flat id for the parallel dimensions.\n+std::tuple<Value, Value, Value>\n+ScanOpConversion::getDelinearizedIds(ConversionPatternRewriter &rewriter,\n+                                     ScanLoweringHelper &helper) const {\n+  auto loc = helper.getLoc();\n+  unsigned axis = helper.getAxis();\n+  auto srcEncoding = helper.getEncoding();\n+\n+  Value threadId = getThreadId(rewriter, loc);\n+  Value warpSize = i32_val(32);\n+  Value warpId = udiv(threadId, warpSize);\n+  Value laneId = urem(threadId, warpSize);\n+\n+  auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcEncoding);\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n+  auto order = triton::gpu::getOrder(srcEncoding);\n+  SmallVector<Value> multiDimLaneId =\n+      delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n+  SmallVector<Value> multiDimWarpId =\n+      delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n+\n+  Value laneIdAxis = multiDimLaneId[axis];\n+  Value warpIdAxis = multiDimWarpId[axis];\n+\n+  multiDimLaneId[axis] = i32_val(0);\n+  threadsPerWarp[axis] = 1;\n+  Value laneIdParallel =\n+      linearize(rewriter, loc, multiDimLaneId, threadsPerWarp, order);\n+  multiDimWarpId[axis] = i32_val(0);\n+  warpsPerCTA[axis] = 1;\n+  Value warpIdParallel =\n+      linearize(rewriter, loc, multiDimWarpId, warpsPerCTA, order);\n+  Value flatIdParallel =\n+      add(laneIdParallel,\n+          mul(warpIdParallel, i32_val(helper.getNonAxisNumThreadsPerWarp())));\n+  return std::make_tuple(laneIdAxis, warpIdAxis, flatIdParallel);\n+}\n+\n+// Lowering using warp shuffle operations to do warp level scan.\n+LogicalResult\n+ScanOpConversion::emitFastScan(triton::ScanOp op, triton::ScanOpAdaptor adaptor,\n+                               ConversionPatternRewriter &rewriter) const {\n+  ScanLoweringHelper helper(op);\n+  auto loc = helper.getLoc();\n+  if (!helper.isSupported())\n+    return failure();\n+\n+  auto [laneIdAxis, warpIdAxis, flatIdParallel] =\n+      getDelinearizedIds(rewriter, helper);\n+  auto input = adaptor.getOperands()[0];\n+  auto type = op.getOperand(0).getType().cast<RankedTensorType>();\n+  SmallVector<Value> srcValues =\n+      getTypeConverter()->unpackLLElements(loc, input, rewriter, type);\n+\n+  // Scan contigous elements in a thread and update `srcValues`.\n+  scanThreadContiguousElements(srcValues, rewriter, helper);\n+  // Apply warp level scan to the last element of each chunk of contiguous\n+  // elements.\n+  warpScan(srcValues, rewriter, helper, laneIdAxis);\n+\n+  // Store the partial reducing for each warp into shared memory.\n+  Type elemPtrTys = LLVM::LLVMPointerType::get(srcValues[0].getType(), 3);\n+  Value baseSharedMemPtr = bitcast(\n+      getSharedMemoryBase(loc, rewriter, op.getOperation()), elemPtrTys);\n+  storeWarpAccumulator(srcValues, rewriter, helper, laneIdAxis, warpIdAxis,\n+                       baseSharedMemPtr, flatIdParallel);\n+  barrier();\n+  // Read back the partial reduction of each warp and accumulate them based on\n+  // warpId. Then update each chunk of contiguous elements by adding the\n+  // accumulated value from the previous lane.\n+  AddPartialReduce(srcValues, rewriter, helper, baseSharedMemPtr, warpIdAxis,\n+                   laneIdAxis, flatIdParallel);\n+\n+  Value results = getTypeConverter()->packLLElements(loc, srcValues, rewriter,\n+                                                     input.getType());\n+  rewriter.replaceOp(op, results);\n+  return success();\n+}\n+} // namespace\n+\n+void populateScanOpToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    ModuleAllocation &allocation,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit) {\n+  patterns.add<ScanOpConversion>(typeConverter, allocation, indexCacheInfo,\n+                                 benefit);\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ScanOpToLLVM.h", "status": "added", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "@@ -0,0 +1,15 @@\n+#ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_SCAN_OP_H\n+#define TRITON_CONVERSION_TRITONGPU_TO_LLVM_SCAN_OP_H\n+\n+#include \"TritonGPUToLLVMBase.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+void populateScanOpToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    ModuleAllocation &allocation,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit);\n+\n+#endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -26,6 +26,7 @@\n #include \"ElementwiseOpToLLVM.h\"\n #include \"LoadStoreOpToLLVM.h\"\n #include \"ReduceOpToLLVM.h\"\n+#include \"ScanOpToLLVM.h\"\n #include \"TritonGPUToLLVM.h\"\n #include \"TypeConverter.h\"\n #include \"ViewOpToLLVM.h\"\n@@ -372,6 +373,8 @@ class ConvertTritonGPUToLLVM\n                                       /*benefit=*/1);\n     populateReduceOpToLLVMPatterns(typeConverter, patterns, allocation,\n                                    indexCacheInfo, /*benefit=*/1);\n+    populateScanOpToLLVMPatterns(typeConverter, patterns, allocation,\n+                                 indexCacheInfo, /*benefit=*/1);\n     populateViewOpToLLVMPatterns(typeConverter, patterns, /*benefit=*/1);\n \n     // Native lowering patterns"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.cpp", "status": "modified", "additions": 17, "deletions": 6, "changes": 23, "file_content_changes": "@@ -160,34 +160,45 @@ Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n   return builder.launch(rewriter, loc, void_ty(ctx));\n }\n \n-Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n-               int i) {\n+static Value commonShflSync(Location loc, ConversionPatternRewriter &rewriter,\n+                            Value val, int i, const std::string &shuffleType,\n+                            const std::string &clamp) {\n   unsigned bits = val.getType().getIntOrFloatBitWidth();\n \n   if (bits == 64) {\n     Type vecTy = vec_ty(f32_ty, 2);\n     Value vec = bitcast(val, vecTy);\n     Value val0 = extract_element(f32_ty, vec, i32_val(0));\n     Value val1 = extract_element(f32_ty, vec, i32_val(1));\n-    val0 = shflSync(loc, rewriter, val0, i);\n-    val1 = shflSync(loc, rewriter, val1, i);\n+    val0 = commonShflSync(loc, rewriter, val0, i, shuffleType, clamp);\n+    val1 = commonShflSync(loc, rewriter, val1, i, shuffleType, clamp);\n     vec = undef(vecTy);\n     vec = insert_element(vecTy, vec, val0, i32_val(0));\n     vec = insert_element(vecTy, vec, val1, i32_val(1));\n     return bitcast(vec, val.getType());\n   }\n \n   PTXBuilder builder;\n-  auto &shfl = builder.create(\"shfl.sync\")->o(\"bfly\").o(\"b32\");\n+  auto &shfl = builder.create(\"shfl.sync\")->o(shuffleType).o(\"b32\");\n   auto *dOpr = builder.newOperand(\"=r\");\n   auto *aOpr = builder.newOperand(val, \"r\");\n   auto *bOpr = builder.newConstantOperand(i);\n-  auto *cOpr = builder.newConstantOperand(\"0x1f\");\n+  auto *cOpr = builder.newConstantOperand(clamp);\n   auto *maskOpr = builder.newConstantOperand(\"0xffffffff\");\n   shfl(dOpr, aOpr, bOpr, cOpr, maskOpr);\n   return builder.launch(rewriter, loc, val.getType(), false);\n }\n \n+Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n+               int i) {\n+  return commonShflSync(loc, rewriter, val, i, \"bfly\", \"0x1f\");\n+}\n+\n+Value shflUpSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n+                 int i) {\n+  return commonShflSync(loc, rewriter, val, i, \"up\", \"0x0\");\n+}\n+\n Value addStringToModule(Location loc, ConversionPatternRewriter &rewriter,\n                         StringRef key, StringRef content) {\n   auto moduleOp = rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -285,6 +285,8 @@ Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n \n Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n                int i);\n+Value shflUpSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n+                 int i);\n \n Value addStringToModule(Location loc, ConversionPatternRewriter &rewriter,\n                         StringRef key, StringRef content);"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 36, "deletions": 3, "changes": 39, "file_content_changes": "@@ -537,6 +537,38 @@ struct TritonReduceReturnPattern\n   }\n };\n \n+struct TritonScanPattern : public OpConversionPattern<triton::ScanOp> {\n+  using OpConversionPattern<triton::ScanOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::ScanOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto newScan = rewriter.create<triton::ScanOp>(\n+        op.getLoc(), adaptor.getOperands(), adaptor.getAxis());\n+    addNamedAttrs(newScan, adaptor.getAttributes());\n+\n+    auto &newCombineOp = newScan.getCombineOp();\n+    rewriter.cloneRegionBefore(op.getCombineOp(), newCombineOp,\n+                               newCombineOp.end());\n+    rewriter.replaceOp(op, newScan.getResult());\n+    return success();\n+  }\n+};\n+\n+struct TritonScanReturnPattern\n+    : public OpConversionPattern<triton::ScanReturnOp> {\n+  using OpConversionPattern<triton::ScanReturnOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::ScanReturnOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::ScanReturnOp>(\n+                      op, adaptor.getResult()),\n+                  adaptor.getAttributes());\n+    return success();\n+  }\n+};\n+\n struct TritonPrintPattern : public OpConversionPattern<triton::PrintOp> {\n   using OpConversionPattern<triton::PrintOp>::OpConversionPattern;\n \n@@ -623,9 +655,10 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n           TritonGenericPattern<triton::PtrToIntOp>,\n           TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n           TritonGenericPattern<triton::AddPtrOp>, TritonCatPattern,\n-          TritonReducePattern, TritonReduceReturnPattern, TritonTransPattern,\n-          TritonExpandDimsPattern, TritonMakeRangePattern, TritonDotPattern,\n-          TritonLoadPattern, TritonStorePattern,\n+          TritonReducePattern, TritonReduceReturnPattern, TritonScanPattern,\n+          TritonScanReturnPattern, TritonTransPattern, TritonExpandDimsPattern,\n+          TritonMakeRangePattern, TritonDotPattern, TritonLoadPattern,\n+          TritonStorePattern,\n           TritonExternElementwisePattern<triton::PureExternElementwiseOp>,\n           TritonExternElementwisePattern<triton::ImpureExternElementwiseOp>,\n           TritonPrintPattern, TritonAssertPattern, TritonAtomicRMWPattern,"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 30, "deletions": 0, "changes": 30, "file_content_changes": "@@ -559,6 +559,36 @@ llvm::SmallVector<Type> ReduceOp::getElementTypes() {\n \n unsigned ReduceOp::getNumOperands() { return this->getOperands().size(); }\n \n+//-- ScanOp --\n+void ScanOp::build(mlir::OpBuilder &builder, mlir::OperationState &state,\n+                   mlir::ValueRange operands, int axis) {\n+  SmallVector<Type> inferredReturnTypes;\n+  for (auto arg : operands)\n+    inferredReturnTypes.push_back(arg.getType());\n+  ReduceOp::build(builder, state, inferredReturnTypes, operands, axis);\n+}\n+\n+mlir::LogicalResult mlir::triton::ScanOp::inferReturnTypes(\n+    MLIRContext *context, std::optional<Location> location, ValueRange operands,\n+    DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,\n+    SmallVectorImpl<Type> &inferredReturnTypes) {\n+  for (auto arg : operands)\n+    inferredReturnTypes.push_back(arg.getType());\n+  return success();\n+}\n+\n+mlir::LogicalResult mlir::triton::ScanOp::verify() {\n+  if (this->getOperands().size() < 1) {\n+    return this->emitOpError() << \"must have at least 1 operand\";\n+  }\n+  for (const auto &operand : this->getOperands()) {\n+    if (!dyn_cast<RankedTensorType>(operand.getType())) {\n+      return this->emitOpError() << \"operands must be RankedTensorType\";\n+    }\n+  }\n+  return success();\n+}\n+\n //-- SplatOp --\n OpFoldResult SplatOp::fold(FoldAdaptor adaptor) {\n   auto value = adaptor.getSrc();"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "@@ -1412,6 +1412,21 @@ void init_triton_ir(py::module &&m) {\n              return self.create<mlir::triton::ReduceReturnOp>(loc,\n                                                               return_values);\n            })\n+      .def(\"create_scan\",\n+           [](mlir::OpBuilder &self, std::vector<mlir::Value> operands,\n+              int axis) -> mlir::OpState {\n+             auto loc = self.getUnknownLoc();\n+             return self.create<mlir::triton::ScanOp>(loc, operands, axis);\n+           })\n+      .def(\"create_scan_ret\",\n+           [](mlir::OpBuilder &self, py::args args) -> mlir::OpState {\n+             auto loc = self.getUnknownLoc();\n+             llvm::SmallVector<mlir::Value> return_values;\n+             for (const auto &arg : args) {\n+               return_values.push_back(py::cast<mlir::Value>(arg));\n+             }\n+             return self.create<mlir::triton::ScanReturnOp>(loc, return_values);\n+           })\n       .def(\"create_ptr_to_int\",\n            [](mlir::OpBuilder &self, mlir::Value &val,\n               mlir::Type &type) -> mlir::Value {"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 106, "deletions": 0, "changes": 106, "file_content_changes": "@@ -1494,6 +1494,112 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n             np.testing.assert_equal(z_ref, z_tri)\n \n \n+scan2d_shapes = [(16, 32), (32, 16), (2, 1024), (1024, 2), (32, 32), (1, 1024)]\n+\n+scan_configs = [\n+    (op, type, shape, 1)\n+    for type in ['int32', 'float32']\n+    for shape in scan2d_shapes\n+    for op in ['cumsum']\n+]\n+\n+\n+@pytest.mark.parametrize(\"op, dtype_str, shape, axis\", scan_configs)\n+def test_scan2d(op, dtype_str, shape, axis, device):\n+    check_type_supported(dtype_str, device)\n+\n+    # triton kernel\n+    @triton.jit\n+    def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexpr):\n+        range_m = tl.arange(0, BLOCK_M)\n+        range_n = tl.arange(0, BLOCK_N)\n+        x = tl.load(X + range_m[:, None] * BLOCK_N + range_n[None, :])\n+        z = GENERATE_TEST_HERE\n+        tl.store(Z + range_m[:, None] * BLOCK_N + range_n[None, :], z)\n+\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=1)'})\n+    # input\n+    rs = RandomState(17)\n+    x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n+    z = np.empty_like(x)\n+    x_tri = to_triton(x, device=device)\n+    numpy_op = {'cumsum': np.cumsum}[op]\n+    z_dtype_str = dtype_str\n+    z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n+    # triton result\n+    z_tri = to_triton(z, device=device)\n+    kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis)\n+    z_tri = to_numpy(z_tri)\n+    # compare\n+    if dtype_str == 'float32':\n+        np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n+    else:\n+        np.testing.assert_equal(z_ref, z_tri)\n+\n+\n+scan_layouts = [\n+    BlockedLayout([1, 4], [4, 8], [4, 1], [1, 0]),\n+    BlockedLayout([1, 4], [8, 4], [4, 1], [1, 0]),\n+    BlockedLayout([4, 1], [4, 8], [1, 4], [1, 0]),\n+    BlockedLayout([2, 2], [4, 8], [2, 2], [1, 0]),\n+    BlockedLayout([2, 2], [8, 4], [2, 2], [1, 0]),\n+]\n+\n+\n+@pytest.mark.parametrize(\"src_layout\", scan_layouts)\n+def test_scan_layouts(src_layout, device):\n+    ir = f\"\"\"\n+    #blocked = {src_layout}\n+    module attributes {{\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32}} {{\n+    tt.func public @kernel_0d1d(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n+      %cst = arith.constant dense<32> : tensor<32x1xi32, #blocked>\n+      %0 = tt.make_range {{end = 32 : i32, start = 0 : i32}} : tensor<32xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>\n+      %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<32xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>) -> tensor<32x1xi32, #blocked>\n+      %2 = arith.muli %1, %cst : tensor<32x1xi32, #blocked>\n+      %3 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<32x1x!tt.ptr<i32>, #blocked>\n+      %4 = tt.addptr %3, %2 : tensor<32x1x!tt.ptr<i32>, #blocked>, tensor<32x1xi32, #blocked>\n+      %5 = tt.make_range {{end = 32 : i32, start = 0 : i32}} : tensor<32xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>\n+      %6 = tt.expand_dims %5 {{axis = 0 : i32}} : (tensor<32xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>) -> tensor<1x32xi32, #blocked>\n+      %7 = tt.broadcast %4 : (tensor<32x1x!tt.ptr<i32>, #blocked>) -> tensor<32x32x!tt.ptr<i32>, #blocked>\n+      %8 = tt.broadcast %6 : (tensor<1x32xi32, #blocked>) -> tensor<32x32xi32, #blocked>\n+      %9 = tt.addptr %7, %8 : tensor<32x32x!tt.ptr<i32>, #blocked>, tensor<32x32xi32, #blocked>\n+      %10 = tt.load %9 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<32x32xi32, #blocked>\n+      %11 = \"tt.scan\"(%10) <{{axis = 1 : i32}}> ({{\n+      ^bb0(%arg2: i32, %arg3: i32):\n+        %16 = arith.addi %arg2, %arg3 : i32\n+        tt.scan.return %16 : i32\n+      }}) : (tensor<32x32xi32, #blocked>) -> tensor<32x32xi32, #blocked>\n+      %12 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<32x1x!tt.ptr<i32>, #blocked>\n+      %13 = tt.addptr %12, %2 : tensor<32x1x!tt.ptr<i32>, #blocked>, tensor<32x1xi32, #blocked>\n+      %14 = tt.broadcast %13 : (tensor<32x1x!tt.ptr<i32>, #blocked>) -> tensor<32x32x!tt.ptr<i32>, #blocked>\n+      %15 = tt.addptr %14, %8 : tensor<32x32x!tt.ptr<i32>, #blocked>, tensor<32x32xi32, #blocked>\n+      tt.store %15, %11 {{cache = 1 : i32, evict = 1 : i32}} : tensor<32x32xi32, #blocked>\n+      tt.return\n+    }}\n+    }}\n+    \"\"\"\n+\n+    import tempfile\n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.ttgir') as f:\n+        f.write(ir)\n+        f.flush()\n+        kernel = triton.compile(f.name)\n+    M = 32\n+    N = 32\n+    rs = RandomState(17)\n+    x = rs.randint(-100, 100, (M, N)).astype('int32')\n+\n+    z = np.zeros((M, N)).astype('int32')\n+    x_tri = torch.tensor(x, device=device)\n+    z_tri = torch.tensor(z, device=device)\n+\n+    kernel[(1, 1, 1)](x_tri, z_tri)\n+\n+    z_ref = np.cumsum(x, axis=1)\n+\n+    np.testing.assert_equal(z_ref, z_tri.cpu().numpy())\n+\n+\n layouts = [\n     BlockedLayout([1, 4], [8, 4], [4, 1], [1, 0]),\n     BlockedLayout([1, 4], [8, 4], [4, 1], [0, 1]),"}, {"filename": "python/triton/interpreter/tl_lang.py", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -623,3 +623,9 @@ def sum(self, input, axis=None):\n     @_tensor_operation\n     def xor_sum(self, input, axis):\n         raise NotImplementedError()\n+\n+    @_tensor_operation\n+    def cumsum(self, input, axis=None):\n+        if axis is None:\n+            return torch.cumsum(input)\n+        return torch.cumsum(input, dim=axis)"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -18,6 +18,7 @@\n     arange,\n     argmin,\n     argmax,\n+    associative_scan,\n     atomic_add,\n     atomic_and,\n     atomic_cas,\n@@ -33,6 +34,7 @@\n     cat,\n     constexpr,\n     cos,\n+    cumsum,\n     debug_barrier,\n     device_assert,\n     device_print,\n@@ -109,6 +111,7 @@\n     \"arange\",\n     \"argmin\",\n     \"argmax\",\n+    \"associative_scan\",\n     \"atomic_add\",\n     \"atomic_and\",\n     \"atomic_cas\",\n@@ -126,6 +129,7 @@\n     \"cdiv\",\n     \"constexpr\",\n     \"cos\",\n+    \"cumsum\",\n     \"debug_barrier\",\n     \"device_assert\",\n     \"device_print\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 61, "deletions": 0, "changes": 61, "file_content_changes": "@@ -1521,6 +1521,67 @@ def xor_sum(input, axis=None, _builder=None, _generator=None):\n                   _builder=_builder, _generator=_generator)\n \n \n+# -----------------------\n+# Scans\n+# -----------------------\n+\n+def _add_scan_docstr(name: str, return_indices_arg: str = None, tie_break_arg: str = None) -> Callable[[T], T]:\n+\n+    def _decorator(func: T) -> T:\n+        docstr = \"\"\"\n+    Returns the {name} of all elements in the :code:`input` tensor along the provided :code:`axis`\n+\n+    :param input: the input values\n+    :param axis: the dimension along which the scan should be done\"\"\"\n+        func.__doc__ = docstr.format(name=name)\n+        return func\n+\n+    return _decorator\n+\n+\n+@builtin\n+def associative_scan(input, axis, combine_fn, _builder=None, _generator=None):\n+    \"\"\"Applies the combine_fn to each elements with a carry in :code:`input` tensors along the provided :code:`axis` and update the carry\n+\n+    :param input: the input tensor, or tuple of tensors\n+    :param axis: the dimension along which the reduction should be done\n+    :param combine_fn: a function to combine two groups of scalar tensors (must be marked with @triton.jit)\n+\n+    \"\"\"\n+    if isinstance(input, tensor):\n+        return associative_scan((input,), axis, combine_fn,\n+                                _builder=_builder, _generator=_generator)[0]\n+\n+    def make_combine_region(scan_op):\n+        in_scalar_tys = [t.type.scalar for t in input]\n+        prototype = function_type(in_scalar_tys, in_scalar_tys * 2)\n+\n+        region = scan_op.get_region(0)\n+        with _insertion_guard(_builder):\n+            param_types = [ty.to_ir(_builder) for ty in prototype.param_types]\n+            block = _builder.create_block_with_parent(region, param_types)\n+            args = [tensor(block.arg(i), ty)\n+                    for i, ty in enumerate(prototype.param_types)]\n+            results = _generator.call_JitFunction(combine_fn, args, kwargs={})\n+            if isinstance(results, tensor):\n+                handles = [results.handle]\n+            else:\n+                handles = [r.handle for r in results]\n+            _builder.create_scan_ret(*handles)\n+    axis = _constexpr_to_value(axis)\n+    return semantic.associative_scan(input, axis, make_combine_region, _builder)\n+\n+# cumsum\n+\n+\n+@jit\n+@_add_scan_docstr(\"cumsum\")\n+def cumsum(input, axis=0):\n+    # todo rename this to a generic function name\n+    input = _promote_reduction_input(input)\n+    return associative_scan(input, axis, _sum_combine)\n+\n+\n # -----------------------\n # Compiler Hint Ops\n # -----------------------"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 26, "deletions": 0, "changes": 26, "file_content_changes": "@@ -1332,6 +1332,32 @@ def wrap_tensor(x, scalar_ty):\n     )\n \n \n+# ===----------------------------------------------------------------------===\n+#                               Associative Scan\n+# ===----------------------------------------------------------------------===\n+\n+\n+def associative_scan(\n+    inputs: Sequence[tl.tensor], axis: int, region_builder_fn, builder: ir.builder\n+) -> Tuple[tl.tensor, ...]:\n+    if len(inputs) != 1:\n+        raise ValueError(\"Current implementation only support single tensor input\")\n+    shape = inputs[0].type.shape\n+\n+    def wrap_tensor(x, scalar_ty):\n+        res_ty = tl.block_type(scalar_ty, shape)\n+        return tl.tensor(x, res_ty)\n+\n+    scan_op = builder.create_scan([t.handle for t in inputs], axis)\n+    region_builder_fn(scan_op)\n+    scan_op.verify()\n+\n+    return tuple(\n+        wrap_tensor(scan_op.get_result(i), inputs[i].type.scalar)\n+        for i in range(len(inputs))\n+    )\n+\n+\n # ===----------------------------------------------------------------------===\n #                               Math\n # ===----------------------------------------------------------------------==="}, {"filename": "test/Conversion/triton_ops.mlir", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -187,3 +187,19 @@ tt.func @print_no_arg(%arg0: !tt.ptr<f32>) {\n   tt.store %arg0, %0 {cache = 1 : i32, evict = 1 : i32} : f32\n   tt.return\n }\n+\n+// CHECK-LABEL: scan_op\n+tt.func @scan_op(%ptr: tensor<1x2x4x!tt.ptr<f32>>, %v : tensor<1x2x4xf32>) {\n+  // CHECK: tt.scan\n+  // CHECK-SAME: axis = 1\n+  // CHECK: tt.scan.return\n+  // CHECK-NEXT: (tensor<1x2x4xf32>) -> tensor<1x2x4xf32>\n+  %a = \"tt.scan\"(%v) <{axis = 1 : i32}>({\n+  ^bb0(%arg0: f32, %arg1: f32):\n+    %add = arith.addf %arg0, %arg1 : f32\n+    tt.scan.return %add : f32\n+  }) : (tensor<1x2x4xf32>) -> tensor<1x2x4xf32>\n+  tt.store %ptr, %a : tensor<1x2x4xf32>\n+  tt.return\n+\n+}"}]