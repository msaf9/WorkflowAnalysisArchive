[{"filename": "python/test/unit/language/assert_helper.py", "status": "modified", "additions": 78, "deletions": 1, "changes": 79, "file_content_changes": "@@ -22,6 +22,13 @@ def kernel_device_assert_scalar(X, Y, BLOCK: tl.constexpr):\n     tl.store(Y + tl.arange(0, BLOCK), x)\n \n \n+@triton.jit(debug=False)\n+def kernel_device_assert_no_debug(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    tl.device_assert(x == 0, \"x != 0\")\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+\n @triton.jit\n def kernel_assert(X, Y, BLOCK: tl.constexpr):\n     x = tl.load(X + tl.arange(0, BLOCK))\n@@ -43,12 +50,82 @@ def test_assert(func: str):\n     if func == \"device_assert\":\n         kernel_device_assert[(1,)](x, y, BLOCK=shape[0])\n         kernel_device_assert_scalar[(1,)](x, y, BLOCK=shape[0])\n+    elif func == \"no_debug\":\n+        # TRITON_DEBUG=True can override the debug flag\n+        kernel_device_assert_no_debug[(1,)](x, y, BLOCK=shape[0])\n     elif func == \"assert\":\n         kernel_assert[(1,)](x, y, BLOCK=shape[0])\n     elif func == \"static_assert\":\n         kernel_static_assert[(1,)](x, y, BLOCK=shape[0])\n     assert_close(y, x)\n \n \n+@triton.jit\n+def jit_device_assert_none(x):\n+    tl.device_assert(x == 0, \"x != 0\")\n+\n+\n+@triton.jit(debug=True)\n+def jit_device_assert_true(x):\n+    tl.device_assert(x == 0, \"x != 0\")\n+\n+\n+@triton.jit(debug=False)\n+def jit_device_assert_false(x):\n+    tl.device_assert(x == 0, \"x != 0\")\n+\n+\n+@triton.jit\n+def kernel_device_assert_nested(X, Y, BLOCK: tl.constexpr, jit_debug: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    if jit_debug == \"true\":\n+        jit_device_assert_true(x)\n+    elif jit_debug == \"false\":\n+        jit_device_assert_false(x)\n+    else:\n+        jit_device_assert_none(x)\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+\n+@triton.jit(debug=True)\n+def kernel_device_assert_nested_true(X, Y, BLOCK: tl.constexpr, jit_debug: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    if jit_debug == \"true\":\n+        jit_device_assert_true(x)\n+    elif jit_debug == \"false\":\n+        jit_device_assert_false(x)\n+    else:\n+        jit_device_assert_none(x)\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+\n+@triton.jit(debug=False)\n+def kernel_device_assert_nested_false(X, Y, BLOCK: tl.constexpr, jit_debug: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    if jit_debug == \"true\":\n+        jit_device_assert_true(x)\n+    elif jit_debug == \"false\":\n+        jit_device_assert_false(x)\n+    else:\n+        jit_device_assert_none(x)\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+\n+def test_assert_nested(caller: str, callee: str):\n+    shape = (128, )\n+    x = torch.arange(0, shape[0], dtype=torch.int32, device='cuda')\n+    y = torch.zeros(shape, dtype=x.dtype, device=\"cuda\")\n+    if caller == \"none\":\n+        kernel_device_assert_nested[(1,)](x, y, BLOCK=shape[0], jit_debug=callee)\n+    elif caller == \"true\":\n+        kernel_device_assert_nested_true[(1,)](x, y, BLOCK=shape[0], jit_debug=callee)\n+    elif caller == \"false\":\n+        kernel_device_assert_nested_false[(1,)](x, y, BLOCK=shape[0], jit_debug=callee)\n+    assert_close(y, x)\n+\n+\n if __name__ == \"__main__\":\n-    test_assert(sys.argv[1])\n+    if len(sys.argv) == 3:\n+        test_assert_nested(sys.argv[1], sys.argv[2])\n+    else:\n+        test_assert(sys.argv[1])"}, {"filename": "python/test/unit/language/test_subprocess.py", "status": "modified", "additions": 28, "deletions": 1, "changes": 29, "file_content_changes": "@@ -9,7 +9,8 @@\n assert_path = os.path.join(dir_path, \"assert_helper.py\")\n \n # TODO: bfloat16 after LLVM-15\n-func_types = [\"device_assert\", \"assert\", \"static_assert\"]\n+func_types = [\"device_assert\", \"assert\", \"static_assert\", \"no_debug\"]\n+nested_types = [(caller, callee) for caller in [\"true\", \"false\", \"none\"] for callee in [\"true\", \"false\", \"none\"]]\n torch_types = [\"int8\", \"uint8\", \"int16\", \"int32\", \"long\", \"float16\", \"float32\", \"float64\"]\n \n \n@@ -51,3 +52,29 @@ def test_assert(func_type: str):\n         assert num_errs == 127\n     else:\n         assert num_errs == 0\n+\n+\n+@pytest.mark.parametrize(\"caller_type, callee_type\", nested_types)\n+def test_assert_nested(caller_type, callee_type):\n+    proc = subprocess.Popen([sys.executable, assert_path, caller_type, callee_type], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=False)\n+    _, errs = proc.communicate()\n+    errs = errs.splitlines()\n+    num_errs = 0\n+    for err in errs:\n+        if \"x != 0\" in err.decode(\"utf-8\"):\n+            num_errs += 1\n+    if caller_type == \"none\":\n+        if callee_type == \"true\":\n+            assert num_errs == 127\n+        else:\n+            assert num_errs == 0\n+    elif caller_type == \"true\":\n+        if callee_type == \"false\":\n+            assert num_errs == 0\n+        else:\n+            assert num_errs == 127\n+    elif caller_type == \"false\":\n+        if callee_type == \"true\":\n+            assert num_errs == 127\n+        else:\n+            assert num_errs == 0"}, {"filename": "python/test/unit/runtime/test_cache.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -160,12 +160,12 @@ def kernel_add(a, b, o, N: tl.constexpr):\n     assert len(kernel_add.cache[device]) == 1\n     kernel_add.debug = False\n     kernel_add.warmup(torch.float32, torch.float32, torch.float32, 32, grid=(1,))\n-    assert len(kernel_add.cache[device]) == 1\n+    assert len(kernel_add.cache[device]) == 2\n     kernel_add.debug = True\n     kernel_add.warmup(torch.float32, torch.float32, torch.float32, 32, grid=(1,))\n-    assert len(kernel_add.cache[device]) == 2\n+    assert len(kernel_add.cache[device]) == 3\n     bins = list(kernel_add.cache[device].values())\n-    assert bins[0].asm['ttir'] != bins[1].asm['ttir']\n+    assert bins[2].asm['ttir'] != bins[1].asm['ttir']\n \n \n @triton.jit"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -859,7 +859,9 @@ def call_JitFunction(self, fn: JITFunction, args, kwargs):\n         if not self.module.has_function(fn_name):\n             prototype = language.function_type([], arg_types)\n             gscope = sys.modules[fn.fn.__module__].__dict__\n-            generator = CodeGenerator(self.builder.context, prototype, gscope, attributes, constants, module=self.module, function_name=fn_name, function_types=self.function_ret_types, debug=fn.debug, noinline=fn.noinline)\n+            # If the callee is not set, we use the same debug setting as the caller\n+            debug = self.debug if fn.debug is None else fn.debug\n+            generator = CodeGenerator(self.builder.context, prototype, gscope, attributes, constants, module=self.module, function_name=fn_name, function_types=self.function_ret_types, debug=debug, noinline=fn.noinline)\n             generator.visit(fn.parse())\n             callee_ret_type = generator.last_ret_type\n             self.function_ret_types[fn_name] = callee_ret_type"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -356,7 +356,7 @@ def __init__(self, fn, version=None, do_not_specialize=None, debug=None, noinlin\n         # when called with a grid using __getitem__\n         self.kernel_decorators = []\n         self.kernel = None\n-        self.debug = os.environ.get(\"TRITON_DEBUG\", \"0\") == \"1\" if debug is None else debug\n+        self.debug = True if os.environ.get(\"TRITON_DEBUG\", \"0\") == \"1\" else debug\n         self.noinline = noinline\n         # annotations\n         normalize_ty = lambda ty: ty.__name__ if isinstance(ty, type) else ty"}]