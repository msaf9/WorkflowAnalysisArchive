[{"filename": "include/triton/codegen/analysis/layout.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -224,7 +224,7 @@ struct scanline_layout: public distributed_layout {\n   int nts(size_t k) { return nts_.at(k); }\n   int contig_per_thread(size_t k) { return nts_.at(k); }\n \n-  int per_thread(size_t k) { return nts(k) * shape_[k] / shape_per_cta(k);}\n+  int per_thread(size_t k) { return contig_per_thread(k) * shape_[k] / shape_per_cta(k);}\n public:\n   // micro tile size. The size of a tile held by a thread block.\n   std::vector<int> mts_;"}, {"filename": "lib/codegen/analysis/align.cc", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -319,8 +319,8 @@ std::vector<unsigned> align::populate_max_contiguous_binop(ir::binary_operator*\n     }\n     if(x->is_int_add_sub()){\n       unsigned lvalue = 1, rvalue = 1;\n-      lvalue = gcd(rhs_max_contiguous[d], lhs_starting_multiple[d]);\n-      rvalue = gcd(lhs_max_contiguous[d], rhs_starting_multiple[d]);\n+      lvalue = gcd(rhs_max_contiguous[d], lhs_cst_info[d].num_cst);\n+      rvalue = gcd(lhs_max_contiguous[d], rhs_cst_info[d].num_cst);\n       value = std::max(lvalue, rvalue);\n     }\n     result.push_back(value);"}, {"filename": "lib/codegen/analysis/layout.cc", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -209,14 +209,15 @@ mma_layout::mma_layout(size_t num_warps,\n     rep_ = {2*pack_size_0, 2*pack_size_1, 1};\n     spw_ = {fpw_[0]*4*rep_[0], fpw_[1]*4*rep_[1], 1};\n     contig_per_thread_ = {1, 1};\n+    order_ = {0, 1};\n   }\n   else{\n     // fpw_ = {1, 1, 1};\n     spw_ = mma_instr_shape_.at(tensor_core_type_); // e.g., {16, 8, 16} for f32.f16.f16.f32\n     contig_per_thread_ = {1, 2};\n+    order_ = {1, 0};\n     // rep_ = {2,  2, 1};\n   }\n-  order_ = {0, 1};\n \n   /* warps per tile */\n   wpt_ = {1, 1, 1};\n@@ -616,8 +617,9 @@ void layouts::run(ir::module &mod) {\n       unsigned axis = red->get_axis();\n       // shape\n       auto shapes = arg->get_type()->get_block_shapes();\n-      scanline_layout *layout = get(arg)->to_scanline();\n-      shapes[axis] = layout->mts(axis);\n+      distributed_layout* layout = dynamic_cast<analysis::distributed_layout*>(get(arg));\n+      shapes[axis] = layout->shape_per_cta(axis) / layout->contig_per_thread(axis);\n+      \n       // create layout\n       layouts_[id] = new shared_layout(layout, axes_->get(arg), shapes, {red}, red->get_type()->get_scalar_ty(), align_, tgt_);\n       tmp_[red] = id;"}, {"filename": "lib/codegen/pass.cc", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -88,6 +88,7 @@ std::unique_ptr<llvm::Module> add_passes_to_emit_bin(ir::module &ir, llvm::LLVMC\n   allocation.run(ir);\n   prefetch_s.run(ir);\n   barriers.run(ir);\n+  // ir.print(std::cout);\n   isel.visit(ir, *llvm);\n   shared_static = allocation.allocated_size();\n   return llvm;"}, {"filename": "lib/codegen/selection/generator.cc", "status": "modified", "additions": 114, "deletions": 56, "changes": 170, "file_content_changes": "@@ -88,6 +88,7 @@ Value* geper::operator()(Value *ptr, Value* off, const std::string& name){\n #define f16_ty               builder_->getHalfTy()\n #define bf16_ty              builder_->getBFloatTy()\n #define f32_ty               builder_->getFloatTy()\n+#define i1_ty                builder_->getInt1Ty()\n #define i8_ty                builder_->getInt8Ty()\n #define i16_ty               builder_->getInt16Ty()\n #define i32_ty               builder_->getInt32Ty()\n@@ -736,6 +737,9 @@ void generator::visit_uncond_branch_inst(ir::uncond_branch_inst* br) {\n  * \\brief Code Generation for a (synchronous) `load`\n  */\n void generator::visit_load_inst(ir::load_inst* x){\n+  BasicBlock *current = builder_->GetInsertBlock();\n+  Module *module = current->getModule();\n+  Value *tid = tgt_->get_local_id(module, *builder_, 0);\n   ir::value *op = x->get_pointer_operand();\n   ir::masked_load_inst *mx = dynamic_cast<ir::masked_load_inst*>(x);\n   Type* ty  = cvt(op->get_type()->get_scalar_ty()->get_pointer_element_ty());\n@@ -775,6 +779,9 @@ void generator::visit_load_inst(ir::load_inst* x){\n         in_off = 0;\n     }\n     Value *pred = mx ? vals_[mx->get_mask_operand()][idx] : builder_->getTrue();\n+    // if(!op->get_type()->is_block_ty()){\n+    //   pred = builder_->CreateAnd(pred, icmp_eq(tid, i32(0)));\n+    // }\n     Value *other = mx ? vals_[mx->get_false_value_operand()][idx] : nullptr;\n     size_t nbits = dtsize*8;\n     // pack sub-words (< 32/64bits) into words\n@@ -878,6 +885,18 @@ void generator::visit_load_inst(ir::load_inst* x){\n   \n     \n     Value *_ret = call(inlineAsm, args);\n+    // if(!op->get_type()->is_block_ty()){\n+    //   Value* cond = icmp_eq(tid, i32(0));\n+    //   Value* shptr = bit_cast(shmem_, ptr_ty(_ret->getType(), 3));\n+    //   Instruction* bar = add_barrier();\n+    //   Instruction *term = llvm::SplitBlockAndInsertIfThen(cond, bar, false);\n+    //   builder_->SetInsertPoint(term);\n+    //   store(_ret, shptr);\n+    //   builder_->SetInsertPoint(bar->getParent());\n+    //   _ret = load(shptr);\n+    //   add_barrier();\n+    // }\n+      \n     // ---\n     // extract and store return values\n     // ---\n@@ -2033,12 +2052,12 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n \n   // create mma & unpack result, m, n, k are offsets in mat\n   auto call_mma = [&](unsigned m, unsigned n, unsigned k) {\n-      unsigned cols_per_thread = num_rep_m * 2;\n+      unsigned cols_per_thread = num_rep_n * 2;\n       std::vector<size_t> idx = {\n-        (m + 0) + (n*2 + 0)*cols_per_thread,\n-        (m + 0) + (n*2 + 1)*cols_per_thread,\n-        (m + 1) + (n*2 + 0)*cols_per_thread,\n-        (m + 1) + (n*2 + 1)*cols_per_thread\n+        (m + 0)*cols_per_thread + (n*2 + 0),\n+        (m + 0)*cols_per_thread + (n*2 + 1),\n+        (m + 1)*cols_per_thread + (n*2 + 0),\n+        (m + 1)*cols_per_thread + (n*2 + 1)\n       };\n       Value *nc = call(mma_ty, mma_fn, \n                        {ha[{m, k}], ha[{m+1, k}], ha[{m, k+1}], ha[{m+1, k+1}],\n@@ -2316,62 +2335,93 @@ inline Value* generator::shfl_sync(Value* acc, int32_t i){\n void generator::visit_reducend_inst_fast(ir::reduce_inst* x, std::function<Value*(Value*,Value*)> do_acc, Value *neutral){\n   //\n   ir::value *arg = x->get_operand(0);\n-  analysis::scanline_layout* layout = layouts_->get(arg)->to_scanline();\n+  analysis::distributed_layout* layout = dynamic_cast<analysis::distributed_layout*>(layouts_->get(arg));\n   std::vector<unsigned> shapes = layout->get_shape();\n-  std::vector<int> order = layout->get_order();\n-  unsigned mts = layout->mts(order[0]);\n-  unsigned nts = layout->nts(order[0]);\n-  unsigned col_per_thread = shapes[order[0]] / mts;\n-  auto idxs = idxs_.at(arg);\n-  size_t n_elts = idxs.size();\n-  //\n-  Type *ret_ty = cvt(x->get_type()->get_scalar_ty());\n-  unsigned addr_space = shmem_->getType()->getPointerAddressSpace();\n-  Value *base = bit_cast(shmem_, ptr_ty(ret_ty, addr_space));\n+\n+  Type* sca_ty = cvt(arg->get_type()->get_scalar_ty());\n+  size_t n_bits = sca_ty->getPrimitiveSizeInBits();\n+\n+  std::string n_bits_str = std::to_string(n_bits);\n+  std::string cst = (n_bits == 64) ? \"l\" : \"r\";\n+\n+  FunctionType *st_shared_ty = FunctionType::get(void_ty, {i1_ty, ptr_ty(sca_ty, 3), sca_ty}, false);\n+  InlineAsm *st_shared = InlineAsm::get(st_shared_ty, \"@$0 st.shared.b\" + n_bits_str + \" [$1], $2;\", \"b,\" + cst + \",\" + cst, true);\n+  FunctionType *ld_shared_ty = FunctionType::get(sca_ty, {i1_ty, ptr_ty(sca_ty, 3)}, false);\n+  InlineAsm *ld_shared = InlineAsm::get(ld_shared_ty, \"@$1 ld.shared.b\" + n_bits_str + \" $0, [$2];\", \"=\" + cst + \",b,\" + cst, true);\n+\n+\n   Value* thread = tgt_->get_local_id(mod_, *builder_, 0);\n   Value* warp = udiv(thread, i32(32));\n   Value* lane = urem(thread, i32(32));\n-  size_t warps_per_inner = std::max<int>(mts/32, 1);\n-  Value* warp_i = udiv(warp, i32(warps_per_inner));\n-  unsigned row_per_thread = std::max<int>(32/mts, 1);\n \n+  unsigned shuffle_width = 0;\n+  unsigned warps_per_inner = 0;\n+  auto arg_vals = vals_.at(arg);\n+  std::vector<indices_t> arg_idxs = idxs_.at(arg);\n+  size_t n_elts = arg_idxs.size();\n+  unsigned col_per_thread;\n+  Value* warp_i;\n+  Value* warp_j;\n+  if(analysis::scanline_layout* scanline = layout->to_scanline()){\n+    std::vector<int> order = layout->get_order();\n+    unsigned mts = scanline->mts(order[0]);\n+    shuffle_width = std::min<int>(mts, 32);\n+    warps_per_inner = std::max<int>(mts/32, 1);\n+    col_per_thread = shapes[order[0]] / mts;\n+    warp_i = udiv(warp, i32(warps_per_inner));\n+    warp_j = urem(warp, i32(warps_per_inner));\n+  }\n+  else if(layout->to_mma()){\n+    shuffle_width = 4; \n+    warps_per_inner = layout->to_mma()->wpt(1);\n+    col_per_thread = 16;\n+    warp_i = axes_.at(a_axes_->get(arg, 0)).thread_id;\n+    warp_j = axes_.at(a_axes_->get(arg, 1)).thread_id;\n+  }\n+\n+  // unsigned col_per_thread = 2 * shapes[order[0]] / layout->shape_per_cta(order[0]);\n+  //\n+  Type *ret_ty = cvt(x->get_type()->get_scalar_ty());\n+  unsigned addr_space = shmem_->getType()->getPointerAddressSpace();\n+  Value *base = bit_cast(shmem_, ptr_ty(ret_ty, addr_space));\n+  // preds\n+  Value* is_lane0 = icmp_eq(lane, i32(0));\n+  Value* is_warp0 = icmp_eq(warp, i32(0));\n+  Value* is_thread0 = icmp_eq(thread, i32(0));\n+  Value* lane_j = urem(lane, i32(shuffle_width));\n+  Value* first_lane_in_col = icmp_eq(lane_j, i32(0));\n+  add_barrier();\n+  // compute partial sum for each warp, and store to shared memory\n   for(size_t i = 0; i < n_elts/col_per_thread; i++){\n     Value* acc;\n     // reduce within thread\n     for(size_t j = 0; j < col_per_thread; j++){\n-      Value* val = vals_[arg][idxs[i*col_per_thread + j]];\n+      Value* val = arg_vals[arg_idxs[i*col_per_thread + j]];\n+      // acc = (j == 0) ? val : do_acc(acc, val);\n       acc = (j == 0) ? val : do_acc(acc, val);\n     }\n     // reduce within warp\n-    for(int k = std::min<int>(mts, 32)/2 ; k > 0; k >>= 1)\n+    for(int k = shuffle_width/2 ; k > 0; k >>= 1)\n       acc = do_acc(acc, shfl_sync(acc, k));\n-    // store warp result in shared memory\n-    Value* ret = acc;\n-    if(mts >= 32){\n-      add_barrier();\n-      store(neutral, gep(base, lane));\n-      add_barrier();\n-      store(acc, gep(base, warp));\n-      add_barrier();\n-      // reduce across warps\n-      Value *cond = icmp_eq(warp, i32(0));\n-      Instruction *barrier = add_barrier();\n-      builder_->SetInsertPoint(barrier->getParent());\n-      Instruction* dummy = builder_->CreateRet(nullptr);\n-      Instruction *term = llvm::SplitBlockAndInsertIfThen(cond, barrier, false);\n-      dummy->removeFromParent();\n-      builder_->SetInsertPoint(term);\n-      ret = load(gep(base, thread));\n-      for(int k = (mts/32)/2; k > 0; k >>= 1){\n-        Value *current = shfl_sync(ret, k);\n-        ret = do_acc(ret, current);\n-      }\n-      store(ret, gep(base, thread));\n-      builder_->SetInsertPoint(barrier->getParent());\n-      ret = load(gep(base, warp));\n-    }\n-    vals_[x][idxs_[x][i]] = ret;\n+    // store partial result to shared memory\n+    auto x_idxs = idxs_[x][i];\n+    Value* x_idx = x_idxs.empty() ? builder_->getInt32(0) : x_idxs[0];\n+    Value* st_off = add(mul(x_idx, i32(warps_per_inner)), warp_j);\n+    call(st_shared, {icmp_eq(lane_j, i32(0)), gep(base, st_off), acc});\n   }\n+  add_barrier();\n+  // at this point, partial accumulator synchronized in shared memory\n+  // Just need to reduce `warp_per_inner` numbers in shared memory\n+  for(size_t i = 0; i < n_elts/col_per_thread; i++){\n+    auto x_idxs = idxs_[x][i];\n+    Value* x_idx = x_idxs.empty() ? builder_->getInt32(0) : x_idxs[0];\n+    Value* ld_off = add(mul(x_idx, i32(warps_per_inner)), urem(lane_j, i32(warps_per_inner)));\n+    Value* acc = call(ld_shared, {builder_->getInt1(true), gep(base, ld_off)});\n+    for(int k = warps_per_inner/2; k > 0; k >>= 1)\n+      acc = do_acc(acc, shfl_sync(acc, k));\n+    vals_[x][idxs_[x][i]] = acc;\n+  }\n+  // add_barrier();\n }\n \n void generator::visit_reducend_inst(ir::reduce_inst* x, std::function<Value*(Value*,Value*)> do_acc, Value *neutral) {\n@@ -2471,8 +2521,12 @@ void generator::visit_reduce_inst(ir::reduce_inst* x) {\n     default: throw std::runtime_error(\"unreachable\");\n   }\n   ir::value *arg = x->get_operand(0);\n+  int cc = tgt_->as_nvidia()->sm();\n   analysis::scanline_layout* scanline = layouts_->get(x->get_operand(0))->to_scanline();\n-  if(scanline && scanline->get_order()[0] == x->get_axis())\n+  analysis::mma_layout* mma = layouts_->get(x->get_operand(0))->to_mma();\n+  bool is_coalesced_scanline = scanline && (scanline->get_order()[0] == x->get_axis());\n+  bool is_a100_mma = mma && (cc >= 80) && (x->get_axis() == 1);\n+  if(is_coalesced_scanline || is_a100_mma)\n     visit_reducend_inst_fast(x, do_acc, neutral);\n   else\n     visit_reducend_inst(x, do_acc, neutral);\n@@ -2665,21 +2719,24 @@ void generator::visit_copy_to_shared_inst(ir::copy_to_shared_inst* cts) {\n   unsigned in_vec = 1;\n   ir::value *arg = cts->get_operand(0);\n   analysis::shared_layout* out_layout = layouts_->get(cts)->to_shared();\n-  analysis::scanline_layout* in_layout = layouts_->get(arg)->to_scanline();\n+  analysis::distributed_layout* in_layout = dynamic_cast<analysis::distributed_layout*>(layouts_->get(arg));\n   auto out_order = out_layout->get_order();\n   auto in_order = in_layout->get_order();\n   // tiles\n   if(out_order == in_order)\n-    in_vec = in_layout->nts(in_order[0]);\n+    in_vec = in_layout->contig_per_thread(in_order[0]);\n   int out_vec = swizzle_->get_vec(out_layout);\n   int min_vec = std::min<int>(out_vec, in_vec);\n   int s = std::max<int>(out_vec / in_vec, 1);\n   //\n   int per_phase = swizzle_->get_per_phase(out_layout);\n   int max_phase = swizzle_->get_max_phase(out_layout);\n   //\n-  int in_ld = in_layout->get_shape()[in_order[0]] / in_layout->mts(in_order[0]);\n-  int n_shared_1 = std::max<int>(per_phase*max_phase / in_layout->mts(in_order[1]), 1);\n+  int mts_0 = in_layout->shape_per_cta(in_order[0]) / in_layout->contig_per_thread(in_order[0]);\n+  int mts_1 = in_layout->shape_per_cta(in_order[1]) / in_layout->contig_per_thread(in_order[1]);\n+\n+  int in_ld = in_layout->get_shape()[in_order[0]] / mts_0;\n+  int n_shared_1 = std::max<int>(per_phase*max_phase / mts_1, 1);\n   int n_shared_0 = std::max<int>(in_vec    / out_vec, 1);\n \n   BasicBlock* CurrBB = builder_->GetInsertBlock();\n@@ -2700,8 +2757,8 @@ void generator::visit_copy_to_shared_inst(ir::copy_to_shared_inst* cts) {\n       // input ptr info\n       int id_0 = id % (in_ld/min_vec);\n       int id_1 = id / (in_ld/min_vec);\n-      int off_0 = id_0 / n_shared_0 * n_shared_0 * in_layout->mts(in_order[0]);\n-      int off_1 = id_1 / n_shared_1 * n_shared_1 * in_layout->mts(in_order[1]);\n+      int off_0 = id_0 / n_shared_0 * n_shared_0 * mts_0;\n+      int off_1 = id_1 / n_shared_1 * n_shared_1 * mts_1;\n       int off = (off_1*shapes[in_order[0]] + off_0);\n       std::pair<int, int> key = {id_1  % n_shared_1, id_0 % n_shared_0};\n       if(ptrs.find(key) == ptrs.end()){\n@@ -3026,8 +3083,7 @@ void generator::visit_layout_mma(analysis::mma_layout* layout) {\n   else{\n     /* warp offset */\n     Value *warp_0 = urem(warp, i32(layout->wpt(0)));\n-    Value *warp_12 = udiv(warp, i32(layout->wpt(0)));\n-    Value *warp_1 = urem(warp_12, i32(layout->wpt(1)));\n+    Value *warp_1 = urem(udiv(warp, i32(layout->wpt(0))), i32(layout->wpt(1)));\n     Value *off_warp_m = mul(warp_0, i32(layout->spw(0)));\n     Value *off_warp_n = mul(warp_1, i32(layout->spw(1)));\n     Value *off_lane_m = urem(lane, _16);\n@@ -3152,7 +3208,9 @@ void generator::visit_basic_block(ir::basic_block * block) {\n   BasicBlock *parent = bbs_[block];\n   builder_->SetInsertPoint(parent);\n   for(ir::instruction *i: block->get_inst_list()){\n+    // i->print(std::cout);\n     visit_value(i);\n+    // std::cout << \"done\" << std::endl;\n   }\n   // Update ir bb -> llvm bb mapping\n   bbs_[block] = builder_->GetInsertBlock();"}, {"filename": "lib/codegen/transform/coalesce.cc", "status": "modified", "additions": 19, "deletions": 0, "changes": 19, "file_content_changes": "@@ -52,6 +52,7 @@ coalesce::coalesce(analysis::align* align, analysis::layouts *layouts)\n //}\n \n void coalesce::run(ir::module &mod) {\n+  std::set<analysis::data_layout*> invalidated;\n   ir::builder& builder = mod.get_builder();\n   // add layout conversion instructions\n   for(ir::function *fn: mod.get_function_list())\n@@ -61,12 +62,29 @@ void coalesce::run(ir::module &mod) {\n     if(dynamic_cast<ir::store_inst*>(i) || dynamic_cast<ir::atomic_rmw_inst*>(i))\n     if(ir::value* op = i->get_operand(1))\n     if(op->get_type()->is_block_ty())\n+    if(op->get_type()->get_tile_rank() == 2)\n+    if(invalidated.find(layout_->get(op)) == invalidated.end())\n     if(layout_->get(op)->to_mma()){\n       ir::instruction* new_op = ir::cvt_layout_inst::create(op);\n       builder.set_insert_point(i);\n       builder.insert(new_op);\n       i->replace_uses_of_with(op, new_op);\n     }\n+    // coalesce before copy_to_shared\n+    // It's dirty, but the backend is being rewritten from scratch. :)\n+    if(dynamic_cast<ir::copy_to_shared_inst*>(i))\n+    if(ir::value* op = i->get_operand(0))\n+    if(op->get_type()->is_block_ty())\n+    if(op->get_type()->get_tile_rank() == 2)\n+    if(invalidated.find(layout_->get(op)) == invalidated.end())\n+    if(layout_->get(op)->to_mma()){\n+      ir::instruction* new_op = ir::cvt_layout_inst::create(op);\n+      builder.set_insert_point(i);\n+      builder.insert(new_op);\n+      op->replace_all_uses_with(new_op);\n+      new_op->replace_uses_of_with(new_op, op);\n+      invalidated.insert(layout_->get(op));\n+    }\n     // uncoalesce after load\n     if(auto x = dynamic_cast<ir::load_inst*>(i))\n     if(x->get_type()->is_block_ty())\n@@ -120,6 +138,7 @@ void coalesce::run(ir::module &mod) {\n       }\n       if(in_contig.size() <= 1 || out_contig==in_contig)\n         continue;\n+      std::cout << \"3!!\" << std::endl;\n       builder.set_insert_point_after(val_inst);\n       auto new_val = builder.insert(ir::cvt_layout_inst::create(val_inst));\n       x->replace_uses_of_with(val_inst, new_val);"}, {"filename": "python/setup.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -79,7 +79,7 @@ def run(self):\n \n     def build_extension(self, ext):\n         llvm_include_dir, llvm_library_dir = get_llvm()\n-        # self.debug = True\n+        self.debug = True\n         extdir = os.path.abspath(os.path.dirname(self.get_ext_fullpath(ext.path)))\n         # create build directories\n         build_suffix = 'debug' if self.debug else 'release'"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 23, "deletions": 0, "changes": 23, "file_content_changes": "@@ -698,6 +698,7 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n \n     rs = RandomState(17)\n     x = numpy_random((shape,), dtype_str=dtype_str, rs=rs)\n+    x[:] = 1\n     # numpy result\n     z_ref = np.sum(x).astype(getattr(np, dtype_str))\n     # triton result\n@@ -1132,3 +1133,25 @@ def kernel(X):\n     x_tri = to_triton(np.empty((256, ), dtype=np.int32))\n     kernel[(1,)](x_tri)\n     np.testing.assert_equal(to_numpy(x_tri), np.arange(0, 256))\n+\n+# -------------\n+# test if\n+# -------------\n+\n+\n+def test_if():\n+\n+    @triton.jit\n+    def kernel(Cond, XTrue, XFalse, Ret):\n+        pid = tl.program_id(0)\n+        cond = tl.load(Cond)\n+        if pid % 2:\n+            tl.store(Ret, tl.load(XTrue))\n+        else:\n+            tl.store(Ret, tl.load(XFalse))\n+\n+    cond = torch.ones(1, dtype=torch.int32, device='cuda')\n+    x_true = torch.tensor([3.14], dtype=torch.float32, device='cuda')\n+    x_false = torch.tensor([1.51], dtype=torch.float32, device='cuda')\n+    ret = torch.empty(1, dtype=torch.float32, device='cuda')\n+    kernel[(1,)](cond, x_true, x_false, ret)"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -32,6 +32,8 @@ def _to_tensor(x, builder):\n         return _to_tensor(x.value, builder)\n     elif isinstance(x, tensor):\n         return x\n+    elif x is None:\n+        return None\n     assert False, f'cannot convert {x} to tensor'\n \n "}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -559,7 +559,7 @@ def cast(input: tl.tensor,\n          dst_ty: tl.dtype,\n          builder: ir.builder) -> tl.tensor:\n     src_ty = input.type\n-    if src_ty.is_block():\n+    if src_ty.is_block() and not dst_ty.is_block():\n         dst_ty = tl.block_type(dst_ty, input.type.get_block_shapes())\n     if src_ty == dst_ty:\n         return input"}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -252,6 +252,7 @@ def matmul_kernel(\n # we can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`\n @triton.jit\n def leaky_relu(x):\n+    x = x + 1\n     return tl.where(x >= 0, x, 0.01 * x)\n \n \n@@ -296,7 +297,7 @@ def matmul(a, b, activation=None):\n torch.manual_seed(0)\n a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n-triton_output = matmul(a, b, activation=None)\n+triton_output = matmul(a, b, activation=leaky_relu)\n torch_output = torch.matmul(a, b)\n print(f\"triton_output={triton_output}\")\n print(f\"torch_output={torch_output}\")\n@@ -305,6 +306,8 @@ def matmul(a, b, activation=None):\n else:\n     print(\"\u274c Triton and Torch differ\")\n \n+print(matmul_kernel.cache_key)\n+exit()\n # %%\n # Benchmark\n # --------------"}]