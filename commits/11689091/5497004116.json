[{"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -49,7 +49,8 @@ SmallVector<unsigned> getContigPerThread(Attribute layout);\n // for thread 0 would be [A_{0, 0}, A_{0, 0}, A_{0, 0}, A_{0, 0}], returns [1,\n // 1]. Whereas for a tensor shape [128, 128], the elements for thread 0 would be\n // [A_{0, 0}, A_{0, 1}, A_{0, 2}, A_{0, 3}], returns [1, 4].\n-SmallVector<unsigned> getUniqueContigPerThread(Type type);\n+SmallVector<unsigned> getUniqueContigPerThread(Attribute layout,\n+                                               ArrayRef<int64_t> tensorShape);\n \n // Returns the number of threads per warp that have access to non-replicated\n // elements of the tensor. E.g. for a blocked layout with sizePerThread = [1,"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -918,7 +918,8 @@ unsigned ModuleAxisInfoAnalysis::getPtrContiguity(Value ptr) {\n   auto order = triton::gpu::getOrder(layout);\n   unsigned align = getPtrAlignment(ptr);\n \n-  auto uniqueContigPerThread = triton::gpu::getUniqueContigPerThread(tensorTy);\n+  auto uniqueContigPerThread =\n+      triton::gpu::getUniqueContigPerThread(layout, tensorTy.getShape());\n   assert(order[0] < uniqueContigPerThread.size() &&\n          \"Unxpected uniqueContigPerThread size\");\n   unsigned contiguity = uniqueContigPerThread[order[0]];"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -60,7 +60,9 @@ unsigned ReduceOpHelper::getInterWarpSizeWithUniqueData() {\n \n unsigned ReduceOpHelper::getIntraWarpSizeWithUniqueData() {\n   auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n-  return std::min(srcReduceDimSize,\n+  unsigned elementPerThreads = triton::gpu::getUniqueContigPerThread(\n+      getSrcLayout(), getSrcShape())[axis];\n+  return std::min(srcReduceDimSize / elementPerThreads,\n                   triton::gpu::getThreadsPerWarpWithUniqueData(\n                       getSrcLayout(), getSrcShape())[axis]);\n }"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 6, "deletions": 11, "changes": 17, "file_content_changes": "@@ -221,28 +221,23 @@ SmallVector<unsigned> getContigPerThread(Attribute layout) {\n   }\n }\n \n-SmallVector<unsigned> getUniqueContigPerThread(Type type) {\n-  if (type.isIntOrIndexOrFloat() || type.isa<triton::PointerType>())\n-    return SmallVector<unsigned>(1, 1);\n-  auto tensorType = type.cast<RankedTensorType>();\n-  auto shape = tensorType.getShape();\n+SmallVector<unsigned> getUniqueContigPerThread(Attribute layout,\n+                                               ArrayRef<int64_t> shape) {\n   // If slice layout, call recursively on parent layout, and drop\n   // sliced dim\n-  if (auto sliceLayout =\n-          tensorType.getEncoding().dyn_cast<SliceEncodingAttr>()) {\n+  if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     auto parentLayout = sliceLayout.getParent();\n     auto parentShape = sliceLayout.paddedShape(shape);\n-    auto parentTy = RankedTensorType::get(\n-        parentShape, tensorType.getElementType(), parentLayout);\n-    auto parentUniqueContigPerThread = getUniqueContigPerThread(parentTy);\n+    auto parentUniqueContigPerThread =\n+        getUniqueContigPerThread(parentLayout, parentShape);\n     parentUniqueContigPerThread.erase(parentUniqueContigPerThread.begin() +\n                                       sliceLayout.getDim());\n     return parentUniqueContigPerThread;\n   }\n   // Base case\n   auto rank = shape.size();\n   SmallVector<unsigned> ret(rank);\n-  auto contigPerThread = getContigPerThread(tensorType.getEncoding());\n+  auto contigPerThread = getContigPerThread(layout);\n   assert(contigPerThread.size() == rank && \"Unexpected contigPerThread size\");\n   for (int d = 0; d < rank; ++d) {\n     ret[d] = std::min<unsigned>(shape[d], contigPerThread[d]);"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 23, "deletions": 24, "changes": 47, "file_content_changes": "@@ -1614,12 +1614,13 @@ def test_scan_layouts(M, N, src_layout, axis, device):\n layouts = [\n     BlockedLayout([1, 4], [8, 4], [4, 1], [1, 0]),\n     BlockedLayout([1, 4], [8, 4], [4, 1], [0, 1]),\n+    BlockedLayout([4, 4], [2, 16], [4, 1], [1, 0]),\n     MmaLayout(version=(2, 0), warps_per_cta=[4, 1]),\n     MmaLayout(version=(2, 0), warps_per_cta=[2, 2])\n ]\n \n \n-@pytest.mark.parametrize(\"M, N\", [[128, 16], [128, 128], [32, 128]])\n+@pytest.mark.parametrize(\"M, N\", [[128, 16], [128, 128], [32, 128], [32, 32]])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n @pytest.mark.parametrize(\"axis\", [0, 1])\n def test_reduce_layouts(M, N, src_layout, axis, device):\n@@ -1630,31 +1631,30 @@ def test_reduce_layouts(M, N, src_layout, axis, device):\n     #blocked = #triton_gpu.blocked<{{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}}>\n     #src = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n-    tt.func public @kernel_0d1d2c3d4c(%arg0: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}, %arg1: i32 {{tt.divisibility = 16 : i32}}, %arg2: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}) {{\n+    tt.func public @kernel_0d1d2c3d4c(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: i32 {{tt.divisibility = 16 : i32}}, %arg2: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n         %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>\n         %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>) -> tensor<{M}x1xi32, #blocked>\n         %2 = tt.splat %arg1 : (i32) -> tensor<{M}x1xi32, #blocked>\n         %3 = arith.muli %1, %2 : tensor<{M}x1xi32, #blocked>\n-        %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<{M}x1x!tt.ptr<f32>, #blocked>\n-        %5 = tt.addptr %4, %3 : tensor<{M}x1x!tt.ptr<f32>, #blocked>, tensor<{M}x1xi32, #blocked>\n+        %4 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x1x!tt.ptr<i32>, #blocked>\n+        %5 = tt.addptr %4, %3 : tensor<{M}x1x!tt.ptr<i32>, #blocked>, tensor<{M}x1xi32, #blocked>\n         %6 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>\n         %7 = tt.expand_dims %6 {{axis = 0 : i32}} : (tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>) -> tensor<1x{N}xi32, #blocked>\n-        %8 = tt.broadcast %5 : (tensor<{M}x1x!tt.ptr<f32>, #blocked>) -> tensor<{M}x{N}x!tt.ptr<f32>, #blocked>\n+        %8 = tt.broadcast %5 : (tensor<{M}x1x!tt.ptr<i32>, #blocked>) -> tensor<{M}x{N}x!tt.ptr<i32>, #blocked>\n         %9 = tt.broadcast %7 : (tensor<1x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #blocked>\n-        %10 = tt.addptr %8, %9 : tensor<{M}x{N}x!tt.ptr<f32>, #blocked>, tensor<{M}x{N}xi32, #blocked>\n-        %11 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<{rdims_2d}x!tt.ptr<f32>, #blocked>\n-        %12 = tt.addptr %11, {store_range} : tensor<{rdims_2d}x!tt.ptr<f32>, #blocked>, tensor<{rdims_2d}xi32, #blocked>\n-        %13 = tt.load %10 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xf32, #blocked>\n-        %14 = triton_gpu.convert_layout %13 : (tensor<{M}x{N}xf32, #blocked>) -> tensor<{M}x{N}xf32, #src>\n+        %10 = tt.addptr %8, %9 : tensor<{M}x{N}x!tt.ptr<i32>, #blocked>, tensor<{M}x{N}xi32, #blocked>\n+        %11 = tt.splat %arg2 : (!tt.ptr<i32>) -> tensor<{rdims_2d}x!tt.ptr<i32>, #blocked>\n+        %12 = tt.addptr %11, {store_range} : tensor<{rdims_2d}x!tt.ptr<i32>, #blocked>, tensor<{rdims_2d}xi32, #blocked>\n+        %13 = tt.load %10 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xi32, #blocked>\n+        %14 = triton_gpu.convert_layout %13 : (tensor<{M}x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #src>\n         %15 = \"tt.reduce\"(%14) ({{\n-        ^bb0(%arg3: f32, %arg4: f32):\n-          %16 = \"triton_gpu.cmpf\"(%arg3, %arg4) {{predicate = 2 : i64}} : (f32, f32) -> i1\n-          %17 = arith.select %16, %arg3, %arg4 : f32\n-          tt.reduce.return %17 : f32\n-        }}) {{axis = {axis} : i32}} : (tensor<{M}x{N}xf32, #src>) -> tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>\n-        %18 = triton_gpu.convert_layout %15 : (tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>) -> tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>\n-        %19 = tt.expand_dims %18 {{axis = {axis} : i32}} : (tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>) -> tensor<{rdims_2d}xf32, #blocked>\n-        tt.store %12, %19 {{cache = 1 : i32, evict = 1 : i32}} : tensor<{rdims_2d}xf32, #blocked>\n+        ^bb0(%arg3: i32, %arg4: i32):\n+          %17 = arith.addi %arg3, %arg4 : i32\n+          tt.reduce.return %17 : i32\n+        }}) {{axis = {axis} : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>\n+        %18 = triton_gpu.convert_layout %15 : (tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>) -> tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>\n+        %19 = tt.expand_dims %18 {{axis = {axis} : i32}} : (tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>) -> tensor<{rdims_2d}xi32, #blocked>\n+        tt.store %12, %19 {{cache = 1 : i32, evict = 1 : i32}} : tensor<{rdims_2d}xi32, #blocked>\n         tt.return\n     }}\n     }}\n@@ -1667,22 +1667,21 @@ def test_reduce_layouts(M, N, src_layout, axis, device):\n         kernel = triton.compile(f.name)\n \n     rs = RandomState(17)\n-    x = rs.randint(0, 4, (M, N)).astype('float32')\n-    x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+    x = rs.randint(0, 20, (M, N)).astype('int32')\n \n     if axis == 0:\n-        z = np.zeros((1, N)).astype('float32')\n+        z = np.zeros((1, N)).astype('int32')\n     else:\n-        z = np.zeros((M, 1)).astype('float32')\n+        z = np.zeros((M, 1)).astype('int32')\n \n     x_tri = torch.tensor(x, device=device)\n     z_tri = torch.tensor(z, device=device)\n \n     pgm = kernel[(1, 1, 4)](x_tri, x_tri.stride(0), z_tri)\n \n-    z_ref = np.max(x, axis=axis, keepdims=True)\n+    z_ref = np.sum(x, axis=axis, keepdims=True)\n \n-    np.testing.assert_allclose(z_ref, z_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n+    np.testing.assert_equal(z_ref, z_tri.cpu().numpy())\n \n \n layouts = ["}]