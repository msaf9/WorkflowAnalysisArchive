[{"filename": "include/triton/Dialect/TritonNvidiaGPU/IR/TritonNvidiaGPUOps.td", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -279,8 +279,7 @@ def TTNG_DotWaitOp : TTNG_Op<\"dot_wait\", []> {\n }\n \n def TTNG_StoreAsyncOp : TTNG_Op<\"store_async\",\n-                              [Source1IsSharedEncoding,\n-                               MemoryEffects<[MemWrite]>]> {\n+                              [MemoryEffects<[MemWrite]>]> {\n   let summary = \"store asynchronous by a tensor pointer\";\n   let arguments = (ins TT_TensorPtr:$dst, TT_Tensor:$src,\n                        DefaultValuedAttr<TT_CacheModifierAttr, \"triton::CacheModifier::NONE\">:$cache);"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 18, "deletions": 0, "changes": 18, "file_content_changes": "@@ -109,6 +109,12 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   return paddedRepShape;\n }\n \n+SmallVector<unsigned>\n+getScratchConfigForStoreAsync(triton::nvidia_gpu::StoreAsyncOp op) {\n+  auto srcTy = op.getSrc().getType().cast<RankedTensorType>();\n+  return convertType<unsigned, int64_t>(getShapePerCTA(srcTy));\n+}\n+\n // TODO: extend beyond scalars\n SmallVector<unsigned> getScratchConfigForAtomicRMW(triton::AtomicRMWOp op) {\n   SmallVector<unsigned> smemShape;\n@@ -244,6 +250,18 @@ class AllocationAnalysis {\n               : elems * std::max<int>(8, srcTy.getElementTypeBitWidth()) / 8;\n       maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes,\n                                                           scratchAlignment);\n+    } else if (auto storeAsyncOp =\n+                   dyn_cast<triton::nvidia_gpu::StoreAsyncOp>(op)) {\n+      auto srcTy = storeAsyncOp.getSrc().getType().cast<RankedTensorType>();\n+      auto srcEncoding = srcTy.getEncoding();\n+      if (!srcEncoding.isa<MmaEncodingAttr>()) {\n+        return;\n+      }\n+      auto smemShape = getScratchConfigForStoreAsync(storeAsyncOp);\n+      unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n+                                       std::multiplies{});\n+      auto bytes = elems * std::max<int>(8, srcTy.getElementTypeBitWidth()) / 8;\n+      maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes, 1024);\n     } else if (auto atomicRMWOp = dyn_cast<triton::AtomicRMWOp>(op)) {\n       auto value = op->getOperand(0);\n       // only scalar requires scratch memory"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 19, "deletions": 18, "changes": 37, "file_content_changes": "@@ -699,15 +699,6 @@ struct ConvertLayoutOpConversion\n     return success();\n   }\n \n-  // Pack two 16-bit values into a 32-bit register.\n-  static Value pack16bitsTo32(ConversionPatternRewriter &rewriter, Location loc,\n-                              Value hb, Value lb) {\n-    hb = zext(i32_ty, bitcast(hb, i16_ty));\n-    lb = zext(i32_ty, bitcast(lb, i16_ty));\n-    Value pack = or_(lb, shl(hb, i32_val(16)));\n-    return pack;\n-  }\n-\n   // blocked -> shared.\n   // Swizzling in shared memory to avoid bank conflict. Normally used for\n   // A/B operands of dots.\n@@ -766,6 +757,15 @@ struct ConvertLayoutOpConversion\n       Value warpId0 = urem(urem(warpId, i32_val(warpsPerCTA[0])),\n                            i32_val(srcShape[0] / instrShape[0]));\n \n+      unsigned inVec =\n+          inOrd == outOrd\n+              ? triton::gpu::getContigPerThread(mmaLayout)[inOrd[0]]\n+              : 1;\n+      unsigned outVec = dstSharedLayout.getVec();\n+      unsigned minVec = std::min(outVec, inVec);\n+      assert(minVec == 2);\n+      auto wordTy = vec_ty(elemTy, minVec);\n+\n       for (int rep = 0; rep < repM; ++rep) {\n         Value rowOfWarp = add(mul(warpId0, i32_val(instrShape[0])),\n                               i32_val(rep * rowsPerRep));\n@@ -779,18 +779,19 @@ struct ConvertLayoutOpConversion\n               numElemsPerSwizzlingRow, true);\n \n           Value addr = gep(elemPtrTy, smemBase, offset);\n-          Value data0 = pack16bitsTo32(rewriter, loc, inVals[elemIdx + 1],\n-                                       inVals[elemIdx + 0]);\n-          Value data1 = pack16bitsTo32(rewriter, loc, inVals[elemIdx + 3],\n-                                       inVals[elemIdx + 2]);\n-          Value data2 = pack16bitsTo32(rewriter, loc, inVals[elemIdx + 5],\n-                                       inVals[elemIdx + 4]);\n-          Value data3 = pack16bitsTo32(rewriter, loc, inVals[elemIdx + 7],\n-                                       inVals[elemIdx + 6]);\n+\n+          Value words[4];\n+          for (unsigned i = 0; i < 8; ++i) {\n+            if (i % minVec == 0)\n+              words[i / 2] = undef(wordTy);\n+            words[i / 2] = insert_element(\n+                wordTy, words[i / 2], inVals[elemIdx + i], i32_val(i % minVec));\n+          }\n \n           rewriter.create<triton::nvgpu::StoreMatrixOp>(\n               loc, bitcast(addr, ptrI8SharedTy),\n-              ValueRange{data0, data1, data2, data3});\n+              ValueRange{bitcast(words[0], i32_ty), bitcast(words[1], i32_ty),\n+                         bitcast(words[2], i32_ty), bitcast(words[3], i32_ty)});\n         }\n       }\n       // TODO[shuhaoj]: change hard code style of numThreads. Hide async_agent"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 288, "deletions": 3, "changes": 291, "file_content_changes": "@@ -3,6 +3,10 @@\n \n #include \"ConvertLayoutOpToLLVM.h\"\n #include \"LoadStoreOpToLLVM.h\"\n+#include \"Utility.h\"\n+\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n \n #include <numeric>\n \n@@ -12,6 +16,7 @@ using namespace mlir::triton;\n using ::mlir::LLVM::delinearize;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n using ::mlir::LLVM::linearize;\n+using ::mlir::triton::gpu::getCTALayout;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n@@ -404,6 +409,18 @@ struct StoreAsyncOpConversion\n   LogicalResult\n   matchAndRewrite(triton::nvidia_gpu::StoreAsyncOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n+    auto srcTy = op.getSrc().getType().cast<RankedTensorType>();\n+    auto srcEncoding = srcTy.getEncoding();\n+    if (srcEncoding.isa<MmaEncodingAttr>()) {\n+      return lowerStoreAsyncWithSlice(op, adaptor, rewriter);\n+    } else {\n+      return lowerStoreAsync(op, adaptor, rewriter);\n+    }\n+  }\n+\n+  LogicalResult lowerStoreAsync(triton::nvidia_gpu::StoreAsyncOp op,\n+                                OpAdaptor adaptor,\n+                                ConversionPatternRewriter &rewriter) const {\n     auto loc = op.getLoc();\n     MLIRContext *ctx = rewriter.getContext();\n \n@@ -413,6 +430,9 @@ struct StoreAsyncOpConversion\n     auto elemTy = srcTy.getElementType();\n \n     auto rank = srcTy.getRank();\n+    // The sotre async op only supports tensor with ranke <= 5.\n+    // Reference:\n+    // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#tensor-dimension-size-and-format\n     assert(rank > 0 && rank <= 5);\n \n     auto moduleOp = op->getParentOfType<ModuleOp>();\n@@ -475,14 +495,14 @@ struct StoreAsyncOpConversion\n                            .cast<RankedTensorType>()\n                            .getShape();\n     auto shapePerCTA = getShapePerCTA(CTASplitNum, tensorShape);\n-    // magic 128 bytes\n+    const uint32_t bytesPerCacheline = 128;\n     uint32_t bytesPerElem = elemTy.getIntOrFloatBitWidth() / 8;\n     uint32_t numBox{1};\n     for (int i = 0; i < rank; ++i) {\n       auto dim = getDimOfOrder(dstOrder, i);\n       auto tNumElems = shapePerCTA[dim];\n-      if (i == 0 && tNumElems * bytesPerElem > 128) {\n-        tNumElems = 128 / bytesPerElem;\n+      if (i == 0 && tNumElems * bytesPerElem > bytesPerCacheline) {\n+        tNumElems = bytesPerCacheline / bytesPerElem;\n         numBox = (shapePerCTA[dim] + tNumElems - 1) / tNumElems;\n       }\n       boxDims.emplace_back(tNumElems);\n@@ -574,6 +594,268 @@ struct StoreAsyncOpConversion\n     return success();\n   }\n \n+  LogicalResult\n+  lowerStoreAsyncWithSlice(triton::nvidia_gpu::StoreAsyncOp op,\n+                           OpAdaptor adaptor,\n+                           ConversionPatternRewriter &rewriter) const {\n+    auto loc = op.getLoc();\n+    MLIRContext *ctx = rewriter.getContext();\n+\n+    auto dst = op.getDst();\n+    auto src = op.getSrc();\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto makeTensorPtr = tensorPtrMap->lookup(op.getOperation());\n+    auto dstTensorTy = makeTensorPtr.getResult()\n+                           .getType()\n+                           .cast<triton::PointerType>()\n+                           .getPointeeType()\n+                           .cast<RankedTensorType>();\n+    auto tensorShape = dstTensorTy.getShape();\n+    auto dstOrder = makeTensorPtr.getOrder();\n+    auto dstElemTy = dstTensorTy.getElementType();\n+\n+    auto rank = srcTy.getRank();\n+    // The sotre async op only supports tensor with ranke <= 5.\n+    // Reference:\n+    // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#tensor-dimension-size-and-format\n+    assert(rank > 0 && rank <= 5);\n+\n+    auto moduleOp = op->getParentOfType<ModuleOp>();\n+    assert(moduleOp && \"Parent ModuleOp not found for StoreAsyncOp\");\n+\n+    auto llFuncOp = op->getParentOfType<LLVM::LLVMFuncOp>();\n+    assert(llFuncOp && \"LLVMFuncOp not found for StoreAsyncOp\");\n+\n+    int numTMADescs = getNumTMADescs(llFuncOp);\n+    assert(numTMADescs > 0);\n+\n+    auto ctaLayout = getCTALayout(dstTensorTy.getEncoding());\n+    // The order of smem should be consistent with gmem.\n+    SmallVector<unsigned> sharedOrder;\n+    for (auto o : makeTensorPtr.getOrder()) {\n+      sharedOrder.emplace_back(o);\n+    }\n+    auto sharedLayout = SharedEncodingAttr::get(ctx, tensorShape, sharedOrder,\n+                                                ctaLayout, dstElemTy);\n+\n+    mlir::triton::gpu::TMAInfo tmaInfo;\n+\n+    tmaInfo.tensorDataType = getCUtensorMapDataType(dstElemTy);\n+    tmaInfo.tensorRank = rank;\n+    assert(tmaMetadata);\n+\n+    unsigned TMADescIdx = tmaMetadata->size();\n+    unsigned numFuncArgs = llFuncOp.getBody().front().getNumArguments();\n+\n+    unsigned globalAddressArgIdx = getArgIdx(makeTensorPtr.getBase());\n+    tmaInfo.globalAddressArgIdx = globalAddressArgIdx;\n+    tmaInfo.TMADescArgIdx = numFuncArgs - numTMADescs + TMADescIdx;\n+\n+    auto getDimOfOrder = [](ArrayRef<int32_t> order, int32_t i) {\n+      auto it = std::find(order.begin(), order.end(), i);\n+      assert(it != order.end());\n+      return std::distance(order.begin(), it);\n+    };\n+\n+    std::vector<int32_t> globalDimsArgIdx;\n+    std::vector<int32_t> globalStridesArgIdx;\n+    // constant values are mapped to (-1 - value)\n+    for (int i = 0; i < rank; ++i) {\n+      int32_t argIdx = -1;\n+      auto dim = getDimOfOrder(dstOrder, i);\n+      argIdx = getArgIdx(makeTensorPtr.getShape()[dim]);\n+      globalDimsArgIdx.emplace_back(argIdx);\n+      // handle constant stride\n+      argIdx = getArgIdx(makeTensorPtr.getStrides()[dim]);\n+      globalStridesArgIdx.emplace_back(argIdx);\n+    }\n+\n+    tmaInfo.globalDimsArgIdx = globalDimsArgIdx;\n+    tmaInfo.globalStridesArgIdx = globalStridesArgIdx;\n+    std::vector<uint32_t> boxDims;\n+    auto CTAsPerCGA = sharedLayout.getCTALayout().getCTAsPerCGA();\n+    auto CTAOrder = sharedLayout.getCTALayout().getCTAOrder();\n+    auto CTASplitNum = sharedLayout.getCTALayout().getCTASplitNum();\n+    auto shapePerCTA = getShapePerCTA(CTASplitNum, tensorShape);\n+\n+    auto srcLayout = srcTy.getEncoding();\n+    auto mmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>();\n+\n+    unsigned numElems = triton::gpu::getTotalElemsPerThread(srcTy);\n+\n+    auto instrShape = mmaLayout.getInstrShape();\n+    auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n+    uint32_t repM =\n+        ceil<unsigned>(shapePerCTA[0], instrShape[0] * warpsPerCTA[0]);\n+    uint32_t numElemsPerRep = numElems / repM;\n+\n+    const uint32_t bytesPerCacheline = 128;\n+    uint32_t bytesPerElem = dstElemTy.getIntOrFloatBitWidth() / 8;\n+    uint32_t numBox{1};\n+    for (int i = 0; i < rank; ++i) {\n+      auto dim = getDimOfOrder(dstOrder, i);\n+      auto tNumElems = shapePerCTA[dim];\n+      if (i == 0 && tNumElems * bytesPerElem > bytesPerCacheline) {\n+        tNumElems = bytesPerCacheline / bytesPerElem;\n+        numBox = (shapePerCTA[dim] + tNumElems - 1) / tNumElems;\n+      }\n+      if (i == 1) {\n+        tNumElems = tNumElems / repM / warpsPerCTA[0];\n+      }\n+      boxDims.emplace_back(tNumElems);\n+    }\n+    std::vector<uint32_t> elementStrides(rank, 1);\n+    tmaInfo.boxDims = boxDims;\n+    tmaInfo.elementStrides = elementStrides;\n+\n+    CUtensorMapSwizzle swizzle = CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_NONE;\n+    assert(((dstElemTy.getIntOrFloatBitWidth() == 16 &&\n+             sharedLayout.getVec() == 8) or\n+            (dstElemTy.getIntOrFloatBitWidth() == 32 &&\n+             sharedLayout.getVec() == 4)) &&\n+           \"Unexpected shared layout for StoreAsyncOp\");\n+    if (sharedLayout.getPerPhase() == 4 && sharedLayout.getMaxPhase() == 2)\n+      swizzle = CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_32B;\n+    else if (sharedLayout.getPerPhase() == 2 && sharedLayout.getMaxPhase() == 4)\n+      swizzle = CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_64B;\n+    else if (sharedLayout.getPerPhase() == 1 && sharedLayout.getMaxPhase() == 8)\n+      swizzle = CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_128B;\n+    else\n+      llvm::report_fatal_error(\"Unsupported shared layout for StoreAsyncOp\");\n+    tmaInfo.swizzle = swizzle;\n+    tmaInfo.interleave = CUtensorMapInterleave::CU_TENSOR_MAP_INTERLEAVE_NONE;\n+    tmaInfo.l2Promotion =\n+        CUtensorMapL2promotion::CU_TENSOR_MAP_L2_PROMOTION_L2_128B;\n+    tmaInfo.oobFill =\n+        CUtensorMapFloatOOBfill::CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE;\n+\n+    tmaMetadata->emplace_back(tmaInfo);\n+\n+    Value llDst = adaptor.getDst();\n+    Value llSrc = adaptor.getSrc();\n+    auto srcShape = srcTy.getShape();\n+    auto dstElemPtrTy = ptr_ty(getTypeConverter()->convertType(dstElemTy), 3);\n+    Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n+    smemBase = bitcast(smemBase, dstElemPtrTy);\n+\n+    SmallVector<Value> offsetVals;\n+    for (auto i = 0; i < srcShape.size(); ++i) {\n+      offsetVals.emplace_back(i32_val(0));\n+    }\n+\n+    Value tmaDesc =\n+        llFuncOp.getBody().front().getArgument(tmaInfo.TMADescArgIdx);\n+    auto ptrI8SharedTy = LLVM::LLVMPointerType::get(\n+        typeConverter->convertType(rewriter.getI8Type()), 3);\n+\n+    auto threadId = getThreadId(rewriter, loc);\n+    Value pred = icmp_eq(urem(threadId, i32_val(32)), i32_val(0));\n+\n+    auto llCoord = getTypeConverter()->unpackLLElements(loc, llDst, rewriter,\n+                                                        dst.getType());\n+    uint32_t boxStride = std::accumulate(boxDims.begin(), boxDims.end(), 1,\n+                                         std::multiplies<uint32_t>());\n+    boxStride = boxStride * repM * warpsPerCTA[0];\n+\n+    Value clusterCTAId = getClusterCTAId(rewriter, loc);\n+    SmallVector<Value> multiDimClusterCTAId =\n+        delinearize(rewriter, loc, clusterCTAId, CTAsPerCGA, CTAOrder);\n+\n+    // rowStride in bytes\n+    uint32_t rowStrideInBytes = shapePerCTA[dstOrder[0]] * bytesPerElem;\n+    uint32_t swizzlingByteWidth =\n+        std::min<uint32_t>(rowStrideInBytes, bytesPerCacheline);\n+\n+    unsigned numElemsPerSwizzlingRow = swizzlingByteWidth / bytesPerElem;\n+    unsigned leadingDimOffset =\n+        numElemsPerSwizzlingRow * shapePerCTA[dstOrder[1]];\n+\n+    uint32_t rowsPerRep = getShapePerCTATile(mmaLayout)[0];\n+\n+    Value warpId = udiv(threadId, i32_val(32));\n+    Value warpId0 = urem(urem(warpId, i32_val(warpsPerCTA[0])),\n+                         i32_val(srcShape[0] / instrShape[0]));\n+    auto srcOrder = triton::gpu::getOrder(srcLayout);\n+    unsigned inVec =\n+        srcOrder == sharedLayout.getOrder()\n+            ? triton::gpu::getContigPerThread(srcLayout)[srcOrder[0]]\n+            : 1;\n+    unsigned outVec = sharedLayout.getVec();\n+    unsigned minVec = std::min(outVec, inVec);\n+    assert(minVec == 2);\n+\n+    auto wordTy = vec_ty(dstElemTy, minVec);\n+\n+    auto inVals = getTypeConverter()->unpackLLElements(loc, adaptor.getSrc(),\n+                                                       rewriter, srcTy);\n+    for (uint32_t b = 0; b < numBox; ++b) {\n+      for (int rep = 0; rep < repM; ++rep) {\n+        Value rowOfWarp = add(mul(warpId0, i32_val(instrShape[0])),\n+                              i32_val(rep * rowsPerRep));\n+        uint32_t elemIdxOffset = rep * numElemsPerRep;\n+\n+        for (unsigned idx = 0; idx < numElemsPerRep / numBox; idx += 8) {\n+          uint32_t elemIdx = elemIdxOffset + b * numElemsPerRep / numBox + idx;\n+\n+          Value offset = rewriter.create<triton::nvgpu::OffsetOfStmatrixV4Op>(\n+              loc, i32_ty, threadId, rowOfWarp,\n+              i32_val(b * numElemsPerRep / numBox + idx), leadingDimOffset,\n+              numElemsPerSwizzlingRow, true);\n+\n+          Value addr = gep(dstElemPtrTy, smemBase, offset);\n+          Value words[4];\n+          for (unsigned i = 0; i < 8; ++i) {\n+            if (i % minVec == 0)\n+              words[i / 2] = undef(wordTy);\n+            words[i / 2] = insert_element(\n+                wordTy, words[i / 2], inVals[elemIdx + i], i32_val(i % minVec));\n+          }\n+\n+          rewriter.create<triton::nvgpu::StoreMatrixOp>(\n+              loc, bitcast(addr, ptrI8SharedTy),\n+              ValueRange{bitcast(words[0], i32_ty), bitcast(words[1], i32_ty),\n+                         bitcast(words[2], i32_ty), bitcast(words[3], i32_ty)});\n+        }\n+        rewriter.create<triton::nvgpu::FenceAsyncSharedOp>(loc, 0);\n+\n+        SmallVector<Value> coord;\n+        // raw coord\n+        for (int i = 0; i < rank; ++i) {\n+          auto dim = getDimOfOrder(dstOrder, i);\n+          coord.push_back(llCoord[dim]);\n+        }\n+        // coord with box and cta offset\n+        for (int i = 0; i < rank; ++i) {\n+          auto dim = getDimOfOrder(dstOrder, i);\n+          if (i == 0) {\n+            coord[i] = add(coord[i], i32_val(b * boxDims[i]));\n+            auto CTAOffset =\n+                mul(multiDimClusterCTAId[dim], i32_val(numBox * boxDims[i]));\n+            coord[i] = add(coord[i], CTAOffset);\n+          } else {\n+            Value blockOffset = i32_val(rep * instrShape[0] * warpsPerCTA[0]);\n+            Value warpOffset = mul(warpId0, i32_val(instrShape[0]));\n+            coord[i] = add(add(coord[i], add(blockOffset, warpOffset)),\n+                           mul(multiDimClusterCTAId[dim],\n+                               i32_val(boxDims[i] * repM * warpsPerCTA[0])));\n+          }\n+        }\n+        Value srcOffset =\n+            add(i32_val(b * boxStride + rep * instrShape[0] * warpsPerCTA[0] *\n+                                            instrShape[1] * warpsPerCTA[1] /\n+                                            numBox),\n+                mul(warpId0, i32_val(instrShape[0] * numElemsPerSwizzlingRow)));\n+        auto srcPtrTy = ptr_ty(getTypeConverter()->convertType(dstElemTy), 3);\n+        Value srcPtrBase = gep(srcPtrTy, smemBase, srcOffset);\n+        auto addr = bitcast(srcPtrBase, ptrI8SharedTy);\n+        rewriter.create<triton::nvgpu::TMAStoreTiledOp>(loc, tmaDesc, addr,\n+                                                        pred, coord);\n+      }\n+    }\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+\n private:\n   CUtensorMapDataType getCUtensorMapDataType(Type ty) const {\n     if (ty.isF16()) {\n@@ -1136,6 +1418,9 @@ struct InsertSliceAsyncV2OpConversion\n     auto rank = resultTy.getRank() - 1;\n \n     // TODO: support any valid rank in (3, 4, 5)\n+    // The sotre async op only supports tensor with ranke <= 5.\n+    // Reference:\n+    // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#tensor-dimension-size-and-format\n     assert(rank > 0 && rank <= 5);\n     SmallVector<unsigned> shape;\n     auto axis = op->getAttrOfType<IntegerAttr>(\"axis\").getInt();"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/MaterializeLoadStore.cpp", "status": "modified", "additions": 26, "deletions": 1, "changes": 27, "file_content_changes": "@@ -150,9 +150,34 @@ void MaterializeLoadStorePass::materializeStoreTilePtr(\n     return;\n   auto loc = store.getLoc();\n   OpBuilder builder(store);\n-  auto *ctx = store.getContext();\n   auto value = store.getValue();\n   auto dst = store.getPtr();\n+\n+  auto cvtOp = llvm::dyn_cast_or_null<mlir::triton::gpu::ConvertLayoutOp>(\n+      value.getDefiningOp());\n+  if (cvtOp) {\n+    auto srcTy = cvtOp.getOperand().getType().cast<RankedTensorType>();\n+    auto dstTy = cvtOp.getResult().getType().cast<RankedTensorType>();\n+    auto elemTy = srcTy.getElementType();\n+    auto srcMmaLayout = srcTy.getEncoding().dyn_cast<MmaEncodingAttr>();\n+    auto dstBlockedLayout = dstTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n+    auto truncFOP = llvm::dyn_cast_or_null<arith::TruncFOp>(\n+        cvtOp.getOperand().getDefiningOp());\n+    unsigned numElems = ttg::getTotalElemsPerThread(srcTy);\n+    auto inOrd = ttg::getOrder(srcTy.getEncoding());\n+    auto outOrd = ttg::getOrder(dstTy.getEncoding());\n+    if (srcMmaLayout && srcMmaLayout.isHopper() && dstBlockedLayout &&\n+        truncFOP && elemTy.getIntOrFloatBitWidth() == 16 && numElems >= 16 &&\n+        inOrd == outOrd) {\n+      builder.create<ttng::StoreAsyncOp>(loc, dst, cvtOp.getOperand());\n+      builder.create<ttg::AsyncBulkCommitGroupOp>(loc);\n+      builder.create<ttg::AsyncBulkWaitOp>(loc, 0);\n+      store->erase();\n+      return;\n+    }\n+  }\n+\n+  auto *ctx = store.getContext();\n   auto storeTy = value.getType().dyn_cast<RankedTensorType>();\n   assert(storeTy);\n   auto storeElemTy = storeTy.getElementType();"}, {"filename": "python/tutorials/10-experimental-tma-store-matrix-multiplication.py", "status": "renamed", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,5 +1,5 @@\n \"\"\"\n-Matrix Multiplication with TMASTG (Experimental)\n+Matrix Multiplication with TMA Store (Experimental)\n ================================================\n In this tutorial, you will write a very short high-performance multiplication kernel that achieves\n performance on parallel with cuBLAS."}]