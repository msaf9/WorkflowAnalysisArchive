[{"filename": ".github/workflows/documentation.yml", "status": "modified", "additions": 8, "deletions": 18, "changes": 26, "file_content_changes": "@@ -5,37 +5,28 @@ on:\n     - cron: \"0 0 * * *\"\n \n jobs:\n-\n   Build-Documentation:\n-\n     runs-on: [self-hosted, V100]\n \n     steps:\n-\n-      - name: Checkout gh-pages\n+      - name: Checkout branch\n         uses: actions/checkout@v2\n         with:\n-          ref: 'gh-pages'\n+          token: ${{ secrets.CI_PAT }}\n \n       - name: Clear docs\n         run: |\n           rm -r /tmp/triton-docs\n         continue-on-error: true\n \n-      - name: Checkout branch\n-        uses: actions/checkout@v2\n-\n       - name: Build docs\n         run: |\n-          git fetch origin main\n           cd docs\n           sphinx-multiversion . _build/html/\n \n-      - name: Publish docs\n+      - name: Update docs\n         run: |\n-          git branch\n-          # update docs\n-          mkdir /tmp/triton-docs;\n+          mkdir /tmp/triton-docs\n           mv docs/_build/html/* /tmp/triton-docs/\n           git checkout gh-pages\n           cp -r CNAME /tmp/triton-docs/\n@@ -45,8 +36,7 @@ jobs:\n           cp -r /tmp/triton-docs/* .\n           git add .\n           git commit -am \"[GH-PAGES] Updated website\"\n-          # publish docs\n-          eval `ssh-agent -s`\n-          DISPLAY=:0 SSH_ASKPASS=~/.ssh/give_pass.sh ssh-add ${{ secrets.SSH_KEY }} <<< ${{ secrets.SSH_PASS }}\n-          git remote set-url origin git@github.com:openai/triton.git\n-          git push\n+\n+      - name: Publish docs\n+        run: |\n+          git push origin gh-pages"}, {"filename": "docs/conf.py", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -86,8 +86,8 @@ def documenter(app, obj, parent):\n autosummary_generate = True\n \n # versioning config\n-smv_tag_whitelist = r'^(v1.1.2)$'\n-smv_branch_whitelist = r'^master$'\n+smv_tag_whitelist = r'^(v2.1.0)$'\n+smv_branch_whitelist = r'^main$'\n smv_remote_whitelist = None\n smv_released_pattern = r'^tags/.*$'\n smv_outputdir_format = '{ref.name}'\n@@ -100,7 +100,8 @@ def documenter(app, obj, parent):\n     'examples_dirs': '../python/tutorials/',\n     'gallery_dirs': 'getting-started/tutorials',\n     'filename_pattern': '',\n-    'ignore_pattern': r'__init__\\.py',\n+    # XXX: Temporarily disable fused attention tutorial on V100\n+    'ignore_pattern': r'__init__\\.py|06-fused-attention\\.py',\n     'within_subsection_order': FileNameSortKey,\n     'reference_url': {\n         'sphinx_gallery': None,"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.h", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -25,8 +25,6 @@ std::unique_ptr<Pass> createTritonGPUVerifier();\n \n std::unique_ptr<Pass> createTritonGPUOptimizeDotOperandsPass();\n \n-std::unique_ptr<Pass> createTritonGPUUpdateMmaForVoltaPass();\n-\n /// Generate the code for registering passes.\n #define GEN_PASS_REGISTRATION\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\""}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -79,7 +79,7 @@ def kernel_call():\n         try:\n             return do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n         except OutOfResources:\n-            return (float('inf'), float('inf'), float('inf'))\n+            return [float('inf'), float('inf'), float('inf')]\n \n     def run(self, *args, **kwargs):\n         self.nargs = dict(zip(self.arg_names, args))"}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 10, "deletions": 3, "changes": 13, "file_content_changes": "@@ -157,7 +157,14 @@\n \n @triton.autotune(\n     configs=[\n-        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n+        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n     ],\n     key=['M', 'N', 'K'],\n )\n@@ -312,15 +319,15 @@ def matmul(a, b, activation=None):\n     triton.testing.Benchmark(\n         x_names=['M', 'N', 'K'],  # argument names to use as an x-axis for the plot\n         x_vals=[\n-            8192\n+            128 * i for i in range(2, 33)\n         ],  # different possible values for `x_name`\n         line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n         # possible values for `line_arg``\n         line_vals=['cublas', 'triton'],\n         # label name for the lines\n         line_names=[\"cuBLAS\", \"Triton\"],\n         # line styles\n-        styles=[('green', '-'), ('green', '--'), ('blue', '-'), ('blue', '--')],\n+        styles=[('green', '-'), ('blue', '-')],\n         ylabel=\"TFLOPS\",  # label name for the y-axis\n         plot_name=\"matmul-performance\",  # name for the plot. Used also as a file name for saving the plot.\n         args={},"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -33,7 +33,7 @@ def _fwd_kernel(\n     offs_n = tl.arange(0, BLOCK_N)\n     offs_d = tl.arange(0, BLOCK_DMODEL)\n     off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :] * stride_kk\n+    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n     off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n     # Initialize pointers to Q, K, V\n     q_ptrs = Q + off_q\n@@ -50,7 +50,7 @@ def _fwd_kernel(\n         # -- compute qk ----\n         k = tl.load(k_ptrs)\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-        qk += tl.dot(q, tl.trans(k))\n+        qk += tl.dot(q, k)\n         qk *= sm_scale\n         qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n         # compute new m\n@@ -62,10 +62,10 @@ def _fwd_kernel(\n         l_curr = tl.sum(p, 1) + l_prev\n         # rescale operands of matmuls\n         l_rcp = 1. / l_curr\n-        p *= l_rcp\n+        p *= l_rcp[:, None]\n         acc *= (l_prev * l_rcp)[:, None]\n         # update acc\n-        p = p.to(tl.float16)\n+        p = p.to(Q.dtype.element_ty)\n         v = tl.load(v_ptrs)\n         acc += tl.dot(p, v)\n         # update m_i and l_i\n@@ -169,18 +169,18 @@ def _bwd_kernel(\n             p = tl.exp(qk * sm_scale - m[:, None])\n             # compute dv\n             do = tl.load(do_ptrs)\n-            dv += tl.dot(tl.trans(p.to(tl.float16)), do)\n+            dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\n             # compute dp = dot(v, do)\n             Di = tl.load(D_ptrs + offs_m_curr)\n             dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n             dp += tl.dot(do, tl.trans(v))\n             # compute ds = p * (dp - delta[:, None])\n             ds = p * dp * sm_scale\n             # compute dk = dot(ds.T, q)\n-            dk += tl.dot(tl.trans(ds.to(tl.float16)), q)\n+            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)\n             # compute dq\n             dq = tl.load(dq_ptrs)\n-            dq += tl.dot(ds.to(tl.float16), k)\n+            dq += tl.dot(ds.to(Q.dtype.element_ty), k)\n             tl.store(dq_ptrs, dq)\n             # increment pointers\n             dq_ptrs += BLOCK_M * stride_qm"}]