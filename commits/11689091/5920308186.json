[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 10, "deletions": 7, "changes": 17, "file_content_changes": "@@ -2899,8 +2899,8 @@ def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n # test if\n # -------------\n \n-\n-@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and_dynamic\", \"if_and_static\"])\n+# TODO(Keren): if_exp_dynamic\n+@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_and_dynamic\", \"if_exp_static\", \"if_and_static\"])\n def test_if(if_type, device):\n \n     @triton.jit\n@@ -2912,8 +2912,10 @@ def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr\n                 tl.store(Ret, tl.load(XTrue))\n             else:\n                 tl.store(Ret, tl.load(XFalse))\n-        elif IfType == \"if_exp\":\n-            tl.store(Ret, tl.load(XTrue)) if pid % 2 else tl.store(Ret, tl.load(XFalse))\n+        elif IfType == \"if_exp_dynamic\":\n+            tl.store(Ret, tl.load(XTrue)) if pid % 2 == 0 else tl.store(Ret, tl.load(XFalse))\n+        elif IfType == \"if_exp_static\":\n+            tl.store(Ret, tl.load(XTrue)) if BoolVar else tl.store(Ret, tl.load(XFalse))\n         elif IfType == \"if_and_dynamic\":\n             if BoolVar and pid % 2 == 0:\n                 tl.store(Ret, tl.load(XTrue))\n@@ -2928,7 +2930,7 @@ def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr\n     cond = torch.ones(1, dtype=torch.int32, device=device)\n     x_true = torch.tensor([3.14], dtype=torch.float32, device=device)\n     x_false = torch.tensor([1.51], dtype=torch.float32, device=device)\n-    ret = torch.empty(1, dtype=torch.float32, device=device)\n+    ret = torch.zeros(1, dtype=torch.float32, device=device)\n \n     kernel[(1,)](cond, x_true, x_false, ret, if_type, True, 1)\n     assert torch.equal(ret, x_true)\n@@ -3209,8 +3211,9 @@ def add_fn_static_cond(x, cond: tl.constexpr):\n         return x + 1\n \n \n+# TODO(Keren): if_exp\n @pytest.mark.parametrize(\"call_type\", [\"attribute\", \"attribute_jit\",\n-                                       \"jit\", \"jit_if\", \"jit_ifexp\", \"jit_expr\",\n+                                       \"jit\", \"jit_if\", \"jit_expr\",\n                                        \"jit_static_cond\", \"jit_noinline\", \"jit_extern\"])\n def test_if_call(call_type, device):\n     @triton.jit\n@@ -3241,7 +3244,7 @@ def kernel(Out, call_type: tl.constexpr):\n                 a = o\n                 a = add_fn_return(a, pid)\n                 o = a\n-        elif call_type == \"jit_ifexp\":\n+        elif call_type == \"jit_if_exp\":\n             # ifexp expression\n             if pid == 0:\n                 a = o"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 21, "deletions": 6, "changes": 27, "file_content_changes": "@@ -64,6 +64,10 @@ def _is_triton_scalar(o: Any) -> bool:\n     return _is_triton_tensor(o) and (not o.type.is_block() or o.type.numel == 1)\n \n \n+def _is_list_like(o: Any) -> bool:\n+    return isinstance(o, (list, tuple))\n+\n+\n def _unwrap_if_constexpr(o: Any):\n     return o.value if isinstance(o, constexpr) else o\n \n@@ -284,6 +288,9 @@ def _set_insertion_point_and_loc(self, ip, loc):\n     # AST visitor\n     #\n     def visit_compound_statement(self, stmts):\n+        # Ensure that stmts is iterable\n+        if not _is_list_like(stmts):\n+            stmts = [stmts]\n         for stmt in stmts:\n             ret_type = self.visit(stmt)\n             if ret_type is not None and isinstance(stmt, ast.Return):\n@@ -413,9 +420,9 @@ def visit_Assign(self, node):\n             raise UnsupportedLanguageConstruct(None, node, \"simultaneous multiple assignment is not supported.\")\n         names = _names[0]\n         values = self.visit(node.value)\n-        if not isinstance(names, tuple):\n+        if not _is_list_like(names):\n             names = [names]\n-        if not isinstance(values, tuple):\n+        if not _is_list_like(values):\n             values = [values]\n         native_nontensor_types = (language.dtype, )\n         for name, value in zip(names, values):\n@@ -619,11 +626,19 @@ def visit_If(self, node):\n     def visit_IfExp(self, node):\n         cond = self.visit(node.test)\n         if _is_triton_tensor(cond):\n-            cond = cond.to(language.int1, _builder=self.builder)\n-        if _unwrap_if_constexpr(cond):\n-            return self.visit(node.body)\n+            raise UnsupportedLanguageConstruct(\n+                None, node,\n+                \"Triton does not support `if` expressions (ternary operators) with dynamic conditions, use `if` statements instead\")\n         else:\n-            return self.visit(node.orelse)\n+            cond = _unwrap_if_constexpr(cond)\n+            if type(cond) not in _condition_types:  # not isinstance - we insist the real thing, no subclasses and no ducks\n+                raise UnsupportedLanguageConstruct(\n+                    None, node, \"`if` conditionals can only accept values of type {{{}}}, not objects of type {}\".format(\n+                        ', '.join(_.__name__ for _ in _condition_types), type(cond).__name__))\n+            if cond:\n+                return self.visit(node.body)\n+            else:\n+                return self.visit(node.orelse)\n \n     def visit_Pass(self, node):\n         pass"}]