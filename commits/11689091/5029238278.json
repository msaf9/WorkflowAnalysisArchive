[{"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 38, "deletions": 3, "changes": 41, "file_content_changes": "@@ -77,6 +77,10 @@ struct ConvertLayoutOpConversion\n         dstLayout.isa<DotOperandEncodingAttr>()) {\n       return lowerMmaToDotOperand(op, adaptor, rewriter);\n     }\n+    if (srcLayout.isa<SharedEncodingAttr>() &&\n+        isaDistributedLayout(dstLayout)) {\n+      return lowerSharedToDistributed(op, adaptor, rewriter);\n+    }\n     // TODO: to be implemented\n     llvm_unreachable(\"unsupported layout conversion\");\n     return failure();\n@@ -482,9 +486,40 @@ struct ConvertLayoutOpConversion\n       }\n     }\n \n-    SmallVector<Type> types(outElems, llvmElemTy);\n-    auto *ctx = llvmElemTy.getContext();\n-    Type structTy = struct_ty(types);\n+    Value result =\n+        getTypeConverter()->packLLElements(loc, outVals, rewriter, dstTy);\n+    rewriter.replaceOp(op, result);\n+\n+    return success();\n+  }\n+\n+  LogicalResult\n+  lowerSharedToDistributed(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+                           ConversionPatternRewriter &rewriter) const {\n+    auto loc = op.getLoc();\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto srcShape = srcTy.getShape();\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    auto dstShape = dstTy.getShape();\n+    assert(dstShape.size() == 2 &&\n+           \"Unexpected rank of ConvertLayout(shared->blocked)\");\n+    auto srcSharedLayout = srcTy.getEncoding().cast<SharedEncodingAttr>();\n+    auto dstLayout = dstTy.getEncoding();\n+    auto inOrd = getOrder(srcSharedLayout);\n+\n+    auto smemObj =\n+        getSharedMemoryObjectFromStruct(loc, adaptor.getSrc(), rewriter);\n+    auto elemTy = getTypeConverter()->convertType(dstTy.getElementType());\n+\n+    auto srcStrides =\n+        getStridesFromShapeAndOrder(srcShape, inOrd, loc, rewriter);\n+    auto dstIndices = emitIndices(loc, rewriter, dstLayout, dstTy);\n+\n+    SmallVector<Value> outVals = loadSharedToDistributed(\n+        dst, dstIndices, src, smemObj, elemTy, loc, rewriter);\n+\n     Value result =\n         getTypeConverter()->packLLElements(loc, outVals, rewriter, dstTy);\n     rewriter.replaceOp(op, result);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 49, "deletions": 5, "changes": 54, "file_content_changes": "@@ -359,6 +359,55 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return ret;\n   }\n \n+  SmallVector<Value>\n+  loadSharedToDistributed(Value dst, ArrayRef<SmallVector<Value>> dstIndices,\n+                          Value src, SharedMemoryObject smemObj, Type elemTy,\n+                          Location loc,\n+                          ConversionPatternRewriter &rewriter) const {\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    auto dstShape = dstTy.getShape();\n+    assert(dstShape.size() == 2 &&\n+           \"Unexpected rank of loadSharedToDistributed\");\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto dstDistributedLayout = dstTy.getEncoding();\n+    if (auto mmaLayout = dstDistributedLayout.dyn_cast<MmaEncodingAttr>()) {\n+      assert((!mmaLayout.isVolta()) &&\n+             \"ConvertLayout Shared->MMAv1 is not supported yet\");\n+    }\n+    auto srcSharedLayout =\n+        srcTy.getEncoding().cast<triton::gpu::SharedEncodingAttr>();\n+    auto srcElemTy = srcTy.getElementType();\n+    auto dstElemTy = dstTy.getElementType();\n+    auto inOrd = triton::gpu::getOrder(srcSharedLayout);\n+    auto outOrd = triton::gpu::getOrder(dstDistributedLayout);\n+    unsigned outVec =\n+        inOrd == outOrd\n+            ? triton::gpu::getContigPerThread(dstDistributedLayout)[outOrd[0]]\n+            : 1;\n+    unsigned inVec = srcSharedLayout.getVec();\n+    unsigned minVec = std::min(outVec, inVec);\n+    unsigned outElems = triton::gpu::getTotalElemsPerThread(dstTy);\n+    assert(outElems == dstIndices.size());\n+\n+    DenseMap<unsigned, Value> sharedPtrs = getSwizzledSharedPtrs(\n+        loc, outVec, dstTy, srcSharedLayout, srcElemTy, smemObj, rewriter,\n+        smemObj.offsets, smemObj.strides);\n+    assert(outElems % minVec == 0 && \"Unexpected number of elements\");\n+    unsigned numVecs = outElems / minVec;\n+    auto wordTy = vec_ty(elemTy, minVec);\n+    SmallVector<Value> outVals(outElems);\n+    for (unsigned i = 0; i < numVecs; ++i) {\n+      Value smemAddr = sharedPtrs[i * minVec];\n+      smemAddr = bitcast(smemAddr, ptr_ty(wordTy, 3));\n+      Value valVec = load(smemAddr);\n+      for (unsigned v = 0; v < minVec; ++v) {\n+        Value currVal = extract_element(dstElemTy, valVec, i32_val(v));\n+        outVals[i * minVec + v] = currVal;\n+      }\n+    }\n+    return outVals;\n+  }\n+\n   void storeDistributedToShared(Value src, Value llSrc,\n                                 ArrayRef<Value> dstStrides,\n                                 ArrayRef<SmallVector<Value>> srcIndices,\n@@ -386,16 +435,11 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n             : 1;\n     unsigned outVec = dstSharedLayout.getVec();\n     unsigned minVec = std::min(outVec, inVec);\n-    unsigned perPhase = dstSharedLayout.getPerPhase();\n-    unsigned maxPhase = dstSharedLayout.getMaxPhase();\n     unsigned numElems = triton::gpu::getTotalElemsPerThread(srcTy);\n     assert(numElems == srcIndices.size());\n     auto inVals =\n         getTypeConverter()->unpackLLElements(loc, llSrc, rewriter, srcTy);\n     auto wordTy = vec_ty(elemTy, minVec);\n-    auto elemPtrTy = ptr_ty(elemTy);\n-    Value outVecVal = i32_val(outVec);\n-    Value minVecVal = i32_val(minVec);\n     Value word;\n \n     SmallVector<Value> srcStrides = {dstStrides[0], dstStrides[1]};"}, {"filename": "python/test/regression/test_functional_regressions.py", "status": "modified", "additions": 68, "deletions": 0, "changes": 68, "file_content_changes": "@@ -1,4 +1,6 @@\n+import numpy as np\n import torch\n+from numpy.random import RandomState\n \n import triton\n import triton.language as tl\n@@ -66,3 +68,69 @@ def chained_matmul_kernel(\n                                 block_k=block_k)\n \n     assert (torch_result == triton_result).all()\n+\n+\n+def test_vecmat():\n+    @triton.jit\n+    def batched_vecmat(\n+        # inputs\n+        A,  # shape: [dim_m, dim_k]\n+        B,  # shape: [dim_m, dim_n, dim_k]\n+        # dimensions\n+        dim_m, dim_n, dim_k,\n+        # outputs\n+        output,\n+        # block information\n+        block_m: tl.constexpr, block_n: tl.constexpr, block_k: tl.constexpr\n+    ):\n+        m_index = tl.program_id(0)\n+        n_index = tl.program_id(1)\n+        # Output tile\n+        output_tile = (m_index * block_m + tl.arange(0, block_m))[:, None] * dim_n \\\n+            + (n_index * block_n + tl.arange(0, block_n))[None, :]\n+\n+        vecmat = tl.zeros([block_m, block_n], dtype=A.dtype.element_ty)\n+        k_blocks = dim_k // block_k\n+        for k_index in range(k_blocks):\n+            # Load A tile\n+            a_tile = (m_index * block_m + tl.arange(0, block_m))[:, None] * dim_k \\\n+                + (k_index * block_k + tl.arange(0, block_k))[None, :]\n+            a = tl.load(A + a_tile)\n+\n+            # Load B tile, transposed to [n, m, k] in order to broadcast A on a\n+            # leading dimension.\n+            b_tile = (m_index * block_m + tl.arange(0, block_m))[None, :, None] * dim_n * dim_k \\\n+                + (n_index * block_n + tl.arange(0, block_n))[:, None, None] * dim_k \\\n+                + (k_index * block_k + tl.arange(0, block_k))[None, None, :]\n+            b = tl.load(B + b_tile)\n+\n+            expanded_a, _ = tl.broadcast(a, b)\n+            vecmat += tl.trans(tl.sum(expanded_a * b, axis=2))\n+\n+        tl.store(output + output_tile, vecmat)\n+\n+    M, N, K = 128, 128, 128\n+    block_m, block_n, block_k = 16, 32, 64\n+\n+    rs = RandomState(17)\n+    A_vec = rs.randint(0, 4, (M, K)).astype('float32')\n+    B_vec = rs.randint(0, 4, (M, N, K)).astype('float32')\n+    A = A_vec\n+    B = B_vec\n+\n+    A_tri = torch.tensor(A, device='cuda')\n+    B_tri = torch.tensor(B, device='cuda')\n+    C_tri = torch.zeros((M, N), dtype=torch.float32, device='cuda')\n+\n+    grid = (M // block_m, N // block_n)\n+\n+    batched_vecmat[grid](A_tri, B_tri, M, N, K, C_tri,\n+                         block_m=block_m, block_n=block_n, block_k=block_k,\n+                         num_warps=4, num_stages=1)\n+\n+    A_expanded = A[:, np.newaxis, :]\n+    A_broadcasted = np.broadcast_to(A_expanded, (M, N, K))\n+    AB = A_broadcasted * B\n+    C_ref = np.sum(AB, axis=2)\n+\n+    np.testing.assert_allclose(C_ref, C_tri.cpu().numpy(), rtol=0.01, atol=1e-3)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 45, "deletions": 8, "changes": 53, "file_content_changes": "@@ -130,6 +130,17 @@ def __str__(self):\n         return f\"#triton_gpu.blocked<{{sizePerThread={self.sz_per_thread}, threadsPerWarp={self.threads_per_warp}, warpsPerCTA={self.warps_per_cta}, order={self.order}}}>\"\n \n \n+class SharedLayout:\n+    def __init__(self, vec, per_phase, max_phase, order):\n+        self.vec = str(vec)\n+        self.per_phase = str(per_phase)\n+        self.max_phase = str(max_phase)\n+        self.order = str(order)\n+\n+    def __str__(self):\n+        return f\"#triton_gpu.shared<{{vec={self.vec}, perPhase={self.per_phase}, maxPhase={self.max_phase}, order={self.order}}}>\"\n+\n+\n @pytest.mark.parametrize(\"dtype_x\", list(dtypes) + [\"bfloat16\"])\n def test_empty_kernel(dtype_x, device='cuda'):\n     SIZE = 128\n@@ -2810,22 +2821,49 @@ def kernel(Out):\n     BlockedLayout([4, 4], [1, 32], [4, 1], [1, 0])\n ]\n \n+intermediate_layouts = [\n+    None,\n+    SharedLayout(1, 1, 1, [1, 0]),\n+    SharedLayout(4, 2, 4, [1, 0]),\n+    SharedLayout(2, 2, 4, [1, 0]),\n+]\n+\n \n @pytest.mark.parametrize(\"shape\", [(128, 128)])\n @pytest.mark.parametrize(\"dtype\", ['float16'])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n+@pytest.mark.parametrize(\"interm_layout\", intermediate_layouts)\n @pytest.mark.parametrize(\"dst_layout\", layouts)\n-def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n+def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device='cuda'):\n     if str(src_layout) == str(dst_layout):\n         pytest.skip()\n     if 'mma' in str(src_layout) and 'mma' in str(dst_layout):\n         pytest.skip()\n \n-    ir = f\"\"\"\n-#src = {src_layout}\n-#dst = {dst_layout}\n-\"\"\" + \"\"\"\n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+    layouts = f\"\"\"\n+    #src = {src_layout}\n+    #dst = {dst_layout}\n+    \"\"\" if interm_layout is None else f\"\"\"\n+    #src = {src_layout}\n+    #interm = {interm_layout}\n+    #dst = {dst_layout}\n+    \"\"\"\n+\n+    conversion = f\"\"\"\n+    %12 = triton_gpu.convert_layout %9 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #dst>\n+    %13 = triton_gpu.convert_layout %11 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #dst>\n+    \"\"\" if interm_layout is None else f\"\"\"\n+    %15 = triton_gpu.convert_layout %9 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #interm>\n+    %16 = triton_gpu.convert_layout %15 : (tensor<128x128xi32, #interm>) -> tensor<128x128xi32, #src>\n+    %17 = triton_gpu.convert_layout %11 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #interm>\n+    %18 = triton_gpu.convert_layout %17 : (tensor<128x128xf16, #interm>) -> tensor<128x128xf16, #src>\n+\n+    %12 = triton_gpu.convert_layout %16 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #dst>\n+    %13 = triton_gpu.convert_layout %18 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #dst>\n+    \"\"\"\n+\n+    ir = layouts + \"\"\"\n+    module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   tt.func public @kernel_0d1d(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n     %cst = arith.constant dense<128> : tensor<128x1xi32, #src>\n     %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #src}>>\n@@ -2840,8 +2878,7 @@ def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n     %10 = tt.addptr %2, %9 : tensor<128x128x!tt.ptr<f16>, #src>, tensor<128x128xi32, #src>\n     %11 = tt.load %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf16, #src>\n     %3 = tt.splat %arg1 : (!tt.ptr<f16>) -> tensor<128x128x!tt.ptr<f16>, #dst>\n-    %12 = triton_gpu.convert_layout %9 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #dst>\n-    %13 = triton_gpu.convert_layout %11 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #dst>\n+    \"\"\" + conversion + \"\"\"\n     %14 = tt.addptr %3, %12 : tensor<128x128x!tt.ptr<f16>, #dst>, tensor<128x128xi32, #dst>\n     tt.store %14, %13 : tensor<128x128xf16, #dst>\n     tt.return"}]