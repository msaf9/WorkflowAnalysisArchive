[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 26, "deletions": 10, "changes": 36, "file_content_changes": "@@ -2334,31 +2334,47 @@ def val_multiplier(val, i):\n     return val * i\n \n \n+@triton.jit(noinline=True)\n+def val_multiplier_noinline(val, i):\n+    return val * i\n+\n+\n @triton.jit\n-def vecmul_kernel(ptr, n_elements, rep):\n+def vecmul_kernel(ptr, n_elements, rep, type: tl.constexpr):\n     pid = tl.program_id(axis=0)\n     offsets = pid * 128 + tl.arange(0, 128)\n     mask = offsets < n_elements\n     vec = tl.load(ptr + offsets, mask=mask)\n     for i in range(1, rep):\n-        vec = val_multiplier(vec, i)\n+        if type == \"inline\":\n+            vec = val_multiplier(vec, i)\n+        else:\n+            vec = val_multiplier_noinline(vec, i)\n     tl.store(ptr + offsets, vec, mask=mask)\n \n \n-def test_call():\n+@pytest.mark.parametrize(\"type\", [\"inline\", \"noinline\"])\n+def test_call(type):\n \n     @triton.jit\n-    def kernel(ptr, n_elements, num1, num2):\n-        vecmul_kernel(ptr, n_elements, num1)\n-        vecmul_kernel(ptr, n_elements, num2)\n+    def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n+        vecmul_kernel(ptr, n_elements, num1, type)\n+        vecmul_kernel(ptr, n_elements, num2, type)\n \n     size = 1024\n     rand_val = numpy_random((size,), dtype_str=\"float32\")\n     rand_val_tri = to_triton(rand_val, device='cuda')\n-    kernel[(size // 128,)](rand_val_tri, size, 3, 5)\n-\n-    ans = rand_val * 1 * 2 * 1 * 2 * 3 * 4\n-    np.testing.assert_equal(to_numpy(rand_val_tri), ans)\n+    err_msg = \"\"\n+    try:\n+        kernel[(size // 128,)](rand_val_tri, size, 3, 5, type)\n+    except Exception as e:\n+        err_msg = str(e)\n+\n+    if type == \"noinline\":\n+        assert err_msg is not \"\"\n+    else:\n+        ans = rand_val * 1 * 2 * 1 * 2 * 3 * 4\n+        np.testing.assert_equal(to_numpy(rand_val_tri), ans)\n \n # -------------\n # test if"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -60,10 +60,21 @@ def _is_constexpr(o: Any) -> bool:\n     return isinstance(o, constexpr)\n \n \n+def _is_triton_scalar(o: Any) -> bool:\n+    return _is_triton_tensor(o) and (not o.type.is_block() or o.type.numel == 1)\n+\n+\n def _unwrap_if_constexpr(o: Any):\n     return o.value if isinstance(o, constexpr) else o\n \n \n+def _check_fn_args(node, fn, args):\n+    if fn.noinline:\n+        for idx, arg in enumerate(args):\n+            if not _is_constexpr(arg) and not _is_triton_scalar(arg):\n+                raise UnsupportedLanguageConstruct(fn.src, node, f'Function {fn.__name__} is marked noinline, but was called with non-scalar argument {fn.arg_names[idx]}:{arg}')\n+\n+\n _condition_types = {bool, int, type(None)}  # Python types accepted for conditionals inside kernels\n \n \n@@ -844,6 +855,7 @@ def visit_Call(self, node):\n             if not self.debug:\n                 return\n         if isinstance(fn, JITFunction):\n+            _check_fn_args(node, fn, args)\n             return self.call_JitFunction(fn, args, kws)\n         if (hasattr(fn, '__self__') and _is_triton_tensor(fn.__self__)) or language.core.is_builtin(fn):\n             extra_kwargs = dict(_builder=self.builder)"}]