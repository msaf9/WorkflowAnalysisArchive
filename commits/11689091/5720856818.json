[{"filename": "python/test/unit/tools/test_aot.py", "status": "modified", "additions": 88, "deletions": 48, "changes": 136, "file_content_changes": "@@ -40,17 +40,20 @@ def kernel(C, A, B,\n   tl.store(C + ms[:, None] * stride_cm + ns[None, :] * stride_cn, c)\n \"\"\"\n \n-test_src = \"\"\"\n+\n+def gen_test_bin(dir, M, N, K, BM, BN, BK):\n+    test_src = '''\n #include <cuda.h>\n #include <stdio.h>\n #include <stdint.h>\n #include <string.h>\n+#include <assert.h>\n #include \"kernel.h\"\n \n static void write_buffer_to_csv(char *filename, int32_t *buffer, int size) {\n     FILE *file = fopen(filename, \"w\");\n     if (file == NULL) {\n-        printf(\\\"Could not open file %s\\\\n\\\", filename);\n+        printf(\"Could not open file %s\\\\n\", filename);\n         return;\n     }\n     for (int i = 0; i < size; i++) {\n@@ -65,19 +68,20 @@ def kernel(C, A, B,\n static void read_csv_to_buffer(char *filename, int16_t *buffer, int size) {\n     FILE *file = fopen(filename, \"r\");\n     if (file == NULL) {\n-        printf(\\\"Could not open file %s\\\\n\\\", filename);\n+        printf(\"Could not open file %s\\\\n\", filename);\n         return;\n     }\n     int index = 0;\n     while (fscanf(file, \"%hd,\", &buffer[index]) != EOF && index < size) {\n         index++;\n     }\n     fclose(file);\n-}\n+}'''\n \n-int main(int argc, char **argv) {\n-  int M = 16, N = 16, K = 16;\n-  int BM = 16, BN = 16, BK = 16;\n+    test_src += f'''\n+int main(int argc, char **argv) {{\n+  int M = {M}, N = {N}, K = {K};\n+  int BM = {M}, BN = {N}, BK = {K};\n \n   // initialize CUDA handles\n   CUdevice dev;\n@@ -105,9 +109,11 @@ def kernel(C, A, B,\n   cuMemcpyHtoD(B, hB, K*N*2);\n \n   // launch kernel\n-  int gX = 1, gY = 1, gZ = 1;\n   cuStreamSynchronize(stream);\n-  matmul_fp16xfp16_16x16x16(stream, M/BM, N/BN, 1, C, A, B, N, 1, K, 1, N, 1);\n+  CUresult ret = matmul_fp16xfp16_16x16x16(stream, M/BM, N/BN, 1, C, A, B, N, 1, K, 1, N, 1);\n+  if (ret != 0) fprintf(stderr, \"kernel launch failed\\\\n\");\n+  assert(ret == 0);\n+\n   cuStreamSynchronize(stream);\n \n   // read data\n@@ -116,65 +122,75 @@ def kernel(C, A, B,\n   cuMemcpyDtoH(hC, C, M*N*4);\n   write_buffer_to_csv(argv[3], hC, M*N);\n \n-\n   // free cuda handles\n   unload_matmul_fp16xfp16_16x16x16();\n   cuMemFree(A);\n   cuMemFree(B);\n   cuMemFree(C);\n   cuCtxDestroy(ctx);\n-}\n-\"\"\"\n+}}\n+'''\n+\n+    with open(os.path.join(dir, \"test.c\"), \"w\") as file:\n+        file.write(test_src)\n+    c_files = glob.glob(os.path.join(dir, \"*.c\"))\n+    subprocess.run([\"gcc\"] + c_files + [\"-I\", cuda_include_dir(),\n+                                        \"-L\", libcuda_dirs()[0],\n+                                        \"-l\", \"cuda\",\n+                                        \"-o\", \"test\"], check=True, cwd=dir)\n+\n+\n+def generate_matmul_launcher(dir, dtype, BM, BN, BK, ha_hb_hints):\n+    kernel_path = os.path.join(dir, \"kernel.py\")\n+    with open(kernel_path, \"w\") as file:\n+        file.write(kernel_src)\n+\n+    kernel_utils_path = os.path.join(dir, \"kernel_utils.py\")\n+    with open(kernel_utils_path, \"w\") as file:\n+        file.write(kernel_utils_src)\n+\n+    compiler_path = os.path.join(triton.tools.__path__[0], \"compile.py\")\n+    linker_path = os.path.join(triton.tools.__path__[0], \"link.py\")\n+\n+    # compile all desired configs\n+    for ha in ha_hb_hints:\n+        for hb in ha_hb_hints:\n+            sig = f'*fp32:16, *{dtype}:16, *{dtype}:16, i32{ha}, i32:1, i32{hb}, i32:1, i32:16, i32:1, {BM}, {BN}, {BK}'\n+            name = f\"matmul_{dtype}x{dtype}_{BM}x{BN}x{BK}\"\n+            subprocess.run([sys.executable, compiler_path, \"-n\", \"kernel\", \"--signature\", sig, \"--out-name\", name, \"-o\", name, \"-w\", \"1\", kernel_path], check=True, cwd=dir)\n+\n+    # link all desired configs\n+    h_files = glob.glob(os.path.join(dir, \"*.h\"))\n+    subprocess.run([sys.executable, linker_path] + h_files + [\"-o\", \"kernel\"], check=True, cwd=dir)\n+\n+\n+def generate_matmul_test_data(dir, M, N, K):\n+    a = np.random.randn(M * K).astype(np.float16).reshape((M, K))\n+    b = np.random.randn(M * K).astype(np.float16).reshape((K, N))\n+    a_path = os.path.join(dir, \"a.csv\")\n+    b_path = os.path.join(dir, \"b.csv\")\n+    c_path = os.path.join(dir, \"c.csv\")\n+    for x, path in [(a, a_path), (b, b_path)]:\n+        x.view(np.int16).ravel().tofile(path, sep=\",\")\n+    return a, b, a_path, b_path, c_path\n \n \n def test_compile_link_matmul():\n     np.random.seed(3)\n \n     with tempfile.TemporaryDirectory() as tmp_dir:\n-        kernel_path = os.path.join(tmp_dir, \"kernel.py\")\n-        with open(kernel_path, \"w\") as file:\n-            file.write(kernel_src)\n-\n-        kernel_utils_path = os.path.join(tmp_dir, \"kernel_utils.py\")\n-        with open(kernel_utils_path, \"w\") as file:\n-            file.write(kernel_utils_src)\n-\n-        compiler_path = os.path.join(triton.tools.__path__[0], \"compile.py\")\n-        linker_path = os.path.join(triton.tools.__path__[0], \"link.py\")\n \n         dtype = \"fp16\"\n-        M, N, K = 16, 16, 16\n         BM, BN, BK = 16, 16, 16\n \n-        # compile all desired configs\n-        hints = [\":16\", \"\"]\n-        for ha in hints:\n-            for hb in hints:\n-                sig = f'*fp32:16, *{dtype}:16, *{dtype}:16, i32{ha}, i32:1, i32{hb}, i32:1, i32:16, i32:1, {BM}, {BN}, {BK}'\n-                name = f\"matmul_{dtype}x{dtype}_{BM}x{BN}x{BK}\"\n-                subprocess.run([sys.executable, compiler_path, \"-n\", \"kernel\", \"--signature\", sig, \"--out-name\", name, \"-o\", name, \"-w\", \"1\", kernel_path], check=True, cwd=tmp_dir)\n-\n-        # link all desired configs\n-        h_files = glob.glob(os.path.join(tmp_dir, \"*.h\"))\n-        subprocess.run([sys.executable, linker_path] + h_files + [\"-o\", \"kernel\"], check=True, cwd=tmp_dir)\n+        generate_matmul_launcher(tmp_dir, dtype, BM, BN, BK, ha_hb_hints=[\"\", \":16\"])\n \n         # compile test case\n-        with open(os.path.join(tmp_dir, \"test.c\"), \"w\") as file:\n-            file.write(test_src)\n-        c_files = glob.glob(os.path.join(tmp_dir, \"*.c\"))\n-        subprocess.run([\"gcc\"] + c_files + [\"-I\", cuda_include_dir(),\n-                                            \"-L\", libcuda_dirs()[0],\n-                                            \"-l\", \"cuda\",\n-                                            \"-o\", \"test\"], check=True, cwd=tmp_dir)\n+        M, N, K = 16, 16, 16\n+        gen_test_bin(tmp_dir, M, N, K, BM, BN, BK)\n \n         # initialize test data\n-        a = np.random.randn(M * K).astype(np.float16).reshape((M, K))\n-        b = np.random.randn(M * K).astype(np.float16).reshape((K, N))\n-        a_path = os.path.join(tmp_dir, \"a.csv\")\n-        b_path = os.path.join(tmp_dir, \"b.csv\")\n-        c_path = os.path.join(tmp_dir, \"c.csv\")\n-        for x, path in [(a, a_path), (b, b_path)]:\n-            x.view(np.int16).ravel().tofile(path, sep=\",\")\n+        a, b, a_path, b_path, c_path = generate_matmul_test_data(tmp_dir, M, N, K)\n \n         # run test case\n         subprocess.run([\"./test\", a_path, b_path, c_path], check=True, cwd=tmp_dir)\n@@ -186,6 +202,30 @@ def test_compile_link_matmul():\n         np.testing.assert_allclose(c_tri, c_ref * c_ref, atol=1e-4, rtol=0.)\n \n \n+def test_launcher_has_no_available_kernel():\n+    np.random.seed(3)\n+\n+    with tempfile.TemporaryDirectory() as tmp_dir:\n+        dtype = \"fp16\"\n+        BM, BN, BK = 16, 16, 16\n+\n+        generate_matmul_launcher(tmp_dir, dtype, BM, BN, BK, ha_hb_hints=[\":1\"])\n+\n+        # compile test case\n+        M, N, K = 16, 16, 16\n+        gen_test_bin(tmp_dir, M, N, K, BM, BN, BK)\n+\n+        # initialize test data\n+        a, b, a_path, b_path, c_path = generate_matmul_test_data(tmp_dir, M, N, K)\n+\n+        # run test case\n+        result = subprocess.run([\"./test\", a_path, b_path, c_path], cwd=tmp_dir, capture_output=True, text=True)\n+\n+        # It should fail since the launcher requires all the strides be 1 while they are not.\n+        assert result.returncode == -6\n+        assert \"kernel launch failed\" in result.stderr\n+\n+\n def test_ttgir_to_ptx():\n     src = \"\"\"\n module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {"}, {"filename": "python/triton/tools/link.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -148,6 +148,8 @@ def make_kernel_dispatcher(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n         src += f\"  if ({conds})\\n\"\n         arg_names = [arg for arg, hint in zip(meta.arg_names, meta.sizes) if hint != 1]\n         src += f\"    return {name}_{meta.sig_hash}_{meta.suffix}(stream, gX, gY, gZ, {', '.join(arg_names)});\\n\"\n+    src += \"\\n\"\n+    src += \"  return CUDA_ERROR_INVALID_VALUE;\\n\"\n     src += \"}\\n\"\n \n     for mode in [\"load\", \"unload\"]:"}]