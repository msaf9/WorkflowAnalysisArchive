[{"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -11,6 +11,7 @@ using ::mlir::LLVM::delinearize;\n using ::mlir::LLVM::linearize;\n using ::mlir::LLVM::shflSync;\n using ::mlir::LLVM::storeShared;\n+using ::mlir::triton::gpu::getCTASplitNum;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n \n@@ -28,6 +29,12 @@ struct ReduceOpConversion\n   LogicalResult\n   matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n+    // When cross-CTA reduction is implemented in the future, this assertion can\n+    // be removed\n+    assert(isReduceWithinCTA(op) &&\n+           \"Layout optimization passes such as PlanCTAPass and \"\n+           \"RemoveLayoutConversionPass should avoid cross-CTA reduction\");\n+\n     if (ReduceOpHelper(op).isFastReduction())\n       return matchAndRewriteFast(op, adaptor, rewriter);\n     return matchAndRewriteBasic(op, adaptor, rewriter);\n@@ -36,6 +43,15 @@ struct ReduceOpConversion\n private:\n   int computeCapability;\n \n+  bool isReduceWithinCTA(triton::ReduceOp op) const {\n+    auto axis = op.getAxis();\n+    ReduceOpHelper helper(op);\n+    auto srcLayout = helper.getSrcLayout();\n+    auto CTASplitNum = getCTASplitNum(srcLayout);\n+    assert(axis < CTASplitNum.size());\n+    return CTASplitNum[axis] == 1;\n+  }\n+\n   void accumulate(ConversionPatternRewriter &rewriter, Region &combineOp,\n                   SmallVector<Value> &acc, ValueRange cur, bool isFirst) const {\n     if (isFirst) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 11, "deletions": 3, "changes": 14, "file_content_changes": "@@ -586,9 +586,17 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n             continue;\n           // This wrapping rule must be consistent with emitCTAOffsetForLayout\n           unsigned splitNum = std::min<unsigned>(shape[dim], CTASplitNum[dim]);\n-          multiDimClusterCTAId[dim] =\n-              urem(multiDimClusterCTAId[dim], i32_val(splitNum));\n-          mask = and_(mask, icmp_eq(multiDimClusterCTAId[dim], _0));\n+          Value repId = udiv(multiDimClusterCTAId[dim], i32_val(splitNum));\n+          // Consider the example where CTAsPerCGA = [4] and CTASplitNum = [2]:\n+          //     CTA0 and CTA2 holds data of block0,\n+          //     CTA1 and CTA3 holds data of block1.\n+          // Only CTA0 and CTA1 are expected to write while CTA2 and CTA3 should\n+          // be masked. We add the following mask:\n+          //     multiDimClusterCTAId[dim] / splitNum == 0\n+          // Actually in all existing cases of multicast, splitNum is always 1.\n+          // The mask is equivalent to:\n+          //     multiDimClusterCTAId[dim] == 0\n+          mask = and_(mask, icmp_eq(repId, _0));\n         }\n       }\n     } else {"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -121,6 +121,11 @@ class SimplifyReduceCvt : public mlir::RewritePattern {\n     auto newEncoding =\n         newOperands[0].getType().cast<RankedTensorType>().getEncoding();\n \n+    // This will result in cross-CTA reduction, which is not implemented yet\n+    if (triton::gpu::getCTASplitNum(newEncoding)[reduce.getAxis()] > 1) {\n+      return failure();\n+    }\n+\n     // this may generate unsupported conversions in the LLVM codegen\n     if (newEncoding.isa<triton::gpu::MmaEncodingAttr>()) {\n       return failure();"}]