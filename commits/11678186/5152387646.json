[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -307,8 +307,7 @@ class ConvertTritonGPUToLLVM\n     // Preprocess\n     decomposeMmaToDotOperand(mod, numWarps);\n     decomposeBlockedToDotOperand(mod);\n-    if (failed(decomposeInsertSliceAsyncOp(mod)))\n-      return signalPassFailure();\n+    decomposeInsertSliceAsyncOp(mod);\n \n     // Allocate shared memory and set barrier\n     ModuleAllocation allocation(mod);\n@@ -487,7 +486,7 @@ class ConvertTritonGPUToLLVM\n     });\n   }\n \n-  LogicalResult decomposeInsertSliceAsyncOp(ModuleOp mod) const {\n+  void decomposeInsertSliceAsyncOp(ModuleOp mod) const {\n     ModuleAxisInfoAnalysis axisInfoAnalysis(mod);\n     // TODO(Keren): This is a hacky knob that may cause performance regression\n     // when decomposition has been performed. We should remove this knob once we\n@@ -515,6 +514,7 @@ class ConvertTritonGPUToLLVM\n       // Get the vectorized load size\n       auto src = insertSliceAsyncOp.getSrc();\n       auto dst = insertSliceAsyncOp.getDst();\n+      auto mask = insertSliceAsyncOp.getMask();\n       auto srcTy = src.getType().cast<RankedTensorType>();\n       auto dstTy = dst.getType().cast<RankedTensorType>();\n       auto srcBlocked =\n@@ -523,6 +523,9 @@ class ConvertTritonGPUToLLVM\n           dstTy.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n       auto resElemTy = dstTy.getElementType();\n       unsigned inVec = axisInfoAnalysis.getPtrContiguity(src);\n+      if (mask)\n+        inVec =\n+            std::min<unsigned>(axisInfoAnalysis.getMaskAlignment(mask), inVec);\n       unsigned outVec = resSharedLayout.getVec();\n       unsigned minVec = std::min(outVec, inVec);\n       auto maxBitWidth =\n@@ -586,7 +589,6 @@ class ConvertTritonGPUToLLVM\n         asyncWaitOp.erase();\n       }\n     });\n-    return success();\n   }\n };\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 512, "deletions": 325, "changes": 837, "file_content_changes": "@@ -8,23 +8,87 @@\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+#include \"llvm/ADT/MapVector.h\"\n \n //===----------------------------------------------------------------------===//\n+// This file implements software pipelining for loops. The implementation here\n+// is inspired by the pipeline pass in Triton (version 2.0) and SCF's\n+// LoopPipelining.\n //\n-// This file implements loop software pipelining\n-// The implementation here is inspired by the pipeline pass in Triton (-v2.0)\n-// and SCF's LoopPipelining.\n+// We divide the loop body into the following phases:\n+// a. Pre-load operations: for instance, index computation.\n+// b. Load operations: loading from global memory to shared memory.\n+// c. Compute operations: for instance, Triton dot.\n+// d. Post-load operations: for instance, index computation.\n+//\n+// To pipeline the loop, we need to:\n+// - Hoist the pipelinable load operations for the first numStages-1 iterations\n+// to the loop pre-header\n+// - Find all the dependencies of the load operations.\n+// - Rematerialize the dependencies for their values at the first numStage-1\n+// iterations\n+// - Assemble the loop body (numStage) and prefetch (numStage + 1).\n+//\n+// In the prologue, the sequence of operations is the same as the original loop\n+// body, following the (a) -> (b) -> (c) -> (d) order. In the loop body,\n+// however, we first execute the compute operations, then pre-load operations,\n+// post-load operations, and eventually the asynchronous load operations - in\n+// the (c) -> (a) -> (d) -> (b) order. This is used to better hide the latency\n+// of the load operations. Because of this, if post-load operations have direct\n+// dependencies on the load operations, we could repeat the post-load\n+// operations. More specifically, this occurs when:\n+// 1. Any load operand has an immediate dependency argument used at numStage-1.\n+// 2. The argument is first defined at numStage-2.\n+// To avoid the repeat, we peeled off post-load operations in the prologue that\n+// satisfy the above two conditions. See the example below for the definition of\n+// immediate and non-immediate dependencies.\n+// If we have a load that immediately depends on a block argument in the\n+// current iteration, it is an immediate dependency. Otherwise, it is a\n+// non-immediate dependency, which means the load depends on a block argument\n+// in the previous iterations.\n+// For example:\n+// scf.for (%arg0, %arg1, %arg2) {\n+//   %0 = load %arg0  <--- immediate dep, this address is initialized before\n+//   numStages-1.\n+//   %1 = load %arg1\n+//   %2 = add %1, %arg2\n+//   %3 = load %2  <--- non-immediate dep, %arg1 must be an\n+//   update-to-date value.\n+// }\n+//\n+// Our pipelining pass share some common characteristics with SCF's\n+// LoopPipelining. However, it is also noteworthy that our pipelining pass has\n+// the following characteristics different from SCF's LoopPipelining:\n+// 1. It can handle loop-carried dependencies of distance greater than 1.\n+// 2. It does not have a complicated epilogue but instead uses masking to handle\n+// boundary conditions.\n+// 3. Each operation/loop-carried argument cannot provide values to both\n+// immediate and non-immediate dependencies. Otherwise, we have to rematerialize\n+// the operation and arguments, which would likely increase register pressure.\n+// For example:\n+// scf.for (%arg0, %arg1, %arg2) {\n+//  %0 = load %arg0\n+//  %1 = load %arg1, %0  <--- %0 is both a post-load op at numStages-2 and a\n+//  pre-load op at numStages-1, so that we need two versions of %0.\n+//  %2 = add %0, %arg2\n+//  scf.yield %arg0, %2, %arg2\n+//  }\n //\n //===----------------------------------------------------------------------===//\n \n+using llvm::MapVector;\n using namespace mlir;\n namespace ttg = triton::gpu;\n \n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n \n-// pass named attrs (e.g., tt.contiguity) from Triton to Triton\n-static void addNamedAttrs(Operation *op, DictionaryAttr dictAttrs) {\n+#define int_attr(num) builder.getI64IntegerAttr(num)\n+\n+namespace {\n+\n+// Pass named attrs (e.g., tt.contiguity) from Triton to Triton\n+void addNamedAttrs(Operation *op, DictionaryAttr dictAttrs) {\n   NamedAttrList attrs = op->getDiscardableAttrs();\n   // Collect the attributes to propagate: the ones in dictAttrs and not yet on\n   // the operation.\n@@ -40,19 +104,13 @@ static void addNamedAttrs(Operation *op, DictionaryAttr dictAttrs) {\n   }\n }\n \n-#define int_attr(num) builder.getI64IntegerAttr(num)\n-\n-namespace {\n-\n class LoopPipeliner {\n-  /// Cache forOp we are working on\n+  /// Cache of ForOp and YieldOp related to this pipeliner.\n   scf::ForOp forOp;\n-\n-  /// Cache YieldOp for this forOp\n   scf::YieldOp yieldOp;\n \n   /// Loads to be pipelined\n-  SetVector<Value> loads;\n+  SetVector<Value> validLoads;\n   /// Smallest data-type for each load (used to optimize swizzle and\n   /// (create DotOpEncoding layout)\n   DenseMap<Value, Type> loadsSmallestType;\n@@ -66,59 +124,111 @@ class LoopPipeliner {\n   DenseMap<Value, SmallVector<Value>> loadStageBuffer;\n   /// load => after extract\n   DenseMap<Value, Value> loadsExtract;\n-  ///\n+\n+  /// Iterator values\n   Value pipelineIterIdx;\n-  ///\n   Value loopIterIdx;\n+  Value nextIV;\n+\n+  /// Yield values\n+  SmallVector<Value> nextBuffers;\n+  SmallVector<Value> extractSlices;\n+  SmallVector<Value> yieldValues;\n \n-  /// Comments on numStages:\n-  ///   [0, numStages-1) are in the prologue\n-  ///   numStages-1 is appended after the loop body\n+  /// The number of stages in the pipeline.\n+  /// Stages in the range of [0, numStages-1) are in the prologue.\n+  /// numStages-1 is appended after the loop body.\n   int numStages;\n \n+  /// Arg indicies\n+  size_t bufferIdx, loadIdx, depArgsBeginIdx, ivIndex;\n+  DenseMap<BlockArgument, size_t> depArgsIdx;\n+\n   /// value (in loop) => value at stage N\n   DenseMap<Value, SmallVector<Value>> valueMapping;\n+  /// loop iter arg => value\n+  DenseMap<BlockArgument, Value> depArgsMapping;\n+  /// forOp value => newForOp value\n+  IRMapping mapping;\n+  /// forOp value => prefetch value\n+  IRMapping nextMapping;\n+\n+  /// Dependency ops by program order\n+  SmallVector<Operation *> orderedDeps;\n+\n+  /// arg => source operand defined stages\n+  DenseMap<BlockArgument, DenseSet<int>> immediateArgStages;\n \n-  /// Block arguments that loads depend on\n+  /// block arguments that loads depend on\n   SetVector<BlockArgument> depArgs;\n \n-  /// If we have a load that immediately depends on a block argument in the\n-  /// current iteration, it is an immediate dependency. Otherwise, it is a\n-  /// non-immediate dependency, which means the load depends on a block argument\n-  /// in the previous iterations.\n-  /// For example:\n-  /// scf.for (%arg0, %arg1, %arg2) {\n-  ///   %0 = load %arg0  <--- immediate dep, this address is initialized at\n-  ///   numStages-2\n-  ///   %1 = load %arg1\n-  ///   %2 = add %1, %arg2\n-  ///   %3 = load %2  <--- non-immediate dep, %arg1 must be an update-to-date\n-  ///   value\n-  /// }\n-  SetVector<BlockArgument> immedidateDepArgs;\n-\n-  SetVector<BlockArgument> nonImmedidateDepArgs;\n-\n-  /// Operations (inside the loop body) that loads depend on\n+  /// operation => source operand defined stages\n+  DenseMap<Operation *, DenseSet<int>> immediateOpStages;\n+\n+  /// operations that loads depend on\n   SetVector<Operation *> depOps;\n \n-  /// collect values that v depends on and are defined inside the loop\n-  void collectDeps(Value v, int stages, SetVector<Value> &deps);\n+  /// Collect all pipelinable ops\n+  LogicalResult collectOps(SetVector<Operation *> &ops);\n+\n+  /// Collect values that `v` depends on and are defined inside the loop\n+  void collectValueDep(Value v, int stage, SetVector<Value> &opDeps);\n+\n+  /// Collect all op dependencies\n+  void collectDeps(SetVector<Operation *> &ops,\n+                   MapVector<Operation *, SetVector<Value>> &opDeps);\n \n+  /// Check if none of the ops has valid uses\n+  LogicalResult checkOpUses(SetVector<Operation *> &ops);\n+\n+  /// Check if ops have dependencies that are not pipelinable\n+  void checkOpDeps(SetVector<Operation *> &ops);\n+\n+  void createBufferTypes();\n+\n+  void createOrderedDeps();\n+\n+  /// Return the stage at which `v` is defined prior to `stage`\n+  int getValueDefStage(Value v, int stage);\n+\n+  /// Map `origin` to `newValue` at `stage`\n   void setValueMapping(Value origin, Value newValue, int stage);\n \n+  /// Map `origin` to `newValue` at `stage` according to the association between\n+  /// yieldOp and forOp\n+  void setValueMappingYield(Value origin, Value newValue, int stage);\n+\n+  /// Map `origin` to `newValue` at the next stage according to the association\n+  /// between yieldOp and forOp\n+  void setValueMappingYield(scf::ForOp newForOp, Value origin, Value newValue);\n+\n+  /// Return the value mapped to `origin` at `stage`, if it exists.\n   Value lookupOrDefault(Value origin, int stage);\n \n+  /// Get the load mask for `loadOp`, given the mapped mask `mappedMask` (if\n+  /// exists) and the current iteration's `loopCond`.\n   Value getLoadMask(triton::LoadOp loadOp, Value mappedMask, Value loopCond,\n                     OpBuilder &builder);\n \n-  /// Returns a empty buffer of size <numStages, ...>\n-  ttg::AllocTensorOp allocateEmptyBuffer(Operation *op, OpBuilder &builder);\n+  /// Return an empty buffer of size <numStages, ...>\n+  ttg::AllocTensorOp allocateEmptyBuffer(triton::LoadOp loadOp,\n+                                         OpBuilder &builder);\n+\n+  /// Collect all args of the new loop\n+  SmallVector<Value> collectNewLoopArgs();\n+\n+  /// Clone the forOp and return the new forOp\n+  scf::ForOp cloneForOp(ArrayRef<Value> newLoopArgs, OpBuilder &builder);\n+\n+  /// Prefetch the next iteration for `newForOp`\n+  void prefetchNextIteration(scf::ForOp newForOp, OpBuilder &builder);\n+\n+  /// Assemble `newForOp`'s yield op\n+  void finalizeYield(scf::ForOp newForOp, OpBuilder &builder);\n \n public:\n   LoopPipeliner(scf::ForOp forOp, int numStages)\n       : forOp(forOp), numStages(numStages) {\n-    // cache yieldOp\n     yieldOp = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n   }\n \n@@ -137,159 +247,246 @@ class LoopPipeliner {\n   friend struct PipelinePass;\n };\n \n-// helpers\n-void LoopPipeliner::setValueMapping(Value origin, Value newValue, int stage) {\n-  if (valueMapping.find(origin) == valueMapping.end())\n-    valueMapping[origin] = SmallVector<Value>(numStages);\n-  valueMapping[origin][stage] = newValue;\n-}\n+/// Collect loads to pipeline. Return success if we can pipeline this loop\n+LogicalResult LoopPipeliner::collectOps(SetVector<Operation *> &ops) {\n+  ModuleOp moduleOp = forOp->getParentOfType<ModuleOp>();\n+  ModuleAxisInfoAnalysis axisInfoAnalysis(moduleOp);\n \n-Value LoopPipeliner::lookupOrDefault(Value origin, int stage) {\n-  if (valueMapping.find(origin) == valueMapping.end())\n-    return origin;\n-  return valueMapping[origin][stage];\n+  // We cannot use forOp.walk(...) here because we only want to visit the\n+  // operations in the loop body block. Nested blocks are handled separately.\n+  for (Operation &op : forOp)\n+    if (auto loadOp = dyn_cast<triton::LoadOp>(&op)) {\n+      auto ptr = loadOp.getPtr();\n+      unsigned vec = axisInfoAnalysis.getPtrContiguity(ptr);\n+\n+      if (auto mask = loadOp.getMask())\n+        vec = std::min<unsigned>(vec, axisInfoAnalysis.getMaskAlignment(mask));\n+\n+      auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n+      if (!tensorTy || tensorTy.getRank() < 2)\n+        continue;\n+      auto ty = tensorTy.getElementType()\n+                    .cast<triton::PointerType>()\n+                    .getPointeeType();\n+      unsigned width = vec * ty.getIntOrFloatBitWidth();\n+      // We do not pipeline all loads for the following reasons:\n+      // 1. On nvidia GPUs, cp.async's cp-size can only be 4, 8 and 16.\n+      // 2. It's likely that pipling small loads won't offer much performance\n+      //    improvement and may even hurt performance by increasing register\n+      //    pressure.\n+      if (width >= 32)\n+        ops.insert(loadOp);\n+    }\n+\n+  if (ops.empty())\n+    return failure();\n+  else\n+    return success();\n }\n \n-void LoopPipeliner::collectDeps(Value v, int stages, SetVector<Value> &deps) {\n+void LoopPipeliner::collectValueDep(Value v, int stage,\n+                                    SetVector<Value> &deps) {\n   // Loop-invariant value, skip\n   if (v.getParentRegion() != &forOp.getLoopBody())\n     return;\n \n-  // Since we only need to peel the loop numStages-1 times, don't worry about\n-  // depends that are too far away\n-  if (stages < 0)\n+  // Since we only need to peel the loop numStages-1 times, don't worry\n+  // about depends that are too far away\n+  if (stage < 0)\n     return;\n \n   if (auto arg = v.dyn_cast<BlockArgument>()) {\n     if (arg.getArgNumber() > 0) {\n-      // Skip the first arg (loop induction variable)\n-      // Otherwise the op idx is arg.getArgNumber()-1\n       deps.insert(v);\n-      collectDeps(yieldOp->getOperand(arg.getArgNumber() - 1), stages - 1,\n-                  deps);\n+      collectValueDep(yieldOp->getOperand(arg.getArgNumber() - 1), stage - 1,\n+                      deps);\n     }\n   } else { // value\n-    // v might be in deps, but we still need to visit v.\n-    // This is because v might depend on value in previous iterations\n     deps.insert(v);\n     for (Value op : v.getDefiningOp()->getOperands())\n-      collectDeps(op, stages, deps);\n+      collectValueDep(op, stage, deps);\n   }\n }\n \n-ttg::AllocTensorOp LoopPipeliner::allocateEmptyBuffer(Operation *op,\n-                                                      OpBuilder &builder) {\n-  // Allocate a buffer for each pipelined tensor\n-  // shape: e.g. (numStages==4), <32x64xbf16> -> <4x32x64xbf16>\n-  Value convertLayout = loadsMapping[op->getResult(0)];\n-  if (auto tensorType = convertLayout.getType().dyn_cast<RankedTensorType>()) {\n-    return builder.create<ttg::AllocTensorOp>(\n-        convertLayout.getLoc(), loadsBufferType[op->getResult(0)]);\n+void LoopPipeliner::collectDeps(\n+    SetVector<Operation *> &ops,\n+    MapVector<Operation *, SetVector<Value>> &valueDeps) {\n+  for (auto op : ops) {\n+    for (Value v : op->getOperands()) {\n+      SetVector<Value> deps;\n+      collectValueDep(v, numStages - 1, deps);\n+      valueDeps[op] = deps;\n+    }\n   }\n-  llvm_unreachable(\"Async copy's return should be of RankedTensorType\");\n }\n \n-/// A load instruction can be pipelined if:\n-///   - the load doesn't depend on any other loads (after loop peeling)\n-///   - (?) this load is not a loop-invariant value (we should run LICM before\n-///                                                  this pass?)\n-LogicalResult LoopPipeliner::initialize() {\n-  Block *loop = forOp.getBody();\n-  ModuleOp moduleOp = forOp->getParentOfType<ModuleOp>();\n-  ModuleAxisInfoAnalysis axisInfoAnalysis(moduleOp);\n-\n-  // can we use forOp.walk(...) here?\n-  SmallVector<triton::LoadOp, 2> validLoads;\n-  for (Operation &op : *loop)\n-    if (auto loadOp = dyn_cast<triton::LoadOp>(&op)) {\n-      auto ptr = loadOp.getPtr();\n-      unsigned vec = axisInfoAnalysis.getPtrContiguity(ptr);\n+LogicalResult LoopPipeliner::checkOpUses(SetVector<Operation *> &ops) {\n+  DenseSet<Operation *> invalidOps;\n+  // Collect all ops' dependencies\n+  MapVector<Operation *, SetVector<Value>> opDeps;\n+  collectDeps(ops, opDeps);\n+\n+  for (Operation *op : ops) {\n+    if (auto loadOp = dyn_cast<triton::LoadOp>(op)) {\n+      // Don't pipeline valid loads that depend on other valid loads\n+      // (Because if a valid load depends on another valid load, this load needs\n+      // to wait on the other load in the prologue, which is against the point\n+      // of the pipeline pass)\n+      bool isCandidate = true;\n+      for (Operation *other : ops)\n+        if (isa<triton::LoadOp>(other))\n+          if (opDeps[op].contains(other->getResult(0))) {\n+            isCandidate = false;\n+            break;\n+          }\n+      // We only pipeline loads that have one covert_layout (to dot_op) use\n+      // TODO: lift this constraint in the future\n+      if (isCandidate && loadOp.getResult().hasOneUse()) {\n+        isCandidate = false;\n+        Operation *use = *loadOp.getResult().getUsers().begin();\n+\n+        // Advance to the first conversion as long as the use resides in shared\n+        // memory and it has a single use itself\n+        while (use) {\n+          if (use->getNumResults() != 1 || !use->getResult(0).hasOneUse())\n+            break;\n+          auto tensorType =\n+              use->getResult(0).getType().dyn_cast<RankedTensorType>();\n+          if (!tensorType.getEncoding().isa<ttg::SharedEncodingAttr>())\n+            break;\n+          use = *use->getResult(0).getUsers().begin();\n+        }\n \n-      if (auto mask = loadOp.getMask())\n-        vec = std::min<unsigned>(vec, axisInfoAnalysis.getMaskAlignment(mask));\n+        if (auto convertLayout = llvm::dyn_cast<ttg::ConvertLayoutOp>(use))\n+          if (auto tensorType = convertLayout.getResult()\n+                                    .getType()\n+                                    .dyn_cast<RankedTensorType>())\n+            if (auto dotOpEnc = tensorType.getEncoding()\n+                                    .dyn_cast<ttg::DotOperandEncodingAttr>()) {\n+              isCandidate = true;\n+              loadsMapping[loadOp] = convertLayout;\n+            }\n+      } else\n+        isCandidate = false;\n \n-      auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n-      if (!tensorTy || tensorTy.getRank() < 2)\n-        continue;\n-      auto ty = tensorTy.getElementType()\n-                    .cast<triton::PointerType>()\n-                    .getPointeeType();\n-      unsigned width = vec * ty.getIntOrFloatBitWidth();\n-      // cp.async's cp-size can only be 4, 8 and 16.\n-      if (width >= 32)\n-        validLoads.push_back(loadOp);\n+      if (!isCandidate)\n+        invalidOps.insert(loadOp);\n+      else\n+        validLoads.insert(loadOp);\n     }\n+  }\n \n-  // Early stop: no need to continue if there is no load in the loop.\n-  if (validLoads.empty())\n-    return failure();\n+  for (Operation *op : invalidOps)\n+    ops.remove(op);\n \n-  // load => values that it depends on\n-  DenseMap<Value, SetVector<Value>> loadDeps;\n-  for (triton::LoadOp loadOp : validLoads) {\n-    SetVector<Value> deps;\n-    for (Value op : loadOp->getOperands())\n-      collectDeps(op, numStages - 1, deps);\n-    loadDeps[loadOp] = deps;\n-  }\n+  if (ops.empty())\n+    return failure();\n+  else\n+    return success();\n+}\n \n-  // Don't pipeline valid loads that depend on other valid loads\n-  // (Because if a valid load depends on another valid load, this load needs to\n-  // wait on the other load in the prologue, which is against the point of the\n-  // pipeline pass)\n-  for (triton::LoadOp loadOp : validLoads) {\n-    bool isCandidate = true;\n-    for (triton::LoadOp other : validLoads) {\n-      if (loadDeps[loadOp].contains(other)) {\n-        isCandidate = false;\n-        break;\n+void LoopPipeliner::checkOpDeps(SetVector<Operation *> &ops) {\n+  SetVector<BlockArgument> nonImmediateDepArgs;\n+  SetVector<Operation *> nonImmediateOps;\n+  for (Operation *op : ops) {\n+    for (Value v : op->getOperands()) {\n+      SetVector<Value> deps;\n+      collectValueDep(v, numStages - 1, deps);\n+      int defStage = getValueDefStage(v, numStages - 1);\n+      assert(defStage >= 0 &&\n+             \"newLoopArgs has null args without a define op. Consider either \"\n+             \"rewrite the loop to reduce cross iteration dependencies or \"\n+             \"increase the num_stages value.\");\n+      for (auto dep : deps) {\n+        auto immediate = deps.front().isa<BlockArgument>();\n+        if (auto arg = dyn_cast<BlockArgument>(dep)) {\n+          depArgs.insert(arg);\n+          if (immediate)\n+            immediateArgStages[arg].insert(defStage);\n+          else\n+            nonImmediateDepArgs.insert(arg);\n+        } else {\n+          depOps.insert(dep.getDefiningOp());\n+          if (immediate)\n+            immediateOpStages[dep.getDefiningOp()].insert(defStage);\n+          else\n+            nonImmediateOps.insert(dep.getDefiningOp());\n+        }\n       }\n     }\n+  }\n \n-    // We only pipeline loads that have one covert_layout (to dot_op) use\n-    // TODO: lift this constraint in the future\n-    if (isCandidate && loadOp.getResult().hasOneUse()) {\n-      isCandidate = false;\n-      Operation *use = *loadOp.getResult().getUsers().begin();\n-\n-      // advance to the first conversion as long\n-      // as the use resides in shared memory and it has\n-      // a single use itself\n-      while (use) {\n-        if (use->getNumResults() != 1 || !use->getResult(0).hasOneUse())\n-          break;\n-        auto tensorType =\n-            use->getResult(0).getType().dyn_cast<RankedTensorType>();\n-        if (!tensorType.getEncoding().isa<ttg::SharedEncodingAttr>())\n-          break;\n-        use = *use->getResult(0).getUsers().begin();\n-      }\n+  // XXX: We could remove the following constraints if we can rematerialize in\n+  // the loop.\n+  // Check if immediateDepArgs and nonImmediateDepArgs are disjoint.\n+  for (auto &[arg, stages] : immediateArgStages) {\n+    assert(stages.size() == 1 &&\n+           \"Triton doesn't support an argument provides values for \"\n+           \"immediate operands of loads from multiple stages. Consider \"\n+           \"removing post load instructions dependency on this argument.\");\n+    assert(!(nonImmediateDepArgs.contains(arg) &&\n+             stages.contains(numStages - 2)) &&\n+           \"Loop-carried arguments provide values for both immediate and \"\n+           \"non-immediate operands of loads. Please consider removing \"\n+           \"pre/post load instructions dependency on this argument.\");\n+  }\n \n-      auto convertLayout = llvm::dyn_cast<ttg::ConvertLayoutOp>(use);\n-      if (!convertLayout)\n-        continue;\n-      auto tensorType =\n-          convertLayout.getResult().getType().dyn_cast<RankedTensorType>();\n-      if (!tensorType)\n-        continue;\n-      auto dotOpEnc =\n-          tensorType.getEncoding().dyn_cast<ttg::DotOperandEncodingAttr>();\n-      if (!dotOpEnc)\n-        continue;\n-      isCandidate = true;\n-      loadsMapping[loadOp] = convertLayout;\n-    }\n+  // Check if immediateOps and nonImmediateOps are disjoint.\n+  for (auto &[op, stages] : immediateOpStages) {\n+    assert(stages.size() == 1 &&\n+           \"Triton doesn't support an operation provides values for \"\n+           \"immediate operands of loads from multiple stages. Consider \"\n+           \"removing post load instructions dependency on this argument.\");\n+    assert(!(nonImmediateOps.contains(op) && stages.contains(numStages - 2)) &&\n+           \"Operations provide values for both immediate and \"\n+           \"non-immediate operands of loads.  Please consider \"\n+           \"removing pre/post load instructions dependency on this \"\n+           \"operation.\");\n+  }\n+}\n \n-    else\n-      isCandidate = false;\n+// helpers\n+void LoopPipeliner::setValueMapping(Value origin, Value newValue, int stage) {\n+  if (valueMapping.find(origin) == valueMapping.end())\n+    valueMapping[origin] = SmallVector<Value>(numStages);\n+  valueMapping[origin][stage] = newValue;\n+}\n \n-    if (isCandidate)\n-      loads.insert(loadOp);\n+void LoopPipeliner::setValueMappingYield(Value origin, Value newValue,\n+                                         int stage) {\n+  for (OpOperand &operand : origin.getUses()) {\n+    if (operand.getOwner() == yieldOp) {\n+      auto yieldIdx = operand.getOperandNumber();\n+      auto value = forOp.getRegionIterArgs()[yieldIdx];\n+      setValueMapping(value, newValue, stage);\n+    }\n   }\n+}\n+\n+void LoopPipeliner::setValueMappingYield(scf::ForOp newForOp, Value origin,\n+                                         Value newValue) {\n+  for (OpOperand &operand : origin.getUses()) {\n+    if (operand.getOwner() == yieldOp) {\n+      auto yieldIdx = operand.getOperandNumber();\n+      auto depYieldIdx = depArgsIdx[forOp.getRegionIterArgs()[yieldIdx]];\n+      auto originArg = forOp.getRegionIterArgs()[yieldIdx];\n+      nextMapping.map(originArg, newValue);\n+      auto newArg = newForOp.getRegionIterArgs()[depYieldIdx];\n+      if (!depArgsMapping.contains(newArg))\n+        depArgsMapping[newArg] = newValue;\n+    }\n+  }\n+}\n+\n+Value LoopPipeliner::lookupOrDefault(Value origin, int stage) {\n+  if (valueMapping.find(origin) == valueMapping.end())\n+    return origin;\n+  return valueMapping[origin][stage];\n+}\n \n-  // we need to find the smallest ocmmon dtype\n-  // since this determines the layout of `mma.sync` operands\n-  // in mixed-precision mode\n+void LoopPipeliner::createBufferTypes() {\n+  // We need to find the smallest common dtype since this determines the layout\n+  // of `mma.sync` operands in mixed-precision mode\n   Type smallestType;\n   for (auto loadCvt : loadsMapping) {\n     auto loadOp = loadCvt.first;\n@@ -316,39 +513,63 @@ LogicalResult LoopPipeliner::initialize() {\n     bufferShape.insert(bufferShape.begin(), numStages);\n     auto sharedEnc = ttg::SharedEncodingAttr::get(\n         ty.getContext(), dotOpEnc, ty.getShape(),\n-        triton::gpu::getOrder(ty.getEncoding()), loadsSmallestType[loadOp]);\n+        ttg::getOrder(ty.getEncoding()), loadsSmallestType[loadOp]);\n     loadsBufferType[loadOp] =\n         RankedTensorType::get(bufferShape, ty.getElementType(), sharedEnc);\n   }\n+}\n \n-  // We have some loads to pipeline\n-  if (!loads.empty()) {\n-    // Update depArgs & depOps\n-    for (Value loadOp : loads) {\n-      auto &deps = loadDeps[loadOp];\n-      for (auto &dep : deps) {\n-        if (auto arg = dep.dyn_cast<BlockArgument>()) {\n-          depArgs.insert(arg);\n-          if (deps.front().isa<BlockArgument>()) {\n-            immedidateDepArgs.insert(arg);\n-          } else {\n-            nonImmedidateDepArgs.insert(arg);\n-          }\n-        } else\n-          depOps.insert(dep.getDefiningOp());\n-      }\n-    }\n-    return success();\n+void LoopPipeliner::createOrderedDeps() {\n+  for (Operation &op : forOp.getLoopBody().front()) {\n+    if (depOps.contains(&op))\n+      orderedDeps.push_back(&op);\n+    else if (op.getNumResults() > 0 && validLoads.contains(op.getResult(0)))\n+      orderedDeps.push_back(&op);\n   }\n+  assert(depOps.size() + validLoads.size() == orderedDeps.size() &&\n+         \"depOps contains invalid values\");\n+}\n \n-  // Check if immedidateDepArgs and nonImmedidateDepArgs are disjoint\n-  // If yes, we cannot pipeline the loop for now\n-  for (BlockArgument arg : immedidateDepArgs)\n-    if (nonImmedidateDepArgs.contains(arg)) {\n-      return failure();\n-    }\n+int LoopPipeliner::getValueDefStage(Value v, int stage) {\n+  if (stage < 0)\n+    return -1;\n+  if (auto arg = v.dyn_cast<BlockArgument>()) {\n+    if (arg.getArgNumber() > 0)\n+      return getValueDefStage(yieldOp->getOperand(arg.getArgNumber() - 1),\n+                              stage - 1);\n+    llvm_unreachable(\"Loop induction variable should not be a dependency\");\n+  } else\n+    return stage;\n+}\n \n-  return failure();\n+ttg::AllocTensorOp LoopPipeliner::allocateEmptyBuffer(triton::LoadOp loadOp,\n+                                                      OpBuilder &builder) {\n+  // Allocate a buffer for each pipelined tensor\n+  // shape: e.g. (numStages==4), <32x64xbf16> -> <4x32x64xbf16>\n+  Value convertLayout = loadsMapping[loadOp];\n+  if (auto tensorType = convertLayout.getType().dyn_cast<RankedTensorType>())\n+    return builder.create<ttg::AllocTensorOp>(convertLayout.getLoc(),\n+                                              loadsBufferType[loadOp]);\n+  llvm_unreachable(\"Async copy's return should be of RankedTensorType\");\n+}\n+\n+LogicalResult LoopPipeliner::initialize() {\n+  // All ops that maybe pipelined\n+  SetVector<Operation *> ops;\n+\n+  if (collectOps(ops).failed())\n+    return failure();\n+\n+  if (checkOpUses(ops).failed())\n+    return failure();\n+\n+  checkOpDeps(ops);\n+\n+  createBufferTypes();\n+\n+  createOrderedDeps();\n+\n+  return success();\n }\n \n Value LoopPipeliner::getLoadMask(triton::LoadOp loadOp, Value mappedMask,\n@@ -375,12 +596,13 @@ Value LoopPipeliner::getLoadMask(triton::LoadOp loadOp, Value mappedMask,\n \n void LoopPipeliner::emitPrologue() {\n   OpBuilder builder(forOp);\n+  // Get init operands for loop carried values\n   for (BlockArgument &arg : forOp.getRegionIterArgs()) {\n     OpOperand &operand = forOp.getOpOperandForRegionIterArg(arg);\n     setValueMapping(arg, operand.get(), 0);\n   }\n \n-  // prologue from [0, numStage-1)\n+  // Emit prologue from [0, numStage-1)\n   Value iv = forOp.getLowerBound();\n   pipelineIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n   for (int stage = 0; stage < numStages - 1; ++stage) {\n@@ -392,38 +614,27 @@ void LoopPipeliner::emitPrologue() {\n     // Special handling for loop condition as there is no condition in ForOp\n     Value loopCond = builder.create<arith::CmpIOp>(\n         iv.getLoc(), arith::CmpIPredicate::slt, iv, forOp.getUpperBound());\n-\n-    // Rematerialize peeled values\n-    SmallVector<Operation *> orderedDeps;\n-    for (Operation &op : forOp.getLoopBody().front()) {\n-      if (depOps.contains(&op))\n-        orderedDeps.push_back(&op);\n-      else if (op.getNumResults() > 0 && loads.contains(op.getResult(0)))\n-        orderedDeps.push_back(&op);\n-    }\n-    assert(depOps.size() + loads.size() == orderedDeps.size() &&\n-           \"depOps contains invalid values\");\n     for (Operation *op : orderedDeps) {\n       Operation *newOp = nullptr;\n-      if (loads.contains(op->getResult(0))) {\n+      if (validLoads.contains(op->getResult(0))) {\n+        auto load = cast<triton::LoadOp>(op);\n         // Allocate empty buffer\n         if (stage == 0) {\n-          loadsBuffer[op->getResult(0)] = allocateEmptyBuffer(op, builder);\n-          loadStageBuffer[op->getResult(0)] = {loadsBuffer[op->getResult(0)]};\n+          loadsBuffer[load] = allocateEmptyBuffer(load, builder);\n+          loadStageBuffer[load] = {loadsBuffer[load]};\n         }\n         // load => copy async\n         if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(op)) {\n           Value newMask =\n               getLoadMask(loadOp, lookupOrDefault(loadOp.getMask(), stage),\n                           loopCond, builder);\n-          // TODO: check if the hardware supports async copy\n-          newOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n+          newOp = builder.create<ttg::InsertSliceAsyncOp>(\n               op->getLoc(), loadsBuffer[loadOp].getType(),\n               lookupOrDefault(loadOp.getPtr(), stage),\n               loadStageBuffer[loadOp][stage], pipelineIterIdx, newMask,\n               lookupOrDefault(loadOp.getOther(), stage), loadOp.getCache(),\n               loadOp.getEvict(), loadOp.getIsVolatile(), /*axis*/ 0);\n-          builder.create<triton::gpu::AsyncCommitGroupOp>(op->getLoc());\n+          builder.create<ttg::AsyncCommitGroupOp>(op->getLoc());\n           loadStageBuffer[loadOp].push_back(newOp->getResult(0));\n         } else\n           llvm_unreachable(\"This should be LoadOp\");\n@@ -439,53 +650,44 @@ void LoopPipeliner::emitPrologue() {\n               loadOp.getBoundaryCheckAttr(), loadOp.getPaddingAttr(),\n               loadOp.getCache(), loadOp.getEvict(), loadOp.getIsVolatile());\n           addNamedAttrs(newOp, op->getDiscardableAttrDictionary());\n-        } else {\n+        } else\n           newOp = builder.clone(*op);\n-        }\n         // Update loop-carried uses\n         for (unsigned opIdx = 0; opIdx < op->getNumOperands(); ++opIdx) {\n           auto it = valueMapping.find(op->getOperand(opIdx));\n           if (it != valueMapping.end()) {\n             Value v = it->second[stage];\n-            assert(v);\n+            assert(v && \"Value not found in valueMapping\");\n             newOp->setOperand(opIdx, v);\n           } // else, op at opIdx is a loop-invariant value\n         }\n       }\n \n-      // Update mapping of results\n-      // if (stage == numStages - 2)\n-      //   continue;\n-\n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n-        Value originalResult = op->getResult(dstIdx);\n-        // copy_async will update the value of its only use\n-        // TODO: load should not be used in the preheader?\n-        if (loads.contains(originalResult)) {\n+        Value originResult = op->getResult(dstIdx);\n+        if (validLoads.contains(originResult))\n           break;\n-          // originalResult = loadsMapping[originalResult];\n-        }\n-        setValueMapping(originalResult, newOp->getResult(dstIdx), stage);\n-        // update mapping for loop-carried values (args)\n-        for (OpOperand &operand : yieldOp->getOpOperands()) {\n-          if (operand.get() == op->getResult(dstIdx))\n-            setValueMapping(\n-                forOp.getRegionIterArgs()[operand.getOperandNumber()],\n-                newOp->getResult(dstIdx), stage + 1);\n-        }\n+        setValueMapping(originResult, newOp->getResult(dstIdx), stage);\n+        // Update mapping for loop-carried values (args)\n+        setValueMappingYield(op->getResult(dstIdx), newOp->getResult(dstIdx),\n+                             stage + 1);\n       }\n     } // for (Operation *op : orderedDeps)\n \n+    // Update pipeline index\n     pipelineIterIdx = builder.create<arith::AddIOp>(\n         iv.getLoc(), pipelineIterIdx,\n         builder.create<arith::ConstantIntOp>(iv.getLoc(), 1, 32));\n+    // Some values have not been used by any ops in the loop body\n+    for (BlockArgument arg : forOp.getRegionIterArgs())\n+      setValueMappingYield(arg, valueMapping[arg][stage], stage + 1);\n   } // for (int stage = 0; stage < numStages - 1; ++stage)\n \n   // async.wait & extract_slice\n-  builder.create<ttg::AsyncWaitOp>(loads[0].getLoc(),\n-                                   loads.size() * (numStages - 2));\n+  builder.create<ttg::AsyncWaitOp>(validLoads.front().getLoc(),\n+                                   validLoads.size() * (numStages - 2));\n   loopIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n-  for (Value loadOp : loads) {\n+  for (Value loadOp : validLoads) {\n     auto bufferType = loadStageBuffer[loadOp][numStages - 1]\n                           .getType()\n                           .cast<RankedTensorType>();\n@@ -494,7 +696,7 @@ void LoopPipeliner::emitPrologue() {\n     sliceType = RankedTensorType::get({bufferShape[1], bufferShape[2]},\n                                       sliceType.getElementType(),\n                                       loadsBufferType[loadOp].getEncoding());\n-    Value extractSlice = builder.create<triton::gpu::ExtractSliceOp>(\n+    Value extractSlice = builder.create<ttg::ExtractSliceOp>(\n         loadOp.getLoc(), sliceType, loadStageBuffer[loadOp][numStages - 1],\n         SmallVector<OpFoldResult>{int_attr(0), int_attr(0), int_attr(0)},\n         SmallVector<OpFoldResult>{int_attr(1),\n@@ -504,7 +706,7 @@ void LoopPipeliner::emitPrologue() {\n     loadsExtract[loadOp] = extractSlice;\n   }\n   // Bump up loopIterIdx, this is used for getting the correct slice for the\n-  // *next* iteration\n+  // `next` iteration\n   loopIterIdx = builder.create<arith::AddIOp>(\n       loopIterIdx.getLoc(), loopIterIdx,\n       builder.create<arith::ConstantIntOp>(loopIterIdx.getLoc(), 1, 32));\n@@ -515,79 +717,77 @@ void LoopPipeliner::emitEpilogue() {\n   OpBuilder builder(forOp);\n   OpBuilder::InsertionGuard g(builder);\n   builder.setInsertionPointAfter(forOp);\n-  builder.create<triton::gpu::AsyncWaitOp>(forOp.getLoc(), 0);\n+  builder.create<ttg::AsyncWaitOp>(forOp.getLoc(), 0);\n }\n \n-scf::ForOp LoopPipeliner::createNewForOp() {\n-  OpBuilder builder(forOp);\n-\n+SmallVector<Value> LoopPipeliner::collectNewLoopArgs() {\n   // Order of new args:\n   //   (original args)\n   //   (insertSliceAsync buffer at stage numStages - 1) for each load\n   //   (extracted tensor) for each load\n-  //   (depArgs at stage numStages - 1):\n-  //   for each dep arg that is not an immediate block argument\n-  //   (depArgs at stage numStages - 2):\n-  //   for each dep arg that is an immediate block argument\n+  //   (depArgs at stage numStages - 1)\n+  //   (depArgs at stage numStages - 2)\n+  //   ...\n   //   (iv at stage numStages - 2)\n   //   (pipeline iteration index)\n   //   (loop iteration index)\n-  SmallVector<Value> newLoopArgs;\n+\n   // We need this to update operands for yield\n   // original block arg => new arg's idx\n-  DenseMap<BlockArgument, size_t> depArgsIdx;\n+  SmallVector<Value> newLoopArgs;\n   for (auto v : forOp.getIterOperands())\n     newLoopArgs.push_back(v);\n \n-  size_t bufferIdx = newLoopArgs.size();\n-  for (Value loadOp : loads)\n+  bufferIdx = newLoopArgs.size();\n+  for (auto loadOp : validLoads)\n     newLoopArgs.push_back(loadStageBuffer[loadOp].back());\n-  size_t loadIdx = newLoopArgs.size();\n-  for (Value loadOp : loads)\n+\n+  loadIdx = newLoopArgs.size();\n+  for (auto loadOp : validLoads)\n     newLoopArgs.push_back(loadsExtract[loadOp]);\n \n-  size_t depArgsBeginIdx = newLoopArgs.size();\n-  for (BlockArgument depArg : depArgs) {\n+  depArgsBeginIdx = newLoopArgs.size();\n+  for (auto depArg : depArgs) {\n     depArgsIdx[depArg] = newLoopArgs.size();\n-    if (immedidateDepArgs.contains(depArg)) {\n+    if (immediateArgStages[depArg].contains(numStages - 2))\n+      // Peel off post load ops in numStage-1\n       newLoopArgs.push_back(valueMapping[depArg][numStages - 2]);\n-    } else\n+    else\n       newLoopArgs.push_back(valueMapping[depArg][numStages - 1]);\n   }\n \n-  size_t nextIVIdx = newLoopArgs.size();\n+  ivIndex = newLoopArgs.size();\n   newLoopArgs.push_back(valueMapping[forOp.getInductionVar()][numStages - 2]);\n   newLoopArgs.push_back(pipelineIterIdx);\n   newLoopArgs.push_back(loopIterIdx);\n+  return newLoopArgs;\n+}\n \n-  for (size_t i = 0; i < newLoopArgs.size(); ++i)\n-    assert(newLoopArgs[i]);\n-\n-  // 1. signature of the new ForOp\n+scf::ForOp LoopPipeliner::cloneForOp(ArrayRef<Value> newLoopArgs,\n+                                     OpBuilder &builder) {\n+  // Clone the original ForOp\n   auto newForOp = builder.create<scf::ForOp>(\n       forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n       forOp.getStep(), newLoopArgs);\n \n-  // 2. body of the new ForOp\n+  // Set mapping on body of the new ForOp\n   builder.setInsertionPointToStart(newForOp.getBody());\n-  IRMapping mapping;\n   for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n     mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n   mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n \n-  // 2. clone the loop body, replace original args with args of the new ForOp\n+  // Clone the loop body, replace original args with args of the new ForOp\n   // Insert async wait if necessary.\n-  DenseSet<Value> isModified;\n   for (Operation &op : forOp.getBody()->without_terminator()) {\n     // is modified\n-    auto it = std::find(loads.begin(), loads.end(), op.getOperand(0));\n-    if (it == loads.end()) {\n+    auto it = std::find(validLoads.begin(), validLoads.end(), op.getOperand(0));\n+    if (it == validLoads.end()) {\n       Operation *newOp = cloneWithInferType(builder, &op, mapping);\n       continue;\n     }\n \n     // we replace the use new load use with a convert layout\n-    size_t i = std::distance(loads.begin(), it);\n+    size_t i = std::distance(validLoads.begin(), it);\n     auto cvtDstTy = op.getResult(0).getType().cast<RankedTensorType>();\n     auto cvtDstEnc =\n         cvtDstTy.getEncoding().dyn_cast<ttg::DotOperandEncodingAttr>();\n@@ -604,53 +804,47 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n         op.getResult(0).getLoc(), newDstTy,\n         newForOp.getRegionIterArgs()[loadIdx + i]);\n     mapping.map(op.getResult(0), cvt.getResult());\n-    isModified.insert(op.getResult(0));\n   }\n \n-  // 3. prefetch the next iteration\n-  SmallVector<Operation *> orderedDeps;\n-  for (Operation &op : forOp.getLoopBody().front()) {\n-    if (depOps.contains(&op))\n-      orderedDeps.push_back(&op);\n-    else if (op.getNumResults() > 0 && loads.contains(op.getResult(0)))\n-      orderedDeps.push_back(&op);\n-  }\n-  assert(depOps.size() + loads.size() == orderedDeps.size() &&\n-         \"depOps contains invalid values\");\n-  IRMapping nextMapping;\n-  DenseMap<BlockArgument, Value> depArgsMapping;\n+  return newForOp;\n+}\n+\n+void LoopPipeliner::prefetchNextIteration(scf::ForOp newForOp,\n+                                          OpBuilder &builder) {\n+  // Map the dep args of the next iteration to the dep args of the current\n   size_t argIdx = 0;\n-  for (BlockArgument arg : depArgs) {\n+  for (auto depArg : depArgs) {\n     BlockArgument nextArg =\n         newForOp.getRegionIterArgs()[argIdx + depArgsBeginIdx];\n-    nextMapping.map(arg, nextArg);\n+    nextMapping.map(depArg, nextArg);\n     ++argIdx;\n   }\n \n   // Special handling for iv & loop condition\n-  Value nextIV = builder.create<arith::AddIOp>(\n-      newForOp.getInductionVar().getLoc(),\n-      newForOp.getRegionIterArgs()[nextIVIdx], newForOp.getStep());\n+  Value curIV = newForOp.getRegionIterArgs()[ivIndex];\n+  nextIV = builder.create<arith::AddIOp>(newForOp.getInductionVar().getLoc(),\n+                                         curIV, newForOp.getStep());\n   Value nextLoopCond =\n       builder.create<arith::CmpIOp>(nextIV.getLoc(), arith::CmpIPredicate::slt,\n                                     nextIV, newForOp.getUpperBound());\n-  nextMapping.map(forOp.getInductionVar(), nextIV);\n \n-  // Slice index\n-  SmallVector<Value> nextBuffers;\n-  SmallVector<Value> extractSlices;\n-\n-  pipelineIterIdx = newForOp.getRegionIterArgs()[nextIVIdx + 1];\n+  pipelineIterIdx = newForOp.getRegionIterArgs()[ivIndex + 1];\n   Value insertSliceIndex = builder.create<arith::RemSIOp>(\n       nextIV.getLoc(), pipelineIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n-  loopIterIdx = newForOp.getRegionIterArgs()[nextIVIdx + 2];\n+  loopIterIdx = newForOp.getRegionIterArgs()[ivIndex + 2];\n   Value extractSliceIndex = builder.create<arith::RemSIOp>(\n       nextIV.getLoc(), loopIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n \n+  // Prefetch load deps\n   for (Operation *op : orderedDeps)\n-    if (!loads.contains(op->getResult(0))) {\n+    if (!validLoads.contains(op->getResult(0))) {\n+      if (immediateOpStages[op].contains(numStages - 2))\n+        // A post load op that provides values for numStage - 2\n+        nextMapping.map(forOp.getInductionVar(), curIV);\n+      else\n+        nextMapping.map(forOp.getInductionVar(), nextIV);\n       Operation *nextOp;\n       if (auto loadOp = dyn_cast<triton::LoadOp>(op)) {\n         auto newMask =\n@@ -664,29 +858,19 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n             loadOp.getCache(), loadOp.getEvict(), loadOp.getIsVolatile());\n         addNamedAttrs(nextOp, op->getDiscardableAttrDictionary());\n         nextMapping.map(loadOp.getResult(), nextOp->getResult(0));\n-      } else {\n+      } else\n         nextOp = builder.clone(*op, nextMapping);\n-      }\n \n-      auto originYield = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n-      for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n-        for (OpOperand &operand : originYield->getOpOperands()) {\n-          if (operand.get() == op->getResult(dstIdx)) {\n-            size_t originIdx = operand.getOperandNumber();\n-            size_t newArgIdx = depArgsIdx[forOp.getRegionIterArgs()[originIdx]];\n-            BlockArgument newArg = newForOp.getRegionIterArgs()[newArgIdx];\n-            nextMapping.map(forOp.getRegionIterArgs()[originIdx],\n-                            nextOp->getResult(dstIdx));\n-            depArgsMapping[newArg] = nextOp->getResult(dstIdx);\n-          }\n-        }\n-      }\n+      for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults()))\n+        setValueMappingYield(newForOp, op->getResult(dstIdx),\n+                             nextOp->getResult(dstIdx));\n     }\n \n+  // loads -> async loads\n   for (Operation *op : orderedDeps) {\n     Operation *nextOp = nullptr;\n     // Update loading mask\n-    if (loads.contains(op->getResult(0))) {\n+    if (validLoads.contains(op->getResult(0))) {\n       auto loadOp = llvm::cast<triton::LoadOp>(op);\n       auto mask = loadOp.getMask();\n       auto newMask =\n@@ -699,24 +883,24 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n           nextMapping.map(loadOp.getMask(), newMask);\n         newMask = nextMapping.lookupOrDefault(mask);\n       }\n-      Value insertAsyncOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n+      Value insertAsyncOp = builder.create<ttg::InsertSliceAsyncOp>(\n           op->getLoc(), loadsBuffer[loadOp].getType(),\n           nextMapping.lookupOrDefault(loadOp.getPtr()),\n           newForOp.getRegionIterArgs()[bufferIdx + nextBuffers.size()],\n           insertSliceIndex, newMask,\n           nextMapping.lookupOrDefault(loadOp.getOther()), loadOp.getCache(),\n           loadOp.getEvict(), loadOp.getIsVolatile(), /*axis*/ 0);\n-      builder.create<triton::gpu::AsyncCommitGroupOp>(op->getLoc());\n+      builder.create<ttg::AsyncCommitGroupOp>(op->getLoc());\n       nextBuffers.push_back(insertAsyncOp);\n-      // ExtractSlice\n+      // Extract slice\n       auto bufferType = insertAsyncOp.getType().cast<RankedTensorType>();\n       auto bufferShape = bufferType.getShape();\n       auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n       sliceType = RankedTensorType::get({bufferShape[1], bufferShape[2]},\n                                         sliceType.getElementType(),\n                                         loadsBufferType[loadOp].getEncoding());\n \n-      nextOp = builder.create<triton::gpu::ExtractSliceOp>(\n+      nextOp = builder.create<ttg::ExtractSliceOp>(\n           op->getLoc(), sliceType, insertAsyncOp,\n           SmallVector<OpFoldResult>{extractSliceIndex, int_attr(0),\n                                     int_attr(0)},\n@@ -727,25 +911,21 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n       extractSlices.push_back(nextOp->getResult(0));\n \n       // Update mapping of results\n-      for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n-        nextMapping.map(op->getResult(dstIdx), nextOp->getResult(dstIdx));\n+      for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults()))\n         // If this is a loop-carried value, update the mapping for yield\n-        auto originYield = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n-        for (OpOperand &operand : originYield->getOpOperands()) {\n-          if (operand.get() == op->getResult(dstIdx)) {\n-            size_t originIdx = operand.getOperandNumber();\n-            size_t newArgIdx = depArgsIdx[forOp.getRegionIterArgs()[originIdx]];\n-            BlockArgument newArg = newForOp.getRegionIterArgs()[newArgIdx];\n-            depArgsMapping[newArg] = nextOp->getResult(dstIdx);\n-          }\n-        }\n-      }\n+        setValueMappingYield(newForOp, op->getResult(dstIdx),\n+                             nextOp->getResult(dstIdx));\n     }\n   }\n \n+  // Some values have not been used by any ops in the loop body\n+  for (BlockArgument arg : forOp.getRegionIterArgs())\n+    setValueMappingYield(newForOp, arg,\n+                         newForOp.getRegionIterArgs()[depArgsIdx[arg]]);\n+\n   // async.wait & extract_slice\n   Operation *asyncWait = builder.create<ttg::AsyncWaitOp>(\n-      loads[0].getLoc(), loads.size() * (numStages - 2));\n+      validLoads[0].getLoc(), validLoads.size() * (numStages - 2));\n   for (auto it = extractSlices.rbegin(); it != extractSlices.rend(); ++it) {\n     // move extract_slice after asyncWait\n     it->getDefiningOp()->moveAfter(asyncWait);\n@@ -758,17 +938,18 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   loopIterIdx = builder.create<arith::AddIOp>(\n       nextIV.getLoc(), loopIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), 1, 32));\n+}\n \n-  // Finally, the YieldOp, need to sync with the order of newLoopArgs\n+void LoopPipeliner::finalizeYield(scf::ForOp newForOp, OpBuilder &builder) {\n   SmallVector<Value> yieldValues;\n-  for (Value v : forOp.getBody()->getTerminator()->getOperands())\n+  for (Value v : yieldOp->getOperands())\n     yieldValues.push_back(mapping.lookup(v));\n   for (Value nextBuffer : nextBuffers)\n     yieldValues.push_back(nextBuffer);\n   for (Value nextSlice : extractSlices)\n     yieldValues.push_back(nextSlice);\n \n-  for (size_t i = depArgsBeginIdx; i < nextIVIdx; ++i) {\n+  for (size_t i = depArgsBeginIdx; i < ivIndex; ++i) {\n     auto arg = newForOp.getRegionIterArgs()[i];\n     assert(depArgsMapping.count(arg) && \"Missing loop-carried value\");\n     yieldValues.push_back(depArgsMapping[arg]);\n@@ -778,8 +959,15 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   yieldValues.push_back(loopIterIdx);\n \n   builder.setInsertionPointToEnd(newForOp.getBody());\n-  builder.create<scf::YieldOp>(forOp.getBody()->getTerminator()->getLoc(),\n-                               yieldValues);\n+  builder.create<scf::YieldOp>(yieldOp->getLoc(), yieldValues);\n+}\n+\n+scf::ForOp LoopPipeliner::createNewForOp() {\n+  OpBuilder builder(forOp);\n+  auto newLoopArgs = collectNewLoopArgs();\n+  auto newForOp = cloneForOp(newLoopArgs, builder);\n+  prefetchNextIteration(newForOp, builder);\n+  finalizeYield(newForOp, builder);\n   return newForOp;\n }\n \n@@ -812,11 +1000,10 @@ struct PipelinePass : public TritonGPUPipelineBase<PipelinePass> {\n         return;\n \n       pipeliner.emitPrologue();\n-\n       scf::ForOp newForOp = pipeliner.createNewForOp();\n       pipeliner.emitEpilogue();\n \n-      // replace the original loop\n+      // Replace the original loop\n       for (unsigned i = 0; i < forOp->getNumResults(); ++i)\n         forOp->getResult(i).replaceAllUsesWith(newForOp->getResult(i));\n       forOp->erase();"}, {"filename": "python/test/regression/test_functional_regressions.py", "status": "modified", "additions": 94, "deletions": 0, "changes": 94, "file_content_changes": "@@ -1,4 +1,5 @@\n import numpy as np\n+import pytest\n import torch\n from numpy.random import RandomState\n \n@@ -134,3 +135,96 @@ def batched_vecmat(\n     C_ref = np.sum(AB, axis=2)\n \n     np.testing.assert_allclose(C_ref, C_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n+\n+\n+@pytest.mark.parametrize(\"type\", [\"pre_load\", \"post_load\", \"post_pre_mixed\", \"post_load_two_iters\", \"post_load_three_iters\"])\n+def test_iv_dependent_matmul(type):\n+    @triton.jit\n+    def kernel(\n+        a_ptr, b_ptr, c_ptr,\n+        M, N, K,\n+        stride_am, stride_ak,\n+        stride_bk, stride_bn,\n+        stride_cm, stride_cn,\n+        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+        type: tl.constexpr\n+    ):\n+        pid = tl.program_id(axis=0)\n+        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+        pid_m = pid // num_pid_n\n+        pid_n = pid % num_pid_n\n+\n+        offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n+        offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n+        offs_k = tl.arange(0, BLOCK_SIZE_K)\n+        a_ptr = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n+        b_ptr = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n+        a_ptrs = a_ptr\n+        b_ptrs = b_ptr\n+        if type == \"post_load_two_iters\":\n+            a_ptrs_next = a_ptr + BLOCK_SIZE_K * stride_ak\n+            b_ptrs_next = b_ptr + BLOCK_SIZE_K * stride_bk\n+        elif type == \"post_load_three_iters\":\n+            a_ptrs_next = a_ptr + BLOCK_SIZE_K * stride_ak\n+            b_ptrs_next = b_ptr + BLOCK_SIZE_K * stride_bk\n+            a_ptrs_next_next = a_ptr + 2 * BLOCK_SIZE_K * stride_ak\n+            b_ptrs_next_next = b_ptr + 2 * BLOCK_SIZE_K * stride_bk\n+\n+        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n+            if type == \"pre_load\":\n+                a_ptrs = a_ptr + k * BLOCK_SIZE_K * stride_ak\n+                b_ptrs = b_ptr + k * BLOCK_SIZE_K * stride_bk\n+            elif type == \"post_pre_mixed\":\n+                a_ptrs = a_ptr + k * BLOCK_SIZE_K * stride_ak\n+            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n+            b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n+            accumulator += tl.dot(a, b)\n+            if type == \"post_load\":\n+                a_ptrs = a_ptr + (k + 1) * BLOCK_SIZE_K * stride_ak\n+                b_ptrs = b_ptr + (k + 1) * BLOCK_SIZE_K * stride_bk\n+            elif type == \"post_pre_mixed\":\n+                b_ptrs = b_ptr + (k + 1) * BLOCK_SIZE_K * stride_bk\n+            elif type == \"post_load_two_iters\":\n+                a_ptrs = a_ptrs_next\n+                b_ptrs = b_ptrs_next\n+                a_ptrs_next = a_ptr + (k + 2) * BLOCK_SIZE_K * stride_ak\n+                b_ptrs_next = b_ptr + (k + 2) * BLOCK_SIZE_K * stride_bk\n+            elif type == \"post_load_three_iters\":\n+                a_ptrs = a_ptrs_next\n+                b_ptrs = b_ptrs_next\n+                a_ptrs_next = a_ptrs_next_next\n+                b_ptrs_next = b_ptrs_next_next\n+                a_ptrs_next_next = a_ptr + (k + 3) * BLOCK_SIZE_K * stride_ak\n+                b_ptrs_next_next = b_ptr + (k + 3) * BLOCK_SIZE_K * stride_bk\n+        c = accumulator.to(tl.float16)\n+\n+        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+        c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n+        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n+        tl.store(c_ptrs, c, mask=c_mask)\n+\n+    M = 256\n+    K = 256\n+    N = 256\n+    BLOCK_SIZE_K = 32\n+    BLOCK_SIZE_N = 32\n+    BLOCK_SIZE_M = 32\n+\n+    a = torch.rand((M, K), device='cuda')\n+    b = torch.rand((K, N), device='cuda')\n+\n+    torch_output = torch.mm(a, b)\n+    triton_output = torch.empty_like(\n+        torch_output, device=torch_output.device)\n+\n+    def grid(META):\n+        return (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n+\n+    num_stages = 4 if type == \"post_load_three_iters\" else 3\n+    kernel[grid](a, b, triton_output, M, N, K, a.stride(0), a.stride(1),\n+                 b.stride(0), b.stride(1), triton_output.stride(0), triton_output.stride(1),\n+                 BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n+                 type=type, num_stages=num_stages)\n+    torch.testing.assert_allclose(torch_output, triton_output, rtol=1e-2, atol=1e-2)"}, {"filename": "python/triton/debugger/memory_map.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n import dataclasses\n \n from triton.debugger import torch_wrapper"}, {"filename": "python/triton/debugger/tl_lang.py", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n import triton\n from .core import ExecutionContext\n from .memory_map import MemoryMap\n@@ -405,7 +407,9 @@ def zeros(self, shape, dtype):\n         return torch.zeros(size=shape, dtype=dtype, device=\"cuda\")\n \n     @_tensor_operation\n-    def dequantize(self, input, scale, shift, nbit, dst_ty=torch.float16):\n+    def dequantize(self, input, scale, shift, nbit, dst_ty=None):\n+        if dst_ty is None:\n+            dst_ty = torch.float16\n         raise NotImplementedError()\n \n     @_tensor_operation"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 119, "deletions": 0, "changes": 119, "file_content_changes": "@@ -313,3 +313,122 @@ tt.func @lut_bmm_vector(%77: tensor<16x16xi64, #BL> {tt.divisibility=16: i32, tt\n   }\n   tt.return %79#0 : tensor<16x16xf32, #C>\n }\n+\n+// CHECK: tt.func @post_load_inv\n+// CHECK: scf.for\n+// CHECK: arith.index_cast\n+// CHECK-DAG: %[[IV:.*]] = arith.index_cast\n+// CHECK: %[[NEXT_IV:.*]] = arith.addi %[[IV]], %c1_i32 : i32\n+// CHECK-NOT: arith.addi %[[NEXT_IV]]\n+tt.func @post_load_inv(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                       %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                       %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                       %arg3: i32 {tt.divisibility = 16 : i32},\n+                       %arg4: i32 {tt.divisibility = 16 : i32},\n+                       %arg5: i32 {tt.divisibility = 16 : i32},\n+                       %arg6: i32 {tt.divisibility = 16 : i32},\n+                       %arg7: i32 {tt.divisibility = 16 : i32},\n+                       %arg8: i32 {tt.divisibility = 16 : i32}) -> tensor<32x32xf32, #C> {\n+  %c0_index = arith.constant 0 : index\n+  %c1_index = arith.constant 1 : index\n+  %c1_i32 = arith.constant 1 : i32\n+  %c32_i32 = arith.constant 32 : i32\n+  %84 = arith.constant 900 : index\n+  %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #C>\n+  %cst_0 = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #AL>\n+  %50 = tt.splat %arg3 : (i32) -> tensor<1x32xi32, #AL>\n+  %59 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %81 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %66 = tt.splat %arg4 : (i32) -> tensor<32x1xi32, #AL>\n+  %60 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %82 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %85:3 = scf.for %arg9 = %c0_index to %84 step %c1_index iter_args(%arg10 = %cst, %arg11 = %59, %arg12 = %81) -> (tensor<32x32xf32, #C>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>)  {\n+    %130 = arith.index_cast %arg9 : index to i32\n+    %107 = arith.muli %130, %c32_i32 : i32\n+    %108 = arith.subi %arg5, %107 : i32\n+    %109 = tt.splat %108 : (i32) -> tensor<1x32xi32, #AL>\n+    %110 = \"triton_gpu.cmpi\"(%50, %109) <{predicate = 2 : i64}> : (tensor<1x32xi32, #AL>, tensor<1x32xi32, #AL>) -> tensor<1x32xi1, #AL>\n+    %111 = tt.broadcast %110 : (tensor<1x32xi1, #AL>) -> tensor<32x32xi1, #AL>\n+    %112 = tt.load %arg11, %111, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #AL>\n+    %113 = tt.splat %108 : (i32) -> tensor<32x1xi32, #AL>\n+    %114 = \"triton_gpu.cmpi\"(%66, %113) <{predicate = 2 : i64}> : (tensor<32x1xi32, #AL>, tensor<32x1xi32, #AL>) -> tensor<32x1xi1, #AL>\n+    %115 = tt.broadcast %114 : (tensor<32x1xi1, #AL>) -> tensor<32x32xi1, #AL>\n+    %116 = tt.load %arg12, %115, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #AL>\n+    %117 = triton_gpu.convert_layout %112 : (tensor<32x32xf32, #AL>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth = 1}>>\n+    %118 = triton_gpu.convert_layout %116 : (tensor<32x32xf32, #AL>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth = 1}>>\n+    %119 = tt.dot %117, %118, %arg10 {allowTF32 = true} : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth = 1}>> * tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth = 1}>> -> tensor<32x32xf32, #C>\n+    %131 = arith.index_cast %arg9 : index to i32\n+    %120 = arith.addi %131, %c1_i32 : i32\n+    %121 = arith.muli %120, %c32_i32 : i32\n+    %122 = tt.splat %121 : (i32) -> tensor<32x32xi32, #AL>\n+    %123 = tt.addptr %60, %122 : tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32xi32, #AL>\n+    %124 = arith.muli %121, %arg7 : i32\n+    %125 = tt.splat %124 : (i32) -> tensor<32x32xi32, #AL>\n+    %126 = tt.addptr %82, %125 : tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32xi32, #AL>\n+    scf.yield %119, %123, %126 : tensor<32x32xf32, #C>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>\n+  }\n+  tt.return %85#0 : tensor<32x32xf32, #C>\n+}\n+\n+// CHECK: tt.func @cross_iter_dep\n+// CHECK: triton_gpu.async_commit_group\n+// CHECK: triton_gpu.async_commit_group\n+// CHECK: triton_gpu.async_commit_group\n+// CHECK: triton_gpu.async_commit_group\n+// CHECK: %[[PTR0:.*]] = tt.addptr\n+// CHECK: %[[PTR1:.*]] = tt.addptr\n+// CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[BUF0:.*]] = %[[PTR0]], {{.*}}, %[[BUF1:.*]] = %[[PTR1]]\n+// CHECK: scf.yield\n+// CHECK-SAME: %[[BUF0]]\n+// CHECK-SAME: %[[BUF1]]\n+tt.func @cross_iter_dep(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                        %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                        %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                        %arg3: i32 {tt.divisibility = 16 : i32},\n+                        %arg4: i32 {tt.divisibility = 16 : i32},\n+                        %arg5: i32 {tt.divisibility = 16 : i32},\n+                        %arg6: i32 {tt.divisibility = 16 : i32},\n+                        %arg7: i32 {tt.divisibility = 16 : i32},\n+                        %arg8: i32 {tt.divisibility = 16 : i32}) -> tensor<32x32xf32, #C> {\n+  %c0_i32 = arith.constant 0 : index\n+  %118 = arith.constant 32 : index\n+  %c1_i32 = arith.constant 1 : index\n+  %c2_i32 = arith.constant 2 : i32\n+  %c32_i32 = arith.constant 32 : i32\n+  %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #C>\n+  %cst_1 = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #AL>\n+  %78 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %110 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %112 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %113 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %116 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %65 = tt.splat %arg3 : (i32) -> tensor<1x32xi32, #AL>\n+  %88 = tt.splat %arg4 : (i32) -> tensor<32x1xi32, #AL>\n+  %80 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %119:5 = scf.for %arg9 = %c0_i32 to %118 step %c1_i32 iter_args(%arg10 = %cst, %arg11 = %78, %arg12 = %110, %arg13 = %113, %arg14 = %116) -> (tensor<32x32xf32, #C>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>)  {\n+    %161 = arith.index_cast %arg9 : index to i32\n+    %141 = arith.muli %161, %c32_i32 : i32\n+    %142 = arith.subi %arg5, %141 : i32\n+    %143 = tt.splat %142 : (i32) -> tensor<1x32xi32, #AL>\n+    %144 = \"triton_gpu.cmpi\"(%65, %143) <{predicate = 2 : i64}> : (tensor<1x32xi32, #AL>, tensor<1x32xi32, #AL>) -> tensor<1x32xi1, #AL>\n+    %145 = tt.broadcast %144 : (tensor<1x32xi1, #AL>) -> tensor<32x32xi1, #AL>\n+    %146 = tt.load %arg11, %145, %cst_1 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #AL>\n+    %147 = tt.splat %142 : (i32) -> tensor<32x1xi32, #AL>\n+    %148 = \"triton_gpu.cmpi\"(%88, %147) <{predicate = 2 : i64}> : (tensor<32x1xi32, #AL>, tensor<32x1xi32, #AL>) -> tensor<32x1xi1, #AL>\n+    %149 = tt.broadcast %148 : (tensor<32x1xi1, #AL>) -> tensor<32x32xi1, #AL>\n+    %150 = tt.load %arg12, %149, %cst_1 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #AL>\n+    %151 = triton_gpu.convert_layout %146 : (tensor<32x32xf32, #AL>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth = 1}>>\n+    %152 = triton_gpu.convert_layout %150 : (tensor<32x32xf32, #AL>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth = 1}>>\n+    %153 = tt.dot %151, %152, %arg10 {allowTF32 = true} : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth = 1}>> * tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth = 1}>> -> tensor<32x32xf32, #C>\n+    %162 = arith.index_cast %arg9 : index to i32\n+    %154 = arith.addi %162, %c2_i32 : i32\n+    %155 = arith.muli %154, %c32_i32 : i32\n+    %156 = tt.splat %155 : (i32) -> tensor<32x32xi32, #AL>\n+    %157 = tt.addptr %80, %156 : tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32xi32, #AL>\n+    %158 = arith.muli %155, %arg7 : i32\n+    %159 = tt.splat %158 : (i32) -> tensor<32x32xi32, #AL>\n+    %160 = tt.addptr %112, %159 : tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32xi32, #AL>\n+    scf.yield %153, %arg13, %arg14, %157, %160 : tensor<32x32xf32, #C>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>\n+  }\n+  tt.return %119#0 : tensor<32x32xf32, #C>\n+}"}]