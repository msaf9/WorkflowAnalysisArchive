[{"filename": "include/triton/Analysis/Allocation.h", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -20,8 +20,6 @@ SmallVector<unsigned>\n getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n                              unsigned &outVec);\n \n-SmallVector<unsigned> getScratchConfigForReduce(triton::ReduceOp op);\n-\n } // namespace triton\n \n /// Modified from llvm-15.0: llvm/ADT/AddressRanges.h"}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 37, "deletions": 0, "changes": 37, "file_content_changes": "@@ -8,6 +8,35 @@\n \n namespace mlir {\n \n+class ReduceOpHelper {\n+public:\n+  explicit ReduceOpHelper(triton::ReduceOp op) : op(op) {\n+    srcTy = op.operand().getType().cast<RankedTensorType>();\n+  }\n+\n+  ArrayRef<int64_t> getSrcShape() { return srcTy.getShape(); }\n+\n+  Attribute getSrcLayout() { return srcTy.getEncoding(); }\n+\n+  bool isFastReduction();\n+\n+  unsigned getInterWarpSize();\n+\n+  unsigned getIntraWarpSize();\n+\n+  unsigned getThreadsReductionAxis();\n+\n+  SmallVector<unsigned> getScratchConfigBasic();\n+\n+  SmallVector<SmallVector<unsigned>> getScratchConfigsFast();\n+\n+  unsigned getScratchSizeInBytes();\n+\n+private:\n+  triton::ReduceOp op;\n+  RankedTensorType srcTy{};\n+};\n+\n bool isSharedEncoding(Value value);\n \n bool maybeSharedAllocationOp(Operation *op);\n@@ -16,6 +45,14 @@ bool maybeAliasOp(Operation *op);\n \n std::string getValueOperandName(Value value, AsmState &state);\n \n+template <typename T_OUT, typename T_IN>\n+inline SmallVector<T_OUT> convertType(ArrayRef<T_IN> in) {\n+  SmallVector<T_OUT> out;\n+  for (const T_IN &i : in)\n+    out.push_back(T_OUT(i));\n+  return out;\n+}\n+\n template <typename Int> Int product(llvm::ArrayRef<Int> arr) {\n   return std::accumulate(arr.begin(), arr.end(), 1, std::multiplies{});\n }"}, {"filename": "include/triton/Dialect/Triton/IR/TritonAttrDefs.td", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -59,7 +59,8 @@ def TT_AtomicRMWAttr : I32EnumAttr<\n         I32EnumAttrCase<\"MAX\", 6, \"max\">,\n         I32EnumAttrCase<\"MIN\", 7, \"min\">,\n         I32EnumAttrCase<\"UMAX\", 8, \"umax\">,\n-        I32EnumAttrCase<\"UMIN\", 9, \"umin\">\n+        I32EnumAttrCase<\"UMIN\", 9, \"umin\">,\n+        I32EnumAttrCase<\"XCHG\", 10, \"exch\">\n     ]> {\n     let cppNamespace = \"::mlir::triton\";\n }"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -351,6 +351,11 @@ def TT_ReduceOp : TT_Op<\"reduce\", [NoSideEffect,\n \n     let assemblyFormat = \"$operand attr-dict `:` type($operand) `->` type($result)\";\n \n+    let extraClassDeclaration = [{\n+        // This member function is marked static because we need to call it before the ReduceOp\n+        // is constructed, see the implementation of create_reduce in triton.cc.\n+        static bool withIndex(mlir::triton::RedOp redOp);\n+    }];\n }\n \n //"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 8, "deletions": 1, "changes": 9, "file_content_changes": "@@ -228,17 +228,24 @@ for\n       unsigned remainingLanes = 32;\n       unsigned remainingThreads = numWarps*32;\n       unsigned remainingWarps = numWarps;\n+      unsigned prevLanes = 1;\n+      unsigned prevWarps = 1;\n       SmallVector<unsigned, 4> threadsPerWarp(rank);\n       SmallVector<unsigned, 4> warpsPerCTA(rank);\n-      for (int _dim = 0; _dim < rank; ++_dim) {\n+      for (int _dim = 0; _dim < rank - 1; ++_dim) {\n         int i = order[_dim];\n         unsigned threadsPerCTA = std::clamp<unsigned>(remainingThreads, 1, shape[i] / sizePerThread[i]);\n         threadsPerWarp[i] = std::clamp<unsigned>(threadsPerCTA, 1, remainingLanes);\n         warpsPerCTA[i] = std::clamp<unsigned>(threadsPerCTA / threadsPerWarp[i], 1, remainingWarps);\n         remainingWarps /= warpsPerCTA[i];\n         remainingLanes /= threadsPerWarp[i];\n         remainingThreads /= threadsPerCTA;\n+        prevLanes *= threadsPerWarp[i];\n+        prevWarps *= warpsPerCTA[i];\n       }\n+      // Expand the last dimension to fill the remaining lanes and warps\n+      threadsPerWarp[order[rank-1]] = 32 / prevLanes;\n+      warpsPerCTA[order[rank-1]] = numWarps / prevWarps;\n \n       return $_get(context, sizePerThread, threadsPerWarp, warpsPerCTA, order);\n "}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 21, "deletions": 43, "changes": 64, "file_content_changes": "@@ -88,36 +88,12 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   return paddedRepShape;\n }\n \n-SmallVector<unsigned> getScratchConfigForReduce(triton::ReduceOp op) {\n-  auto srcTy = op.operand().getType().cast<RankedTensorType>();\n-  auto srcLayout = srcTy.getEncoding();\n-  auto srcShape = srcTy.getShape();\n-  auto axis = op.axis();\n-\n-  bool fastReduce = axis == getOrder(srcLayout)[0];\n-\n-  SmallVector<unsigned> smemShape;\n-  for (auto d : srcShape)\n-    smemShape.push_back(d);\n-\n-  if (fastReduce) {\n-    unsigned sizeInterWarps = gpu::getWarpsPerCTA(srcLayout)[axis];\n-    smemShape[axis] = sizeInterWarps;\n-  } else {\n-    unsigned threadsPerCTAAxis = gpu::getThreadsPerWarp(srcLayout)[axis] *\n-                                 gpu::getWarpsPerCTA(srcLayout)[axis];\n-    smemShape[axis] = threadsPerCTAAxis;\n-  }\n-\n-  return smemShape;\n-}\n-\n // TODO: extend beyond scalars\n SmallVector<unsigned> getScratchConfigForAtomicRMW(triton::AtomicRMWOp op) {\n   SmallVector<unsigned> smemShape;\n-  auto ptrTy = op.ptr().getType();\n-  if (auto tensorType = ptrTy.dyn_cast<RankedTensorType>()) {\n-    // do nothing or just assert because shared memory is not used in tensor\n+  if (op.ptr().getType().isa<RankedTensorType>()) {\n+    // do nothing or just assert because shared memory is not used in tensor up\n+    // to now\n   } else {\n     // need only bytes for scalar\n     // always vec = 1 and elemsPerThread = 1 for scalar?\n@@ -126,6 +102,10 @@ SmallVector<unsigned> getScratchConfigForAtomicRMW(triton::AtomicRMWOp op) {\n   return smemShape;\n }\n \n+SmallVector<unsigned> getScratchConfigForAtomicCAS(triton::AtomicCASOp op) {\n+  return SmallVector<unsigned>{1};\n+}\n+\n class AllocationAnalysis {\n public:\n   AllocationAnalysis(Operation *operation, Allocation *allocation)\n@@ -174,22 +154,9 @@ class AllocationAnalysis {\n   /// Initializes temporary shared memory for a given operation.\n   void getScratchValueSize(Operation *op) {\n     if (auto reduceOp = dyn_cast<triton::ReduceOp>(op)) {\n-      // TODO(Keren): Reduce with index is not supported yet.\n-      auto value = op->getOperand(0);\n-      if (auto tensorType = value.getType().dyn_cast<RankedTensorType>()) {\n-        auto srcLayout = tensorType.getEncoding();\n-        bool fastReduce = reduceOp.axis() == getOrder(srcLayout)[0];\n-        auto smemShape = getScratchConfigForReduce(reduceOp);\n-        unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n-                                         std::multiplies{});\n-        if (fastReduce) {\n-          auto mod = op->getParentOfType<ModuleOp>();\n-          unsigned numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n-          elems = std::max<unsigned>(elems, numWarps * 32);\n-        }\n-        auto bytes = elems * tensorType.getElementTypeBitWidth() / 8;\n-        allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n-      }\n+      ReduceOpHelper helper(reduceOp);\n+      unsigned bytes = helper.getScratchSizeInBytes();\n+      allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     } else if (auto cvtLayout = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n       auto srcTy = cvtLayout.src().getType().cast<RankedTensorType>();\n       auto dstTy = cvtLayout.result().getType().cast<RankedTensorType>();\n@@ -230,6 +197,17 @@ class AllocationAnalysis {\n                          : elems * elemTy.getIntOrFloatBitWidth() / 8;\n         allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n       }\n+    } else if (auto atomicCASOp = dyn_cast<triton::AtomicCASOp>(op)) {\n+      auto value = op->getOperand(0);\n+      auto smemShape = getScratchConfigForAtomicCAS(atomicCASOp);\n+      unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n+                                       std::multiplies{});\n+      auto elemTy =\n+          value.getType().cast<triton::PointerType>().getPointeeType();\n+      auto bytes = elemTy.isa<triton::PointerType>()\n+                       ? elems * kPtrBitWidth / 8\n+                       : elems * elemTy.getIntOrFloatBitWidth() / 8;\n+      allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     }\n   }\n "}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 81, "deletions": 0, "changes": 81, "file_content_changes": "@@ -5,6 +5,87 @@\n \n namespace mlir {\n \n+bool ReduceOpHelper::isFastReduction() {\n+  auto srcLayout = srcTy.getEncoding();\n+  auto axis = op.axis();\n+  return axis == triton::gpu::getOrder(srcLayout)[0];\n+}\n+\n+unsigned ReduceOpHelper::getInterWarpSize() {\n+  auto srcLayout = srcTy.getEncoding();\n+  auto srcShape = srcTy.getShape();\n+  auto axis = op.axis();\n+  auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n+  unsigned sizeIntraWarps = getIntraWarpSize();\n+  return std::min(srcReduceDimSize / sizeIntraWarps,\n+                  triton::gpu::getWarpsPerCTA(srcLayout)[axis]);\n+}\n+\n+unsigned ReduceOpHelper::getIntraWarpSize() {\n+  auto srcLayout = srcTy.getEncoding();\n+  auto srcShape = srcTy.getShape();\n+  auto axis = op.axis();\n+  auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n+  return std::min(srcReduceDimSize,\n+                  triton::gpu::getThreadsPerWarp(srcLayout)[axis]);\n+}\n+\n+unsigned ReduceOpHelper::getThreadsReductionAxis() {\n+  auto srcLayout = srcTy.getEncoding();\n+  auto axis = op.axis();\n+  return triton::gpu::getThreadsPerWarp(srcLayout)[axis] *\n+         triton::gpu::getWarpsPerCTA(srcLayout)[axis];\n+}\n+\n+SmallVector<unsigned> ReduceOpHelper::getScratchConfigBasic() {\n+  auto axis = op.axis();\n+  auto smemShape = convertType<unsigned>(getSrcShape());\n+  smemShape[axis] = std::min(smemShape[axis], getThreadsReductionAxis());\n+  return smemShape;\n+}\n+\n+SmallVector<SmallVector<unsigned>> ReduceOpHelper::getScratchConfigsFast() {\n+  auto axis = op.axis();\n+  SmallVector<SmallVector<unsigned>> smemShapes(3);\n+\n+  /// shared memory block0\n+  smemShapes[0] = convertType<unsigned>(getSrcShape());\n+  smemShapes[0][axis] = getInterWarpSize();\n+\n+  /// FIXME(Qingyi): This size is actually larger than required.\n+  /// shared memory block1:\n+  auto mod = op.getOperation()->getParentOfType<ModuleOp>();\n+  unsigned numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+  smemShapes[1].push_back(numWarps * 32);\n+\n+  /// FIXME(Qingyi): This requirement is actually not necessary, because it is\n+  /// always smaller than smemShapes[0] shared memory block2\n+  smemShapes[2] = convertType<unsigned>(getSrcShape());\n+  smemShapes[2].erase(smemShapes[2].begin() + axis);\n+\n+  return smemShapes;\n+}\n+\n+unsigned ReduceOpHelper::getScratchSizeInBytes() {\n+  unsigned elems = 0;\n+  if (isFastReduction()) {\n+    auto smemShapes = getScratchConfigsFast();\n+    for (const auto &smemShape : smemShapes)\n+      elems = std::max(elems, product<unsigned>(smemShape));\n+  } else {\n+    auto smemShape = getScratchConfigBasic();\n+    elems = product<unsigned>(smemShape);\n+  }\n+\n+  auto tensorType = op.operand().getType().cast<RankedTensorType>();\n+  unsigned bytes = elems * tensorType.getElementTypeBitWidth() / 8;\n+\n+  if (triton::ReduceOp::withIndex(op.redOp()))\n+    bytes += elems * sizeof(int32_t);\n+\n+  return bytes;\n+}\n+\n bool isSharedEncoding(Value value) {\n   auto type = value.getType();\n   if (auto tensorType = type.dyn_cast<RankedTensorType>()) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotHelpers.h", "status": "added", "additions": 1835, "deletions": 0, "changes": 1835, "file_content_changes": "@@ -0,0 +1,1835 @@\n+#ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_DOT_HELPERS_H\n+#define TRITON_CONVERSION_TRITONGPU_TO_LLVM_DOT_HELPERS_H\n+\n+#include \"./Utility.h\"\n+#include \"mlir/Analysis/SliceAnalysis.h\"\n+#include \"mlir/Conversion/ArithmeticToLLVM/ArithmeticToLLVM.h\"\n+#include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n+#include \"mlir/Conversion/LLVMCommon/LoweringOptions.h\"\n+#include \"mlir/Conversion/LLVMCommon/Pattern.h\"\n+#include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n+#include \"mlir/Conversion/SCFToStandard/SCFToStandard.h\"\n+#include \"mlir/Conversion/StandardToLLVM/ConvertStandardToLLVM.h\"\n+#include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"\n+#include \"mlir/Dialect/GPU/GPUDialect.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/TypeUtilities.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n+#include \"triton/Analysis/Allocation.h\"\n+#include \"triton/Analysis/AxisInfo.h\"\n+#include \"triton/Analysis/Membar.h\"\n+#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Conversion/MLIRTypes.h\"\n+#include \"triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"llvm/Support/Format.h\"\n+#include \"llvm/Support/FormatVariadic.h\"\n+#include <memory>\n+#include <numeric>\n+#include <string>\n+\n+namespace mlir {\n+namespace LLVM {\n+using namespace mlir::triton;\n+using ::mlir::triton::gpu::BlockedEncodingAttr;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n+\n+// Forward declaration necessary functions locates in TritonGPUToLLVM.cpp .\n+llvm::SmallVector<mlir::Value>\n+getElementsFromStruct(mlir::Location loc, mlir::Value llvmStruct,\n+                      mlir::ConversionPatternRewriter &rewriter);\n+\n+mlir::LLVM::SharedMemoryObject\n+getSharedMemoryObjectFromStruct(mlir::Location loc, mlir::Value llvmStruct,\n+                                mlir::ConversionPatternRewriter &rewriter);\n+\n+// Helper for conversion of DotOp with mma<version=1>, that is sm<80\n+struct DotOpMmaV1ConversionHelper {\n+  MmaEncodingAttr mmaLayout;\n+  ArrayRef<unsigned> wpt;\n+\n+  using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n+\n+  explicit DotOpMmaV1ConversionHelper(MmaEncodingAttr mmaLayout)\n+      : mmaLayout(mmaLayout), wpt(mmaLayout.getWarpsPerCTA()) {}\n+\n+  int getRepM(int M) const {\n+    return std::max<int>(M / (wpt[0] * instrShape[0]), 1);\n+  }\n+  int getRepN(int N) const {\n+    return std::max<int>(N / (wpt[1] * instrShape[1]), 1);\n+  }\n+\n+  static ArrayRef<unsigned> getMmaInstrShape() { return instrShape; }\n+\n+  static Type getMmaRetType(TensorType operand) {\n+    auto *ctx = operand.getContext();\n+    Type fp32Ty = type::f32Ty(ctx);\n+    // f16*f16+f32->f32\n+    return struct_ty(SmallVector<Type>{8, fp32Ty});\n+  }\n+\n+  // number of fp16x2 elements for $a.\n+  int numElemsPerThreadA(RankedTensorType tensorTy) const {\n+    auto shape = tensorTy.getShape();\n+    auto order = getOrder();\n+\n+    bool isARow = order[0] != 0;\n+    bool isAVec4 = !isARow && shape[order[0]] <= 16; // fp16*4 = 16bytes\n+    int packSize0 = (isARow || isAVec4) ? 1 : 2;\n+\n+    SmallVector<int> fpw({2, 2, 1});\n+    int repM = 2 * packSize0;\n+    int repK = 1;\n+    int spwM = fpw[0] * 4 * repM;\n+    SmallVector<int> rep({repM, 0, repK}); // pad N with 0\n+    SmallVector<int> spw({spwM, 0, 1});    // pad N with 0\n+\n+    int NK = shape[1];\n+    unsigned numM = rep[0] * shape[0] / (spw[0] * wpt[0]);\n+\n+    // NOTE: We couldn't get the vec from the shared layout.\n+    // int vecA = sharedLayout.getVec();\n+    // TODO[Superjomn]: Consider the case when vecA > 4\n+    bool vecGt4 = false;\n+    int elemsPerLd = vecGt4 ? 4 : 2;\n+    return (numM / 2) * (NK / 4) * elemsPerLd;\n+  }\n+\n+  // number of fp16x2 elements for $b.\n+  int numElemsPerThreadB(RankedTensorType tensorTy) const {\n+    auto shape = tensorTy.getShape();\n+    auto order = getOrder();\n+    bool isBRow = order[0] != 0;\n+    bool isBVec4 = isBRow && shape[order[0]] <= 16;\n+    int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n+    SmallVector<int> fpw({2, 2, 1});\n+    SmallVector<int> rep({0, 2 * packSize1, 1});       // pad M with 0\n+    SmallVector<int> spw({0, fpw[1] * 4 * rep[1], 1}); // pad M with 0\n+    // NOTE: We couldn't get the vec from the shared layout.\n+    // int vecB = sharedLayout.getVec();\n+    // TODO[Superjomn]: Consider the case when vecA > 4\n+    bool vecGt4 = false;\n+    int elemsPerLd = vecGt4 ? 4 : 2;\n+    int NK = shape[0];\n+\n+    unsigned numN = rep[1] * shape[1] / (spw[1] * wpt[0]);\n+    return (numN / 2) * (NK / 4) * elemsPerLd;\n+  }\n+\n+  // Loading $a from smem to registers, returns a LLVM::Struct.\n+  Value loadA(Value A, const SharedMemoryObject &smemObj, Value thread,\n+              Location loc, ConversionPatternRewriter &rewriter) const;\n+\n+  // Loading $b from smem to registers, returns a LLVM::Struct.\n+  Value loadB(Value B, const SharedMemoryObject &smemObj, Value thread,\n+              Location loc, ConversionPatternRewriter &rewriter) const;\n+\n+  static ArrayRef<unsigned> getOrder() { return mmaOrder; }\n+\n+  // Compute the offset of the matrix to load.\n+  // Returns offsetAM, offsetAK, offsetBN, offsetBK.\n+  // NOTE, the information M(from $a) and N(from $b) couldn't be retrieved at\n+  // the same time in the usage in convert_layout[shared->dot_op], we leave\n+  // the noexist info to be 0 and only use the desired argument from the\n+  // composed result. In this way we want to retain the original code\n+  // structure in convert_mma884 method for easier debugging.\n+  std::tuple<Value, Value, Value, Value>\n+  computeOffsets(Value threadId, bool isARow, bool isBRow, ArrayRef<int> fpw,\n+                 ArrayRef<int> spw, ArrayRef<int> rep,\n+                 ConversionPatternRewriter &rewriter, Location loc) const;\n+\n+  // Extract values belong to $a or $b from a LLVMStruct, the shape is n0xn1.\n+  DotOpMmaV1ConversionHelper::ValueTable\n+  extractLoadedOperand(Value llStruct, int NK,\n+                       ConversionPatternRewriter &rewriter) const;\n+\n+private:\n+  static constexpr unsigned instrShape[] = {16, 16, 4};\n+  static constexpr unsigned mmaOrder[] = {0, 1};\n+};\n+\n+// Helper for conversion of DotOp with mma<version=2>, that is sm>=80\n+struct DotOpMmaV2ConversionHelper {\n+  enum class TensorCoreType : uint8_t {\n+    // floating-point tensor core instr\n+    FP32_FP16_FP16_FP32 = 0, // default\n+    FP32_BF16_BF16_FP32,\n+    FP32_TF32_TF32_FP32,\n+    // integer tensor core instr\n+    INT32_INT1_INT1_INT32, // Not implemented\n+    INT32_INT4_INT4_INT32, // Not implemented\n+    INT32_INT8_INT8_INT32, // Not implemented\n+    //\n+    NOT_APPLICABLE,\n+  };\n+\n+  MmaEncodingAttr mmaLayout;\n+  MLIRContext *ctx{};\n+\n+  explicit DotOpMmaV2ConversionHelper(MmaEncodingAttr mmaLayout)\n+      : mmaLayout(mmaLayout) {\n+    ctx = mmaLayout.getContext();\n+  }\n+\n+  void deduceMmaType(DotOp op) const { mmaType = getMmaType(op); }\n+  void deduceMmaType(Type operandTy) const {\n+    mmaType = getTensorCoreTypeFromOperand(operandTy);\n+  }\n+\n+  // Get the M and N of mma instruction shape.\n+  static std::tuple<int, int> getInstrShapeMN() {\n+    // According to DotOpConversionHelper::mmaInstrShape, all the M,N are\n+    // {16,8}\n+    return {16, 8};\n+  }\n+\n+  static std::tuple<int, int> getRepMN(const RankedTensorType &tensorTy) {\n+    auto mmaLayout = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n+    auto wpt = mmaLayout.getWarpsPerCTA();\n+\n+    int M = tensorTy.getShape()[0];\n+    int N = tensorTy.getShape()[1];\n+    auto [instrM, instrN] = getInstrShapeMN();\n+    int repM = std::max<int>(M / (wpt[0] * instrM), 1);\n+    int repN = std::max<int>(N / (wpt[1] * instrN), 1);\n+    return {repM, repN};\n+  }\n+\n+  Type getShemPtrTy() const {\n+    switch (mmaType) {\n+    case TensorCoreType::FP32_FP16_FP16_FP32:\n+      return ptr_ty(type::f16Ty(ctx), 3);\n+    case TensorCoreType::FP32_BF16_BF16_FP32:\n+      return ptr_ty(type::bf16Ty(ctx), 3);\n+    case TensorCoreType::FP32_TF32_TF32_FP32:\n+      return ptr_ty(type::f32Ty(ctx), 3);\n+    case TensorCoreType::INT32_INT8_INT8_INT32:\n+      return ptr_ty(type::i8Ty(ctx), 3);\n+    default:\n+      llvm::report_fatal_error(\"mma16816 data type not supported\");\n+    }\n+    return Type{};\n+  }\n+\n+  // The type of matrix that loaded by either a ldmatrix or composed lds.\n+  Type getMatType() const {\n+    Type fp32Ty = type::f32Ty(ctx);\n+    Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n+    Type bf16x2Ty = vec_ty(type::bf16Ty(ctx), 2);\n+    // floating point types\n+    Type fp16x2Pack4Ty =\n+        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp16x2Ty));\n+    Type bf16x2Pack4Ty =\n+        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, bf16x2Ty));\n+    Type fp32Pack4Ty =\n+        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n+    // integer types\n+    Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n+    Type i8x4Pack4Ty =\n+        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i8x4Ty));\n+\n+    switch (mmaType) {\n+    case TensorCoreType::FP32_FP16_FP16_FP32:\n+      return fp16x2Pack4Ty;\n+    case TensorCoreType::FP32_BF16_BF16_FP32:\n+      return bf16x2Pack4Ty;\n+    case TensorCoreType::FP32_TF32_TF32_FP32:\n+      return fp32Pack4Ty;\n+    case TensorCoreType::INT32_INT8_INT8_INT32:\n+      return i8x4Pack4Ty;\n+    default:\n+      llvm::report_fatal_error(\"Unsupported mma type found\");\n+    }\n+\n+    return Type{};\n+  }\n+\n+  Type getLoadElemTy() {\n+    switch (mmaType) {\n+    case TensorCoreType::FP32_FP16_FP16_FP32:\n+      return vec_ty(type::f16Ty(ctx), 2);\n+    case TensorCoreType::FP32_BF16_BF16_FP32:\n+      return vec_ty(type::bf16Ty(ctx), 2);\n+    case TensorCoreType::FP32_TF32_TF32_FP32:\n+      return type::f32Ty(ctx);\n+    case TensorCoreType::INT32_INT8_INT8_INT32:\n+      return type::i32Ty(ctx);\n+    default:\n+      llvm::report_fatal_error(\"Unsupported mma type found\");\n+    }\n+\n+    return Type{};\n+  }\n+\n+  Type getMmaRetType() const {\n+    Type fp32Ty = type::f32Ty(ctx);\n+    Type i32Ty = type::i32Ty(ctx);\n+    Type fp32x4Ty =\n+        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n+    Type i32x4Ty =\n+        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i32Ty));\n+    switch (mmaType) {\n+    case TensorCoreType::FP32_FP16_FP16_FP32:\n+      return fp32x4Ty;\n+    case TensorCoreType::FP32_BF16_BF16_FP32:\n+      return fp32x4Ty;\n+    case TensorCoreType::FP32_TF32_TF32_FP32:\n+      return fp32x4Ty;\n+    case TensorCoreType::INT32_INT8_INT8_INT32:\n+      return i32x4Ty;\n+    default:\n+      llvm::report_fatal_error(\"Unsupported mma type found\");\n+    }\n+\n+    return Type{};\n+  }\n+\n+  ArrayRef<int> getMmaInstrShape() const {\n+    assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n+           \"Unknown mma type found.\");\n+    return mmaInstrShape.at(mmaType);\n+  }\n+\n+  static ArrayRef<int> getMmaInstrShape(TensorCoreType tensorCoreType) {\n+    assert(tensorCoreType != TensorCoreType::NOT_APPLICABLE &&\n+           \"Unknown mma type found.\");\n+    return mmaInstrShape.at(tensorCoreType);\n+  }\n+\n+  ArrayRef<int> getMmaMatShape() const {\n+    assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n+           \"Unknown mma type found.\");\n+    return mmaMatShape.at(mmaType);\n+  }\n+\n+  // Deduce the TensorCoreType from either $a or $b's type.\n+  static TensorCoreType getTensorCoreTypeFromOperand(Type operandTy) {\n+    auto tensorTy = operandTy.cast<RankedTensorType>();\n+    auto elemTy = tensorTy.getElementType();\n+    if (elemTy.isF16())\n+      return TensorCoreType::FP32_FP16_FP16_FP32;\n+    if (elemTy.isF32())\n+      return TensorCoreType::FP32_TF32_TF32_FP32;\n+    if (elemTy.isBF16())\n+      return TensorCoreType::FP32_BF16_BF16_FP32;\n+    if (elemTy.isInteger(8))\n+      return TensorCoreType::INT32_INT8_INT8_INT32;\n+    return TensorCoreType::NOT_APPLICABLE;\n+  }\n+\n+  int getVec() const {\n+    assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n+           \"Unknown mma type found.\");\n+    return mmaInstrVec.at(mmaType);\n+  }\n+\n+  StringRef getMmaInstr() const {\n+    assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n+           \"Unknown mma type found.\");\n+    return mmaInstrPtx.at(mmaType);\n+  }\n+\n+  static TensorCoreType getMmaType(triton::DotOp op) {\n+    Value A = op.a();\n+    Value B = op.b();\n+    auto aTy = A.getType().cast<RankedTensorType>();\n+    auto bTy = B.getType().cast<RankedTensorType>();\n+    // d = a*b + c\n+    auto dTy = op.d().getType().cast<RankedTensorType>();\n+\n+    if (dTy.getElementType().isF32()) {\n+      if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n+        return TensorCoreType::FP32_FP16_FP16_FP32;\n+      if (aTy.getElementType().isBF16() && bTy.getElementType().isBF16())\n+        return TensorCoreType::FP32_BF16_BF16_FP32;\n+      if (aTy.getElementType().isF32() && bTy.getElementType().isF32() &&\n+          op.allowTF32())\n+        return TensorCoreType::FP32_TF32_TF32_FP32;\n+    } else if (dTy.getElementType().isInteger(32)) {\n+      if (aTy.getElementType().isInteger(8) &&\n+          bTy.getElementType().isInteger(8))\n+        return TensorCoreType::INT32_INT8_INT8_INT32;\n+    }\n+\n+    return TensorCoreType::NOT_APPLICABLE;\n+  }\n+\n+private:\n+  mutable TensorCoreType mmaType{TensorCoreType::NOT_APPLICABLE};\n+\n+  // Used on nvidia GPUs mma layout .version == 2\n+  // Refer to\n+  // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-storage\n+  // for more details.\n+  inline static const std::map<TensorCoreType, llvm::SmallVector<int>>\n+      mmaInstrShape = {\n+          {TensorCoreType::FP32_FP16_FP16_FP32, {16, 8, 16}},\n+          {TensorCoreType::FP32_BF16_BF16_FP32, {16, 8, 16}},\n+          {TensorCoreType::FP32_TF32_TF32_FP32, {16, 8, 8}},\n+\n+          {TensorCoreType::INT32_INT1_INT1_INT32, {16, 8, 256}},\n+          {TensorCoreType::INT32_INT4_INT4_INT32, {16, 8, 64}},\n+          {TensorCoreType::INT32_INT8_INT8_INT32, {16, 8, 32}},\n+  };\n+\n+  // shape of matrices loaded by ldmatrix (m-n-k, for mxk & kxn matrices)\n+  // Refer to\n+  // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-ldmatrix\n+  // for more details.\n+  inline static const std::map<TensorCoreType, llvm::SmallVector<int>>\n+      mmaMatShape = {\n+          {TensorCoreType::FP32_FP16_FP16_FP32, {8, 8, 8}},\n+          {TensorCoreType::FP32_BF16_BF16_FP32, {8, 8, 8}},\n+          {TensorCoreType::FP32_TF32_TF32_FP32, {8, 8, 4}},\n+\n+          {TensorCoreType::INT32_INT1_INT1_INT32, {8, 8, 64}},\n+          {TensorCoreType::INT32_INT4_INT4_INT32, {8, 8, 32}},\n+          {TensorCoreType::INT32_INT8_INT8_INT32, {8, 8, 16}},\n+  };\n+\n+  // Supported mma instruction in PTX.\n+  // Refer to\n+  // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-for-mma\n+  // for more details.\n+  inline static const std::map<TensorCoreType, std::string> mmaInstrPtx = {\n+      {TensorCoreType::FP32_FP16_FP16_FP32,\n+       \"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\"},\n+      {TensorCoreType::FP32_BF16_BF16_FP32,\n+       \"mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\"},\n+      {TensorCoreType::FP32_TF32_TF32_FP32,\n+       \"mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\"},\n+\n+      {TensorCoreType::INT32_INT1_INT1_INT32,\n+       \"mma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.xor.popc\"},\n+      {TensorCoreType::INT32_INT4_INT4_INT32,\n+       \"mma.sync.aligned.m16n8k64.row.col.satfinite.s32.s4.s4.s32\"},\n+      {TensorCoreType::INT32_INT8_INT8_INT32,\n+       \"mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32\"},\n+  };\n+\n+  // vector length per ldmatrix (16*8/element_size_in_bits)\n+  inline static const std::map<TensorCoreType, uint8_t> mmaInstrVec = {\n+      {TensorCoreType::FP32_FP16_FP16_FP32, 8},\n+      {TensorCoreType::FP32_BF16_BF16_FP32, 8},\n+      {TensorCoreType::FP32_TF32_TF32_FP32, 4},\n+\n+      {TensorCoreType::INT32_INT1_INT1_INT32, 128},\n+      {TensorCoreType::INT32_INT4_INT4_INT32, 32},\n+      {TensorCoreType::INT32_INT8_INT8_INT32, 16},\n+  };\n+};\n+\n+// Data loader for mma.16816 instruction.\n+class MMA16816SmemLoader {\n+public:\n+  MMA16816SmemLoader(int wpt, ArrayRef<uint32_t> order, uint32_t kOrder,\n+                     ArrayRef<Value> smemStrides, ArrayRef<int64_t> tileShape,\n+                     ArrayRef<int> instrShape, ArrayRef<int> matShape,\n+                     int perPhase, int maxPhase, int elemBytes,\n+                     ConversionPatternRewriter &rewriter,\n+                     TypeConverter *typeConverter, const Location &loc)\n+      : order(order.begin(), order.end()), kOrder(kOrder),\n+        tileShape(tileShape.begin(), tileShape.end()),\n+        instrShape(instrShape.begin(), instrShape.end()),\n+        matShape(matShape.begin(), matShape.end()), perPhase(perPhase),\n+        maxPhase(maxPhase), elemBytes(elemBytes), rewriter(rewriter), loc(loc),\n+        ctx(rewriter.getContext()) {\n+    cMatShape = matShape[order[0]];\n+    sMatShape = matShape[order[1]];\n+\n+    sStride = smemStrides[order[1]];\n+\n+    // rule: k must be the fast-changing axis.\n+    needTrans = kOrder != order[0];\n+    canUseLdmatrix = elemBytes == 2 || (!needTrans); // b16\n+\n+    if (canUseLdmatrix) {\n+      // Each CTA, the warps is arranged as [1xwpt] if not transposed,\n+      // otherwise [wptx1], and each warp will perform a mma.\n+      numPtrs =\n+          tileShape[order[0]] / (needTrans ? wpt : 1) / instrShape[order[0]];\n+    } else {\n+      numPtrs = tileShape[order[0]] / wpt / matShape[order[0]];\n+    }\n+    numPtrs = std::max<int>(numPtrs, 2);\n+\n+    // Special rule for i8/u8, 4 ptrs for each matrix\n+    if (!canUseLdmatrix && elemBytes == 1)\n+      numPtrs *= 4;\n+\n+    int loadStrideInMat[2];\n+    loadStrideInMat[kOrder] =\n+        2; // instrShape[kOrder] / matShape[kOrder], always 2\n+    loadStrideInMat[kOrder ^ 1] =\n+        wpt * (instrShape[kOrder ^ 1] / matShape[kOrder ^ 1]);\n+\n+    pLoadStrideInMat = loadStrideInMat[order[0]];\n+    sMatStride =\n+        loadStrideInMat[order[1]] / (instrShape[order[1]] / matShape[order[1]]);\n+\n+    // Each matArr contains warpOffStride matrices.\n+    matArrStride = kOrder == 1 ? 1 : wpt;\n+    warpOffStride = instrShape[kOrder ^ 1] / matShape[kOrder ^ 1];\n+  }\n+\n+  // lane = thread % 32\n+  // warpOff = (thread/32) % wpt(0)\n+  llvm::SmallVector<Value> computeOffsets(Value warpOff, Value lane,\n+                                          Value cSwizzleOffset) {\n+    if (canUseLdmatrix)\n+      return computeLdmatrixMatOffs(warpOff, lane, cSwizzleOffset);\n+    else if (elemBytes == 4 && needTrans)\n+      return computeB32MatOffs(warpOff, lane, cSwizzleOffset);\n+    else if (elemBytes == 1 && needTrans)\n+      return computeB8MatOffs(warpOff, lane, cSwizzleOffset);\n+    else\n+      llvm::report_fatal_error(\"Invalid smem load config\");\n+\n+    return {};\n+  }\n+\n+  int getNumPtrs() const { return numPtrs; }\n+\n+  // Compute the offset to the matrix this thread(indexed by warpOff and lane)\n+  // mapped to.\n+  SmallVector<Value> computeLdmatrixMatOffs(Value warpId, Value lane,\n+                                            Value cSwizzleOffset) {\n+    // 4x4 matrices\n+    Value c = urem(lane, i32_val(8));\n+    Value s = udiv(lane, i32_val(8)); // sub-warp-id\n+\n+    // Decompose s => s_0, s_1, that is the coordinate in 2x2 matrices in a\n+    // warp\n+    Value s0 = urem(s, i32_val(2));\n+    Value s1 = udiv(s, i32_val(2));\n+\n+    // We use different orders for a and b for better performance.\n+    Value kMatArr = kOrder == 1 ? s1 : s0;\n+    Value nkMatArr = kOrder == 1 ? s0 : s1;\n+\n+    // matrix coordinate inside a CTA, the matrix layout is [2x2wpt] for A and\n+    // [2wptx2] for B. e.g. Setting wpt=3, The data layout for A(kOrder=1) is\n+    //   |0 0 1 1 2 2| -> 0,1,2 are the warpids\n+    //   |0 0 1 1 2 2|\n+    //\n+    // for B(kOrder=0) is\n+    //   |0 0|  -> 0,1,2 are the warpids\n+    //   |1 1|\n+    //   |2 2|\n+    //   |0 0|\n+    //   |1 1|\n+    //   |2 2|\n+    // Note, for each warp, it handles a 2x2 matrices, that is the coordinate\n+    // address (s0,s1) annotates.\n+\n+    Value matOff[2];\n+    matOff[kOrder ^ 1] = add(\n+        mul(warpId, i32_val(warpOffStride)),   // warp offset\n+        mul(nkMatArr, i32_val(matArrStride))); // matrix offset inside a warp\n+    matOff[kOrder] = kMatArr;\n+\n+    // Physical offset (before swizzling)\n+    Value cMatOff = matOff[order[0]];\n+    Value sMatOff = matOff[order[1]];\n+    Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n+    cMatOff = add(cMatOff, cSwizzleMatOff);\n+\n+    // row offset inside a matrix, each matrix has 8 rows.\n+    Value sOffInMat = c;\n+\n+    SmallVector<Value> offs(numPtrs);\n+    Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n+    Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n+    for (int i = 0; i < numPtrs; ++i) {\n+      Value cMatOffI = add(cMatOff, i32_val(i * pLoadStrideInMat));\n+      cMatOffI = xor_(cMatOffI, phase);\n+      offs[i] = add(mul(cMatOffI, i32_val(cMatShape)), mul(sOff, sStride));\n+    }\n+\n+    return offs;\n+  }\n+\n+  // Compute 32-bit matrix offsets.\n+  SmallVector<Value> computeB32MatOffs(Value warpOff, Value lane,\n+                                       Value cSwizzleOffset) {\n+    assert(needTrans && \"Only used in transpose mode.\");\n+    // Load tf32 matrices with lds32\n+    Value cOffInMat = udiv(lane, i32_val(4));\n+    Value sOffInMat = urem(lane, i32_val(4));\n+\n+    Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n+    SmallVector<Value> offs(numPtrs);\n+\n+    for (int mat = 0; mat < 4; ++mat) { // Load 4 mats each time\n+      int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n+      int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n+      if (kMatArrInt > 0) // we don't need pointers for k\n+        continue;\n+      Value kMatArr = i32_val(kMatArrInt);\n+      Value nkMatArr = i32_val(nkMatArrInt);\n+\n+      Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n+                          mul(nkMatArr, i32_val(matArrStride)));\n+      Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n+      cMatOff = add(cMatOff, cSwizzleMatOff);\n+\n+      Value sMatOff = kMatArr;\n+      Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n+      // FIXME: (kOrder == 1?) is really dirty hack\n+      for (int i = 0; i < numPtrs / 2; ++i) {\n+        Value cMatOffI =\n+            add(cMatOff, i32_val(i * pLoadStrideInMat * (kOrder == 1 ? 1 : 2)));\n+        cMatOffI = xor_(cMatOffI, phase);\n+        Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n+        cOff = urem(cOff, i32_val(tileShape[order[0]]));\n+        sOff = urem(sOff, i32_val(tileShape[order[1]]));\n+        offs[2 * i + nkMatArrInt] = add(cOff, mul(sOff, sStride));\n+      }\n+    }\n+    return offs;\n+  }\n+\n+  // compute 8-bit matrix offset.\n+  SmallVector<Value> computeB8MatOffs(Value warpOff, Value lane,\n+                                      Value cSwizzleOffset) {\n+    assert(needTrans && \"Only used in transpose mode.\");\n+    Value cOffInMat = udiv(lane, i32_val(4));\n+    Value sOffInMat =\n+        mul(urem(lane, i32_val(4)), i32_val(4)); // each thread load 4 cols\n+\n+    SmallVector<Value> offs(numPtrs);\n+    for (int mat = 0; mat < 4; ++mat) {\n+      int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n+      int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n+      if (kMatArrInt > 0) // we don't need pointers for k\n+        continue;\n+      Value kMatArr = i32_val(kMatArrInt);\n+      Value nkMatArr = i32_val(nkMatArrInt);\n+\n+      Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n+                          mul(nkMatArr, i32_val(matArrStride)));\n+      Value sMatOff = kMatArr;\n+\n+      for (int loadx4Off = 0; loadx4Off < numPtrs / 8; ++loadx4Off) {\n+        for (int elemOff = 0; elemOff < 4; ++elemOff) {\n+          int ptrOff = loadx4Off * 8 + nkMatArrInt * 4 + elemOff;\n+          Value cMatOffI = add(cMatOff, i32_val(loadx4Off * pLoadStrideInMat *\n+                                                (kOrder == 1 ? 1 : 2)));\n+          Value sOffInMatElem = add(sOffInMat, i32_val(elemOff));\n+\n+          // disable swizzling ...\n+\n+          Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n+          Value sOff = add(sOffInMatElem, mul(sMatOff, i32_val(sMatShape)));\n+          // To prevent out-of-bound access when tile is too small.\n+          cOff = urem(cOff, i32_val(tileShape[order[0]]));\n+          sOff = urem(sOff, i32_val(tileShape[order[1]]));\n+          offs[ptrOff] = add(cOff, mul(sOff, sStride));\n+        }\n+      }\n+    }\n+    return offs;\n+  }\n+\n+  // Load 4 matrices and returns 4 vec<2> elements.\n+  std::tuple<Value, Value, Value, Value>\n+  loadX4(int mat0, int mat1, ArrayRef<Value> offs, ArrayRef<Value> ptrs,\n+         Type ldmatrixRetTy, Type shemPtrTy) const {\n+    assert(mat0 % 2 == 0 && mat1 % 2 == 0 &&\n+           \"smem matrix load must be aligned\");\n+    int matIdx[2] = {mat0, mat1};\n+\n+    int ptrIdx{-1};\n+\n+    if (canUseLdmatrix)\n+      ptrIdx = matIdx[order[0]] / (instrShape[order[0]] / matShape[order[0]]);\n+    else if (elemBytes == 4 && needTrans)\n+      ptrIdx = matIdx[order[0]];\n+    else if (elemBytes == 1 && needTrans)\n+      ptrIdx = matIdx[order[0]] * 4;\n+    else\n+      llvm::report_fatal_error(\"unsupported mma type found\");\n+\n+    // The main difference with the original triton code is we removed the\n+    // prefetch-related logic here for the upstream optimizer phase should\n+    // take care with it, and that is transparent in dot conversion.\n+    auto getPtr = [&](int idx) { return ptrs[idx]; };\n+\n+    Value ptr = getPtr(ptrIdx);\n+\n+    if (canUseLdmatrix) {\n+      Value sOffset =\n+          mul(i32_val(matIdx[order[1]] * sMatStride * sMatShape), sStride);\n+      Value sOffsetPtr = gep(shemPtrTy, ptr, sOffset);\n+\n+      PTXBuilder builder;\n+      // ldmatrix.m8n8.x4 returns 4x2xfp16(that is 4xb32) elements for a\n+      // thread.\n+      auto resArgs = builder.newListOperand(4, \"=r\");\n+      auto addrArg = builder.newAddrOperand(sOffsetPtr, \"r\");\n+\n+      auto ldmatrix = builder.create(\"ldmatrix.sync.aligned.m8n8.x4\")\n+                          ->o(\"trans\", needTrans /*predicate*/)\n+                          .o(\"shared.b16\");\n+      ldmatrix(resArgs, addrArg);\n+\n+      // The result type is 4xi32, each i32 is composed of 2xf16\n+      // elements(adjacent two columns in a row)\n+      Value resV4 = builder.launch(rewriter, loc, ldmatrixRetTy);\n+\n+      auto getIntAttr = [&](int v) {\n+        return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n+      };\n+\n+      // The struct should have exactly the same element types.\n+      Type elemType = resV4.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n+\n+      return {extract_val(elemType, resV4, getIntAttr(0)),\n+              extract_val(elemType, resV4, getIntAttr(1)),\n+              extract_val(elemType, resV4, getIntAttr(2)),\n+              extract_val(elemType, resV4, getIntAttr(3))};\n+    } else if (elemBytes == 4 &&\n+               needTrans) { // Use lds.32 to load tf32 matrices\n+      Value ptr2 = getPtr(ptrIdx + 1);\n+      assert(sMatStride == 1);\n+      int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n+      Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n+      int sOffsetArrElem = sMatStride * sMatShape;\n+      Value sOffsetArrElemVal =\n+          add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n+\n+      Value elems[4];\n+      Type elemTy = type::f32Ty(ctx);\n+      Type elemPtrTy = ptr_ty(elemTy);\n+      if (kOrder == 1) {\n+        elems[0] = load(gep(elemPtrTy, ptr, i32_val(sOffsetElem)));\n+        elems[1] = load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem)));\n+        elems[2] =\n+            load(gep(elemPtrTy, ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n+        elems[3] =\n+            load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n+      } else {\n+        elems[0] = load(gep(elemPtrTy, ptr, i32_val(sOffsetElem)));\n+        elems[2] = load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem)));\n+        elems[1] =\n+            load(gep(elemPtrTy, ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n+        elems[3] =\n+            load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n+      }\n+      return {elems[0], elems[1], elems[2], elems[3]};\n+\n+    } else if (elemBytes == 1 && needTrans) { // work with int8\n+      std::array<std::array<Value, 4>, 2> ptrs;\n+      ptrs[0] = {\n+          getPtr(ptrIdx),\n+          getPtr(ptrIdx + 1),\n+          getPtr(ptrIdx + 2),\n+          getPtr(ptrIdx + 3),\n+      };\n+\n+      ptrs[1] = {\n+          getPtr(ptrIdx + 4),\n+          getPtr(ptrIdx + 5),\n+          getPtr(ptrIdx + 6),\n+          getPtr(ptrIdx + 7),\n+      };\n+\n+      assert(sMatStride == 1);\n+      int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n+      Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n+      int sOffsetArrElem = 1 * (sMatStride * sMatShape);\n+      Value sOffsetArrElemVal =\n+          add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n+\n+      std::array<Value, 4> i8v4Elems;\n+      std::array<Value, 4> i32Elems;\n+      i8v4Elems.fill(\n+          rewriter.create<LLVM::UndefOp>(loc, vec_ty(type::i8Ty(ctx), 4)));\n+\n+      Value i8Elems[4][4];\n+      Type elemTy = type::i8Ty(ctx);\n+      Type elemPtrTy = ptr_ty(elemTy);\n+      Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n+      if (kOrder == 1) {\n+        for (int i = 0; i < 2; ++i)\n+          for (int j = 0; j < 4; ++j)\n+            i8Elems[i][j] = load(gep(elemPtrTy, ptrs[i][j], sOffsetElemVal));\n+\n+        for (int i = 2; i < 4; ++i)\n+          for (int j = 0; j < 4; ++j)\n+            i8Elems[i][j] =\n+                load(gep(elemPtrTy, ptrs[i - 2][j], sOffsetArrElemVal));\n+\n+        for (int m = 0; m < 4; ++m) {\n+          for (int e = 0; e < 4; ++e)\n+            i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n+                                          i8Elems[m][e], i32_val(e));\n+          i32Elems[m] = bitcast(i8v4Elems[m], i8x4Ty);\n+        }\n+      } else { // k first\n+        for (int j = 0; j < 4; ++j)\n+          i8Elems[0][j] = load(gep(elemPtrTy, ptrs[0][j], sOffsetElemVal));\n+        for (int j = 0; j < 4; ++j)\n+          i8Elems[2][j] = load(gep(elemPtrTy, ptrs[1][j], sOffsetElemVal));\n+        for (int j = 0; j < 4; ++j)\n+          i8Elems[1][j] = load(gep(elemPtrTy, ptrs[0][j], sOffsetArrElemVal));\n+        for (int j = 0; j < 4; ++j)\n+          i8Elems[3][j] = load(gep(elemPtrTy, ptrs[1][j], sOffsetArrElemVal));\n+\n+        for (int m = 0; m < 4; ++m) {\n+          for (int e = 0; e < 4; ++e)\n+            i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n+                                          i8Elems[m][e], i32_val(e));\n+          i32Elems[m] = bitcast(i8v4Elems[m], i8x4Ty);\n+        }\n+      }\n+\n+      return {i32Elems[0], i32Elems[1], i32Elems[2], i32Elems[3]};\n+    }\n+\n+    assert(false && \"Invalid smem load\");\n+    return {Value{}, Value{}, Value{}, Value{}};\n+  }\n+\n+private:\n+  SmallVector<uint32_t> order;\n+  int kOrder;\n+  SmallVector<int64_t> tileShape;\n+  SmallVector<int> instrShape;\n+  SmallVector<int> matShape;\n+  int perPhase;\n+  int maxPhase;\n+  int elemBytes;\n+  ConversionPatternRewriter &rewriter;\n+  const Location &loc;\n+  MLIRContext *ctx{};\n+\n+  int cMatShape;\n+  int sMatShape;\n+\n+  Value sStride;\n+\n+  bool needTrans;\n+  bool canUseLdmatrix;\n+\n+  int numPtrs;\n+\n+  int pLoadStrideInMat;\n+  int sMatStride;\n+\n+  int matArrStride;\n+  int warpOffStride;\n+};\n+\n+// This class helps to adapt the existing DotOpConversion to the latest\n+// DotOpOperand layout design. It decouples the exising implementation to two\n+// parts:\n+// 1. loading the specific operand matrix(for $a, $b, $c) from smem\n+// 2. passing the loaded value and perform the mma codegen\n+struct MMA16816ConversionHelper {\n+  MmaEncodingAttr mmaLayout;\n+  ArrayRef<unsigned int> wpt;\n+  SmallVector<unsigned int> properWpt;\n+\n+  Value thread, lane, warp;\n+\n+  DotOpMmaV2ConversionHelper helper;\n+  ConversionPatternRewriter &rewriter;\n+  TypeConverter *typeConverter;\n+  Location loc;\n+  MLIRContext *ctx{};\n+\n+  using ValueTable = std::map<std::pair<unsigned, unsigned>, Value>;\n+\n+  // dotOperand: type of either one operand of dotOp.\n+  MMA16816ConversionHelper(Type dotOperand, MmaEncodingAttr mmaLayout,\n+                           Value thread, ConversionPatternRewriter &rewriter,\n+                           TypeConverter *typeConverter, Location loc)\n+      : mmaLayout(mmaLayout), thread(thread), helper(mmaLayout),\n+        rewriter(rewriter), typeConverter(typeConverter), loc(loc),\n+        ctx(mmaLayout.getContext()), wpt(mmaLayout.getWarpsPerCTA()) {\n+    helper.deduceMmaType(dotOperand);\n+\n+    Value _32 = i32_val(32);\n+    lane = urem(thread, _32);\n+    warp = udiv(thread, _32);\n+  }\n+\n+  // Get a warpId for M axis.\n+  Value getWarpM(int M) const {\n+    auto matShape = helper.getMmaMatShape();\n+    return urem(urem(warp, i32_val(wpt[0])), i32_val(M / matShape[0]));\n+  }\n+\n+  // Get a warpId for N axis.\n+  Value getWarpN(int N) const {\n+    auto matShape = helper.getMmaMatShape();\n+    Value warpMN = udiv(warp, i32_val(wpt[0]));\n+    return urem(urem(warpMN, i32_val(wpt[1])), i32_val(N / matShape[1]));\n+  }\n+\n+  // Get the mmaInstrShape deducing either from $a or $b.\n+  std::tuple<int, int, int> getMmaInstrShape(Type operand) const {\n+    helper.deduceMmaType(operand);\n+    auto mmaInstrShape = helper.getMmaInstrShape();\n+    int mmaInstrM = mmaInstrShape[0];\n+    int mmaInstrN = mmaInstrShape[1];\n+    int mmaInstrK = mmaInstrShape[2];\n+    return std::make_tuple(mmaInstrM, mmaInstrN, mmaInstrK);\n+  }\n+\n+  // Get the mmaMatShape deducing either from $a or $b.\n+  std::tuple<int, int, int> getMmaMatShape(Type operand) const {\n+    helper.deduceMmaType(operand);\n+    auto matShape = helper.getMmaMatShape();\n+    int matShapeM = matShape[0];\n+    int matShapeN = matShape[1];\n+    int matShapeK = matShape[2];\n+    return std::make_tuple(matShapeM, matShapeN, matShapeK);\n+  }\n+\n+  // \\param operand is either $a or $b's type.\n+  inline int getNumRepM(Type operand, int M) const {\n+    return getNumRepM(operand, M, wpt[0]);\n+  }\n+\n+  // \\param operand is either $a or $b's type.\n+  inline int getNumRepN(Type operand, int N) const {\n+    return getNumRepN(operand, N, wpt[1]);\n+  }\n+\n+  // \\param operand is either $a or $b's type.\n+  inline int getNumRepK(Type operand, int K) const {\n+    return getNumRepK_(operand, K);\n+  }\n+\n+  static int getNumRepM(Type operand, int M, int wpt) {\n+    auto tensorCoreType =\n+        DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(operand);\n+    int mmaInstrM =\n+        DotOpMmaV2ConversionHelper::getMmaInstrShape(tensorCoreType)[0];\n+    return std::max<int>(M / (wpt * mmaInstrM), 1);\n+  }\n+\n+  static int getNumRepN(Type operand, int N, int wpt) {\n+    auto tensorCoreType =\n+        DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(operand);\n+    int mmaInstrN =\n+        DotOpMmaV2ConversionHelper::getMmaInstrShape(tensorCoreType)[1];\n+    return std::max<int>(N / (wpt * mmaInstrN), 1);\n+  }\n+\n+  static int getNumRepK_(Type operand, int K) {\n+    auto tensorCoreType =\n+        DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(operand);\n+    int mmaInstrK =\n+        DotOpMmaV2ConversionHelper::getMmaInstrShape(tensorCoreType)[2];\n+    return std::max<int>(K / mmaInstrK, 1);\n+  }\n+\n+  // Get number of elements per thread for $a operand.\n+  static size_t getANumElemsPerThread(RankedTensorType operand, int wpt) {\n+    auto shape = operand.getShape();\n+    int repM = getNumRepM(operand, shape[0], wpt);\n+    int repK = getNumRepK_(operand, shape[1]);\n+    return 4 * repM * repK;\n+  }\n+\n+  // Get number of elements per thread for $b operand.\n+  static size_t getBNumElemsPerThread(RankedTensorType operand, int wpt) {\n+    auto shape = operand.getShape();\n+    int repK = getNumRepK_(operand, shape[0]);\n+    int repN = getNumRepN(operand, shape[1], wpt);\n+    return 4 * std::max(repN / 2, 1) * repK;\n+  }\n+\n+  // Loading $a from smem to registers, returns a LLVM::Struct.\n+  Value loadA(Value tensor, const SharedMemoryObject &smemObj) const {\n+    auto aTensorTy = tensor.getType().cast<RankedTensorType>();\n+    auto layout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+\n+    SmallVector<int64_t> shape(aTensorTy.getShape().begin(),\n+                               aTensorTy.getShape().end());\n+    // TODO[Superjomn]: transA cannot be accessed in ConvertLayoutOp.\n+    bool transA = false;\n+    if (transA) {\n+      std::swap(shape[0], shape[1]);\n+    }\n+\n+    ValueTable ha;\n+    std::function<void(int, int)> loadFn;\n+    auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(aTensorTy);\n+    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(aTensorTy);\n+\n+    int numRepM = getNumRepM(aTensorTy, shape[0]);\n+    int numRepK = getNumRepK(aTensorTy, shape[1]);\n+\n+    if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n+      Value warpM = getWarpM(shape[0]);\n+      // load from smem\n+      loadFn = getLoadMatrixFn(\n+          tensor, smemObj, mmaLayout, mmaLayout.getWarpsPerCTA()[0] /*wpt*/,\n+          1 /*kOrder*/, {mmaInstrM, mmaInstrK} /*instrShape*/,\n+          {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, ha /*vals*/,\n+          true /*isA*/);\n+    } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n+      // load from registers, used in gemm fuse\n+      // TODO(Superjomn) Port the logic.\n+      assert(false && \"Loading A from register is not supported yet.\");\n+    } else {\n+      assert(false && \"A's layout is not supported.\");\n+    }\n+\n+    // step1. Perform loading.\n+    for (int m = 0; m < numRepM; ++m)\n+      for (int k = 0; k < numRepK; ++k)\n+        loadFn(2 * m, 2 * k);\n+\n+    // step2. Format the values to LLVM::Struct to passing to mma codegen.\n+    return composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK);\n+  }\n+\n+  // Loading $b from smem to registers, returns a LLVM::Struct.\n+  Value loadB(Value tensor, const SharedMemoryObject &smemObj) {\n+    ValueTable hb;\n+    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+    auto layout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+\n+    SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n+                               tensorTy.getShape().end());\n+\n+    // TODO[Superjomn]: transB cannot be accessed in ConvertLayoutOp.\n+    bool transB = false;\n+    if (transB) {\n+      std::swap(shape[0], shape[1]);\n+    }\n+\n+    auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(tensorTy);\n+    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(tensorTy);\n+    int numRepK = getNumRepK(tensorTy, shape[0]);\n+    int numRepN = getNumRepN(tensorTy, shape[1]);\n+\n+    Value warpN = getWarpN(shape[1]);\n+    auto loadFn = getLoadMatrixFn(\n+        tensor, smemObj, mmaLayout, mmaLayout.getWarpsPerCTA()[1] /*wpt*/,\n+        0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShape*/,\n+        {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/,\n+        false /*isA*/);\n+\n+    for (int n = 0; n < std::max(numRepN / 2, 1); ++n) {\n+      for (int k = 0; k < numRepK; ++k)\n+        loadFn(2 * n, 2 * k);\n+    }\n+\n+    Value result = composeValuesToDotOperandLayoutStruct(\n+        hb, std::max(numRepN / 2, 1), numRepK);\n+    return result;\n+  }\n+\n+  // Loading $c to registers, returns a Value.\n+  Value loadC(Value tensor, Value llTensor) const {\n+    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+    auto [repM, repN] = DotOpMmaV2ConversionHelper::getRepMN(tensorTy);\n+    size_t fcSize = 4 * repM * repN;\n+\n+    assert(tensorTy.getEncoding().isa<MmaEncodingAttr>() &&\n+           \"Currently, we only support $c with a mma layout.\");\n+    // Load a normal C tensor with mma layout, that should be a\n+    // LLVM::struct with fcSize elements.\n+    auto structTy = llTensor.getType().cast<LLVM::LLVMStructType>();\n+    assert(structTy.getBody().size() == fcSize &&\n+           \"DotOp's $c operand should pass the same number of values as $d in \"\n+           \"mma layout.\");\n+    return llTensor;\n+  }\n+\n+  // Conduct the Dot conversion.\n+  // \\param a, \\param b, \\param c and \\param d are DotOp operands.\n+  // \\param loadedA, \\param loadedB, \\param loadedC, all of them are result of\n+  // loading.\n+  LogicalResult convertDot(Value a, Value b, Value c, Value d, Value loadedA,\n+                           Value loadedB, Value loadedC, DotOp op,\n+                           DotOpAdaptor adaptor) const {\n+    helper.deduceMmaType(op);\n+\n+    auto aTensorTy = a.getType().cast<RankedTensorType>();\n+    auto dTensorTy = d.getType().cast<RankedTensorType>();\n+\n+    SmallVector<int64_t> aShape(aTensorTy.getShape().begin(),\n+                                aTensorTy.getShape().end());\n+    if (op.transA())\n+      std::swap(aShape[0], aShape[1]);\n+\n+    auto dShape = dTensorTy.getShape();\n+\n+    // shape / shape_per_cta\n+    int numRepM = getNumRepM(aTensorTy, dShape[0]);\n+    int numRepN = getNumRepN(aTensorTy, dShape[1]);\n+    int numRepK = getNumRepK(aTensorTy, aShape[1]);\n+\n+    ValueTable ha =\n+        getValuesFromDotOperandLayoutStruct(loadedA, numRepM, numRepK);\n+    ValueTable hb = getValuesFromDotOperandLayoutStruct(\n+        loadedB, std::max(numRepN / 2, 1), numRepK);\n+    auto fc = getElementsFromStruct(loc, loadedC, rewriter);\n+\n+    auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n+      unsigned colsPerThread = numRepN * 2;\n+      PTXBuilder builder;\n+      auto &mma = *builder.create(helper.getMmaInstr().str());\n+      auto retArgs = builder.newListOperand(4, \"=r\");\n+      auto aArgs = builder.newListOperand({\n+          {ha[{m, k}], \"r\"},\n+          {ha[{m + 1, k}], \"r\"},\n+          {ha[{m, k + 1}], \"r\"},\n+          {ha[{m + 1, k + 1}], \"r\"},\n+      });\n+      auto bArgs =\n+          builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n+      auto cArgs = builder.newListOperand();\n+      for (int i = 0; i < 4; ++i) {\n+        cArgs->listAppend(builder.newOperand(fc[m * colsPerThread + 4 * n + i],\n+                                             std::to_string(i)));\n+        // reuse the output registers\n+      }\n+\n+      mma(retArgs, aArgs, bArgs, cArgs);\n+      Value mmaOut = builder.launch(rewriter, loc, helper.getMmaRetType());\n+\n+      auto getIntAttr = [&](int v) {\n+        return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n+      };\n+\n+      Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n+      for (int i = 0; i < 4; ++i)\n+        fc[m * colsPerThread + 4 * n + i] =\n+            extract_val(elemTy, mmaOut, getIntAttr(i));\n+    };\n+\n+    for (int k = 0; k < numRepK; ++k)\n+      for (int m = 0; m < numRepM; ++m)\n+        for (int n = 0; n < numRepN; ++n)\n+          callMma(2 * m, n, 2 * k);\n+\n+    Type resElemTy = dTensorTy.getElementType();\n+\n+    for (auto &elem : fc) {\n+      elem = bitcast(elem, resElemTy);\n+    }\n+\n+    // replace with new packed result\n+    Type structTy = LLVM::LLVMStructType::getLiteral(\n+        ctx, SmallVector<Type>(fc.size(), resElemTy));\n+    Value res = getStructFromElements(loc, fc, rewriter, structTy);\n+    rewriter.replaceOp(op, res);\n+\n+    return success();\n+  }\n+\n+private:\n+  std::function<void(int, int)>\n+  getLoadMatrixFn(Value tensor, const SharedMemoryObject &smemObj,\n+                  MmaEncodingAttr mmaLayout, int wpt, uint32_t kOrder,\n+                  ArrayRef<int> instrShape, ArrayRef<int> matShape,\n+                  Value warpId, ValueTable &vals, bool isA) const {\n+    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+    // We assumes that the input operand of Dot should be from shared layout.\n+    // TODO(Superjomn) Consider other layouts if needed later.\n+    auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+    const int perPhase = sharedLayout.getPerPhase();\n+    const int maxPhase = sharedLayout.getMaxPhase();\n+    const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n+    auto order = sharedLayout.getOrder();\n+\n+    // the original register_lds2, but discard the prefetch logic.\n+    auto ld2 = [](ValueTable &vals, int mn, int k, Value val) {\n+      vals[{mn, k}] = val;\n+    };\n+\n+    // (a, b) is the coordinate.\n+    auto load = [=, &vals, &ld2](int a, int b) {\n+      MMA16816SmemLoader loader(\n+          wpt, sharedLayout.getOrder(), kOrder, smemObj.strides,\n+          tensorTy.getShape() /*tileShape*/, instrShape, matShape, perPhase,\n+          maxPhase, elemBytes, rewriter, typeConverter, loc);\n+      Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+      SmallVector<Value> offs =\n+          loader.computeOffsets(warpId, lane, cSwizzleOffset);\n+      const int numPtrs = loader.getNumPtrs();\n+      SmallVector<Value> ptrs(numPtrs);\n+\n+      Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n+      Type smemPtrTy = helper.getShemPtrTy();\n+      for (int i = 0; i < numPtrs; ++i) {\n+        ptrs[i] =\n+            bitcast(gep(smemPtrTy, smemBase, ValueRange({offs[i]})), smemPtrTy);\n+      }\n+\n+      auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n+          (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n+          ptrs, helper.getMatType(), helper.getShemPtrTy());\n+\n+      if (isA) {\n+        ld2(vals, a, b, ha0);\n+        ld2(vals, a + 1, b, ha1);\n+        ld2(vals, a, b + 1, ha2);\n+        ld2(vals, a + 1, b + 1, ha3);\n+      } else {\n+        ld2(vals, a, b, ha0);\n+        ld2(vals, a + 1, b, ha2);\n+        ld2(vals, a, b + 1, ha1);\n+        ld2(vals, a + 1, b + 1, ha3);\n+      }\n+    };\n+\n+    return load;\n+  }\n+\n+  // Compose a map of Values to a LLVM::Struct.\n+  // The layout is a list of Value with coordinate of (i,j), the order is as\n+  // the follows:\n+  // [\n+  //  (0,0), (0,1), (1,0), (1,1), # i=0, j=0\n+  //  (0,2), (0,3), (1,2), (1,3), # i=0, j=1\n+  //  (0,4), (0,5), (1,4), (1,5), # i=0, j=2\n+  //  ...\n+  //  (2,0), (2,1), (3,0), (3,1), # i=1, j=0\n+  //  (2,2), (2,3), (3,2), (3,3), # i=1, j=1\n+  //  (2,4), (2,5), (3,4), (3,5), # i=1, j=2\n+  //  ...\n+  // ]\n+  // i \\in [0, n0) and j \\in [0, n1)\n+  // There should be \\param n0 * \\param n1 elements in the output Struct.\n+  Value composeValuesToDotOperandLayoutStruct(const ValueTable &vals, int n0,\n+                                              int n1) const {\n+    std::vector<Value> elems;\n+    for (int m = 0; m < n0; ++m)\n+      for (int k = 0; k < n1; ++k) {\n+        elems.push_back(vals.at({2 * m, 2 * k}));\n+        elems.push_back(vals.at({2 * m, 2 * k + 1}));\n+        elems.push_back(vals.at({2 * m + 1, 2 * k}));\n+        elems.push_back(vals.at({2 * m + 1, 2 * k + 1}));\n+      }\n+\n+    assert(!elems.empty());\n+\n+    Type elemTy = elems[0].getType();\n+    Type structTy = LLVM::LLVMStructType::getLiteral(\n+        ctx, SmallVector<Type>(elems.size(), elemTy));\n+    auto result = getStructFromElements(loc, elems, rewriter, structTy);\n+    return result;\n+  }\n+\n+  ValueTable getValuesFromDotOperandLayoutStruct(Value value, int n0,\n+                                                 int n1) const {\n+    auto elems = getElementsFromStruct(loc, value, rewriter);\n+\n+    int offset{};\n+    ValueTable vals;\n+    for (int i = 0; i < n0; ++i) {\n+      for (int j = 0; j < n1; j++) {\n+        vals[{2 * i, 2 * j}] = elems[offset++];\n+        vals[{2 * i, 2 * j + 1}] = elems[offset++];\n+        vals[{2 * i + 1, 2 * j}] = elems[offset++];\n+        vals[{2 * i + 1, 2 * j + 1}] = elems[offset++];\n+      }\n+    }\n+    return vals;\n+  }\n+};\n+\n+// Helper for conversion of FMA DotOp.\n+struct DotOpFMAConversionHelper {\n+  Attribute layout;\n+  MLIRContext *ctx{};\n+\n+  using ValueTable = std::map<std::pair<int, int>, Value>;\n+\n+  explicit DotOpFMAConversionHelper(Attribute layout)\n+      : layout(layout), ctx(layout.getContext()) {}\n+\n+  SmallVector<Value> getThreadIds(Value threadId,\n+                                  ArrayRef<unsigned> shapePerCTA,\n+                                  ArrayRef<unsigned> order,\n+                                  ConversionPatternRewriter &rewriter,\n+                                  Location loc) const;\n+\n+  Value loadA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n+              Location loc, ConversionPatternRewriter &rewriter) const;\n+\n+  Value loadB(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n+              Location loc, ConversionPatternRewriter &rewriter) const;\n+\n+  ValueTable getValueTableFromStruct(Value val, int K, int n0, int shapePerCTA,\n+                                     int sizePerThread,\n+                                     ConversionPatternRewriter &rewriter,\n+                                     Location loc) const;\n+\n+  Value getStructFromValueTable(const ValueTable &vals,\n+                                ConversionPatternRewriter &rewriter,\n+                                Location loc) const {\n+    SmallVector<Type> elemTypes(vals.size(), f32_ty);\n+    SmallVector<Value> elems;\n+    elems.reserve(vals.size());\n+    for (auto &item : vals) {\n+      elems.push_back(item.second);\n+    }\n+\n+    Type structTy = struct_ty(elemTypes);\n+    return getStructFromElements(loc, elems, rewriter, structTy);\n+  }\n+  // get number of elements per thread for $a or $b.\n+  static int getNumElemsPerThread(ArrayRef<int64_t> shape,\n+                                  DotOperandEncodingAttr dotOpLayout) {\n+    auto blockedLayout = dotOpLayout.getParent().cast<BlockedEncodingAttr>();\n+    auto shapePerCTA = getShapePerCTA(blockedLayout);\n+    auto sizePerThread = getSizePerThread(blockedLayout);\n+    auto order = blockedLayout.getOrder();\n+\n+    // TODO[Superjomn]: we assume the k aixs is fixed for $a and $b here, fix it\n+    // if not.\n+    int K = dotOpLayout.getOpIdx() == 0 ? shape[1] : shape[0];\n+    int otherDim = dotOpLayout.getOpIdx() == 1 ? shape[1] : shape[0];\n+\n+    bool isM = dotOpLayout.getOpIdx() == 0;\n+    int shapePerCTAMN = getShapePerCTAForMN(blockedLayout, isM);\n+    int sizePerThreadMN = getsizePerThreadForMN(blockedLayout, isM);\n+    return K * std::max<int>(otherDim / shapePerCTAMN, 1) * sizePerThreadMN;\n+  }\n+\n+  // Get shapePerCTA for M or N axis.\n+  static int getShapePerCTAForMN(BlockedEncodingAttr layout, bool isM) {\n+    auto order = layout.getOrder();\n+    auto shapePerCTA = getShapePerCTA(layout);\n+\n+    int mShapePerCTA =\n+        order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+    int nShapePerCTA =\n+        order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+    return isM ? mShapePerCTA : nShapePerCTA;\n+  }\n+\n+  // Get sizePerThread for M or N axis.\n+  static int getsizePerThreadForMN(BlockedEncodingAttr layout, bool isM) {\n+    auto order = layout.getOrder();\n+    auto sizePerThread = getSizePerThread(layout);\n+\n+    int mSizePerThread =\n+        order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+    int nSizePerThread =\n+        order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+    return isM ? mSizePerThread : nSizePerThread;\n+  }\n+};\n+\n+Value DotOpMmaV1ConversionHelper::loadA(\n+    Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n+    ConversionPatternRewriter &rewriter) const {\n+\n+  auto *ctx = rewriter.getContext();\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n+                             tensorTy.getShape().end());\n+  SmallVector<unsigned> order(sharedLayout.getOrder().begin(),\n+                              sharedLayout.getOrder().end());\n+\n+  // TODO [Superjomn]: transA cannot be accessed in ConvertLayoutOp.\n+  bool transA = false;\n+  if (transA) {\n+    std::swap(shape[0], shape[1]);\n+    std::swap(order[0], order[1]);\n+  }\n+\n+  Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+\n+  bool isARow = order[0] != 0;\n+  bool isAVec4 = !isARow && shape[order[0]] <= 16; // fp16*4 = 16bytes\n+  int packSize0 = (isARow || isAVec4) ? 1 : 2;\n+\n+  SmallVector<int> fpw({2, 2, 1});\n+  int repM = 2 * packSize0;\n+  int repK = 1;\n+  int spwM = fpw[0] * 4 * repM;\n+  SmallVector<int> rep({repM, 0, repK}); // pad N with 0\n+  SmallVector<int> spw({spwM, 0, 1});    // pad N with 0\n+\n+  int vecA = sharedLayout.getVec();\n+\n+  auto strides = smemObj.strides;\n+  Value strideAM = isARow ? strides[0] : i32_val(1);\n+  Value strideAK = isARow ? i32_val(1) : strides[1];\n+  Value strideA0 = isARow ? strideAK : strideAM;\n+  Value strideA1 = isARow ? strideAM : strideAK;\n+\n+  int strideRepM = wpt[0] * fpw[0] * 8;\n+  int strideRepK = 1;\n+\n+  auto [offsetAM, offsetAK, _0, _1] =\n+      computeOffsets(thread, isARow, false, fpw, spw, rep, rewriter, loc);\n+\n+  // swizzling\n+  int perPhaseA = sharedLayout.getPerPhase();\n+  int maxPhaseA = sharedLayout.getMaxPhase();\n+  int stepA0 = isARow ? strideRepK : strideRepM;\n+  int numPtrA = std::max(2 * perPhaseA * maxPhaseA / stepA0, 1);\n+  int NK = shape[1];\n+\n+  // pre-compute pointer lanes\n+  Value offA0 = isARow ? offsetAK : offsetAM;\n+  Value offA1 = isARow ? offsetAM : offsetAK;\n+  Value phaseA = urem(udiv(offA1, i32_val(perPhaseA)), i32_val(maxPhaseA));\n+  offA0 = add(offA0, cSwizzleOffset);\n+  SmallVector<Value> offA(numPtrA);\n+  for (int i = 0; i < numPtrA; i++) {\n+    Value offA0I = add(offA0, i32_val(i * (isARow ? 4 : strideRepM)));\n+    offA0I = udiv(offA0I, i32_val(vecA));\n+    offA0I = xor_(offA0I, phaseA);\n+    offA0I = mul(offA0I, i32_val(vecA));\n+    offA[i] = add(mul(offA0I, strideA0), mul(offA1, strideA1));\n+  }\n+\n+  Type f16x2Ty = vec_ty(f16_ty, 2);\n+  // One thread get 8 elements as result\n+  Type retTy =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector(8, type::f32Ty(ctx)));\n+\n+  // prepare arguments\n+  SmallVector<Value> ptrA(numPtrA);\n+\n+  std::map<std::pair<int, int>, std::pair<Value, Value>> has;\n+  auto smem = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n+  for (int i = 0; i < numPtrA; i++)\n+    ptrA[i] = gep(ptr_ty(f16_ty), smem, offA[i]);\n+\n+  auto instrShape = getMmaInstrShape();\n+  unsigned numM = std::max<int>(rep[0] * shape[0] / (spw[0] * wpt[0]), 1);\n+\n+  Type f16PtrTy = ptr_ty(f16_ty);\n+\n+  auto ld = [&](decltype(has) &vals, int m, int k, Value val0, Value val1) {\n+    vals[{m, k}] = {val0, val1};\n+  };\n+  auto loadA = [&](int m, int k) {\n+    int offidx = (isARow ? k / 4 : m) % numPtrA;\n+    Value thePtrA = gep(f16PtrTy, smem, offA[offidx]);\n+\n+    int stepAM = isARow ? m : m / numPtrA * numPtrA;\n+    int stepAK = isARow ? k / (numPtrA * vecA) * (numPtrA * vecA) : k;\n+    Value offset = add(mul(i32_val(stepAM * strideRepM), strideAM),\n+                       mul(i32_val(stepAK), strideAK));\n+    Value pa = gep(f16PtrTy, thePtrA, offset);\n+    Type aPtrTy = ptr_ty(vec_ty(i32_ty, std::max<int>(vecA / 2, 1)), 3);\n+    Value ha = load(bitcast(pa, aPtrTy));\n+    // record lds that needs to be moved\n+    Value ha00 = bitcast(extract_element(ha, i32_val(0)), f16x2Ty);\n+    Value ha01 = bitcast(extract_element(ha, i32_val(1)), f16x2Ty);\n+    ld(has, m, k, ha00, ha01);\n+\n+    if (vecA > 4) {\n+      Value ha10 = bitcast(extract_element(ha, i32_val(2)), f16x2Ty);\n+      Value ha11 = bitcast(extract_element(ha, i32_val(3)), f16x2Ty);\n+      if (isARow)\n+        ld(has, m, k + 4, ha10, ha11);\n+      else\n+        ld(has, m + 1, k, ha10, ha11);\n+    }\n+  };\n+\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned m = 0; m < numM / 2; ++m)\n+      if (!has.count({m, k}))\n+        loadA(m, k);\n+\n+  SmallVector<Value> elems;\n+  elems.reserve(has.size() * 2);\n+  auto vecTy = vec_ty(f16_ty, 2);\n+  for (auto item : has) { // has is a map, the key should be ordered.\n+    elems.push_back(item.second.first);\n+    elems.push_back(item.second.second);\n+  }\n+\n+  Type resTy = struct_ty(SmallVector<Type>(elems.size(), f16x2Ty));\n+  Value res = getStructFromElements(loc, elems, rewriter, resTy);\n+  return res;\n+}\n+\n+Value DotOpMmaV1ConversionHelper::loadB(\n+    Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n+    ConversionPatternRewriter &rewriter) const {\n+  // smem\n+  Value smem = smemObj.base;\n+  auto strides = smemObj.strides;\n+\n+  auto *ctx = rewriter.getContext();\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+\n+  SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n+                             tensorTy.getShape().end());\n+  SmallVector<unsigned> order(sharedLayout.getOrder().begin(),\n+                              sharedLayout.getOrder().end());\n+\n+  // TODO [Superjomn]: transB cannot be accessed in ConvertLayoutOp.\n+  bool transB = false;\n+\n+  if (transB) {\n+    std::swap(order[0], order[1]);\n+    std::swap(shape[0], shape[1]);\n+  }\n+\n+  bool isBRow = order[0] != 0;\n+  bool isBVec4 = isBRow && shape[order[0]] <= 16;\n+  int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n+  SmallVector<int> fpw({2, 2, 1});\n+  SmallVector<int> rep({0, 2 * packSize1, 1});       // pad M with 0\n+  SmallVector<int> spw({0, fpw[1] * 4 * rep[1], 1}); // pad M with 0\n+  int vecB = sharedLayout.getVec();\n+  Value strideBN = isBRow ? i32_val(1) : strides[1];\n+  Value strideBK = isBRow ? strides[0] : i32_val(1);\n+  Value strideB0 = isBRow ? strideBN : strideBK;\n+  Value strideB1 = isBRow ? strideBK : strideBN;\n+  int strideRepN = wpt[1] * fpw[1] * 8;\n+  int strideRepK = 1;\n+\n+  // swizzling\n+  int perPhaseA = sharedLayout.getPerPhase();\n+  int maxPhaseA = sharedLayout.getMaxPhase();\n+  int perPhaseB = sharedLayout.getPerPhase();\n+  int maxPhaseB = sharedLayout.getMaxPhase();\n+  int stepB0 = isBRow ? strideRepN : strideRepK;\n+  int numPtrB = std::max(2 * perPhaseB * maxPhaseB / stepB0, 1);\n+  int NK = shape[0];\n+\n+  auto [_0, _1, offsetBN, offsetBK] =\n+      computeOffsets(thread, false, isBRow, fpw, spw, rep, rewriter, loc);\n+  if (transB)\n+    std::swap(offsetBK, offsetBN);\n+\n+  Value offB0 = isBRow ? offsetBN : offsetBK;\n+  Value offB1 = isBRow ? offsetBK : offsetBN;\n+  Value phaseB = urem(udiv(offB1, i32_val(perPhaseB)), i32_val(maxPhaseB));\n+  Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+  offB0 = add(offB0, cSwizzleOffset);\n+  SmallVector<Value> offB(numPtrB);\n+  for (int i = 0; i < numPtrB; ++i) {\n+    Value offB0I = add(offB0, i32_val(i * (isBRow ? strideRepN : 4)));\n+    offB0I = udiv(offB0I, i32_val(vecB));\n+    offB0I = xor_(offB0I, phaseB);\n+    offB0I = mul(offB0I, i32_val(vecB));\n+    offB[i] = add(mul(offB0I, strideB0), mul(offB1, strideB1));\n+  }\n+\n+  Type f16PtrTy = ptr_ty(f16_ty);\n+  Type f16x2Ty = vec_ty(f16_ty, 2);\n+\n+  SmallVector<Value> ptrB(numPtrB);\n+  ValueTable hbs;\n+  for (int i = 0; i < numPtrB; ++i)\n+    ptrB[i] = gep(ptr_ty(f16_ty), smem, offB[i]);\n+\n+  auto ld = [&](decltype(hbs) &vals, int m, int k, Value val0, Value val1) {\n+    vals[{m, k}] = {val0, val1};\n+  };\n+\n+  auto loadB = [&](int n, int K) {\n+    int offidx = (isBRow ? n : K / 4) % numPtrB;\n+    Value thePtrB = ptrB[offidx];\n+\n+    int stepBN = isBRow ? n / numPtrB * numPtrB : n;\n+    int stepBK = isBRow ? K : K / (numPtrB * vecB) * (numPtrB * vecB);\n+    Value offset = add(mul(i32_val(stepBN * strideRepN), strideBN),\n+                       mul(i32_val(stepBK), strideBK));\n+    Value pb = gep(f16PtrTy, thePtrB, offset);\n+    Value hb =\n+        load(bitcast(pb, ptr_ty(vec_ty(i32_ty, std::max(vecB / 2, 1)), 3)));\n+    // record lds that needs to be moved\n+    Value hb00 = bitcast(extract_element(hb, i32_val(0)), f16x2Ty);\n+    Value hb01 = bitcast(extract_element(hb, i32_val(1)), f16x2Ty);\n+    ld(hbs, n, K, hb00, hb01);\n+    if (vecB > 4) {\n+      Value hb10 = bitcast(extract_element(hb, i32_val(2)), f16x2Ty);\n+      Value hb11 = bitcast(extract_element(hb, i32_val(3)), f16x2Ty);\n+      if (isBRow)\n+        ld(hbs, n + 1, K, hb10, hb11);\n+      else\n+        ld(hbs, n, K + 4, hb10, hb11);\n+    }\n+  };\n+\n+  unsigned numN = rep[1] * shape[1] / (spw[1] * wpt[0]);\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned n = 0; n < numN / 2; ++n) {\n+      if (!hbs.count({n, k}))\n+        loadB(n, k);\n+    }\n+\n+  SmallVector<Value> elems;\n+  for (auto &item : hbs) { // has is a map, the key should be ordered.\n+    elems.push_back(item.second.first);\n+    elems.push_back(item.second.second);\n+  }\n+  Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n+  Type resTy = struct_ty(SmallVector<Type>(elems.size(), fp16x2Ty));\n+  Value res = getStructFromElements(loc, elems, rewriter, resTy);\n+  return res;\n+}\n+\n+std::tuple<Value, Value, Value, Value>\n+DotOpMmaV1ConversionHelper::computeOffsets(Value threadId, bool isARow,\n+                                           bool isBRow, ArrayRef<int> fpw,\n+                                           ArrayRef<int> spw, ArrayRef<int> rep,\n+                                           ConversionPatternRewriter &rewriter,\n+                                           Location loc) const {\n+  auto *ctx = rewriter.getContext();\n+  Value _1 = i32_val(1);\n+  Value _3 = i32_val(3);\n+  Value _4 = i32_val(4);\n+  Value _16 = i32_val(16);\n+  Value _32 = i32_val(32);\n+\n+  Value lane = urem(threadId, _32);\n+  Value warp = udiv(threadId, _32);\n+\n+  // warp offset\n+  Value warp0 = urem(warp, i32_val(wpt[0]));\n+  Value warp12 = udiv(warp, i32_val(wpt[0]));\n+  Value warp1 = urem(warp12, i32_val(wpt[1]));\n+  Value warpMOff = mul(warp0, i32_val(spw[0]));\n+  Value warpNOff = mul(warp1, i32_val(spw[1]));\n+  // Quad offset\n+  Value quadMOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[0]));\n+  Value quadNOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[1]));\n+  // Pair offset\n+  Value pairMOff = udiv(urem(lane, _16), _4);\n+  pairMOff = urem(pairMOff, i32_val(fpw[0]));\n+  pairMOff = mul(pairMOff, _4);\n+  Value pairNOff = udiv(urem(lane, _16), _4);\n+  pairNOff = udiv(pairNOff, i32_val(fpw[0]));\n+  pairNOff = urem(pairNOff, i32_val(fpw[1]));\n+  pairNOff = mul(pairNOff, _4);\n+  // scale\n+  pairMOff = mul(pairMOff, i32_val(rep[0] / 2));\n+  quadMOff = mul(quadMOff, i32_val(rep[0] / 2));\n+  pairNOff = mul(pairNOff, i32_val(rep[1] / 2));\n+  quadNOff = mul(quadNOff, i32_val(rep[1] / 2));\n+  // Quad pair offset\n+  Value laneMOff = add(pairMOff, quadMOff);\n+  Value laneNOff = add(pairNOff, quadNOff);\n+  // A offset\n+  Value offsetAM = add(warpMOff, laneMOff);\n+  Value offsetAK = and_(lane, _3);\n+  // B offset\n+  Value offsetBN = add(warpNOff, laneNOff);\n+  Value offsetBK = and_(lane, _3);\n+  // i indices\n+  Value offsetCM = add(and_(lane, _1), offsetAM);\n+  if (isARow) {\n+    offsetAM = add(offsetAM, urem(threadId, _4));\n+    offsetAK = i32_val(0);\n+  }\n+  if (!isBRow) {\n+    offsetBN = add(offsetBN, urem(threadId, _4));\n+    offsetBK = i32_val(0);\n+  }\n+\n+  return std::make_tuple(offsetAM, offsetAK, offsetBN, offsetBK);\n+}\n+\n+DotOpMmaV1ConversionHelper::ValueTable\n+DotOpMmaV1ConversionHelper::extractLoadedOperand(\n+    Value llStruct, int NK, ConversionPatternRewriter &rewriter) const {\n+  ValueTable rcds;\n+  SmallVector<Value> elems =\n+      getElementsFromStruct(llStruct.getLoc(), llStruct, rewriter);\n+\n+  for (int k = 0, offset = 0, i = 0; k < NK && offset < elems.size();\n+       k += 4, i++, offset += 2) {\n+    rcds[{i, k}] = std::make_pair(elems[offset], elems[offset + 1]);\n+  }\n+\n+  return rcds;\n+}\n+\n+Value DotOpFMAConversionHelper::loadA(\n+    Value A, Value llA, BlockedEncodingAttr dLayout, Value thread, Location loc,\n+    ConversionPatternRewriter &rewriter) const {\n+  auto aTensorTy = A.getType().cast<RankedTensorType>();\n+  auto aLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto aShape = aTensorTy.getShape();\n+\n+  auto aOrder = aLayout.getOrder();\n+  auto order = dLayout.getOrder();\n+\n+  bool isARow = aOrder[0] == 1;\n+\n+  int strideAM = isARow ? aShape[1] : 1;\n+  int strideAK = isARow ? 1 : aShape[0];\n+  int strideA0 = isARow ? strideAK : strideAM;\n+  int strideA1 = isARow ? strideAM : strideAK;\n+  int lda = isARow ? strideAM : strideAK;\n+  int aNumPtr = 8;\n+  int bNumPtr = 8;\n+  int NK = aShape[1];\n+\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+  auto sizePerThread = getSizePerThread(dLayout);\n+\n+  Value _0 = i32_val(0);\n+\n+  Value mContig = i32_val(sizePerThread[order[1]]);\n+  Value nContig = i32_val(sizePerThread[order[0]]);\n+\n+  // threadId in blocked layout\n+  auto threadIds = getThreadIds(thread, shapePerCTA, order, rewriter, loc);\n+\n+  Value threadIdM = threadIds[0];\n+  Value threadIdN = threadIds[1];\n+\n+  Value offA0 = isARow ? _0 : mul(threadIdM, mContig);\n+  Value offA1 = isARow ? mul(threadIdM, mContig) : _0;\n+  SmallVector<Value> aOff(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i) {\n+    aOff[i] = add(mul(offA0, i32_val(strideA0)), mul(offA1, i32_val(strideA1)));\n+  }\n+\n+  auto aSmem = getSharedMemoryObjectFromStruct(loc, llA, rewriter);\n+\n+  Type f32PtrTy = ptr_ty(f32_ty);\n+  SmallVector<Value> aPtrs(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i)\n+    aPtrs[i] = gep(f32PtrTy, aSmem.base, aOff[i]);\n+\n+  ValueTable has;\n+  int M = aShape[aOrder[1]];\n+\n+  int mShapePerCTA = getShapePerCTAForMN(dLayout, true /*isM*/);\n+  int mSizePerThread = getsizePerThreadForMN(dLayout, true /*isM*/);\n+\n+  for (unsigned k = 0; k < NK; ++k) {\n+    for (unsigned m = 0; m < M; m += mShapePerCTA)\n+      for (unsigned mm = 0; mm < mSizePerThread; ++mm)\n+        if (!has.count({m + mm, k})) {\n+          Value pa = gep(f32PtrTy, aPtrs[0],\n+                         i32_val((m + mm) * strideAM + k * strideAK));\n+          Value va = load(pa);\n+          has[{m + mm, k}] = va;\n+        }\n+  }\n+\n+  return getStructFromValueTable(has, rewriter, loc);\n+}\n+\n+Value DotOpFMAConversionHelper::loadB(\n+    Value B, Value llB, BlockedEncodingAttr dLayout, Value thread, Location loc,\n+    ConversionPatternRewriter &rewriter) const {\n+\n+  auto bTensorTy = B.getType().cast<RankedTensorType>();\n+  auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto bShape = bTensorTy.getShape();\n+\n+  auto bOrder = bLayout.getOrder();\n+  auto order = dLayout.getOrder();\n+\n+  bool isBRow = bOrder[0] == 1;\n+\n+  int strideBN = isBRow ? 1 : bShape[0];\n+  int strideBK = isBRow ? bShape[1] : 1;\n+  int strideB0 = isBRow ? strideBN : strideBK;\n+  int strideB1 = isBRow ? strideBK : strideBN;\n+  int ldb = isBRow ? strideBK : strideBN;\n+  int bNumPtr = 8;\n+  int NK = bShape[0];\n+\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+  auto sizePerThread = getSizePerThread(dLayout);\n+\n+  Value _0 = i32_val(0);\n+\n+  Value mContig = i32_val(sizePerThread[order[1]]);\n+  Value nContig = i32_val(sizePerThread[order[0]]);\n+\n+  // threadId in blocked layout\n+  auto threadIds = getThreadIds(thread, shapePerCTA, order, rewriter, loc);\n+  Value threadIdN = threadIds[1];\n+\n+  Value offB0 = isBRow ? mul(threadIdN, nContig) : _0;\n+  Value offB1 = isBRow ? _0 : mul(threadIdN, nContig);\n+  SmallVector<Value> bOff(bNumPtr);\n+  for (int i = 0; i < bNumPtr; ++i) {\n+    bOff[i] = add(mul(offB0, i32_val(strideB0)), mul(offB1, i32_val(strideB1)));\n+  }\n+\n+  auto bSmem = getSharedMemoryObjectFromStruct(loc, llB, rewriter);\n+\n+  Type f32PtrTy = ptr_ty(f32_ty);\n+  SmallVector<Value> bPtrs(bNumPtr);\n+  for (int i = 0; i < bNumPtr; ++i)\n+    bPtrs[i] = gep(f32PtrTy, bSmem.base, bOff[i]);\n+\n+  int N = bShape[bOrder[0]];\n+  ValueTable hbs;\n+\n+  int nShapePerCTA = getShapePerCTAForMN(dLayout, false /*isM*/);\n+  int nSizePerThread = getsizePerThreadForMN(dLayout, false /*isM*/);\n+\n+  for (unsigned k = 0; k < NK; ++k)\n+    for (unsigned n = 0; n < N; n += nShapePerCTA)\n+      for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n+        Value pb = gep(f32PtrTy, bPtrs[0],\n+                       i32_val((n + nn) * strideBN + k * strideBK));\n+        Value vb = load(pb);\n+        hbs[{n + nn, k}] = vb;\n+      }\n+\n+  return getStructFromValueTable(hbs, rewriter, loc);\n+}\n+\n+DotOpFMAConversionHelper::ValueTable\n+DotOpFMAConversionHelper::getValueTableFromStruct(\n+    Value val, int K, int n0, int shapePerCTA, int sizePerThread,\n+    ConversionPatternRewriter &rewriter, Location loc) const {\n+  ValueTable res;\n+  auto elems = getElementsFromStruct(loc, val, rewriter);\n+  int id = 0;\n+  std::set<std::pair<int, int>> keys; // ordered\n+  for (unsigned k = 0; k < K; ++k) {\n+    for (unsigned m = 0; m < n0; m += shapePerCTA)\n+      for (unsigned mm = 0; mm < sizePerThread; ++mm) {\n+        keys.insert({m + mm, k});\n+      }\n+  }\n+\n+  for (auto &key : llvm::enumerate(keys)) {\n+    res[key.value()] = elems[key.index()];\n+  }\n+\n+  return res;\n+}\n+SmallVector<Value> DotOpFMAConversionHelper::getThreadIds(\n+    Value threadId, ArrayRef<unsigned int> shapePerCTA,\n+    ArrayRef<unsigned int> order, ConversionPatternRewriter &rewriter,\n+    Location loc) const {\n+  int dim = order.size();\n+  SmallVector<Value> threadIds(dim);\n+  for (unsigned k = 0; k < dim - 1; k++) {\n+    Value dimK = i32_val(shapePerCTA[order[k]]);\n+    Value rem = urem(threadId, dimK);\n+    threadId = udiv(threadId, dimK);\n+    threadIds[order[k]] = rem;\n+  }\n+  Value dimK = i32_val(shapePerCTA[order[dim - 1]]);\n+  threadIds[order[dim - 1]] = urem(threadId, dimK);\n+  return threadIds;\n+}\n+\n+} // namespace LLVM\n+} // namespace mlir\n+\n+#endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 377, "deletions": 2049, "changes": 2426, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "added", "additions": 280, "deletions": 0, "changes": 280, "file_content_changes": "@@ -0,0 +1,280 @@\n+#ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_UTILITY_H\n+#define TRITON_CONVERSION_TRITONGPU_TO_LLVM_UTILITY_H\n+\n+#include \"mlir/Analysis/SliceAnalysis.h\"\n+#include \"mlir/Conversion/ArithmeticToLLVM/ArithmeticToLLVM.h\"\n+#include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n+#include \"mlir/Conversion/LLVMCommon/LoweringOptions.h\"\n+#include \"mlir/Conversion/LLVMCommon/Pattern.h\"\n+#include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n+#include \"mlir/Conversion/SCFToStandard/SCFToStandard.h\"\n+#include \"mlir/Conversion/StandardToLLVM/ConvertStandardToLLVM.h\"\n+#include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"\n+#include \"mlir/Dialect/GPU/GPUDialect.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/TypeUtilities.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n+#include \"triton/Analysis/Allocation.h\"\n+#include \"triton/Analysis/AxisInfo.h\"\n+#include \"triton/Analysis/Membar.h\"\n+#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Conversion/MLIRTypes.h\"\n+#include \"triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h\"\n+#include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPU.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"llvm/Support/Format.h\"\n+#include \"llvm/Support/FormatVariadic.h\"\n+#include <memory>\n+#include <numeric>\n+\n+// Shortcuts for some commonly used LLVM ops to keep code simple and intuitive\n+#define inttoptr(...) rewriter.create<LLVM::IntToPtrOp>(loc, __VA_ARGS__)\n+#define ptrtoint(...) rewriter.create<LLVM::PtrToIntOp>(loc, __VA_ARGS__)\n+#define zext(...) rewriter.create<LLVM::ZExtOp>(loc, __VA_ARGS__)\n+#define udiv(...) rewriter.create<LLVM::UDivOp>(loc, __VA_ARGS__)\n+#define urem(...) rewriter.create<LLVM::URemOp>(loc, __VA_ARGS__)\n+#define add(...) rewriter.create<LLVM::AddOp>(loc, __VA_ARGS__)\n+#define sub(...) rewriter.create<LLVM::SubOp>(loc, __VA_ARGS__)\n+#define fadd(...) rewriter.create<LLVM::FAddOp>(loc, __VA_ARGS__)\n+#define mul(...) rewriter.create<LLVM::MulOp>(loc, __VA_ARGS__)\n+#define smax(...) rewriter.create<LLVM::SMaxOp>(loc, __VA_ARGS__)\n+#define umax(...) rewriter.create<LLVM::UMaxOp>(loc, __VA_ARGS__)\n+#define fmax(...) rewriter.create<LLVM::MaxNumOp>(loc, __VA_ARGS__)\n+#define smin(...) rewriter.create<LLVM::SMinOp>(loc, __VA_ARGS__)\n+#define umin(...) rewriter.create<LLVM::UMinOp>(loc, __VA_ARGS__)\n+#define fmin(...) rewriter.create<LLVM::MinNumOp>(loc, __VA_ARGS__)\n+#define and_(...) rewriter.create<LLVM::AndOp>(loc, __VA_ARGS__)\n+#define xor_(...) rewriter.create<LLVM::XOrOp>(loc, __VA_ARGS__)\n+#define bitcast(val__, type__)                                                 \\\n+  rewriter.create<LLVM::BitcastOp>(loc, type__, val__)\n+#define gep(...) rewriter.create<LLVM::GEPOp>(loc, __VA_ARGS__)\n+#define ptr_ty(...) LLVM::LLVMPointerType::get(__VA_ARGS__)\n+#define insert_val(...) rewriter.create<LLVM::InsertValueOp>(loc, __VA_ARGS__)\n+#define extract_val(...) rewriter.create<LLVM::ExtractValueOp>(loc, __VA_ARGS__)\n+#define insert_element(...)                                                    \\\n+  rewriter.create<LLVM::InsertElementOp>(loc, __VA_ARGS__)\n+#define extract_element(...)                                                   \\\n+  rewriter.create<LLVM::ExtractElementOp>(loc, __VA_ARGS__)\n+#define load(...) rewriter.create<LLVM::LoadOp>(loc, __VA_ARGS__)\n+#define store(val, ptr) rewriter.create<LLVM::StoreOp>(loc, val, ptr)\n+#define fcmp_ogt(lhs, rhs)                                                     \\\n+  rewriter.create<LLVM::FCmpOp>(loc, rewriter.getI1Type(),                     \\\n+                                LLVM::FCmpPredicate::ogt, lhs, rhs)\n+#define fcmp_olt(lhs, rhs)                                                     \\\n+  rewriter.create<LLVM::FCmpOp>(loc, rewriter.getI1Type(),                     \\\n+                                LLVM::FCmpPredicate::olt, lhs, rhs)\n+#define icmp_eq(...)                                                           \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::eq, __VA_ARGS__)\n+#define icmp_ne(...)                                                           \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::ne, __VA_ARGS__)\n+#define icmp_slt(...)                                                          \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::slt, __VA_ARGS__)\n+#define icmp_sle(...)                                                          \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::sle, __VA_ARGS__)\n+#define icmp_sgt(...)                                                          \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::sgt, __VA_ARGS__)\n+#define icmp_sge(...)                                                          \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::sge, __VA_ARGS__)\n+#define icmp_ult(...)                                                          \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::ult, __VA_ARGS__)\n+#define icmp_ule(...)                                                          \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::ule, __VA_ARGS__)\n+#define icmp_ugt(...)                                                          \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::ugt, __VA_ARGS__)\n+#define icmp_uge(...)                                                          \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::uge, __VA_ARGS__)\n+#define select(...) rewriter.create<LLVM::SelectOp>(loc, __VA_ARGS__)\n+#define address_of(...) rewriter.create<LLVM::AddressOfOp>(loc, __VA_ARGS__)\n+#define barrier() rewriter.create<mlir::gpu::BarrierOp>(loc)\n+#define undef(...) rewriter.create<LLVM::UndefOp>(loc, __VA_ARGS__)\n+#define i32_ty rewriter.getIntegerType(32)\n+#define ui32_ty rewriter.getIntegerType(32, false)\n+#define f16_ty rewriter.getF16Type()\n+#define bf16_ty rewriter.getBF16Type()\n+#define i8_ty rewriter.getIntegerType(8)\n+#define f32_ty rewriter.getF32Type()\n+#define f64_ty rewriter.getF64Type()\n+#define vec_ty(type, num) VectorType::get(num, type)\n+#define f32_val(...) LLVM::createConstantF32(loc, rewriter, __VA_ARGS__)\n+#define f64_val(...) LLVM::createConstantF64(loc, rewriter, __VA_ARGS__)\n+#define void_ty(ctx) LLVM::LLVMVoidType::get(ctx)\n+#define struct_ty(...) LLVM::LLVMStructType::getLiteral(ctx, __VA_ARGS__)\n+\n+// Creator for constant\n+#define i32_val(...) LLVM::createConstantI32(loc, rewriter, __VA_ARGS__)\n+#define int_val(width, val)                                                    \\\n+  LLVM::createLLVMIntegerConstant(rewriter, loc, width, val)\n+#define idx_val(...)                                                           \\\n+  LLVM::createIndexConstant(rewriter, loc, this->getTypeConverter(),           \\\n+                            __VA_ARGS__)\n+\n+namespace mlir {\n+namespace LLVM {\n+\n+static Value getStructFromElements(Location loc, ValueRange resultVals,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type structType) {\n+  if (!structType.isa<LLVM::LLVMStructType>()) {\n+    return *resultVals.begin();\n+  }\n+\n+  Value llvmStruct = rewriter.create<LLVM::UndefOp>(loc, structType);\n+  for (const auto &v : llvm::enumerate(resultVals)) {\n+    assert(v.value() && \"can not insert null values\");\n+    llvmStruct = insert_val(structType, llvmStruct, v.value(),\n+                            rewriter.getI64ArrayAttr(v.index()));\n+  }\n+  return llvmStruct;\n+}\n+\n+static SmallVector<Value>\n+getElementsFromStruct(Location loc, Value llvmStruct,\n+                      ConversionPatternRewriter &rewriter) {\n+  if (llvmStruct.getType().isIntOrIndexOrFloat() ||\n+      llvmStruct.getType().isa<triton::PointerType>() ||\n+      llvmStruct.getType().isa<LLVM::LLVMPointerType>())\n+    return {llvmStruct};\n+  ArrayRef<Type> types =\n+      llvmStruct.getType().cast<LLVM::LLVMStructType>().getBody();\n+  SmallVector<Value> results(types.size());\n+  for (unsigned i = 0; i < types.size(); ++i) {\n+    Type type = types[i];\n+    results[i] = extract_val(type, llvmStruct, rewriter.getI64ArrayAttr(i));\n+  }\n+  return results;\n+}\n+\n+namespace {\n+using namespace mlir::triton;\n+\n+// Create a 32-bit integer constant.\n+Value createConstantI32(Location loc, PatternRewriter &rewriter, int32_t v) {\n+  auto i32ty = rewriter.getIntegerType(32);\n+  return rewriter.create<LLVM::ConstantOp>(loc, i32ty,\n+                                           IntegerAttr::get(i32ty, v));\n+}\n+\n+Value createConstantF32(Location loc, PatternRewriter &rewriter, float v) {\n+  auto type = type::f32Ty(rewriter.getContext());\n+  return rewriter.create<LLVM::ConstantOp>(loc, type,\n+                                           rewriter.getF32FloatAttr(v));\n+}\n+\n+Value createConstantF64(Location loc, PatternRewriter &rewriter, float v) {\n+  auto type = type::f64Ty(rewriter.getContext());\n+  return rewriter.create<LLVM::ConstantOp>(loc, type,\n+                                           rewriter.getF64FloatAttr(v));\n+}\n+\n+// Create an index type constant.\n+Value createIndexConstant(OpBuilder &builder, Location loc,\n+                          TypeConverter *converter, int64_t value) {\n+  Type ty = converter->convertType(builder.getIndexType());\n+  return builder.create<LLVM::ConstantOp>(loc, ty,\n+                                          builder.getIntegerAttr(ty, value));\n+}\n+\n+// Create an integer constant of \\param width bits.\n+Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n+                                int64_t value) {\n+  Type ty = builder.getIntegerType(width);\n+  return builder.create<LLVM::ConstantOp>(loc, ty,\n+                                          builder.getIntegerAttr(ty, value));\n+}\n+\n+} // namespace\n+\n+/// Helper function to get strides from a given shape and its order\n+static SmallVector<Value>\n+getStridesFromShapeAndOrder(ArrayRef<int64_t> shape, ArrayRef<unsigned> order,\n+                            Location loc, ConversionPatternRewriter &rewriter) {\n+  auto rank = shape.size();\n+  SmallVector<Value> strides(rank);\n+  auto stride = 1;\n+  for (auto idx : order) {\n+    strides[idx] = i32_val(stride);\n+    stride *= shape[idx];\n+  }\n+  return strides;\n+}\n+\n+struct SharedMemoryObject {\n+  Value base; // i32 ptr. The start address of the shared memory object.\n+  // We need to store strides as Values but not integers because the\n+  // extract_slice instruction can take a slice at artibary offsets.\n+  // Take $a[16:32, 16:32] as an example, though we know the stride of $a[0] is\n+  // 32, we need to let the instruction that uses $a to be aware of that.\n+  // Otherwise, when we use $a, we only know that the shape of $a is 16x16. If\n+  // we store strides into an attribute array of integers, the information\n+  // cannot pass through block argument assignment because attributes are\n+  // associated with operations but not Values.\n+  // TODO(Keren): We may need to figure out a way to store strides as integers\n+  // if we want to support more optimizations.\n+  SmallVector<Value>\n+      strides; // i32 int. The strides of the shared memory object.\n+  SmallVector<Value> offsets; // i32 int. The offsets of the shared memory\n+  // objects from the originally allocated object.\n+\n+  SharedMemoryObject(Value base, ArrayRef<Value> strides,\n+                     ArrayRef<Value> offsets)\n+      : base(base), strides(strides.begin(), strides.end()),\n+        offsets(offsets.begin(), offsets.end()) {}\n+\n+  SharedMemoryObject(Value base, ArrayRef<int64_t> shape,\n+                     ArrayRef<unsigned> order, Location loc,\n+                     ConversionPatternRewriter &rewriter)\n+      : base(base) {\n+    strides = getStridesFromShapeAndOrder(shape, order, loc, rewriter);\n+\n+    for (auto idx : order) {\n+      offsets.emplace_back(i32_val(0));\n+    }\n+  }\n+\n+  SmallVector<Value> getElems() const {\n+    SmallVector<Value> elems;\n+    elems.push_back(base);\n+    elems.append(strides.begin(), strides.end());\n+    elems.append(offsets.begin(), offsets.end());\n+    return elems;\n+  }\n+\n+  SmallVector<Type> getTypes() const {\n+    SmallVector<Type> types;\n+    types.push_back(base.getType());\n+    types.append(strides.size(), IntegerType::get(base.getContext(), 32));\n+    types.append(offsets.size(), IntegerType::get(base.getContext(), 32));\n+    return types;\n+  }\n+\n+  Value getCSwizzleOffset(int order) const {\n+    assert(order >= 0 && order < strides.size());\n+    return offsets[order];\n+  }\n+\n+  Value getBaseBeforeSwizzle(int order, Location loc,\n+                             ConversionPatternRewriter &rewriter) const {\n+    Value cSwizzleOffset = getCSwizzleOffset(order);\n+    Value offset = sub(i32_val(0), cSwizzleOffset);\n+    Type type = base.getType();\n+    return gep(type, base, offset);\n+  }\n+};\n+\n+static SharedMemoryObject\n+getSharedMemoryObjectFromStruct(Location loc, Value llvmStruct,\n+                                ConversionPatternRewriter &rewriter) {\n+  auto elems = getElementsFromStruct(loc, llvmStruct, rewriter);\n+  auto rank = (elems.size() - 1) / 2;\n+  return {/*base=*/elems[0],\n+          /*strides=*/{elems.begin() + 1, elems.begin() + 1 + rank},\n+          /*offsets=*/{elems.begin() + 1 + rank, elems.end()}};\n+}\n+\n+} // namespace LLVM\n+} // namespace mlir\n+\n+#endif"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -278,6 +278,20 @@ struct TritonStorePattern : public OpConversionPattern<triton::StoreOp> {\n   }\n };\n \n+struct TritonAtomicCASPattern\n+    : public OpConversionPattern<triton::AtomicCASOp> {\n+  using OpConversionPattern<triton::AtomicCASOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::AtomicCASOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    rewriter.replaceOpWithNewOp<triton::AtomicCASOp>(\n+        op, typeConverter->convertType(op.getType()), \n+        adaptor.ptr(), adaptor.cmp(), adaptor.val());\n+    return success();\n+  }\n+};\n+\n struct TritonAtomicRMWPattern\n     : public OpConversionPattern<triton::AtomicRMWOp> {\n   using OpConversionPattern<triton::AtomicRMWOp>::OpConversionPattern;"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 15, "deletions": 2, "changes": 17, "file_content_changes": "@@ -240,12 +240,16 @@ mlir::LogicalResult mlir::triton::ReduceOp::inferReturnTypes(\n   Value arg = operands[0];\n   auto argTy = arg.getType().cast<RankedTensorType>();\n   auto argEltTy = argTy.getElementType();\n+  auto i32Ty = IntegerType::get(argEltTy.getContext(), 32);\n+  auto redOp = attributes.get(\"redOp\").cast<mlir::triton::RedOpAttr>().getValue();\n+  bool withIndex = mlir::triton::ReduceOp::withIndex(redOp);\n+  auto retEltTy = withIndex ? i32Ty : argEltTy;\n   auto retShape = argTy.getShape().vec();\n   int axis = attributes.get(\"axis\").cast<IntegerAttr>().getInt();\n   retShape.erase(retShape.begin() + axis);\n   if (retShape.empty()) {\n     // 0d-tensor -> scalar\n-    inferredReturnTypes.push_back(argEltTy);\n+    inferredReturnTypes.push_back(retEltTy);\n   } else {\n     // nd-tensor where n >= 1\n     // infer encoding\n@@ -264,11 +268,20 @@ mlir::LogicalResult mlir::triton::ReduceOp::inferReturnTypes(\n     }\n     // create type\n     inferredReturnTypes.push_back(\n-        RankedTensorType::get(retShape, argEltTy, retEncoding));\n+        RankedTensorType::get(retShape, retEltTy, retEncoding));\n   }\n   return mlir::success();\n }\n \n+bool mlir::triton::ReduceOp::withIndex(mlir::triton::RedOp redOp) {\n+  return redOp == mlir::triton::RedOp::ARGMIN ||\n+         redOp == mlir::triton::RedOp::ARGMAX ||\n+         redOp == mlir::triton::RedOp::ARGUMIN ||\n+         redOp == mlir::triton::RedOp::ARGUMAX ||\n+         redOp == mlir::triton::RedOp::ARGFMIN ||\n+         redOp == mlir::triton::RedOp::ARGFMAX;\n+}\n+\n //-- SplatOp --\n OpFoldResult SplatOp::fold(ArrayRef<Attribute> operands) {\n   auto constOperand = src().getDefiningOp<arith::ConstantOp>();"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 23, "deletions": 7, "changes": 30, "file_content_changes": "@@ -78,9 +78,9 @@ SmallVector<unsigned> getThreadsPerWarp(Attribute layout) {\n   }\n   if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     if (mmaLayout.getVersion() == 1)\n-      return SmallVector<unsigned>{4, 8};\n+      return {4, 8};\n     if (mmaLayout.getVersion() == 2)\n-      return SmallVector<unsigned>{8, 4};\n+      return {8, 4};\n   }\n   assert(0 && \"getThreadsPerWarp not implemented\");\n   return {};\n@@ -106,9 +106,13 @@ SmallVector<unsigned> getSizePerThread(Attribute layout) {\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     return getSizePerThread(sliceLayout.getParent());\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    assert(mmaLayout.getVersion() == 2 &&\n-           \"mmaLayout version = 1 is not implemented yet\");\n-    return SmallVector<unsigned>{2, 2};\n+    if (mmaLayout.getVersion() == 2) {\n+      return {2, 2};\n+    } else if (mmaLayout.getVersion() == 1) {\n+      return {2, 4};\n+    } else {\n+      llvm_unreachable(\"Unexpected mma version\");\n+    }\n   } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n     auto parentLayout = dotLayout.getParent();\n     assert(parentLayout && \"DotOperandEncodingAttr must have a parent\");\n@@ -194,6 +198,16 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n       assert(0 && \"DotOperandEncodingAttr non-MmaEncodingAttr parent not \"\n                   \"supported yet\");\n     }\n+  } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    if (mmaLayout.getVersion() == 2) {\n+      return {16 * mmaLayout.getWarpsPerCTA()[0],\n+              8 * mmaLayout.getWarpsPerCTA()[1]};\n+    } else if (mmaLayout.getVersion() == 1) {\n+      return {16 * mmaLayout.getWarpsPerCTA()[0],\n+              16 * mmaLayout.getWarpsPerCTA()[1]};\n+    } else {\n+      llvm_unreachable(\"Unexpected mma version\");\n+    }\n   } else {\n     assert(0 && \"Unimplemented usage of getShapePerCTA\");\n   }\n@@ -205,9 +219,9 @@ SmallVector<unsigned> getOrder(const Attribute &layout) {\n     return SmallVector<unsigned>(blockedLayout.getOrder().begin(),\n                                  blockedLayout.getOrder().end());\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    return SmallVector<unsigned>{1, 0};\n+    return {1, 0};\n   } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n-    return SmallVector<unsigned>{1, 0};\n+    return {1, 0};\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     SmallVector<unsigned> parentOrder = getOrder(sliceLayout.getParent());\n     unsigned dim = sliceLayout.getDim();\n@@ -358,6 +372,8 @@ unsigned MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n     unsigned elemsCol = ceil<unsigned>(shape[0], 16 * getWarpsPerCTA()[0]) * 2;\n     unsigned elemsRow = ceil<unsigned>(shape[1], 8 * getWarpsPerCTA()[1]) * 2;\n     res = elemsCol * elemsRow;\n+  } else {\n+    llvm_unreachable(\"Unexpected mma version\");\n   }\n \n   return res;"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 17, "deletions": 7, "changes": 24, "file_content_changes": "@@ -105,7 +105,7 @@ void init_triton_ir(py::module &&m) {\n       .value(\"AND\", mlir::triton::RMWOp::AND)\n       .value(\"OR\", mlir::triton::RMWOp::OR)\n       .value(\"XOR\", mlir::triton::RMWOp::XOR)\n-      // .value(\"XCHG\", mlir::triton::RMWOp::Xchg)\n+      .value(\"XCHG\", mlir::triton::RMWOp::XCHG)\n       .value(\"MAX\", mlir::triton::RMWOp::MAX)\n       .value(\"MIN\", mlir::triton::RMWOp::MIN)\n       .value(\"UMIN\", mlir::triton::RMWOp::UMIN)\n@@ -1095,9 +1095,18 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::OpBuilder &self, mlir::Value &ptr, mlir::Value &cmp,\n               mlir::Value &val) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n-             auto ptrType = mlir::getElementTypeOrSelf(ptr)\n-                                .cast<mlir::triton::PointerType>();\n-             mlir::Type dstType = ptrType.getPointeeType();\n+             mlir::Type dstType;\n+             if (auto srcTensorType = ptr.getType().dyn_cast<mlir::RankedTensorType>()) {\n+               mlir::Type dstElemType = srcTensorType.getElementType()\n+                                            .cast<mlir::triton::PointerType>()\n+                                            .getPointeeType();\n+               dstType = mlir::RankedTensorType::get(srcTensorType.getShape(),\n+                                                     dstElemType);\n+             } else {\n+               auto ptrType = mlir::getElementTypeOrSelf(ptr)\n+                                  .cast<mlir::triton::PointerType>();\n+               dstType = ptrType.getPointeeType();\n+             }\n              return self.create<mlir::triton::AtomicCASOp>(loc, dstType, ptr,\n                                                            cmp, val);\n            })\n@@ -1186,10 +1195,11 @@ void init_triton_ir(py::module &&m) {\n                  operand.getType().dyn_cast<mlir::RankedTensorType>();\n              std::vector<int64_t> shape = inputTensorType.getShape();\n              shape.erase(shape.begin() + axis);\n-             mlir::Type resType = inputTensorType.getElementType();\n+             bool withIndex = mlir::triton::ReduceOp::withIndex(redOp);\n+             mlir::Type resType = withIndex ? self.getI32Type()\n+                                            : inputTensorType.getElementType();\n              if (!shape.empty()) {\n-               resType = mlir::RankedTensorType::get(\n-                   shape, inputTensorType.getElementType());\n+               resType = mlir::RankedTensorType::get(shape, resType);\n              }\n              return self.create<mlir::triton::ReduceOp>(loc, resType, redOp,\n                                                         operand, axis);"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 23, "deletions": 7, "changes": 30, "file_content_changes": "@@ -648,10 +648,11 @@ def kernel(X, Z):\n     else:\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n \n-#TODO[dongdongl]:add more cases with size of tensor less than warp size\n-@pytest.mark.parametrize(\"axis\", [0, 1])\n-def test_tensor_atomic_rmw(axis, device=\"cuda\"):\n-    shape0, shape1 = 8, 8\n+\n+@pytest.mark.parametrize(\"shape, axis\",\n+                         [(shape, axis) for shape in [(2, 2), (2, 8), (8, 2), (8, 8), (32, 32)] for axis in [0, 1]])\n+def test_tensor_atomic_rmw(shape, axis, device=\"cuda\"):\n+    shape0, shape1 = shape\n     # triton kernel\n \n     @triton.jit\n@@ -660,14 +661,19 @@ def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr)\n         off1 = tl.arange(0, SHAPE1)\n         x = tl.load(X + off0[:, None] * SHAPE1 + off1[None, :])\n         z = tl.sum(x, axis=AXIS)\n-        tl.atomic_add(Z + off0, z)\n+        if AXIS == 1:\n+            tl.atomic_add(Z + off0, z)\n+        else:\n+            tl.atomic_add(Z + off1, z)\n     rs = RandomState(17)\n     x = numpy_random((shape0, shape1), dtype_str=\"float32\", rs=rs)\n+    print(x)\n     # reference result\n-    z_ref = np.sum(x, axis=axis)\n+    z_ref = np.sum(x, axis=axis, keepdims=False)\n     # triton result\n     x_tri = to_triton(x, device=device)\n-    z_tri = to_triton(np.zeros((shape0,), dtype=\"float32\"), device=device)\n+    z_shape = (shape0, ) if axis == 1 else (shape1, )\n+    z_tri = to_triton(np.zeros(z_shape, dtype=\"float32\"), device=device)\n     kernel[(1,)](z_tri, x_tri, axis, shape0, shape1)\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n \n@@ -700,6 +706,16 @@ def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr)\n #     serialized_add[(64,)](data, Lock)\n #     triton.testing.assert_almost_equal(data, ref)\n \n+def test_simple_atomic_cas():\n+    # 1. make sure that atomic_cas changes the original value (Lock)\n+    @triton.jit\n+    def change_value(Lock):\n+        tl.atomic_cas(Lock, 0, 1)\n+\n+    Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n+    change_value[(1,)](Lock)\n+\n+    assert (Lock[0] == 1)\n \n # # ---------------\n # # test cast"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -187,7 +187,8 @@ def get_proper_err(a, b, golden):\n     [128, 256, 128, 4, 128, 256, 32, False, False],\n     [256, 128, 64, 4, 256, 128, 16, False, False],\n     [128, 64, 128, 4, 128, 64, 32, False, False],\n-    # [16, 16, 64, 4, 16, 16, 16, False, False],  # TODO failed due to pipeline pass\n+    [16, 16, 64, 4, 16, 16, 16, False, False],\n+    [32, 32, 64, 4, 32, 32, 32, False, False],\n     # trans\n     [128, 64, 128, 4, 128, 64, 32, True, False],\n     [128, 64, 128, 4, 128, 64, 32, False, True],"}, {"filename": "python/tests/test_reduce.py", "status": "modified", "additions": 35, "deletions": 7, "changes": 42, "file_content_changes": "@@ -1,4 +1,5 @@\n import pytest\n+import numpy as np\n import torch\n from torch.testing import assert_close\n \n@@ -13,7 +14,9 @@\n dtype_mapping = {dtype_str: torch.__dict__[dtype_str] for dtype_str in dtypes}\n \n \n-def get_reduced_dtype(dtype):\n+def get_reduced_dtype(op, dtype):\n+    if op in ['argmin', 'argmax']:\n+        return torch.int32\n     if dtype in [torch.int8, torch.int16, torch.uint8]:\n         return torch.int32\n     if dtype in [torch.bfloat16]:\n@@ -48,16 +51,19 @@ def reduce2d_kernel(x_ptr, z_ptr, axis: tl.constexpr, block_m: tl.constexpr, blo\n \n reduce1d_configs = [\n     (op, dtype, shape)\n-    for op in ['sum', 'min', 'max']\n+    for op in ['sum', 'min', 'max', 'argmin', 'argmax', 'xor_sum']\n     for dtype in dtypes\n     for shape in [4, 8, 16, 32, 64, 128, 512, 1024]\n ]\n \n \n @pytest.mark.parametrize('op, dtype, shape', reduce1d_configs)\n def test_reduce1d(op, dtype, shape):\n+    if op == 'xor_sum' and dtype in float_dtypes:\n+        return\n+\n     dtype = dtype_mapping[dtype]\n-    reduced_dtype = get_reduced_dtype(dtype)\n+    reduced_dtype = get_reduced_dtype(op, dtype)\n \n     if dtype.is_floating_point:\n         x = torch.randn((shape,), device='cuda', dtype=dtype)\n@@ -79,8 +85,17 @@ def test_reduce1d(op, dtype, shape):\n         golden_z = torch.sum(x, dtype=reduced_dtype)\n     elif op == 'min':\n         golden_z = torch.min(x).to(reduced_dtype)\n-    else:\n+    elif op == 'max':\n         golden_z = torch.max(x).to(reduced_dtype)\n+    elif op == 'argmin':\n+        golden_z = torch.argmin(x).to(reduced_dtype)\n+    elif op == 'argmax':\n+        golden_z = torch.argmax(x).to(reduced_dtype)\n+    elif op == 'xor_sum':\n+        sum_npy = np.bitwise_xor.reduce(x.cpu().numpy())\n+        golden_z = torch.tensor(sum_npy, dtype=reduced_dtype).cuda()\n+    else:\n+        raise RuntimeError(f'Unknwon reduce op {op}')\n \n     if dtype.is_floating_point and op == 'sum':\n         if shape >= 256:\n@@ -95,7 +110,7 @@ def test_reduce1d(op, dtype, shape):\n \n reduce2d_configs = [\n     (op, dtype, shape, axis)\n-    for op in ['sum', 'min', 'max']\n+    for op in ['sum', 'min', 'max', 'argmin', 'argmax', 'xor_sum']\n     for dtype in dtypes\n     for shape in [(1, 4), (1, 8), (1, 16), (1, 32), (2, 32), (4, 32), (4, 128), (32, 64)]\n     for axis in [0, 1]\n@@ -104,8 +119,11 @@ def test_reduce1d(op, dtype, shape):\n \n @pytest.mark.parametrize('op, dtype, shape, axis', reduce2d_configs)\n def test_reduce2d(op, dtype, shape, axis):\n+    if op == 'xor_sum' and dtype in float_dtypes:\n+        return\n+\n     dtype = dtype_mapping[dtype]\n-    reduced_dtype = get_reduced_dtype(dtype)\n+    reduced_dtype = get_reduced_dtype(op, dtype)\n     reduced_shape = (shape[1 - axis],)\n \n     if dtype.is_floating_point:\n@@ -123,8 +141,18 @@ def test_reduce2d(op, dtype, shape, axis):\n         golden_z = torch.sum(x, dim=axis, keepdim=False, dtype=reduced_dtype)\n     elif op == 'min':\n         golden_z = torch.min(x, dim=axis, keepdim=False)[0].to(reduced_dtype)\n-    else:\n+    elif op == 'max':\n         golden_z = torch.max(x, dim=axis, keepdim=False)[0].to(reduced_dtype)\n+    elif op == 'argmin':\n+        golden_z = torch.argmin(x, dim=axis, keepdim=False).to(reduced_dtype)\n+    elif op == 'argmax':\n+        golden_z = torch.argmax(x, dim=axis, keepdim=False).to(reduced_dtype)\n+    elif op == 'xor_sum':\n+        sum_npy = np.bitwise_xor.reduce(x.cpu().numpy(), axis=axis, keepdims=False)\n+        golden_z = torch.tensor(sum_npy, dtype=reduced_dtype).cuda()\n+    else:\n+        raise RuntimeError(f'Unknwon reduce op {op}')\n+\n     if dtype.is_floating_point and op == 'sum':\n         if shape[axis] >= 256:\n             assert_close(z, golden_z, rtol=0.05, atol=0.1)"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -1041,13 +1041,27 @@ def max(input, axis, _builder=None):\n     return semantic.max(input, axis, _builder)\n \n \n+@builtin\n+@_add_reduction_docstr(\"maximum index\")\n+def argmax(input, axis, _builder=None):\n+    axis = _constexpr_to_value(axis)\n+    return semantic.argmax(input, axis, _builder)\n+\n+\n @builtin\n @_add_reduction_docstr(\"minimum\")\n def min(input, axis, _builder=None):\n     axis = _constexpr_to_value(axis)\n     return semantic.min(input, axis, _builder)\n \n \n+@builtin\n+@_add_reduction_docstr(\"minimum index\")\n+def argmin(input, axis, _builder=None):\n+    axis = _constexpr_to_value(axis)\n+    return semantic.argmin(input, axis, _builder)\n+\n+\n @builtin\n @_add_reduction_docstr(\"sum\")\n def sum(input, axis, _builder=None):"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -1061,10 +1061,18 @@ def min(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n     return reduce_impl(input, axis, builder, \"min\", ir.REDUCE_OP.FMIN, ir.REDUCE_OP.MIN)\n \n \n+def argmin(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n+    return reduce_impl(input, axis, builder, \"argmin\", ir.REDUCE_OP.ARGFMIN, ir.REDUCE_OP.ARGMIN)\n+\n+\n def max(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n     return reduce_impl(input, axis, builder, \"max\", ir.REDUCE_OP.FMAX, ir.REDUCE_OP.MAX)\n \n \n+def argmax(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n+    return reduce_impl(input, axis, builder, \"argmax\", ir.REDUCE_OP.ARGFMAX, ir.REDUCE_OP.ARGMAX)\n+\n+\n def sum(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n     return reduce_impl(input, axis, builder, \"sum\", ir.REDUCE_OP.FADD, ir.REDUCE_OP.ADD)\n "}, {"filename": "test/Conversion/triton_to_tritongpu.mlir", "status": "modified", "additions": 27, "deletions": 1, "changes": 28, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -convert-triton-to-tritongpu=num-warps=2 | FileCheck %s\n+// RUN: triton-opt %s -split-input-file -convert-triton-to-tritongpu=num-warps=2 | FileCheck %s\n \n func @ops() {\n   // CHECK: module attributes {\"triton_gpu.num-warps\" = 2 : i32} {{.*}}\n@@ -9,6 +9,8 @@ func @ops() {\n   return\n }\n \n+// -----\n+\n func @load_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n   // Test if LoadOp is lowered properly (see #771)\n   %ptrs = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n@@ -25,3 +27,27 @@ func @load_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n   tt.store %ptrs, %c : tensor<128xf32>\n   return\n }\n+\n+// -----\n+\n+func @reduce_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n+  // Test if the total number of threadsPerWarp is 32\n+  // Test if the total number of warps is 2\n+  // CHECK: #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 8], warpsPerCTA = [1, 2], order = [0, 1]}>\n+  // CHECK: #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 2], order = [0, 1]}>\n+  // CHECK: #blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [16, 2], warpsPerCTA = [1, 2], order = [0, 1]}>\n+  // CHECK: module attributes {\"triton_gpu.num-warps\" = 2 : i32} {{.*}}\n+  %c0 = arith.constant dense<1.00e+00> : tensor<4x4xf32>\n+  %c1 = arith.constant dense<2.00e+00> : tensor<8x2xf32>\n+  %c2 = arith.constant dense<3.00e+00> : tensor<16x16xf32>\n+  // CHECK: tensor<4x4xf32, #blocked0> -> tensor<4xf32, #triton_gpu.slice<{dim = 0, parent = #blocked0}>>\n+  %c0_ = tt.reduce %c0 {redOp = 1 : i32, axis = 0 : i32} : tensor<4x4xf32> -> tensor<4xf32>\n+  // CHECK: tensor<8x2xf32, #blocked1> -> tensor<2xf32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>\n+  %c1_ = tt.reduce %c1 {redOp = 1 : i32, axis = 0 : i32} : tensor<8x2xf32> -> tensor<2xf32>\n+  // CHECK: tensor<8x2xf32, #blocked1> -> tensor<8xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+  %c2_ = tt.reduce %c1 {redOp = 1 : i32, axis = 1 : i32} : tensor<8x2xf32> -> tensor<8xf32>\n+  // CHECK: tensor<16x16xf32, #blocked2> -> tensor<16xf32, #triton_gpu.slice<{dim = 0, parent = #blocked2}>>\n+  %c3_ = tt.reduce %c2 {redOp = 1 : i32, axis = 0 : i32} : tensor<16x16xf32> -> tensor<16xf32>\n+\n+  return\n+}"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 22, "deletions": 2, "changes": 24, "file_content_changes": "@@ -712,8 +712,8 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n-  // CHECK-LABEL: convert_layout_mma_block\n-  func @convert_layout_mma_blocked(%arg0: tensor<32x16xf32, #mma>) {\n+  // CHECK-LABEL: convert_layout_mmav2_block\n+  func @convert_layout_mmav2_blocked(%arg0: tensor<32x16xf32, #mma>) {\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n     // CHECK: llvm.store\n@@ -728,6 +728,26 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n \n // -----\n \n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#mma = #triton_gpu.mma<{version = 1, warpsPerCTA = [2, 1]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n+  // CHECK-LABEL: convert_layout_mmav1_block\n+  func @convert_layout_mmav1_blocked(%arg0: tensor<32x16xf32, #mma>) {\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<32x16xf32, #mma>) -> tensor<32x16xf32, #blocked0>\n+    return\n+  }\n+}\n+\n+// -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>\n #shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {"}]