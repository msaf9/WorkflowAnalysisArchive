[{"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 8, "deletions": 1, "changes": 9, "file_content_changes": "@@ -72,7 +72,7 @@ class ScanLoweringHelper {\n   // Return true if the lowering of the scan op is supported.\n   bool isSupported();\n   // Return the number of elements per thread along axis dim.\n-  unsigned getAxisNumElementsPerThreads();\n+  unsigned getAxisNumElementsPerThread();\n   // Return the number of elements per thread along non-axis dims.\n   unsigned getNonAxisNumElementsPerThread();\n   // Return the number of threads per warp along non-axis dims.\n@@ -90,6 +90,13 @@ class ScanLoweringHelper {\n   // Return the size of the scratch space needed for scan lowering.\n   unsigned getScratchSizeInBytes();\n \n+  // Stride between contiguous element along axis dim.\n+  unsigned getAxisElementStride();\n+  // Stride between contiguous threads along axis dim.\n+  unsigned getAxisThreadStride();\n+  // Stride between contiguous blocks along axis dim.\n+  unsigned getAxisBlockStride();\n+\n   Location getLoc() { return scanOp.getLoc(); }\n   unsigned getAxis() { return scanOp.getAxis(); }\n   triton::gpu::BlockedEncodingAttr getEncoding();"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 45, "deletions": 9, "changes": 54, "file_content_changes": "@@ -117,7 +117,7 @@ bool ReduceOpHelper::isSupportedLayout() {\n   return false;\n }\n \n-unsigned ScanLoweringHelper::getAxisNumElementsPerThreads() {\n+unsigned ScanLoweringHelper::getAxisNumElementsPerThread() {\n   return getEncoding().getSizePerThread()[getAxis()];\n }\n \n@@ -181,11 +181,9 @@ unsigned ScanLoweringHelper::getNonAxisNumBlocks() {\n \n bool ScanLoweringHelper::isSupported() {\n   // TODO: Support the following cases:\n-  // 1. Scan on the non-fast changing dimension\n-  // 2. Scan on non-blocking encodings\n-  // 3. Scan with multiple operands\n-  if (getAxis() != triton::gpu::getOrder(srcEncoding)[0] ||\n-      !isa<triton::gpu::BlockedEncodingAttr>(srcEncoding))\n+  // 1. Scan on non-blocking encodings\n+  // 2. Scan with multiple operands\n+  if (!isa<triton::gpu::BlockedEncodingAttr>(srcEncoding))\n     return false;\n   if (scanOp.getNumOperands() != 1)\n     return false;\n@@ -194,16 +192,54 @@ bool ScanLoweringHelper::isSupported() {\n \n unsigned ScanLoweringHelper::getScratchSizeInBytes() {\n   auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n-  unsigned numElement =\n+  unsigned tensorSizeInBytes =\n       type.getNumElements() * type.getElementTypeBitWidth() / 8;\n-  return numElement /\n-         (getAxisNumElementsPerThreads() * getAxisNumThreadsPerWarp());\n+  return tensorSizeInBytes /\n+         (getAxisNumElementsPerThread() * getAxisNumThreadsPerWarp());\n }\n \n triton::gpu::BlockedEncodingAttr ScanLoweringHelper::getEncoding() {\n   return srcEncoding.cast<triton::gpu::BlockedEncodingAttr>();\n }\n \n+unsigned ScanLoweringHelper::getAxisElementStride() {\n+  auto order = triton::gpu::getOrder(srcEncoding);\n+  unsigned stride = 1;\n+  for (unsigned dim : order) {\n+    if (dim == getAxis())\n+      return stride;\n+    stride *= getEncoding().getSizePerThread()[dim];\n+  }\n+  llvm_unreachable(\"Axis not found in order\");\n+}\n+\n+unsigned ScanLoweringHelper::getAxisThreadStride() {\n+  auto order = triton::gpu::getOrder(srcEncoding);\n+  unsigned stride = 1;\n+  for (unsigned dim : order) {\n+    if (dim == getAxis())\n+      return stride;\n+    stride *= getEncoding().getThreadsPerWarp()[dim];\n+  }\n+  llvm_unreachable(\"Axis not found in order\");\n+}\n+\n+unsigned ScanLoweringHelper::getAxisBlockStride() {\n+  auto order = triton::gpu::getOrder(srcEncoding);\n+  unsigned stride = 1;\n+  auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n+  auto sizePerThreads = triton::gpu::getSizePerThread(srcEncoding);\n+  auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcEncoding);\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n+  for (unsigned dim : order) {\n+    if (dim == getAxis())\n+      return stride;\n+    stride *= type.getShape()[dim] /\n+              (sizePerThreads[dim] * threadsPerWarp[dim] * warpsPerCTA[dim]);\n+  }\n+  llvm_unreachable(\"Axis not found in order\");\n+}\n+\n bool maybeSharedAllocationOp(Operation *op) {\n   // TODO(Keren): This function can be replaced by adding\n   // MemoryEffectOpInterface. We can then use the MemoryEffectOpInterface to"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ScanOpToLLVM.cpp", "status": "modified", "additions": 98, "deletions": 81, "changes": 179, "file_content_changes": "@@ -37,19 +37,21 @@ static void accumulate(ConversionPatternRewriter &rewriter, Region &combineOp,\n static void scanThreadContiguousElements(SmallVector<Value> &srcValues,\n                                          ConversionPatternRewriter &rewriter,\n                                          ScanLoweringHelper &helper) {\n-  // TODO: this assumes that axis is the fastest moving dimension. We should\n-  // relax that.\n-  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThreads();\n-  // Loop through the blocks of contiguous elements.\n-  for (unsigned j = 0; j < srcValues.size(); j += scanElementsPerThreads) {\n-    // Reset the accumulator at the beginning of each block of contiguous\n-    // elements.\n-    Value acc;\n-    // Loop through the contiguous elements.\n-    for (unsigned i = 0; i < scanElementsPerThreads; ++i) {\n-      accumulate(rewriter, helper.getCombineOp(), acc, srcValues[i + j]);\n-      srcValues[i + j] = acc;\n-    }\n+  // Depending on layout contiguous elements along axis dim may not be\n+  // contiguous in srcValues. Keep track of what elements belong to the same\n+  // chunk of contiguous elements.\n+  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThread();\n+  unsigned parallelElementsPerThread = helper.getAxisNumElementsPerThread();\n+  unsigned numChunks = srcValues.size() / scanElementsPerThreads;\n+  unsigned stride = helper.getAxisElementStride();\n+  SmallVector<Value> accs(numChunks);\n+  for (unsigned srcIndex = 0; srcIndex < srcValues.size(); srcIndex++) {\n+    unsigned accIndex = (srcIndex % stride) +\n+                        ((srcIndex / stride) / scanElementsPerThreads) * stride;\n+\n+    accumulate(rewriter, helper.getCombineOp(), accs[accIndex],\n+               srcValues[srcIndex]);\n+    srcValues[srcIndex] = accs[accIndex];\n   }\n }\n \n@@ -59,20 +61,25 @@ static void warpScan(SmallVector<Value> &srcValues,\n                      ConversionPatternRewriter &rewriter,\n                      ScanLoweringHelper &helper, Value laneId) {\n   Location loc = helper.getLoc();\n-  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThreads();\n-  for (unsigned j = scanElementsPerThreads - 1; j < srcValues.size();\n-       j += scanElementsPerThreads) {\n-    Value acc = srcValues[j];\n-    unsigned scanDim = helper.getAxisNumThreadsPerWarp();\n+  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThread();\n+  unsigned elementStride = helper.getAxisElementStride();\n+  unsigned threadStride = helper.getAxisThreadStride();\n+  unsigned scanDim = helper.getAxisNumThreadsPerWarp();\n+  for (unsigned srcIndex = 0; srcIndex < srcValues.size(); srcIndex++) {\n+    unsigned elementIdx = (srcIndex / elementStride) % scanElementsPerThreads;\n+    // Only consider the last element of each contiguous chunk of elements.\n+    if (elementIdx != scanElementsPerThreads - 1)\n+      continue;\n     // Reduce within warps.\n-    for (unsigned i = 1; i <= scanDim / 2; i = i << 1) {\n-      Value shfl = shflUpSync(loc, rewriter, acc, i);\n+    Value acc = srcValues[srcIndex];\n+    for (unsigned i = 1; i <= (scanDim) / 2; i = i << 1) {\n+      Value shfl = shflUpSync(loc, rewriter, acc, i * threadStride);\n       Value tempAcc = acc;\n       accumulate(rewriter, helper.getCombineOp(), tempAcc, shfl);\n       Value mask = icmp_slt(laneId, i32_val(i));\n       acc = select(mask, acc, tempAcc);\n     }\n-    srcValues[j] = acc;\n+    srcValues[srcIndex] = acc;\n   }\n }\n \n@@ -88,19 +95,24 @@ static void storeWarpAccumulator(SmallVector<Value> &srcValues,\n                                  Value warpId, Value baseSharedMemPtr,\n                                  Value parallelLaneId) {\n   Location loc = helper.getLoc();\n-  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThreads();\n+  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThread();\n   unsigned scanDim = helper.getAxisNumThreadsPerWarp();\n   unsigned numParallelLane = helper.getNonAxisNumThreadsPerCTA();\n   unsigned numWarps = helper.getAxisNumWarps();\n   unsigned chunkId = 0;\n-  for (unsigned j = scanElementsPerThreads - 1; j < srcValues.size();\n-       j += scanElementsPerThreads, ++chunkId) {\n-    Value lastElement = srcValues[j];\n+  unsigned elementStride = helper.getAxisElementStride();\n+  for (unsigned srcIndex = 0; srcIndex < srcValues.size(); srcIndex++) {\n+    unsigned elementIdx = (srcIndex / elementStride) % scanElementsPerThreads;\n+    // Only consider the last element of each contiguous chunk of elements.\n+    if (elementIdx != scanElementsPerThreads - 1)\n+      continue;\n+    Value lastElement = srcValues[srcIndex];\n     Value mask = icmp_eq(laneId, i32_val(scanDim - 1));\n     Value index = add(parallelLaneId, mul(warpId, i32_val(numParallelLane)));\n     index = add(index, i32_val(chunkId * numParallelLane * numWarps));\n     Value writePtr = gep(baseSharedMemPtr.getType(), baseSharedMemPtr, index);\n     storeShared(rewriter, loc, writePtr, lastElement, mask);\n+    chunkId++;\n   }\n }\n \n@@ -116,8 +128,10 @@ static void AddPartialReduce(SmallVector<Value> &srcValues,\n   Location loc = helper.getLoc();\n   unsigned numParallelLane = helper.getNonAxisNumThreadsPerCTA();\n   unsigned numWarps = helper.getAxisNumWarps();\n-  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThreads();\n+  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThread();\n   unsigned parallelElementsPerThread = helper.getNonAxisNumElementsPerThread();\n+  unsigned elementStride = helper.getAxisElementStride();\n+  unsigned threadStride = helper.getAxisThreadStride();\n   Value maskFirstWarp = icmp_eq(warpId, i32_val(0));\n   Value maskFirstLane = icmp_eq(laneId, i32_val(0));\n   Value maskFirstThread = and_(maskFirstWarp, maskFirstLane);\n@@ -133,63 +147,66 @@ static void AddPartialReduce(SmallVector<Value> &srcValues,\n   SmallVector<Accumulator> accumulators(numParallelBlocks *\n                                         parallelElementsPerThread);\n   unsigned chunkId = 0;\n-  for (unsigned parallelBlockId = 0; parallelBlockId < numParallelBlocks;\n-       ++parallelBlockId) {\n-    for (unsigned scanBlockId = 0; scanBlockId < numScanBlocks; ++scanBlockId) {\n-      for (unsigned parallelElementId = 0;\n-           parallelElementId < parallelElementsPerThread; ++parallelElementId) {\n-        unsigned accumulatorIndex =\n-            parallelElementId + parallelBlockId * parallelElementsPerThread;\n-        Accumulator &accumulator = accumulators[accumulatorIndex];\n-        for (unsigned i = 0; i < numWarps; ++i) {\n-          Value index = add(parallelLaneId, i32_val(numParallelLane *\n-                                                    (i + chunkId * numWarps)));\n-          Value ptr = gep(sharedMemoryPtr.getType(), sharedMemoryPtr, index);\n-          Value partialReduce = load(ptr);\n-          if (!accumulator.acc) {\n-            accumulator.acc = partialReduce;\n-            accumulator.maskedAcc = partialReduce;\n-            continue;\n-          }\n-          accumulate(rewriter, helper.getCombineOp(), accumulator.acc,\n-                     partialReduce);\n-          Value mask = icmp_slt(warpId, i32_val(i + 1));\n-          accumulator.maskedAcc =\n-              select(mask, accumulator.maskedAcc, accumulator.acc);\n-        }\n-        unsigned lastElementIndex =\n-            chunkId * scanElementsPerThreads + scanElementsPerThreads - 1;\n-        Value temp = srcValues[lastElementIndex];\n-        accumulate(rewriter, helper.getCombineOp(), temp,\n-                   accumulator.maskedAcc);\n-        if (scanBlockId == 0) {\n-          // For the first warp and first chunk we don't have anything to\n-          // accumulate.\n-          temp = select(maskFirstWarp, srcValues[lastElementIndex], temp);\n-        }\n-        srcValues[lastElementIndex] = temp;\n-\n-        // Update the rest of the contiguous elements.\n-        Value lastElement =\n-            shflUpSync(loc, rewriter, srcValues[lastElementIndex], 1);\n-        lastElement = select(maskFirstLane, accumulator.maskedAcc, lastElement);\n-        for (unsigned i = 1; i < scanElementsPerThreads; ++i) {\n-          Value laneValue = srcValues[lastElementIndex - i];\n-          accumulate(rewriter, helper.getCombineOp(), laneValue, lastElement);\n-          if (scanBlockId == 0) {\n-            // For the first warp and first chunk we don't have anything to\n-            // accumulate.\n-            laneValue = select(maskFirstThread, srcValues[lastElementIndex - i],\n-                               laneValue);\n-          }\n-          srcValues[lastElementIndex - i] = laneValue;\n-        }\n-        // For the next chunk start back from the value containing the\n-        // accumulated value of all the warps.\n-        accumulator.maskedAcc = accumulator.acc;\n-        chunkId++;\n+  unsigned blockStride = helper.getAxisBlockStride();\n+  for (unsigned srcIndex = 0; srcIndex < srcValues.size(); srcIndex++) {\n+    unsigned elementIdx = (srcIndex / elementStride) % scanElementsPerThreads;\n+    // Only consider the last element of each contiguous chunk of elements.\n+    if (elementIdx != scanElementsPerThreads - 1)\n+      continue;\n+    // Accumulate the partial reduction from shared memory. Decide which\n+    // accumulator to combine based on whether the elements belong to the same\n+    // dimension along axis.\n+    unsigned blockId = chunkId / parallelElementsPerThread;\n+    unsigned parallelBlockId =\n+        blockId % blockStride +\n+        ((blockId / blockStride) / numScanBlocks) * blockStride;\n+    unsigned accumulatorIndex = chunkId % parallelElementsPerThread +\n+                                parallelBlockId * parallelElementsPerThread;\n+    Accumulator &accumulator = accumulators[accumulatorIndex];\n+    for (unsigned i = 0; i < numWarps; ++i) {\n+      Value index = add(parallelLaneId,\n+                        i32_val(numParallelLane * (i + chunkId * numWarps)));\n+      Value ptr = gep(sharedMemoryPtr.getType(), sharedMemoryPtr, index);\n+      Value partialReduce = load(ptr);\n+      if (!accumulator.acc) {\n+        accumulator.acc = partialReduce;\n+        accumulator.maskedAcc = partialReduce;\n+        continue;\n+      }\n+      accumulate(rewriter, helper.getCombineOp(), accumulator.acc,\n+                 partialReduce);\n+      Value mask = icmp_slt(warpId, i32_val(i + 1));\n+      accumulator.maskedAcc =\n+          select(mask, accumulator.maskedAcc, accumulator.acc);\n+    }\n+    Value temp = srcValues[srcIndex];\n+    accumulate(rewriter, helper.getCombineOp(), temp, accumulator.maskedAcc);\n+    unsigned axisBlockId = (blockId / blockStride) % numScanBlocks;\n+    if (axisBlockId == 0) {\n+      // For the first warp and first chunk we don't have anything to\n+      // accumulate.\n+      temp = select(maskFirstWarp, srcValues[srcIndex], temp);\n+    }\n+    srcValues[srcIndex] = temp;\n+    // Update the rest of the contiguous elements.\n+    Value lastElement =\n+        shflUpSync(loc, rewriter, srcValues[srcIndex], threadStride);\n+    lastElement = select(maskFirstLane, accumulator.maskedAcc, lastElement);\n+    for (unsigned i = 1; i < scanElementsPerThreads; ++i) {\n+      Value laneValue = srcValues[srcIndex - i * elementStride];\n+      accumulate(rewriter, helper.getCombineOp(), laneValue, lastElement);\n+      if (axisBlockId == 0) {\n+        // For the first warp and first chunk we don't have anything to\n+        // accumulate.\n+        laneValue = select(maskFirstThread,\n+                           srcValues[srcIndex - i * elementStride], laneValue);\n       }\n+      srcValues[srcIndex - i * elementStride] = laneValue;\n     }\n+    // For the next chunk start back from the value containing the\n+    // accumulated value of all the warps.\n+    accumulator.maskedAcc = accumulator.acc;\n+    chunkId++;\n   }\n }\n "}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 13, "deletions": 5, "changes": 18, "file_content_changes": "@@ -1497,8 +1497,9 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n scan2d_shapes = [(16, 32), (32, 16), (2, 1024), (1024, 2), (32, 32), (1, 1024)]\n \n scan_configs = [\n-    (op, type, shape, 1)\n+    (op, type, shape, axis)\n     for type in ['int32', 'float32']\n+    for axis in [1, 0]\n     for shape in scan2d_shapes\n     for op in ['cumsum']\n ]\n@@ -1517,7 +1518,7 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n         z = GENERATE_TEST_HERE\n         tl.store(Z + range_m[:, None] * BLOCK_N + range_n[None, :], z)\n \n-    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=1)'})\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis={axis})'})\n     # input\n     rs = RandomState(17)\n     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n@@ -1538,6 +1539,12 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n \n \n scan_layouts = [\n+    BlockedLayout([1, 4], [4, 8], [4, 1], [0, 1]),\n+    BlockedLayout([1, 4], [8, 4], [4, 1], [0, 1]),\n+    BlockedLayout([4, 1], [4, 8], [1, 4], [0, 1]),\n+    BlockedLayout([2, 2], [4, 8], [2, 2], [0, 1]),\n+    BlockedLayout([2, 2], [8, 4], [2, 2], [0, 1]),\n+\n     BlockedLayout([1, 4], [4, 8], [4, 1], [1, 0]),\n     BlockedLayout([1, 4], [8, 4], [4, 1], [1, 0]),\n     BlockedLayout([4, 1], [4, 8], [1, 4], [1, 0]),\n@@ -1547,7 +1554,8 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n \n \n @pytest.mark.parametrize(\"src_layout\", scan_layouts)\n-def test_scan_layouts(src_layout, device):\n+@pytest.mark.parametrize(\"axis\", [0, 1])\n+def test_scan_layouts(src_layout, axis, device):\n     ir = f\"\"\"\n     #blocked = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32}} {{\n@@ -1564,7 +1572,7 @@ def test_scan_layouts(src_layout, device):\n       %8 = tt.broadcast %6 : (tensor<1x32xi32, #blocked>) -> tensor<32x32xi32, #blocked>\n       %9 = tt.addptr %7, %8 : tensor<32x32x!tt.ptr<i32>, #blocked>, tensor<32x32xi32, #blocked>\n       %10 = tt.load %9 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<32x32xi32, #blocked>\n-      %11 = \"tt.scan\"(%10) <{{axis = 1 : i32}}> ({{\n+      %11 = \"tt.scan\"(%10) <{{axis = {axis} : i32}}> ({{\n       ^bb0(%arg2: i32, %arg3: i32):\n         %16 = arith.addi %arg2, %arg3 : i32\n         tt.scan.return %16 : i32\n@@ -1595,7 +1603,7 @@ def test_scan_layouts(src_layout, device):\n \n     kernel[(1, 1, 1)](x_tri, z_tri)\n \n-    z_ref = np.cumsum(x, axis=1)\n+    z_ref = np.cumsum(x, axis=axis)\n \n     np.testing.assert_equal(z_ref, z_tri.cpu().numpy())\n "}]