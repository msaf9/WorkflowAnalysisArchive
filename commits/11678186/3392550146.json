[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 123, "deletions": 37, "changes": 160, "file_content_changes": "@@ -3654,18 +3654,6 @@ class DotOpFMAConversionHelper {\n     return operand.getElementType().isF32();\n   }\n \n-  SmallVector<unsigned> getOrder() const {\n-    SmallVector<unsigned> order(2);\n-    if (mmaLayout.getVersion() == 1)\n-      order = {0, 1};\n-    else if (mmaLayout.getVersion() == 0)\n-      order = {1, 0};\n-    else {\n-      assert(false && \"Unexpected MMA version found.\");\n-    }\n-    return order;\n-  }\n-\n   Value loadA(Value tensor, Value llTensor, Value threadId, Location loc,\n               Value smem, ConversionPatternRewriter &rewriter) const {\n \n@@ -3674,7 +3662,6 @@ class DotOpFMAConversionHelper {\n     auto aShape = tensorTy.getShape();\n     auto aLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n     auto aOrder = aLayout.getOrder();\n-    auto order = getOrder();\n \n     bool isARow = aOrder[0] == 1;\n \n@@ -3688,14 +3675,115 @@ class DotOpFMAConversionHelper {\n     int aNumPtr = 8;\n     int bNumPtr = 8;\n     int aVec = 2;\n-    int NK = aShape[isARow ? 1 : 0];\n \n-    return Value{};\n+    Value _0 = i32_val(0);\n+    Value _1 = i32_val(1);\n+\n+    Value mContig = _1;\n+    Value nContig = _1;\n+\n+    Value offA0 = isARow ? _0 : mul(threadId, mContig);\n+    Value offA1 = isARow ? mul(threadId, mContig) : _0;\n+    SmallVector<Value> aOff(aNumPtr);\n+    for (int i = 0; i < aNumPtr; ++i) {\n+      aOff[i] =\n+          add(mul(offA0, i32_val(strideA0)), mul(offA1, i32_val(strideA1)));\n+    }\n+\n+    Type f32PtrTy = ptr_ty(f32_ty);\n+    SmallVector<Value> aPtrs(aNumPtr);\n+    for (int i = 0; i < aNumPtr; ++i)\n+      aPtrs[i] = gep(f32PtrTy, llTensor, aOff[i]);\n+\n+    ValueTable has;\n+\n+    auto aShapePerCTA = getShapePerCTA(aLayout);\n+    auto sizePerThread = getSizePerThread(aLayout);\n+    int M = isARow ? aShape[0] : aShape[1];\n+    int K = isARow ? aShape[1] : aShape[0];\n+\n+    for (unsigned k = 0; k < K; k++)\n+      for (unsigned m = 0; m < M; m += aShapePerCTA[aOrder[1]])\n+        for (unsigned mm = 0; mm < sizePerThread[aOrder[1]]; ++mm) {\n+          Value pa = gep(f32PtrTy, aPtrs[0],\n+                         i32_val((m + mm) * strideAM + k * strideAK));\n+          Value va = load(pa);\n+          has[{m + mm, k}] = va;\n+        }\n+\n+    SmallVector<Value> values;\n+    for (auto &item : has)\n+      values.push_back(item.second);\n+    Type structTy =\n+        struct_ty(SmallVector<Type>(values.size(), values[0].getType()));\n+\n+    return getStructFromElements(loc, values, rewriter, structTy);\n   }\n \n   Value loadB(Value tensor, Value llTensor, Value threadId, Location loc,\n               Value smem, ConversionPatternRewriter &rewriter) const {\n-    return Value{};\n+\n+    auto *ctx = rewriter.getContext();\n+    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+    auto bShape = tensorTy.getShape();\n+    auto bLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+    auto bOrder = bLayout.getOrder();\n+\n+    bool isBRow = bOrder[0] == 1;\n+\n+    int strideBN = isBRow ? 1 : bShape[0];\n+    int strideBK = isBRow ? bShape[1] : 1;\n+    int strideB0 = isBRow ? strideBN : strideBK;\n+    int strideB1 = isBRow ? strideBK : strideBN;\n+    int ldb = isBRow ? strideBK : strideBN;\n+    int bPerPhase = bLayout.getPerPhase();\n+    int bMaxPhase = bLayout.getMaxPhase();\n+    int bNumPtr = 8;\n+    int bVec = 4;\n+\n+    auto bShapePerCTA = getShapePerCTA(bLayout);\n+    auto sizePerThread = getSizePerThread(bLayout);\n+\n+    Value _0 = i32_val(0);\n+    Value _1 = i32_val(1);\n+\n+    Value mContig = _1;\n+    Value nContig = _1;\n+\n+    Value offB0 = isBRow ? mul(threadId, nContig) : _0;\n+    Value offB1 = isBRow ? _0 : mul(threadId, nContig);\n+    SmallVector<Value> bOff(bNumPtr);\n+    for (int i = 0; i < bNumPtr; ++i) {\n+      bOff[i] =\n+          add(mul(offB0, i32_val(strideB0)), mul(offB1, i32_val(strideB1)));\n+    }\n+\n+    Type f32PtrTy = ptr_ty(f32_ty);\n+    SmallVector<Value> bPtrs(bNumPtr);\n+    for (int i = 0; i < bNumPtr; ++i)\n+      bPtrs[i] = gep(f32PtrTy, llTensor, bOff[i]);\n+\n+    ValueTable hbs;\n+\n+    int K = isBRow ? bShape[0] : bShape[1];\n+    int N = isBRow ? bShape[1] : bShape[0];\n+\n+    for (int k = 0; k < K; ++k)\n+      for (unsigned n = 0; n < N; n += bShapePerCTA[bOrder[0]])\n+        for (unsigned nn = 0; nn < sizePerThread[bOrder[0]]; ++nn) {\n+          Value pb = gep(f32PtrTy, bPtrs[0],\n+                         i32_val((n + nn) * strideBN + k * strideBK));\n+          Value vb = load(pb);\n+          hbs[{n + nn, k}] = vb;\n+        }\n+\n+    SmallVector<Value> values;\n+    for (auto &item : hbs)\n+      values.push_back(item.second);\n+    Type structTy =\n+        struct_ty(SmallVector<Type>(values.size(), values[0].getType()));\n+\n+    return getStructFromElements(loc, values, rewriter, structTy);\n   }\n \n   ValueTable extractLoadedOperand(Value llTensor) const { return ValueTable{}; }\n@@ -3738,18 +3826,15 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n                                        rewriter, getTypeConverter(),\n                                        op.getLoc());\n \n-    if (dotOperandLayout.getOpIdx() == 0) {\n-      // operand $a\n+    if (dotOperandLayout.getOpIdx() == 0) { // operand $a\n       res = mmaHelper.loadA(src, adaptor.src());\n-    } else if (dotOperandLayout.getOpIdx() == 1) {\n-      // operand $b\n+    } else if (dotOperandLayout.getOpIdx() == 1) { // operand $b\n       res = mmaHelper.loadB(src, adaptor.src());\n     }\n   } else if (!isOuter && mmaLayout.getVersion() == 1 &&\n              isHMMA) { // tensor core v1\n     DotOpMmaV1ConversionHelper helper(mmaLayout);\n-    if (dotOperandLayout.getOpIdx() == 0) {\n-      // operand $a\n+    if (dotOperandLayout.getOpIdx() == 0) { // operand $a\n       res = helper.loadA(src, adaptor.src(), getThreadId(rewriter, loc),\n                          adaptor.src(), loc, rewriter);\n     } else if (dotOperandLayout.getOpIdx() == 1) {\n@@ -3758,7 +3843,14 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n                          adaptor.src(), loc, rewriter);\n     }\n   } else if (DotOpFMAConversionHelper::useFMA(dstTensorTy)) { // fmadot\n-\n+    DotOpMmaV1ConversionHelper helper(mmaLayout);\n+    if (dotOperandLayout.getOpIdx() == 0) { // operand $a\n+      res = helper.loadA(src, adaptor.src(), getThreadId(rewriter, loc),\n+                         adaptor.src(), loc, rewriter);\n+    } else if (dotOperandLayout.getOpIdx() == 1) { // operand $b\n+      res = helper.loadB(src, adaptor.src(), getThreadId(rewriter, loc),\n+                         adaptor.src(), loc, rewriter);\n+    }\n   } else {\n     assert(false && \"Unsupported mma layout found\");\n   }\n@@ -4245,26 +4337,20 @@ DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n \n   auto aLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n   auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n-  auto cLayout = cTensorTy.getEncoding().cast<MmaEncodingAttr>();\n-  auto dLayout = dTensorTy.getEncoding().cast<MmaEncodingAttr>();\n+  auto cLayout = cTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto dLayout = dTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n \n   auto aOrder = aLayout.getOrder();\n   auto bOrder = bLayout.getOrder();\n \n-  // According to the original logic, if target.sm < 80, get a {0,1} or get a\n-  // {1,0}\n-  SmallVector<int> order(2);\n-  if (dLayout.getVersion() == 1)\n-    order = {0, 1};\n-  else\n-    order = {1, 0};\n+  auto order = dLayout.getOrder();\n \n   bool isARow = aOrder[0] == 1;\n   bool isBRow = bOrder[0] == 1;\n \n   int strideAM = isARow ? aShape[1] : 1;\n   int strideAK = isARow ? 1 : aShape[0];\n-  int strideBN = isBRow ? 1 : aShape[0];\n+  int strideBN = isBRow ? 1 : bShape[0];\n   int strideBK = isBRow ? bShape[1] : 1;\n   int strideA0 = isARow ? strideAK : strideAM;\n   int strideA1 = isARow ? strideAM : strideAK;\n@@ -4315,9 +4401,9 @@ DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n     bPtrs[i] = gep(f32PtrTy, adaptor.b(), bOff[i]);\n \n   // TODO initialize ret with $c.\n-  std::map<std::pair<int, int>, Value> has, hbs;\n+  DotOpFMAConversionHelper::ValueTable has, hbs;\n   auto cc = getElementsFromStruct(loc, adaptor.c(), rewriter);\n-  SmallVector<Value> ret(cShape[0] * cShape[1], cc[0]);\n+  SmallVector<Value> ret = cc;\n \n   for (unsigned k = 0; k < NK; k++) {\n     int z = 0;\n@@ -4982,8 +5068,8 @@ void ConvertTritonGPUToLLVM::initSharedMemory(\n   OpBuilder b(mod.getBodyRegion());\n   auto loc = mod.getLoc();\n   auto elemTy = typeConverter.convertType(b.getIntegerType(8));\n-  // Set array size 0 and external linkage indicates that we use dynamic shared\n-  // allocation to allow a larger shared memory size for each kernel.\n+  // Set array size 0 and external linkage indicates that we use dynamic\n+  // shared allocation to allow a larger shared memory size for each kernel.\n   auto arrayTy = LLVM::LLVMArrayType::get(elemTy, 0);\n   auto global = b.create<LLVM::GlobalOp>(\n       loc, arrayTy, /*isConstant=*/false, LLVM::Linkage::External,"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -117,10 +117,12 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n                   \"BlockedEncodingAttr not implemented\");\n     }\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    assert(mmaLayout.getVersion() == 2 &&\n-           \"mmaLayout version = 1 is not implemented yet\");\n-    return {16 * mmaLayout.getWarpsPerCTA()[0],\n-            8 * mmaLayout.getWarpsPerCTA()[1]};\n+    if (mmaLayout.getVersion() == 2)\n+      return {16 * mmaLayout.getWarpsPerCTA()[0],\n+              8 * mmaLayout.getWarpsPerCTA()[1]};\n+    if (mmaLayout.getVersion() == 1)\n+      return {16 * mmaLayout.getWarpsPerCTA()[0],\n+              16 * mmaLayout.getWarpsPerCTA()[1]};\n   } else {\n     assert(0 && \"Unimplemented usage of getShapePerCTA\");\n   }"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 6, "deletions": 3, "changes": 9, "file_content_changes": "@@ -797,11 +797,14 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  func @matmul884_kernel_dot_operand_layout(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  func @matmul_fmadot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %a:tensor<32x16xf32, #shared>, %b:tensor<16x32xf32, #shared>) {\n-    %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #mma>\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #blocked>\n     // CHECK: llvm.intr.fmuladd\n-    %28 = tt.dot %a, %b, %cst {allowTF32 = false, transA = false, transB = false} : tensor<32x16xf32, #shared> * tensor<16x32xf32, #shared> -> tensor<16x16xf32, #mma>\n+    %28 = tt.dot %a, %b, %cst {allowTF32 = false, transA = false, transB = false} : tensor<32x16xf32, #shared> * tensor<16x32xf32, #shared> -> tensor<32x32xf32, #blocked>\n+    %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n+    %36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x32x!tt.ptr<f32>, #blocked>\n+    tt.store %36, %28 : tensor<32x32xf32, #blocked>\n     return\n   }\n }"}]