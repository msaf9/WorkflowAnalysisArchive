[{"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -73,7 +73,7 @@ def nvsmi(attrs):\n @pytest.mark.parametrize('M, N, K, dtype_str',\n                          [(M, N, K, dtype_str)\n                           for M, N, K in matmul_data[DEVICE_NAME].keys()\n-                          for dtype_str in ['float16']])\n+                          for dtype_str in ['float16', 'float32', 'int8']])\n def test_matmul(M, N, K, dtype_str):\n     if dtype_str in ['float32', 'int8'] and DEVICE_NAME != 'a100':\n         pytest.skip('Only test float32 & int8 on a100')\n@@ -92,7 +92,7 @@ def test_matmul(M, N, K, dtype_str):\n         a = torch.randn((M, K), dtype=dtype, device='cuda')\n         b = torch.randn((K, N), dtype=dtype, device='cuda')\n     fn = lambda: triton.ops.matmul(a, b)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=25, rep=100)\n+    ms = triton.testing.do_bench(fn, percentiles=None, warmup=25, rep=200)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     triton.testing.assert_almost_equal(cur_gpu_util, ref_gpu_util, decimal=2)\n@@ -151,7 +151,7 @@ def test_elementwise(N):\n     y = torch.randn_like(z)\n     grid = lambda args: (triton.cdiv(N, args['BLOCK_SIZE']), )\n     fn = lambda: _add[grid](x, y, z, N, BLOCK_SIZE=1024)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=25, rep=250)\n+    ms = triton.testing.do_bench(fn, percentiles=None, warmup=25, rep=200)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     triton.testing.assert_almost_equal(cur_gpu_util, ref_gpu_util, decimal=2)"}]