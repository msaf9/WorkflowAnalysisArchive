[{"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 20, "deletions": 28, "changes": 48, "file_content_changes": "@@ -157,15 +157,6 @@\n @triton.autotune(\n     configs=[\n         triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n-        # triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n-        # triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        # triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        # triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        # triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        # triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        # triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n-        # triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n-        # triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n     ],\n     key=['M', 'N', 'K'],\n )\n@@ -236,8 +227,8 @@ def matmul_kernel(\n         b_ptrs += BLOCK_SIZE_K * stride_bk\n     # you can fuse arbitrary activation functions here\n     # while the accumulator is still in FP32!\n-    if ACTIVATION == \"leaky_relu\":\n-        accumulator = leaky_relu(accumulator)\n+    if ACTIVATION:\n+        accumulator = ACTIVATION(accumulator)\n     c = accumulator.to(tl.float16)\n \n     # -----------------------------------------------------------\n@@ -249,10 +240,13 @@ def matmul_kernel(\n     tl.store(c_ptrs, c, mask=c_mask)\n \n \n+# matmul_kernel = triton.compile(\"./matmul-2.ttgir\", num_warps=8)\n+\n # we can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`\n+\n+\n @triton.jit\n def leaky_relu(x):\n-    x = x + 1\n     return tl.where(x >= 0, x, 0.01 * x)\n \n \n@@ -261,7 +255,7 @@ def leaky_relu(x):\n # and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel\n \n \n-def matmul(a, b, activation=\"\"):\n+def matmul(a, b, activation=None):\n     # checks constraints\n     assert a.shape[1] == b.shape[0], \"incompatible dimensions\"\n     assert a.is_contiguous(), \"matrix A must be contiguous\"\n@@ -285,6 +279,14 @@ def matmul(a, b, activation=\"\"):\n         c.stride(0), c.stride(1),\n         ACTIVATION=activation,\n     )\n+    # grid = (triton.cdiv(M, 128) * triton.cdiv(N, 256), 1, 1)\n+    # matmul_kernel[grid](\n+    #     a.data_ptr(), b.data_ptr(), c.data_ptr(),\n+    #     M, N, K,\n+    #     a.stride(0),\n+    #     b.stride(0),\n+    #     c.stride(0)\n+    # )\n     return c\n \n \n@@ -297,7 +299,7 @@ def matmul(a, b, activation=\"\"):\n torch.manual_seed(0)\n a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n-triton_output = matmul(a, b)\n+triton_output = matmul(a, b, activation=None)\n torch_output = torch.matmul(a, b)\n print(f\"triton_output={triton_output}\")\n print(f\"torch_output={torch_output}\")\n@@ -319,16 +321,15 @@ def matmul(a, b, activation=\"\"):\n     triton.testing.Benchmark(\n         x_names=['M', 'N', 'K'],  # argument names to use as an x-axis for the plot\n         x_vals=[\n-            # 128 * i for i in range(2, 33)\n-            8192,\n+            8192\n         ],  # different possible values for `x_name`\n         line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n         # possible values for `line_arg``\n         line_vals=['cublas', 'triton'],\n         # label name for the lines\n         line_names=[\"cuBLAS\", \"Triton\"],\n         # line styles\n-        styles=[('green', '-'), ('blue', '-')],\n+        styles=[('green', '-'), ('green', '--'), ('blue', '-'), ('blue', '--')],\n         ylabel=\"TFLOPS\",  # label name for the y-axis\n         plot_name=\"matmul-performance\",  # name for the plot. Used also as a file name for saving the plot.\n         args={},\n@@ -338,18 +339,9 @@ def benchmark(M, N, K, provider):\n     a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n     b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n     if provider == 'cublas':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b))\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), rep=500)\n     if provider == 'triton':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b))\n-    if provider == 'cublas + relu':\n-        torch_relu = torch.nn.ReLU(inplace=True)\n-        ms, min_ms, max_ms = triton.testing.do_bench(\n-            lambda: torch_relu(torch.matmul(a, b))\n-        )\n-    if provider == 'triton + relu':\n-        ms, min_ms, max_ms = triton.testing.do_bench(\n-            lambda: matmul(a, b, activation=\"leaky_relu\")\n-        )\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), rep=500)\n     perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n     return perf(ms), perf(max_ms), perf(min_ms)\n "}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -357,5 +357,4 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.f\n         ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n         return ms\n \n-\n-bench_flash_attention.run(save_path='.', print_data=True)\n+# bench_flash_attention.run(save_path='.', print_data=True)"}]