[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 11, "deletions": 3, "changes": 14, "file_content_changes": "@@ -3,10 +3,9 @@ name: Integration Tests\n on:\n   workflow_dispatch:\n   pull_request:\n-    branches:\n-      - main\n-      - triton-mlir\n+    branches: [main]\n   merge_group:\n+    branches: [main]\n     types: [checks_requested]\n \n concurrency:\n@@ -100,3 +99,12 @@ jobs:\n           cd python/\n           cd \"build/$(ls build)\"\n           ctest\n+\n+      - name: Regression tests\n+        if: ${{ contains(matrix.runner, 'A100') }}\n+        run: |\n+          cd python/test/regression\n+          sudo nvidia-smi -i 0 -pm 1\n+          sudo nvidia-smi -i 0 --lock-gpu-clocks=1350,1350\n+          pytest -vs .\n+          sudo nvidia-smi -i 0 -rgc\n\\ No newline at end of file"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 64, "deletions": 25, "changes": 89, "file_content_changes": "@@ -8,7 +8,7 @@\n import triton.language as tl\n from triton.testing import get_dram_gbps, get_max_tensorcore_tflops\n \n-DEVICE_NAME = 'v100'\n+DEVICE_NAME = {7: 'v100', 8: 'a100'}[torch.cuda.get_device_capability()[0]]\n \n #######################\n # Utilities\n@@ -34,7 +34,6 @@ def nvsmi(attrs):\n matmul_data = {\n     'v100': {\n         # square\n-        (256, 256, 256): {'float16': 0.027},\n         (512, 512, 512): {'float16': 0.158},\n         (1024, 1024, 1024): {'float16': 0.466},\n         (2048, 2048, 2048): {'float16': 0.695},\n@@ -51,29 +50,26 @@ def nvsmi(attrs):\n         (4096, 64, 4096): {'float16': 0.264},\n         (8192, 64, 8192): {'float16': 0.452},\n     },\n+    # NOTE:\n+    # A100 in the CI server is slow-ish for some reason.\n+    # On some other servers, we are getting about 90% peak for 8kx8x8k float16\n     'a100': {\n-        (256, 256, 256): {'float16': 0.010, 'float32': 0.0214, 'int8': 0.006},\n-        (512, 512, 512): {'float16': 0.061, 'float32': 0.109, 'int8': 0.030},\n-        (1024, 1024, 1024): {'float16': 0.287, 'float32': 0.331, 'int8': 0.169},\n-        (2048, 2048, 2048): {'float16': 0.604, 'float32': 0.599, 'int8': 0.385},\n-        (4096, 4096, 4096): {'float16': 0.842, 'float32': 0.862, 'int8': 0.711},\n-        (8192, 8192, 8192): {'float16': 0.896, 'float32': 0.932, 'int8': 0.860},\n+        (512, 512, 512): {'float16': 0.08, 'float32': 0.13, 'int8': 0.05},\n+        (1024, 1024, 1024): {'float16': 0.33, 'float32': 0.35, 'int8': 0.169},\n+        (2048, 2048, 2048): {'float16': 0.64, 'float32': 0.57, 'int8': 0.34},\n+        (4096, 4096, 4096): {'float16': 0.80, 'float32': 0.75, 'int8': 0.46},\n+        (8192, 8192, 8192): {'float16': 0.77, 'float32': 0.85, 'int8': 0.51},\n         # tall-skinny\n         (16, 1024, 1024): {'float16': 0.0077, 'float32': 0.0127, 'int8': 0.005},\n         (16, 4096, 4096): {'float16': 0.0363, 'float32': 0.0457, 'int8': 0.0259},\n-        (16, 8192, 8192): {'float16': 0.0564, 'float32': 0.0648, 'int8': 0.0431},\n+        (16, 8192, 8192): {'float16': 0.07, 'float32': 0.0648, 'int8': 0.0431},\n         (64, 1024, 1024): {'float16': 0.0271, 'float32': 0.0509, 'int8': 0.0169},\n-        (64, 4096, 4096): {'float16': 0.141, 'float32': 0.162, 'int8': 0.097},\n-        (64, 8192, 8192): {'float16': 0.244, 'float32': 0.257, 'int8': 0.174},\n+        (64, 4096, 4096): {'float16': 0.16, 'float32': 0.162, 'int8': 0.097},\n+        (64, 8192, 8192): {'float16': 0.30, 'float32': 0.257, 'int8': 0.174},\n         (1024, 64, 1024): {'float16': 0.0263, 'float32': 0.0458, 'int8': 0.017},\n-        (4096, 64, 4096): {'float16': 0.135, 'float32': 0.177, 'int8': 0.102},\n-        (8192, 64, 8192): {'float16': 0.216, 'float32': 0.230, 'int8': 0.177},\n+        (4096, 64, 4096): {'float16': 0.16, 'float32': 0.177, 'int8': 0.102},\n+        (8192, 64, 8192): {'float16': 0.25, 'float32': 0.230, 'int8': 0.177},\n     }\n-    #   # deep reductions\n-    #   (64  , 64  , 16384) : {'a100': 0.},\n-    #   (64  , 64  , 65536) : {'a100': 0.},\n-    #   (256 , 256 , 8192 ) : {'a100': 0.},\n-    #   (256 , 256 , 32768) : {'a100': 0.},\n }\n \n \n@@ -88,9 +84,7 @@ def test_matmul(M, N, K, dtype_str):\n     torch.manual_seed(0)\n     ref_gpu_util = matmul_data[DEVICE_NAME][(M, N, K)][dtype_str]\n     cur_sm_clock = nvsmi(['clocks.current.sm'])[0]\n-    ref_sm_clock = sm_clocks[DEVICE_NAME]\n     max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n-    assert abs(cur_sm_clock - ref_sm_clock) < 10, f'GPU SMs must run at {ref_sm_clock} MHz'\n     if dtype == torch.int8:\n         a = torch.randint(-128, 127, (M, K), dtype=dtype, device='cuda')\n         b = torch.randint(-128, 127, (N, K), dtype=dtype, device='cuda')\n@@ -99,7 +93,7 @@ def test_matmul(M, N, K, dtype_str):\n         a = torch.randn((M, K), dtype=dtype, device='cuda')\n         b = torch.randn((K, N), dtype=dtype, device='cuda')\n     fn = lambda: triton.ops.matmul(a, b)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=25, rep=1000)\n+    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=300)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     triton.testing.assert_almost_equal(cur_gpu_util, ref_gpu_util, decimal=2)\n@@ -149,16 +143,61 @@ def _add(x_ptr, y_ptr, output_ptr, n_elements,\n def test_elementwise(N):\n     torch.manual_seed(0)\n     ref_gpu_util = elementwise_data[DEVICE_NAME][N]\n-    cur_mem_clock = nvsmi(['clocks.current.memory'])[0]\n-    ref_mem_clock = mem_clocks[DEVICE_NAME]\n     max_gpu_perf = get_dram_gbps()\n-    assert abs(cur_mem_clock - ref_mem_clock) < 10, f'GPU memory must run at {ref_mem_clock} MHz'\n     z = torch.empty((N, ), dtype=torch.float16, device='cuda')\n     x = torch.randn_like(z)\n     y = torch.randn_like(z)\n     grid = lambda args: (triton.cdiv(N, args['BLOCK_SIZE']), )\n     fn = lambda: _add[grid](x, y, z, N, BLOCK_SIZE=1024)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=25, rep=250)\n+    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=300)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     triton.testing.assert_almost_equal(cur_gpu_util, ref_gpu_util, decimal=2)\n+\n+#######################\n+# Flash-Attention\n+#######################\n+\n+\n+flash_attention_data = {\n+    \"a100\": {\n+        (4, 48, 4096, 64, 'forward', 'float16'): 0.37,\n+        (4, 48, 4096, 64, 'backward', 'float16'): 0.25,\n+    }\n+}\n+\n+\n+@pytest.mark.parametrize(\"Z, H, N_CTX, D_HEAD\", [[4, 48, 4096, 64]])\n+@pytest.mark.parametrize(\"mode\", ['forward', 'backward'])\n+@pytest.mark.parametrize(\"dtype_str\", ['float16'])\n+def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n+    is_backward = mode == 'backward'\n+    capability = torch.cuda.get_device_capability()\n+    if capability[0] < 8:\n+        pytest.skip(\"Flash attention only supported for compute capability < 80\")\n+    torch.manual_seed(20)\n+    dtype = {'float16': torch.float16, 'float32': torch.float32, 'int8': torch.int8}[dtype_str]\n+    # init data\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n+    sm_scale = 0.2\n+    # benchmark\n+    fn = lambda: triton.ops.attention(q, k, v, sm_scale)\n+    if is_backward:\n+        o = fn()\n+        do = torch.randn_like(o)\n+        fn = lambda: o.backward(do, retain_graph=True)\n+    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=300)\n+    # compute flops\n+    flops_per_matmul = 2. * Z * H * N_CTX * N_CTX * D_HEAD * 0.5\n+    total_flops = 2 * flops_per_matmul\n+    if is_backward:\n+        total_flops *= 2.5  # 2.0(bwd) + 0.5(recompute)\n+    cur_gpu_perf = total_flops / ms * 1e-9\n+    # maximum flops\n+    cur_sm_clock = nvsmi(['clocks.current.sm'])[0]\n+    max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n+    cur_gpu_util = cur_gpu_perf / max_gpu_perf\n+    ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, mode, dtype_str)]\n+    triton.testing.assert_almost_equal(cur_gpu_util, ref_gpu_util, decimal=2)"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 9, "deletions": 4, "changes": 13, "file_content_changes": "@@ -9,6 +9,8 @@\n \n T = TypeVar('T')\n \n+TRITON_MAX_TENSOR_NUMEL = 131072\n+\n \n def _to_tensor(x, builder):\n     if isinstance(x, bool):\n@@ -254,6 +256,8 @@ def __init__(self, element_ty: dtype, shape: List):\n         self.numel = 1\n         for s in self.shape:\n             self.numel *= s\n+        if self.numel > TRITON_MAX_TENSOR_NUMEL:\n+            raise ValueError(f\"numel ({self.numel}) exceeds triton maximum tensor numel ({TRITON_MAX_TENSOR_NUMEL})\")\n \n         self.name = self.__str__()\n \n@@ -702,12 +706,13 @@ def num_programs(axis, _builder=None):\n @builtin\n def arange(start, end, _builder=None):\n     \"\"\"\n-    Returns contiguous values within the open interval [:code:`start`, :code:`end`).\n+    Returns contiguous values within the left-closed and right-open interval [:code:`start`, :code:`end`). \\\n+    End - Start must be less than or equal to TRITON_MAX_TENSOR_NUMEL = 131072\n \n     :param start: Start of the interval. Must be a power of two.\n-    :type start: int\n-    :param stop: End of the interval. Must be a power of two >= start.\n-    :type stop: int\n+    :type start: int32\n+    :param end: End of the interval. Must be a power of two > start.\n+    :type end: int32\n     \"\"\"\n     start = _constexpr_to_value(start)\n     end = _constexpr_to_value(end)"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -478,6 +478,12 @@ def not_equal(input: tl.tensor,\n def arange(start: int, end: int, builder: ir.builder) -> tl.tensor:\n     if not isinstance(start, int) or not isinstance(end, int):\n         raise ValueError(\"arange's arguments must be of type tl.constexpr\")\n+    is_start_int64 = bool(start >> 32)\n+    is_end_int64 = bool(end >> 32)\n+    if is_start_int64 or is_end_int64:\n+        raise ValueError(\"arange must fit in int32\")\n+    if end <= start:\n+        raise ValueError(\"arange's end argument must be greater than the start argument\")\n \n     shape = [end - start]\n     ret_ty = tl.block_type(tl.int32, shape)"}]