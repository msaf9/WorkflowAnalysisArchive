[{"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -57,7 +57,7 @@ def nvsmi(attrs):\n         (512, 512, 512): {'float16': 0.08, 'float32': 0.13, 'int8': 0.05},\n         (1024, 1024, 1024): {'float16': 0.33, 'float32': 0.35, 'int8': 0.169},\n         (2048, 2048, 2048): {'float16': 0.64, 'float32': 0.57, 'int8': 0.34},\n-        (4096, 4096, 4096): {'float16': 0.80, 'float32': 0.75, 'int8': 0.46},\n+        (4096, 4096, 4096): {'float16': 0.81, 'float32': 0.75, 'int8': 0.46},\n         (8192, 8192, 8192): {'float16': 0.77, 'float32': 0.85, 'int8': 0.51},\n         # tall-skinny\n         (16, 1024, 1024): {'float16': 0.0077, 'float32': 0.0127, 'int8': 0.005},\n@@ -96,7 +96,7 @@ def test_matmul(M, N, K, dtype_str):\n     ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=300)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    triton.testing.assert_almost_equal(cur_gpu_util, ref_gpu_util, decimal=2)\n+    assert triton.testing.allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n \n #######################\n@@ -149,10 +149,10 @@ def test_elementwise(N):\n     y = torch.randn_like(z)\n     grid = lambda args: (triton.cdiv(N, args['BLOCK_SIZE']), )\n     fn = lambda: _add[grid](x, y, z, N, BLOCK_SIZE=1024)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=300)\n+    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=500)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    triton.testing.assert_almost_equal(cur_gpu_util, ref_gpu_util, decimal=2)\n+    assert triton.testing.allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n #######################\n # Flash-Attention\n@@ -188,7 +188,7 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n         o = fn()\n         do = torch.randn_like(o)\n         fn = lambda: o.backward(do, retain_graph=True)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=300)\n+    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=500)\n     # compute flops\n     flops_per_matmul = 2. * Z * H * N_CTX * N_CTX * D_HEAD * 0.5\n     total_flops = 2 * flops_per_matmul\n@@ -200,4 +200,4 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n     max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, mode, dtype_str)]\n-    triton.testing.assert_almost_equal(cur_gpu_util, ref_gpu_util, decimal=2)\n+    assert triton.testing.allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 10, "deletions": 2, "changes": 12, "file_content_changes": "@@ -16,7 +16,6 @@\n import warnings\n from collections import namedtuple\n from pathlib import Path\n-from sysconfig import get_paths\n from typing import Any, Callable, Dict, Tuple, Union\n \n import setuptools\n@@ -1393,7 +1392,16 @@ def _build(name, src, srcdir):\n         cc = gcc if gcc is not None else clang\n         if cc is None:\n             raise RuntimeError(\"Failed to find C compiler. Please specify via CC environment variable.\")\n-    py_include_dir = get_paths()[\"include\"]\n+    # This function was renamed and made public in Python 3.10\n+    if hasattr(sysconfig, 'get_default_scheme'):\n+        scheme = sysconfig.get_default_scheme()\n+    else:\n+        scheme = sysconfig._get_default_scheme()\n+    # 'posix_local' is a custom scheme on Debian. However, starting Python 3.10, the default install\n+    # path changes to include 'local'. This change is required to use triton with system-wide python.\n+    if scheme == 'posix_local':\n+        scheme = 'posix_prefix'\n+    py_include_dir = sysconfig.get_paths(scheme=scheme)[\"include\"]\n \n     cc_cmd = [cc, src, \"-O3\", f\"-I{cu_include_dir}\", f\"-I{py_include_dir}\", f\"-I{srcdir}\", \"-shared\", \"-fPIC\", \"-lcuda\", \"-o\", so]\n     cc_cmd += [f\"-L{dir}\" for dir in cuda_lib_dirs]"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 7, "deletions": 4, "changes": 11, "file_content_changes": "@@ -93,20 +93,23 @@ def assert_almost_equal(x, y, decimal=2, err_msg=''):\n     npt.assert_array_almost_equal(x, y, err_msg=err_msg, decimal=decimal)\n \n \n-def allclose(x, y, tol=1e-2):\n+def allclose(x, y, atol=0, rtol=1e-2):\n+    if not isinstance(x, torch.Tensor):\n+        x = torch.tensor(x)\n+    if not isinstance(y, torch.Tensor):\n+        y = torch.tensor(y)\n     if x.dtype != y.dtype:\n         raise RuntimeError(f'{x.dtype} did not match with {x.dtype}')\n     if x.shape != y.shape:\n         raise RuntimeError(f'{x.shape} did not match with {y.shape}')\n     if x.dtype == torch.bool:\n         return torch.sum(x ^ y) == 0\n     if x.dtype in [torch.int8, torch.int16, torch.int32, torch.int64]:\n-        tol = 0\n+        rtol = 0\n     diff = abs(x - y)\n     x_max = torch.max(x)\n     y_max = torch.max(y)\n-    err = torch.max(diff) / torch.max(x_max, y_max)\n-    return err <= tol\n+    return torch.max(diff) <= atol + rtol * torch.max(x_max, y_max)\n \n \n def nvsmi(attrs):"}]