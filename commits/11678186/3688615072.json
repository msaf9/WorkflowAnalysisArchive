[{"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 16, "deletions": 5, "changes": 21, "file_content_changes": "@@ -585,13 +585,15 @@ class RematerializeForward : public mlir::RewritePattern {\n // -----------------------------------------------------------------------------\n namespace {\n int computeCapabilityToMMAVersion(int computeCapability) {\n-  if (computeCapability < 80) {\n+  if (computeCapability < 70) {\n+    return 0;\n+  } else if (computeCapability < 80) {\n     return 1;\n   } else if (computeCapability < 90) {\n     return 2;\n   } else {\n     assert(false && \"computeCapability > 90 not supported\");\n-    return 0;\n+    return 3;\n   }\n }\n \n@@ -792,16 +794,25 @@ class BlockedToMMA : public mlir::RewritePattern {\n \n     auto A = dotOp.getOperand(0).getType().cast<RankedTensorType>();\n     auto B = dotOp.getOperand(1).getType().cast<RankedTensorType>();\n+\n+\n     // for FMA, should retain the blocked layout.\n-    if (A.getElementType().isF32() && B.getElementType().isF32() &&\n-        (computeCapability < 75 || !dotOp.allowTF32()))\n+    int version = computeCapabilityToMMAVersion(computeCapability);\n+    auto aElemTy = A.getElementType();\n+    auto bElemTy = B.getElementType();\n+    bool useTensorCores = (aElemTy.isF16() && bElemTy.isF16()) ||\n+           (aElemTy.isBF16() && bElemTy.isBF16()) ||\n+           (aElemTy.isF32() && bElemTy.isF32() && dotOp.allowTF32() &&\n+            version >= 2) ||\n+           (aElemTy.isInteger(8) && bElemTy.isInteger(8) &&\n+            version >= 2);\n+    if (!useTensorCores)\n       return failure();\n \n     // get MMA encoding for the given number of warps\n     auto retShape = oldRetType.getShape();\n     auto mod = op->getParentOfType<mlir::ModuleOp>();\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n-    int version = computeCapabilityToMMAVersion(computeCapability);\n \n     auto newRetType = RankedTensorType::get(\n         retShape, oldRetType.getElementType(),"}]