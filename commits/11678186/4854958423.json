[{"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "file_content_changes": "@@ -32,8 +32,14 @@ SmallVector<unsigned> getWarpsPerCTA(Attribute layout);\n SmallVector<unsigned> getSizePerThread(Attribute layout);\n \n SmallVector<unsigned> getContigPerThread(Attribute layout);\n-SmallVector<unsigned> getThreadsPerWarpWithUniqueData(Attribute layout);\n-SmallVector<unsigned> getWarpsPerCTAWithUniqueData(Attribute layout);\n+\n+SmallVector<unsigned>\n+getThreadsPerWarpWithUniqueData(Attribute layout,\n+                                ArrayRef<int64_t> tensorShape);\n+\n+SmallVector<unsigned>\n+getWarpsPerCTAWithUniqueData(Attribute layout, ArrayRef<int64_t> tensorShape);\n+\n SmallVector<unsigned> getUniqueContigPerThread(Type type);\n \n SmallVector<unsigned> getThreadsPerCTA(Attribute layout);"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 10, "deletions": 8, "changes": 18, "file_content_changes": "@@ -16,22 +16,24 @@ bool ReduceOpHelper::isFastReduction() {\n unsigned ReduceOpHelper::getInterWarpSize() {\n   auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n   unsigned sizeIntraWarps = getIntraWarpSize();\n-  return std::min(\n-      srcReduceDimSize / sizeIntraWarps,\n-      triton::gpu::getWarpsPerCTAWithUniqueData(getSrcLayout())[axis]);\n+  return std::min(srcReduceDimSize / sizeIntraWarps,\n+                  triton::gpu::getWarpsPerCTAWithUniqueData(\n+                      getSrcLayout(), getSrcShape())[axis]);\n }\n \n unsigned ReduceOpHelper::getIntraWarpSize() {\n   auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n-  return std::min(\n-      srcReduceDimSize,\n-      triton::gpu::getThreadsPerWarpWithUniqueData(getSrcLayout())[axis]);\n+  return std::min(srcReduceDimSize,\n+                  triton::gpu::getThreadsPerWarpWithUniqueData(\n+                      getSrcLayout(), getSrcShape())[axis]);\n }\n \n unsigned ReduceOpHelper::getThreadsReductionAxis() {\n   auto srcLayout = getSrcLayout();\n-  return triton::gpu::getThreadsPerWarpWithUniqueData(srcLayout)[axis] *\n-         triton::gpu::getWarpsPerCTAWithUniqueData(srcLayout)[axis];\n+  auto srcShape = getSrcShape();\n+  return triton::gpu::getThreadsPerWarpWithUniqueData(srcLayout,\n+                                                      srcShape)[axis] *\n+         triton::gpu::getWarpsPerCTAWithUniqueData(srcLayout, srcShape)[axis];\n }\n \n SmallVector<unsigned> ReduceOpHelper::getScratchConfigBasic() {"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 38, "deletions": 14, "changes": 52, "file_content_changes": "@@ -94,15 +94,26 @@ SmallVector<unsigned> getThreadsPerWarp(Attribute layout) {\n   return {};\n }\n \n-SmallVector<unsigned> getThreadsPerWarpWithUniqueData(Attribute layout) {\n+SmallVector<unsigned>\n+getThreadsPerWarpWithUniqueData(Attribute layout,\n+                                ArrayRef<int64_t> tensorShape) {\n   if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n-    auto parent = sliceLayout.getParent();\n-    auto parentThreadsPerWarp = getThreadsPerWarpWithUniqueData(parent);\n+    auto parentLayout = sliceLayout.getParent();\n+    auto parentShape = sliceLayout.paddedShape(tensorShape);\n+    auto parentThreadsPerWarp =\n+        getThreadsPerWarpWithUniqueData(parentLayout, parentShape);\n     SmallVector<unsigned> threadsPerWarp = parentThreadsPerWarp;\n     threadsPerWarp.erase(threadsPerWarp.begin() + sliceLayout.getDim());\n     return threadsPerWarp;\n   }\n-  return getThreadsPerWarp(layout);\n+  auto threadsPerWarp = getThreadsPerWarp(layout);\n+  assert(threadsPerWarp.size() == tensorShape.size() &&\n+         \"layout and tensor shape must have the same rank\");\n+  for (unsigned i = 0; i < threadsPerWarp.size(); i++) {\n+    threadsPerWarp[i] = std::min<unsigned>(threadsPerWarp[i], tensorShape[i]);\n+  }\n+\n+  return threadsPerWarp;\n }\n \n SmallVector<unsigned> getWarpsPerCTA(Attribute layout) {\n@@ -127,15 +138,28 @@ SmallVector<unsigned> getWarpsPerCTA(Attribute layout) {\n   return {};\n }\n \n-SmallVector<unsigned> getWarpsPerCTAWithUniqueData(Attribute layout) {\n+SmallVector<unsigned>\n+getWarpsPerCTAWithUniqueData(Attribute layout, ArrayRef<int64_t> tensorShape) {\n   if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n-    auto parent = sliceLayout.getParent();\n-    auto parentWarpsPerCTA = getWarpsPerCTAWithUniqueData(parent);\n+    auto parentLayout = sliceLayout.getParent();\n+    auto parentShape = sliceLayout.paddedShape(tensorShape);\n+    auto parentWarpsPerCTA =\n+        getWarpsPerCTAWithUniqueData(parentLayout, parentShape);\n     SmallVector<unsigned> warpsPerCTA = parentWarpsPerCTA;\n     warpsPerCTA.erase(warpsPerCTA.begin() + sliceLayout.getDim());\n     return warpsPerCTA;\n   }\n-  return getWarpsPerCTA(layout);\n+  auto warpsPerCTA = getWarpsPerCTA(layout);\n+  assert(warpsPerCTA.size() == tensorShape.size() &&\n+         \"layout and tensor shape must have the same rank\");\n+  for (unsigned i = 0; i < warpsPerCTA.size(); i++) {\n+    auto sizePerWarp =\n+        getSizePerThread(layout)[i] * getThreadsPerWarp(layout)[i];\n+    auto maxWarpsPerDim = ceil<unsigned>(tensorShape[i], sizePerWarp);\n+    warpsPerCTA[i] = std::min<unsigned>(warpsPerCTA[i], maxWarpsPerDim);\n+  }\n+\n+  return warpsPerCTA;\n }\n \n SmallVector<unsigned> getSizePerThread(Attribute layout) {\n@@ -1114,9 +1138,9 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n       return mlir::failure();\n     }\n     auto newType = op->getResult(0).getType().cast<RankedTensorType>();\n-    // Ensure that the new insert_slice op is placed in the same place as the\n-    // old insert_slice op. Otherwise, the new insert_slice op may be placed\n-    // after the async_wait op, which is not allowed.\n+    // Ensure that the new insert_slice op is placed in the same place as\n+    // the old insert_slice op. Otherwise, the new insert_slice op may be\n+    // placed after the async_wait op, which is not allowed.\n     OpBuilder::InsertionGuard guard(rewriter);\n     rewriter.setInsertionPoint(insert_slice);\n     auto newArg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n@@ -1144,9 +1168,9 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n     auto resType = RankedTensorType::get(\n         origResType.getShape(), origResType.getElementType(),\n         extract_slice.getType().cast<RankedTensorType>().getEncoding());\n-    // Ensure that the new extract_slice op is placed in the same place as the\n-    // old extract_slice op. Otherwise, the new extract_slice op may be placed\n-    // after the async_wait op, which is not allowed.\n+    // Ensure that the new extract_slice op is placed in the same place as\n+    // the old extract_slice op. Otherwise, the new extract_slice op may be\n+    // placed after the async_wait op, which is not allowed.\n     OpBuilder::InsertionGuard guard(rewriter);\n     rewriter.setInsertionPoint(extract_slice);\n     auto newArg = rewriter.create<triton::gpu::ConvertLayoutOp>("}]