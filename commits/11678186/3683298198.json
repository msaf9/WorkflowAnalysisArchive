[{"filename": "lib/Conversion/TritonGPUToLLVM/DotHelpers.h", "status": "modified", "additions": 22, "deletions": 39, "changes": 61, "file_content_changes": "@@ -100,50 +100,42 @@ struct DotOpMmaV1ConversionHelper {\n   }\n \n   // Get the number of fp16x2 elements for $a.\n-  // \\param shapeTransed: A's shape or reordered shape if transpose needed.\n-  // \\param orderTransed: the order or reordered order if transpose needed.\n-  unsigned getNumM(ArrayRef<int64_t> shapeTransed, bool isARow) const {\n-    AParam param(isARow, shapeTransed);\n+  unsigned getNumM(ArrayRef<int64_t> shape, bool isARow) const {\n+    AParam param(isARow, shape);\n \n-    unsigned numM = param.rep[0] * shapeTransed[0] / (param.spw[0] * wpt[0]);\n+    unsigned numM = param.rep[0] * shape[0] / (param.spw[0] * wpt[0]);\n     return numM;\n   }\n \n   // Get the number of fp16x2 elements for $b.\n-  // \\param shapeTransed: B' shape or reordered shape if transpose needed.\n-  // \\param orderTransed: the order or reordered order if transpose needed.\n-  unsigned getNumN(ArrayRef<int64_t> shapeTransed, bool isBRow) const {\n-    BParam param(isBRow, shapeTransed);\n+  unsigned getNumN(ArrayRef<int64_t> shape, bool isBRow) const {\n+    BParam param(isBRow, shape);\n \n-    unsigned numN = param.rep[1] * shapeTransed[1] / (param.spw[1] * wpt[1]);\n+    unsigned numN = param.rep[1] * shape[1] / (param.spw[1] * wpt[1]);\n     return numN;\n   }\n \n-  int numElemsPerThreadA(ArrayRef<int64_t> shapeTransed, bool isRow,\n-                         int vec) const {\n-    int numM = getNumM(shapeTransed, isRow);\n-    int NK = shapeTransed[1];\n+  int numElemsPerThreadA(ArrayRef<int64_t> shape, bool isRow, int vec) const {\n+    int numM = getNumM(shape, isRow);\n+    int NK = shape[1];\n     int elemsPerLd = vec > 4 ? 4 : 2;\n     return (numM / 2) * (NK / 4) * elemsPerLd;\n   }\n \n-  int numElemsPerThreadB(ArrayRef<int64_t> shapeTransed, bool isRow,\n-                         int vec) const {\n-    unsigned numN = getNumN(shapeTransed, isRow);\n-    int NK = shapeTransed[0];\n+  int numElemsPerThreadB(ArrayRef<int64_t> shape, bool isRow, int vec) const {\n+    unsigned numN = getNumN(shape, isRow);\n+    int NK = shape[0];\n     int elemsPerLd = vec > 4 ? 4 : 2;\n     return (numN / 2) * (NK / 4) * elemsPerLd;\n   }\n \n   // Loading $a from smem to registers, returns a LLVM::Struct.\n-  Value loadA(Value A, bool transA, const SharedMemoryObject &smemObj,\n-              Value thread, Location loc,\n-              ConversionPatternRewriter &rewriter) const;\n+  Value loadA(Value A, const SharedMemoryObject &smemObj, Value thread,\n+              Location loc, ConversionPatternRewriter &rewriter) const;\n \n   // Loading $b from smem to registers, returns a LLVM::Struct.\n-  Value loadB(Value B, bool transB, const SharedMemoryObject &smemObj,\n-              Value thread, Location loc,\n-              ConversionPatternRewriter &rewriter) const;\n+  Value loadB(Value B, const SharedMemoryObject &smemObj, Value thread,\n+              Location loc, ConversionPatternRewriter &rewriter) const;\n \n   static ArrayRef<unsigned> getOrder() { return mmaOrder; }\n \n@@ -1335,8 +1327,8 @@ struct DotOpFMAConversionHelper {\n };\n \n Value DotOpMmaV1ConversionHelper::loadA(\n-    Value tensor, bool transA, const SharedMemoryObject &smemObj, Value thread,\n-    Location loc, ConversionPatternRewriter &rewriter) const {\n+    Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n+    ConversionPatternRewriter &rewriter) const {\n \n   auto *ctx = rewriter.getContext();\n   auto tensorTy = tensor.getType().cast<RankedTensorType>();\n@@ -1355,11 +1347,6 @@ Value DotOpMmaV1ConversionHelper::loadA(\n   auto [offsetAM, offsetAK, _0, _1] = computeOffsets(\n       thread, isARow, false, fpw, param.spw, param.rep, rewriter, loc);\n \n-  if (transA) {\n-    std::swap(shape[0], shape[1]);\n-    std::swap(offsetAM, offsetAK);\n-    std::swap(order[0], order[1]);\n-  }\n   int vecA = sharedLayout.getVec();\n \n   auto strides = smemObj.strides;\n@@ -1423,7 +1410,7 @@ Value DotOpMmaV1ConversionHelper::loadA(\n     ld(has, m, k, ha00, ha01);\n \n     if (vecA > 4) {\n-      assert(false); // DEBUG\n+      assert(false) << \"vecA > 4 is not supported yet\";\n       Value ha10 = bitcast(extract_element(ha, i32_val(2)), f16x2Ty);\n       Value ha11 = bitcast(extract_element(ha, i32_val(3)), f16x2Ty);\n       if (isARow)\n@@ -1451,8 +1438,8 @@ Value DotOpMmaV1ConversionHelper::loadA(\n }\n \n Value DotOpMmaV1ConversionHelper::loadB(\n-    Value tensor, bool transB, const SharedMemoryObject &smemObj, Value thread,\n-    Location loc, ConversionPatternRewriter &rewriter) const {\n+    Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n+    ConversionPatternRewriter &rewriter) const {\n   // smem\n   auto strides = smemObj.strides;\n \n@@ -1479,11 +1466,6 @@ Value DotOpMmaV1ConversionHelper::loadB(\n \n   auto [_0, _1, offsetBN, offsetBK] = computeOffsets(\n       thread, false, isBRow, fpw, param.spw, param.rep, rewriter, loc);\n-  if (transB) {\n-    std::swap(order[0], order[1]);\n-    std::swap(shape[0], shape[1]);\n-    std::swap(offsetBK, offsetBN);\n-  }\n \n   // swizzling\n   int perPhaseB = sharedLayout.getPerPhase();\n@@ -1536,6 +1518,7 @@ Value DotOpMmaV1ConversionHelper::loadB(\n     Value hb01 = bitcast(extract_element(hb, i32_val(1)), f16x2Ty);\n     ld(hbs, n, K, hb00, hb01);\n     if (vecB > 4) {\n+      assert(false) << \"vecA > 4 is not supported yet\";\n       Value hb10 = bitcast(extract_element(hb, i32_val(2)), f16x2Ty);\n       Value hb11 = bitcast(extract_element(hb, i32_val(3)), f16x2Ty);\n       if (isBRow)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 6, "deletions": 9, "changes": 15, "file_content_changes": "@@ -3446,15 +3446,11 @@ Value ConvertLayoutOpConversion::lowerSharedToDotOperandMMA(\n     }\n \n     if (dotOperandLayout.getOpIdx() == 0) { // operand $a\n-      // TODO[Superjomn]: transA is not available here.\n-      bool transA = false;\n-      res = helper.loadA(src, transA, smemObj, getThreadId(rewriter, loc), loc,\n-                         rewriter);\n+      res =\n+          helper.loadA(src, smemObj, getThreadId(rewriter, loc), loc, rewriter);\n     } else if (dotOperandLayout.getOpIdx() == 1) { // operand $b\n-      // TODO[Superjomn]: transB is not available here.\n-      bool transB = false;\n-      res = helper.loadB(src, transB, smemObj, getThreadId(rewriter, loc), loc,\n-                         rewriter);\n+      res =\n+          helper.loadB(src, smemObj, getThreadId(rewriter, loc), loc, rewriter);\n     }\n   } else {\n     assert(false && \"Unsupported mma layout found\");\n@@ -3948,7 +3944,8 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n             std::swap(order[0], order[1]);\n           }\n \n-          // TODO[Superjomn] vec is not available here.\n+          // TODO[Superjomn] vec is not available here, but currently the vec\n+          // seems always no greater than 4, so we simply assign 4 here.\n           bool vec = 4;\n           if (dotOpLayout.getOpIdx() == 0) { // $a\n             int elems = helper.numElemsPerThreadA(shape, order[0] == 1, vec);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -802,7 +802,6 @@ class BlockedToMMA : public mlir::RewritePattern {\n     auto mod = op->getParentOfType<mlir::ModuleOp>();\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n     int version = computeCapabilityToMMAVersion(computeCapability);\n-    version = 1;\n \n     auto newRetType = RankedTensorType::get(\n         retShape, oldRetType.getElementType(),"}]