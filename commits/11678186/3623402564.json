[{"filename": "include/triton/Analysis/AxisInfo.h", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -131,6 +131,12 @@ class AxisInfoAnalysis : public ForwardDataFlowAnalysis<AxisInfo> {\n   ChangeResult\n   visitOperation(Operation *op,\n                  ArrayRef<LatticeElement<AxisInfo> *> operands) override;\n+\n+  unsigned getPtrVectorSize(Value ptr);\n+\n+  unsigned getPtrAlignment(Value ptr);\n+\n+  unsigned getMaskAlignment(Value mask);\n };\n \n } // namespace mlir"}, {"filename": "include/triton/Analysis/Membar.h", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -29,7 +29,11 @@ class MembarAnalysis {\n   /// The following circumstances are not considered yet:\n   /// - Double buffers\n   /// - N buffers\n-  MembarAnalysis(Allocation *allocation) : allocation(allocation) { run(); }\n+  MembarAnalysis(Allocation *allocation) : allocation(allocation) {}\n+\n+  /// Runs the membar analysis to the given operation, inserts a barrier if\n+  /// necessary.\n+  void run();\n \n private:\n   struct RegionInfo {\n@@ -82,10 +86,6 @@ class MembarAnalysis {\n     }\n   };\n \n-  /// Runs the membar analysis to the given operation, inserts a barrier if\n-  /// necessary.\n-  void run();\n-\n   /// Applies the barrier analysis based on the SCF dialect, in which each\n   /// region has a single basic block only.\n   /// Example:"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -295,6 +295,18 @@ def TT_CatOp : TT_Op<\"cat\", [NoSideEffect,\n     let assemblyFormat = \"$lhs `,` $rhs attr-dict `:` functional-type(operands, results)\";\n }\n \n+def TT_TransOp : TT_Op<\"trans\", [NoSideEffect,\n+                               SameOperandsAndResultElementType]> {\n+\n+    let summary = \"transpose a tensor\";\n+\n+    let arguments = (ins TT_Tensor:$src);\n+\n+    let results = (outs TT_Tensor:$result);\n+\n+    let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n+}\n+\n //\n // SPMD Ops\n //"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -25,11 +25,13 @@ namespace gpu {\n \n unsigned getElemsPerThread(Type type);\n \n-SmallVector<unsigned> getThreadsPerWarp(Attribute layout);\n+SmallVector<unsigned> getThreadsPerWarp(const Attribute &layout);\n \n-SmallVector<unsigned> getWarpsPerCTA(Attribute layout);\n+SmallVector<unsigned> getWarpsPerCTA(const Attribute &layout);\n \n-SmallVector<unsigned> getSizePerThread(Attribute layout);\n+SmallVector<unsigned> getSizePerThread(const Attribute &layout);\n+\n+SmallVector<unsigned> getContigPerThread(Attribute layout);\n \n SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout);\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 31, "deletions": 27, "changes": 58, "file_content_changes": "@@ -92,12 +92,16 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n         unsigned inner = (opIdx == 0) ? 0 : 1;\n \n         // ---- begin version 1 ----\n-        // TODO: handle rep (see\n-        // https://github.com/openai/triton/blob/master/lib/codegen/analysis/layout.cc#L209)\n         if (version == 1) {\n+          bool is_row = order[0] != 0;\n+          bool is_vec4 = opIdx == 0 ? is_row && (shape[order[0]] <= 16) :\n+                                      !is_row && (shape[order[0]] <= 16);\n+          int pack_size = opIdx == 0 ? ((is_row || is_vec4) ? 1 : 2) :\n+                                       ((is_row && !is_vec4) ? 2 : 1);\n+          int rep = 2 * pack_size;\n           int maxPhase = (order[inner] == 1 ? 8 : 4) / perPhase;\n-          return $_get(context, 1, perPhase, maxPhase, order);\n-        } \n+          return $_get(context, 2 * rep, perPhase, maxPhase, order);\n+        }\n \n         // ---- begin version 2 ----\n         if (version == 2) {\n@@ -293,37 +297,37 @@ partitioned between warps.\n // -------------------------------- version = 1 --------------------------- //\n \n For first-gen tensor cores, the implicit warpTileSize is [16, 16].\n-Information about this layout can be found in the official PTX documentation\n+Note: the layout is different from the recommended in PTX ISA\n https://docs.nvidia.com/cuda/parallel-thread-execution/index.html\n (mma.884 section, FP32 accumulator).\n \n For example, the matrix L corresponding to blockTileSize=[32,16] is:\n \n                                warp 0\n --------------------------------/\\-------------------------------\n-[ 0   0   2   2   0   0   2   2    4   4   6   6   4   4   6   6 ]\n-[ 1   1   3   3   1   1   3   3    5   5   7   7   5   5   7   7 ]\n-[ 0   0   2   2   0   0   2   2    4   4   6   6   4   4   6   6 ]\n-[ 1   1   3   3   1   1   3   3    5   5   7   7   5   5   7   7 ]\n-[ 16  16  18  18  16  16  18  18   20  20  22  22  20  20  22  22]\n-[ 17  17  19  19  17  17  19  19   21  21  23  23  21  21  23  23]\n-[ 16  16  18  18  16  16  18  18   20  20  22  22  20  20  22  22]\n-[ 17  17  19  19  17  17  19  19   21  21  23  23  21  21  23  23]\n-[ 8   8   10  10  8   8   10  10   12  12  14  14  12  12  14  14]\n-[ 9   9   11  11  9   9   11  11   13  13  15  15  13  13  15  15]\n-[ ..............................................................\n-[ ..............................................................\n-[ 24  24  26  26  24  24  26  26   28  28  30  30  28  28  30  30]\n-[ 25  25  27  27  25  25  27  27   29  29  31  31  29  29  31  31]\n-\n-                         warp 1 = warp0 + 32\n+[ 0   0   2   2   8   8   10  10   0   0   2   2   8   8   10  10 ]\n+[ 1   1   3   3   9   9   11  11   1   1   3   3   9   9   11  11 ]\n+[ 0   0   2   2   8   8   10  10   0   0   2   2   8   8   10  10 ]\n+[ 1   1   3   3   9   9   11  11   1   1   3   3   9   9   11  11 ]\n+[ 4   4   6   6   12  12  14  14   4   4   6   6   12  12  14  14 ]\n+[ 5   5   7   7   13  13  15  15   5   5   7   7   13  13  15  15 ]\n+[ 4   4   6   6   12  12  14  14   4   4   6   6   12  12  14  14 ]\n+[ 5   5   7   7   13  13  15  15   5   5   7   7   13  13  15  15 ]\n+[ 16  16  18  18  20  20  22  22   16  16  18  18  20  20  22  22 ]\n+[ 17  17  19  19  21  21  23  23   17  17  19  19  21  21  23  23 ]\n+[ 16  16  18  18  20  20  22  22   16  16  18  18  20  20  22  22 ]\n+[ 17  17  19  19  21  21  23  23   17  17  19  19  21  21  23  23 ]\n+[ 24  24  26  26  28  28  30  30   24  24  26  26  28  28  30  30 ]\n+[ 25  25  27  27  29  29  31  31   25  25  27  27  29  29  31  31 ]\n+[ 24  24  26  26  28  28  30  30   24  24  26  26  28  28  30  30 ]\n+[ 25  25  27  27  29  29  31  31   25  25  27  27  29  29  31  31 ]\n+\n+                          warp 1 = warp0 + 32\n --------------------------------/\\-------------------------------\n-[ 32  32  34  34  32  32  34  34   36  36  38  38  36  36  38  38]\n-[ 33  33  35  35  33  33  35  35   37  37  39  39  37  37  39  39]\n-[ ..............................................................\n-[ ..............................................................\n-[ 56  56  58  58  56  56  58  58   60  60  62  62  60  60  62  62]\n-[ 57  57  59  59  57  57  59  59   61  61  63  63  61  61  63  63]\n+[ 32  32  34  34  40  40  42  42   32  32  34  34  40  40  42  42 ]\n+[ 33  33  35  35  41  41  43  43   33  33  35  35  41  41  43  43 ]\n+[ ............................................................... ]\n+\n \n // -------------------------------- version = 2 --------------------------- //\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -151,6 +151,10 @@ def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n   //  attr-dict `:` type($src) `->` type($dst)\n   //}];\n \n+  let extraClassDeclaration = [{\n+      static DenseSet<unsigned> getEligibleLoadByteWidth(int computeCapability);\n+  }];\n+\n   // The custom parser could be replaced with oilist in LLVM-16\n   let parser = [{ return parseInsertSliceAsyncOp(parser, result); }];\n "}, {"filename": "lib/Analysis/Alias.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -26,7 +26,7 @@ ChangeResult SharedMemoryAliasAnalysis::visitOperation(\n     // These ops may allocate a new shared memory buffer.\n     auto result = op->getResult(0);\n     // FIXME(Keren): extract and insert are always alias for now\n-    if (isa<tensor::ExtractSliceOp>(op)) {\n+    if (isa<tensor::ExtractSliceOp, triton::TransOp>(op)) {\n       // extract_slice %src\n       aliasInfo = AliasInfo(operands[0]->getValue());\n       pessimistic = false;"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -13,6 +13,7 @@\n \n using ::mlir::triton::gpu::BlockedEncodingAttr;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::getContigPerThread;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::getSizePerThread;\n@@ -60,8 +61,8 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   assert(srcLayout && dstLayout &&\n          \"Unexpect layout in getScratchConfigForCvtLayout()\");\n   auto [inOrd, outOrd] = getCvtOrder(srcLayout, dstLayout);\n-  unsigned srcContigPerThread = getSizePerThread(srcLayout)[inOrd[0]];\n-  unsigned dstContigPerThread = getSizePerThread(dstLayout)[outOrd[0]];\n+  unsigned srcContigPerThread = getContigPerThread(srcLayout)[inOrd[0]];\n+  unsigned dstContigPerThread = getContigPerThread(dstLayout)[outOrd[0]];\n   // TODO: Fix the legacy issue that ourOrd[0] == 0 always means\n   //       that we cannot do vectorization.\n   inVec = outOrd[0] == 0 ? 1 : inOrd[0] == 0 ? 1 : srcContigPerThread;"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 57, "deletions": 0, "changes": 57, "file_content_changes": "@@ -132,6 +132,7 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n           AxisInfo::DimVectorT(ty.getShape().begin(), ty.getShape().end()));\n     }\n   }\n+  // TODO: refactor & complete binary ops\n   // Addition\n   if (llvm::isa<arith::AddIOp, triton::AddPtrOp>(op)) {\n     auto newContiguity = [&](AxisInfo lhs, AxisInfo rhs, int d) {\n@@ -159,6 +160,20 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n     curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n                          newContiguity, newDivisibility, newConstancy);\n   }\n+  // Remainder\n+  if (llvm::isa<arith::RemSIOp, arith::RemUIOp>(op)) {\n+    auto newContiguity = [](AxisInfo lhs, AxisInfo rhs, int d) {\n+      return gcd(lhs.getContiguity(d), rhs.getDivisibility(d));\n+    };\n+    auto newDivisibility = [](AxisInfo lhs, AxisInfo rhs, int d) {\n+      return gcd(lhs.getDivisibility(d), rhs.getDivisibility(d));\n+    };\n+    auto newConstancy = [](AxisInfo lhs, AxisInfo rhs, int d) {\n+      return gcd(lhs.getConstancy(d), rhs.getConstancy(d));\n+    };\n+    curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n+                         newContiguity, newDivisibility, newConstancy);\n+  }\n   // TODO: All other binary ops\n   if (llvm::isa<arith::AndIOp, arith::OrIOp>(op)) {\n     auto newContiguity = [](AxisInfo lhs, AxisInfo rhs, int d) { return 1; };\n@@ -261,4 +276,46 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n   return result;\n }\n \n+unsigned AxisInfoAnalysis::getPtrVectorSize(Value ptr) {\n+  auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n+  if (!tensorTy)\n+    return 1;\n+  auto layout = tensorTy.getEncoding();\n+  auto shape = tensorTy.getShape();\n+\n+  // Here order should be ordered by contiguous first, so the first element\n+  // should have the largest contiguous.\n+  auto order = triton::gpu::getOrder(layout);\n+  unsigned align = getPtrAlignment(ptr);\n+\n+  unsigned contigPerThread = triton::gpu::getSizePerThread(layout)[order[0]];\n+  unsigned vec = std::min(align, contigPerThread);\n+  vec = std::min<unsigned>(shape[order[0]], vec);\n+\n+  return vec;\n+}\n+\n+unsigned AxisInfoAnalysis::getPtrAlignment(Value ptr) {\n+  auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n+  if (!tensorTy)\n+    return 1;\n+  auto axisInfo = lookupLatticeElement(ptr)->getValue();\n+  auto layout = tensorTy.getEncoding();\n+  auto order = triton::gpu::getOrder(layout);\n+  unsigned maxMultiple = axisInfo.getDivisibility(order[0]);\n+  unsigned maxContig = axisInfo.getContiguity(order[0]);\n+  unsigned alignment = std::min(maxMultiple, maxContig);\n+  return alignment;\n+}\n+\n+unsigned AxisInfoAnalysis::getMaskAlignment(Value mask) {\n+  auto tensorTy = mask.getType().dyn_cast<RankedTensorType>();\n+  if (!tensorTy)\n+    return 1;\n+  auto maskOrder = triton::gpu::getOrder(tensorTy.getEncoding());\n+  auto maskAxis = lookupLatticeElement(mask)->getValue();\n+  auto alignment = std::max<unsigned>(maskAxis.getConstancy(maskOrder[0]), 1);\n+  return alignment;\n+}\n+\n } // namespace mlir"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 40, "deletions": 17, "changes": 57, "file_content_changes": "@@ -24,21 +24,43 @@ void MembarAnalysis::dfsOperation(Operation *operation,\n     // scf.if only: two regions\n     // scf.for: one region\n     RegionInfo curRegionInfo;\n-    for (auto &region : operation->getRegions()) {\n-      // Copy the parent info as the current info.\n-      RegionInfo regionInfo = *parentRegionInfo;\n-      for (auto &block : region.getBlocks()) {\n-        assert(region.getBlocks().size() == 1 &&\n-               \"Multiple blocks in a region is not supported\");\n-        for (auto &op : block.getOperations()) {\n-          // Traverse the nested operation.\n-          dfsOperation(&op, &regionInfo, builder);\n+    auto traverseRegions = [&]() -> auto{\n+      for (auto &region : operation->getRegions()) {\n+        // Copy the parent info as the current info.\n+        RegionInfo regionInfo = *parentRegionInfo;\n+        for (auto &block : region.getBlocks()) {\n+          assert(region.getBlocks().size() == 1 &&\n+                 \"Multiple blocks in a region is not supported\");\n+          for (auto &op : block.getOperations()) {\n+            // Traverse the nested operation.\n+            dfsOperation(&op, &regionInfo, builder);\n+          }\n         }\n+        curRegionInfo.join(regionInfo);\n       }\n-      curRegionInfo.join(regionInfo);\n+      // Set the parent region info as the union of the nested region info.\n+      *parentRegionInfo = curRegionInfo;\n+    };\n+\n+    traverseRegions();\n+    if (isa<scf::ForOp>(operation)) {\n+      // scf.for can have two possible inputs: the init value and the\n+      // previous iteration's result. Although we've applied alias analysis,\n+      // there could be unsynced memory accesses on reused memories.\n+      // For example, consider the following code:\n+      // %1 = convert_layout %0: blocked -> shared\n+      // ...\n+      // gpu.barrier\n+      // ...\n+      // %5 = convert_layout %4 : shared -> dot\n+      // %6 = tt.dot %2, %5\n+      // scf.yield\n+      //\n+      // Though %5 could be released before scf.yield, it may shared the same\n+      // memory with %1. So we actually have to insert a barrier before %1 to\n+      // make sure the memory is synced.\n+      traverseRegions();\n     }\n-    // Set the parent region info as the union of the nested region info.\n-    *parentRegionInfo = curRegionInfo;\n   }\n }\n \n@@ -49,8 +71,7 @@ void MembarAnalysis::transfer(Operation *op, RegionInfo *regionInfo,\n     // Do not insert barriers before control flow operations and\n     // alloc/extract/insert\n     // alloc is an allocation op without memory write.\n-    // In contrast, arith.constant is an allocation op with memory write.\n-    // FIXME(Keren): extract is always alias for now\n+    // FIXME(Keren): extract_slice is always alias for now\n     return;\n   }\n \n@@ -60,9 +81,11 @@ void MembarAnalysis::transfer(Operation *op, RegionInfo *regionInfo,\n     return;\n   }\n \n-  if (isa<triton::gpu::AsyncWaitOp>(op)) {\n-    // If the current op is an async wait, we insert a barrier op and sync\n-    // previous reads and writes.\n+  if (isa<triton::gpu::AsyncWaitOp>(op) &&\n+      !isa<gpu::BarrierOp>(op->getNextNode())) {\n+    // If the current op is an async wait and the next op is not a barrier we\n+    // insert a barrier op and sync\n+    regionInfo->sync();\n     OpBuilder::InsertionGuard g(*builder);\n     builder->setInsertionPointAfter(op);\n     builder->create<gpu::BarrierOp>(op->getLoc());"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 1, "deletions": 6, "changes": 7, "file_content_changes": "@@ -58,11 +58,6 @@ SmallVector<SmallVector<unsigned>> ReduceOpHelper::getScratchConfigsFast() {\n   unsigned numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n   smemShapes[1].push_back(numWarps * 32);\n \n-  /// FIXME(Qingyi): This requirement is actually not necessary, because it is\n-  /// always smaller than smemShapes[0] shared memory block2\n-  smemShapes[2] = convertType<unsigned>(getSrcShape());\n-  smemShapes[2].erase(smemShapes[2].begin() + axis);\n-\n   return smemShapes;\n }\n \n@@ -110,7 +105,7 @@ bool maybeSharedAllocationOp(Operation *op) {\n }\n \n bool maybeAliasOp(Operation *op) {\n-  return isa<tensor::ExtractSliceOp>(op) ||\n+  return isa<tensor::ExtractSliceOp>(op) || isa<triton::TransOp>(op) ||\n          isa<triton::gpu::InsertSliceAsyncOp>(op) ||\n          isa<tensor::InsertSliceOp>(op);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotHelpers.h", "status": "modified", "additions": 13, "deletions": 11, "changes": 24, "file_content_changes": "@@ -708,19 +708,19 @@ class MMA16816SmemLoader {\n       Type elemTy = type::f32Ty(ctx);\n       Type elemPtrTy = ptr_ty(elemTy);\n       if (kOrder == 1) {\n-        elems[0] = load(gep(elemPtrTy, ptr, i32_val(sOffsetElem)));\n-        elems[1] = load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem)));\n+        elems[0] = load(gep(elemPtrTy, ptr, sOffsetElemVal));\n+        elems[1] = load(gep(elemPtrTy, ptr2, sOffsetElemVal));\n         elems[2] =\n-            load(gep(elemPtrTy, ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n+            load(gep(elemPtrTy, ptr, sOffsetArrElemVal));\n         elems[3] =\n-            load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n+            load(gep(elemPtrTy, ptr2, sOffsetArrElemVal));\n       } else {\n-        elems[0] = load(gep(elemPtrTy, ptr, i32_val(sOffsetElem)));\n-        elems[2] = load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem)));\n+        elems[0] = load(gep(elemPtrTy, ptr, sOffsetElemVal));\n+        elems[2] = load(gep(elemPtrTy, ptr2, sOffsetElemVal));\n         elems[1] =\n-            load(gep(elemPtrTy, ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n+            load(gep(elemPtrTy, ptr, sOffsetArrElemVal));\n         elems[3] =\n-            load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n+            load(gep(elemPtrTy, ptr2, sOffsetArrElemVal));\n       }\n       return {elems[0], elems[1], elems[2], elems[3]};\n \n@@ -973,8 +973,9 @@ struct MMA16816ConversionHelper {\n     if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n       Value warpM = getWarpM(shape[0]);\n       // load from smem\n+      int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[0], shape[0] / matShapeM);\n       loadFn = getLoadMatrixFn(\n-          tensor, smemObj, mmaLayout, mmaLayout.getWarpsPerCTA()[0] /*wpt*/,\n+          tensor, smemObj, mmaLayout, wpt /*wpt*/,\n           1 /*kOrder*/, {mmaInstrM, mmaInstrK} /*instrShape*/,\n           {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, ha /*vals*/,\n           true /*isA*/);\n@@ -1016,8 +1017,9 @@ struct MMA16816ConversionHelper {\n     int numRepN = getNumRepN(tensorTy, shape[1]);\n \n     Value warpN = getWarpN(shape[1]);\n+    int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[1], shape[1] / matShapeN);\n     auto loadFn = getLoadMatrixFn(\n-        tensor, smemObj, mmaLayout, mmaLayout.getWarpsPerCTA()[1] /*wpt*/,\n+        tensor, smemObj, mmaLayout,  wpt /*wpt*/,\n         0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShape*/,\n         {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/,\n         false /*isA*/);\n@@ -1136,7 +1138,7 @@ struct MMA16816ConversionHelper {\n   std::function<void(int, int)>\n   getLoadMatrixFn(Value tensor, const SharedMemoryObject &smemObj,\n                   MmaEncodingAttr mmaLayout, int wpt, uint32_t kOrder,\n-                  ArrayRef<int> instrShape, ArrayRef<int> matShape,\n+                  SmallVector<int> instrShape, SmallVector<int> matShape,\n                   Value warpId, ValueTable &vals, bool isA) const {\n     auto tensorTy = tensor.getType().cast<RankedTensorType>();\n     // We assumes that the input operand of Dot should be from shared layout."}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 289, "deletions": 302, "changes": 591, "file_content_changes": "@@ -43,6 +43,8 @@ using ::mlir::LLVM::getStridesFromShapeAndOrder;\n using ::mlir::LLVM::getStructFromElements;\n using ::mlir::LLVM::MMA16816ConversionHelper;\n using ::mlir::LLVM::SharedMemoryObject;\n+using ::mlir::LLVM::shflSync;\n+using ::mlir::LLVM::storeShared;\n using ::mlir::triton::gpu::BlockedEncodingAttr;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::getElemsPerThread;\n@@ -307,20 +309,6 @@ T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape,\n                                reorder(shape, order));\n }\n \n-Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n-                  Value val, Value pred) {\n-  MLIRContext *ctx = rewriter.getContext();\n-  unsigned bits = val.getType().getIntOrFloatBitWidth();\n-  const char *c = bits == 64 ? \"l\" : (bits == 16 ? \"h\" : \"r\");\n-\n-  PTXBuilder builder;\n-  auto *ptrOpr = builder.newAddrOperand(ptr, \"r\");\n-  auto *valOpr = builder.newOperand(val, c);\n-  auto &st = builder.create<>(\"st\")->shared().b(bits);\n-  st(ptrOpr, valOpr).predicate(pred, \"b\");\n-  return builder.launch(rewriter, loc, void_ty(ctx));\n-}\n-\n struct ConvertTritonGPUOpToLLVMPatternBase {\n   static Value\n   getStructFromSharedMemoryObject(Location loc,\n@@ -585,12 +573,11 @@ class ConvertTritonGPUOpToLLVMPattern\n   emitOffsetForMmaLayoutV2(const MmaEncodingAttr &mmaLayout,\n                            ArrayRef<int64_t> shape) const {\n     SmallVector<SmallVector<unsigned>> ret;\n+\n     for (unsigned i = 0; i < shape[0]; i += getShapePerCTA(mmaLayout)[0]) {\n       for (unsigned j = 0; j < shape[1]; j += getShapePerCTA(mmaLayout)[1]) {\n         ret.push_back({i, j});\n         ret.push_back({i, j + 1});\n-      }\n-      for (unsigned j = 0; j < shape[1]; j += getShapePerCTA(mmaLayout)[1]) {\n         ret.push_back({i + 8, j});\n         ret.push_back({i + 8, j + 1});\n       }\n@@ -657,22 +644,38 @@ class ConvertTritonGPUOpToLLVMPattern\n     return multiDimIdx;\n   }\n \n+  struct SmallVectorKeyInfo {\n+    static unsigned getHashValue(const SmallVector<unsigned> &key) {\n+      return llvm::hash_combine_range(key.begin(), key.end());\n+    }\n+    static bool isEqual(const SmallVector<unsigned> &lhs,\n+                        const SmallVector<unsigned> &rhs) {\n+      return lhs == rhs;\n+    }\n+    static SmallVector<unsigned> getEmptyKey() {\n+      return SmallVector<unsigned>();\n+    }\n+    static SmallVector<unsigned> getTombstoneKey() {\n+      return {std::numeric_limits<unsigned>::max()};\n+    }\n+  };\n+\n   SmallVector<SmallVector<Value>>\n   emitIndicesForSliceLayout(Location loc, ConversionPatternRewriter &rewriter,\n                             const SliceEncodingAttr &sliceLayout,\n                             ArrayRef<int64_t> shape) const {\n     auto parent = sliceLayout.getParent();\n     unsigned dim = sliceLayout.getDim();\n     size_t rank = shape.size();\n-    auto paddedIndices =\n+    auto parentIndices =\n         emitIndices(loc, rewriter, parent, sliceLayout.paddedShape(shape));\n-    unsigned numIndices = paddedIndices.size();\n-    SmallVector<SmallVector<Value>> resultIndices(numIndices);\n-    for (unsigned i = 0; i < numIndices; ++i)\n-      for (unsigned d = 0; d < rank + 1; ++d)\n-        if (d != dim)\n-          resultIndices[i].push_back(paddedIndices[i][d]);\n-\n+    unsigned numIndices = parentIndices.size();\n+    SmallVector<SmallVector<Value>> resultIndices;\n+    for (unsigned i = 0; i < numIndices; ++i) {\n+      SmallVector<Value> indices = parentIndices[i];\n+      indices.erase(indices.begin() + dim);\n+      resultIndices.push_back(indices);\n+    }\n     return resultIndices;\n   }\n \n@@ -818,7 +821,7 @@ struct ArithConstantSplatOpConversion\n // Contains some helper functions for both Load and Store conversions.\n struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n   explicit LoadStoreConversionBase(AxisInfoAnalysis &axisAnalysisPass)\n-      : AxisAnalysisPass(axisAnalysisPass) {}\n+      : axisAnalysisPass(axisAnalysisPass) {}\n \n   // Get corresponding LLVM element values of \\param value.\n   static SmallVector<Value> getLLVMElems(Value value, Value llValue,\n@@ -834,51 +837,15 @@ struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n   }\n \n   unsigned getVectorSize(Value ptr) const {\n-    auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n-    if (!tensorTy)\n-      return 1;\n-    auto layout = tensorTy.getEncoding();\n-    auto shape = tensorTy.getShape();\n-\n-    auto axisInfo = getAxisInfo(ptr);\n-    // Here order should be ordered by contiguous first, so the first element\n-    // should have the largest contiguous.\n-    auto order = getOrder(layout);\n-    unsigned align = getAlignment(ptr, layout);\n-\n-    unsigned contigPerThread = getSizePerThread(layout)[order[0]];\n-    unsigned vec = std::min(align, contigPerThread);\n-    vec = std::min<unsigned>(shape[order[0]], vec);\n-\n-    return vec;\n-  }\n-\n-  unsigned getAlignment(Value val, const Attribute &layout) const {\n-    auto axisInfo = getAxisInfo(val);\n-    auto order = getOrder(layout);\n-    unsigned maxMultiple = axisInfo->getDivisibility(order[0]);\n-    unsigned maxContig = axisInfo->getContiguity(order[0]);\n-    unsigned alignment = std::min(maxMultiple, maxContig);\n-    return alignment;\n+    return axisAnalysisPass.getPtrVectorSize(ptr);\n   }\n \n   unsigned getMaskAlignment(Value mask) const {\n-    auto tensorTy = mask.getType().cast<RankedTensorType>();\n-    auto maskOrder = getOrder(tensorTy.getEncoding());\n-    auto maskAxis = getAxisInfo(mask);\n-    return std::max<int>(maskAxis->getConstancy(maskOrder[0]), 1);\n-  }\n-\n-  llvm::Optional<AxisInfo> getAxisInfo(Value val) const {\n-    if (auto it = AxisAnalysisPass.lookupLatticeElement(val)) {\n-      return it->getValue();\n-    }\n-\n-    return llvm::Optional<AxisInfo>{};\n+    return axisAnalysisPass.getMaskAlignment(mask);\n   }\n \n protected:\n-  AxisInfoAnalysis &AxisAnalysisPass;\n+  AxisInfoAnalysis &axisAnalysisPass;\n };\n \n struct LoadOpConversion\n@@ -1231,92 +1198,24 @@ struct BroadcastOpConversion\n     unsigned rank = srcTy.getRank();\n     assert(rank == resultTy.getRank());\n     auto order = triton::gpu::getOrder(srcLayout);\n-\n-    SmallVector<int64_t> srcLogicalShape(2 * rank);\n-    SmallVector<unsigned> srcLogicalOrder(2 * rank);\n-    SmallVector<int64_t> resultLogicalShape(2 * rank);\n-    SmallVector<unsigned> broadcastDims;\n-    for (unsigned d = 0; d < rank; ++d) {\n-      unsigned resultShapePerCTA =\n-          triton::gpu::getSizePerThread(resultLayout)[d] *\n-          triton::gpu::getThreadsPerWarp(resultLayout)[d] *\n-          triton::gpu::getWarpsPerCTA(resultLayout)[d];\n-      int64_t numCtas = ceil<unsigned>(resultShape[d], resultShapePerCTA);\n-      if (srcShape[d] != resultShape[d]) {\n-        assert(srcShape[d] == 1);\n-        broadcastDims.push_back(d);\n-        srcLogicalShape[d] = 1;\n-        srcLogicalShape[d + rank] =\n-            std::max<unsigned>(1, triton::gpu::getSizePerThread(srcLayout)[d]);\n-      } else {\n-        srcLogicalShape[d] = numCtas;\n-        srcLogicalShape[d + rank] =\n-            triton::gpu::getSizePerThread(resultLayout)[d];\n-      }\n-      resultLogicalShape[d] = numCtas;\n-      resultLogicalShape[d + rank] =\n-          triton::gpu::getSizePerThread(resultLayout)[d];\n-\n-      srcLogicalOrder[d] = order[d] + rank;\n-      srcLogicalOrder[d + rank] = order[d];\n+    auto srcOffsets = emitOffsetForLayout(srcLayout, srcShape);\n+    auto resultOffsets = emitOffsetForLayout(resultLayout, resultShape);\n+    SmallVector<Value> srcVals = getElementsFromStruct(loc, src, rewriter);\n+    DenseMap<SmallVector<unsigned>, Value, SmallVectorKeyInfo> srcValues;\n+    for (size_t i = 0; i < srcOffsets.size(); i++) {\n+      srcValues[srcOffsets[i]] = srcVals[i];\n     }\n-    int64_t duplicates = 1;\n-    SmallVector<int64_t> broadcastSizes(broadcastDims.size() * 2);\n-    SmallVector<unsigned> broadcastOrder(broadcastDims.size() * 2);\n-    for (auto it : llvm::enumerate(broadcastDims)) {\n-      // Incase there are multiple indices in the src that is actually\n-      // calculating the same element, srcLogicalShape may not need to be 1.\n-      // Such as the case when src of shape [256, 1], and with a blocked\n-      // layout: sizePerThread: [1, 4];  threadsPerWarp: [1, 32]; warpsPerCTA:\n-      // [1, 2]\n-      int64_t d = resultLogicalShape[it.value()] / srcLogicalShape[it.value()];\n-      broadcastSizes[it.index()] = d;\n-      broadcastOrder[it.index()] = srcLogicalOrder[it.value()];\n-      duplicates *= d;\n-      d = resultLogicalShape[it.value() + rank] /\n-          srcLogicalShape[it.value() + rank];\n-      broadcastSizes[it.index() + broadcastDims.size()] = d;\n-      broadcastOrder[it.index() + broadcastDims.size()] =\n-          srcLogicalOrder[it.value() + rank];\n-      duplicates *= d;\n-    }\n-    auto argsort = [](SmallVector<unsigned> input) {\n-      SmallVector<unsigned> idx(input.size());\n-      std::iota(idx.begin(), idx.end(), 0);\n-      std::sort(idx.begin(), idx.end(), [&input](unsigned a, unsigned b) {\n-        return input[a] < input[b];\n-      });\n-      return idx;\n-    };\n-    broadcastOrder = argsort(broadcastOrder);\n-\n-    unsigned srcElems = getElemsPerThread(srcTy);\n-    auto srcVals = getElementsFromStruct(loc, src, rewriter);\n-    unsigned resultElems = getElemsPerThread(resultTy);\n-    SmallVector<Value> resultVals(resultElems);\n-    for (unsigned i = 0; i < srcElems; ++i) {\n-      auto srcMultiDim =\n-          getMultiDimIndex<int64_t>(i, srcLogicalShape, srcLogicalOrder);\n-      for (int64_t j = 0; j < duplicates; ++j) {\n-        auto resultMultiDim = srcMultiDim;\n-        auto bcastMultiDim =\n-            getMultiDimIndex<int64_t>(j, broadcastSizes, broadcastOrder);\n-        for (auto bcastDim : llvm::enumerate(broadcastDims)) {\n-          resultMultiDim[bcastDim.value()] += bcastMultiDim[bcastDim.index()];\n-          resultMultiDim[bcastDim.value() + rank] +=\n-              bcastMultiDim[bcastDim.index() + broadcastDims.size()] *\n-              srcLogicalShape[bcastDim.index() + broadcastDims.size()];\n-        }\n-        auto resultLinearIndex = getLinearIndex<int64_t>(\n-            resultMultiDim, resultLogicalShape, srcLogicalOrder);\n-        resultVals[resultLinearIndex] = srcVals[i];\n-      }\n+    SmallVector<Value> resultVals;\n+    for (size_t i = 0; i < resultOffsets.size(); i++) {\n+      auto offset = resultOffsets[i];\n+      for (size_t j = 0; j < srcShape.size(); j++)\n+        if (srcShape[j] == 1)\n+          offset[j] = 0;\n+      resultVals.push_back(srcValues.lookup(offset));\n     }\n     auto llvmStructTy = getTypeConverter()->convertType(resultTy);\n-\n     Value resultStruct =\n         getStructFromElements(loc, resultVals, rewriter, llvmStructTy);\n-\n     rewriter.replaceOp(op, {resultStruct});\n     return success();\n   }\n@@ -1342,9 +1241,6 @@ struct ReduceOpConversion\n                            RedOp redOp, Value &acc, Value &accIndex, Value cur,\n                            Value curIndex, bool isFirst) const;\n \n-  Value shflSync(ConversionPatternRewriter &rewriter, Location loc, Value val,\n-                 int i) const;\n-\n   // Use shared memory for reduction within warps and across warps\n   LogicalResult matchAndRewriteBasic(triton::ReduceOp op, OpAdaptor adaptor,\n                                      ConversionPatternRewriter &rewriter) const;\n@@ -1472,34 +1368,6 @@ void ReduceOpConversion::accumulateWithIndex(\n   }\n }\n \n-Value ReduceOpConversion::shflSync(ConversionPatternRewriter &rewriter,\n-                                   Location loc, Value val, int i) const {\n-  unsigned bits = val.getType().getIntOrFloatBitWidth();\n-\n-  if (bits == 64) {\n-    Type vecTy = vec_ty(f32_ty, 2);\n-    Value vec = bitcast(val, vecTy);\n-    Value val0 = extract_element(f32_ty, vec, i32_val(0));\n-    Value val1 = extract_element(f32_ty, vec, i32_val(1));\n-    val0 = shflSync(rewriter, loc, val0, i);\n-    val1 = shflSync(rewriter, loc, val1, i);\n-    vec = undef(vecTy);\n-    vec = insert_element(vecTy, vec, val0, i32_val(0));\n-    vec = insert_element(vecTy, vec, val1, i32_val(1));\n-    return bitcast(vec, val.getType());\n-  }\n-\n-  PTXBuilder builder;\n-  auto &shfl = builder.create(\"shfl.sync\")->o(\"bfly\").o(\"b32\");\n-  auto *dOpr = builder.newOperand(\"=r\");\n-  auto *aOpr = builder.newOperand(val, \"r\");\n-  auto *bOpr = builder.newConstantOperand(i);\n-  auto *cOpr = builder.newConstantOperand(\"0x1f\");\n-  auto *maskOpr = builder.newConstantOperand(\"0xffffffff\");\n-  shfl(dOpr, aOpr, bOpr, cOpr, maskOpr);\n-  return builder.launch(rewriter, loc, val.getType(), false);\n-}\n-\n LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n     triton::ReduceOp op, OpAdaptor adaptor,\n     ConversionPatternRewriter &rewriter) const {\n@@ -1665,7 +1533,6 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   auto smemShapes = helper.getScratchConfigsFast();\n   unsigned elems = product<unsigned>(smemShapes[0]);\n   unsigned maxElems = std::max(elems, product<unsigned>(smemShapes[1]));\n-  maxElems = std::max(maxElems, product<unsigned>(smemShapes[2]));\n   Value indexSmemBase = gep(elemPtrTy, smemBase, i32_val(maxElems));\n   indexSmemBase = bitcast(indexSmemBase, indexPtrTy);\n \n@@ -1725,11 +1592,11 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n \n     // reduce within warps\n     for (unsigned N = sizeIntraWarps / 2; N > 0; N >>= 1) {\n-      Value shfl = shflSync(rewriter, loc, acc, N);\n+      Value shfl = shflSync(loc, rewriter, acc, N);\n       if (!withIndex) {\n         accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n       } else {\n-        Value shflIndex = shflSync(rewriter, loc, accIndex, N);\n+        Value shflIndex = shflSync(loc, rewriter, accIndex, N);\n         accumulateWithIndex(rewriter, loc, op.redOp(), acc, accIndex, shfl,\n                             shflIndex, false);\n       }\n@@ -1750,8 +1617,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   barrier();\n \n   // the second round of shuffle reduction\n-  //   now the problem size: sizeInterWarps, s1, s2, .. , sn  =>\n-  //                                      1, s1, s2, .. , sn\n+  //   now the problem size: sizeInterWarps, s1, s2, .. , sn\n   //   where sizeInterWarps is 2^m\n   //\n   // each thread needs to process:\n@@ -1762,6 +1628,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   Value readOffset = threadId;\n   for (unsigned round = 0; round < elemsPerThread; ++round) {\n     Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n+    // FIXME(Qingyi): need predicate icmp_slt(threadId, i32_val(sizeInerWarps))\n     Value acc = load(readPtr);\n     Value accIndex;\n     if (withIndex) {\n@@ -1770,17 +1637,18 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n     }\n \n     for (unsigned N = sizeInterWarps / 2; N > 0; N >>= 1) {\n-      Value shfl = shflSync(rewriter, loc, acc, N);\n+      Value shfl = shflSync(loc, rewriter, acc, N);\n       if (!withIndex) {\n         accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n       } else {\n-        Value shflIndex = shflSync(rewriter, loc, accIndex, N);\n+        Value shflIndex = shflSync(loc, rewriter, accIndex, N);\n         accumulateWithIndex(rewriter, loc, op.redOp(), acc, accIndex, shfl,\n                             shflIndex, false);\n       }\n     }\n \n-    Value writeOffset = udiv(readOffset, i32_val(sizeInterWarps));\n+    // only the first thread in each sizeInterWarps is writing\n+    Value writeOffset = readOffset;\n     Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n     Value threadIsNeeded = icmp_slt(threadId, i32_val(elems));\n     Value laneIdModSizeInterWarps = urem(laneId, i32_val(sizeInterWarps));\n@@ -1807,22 +1675,17 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   if (auto resultTy = op.getType().dyn_cast<RankedTensorType>()) {\n     // nd-tensor where n >= 1\n     auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n-    SmallVector<unsigned> resultOrd;\n-    for (auto ord : order) {\n-      if (ord != 0)\n-        resultOrd.push_back(ord - 1);\n-    }\n-\n+    auto resultShape = resultTy.getShape();\n     unsigned resultElems = getElemsPerThread(resultTy);\n-    auto resultIndices =\n-        emitIndices(loc, rewriter, resultLayout, resultTy.getShape());\n+    auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultShape);\n     assert(resultIndices.size() == resultElems);\n \n     SmallVector<Value> resultVals(resultElems);\n     for (size_t i = 0; i < resultElems; ++i) {\n       SmallVector<Value> readIdx = resultIndices[i];\n+      readIdx.insert(readIdx.begin() + axis, i32_val(0));\n       Value readOffset =\n-          linearize(rewriter, loc, readIdx, smemShapes[2], resultOrd);\n+          linearize(rewriter, loc, readIdx, smemShapes[0], order);\n       Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n       Value indexReadPtr = gep(indexPtrTy, indexSmemBase, readOffset);\n       resultVals[i] = withIndex ? load(indexReadPtr) : load(readPtr);\n@@ -2012,9 +1875,9 @@ struct PrintfOpConversion\n \n     Value globalPtr =\n         rewriter.create<LLVM::AddressOfOp>(UnknownLoc::get(context), global);\n-    Value stringStart =\n-        rewriter.create<LLVM::GEPOp>(UnknownLoc::get(context), int8Ptr,\n-                                     globalPtr, mlir::ValueRange({zero, zero}));\n+    Value stringStart = rewriter.create<LLVM::GEPOp>(\n+        UnknownLoc::get(context), int8Ptr, globalPtr,\n+        SmallVector<Value>({zero, zero}));\n \n     Value bufferPtr =\n         rewriter.create<LLVM::NullOp>(UnknownLoc::get(context), int8Ptr);\n@@ -2049,7 +1912,7 @@ struct PrintfOpConversion\n                                                    int8Ptr, allocated);\n     }\n \n-    ValueRange operands{stringStart, bufferPtr};\n+    SmallVector<Value> operands{stringStart, bufferPtr};\n     rewriter.create<LLVM::CallOp>(UnknownLoc::get(context), funcOp, operands);\n   }\n };\n@@ -2075,7 +1938,10 @@ struct MakeRangeOpConversion\n     auto idxs = emitIndices(loc, rewriter, layout, shape);\n     unsigned elems = idxs.size();\n     SmallVector<Value> retVals(elems);\n-    for (const auto &multiDim : llvm::enumerate(idxs)) {\n+    // TODO: slice layout has more elements than expected.\n+    // Unexpected behavior for make range, but genereally ok when followed by\n+    // expand dims + broadcast. very weird behavior otherwise potentially.\n+    for (const auto multiDim : llvm::enumerate(idxs)) {\n       assert(multiDim.value().size() == 1);\n       retVals[multiDim.index()] = add(multiDim.value()[0], start);\n     }\n@@ -2122,7 +1988,7 @@ struct GetNumProgramsOpConversion\n     Location loc = op->getLoc();\n     assert(op.axis() < 3);\n \n-    Value blockId = rewriter.create<::mlir::gpu::BlockDimOp>(\n+    Value blockId = rewriter.create<::mlir::gpu::GridDimOp>(\n         loc, rewriter.getIndexType(), dims[op.axis()]);\n     auto llvmIndexTy = getTypeConverter()->getIndexType();\n     rewriter.replaceOpWithNewOp<UnrealizedConversionCastOp>(\n@@ -2778,6 +2644,58 @@ struct ConvertLayoutOpConversion\n          dstLayout.isa<SliceEncodingAttr>())) {\n       return lowerDistributedToDistributed(op, adaptor, rewriter);\n     }\n+    // dot_op<opIdx=0, parent=#mma> = #mma\n+    // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>\n+    if (srcLayout.isa<MmaEncodingAttr>() &&\n+        dstLayout.isa<DotOperandEncodingAttr>()) {\n+      auto srcMmaLayout = srcLayout.cast<MmaEncodingAttr>();\n+      auto dstDotLayout = dstLayout.cast<DotOperandEncodingAttr>();\n+      if (srcMmaLayout.getWarpsPerCTA()[1] == 1 &&\n+          dstDotLayout.getOpIdx() == 0 &&\n+          dstDotLayout.getParent() == srcMmaLayout) {\n+        // get source values\n+        Location loc = op->getLoc();\n+        auto vals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n+        unsigned elems = getElemsPerThread(srcTy);\n+        Type elemTy =\n+            this->getTypeConverter()->convertType(srcTy.getElementType());\n+        // for the destination type, we need to pack values together\n+        // so they can be consumed by tensor core operations\n+        unsigned vecSize =\n+            std::max<unsigned>(32 / elemTy.getIntOrFloatBitWidth(), 1);\n+        Type vecTy = vec_ty(elemTy, vecSize);\n+        SmallVector<Type> types(elems / vecSize, vecTy);\n+        SmallVector<Value> vecVals;\n+        for (unsigned i = 0; i < elems; i += vecSize) {\n+          Value packed = rewriter.create<LLVM::UndefOp>(loc, vecTy);\n+          for (unsigned j = 0; j < vecSize; j++)\n+            packed = insert_element(vecTy, packed, vals[i + j], i32_val(j));\n+          vecVals.push_back(packed);\n+        }\n+\n+        // This needs to be ordered the same way that\n+        // ldmatrix.x4 would order it\n+        // TODO: this needs to be refactor so we don't\n+        // implicitly depends on how emitOffsetsForMMAV2\n+        // is implemented\n+        SmallVector<Value> reorderedVals;\n+        for (unsigned i = 0; i < vecVals.size(); i += 4) {\n+          reorderedVals.push_back(vecVals[i]);\n+          reorderedVals.push_back(vecVals[i + 2]);\n+          reorderedVals.push_back(vecVals[i + 1]);\n+          reorderedVals.push_back(vecVals[i + 3]);\n+        }\n+\n+        // return composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK);\n+\n+        Type structTy =\n+            LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n+        Value view =\n+            getStructFromElements(loc, reorderedVals, rewriter, structTy);\n+        rewriter.replaceOp(op, view);\n+        return success();\n+      }\n+    }\n     // TODO: to be implemented\n     llvm_unreachable(\"unsupported layout conversion\");\n     return failure();\n@@ -2798,6 +2716,9 @@ struct ConvertLayoutOpConversion\n     auto dstSharedLayout = dstTy.getEncoding().cast<SharedEncodingAttr>();\n     auto inOrd = srcBlockedLayout.getOrder();\n     auto outOrd = dstSharedLayout.getOrder();\n+    if (inOrd != outOrd)\n+      llvm_unreachable(\n+          \"blocked -> shared with different order not yet implemented\");\n     unsigned inVec =\n         inOrd == outOrd ? srcBlockedLayout.getSizePerThread()[inOrd[0]] : 1;\n     unsigned outVec = dstSharedLayout.getVec();\n@@ -2857,7 +2778,8 @@ struct ConvertLayoutOpConversion\n             getMultiDimIndex<unsigned>(linearRepIdx, reps, inOrd);\n         for (unsigned linearWordIdx = 0; linearWordIdx < numWordsEachRep;\n              ++linearWordIdx) {\n-          // step 1: recover the multidim_index from the index of input_elements\n+          // step 1: recover the multidim_index from the index of\n+          // input_elements\n           auto multiDimWordIdx =\n               getMultiDimIndex<unsigned>(linearWordIdx, wordsInEachRep, inOrd);\n           SmallVector<Value> multiDimIdx(2);\n@@ -2901,7 +2823,7 @@ struct ConvertLayoutOpConversion\n           emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n       SmallVector<Value> multiDimOffset(rank);\n       SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n-          elemId, blockedLayout.getSizePerThread(), blockedLayout.getOrder());\n+          elemId, getSizePerThread(layout), getOrder(layout));\n       for (unsigned d = 0; d < rank; ++d) {\n         multiDimOffset[d] = add(multiDimOffsetFirstElem[d],\n                                 idx_val(multiDimCTAInRepId[d] * shapePerCTA[d] +\n@@ -2949,58 +2871,59 @@ struct ConvertLayoutOpConversion\n         Value mmaThreadIdInGrp = urem(laneId, _4);\n         Value mmaThreadIdInGrpM2 = mul(mmaThreadIdInGrp, _2);\n         Value mmaThreadIdInGrpM2P1 = add(mmaThreadIdInGrpM2, _1);\n-        Value colWarpOffset = mul(multiDimWarpId[0], _16);\n-        mmaColIdx[0] = add(mmaGrpId, colWarpOffset);\n-        mmaColIdx[1] = add(mmaGrpIdP8, colWarpOffset);\n-        Value rowWarpOffset = mul(multiDimWarpId[1], _8);\n-        mmaRowIdx[0] = add(mmaThreadIdInGrpM2, rowWarpOffset);\n-        mmaRowIdx[1] = add(mmaThreadIdInGrpM2P1, rowWarpOffset);\n+        Value rowWarpOffset = mul(multiDimWarpId[0], _16);\n+        mmaRowIdx[0] = add(mmaGrpId, rowWarpOffset);\n+        mmaRowIdx[1] = add(mmaGrpIdP8, rowWarpOffset);\n+        Value colWarpOffset = mul(multiDimWarpId[1], _8);\n+        mmaColIdx[0] = add(mmaThreadIdInGrpM2, colWarpOffset);\n+        mmaColIdx[1] = add(mmaThreadIdInGrpM2P1, colWarpOffset);\n       } else if (mmaLayout.getVersion() == 1) {\n         multiDimWarpId[0] = urem(multiDimWarpId[0], idx_val(shape[0] / 16));\n         multiDimWarpId[1] = urem(multiDimWarpId[1], idx_val(shape[1] / 16));\n-        Value partId = udiv(laneId, _4);\n-        Value partIdDiv4 = udiv(partId, _4);\n-        Value partIdRem4 = urem(partId, _4);\n-        Value partRowOffset = mul(udiv(partIdRem4, _2), _8);\n-        partRowOffset = add(mul(partIdDiv4, _4), partRowOffset);\n-        Value partColOffset = mul(urem(partIdRem4, _2), _8);\n-        Value colOffset = add(mul(multiDimWarpId[0], _16), partColOffset);\n-        Value rowOffset = add(mul(multiDimWarpId[1], _16), partRowOffset);\n-        mmaRowIdx[0] = add(urem(laneId, _2), rowOffset);\n+        Value laneIdDiv16 = udiv(laneId, _16);\n+        Value laneIdRem16 = urem(laneId, _16);\n+        Value laneIdRem2 = urem(laneId, _2);\n+        Value laneIdRem16Div8 = udiv(laneIdRem16, _8);\n+        Value laneIdRem16Div4 = udiv(laneIdRem16, _4);\n+        Value laneIdRem16Div4Rem2 = urem(laneIdRem16Div4, _2);\n+        Value laneIdRem4Div2 = udiv(urem(laneId, _4), _2);\n+        mmaRowIdx[0] =\n+            add(add(mul(laneIdDiv16, _8), mul(laneIdRem16Div4Rem2, _4)),\n+                laneIdRem2);\n         mmaRowIdx[1] = add(mmaRowIdx[0], _2);\n-        mmaColIdx[0] = add(udiv(urem(laneId, _4), _2), colOffset);\n+        mmaColIdx[0] = add(mul(laneIdRem16Div8, _4), mul(laneIdRem4Div2, _2));\n         mmaColIdx[1] = add(mmaColIdx[0], _1);\n-        mmaColIdx[2] = add(mmaColIdx[0], _4);\n-        mmaColIdx[3] = add(mmaColIdx[0], idx_val(5));\n+        mmaColIdx[2] = add(mmaColIdx[0], _8);\n+        mmaColIdx[3] = add(mmaColIdx[0], idx_val(9));\n       } else {\n         llvm_unreachable(\"Unexpected MMALayout version\");\n       }\n \n       assert(rank == 2);\n       SmallVector<Value> multiDimOffset(rank);\n       if (mmaLayout.getVersion() == 2) {\n-        multiDimOffset[0] = elemId < 2 ? mmaColIdx[0] : mmaColIdx[1];\n-        multiDimOffset[1] = elemId % 2 == 0 ? mmaRowIdx[0] : mmaRowIdx[1];\n+        multiDimOffset[0] = elemId < 2 ? mmaRowIdx[0] : mmaRowIdx[1];\n+        multiDimOffset[1] = elemId % 2 == 0 ? mmaColIdx[0] : mmaColIdx[1];\n         multiDimOffset[0] = add(\n             multiDimOffset[0], idx_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n         multiDimOffset[1] = add(\n             multiDimOffset[1], idx_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n       } else if (mmaLayout.getVersion() == 1) {\n         // the order of elements in a thread:\n-        //   c0, c1, c4, c5\n-        //   c2, c3, c6, c7\n+        //   c0, c1, ...  c4, c5\n+        //   c2, c3, ...  c6, c7\n         if (elemId < 2) {\n-          multiDimOffset[0] = mmaColIdx[elemId % 2];\n-          multiDimOffset[1] = mmaRowIdx[0];\n+          multiDimOffset[0] = mmaRowIdx[0];\n+          multiDimOffset[1] = mmaColIdx[elemId % 2];\n         } else if (elemId >= 2 && elemId < 4) {\n-          multiDimOffset[0] = mmaColIdx[elemId % 2];\n-          multiDimOffset[1] = mmaRowIdx[1];\n+          multiDimOffset[0] = mmaRowIdx[1];\n+          multiDimOffset[1] = mmaColIdx[elemId % 2];\n         } else if (elemId >= 4 && elemId < 6) {\n-          multiDimOffset[0] = mmaColIdx[elemId % 2 + 2];\n-          multiDimOffset[1] = mmaRowIdx[0];\n+          multiDimOffset[0] = mmaRowIdx[0];\n+          multiDimOffset[1] = mmaColIdx[elemId % 2 + 2];\n         } else if (elemId >= 6) {\n-          multiDimOffset[0] = mmaColIdx[elemId % 2 + 2];\n-          multiDimOffset[1] = mmaRowIdx[1];\n+          multiDimOffset[0] = mmaRowIdx[1];\n+          multiDimOffset[1] = mmaColIdx[elemId % 2 + 2];\n         }\n         multiDimOffset[0] = add(\n             multiDimOffset[0], idx_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n@@ -3099,6 +3022,7 @@ void ConvertLayoutOpConversion::processReplica(\n                             multiDimCTAInRepId, shapePerCTA);\n       Value offset =\n           linearize(rewriter, loc, multiDimOffset, paddedRepShape, outOrd);\n+\n       auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n       Value ptr = gep(elemPtrTy, smemBase, offset);\n       auto vecTy = vec_ty(llvmElemTy, vec);\n@@ -3111,7 +3035,6 @@ void ConvertLayoutOpConversion::processReplica(\n             currVal = zext(llvmElemTy, currVal);\n           else if (isPtr)\n             currVal = ptrtoint(llvmElemTy, currVal);\n-\n           valVec = insert_element(vecTy, valVec, currVal, idx_val(v));\n         }\n         store(valVec, ptr);\n@@ -3402,10 +3325,10 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n   // We cannot get both the operand types(in TypeConverter), here we assume the\n   // types of both the operands are identical here.\n   // TODO[Superjomn]: Find a better way to implement it.\n-  static bool isDotHMMA(TensorType operand, bool allowTF32, int mmaVersion) {\n+  static bool isDotHMMA(TensorType operand, int mmaVersion) {\n     auto elemTy = operand.getElementType();\n     return elemTy.isF16() || elemTy.isBF16() ||\n-           (elemTy.isF32() && allowTF32 && mmaVersion >= 2) ||\n+           (elemTy.isF32() && mmaVersion >= 2) ||\n            (elemTy.isInteger(8) && mmaVersion >= 2);\n   }\n \n@@ -3429,11 +3352,7 @@ Value ConvertLayoutOpConversion::lowerSharedToDotOperandMMA(\n   Value src = op.src();\n   Value dst = op.result();\n   auto dstTensorTy = dst.getType().cast<RankedTensorType>();\n-  // TODO[Superjomn]: allowTF32 is not accessible here for it is an attribute of\n-  // an Op instance.\n-  bool allowTF32 = false;\n-  bool isHMMA = DotOpConversion::isDotHMMA(dstTensorTy, allowTF32,\n-                                           mmaLayout.getVersion());\n+  bool isHMMA = DotOpConversion::isDotHMMA(dstTensorTy, mmaLayout.getVersion());\n \n   auto smemObj = getSharedMemoryObjectFromStruct(loc, adaptor.src(), rewriter);\n   Value res;\n@@ -3496,25 +3415,16 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n   } else if (auto blockedLayout =\n                  dotOperandLayout.getParent()\n                      .dyn_cast_or_null<BlockedEncodingAttr>()) {\n-    // TODO[Superjomn]: the allowTF32 is not available in ConvertLayoutOp for it\n-    // is an attribute of DotOp.\n-    bool allowTF32 = false;\n-    bool isFMADot = dstTensorTy.getElementType().isF32() && !allowTF32;\n-    if (isFMADot) {\n-      auto dotOpLayout =\n-          dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n-      auto blockedLayout = dotOpLayout.getParent().cast<BlockedEncodingAttr>();\n-      DotOpFMAConversionHelper helper(blockedLayout);\n-      auto thread = getThreadId(rewriter, loc);\n-      if (dotOpLayout.getOpIdx() == 0) { // $a\n-        res = helper.loadA(src, adaptor.src(), blockedLayout, thread, loc,\n-                           rewriter);\n-      } else { // $b\n-        res = helper.loadB(src, adaptor.src(), blockedLayout, thread, loc,\n-                           rewriter);\n-      }\n-    } else\n-      assert(false && \"Unsupported dot operand layout found\");\n+    auto dotOpLayout = dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n+    DotOpFMAConversionHelper helper(blockedLayout);\n+    auto thread = getThreadId(rewriter, loc);\n+    if (dotOpLayout.getOpIdx() == 0) { // $a\n+      res = helper.loadA(src, adaptor.src(), blockedLayout, thread, loc,\n+                         rewriter);\n+    } else { // $b\n+      res = helper.loadB(src, adaptor.src(), blockedLayout, thread, loc,\n+                         rewriter);\n+    }\n   } else {\n     assert(false && \"Unsupported dot operand layout found\");\n   }\n@@ -3623,6 +3533,8 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n \n   // initialize accumulators\n   SmallVector<Value> acc = getElementsFromStruct(loc, loadedC, rewriter);\n+  size_t resSize = acc.size();\n+  SmallVector<Value> resVals(resSize);\n \n   auto callMMA = [&](unsigned m, unsigned n, unsigned k) {\n     auto ha = has[{m, k}];\n@@ -3666,8 +3578,14 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n     auto getIntAttr = [&](int v) {\n       return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n     };\n-    for (unsigned i = 0; i < 8; i++)\n-      acc[idx[i]] = extract_val(f32_ty, res, getIntAttr(i));\n+\n+    for (unsigned i = 0; i < 8; i++) {\n+      Value elem = extract_val(f32_ty, res, getIntAttr(i));\n+      acc[idx[i]] = elem;\n+      // TODO[goostavz]: double confirm this when m/n/k = [32, 32, x] has been\n+      // verified before MMA\n+      resVals[(m * numN / 2 + n) * 8 + i] = elem;\n+    }\n   };\n \n   for (unsigned k = 0; k < NK; k += 4)\n@@ -3676,12 +3594,10 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n         callMMA(m, n, k);\n       }\n \n-  // replace with new packed result\n   Type structTy = LLVM::LLVMStructType::getLiteral(\n-      ctx, SmallVector<Type>(acc.size(), type::f32Ty(ctx)));\n-  Value res = getStructFromElements(loc, acc, rewriter, structTy);\n+      ctx, SmallVector<Type>(resSize, type::f32Ty(ctx)));\n+  Value res = getStructFromElements(loc, resVals, rewriter, structTy);\n   rewriter.replaceOp(op, res);\n-\n   return success();\n }\n \n@@ -3784,6 +3700,33 @@ DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n \n /// ====================== mma codegen end ============================\n \n+/// ====================== trans codegen begin ============================\n+\n+struct TransOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::TransOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::TransOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::TransOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Location loc = op->getLoc();\n+    auto srcSmemObj =\n+        getSharedMemoryObjectFromStruct(loc, adaptor.src(), rewriter);\n+    SmallVector<Value> dstStrides = {srcSmemObj.strides[1],\n+                                     srcSmemObj.strides[0]};\n+    SmallVector<Value> dstOffsets = {srcSmemObj.offsets[1],\n+                                     srcSmemObj.offsets[0]};\n+    auto dstSmemObj =\n+        SharedMemoryObject(srcSmemObj.base, dstStrides, dstOffsets);\n+    auto retVal = getStructFromSharedMemoryObject(loc, dstSmemObj, rewriter);\n+    rewriter.replaceOp(op, retVal);\n+    return success();\n+  }\n+};\n+\n+/// ====================== trans codegen end ============================\n+\n Value convertSplatLikeOpWithMmaLayout(const MmaEncodingAttr &layout,\n                                       Type resType, Type elemType,\n                                       Value constVal,\n@@ -3847,13 +3790,6 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n     auto ctx = type.getContext();\n     Attribute layout = type.getEncoding();\n     auto shape = type.getShape();\n-\n-    // TODO[Keren, Superjomn]: fix it, allowTF32 is not accessible here for it\n-    // is bound to an Op instance.\n-    bool allowTF32 = false;\n-    bool isFMADot = type.getElementType().isF32() && !allowTF32 &&\n-                    layout.dyn_cast_or_null<DotOperandEncodingAttr>();\n-\n     if (layout &&\n         (layout.isa<BlockedEncodingAttr>() || layout.isa<SliceEncodingAttr>() ||\n          layout.isa<MmaEncodingAttr>())) {\n@@ -3877,37 +3813,39 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n       return LLVM::LLVMStructType::getLiteral(ctx, types);\n     } else if (auto dotOpLayout =\n                    layout.dyn_cast_or_null<DotOperandEncodingAttr>()) {\n-      if (isFMADot) { // for parent is blocked layout\n+      if (dotOpLayout.getParent()\n+              .isa<BlockedEncodingAttr>()) { // for parent is blocked layout\n         int numElemsPerThread =\n             DotOpFMAConversionHelper::getNumElemsPerThread(shape, dotOpLayout);\n \n         return LLVM::LLVMStructType::getLiteral(\n             ctx, SmallVector<Type>(numElemsPerThread, type::f32Ty(ctx)));\n-\n       } else { // for parent is MMA layout\n         auto mmaLayout = dotOpLayout.getParent().cast<MmaEncodingAttr>();\n         auto wpt = mmaLayout.getWarpsPerCTA();\n         Type elemTy = convertType(type.getElementType());\n-        auto vecSize = 1;\n-        if (elemTy.getIntOrFloatBitWidth() == 16) {\n-          vecSize = 2;\n-        } else if (elemTy.getIntOrFloatBitWidth() == 8) {\n-          vecSize = 4;\n-        } else {\n-          assert(false && \"Unsupported element type\");\n-        }\n-        Type vecTy = vec_ty(elemTy, vecSize);\n         if (mmaLayout.getVersion() == 2) {\n+          const llvm::DenseMap<int, Type> targetTyMap = {\n+              {32, elemTy},\n+              {16, vec_ty(elemTy, 2)},\n+              {8, vec_ty(elemTy, 4)},\n+          };\n+          Type targetTy;\n+          if (targetTyMap.count(elemTy.getIntOrFloatBitWidth())) {\n+            targetTy = targetTyMap.lookup(elemTy.getIntOrFloatBitWidth());\n+          } else {\n+            assert(false && \"Unsupported element type\");\n+          }\n           if (dotOpLayout.getOpIdx() == 0) { // $a\n             int elems =\n                 MMA16816ConversionHelper::getANumElemsPerThread(type, wpt[0]);\n             return LLVM::LLVMStructType::getLiteral(\n-                ctx, SmallVector<Type>(elems, vecTy));\n+                ctx, SmallVector<Type>(elems, targetTy));\n           }\n           if (dotOpLayout.getOpIdx() == 1) { // $b\n             int elems =\n                 MMA16816ConversionHelper::getBNumElemsPerThread(type, wpt[1]);\n-            return struct_ty(SmallVector<Type>(elems, vecTy));\n+            return struct_ty(SmallVector<Type>(elems, targetTy));\n           }\n         }\n \n@@ -4037,10 +3975,10 @@ struct InsertSliceAsyncOpConversion\n     // %other\n     SmallVector<Value> otherElems;\n     if (llOther) {\n-      // TODO(Keren): support \"other\" tensor.\n+      // FIXME(Keren): always assume other is 0 for now\n       // It's not necessary for now because the pipeline pass will skip\n       // generating insert_slice_async if the load op has any \"other\" tensor.\n-      assert(false && \"insert_slice_async: Other value not supported yet\");\n+      // assert(false && \"insert_slice_async: Other value not supported yet\");\n       otherElems = getLLVMElems(other, llOther, rewriter, loc);\n       assert(srcElems.size() == otherElems.size());\n     }\n@@ -4611,6 +4549,7 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<ViewLikeOpConversion<triton::ExpandDimsOp>>(typeConverter,\n                                                            benefit);\n   patterns.add<DotOpConversion>(typeConverter, allocation, smem, benefit);\n+  patterns.add<TransOpConversion>(typeConverter, benefit);\n   patterns.add<PrintfOpConversion>(typeConverter, benefit);\n }\n \n@@ -4645,30 +4584,68 @@ class ConvertTritonGPUToLLVM\n     });\n   }\n \n-  void decomposeInsertSliceAsyncOp(ModuleOp mod,\n-                                   TritonGPUToLLVMTypeConverter &converter) {\n-    // cp.async is supported in Ampere and later\n-    if (computeCapability >= 80)\n-      return;\n-\n+  void decomposeInsertSliceAsyncOp(ModuleOp mod) {\n+    AxisInfoAnalysis axisInfoAnalysis(mod.getContext());\n+    axisInfoAnalysis.run(mod);\n+    // TODO(Keren): This is a hacky knob that may cause performance regression\n+    // when decomposition has been performed. We should remove this knob once we\n+    // have thorough analysis on async wait. Currently, we decompose\n+    // `insert_slice_async` into `load` and `insert_slice` without knowing which\n+    // `async_wait` is responsible for the `insert_slice_async`. To guarantee\n+    // correctness, we blindly set the `async_wait` to wait for all async ops.\n+    //\n+    // There are two options to improve this:\n+    // 1. We can perform a dataflow analysis to find the `async_wait` that is\n+    // responsible for the `insert_slice_async` in the backend.\n+    // 2. We can modify the pipeline to perform the decomposition before the\n+    // `async_wait` is inserted. However, it is also risky because we don't know\n+    // the correct vectorized shape yet in the pipeline pass. Making the\n+    // pipeline pass aware of the vectorization could introduce additional\n+    // dependencies on the AxisInfoAnalysis and the Coalesce analysis.\n+    bool decomposed = false;\n     // insert_slice_async %src, %dst, %idx, %mask, %other\n     // =>\n     // %tmp = load %src, %mask, %other\n     // %res = insert_slice %tmp into %dst[%idx]\n     mod.walk([&](triton::gpu::InsertSliceAsyncOp insertSliceAsyncOp) -> void {\n       OpBuilder builder(insertSliceAsyncOp);\n-      // load\n-      auto srcTy = insertSliceAsyncOp.src().getType().cast<RankedTensorType>();\n-      auto dstTy = insertSliceAsyncOp.getType().cast<RankedTensorType>();\n+\n+      // Get the vectorized load size\n+      auto src = insertSliceAsyncOp.src();\n+      auto dst = insertSliceAsyncOp.dst();\n+      auto srcTy = src.getType().cast<RankedTensorType>();\n+      auto dstTy = dst.getType().cast<RankedTensorType>();\n       auto srcBlocked =\n           srcTy.getEncoding().dyn_cast<triton::gpu::BlockedEncodingAttr>();\n-      auto elemTy = converter.convertType(dstTy.getElementType());\n-      auto tmpTy = RankedTensorType::get(srcTy.getShape(), elemTy, srcBlocked);\n+      auto resSharedLayout =\n+          dstTy.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n+      auto resElemTy = dstTy.getElementType();\n+      unsigned inVec = axisInfoAnalysis.getPtrVectorSize(src);\n+      unsigned outVec = resSharedLayout.getVec();\n+      unsigned minVec = std::min(outVec, inVec);\n+      auto maxBitWidth =\n+          std::max<unsigned>(128, resElemTy.getIntOrFloatBitWidth());\n+      auto vecBitWidth = resElemTy.getIntOrFloatBitWidth() * minVec;\n+      auto bitWidth = std::min<unsigned>(maxBitWidth, vecBitWidth);\n+      auto byteWidth = bitWidth / 8;\n+\n+      // If the load byte width is not eligible or the current compute\n+      // capability does not support async copy, then we do decompose\n+      if (triton::gpu::InsertSliceAsyncOp::getEligibleLoadByteWidth(\n+              computeCapability)\n+              .contains(byteWidth) &&\n+          computeCapability >= 80)\n+        return;\n+\n+      // load\n+      auto tmpTy =\n+          RankedTensorType::get(srcTy.getShape(), resElemTy, srcBlocked);\n       auto loadOp = builder.create<triton::LoadOp>(\n           insertSliceAsyncOp.getLoc(), tmpTy, insertSliceAsyncOp.src(),\n           insertSliceAsyncOp.mask(), insertSliceAsyncOp.other(),\n           insertSliceAsyncOp.cache(), insertSliceAsyncOp.evict(),\n           insertSliceAsyncOp.isVolatile());\n+\n       // insert_slice\n       auto axis = insertSliceAsyncOp.axis();\n       auto intAttr = [&](int64_t v) { return builder.getI64IntegerAttr(v); };\n@@ -4686,10 +4663,20 @@ class ConvertTritonGPUToLLVM\n       // Replace\n       insertSliceAsyncOp.replaceAllUsesWith(insertSliceOp.getResult());\n       insertSliceAsyncOp.erase();\n+      decomposed = true;\n     });\n \n+    // async wait is supported in Ampere and later\n     mod.walk([&](triton::gpu::AsyncWaitOp asyncWaitOp) -> void {\n-      asyncWaitOp.erase();\n+      if (computeCapability < 80) {\n+        asyncWaitOp.erase();\n+      } else if (decomposed) {\n+        OpBuilder builder(asyncWaitOp);\n+        // Wait for all previous async ops\n+        auto newAsyncWaitOp = builder.create<triton::gpu::AsyncWaitOp>(\n+            asyncWaitOp.getLoc(), builder.getI64IntegerAttr(0));\n+        asyncWaitOp.erase();\n+      }\n     });\n   }\n \n@@ -4712,7 +4699,7 @@ class ConvertTritonGPUToLLVM\n \n     // step 1: Decompose unoptimized layout conversions to use shared memory\n     // step 2: Decompose insert_slice_async to use load + insert_slice for\n-    // pre-Ampere architectures\n+    // pre-Ampere architectures or unsupported vectorized load sizes\n     // step 3: Allocate shared memories and insert barriers\n     // step 4: Convert SCF to CFG\n     // step 5: Convert FuncOp to LLVMFuncOp via partial conversion\n@@ -4722,13 +4709,13 @@ class ConvertTritonGPUToLLVM\n     // separation between 1/4 is that, step 3 is out of the scope of Dialect\n     // Conversion, thus we need to make sure the smem is not revised during the\n     // conversion of step 4.\n-\n     decomposeBlockedToDotOperand(mod);\n \n-    decomposeInsertSliceAsyncOp(mod, typeConverter);\n+    decomposeInsertSliceAsyncOp(mod);\n \n     Allocation allocation(mod);\n-    MembarAnalysis membar(&allocation);\n+    MembarAnalysis membarPass(&allocation);\n+    membarPass.run();\n \n     RewritePatternSet scf_patterns(context);\n     mlir::populateLoopToStdConversionPatterns(scf_patterns);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 50, "deletions": 13, "changes": 63, "file_content_changes": "@@ -113,10 +113,11 @@\n \n namespace mlir {\n namespace LLVM {\n+using namespace mlir::triton;\n \n-static Value getStructFromElements(Location loc, ValueRange resultVals,\n-                                   ConversionPatternRewriter &rewriter,\n-                                   Type structType) {\n+Value getStructFromElements(Location loc, ValueRange resultVals,\n+                            ConversionPatternRewriter &rewriter,\n+                            Type structType) {\n   if (!structType.isa<LLVM::LLVMStructType>()) {\n     return *resultVals.begin();\n   }\n@@ -130,9 +131,8 @@ static Value getStructFromElements(Location loc, ValueRange resultVals,\n   return llvmStruct;\n }\n \n-static SmallVector<Value>\n-getElementsFromStruct(Location loc, Value llvmStruct,\n-                      ConversionPatternRewriter &rewriter) {\n+SmallVector<Value> getElementsFromStruct(Location loc, Value llvmStruct,\n+                                         ConversionPatternRewriter &rewriter) {\n   if (llvmStruct.getType().isIntOrIndexOrFloat() ||\n       llvmStruct.getType().isa<triton::PointerType>() ||\n       llvmStruct.getType().isa<LLVM::LLVMPointerType>())\n@@ -147,9 +147,6 @@ getElementsFromStruct(Location loc, Value llvmStruct,\n   return results;\n }\n \n-namespace {\n-using namespace mlir::triton;\n-\n // Create a 32-bit integer constant.\n Value createConstantI32(Location loc, PatternRewriter &rewriter, int32_t v) {\n   auto i32ty = rewriter.getIntegerType(32);\n@@ -185,10 +182,8 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n                                           builder.getIntegerAttr(ty, value));\n }\n \n-} // namespace\n-\n /// Helper function to get strides from a given shape and its order\n-static SmallVector<Value>\n+SmallVector<Value>\n getStridesFromShapeAndOrder(ArrayRef<int64_t> shape, ArrayRef<unsigned> order,\n                             Location loc, ConversionPatternRewriter &rewriter) {\n   auto rank = shape.size();\n@@ -264,7 +259,7 @@ struct SharedMemoryObject {\n   }\n };\n \n-static SharedMemoryObject\n+SharedMemoryObject\n getSharedMemoryObjectFromStruct(Location loc, Value llvmStruct,\n                                 ConversionPatternRewriter &rewriter) {\n   auto elems = getElementsFromStruct(loc, llvmStruct, rewriter);\n@@ -274,6 +269,48 @@ getSharedMemoryObjectFromStruct(Location loc, Value llvmStruct,\n           /*offsets=*/{elems.begin() + 1 + rank, elems.end()}};\n }\n \n+Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n+                  Value val, Value pred) {\n+  MLIRContext *ctx = rewriter.getContext();\n+  unsigned bits = val.getType().getIntOrFloatBitWidth();\n+  const char *c = bits == 64 ? \"l\" : (bits == 16 ? \"h\" : \"r\");\n+\n+  PTXBuilder builder;\n+  auto *ptrOpr = builder.newAddrOperand(ptr, \"r\");\n+  auto *valOpr = builder.newOperand(val, c);\n+  auto &st = builder.create<>(\"st\")->shared().b(bits);\n+  st(ptrOpr, valOpr).predicate(pred, \"b\");\n+  return builder.launch(rewriter, loc, void_ty(ctx));\n+}\n+\n+Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n+               int i) {\n+  unsigned bits = val.getType().getIntOrFloatBitWidth();\n+\n+  if (bits == 64) {\n+    Type vecTy = vec_ty(f32_ty, 2);\n+    Value vec = bitcast(val, vecTy);\n+    Value val0 = extract_element(f32_ty, vec, i32_val(0));\n+    Value val1 = extract_element(f32_ty, vec, i32_val(1));\n+    val0 = shflSync(loc, rewriter, val0, i);\n+    val1 = shflSync(loc, rewriter, val1, i);\n+    vec = undef(vecTy);\n+    vec = insert_element(vecTy, vec, val0, i32_val(0));\n+    vec = insert_element(vecTy, vec, val1, i32_val(1));\n+    return bitcast(vec, val.getType());\n+  }\n+\n+  PTXBuilder builder;\n+  auto &shfl = builder.create(\"shfl.sync\")->o(\"bfly\").o(\"b32\");\n+  auto *dOpr = builder.newOperand(\"=r\");\n+  auto *aOpr = builder.newOperand(val, \"r\");\n+  auto *bOpr = builder.newConstantOperand(i);\n+  auto *cOpr = builder.newConstantOperand(\"0x1f\");\n+  auto *maskOpr = builder.newConstantOperand(\"0xffffffff\");\n+  shfl(dOpr, aOpr, bOpr, cOpr, maskOpr);\n+  return builder.launch(rewriter, loc, val.getType(), false);\n+}\n+\n } // namespace LLVM\n } // namespace mlir\n "}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 97, "deletions": 6, "changes": 103, "file_content_changes": "@@ -252,6 +252,51 @@ struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n   }\n };\n \n+struct TritonTransPattern : public OpConversionPattern<triton::TransOp> {\n+\n+  using OpConversionPattern<triton::TransOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::TransOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Value src = adaptor.src();\n+    auto srcType = src.getType().cast<RankedTensorType>();\n+    Attribute srcEncoding = srcType.getEncoding();\n+    if (!srcEncoding)\n+      return failure();\n+    if (!srcEncoding.isa<triton::gpu::SharedEncodingAttr>()) {\n+      // TODO: end-to-end correctness is broken if\n+      // the input is blocked and the output is shared\n+      // with different order. Maybe a backend issue in BlockedToShared?\n+      SmallVector<unsigned> order = {1, 0};\n+      if (auto srcBlockedEncoding =\n+              srcEncoding.dyn_cast<triton::gpu::BlockedEncodingAttr>())\n+        llvm::copy(srcBlockedEncoding.getOrder(), order.begin());\n+      srcEncoding =\n+          triton::gpu::SharedEncodingAttr::get(getContext(), 1, 1, 1, order);\n+      srcType = RankedTensorType::get(srcType.getShape(),\n+                                      srcType.getElementType(), srcEncoding);\n+      src = rewriter.create<triton::gpu::ConvertLayoutOp>(src.getLoc(), srcType,\n+                                                          src);\n+    }\n+    auto srcSharedEncoding =\n+        srcEncoding.cast<triton::gpu::SharedEncodingAttr>();\n+    SmallVector<unsigned> retOrder(srcSharedEncoding.getOrder().begin(),\n+                                   srcSharedEncoding.getOrder().end());\n+    SmallVector<int64_t> retShapes(srcType.getShape().begin(),\n+                                   srcType.getShape().end());\n+    std::reverse(retOrder.begin(), retOrder.end());\n+    std::reverse(retShapes.begin(), retShapes.end());\n+    auto retEncoding =\n+        triton::gpu::SharedEncodingAttr::get(getContext(), 1, 1, 1, retOrder);\n+    auto retType =\n+        RankedTensorType::get(retShapes, srcType.getElementType(), retEncoding);\n+\n+    rewriter.replaceOpWithNewOp<triton::TransOp>(op, retType, src);\n+    return success();\n+  }\n+};\n+\n struct TritonLoadPattern : public OpConversionPattern<triton::LoadOp> {\n   using OpConversionPattern<triton::LoadOp>::OpConversionPattern;\n \n@@ -286,8 +331,8 @@ struct TritonAtomicCASPattern\n   matchAndRewrite(triton::AtomicCASOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     rewriter.replaceOpWithNewOp<triton::AtomicCASOp>(\n-        op, typeConverter->convertType(op.getType()), \n-        adaptor.ptr(), adaptor.cmp(), adaptor.val());\n+        op, typeConverter->convertType(op.getType()), adaptor.ptr(),\n+        adaptor.cmp(), adaptor.val());\n     return success();\n   }\n };\n@@ -390,9 +435,10 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n       TritonGenericPattern<triton::PtrToIntOp>,\n       TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n       TritonGenericPattern<triton::AddPtrOp>, TritonReducePattern,\n-      TritonExpandDimsPattern, TritonMakeRangePattern, TritonDotPattern,\n-      TritonLoadPattern, TritonStorePattern, TritonExtElemwisePattern,\n-      TritonPrintfPattern, TritonAtomicRMWPattern>(typeConverter, context);\n+      TritonTransPattern, TritonExpandDimsPattern, TritonMakeRangePattern,\n+      TritonDotPattern, TritonLoadPattern, TritonStorePattern,\n+      TritonExtElemwisePattern, TritonPrintfPattern, TritonAtomicRMWPattern>(\n+      typeConverter, context);\n }\n \n //\n@@ -456,10 +502,55 @@ struct SCFYieldPattern : public OpConversionPattern<scf::YieldOp> {\n   }\n };\n \n+// This is borrowed from ConvertFIfOpTypes in\n+//    SCF/Transforms/StructuralTypeConversions.cpp\n+class SCFIfPattern : public OpConversionPattern<scf::IfOp> {\n+public:\n+  using OpConversionPattern<scf::IfOp>::OpConversionPattern;\n+  LogicalResult\n+  matchAndRewrite(scf::IfOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    // TODO: Generalize this to any type conversion, not just 1:1.\n+    //\n+    // We need to implement something more sophisticated here that tracks which\n+    // types convert to which other types and does the appropriate\n+    // materialization logic.\n+    // For example, it's possible that one result type converts to 0 types and\n+    // another to 2 types, so newResultTypes would at least be the right size to\n+    // not crash in the llvm::zip call below, but then we would set the the\n+    // wrong type on the SSA values! These edge cases are also why we cannot\n+    // safely use the TypeConverter::convertTypes helper here.\n+    SmallVector<Type> newResultTypes;\n+    for (auto type : op.getResultTypes()) {\n+      Type newType = typeConverter->convertType(type);\n+      if (!newType)\n+        return rewriter.notifyMatchFailure(op, \"not a 1:1 type conversion\");\n+      newResultTypes.push_back(newType);\n+    }\n+\n+    // See comments in the ForOp pattern for why we clone without regions and\n+    // then inline.\n+    scf::IfOp newOp =\n+        cast<scf::IfOp>(rewriter.cloneWithoutRegions(*op.getOperation()));\n+    rewriter.inlineRegionBefore(op.getThenRegion(), newOp.getThenRegion(),\n+                                newOp.getThenRegion().end());\n+    rewriter.inlineRegionBefore(op.getElseRegion(), newOp.getElseRegion(),\n+                                newOp.getElseRegion().end());\n+\n+    // Update the operands and types.\n+    newOp->setOperands(adaptor.getOperands());\n+    for (auto t : llvm::zip(newOp.getResults(), newResultTypes))\n+      std::get<0>(t).setType(std::get<1>(t));\n+    rewriter.replaceOp(op, newOp.getResults());\n+    return success();\n+  }\n+};\n+\n void populateSCFPatterns(TritonGPUTypeConverter &typeConverter,\n                          RewritePatternSet &patterns) {\n   MLIRContext *context = patterns.getContext();\n-  patterns.add<SCFYieldPattern, SCFForPattern>(typeConverter, context);\n+  patterns.add<SCFYieldPattern, SCFForPattern, SCFIfPattern>(typeConverter,\n+                                                             context);\n }\n \n class ConvertTritonToTritonGPU"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -241,7 +241,8 @@ mlir::LogicalResult mlir::triton::ReduceOp::inferReturnTypes(\n   auto argTy = arg.getType().cast<RankedTensorType>();\n   auto argEltTy = argTy.getElementType();\n   auto i32Ty = IntegerType::get(argEltTy.getContext(), 32);\n-  auto redOp = attributes.get(\"redOp\").cast<mlir::triton::RedOpAttr>().getValue();\n+  auto redOp =\n+      attributes.get(\"redOp\").cast<mlir::triton::RedOpAttr>().getValue();\n   bool withIndex = mlir::triton::ReduceOp::withIndex(redOp);\n   auto retEltTy = withIndex ? i32Ty : argEltTy;\n   auto retShape = argTy.getShape().vec();"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 23, "deletions": 3, "changes": 26, "file_content_changes": "@@ -71,7 +71,7 @@ unsigned getElemsPerThread(Type type) {\n   return getElemsPerThread(tensorType.getEncoding(), tensorType.getShape());\n }\n \n-SmallVector<unsigned> getThreadsPerWarp(Attribute layout) {\n+SmallVector<unsigned> getThreadsPerWarp(const Attribute &layout) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return SmallVector<unsigned>(blockedLayout.getThreadsPerWarp().begin(),\n                                  blockedLayout.getThreadsPerWarp().end());\n@@ -86,7 +86,7 @@ SmallVector<unsigned> getThreadsPerWarp(Attribute layout) {\n   return {};\n }\n \n-SmallVector<unsigned> getWarpsPerCTA(Attribute layout) {\n+SmallVector<unsigned> getWarpsPerCTA(const Attribute &layout) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return SmallVector<unsigned>(blockedLayout.getWarpsPerCTA().begin(),\n                                  blockedLayout.getWarpsPerCTA().end());\n@@ -99,7 +99,7 @@ SmallVector<unsigned> getWarpsPerCTA(Attribute layout) {\n   return {};\n }\n \n-SmallVector<unsigned> getSizePerThread(Attribute layout) {\n+SmallVector<unsigned> getSizePerThread(const Attribute &layout) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return SmallVector<unsigned>(blockedLayout.getSizePerThread().begin(),\n                                  blockedLayout.getSizePerThread().end());\n@@ -109,6 +109,8 @@ SmallVector<unsigned> getSizePerThread(Attribute layout) {\n     if (mmaLayout.getVersion() == 2) {\n       return {2, 2};\n     } else if (mmaLayout.getVersion() == 1) {\n+      // Note: here the definition of sizePerThread is obscure, which doesn't\n+      // mean vecSize=4 can be supported in the last dimension.\n       return {2, 4};\n     } else {\n       llvm_unreachable(\"Unexpected mma version\");\n@@ -140,6 +142,15 @@ SmallVector<unsigned> getSizePerThread(Attribute layout) {\n   }\n }\n \n+SmallVector<unsigned> getContigPerThread(Attribute layout) {\n+  if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    assert(mmaLayout.getVersion() == 1 || mmaLayout.getVersion() == 2);\n+    return {1, 2};\n+  } else {\n+    return getSizePerThread(layout);\n+  }\n+}\n+\n SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout) {\n   SmallVector<unsigned> threads;\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n@@ -648,6 +659,15 @@ void printInsertSliceAsyncOp(OpAsmPrinter &printer,\n   printer.printStrippedAttrOrType(insertSliceAsyncOp.result().getType());\n }\n \n+DenseSet<unsigned>\n+InsertSliceAsyncOp::getEligibleLoadByteWidth(int computeCapability) {\n+  DenseSet<unsigned> validLoadBytes;\n+  if (computeCapability >= 80) {\n+    validLoadBytes = {4, 8, 16};\n+  }\n+  return validLoadBytes;\n+}\n+\n //===----------------------------------------------------------------------===//\n // ASM Interface (i.e.: alias)\n //===----------------------------------------------------------------------===//"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 100, "deletions": 23, "changes": 123, "file_content_changes": "@@ -50,10 +50,25 @@ class DecomposeDotOperand : public mlir::RewritePattern {\n     auto dstType = convert.getType().cast<RankedTensorType>();\n     if (srcType.getEncoding().isa<triton::gpu::BlockedEncodingAttr>() &&\n         dstType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>()) {\n-      auto tmpType =\n-          RankedTensorType::get(dstType.getShape(), dstType.getElementType(),\n-                                triton::gpu::SharedEncodingAttr::get(\n-                                    op->getContext(), 1, 1, 1, {1, 0}));\n+      auto dstDotOperand =\n+          dstType.getEncoding().cast<triton::gpu::DotOperandEncodingAttr>();\n+      auto dstParent = dstDotOperand.getParent();\n+      if (dstDotOperand.getOpIdx() == 1 ||\n+          !dstParent.isa<triton::gpu::MmaEncodingAttr>())\n+        return mlir::failure();\n+      auto dstParentMma = dstParent.cast<triton::gpu::MmaEncodingAttr>();\n+      if (dstParentMma.getVersion() == 1 ||\n+          dstParentMma.getWarpsPerCTA()[1] > 1)\n+        return mlir::failure();\n+      SetVector<Operation *> bwdSlices;\n+      mlir::getBackwardSlice(convert.getResult(), &bwdSlices);\n+      if (llvm::find_if(bwdSlices, [](Operation *op) {\n+            return isa<triton::DotOp>(op);\n+          }) == bwdSlices.end())\n+        return mlir::failure();\n+\n+      auto tmpType = RankedTensorType::get(\n+          dstType.getShape(), dstType.getElementType(), dstParentMma);\n       auto tmp = rewriter.create<triton::gpu::ConvertLayoutOp>(\n           convert.getLoc(), tmpType, convert.getOperand());\n       auto newConvert = rewriter.create<triton::gpu::ConvertLayoutOp>(\n@@ -81,8 +96,11 @@ class SimplifyConversion : public mlir::RewritePattern {\n     auto convert = llvm::cast<triton::gpu::ConvertLayoutOp>(op);\n     // we don't handle conversions to DotOperandEncodingAttr\n     // this is a heuristics to accommodate fused attention\n-    // if (dstType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n-    //   return mlir::failure();\n+    auto srcType = convert.getOperand().getType().cast<RankedTensorType>();\n+    auto dstType = convert.getType().cast<RankedTensorType>();\n+    if (dstType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>() &&\n+        srcType.getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n+      return mlir::failure();\n     // convert to the same layout -- we can delete\n     if (op->getResultTypes() == op->getOperandTypes()) {\n       rewriter.replaceOp(op, op->getOperands());\n@@ -160,6 +178,10 @@ class SimplifyConversion : public mlir::RewritePattern {\n           !isSharedEncoding(convert.getResult())) {\n         return mlir::failure();\n       }\n+      if (isSharedEncoding(convert.getOperand()) &&\n+          isSharedEncoding(convert.getResult())) {\n+        return mlir::failure();\n+      }\n       auto srcType = convert.getOperand().getType().cast<RankedTensorType>();\n       auto srcShared =\n           srcType.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n@@ -586,13 +608,9 @@ mmaVersionToShapePerWarp(int version, const ArrayRef<int64_t> &shape,\n   }\n }\n \n-template <int version>\n-SmallVector<unsigned, 2> warpsPerTile(const ArrayRef<int64_t> shape,\n-                                      int numWarps);\n-\n-template <>\n-SmallVector<unsigned, 2> warpsPerTile<1>(const ArrayRef<int64_t> shape,\n-                                         int numWarps) {\n+SmallVector<unsigned, 2> warpsPerTileV1(triton::DotOp dotOp,\n+                                        const ArrayRef<int64_t> shape,\n+                                        int numWarps) {\n   SmallVector<unsigned, 2> ret = {1, 1};\n   SmallVector<int64_t, 2> shapePerWarp =\n       mmaVersionToShapePerWarp(1, shape, numWarps);\n@@ -611,17 +629,25 @@ SmallVector<unsigned, 2> warpsPerTile<1>(const ArrayRef<int64_t> shape,\n   return ret;\n }\n \n-template <>\n-SmallVector<unsigned, 2> warpsPerTile<2>(const ArrayRef<int64_t> shape,\n-                                         int numWarps) {\n+SmallVector<unsigned, 2> warpsPerTileV2(triton::DotOp dotOp,\n+                                        const ArrayRef<int64_t> shape,\n+                                        int numWarps) {\n+  SetVector<Operation *> slices;\n+  mlir::getForwardSlice(dotOp.getResult(), &slices);\n+  if (llvm::find_if(slices, [](Operation *op) {\n+        return isa<triton::DotOp>(op);\n+      }) != slices.end())\n+    return {(unsigned)numWarps, 1};\n+\n   SmallVector<unsigned, 2> ret = {1, 1};\n-  SmallVector<int64_t, 2> shapePerWarp =\n-      mmaVersionToShapePerWarp(2, shape, numWarps);\n+  SmallVector<int64_t, 2> shapePerWarp = {16, 8};\n+  bool changed = false;\n   // TODO (@daadaada): double-check.\n   // original logic in\n   // https://github.com/openai/triton/blob/master/lib/codegen/analysis/layout.cc#L252\n   // seems buggy for shape = [32, 16] ?\n   do {\n+    changed = false;\n     if (ret[0] * ret[1] >= numWarps)\n       break;\n     if (shape[0] / shapePerWarp[0] / ret[0] >=\n@@ -638,6 +664,55 @@ SmallVector<unsigned, 2> warpsPerTile<2>(const ArrayRef<int64_t> shape,\n }\n \n } // namespace\n+\n+class OptimizeBlockedToShared : public mlir::RewritePattern {\n+public:\n+  OptimizeBlockedToShared(mlir::MLIRContext *context)\n+      : RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(), 1,\n+                       context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto cvt = cast<triton::gpu::ConvertLayoutOp>(op);\n+    auto srcType = cvt.getOperand().getType().cast<RankedTensorType>();\n+    auto dstType = cvt.getResult().getType().cast<RankedTensorType>();\n+    auto srcBlockedLayout =\n+        srcType.getEncoding().dyn_cast<triton::gpu::BlockedEncodingAttr>();\n+    auto dstSharedLayout =\n+        dstType.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n+    if (!srcBlockedLayout || !dstSharedLayout)\n+      return failure();\n+    if (srcBlockedLayout.getOrder() == dstSharedLayout.getOrder())\n+      return failure();\n+    // For now only works if single use is transpose\n+    // TODO: rematerialize #shared uses\n+    auto users = op->getUsers();\n+    if (std::distance(users.begin(), users.end()) != 1 ||\n+        !isa<triton::TransOp>(*users.begin()))\n+      return failure();\n+\n+    auto tmpShared = triton::gpu::SharedEncodingAttr::get(\n+        op->getContext(), dstSharedLayout.getVec(),\n+        dstSharedLayout.getPerPhase(), dstSharedLayout.getMaxPhase(),\n+        srcBlockedLayout.getOrder());\n+    auto tmpType = RankedTensorType::get(srcType.getShape(),\n+                                         srcType.getElementType(), tmpShared);\n+    auto tmpCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+        op->getLoc(), tmpType, cvt.getOperand());\n+\n+    auto newDstType = RankedTensorType::get(\n+        users.begin()->getResultTypes()[0].cast<RankedTensorType>().getShape(),\n+        srcType.getElementType(), dstSharedLayout);\n+\n+    auto newTrans = rewriter.create<triton::TransOp>(op->getLoc(), newDstType,\n+                                                     tmpCvt.getResult());\n+\n+    rewriter.replaceOp(*users.begin(), newTrans.getResult());\n+    return success();\n+  }\n+};\n+\n class BlockedToMMA : public mlir::RewritePattern {\n   int computeCapability;\n \n@@ -646,13 +721,14 @@ class BlockedToMMA : public mlir::RewritePattern {\n       : mlir::RewritePattern(triton::DotOp::getOperationName(), 2, context),\n         computeCapability(computeCapability) {}\n \n-  static SmallVector<unsigned, 2> getWarpsPerTile(const ArrayRef<int64_t> shape,\n+  static SmallVector<unsigned, 2> getWarpsPerTile(triton::DotOp dotOp,\n+                                                  const ArrayRef<int64_t> shape,\n                                                   int version, int numWarps) {\n     switch (version) {\n     case 1:\n-      return warpsPerTile<1>(shape, numWarps);\n+      return warpsPerTileV1(dotOp, shape, numWarps);\n     case 2:\n-      return warpsPerTile<2>(shape, numWarps);\n+      return warpsPerTileV2(dotOp, shape, numWarps);\n     default:\n       assert(false && \"not supported version\");\n       return {0, 0};\n@@ -684,7 +760,7 @@ class BlockedToMMA : public mlir::RewritePattern {\n         retShape, oldRetType.getElementType(),\n         triton::gpu::MmaEncodingAttr::get(\n             oldRetType.getContext(), version,\n-            getWarpsPerTile(retShape, version, numWarps)));\n+            getWarpsPerTile(dotOp, retShape, version, numWarps)));\n     // convert accumulator\n     auto oldAcc = dotOp.getOperand(2);\n     auto newAcc = rewriter.create<triton::gpu::ConvertLayoutOp>(\n@@ -731,8 +807,9 @@ class TritonGPUCombineOpsPass\n \n     mlir::RewritePatternSet patterns(context);\n \n+    patterns.add<OptimizeBlockedToShared>(context);\n     patterns.add<SimplifyConversion>(context);\n-    // patterns.add<DecomposeDotOperand>(context);\n+    patterns.add<DecomposeDotOperand>(context);\n     patterns.add<RematerializeBackward>(context);\n     patterns.add<RematerializeForward>(context);\n     patterns.add<MoveConvertOutOfLoop>(context);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 37, "deletions": 39, "changes": 76, "file_content_changes": "@@ -25,18 +25,20 @@ static Type getI1SameShape(Value v) {\n                                tensorType.getEncoding());\n }\n \n+#define int_attr(num) builder.getI64IntegerAttr(num)\n+\n namespace {\n \n class LoopPipeliner {\n-  /// cache forOp we are working on\n+  /// Cache forOp we are working on\n   scf::ForOp forOp;\n \n-  /// cache YieldOp for this forOp\n+  /// Cache YieldOp for this forOp\n   scf::YieldOp yieldOp;\n \n-  /// loads to be pipelined\n+  /// Loads to be pipelined\n   SetVector<Value> loads;\n-  /// the value that each load will be mapped to (after layout conversion)\n+  /// The value that each load will be mapped to (after layout conversion)\n   DenseMap<Value, Value> loadsMapping;\n   /// load => buffer\n   DenseMap<Value, Value> loadsBuffer;\n@@ -51,7 +53,7 @@ class LoopPipeliner {\n   ///\n   Value loopIterIdx;\n \n-  /// comments on numStages:\n+  /// Comments on numStages:\n   ///   [0, numStages-1) are in the prologue\n   ///   numStages-1 is appended after the loop body\n   int numStages;\n@@ -61,6 +63,7 @@ class LoopPipeliner {\n \n   /// Block arguments that loads depend on\n   DenseSet<BlockArgument> depArgs;\n+\n   /// Operations (inside the loop body) that loads depend on\n   DenseSet<Operation *> depOps;\n \n@@ -71,7 +74,7 @@ class LoopPipeliner {\n \n   Value lookupOrDefault(Value origin, int stage);\n \n-  /// returns a empty buffer of size <numStages, ...>\n+  /// Returns a empty buffer of size <numStages, ...>\n   ttg::AllocTensorOp allocateEmptyBuffer(Operation *op, OpBuilder &builder);\n \n public:\n@@ -84,7 +87,7 @@ class LoopPipeliner {\n   /// Collect loads to pipeline. Return success if we can pipeline this loop\n   LogicalResult initialize();\n \n-  /// emit pipelined loads (before loop body)\n+  /// Emit pipelined loads (before loop body)\n   void emitPrologue();\n \n   /// emit pipelined loads (after loop body)\n@@ -134,7 +137,7 @@ void LoopPipeliner::collectDeps(Value v, int stages, DenseSet<Value> &deps) {\n \n ttg::AllocTensorOp LoopPipeliner::allocateEmptyBuffer(Operation *op,\n                                                       OpBuilder &builder) {\n-  // allocate a buffer for each pipelined tensor\n+  // Allocate a buffer for each pipelined tensor\n   // shape: e.g. (numStages==4), <32x64xbf16> -> <4x32x64xbf16>\n   Value convertLayout = loadsMapping[op->getResult(0)];\n   if (auto tensorType = convertLayout.getType().dyn_cast<RankedTensorType>()) {\n@@ -215,9 +218,9 @@ LogicalResult LoopPipeliner::initialize() {\n       loads.insert(loadOp);\n   }\n \n-  // we have some loads to pipeline\n+  // We have some loads to pipeline\n   if (!loads.empty()) {\n-    // update depArgs & depOps\n+    // Update depArgs & depOps\n     for (Value loadOp : loads) {\n       for (Value dep : loadDeps[loadOp]) {\n         // TODO: we should record the stage that the value is depended on\n@@ -244,23 +247,20 @@ void LoopPipeliner::emitPrologue() {\n     setValueMapping(arg, operand.get(), 0);\n   }\n \n-  // helper to construct int attribute\n-  auto intAttr = [&](int64_t val) { return builder.getI64IntegerAttr(val); };\n-\n   // prologue from [0, numStage-1)\n   Value iv = forOp.getLowerBound();\n   pipelineIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n   for (int stage = 0; stage < numStages - 1; ++stage) {\n-    // special handling for induction variable as the increment is implicit\n+    // Special handling for induction variable as the increment is implicit\n     if (stage != 0)\n       iv = builder.create<arith::AddIOp>(iv.getLoc(), iv, forOp.getStep());\n     setValueMapping(forOp.getInductionVar(), iv, stage);\n \n-    // special handling for loop condition as there is no condition in ForOp\n+    // Special handling for loop condition as there is no condition in ForOp\n     Value loopCond = builder.create<arith::CmpIOp>(\n         iv.getLoc(), arith::CmpIPredicate::slt, iv, forOp.getUpperBound());\n \n-    // rematerialize peeled values\n+    // Rematerialize peeled values\n     SmallVector<Operation *> orderedDeps;\n     for (Operation &op : forOp.getLoopBody().front()) {\n       if (depOps.contains(&op))\n@@ -314,7 +314,7 @@ void LoopPipeliner::emitPrologue() {\n         }\n       }\n \n-      // update mapping of results\n+      // Update mapping of results\n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n         Value originalResult = op->getResult(dstIdx);\n         // copy_async will update the value of its only use\n@@ -350,13 +350,14 @@ void LoopPipeliner::emitPrologue() {\n                               loadsBufferType[loadOp].getEncoding());\n     Value extractSlice = builder.create<tensor::ExtractSliceOp>(\n         loadOp.getLoc(), sliceType, loadStageBuffer[loadOp][numStages - 1],\n-        SmallVector<OpFoldResult>{intAttr(0), intAttr(0), intAttr(0)},\n-        SmallVector<OpFoldResult>{intAttr(1), intAttr(sliceType.getShape()[0]),\n-                                  intAttr(sliceType.getShape()[1])},\n-        SmallVector<OpFoldResult>{intAttr(1), intAttr(1), intAttr(1)});\n+        SmallVector<OpFoldResult>{int_attr(0), int_attr(0), int_attr(0)},\n+        SmallVector<OpFoldResult>{int_attr(1),\n+                                  int_attr(sliceType.getShape()[0]),\n+                                  int_attr(sliceType.getShape()[1])},\n+        SmallVector<OpFoldResult>{int_attr(1), int_attr(1), int_attr(1)});\n     loadsExtract[loadOp] = extractSlice;\n   }\n-  // bump up loopIterIdx, this is used for getting the correct slice for the\n+  // Bump up loopIterIdx, this is used for getting the correct slice for the\n   // *next* iteration\n   loopIterIdx = builder.create<arith::AddIOp>(\n       loopIterIdx.getLoc(), loopIterIdx,\n@@ -365,9 +366,6 @@ void LoopPipeliner::emitPrologue() {\n \n void LoopPipeliner::emitEpilogue() {\n   // If there's any outstanding async copies, we need to wait for them.\n-  // TODO(Keren): We may want to completely avoid the async copies in the last\n-  // few iterations by setting is_masked attribute to true. We don't want to use\n-  // the mask operand because it's a tensor but not a scalar.\n   OpBuilder builder(forOp);\n   OpBuilder::InsertionGuard g(builder);\n   builder.setInsertionPointAfter(forOp);\n@@ -376,9 +374,8 @@ void LoopPipeliner::emitEpilogue() {\n \n scf::ForOp LoopPipeliner::createNewForOp() {\n   OpBuilder builder(forOp);\n-  auto intAttr = [&](int64_t v) { return builder.getI64IntegerAttr(v); };\n \n-  // order of new args:\n+  // Order of new args:\n   //   (original args),\n   //   (insertSliceAsync buffer at stage numStages - 1)  for each load\n   //   (extracted tensor)  for each load\n@@ -465,15 +462,15 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n                     newForOp.getRegionIterArgs()[argIdx + depArgsBeginIdx]);\n     ++argIdx;\n   }\n-  // special handling for iv & loop condition\n+  // Special handling for iv & loop condition\n   Value nextIV = builder.create<arith::AddIOp>(\n       newForOp.getInductionVar().getLoc(),\n       newForOp.getRegionIterArgs()[nextIVIdx], newForOp.getStep());\n   Value nextLoopCond =\n       builder.create<arith::CmpIOp>(nextIV.getLoc(), arith::CmpIPredicate::slt,\n                                     nextIV, newForOp.getUpperBound());\n \n-  // slice index\n+  // Slice index\n   SmallVector<Value> nextBuffers;\n   SmallVector<Value> extractSlices;\n \n@@ -490,7 +487,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n \n   for (Operation *op : orderedDeps) {\n     Operation *nextOp = nullptr;\n-    // update loading mask\n+    // Update loading mask\n     if (loads.contains(op->getResult(0))) {\n       auto loadOp = llvm::cast<triton::LoadOp>(op);\n       Value mask = loadOp.mask();\n@@ -500,7 +497,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n             mask.getLoc(), mask.getType(), nextLoopCond);\n         newMask = builder.create<arith::AndIOp>(\n             mask.getLoc(), splatCond, nextMapping.lookupOrDefault(mask));\n-        // if mask is defined outside the loop, don't update the map more than\n+        // If mask is defined outside the loop, don't update the map more than\n         // once\n         if (!(forOp.isDefinedOutsideOfLoop(mask) && nextMapping.contains(mask)))\n           nextMapping.map(mask, newMask);\n@@ -522,18 +519,19 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n                                         loadsBufferType[loadOp].getEncoding());\n       nextOp = builder.create<tensor::ExtractSliceOp>(\n           op->getLoc(), sliceType, insertAsyncOp,\n-          SmallVector<OpFoldResult>{extractSliceIndex, intAttr(0), intAttr(0)},\n-          SmallVector<OpFoldResult>{intAttr(1),\n-                                    intAttr(sliceType.getShape()[0]),\n-                                    intAttr(sliceType.getShape()[1])},\n-          SmallVector<OpFoldResult>{intAttr(1), intAttr(1), intAttr(1)});\n+          SmallVector<OpFoldResult>{extractSliceIndex, int_attr(0),\n+                                    int_attr(0)},\n+          SmallVector<OpFoldResult>{int_attr(1),\n+                                    int_attr(sliceType.getShape()[0]),\n+                                    int_attr(sliceType.getShape()[1])},\n+          SmallVector<OpFoldResult>{int_attr(1), int_attr(1), int_attr(1)});\n       extractSlices.push_back(nextOp->getResult(0));\n     } else\n       nextOp = builder.clone(*op, nextMapping);\n-    // update mapping of results\n+    // Update mapping of results\n     for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n       nextMapping.map(op->getResult(dstIdx), nextOp->getResult(dstIdx));\n-      // if this is a loop-carried value, update the mapping for yield\n+      // If this is a loop-carried value, update the mapping for yield\n       auto originYield = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n       for (OpOperand &operand : originYield->getOpOperands()) {\n         if (operand.get() == op->getResult(dstIdx)) {\n@@ -583,7 +581,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n     it->getDefiningOp()->moveAfter(asyncWait);\n   }\n \n-  // bump iteration count\n+  // Bump iteration count\n   pipelineIterIdx = builder.create<arith::AddIOp>(\n       nextIV.getLoc(), pipelineIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), 1, 32));"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -131,6 +131,11 @@ LogicalResult Prefetcher::initialize() {\n   if (dotsInFor.empty())\n     return failure();\n \n+  // TODO: segfault (original for still has uses)\n+  // when used in flash attention that has 2 dots in the loop\n+  if (dotsInFor.size() > 1)\n+    return failure();\n+\n   // returns source of cvt\n   auto getPrefetchSrc = [](Value v) -> Value {\n     if (auto cvt = v.getDefiningOp<triton::gpu::ConvertLayoutOp>())"}, {"filename": "python/setup.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -25,8 +25,8 @@ def get_build_type():\n     elif check_env_flag(\"REL_WITH_DEB_INFO\"):\n         return \"RelWithDebInfo\"\n     else:\n-        return \"Debug\"\n-        # TODO(Keren): Restore this before we merge into master\n+        return \"RelWithDebInfo\"\n+        # TODO: change to release when stable enough\n         #return \"Release\"\n \n "}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 35, "deletions": 1, "changes": 36, "file_content_changes": "@@ -11,6 +11,7 @@\n #include \"mlir/Parser.h\"\n #include \"mlir/Support/FileUtilities.h\"\n \n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"triton/Analysis/Allocation.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h\"\n #include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPU.h\"\n@@ -115,6 +116,10 @@ void init_triton_ir(py::module &&m) {\n       .def(py::init<>())\n       .def(\"load_triton\", [](mlir::MLIRContext &self) {\n         self.getOrLoadDialect<mlir::triton::TritonDialect>();\n+        // we load LLVM because the frontend uses LLVM.undef for\n+        // some placeholders\n+        self.getOrLoadDialect<mlir::triton::TritonDialect>();\n+        self.getOrLoadDialect<mlir::LLVM::LLVMDialect>();\n       });\n   // .def(py::init([](){\n   //   mlir::MLIRContext context;\n@@ -187,6 +192,7 @@ void init_triton_ir(py::module &&m) {\n                /* issue a warning */\n              }\n            })\n+      .def(\"get_context\", &mlir::Value::getContext)\n       .def(\"replace_all_uses_with\",\n            [](mlir::Value &self, mlir::Value &newValue) {\n              self.replaceAllUsesWith(newValue);\n@@ -335,10 +341,21 @@ void init_triton_ir(py::module &&m) {\n         return funcs[0];\n       });\n \n+  m.def(\"make_attr\",\n+        [](const std::vector<int> &values, mlir::MLIRContext &context) {\n+          return mlir::DenseIntElementsAttr::get(\n+                     mlir::RankedTensorType::get(\n+                         {static_cast<int64_t>(values.size())},\n+                         mlir::IntegerType::get(&context, 32)),\n+                     values)\n+              .cast<mlir::Attribute>();\n+        });\n+\n   m.def(\n       \"parse_mlir_module\",\n       [](const std::string &inputFilename, mlir::MLIRContext &context) {\n         // initialize registry\n+        // note: we initialize llvm for undef\n         mlir::DialectRegistry registry;\n         registry.insert<mlir::triton::TritonDialect,\n                         mlir::triton::gpu::TritonGPUDialect,\n@@ -1068,6 +1085,16 @@ void init_triton_ir(py::module &&m) {\n                  mlir::RankedTensorType::get(shape, lhsType.getElementType()),\n                  lhs, rhs);\n            })\n+      .def(\"create_trans\",\n+           [](mlir::OpBuilder &self, mlir::Value &arg) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             auto argType = arg.getType().dyn_cast<mlir::RankedTensorType>();\n+             auto argEltType = argType.getElementType();\n+             std::vector<int64_t> retShape = argType.getShape();\n+             std::reverse(retShape.begin(), retShape.end());\n+             return self.create<mlir::triton::TransOp>(\n+                 loc, mlir::RankedTensorType::get(retShape, argEltType), arg);\n+           })\n       .def(\"create_broadcast\",\n            [](mlir::OpBuilder &self, mlir::Value &arg,\n               std::vector<int64_t> &shape) -> mlir::Value {\n@@ -1096,7 +1123,8 @@ void init_triton_ir(py::module &&m) {\n               mlir::Value &val) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n              mlir::Type dstType;\n-             if (auto srcTensorType = ptr.getType().dyn_cast<mlir::RankedTensorType>()) {\n+             if (auto srcTensorType =\n+                     ptr.getType().dyn_cast<mlir::RankedTensorType>()) {\n                mlir::Type dstElemType = srcTensorType.getElementType()\n                                             .cast<mlir::triton::PointerType>()\n                                             .getPointeeType();\n@@ -1232,6 +1260,12 @@ void init_triton_ir(py::module &&m) {\n                  mlir::StringAttr::get(self.getContext(),\n                                        llvm::StringRef(prefix)),\n                  values);\n+           })\n+      // Undef\n+      .def(\"create_undef\",\n+           [](mlir::OpBuilder &self, mlir::Type &type) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return self.create<::mlir::LLVM::UndefOp>(loc, type);\n            });\n \n   py::class_<mlir::PassManager>(m, \"pass_manager\")"}, {"filename": "python/tests/test_blocksparse.py", "status": "added", "additions": 188, "deletions": 0, "changes": 188, "file_content_changes": "@@ -0,0 +1,188 @@\n+import pytest\n+import torch\n+\n+import triton\n+\n+# TODO: float32 fails\n+\n+@pytest.mark.parametrize(\"MODE\", [\"sdd\", \"dds\", \"dsd\"])\n+@pytest.mark.parametrize(\"TRANS_B\", [False, True])\n+@pytest.mark.parametrize(\"TRANS_A\", [False, True])\n+@pytest.mark.parametrize(\"BLOCK\", [16, 32, 64])\n+@pytest.mark.parametrize(\"DTYPE\", [torch.float16])\n+def test_matmul(MODE, TRANS_A, TRANS_B, BLOCK, DTYPE, Z=3, H=2, M=512, N=256, K=384):\n+    seed = 0\n+    torch.manual_seed(seed)\n+    is_sdd = MODE == \"sdd\"\n+    is_dsd = MODE == \"dsd\"\n+    is_dds = MODE == \"dds\"\n+    do_sparsify = lambda x: triton.testing.sparsify_tensor(x, layout, BLOCK)\n+    do_mask = lambda x: triton.testing.mask_tensor(x, layout, BLOCK)\n+    # create inputs\n+    # create op\n+    a_shape = (Z, H, K, M) if TRANS_A else (Z, H, M, K)\n+    b_shape = (Z, H, N, K) if TRANS_B else (Z, H, K, N)\n+    c_shape = (Z, H, M, N)\n+    shape = {\n+        \"sdd\": (M, N),\n+        \"dsd\": (a_shape[2], a_shape[3]),\n+        \"dds\": (b_shape[2], b_shape[3]),\n+    }[MODE]\n+    layout = torch.randint(2, (H, shape[0] // BLOCK, shape[1] // BLOCK))\n+    layout[1, 2, :] = 0\n+    layout[1, :, 1] = 0\n+    # create data\n+    a_ref, a_tri = triton.testing.make_pair(a_shape, alpha=.1, dtype=DTYPE)\n+    b_ref, b_tri = triton.testing.make_pair(b_shape, alpha=.1, dtype=DTYPE)\n+    dc_ref, dc_tri = triton.testing.make_pair(c_shape, dtype=DTYPE)\n+    # compute [torch]\n+    dc_ref = do_mask(dc_ref) if is_sdd else dc_ref\n+    a_ref = do_mask(a_ref) if is_dsd else a_ref\n+    b_ref = do_mask(b_ref) if is_dds else b_ref\n+    a_ref.requires_grad_().retain_grad()\n+    b_ref.requires_grad_().retain_grad()\n+    c_ref = torch.matmul(a_ref.transpose(2, 3) if TRANS_A else a_ref,\n+                         b_ref.transpose(2, 3) if TRANS_B else b_ref)\n+    c_ref.backward(dc_ref)\n+    c_ref = do_sparsify(c_ref) if is_sdd else c_ref\n+    da_ref = do_sparsify(a_ref.grad) if is_dsd else a_ref.grad\n+    db_ref = do_sparsify(b_ref.grad) if is_dds else b_ref.grad\n+    # triton result\n+    dc_tri = do_sparsify(dc_tri) if is_sdd else dc_tri\n+    a_tri = do_sparsify(a_tri) if is_dsd else a_tri\n+    b_tri = do_sparsify(b_tri) if is_dds else b_tri\n+    a_tri.requires_grad_().retain_grad()\n+    b_tri.requires_grad_().retain_grad()\n+    op = triton.ops.blocksparse.matmul(layout, BLOCK, MODE, trans_a=TRANS_A, trans_b=TRANS_B, device=\"cuda\")\n+    c_tri = triton.testing.catch_oor(lambda: op(a_tri, b_tri), pytest)\n+    triton.testing.catch_oor(lambda: c_tri.backward(dc_tri), pytest)\n+    da_tri = a_tri.grad\n+    db_tri = b_tri.grad\n+    # compare\n+    triton.testing.assert_almost_equal(c_ref, c_tri)\n+    triton.testing.assert_almost_equal(da_ref, da_tri)\n+    triton.testing.assert_almost_equal(db_ref, db_tri)\n+\n+\n+configs = [\n+    (16, 256),\n+    (32, 576),\n+    (64, 1871),\n+    (128, 2511),\n+]\n+\n+\n+@pytest.mark.parametrize(\"is_dense\", [False, True])\n+@pytest.mark.parametrize(\"BLOCK, WIDTH\", configs)\n+def test_softmax(BLOCK, WIDTH, is_dense, Z=2, H=2, is_causal=True, scale=0.4):\n+    # set seed\n+    torch.random.manual_seed(0)\n+    Z, H, M, N = 2, 3, WIDTH, WIDTH\n+    # initialize layout\n+    # make sure each row has at least one non-zero element\n+    layout = torch.randint(2, (H, M // BLOCK, N // BLOCK))\n+    if is_dense:\n+        layout[:] = 1\n+    else:\n+        layout[1, 2, :] = 0\n+        layout[1, :, 1] = 0\n+    # initialize data\n+    a_shape = (Z, H, M, N)\n+    a_ref, a_tri = triton.testing.make_pair(a_shape)\n+    dout_ref, dout_tri = triton.testing.make_pair(a_shape)\n+    # compute [torch]\n+    a_ref = triton.testing.mask_tensor(a_ref, layout, BLOCK, value=float(\"-inf\"))\n+    a_ref.retain_grad()\n+    at_mask = torch.ones((M, N), device=\"cuda\")\n+    if is_causal:\n+        at_mask = torch.tril(at_mask)\n+    M = at_mask[None, None, :, :] + torch.zeros_like(a_ref)\n+    a_ref[M == 0] = float(\"-inf\")\n+    out_ref = torch.softmax(a_ref * scale, -1)\n+    out_ref.backward(dout_ref)\n+    out_ref = triton.testing.sparsify_tensor(out_ref, layout, BLOCK)\n+    da_ref = triton.testing.sparsify_tensor(a_ref.grad, layout, BLOCK)\n+    # compute [triton]\n+    a_tri = triton.testing.sparsify_tensor(a_tri, layout, BLOCK)\n+    a_tri.retain_grad()\n+    dout_tri = triton.testing.sparsify_tensor(dout_tri, layout, BLOCK)\n+    op = triton.ops.blocksparse.softmax(layout, BLOCK, device=\"cuda\", is_dense=is_dense)\n+    out_tri = op(a_tri, scale=scale, is_causal=is_causal)\n+    out_tri.backward(dout_tri)\n+    da_tri = a_tri.grad\n+    # compare\n+    triton.testing.assert_almost_equal(out_tri, out_ref)\n+    triton.testing.assert_almost_equal(da_tri, da_ref)\n+\n+\n+@pytest.mark.parametrize(\"block\", [16, 32, 64])\n+@pytest.mark.parametrize(\"dtype\", [torch.float16])\n+def test_attention_fwd_bwd(\n+    block,\n+    dtype,\n+    input_scale=1.0,\n+    scale=1 / 8.0,\n+    n_ctx=256,\n+    batch_size=2,\n+    n_heads=2,\n+):\n+    # inputs\n+    qkv_shape = (batch_size, n_heads, n_ctx, 64)\n+    qkvs = [\n+        torch.nn.Parameter(input_scale * torch.randn(qkv_shape), requires_grad=True).to(dtype).cuda() for _ in range(3)\n+    ]\n+\n+    # Triton:\n+    n_blocks = n_ctx // block\n+    layout = torch.tril(torch.ones([n_heads, n_blocks, n_blocks], dtype=torch.long))\n+    query, key, value = [x.clone() for x in qkvs]\n+    query.retain_grad()\n+    key.retain_grad()\n+    value.retain_grad()\n+    attn_out = triton_attention(layout, block, query=query, key=key, value=value, scale=scale)\n+    # ad hoc loss\n+    loss = (attn_out ** 2).mean()\n+    loss.backward()\n+    grads = [query.grad, key.grad, value.grad]\n+\n+    # Torch version:\n+    torch_q, torch_k, torch_v = [x.clone() for x in qkvs]\n+    attn_mask = torch.ones([n_ctx, n_ctx], device=\"cuda\", dtype=dtype)\n+    attn_mask = torch.tril(attn_mask, diagonal=0)\n+    attn_mask = 1e6 * (-1 + (attn_mask.reshape((1, 1, n_ctx, n_ctx)).cuda()))\n+    torch_q.retain_grad()\n+    torch_k.retain_grad()\n+    torch_v.retain_grad()\n+    scores = scale * torch.einsum(\"bhsd,bhtd->bhst\", torch_q, torch_k)\n+    scores = scores + attn_mask\n+    probs = torch.softmax(scores, dim=-1)\n+    torch_attn_out = torch.einsum(\"bhst,bhtd->bhsd\", probs, torch_v)\n+    # ad hoc loss\n+    torch_loss = (torch_attn_out ** 2).mean()\n+    torch_loss.backward()\n+    torch_grads = [torch_q.grad, torch_k.grad, torch_v.grad]\n+\n+    # comparison\n+    # print(f\"Triton loss {loss} and torch loss {torch_loss}.  Also checking grads...\")\n+    triton.testing.assert_almost_equal(loss, torch_loss)\n+    for g1, g2 in zip(grads, torch_grads):\n+        triton.testing.assert_almost_equal(g1, g2)\n+\n+\n+@pytest.mark.parametrize(\"block\", [16, 32, 64])\n+def triton_attention(\n+    layout,\n+    block: int,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    scale: float,\n+):\n+    sparse_dot_sdd_nt = triton.ops.blocksparse.matmul(layout, block, \"sdd\", trans_a=False, trans_b=True, device=value.device)\n+    sparse_dot_dsd_nn = triton.ops.blocksparse.matmul(layout, block, \"dsd\", trans_a=False, trans_b=False, device=value.device)\n+    sparse_softmax = triton.ops.blocksparse.softmax(layout, block, device=value.device)\n+\n+    w = sparse_dot_sdd_nt(query, key)\n+    w = sparse_softmax(w, scale=scale, is_causal=True)\n+    a = sparse_dot_dsd_nn(w, value)\n+    return a"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 140, "deletions": 147, "changes": 287, "file_content_changes": "@@ -667,7 +667,6 @@ def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr)\n             tl.atomic_add(Z + off1, z)\n     rs = RandomState(17)\n     x = numpy_random((shape0, shape1), dtype_str=\"float32\", rs=rs)\n-    print(x)\n     # reference result\n     z_ref = np.sum(x, axis=axis, keepdims=False)\n     # triton result\n@@ -677,36 +676,7 @@ def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr)\n     kernel[(1,)](z_tri, x_tri, axis, shape0, shape1)\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n \n-# def test_atomic_cas():\n-#     # 1. make sure that atomic_cas changes the original value (Lock)\n-#     @triton.jit\n-#     def change_value(Lock):\n-#         tl.atomic_cas(Lock, 0, 1)\n-\n-#     Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n-#     change_value[(1,)](Lock)\n-\n-#     assert (Lock[0] == 1)\n-\n-#     # 2. only one block enters the critical section\n-#     @triton.jit\n-#     def serialized_add(data, Lock):\n-#         ptrs = data + tl.arange(0, 128)\n-#         while tl.atomic_cas(Lock, 0, 1) == 1:\n-#             pass\n-\n-#         tl.store(ptrs, tl.load(ptrs) + 1.0)\n-\n-#         # release lock\n-#         tl.atomic_xchg(Lock, 0)\n-\n-#     Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n-#     data = torch.zeros((128,), device='cuda', dtype=torch.float32)\n-#     ref = torch.full((128,), 64.0)\n-#     serialized_add[(64,)](data, Lock)\n-#     triton.testing.assert_almost_equal(data, ref)\n-\n-def test_simple_atomic_cas():\n+def test_atomic_cas():\n     # 1. make sure that atomic_cas changes the original value (Lock)\n     @triton.jit\n     def change_value(Lock):\n@@ -717,6 +687,25 @@ def change_value(Lock):\n \n     assert (Lock[0] == 1)\n \n+    # 2. only one block enters the critical section\n+    @triton.jit\n+    def serialized_add(data, Lock):\n+        ptrs = data + tl.arange(0, 128)\n+        while tl.atomic_cas(Lock, 0, 1) == 1:\n+            pass\n+\n+        tl.store(ptrs, tl.load(ptrs) + 1.0)\n+\n+        # release lock\n+        tl.atomic_xchg(Lock, 0)\n+\n+    Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n+    data = torch.zeros((128,), device='cuda', dtype=torch.float32)\n+    ref = torch.full((128,), 64.0)\n+    serialized_add[(64,)](data, Lock)\n+    triton.testing.assert_almost_equal(data, ref)\n+\n+\n # # ---------------\n # # test cast\n # # ---------------\n@@ -1077,122 +1066,126 @@ def kernel(X, stride_xm, stride_xn,\n # # ---------------\n \n \n-# @pytest.mark.parametrize(\"epilogue, allow_tf32, dtype\",\n-#                          [(epilogue, allow_tf32, dtype)\n-#                           for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols', 'softmax', 'chain-dot']\n-#                           for allow_tf32 in [True, False]\n-#                           for dtype in ['float16']\n-#                           if not (allow_tf32 and (dtype in ['float16']))])\n-# def test_dot(epilogue, allow_tf32, dtype, device='cuda'):\n-#     cc = _triton.runtime.cc(_triton.runtime.backend.CUDA, torch.cuda.current_device())\n-#     if cc < 80:\n-#         if dtype == 'int8':\n-#             pytest.skip(\"Only test int8 on devices with sm >= 80\")\n-#         elif dtype == 'float32' and allow_tf32:\n-#             pytest.skip(\"Only test tf32 on devices with sm >= 80\")\n-\n-#     M, N, K = 128, 128, 64\n-#     num_warps = 8\n-#     trans_a, trans_b = False, False\n-\n-#     # triton kernel\n-#     @triton.jit\n-#     def kernel(X, stride_xm, stride_xk,\n-#                Y, stride_yk, stride_yn,\n-#                W, stride_wn, stride_wl,\n-#                Z, stride_zm, stride_zn,\n-#                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n-#                ADD_MATRIX: tl.constexpr, ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr,\n-#                ALLOW_TF32: tl.constexpr,\n-#                DO_SOFTMAX: tl.constexpr, CHAIN_DOT: tl.constexpr,\n-#                TRANS_A: tl.constexpr, TRANS_B: tl.constexpr):\n-#         off_m = tl.arange(0, BLOCK_M)\n-#         off_n = tl.arange(0, BLOCK_N)\n-#         off_l = tl.arange(0, BLOCK_N)\n-#         off_k = tl.arange(0, BLOCK_K)\n-#         Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk\n-#         Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn\n-#         Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl\n-#         Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn\n-#         z = tl.dot(tl.load(Xs), tl.load(Ys), trans_a=TRANS_A, trans_b=TRANS_B, allow_tf32=ALLOW_TF32)\n-#         if ADD_MATRIX:\n-#             z += tl.load(Zs)\n-#         if ADD_ROWS:\n-#             ZRs = Z + off_m * stride_zm\n-#             z += tl.load(ZRs)[:, None]\n-#         if ADD_COLS:\n-#             ZCs = Z + off_n * stride_zn\n-#             z += tl.load(ZCs)[None, :]\n-#         if DO_SOFTMAX:\n-#             max = tl.max(z, 1)\n-#             z = z - max[:, None]\n-#             num = tl.exp(z)\n-#             den = tl.sum(num, 1)\n-#             z = num / den[:, None]\n-#         if CHAIN_DOT:\n-#             # tl.store(Zs, z)\n-#             # tl.debug_barrier()\n-#             z = tl.dot(z.to(tl.float16), tl.load(Ws), trans_a=TRANS_A)\n-#         tl.store(Zs, z)\n-#     # input\n-#     rs = RandomState(17)\n-#     x = numpy_random((K, M) if trans_a else (M, K), dtype_str=dtype, rs=rs) * .1\n-#     y = numpy_random((N, K) if trans_b else (K, N), dtype_str=dtype, rs=rs) * .1\n-#     w = numpy_random((N, N), dtype_str=dtype, rs=rs) * .1\n-#     if allow_tf32:\n-#         x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n-#         y = (y.view('uint32') & np.uint32(0xffffe000)).view('float32')\n-#         w = (w.view('uint32') & np.uint32(0xffffe000)).view('float32')\n-#     x_tri = to_triton(x, device=device)\n-#     y_tri = to_triton(y, device=device)\n-#     w_tri = to_triton(w, device=device)\n-#     # triton result\n-#     z = 1 + numpy_random((M, N), dtype_str=dtype, rs=rs) * .1\n-#     z_tri = to_triton(z, device=device)\n-#     if epilogue == 'trans':\n-#         z_tri = torch.as_strided(z_tri, (M, N), z_tri.stride()[::-1])\n-#     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n-#                          y_tri, y_tri.stride(0), y_tri.stride(1),\n-#                          w_tri, w_tri.stride(0), w_tri.stride(1),\n-#                          z_tri, z_tri.stride(0), z_tri.stride(1),\n-#                          TRANS_A=trans_a, TRANS_B=trans_b,\n-#                          BLOCK_M=M, BLOCK_K=K, BLOCK_N=N,\n-#                          ADD_MATRIX=epilogue == 'add-matrix',\n-#                          ADD_ROWS=epilogue == 'add-rows',\n-#                          ADD_COLS=epilogue == 'add-cols',\n-#                          DO_SOFTMAX=epilogue == 'softmax',\n-#                          CHAIN_DOT=epilogue == 'chain-dot',\n-#                          ALLOW_TF32=allow_tf32,\n-#                          num_warps=num_warps)\n-#     # torch result\n-#     x_ref = x.T if trans_a else x\n-#     y_ref = y.T if trans_b else y\n-#     z_ref = np.matmul(x_ref, y_ref)\n-#     if epilogue == 'add-matrix':\n-#         z_ref += z\n-#     if epilogue == 'add-rows':\n-#         z_ref += z[:, 0][:, None]\n-#     if epilogue == 'add-cols':\n-#         z_ref += z[0, :][None, :]\n-#     if epilogue == 'softmax':\n-#         num = np.exp(z_ref - np.max(z_ref, axis=-1, keepdims=True))\n-#         denom = np.sum(num, axis=-1, keepdims=True)\n-#         z_ref = num / denom\n-#     if epilogue == 'chain-dot':\n-#         z_ref = np.matmul(z_ref.T if trans_a else z_ref, w)\n-#     # compare\n-#     # print(z_ref[:,0], z_tri[:,0])\n-#     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n-#     # make sure ld/st are vectorized\n-#     ptx = pgm.asm['ptx']\n-#     assert 'ld.global.v4' in ptx\n-#     assert 'st.global.v4' in ptx\n-#     if allow_tf32:\n-#         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' in ptx\n-#     elif dtype == 'float32':\n-#         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' not in ptx\n-#     elif dtype == 'int8':\n-#         assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n+@pytest.mark.parametrize(\"epilogue, allow_tf32, dtype\",\n+                         [(epilogue, allow_tf32, dtype)\n+                          for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols', 'softmax', 'chain-dot']\n+                          for allow_tf32 in [True, False]\n+                          for dtype in ['float16']\n+                          if not (allow_tf32 and (dtype in ['float16']))])\n+def test_dot(epilogue, allow_tf32, dtype, device='cuda'):\n+    capability = torch.cuda.get_device_capability()\n+    if capability[0] < 80:\n+        if dtype == 'int8':\n+            pytest.skip(\"Only test int8 on devices with sm >= 80\")\n+        elif dtype == 'float32' and allow_tf32:\n+            pytest.skip(\"Only test tf32 on devices with sm >= 80\")\n+\n+    M, N, K = 64, 64, 64\n+    num_warps = 4\n+    trans_a, trans_b = False, False\n+\n+    # triton kernel\n+    @triton.jit\n+    def kernel(X, stride_xm, stride_xk,\n+               Y, stride_yk, stride_yn,\n+               W, stride_wn, stride_wl,\n+               Z, stride_zm, stride_zn,\n+               BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n+               ADD_MATRIX: tl.constexpr, ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr,\n+               ALLOW_TF32: tl.constexpr,\n+               DO_SOFTMAX: tl.constexpr, CHAIN_DOT: tl.constexpr,\n+               TRANS_A: tl.constexpr, TRANS_B: tl.constexpr):\n+        off_m = tl.arange(0, BLOCK_M)\n+        off_n = tl.arange(0, BLOCK_N)\n+        off_l = tl.arange(0, BLOCK_N)\n+        off_k = tl.arange(0, BLOCK_K)\n+        Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk\n+        Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn\n+        Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl\n+        Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn\n+        x = tl.load(Xs)\n+        y = tl.load(Ys)\n+        x = tl.trans(x) if TRANS_A else x\n+        y = tl.trans(y) if TRANS_B else y\n+        z = tl.dot(x, y, allow_tf32=ALLOW_TF32)\n+        if ADD_MATRIX:\n+            z += tl.load(Zs)\n+        if ADD_ROWS:\n+            ZRs = Z + off_m * stride_zm\n+            z += tl.load(ZRs)[:, None]\n+        if ADD_COLS:\n+            ZCs = Z + off_n * stride_zn\n+            z += tl.load(ZCs)[None, :]\n+        if DO_SOFTMAX:\n+            max = tl.max(z, 1)\n+            z = z - max[:, None]\n+            num = tl.exp(z)\n+            den = tl.sum(num, 1)\n+            z = num / den[:, None]\n+        if CHAIN_DOT:\n+            # tl.store(Zs, z)\n+            # tl.debug_barrier()\n+            z = tl.dot(tl.trans(z.to(tl.float16)), tl.load(Ws))\n+        tl.store(Zs, z)\n+    # input\n+    rs = RandomState(17)\n+    x = numpy_random((K, M) if trans_a else (M, K), dtype_str=dtype, rs=rs) * .1\n+    y = numpy_random((N, K) if trans_b else (K, N), dtype_str=dtype, rs=rs) * .1\n+    w = numpy_random((N, N), dtype_str=dtype, rs=rs) * .1\n+    if allow_tf32:\n+        x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+        y = (y.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+        w = (w.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+    x_tri = to_triton(x, device=device)\n+    y_tri = to_triton(y, device=device)\n+    w_tri = to_triton(w, device=device)\n+    # triton result\n+    z = 1 + numpy_random((M, N), dtype_str=dtype, rs=rs) * .1\n+    z_tri = to_triton(z, device=device)\n+    if epilogue == 'trans':\n+        z_tri = torch.as_strided(z_tri, (M, N), z_tri.stride()[::-1])\n+    pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n+                         y_tri, y_tri.stride(0), y_tri.stride(1),\n+                         w_tri, w_tri.stride(0), w_tri.stride(1),\n+                         z_tri, z_tri.stride(0), z_tri.stride(1),\n+                         TRANS_A=trans_a, TRANS_B=trans_b,\n+                         BLOCK_M=M, BLOCK_K=K, BLOCK_N=N,\n+                         ADD_MATRIX=epilogue == 'add-matrix',\n+                         ADD_ROWS=epilogue == 'add-rows',\n+                         ADD_COLS=epilogue == 'add-cols',\n+                         DO_SOFTMAX=epilogue == 'softmax',\n+                         CHAIN_DOT=epilogue == 'chain-dot',\n+                         ALLOW_TF32=allow_tf32,\n+                         num_warps=num_warps)\n+    # torch result\n+    x_ref = x.T if trans_a else x\n+    y_ref = y.T if trans_b else y\n+    z_ref = np.matmul(x_ref, y_ref)\n+    if epilogue == 'add-matrix':\n+        z_ref += z\n+    if epilogue == 'add-rows':\n+        z_ref += z[:, 0][:, None]\n+    if epilogue == 'add-cols':\n+        z_ref += z[0, :][None, :]\n+    if epilogue == 'softmax':\n+        num = np.exp(z_ref - np.max(z_ref, axis=-1, keepdims=True))\n+        denom = np.sum(num, axis=-1, keepdims=True)\n+        z_ref = num / denom\n+    if epilogue == 'chain-dot':\n+        z_ref = np.matmul(z_ref.T, w)\n+    # compare\n+    # print(z_ref[:,0], z_tri[:,0])\n+    np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+    # make sure ld/st are vectorized\n+    ptx = pgm.asm['ptx']\n+    assert 'ld.global.v4' in ptx\n+    assert 'st.global.v4' in ptx\n+    if allow_tf32:\n+        assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' in ptx\n+    elif dtype == 'float32':\n+        assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' not in ptx\n+    elif dtype == 'int8':\n+        assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n \n \n # def test_dot_without_load():"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 26, "deletions": 15, "changes": 41, "file_content_changes": "@@ -172,8 +172,9 @@ def get_proper_err(a, b, golden):\n     [128, 64, 128, 4, 128, 64, 128, False, False],\n     [16, 16, 16, 16, 16, 16, 16, False, False],  # wpt overflow issue\n     # K-Forloop\n-    [32, 32, 64, 4, 32, 32, 32, False, False], # Single shared encoding\n-    [16, 16, 128, 4, 16, 16, 16, False, False], # Single shared encoding and small k\n+    #[16, 16, 64, 4, 8, 8, 8, False, False],  # Wrap threads\n+    [32, 32, 64, 4, 32, 32, 32, False, False],  # Single shared encoding\n+    [16, 16, 128, 4, 16, 16, 16, False, False],  # Single shared encoding and small k\n     [64, 32, 128, 4, 64, 32, 64, False, False],\n     [128, 16, 128, 4, 128, 16, 32, False, False],\n     [32, 16, 128, 4, 32, 16, 32, False, False],\n@@ -219,14 +220,17 @@ def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLO\n     assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err), check_dtype=False)\n \n \n-@pytest.mark.parametrize('M,N,K,num_warps,block_M,block_N,block_K', [\n-    [32, 32, 16, 4, 32, 32, 16],\n-    [32, 16, 16, 4, 32, 32, 16],\n-    [128, 8, 8, 4, 32, 32, 16],\n-    # TODO[Superjomn]: fix it later\n-    # [127, 41, 43, 4, 32, 32, 16],\n+@pytest.mark.parametrize('M,N,K,num_warps,block_M,block_N,block_K,allow_tf32', [\n+    [32, 32, 16, 4, 32, 32, 16, False],\n+    [32, 32, 16, 4, 32, 32, 16, True],\n+    [32, 16, 16, 4, 32, 32, 16, False],\n+    [32, 16, 16, 4, 32, 32, 16, True],\n+    [127, 41, 43, 4, 32, 32, 16, False],\n+    [127, 41, 43, 4, 32, 32, 16, True],\n+    [128, 8, 8, 4, 32, 32, 16, False],\n+    [128, 8, 8, 4, 32, 32, 16, True]\n ])\n-def test_gemm_fmadot(M, N, K, num_warps, block_M, block_N, block_K):\n+def test_gemm_fp32(M, N, K, num_warps, block_M, block_N, block_K, allow_tf32):\n     @triton.jit\n     def matmul_kernel(\n         a_ptr, b_ptr, c_ptr,\n@@ -235,6 +239,7 @@ def matmul_kernel(\n         stride_bk, stride_bn,\n         stride_cm, stride_cn,\n         BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+        ALLOW_TF32: tl.constexpr\n     ):\n         pid = tl.program_id(axis=0)\n         # num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n@@ -252,10 +257,9 @@ def matmul_kernel(\n         for k in range(0, K, BLOCK_SIZE_K):\n             a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < K)\n             b_mask = (offs_k[:, None] < K) & (offs_bn[None, :] < N)\n-            a = tl.load(a_ptrs, a_mask)\n-            b = tl.load(b_ptrs, b_mask)\n-            # NOTE the allow_tf32 should be false to force the dot op to do fmadot lowering\n-            accumulator += tl.dot(a, b, allow_tf32=False)\n+            a = tl.load(a_ptrs, a_mask, other=0.0)\n+            b = tl.load(b_ptrs, b_mask, other=0.0)\n+            accumulator += tl.dot(a, b, allow_tf32=ALLOW_TF32)\n             a_ptrs += BLOCK_SIZE_K * stride_ak\n             b_ptrs += BLOCK_SIZE_K * stride_bk\n             offs_k += BLOCK_SIZE_K\n@@ -266,6 +270,9 @@ def matmul_kernel(\n         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n         tl.store(c_ptrs, accumulator, c_mask)\n \n+    # Configure the pytorch counterpart\n+    torch.backends.cuda.matmul.allow_tf32 = allow_tf32\n+\n     a = torch.randn((M, K), device='cuda', dtype=torch.float32)\n     b = torch.randn((K, N), device='cuda', dtype=torch.float32)\n     c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n@@ -276,8 +283,12 @@ def matmul_kernel(\n                         stride_am=a.stride(0), stride_ak=a.stride(1),\n                         stride_bk=b.stride(0), stride_bn=b.stride(1),\n                         stride_cm=c.stride(0), stride_cn=c.stride(1),\n-                        BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N, BLOCK_SIZE_K=block_K)\n+                        BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N, BLOCK_SIZE_K=block_K, ALLOW_TF32=allow_tf32)\n \n     golden = torch.matmul(a, b)\n     golden_abs_err, golden_rel_err = get_proper_err(a, b, golden)\n-    torch.testing.assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err))\n+    if allow_tf32:\n+        # TF32 is not accurate enough\n+        torch.testing.assert_close(c, golden, rtol=max(1e-2, 1.5 * golden_rel_err), atol=max(1e-2, 1.5 * golden_abs_err))\n+    else:\n+        torch.testing.assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err))"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 97, "deletions": 60, "changes": 157, "file_content_changes": "@@ -359,7 +359,7 @@ def visit_If(self, node):\n             cond = cond.to(triton.language.int1, _builder=self.builder)\n             with enter_sub_region(self) as sr:\n                 liveins, ip_block = sr\n-\n+                liveins_copy = liveins.copy()\n                 then_block = self.builder.create_block()\n                 self.builder.set_insertion_point_to_start(then_block)\n                 self.visit_compound_statement(node.body)\n@@ -394,7 +394,15 @@ def visit_If(self, node):\n                             if then_defs[then_name].type == else_defs[else_name].type:\n                                 names.append(then_name)\n                                 ret_types.append(then_defs[then_name].type)\n-\n+                \n+                # defined in else block but not in then block\n+                # to find in parent scope and yield them\n+                for else_name in else_defs:\n+                    if else_name in liveins and else_name not in then_defs:\n+                        if else_defs[else_name].type == liveins[else_name].type:\n+                            names.append(else_name)\n+                            ret_types.append(else_defs[else_name].type)\n+                            then_defs[else_name] = liveins_copy[else_name]\n                 self.builder.set_insertion_point_to_end(ip_block)\n \n                 if then_defs or node.orelse:  # with else block\n@@ -528,8 +536,7 @@ def visit_While(self, node):\n                                                                 [ty.to_ir(self.builder) for ty in ret_types])\n             loop_block.merge_block_before(after_block)\n             self.builder.set_insertion_point_to_end(after_block)\n-            if len(yields) > 0:\n-                self.builder.create_yield_op([y.handle for y in yields])\n+            self.builder.create_yield_op([y.handle for y in yields])\n \n         # update global uses in while_op\n         for i, name in enumerate(names):\n@@ -594,11 +601,8 @@ def visit_For(self, node):\n         ub = self.builder.create_to_index(ub)\n         step = self.builder.create_to_index(step)\n         # Create placeholder for the loop induction variable\n-        # We can use any value because the variable isn't a constexpr\n-        # but use a distinctive value (of the right type) to ease debugging\n-        st_target = ast.Name(id=node.target.id, ctx=ast.Store())\n-        init_node = ast.Assign(targets=[st_target], value=ast.Num(value=0xBADF00D))\n-        self.visit(init_node)\n+        iv = self.builder.create_undef(self.builder.get_int32_ty())\n+        self.set_value(node.target.id, triton.language.core.tensor(iv, triton.language.core.int32))\n \n         with enter_sub_region(self) as sr:\n             liveins, insert_block = sr\n@@ -1014,6 +1018,7 @@ def ty_to_cpp(ty):\n         \"u32\": \"uint32_t\",\n         \"u64\": \"uint64_t\",\n         \"fp32\": \"float\",\n+        \"f32\": \"float\",\n     }[ty]\n \n \n@@ -1044,6 +1049,7 @@ def _extracted_type(ty):\n             'u32': 'uint32_t',\n             'u64': 'uint64_t',\n             'fp32': 'float',\n+            'f32': 'float',\n             'fp64': 'double',\n         }[ty]\n \n@@ -1343,7 +1349,31 @@ def make_hash(fn, **kwargs):\n         key = f\"{fn.cache_key}-{''.join(signature.values())}-{configs_key}-{constants}-{num_warps}-{num_stages}\"\n         return hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n     assert isinstance(fn, str)\n-    return hashlib.md5(Path(fn).read_text().encode(\"utf-8\")).hexdigest()\n+    return hashlib.md5((Path(fn).read_text() + triton.runtime.jit.version_key()).encode(\"utf-8\")).hexdigest()\n+\n+\n+# - ^\\s*func\\s+ : match the start of the string, any leading whitespace, the keyword func, \n+#    and any following whitespace\n+# - (public\\s+)? : optionally match the keyword public and any following whitespace\n+# - (@\\w+) : match an @ symbol followed by one or more word characters \n+#   (letters, digits, or underscores), and capture it as group 1 (the function name)\n+# - (\\((?:%\\w+: \\S+(?: \\{\\S+ = \\S+ : \\S+\\})?(?:, )?)*\\)) : match a pair of parentheses enclosing \n+#   zero or more arguments separated by commas, and capture it as group 2 (the argument list)\n+mlir_prototype_pattern = r'^\\s*func\\s+(?:public\\s+)?(@\\w+)(\\((?:%\\w+: \\S+(?: \\{\\S+ = \\S+ : \\S+\\})?(?:, )?)*\\))\\s*\\{\\s*$'\n+ptx_prototype_pattern = r\"\\.(?:visible|extern)\\s+\\.(?:entry|func)\\s+(\\w+)\\s*\\(([^)]*)\\)\"\n+prototype_pattern = {\n+    \"ttir\": mlir_prototype_pattern,\n+    \"ttgir\": mlir_prototype_pattern,\n+    \"ptx\": ptx_prototype_pattern,\n+}\n+\n+mlir_arg_type_pattern = r'%\\w+: ([^,^\\)\\s]+)(?: \\{\\S+ = \\S+ : \\S+\\})?,?'\n+ptx_arg_type_pattern = r\"\\.param\\s+\\.(\\w+)\"\n+arg_type_pattern = {\n+    \"ttir\": mlir_arg_type_pattern,\n+    \"ttgir\": mlir_arg_type_pattern,\n+    \"ptx\": ptx_arg_type_pattern,\n+}\n \n \n # def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: int = 4, num_stages: int = 3, extern_libs=None, configs=None):\n@@ -1354,6 +1384,27 @@ def compile(fn, **kwargs):\n     context = _triton.ir.context()\n     asm = dict()\n     constants = kwargs.get(\"constants\", dict())\n+    num_warps = kwargs.get(\"num_warps\", 4)\n+    num_stages = kwargs.get(\"num_stages\", 3)\n+    extern_libs = kwargs.get(\"extern_libs\", dict())\n+    device = kwargs.get(\"device\", torch.cuda.current_device())\n+    capability = torch.cuda.get_device_capability()\n+    capability = capability[0]*10 + capability[1]\n+    # build compilation stages\n+    stages = {\n+      \"ast\" : (lambda path: fn, None),\n+      \"ttir\": (lambda path: _triton.ir.parse_mlir_module(path, context), \n+               lambda src: ast_to_ttir(src, signature, configs[0], constants)),\n+      \"ttgir\": (lambda path: _triton.ir.parse_mlir_module(path, context), \n+                lambda src: ttir_to_ttgir(src, num_warps, num_stages, capability)),\n+      \"llir\": (lambda path: Path(path).read_bytes(), \n+              lambda src: ttgir_to_llir(src, extern_libs, capability)),\n+      \"ptx\":  (lambda path: Path(path).read_text(), \n+              lambda src: llir_to_ptx(src, capability)),\n+      \"cubin\": (lambda path: Path(path).read_bytes(), \n+               lambda src: ptx_to_cubin(src, capability))\n+    }\n+    # find out the signature of the function\n     if isinstance(fn, triton.runtime.JITFunction):\n         configs = kwargs.get(\"configs\", None)\n         signature = kwargs[\"signature\"]\n@@ -1368,13 +1419,17 @@ def compile(fn, **kwargs):\n         kwargs[\"signature\"] = signature\n     else:\n         assert isinstance(fn, str)\n-        name, ir = os.path.basename(fn).split(\".\")\n-        assert ir == \"ttgir\"\n-        asm[ir] = _triton.ir.parse_mlir_module(fn, context)\n-        function = asm[ir].get_single_function()\n-        param_tys = [convert_type_repr(str(ty)) for ty in function.type.param_types()]\n+        _, ir = os.path.basename(fn).split(\".\")\n+        src = Path(fn).read_text()\n+        import re\n+        match = re.search(prototype_pattern[ir], src, re.MULTILINE)\n+        name, signature = match.group(1), match.group(2)\n+        print(name, signature)\n+        types = re.findall(arg_type_pattern[ir], signature)\n+        print(types)\n+        param_tys = [convert_type_repr(ty) for ty in types]\n         signature = {k: v for k, v in enumerate(param_tys)}\n-        first_stage = 2\n+        first_stage = list(stages.keys()).index(ir)\n \n     # cache manager\n     so_path = make_stub(name, signature, constants)\n@@ -1384,58 +1439,42 @@ def compile(fn, **kwargs):\n     if isinstance(fn, triton.runtime.JITFunction):\n         name, ext = fn.__name__, \"ast\"\n     else:\n-        name, ext = os.path.basename(fn).split(\".\")\n-    # initialize compilation params\n-    num_warps = kwargs.get(\"num_warps\", 4)\n-    num_stages = kwargs.get(\"num_stages\", 3)\n-    extern_libs = kwargs.get(\"extern_libs\", dict())\n-    device = kwargs.get(\"device\", torch.cuda.current_device())\n-    compute_capability = torch.cuda.get_device_capability(device)\n-    compute_capability = compute_capability[0] * 10 + compute_capability[1]\n+      name, ext = os.path.basename(fn).split(\".\")\n+\n     # load metadata if any\n     metadata = None\n     if fn_cache_manager.has_file(f'{name}.json'):\n         with open(fn_cache_manager._make_path(f\"{name}.json\")) as f:\n             metadata = json.load(f)\n     else:\n-        metadata = {\"num_warps\": num_warps, \"num_stages\": num_stages, \"ctime\": dict()}\n-    # build compilation stages\n-    stages = {\n-        \"ast\": (lambda path: fn, None),\n-        \"ttir\": (lambda path: _triton.ir.parse_mlir_module(path, context),\n-                 lambda src: ast_to_ttir(src, signature, configs[0], constants)),\n-        \"ttgir\": (lambda path: _triton.ir.parse_mlir_module(path, context),\n-                  lambda src: ttir_to_ttgir(src, num_warps, num_stages, compute_capability)),\n-        \"llir\": (lambda path: Path(path).read_bytes(),\n-                 lambda src: ttgir_to_llir(src, extern_libs, compute_capability)),\n-        \"ptx\": (lambda path: Path(path).read_text(),\n-                lambda src: llir_to_ptx(src, compute_capability)),\n-        \"cubin\": (lambda path: Path(path).read_bytes(),\n-                  lambda src: ptx_to_cubin(src, compute_capability))\n-    }\n+      metadata = {\"num_warps\": num_warps, \"num_stages\": num_stages, \"ctime\": dict()}\n+      if ext == \"ptx\":\n+        assert \"shared\" in kwargs, \"ptx compilation must provide shared memory size\"\n+        metadata[\"shared\"] = kwargs[\"shared\"]\n+\n     first_stage = list(stages.keys()).index(ext)\n     asm = dict()\n     module = fn\n     # run compilation pipeline  and populate metadata\n     for ir, (parse, compile) in list(stages.items())[first_stage:]:\n-        path = fn_cache_manager._make_path(f\"{name}.{ir}\")\n-        if ir == ext:\n-            next_module = parse(fn)\n-        elif os.path.exists(path) and \\\n-                ir in metadata[\"ctime\"] and \\\n-                os.path.getctime(path) == metadata[\"ctime\"][ir]:\n-            next_module = parse(path)\n-        else:\n-            next_module = compile(module)\n-            fn_cache_manager.put(next_module, f\"{name}.{ir}\")\n-        if os.path.exists(path):\n-            metadata[\"ctime\"][ir] = os.path.getctime(path)\n-        asm[ir] = next_module if ir == \"cubin\" else str(next_module)\n-        if ir == \"llir\" and \"shared\" not in metadata:\n-            metadata[\"shared\"] = _triton.get_shared_memory_size(module)\n-        if ir == \"ptx\":\n-            metadata[\"name\"] = ptx_get_kernel_name(next_module)\n-        module = next_module\n+      path = fn_cache_manager._make_path(f\"{name}.{ir}\")\n+      if ir == ext:\n+        next_module = parse(fn)\n+      elif os.path.exists(path) and\\\n+           ir in metadata[\"ctime\"] and\\\n+           os.path.getctime(path) == metadata[\"ctime\"][ir]:\n+        next_module = parse(path)\n+      else:\n+        next_module = compile(module)\n+        fn_cache_manager.put(next_module, f\"{name}.{ir}\")\n+      if os.path.exists(path):\n+        metadata[\"ctime\"][ir] = os.path.getctime(path)\n+      asm[ir] = next_module if ir == \"cubin\" else str(next_module)\n+      if ir == \"llir\" and \"shared\" not in metadata:\n+        metadata[\"shared\"] = _triton.get_shared_memory_size(module)\n+      if ir == \"ptx\":\n+        metadata[\"name\"] = ptx_get_kernel_name(next_module)\n+      module = next_module\n     # write-back metadata\n     fn_cache_manager.put(json.dumps(metadata), f\"{name}.json\", binary=False)\n     # return handle to compiled kernel\n@@ -1515,7 +1554,7 @@ def _generate_src(self):\n            }\n         }\n \n-        #define CUDA_CHECK(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n+        #define CUDA_CHECK(ans) { gpuAssert((ans), __FILE__, __LINE__); if(PyErr_Occurred()) return NULL; }\n \n         static PyObject* loadBinary(PyObject* self, PyObject* args) {\n             const char* name;\n@@ -1530,7 +1569,6 @@ def _generate_src(self):\n             CUmodule mod;\n             int32_t n_regs = 0;\n             int32_t n_spills = 0;\n-            Py_BEGIN_ALLOW_THREADS;\n             // create driver handles\n             CUDA_CHECK(cuModuleLoadData(&mod, data));\n             CUDA_CHECK(cuModuleGetFunction(&fun, mod, name));\n@@ -1548,7 +1586,6 @@ def _generate_src(self):\n               CUDA_CHECK(cuFuncGetAttribute(&shared_static, CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES, fun));\n               CUDA_CHECK(cuFuncSetAttribute(fun, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES, shared_optin - shared_static));\n             }\n-            Py_END_ALLOW_THREADS;\n \n             if(PyErr_Occurred()) {\n               return NULL;"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -406,7 +406,7 @@ def __ne__(self, other):\n         return constexpr(self.value != other.value)\n \n     def __bool__(self):\n-        return constexpr(bool(self.value))\n+        return bool(self.value)\n \n     def __neg__(self):\n         return constexpr(-self.value)\n@@ -615,6 +615,7 @@ def __getitem__(self, slices, _builder=None):\n                 assert False, \"unsupported\"\n         return ret\n \n+\n     # x[:, None, :, None]\n     # x = expand_dims(x, axis=1)\n     # x = expand_dims(x, axis=2)\n@@ -738,6 +739,9 @@ def broadcast_to(input, shape, _builder=None):\n     \"\"\"\n     return semantic.broadcast_impl_shape(input, shape, _builder)\n \n+@builtin\n+def trans(input, _builder=None):\n+    return semantic.trans(input, _builder)\n \n @builtin\n def cat(input, other, _builder=None):"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 12, "deletions": 7, "changes": 19, "file_content_changes": "@@ -502,6 +502,11 @@ def cat(lhs: tl.tensor, rhs: tl.tensor, builder: ir.builder) -> tl.tensor:\n     # TODO: check types\n     return tl.tensor(builder.create_cat(lhs.handle, rhs.handle), lhs.type)\n \n+def trans(input: tl.tensor, builder: ir.builder) -> tl.tensor:\n+    if len(input.shape) != 2:\n+        raise ValueError(\"Only 2D tensors can be transposed\")\n+    ret_type = tl.block_type(input.type.scalar, [input.shape[1], input.shape[0]])\n+    return tl.tensor(builder.create_trans(input.handle), ret_type)\n \n def broadcast_impl_shape(input: tl.tensor,\n                          shape: List[int],\n@@ -1117,16 +1122,16 @@ def sqrt(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n ##\n \n def multiple_of(x: tl.tensor, values: List[int]) -> tl.tensor:\n-    if len(x.shape) != len(values):\n-        raise ValueError(\"Shape of input to multiple_of does not match the length of values\")\n-    x.handle.multiple_of(values)\n-    return x\n-\n-\n+     if len(x.shape) != len(values):\n+         raise ValueError(\"Shape of input to multiple_of does not match the length of values\")\n+     x.handle.set_attr(\"tt.divisibility\", ir.make_attr(values, x.handle.get_context()))\n+     return x\n+ \n+ \n def max_contiguous(x: tl.tensor, values: List[int]) -> tl.tensor:\n     if len(x.shape) != len(values):\n         raise ValueError(\"Shape of input to max_contiguous does not match the length of values\")\n-    x.handle.max_contiguous(values)\n+    x.handle.set_attr(\"tt.contiguity\", ir.make_attr(values, x.handle.get_context()))\n     return x\n \n "}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -34,12 +34,12 @@ def sparsify_tensor(x, mask, block):\n     return ret\n \n \n-def make_pair(shape, device=\"cuda\", alpha=1e-2, beta=0., trans=False, data=None):\n+def make_pair(shape, device=\"cuda\", alpha=1e-2, beta=0., trans=False, data=None, dtype=torch.float32):\n     if data is None:\n-        data = torch.randn(shape, dtype=torch.float32, device=device)\n+        data = torch.randn(shape, dtype=torch.float32, requires_grad=True, device=device)\n     ref_ret = data\n     ref_ret = ref_ret * alpha + beta\n-    ref_ret = ref_ret.half().float()\n+    ref_ret = ref_ret.half().to(dtype)\n     if trans:\n         ref_ret = ref_ret.t().requires_grad_()\n     ref_ret = ref_ret.detach().requires_grad_()"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 66, "deletions": 14, "changes": 80, "file_content_changes": "@@ -50,7 +50,7 @@ def _fwd_kernel(\n         # -- compute qk ----\n         k = tl.load(k_ptrs + start_n * stride_kn)\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-        qk += tl.dot(q, k, trans_b=True)\n+        qk += tl.dot(q, tl.trans(k))\n         qk *= sm_scale\n         qk += tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), 0, float(\"-inf\"))\n         # -- compute m_ij, p, l_ij\n@@ -165,26 +165,26 @@ def _bwd_kernel(\n             q = tl.load(q_ptrs)\n             # recompute p = softmax(qk, dim=-1).T\n             # NOTE: `do` is pre-divided by `l`; no normalization here\n-            qk = tl.dot(q, k, trans_b=True)\n+            qk = tl.dot(q, tl.trans(k))\n             qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\"-inf\"))\n             m = tl.load(m_ptrs + offs_m_curr)\n             p = tl.exp(qk * sm_scale - m[:, None])\n             # compute dv\n             do = tl.load(do_ptrs)\n-            dv += tl.dot(p.to(tl.float16), do, trans_a=True)\n+            dv += tl.dot(tl.trans(p.to(tl.float16)), do)\n             # compute dp = dot(v, do)\n             Di = tl.load(D_ptrs + offs_m_curr)\n             dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n-            dp += tl.dot(do, v, trans_b=True)\n+            dp += tl.dot(do, tl.trans(v))\n             # compute ds = p * (dp - delta[:, None])\n             ds = p * dp * sm_scale\n             # compute dk = dot(ds.T, q)\n-            dk += tl.dot(ds.to(tl.float16), q, trans_a=True)\n-            # # compute dq\n+            dk += tl.dot(tl.trans(ds.to(tl.float16)), q)\n+            # compute dq\n             dq = tl.load(dq_ptrs)\n             dq += tl.dot(ds.to(tl.float16), k)\n             tl.store(dq_ptrs, dq)\n-            # # increment pointers\n+            # increment pointers\n             dq_ptrs += BLOCK_M * stride_qm\n             q_ptrs += BLOCK_M * stride_qm\n             do_ptrs += BLOCK_M * stride_qm\n@@ -195,6 +195,7 @@ def _bwd_kernel(\n         tl.store(dk_ptrs, dk)\n \n \n+empty = torch.empty(128, device=\"cuda\")\n class _attention(torch.autograd.Function):\n \n     @staticmethod\n@@ -205,7 +206,7 @@ def forward(ctx, q, k, v, sm_scale):\n         assert Lq == Lk and Lk == Lv\n         assert Lk in {16, 32, 64, 128}\n         o = torch.empty_like(q)\n-        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1])\n+        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n         tmp = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n@@ -224,6 +225,7 @@ def forward(ctx, q, k, v, sm_scale):\n             BLOCK_DMODEL=Lk, num_warps=num_warps,\n             num_stages=1,\n         )\n+\n         ctx.save_for_backward(q, k, v, o, L, m)\n         ctx.BLOCK = BLOCK\n         ctx.grid = grid\n@@ -268,13 +270,13 @@ def backward(ctx, do):\n attention = _attention.apply\n \n \n-@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(3, 2, 2048, 64)])\n+@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     torch.manual_seed(20)\n-    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0, std=.5).requires_grad_()\n-    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0, std=.5).requires_grad_()\n-    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0, std=.5).requires_grad_()\n-    sm_scale = 0.3\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n+    sm_scale = 0.2\n     dout = torch.randn_like(q)\n     # reference implementation\n     M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n@@ -283,13 +285,16 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n         for h in range(H):\n             p[:, :, M == 0] = float(\"-inf\")\n     p = torch.softmax(p.float(), dim=-1).half()\n+    # p = torch.exp(p)\n     ref_out = torch.matmul(p, v)\n     ref_out.backward(dout)\n     ref_dv, v.grad = v.grad.clone(), None\n     ref_dk, k.grad = k.grad.clone(), None\n     ref_dq, q.grad = q.grad.clone(), None\n-    # triton implementation\n+    # # triton implementation\n     tri_out = attention(q, k, v, sm_scale)\n+    # print(ref_out)\n+    # print(tri_out)\n     tri_out.backward(dout)\n     tri_dv, v.grad = v.grad.clone(), None\n     tri_dk, k.grad = k.grad.clone(), None\n@@ -299,3 +304,50 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     triton.testing.assert_almost_equal(ref_dv, tri_dv)\n     triton.testing.assert_almost_equal(ref_dk, tri_dk)\n     triton.testing.assert_almost_equal(ref_dq, tri_dq)\n+\n+BATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\n+# vary seq length for fixed head and batch=4\n+configs = [triton.testing.Benchmark(\n+    x_names=['N_CTX'],\n+    x_vals=[2**i for i in range(10, 16)],\n+    line_arg='provider',\n+    line_vals=['triton'],\n+    line_names=['Triton'],\n+    styles=[('red', '-'), ('blue', '-')],\n+    ylabel='ms',\n+    plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n+    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode}\n+) for mode in ['fwd']]\n+\n+\n+@triton.testing.perf_report(configs)\n+def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.float16, device=\"cuda\"):\n+    assert mode in ['fwd', 'bwd']\n+    warmup = 25\n+    rep = 100\n+    if provider == \"triton\":\n+        q = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+        k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+        v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n+        sm_scale = 1.3\n+        fn = lambda: attention(q, k, v, sm_scale)\n+        if mode == 'bwd':\n+            o = fn()\n+            do = torch.randn_like(o)\n+            fn = lambda: o.backward(do, retain_graph=True)\n+        ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n+        return ms\n+    if provider == \"flash\":\n+        lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n+        cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n+        cu_seqlens[1:] = lengths.cumsum(0)\n+        qkv = torch.randn((BATCH * N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\n+        fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=True)\n+        if mode == 'bwd':\n+            o = fn()\n+            do = torch.randn_like(o)\n+            fn = lambda: o.backward(do, retain_graph=True)\n+        ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n+        return ms\n+\n+# bench_flash_attention.run(save_path='.', print_data=True)\n\\ No newline at end of file"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 46, "deletions": 2, "changes": 48, "file_content_changes": "@@ -261,14 +261,58 @@ func @for_alias(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B :\n   %cst0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n-    %cst1 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    %cst1 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #AL>\n     // CHECK-NEXT: Membar 6\n-    %cst2 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    %cst2 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #AL>\n     scf.yield %c_shared, %a_shared, %b_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n   // CHECK-NEXT: Membar 9\n   %cst3 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n   return\n }\n \n+// Although cst2 is not an argument of scf.yield, its memory is reused by cst1.\n+// So we need a barrier both before and after cst1\n+// CHECK-LABEL: for_reuse\n+func @for_reuse(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  // CHECK-NEXT: Membar 2\n+  %cst0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    // CHECK-NEXT: Membar 5\n+    %cst1 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    // CHECK-NEXT: Membar 7\n+    %cst2 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    scf.yield %c_shared, %a_shared, %b_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n+  }\n+  // CHECK-NEXT: Membar 10\n+  %cst3 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n+  return\n+}\n+\n+\n+// CHECK-LABEL: for_reuse_nested\n+func @for_reuse_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  // CHECK-NEXT: Membar 2\n+  %cst0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    // CHECK-NEXT: Membar 5\n+    %cst1 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    %a_shared_next, %b_shared_next, %c_shared_next = scf.for %ivv = %lb to %ub step %step iter_args(%a_shared_nested = %a_shared_init, %b_shared_nested = %b_shared_init, %c_shared_nested = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+      // CHECK-NEXT: Membar 7\n+      %cst2 = tt.cat %a_shared_nested, %b_shared_nested {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+      scf.yield %c_shared_nested, %a_shared_nested, %b_shared_nested : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n+    }\n+    scf.yield %c_shared, %a_shared, %b_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n+  }\n+  // CHECK-NEXT: Membar 11\n+  %cst3 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n+  return\n+}\n+\n }"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 87, "deletions": 5, "changes": 92, "file_content_changes": "@@ -387,6 +387,45 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // -----\n \n+#block0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [4], warpsPerCTA = [4], order = [0]}>\n+#block1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [8], warpsPerCTA = [4], order = [0]}>\n+#block2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 1], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#block3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 8], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#slice2d1 = #triton_gpu.slice<{dim = 1, parent=#block2}>\n+#slice3d0 = #triton_gpu.slice<{dim = 0, parent=#block3}>\n+#AL = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#A = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: basic_insert_slice_async_fallback\n+  func @basic_insert_slice_async_fallback(%arg0: !tt.ptr<f16> {tt.divisibility = 1 : i32}) {\n+    %off0_ = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #slice2d1>\n+    %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<64xi32, #slice3d0>\n+    %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<16xi32, #slice2d1>) -> tensor<16x1xi32, #block2>\n+    %off1 = tt.expand_dims %off1_ {axis = 0 : i32} : (tensor<64xi32, #slice3d0>) -> tensor<1x64xi32, #block3>\n+    %broadcast_off0_scalar = tt.broadcast %off0 : (tensor<16x1xi32, #block2>) -> tensor<16x64xi32, #block2>\n+    %cst_scalar = arith.constant 64 : i32\n+    %cst = tt.splat %cst_scalar : (i32) -> tensor<16x64xi32, #block2>\n+    %broadcast_off0_ = arith.muli %broadcast_off0_scalar, %cst : tensor<16x64xi32, #block2>\n+    %broadcast_off1_ = tt.broadcast %off1 : (tensor<1x64xi32, #block3>) -> tensor<16x64xi32, #block3>\n+    %broadcast_off0 = triton_gpu.convert_layout %broadcast_off0_ : (tensor<16x64xi32, #block2>) -> tensor<16x64xi32, #AL>\n+    %broadcast_off1 = triton_gpu.convert_layout %broadcast_off1_ : (tensor<16x64xi32, #block3>) -> tensor<16x64xi32, #AL>\n+    %off = arith.addi %broadcast_off0, %broadcast_off1 : tensor<16x64xi32, #AL>\n+    %a_init = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<16x64x!tt.ptr<f16>, #AL>\n+    %a_ptr = tt.addptr %a_init, %off : tensor<16x64x!tt.ptr<f16>, #AL>\n+    %tensor = triton_gpu.alloc_tensor : tensor<2x16x64xf16, #A>\n+    %index = arith.constant 1 : i32\n+\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<8xi32>, 3>\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<8xi32>, 3>\n+    %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x64x!tt.ptr<f16>, #AL> -> tensor<2x16x64xf16, #A>\n+    return\n+  }\n+}\n+\n+// -----\n+\n #block0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [4], warpsPerCTA = [4], order = [0]}>\n #block1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [8], warpsPerCTA = [4], order = [0]}>\n #block2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 1], warpsPerCTA = [4, 1], order = [1, 0]}>\n@@ -735,9 +774,13 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_layout_mmav1_block\n   func @convert_layout_mmav1_blocked(%arg0: tensor<32x16xf32, #mma>) {\n     // CHECK: llvm.store\n-    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n     // CHECK: llvm.store\n-    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n     // CHECK: nvvm.barrier0\n     // CHECK: llvm.load\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n@@ -880,6 +923,45 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // -----\n \n+#mma = #triton_gpu.mma<{version=2, warpsPerCTA=[2, 2]}>\n+#shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: matmul_tf32dot\n+  func @matmul_tf32dot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  %a:tensor<32x16xf32, #shared>, %b:tensor<16x32xf32, #shared>) {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n+    // CHECK-SAME: (f32, f32, f32, f32)\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n+    // CHECK-SAME: (f32, f32, f32, f32)\n+    %a_mat = triton_gpu.convert_layout %a : (tensor<32x16xf32, #shared>) -> tensor<32x16xf32, #dot_operand_a>\n+    %b_mat = triton_gpu.convert_layout %b : (tensor<16x32xf32, #shared>) -> tensor<16x32xf32, #dot_operand_b>\n+\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\n+    %28 = tt.dot %a_mat, %b_mat, %cst {allowTF32 = true, transA = false, transB = false} : tensor<32x16xf32, #dot_operand_a> * tensor<16x32xf32, #dot_operand_b> -> tensor<32x32xf32, #mma>\n+    %38 = triton_gpu.convert_layout %28 : (tensor<32x32xf32, #mma>) -> tensor<32x32xf32, #blocked>\n+\n+    %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n+    %36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x32x!tt.ptr<f32>, #blocked>\n+    tt.store %36, %38 : tensor<32x32xf32, #blocked>\n+    return\n+  }\n+}\n+\n+// -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: atomic_add_f32\n@@ -917,9 +999,9 @@ func @test_get_program_id(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n func @test_get_num_program(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n-  // CHECK: nvvm.read.ptx.sreg.ntid.x\n-  // CHECK: nvvm.read.ptx.sreg.ntid.y\n-  // CHECK: nvvm.read.ptx.sreg.ntid.z\n+  // CHECK: nvvm.read.ptx.sreg.nctaid.x\n+  // CHECK: nvvm.read.ptx.sreg.nctaid.y\n+  // CHECK: nvvm.read.ptx.sreg.nctaid.z\n   %blockdimx = tt.get_num_programs {axis=0:i32} : i32\n   %blockdimy = tt.get_num_programs {axis=1:i32} : i32\n   %blockdimz = tt.get_num_programs {axis=2:i32} : i32"}, {"filename": "test/lib/Analysis/TestMembar.cpp", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -26,7 +26,9 @@ struct TestMembarPass\n     auto op_name = SymbolTable::getSymbolName(operation).getValue().str();\n     os << op_name << \"\\n\";\n     Allocation allocation(operation);\n-    MembarAnalysis analysis(&allocation);\n+    MembarAnalysis membarPass(&allocation);\n+    membarPass.run();\n+\n     size_t operationId = 0;\n     operation->walk<WalkOrder::PreOrder>([&](Operation *op) {\n       if (isa<gpu::BarrierOp>(op)) {"}]