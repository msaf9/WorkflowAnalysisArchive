[{"filename": "include/triton/Dialect/Triton/IR/Types.h", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -1,10 +1,17 @@\n #ifndef TRITON_IR_TYPES_H_\n #define TRITON_IR_TYPES_H_\n \n+#include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/TypeSupport.h\"\n #include \"mlir/IR/Types.h\"\n \n #define GET_TYPEDEF_CLASSES\n #include \"triton/Dialect/Triton/IR/Types.h.inc\"\n \n+namespace mlir {\n+\n+unsigned getPointeeBitWidth(RankedTensorType tensorTy);\n+\n+}\n+\n #endif // TRITON_IR_TYPES_H_"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -850,8 +850,11 @@ unsigned AxisInfoAnalysis::getPtrAlignment(Value ptr) {\n   auto axisInfo = lookupLatticeElement(ptr)->getValue();\n   auto layout = tensorTy.getEncoding();\n   auto order = triton::gpu::getOrder(layout);\n-  auto maxMultiple = axisInfo.getDivisibility(order[0]);\n+  auto maxMultipleBytes = axisInfo.getDivisibility(order[0]);\n   auto maxContig = axisInfo.getContiguity(order[0]);\n+  auto elemNumBits = getPointeeBitWidth(tensorTy);\n+  auto elemNumBytes = std::max<unsigned>(elemNumBits / 8, 1);\n+  auto maxMultiple = std::max<int64_t>(maxMultipleBytes / elemNumBytes, 1);\n   unsigned alignment = std::min(maxMultiple, maxContig);\n   return alignment;\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 8, "changes": 10, "file_content_changes": "@@ -43,14 +43,9 @@ struct LoadStoreConversionBase {\n     if (!tensorTy)\n       return 1;\n     auto contiguity = getContiguity(ptr);\n-    unsigned numElemBits = 0;\n-    auto ptrTy = tensorTy.getElementType().cast<triton::PointerType>();\n-    auto pointeeType = ptrTy.getPointeeType();\n-    numElemBits = pointeeType.isa<triton::Float8Type>()\n-                      ? 8\n-                      : pointeeType.getIntOrFloatBitWidth();\n+    auto pointeeBitWidth = getPointeeBitWidth(tensorTy);\n     // The maximum vector size is 128 bits on NVIDIA GPUs.\n-    return std::min<unsigned>(128 / numElemBits, contiguity);\n+    return std::min<unsigned>(128 / pointeeBitWidth, contiguity);\n   }\n \n   unsigned getMaskAlignment(Value mask) const {\n@@ -781,7 +776,6 @@ struct InsertSliceAsyncOpConversion\n     auto srcIndices = emitIndices(loc, rewriter, srcBlockedLayout, srcShape);\n \n     for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n-\n       // 16 * 8 = 128bits\n       auto maxBitWidth =\n           std::max<unsigned>(128, resElemTy.getIntOrFloatBitWidth());"}, {"filename": "lib/Dialect/Triton/IR/Types.cpp", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -37,3 +37,15 @@ Type PointerType::parse(AsmParser &parser) {\n void PointerType::print(AsmPrinter &printer) const {\n   printer << \"<\" << getPointeeType() << \">\";\n }\n+\n+namespace mlir {\n+\n+unsigned getPointeeBitWidth(RankedTensorType tensorTy) {\n+  auto ptrTy = tensorTy.getElementType().cast<triton::PointerType>();\n+  auto pointeeType = ptrTy.getPointeeType();\n+  return pointeeType.isa<triton::Float8Type>()\n+             ? 8\n+             : pointeeType.getIntOrFloatBitWidth();\n+}\n+\n+} // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "modified", "additions": 5, "deletions": 7, "changes": 12, "file_content_changes": "@@ -31,15 +31,13 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n \n     // Thread tile size depends on memory alignment\n     SmallVector<unsigned, 4> sizePerThread(rank, 1);\n-    PointerType ptrType = origType.getElementType().cast<PointerType>();\n-    auto pointeeType = ptrType.getPointeeType();\n-    unsigned numBits = pointeeType.isa<triton::Float8Type>()\n-                           ? 8\n-                           : pointeeType.getIntOrFloatBitWidth();\n-    unsigned maxMultiple = info.getDivisibility(order[0]);\n+    unsigned elemNumBits = getPointeeBitWidth(origType);\n+    unsigned elemNumBytes = std::max(elemNumBits / 8, 1u);\n+    unsigned maxMultipleBytes = info.getDivisibility(order[0]);\n+    unsigned maxMultiple = std::max(maxMultipleBytes / elemNumBytes, 1u);\n     unsigned maxContig = info.getContiguity(order[0]);\n     unsigned alignment = std::min(maxMultiple, maxContig);\n-    unsigned perThread = std::min(alignment, 128 / numBits);\n+    unsigned perThread = std::min(alignment, 128 / elemNumBits);\n     sizePerThread[order[0]] = std::min<int>(perThread, numElemsPerThread);\n \n     SmallVector<unsigned> dims(rank);"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 48, "deletions": 3, "changes": 51, "file_content_changes": "@@ -128,7 +128,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n   // CHECK-LABEL: global_load_store_vec4\n-  func @global_load_store_vec4(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n+  func @global_load_store_vec4(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n     %c256_i32 = arith.constant 256 : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n@@ -192,10 +192,55 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n \n // -----\n \n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: global_load_store_vec2\n+    func @global_load_store_vec2(%arg0: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg3: i32) {\n+    %c256_i32 = arith.constant 256 : i32\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.muli %0, %c256_i32 : i32\n+    %2 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n+    %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n+    %4 = arith.addi %3, %2 : tensor<256xi32, #blocked0>\n+    %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %6 = tt.addptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n+    %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n+\n+    // Load 8 elements from A with four vectorized load instruction\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+\n+    // Load 8 elements from B with four vectorized load instruction\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+\n+    %9 = tt.load %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n+    %10 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n+    %11 = arith.addf %9, %10 : tensor<256xf32, #blocked0>\n+    %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n+\n+    // Store 8 elements to global with four vectorized store instruction\n+    // CHECK: @${{.*}} st.global.v2.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}} };\n+    // CHECK: @${{.*}} st.global.v2.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}} };\n+    // CHECK: @${{.*}} st.global.v2.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}} };\n+    // CHECK: @${{.*}} st.global.v2.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}} };\n+    tt.store %13, %11 : tensor<256xf32, #blocked0>\n+    return\n+  }\n+}\n+\n+// -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: global_load_store_vec8\n-    func @global_load_store_vec8(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n+    func @global_load_store_vec8(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n     %c256_i32 = arith.constant 256 : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n@@ -436,7 +481,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #A = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_insert_slice_async_v4\n-  func @basic_insert_slice_async_v4(%arg0: !tt.ptr<f32> {tt.divisibility = 8 : i32}) {\n+  func @basic_insert_slice_async_v4(%arg0: !tt.ptr<f32> {tt.divisibility = 32 : i32}) {\n     %off0_ = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #slice2d1>\n     %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<64xi32, #slice3d0>\n     %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<16xi32, #slice2d1>) -> tensor<16x1xi32, #block2>"}]