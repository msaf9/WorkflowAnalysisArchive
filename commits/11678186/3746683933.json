[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -731,6 +731,10 @@ def serialized_add(data, Lock):\n     (f'int{x}', f'uint{x}', True) for x in [8, 16, 32, 64]\n ])\n def test_cast(dtype_x, dtype_z, bitcast, device='cuda'):\n+    # bfloat16 on cc < 80 will not be tested\n+    check_type_supported(dtype_x) \n+    check_type_supported(dtype_z)\n+\n     # This is tricky because numpy doesn't have bfloat, and torch doesn't have uints.\n     x0 = 43 if dtype_x in int_dtypes else 43.5\n     if dtype_x in float_dtypes and dtype_z == 'int1':\n@@ -965,6 +969,8 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n \n @pytest.mark.parametrize(\"op, dtype_str, shape, axis\", reduce_configs1 + reduce_configs2)\n def test_reduce2d(op, dtype_str, shape, axis, device='cuda'):\n+    check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n+\n     # triton kernel\n     @triton.jit\n     def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexpr):"}]