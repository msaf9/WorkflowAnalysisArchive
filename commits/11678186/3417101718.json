[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 26, "deletions": 23, "changes": 49, "file_content_changes": "@@ -2640,7 +2640,7 @@ class MMA16816SmemLoader {\n   // compute 8-bit matrix offset.\n   SmallVector<Value> computeB8MatOffs(Value warpOff, Value lane) {\n     assert(needTrans && \"Only used in transpose mode.\");\n-    Value cOffInMat = udiv(lane, i32_val(4));\n+    Value cOffInMat = udiv(lane, i32_val(4)); // groupId\n     Value sOffInMat =\n         mul(urem(lane, i32_val(4)), i32_val(4)); // each thread load 4 cols\n \n@@ -2692,10 +2692,8 @@ class MMA16816SmemLoader {\n       ptrIdx = matIdx[order[0]] / (instrShape[order[0]] / matShape[order[0]]);\n     else if (elemBytes == 4 && needTrans)\n       ptrIdx = matIdx[order[0]];\n-    else if (elemBytes == 1 && needTrans)\n-      ptrIdx = matIdx[order[0]] * 4;\n     else\n-      llvm::report_fatal_error(\"unsupported mma type found\");\n+      ptrIdx = matIdx[order[0]] * 4;\n \n     // The main difference with the original triton code is we removed the\n     // prefetch-related logic here for the upstream optimizer phase should\n@@ -2782,16 +2780,14 @@ class MMA16816SmemLoader {\n       };\n \n       assert(sMatStride == 1);\n-      int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n-      Value sOffsetElemVal = mul(i32_val(sOffsetElem), sTileStride);\n-      int sOffsetArrElem = 1 * (sMatStride * sMatShape);\n+      Value sOffsetElemVal = mul(\n+          i32_val(matIdx[order[1]] * (sMatStride * sMatShape)), sTileStride);\n       Value sOffsetArrElemVal =\n-          add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sTileStride));\n+          mul(i32_val(1 * (sMatStride * sMatShape)), sTileStride);\n \n       std::array<Value, 4> i8v4Elems;\n       std::array<Value, 4> i32Elems;\n-      i8v4Elems.fill(\n-          rewriter.create<LLVM::UndefOp>(loc, vec_ty(type::i8Ty(ctx), 4)));\n+      i8v4Elems.fill(undef(vec_ty(i8_ty, 4)));\n \n       Value i8Elems[4][4];\n       Type elemTy = type::i8Ty(ctx);\n@@ -2804,7 +2800,7 @@ class MMA16816SmemLoader {\n         for (int i = 2; i < 4; ++i)\n           for (int j = 0; j < 4; ++j)\n             i8Elems[i][j] =\n-                load(gep(elemPtrTy, ptrs[i - 2][j], sOffsetElemVal));\n+                load(gep(elemPtrTy, ptrs[i - 2][j], sOffsetArrElemVal));\n \n         for (int m = 0; m < 4; ++m) {\n           for (int e = 0; e < 4; ++e)\n@@ -3511,9 +3507,9 @@ struct MMA16816ConversionHelper {\n         0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShpae*/,\n         {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/);\n \n-    for (int n = 0; n < std::max(numRepN / 2, 1); ++n) {\n+    for (int n = 0; n < numRepN; n += 2) {\n       for (int k = 0; k < numRepK; ++k)\n-        loadFn(2 * n, 2 * k);\n+        loadFn(n, 2 * k);\n     }\n \n #if PRINT_AB\n@@ -3593,6 +3589,11 @@ struct MMA16816ConversionHelper {\n     llvm::outs() << \"mma.instr: \" << helper.getMmaInstr() << \"\\n\";\n     auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n       unsigned colsPerThread = numRepN * 2;\n+      SmallVector<size_t> idx({(m + 0) * colsPerThread + (n * 2) + 0,\n+                               (m + 0) * colsPerThread + (n * 2) + 1,\n+                               (m + 1) * colsPerThread + (n * 2) + 0,\n+                               (m + 1) * colsPerThread + (n * 2) + 1});\n+\n       PTXBuilder builder;\n       auto &mma = *builder.create(helper.getMmaInstr().str());\n       auto retArgs = builder.newListOperand(4, \"=r\");\n@@ -3605,9 +3606,9 @@ struct MMA16816ConversionHelper {\n       auto bArgs =\n           builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n       auto cArgs = builder.newListOperand();\n+\n       for (int i = 0; i < 4; ++i) {\n-        cArgs->listAppend(builder.newOperand(fc[m * colsPerThread + 4 * n + i],\n-                                             std::to_string(i)));\n+        cArgs->listAppend(builder.newOperand(fc[idx[i]], std::to_string(i)));\n         // reuse the output registers\n       }\n \n@@ -3620,19 +3621,21 @@ struct MMA16816ConversionHelper {\n         SmallVector<Value> vals;\n         for (int i = 0; i < 4; i++)\n           vals.push_back(extract_element(i8_ty, val, i32_val(i)));\n-        LLVM::llPrintf(\"t-%d $\" + hint.str() + \": (m%d, n%d) = [%d,%d,%d,%d]\",\n+        LLVM::llPrintf(\"t-%d $\" + hint.str() + \": (%d, %d) = [%d,%d,%d,%d]\",\n                        {thread, i32_val(m), i32_val(n),\n                         // data\n                         vals[0], vals[1], vals[2], vals[3]},\n                        rewriter);\n       };\n \n-      print_val(m, k, ha, \"a\");\n-      // print_a(m+1, k, ha);\n-      // print_a(m, k+1, ha);\n-      // print_a(m+1, k+1, ha);\n+      // print_val(m, k, ha, \"a0\");\n+      // print_val(m+1, k, ha, \"a1\");\n+      // print_val(m, k+1, ha, \"a2\");\n+      // print_val(m+1, k+1, ha, \"a3\");\n \n-      print_val(n, k, hb, \"b\");\n+      print_val(n, k, hb, \"b0\");\n+      print_val(n, k + 1, hb, \"b1\");\n+      printf(\"====\\n\");\n \n       // DEBUG: print $b\n       // DEBUG: print $c\n@@ -3647,10 +3650,10 @@ struct MMA16816ConversionHelper {\n       llvm::outs() << \"mmaOut.getType: \" << mmaOut.getType() << \"\\n\";\n       Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n       for (int i = 0; i < 4; ++i)\n-        fc[m * colsPerThread + 4 * n + i] =\n-            extract_val(elemTy, mmaOut, getIntAttr(i));\n+        fc[idx[i]] = extract_val(elemTy, mmaOut, getIntAttr(i));\n     };\n \n+    printf(\"numRep (%d,%d,%d)\\n\", numRepM, numRepN, numRepK);\n     for (int k = 0; k < numRepK; ++k)\n       for (int m = 0; m < numRepM; ++m)\n         for (int n = 0; n < numRepN; ++n)"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 27, "deletions": 20, "changes": 47, "file_content_changes": "@@ -1,3 +1,5 @@\n+import random\n+\n import pytest\n import torch\n from torch.testing import assert_close\n@@ -62,27 +64,31 @@ def matmul_no_scf_kernel(\n     [64, 128, 128, 2],\n ])\n def test_gemm_no_scf_int8(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n-    a = torch.randint(-1, 0, (SIZE_M, SIZE_K), device='cuda', dtype=torch.int8)\n-    b = torch.randint(-1, 0, (SIZE_K, SIZE_N), device='cuda', dtype=torch.int8)\n+    a = torch.randint(-5, 5, (SIZE_M, SIZE_K), device='cuda', dtype=torch.int8)\n+    b = torch.randint(-5, 5, (SIZE_K, SIZE_N), device='cuda', dtype=torch.int8)\n     c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.int32)\n \n-    # for debug\n-    offset = -128\n-    for i in range(len(a)):\n-        for j in range(len(a[0])):\n-            a.data[i][j] = offset\n-            offset += 1\n-            if offset > 127: offset = -offset\n-    print('a', a)\n-\n-    offset = -128\n-    for i in range(len(b)):\n-        for j in range(len(b[0])):\n-            b.data[i][j] = offset\n-            offset += 1\n-            if offset > 127: offset = -offset\n-    print('b', b)\n-    # end for debug\n+    if True:\n+        data_domain = (-5, 5)\n+        random.seed(0)\n+        # for debug\n+        offset = -128\n+        for i in range(len(a)):\n+            for j in range(len(a[0])):\n+                a.data[i][j] = random.randint(*data_domain)\n+                offset += 1\n+                if offset > 127: offset = -offset\n+        print('a', a)\n+\n+        offset = -128\n+        for i in range(len(b)):\n+            for j in range(len(b[0])):\n+                #b.data[i][j] = offset\n+                b.data[i][j] = random.randint(*data_domain)\n+                offset += 1\n+                if offset > 127: offset = -offset\n+        print('b', b)\n+        # end for debug\n \n     grid = lambda META: (1, )\n     matmul_no_scf_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n@@ -190,4 +196,5 @@ def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLO\n     assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err), check_dtype=False)\n \n \n-test_gemm_no_scf_int8(SIZE_M=16, SIZE_N=8, SIZE_K=32, NUM_WARPS=1)\n+#test_gemm_no_scf_int8(SIZE_M=16, SIZE_N=8, SIZE_K=32, NUM_WARPS=1)\n+test_gemm_no_scf_int8(SIZE_M=16, SIZE_N=16, SIZE_K=32, NUM_WARPS=1)"}]