[{"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -81,8 +81,8 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n                 (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n                 (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n-            ] for ADTYPE, BDTYPE in [(\"float8e4\", \"float8e5\"),\n-                                     (\"float8e4\", \"float16\"),\n+            ] for ADTYPE, BDTYPE in [(\"float8e4nv\", \"float8e5\"),\n+                                     (\"float8e4nv\", \"float16\"),\n                                      (\"float16\", \"float8e5\"),\n                                      (\"float16\", \"float32\"),\n                                      (\"float32\", \"float16\"),\n@@ -97,7 +97,7 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                                      (\"float8e4b15\", \"float16\"),\n                                      (\"float16\", \"float8e4b15\"),\n                                      (\"float8e5\", \"float8e5\"),\n-                                     (\"float8e4\", \"float8e4\"),\n+                                     (\"float8e4nv\", \"float8e4nv\"),\n                                      (\"int8\", \"int8\")]\n         ]\n     ),\n@@ -108,8 +108,8 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT,\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n     if capability[0] < 8 and (ADTYPE == \"bfloat16\" or BDTYPE == \"bfloat16\"):\n         pytest.skip(\"Only test bfloat16 on devices with sm >= 80\")\n-    if capability[0] < 9 and (ADTYPE == \"float8e4\" or BDTYPE == \"float8e4\"):\n-        pytest.skip(\"Only test float8e4 on devices with sm >= 90\")\n+    if capability[0] < 9 and (ADTYPE == \"float8e4nv\" or BDTYPE == \"float8e4nv\"):\n+        pytest.skip(\"Only test float8e4nv on devices with sm >= 90\")\n     if (ADTYPE == \"bfloat16\" or BDTYPE == \"bfloat16\") and SPLIT_K != 1:\n         pytest.skip(\"bfloat16 matmuls don't allow split_k for now\")\n     torch.manual_seed(0)\n@@ -133,7 +133,7 @@ def maybe_upcast(x, dtype, is_float8):\n \n     def init_input(m, n, dtype):\n         if 'float8' in dtype:\n-            ewidth = {'float8e4b15': 4, 'float8e4': 4, 'float8e5': 5}[dtype]\n+            ewidth = {'float8e4b15': 4, 'float8e4nv': 4, 'float8e5': 5}[dtype]\n             sign = torch.randint(2, size=(m, n), device=\"cuda\", dtype=torch.int8) * 128\n             val = torch.randint(2**3 - 1, size=(m, n), device=\"cuda\", dtype=torch.int8) << 7 - ewidth\n             return sign | val"}]