[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 12, "deletions": 28, "changes": 40, "file_content_changes": "@@ -984,32 +984,6 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return ret;\n   }\n \n-  SmallVector<Value>\n-  emitBaseIndexForMmaLayoutV2(Location loc, ConversionPatternRewriter &rewriter,\n-                              const MmaEncodingAttr &mmaLayout,\n-                              RankedTensorType type) const {\n-    auto shape = type.getShape();\n-    auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n-    assert(warpsPerCTA.size() == 2);\n-    auto order = triton::gpu::getOrder(mmaLayout);\n-    Value threadId = getThreadId(rewriter, loc);\n-    Value warpSize = i32_val(32);\n-    Value laneId = urem(threadId, warpSize);\n-    Value warpId = udiv(threadId, warpSize);\n-\n-    SmallVector<Value> multiDimWarpId =\n-        delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n-    multiDimWarpId[0] = urem(multiDimWarpId[0], i32_val(shape[0] / 16));\n-    multiDimWarpId[1] = urem(multiDimWarpId[1], i32_val(shape[1] / 8));\n-    Value offWarp0 = mul(multiDimWarpId[0], i32_val(16));\n-    Value offWarp1 = mul(multiDimWarpId[1], i32_val(8));\n-\n-    SmallVector<Value> multiDimBase(2);\n-    multiDimBase[0] = add(udiv(laneId, i32_val(4)), offWarp0);\n-    multiDimBase[1] = add(mul(i32_val(2), urem(laneId, i32_val(4))), offWarp1);\n-    return multiDimBase;\n-  }\n-\n   SmallVector<SmallVector<unsigned>>\n   emitOffsetForMmaLayoutV2(const MmaEncodingAttr &mmaLayout,\n                            RankedTensorType type) const {\n@@ -1062,8 +1036,18 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     else\n       warpsN = shape[1] / instrShape[1];\n \n-    SmallVector<Value> multiDimWarpId =\n-        delinearize(rewriter, loc, warpId, _warpsPerCTA, order);\n+    SmallVector<Value> multiDimWarpId(2);\n+    if (mmaLayout.isHopper()) {\n+      // TODO[goostavz]: the tiling order from CTA->warp level is different for\n+      // MMAv2/3. This is a workaround since we don't explicitly have warpGrp level\n+      // in the layout definition, and the tiling order of warpGrp->warp must be fixed\n+      // to meet the HW's needs. We may need to consider to explicitly define warpGrpPerCTA\n+      // for MMAv3 layout.\n+      multiDimWarpId[0] = urem(warpId, warpsPerCTA[0]);\n+      multiDimWarpId[1] = urem(udiv(warpId, warpsPerCTA[0]), warpsPerCTA[1]);\n+    } else {\n+      multiDimWarpId = delinearize(rewriter, loc, warpId, _warpsPerCTA, order);\n+    }\n     Value warpId0 = urem(multiDimWarpId[0], i32_val(warpsM));\n     Value warpId1 = urem(multiDimWarpId[1], i32_val(warpsN));\n "}, {"filename": "python/test/unit/hopper/test_gemm.py", "status": "modified", "additions": 28, "deletions": 35, "changes": 63, "file_content_changes": "@@ -152,6 +152,7 @@ def matmul_kernel(\n     DO_SOFTMAX: tl.constexpr, CHAIN_DOT: tl.constexpr,\n     A_ORDER_0: tl.constexpr, A_ORDER_1: tl.constexpr,\n     B_ORDER_0: tl.constexpr, B_ORDER_1: tl.constexpr,\n+    W_ORDER_0: tl.constexpr, W_ORDER_1: tl.constexpr,\n     Z_ORDER_0: tl.constexpr, Z_ORDER_1: tl.constexpr\n ):\n     pid = tl.program_id(axis=0)\n@@ -170,8 +171,9 @@ def matmul_kernel(\n                                    offsets=(block_offset_m, 0), block_shape=(BLOCK_M, BLOCK_K), order=(A_ORDER_0, A_ORDER_1))\n     b_tile_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(stride_bk, stride_bn),\n                                    offsets=(0, block_offset_n), block_shape=(BLOCK_K, BLOCK_N), order=(B_ORDER_0, B_ORDER_1))\n+    # for chain-dot, BLOCK_N must always be equal to N, and each program loads the whole W matrix\n     w_tile_ptr = tl.make_block_ptr(base=w_ptr, shape=(N, N), strides=(stride_wm, stride_wn),\n-                                   offsets=(0, block_offset_n), block_shape=(BLOCK_N, BLOCK_N), order=(Z_ORDER_1, Z_ORDER_0))\n+                                   offsets=(0, 0), block_shape=(BLOCK_N, BLOCK_N), order=(W_ORDER_0, W_ORDER_1))\n     z = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n \n     offs_m = block_offset_m + tl.arange(0, BLOCK_M)\n@@ -216,7 +218,7 @@ def matmul_kernel(\n         tl.store(z_ptrs, z, mask=mask)\n \n \n-@pytest.mark.parametrize('BLOCK_M,BLOCK_N,BLOCK_K,NUM_WARPS,NUM_CTAS,M,N,K,TRANS_A,TRANS_B,TRANS_C,epilogue,out_dtype,USE_TMA_STORE,NUM_STAGES,ENABLE_WS',\n+@pytest.mark.parametrize('BLOCK_M,BLOCK_N,BLOCK_K,NUM_WARPS,NUM_CTAS,M,N,K,TRANS_A,TRANS_B,TRANS_OUTPUT,epilogue,out_dtype,USE_TMA_STORE,NUM_STAGES,ENABLE_WS',\n                          [(128, 128, 64, 4, 1, *shape_w_c, 'none', out_dtype, use_tma_store, 3, enable_ws)\n                           for shape_w_c in [\n                              # badcase from cublas-important-layers\n@@ -228,7 +230,7 @@ def matmul_kernel(\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n                              for enable_ws in [False, True]\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_c, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n+                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n                               # softmax works for one CTA\n                               for shape_w_c in [\n                              [64, 64, 16, 4, 1, 64, 64, 64],\n@@ -242,10 +244,10 @@ def matmul_kernel(\n                              for use_tma_store in [False, True]\n                              for trans_a in [False, True]\n                              for trans_b in [False, True]\n-                             for trans_c in [False, True]\n+                             for trans_output in [False, True]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_c, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n+                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n                               for shape_w_c in [\n                              [64, 64, 16, 4, 1, 128, 128, 64],\n                              *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4, 8] for num_ctas in [1, 2, 4]],\n@@ -267,11 +269,11 @@ def matmul_kernel(\n                              for use_tma_store in [False, True]\n                              for trans_a in [False, True]\n                              for trans_b in [False, True]\n-                             for trans_c in [False, True]\n+                             for trans_output in [False, True]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n-                             if not (epilogue == 'chain-dot' and (shape_w_c[5] is not None or shape_w_c[0] != shape_w_c[1]))\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_c, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                             if not (epilogue == 'chain-dot' and (shape_w_c[1] != shape_w_c[6]))\n+                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                               for shape_w_c in [\n                              [64, 64, 32, 4, 1, 128, 256, 64],\n                              [128, 128, 16, 4, 4, 512, 256, 64],\n@@ -290,34 +292,34 @@ def matmul_kernel(\n                              for use_tma_store in [False, True]\n                              for trans_a in [False, True]\n                              for trans_b in [False, True]\n-                             for trans_c in [False, True]\n+                             for trans_output in [False, True]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n-                         ] + [(64, n, 16, 4, 1, 512, 256, 256, False, True, trans_c, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                         ] + [(64, n, 16, 4, 1, 512, 256, 256, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                               # loop over instr shapes\n                               for n in [16, 32, 64, 128, 256]\n-                              for trans_c in [False, True]\n+                              for trans_output in [False, True]\n                               for out_dtype in ['float16', 'float32']\n                               for use_tma_store in [False, True]\n                               for num_stages in [2, 4, 5, 7]\n                               for enable_ws in [False, True]\n-                              ] + [(*shape_w_c, *shape, False, True, trans_c, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                              ] + [(*shape_w_c, *shape, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                                    # irregular shapes\n                                    for shape_w_c in [\n                                        [128, 128, 64, 4, 1],\n                                        [256, 128, 64, 4, 2],\n                                        [128, 128, 128, 4, 2],\n                               ]\n                              for shape in list(itertools.product([*range(512, 4096, 360)], [*range(512, 4096, 360)], [512, 1024]))\n-                             for trans_c in [False, True]\n+                             for trans_output in [False, True]\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n                              for num_stages in [2, 3, 4]\n                              for enable_ws in [False, True]\n                          ])\n @pytest.mark.skipif(torch.cuda.get_device_capability()\n                     [0] < 9, reason=\"Requires compute capability >= 9\")\n-def test_gemm(BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A, TRANS_B, TRANS_C, epilogue, out_dtype, USE_TMA_STORE, NUM_STAGES, ENABLE_WS):\n+def test_gemm(BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A, TRANS_B, TRANS_OUTPUT, epilogue, out_dtype, USE_TMA_STORE, NUM_STAGES, ENABLE_WS):\n     if '-'.join(map(str, [BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A, TRANS_B])) in [\n         '16-32-64-4-4-512-256-64-True-False',\n         '16-32-64-4-4-512-256-64-True-True',\n@@ -334,13 +336,7 @@ def test_gemm(BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A,\n         '16-32-64-8-2-256-256-256-False',\n         '16-32-64-8-2-256-256-256-True',\n     ]:\n-        pytest.skip('illegal memory access.')\n-\n-    # with ENABLE_TMA=1 and ENABLE_MMA_V3=1\n-    if '-'.join(map(str, [BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K])) in [\n-        '64-64-32-8-1-128-256-64',\n-    ]:\n-        pytest.skip('Tensor-likes are not close!')\n+        pytest.skip('Known legacy issue, ldmatrix can only support x4')\n \n     if NUM_CTAS > 1 and NUM_WARPS == 8:\n         pytest.skip('Tensor-likes are not close!')\n@@ -381,27 +377,23 @@ def test_gemm(BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A,\n \n     # avoid out of memory\n     if epilogue in ['add-matrix', 'add-rows', 'add-cols']:\n-        if (TRANS_C):\n-            bias = torch.randn((M, N), device='cuda', dtype=torch_out_dtype)\n-        else:\n+        if (TRANS_OUTPUT):\n             bias = torch.randn((N, M), device='cuda', dtype=torch_out_dtype).T\n+        else:\n+            bias = torch.randn((M, N), device='cuda', dtype=torch_out_dtype)\n     else:\n         bias = torch.randn((1, 1), device='cuda', dtype=torch_out_dtype)\n \n-    if epilogue == 'chain-dot':\n-        if (TRANS_C):\n-            w = torch.randn((N, N), device='cuda', dtype=torch.float16).T\n-        else:\n-            w = torch.randn((M, M), device='cuda', dtype=torch.float16)\n-    else:\n-        w = torch.randn((1, 1), device='cuda', dtype=torch.float16).T\n+    # for chain-dot only\n+    w = torch.randn((N, N), device='cuda', dtype=torch.float16).T\n+    w_order = [0, 1]\n \n-    if (TRANS_C):\n-        z = torch.full((M, N), 1., device='cuda', dtype=torch_out_dtype)\n-        z_order = [1, 0]\n-    else:\n+    if (TRANS_OUTPUT):\n         z = torch.full((N, M), 1., device='cuda', dtype=torch_out_dtype).T\n         z_order = [0, 1]\n+    else:\n+        z = torch.full((M, N), 1., device='cuda', dtype=torch_out_dtype)\n+        z_order = [1, 0]\n \n     # torch result\n     a_f32 = a.to(torch.float32)\n@@ -445,6 +437,7 @@ def grid(META):\n                               CHAIN_DOT=epilogue == 'chain-dot',\n                               A_ORDER_0=a_order[0], A_ORDER_1=a_order[1],\n                               B_ORDER_0=b_order[0], B_ORDER_1=b_order[1],\n+                              W_ORDER_0=w_order[0], W_ORDER_1=w_order[1],\n                               Z_ORDER_0=z_order[0], Z_ORDER_1=z_order[1],\n                               num_warps=NUM_WARPS, num_ctas=NUM_CTAS, num_stages=NUM_STAGES,\n                               enable_warp_specialization=ENABLE_WS)"}]