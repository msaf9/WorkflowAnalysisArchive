[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -3,10 +3,9 @@ name: Integration Tests\n on:\n   workflow_dispatch:\n   pull_request:\n-    branches:\n-      - main\n-      - triton-mlir\n+    branches: [main]\n   merge_group:\n+    branches: [main]\n     types: [checks_requested]\n \n concurrency:"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 45, "deletions": 0, "changes": 45, "file_content_changes": "@@ -153,3 +153,48 @@ def test_elementwise(N):\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     triton.testing.assert_almost_equal(cur_gpu_util, ref_gpu_util, decimal=2)\n+\n+#######################\n+# Flash-Attention\n+#######################\n+\n+flash_attention_data = {\n+    \"a100\": {\n+        (4, 48, 4096, 64, 'forward', 'float16'): 0.37,\n+        (4, 48, 4096, 64, 'backward', 'float16'): 0.25,\n+    }\n+}\n+@pytest.mark.parametrize(\"Z, H, N_CTX, D_HEAD\", [[4, 48, 4096, 64]])\n+@pytest.mark.parametrize(\"mode\", ['forward', 'backward'])\n+@pytest.mark.parametrize(\"dtype_str\", ['float16'])\n+def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n+    is_backward = mode == 'backward'\n+    capability = torch.cuda.get_device_capability()\n+    if capability[0] < 8:\n+        pytest.skip(\"Flash attention only supported for compute capability < 80\")\n+    torch.manual_seed(20)\n+    dtype = {'float16': torch.float16, 'float32': torch.float32, 'int8': torch.int8}[dtype_str]\n+    # init data\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n+    sm_scale = 0.2\n+    # benchmark\n+    fn = lambda: triton.ops.attention(q, k, v, sm_scale)\n+    if is_backward:\n+        o = fn()\n+        do = torch.randn_like(o)\n+        fn = lambda: o.backward(do, retain_graph=True)\n+    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=300)\n+    # compute flops\n+    flops_per_matmul = 2.*Z*H*N_CTX*N_CTX*D_HEAD*0.5\n+    total_flops = 2*flops_per_matmul\n+    if is_backward:\n+        total_flops *= 2.5 # 2.0(bwd) + 0.5(recompute)\n+    cur_gpu_perf = total_flops/ms * 1e-9\n+    # maximum flops\n+    cur_sm_clock = nvsmi(['clocks.current.sm'])[0]\n+    max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n+    cur_gpu_util = cur_gpu_perf / max_gpu_perf\n+    ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, mode, dtype_str)]\n+    triton.testing.assert_almost_equal(cur_gpu_util, ref_gpu_util, decimal=2)\n\\ No newline at end of file"}]