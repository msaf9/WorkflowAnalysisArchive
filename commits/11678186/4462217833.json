[{"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 29, "deletions": 4, "changes": 33, "file_content_changes": "@@ -686,9 +686,12 @@ class CFBranchPattern : public OpConversionPattern<cf::BranchOp> {\n   LogicalResult\n   matchAndRewrite(cf::BranchOp op, cf::BranchOp::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    addNamedAttrs(rewriter.replaceOpWithNewOp<cf::BranchOp>(\n-                      op, op.getSuccessor(), adaptor.getOperands()),\n-                  adaptor.getAttributes());\n+    auto converter = getTypeConverter();\n+    auto newOp = rewriter.replaceOpWithNewOp<cf::BranchOp>(\n+        op, op.getSuccessor(), adaptor.getOperands());\n+    if (failed(rewriter.convertRegionTypes(newOp.getSuccessor()->getParent(),\n+                                           *converter)))\n+      return failure();\n     return success();\n   }\n };\n@@ -713,14 +716,36 @@ class CFCondBranchPattern : public OpConversionPattern<cf::CondBranchOp> {\n     if (failed(rewriter.convertRegionTypes(newOp.getFalseDest()->getParent(),\n                                            *converter)))\n       return failure();\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+};\n+\n+class FuncOpPattern : public OpConversionPattern<func::FuncOp> {\n+public:\n+  using OpConversionPattern<func::FuncOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(func::FuncOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto converter = getTypeConverter();\n+    auto newOp = rewriter.replaceOpWithNewOp<func::FuncOp>(\n+        op, op.getName(), op.getFunctionType());\n+    addNamedAttrs(newOp, adaptor.getAttributes());\n+    rewriter.inlineRegionBefore(op.getBody(), newOp.getBody(),\n+                                newOp.getBody().end());\n+    if (failed(rewriter.convertRegionTypes(&newOp.getBody(), *converter)))\n+      return failure();\n+\n     return success();\n   }\n };\n \n void populateCFPatterns(TritonGPUTypeConverter &typeConverter,\n                         RewritePatternSet &patterns) {\n   MLIRContext *context = patterns.getContext();\n-  patterns.add<CFBranchPattern, CFCondBranchPattern>(typeConverter, context);\n+  patterns.add<FuncOpPattern, CFCondBranchPattern, CFBranchPattern>(\n+      typeConverter, context);\n }\n //\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -595,7 +595,7 @@ class TritonGPURemoveLayoutConversionsPass\n     patterns.add<DecomposeDotOperand>(context);\n     patterns.add<ConvertDotConvert>(context);\n \n-    if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n+    if (mlir::applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n       signalPassFailure();\n     }\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/TritonGPUConversion.cpp", "status": "modified", "additions": 16, "deletions": 11, "changes": 27, "file_content_changes": "@@ -14,11 +14,10 @@ using namespace mlir::triton::gpu;\n TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n                                                int numWarps)\n     : context(context), numWarps(numWarps) {\n-  // TODO: how does MLIR pick the right conversion?\n   addConversion([](Type type) { return type; });\n   addConversion([this](RankedTensorType tensorType) -> RankedTensorType {\n     // types with encoding are already in the right format\n-    // TODO: check for layout encodings specifically\n+    // TODO: check for layout encodings more specifically\n     if (tensorType.getEncoding())\n       return tensorType;\n     // pessimistic values for attributes:\n@@ -41,16 +40,19 @@ TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n   // This will create newArg, and map(origArg, newArg)\n   addArgumentMaterialization([&](OpBuilder &builder,\n                                  RankedTensorType tensorType, ValueRange inputs,\n-                                 Location loc) {\n-    llvm_unreachable(\"Argument rematerialization not implemented\");\n+                                 Location loc) -> llvm::Optional<Value> {\n+    llvm_unreachable(\"Argument rematerialization should not happen in Triton \"\n+                     \"-> TritonGPU conversion\");\n     return std::nullopt;\n   });\n \n   // If the origValue still has live user(s), use this to\n   // convert origValue to newValue\n   addSourceMaterialization([&](OpBuilder &builder, RankedTensorType tensorType,\n-                               ValueRange inputs, Location loc) {\n-    llvm_unreachable(\"Source rematerialization not implemented\");\n+                               ValueRange inputs,\n+                               Location loc) -> llvm::Optional<Value> {\n+    llvm_unreachable(\"Source rematerialization should not happen in Triton -> \"\n+                     \"TritonGPU Conversion\");\n     return std::nullopt;\n   });\n \n@@ -62,9 +64,6 @@ TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n     auto cast =\n         builder.create<triton::gpu::ConvertLayoutOp>(loc, tensorType, inputs);\n     return Optional<Value>(cast.getResult());\n-    // return Optional<Value>(cast.getResult(0));\n-    // llvm_unreachable(\"Not implemented\");\n-    // return std::nullopt;\n   });\n }\n \n@@ -82,10 +81,16 @@ TritonGPUConversionTarget::TritonGPUConversionTarget(\n                scf::ReduceReturnOp>();\n \n   addDynamicallyLegalDialect<arith::ArithDialect, math::MathDialect,\n-                             triton::TritonDialect, scf::SCFDialect>(\n+                             func::FuncDialect, triton::TritonDialect,\n+                             cf::ControlFlowDialect, scf::SCFDialect>(\n       [&](Operation *op) {\n-        if (typeConverter.isLegal(op))\n+        bool hasLegalRegions = true;\n+        for (auto &region : op->getRegions()) {\n+          hasLegalRegions = hasLegalRegions && typeConverter.isLegal(&region);\n+        }\n+        if (hasLegalRegions && typeConverter.isLegal(op)) {\n           return true;\n+        }\n         return false;\n       });\n "}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -261,7 +261,7 @@ translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module) {\n   }\n \n   auto optPipeline = mlir::makeOptimizingTransformer(\n-      /*optLevel=*/0, /*sizeLevel=*/0,\n+      /*optLevel=*/3, /*sizeLevel=*/0,\n       /*targetMachine=*/nullptr);\n \n   if (auto err = optPipeline(llvmModule.get())) {"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -56,7 +56,7 @@ def nvsmi(attrs):\n     'a100': {\n         (512, 512, 512): {'float16': 0.08, 'float32': 0.13, 'int8': 0.05},\n         (1024, 1024, 1024): {'float16': 0.33, 'float32': 0.35, 'int8': 0.169},\n-        (2048, 2048, 2048): {'float16': 0.59, 'float32': 0.57, 'int8': 0.34},\n+        (2048, 2048, 2048): {'float16': 0.62, 'float32': 0.57, 'int8': 0.34},\n         (4096, 4096, 4096): {'float16': 0.81, 'float32': 0.75, 'int8': 0.46},\n         (8192, 8192, 8192): {'float16': 0.77, 'float32': 0.85, 'int8': 0.51},\n         # tall-skinny"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 29, "deletions": 18, "changes": 47, "file_content_changes": "@@ -520,6 +520,17 @@ def test_unary_op(dtype_x, expr, device='cuda'):\n def test_math_op(expr, device='cuda'):\n     _test_unary('float32', f'tl.{expr}(x)', f'np.{expr}(x) ', device=device)\n \n+# ----------------\n+# test abs\n+# ----------------\n+\n+\n+@pytest.mark.parametrize(\"dtype_x\", [\n+    (dtype_x)\n+    for dtype_x in dtypes_with_bfloat16\n+])\n+def test_abs(dtype_x, device='cuda'):\n+    _test_unary(dtype_x, 'tl.abs(x)', 'np.abs(x) ', device=device)\n \n # ----------------\n # test indexing\n@@ -1791,11 +1802,11 @@ def _kernel(dst):\n \n \n @pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n-                         [('int32', 'libdevice.ffs', ''),\n-                          ('float32', 'libdevice.log2', ''),\n-                          ('float32', 'libdevice.pow', tl.libdevice.LIBDEVICE_PATH),\n-                          ('float64', 'libdevice.norm4d', '')])\n-def test_libdevice_tensor(dtype_str, expr, lib_path):\n+                         [('int32', 'math.ffs', ''),\n+                          ('float32', 'math.log2', ''),\n+                          ('float32', 'math.pow', tl.math.LIBDEVICE_PATH),\n+                          ('float64', 'math.norm4d', '')])\n+def test_math_tensor(dtype_str, expr, lib_path):\n \n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n@@ -1808,37 +1819,37 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n     # limit the range of integers so that the sum does not overflow\n     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n \n-    if expr == 'libdevice.log2':\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.broadcast_to(tl.libdevice.log2(5.0), x.shape)'})\n+    if expr == 'math.log2':\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.broadcast_to(tl.math.log2(5.0), x.shape)'})\n         y_ref = np.log2(5.0)\n-    elif expr == 'libdevice.ffs':\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.ffs(x)'})\n+    elif expr == 'math.ffs':\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.ffs(x)'})\n         y_ref = np.zeros(shape, dtype=x.dtype)\n         for i in range(shape[0]):\n             y_ref[i] = (int(x[i]) & int(-x[i])).bit_length()\n-    elif expr == 'libdevice.pow':\n+    elif expr == 'math.pow':\n         # numpy does not allow negative factors in power, so we use abs()\n         x = np.abs(x)\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.pow(x, x)'})\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.pow(x, x)'})\n         y_ref = np.power(x, x)\n-    elif expr == 'libdevice.norm4d':\n-        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.norm4d(x, x, x, x)'})\n+    elif expr == 'math.norm4d':\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.norm4d(x, x, x, x)'})\n         y_ref = np.sqrt(4 * np.power(x, 2))\n \n     x_tri = to_triton(x)\n     # triton result\n     y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n     kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n     # compare\n-    if expr == 'libdevice.ffs':\n+    if expr == 'math.ffs':\n         np.testing.assert_equal(y_ref, to_numpy(y_tri))\n     else:\n         np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n \n \n @pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n-                         [('float32', 'libdevice.pow', '')])\n-def test_libdevice_scalar(dtype_str, expr, lib_path):\n+                         [('float32', 'math.pow', '')])\n+def test_math_scalar(dtype_str, expr, lib_path):\n \n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n@@ -1854,13 +1865,13 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n \n     # numpy does not allow negative factors in power, so we use abs()\n     x = np.abs(x)\n-    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.pow(x, x)'})\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.pow(x, x)'})\n     y_ref[:] = np.power(x, x)\n \n     # triton result\n     x_tri = to_triton(x)[0].item()\n     y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n-    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n+    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'math': lib_path})\n     # compare\n     np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n "}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -341,11 +341,13 @@ def visit_Assign(self, node):\n             names = [names]\n         if not isinstance(values, tuple):\n             values = [values]\n+        native_nontensor_types = (triton.language.dtype, )\n         for name, value in zip(names, values):\n             # by default, constexpr are assigned into python variable\n             if isinstance(value, triton.language.constexpr):\n                 value = value.value\n-            if not isinstance(value, triton.language.tensor):\n+            if not isinstance(value, triton.language.tensor) and \\\n+               not isinstance(value, native_nontensor_types):\n                 value = triton.language.core._to_tensor(value, self.builder)\n             self.set_value(name, value)\n "}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -5,7 +5,7 @@\n     ir,\n     builtin,\n )\n-from . import libdevice\n+from . import math\n from .core import (\n     abs,\n     arange,\n@@ -141,7 +141,7 @@\n     \"int64\",\n     \"int8\",\n     \"ir\",\n-    \"libdevice\",\n+    \"math\",\n     \"load\",\n     \"log\",\n     \"max\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 10, "deletions": 1, "changes": 11, "file_content_changes": "@@ -1215,7 +1215,16 @@ def max_contiguous(input, values, _builder=None):\n \n @triton.jit\n def abs(x):\n-    return where(x >= 0, x, -x)\n+    x_dtype = x.dtype\n+    if x_dtype.is_floating():\n+        num_bits: constexpr = x.dtype.primitive_bitwidth\n+        int_dtype = dtype(f'int{num_bits}')\n+        mask = 2 ** (num_bits - 1) - 1\n+        ret = x.to(int_dtype, bitcast=True) & mask.to(int_dtype)\n+        ret = ret.to(x_dtype, bitcast=True)\n+    else:\n+        ret = where(x >= 0, x, -x)\n+    return ret\n \n \n @triton.jit"}, {"filename": "python/triton/language/math.py", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "python/triton/language/libdevice.py"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -1188,14 +1188,14 @@ def xor_sum(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n def umulhi(x: tl.tensor, y: tl.tensor, builder: ir.builder) -> tl.tensor:\n     x, y = binary_op_type_checking_impl(x, y, builder)\n     # FIXME(Keren): not portable, should be fixed\n-    from . import libdevice\n-    return libdevice.mulhi(x, y, _builder=builder)\n+    from . import math\n+    return math.mulhi(x, y, _builder=builder)\n \n \n def floor(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n     # FIXME(Keren): not portable, should be fixed\n-    from . import libdevice\n-    return libdevice.floor(x, _builder=builder)\n+    from . import math\n+    return math.floor(x, _builder=builder)\n \n \n def exp(x: tl.tensor, builder: ir.builder) -> tl.tensor:"}]