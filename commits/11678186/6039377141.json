[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "@@ -1533,6 +1533,9 @@ def get_reduced_dtype(dtype_str, op):\n def test_reduce1d(op, dtype_str, shape, num_ctas, device):\n     check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n \n+    if is_hip():\n+        pytest.skip(f\"test_reduce1d not supported on HIP\")\n+\n     # triton kernel\n     @triton.jit\n     def kernel(X, Z, BLOCK: tl.constexpr):\n@@ -1623,6 +1626,8 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n def test_reduce2d(op, dtype_str, shape, axis, num_ctas, device):\n     check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n \n+    if is_hip():\n+        pytest.skip(f\"test_reduce2d not supported on HIP\")\n     # triton kernel\n     @triton.jit\n     def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexpr):\n@@ -1888,6 +1893,8 @@ def test_reduce_layouts(M, N, src_layout, axis, device):\n @pytest.mark.parametrize(\"M\", [32, 64, 128, 256])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n def test_store_op(M, src_layout, device):\n+    if is_hip():\n+        pytest.skip(\"test_convert1d is not supported yet in HIP\")\n \n     ir = f\"\"\"\n     #src = {src_layout}\n@@ -2225,6 +2232,8 @@ def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, o\n             pytest.skip(f\"test_dot{(M, N, K)} not supported on HIP. Reduce Warp to work\")\n         if M == 16 or N == 16 or K == 16:\n             pytest.skip(f\"test_dot{(M, N, K)} segfaults on HIP\")\n+        if epilogue == \"softmax\":\n+            pytest.skip(f\"test_dot{epilogue} segfaults on HIP\")\n \n     torch.backends.cuda.matmul.allow_tf32 = allow_tf32\n \n@@ -3164,6 +3173,9 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n def test_inline_asm(num_ctas, device):\n     check_cuda_only(device)\n \n+    if is_hip():\n+        pytest.skip(\"test_inline_asm is not supported in HIP\")\n+\n     @triton.jit\n     def kernel(X, Y, Z, n: tl.constexpr, BLOCK: tl.constexpr):\n         x = tl.load(X + tl.arange(0, BLOCK))\n@@ -3190,6 +3202,9 @@ def kernel(X, Y, Z, n: tl.constexpr, BLOCK: tl.constexpr):\n def test_inline_asm_packed(num_ctas, device):\n     check_cuda_only(device)\n \n+    if is_hip():\n+        pytest.skip(\"test_inline_asm is not supported in HIP\")\n+\n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n         x = tl.load(X + tl.arange(0, BLOCK))"}, {"filename": "third_party/amd_hip_backend", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1 +1 @@\n-Subproject commit 7fca339745ad6052f22175f6c1897095f20e7c22\n+Subproject commit d0ad70d55df3ebe11cc80bbb364a91551e6b6248"}]