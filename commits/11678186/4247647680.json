[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 22, "deletions": 8, "changes": 30, "file_content_changes": "@@ -3,9 +3,10 @@ name: Integration Tests\n on:\n   workflow_dispatch:\n   pull_request:\n-    branches:\n-      - main\n-      - triton-mlir\n+    branches: [main]\n+  merge_group:\n+    branches: [main]\n+    types: [checks_requested]\n \n concurrency:\n   group: ${{ github.ref }}\n@@ -21,7 +22,7 @@ jobs:\n         id: set-matrix\n         run: |\n           if [ x\"${{ github.repository }}\" == x\"openai/triton\" ]; then\n-            echo '::set-output name=matrix::[[\"self-hosted\", \"A10\"], [\"self-hosted\", \"V100\"], \"macos-10.15\"]'\n+            echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"V100\"], \"macos-10.15\"]'\n           else\n             echo '::set-output name=matrix::[\"ubuntu-latest\", \"macos-10.15\"]'\n           fi\n@@ -43,29 +44,33 @@ jobs:\n         run: |\n           rm -rf ~/.triton/cache/\n \n+      - name: Update path\n+        run: |\n+          echo \"$HOME/.local/bin/\" >> $GITHUB_PATH\n+\n       - name: Check imports\n         if: ${{ matrix.runner != 'macos-10.15' }}\n         run: |\n-          pip install isort\n+          pip3 install isort\n           isort -c ./python || ( echo '::error title=Imports not sorted::Please run \\\"isort ./python\\\"' ; exit 1 )\n \n       - name: Check python style\n         if: ${{ matrix.runner != 'macos-10.15' }}\n         run: |\n-          pip install autopep8\n+          pip3 install autopep8\n           autopep8 -a -r -d --exit-code ./python || ( echo '::error title=Style issues::Please run \\\"autopep8 -a -r -i ./python\\\"' ; exit 1 )\n \n       - name: Check cpp style\n         if: ${{ matrix.runner != 'macos-10.15' }}\n         run: |\n-          pip install clang-format\n+          pip3 install clang-format\n           find . -regex '.*\\.\\(cpp\\|hpp\\|h\\|cc\\)' -not -path \"./python/triton/*\" -not -path \"./python/build/*\" -not -path \"./include/triton/external/*\" -print0 | xargs -0 -n1 clang-format -style=file --dry-run -Werror -i ||\n           (echo '::error title=Style issues:: Please run `find . -regex \".*\\.\\(cpp\\|hpp\\|h\\|cc\\)\" -not -path \"./python/triton/*\" -not -path \"./python/build/*\" -not -path \"./include/triton/external/*\" -print0 | xargs -0 -n1 clang-format -style=file -i`' ; exit 1)\n \n       - name: Flake8\n         if: ${{ matrix.runner != 'macos-10.15' }}\n         run: |\n-          pip install flake8\n+          pip3 install flake8\n           flake8 --config ./python/setup.cfg ./python || ( echo '::error::Flake8 failed; see logs for errors.' ; exit 1 )\n \n       - name: Install Triton\n@@ -94,3 +99,12 @@ jobs:\n           cd python/\n           cd \"build/$(ls build)\"\n           ctest\n+\n+      - name: Regression tests\n+        if: ${{ contains(matrix.runner, 'A100') }}\n+        run: |\n+          cd python/test/regression\n+          sudo nvidia-smi -i 0 -pm 1\n+          sudo nvidia-smi -i 0 --lock-gpu-clocks=1350,1350\n+          pytest -vs .\n+          sudo nvidia-smi -i 0 -rgc\n\\ No newline at end of file"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -25,6 +25,8 @@ def TTG_ConvertLayoutOp : TTG_Op<\"convert_layout\",\n \n   let results = (outs TT_Tensor:$result);\n \n+  let hasCanonicalizeMethod = 1;\n+\n   let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n }\n "}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.h", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "file_content_changes": "@@ -6,7 +6,9 @@\n namespace mlir {\n std::unique_ptr<Pass> createTritonGPUPipelinePass(int numStages = 2);\n \n-// TODO(Keren): prefetch pass not working yet\n+std::unique_ptr<Pass>\n+createTritonGPUAccelerateMatmulPass(int computeCapability = 80);\n+\n std::unique_ptr<Pass> createTritonGPUPrefetchPass();\n \n std::unique_ptr<Pass> createTritonGPUCanonicalizeLoopsPass();\n@@ -17,10 +19,12 @@ std::unique_ptr<Pass> createTritonGPUReorderInstructionsPass();\n \n std::unique_ptr<Pass> createTritonGPUDecomposeConversionsPass();\n \n-std::unique_ptr<Pass> createTritonGPUCombineOpsPass(int computeCapability = 80);\n+std::unique_ptr<Pass> createTritonGPURemoveLayoutConversionsPass();\n \n std::unique_ptr<Pass> createTritonGPUVerifier();\n \n+std::unique_ptr<Pass> createTritonGPUFuseTranspositionsPass();\n+\n std::unique_ptr<Pass> createTritonGPUUpdateMmaForVoltaPass();\n \n /// Generate the code for registering passes."}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.td", "status": "modified", "additions": 42, "deletions": 28, "changes": 70, "file_content_changes": "@@ -7,7 +7,8 @@ def TritonGPUPipeline : Pass<\"tritongpu-pipeline\", \"mlir::ModuleOp\"> {\n   let summary = \"pipeline\";\n \n   let description = [{\n-    Unroll loops to hide global memory -> shared memory latency.\n+    Replace `LoadOp` in loops by `InsertSliceAsyncOp` instructions that asynchronously construct the data\n+    needed at the next iteration\n   }];\n \n   let constructor = \"mlir::createTritonGPUPipelinePass()\";\n@@ -27,7 +28,8 @@ def TritonGPUPrefetch : Pass<\"tritongpu-prefetch\", \"mlir::ModuleOp\"> {\n   let summary = \"prefetch\";\n \n   let description = [{\n-    Prefetch operands (a and b) of tt.dot into shared memory to hide shared memory -> register latency.\n+    Decompose `DotOp` instructions in loops into several finer-grained `DotOp`\n+    that may have their operands constructed at the end of the previous iteration\n   }];\n \n   let constructor = \"mlir::createTritonGPUPrefetchPass()\";\n@@ -37,6 +39,41 @@ def TritonGPUPrefetch : Pass<\"tritongpu-prefetch\", \"mlir::ModuleOp\"> {\n                            \"mlir::arith::ArithDialect\"];\n }\n \n+def TritonGPUAccelerateMatmul : Pass<\"tritongpu-accelerate-matmul\", \"mlir::ModuleOp\"> {\n+  let summary = \"accelerate matmul\";\n+\n+  let description = [{\n+    Optimize the input/output layout of `dot` instruction to make them compatible hardware accelerators\n+    (e.g., Nvidia tensor cores)\n+  }];\n+\n+  let constructor = \"mlir::createTritonGPUAccelerateMatmulPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::TritonDialect\"];\n+\n+  let options = [\n+    Option<\"computeCapability\", \"compute-capability\",\n+           \"int32_t\", /*default*/\"80\",\n+           \"device compute capability\">\n+  ];\n+\n+}\n+\n+def TritonGPUFuseTranspositions : Pass<\"tritongpu-fuse-transposition\", \"mlir::ModuleOp\"> {\n+  let summary = \"fuse transpositions\";\n+\n+  let description = [{\n+    Re-arranged layouts of tensors used as matrix multiplication operands so as to promote the use of\n+    hardware-accelerated transpositions.\n+  }];\n+\n+  let constructor = \"mlir::createTritonGPUFuseTranspositionsPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::TritonDialect\"];\n+}\n+\n def TritonGPUCoalesce: Pass<\"tritongpu-coalesce\", \"mlir::ModuleOp\"> {\n   let summary = \"coalesce\";\n \n@@ -49,26 +86,16 @@ def TritonGPUCoalesce: Pass<\"tritongpu-coalesce\", \"mlir::ModuleOp\"> {\n   let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\"];\n }\n \n-def TritonGPUCombineOps : Pass<\"tritongpu-combine\", \"mlir::ModuleOp\"> {\n-  let summary = \"combine triton gpu ops\";\n+def TritonGPURemoveLayoutConversions : Pass<\"tritongpu-remove-layout-conversions\", \"mlir::ModuleOp\"> {\n+  let summary = \"remove superfluous layout conversions\";\n \n   let description = [{\n-    convert_layout(convert_layout(%src, #LAYOUT_0), #LAYOUT_1) =>\n-      convert_layout(%src, #LAYOUT_1)\n-\n-    convert_layout(%src, #LAYOUT) => %src if %src.layout() == #LAYOUT\n   }];\n \n-  let constructor = \"mlir::createTritonGPUCombineOpsPass()\";\n+  let constructor = \"mlir::createTritonGPURemoveLayoutConversionsPass()\";\n \n   let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n                            \"mlir::triton::TritonDialect\"];\n-\n-  let options = [\n-    Option<\"computeCapability\", \"compute-capability\",\n-           \"int32_t\", /*default*/\"80\",\n-           \"device compute capability\">\n-  ];\n }\n \n def TritonGPUReorderInstructions: Pass<\"tritongpu-reorder-instructions\", \"mlir::ModuleOp\"> {\n@@ -95,19 +122,6 @@ def TritonGPUDecomposeConversions: Pass<\"tritongpu-decompose-conversions\", \"mlir\n                            \"mlir::triton::TritonDialect\"];\n }\n \n-def TritonGPUCanonicalizeLoops: Pass<\"tritongpu-canonicalize-loops\", \"mlir::ModuleOp\"> {\n-  let summary = \"canonicalize scf.ForOp ops\";\n-\n-  let description = [{\n-    This implements some optimizations that are missing in the standard scf.ForOp\n-    canonicalizer.\n-  }];\n-\n-  let constructor = \"mlir::createTritonGPUCanonicalizeLoopsPass()\";\n-\n-  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\"];\n-}\n-\n def UpdateMmaForVolta : Pass<\"tritongpu-update-mma-for-volta\", \"mlir::ModuleOp\"> {\n   let summary = \"Update mma encodings for Volta\";\n "}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 135, "deletions": 0, "changes": 135, "file_content_changes": "@@ -790,6 +790,141 @@ struct TritonGPUInferLayoutInterface\n   }\n };\n \n+//===----------------------------------------------------------------------===//\n+// Canonicalizer\n+//===----------------------------------------------------------------------===//\n+\n+LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n+                                            PatternRewriter &rewriter) {\n+  // we don't handle conversions to DotOperandEncodingAttr\n+  // this is a heuristics to accommodate fused attention\n+  auto srcType = op.getOperand().getType().cast<RankedTensorType>();\n+  auto dstType = op.getType().cast<RankedTensorType>();\n+  if (dstType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>() &&\n+      srcType.getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n+    return mlir::failure();\n+  // convert to the same layout -- we can delete\n+  if (op->getResultTypes() == op->getOperandTypes()) {\n+    rewriter.replaceOp(op, op->getOperands());\n+    return mlir::success();\n+  }\n+  Operation *arg = op->getOperand(0).getDefiningOp();\n+  // block argument\n+  if (!arg)\n+    return mlir::failure();\n+  // cvt(view) -> view\n+  if (auto view = dyn_cast<triton::ViewOp>(arg)) {\n+    rewriter.replaceOpWithNewOp<triton::ViewOp>(op, op->getResult(0).getType(),\n+                                                view.getResult());\n+    return mlir::success();\n+  }\n+  // cvt(alloc_tensor(x), type2) -> alloc_tensor(x, type2)\n+  auto alloc_tensor = dyn_cast<triton::gpu::AllocTensorOp>(arg);\n+  if (alloc_tensor) {\n+    if (!isSharedEncoding(op->getResult(0))) {\n+      return mlir::failure();\n+    }\n+    rewriter.replaceOpWithNewOp<triton::gpu::AllocTensorOp>(\n+        op, op->getResult(0).getType());\n+    return mlir::success();\n+  }\n+  // cvt(insert_slice(x), type2) -> insert_slice(cvt(x, type2))\n+  auto insert_slice = dyn_cast<triton::gpu::InsertSliceAsyncOp>(arg);\n+  if (insert_slice) {\n+    if (!isSharedEncoding(op->getResult(0))) {\n+      return mlir::failure();\n+    }\n+    auto newType = op->getResult(0).getType().cast<RankedTensorType>();\n+    // Ensure that the new insert_slice op is placed in the same place as the\n+    // old insert_slice op. Otherwise, the new insert_slice op may be placed\n+    // after the async_wait op, which is not allowed.\n+    OpBuilder::InsertionGuard guard(rewriter);\n+    rewriter.setInsertionPoint(insert_slice);\n+    auto newArg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+        op->getLoc(), newType, insert_slice.getDst());\n+    rewriter.replaceOpWithNewOp<triton::gpu::InsertSliceAsyncOp>(\n+        op, newType, insert_slice.getSrc(), newArg.getResult(),\n+        insert_slice.getIndex(), insert_slice.getMask(),\n+        insert_slice.getOther(), insert_slice.getCache(),\n+        insert_slice.getEvict(), insert_slice.getIsVolatile(),\n+        insert_slice.getAxis());\n+    return mlir::success();\n+  }\n+  // cvt(extract_slice(x), type2) -> extract_slice(cvt(x, type2))\n+  auto extract_slice = dyn_cast<tensor::ExtractSliceOp>(arg);\n+  if (extract_slice) {\n+    if (!isSharedEncoding(op->getResult(0))) {\n+      return mlir::failure();\n+    }\n+    auto origType =\n+        extract_slice.getSource().getType().cast<RankedTensorType>();\n+    auto newType = RankedTensorType::get(\n+        origType.getShape(), origType.getElementType(),\n+        op->getResult(0).getType().cast<RankedTensorType>().getEncoding());\n+    auto origResType = op->getResult(0).getType().cast<RankedTensorType>();\n+    auto resType = RankedTensorType::get(\n+        origResType.getShape(), origResType.getElementType(),\n+        extract_slice.getType().cast<RankedTensorType>().getEncoding());\n+    // Ensure that the new extract_slice op is placed in the same place as the\n+    // old extract_slice op. Otherwise, the new extract_slice op may be placed\n+    // after the async_wait op, which is not allowed.\n+    OpBuilder::InsertionGuard guard(rewriter);\n+    rewriter.setInsertionPoint(extract_slice);\n+    auto newArg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+        op->getLoc(), newType, extract_slice.getSource());\n+    rewriter.replaceOpWithNewOp<tensor::ExtractSliceOp>(\n+        op, resType, newArg.getResult(), extract_slice.offsets(),\n+        extract_slice.sizes(), extract_slice.strides(),\n+        extract_slice.static_offsets(), extract_slice.static_sizes(),\n+        extract_slice.static_strides());\n+    return mlir::success();\n+  }\n+\n+  // cvt(cvt(x, type1), type2) -> cvt(x, type2)\n+  if (llvm::isa<triton::gpu::ConvertLayoutOp>(arg)) {\n+    if (arg->getOperand(0).getDefiningOp() &&\n+        !isSharedEncoding(arg->getOperand(0)) &&\n+        isSharedEncoding(op.getOperand()) &&\n+        !isSharedEncoding(op.getResult())) {\n+      return mlir::failure();\n+    }\n+    if (isSharedEncoding(op.getOperand()) && isSharedEncoding(op.getResult())) {\n+      return mlir::failure();\n+    }\n+    auto srcType = op.getOperand().getType().cast<RankedTensorType>();\n+    auto srcShared =\n+        srcType.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n+    if (srcShared && srcShared.getVec() > 1)\n+      return mlir::failure();\n+    rewriter.replaceOpWithNewOp<triton::gpu::ConvertLayoutOp>(\n+        op, op->getResultTypes().front(), arg->getOperand(0));\n+    return mlir::success();\n+  }\n+  // cvt(type1, splat(type2, x)) -> splat(type1, x)\n+  if (auto splat = llvm::dyn_cast<triton::SplatOp>(arg)) {\n+    rewriter.replaceOpWithNewOp<triton::SplatOp>(op, op->getResultTypes(),\n+                                                 splat.getSrc());\n+    return mlir::success();\n+  }\n+  // cvt(type1, make_range(type2, x)) -> make_range(type1, x)\n+  if (auto range = llvm::dyn_cast<triton::MakeRangeOp>(arg)) {\n+    rewriter.replaceOpWithNewOp<triton::MakeRangeOp>(\n+        op, op->getResultTypes(), range.getStart(), range.getEnd());\n+    return mlir::success();\n+  }\n+  // cvt(type, constant) -> constant\n+  if (auto cst = llvm::dyn_cast<arith::ConstantOp>(arg))\n+    if (auto ret = cst.getValue().dyn_cast<SplatElementsAttr>()) {\n+      auto newRet = SplatElementsAttr::get(op->getResultTypes().front(),\n+                                           ret.getSplatValue<Attribute>());\n+      rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, newRet);\n+      return mlir::success();\n+    }\n+  return mlir::failure();\n+}\n+\n+//===----------------------------------------------------------------------===//\n+\n void TritonGPUDialect::initialize() {\n   addAttributes<\n #define GET_ATTRDEF_LIST"}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "added", "additions": 212, "deletions": 0, "changes": 212, "file_content_changes": "@@ -0,0 +1,212 @@\n+#include \"mlir/Support/LogicalResult.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+#include <memory>\n+\n+using namespace mlir;\n+namespace {\n+using triton::DotOp;\n+using triton::gpu::ConvertLayoutOp;\n+using triton::gpu::DotOperandEncodingAttr;\n+using triton::gpu::MmaEncodingAttr;\n+using triton::gpu::SliceEncodingAttr;\n+\n+int computeCapabilityToMMAVersion(int computeCapability) {\n+  if (computeCapability < 70) {\n+    return 0;\n+  } else if (computeCapability < 80) {\n+    return 1;\n+  } else if (computeCapability < 90) {\n+    return 2;\n+  } else if (computeCapability < 100) {\n+    // FIXME: temporarily add this to pass unis tests\n+    return 2;\n+  } else {\n+    assert(false && \"computeCapability > 100 not supported\");\n+    return 3;\n+  }\n+}\n+\n+SmallVector<int64_t, 2> mmaVersionToShapePerWarp(int version) {\n+  if (version == 1)\n+    return {16, 16};\n+  else if (version == 2)\n+    return {16, 8};\n+  else {\n+    assert(false && \"version not supported\");\n+    return {0, 0};\n+  }\n+}\n+\n+SmallVector<unsigned, 2> warpsPerTileV1(const ArrayRef<int64_t> shape,\n+                                        int numWarps) {\n+  // Set a default value that ensures product of wpt equals numWarps\n+  return {static_cast<unsigned>(numWarps), 1};\n+}\n+\n+SmallVector<unsigned, 2> warpsPerTileV2(triton::DotOp dotOp,\n+                                        const ArrayRef<int64_t> shape,\n+                                        int numWarps) {\n+  SetVector<Operation *> slices;\n+  mlir::getForwardSlice(dotOp.getResult(), &slices);\n+  if (llvm::find_if(slices, [](Operation *op) {\n+        return isa<triton::DotOp>(op);\n+      }) != slices.end())\n+    return {(unsigned)numWarps, 1};\n+\n+  SmallVector<unsigned, 2> ret = {1, 1};\n+  SmallVector<int64_t, 2> shapePerWarp = {16, 8};\n+  bool changed = false;\n+  // TODO (@daadaada): double-check.\n+  // original logic in\n+  // https://github.com/openai/triton/blob/master/lib/codegen/analysis/layout.cc#L252\n+  // seems buggy for shape = [32, 16] ?\n+  do {\n+    changed = false;\n+    if (ret[0] * ret[1] >= numWarps)\n+      break;\n+    if (shape[0] / shapePerWarp[0] / ret[0] >=\n+        shape[1] / (shapePerWarp[1] * 2) / ret[1]) {\n+      if (ret[0] < shape[0] / shapePerWarp[0]) {\n+        ret[0] *= 2;\n+      } else\n+        ret[1] *= 2;\n+    } else {\n+      ret[1] *= 2;\n+    }\n+  } while (true);\n+  return ret;\n+}\n+\n+class BlockedToMMA : public mlir::RewritePattern {\n+  int computeCapability;\n+  mutable int mmaV1Counter{}; // used to generate ID for MMAv1 encoding\n+\n+public:\n+  BlockedToMMA(mlir::MLIRContext *context, int computeCapability)\n+      : mlir::RewritePattern(triton::DotOp::getOperationName(), 2, context),\n+        computeCapability(computeCapability) {}\n+\n+  static SmallVector<unsigned, 2> getWarpsPerTile(triton::DotOp dotOp,\n+                                                  const ArrayRef<int64_t> shape,\n+                                                  int version, int numWarps) {\n+    switch (version) {\n+    case 1:\n+      return warpsPerTileV1(shape, numWarps);\n+    case 2:\n+      return warpsPerTileV2(dotOp, shape, numWarps);\n+    default:\n+      llvm_unreachable(\"unsupported MMA version\");\n+    }\n+  }\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto dotOp = cast<triton::DotOp>(op);\n+    // TODO: Check data-types and SM compatibility\n+    auto oldRetType = dotOp.getResult().getType().cast<RankedTensorType>();\n+    if (!oldRetType.getEncoding() ||\n+        oldRetType.getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n+      return failure();\n+\n+    // for FMA, should retain the blocked layout.\n+    int versionMajor = computeCapabilityToMMAVersion(computeCapability);\n+    if (!supportMMA(dotOp, versionMajor))\n+      return failure();\n+\n+    // get MMA encoding for the given number of warps\n+    auto retShape = oldRetType.getShape();\n+    auto mod = op->getParentOfType<mlir::ModuleOp>();\n+    int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+\n+    auto warpsPerTile =\n+        getWarpsPerTile(dotOp, retShape, versionMajor, numWarps);\n+    triton::gpu::MmaEncodingAttr mmaEnc;\n+    if (versionMajor == 1) {\n+      mmaEnc = triton::gpu::MmaEncodingAttr::get(\n+          oldRetType.getContext(), versionMajor, numWarps, mmaV1Counter++);\n+    } else if (versionMajor == 2) {\n+      mmaEnc = triton::gpu::MmaEncodingAttr::get(\n+          oldRetType.getContext(), versionMajor, 0 /*versionMinor*/,\n+          warpsPerTile);\n+    } else {\n+      llvm_unreachable(\"Mma layout only supports versionMajor in {1, 2}\");\n+    }\n+    auto newRetType =\n+        RankedTensorType::get(retShape, oldRetType.getElementType(), mmaEnc);\n+\n+    // convert accumulator\n+    auto oldAcc = dotOp.getOperand(2);\n+    auto newAcc = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+        oldAcc.getLoc(), newRetType, oldAcc);\n+    Value a = dotOp.getA();\n+    Value b = dotOp.getB();\n+    auto oldAType = a.getType().cast<RankedTensorType>();\n+    auto oldBType = b.getType().cast<RankedTensorType>();\n+    auto oldAOrder = oldAType.getEncoding()\n+                         .cast<triton::gpu::DotOperandEncodingAttr>()\n+                         .getParent()\n+                         .cast<triton::gpu::BlockedEncodingAttr>()\n+                         .getOrder();\n+    auto oldBOrder = oldBType.getEncoding()\n+                         .cast<triton::gpu::DotOperandEncodingAttr>()\n+                         .getParent()\n+                         .cast<triton::gpu::BlockedEncodingAttr>()\n+                         .getOrder();\n+    Attribute isMMAv1RowA;\n+    Attribute isMMAv1RowB;\n+    if (versionMajor == 1) {\n+      isMMAv1RowA = BoolAttr::get(getContext(), oldAOrder[0] == 1);\n+      isMMAv1RowB = BoolAttr::get(getContext(), oldBOrder[0] == 1);\n+    }\n+\n+    auto newAType = RankedTensorType::get(\n+        oldAType.getShape(), oldAType.getElementType(),\n+        triton::gpu::DotOperandEncodingAttr::get(\n+            oldAType.getContext(), 0, newRetType.getEncoding(), isMMAv1RowA));\n+    auto newBType = RankedTensorType::get(\n+        oldBType.getShape(), oldBType.getElementType(),\n+        triton::gpu::DotOperandEncodingAttr::get(\n+            oldBType.getContext(), 1, newRetType.getEncoding(), isMMAv1RowB));\n+\n+    a = rewriter.create<triton::gpu::ConvertLayoutOp>(a.getLoc(), newAType, a);\n+    b = rewriter.create<triton::gpu::ConvertLayoutOp>(b.getLoc(), newBType, b);\n+    auto newDot = rewriter.create<triton::DotOp>(\n+        dotOp.getLoc(), newRetType, a, b, newAcc, dotOp.getAllowTF32());\n+\n+    rewriter.replaceOpWithNewOp<triton::gpu::ConvertLayoutOp>(\n+        op, oldRetType, newDot.getResult());\n+    return success();\n+  }\n+};\n+} // namespace\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n+\n+class TritonGPUAccelerateMatmulPass\n+    : public TritonGPUAccelerateMatmulBase<TritonGPUAccelerateMatmulPass> {\n+public:\n+  TritonGPUAccelerateMatmulPass() = default;\n+  TritonGPUAccelerateMatmulPass(int computeCapability) {\n+    this->computeCapability = computeCapability;\n+  }\n+  void runOnOperation() override {\n+    MLIRContext *context = &getContext();\n+    ModuleOp m = getOperation();\n+\n+    mlir::RewritePatternSet patterns(context);\n+    patterns.add<::BlockedToMMA>(context, computeCapability);\n+    if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n+      signalPassFailure();\n+    }\n+  }\n+};\n+\n+std::unique_ptr<Pass>\n+mlir::createTritonGPUAccelerateMatmulPass(int computeCapability) {\n+  return std::make_unique<TritonGPUAccelerateMatmulPass>(computeCapability);\n+}"}, {"filename": "lib/Dialect/TritonGPU/Transforms/CMakeLists.txt", "status": "modified", "additions": 4, "deletions": 8, "changes": 12, "file_content_changes": "@@ -1,22 +1,18 @@\n-set(LLVM_TARGET_DEFINITIONS Combine.td)\n-mlir_tablegen(TritonGPUCombine.inc -gen-rewriters)\n-add_public_tablegen_target(TritonGPUCombineIncGen)\n-\n add_mlir_dialect_library(TritonGPUTransforms\n+  AccelerateMatmul.cpp\n   Coalesce.cpp\n-  CanonicalizeLoops.cpp\n-  Combine.cpp\n+  DecomposeConversions.cpp\n+  FuseTranspositions.cpp\n   Pipeline.cpp\n   Prefetch.cpp\n+  RemoveLayoutConversions.cpp\n   ReorderInstructions.cpp\n-  DecomposeConversions.cpp\n   TritonGPUConversion.cpp\n   UpdateMmaForVolta.cpp\n   Utility.cpp\n \n   DEPENDS\n   TritonGPUTransformsIncGen\n-  TritonGPUCombineIncGen\n \n   LINK_LIBS PUBLIC\n   TritonIR"}, {"filename": "lib/Dialect/TritonGPU/Transforms/CanonicalizeLoops.cpp", "status": "removed", "additions": 0, "deletions": 55, "changes": 55, "file_content_changes": "@@ -1,55 +0,0 @@\n-#include \"mlir/Analysis/SliceAnalysis.h\"\n-#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n-#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n-\n-using namespace mlir;\n-using namespace mlir::triton;\n-\n-#define GEN_PASS_CLASSES\n-#include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n-\n-namespace {\n-\n-struct CanonicalizePass\n-    : public TritonGPUCanonicalizeLoopsBase<CanonicalizePass> {\n-  CanonicalizePass() = default;\n-\n-  void runOnOperation() override {\n-\n-    // Canonicalize pass may have created dead code that\n-    // standard scf.for canonicalization cannot handle\n-    // as of LLVM 14. For example, the iteration arguments\n-    // for the pointer of the synchronous loads that are\n-    // discarded.\n-    // The following piece of code is a workaround to\n-    // very crudely remove dead code, by making an iteration\n-    // argument yield itself if it is not used to create\n-    // side effects anywhere.\n-    getOperation()->walk([&](scf::ForOp forOp) -> void {\n-      for (size_t i = 0; i < forOp.getNumResults(); ++i) {\n-        // condition 1: no other iter arguments depend on it\n-        SetVector<Operation *> fwdSlice;\n-        mlir::getForwardSlice(forOp.getRegionIterArgs()[i], &fwdSlice);\n-        Operation *yieldOp = forOp.getBody()->getTerminator();\n-        bool noOtherDependency = std::all_of(\n-            yieldOp->operand_begin(), yieldOp->operand_end(), [&](Value arg) {\n-              return arg == yieldOp->getOperand(i) ||\n-                     !fwdSlice.contains(arg.getDefiningOp());\n-            });\n-        // condition 2: final value is not used after the loop\n-        auto retVal = forOp.getResult(i);\n-        bool noUserAfterLoop = retVal.getUsers().empty();\n-        // yielding the region iter arg will cause loop canonicalization\n-        // to clean up the dead code\n-        if (noOtherDependency && noUserAfterLoop) {\n-          yieldOp->setOperand(i, forOp.getRegionIterArgs()[i]);\n-        }\n-      }\n-    });\n-  }\n-};\n-} // anonymous namespace\n-\n-std::unique_ptr<Pass> mlir::createTritonGPUCanonicalizeLoopsPass() {\n-  return std::make_unique<CanonicalizePass>();\n-}\n\\ No newline at end of file"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.td", "status": "removed", "additions": 0, "deletions": 8, "changes": 8, "file_content_changes": "@@ -1,8 +0,0 @@\n-#ifndef TRITONGPU_PATTERNS\n-#define TRITONGPU_PATTERNS\n-\n-include \"triton/Dialect/TritonGPU/IR/TritonGPUOps.td\"\n-include \"triton/Dialect/Triton/IR/TritonOps.td\"\n-include \"mlir/IR/PatternBase.td\"\n-\n-#endif"}, {"filename": "lib/Dialect/TritonGPU/Transforms/FuseTranspositions.cpp", "status": "added", "additions": 153, "deletions": 0, "changes": 153, "file_content_changes": "@@ -0,0 +1,153 @@\n+#include \"Utility.h\"\n+#include \"mlir/Pass/PassManager.h\"\n+#include \"mlir/Support/LogicalResult.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"mlir/Transforms/Passes.h\"\n+#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+#include <memory>\n+\n+using namespace mlir;\n+namespace {\n+using triton::DotOp;\n+using triton::gpu::ConvertLayoutOp;\n+using triton::gpu::DotOperandEncodingAttr;\n+using triton::gpu::MmaEncodingAttr;\n+using triton::gpu::SliceEncodingAttr;\n+\n+class OptimizeConvertToDotOperand : public mlir::RewritePattern {\n+public:\n+  explicit OptimizeConvertToDotOperand(mlir::MLIRContext *context)\n+      : RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(), 1,\n+                       context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto cvt = cast<triton::gpu::ConvertLayoutOp>(op);\n+    auto srcType = cvt.getOperand().getType().cast<RankedTensorType>();\n+    auto dstType = cvt.getResult().getType().cast<RankedTensorType>();\n+    // order\n+    ArrayRef<unsigned> order;\n+    if (auto srcBlockedLayout =\n+            srcType.getEncoding().dyn_cast<triton::gpu::BlockedEncodingAttr>())\n+      order = srcBlockedLayout.getOrder();\n+    else if (auto srcSharedLayout =\n+                 srcType.getEncoding()\n+                     .dyn_cast<triton::gpu::SharedEncodingAttr>())\n+      order = srcSharedLayout.getOrder();\n+    else\n+      return failure();\n+    // dot operand output\n+    auto dstDotOperandLayout =\n+        dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n+    if (!dstDotOperandLayout)\n+      return failure();\n+    if (!dstDotOperandLayout.getIsMMAv1Row())\n+      return failure();\n+    bool isMMAv1Row =\n+        dstDotOperandLayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    if ((order[0] == 1 && isMMAv1Row) || (order[0] == 0 && !isMMAv1Row))\n+      return failure();\n+\n+    auto newIsRow = BoolAttr::get(op->getContext(), !isMMAv1Row);\n+    auto newDstEncoding = triton::gpu::DotOperandEncodingAttr::get(\n+        op->getContext(), dstDotOperandLayout.getOpIdx(),\n+        dstDotOperandLayout.getParent(), newIsRow);\n+    auto newDstType = RankedTensorType::get(\n+        dstType.getShape(), dstType.getElementType(), newDstEncoding);\n+    auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+        op->getLoc(), newDstType, cvt.getOperand());\n+    rewriter.replaceOp(op, newCvt.getResult());\n+    return success();\n+  }\n+};\n+\n+// convert(trans(convert(arg)))\n+// x = convert_layout arg: #distributed -> #shared_x\n+// y = trans x: #shared_x -> #shared_y\n+// z = convert_layout y: #shared_y -> #dot_operand\n+class ConvertTransConvert : public mlir::RewritePattern {\n+\n+public:\n+  ConvertTransConvert(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n+                             1, context) {}\n+\n+  LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto dstOp = cast<triton::gpu::ConvertLayoutOp>(op);\n+    auto tmpOp =\n+        dyn_cast_or_null<triton::TransOp>(dstOp.getSrc().getDefiningOp());\n+    if (!tmpOp)\n+      return mlir::failure();\n+    auto srcOp = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n+        tmpOp.getSrc().getDefiningOp());\n+    if (!srcOp)\n+      return mlir::failure();\n+    auto arg = srcOp.getSrc();\n+    auto X = tmpOp.getSrc();\n+    // types\n+    auto argType = arg.getType().cast<RankedTensorType>();\n+    auto XType = X.getType().cast<RankedTensorType>();\n+    auto ZType = dstOp.getResult().getType().cast<RankedTensorType>();\n+    // encodings\n+    auto argEncoding = argType.getEncoding();\n+    auto XEncoding =\n+        XType.getEncoding().cast<triton::gpu::SharedEncodingAttr>();\n+    auto ZEncoding =\n+        ZType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n+    if (!ZEncoding)\n+      return mlir::failure();\n+    // new X encoding\n+    auto newXOrder = triton::gpu::getOrder(argEncoding);\n+    auto newXEncoding = triton::gpu::SharedEncodingAttr::get(\n+        getContext(), ZEncoding, XType.getShape(), newXOrder,\n+        XType.getElementType());\n+    auto newXType = RankedTensorType::get(XType.getShape(),\n+                                          XType.getElementType(), newXEncoding);\n+    if (XEncoding == newXEncoding)\n+      return mlir::failure();\n+\n+    auto newX = rewriter.create<triton::gpu::ConvertLayoutOp>(srcOp.getLoc(),\n+                                                              newXType, arg);\n+    auto newY = rewriter.create<triton::TransOp>(tmpOp.getLoc(), newX);\n+    rewriter.replaceOpWithNewOp<triton::gpu::ConvertLayoutOp>(dstOp, ZType,\n+                                                              newY);\n+    return mlir::success();\n+  }\n+};\n+\n+} // namespace\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n+\n+class TritonGPUFuseTranspositionsPass\n+    : public TritonGPUFuseTranspositionsBase<TritonGPUFuseTranspositionsPass> {\n+public:\n+  TritonGPUFuseTranspositionsPass() = default;\n+\n+  void runOnOperation() override {\n+    MLIRContext *context = &getContext();\n+    ModuleOp m = getOperation();\n+\n+    mlir::PassManager pm(m.getContext());\n+    pm.addPass(mlir::createCanonicalizerPass());\n+    auto ret = pm.run(m);\n+\n+    mlir::RewritePatternSet patterns(context);\n+    patterns.add<OptimizeConvertToDotOperand>(context);\n+    patterns.add<ConvertTransConvert>(context);\n+    if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n+      signalPassFailure();\n+    if (fixupLoops(m).failed())\n+      signalPassFailure();\n+  }\n+};\n+\n+std::unique_ptr<Pass> mlir::createTritonGPUFuseTranspositionsPass() {\n+  return std::make_unique<TritonGPUFuseTranspositionsPass>();\n+}"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "renamed", "additions": 11, "deletions": 528, "changes": 539, "file_content_changes": "@@ -22,7 +22,6 @@\n \n using namespace mlir;\n namespace {\n-#include \"TritonGPUCombine.inc\"\n using triton::DotOp;\n using triton::gpu::ConvertLayoutOp;\n using triton::gpu::DotOperandEncodingAttr;\n@@ -139,132 +138,7 @@ class SimplifyConversion : public mlir::RewritePattern {\n     if (!llvm::isa<triton::gpu::ConvertLayoutOp>(op))\n       return mlir::failure();\n     auto convert = llvm::cast<triton::gpu::ConvertLayoutOp>(op);\n-    // we don't handle conversions to DotOperandEncodingAttr\n-    // this is a heuristics to accommodate fused attention\n-    auto srcType = convert.getOperand().getType().cast<RankedTensorType>();\n-    auto dstType = convert.getType().cast<RankedTensorType>();\n-    if (dstType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>() &&\n-        srcType.getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n-      return mlir::failure();\n-    // convert to the same layout -- we can delete\n-    if (op->getResultTypes() == op->getOperandTypes()) {\n-      rewriter.replaceOp(op, op->getOperands());\n-      return mlir::success();\n-    }\n-    Operation *arg = op->getOperand(0).getDefiningOp();\n-    // block argument\n-    if (!arg)\n-      return mlir::failure();\n-    // cvt(view) -> view\n-    if (auto view = dyn_cast<triton::ViewOp>(arg)) {\n-      rewriter.replaceOpWithNewOp<triton::ViewOp>(\n-          op, op->getResult(0).getType(), view.getResult());\n-      return mlir::success();\n-    }\n-    // cvt(alloc_tensor(x), type2) -> alloc_tensor(x, type2)\n-    auto alloc_tensor = dyn_cast<triton::gpu::AllocTensorOp>(arg);\n-    if (alloc_tensor) {\n-      if (!isSharedEncoding(op->getResult(0))) {\n-        return mlir::failure();\n-      }\n-      rewriter.replaceOpWithNewOp<triton::gpu::AllocTensorOp>(\n-          op, op->getResult(0).getType());\n-      return mlir::success();\n-    }\n-    // cvt(insert_slice(x), type2) -> insert_slice(cvt(x, type2))\n-    auto insert_slice = dyn_cast<triton::gpu::InsertSliceAsyncOp>(arg);\n-    if (insert_slice) {\n-      if (!isSharedEncoding(op->getResult(0))) {\n-        return mlir::failure();\n-      }\n-      auto newType = op->getResult(0).getType().cast<RankedTensorType>();\n-      // Ensure that the new insert_slice op is placed in the same place as the\n-      // old insert_slice op. Otherwise, the new insert_slice op may be placed\n-      // after the async_wait op, which is not allowed.\n-      OpBuilder::InsertionGuard guard(rewriter);\n-      rewriter.setInsertionPoint(insert_slice);\n-      auto newArg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-          op->getLoc(), newType, insert_slice.getDst());\n-      rewriter.replaceOpWithNewOp<triton::gpu::InsertSliceAsyncOp>(\n-          op, newType, insert_slice.getSrc(), newArg.getResult(),\n-          insert_slice.getIndex(), insert_slice.getMask(),\n-          insert_slice.getOther(), insert_slice.getCache(),\n-          insert_slice.getEvict(), insert_slice.getIsVolatile(),\n-          insert_slice.getAxis());\n-      return mlir::success();\n-    }\n-    // cvt(extract_slice(x), type2) -> extract_slice(cvt(x, type2))\n-    auto extract_slice = dyn_cast<tensor::ExtractSliceOp>(arg);\n-    if (extract_slice) {\n-      if (!isSharedEncoding(op->getResult(0))) {\n-        return mlir::failure();\n-      }\n-      auto origType =\n-          extract_slice.getSource().getType().cast<RankedTensorType>();\n-      auto newType = RankedTensorType::get(\n-          origType.getShape(), origType.getElementType(),\n-          op->getResult(0).getType().cast<RankedTensorType>().getEncoding());\n-      auto origResType = op->getResult(0).getType().cast<RankedTensorType>();\n-      auto resType = RankedTensorType::get(\n-          origResType.getShape(), origResType.getElementType(),\n-          extract_slice.getType().cast<RankedTensorType>().getEncoding());\n-      // Ensure that the new extract_slice op is placed in the same place as the\n-      // old extract_slice op. Otherwise, the new extract_slice op may be placed\n-      // after the async_wait op, which is not allowed.\n-      OpBuilder::InsertionGuard guard(rewriter);\n-      rewriter.setInsertionPoint(extract_slice);\n-      auto newArg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-          op->getLoc(), newType, extract_slice.getSource());\n-      rewriter.replaceOpWithNewOp<tensor::ExtractSliceOp>(\n-          op, resType, newArg.getResult(), extract_slice.offsets(),\n-          extract_slice.sizes(), extract_slice.strides(),\n-          extract_slice.static_offsets(), extract_slice.static_sizes(),\n-          extract_slice.static_strides());\n-      return mlir::success();\n-    }\n-\n-    // cvt(cvt(x, type1), type2) -> cvt(x, type2)\n-    if (llvm::isa<triton::gpu::ConvertLayoutOp>(arg)) {\n-      if (arg->getOperand(0).getDefiningOp() &&\n-          !isSharedEncoding(arg->getOperand(0)) &&\n-          isSharedEncoding(convert.getOperand()) &&\n-          !isSharedEncoding(convert.getResult())) {\n-        return mlir::failure();\n-      }\n-      if (isSharedEncoding(convert.getOperand()) &&\n-          isSharedEncoding(convert.getResult())) {\n-        return mlir::failure();\n-      }\n-      auto srcType = convert.getOperand().getType().cast<RankedTensorType>();\n-      auto srcShared =\n-          srcType.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n-      if (srcShared && srcShared.getVec() > 1)\n-        return mlir::failure();\n-      rewriter.replaceOpWithNewOp<triton::gpu::ConvertLayoutOp>(\n-          op, op->getResultTypes().front(), arg->getOperand(0));\n-      return mlir::success();\n-    }\n-    // cvt(type1, splat(type2, x)) -> splat(type1, x)\n-    if (auto splat = llvm::dyn_cast<triton::SplatOp>(arg)) {\n-      rewriter.replaceOpWithNewOp<triton::SplatOp>(op, op->getResultTypes(),\n-                                                   splat.getSrc());\n-      return mlir::success();\n-    }\n-    // cvt(type1, make_range(type2, x)) -> make_range(type1, x)\n-    if (auto range = llvm::dyn_cast<triton::MakeRangeOp>(arg)) {\n-      rewriter.replaceOpWithNewOp<triton::MakeRangeOp>(\n-          op, op->getResultTypes(), range.getStart(), range.getEnd());\n-      return mlir::success();\n-    }\n-    // cvt(type, constant) -> constant\n-    if (auto cst = llvm::dyn_cast<arith::ConstantOp>(arg))\n-      if (auto ret = cst.getValue().dyn_cast<SplatElementsAttr>()) {\n-        auto newRet = SplatElementsAttr::get(op->getResultTypes().front(),\n-                                             ret.getSplatValue<Attribute>());\n-        rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, newRet);\n-        return mlir::success();\n-      }\n-    return mlir::failure();\n+    return ConvertLayoutOp::canonicalize(convert, rewriter);\n   }\n };\n \n@@ -568,9 +442,9 @@ class MoveConvertOutOfIf : public mlir::RewritePattern {\n };\n \n //\n-class FoldConvertAndReduce : public mlir::RewritePattern {\n+class RematerializeForward : public mlir::RewritePattern {\n public:\n-  explicit FoldConvertAndReduce(mlir::MLIRContext *context)\n+  explicit RematerializeForward(mlir::MLIRContext *context)\n       : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n                              1, context) {}\n \n@@ -837,390 +711,6 @@ class MoveConvertOutOfLoop : public mlir::RewritePattern {\n   }\n };\n \n-// -----------------------------------------------------------------------------\n-//\n-// -----------------------------------------------------------------------------\n-\n-class RematerializeForward : public mlir::RewritePattern {\n-public:\n-  explicit RematerializeForward(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             2, context) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *_cvtOp,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto cvt = cast<triton::gpu::ConvertLayoutOp>(_cvtOp);\n-    auto forOp = dyn_cast<scf::ForOp>(cvt->getParentOp());\n-    if (!forOp)\n-      return mlir::failure();\n-    auto isInLoop = [&](Operation *op) { return op->getParentOp() == forOp; };\n-\n-    SetVector<Operation *> cvtSlices;\n-    auto filter = [&](Operation *op) {\n-      return isInLoop(op) &&\n-             !isa<triton::LoadOp, triton::StoreOp, triton::AtomicRMWOp,\n-                  triton::AtomicCASOp>(op) &&\n-             !isa<triton::DotOp>(op) && !isa<scf::YieldOp>(op) &&\n-             !isa<triton::gpu::ConvertLayoutOp>(op);\n-    };\n-    mlir::getForwardSlice(cvt.getResult(), &cvtSlices, filter);\n-    if (cvtSlices.empty())\n-      return failure();\n-\n-    for (Operation *op : cvtSlices) {\n-      if (!isa<triton::ViewOp, triton::CatOp>(op) &&\n-          !op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n-          !op->hasTrait<mlir::OpTrait::Elementwise>() &&\n-          !isa<triton::StoreOp>(op))\n-        return failure();\n-      for (Value arg : op->getOperands()) {\n-        Operation *argOp = arg.getDefiningOp();\n-        if (argOp && (argOp != cvt) &&\n-            !isa<arith::ConstantOp, triton::SplatOp, triton::MakeRangeOp>(\n-                argOp)) {\n-          return failure();\n-        }\n-      }\n-    }\n-\n-    // Otherwise, we push the conversion forward\n-    // since we'll be able to move it out of\n-    // the loop once it reaches the yield op\n-    pushConversionForward(cvt, cvtSlices, rewriter);\n-    return success();\n-  }\n-};\n-\n-// -----------------------------------------------------------------------------\n-//\n-// -----------------------------------------------------------------------------\n-namespace {\n-int computeCapabilityToMMAVersion(int computeCapability) {\n-  if (computeCapability < 70) {\n-    return 0;\n-  } else if (computeCapability < 80) {\n-    return 1;\n-  } else if (computeCapability < 90) {\n-    return 2;\n-  } else if (computeCapability < 100) {\n-    // FIXME: temporarily add this to pass unis tests\n-    return 2;\n-  } else {\n-    assert(false && \"computeCapability > 100 not supported\");\n-    return 3;\n-  }\n-}\n-\n-SmallVector<int64_t, 2> mmaVersionToShapePerWarp(int version) {\n-  if (version == 1)\n-    return {16, 16};\n-  else if (version == 2)\n-    return {16, 8};\n-  else {\n-    assert(false && \"version not supported\");\n-    return {0, 0};\n-  }\n-}\n-\n-SmallVector<unsigned, 2> warpsPerTileV1(const ArrayRef<int64_t> shape,\n-                                        int numWarps) {\n-  // Set a default value and ensure product of wpt equals numWarps\n-  return {static_cast<unsigned>(numWarps), 1};\n-}\n-\n-SmallVector<unsigned, 2> warpsPerTileV2(triton::DotOp dotOp,\n-                                        const ArrayRef<int64_t> shape,\n-                                        int numWarps) {\n-  SetVector<Operation *> slices;\n-  mlir::getForwardSlice(dotOp.getResult(), &slices);\n-  if (llvm::find_if(slices, [](Operation *op) {\n-        return isa<triton::DotOp>(op);\n-      }) != slices.end())\n-    return {(unsigned)numWarps, 1};\n-\n-  SmallVector<unsigned, 2> ret = {1, 1};\n-  SmallVector<int64_t, 2> shapePerWarp = {16, 8};\n-  bool changed = false;\n-  // TODO (@daadaada): double-check.\n-  // original logic in\n-  // https://github.com/openai/triton/blob/master/lib/codegen/analysis/layout.cc#L252\n-  // seems buggy for shape = [32, 16] ?\n-  do {\n-    changed = false;\n-    if (ret[0] * ret[1] >= numWarps)\n-      break;\n-    if (shape[0] / shapePerWarp[0] / ret[0] >=\n-        shape[1] / (shapePerWarp[1] * 2) / ret[1]) {\n-      if (ret[0] < shape[0] / shapePerWarp[0]) {\n-        ret[0] *= 2;\n-      } else\n-        ret[1] *= 2;\n-    } else {\n-      ret[1] *= 2;\n-    }\n-  } while (true);\n-  return ret;\n-}\n-\n-} // namespace\n-\n-class OptimizeBlockedToShared : public mlir::RewritePattern {\n-public:\n-  explicit OptimizeBlockedToShared(mlir::MLIRContext *context)\n-      : RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(), 1,\n-                       context) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto cvt = cast<triton::gpu::ConvertLayoutOp>(op);\n-    auto srcType = cvt.getOperand().getType().cast<RankedTensorType>();\n-    auto dstType = cvt.getResult().getType().cast<RankedTensorType>();\n-    auto srcBlockedLayout =\n-        srcType.getEncoding().dyn_cast<triton::gpu::BlockedEncodingAttr>();\n-    auto dstSharedLayout =\n-        dstType.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n-    if (!srcBlockedLayout || !dstSharedLayout)\n-      return failure();\n-    if (srcBlockedLayout.getOrder() == dstSharedLayout.getOrder())\n-      return failure();\n-    // For now only works if single use is transpose\n-    // TODO: rematerialize #shared uses\n-    auto users = op->getUsers();\n-    if (std::distance(users.begin(), users.end()) != 1 ||\n-        !isa<triton::TransOp>(*users.begin()))\n-      return failure();\n-\n-    auto tmpShared = triton::gpu::SharedEncodingAttr::get(\n-        op->getContext(), dstSharedLayout.getVec(),\n-        dstSharedLayout.getPerPhase(), dstSharedLayout.getMaxPhase(),\n-        srcBlockedLayout.getOrder());\n-    auto tmpType = RankedTensorType::get(srcType.getShape(),\n-                                         srcType.getElementType(), tmpShared);\n-    auto tmpCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        op->getLoc(), tmpType, cvt.getOperand());\n-\n-    auto newDstType = RankedTensorType::get(\n-        users.begin()->getResultTypes()[0].cast<RankedTensorType>().getShape(),\n-        srcType.getElementType(), dstSharedLayout);\n-\n-    auto newTrans = rewriter.create<triton::TransOp>(op->getLoc(), newDstType,\n-                                                     tmpCvt.getResult());\n-\n-    rewriter.replaceOp(*users.begin(), newTrans.getResult());\n-    return success();\n-  }\n-};\n-\n-class OptimizeConvertToDotOperand : public mlir::RewritePattern {\n-public:\n-  explicit OptimizeConvertToDotOperand(mlir::MLIRContext *context)\n-      : RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(), 1,\n-                       context) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto cvt = cast<triton::gpu::ConvertLayoutOp>(op);\n-    auto srcType = cvt.getOperand().getType().cast<RankedTensorType>();\n-    auto dstType = cvt.getResult().getType().cast<RankedTensorType>();\n-    // order\n-    ArrayRef<unsigned> order;\n-    if (auto srcBlockedLayout =\n-            srcType.getEncoding().dyn_cast<triton::gpu::BlockedEncodingAttr>())\n-      order = srcBlockedLayout.getOrder();\n-    else if (auto srcSharedLayout =\n-                 srcType.getEncoding()\n-                     .dyn_cast<triton::gpu::SharedEncodingAttr>())\n-      order = srcSharedLayout.getOrder();\n-    else\n-      return failure();\n-    // dot operand output\n-    auto dstDotOperandLayout =\n-        dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n-    if (!dstDotOperandLayout)\n-      return failure();\n-    if (!dstDotOperandLayout.getIsMMAv1Row())\n-      return failure();\n-    bool isMMAv1Row =\n-        dstDotOperandLayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n-    if ((order[0] == 1 && isMMAv1Row) || (order[0] == 0 && !isMMAv1Row))\n-      return failure();\n-\n-    auto newIsRow = BoolAttr::get(op->getContext(), !isMMAv1Row);\n-    auto newDstEncoding = triton::gpu::DotOperandEncodingAttr::get(\n-        op->getContext(), dstDotOperandLayout.getOpIdx(),\n-        dstDotOperandLayout.getParent(), newIsRow);\n-    auto newDstType = RankedTensorType::get(\n-        dstType.getShape(), dstType.getElementType(), newDstEncoding);\n-    auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        op->getLoc(), newDstType, cvt.getOperand());\n-    rewriter.replaceOp(op, newCvt.getResult());\n-    return success();\n-  }\n-};\n-\n-class BlockedToMMA : public mlir::RewritePattern {\n-  int computeCapability;\n-  mutable int mmaV1Counter{}; // used to generate ID for MMAv1 encoding\n-\n-public:\n-  BlockedToMMA(mlir::MLIRContext *context, int computeCapability)\n-      : mlir::RewritePattern(triton::DotOp::getOperationName(), 2, context),\n-        computeCapability(computeCapability) {}\n-\n-  static SmallVector<unsigned, 2> getWarpsPerTile(triton::DotOp dotOp,\n-                                                  const ArrayRef<int64_t> shape,\n-                                                  int version, int numWarps) {\n-    switch (version) {\n-    case 1:\n-      return warpsPerTileV1(shape, numWarps);\n-    case 2:\n-      return warpsPerTileV2(dotOp, shape, numWarps);\n-    default:\n-      assert(false && \"not supported version\");\n-      return {0, 0};\n-    }\n-  }\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto dotOp = cast<triton::DotOp>(op);\n-    // TODO: Check data-types and SM compatibility\n-    auto oldRetType = dotOp.getResult().getType().cast<RankedTensorType>();\n-    if (!oldRetType.getEncoding() ||\n-        oldRetType.getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n-      return failure();\n-\n-    // for FMA, should retain the blocked layout.\n-    int versionMajor = computeCapabilityToMMAVersion(computeCapability);\n-    if (!supportMMA(dotOp, versionMajor))\n-      return failure();\n-\n-    // get MMA encoding for the given number of warps\n-    auto retShape = oldRetType.getShape();\n-    auto mod = op->getParentOfType<mlir::ModuleOp>();\n-    int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n-\n-    auto warpsPerTile =\n-        getWarpsPerTile(dotOp, retShape, versionMajor, numWarps);\n-    triton::gpu::MmaEncodingAttr mmaEnc;\n-    if (versionMajor == 1) {\n-      mmaEnc = triton::gpu::MmaEncodingAttr::get(\n-          oldRetType.getContext(), versionMajor, numWarps, mmaV1Counter++);\n-    } else if (versionMajor == 2) {\n-      mmaEnc = triton::gpu::MmaEncodingAttr::get(\n-          oldRetType.getContext(), versionMajor, 0 /*versionMinor*/,\n-          warpsPerTile);\n-    } else {\n-      assert(false && \"Mma layout only support versionMajor of 1 or 2\");\n-    }\n-    auto newRetType =\n-        RankedTensorType::get(retShape, oldRetType.getElementType(), mmaEnc);\n-\n-    // convert accumulator\n-    auto oldAcc = dotOp.getOperand(2);\n-    auto newAcc = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        oldAcc.getLoc(), newRetType, oldAcc);\n-    Value a = dotOp.getA();\n-    Value b = dotOp.getB();\n-    auto oldAType = a.getType().cast<RankedTensorType>();\n-    auto oldBType = b.getType().cast<RankedTensorType>();\n-    auto oldAOrder = oldAType.getEncoding()\n-                         .cast<triton::gpu::DotOperandEncodingAttr>()\n-                         .getParent()\n-                         .cast<triton::gpu::BlockedEncodingAttr>()\n-                         .getOrder();\n-    auto oldBOrder = oldBType.getEncoding()\n-                         .cast<triton::gpu::DotOperandEncodingAttr>()\n-                         .getParent()\n-                         .cast<triton::gpu::BlockedEncodingAttr>()\n-                         .getOrder();\n-    Attribute isMMAv1RowA;\n-    Attribute isMMAv1RowB;\n-    if (versionMajor == 1) {\n-      isMMAv1RowA = BoolAttr::get(getContext(), oldAOrder[0] == 1);\n-      isMMAv1RowB = BoolAttr::get(getContext(), oldBOrder[0] == 1);\n-    }\n-\n-    auto newAType = RankedTensorType::get(\n-        oldAType.getShape(), oldAType.getElementType(),\n-        triton::gpu::DotOperandEncodingAttr::get(\n-            oldAType.getContext(), 0, newRetType.getEncoding(), isMMAv1RowA));\n-    auto newBType = RankedTensorType::get(\n-        oldBType.getShape(), oldBType.getElementType(),\n-        triton::gpu::DotOperandEncodingAttr::get(\n-            oldBType.getContext(), 1, newRetType.getEncoding(), isMMAv1RowB));\n-\n-    a = rewriter.create<triton::gpu::ConvertLayoutOp>(a.getLoc(), newAType, a);\n-    b = rewriter.create<triton::gpu::ConvertLayoutOp>(b.getLoc(), newBType, b);\n-    auto newDot = rewriter.create<triton::DotOp>(\n-        dotOp.getLoc(), newRetType, a, b, newAcc, dotOp.getAllowTF32());\n-\n-    rewriter.replaceOpWithNewOp<triton::gpu::ConvertLayoutOp>(\n-        op, oldRetType, newDot.getResult());\n-    return success();\n-  }\n-};\n-\n-// Convert + trans + convert\n-// x = convert_layout distributed -> #shared_x\n-// y = trans x -> #shared_y\n-// z = convert_layout y -> #dot_operand\n-class ConvertTransConvert : public mlir::RewritePattern {\n-\n-public:\n-  ConvertTransConvert(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             1, context) {}\n-\n-  LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto dstOp = cast<triton::gpu::ConvertLayoutOp>(op);\n-    auto tmpOp =\n-        dyn_cast_or_null<triton::TransOp>(dstOp.getSrc().getDefiningOp());\n-    if (!tmpOp)\n-      return mlir::failure();\n-    auto srcOp = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n-        tmpOp.getSrc().getDefiningOp());\n-    if (!srcOp)\n-      return mlir::failure();\n-    auto arg = srcOp.getSrc();\n-    auto X = tmpOp.getSrc();\n-    // types\n-    auto argType = arg.getType().cast<RankedTensorType>();\n-    auto XType = X.getType().cast<RankedTensorType>();\n-    auto ZType = dstOp.getResult().getType().cast<RankedTensorType>();\n-    // encodings\n-    auto argEncoding = argType.getEncoding();\n-    auto XEncoding =\n-        XType.getEncoding().cast<triton::gpu::SharedEncodingAttr>();\n-    auto ZEncoding =\n-        ZType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n-    if (!ZEncoding)\n-      return mlir::failure();\n-    // new X encoding\n-    auto newXOrder = triton::gpu::getOrder(argEncoding);\n-    auto newXEncoding = triton::gpu::SharedEncodingAttr::get(\n-        getContext(), ZEncoding, XType.getShape(), newXOrder,\n-        XType.getElementType());\n-    auto newXType = RankedTensorType::get(XType.getShape(),\n-                                          XType.getElementType(), newXEncoding);\n-    if (XEncoding == newXEncoding)\n-      return mlir::failure();\n-\n-    auto newX = rewriter.create<triton::gpu::ConvertLayoutOp>(srcOp.getLoc(),\n-                                                              newXType, arg);\n-    auto newY = rewriter.create<triton::TransOp>(tmpOp.getLoc(), newX);\n-    rewriter.replaceOpWithNewOp<triton::gpu::ConvertLayoutOp>(dstOp, ZType,\n-                                                              newY);\n-    return mlir::success();\n-  }\n-};\n-\n //\n class ConvertDotConvert : public mlir::RewritePattern {\n public:\n@@ -1272,31 +762,25 @@ class ConvertDotConvert : public mlir::RewritePattern {\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n \n-class TritonGPUCombineOpsPass\n-    : public TritonGPUCombineOpsBase<TritonGPUCombineOpsPass> {\n+class TritonGPURemoveLayoutConversionsPass\n+    : public TritonGPURemoveLayoutConversionsBase<\n+          TritonGPURemoveLayoutConversionsPass> {\n public:\n-  TritonGPUCombineOpsPass() = default;\n-  TritonGPUCombineOpsPass(int computeCapability) {\n-    this->computeCapability = computeCapability;\n-  }\n+  TritonGPURemoveLayoutConversionsPass() = default;\n+\n   void runOnOperation() override {\n     MLIRContext *context = &getContext();\n     ModuleOp m = getOperation();\n \n     mlir::RewritePatternSet patterns(context);\n \n-    patterns.add<OptimizeBlockedToShared>(context);\n-    patterns.add<OptimizeConvertToDotOperand>(context);\n     patterns.add<SimplifyConversion>(context);\n     patterns.add<SimplifyReduceCvt>(context);\n-    patterns.add<FoldConvertAndReduce>(context);\n-    patterns.add<DecomposeDotOperand>(context);\n     patterns.add<RematerializeBackward>(context);\n     patterns.add<RematerializeForward>(context);\n     patterns.add<MoveConvertOutOfLoop>(context);\n     patterns.add<MoveConvertOutOfIf>(context);\n-    patterns.add<BlockedToMMA>(context, computeCapability);\n-    patterns.add<ConvertTransConvert>(context);\n+    patterns.add<DecomposeDotOperand>(context);\n     patterns.add<ConvertDotConvert>(context);\n \n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n@@ -1309,7 +793,6 @@ class TritonGPUCombineOpsPass\n   }\n };\n \n-std::unique_ptr<Pass>\n-mlir::createTritonGPUCombineOpsPass(int computeCapability) {\n-  return std::make_unique<TritonGPUCombineOpsPass>(computeCapability);\n+std::unique_ptr<Pass> mlir::createTritonGPURemoveLayoutConversionsPass() {\n+  return std::make_unique<TritonGPURemoveLayoutConversionsPass>();\n }"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -136,6 +136,12 @@ static std::map<std::string, std::string> getExternLibs(mlir::ModuleOp module) {\n \n   if (!funcs.empty()) {\n     static const std::string libdevice = \"libdevice\";\n+    // first search for environmental path\n+    std::string env_path = ::triton::tools::getenv(\"TRITON_LIBDEVICE_PATH\");\n+    if (!env_path.empty()) {\n+      externLibs.try_emplace(libdevice, env_path);\n+      return externLibs;\n+    }\n     namespace fs = std::filesystem;\n     // Search for libdevice relative to its library path if used from Python\n     // Then native code is in `triton/_C/libtriton.so` and libdevice in"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 11, "deletions": 3, "changes": 14, "file_content_changes": "@@ -1375,7 +1375,7 @@ void init_triton_ir(py::module &&m) {\n       .def(\n           \"add_sccp_pass\",\n           [](mlir::PassManager &self) { self.addPass(mlir::createSCCPPass()); })\n-      .def(\"add_coalesce_pass\",\n+      .def(\"add_tritongpu_coalesce_pass\",\n            [](mlir::PassManager &self) {\n              self.addPass(mlir::createTritonGPUCoalescePass());\n            })\n@@ -1414,10 +1414,18 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::PassManager &self) {\n              self.addPass(mlir::createTritonGPUPrefetchPass());\n            })\n-      .def(\"add_tritongpu_combine_pass\",\n+      .def(\"add_tritongpu_accelerate_matmul_pass\",\n            [](mlir::PassManager &self, int computeCapability) {\n              self.addPass(\n-                 mlir::createTritonGPUCombineOpsPass(computeCapability));\n+                 mlir::createTritonGPUAccelerateMatmulPass(computeCapability));\n+           })\n+      .def(\"add_tritongpu_fuse_transpositions_pass\",\n+           [](mlir::PassManager &self) {\n+             self.addPass(mlir::createTritonGPUFuseTranspositionsPass());\n+           })\n+      .def(\"add_tritongpu_remove_layout_conversions_pass\",\n+           [](mlir::PassManager &self) {\n+             self.addPass(mlir::createTritonGPURemoveLayoutConversionsPass());\n            })\n       .def(\"add_tritongpu_update_mma_for_volta_pass\",\n            [](mlir::PassManager &self) {"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 66, "deletions": 27, "changes": 93, "file_content_changes": "@@ -8,7 +8,7 @@\n import triton.language as tl\n from triton.testing import get_dram_gbps, get_max_tensorcore_tflops\n \n-DEVICE_NAME = 'v100'\n+DEVICE_NAME = {7: 'v100', 8: 'a100'}[torch.cuda.get_device_capability()[0]]\n \n #######################\n # Utilities\n@@ -34,7 +34,6 @@ def nvsmi(attrs):\n matmul_data = {\n     'v100': {\n         # square\n-        (256, 256, 256): {'float16': 0.027},\n         (512, 512, 512): {'float16': 0.158},\n         (1024, 1024, 1024): {'float16': 0.466},\n         (2048, 2048, 2048): {'float16': 0.695},\n@@ -51,29 +50,26 @@ def nvsmi(attrs):\n         (4096, 64, 4096): {'float16': 0.264},\n         (8192, 64, 8192): {'float16': 0.452},\n     },\n+    # NOTE:\n+    # A100 in the CI server is slow-ish for some reason.\n+    # On some other servers, we are getting about 90% peak for 8kx8x8k float16\n     'a100': {\n-        (256, 256, 256): {'float16': 0.010, 'float32': 0.0214, 'int8': 0.006},\n-        (512, 512, 512): {'float16': 0.061, 'float32': 0.109, 'int8': 0.030},\n-        (1024, 1024, 1024): {'float16': 0.287, 'float32': 0.331, 'int8': 0.169},\n-        (2048, 2048, 2048): {'float16': 0.604, 'float32': 0.599, 'int8': 0.385},\n-        (4096, 4096, 4096): {'float16': 0.842, 'float32': 0.862, 'int8': 0.711},\n-        (8192, 8192, 8192): {'float16': 0.896, 'float32': 0.932, 'int8': 0.860},\n+        (512, 512, 512): {'float16': 0.08, 'float32': 0.13, 'int8': 0.05},\n+        (1024, 1024, 1024): {'float16': 0.33, 'float32': 0.35, 'int8': 0.169},\n+        (2048, 2048, 2048): {'float16': 0.64, 'float32': 0.57, 'int8': 0.34},\n+        (4096, 4096, 4096): {'float16': 0.81, 'float32': 0.75, 'int8': 0.46},\n+        (8192, 8192, 8192): {'float16': 0.77, 'float32': 0.85, 'int8': 0.51},\n         # tall-skinny\n         (16, 1024, 1024): {'float16': 0.0077, 'float32': 0.0127, 'int8': 0.005},\n         (16, 4096, 4096): {'float16': 0.0363, 'float32': 0.0457, 'int8': 0.0259},\n-        (16, 8192, 8192): {'float16': 0.0564, 'float32': 0.0648, 'int8': 0.0431},\n+        (16, 8192, 8192): {'float16': 0.07, 'float32': 0.0648, 'int8': 0.0431},\n         (64, 1024, 1024): {'float16': 0.0271, 'float32': 0.0509, 'int8': 0.0169},\n-        (64, 4096, 4096): {'float16': 0.141, 'float32': 0.162, 'int8': 0.097},\n-        (64, 8192, 8192): {'float16': 0.244, 'float32': 0.257, 'int8': 0.174},\n+        (64, 4096, 4096): {'float16': 0.16, 'float32': 0.162, 'int8': 0.097},\n+        (64, 8192, 8192): {'float16': 0.30, 'float32': 0.257, 'int8': 0.174},\n         (1024, 64, 1024): {'float16': 0.0263, 'float32': 0.0458, 'int8': 0.017},\n-        (4096, 64, 4096): {'float16': 0.135, 'float32': 0.177, 'int8': 0.102},\n-        (8192, 64, 8192): {'float16': 0.216, 'float32': 0.230, 'int8': 0.177},\n+        (4096, 64, 4096): {'float16': 0.16, 'float32': 0.177, 'int8': 0.102},\n+        (8192, 64, 8192): {'float16': 0.25, 'float32': 0.230, 'int8': 0.177},\n     }\n-    #   # deep reductions\n-    #   (64  , 64  , 16384) : {'a100': 0.},\n-    #   (64  , 64  , 65536) : {'a100': 0.},\n-    #   (256 , 256 , 8192 ) : {'a100': 0.},\n-    #   (256 , 256 , 32768) : {'a100': 0.},\n }\n \n \n@@ -88,9 +84,7 @@ def test_matmul(M, N, K, dtype_str):\n     torch.manual_seed(0)\n     ref_gpu_util = matmul_data[DEVICE_NAME][(M, N, K)][dtype_str]\n     cur_sm_clock = nvsmi(['clocks.current.sm'])[0]\n-    ref_sm_clock = sm_clocks[DEVICE_NAME]\n     max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n-    assert abs(cur_sm_clock - ref_sm_clock) < 10, f'GPU SMs must run at {ref_sm_clock} MHz'\n     if dtype == torch.int8:\n         a = torch.randint(-128, 127, (M, K), dtype=dtype, device='cuda')\n         b = torch.randint(-128, 127, (N, K), dtype=dtype, device='cuda')\n@@ -99,10 +93,10 @@ def test_matmul(M, N, K, dtype_str):\n         a = torch.randn((M, K), dtype=dtype, device='cuda')\n         b = torch.randn((K, N), dtype=dtype, device='cuda')\n     fn = lambda: triton.ops.matmul(a, b)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=25, rep=1000)\n+    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=300)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    triton.testing.assert_almost_equal(cur_gpu_util, ref_gpu_util, decimal=2)\n+    assert triton.testing.allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n \n #######################\n@@ -149,16 +143,61 @@ def _add(x_ptr, y_ptr, output_ptr, n_elements,\n def test_elementwise(N):\n     torch.manual_seed(0)\n     ref_gpu_util = elementwise_data[DEVICE_NAME][N]\n-    cur_mem_clock = nvsmi(['clocks.current.memory'])[0]\n-    ref_mem_clock = mem_clocks[DEVICE_NAME]\n     max_gpu_perf = get_dram_gbps()\n-    assert abs(cur_mem_clock - ref_mem_clock) < 10, f'GPU memory must run at {ref_mem_clock} MHz'\n     z = torch.empty((N, ), dtype=torch.float16, device='cuda')\n     x = torch.randn_like(z)\n     y = torch.randn_like(z)\n     grid = lambda args: (triton.cdiv(N, args['BLOCK_SIZE']), )\n     fn = lambda: _add[grid](x, y, z, N, BLOCK_SIZE=1024)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=25, rep=250)\n+    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=500)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    triton.testing.assert_almost_equal(cur_gpu_util, ref_gpu_util, decimal=2)\n+    assert triton.testing.allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+\n+#######################\n+# Flash-Attention\n+#######################\n+\n+\n+flash_attention_data = {\n+    \"a100\": {\n+        (4, 48, 4096, 64, 'forward', 'float16'): 0.37,\n+        (4, 48, 4096, 64, 'backward', 'float16'): 0.25,\n+    }\n+}\n+\n+\n+@pytest.mark.parametrize(\"Z, H, N_CTX, D_HEAD\", [[4, 48, 4096, 64]])\n+@pytest.mark.parametrize(\"mode\", ['forward', 'backward'])\n+@pytest.mark.parametrize(\"dtype_str\", ['float16'])\n+def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n+    is_backward = mode == 'backward'\n+    capability = torch.cuda.get_device_capability()\n+    if capability[0] < 8:\n+        pytest.skip(\"Flash attention only supported for compute capability < 80\")\n+    torch.manual_seed(20)\n+    dtype = {'float16': torch.float16, 'float32': torch.float32, 'int8': torch.int8}[dtype_str]\n+    # init data\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n+    sm_scale = 0.2\n+    # benchmark\n+    fn = lambda: triton.ops.attention(q, k, v, sm_scale)\n+    if is_backward:\n+        o = fn()\n+        do = torch.randn_like(o)\n+        fn = lambda: o.backward(do, retain_graph=True)\n+    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=500)\n+    # compute flops\n+    flops_per_matmul = 2. * Z * H * N_CTX * N_CTX * D_HEAD * 0.5\n+    total_flops = 2 * flops_per_matmul\n+    if is_backward:\n+        total_flops *= 2.5  # 2.0(bwd) + 0.5(recompute)\n+    cur_gpu_perf = total_flops / ms * 1e-9\n+    # maximum flops\n+    cur_sm_clock = nvsmi(['clocks.current.sm'])[0]\n+    max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n+    cur_gpu_util = cur_gpu_perf / max_gpu_perf\n+    ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, mode, dtype_str)]\n+    assert triton.testing.allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 28, "deletions": 23, "changes": 51, "file_content_changes": "@@ -16,7 +16,6 @@\n import warnings\n from collections import namedtuple\n from pathlib import Path\n-from sysconfig import get_paths\n from typing import Any, Callable, Dict, Tuple, Union\n \n import setuptools\n@@ -976,32 +975,32 @@ def ast_to_ttir(fn, signature, specialization, constants):\n     return optimize_triton_ir(mod)\n \n \n-def ttir_to_ttgir(mod, num_warps, num_stages, compute_capability):\n+def ttir_to_ttgir(mod, num_warps):\n     pm = _triton.ir.pass_manager(mod.context)\n     pm.add_convert_triton_to_tritongpu_pass(num_warps)\n+    pm.run(mod)\n+    return mod\n+\n+\n+def optimize_ttgir(mod, num_stages, compute_capability):\n+    pm = _triton.ir.pass_manager(mod.context)\n     pm.enable_debug()\n-    pm.add_coalesce_pass()\n-    # The combine pass converts blocked layout to mma layout\n-    # for dot ops so that pipeline can get shared memory swizzled correctly.\n-    pm.add_tritongpu_combine_pass(compute_capability)\n+    pm.add_tritongpu_coalesce_pass()\n+    pm.add_tritongpu_accelerate_matmul_pass(compute_capability)\n+    pm.add_tritongpu_remove_layout_conversions_pass()\n+    pm.add_tritongpu_fuse_transpositions_pass()\n     pm.add_tritongpu_pipeline_pass(num_stages)\n-    # Prefetch must be done after pipeline pass because pipeline pass\n-    # extracts slices from the original tensor.\n     pm.add_tritongpu_prefetch_pass()\n-    pm.add_canonicalizer_pass()\n-    pm.add_cse_pass()\n-    pm.add_tritongpu_combine_pass(compute_capability)\n-    pm.add_licm_pass()\n-    pm.add_tritongpu_combine_pass(compute_capability)\n-    pm.add_cse_pass()\n+    pm.add_tritongpu_fuse_transpositions_pass()\n+    pm.add_tritongpu_remove_layout_conversions_pass()\n     pm.add_tritongpu_decompose_conversions_pass()\n     if compute_capability // 10 == 7:\n         # The update_mma_for_volta pass helps to compute some information for MMA encoding specifically for MMAv1\n         # NOTE this pass should be placed after all the passes those modifies mma layout\n         pm.add_tritongpu_update_mma_for_volta_pass()\n+    pm.add_tritongpu_reorder_instructions_pass()\n     pm.add_cse_pass()\n     pm.add_symbol_dce_pass()\n-    pm.add_tritongpu_reorder_instructions_pass()\n     pm.run(mod)\n     return mod\n \n@@ -1316,11 +1315,6 @@ def default_cache_dir():\n     return os.path.join(os.environ[\"HOME\"], \".triton\", \"cache\")\n \n \n-def default_cuda_dir():\n-    default_dir = \"/usr/local/cuda\"\n-    return os.getenv(\"CUDA_HOME\", default=default_dir)\n-\n-\n class CacheManager:\n \n     def __init__(self, key):\n@@ -1378,7 +1372,9 @@ def quiet():\n \n def _build(name, src, srcdir):\n     cuda_lib_dirs = libcuda_dirs()\n-    cuda_path = os.environ.get('CUDA_PATH', default_cuda_dir())\n+    base_dir = os.path.dirname(__file__)\n+    cuda_path = os.path.join(base_dir, \"third_party\", \"cuda\")\n+\n     cu_include_dir = os.path.join(cuda_path, \"include\")\n     triton_include_dir = os.path.join(os.path.dirname(__file__), \"include\")\n     cuda_header = os.path.join(cu_include_dir, \"cuda.h\")\n@@ -1396,7 +1392,16 @@ def _build(name, src, srcdir):\n         cc = gcc if gcc is not None else clang\n         if cc is None:\n             raise RuntimeError(\"Failed to find C compiler. Please specify via CC environment variable.\")\n-    py_include_dir = get_paths()[\"include\"]\n+    # This function was renamed and made public in Python 3.10\n+    if hasattr(sysconfig, 'get_default_scheme'):\n+        scheme = sysconfig.get_default_scheme()\n+    else:\n+        scheme = sysconfig._get_default_scheme()\n+    # 'posix_local' is a custom scheme on Debian. However, starting Python 3.10, the default install\n+    # path changes to include 'local'. This change is required to use triton with system-wide python.\n+    if scheme == 'posix_local':\n+        scheme = 'posix_prefix'\n+    py_include_dir = sysconfig.get_paths(scheme=scheme)[\"include\"]\n \n     cc_cmd = [cc, src, \"-O3\", f\"-I{cu_include_dir}\", f\"-I{py_include_dir}\", f\"-I{srcdir}\", \"-shared\", \"-fPIC\", \"-lcuda\", \"-o\", so]\n     cc_cmd += [f\"-L{dir}\" for dir in cuda_lib_dirs]\n@@ -1560,7 +1565,7 @@ def compile(fn, **kwargs):\n         \"ttir\": (lambda path: parse_mlir_module(path, context),\n                  lambda src: ast_to_ttir(src, signature, configs[0], constants)),\n         \"ttgir\": (lambda path: parse_mlir_module(path, context),\n-                  lambda src: ttir_to_ttgir(src, num_warps, num_stages, capability)),\n+                  lambda src: optimize_ttgir(ttir_to_ttgir(src, num_warps), num_stages, capability)),\n         \"llir\": (lambda path: Path(path).read_text(),\n                  lambda src: ttgir_to_llir(src, extern_libs, capability)),\n         \"ptx\": (lambda path: Path(path).read_text(),"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 9, "deletions": 4, "changes": 13, "file_content_changes": "@@ -9,6 +9,8 @@\n \n T = TypeVar('T')\n \n+TRITON_MAX_TENSOR_NUMEL = 131072\n+\n \n def _to_tensor(x, builder):\n     if isinstance(x, bool):\n@@ -254,6 +256,8 @@ def __init__(self, element_ty: dtype, shape: List):\n         self.numel = 1\n         for s in self.shape:\n             self.numel *= s\n+        if self.numel > TRITON_MAX_TENSOR_NUMEL:\n+            raise ValueError(f\"numel ({self.numel}) exceeds triton maximum tensor numel ({TRITON_MAX_TENSOR_NUMEL})\")\n \n         self.name = self.__str__()\n \n@@ -702,12 +706,13 @@ def num_programs(axis, _builder=None):\n @builtin\n def arange(start, end, _builder=None):\n     \"\"\"\n-    Returns contiguous values within the open interval [:code:`start`, :code:`end`).\n+    Returns contiguous values within the left-closed and right-open interval [:code:`start`, :code:`end`). \\\n+    End - Start must be less than or equal to TRITON_MAX_TENSOR_NUMEL = 131072\n \n     :param start: Start of the interval. Must be a power of two.\n-    :type start: int\n-    :param stop: End of the interval. Must be a power of two >= start.\n-    :type stop: int\n+    :type start: int32\n+    :param end: End of the interval. Must be a power of two > start.\n+    :type end: int32\n     \"\"\"\n     start = _constexpr_to_value(start)\n     end = _constexpr_to_value(end)"}, {"filename": "python/triton/language/libdevice.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -3,7 +3,8 @@\n from .. import impl\n from . import core, extern\n \n-LIBDEVICE_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"third_party\", \"cuda\", \"lib\", \"libdevice.10.bc\")\n+LOCAL_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"third_party\", \"cuda\", \"lib\", \"libdevice.10.bc\")\n+LIBDEVICE_PATH = os.getenv(\"TRITON_LIBDEVICE_PATH\", LOCAL_PATH)\n \n \n @impl.extern"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -478,6 +478,12 @@ def not_equal(input: tl.tensor,\n def arange(start: int, end: int, builder: ir.builder) -> tl.tensor:\n     if not isinstance(start, int) or not isinstance(end, int):\n         raise ValueError(\"arange's arguments must be of type tl.constexpr\")\n+    is_start_int64 = bool(start >> 32)\n+    is_end_int64 = bool(end >> 32)\n+    if is_start_int64 or is_end_int64:\n+        raise ValueError(\"arange must fit in int32\")\n+    if end <= start:\n+        raise ValueError(\"arange's end argument must be greater than the start argument\")\n \n     shape = [end - start]\n     ret_ty = tl.block_type(tl.int32, shape)\n@@ -647,6 +653,8 @@ def cast(input: tl.tensor,\n          dst_ty: tl.dtype,\n          builder: ir.builder) -> tl.tensor:\n     src_ty = input.type\n+    if isinstance(dst_ty, tl.constexpr):\n+        dst_ty = dst_ty.value\n     if src_ty.is_block():\n         dst_ty = tl.block_type(dst_ty, input.type.get_block_shapes())\n     if src_ty == dst_ty:"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 7, "deletions": 4, "changes": 11, "file_content_changes": "@@ -93,20 +93,23 @@ def assert_almost_equal(x, y, decimal=2, err_msg=''):\n     npt.assert_array_almost_equal(x, y, err_msg=err_msg, decimal=decimal)\n \n \n-def allclose(x, y, tol=1e-2):\n+def allclose(x, y, atol=0, rtol=1e-2):\n+    if not isinstance(x, torch.Tensor):\n+        x = torch.tensor(x)\n+    if not isinstance(y, torch.Tensor):\n+        y = torch.tensor(y)\n     if x.dtype != y.dtype:\n         raise RuntimeError(f'{x.dtype} did not match with {x.dtype}')\n     if x.shape != y.shape:\n         raise RuntimeError(f'{x.shape} did not match with {y.shape}')\n     if x.dtype == torch.bool:\n         return torch.sum(x ^ y) == 0\n     if x.dtype in [torch.int8, torch.int16, torch.int32, torch.int64]:\n-        tol = 0\n+        rtol = 0\n     diff = abs(x - y)\n     x_max = torch.max(x)\n     y_max = torch.max(y)\n-    err = torch.max(diff) / torch.max(x_max, y_max)\n-    return err <= tol\n+    return torch.max(diff) <= atol + rtol * torch.max(x_max, y_max)\n \n \n def nvsmi(attrs):"}, {"filename": "python/triton/tools/aot.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -42,7 +42,8 @@\n         raise argparse.ArgumentError(None, \"Must specify --sm for PTX compilation\")\n \n     # triton-ir -> triton-gpu-ir\n-    module = triton.compiler.ttir_to_ttgir(module, num_warps=4, num_stages=3, compute_capability=args.sm)\n+    module = triton.compiler.ttir_to_ttgir(module, num_warps=4)\n+    module = triton.compiler.optimize_ttgir(module, num_stages=3, compute_capability=args.sm)\n     if args.target == 'triton-gpu-ir':\n         print(module.str())\n         sys.exit(0)"}, {"filename": "python/triton/tools/build_extern.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -289,7 +289,8 @@ def _output_stubs(self) -> str:\n         #   return extern.dispatch(\"libdevice\", <path>, <args>, <arg_type_symbol_dict>, _builder)\n         import_str = \"from . import core, extern\\n\"\n         import_str += \"import os\\n\"\n-        header_str = \"LIBDEVICE_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \\\"..\\\", \\\"third_party\\\", \\\"cuda\\\", \\\"lib\\\", \\\"libdevice.10.bc\\\")\"\n+        header_str = \"LOCAL_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \\\"..\\\", \\\"third_party\\\", \\\"cuda\\\", \\\"lib\\\", \\\"libdevice.10.bc\\\")\\n\"\n+        header_str += \"LIBDEVICE_PATH = os.getenv(\\\"TRITON_LIBDEVICE_PATH\\\", LOCAL_PATH)\\n\"\n         func_str = \"\"\n         for symbols in self._symbol_groups.values():\n             func_str += \"@extern.extern\\n\""}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -223,6 +223,7 @@ def forward(ctx, q, k, v, sm_scale):\n             BLOCK_DMODEL=Lk, num_warps=num_warps,\n             num_stages=2,\n         )\n+        # print(h.asm[\"ttgir\"])\n \n         ctx.save_for_backward(q, k, v, o, L, m)\n         ctx.grid = grid\n@@ -260,6 +261,7 @@ def backward(ctx, do):\n             BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n             num_stages=1,\n         )\n+        # print(h.asm[\"ttgir\"])\n         return dq, dk, dv, None\n \n "}, {"filename": "test/TritonGPU/coalesce.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -split-input-file -tritongpu-coalesce -canonicalize | FileCheck %s\n+// RUN: triton-opt %s -split-input-file -tritongpu-coalesce | FileCheck %s\n \n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -split-input-file -tritongpu-combine 2>&1 | FileCheck %s\n+// RUN: triton-opt %s -split-input-file -tritongpu-remove-layout-conversions 2>&1 | FileCheck %s\n \n #layout0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n #layout1 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>"}, {"filename": "test/TritonGPU/matmul.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -split-input-file -convert-triton-to-tritongpu -tritongpu-combine -tritongpu-pipeline=num-stages=3 -tritongpu-combine -test-print-allocation 2>&1 | FileCheck %s\n+// RUN: triton-opt %s -split-input-file -convert-triton-to-tritongpu -tritongpu-remove-layout-conversions -tritongpu-pipeline=num-stages=3 -test-print-allocation 2>&1 | FileCheck %s\n \n // CHECK: offset = 0, size = 49152\n // CHECK: offset = 49152, size = 49152"}, {"filename": "test/TritonGPU/update-mma-for-volta.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -split-input-file -tritongpu-combine -tritongpu-update-mma-for-volta 2>&1 | FileCheck %s\n+// RUN: triton-opt %s -split-input-file -tritongpu-fuse-transposition -tritongpu-update-mma-for-volta 2>&1 | FileCheck %s\n \n // -----\n "}]