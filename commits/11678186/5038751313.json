[{"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -83,6 +83,7 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n         if(!mmaEnc)\n           return $_get(context, 1, 1, 1, order);\n \n+\n         int opIdx = dotOpEnc.getOpIdx();\n \n         // number of rows per phase"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 7, "deletions": 4, "changes": 11, "file_content_changes": "@@ -303,6 +303,8 @@ MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> offs,\n     return {extract_val(elemTy, resV4, 0), extract_val(elemTy, resV4, 1),\n             extract_val(elemTy, resV4, 2), extract_val(elemTy, resV4, 3)};\n   } else {\n+    if (needTrans && (4 / elemBytes) != kWidth)\n+      llvm_unreachable(\"unimplemented Shared -> DotOperandMmav2 code path\");\n     // base pointers\n     std::array<std::array<Value, 4>, 2> ptrs;\n     int vecWidth = 4 / elemBytes;\n@@ -329,8 +331,6 @@ MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> offs,\n     // row + trans and col + no-trans are equivalent\n     bool isActualTrans =\n         (needTrans && kOrder == 1) || (!needTrans && kOrder == 0);\n-    if (isActualTrans)\n-      std::swap(vptrs[1], vptrs[2]);\n     // pack loaded vectors into 4 32-bit values\n     int inc = needTrans ? 1 : kWidth;\n     VectorType packedTy = vec_ty(int_ty(8 * elemBytes), inc);\n@@ -348,12 +348,15 @@ MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> offs,\n         Value val = load(ptr);\n         Value canonval = bitcast(val, vec_ty(canonInt, canonWidth));\n         for (int w = 0; w < canonWidth; ++w) {\n-          retElems[idx + w * kWidth / vecWidth] =\n-              insert_element(retElems[idx + w * kWidth / vecWidth],\n+          int ridx = idx + w * kWidth / vecWidth;\n+          retElems[ridx] =\n+              insert_element(retElems[ridx],\n                              extract_element(canonval, i32_val(w)), i32_val(e));\n         }\n       }\n     }\n+    if (isActualTrans)\n+      std::swap(retElems[1], retElems[2]);\n     return {bitcast(retElems[0], i32_ty), bitcast(retElems[1], i32_ty),\n             bitcast(retElems[2], i32_ty), bitcast(retElems[3], i32_ty)};\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 55, "deletions": 7, "changes": 62, "file_content_changes": "@@ -81,15 +81,18 @@ class MoveOpAfterLayoutConversion : public mlir::RewritePattern {\n       : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n                              1, context) {}\n \n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n+  static mlir::LogicalResult\n+  isBlockedToDotOperand(mlir::Operation *op,\n+                        triton::gpu::DotOperandEncodingAttr &retEncoding,\n+                        triton::gpu::BlockedEncodingAttr &srcEncoding) {\n+    if (!op)\n+      return failure();\n     auto cvt = cast<triton::gpu::ConvertLayoutOp>(op);\n     auto srcTy = cvt.getOperand().getType().cast<RankedTensorType>();\n     auto retTy = cvt.getResult().getType().dyn_cast<RankedTensorType>();\n-    auto retEncoding =\n+    retEncoding =\n         retTy.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n-    auto srcEncoding =\n+    srcEncoding =\n         srcTy.getEncoding().dyn_cast<triton::gpu::BlockedEncodingAttr>();\n     if (!retTy)\n       return failure();\n@@ -101,6 +104,51 @@ class MoveOpAfterLayoutConversion : public mlir::RewritePattern {\n       return failure();\n     if (!srcEncoding)\n       return failure();\n+    return success();\n+  }\n+\n+  static bool isTrans(const triton::gpu::DotOperandEncodingAttr &retEncoding,\n+                      const triton::gpu::BlockedEncodingAttr &srcEncoding) {\n+    int kOrder = retEncoding.getOpIdx() ^ 1;\n+    return kOrder != srcEncoding.getOrder()[0];\n+  }\n+\n+  static bool isDotNT(triton::DotOp dotOp) {\n+    triton::gpu::DotOperandEncodingAttr aRetEncoding;\n+    triton::gpu::DotOperandEncodingAttr bRetEncoding;\n+    triton::gpu::BlockedEncodingAttr aSrcEncoding;\n+    triton::gpu::BlockedEncodingAttr bSrcEncoding;\n+    if (isBlockedToDotOperand(dotOp.getOperand(0).getDefiningOp(), aRetEncoding,\n+                              aSrcEncoding)\n+            .failed())\n+      return false;\n+    if (isBlockedToDotOperand(dotOp.getOperand(1).getDefiningOp(), bRetEncoding,\n+                              bSrcEncoding)\n+            .failed())\n+      return false;\n+    if (!aRetEncoding || !bRetEncoding || !aSrcEncoding || !bSrcEncoding)\n+      return false;\n+    return !isTrans(aRetEncoding, aSrcEncoding) &&\n+           !isTrans(bRetEncoding, bSrcEncoding);\n+  }\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto cvt = cast<triton::gpu::ConvertLayoutOp>(op);\n+    triton::gpu::DotOperandEncodingAttr retEncoding;\n+    triton::gpu::BlockedEncodingAttr srcEncoding;\n+    if (isBlockedToDotOperand(op, retEncoding, srcEncoding).failed())\n+      return mlir::failure();\n+\n+    // only supports dot NT\n+    auto users = cvt->getUsers();\n+    auto dotOp = dyn_cast_or_null<DotOp>(*users.begin());\n+    if (!dotOp)\n+      return failure();\n+    if (!isDotNT(dotOp))\n+      return failure();\n+\n     // don't move things around when cvt operand is a block arg\n     Operation *argOp = cvt.getOperand().getDefiningOp();\n     if (!argOp)\n@@ -129,8 +177,8 @@ class MoveOpAfterLayoutConversion : public mlir::RewritePattern {\n         return failure();\n       // we don't want to use ldmatrix for 8-bit data that requires trans\n       // since Nvidia GPUs can't do it efficiently\n-      bool isTrans =\n-          (retEncoding.getOpIdx() == 1) ^ (srcEncoding.getOrder()[0] == 0);\n+      int kOrder = retEncoding.getOpIdx() ^ 1;\n+      bool isTrans = kOrder != srcEncoding.getOrder()[0];\n       bool isInt8 = srcTy.getElementType().getIntOrFloatBitWidth() == 8;\n       if (isTrans && isInt8)\n         return failure();"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 68, "deletions": 28, "changes": 96, "file_content_changes": "@@ -1969,6 +1969,23 @@ def kernel_dynamic(out, val, dtype: tl.constexpr):\n     assert torch.all(out_dynamic == 2)\n \n \n+@pytest.mark.parametrize(\"literal, dtype_str\",\n+                         [(1e+50, \"f64\"), (1e+10, \"f32\"), (1.0, \"f32\"),\n+                          ('float(\"inf\")', \"f32\"), ('float(\"-inf\")', \"f32\"),\n+                          ('float(\"nan\")', \"f32\"), ('float(\"-nan\")', \"f32\"),\n+                          (0., \"f32\"),\n+                          (5, \"i32\"), (2**40, \"i64\"),])\n+def test_constexpr(literal, dtype_str):\n+    @triton.jit\n+    def kernel(out_ptr):\n+        val = GENERATE_TEST_HERE\n+        tl.store(out_ptr.to(tl.pointer_type(val.dtype)), val)\n+\n+    kernel_patched = patch_kernel(kernel, {'GENERATE_TEST_HERE': f\"{literal}\"})\n+    out = torch.zeros((1,), dtype=torch.float32, device=\"cuda\")\n+    h = kernel_patched[(1,)](out)\n+    assert re.search(r\"arith.constant .* : \" + dtype_str, h.asm[\"ttir\"]) is not None\n+\n # TODO: uncomment once DotOperandEncoding::getElemsPerThread is implemented\n # @pytest.mark.parametrize(\"dtype_str\", ['float32', 'float16'])\n # def test_dot_without_load(dtype_str):\n@@ -2639,41 +2656,64 @@ def add_fn_static_cond(x, cond: tl.constexpr):\n         return x + 1\n \n \n-@pytest.mark.parametrize(\"call_type\", [\"attribute\", \"jit_function\", \"jit_function_return\",\n-                                       \"ifexp\", \"expr\", \"jit_function_static_cond\", \"jit_function_noinline\"])\n+@pytest.mark.parametrize(\"call_type\", [\"attribute\", \"attribute_jit\",\n+                                       \"jit\", \"jit_if\", \"jit_ifexp\", \"jit_expr\",\n+                                       \"jit_static_cond\", \"jit_noinline\", \"jit_extern\"])\n def test_if_call(call_type):\n     @triton.jit\n     def kernel(Out, call_type: tl.constexpr):\n         pid = tl.program_id(0)\n         o = tl.load(Out)\n-        if pid == 0:\n-            if call_type == \"attribute\":\n-                # call attribute\n-                a = o + 1\n-                a = a.to(tl.int32).to(tl.int32)\n+        if call_type == \"attribute\":\n+            # call attribute\n+            if pid == 0:\n+                a = o\n+                a = a.to(tl.int32).to(tl.int32) + 1\n                 o = a\n-            else:\n+        elif call_type == \"attribute_jit\":\n+            # call attribute and jit function\n+            if pid == 0:\n+                a = o\n+                a = tl.load(Out + add_fn(a) - 1).to(tl.int32) + 1\n+                o = a\n+        elif call_type == \"jit\":\n+            if pid == 0:\n+                # regular function call\n+                a = o\n+                a = add_fn(a)\n+                o = a\n+        elif call_type == \"jit_if\":\n+            # function without end_if block\n+            if pid == 0:\n+                a = o\n+                a = add_fn_return(a, pid)\n+                o = a\n+        elif call_type == \"jit_ifexp\":\n+            # ifexp expression\n+            if pid == 0:\n                 a = o\n-                if call_type == \"jit_function\":\n-                    # regular function call\n-                    a = add_fn(a)\n-                elif call_type == \"jit_function_return\":\n-                    # function without end_if block\n-                    a = add_fn_return(a, pid)\n-                elif call_type == \"ifexp\":\n-                    # ifexp expression\n-                    a = add_fn(a) if pid == 0 else add_fn_return(a, pid)\n-                elif call_type == \"expr\":\n-                    if pid == 1:\n-                        return\n-                    a = add_fn(a)\n-                    if pid == 0:\n-                        # call without return\n-                        add_fn_expr(Out, a)\n-                elif call_type == \"jit_function_static_cond\":\n-                    a = add_fn_static_cond(a, call_type)\n-                elif call_type == \"jit_function_noinline\":\n-                    a = add_fn_noinline(a)\n+                a = add_fn(a) if pid == 0 else add_fn_return(a, pid)\n+                o = a\n+        elif call_type == \"jit_expr\":\n+            # call without return\n+            if pid == 0:\n+                a = o + 1\n+                add_fn_expr(Out, a)\n+                o = a\n+        elif call_type == \"jit_static_cond\":\n+            if pid == 0:\n+                a = o + 1\n+                add_fn_static_cond(o, call_type)\n+                o = a\n+        elif call_type == \"jit_noinline\":\n+            if pid == 0:\n+                a = o + 1\n+                add_fn_noinline(a)\n+                o = a\n+        elif call_type == \"jit_extern\":\n+            if pid == 0:\n+                a = o + 1\n+                tl.cdiv(a, a)\n                 o = a\n \n         tl.store(Out, o)"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 92, "deletions": 58, "changes": 150, "file_content_changes": "@@ -97,6 +97,97 @@ def __exit__(self, *args, **kwargs):\n         self.generator.local_defs = self.prev_defs\n \n \n+# Check if the given syntax node has an \"early\" return\n+class ContainsReturnChecker(ast.NodeVisitor):\n+    def __init__(self, gscope):\n+        self.gscope = gscope\n+\n+    def _visit_stmts(self, body) -> bool:\n+        for s in body:\n+            if self.visit(s):\n+                return True\n+        return False\n+\n+    def _visit_function(self, fn) -> bool:\n+        # Currently we only support JITFunctions defined in the global scope\n+        if isinstance(fn, JITFunction) and not fn.noinline:\n+            fn_node = fn.parse()\n+            return ContainsReturnChecker(self.gscope).visit(fn_node)\n+        return False\n+\n+    def generic_visit(self, node) -> bool:\n+        ret = False\n+        for _, value in ast.iter_fields(node):\n+            if isinstance(value, list):\n+                for item in value:\n+                    if isinstance(item, ast.AST):\n+                        ret = ret or self.visit(item)\n+            elif isinstance(value, ast.AST):\n+                ret = ret or self.visit(value)\n+        return ret\n+\n+    def visit_Attribute(self, node: ast.Attribute) -> bool:\n+        # If the left part is a name, it's possible that\n+        # we call triton native function or a jit function from another module.\n+        # If the left part is not a name, it must return a tensor or a constexpr\n+        # whose methods do not contain return statements\n+        # e.g., (tl.load(x)).to(y)\n+        # So we only check if the expressions within value have return or not\n+        if isinstance(node.value, ast.Name):\n+            if node.value.id in self.gscope:\n+                value = self.gscope[node.value.id]\n+                fn = getattr(value, node.attr)\n+                return self._visit_function(fn)\n+            return False\n+        return self.visit(node.value)\n+\n+    def visit_Name(self, node: ast.Name) -> bool:\n+        if type(node.ctx) == ast.Store:\n+            return False\n+        if node.id in self.gscope:\n+            fn = self.gscope[node.id]\n+            return self._visit_function(fn)\n+        return False\n+\n+    def visit_Return(self, node: ast.Return) -> bool:\n+        return True\n+\n+    def visit_Assign(self, node: ast.Assign) -> bool:\n+        # There couldn't be an early return\n+        # x = ...\n+        return False\n+\n+    def visit_AugAssign(self, node: ast.AugAssign) -> bool:\n+        # There couldn't be an early return\n+        # x += ...\n+        return False\n+\n+    def visit_Module(self, node: ast.Module) -> bool:\n+        return self._visit_stmts(node.body)\n+\n+    def visit_FunctionDef(self, node: ast.FunctionDef) -> bool:\n+        return self._visit_stmts(node.body)\n+\n+    def visit_If(self, node: ast.If) -> bool:\n+        # TODO: optimize the following case in which we actually don't have\n+        # a return when static_cond is false:\n+        # if dynamic_cond\n+        #   if static_cond\n+        #     func_with_return\n+        #   else\n+        #     func_without_return\n+        ret = self._visit_stmts(node.body)\n+        if node.orelse:\n+            ret = ret or self._visit_stmts(node.orelse)\n+        return ret\n+\n+    def visit_IfExp(self, node: ast.IfExp) -> bool:\n+        return self.visit(node.body) or self.visit(node.orelse)\n+\n+    def visit_Call(self, node: ast.Call) -> bool:\n+        return self.visit(node.func)\n+\n+\n class CodeGenerator(ast.NodeVisitor):\n     def __init__(self, context, prototype, gscope, attributes, constants, function_name,\n                  module=None, is_kernel=False, function_types: Optional[Dict] = None,\n@@ -166,63 +257,6 @@ def visit_compound_statement(self, stmts):\n             if ret_type is not None and isinstance(stmt, ast.Return):\n                 self.last_ret_type = ret_type\n \n-    # TODO: should be its own AST visitor\n-    def contains_return_op(self, node):\n-        if isinstance(node, ast.Return):\n-            return True\n-        elif isinstance(node, ast.Assign):\n-            return self.contains_return_op(node.value)\n-        elif isinstance(node, ast.Module):\n-            pred = lambda s: self.contains_return_op(s)\n-            return any(pred(s) for s in node.body)\n-        elif isinstance(node, ast.FunctionDef):\n-            pred = lambda s: self.contains_return_op(s)\n-            return any(pred(s) for s in node.body)\n-        elif isinstance(node, ast.Call):\n-            def check_undefined_name(cur_node):\n-                # Check if name is an undefined local variable,\n-                # which can only be a tensor or a constexpr\n-                if isinstance(cur_node.func, ast.Attribute):\n-                    if isinstance(cur_node.func.value, ast.Name):\n-                        name = cur_node.func.value.id\n-                        if name not in self.lscope and name not in self.gscope:\n-                            return True\n-                        return False\n-                    # chain of calls\n-                    # e.g., tl.load(a).to(tl.float32)\n-                    return check_undefined_name(cur_node.func.value)\n-                return False\n-            if check_undefined_name(node):\n-                return False\n-            fn = self.visit(node.func)\n-            if isinstance(fn, JITFunction) and fn.noinline is not True:\n-                old_gscope = self.gscope\n-                self.gscope = sys.modules[fn.fn.__module__].__dict__\n-                ret = self.contains_return_op(fn.parse())\n-                self.gscope = old_gscope\n-                return ret\n-            return False\n-        elif isinstance(node, ast.If):\n-            pred = lambda s: self.contains_return_op(s)\n-            ret = any(pred(s) for s in node.body)\n-            if node.orelse:\n-                ret = ret or any(pred(s) for s in node.orelse)\n-            return ret\n-        elif isinstance(node, ast.IfExp):\n-            return self.contains_return_op(node.body) or self.contains_return_op(node.orelse)\n-        elif isinstance(node, ast.Expr):\n-            ret = False\n-            for _, value in ast.iter_fields(node):\n-                if isinstance(value, list):\n-                    for item in value:\n-                        if isinstance(item, ast.AST):\n-                            ret = ret or self.contains_return_op(item)\n-                elif isinstance(value, ast.AST):\n-                    ret = ret or self.contains_return_op(value)\n-            return ret\n-        else:\n-            return False\n-\n     def visit_Module(self, node):\n         ast.NodeVisitor.generic_visit(self, node)\n \n@@ -526,7 +560,7 @@ def visit_If(self, node):\n         cond = self.visit(node.test)\n         if _is_triton_tensor(cond):\n             cond = cond.to(language.int1, _builder=self.builder)\n-            if self.scf_stack or not self.contains_return_op(node):\n+            if self.scf_stack or not ContainsReturnChecker(self.gscope).visit(node):\n                 self.visit_if_scf(cond, node)\n             else:\n                 self.visit_if_top_level(cond, node)"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 11, "deletions": 1, "changes": 12, "file_content_changes": "@@ -55,7 +55,17 @@ def _to_tensor(x, builder):\n         else:\n             raise RuntimeError(f'Nonrepresentable integer {x}.')\n     elif isinstance(x, float):\n-        return tensor(builder.get_fp32(x), float32)\n+        min_float32 = 2 ** -126\n+        max_float32 = (2 - 2**-23) * 2**127\n+        abs_x = __builtins__['abs'](x)\n+        if abs_x == float(\"inf\") or\\\n+           abs_x == 0.0 or \\\n+           x != x or \\\n+           min_float32 <= abs_x <= max_float32:\n+            return tensor(builder.get_fp32(x), float32)\n+        else:\n+            return tensor(builder.get_fp64(x), float64)\n+\n     elif isinstance(x, constexpr):\n         return _to_tensor(x.value, builder)\n     elif isinstance(x, tensor):"}, {"filename": "test/TritonGPU/dot-operands.mlir", "status": "modified", "additions": 91, "deletions": 19, "changes": 110, "file_content_changes": "@@ -6,8 +6,10 @@\n #Cv1 = #triton_gpu.mma<{versionMajor = 1, warpsPerCTA = [4, 1]}>\n #Av1 = #triton_gpu.dot_op<{opIdx = 0, parent = #Cv1}>\n #Bv1 = #triton_gpu.dot_op<{opIdx = 1, parent = #Cv1}>\n-#AL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n-#BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#ALR = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#ALC = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [0, 1]}>\n+#BLR = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#BLC = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [0, 1]}>\n \n // CHECK: tt.func @push_elementwise1\n // CHECK: %[[ALOAD:.*]] = tt.load %arg0\n@@ -17,36 +19,106 @@\n // CHECK: %[[C:.*]] = tt.dot %[[AF16]]\n // CHECK: tt.return %[[C]] : tensor<16x16xf32, #mma>\n tt.func @push_elementwise1(\n-                   %pa: tensor<16x16x!tt.ptr<i8>, #AL> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n-                   %pb: tensor<16x16x!tt.ptr<f16>, #BL> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %pa: tensor<16x16x!tt.ptr<i8>, #ALR> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %pb: tensor<16x16x!tt.ptr<f16>, #BLC> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n                    %c: tensor<16x16xf32, #Cv2>) -> tensor<16x16xf32, #Cv2>{\n-  %ai8 = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xi8, #AL>\n-  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BL>\n-  %af8 = tt.bitcast %ai8: tensor<16x16xi8, #AL> -> tensor<16x16xf8E5M2, #AL>\n-  %a = tt.fp_to_fp %af8: tensor<16x16xf8E5M2, #AL> -> tensor<16x16xf16, #AL>\n-  %dota = triton_gpu.convert_layout %a : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #Av2>\n-  %dotb = triton_gpu.convert_layout %b : (tensor<16x16xf16, #BL>) -> tensor<16x16xf16, #Bv2>\n+  %ai8 = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xi8, #ALR>\n+  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BLC>\n+  %af8 = tt.bitcast %ai8: tensor<16x16xi8, #ALR> -> tensor<16x16xf8E5M2, #ALR>\n+  %a = tt.fp_to_fp %af8: tensor<16x16xf8E5M2, #ALR> -> tensor<16x16xf16, #ALR>\n+  %dota = triton_gpu.convert_layout %a : (tensor<16x16xf16, #ALR>) -> tensor<16x16xf16, #Av2>\n+  %dotb = triton_gpu.convert_layout %b : (tensor<16x16xf16, #BLC>) -> tensor<16x16xf16, #Bv2>\n   %newc = tt.dot %dota, %dotb, %c {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #Av2> * tensor<16x16xf16, #Bv2> -> tensor<16x16xf32, #Cv2>\n   tt.return %newc : tensor<16x16xf32, #Cv2>\n }\n \n+\n+// Not modified for row-row\n // CHECK: tt.func @push_elementwise2\n // CHECK: %[[ALOAD:.*]] = tt.load %arg0\n // CHECK: %[[AF8E5:.*]] = tt.bitcast %[[ALOAD]]\n // CHECK: %[[AF16:.*]] = tt.fp_to_fp %[[AF8E5]]\n // CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[AF16]]\n // CHECK: %[[C:.*]] = tt.dot %[[ACVT]]\n-// CHECK: tt.return %[[C]] : tensor<16x16xf32, #mma1>\n+// CHECK: tt.return %[[C]] : tensor<16x16xf32, #mma>\n tt.func @push_elementwise2(\n-                   %pa: tensor<16x16x!tt.ptr<i8>, #AL> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n-                   %pb: tensor<16x16x!tt.ptr<f16>, #BL> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %pa: tensor<16x16x!tt.ptr<i8>, #ALR> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %pb: tensor<16x16x!tt.ptr<f16>, #BLR> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %c: tensor<16x16xf32, #Cv2>) -> tensor<16x16xf32, #Cv2>{\n+  %ai8 = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xi8, #ALR>\n+  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BLR>\n+  %af8 = tt.bitcast %ai8: tensor<16x16xi8, #ALR> -> tensor<16x16xf8E5M2, #ALR>\n+  %a = tt.fp_to_fp %af8: tensor<16x16xf8E5M2, #ALR> -> tensor<16x16xf16, #ALR>\n+  %dota = triton_gpu.convert_layout %a : (tensor<16x16xf16, #ALR>) -> tensor<16x16xf16, #Av2>\n+  %dotb = triton_gpu.convert_layout %b : (tensor<16x16xf16, #BLR>) -> tensor<16x16xf16, #Bv2>\n+  %newc = tt.dot %dota, %dotb, %c {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #Av2> * tensor<16x16xf16, #Bv2> -> tensor<16x16xf32, #Cv2>\n+  tt.return %newc : tensor<16x16xf32, #Cv2>\n+}\n+\n+\n+// Not modified for col-row\n+// CHECK: tt.func @push_elementwise3\n+// CHECK: %[[ALOAD:.*]] = tt.load %arg0\n+// CHECK: %[[AF8E5:.*]] = tt.bitcast %[[ALOAD]]\n+// CHECK: %[[AF16:.*]] = tt.fp_to_fp %[[AF8E5]]\n+// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[AF16]]\n+// CHECK: %[[C:.*]] = tt.dot %[[ACVT]]\n+// CHECK: tt.return %[[C]] : tensor<16x16xf32, #mma>\n+tt.func @push_elementwise3(\n+                   %pa: tensor<16x16x!tt.ptr<i8>, #ALC> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %pb: tensor<16x16x!tt.ptr<f16>, #BLR> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %c: tensor<16x16xf32, #Cv2>) -> tensor<16x16xf32, #Cv2>{\n+  %ai8 = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xi8, #ALC>\n+  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BLR>\n+  %af8 = tt.bitcast %ai8: tensor<16x16xi8, #ALC> -> tensor<16x16xf8E5M2, #ALC>\n+  %a = tt.fp_to_fp %af8: tensor<16x16xf8E5M2, #ALC> -> tensor<16x16xf16, #ALC>\n+  %dota = triton_gpu.convert_layout %a : (tensor<16x16xf16, #ALC>) -> tensor<16x16xf16, #Av2>\n+  %dotb = triton_gpu.convert_layout %b : (tensor<16x16xf16, #BLR>) -> tensor<16x16xf16, #Bv2>\n+  %newc = tt.dot %dota, %dotb, %c {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #Av2> * tensor<16x16xf16, #Bv2> -> tensor<16x16xf32, #Cv2>\n+  tt.return %newc : tensor<16x16xf32, #Cv2>\n+}\n+\n+// Not modified for col-col\n+// CHECK: tt.func @push_elementwise4\n+// CHECK: %[[ALOAD:.*]] = tt.load %arg0\n+// CHECK: %[[AF8E5:.*]] = tt.bitcast %[[ALOAD]]\n+// CHECK: %[[AF16:.*]] = tt.fp_to_fp %[[AF8E5]]\n+// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[AF16]]\n+// CHECK: %[[C:.*]] = tt.dot %[[ACVT]]\n+// CHECK: tt.return %[[C]] : tensor<16x16xf32, #mma>\n+tt.func @push_elementwise4(\n+                   %pa: tensor<16x16x!tt.ptr<i8>, #ALC> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %pb: tensor<16x16x!tt.ptr<f16>, #BLC> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %c: tensor<16x16xf32, #Cv2>) -> tensor<16x16xf32, #Cv2>{\n+  %ai8 = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xi8, #ALC>\n+  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BLC>\n+  %af8 = tt.bitcast %ai8: tensor<16x16xi8, #ALC> -> tensor<16x16xf8E5M2, #ALC>\n+  %a = tt.fp_to_fp %af8: tensor<16x16xf8E5M2, #ALC> -> tensor<16x16xf16, #ALC>\n+  %dota = triton_gpu.convert_layout %a : (tensor<16x16xf16, #ALC>) -> tensor<16x16xf16, #Av2>\n+  %dotb = triton_gpu.convert_layout %b : (tensor<16x16xf16, #BLC>) -> tensor<16x16xf16, #Bv2>\n+  %newc = tt.dot %dota, %dotb, %c {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #Av2> * tensor<16x16xf16, #Bv2> -> tensor<16x16xf32, #Cv2>\n+  tt.return %newc : tensor<16x16xf32, #Cv2>\n+}\n+\n+\n+// Not modified for Volta\n+// CHECK: tt.func @push_elementwise5\n+// CHECK: %[[ALOAD:.*]] = tt.load %arg0\n+// CHECK: %[[AF8E5:.*]] = tt.bitcast %[[ALOAD]]\n+// CHECK: %[[AF16:.*]] = tt.fp_to_fp %[[AF8E5]]\n+// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[AF16]]\n+// CHECK: %[[C:.*]] = tt.dot %[[ACVT]]\n+// CHECK: tt.return %[[C]] : tensor<16x16xf32, #mma1>\n+tt.func @push_elementwise5(\n+                   %pa: tensor<16x16x!tt.ptr<i8>, #ALR> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %pb: tensor<16x16x!tt.ptr<f16>, #BLC> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n                    %c: tensor<16x16xf32, #Cv1>) -> tensor<16x16xf32, #Cv1>{\n-  %ai8 = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xi8, #AL>\n-  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BL>\n-  %af8 = tt.bitcast %ai8: tensor<16x16xi8, #AL> -> tensor<16x16xf8E5M2, #AL>\n-  %a = tt.fp_to_fp %af8: tensor<16x16xf8E5M2, #AL> -> tensor<16x16xf16, #AL>\n-  %dota = triton_gpu.convert_layout %a : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #Av1>\n-  %dotb = triton_gpu.convert_layout %b : (tensor<16x16xf16, #BL>) -> tensor<16x16xf16, #Bv1>\n+  %ai8 = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xi8, #ALR>\n+  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BLC>\n+  %af8 = tt.bitcast %ai8: tensor<16x16xi8, #ALR> -> tensor<16x16xf8E5M2, #ALR>\n+  %a = tt.fp_to_fp %af8: tensor<16x16xf8E5M2, #ALR> -> tensor<16x16xf16, #ALR>\n+  %dota = triton_gpu.convert_layout %a : (tensor<16x16xf16, #ALR>) -> tensor<16x16xf16, #Av1>\n+  %dotb = triton_gpu.convert_layout %b : (tensor<16x16xf16, #BLC>) -> tensor<16x16xf16, #Bv1>\n   %newc = tt.dot %dota, %dotb, %c {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #Av1> * tensor<16x16xf16, #Bv1> -> tensor<16x16xf32, #Cv1>\n   tt.return %newc : tensor<16x16xf32, #Cv1>\n }"}]