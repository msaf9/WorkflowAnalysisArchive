[{"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "file_content_changes": "@@ -136,8 +136,14 @@ bool FpToFpOp::areCastCompatible(::mlir::TypeRange inputs,\n     srcEltType = srcTensorType.getElementType();\n     dstEltType = dstTensorType.getElementType();\n   }\n-  return srcEltType.isa<::mlir::FloatType>() &&\n-         dstEltType.isa<::mlir::FloatType>();\n+  // Check whether fp8 <=> fp16, bf16, f32, f64\n+  // Make `srcEltType` always the fp8 side\n+  if (dstEltType.dyn_cast<mlir::triton::Float8Type>())\n+    std::swap(srcEltType, dstEltType);\n+  if (!srcEltType.dyn_cast<mlir::triton::Float8Type>())\n+    return false;\n+  return dstEltType.isF16() || dstEltType.isBF16() || dstEltType.isF32() ||\n+         dstEltType.isF64();\n }\n \n //-- StoreOp --"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -1738,22 +1738,23 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n # test control flow\n # -----------------------\n \n+\n def test_for_iv_int64():\n-    \n+\n     @triton.jit\n     def kernel(Out, lo, hi):\n         acc = 0\n         acc = acc.to(tl.int64)\n         for i in range(lo, hi):\n             acc += i\n         tl.store(Out, acc)\n-   \n+\n     lo = 2**35\n     hi = 2**35 + 20\n     out = to_triton(np.zeros((1,), dtype=np.int64), device='cuda')\n     kernel[(1,)](out, lo, hi)\n     assert out[0] == sum(range(lo, hi))\n-        \n+\n \n def test_if_else():\n "}]