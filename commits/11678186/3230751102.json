[{"filename": ".gitignore", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -7,3 +7,6 @@ python/build/\n python/triton.egg-info/\n python/triton/_C/libtriton.pyd\n python/triton/_C/libtriton.so\n+\n+.vscode\n+.vs"}, {"filename": "CMakeLists.txt", "status": "modified", "additions": 83, "deletions": 69, "changes": 152, "file_content_changes": "@@ -3,6 +3,8 @@ include(ExternalProject)\n \n set(CMAKE_CXX_STANDARD 17)\n \n+set(CMAKE_INCLUDE_CURRENT_DIR ON)\n+\n if(NOT TRITON_LLVM_BUILD_DIR)\n     set(TRITON_LLVM_BUILD_DIR ${CMAKE_BINARY_DIR})\n endif()\n@@ -15,8 +17,8 @@ if(NOT WIN32)\n endif()\n \n # Options\n-option(BUILD_TUTORIALS \"Build C++ Triton tutorials\" ON)\n-option(BUILD_PYTHON_MODULE \"Build Python Triton bindings\" OFF)\n+option(TRITON_BUILD_TUTORIALS \"Build C++ Triton tutorials\" ON)\n+option(TRITON_BUILD_PYTHON_MODULE \"Build Python Triton bindings\" OFF)\n \n # Default build type\n if(NOT CMAKE_BUILD_TYPE)\n@@ -47,7 +49,8 @@ endif()\n ##########\n # LLVM\n ##########\n-if(\"${LLVM_LIBRARY_DIR}\" STREQUAL \"\")\n+if (NOT MLIR_DIR)\n+  if(NOT LLVM_LIBRARY_DIR)\n     if(WIN32)\n       find_package(LLVM 13 REQUIRED COMPONENTS nvptx amdgpu)\n \n@@ -66,80 +69,90 @@ if(\"${LLVM_LIBRARY_DIR}\" STREQUAL \"\")\n     if(APPLE)\n       set(CMAKE_OSX_DEPLOYMENT_TARGET \"10.14\")\n     endif()\n-# sometimes we don't want to use llvm-config, since it may have been downloaded for some specific linux distros\n-else()\n+  # sometimes we don't want to use llvm-config, since it may have been downloaded for some specific linux distros\n+  else()\n     set(LLVM_LDFLAGS \"-L${LLVM_LIBRARY_DIR}\")\n     set(LLVM_LIBRARIES\n-libLLVMNVPTXCodeGen.a\n-libLLVMNVPTXDesc.a\n-libLLVMNVPTXInfo.a\n-libLLVMAMDGPUDisassembler.a\n-libLLVMMCDisassembler.a\n-libLLVMAMDGPUCodeGen.a\n-libLLVMMIRParser.a\n-libLLVMGlobalISel.a\n-libLLVMSelectionDAG.a\n-libLLVMipo.a\n-libLLVMInstrumentation.a\n-libLLVMVectorize.a\n-libLLVMLinker.a\n-libLLVMIRReader.a\n-libLLVMAsmParser.a\n-libLLVMFrontendOpenMP.a\n-libLLVMAsmPrinter.a\n-libLLVMDebugInfoDWARF.a\n-libLLVMCodeGen.a\n-libLLVMTarget.a\n-libLLVMScalarOpts.a\n-libLLVMInstCombine.a\n-libLLVMAggressiveInstCombine.a\n-libLLVMTransformUtils.a\n-libLLVMBitWriter.a\n-libLLVMAnalysis.a\n-libLLVMProfileData.a\n-libLLVMObject.a\n-libLLVMTextAPI.a\n-libLLVMBitReader.a\n-libLLVMAMDGPUAsmParser.a\n-libLLVMMCParser.a\n-libLLVMAMDGPUDesc.a\n-libLLVMAMDGPUUtils.a\n-libLLVMMC.a\n-libLLVMDebugInfoCodeView.a\n-libLLVMDebugInfoMSF.a\n-libLLVMCore.a\n-libLLVMRemarks.a\n-libLLVMBitstreamReader.a\n-libLLVMBinaryFormat.a\n-libLLVMAMDGPUInfo.a\n-libLLVMSupport.a\n-libLLVMDemangle.a\n-libLLVMPasses.a\n-libLLVMAnalysis.a\n-libLLVMTransformUtils.a\n-libLLVMScalarOpts.a\n-libLLVMTransformUtils.a\n-libLLVMipo.a\n-libLLVMObjCARCOpts.a\n-libLLVMCoroutines.a\n-libLLVMAnalysis.a\n-)\n+      libLLVMNVPTXCodeGen.a\n+      libLLVMNVPTXDesc.a\n+      libLLVMNVPTXInfo.a\n+      libLLVMAMDGPUDisassembler.a\n+      libLLVMMCDisassembler.a\n+      libLLVMAMDGPUCodeGen.a\n+      libLLVMMIRParser.a\n+      libLLVMGlobalISel.a\n+      libLLVMSelectionDAG.a\n+      libLLVMipo.a\n+      libLLVMInstrumentation.a\n+      libLLVMVectorize.a\n+      libLLVMLinker.a\n+      libLLVMIRReader.a\n+      libLLVMAsmParser.a\n+      libLLVMFrontendOpenMP.a\n+      libLLVMAsmPrinter.a\n+      libLLVMDebugInfoDWARF.a\n+      libLLVMCodeGen.a\n+      libLLVMTarget.a\n+      libLLVMScalarOpts.a\n+      libLLVMInstCombine.a\n+      libLLVMAggressiveInstCombine.a\n+      libLLVMTransformUtils.a\n+      libLLVMBitWriter.a\n+      libLLVMAnalysis.a\n+      libLLVMProfileData.a\n+      libLLVMObject.a\n+      libLLVMTextAPI.a\n+      libLLVMBitReader.a\n+      libLLVMAMDGPUAsmParser.a\n+      libLLVMMCParser.a\n+      libLLVMAMDGPUDesc.a\n+      libLLVMAMDGPUUtils.a\n+      libLLVMMC.a\n+      libLLVMDebugInfoCodeView.a\n+      libLLVMDebugInfoMSF.a\n+      libLLVMCore.a\n+      libLLVMRemarks.a\n+      libLLVMBitstreamReader.a\n+      libLLVMBinaryFormat.a\n+      libLLVMAMDGPUInfo.a\n+      libLLVMSupport.a\n+      libLLVMDemangle.a\n+      libLLVMPasses.a\n+      libLLVMAnalysis.a\n+      libLLVMTransformUtils.a\n+      libLLVMScalarOpts.a\n+      libLLVMTransformUtils.a\n+      libLLVMipo.a\n+      libLLVMObjCARCOpts.a\n+      libLLVMCoroutines.a\n+      libLLVMAnalysis.a\n+    )\n+  endif()\n+  set (MLIR_DIR ${LLVM_LIBRARY_DIR}/cmake/mlir)\n endif()\n-include_directories(${LLVM_INCLUDE_DIRS})\n \n # Python module\n-if(BUILD_PYTHON_MODULE)\n+if(TRITON_BUILD_PYTHON_MODULE)\n     message(STATUS \"Adding Python module\")\n-    set(PYTHON_SRC_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/src)\n-    include_directories(\".\" ${PYTHON_SRC_PATH} ${PYTHON_INCLUDE_DIRS})\n-    link_directories(${PYTHON_LINK_DIRS})\n+    if (PYTHON_INCLUDE_DIRS)\n+      set(PYTHON_SRC_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/src)\n+      include_directories(\".\" ${PYTHON_SRC_PATH} ${PYTHON_INCLUDE_DIRS})\n+      link_directories(${PYTHON_LINK_DIRS})\n+    else()\n+      find_package(Python3 REQUIRED COMPONENTS Development)\n+      set(PYTHON_SRC_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/src)\n+      include_directories(\".\" ${PYTHON_SRC_PATH} ${Python3_INCLUDE_DIRS})\n+      link_directories(${Python3_LIBRARY_DIRS})\n+      link_libraries(${Python3_LIBRARIES})\n+      add_link_options(${Python3_LINK_OPTIONS})\n+    endif()\n     set(PYTHON_SRC ${PYTHON_SRC_PATH}/main.cc ${PYTHON_SRC_PATH}/triton.cc)\n endif()\n \n \n # # Triton\n # file(GLOB_RECURSE LIBTRITON_SRC lib/*.cc)\n-# if (WIN32 AND BUILD_PYTHON_MODULE)\n+# if (WIN32 AND TRITON_BUILD_PYTHON_MODULE)\n #     find_package(Python3 REQUIRED COMPONENTS Development)\n #     Python3_add_library(triton SHARED ${LIBTRITON_SRC} ${PYTHON_SRC})\n #     set_target_properties(triton PROPERTIES SUFFIX \".pyd\")\n@@ -150,17 +163,18 @@ endif()\n \n \n # MLIR\n-find_package(MLIR REQUIRED CONFIG PATHS ${LLVM_LIBRARY_DIR}/cmake/mlir)\n+find_package(MLIR REQUIRED CONFIG PATHS ${MLIR_DIR})\n \n-list(APPEND CMAKE_MODULE_PATH ${LLVM_LIBRARY_DIR}/cmake/llvm)\n-list(APPEND CMAKE_MODULE_PATH ${LLVM_LIBRARY_DIR}/cmake/mlir)\n+list(APPEND CMAKE_MODULE_PATH \"${MLIR_CMAKE_DIR}\")\n+list(APPEND CMAKE_MODULE_PATH \"${LLVM_CMAKE_DIR}\")\n \n include(TableGen) # required by AddMLIR\n include(AddLLVM)\n include(AddMLIR)\n # include(HandleLLVMOptions) # human-friendly error message\n \n include_directories(${MLIR_INCLUDE_DIRS})\n+include_directories(${LLVM_INCLUDE_DIRS})\n include_directories(${PROJECT_SOURCE_DIR}/include)\n include_directories(${PROJECT_BINARY_DIR}/include) # Tablegen'd files\n # link_directories(${LLVM_LIBRARY_DIR})\n@@ -209,7 +223,7 @@ else()\n endif()\n \n \n-if(BUILD_PYTHON_MODULE AND NOT WIN32)\n+if(TRITON_BUILD_PYTHON_MODULE AND NOT WIN32)\n     set(CMAKE_SHARED_LIBRARY_SUFFIX \".so\")\n     # Check if the platform is MacOS\n     if(APPLE)"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -8,6 +8,8 @@ std::unique_ptr<Pass> createTritonGPUPipelinePass(int numStages = 2);\n \n std::unique_ptr<Pass> createTritonGPUCanonicalizeLoopsPass();\n \n+std::unique_ptr<Pass> createTritonGPUSwizzlePass();\n+\n std::unique_ptr<Pass> createTritonGPUCoalescePass();\n \n std::unique_ptr<Pass> createTritonGPUCombineOpsPass();"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.td", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -51,6 +51,18 @@ def TritonGPUCombineOps : Pass<\"tritongpu-combine\", \"mlir::ModuleOp\"> {\n                            \"mlir::triton::TritonDialect\"];\n }\n \n+def TritonGPUSwizzle : Pass<\"tritongpu-swizzle\", \"mlir::ModuleOp\"> {\n+  let summary = \"swizzle\";\n+\n+  let description = [{\n+    Inserts conversions to swizzled layout so as to avoid shared memory bank conflicts.\n+  }];\n+\n+  let constructor = \"mlir::createTritonGPUSwizzlePass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\"];\n+}\n+\n def TritonGPUCanonicalizeLoops: Pass<\"tritongpu-canonicalize-loops\", \"mlir::ModuleOp\"> {\n   let summary = \"canonicalize scf.ForOp ops\";\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/PtxAsmFormat.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -145,7 +145,7 @@ std::string PTXInstrExecution::dump() const {\n     if (!pred->repr)\n       os << \"@\" << pred->dump() << \" \";\n     else\n-      os << pred->repr(pred->idx);\n+      os << pred->repr(pred->idx) << \" \";\n   }\n \n   std::string instrRepr = strJoin(instr->instrParts, \".\");"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 155, "deletions": 17, "changes": 172, "file_content_changes": "@@ -114,16 +114,16 @@ namespace type = mlir::triton::type;\n \n class TritonGPUToLLVMTypeConverter;\n \n-// TODO: keep these before we have better debug log utilities\n+// TODO[goostavz]: Remove these methods after we have better debug log utilities\n template <typename T>\n-void print_array(ArrayRef<T> array, const std::string &str) {\n-  std::cout << str << \": \";\n+void printArray(ArrayRef<T> array, const std::string &info) {\n+  std::cout << info << \": \";\n   for (const T &e : array)\n     std::cout << e << \",\";\n   std::cout << std::endl;\n }\n-template <typename T> void print_scalar(const T &e, const std::string &str) {\n-  std::cout << str << \": \" << e << std::endl;\n+template <typename T> void printScalar(const T &e, const std::string &info) {\n+  std::cout << info << \": \" << e << std::endl;\n }\n \n // FuncOpConversion/FuncOpConversionBase is borrowed from\n@@ -808,15 +808,23 @@ struct StoreOpConversion\n     auto valueElems = getLLVMElems(value, llValue, layout, rewriter, loc);\n     assert(ptrElems.size() == valueElems.size());\n \n+    // Determine the vectorization size\n+    size_t vec = getVectorizeSize(ptr, layout);\n     SmallVector<Value> maskElems;\n     if (llMask) {\n       maskElems = getLLVMElems(mask, llMask, layout, rewriter, loc);\n       assert(valueElems.size() == maskElems.size());\n+      auto maskOrder = mask.getType()\n+                           .cast<RankedTensorType>()\n+                           .getEncoding()\n+                           .cast<BlockedEncodingAttr>()\n+                           .getOrder();\n+\n+      auto maskAxis = getAxisInfo(mask);\n+      size_t maskAlign = std::max<int>(maskAxis->getConstancy(maskOrder[0]), 1);\n+      vec = std::min(vec, maskAlign);\n     }\n \n-    // Determine the vectorization size\n-    size_t vec = getVectorizeSize(ptr, layout);\n-\n     const size_t dtsize =\n         std::max<int>(1, valueElemTy.getIntOrFloatBitWidth() / 8);\n     const size_t valueElemNbits = dtsize * 8;\n@@ -1376,13 +1384,15 @@ struct ExtractSliceOpConversion\n   }\n };\n \n-template <typename SourceOp, typename DestOp>\n-class BinaryOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n+// A CRTP style of base class.\n+template <typename SourceOp, typename DestOp, typename ConcreteT>\n+class BinaryOpConversionBase\n+    : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n public:\n   using OpAdaptor = typename SourceOp::Adaptor;\n \n-  explicit BinaryOpConversion(LLVMTypeConverter &typeConverter,\n-                              PatternBenefit benefit = 1)\n+  explicit BinaryOpConversionBase(LLVMTypeConverter &typeConverter,\n+                                  PatternBenefit benefit = 1)\n       : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n \n   LogicalResult\n@@ -1403,20 +1413,140 @@ class BinaryOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n         this->getTypeConverter()->convertType(resultTy.getElementType());\n     SmallVector<Type> types(elems, elemTy);\n     Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n-    auto lhss =\n-        this->getElementsFromStruct(loc, adaptor.getLhs(), elems, rewriter);\n-    auto rhss =\n-        this->getElementsFromStruct(loc, adaptor.getRhs(), elems, rewriter);\n+\n+    auto *concreteThis = static_cast<const ConcreteT *>(this);\n+    auto lhss = this->getElementsFromStruct(loc, concreteThis->getLhs(adaptor),\n+                                            elems, rewriter);\n+    auto rhss = this->getElementsFromStruct(loc, concreteThis->getRhs(adaptor),\n+                                            elems, rewriter);\n     SmallVector<Value> resultVals(elems);\n     for (unsigned i = 0; i < elems; ++i) {\n-      resultVals[i] = rewriter.create<DestOp>(loc, elemTy, lhss[i], rhss[i]);\n+      resultVals[i] = concreteThis->createDestOp(op, rewriter, elemTy, lhss[i],\n+                                                 rhss[i], loc);\n     }\n     Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n     rewriter.replaceOp(op, view);\n     return success();\n   }\n };\n \n+template <typename SourceOp, typename DestOp>\n+struct BinaryOpConversion\n+    : public BinaryOpConversionBase<SourceOp, DestOp,\n+                                    BinaryOpConversion<SourceOp, DestOp>> {\n+\n+  explicit BinaryOpConversion(LLVMTypeConverter &typeConverter,\n+                              PatternBenefit benefit = 1)\n+      : BinaryOpConversionBase<SourceOp, DestOp,\n+                               BinaryOpConversion<SourceOp, DestOp>>(\n+            typeConverter, benefit) {}\n+\n+  using OpAdaptor = typename SourceOp::Adaptor;\n+  // An interface to support variant DestOp builder.\n+  DestOp createDestOp(SourceOp op, ConversionPatternRewriter &rewriter,\n+                      Type elemTy, Value lhs, Value rhs, Location loc) const {\n+    return rewriter.create<DestOp>(loc, elemTy, lhs, rhs);\n+  }\n+\n+  // Get the left operand of the op.\n+  Value getLhs(OpAdaptor adaptor) const { return adaptor.getLhs(); }\n+  // Get the right operand of the op.\n+  Value getRhs(OpAdaptor adaptor) const { return adaptor.getRhs(); }\n+};\n+\n+struct CmpIOpConversion\n+    : public BinaryOpConversionBase<triton::gpu::CmpIOp, LLVM::ICmpOp,\n+                                    CmpIOpConversion> {\n+  explicit CmpIOpConversion(LLVMTypeConverter &typeConverter,\n+                            PatternBenefit benefit = 1)\n+      : BinaryOpConversionBase(typeConverter, benefit) {}\n+\n+  // An interface to support variant DestOp builder.\n+  LLVM::ICmpOp createDestOp(triton::gpu::CmpIOp op,\n+                            ConversionPatternRewriter &rewriter, Type elemTy,\n+                            Value lhs, Value rhs, Location loc) const {\n+    return rewriter.create<LLVM::ICmpOp>(\n+        loc, elemTy, ArithCmpIPredicteToLLVM(op.predicate()), lhs, rhs);\n+  }\n+\n+  // Get the left operand of the op.\n+  Value getLhs(OpAdaptor adaptor) const { return adaptor.lhs(); }\n+  // Get the right operand of the op.\n+  Value getRhs(OpAdaptor adaptor) const { return adaptor.rhs(); }\n+\n+  static LLVM::ICmpPredicate\n+  ArithCmpIPredicteToLLVM(arith::CmpIPredicate predicate) {\n+    switch (predicate) {\n+#define __PRED_ENUM(item__)                                                    \\\n+  case arith::CmpIPredicate::item__:                                           \\\n+    return LLVM::ICmpPredicate::item__\n+\n+      __PRED_ENUM(eq);\n+      __PRED_ENUM(ne);\n+      __PRED_ENUM(sgt);\n+      __PRED_ENUM(sge);\n+      __PRED_ENUM(slt);\n+      __PRED_ENUM(sle);\n+      __PRED_ENUM(ugt);\n+      __PRED_ENUM(uge);\n+      __PRED_ENUM(ult);\n+      __PRED_ENUM(ule);\n+\n+#undef __PRED_ENUM\n+    }\n+    return LLVM::ICmpPredicate::eq;\n+  }\n+};\n+\n+struct CmpFOpConversion\n+    : public BinaryOpConversionBase<triton::gpu::CmpFOp, LLVM::FCmpOp,\n+                                    CmpFOpConversion> {\n+  explicit CmpFOpConversion(LLVMTypeConverter &typeConverter,\n+                            PatternBenefit benefit = 1)\n+      : BinaryOpConversionBase(typeConverter, benefit) {}\n+\n+  // An interface to support variant DestOp builder.\n+  LLVM::FCmpOp createDestOp(triton::gpu::CmpFOp op,\n+                            ConversionPatternRewriter &rewriter, Type elemTy,\n+                            Value lhs, Value rhs, Location loc) const {\n+    return rewriter.create<LLVM::FCmpOp>(\n+        loc, elemTy, ArithCmpFPredicteToLLVM(op.predicate()), lhs, rhs);\n+  }\n+\n+  // Get the left operand of the op.\n+  Value getLhs(OpAdaptor adaptor) const { return adaptor.lhs(); }\n+  // Get the right operand of the op.\n+  Value getRhs(OpAdaptor adaptor) const { return adaptor.rhs(); }\n+\n+  static LLVM::FCmpPredicate\n+  ArithCmpFPredicteToLLVM(arith::CmpFPredicate predicate) {\n+    switch (predicate) {\n+#define __PRED_ENUM(item__, item1__)                                           \\\n+  case arith::CmpFPredicate::item__:                                           \\\n+    return LLVM::FCmpPredicate::item1__\n+\n+      __PRED_ENUM(OEQ, oeq);\n+      __PRED_ENUM(ONE, one);\n+      __PRED_ENUM(OGT, ogt);\n+      __PRED_ENUM(OGE, oge);\n+      __PRED_ENUM(OLT, olt);\n+      __PRED_ENUM(OLE, ole);\n+      __PRED_ENUM(ORD, ord);\n+      __PRED_ENUM(UEQ, ueq);\n+      __PRED_ENUM(UGT, ugt);\n+      __PRED_ENUM(ULT, ult);\n+      __PRED_ENUM(ULE, ule);\n+      __PRED_ENUM(UNE, une);\n+      __PRED_ENUM(UNO, uno);\n+      __PRED_ENUM(AlwaysTrue, _true);\n+      __PRED_ENUM(AlwaysFalse, _false);\n+\n+#undef __PRED_ENUM\n+    }\n+    return LLVM::FCmpPredicate::_true;\n+  }\n+};\n+\n struct ConvertLayoutOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::ConvertLayoutOp> {\n public:\n@@ -3011,6 +3141,14 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                                                benefit);\n   patterns.add<BinaryOpConversion<arith::MulFOp, LLVM::FMulOp>>(typeConverter,\n                                                                 benefit);\n+\n+  patterns.add<BinaryOpConversion<arith::AndIOp, LLVM::AndOp>>(typeConverter,\n+                                                               benefit);\n+  patterns.add<BinaryOpConversion<arith::OrIOp, LLVM::OrOp>>(typeConverter,\n+                                                             benefit);\n+\n+  patterns.add<CmpIOpConversion>(typeConverter, benefit);\n+  patterns.add<CmpFOpConversion>(typeConverter, benefit);\n   patterns.add<BroadcastOpConversion>(typeConverter, benefit);\n   patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n                                           benefit);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -7,6 +7,7 @@ add_mlir_dialect_library(TritonGPUTransforms\n   CanonicalizeLoops.cpp\n   Combine.cpp\n   Pipeline.cpp\n+  Swizzle.cpp\n   Verifier.cpp\n   TritonGPUConversion.cpp\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/Swizzle.cpp", "status": "added", "additions": 105, "deletions": 0, "changes": 105, "file_content_changes": "@@ -0,0 +1,105 @@\n+#include \"mlir/Analysis/SliceAnalysis.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n+\n+namespace {\n+\n+struct SwizzlePass : public TritonGPUSwizzleBase<SwizzlePass> {\n+  SwizzlePass() = default;\n+\n+  struct SwizzleInfo {\n+    int vec;\n+    int perPhase;\n+    int maxPhase;\n+  };\n+\n+  SwizzleInfo getSwizzleMMA(int opIdx, triton::gpu::MmaEncodingAttr retEncoding,\n+                            RankedTensorType ty) {\n+    SwizzleInfo noSwizzling = {1, 1, 1};\n+    int version = retEncoding.getVersion();\n+    auto tyEncoding = ty.getEncoding().cast<triton::gpu::SharedEncodingAttr>();\n+    auto order = tyEncoding.getOrder();\n+    // number of rows per phase\n+    int perPhase = 128 / (ty.getShape()[order[0]] *\n+                          (ty.getElementType().getIntOrFloatBitWidth() / 8));\n+    perPhase = std::max<int>(perPhase, 1);\n+    // index of the inner dimension in `order`\n+    int inner = (opIdx == 0) ? 0 : 1;\n+    if (version == 1) {\n+      int maxPhase = (order[inner] == 1 ? 8 : 4) / perPhase;\n+      // TODO: handle rep (see\n+      // https://github.com/openai/triton/blob/master/lib/codegen/analysis/layout.cc#L209)\n+      int vec = 1;\n+      return SwizzleInfo{vec, perPhase, maxPhase};\n+    } else if (version == 2) {\n+      auto eltTy = ty.getElementType();\n+      std::vector<size_t> mat_shape = {8, 8,\n+                                       2 * 64 / eltTy.getIntOrFloatBitWidth()};\n+      // for now, disable swizzle when using transposed int8 tensor cores\n+      bool is_int8_mma = ty.getElementType().isInteger(8);\n+      if (is_int8_mma && order[0] == inner)\n+        return noSwizzling;\n+      // compute swizzling for A operand\n+      if (opIdx == 0) {\n+        int vec = order[0] == 1 ? mat_shape[2] : mat_shape[0]; // k : m\n+        int mmaStride = order[0] == 1 ? mat_shape[0] : mat_shape[2];\n+        int maxPhase = mmaStride / perPhase;\n+        std::cout << perPhase << \" \" << mat_shape[0] << \" \" << mat_shape[1]\n+                  << \" \" << mat_shape[2] << std::endl;\n+        return SwizzleInfo{vec, perPhase, maxPhase};\n+      }\n+      // compute swizzling for B operand\n+      else if (opIdx == 1) {\n+        int vec = order[0] == 1 ? mat_shape[1] : mat_shape[2]; // n : k\n+        int mmaStride = order[0] == 1 ? mat_shape[2] : mat_shape[1];\n+        int maxPhase = mmaStride / perPhase;\n+        return SwizzleInfo{vec, perPhase, maxPhase};\n+      } else {\n+        llvm_unreachable(\"invalid operand index\");\n+      }\n+    } else\n+      llvm_unreachable(\"unsupported swizzling for provided MMA version\");\n+  }\n+\n+  void runOnOperation() override {\n+    Operation *op = getOperation();\n+    MLIRContext *context = &getContext();\n+    op->walk([&](triton::DotOp dotOp) -> void {\n+      OpBuilder builder(dotOp);\n+      auto _retEncoding =\n+          dotOp.getResult().getType().cast<RankedTensorType>().getEncoding();\n+      auto retEncoding = _retEncoding.dyn_cast<triton::gpu::MmaEncodingAttr>();\n+      if (!retEncoding)\n+        return;\n+      for (int opIdx : {0, 1}) {\n+        Value op = dotOp.getOperand(opIdx);\n+        auto ty = op.getType().template cast<RankedTensorType>();\n+        // compute new swizzled encoding\n+        SwizzleInfo swizzle = getSwizzleMMA(opIdx, retEncoding, ty);\n+        auto newEncoding = triton::gpu::SharedEncodingAttr::get(\n+            &getContext(), swizzle.vec, swizzle.perPhase, swizzle.maxPhase,\n+            ty.getEncoding()\n+                .cast<triton::gpu::SharedEncodingAttr>()\n+                .getOrder());\n+        // create conversion\n+        auto newType = RankedTensorType::get(ty.getShape(), ty.getElementType(),\n+                                             newEncoding);\n+        Operation *newOp = builder.create<triton::gpu::ConvertLayoutOp>(\n+            op.getLoc(), newType, op);\n+        // bind new op to dot operand\n+        dotOp->replaceUsesOfWith(op, newOp->getResult(0));\n+      }\n+    });\n+  }\n+};\n+} // anonymous namespace\n+\n+std::unique_ptr<Pass> mlir::createTritonGPUSwizzlePass() {\n+  return std::make_unique<SwizzlePass>();\n+}\n\\ No newline at end of file"}, {"filename": "python/setup.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -97,8 +97,8 @@ def build_extension(self, ext):\n         python_include_dirs = [distutils.sysconfig.get_python_inc()] + ['/usr/local/cuda/include']\n         cmake_args = [\n             \"-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=\" + extdir,\n-            \"-DBUILD_TUTORIALS=OFF\",\n-            \"-DBUILD_PYTHON_MODULE=ON\",\n+            \"-DTRITON_BUILD_TUTORIALS=OFF\",\n+            \"-DTRITON_BUILD_PYTHON_MODULE=ON\",\n             \"-DLLVM_INCLUDE_DIRS=\" + llvm_include_dir,\n             \"-DLLVM_LIBRARY_DIR=\" + llvm_library_dir,\n             # '-DPYTHON_EXECUTABLE=' + sys.executable,"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -1210,6 +1210,8 @@ void init_triton_translation(py::module &m) {\n         llvm::LLVMContext llvmContext;\n         auto llvmModule =\n             ::mlir::triton::translateTritonGPUToLLVMIR(&llvmContext, op);\n+        if (!llvmModule)\n+          llvm::report_fatal_error(\"Failed to translate TritonGPU to LLVM IR.\");\n \n         std::string str;\n         llvm::raw_string_ostream os(str);"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,6 +1,6 @@\n import pytest\n import torch\n-from torch.testing import assert_allclose\n+from torch.testing import assert_close\n \n import triton\n import triton.language as tl\n@@ -49,4 +49,4 @@ def test_gemm_impl(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n                         num_warps=NUM_WARPS)\n     golden = torch.matmul(a, b)\n     torch.set_printoptions(profile=\"full\")\n-    assert_allclose(c, golden, rtol=1e-3, atol=1e-3)\n+    assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)"}, {"filename": "python/tests/test_transpose.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,6 +1,6 @@\n import pytest\n import torch\n-from torch.testing import assert_allclose\n+from torch.testing import assert_close\n \n import triton\n import triton.language as tl\n@@ -44,4 +44,4 @@ def test_convert_layout_impl(NUM_WARPS, SIZE_M, SIZE_N):\n     z = torch.empty((SIZE_N, SIZE_M), device=x.device, dtype=x.dtype)\n     kernel[grid](x_ptr=x, stride_xm=x.stride(0), z_ptr=z, stride_zn=z.stride(0), SIZE_M=SIZE_M, SIZE_N=SIZE_N, num_warps=NUM_WARPS)\n     golden_z = torch.t(x)\n-    assert_allclose(z, golden_z, rtol=1e-7, atol=1e-7)\n+    assert_close(z, golden_z, rtol=1e-7, atol=1e-7, check_dtype=False)"}, {"filename": "python/tests/test_vecadd.py", "status": "modified", "additions": 179, "deletions": 43, "changes": 222, "file_content_changes": "@@ -1,79 +1,215 @@\n+import math\n+import random\n+\n import pytest\n import torch\n-from torch.testing import assert_allclose\n+from torch.testing import assert_close\n \n import triton\n import triton.language as tl\n \n \n-@pytest.mark.parametrize('NUM_WARPS, BLOCK_SIZE', [\n-    [4, 256],\n-    [2, 256],\n-    [1, 256],\n+@pytest.mark.parametrize('num_warps, block_size, iter_size', [\n+    [4, 256, 1],\n+    [4, 1024, 256],\n ])\n-def test_vecadd_no_mask(NUM_WARPS, BLOCK_SIZE):\n+def test_vecadd_scf_no_mask(num_warps, block_size, iter_size):\n \n     @triton.jit\n     def kernel(x_ptr,\n                y_ptr,\n                z_ptr,\n-               BLOCK_SIZE: tl.constexpr):\n+               block_size,\n+               iter_size: tl.constexpr):\n         pid = tl.program_id(axis=0)\n-        offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n-        x_ptrs = x_ptr + offset\n-        y_ptrs = y_ptr + offset\n-        x = tl.load(x_ptrs)\n-        y = tl.load(y_ptrs)\n-        z = x + y\n-        z_ptrs = z_ptr + offset\n-        tl.store(z_ptrs, z)\n+        for i in range(0, block_size, iter_size):\n+            offset = pid * block_size + tl.arange(0, iter_size)\n+            x_ptrs = x_ptr + offset\n+            y_ptrs = y_ptr + offset\n \n-    x = torch.randn((BLOCK_SIZE,), device='cuda', dtype=torch.float32)\n-    y = torch.randn((BLOCK_SIZE,), device='cuda', dtype=torch.float32)\n-    z = torch.empty((BLOCK_SIZE,), device=x.device, dtype=x.dtype)\n+            x = tl.load(x_ptrs)\n+            y = tl.load(y_ptrs)\n+            z = x + y\n+            z_ptrs = z_ptr + offset\n+            tl.store(z_ptrs, z)\n \n-    grid = lambda EA: (x.shape.numel() // BLOCK_SIZE,)\n-    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z, BLOCK_SIZE=BLOCK_SIZE, num_warps=NUM_WARPS)\n+            x_ptr += iter_size\n+            y_ptr += iter_size\n+            z_ptr += iter_size\n+\n+    x = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n+    y = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n+    z = torch.empty((block_size,), device=x.device, dtype=x.dtype)\n+\n+    grid = lambda EA: (x.shape.numel() // (block_size),)\n+    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z,\n+                 block_size=x.shape[0], iter_size=iter_size, num_warps=num_warps)\n \n     golden_z = x + y\n-    assert_allclose(z, golden_z, rtol=1e-7, atol=1e-7)\n+    assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n \n \n-@pytest.mark.parametrize('NUM_WARPS, BLOCK_SIZE, ITER_SIZE', [\n-    [4, 256, 1],\n-    [4, 1024, 256],\n+@pytest.mark.parametrize('shape, num_warps, block_size, iter_size', [\n+    [(127, 3), 2, 128, 1],\n+    [(127, 3), 2, 128, 32],\n ])\n-def test_vecadd_scf_no_mask(NUM_WARPS, BLOCK_SIZE, ITER_SIZE):\n-\n+def test_vecadd_scf_mask(shape, num_warps, block_size, iter_size):\n     @triton.jit\n     def kernel(x_ptr,\n                y_ptr,\n                z_ptr,\n-               BLOCK_SIZE,\n-               ITER_SIZE: tl.constexpr):\n+               num_elements,\n+               block_size: tl.constexpr,\n+               iter_size: tl.constexpr\n+               ):\n+        '''\n+        @block_size: size of a block\n+        @iter_size: size of the iteration, a block has multiple iterations\n+        @num_elements: number of elements\n+        '''\n         pid = tl.program_id(axis=0)\n-        for i in range(0, BLOCK_SIZE, ITER_SIZE):\n-            offset = pid * BLOCK_SIZE + tl.arange(0, ITER_SIZE)\n+        for i in range(math.ceil(block_size / iter_size)):\n+            # TODO: a bug here, if put the offset outside the forloop, there will be a GPU mis-aligned error.\n+            offset = pid * block_size + tl.arange(0, iter_size)\n             x_ptrs = x_ptr + offset\n             y_ptrs = y_ptr + offset\n-            x = tl.load(x_ptrs)\n-            y = tl.load(y_ptrs)\n+\n+            x = tl.load(x_ptrs, mask=offset < num_elements)\n+            y = tl.load(y_ptrs, mask=offset < num_elements)\n             z = x + y\n             z_ptrs = z_ptr + offset\n-            tl.store(z_ptrs, z)\n-            x_ptr += ITER_SIZE\n-            y_ptr += ITER_SIZE\n-            z_ptr += ITER_SIZE\n+            tl.store(z_ptrs, z, mask=offset < num_elements)\n+\n+            x_ptr += iter_size\n+            y_ptr += iter_size\n+            z_ptr += iter_size\n \n-    x = torch.randn((BLOCK_SIZE,), device='cuda', dtype=torch.float32)\n-    y = torch.randn((BLOCK_SIZE,), device='cuda', dtype=torch.float32)\n-    z = torch.empty((BLOCK_SIZE,), device=x.device, dtype=x.dtype)\n+    x = torch.randn(shape, device='cuda', dtype=torch.float32)\n+    y = torch.randn(shape, device='cuda', dtype=torch.float32)\n+    z = torch.empty(shape, device=x.device, dtype=x.dtype)\n \n-    grid = lambda EA: (x.shape.numel() // (BLOCK_SIZE),)\n+    grid = lambda EA: (math.ceil(x.numel() / block_size),)\n     kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z,\n-                 BLOCK_SIZE=x.shape[0], ITER_SIZE=ITER_SIZE, num_warps=NUM_WARPS)\n+                 block_size=x.shape[0], iter_size=iter_size, num_warps=num_warps,\n+                 num_elements=x.numel())\n+\n+    golden_z = x + y\n+    assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n+\n+\n+def vecadd_no_scf_tester(num_warps, block_size, shape):\n+    @triton.jit\n+    def kernel(x_ptr,\n+               y_ptr,\n+               z_ptr,\n+               n_elements,\n+               block_size_N: tl.constexpr):\n+        pid = tl.program_id(axis=0)\n+\n+        offset = pid * block_size_N + tl.arange(0, block_size_N)\n+        x_ptrs = x_ptr + offset\n+        y_ptrs = y_ptr + offset\n+\n+        mask = offset < n_elements\n+\n+        x = tl.load(x_ptrs, mask=mask)\n+        y = tl.load(y_ptrs, mask=mask)\n+        z = x + y\n+        z_ptrs = z_ptr + offset\n+        tl.store(z_ptrs, z, mask=mask)\n+\n+    x = torch.randn(shape, device='cuda', dtype=torch.float32)\n+    y = torch.randn(shape, device='cuda', dtype=torch.float32)\n+    z = torch.empty(shape, device=x.device, dtype=x.dtype)\n+\n+    grid = lambda EA: (math.ceil(x.shape.numel() / block_size),)\n+    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z, n_elements=x.shape.numel(), block_size_N=block_size, num_warps=num_warps)\n \n     golden_z = x + y\n-    assert_allclose(z, golden_z, rtol=1e-7, atol=1e-7)\n+    assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n+\n+\n+def vecadd_fcmp_no_scf_tester(num_warps, block_size, shape):\n+    '''\n+    vecadd tester with float comparation as load/store mask.\n+    '''\n+    @triton.jit\n+    def kernel(x_ptr,\n+               y_ptr,\n+               z_ptr,\n+               n_elements,\n+               block_size_N: tl.constexpr):\n+        pid = tl.program_id(axis=0)\n+\n+        offset = pid * block_size_N + tl.arange(0, block_size_N)\n+        x_ptrs = x_ptr + offset\n+        y_ptrs = y_ptr + offset\n+\n+        io_mask = offset < n_elements\n+        x = tl.load(x_ptrs, mask=io_mask)\n+        y = tl.load(y_ptrs, mask=io_mask)\n \n-# TODO: test_vecadd with mask\n+        z = x + y\n+        val_mask = offset < n_elements and (z < 0. or z > 1.)\n+\n+        z_ptrs = z_ptr + offset\n+        tl.store(z_ptrs, z, mask=val_mask)\n+\n+    x = torch.randn(shape, device='cuda', dtype=torch.float32)\n+    y = torch.randn(shape, device='cuda', dtype=torch.float32)\n+    z = torch.zeros(shape, device=x.device, dtype=x.dtype)\n+\n+    grid = lambda EA: (math.ceil(x.shape.numel() / block_size),)\n+    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z, n_elements=x.shape.numel(), block_size_N=block_size, num_warps=num_warps)\n+\n+    golden_z: torch.Tensor = x + y\n+    gz_data = torch.flatten(golden_z)\n+    for i in range(golden_z.numel()):\n+        gz_data[i] = gz_data[i] if gz_data[i] < 0. or gz_data[i] > 1. else 0.\n+\n+    assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n+\n+\n+@pytest.mark.parametrize('num_warps, block_size, shape', [\n+    [4, 256, (256,)],\n+    [2, 256, (256,)],\n+    [1, 256, (256,)],\n+    [4, 16, (256,)],\n+    [2, 64, (256,)],\n+    [1, 128, (256,)],\n+])\n+def test_vecadd_no_scf(num_warps, block_size, shape):\n+    vecadd_no_scf_tester(num_warps, block_size, shape)\n+\n+\n+@pytest.mark.parametrize('num_warps, block_size, shape', [\n+    [1, 128, (256 + 1,)],\n+    [1, 256, (256 + 1,)],\n+    [2, 256, (3, 256 + 7)],\n+    [4, 256, (3, 256 + 7)],\n+])\n+def test_vecadd__no_scf_masked(num_warps, block_size, shape):\n+    vecadd_no_scf_tester(num_warps, block_size, shape)\n+\n+\n+def test_vecadd_no_scf_masked_randomly():\n+    random.seed(0)  # fix seed to make random test reproducible\n+    for i in range(10):\n+        num_elements = random.randint(128, 2048)\n+        shape = (num_elements,)\n+        max_warps = num_elements // 32  # floor div\n+        for num_warps in range(1, max_warps):\n+            is_power2 = num_warps & (num_warps - 1) == 0 and num_warps != 0\n+            if not is_power2: continue\n+            block_size = min(32, num_warps * 32)\n+            vecadd_no_scf_tester(num_warps, block_size, shape)\n+\n+\n+@pytest.mark.parametrize('num_warps, block_size, shape', [\n+    [1, 128, (256 + 1,)],\n+    [1, 256, (256 + 1,)],\n+    [2, 256, (3, 256 + 7)],\n+    [4, 256, (3, 256 + 7)],\n+])\n+def test_vecadd_fcmp_no_scf_masked(num_warps, block_size, shape):\n+    vecadd_fcmp_no_scf_tester(num_warps, block_size, shape)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 22, "deletions": 0, "changes": 22, "file_content_changes": "@@ -699,6 +699,28 @@ def visit_Call(self, node):\n     def visit_Constant(self, node):\n         return triton.language.constexpr(node.value)\n \n+    def visit_BoolOp(self, node: ast.BoolOp):\n+        assert len(node.values) == 2\n+        lhs = self.visit(node.values[0])\n+        rhs = self.visit(node.values[1])\n+        if isinstance(lhs, triton.language.constexpr):\n+            lhs = lhs.value\n+        if isinstance(rhs, triton.language.constexpr):\n+            rhs = rhs.value\n+\n+        fn = {\n+            ast.And: 'logical_and',\n+            ast.Or: 'logical_or',\n+        }[type(node.op)]\n+\n+        if self.is_triton_tensor(lhs):\n+            return getattr(lhs, fn)(rhs, _builder=self.builder)\n+        elif self.is_triton_tensor(rhs):\n+            fn = fn[:2] + 'r' + fn[2:]\n+            return getattr(rhs, fn)(lhs, _builder=self.builder)\n+        else:\n+            return getattr(lhs, fn)(rhs)\n+\n     if sys.version_info < (3, 8):\n         def visit_NameConstant(self, node):\n             return triton.language.constexpr(node.value)"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 10, "deletions": 2, "changes": 12, "file_content_changes": "@@ -361,8 +361,6 @@ def __floordiv__(self, other):\n     def __rfloordiv__(self, other):\n         return other.value // self.value\n \n-    #\n-\n     def __gt__(self, other):\n         return self.value > other.value\n \n@@ -557,6 +555,16 @@ def __ne__(self, other, _builder=None):\n         other = _to_tensor(other, _builder)\n         return semantic.not_equal(self, other, _builder)\n \n+    @builtin\n+    def logical_and(self, other, _builder=None):\n+        other = _to_tensor(other, _builder)\n+        return semantic.logical_and(self, other, _builder)\n+\n+    @builtin\n+    def logical_or(self, other, _builder=None):\n+        other = _to_tensor(other, _builder)\n+        return semantic.logical_or(self, other, _builder)\n+\n     @builtin\n     def __getitem__(self, slices, _builder=None):\n         if isinstance(slices, slice):"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -285,6 +285,22 @@ def xor_(input: tl.tensor,\n     return tl.tensor(builder.create_xor(input.handle, other.handle), input.type)\n \n \n+def logical_and(input: tl.tensor, other: tl.tensor, builder: ir.builder) -> tl.tensor:\n+    if not input.type.is_int1():\n+        input = bitcast(input, tl.dtype(\"int1\"), builder)\n+    if not other.type.is_int1():\n+        other = bitcast(other, tl.dtype(\"int1\"), builder)\n+    return and_(input, other, builder)\n+\n+\n+def logical_or(input: tl.tensor, other: tl.tensor, builder: ir.builder) -> tl.tensor:\n+    if not input.type.is_int1():\n+        input = bitcast(input, tl.dtype(\"int1\"), builder)\n+    if not other.type.is_int1():\n+        other = bitcast(other, tl.dtype(\"int1\"), builder)\n+    return or_(input, other, builder)\n+\n+\n def lshr(input: tl.tensor,\n          other: tl.tensor,\n          builder: ir.builder) -> tl.tensor:"}, {"filename": "test/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -18,7 +18,7 @@ set(TRITON_TEST_DEPENDS\n \n add_lit_testsuite(check-triton \"Running the triton regression tests\"\n   ${CMAKE_CURRENT_BINARY_DIR}\n-  DEPENDS ${STANDALONE_TEST_DEPENDS}\n+  DEPENDS ${TRITON_TEST_DEPENDS}\n   )\n set_target_properties(check-triton PROPERTIES FOLDER \"Tests\")\n "}, {"filename": "test/TritonGPU/swizzle.mlir", "status": "added", "additions": 71, "deletions": 0, "changes": 71, "file_content_changes": "@@ -0,0 +1,71 @@\n+// RUN: triton-opt %s -split-input-file -tritongpu-swizzle | FileCheck %s\n+\n+#shared = #triton_gpu.shared<{vec=1, perPhase=1, maxPhase=1 ,order = [1, 0]}>\n+#mma1w = #triton_gpu.mma<{version=2, warpsPerCTA=[1, 1]}>\n+#mma2w = #triton_gpu.mma<{version=2, warpsPerCTA=[1, 2]}>\n+#mma4w = #triton_gpu.mma<{version=2, warpsPerCTA=[2, 2]}>\n+#mma8w = #triton_gpu.mma<{version=2, warpsPerCTA=[2, 4]}>\n+\n+// CHECK: [[shared_v8p1m8:#.*]] = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 8, order = [1, 0]}>\n+// CHECK: [[shared_v8p2m4:#.*]] = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+// CHECK: [[shared_v8p4m2:#.*]] = #triton_gpu.shared<{vec = 8, perPhase = 4, maxPhase = 2, order = [1, 0]}>\n+\n+#shared2 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#shared3 = #triton_gpu.shared<{vec = 8, perPhase = 4, maxPhase = 2, order = [1, 0]}>\n+\n+\n+module attributes {\"triton_gpu.num-warps\" = 8 : i32} {\n+  // CHECK-LABEL: swizzle_mma_f16_128x256x64_w8\n+  func @swizzle_mma_f16_128x256x64_w8(%A: tensor<128x64xf16, #shared>, %B: tensor<64x256xf16, #shared>) {\n+    %cst0 = arith.constant dense<0.000000e+00> : tensor<128x256xf32, #mma8w>\n+    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<128x64xf16, {{.*}}>) -> tensor<128x64xf16, [[shared_v8p1m8]]>\n+    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<64x256xf16, {{.*}}>) -> tensor<64x256xf16, [[shared_v8p1m8]]>\n+    %D = tt.dot %A, %B, %cst0 {allowTF32 = true} : tensor<128x64xf16, #shared> * tensor<64x256xf16, #shared> -> tensor<128x256xf32, #mma8w>\n+    return\n+  }\n+}\n+\n+\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: swizzle_mma_f16_128x128x64_w4\n+  func @swizzle_mma_f16_128x128x64_w4(%A: tensor<128x64xf16, #shared>, %B: tensor<64x128xf16, #shared>) {\n+    %cst0 = arith.constant dense<0.000000e+00> : tensor<128x128xf32, #mma4w>\n+    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<128x64xf16, {{.*}}>) -> tensor<128x64xf16, [[shared_v8p1m8]]>\n+    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<64x128xf16, {{.*}}>) -> tensor<64x128xf16, [[shared_v8p1m8]]>\n+    %D = tt.dot %A, %B, %cst0 {allowTF32 = true} : tensor<128x64xf16, #shared> * tensor<64x128xf16, #shared> -> tensor<128x128xf32, #mma4w>\n+    return\n+  }\n+}\n+\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: swizzle_mma_f16_128x128x32_w4\n+  func @swizzle_mma_f16_128x128x32_w4(%A: tensor<128x32xf16, #shared>, %B: tensor<32x128xf16, #shared>) {\n+    %cst0 = arith.constant dense<0.000000e+00> : tensor<128x128xf32, #mma4w>\n+    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<128x32xf16, {{.*}}>) -> tensor<128x32xf16, [[shared_v8p2m4]]>\n+    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<32x128xf16, {{.*}}>) -> tensor<32x128xf16, [[shared_v8p1m8]]>\n+    %D = tt.dot %A, %B, %cst0 {allowTF32 = true} : tensor<128x32xf16, #shared> * tensor<32x128xf16, #shared> -> tensor<128x128xf32, #mma4w>\n+    return\n+  }\n+}\n+\n+module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n+  // CHECK-LABEL: swizzle_mma_f16_32x32x32_w2\n+  func @swizzle_mma_f16_32x32x32_w2(%A: tensor<32x32xf16, #shared>, %B: tensor<32x32xf16, #shared>) {\n+    %cst0 = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma2w>\n+    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<32x32xf16, {{.*}}>) -> tensor<32x32xf16, [[shared_v8p2m4]]>\n+    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<32x32xf16, {{.*}}>) -> tensor<32x32xf16, [[shared_v8p2m4]]>\n+    %D = tt.dot %A, %B, %cst0 {allowTF32 = true} : tensor<32x32xf16, #shared> * tensor<32x32xf16, #shared> -> tensor<32x32xf32, #mma2w>\n+    return\n+  }\n+}\n+\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: swizzle_mma_f16_16x16x16_w1\n+  func @swizzle_mma_f16_16x16x16_w1(%A: tensor<16x16xf16, #shared>, %B: tensor<16x16xf16, #shared>) {\n+    %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #mma1w>\n+    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<16x16xf16, {{.*}}>) -> tensor<16x16xf16, [[shared_v8p4m2]]>\n+    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<16x16xf16, {{.*}}>) -> tensor<16x16xf16, [[shared_v8p4m2]]>\n+    %D = tt.dot %A, %B, %cst0 {allowTF32 = true} : tensor<16x16xf16, #shared> * tensor<16x16xf16, #shared> -> tensor<16x16xf32, #mma1w>\n+    return\n+  }\n+}"}]