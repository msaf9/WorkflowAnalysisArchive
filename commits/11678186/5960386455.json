[{"filename": "python/triton/runtime/backends/cuda.c", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -235,7 +235,7 @@ static PyObject *loadBinary(PyObject *self, PyObject *args) {\n   // create driver handles\n   CUcontext pctx = 0;\n \n-  Py_BEGIN_ALLOW_THREADS\n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuCtxGetCurrent(&pctx));\n   if (!pctx) {\n     CUDA_CHECK(cuDevicePrimaryCtxRetain(&pctx, device));\n@@ -266,7 +266,7 @@ static PyObject *loadBinary(PyObject *self, PyObject *args) {\n         cuFuncSetAttribute(fun, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES,\n                            shared_optin - shared_static));\n   }\n-  Py_END_ALLOW_THREADS\n+  Py_END_ALLOW_THREADS;\n \n   if (PyErr_Occurred()) {\n     return NULL;\n@@ -284,9 +284,9 @@ static PyObject *memAlloc(PyObject *self, PyObject *args) {\n     return NULL; // Error parsing arguments\n   }\n \n-  Py_BEGIN_ALLOW_THREADS\n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuMemAlloc(&dptr, bytesize));\n-  Py_END_ALLOW_THREADS\n+  Py_END_ALLOW_THREADS;\n \n   return PyLong_FromUnsignedLongLong((unsigned long long)dptr);\n }\n@@ -305,9 +305,9 @@ static PyObject *memcpyHtoD(PyObject *self, PyObject *args) {\n   dstDevice = (CUdeviceptr)dstDevicePtr;\n   srcHost = (const void *)srcHostPtr;\n \n-  Py_BEGIN_ALLOW_THREADS\n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuMemcpyHtoD(dstDevice, srcHost, byteCount));\n-  Py_END_ALLOW_THREADS\n+  Py_END_ALLOW_THREADS;\n \n   Py_RETURN_NONE;\n }\n@@ -319,9 +319,9 @@ static PyObject *memFree(PyObject *self, PyObject *args) {\n     return NULL; // Error parsing arguments\n   }\n \n-  Py_BEGIN_ALLOW_THREADS\n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuMemFree(dptr));\n-  Py_END_ALLOW_THREADS\n+  Py_END_ALLOW_THREADS;\n \n   Py_RETURN_NONE;\n }\n@@ -409,12 +409,12 @@ static PyObject *tensorMapEncodeTiled(PyObject *self, PyObject *args) {\n     cuTensorMapEncodeTiledHandle = getCuTensorMapEncodeTiledHandle();\n   }\n   // Call the function\n-  Py_BEGIN_ALLOW_THREADS\n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuTensorMapEncodeTiledHandle(\n       tensorMap, tensorDataType, tensorRank, globalAddress, globalDim,\n       globalStrides, boxDim, elementStrides, interleave, swizzle, l2Promotion,\n       oobFill));\n-  Py_END_ALLOW_THREADS\n+  Py_END_ALLOW_THREADS;\n \n   // Clean up\n   free(globalDim);"}]