[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 274, "deletions": 150, "changes": 424, "file_content_changes": "@@ -340,9 +340,10 @@ struct ConvertTritonGPUOpToLLVMPatternBase {\n   getElementsFromStruct(Location loc, Value llvmStruct, unsigned elems,\n                         ConversionPatternRewriter &rewriter) {\n     SmallVector<Value> results(elems);\n-    for (unsigned i = 0; i < elems; ++i) {\n-      Type type =\n-          llvmStruct.getType().cast<LLVM::LLVMStructType>().getBody()[i];\n+    ArrayRef<Type> types =\n+        llvmStruct.getType().cast<LLVM::LLVMStructType>().getBody();\n+    for (unsigned i = 0; i < types.size(); ++i) {\n+      Type type = types[i];\n       results[i] = extract_val(type, llvmStruct, rewriter.getI64ArrayAttr(i));\n     }\n     return results;\n@@ -2661,75 +2662,207 @@ struct DotOpConversionHelper {\n   DotOp dot;\n };\n \n-LogicalResult\n-DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n-                                 ConversionPatternRewriter &rewriter) const {\n-  Location loc = op->getLoc();\n-  MLIRContext *ctx = op->getContext();\n-  // D = A * B + C\n-  Value A = op.a();\n-  Value B = op.b();\n-  Value C = op.c();\n-  Value D = op.getResult();\n-  bool allowTF32 = op.allowTF32();\n+// This class helps to adapt the existing DotOpConversion to the latest\n+// DotOpOperand layout design. It decouples the exising implementation to two\n+// parts:\n+// 1. loading the specific operand matrix(for $a, $b, $c) from smem\n+// 2. passing the loaded value and perform the mma codegen\n+struct MMA16816ConversionHelper {\n+  Value A, B, C, D;\n+  RankedTensorType aTensorTy, bTensorTy, dTensorTy;\n+  ArrayRef<int64_t> aShape, bShape, dShape;\n+  MmaEncodingAttr mmaLayout;\n+  ArrayRef<unsigned int> wpt;\n \n-  auto aTensorTy = A.getType().cast<RankedTensorType>();\n-  auto bTensorTy = B.getType().cast<RankedTensorType>();\n-  auto dTensorTy = D.getType().cast<RankedTensorType>();\n+  int mmaInstrM{-1}, mmaInstrN{-1}, mmaInstrK{-1};\n+  int matShapeM{-1}, matShapeN{-1}, matShapeK{-1};\n+  int numRepM{-1}, numRepN{-1}, numRepK{-1};\n+  Value thread, lane, warp, warpMN, warpN, warpM;\n+  size_t aElemBytes{}, bElemBytes{};\n \n-  auto aShape = aTensorTy.getShape();\n-  auto bShape = bTensorTy.getShape();\n-  auto dShape = dTensorTy.getShape();\n+  DotOpConversionHelper helper;\n+  triton::DotOp op;\n+  DotOpAdaptor adapter;\n+  ConversionPatternRewriter &rewriter;\n+  TypeConverter *typeConverter;\n+  Location loc;\n+  MLIRContext *ctx{};\n \n-  auto mmaLayout = dTensorTy.getEncoding().cast<MmaEncodingAttr>();\n+  using ValueTable = std::map<std::pair<unsigned, unsigned>, Value>;\n+\n+  MMA16816ConversionHelper(triton::DotOp op, Value thread, DotOpAdaptor adapter,\n+                           ConversionPatternRewriter &rewriter,\n+                           TypeConverter *typeConverter, Location loc)\n+      : helper(op), op(op), adapter(adapter), rewriter(rewriter),\n+        typeConverter(typeConverter), loc(loc), ctx(op.getContext()),\n+        thread(thread) {\n+    A = op.a();\n+    B = op.b();\n+    C = op.c();\n+    D = op.c();\n+\n+    aTensorTy = A.getType().cast<RankedTensorType>();\n+    bTensorTy = B.getType().cast<RankedTensorType>();\n+    dTensorTy = D.getType().cast<RankedTensorType>();\n+\n+    aShape = aTensorTy.getShape();\n+    bShape = bTensorTy.getShape();\n+    dShape = dTensorTy.getShape();\n+\n+    mmaLayout = dTensorTy.getEncoding().cast<MmaEncodingAttr>();\n+\n+    wpt = mmaLayout.getWarpsPerCTA();\n+\n+    auto mmaInstrShape = helper.getMmaInstrShape();\n+    mmaInstrM = mmaInstrShape[0];\n+    mmaInstrN = mmaInstrShape[1];\n+    mmaInstrK = mmaInstrShape[2];\n+\n+    auto matShape = helper.getMmaMatShape();\n+    matShapeM = matShape[0];\n+    matShapeN = matShape[1];\n+    matShapeK = matShape[2];\n+\n+    int NK = aShape[1];\n+    // shape / shape_per_cta\n+    numRepM = std::max<int>(dShape[0] / (wpt[0] * mmaInstrM), 1);\n+    numRepN = std::max<int>(dShape[1] / (wpt[1] * mmaInstrN), 1);\n+    numRepK = std::max<int>(NK / mmaInstrK, 1);\n+\n+    Value _32 = i32_val(32);\n+    lane = urem(thread, _32);\n+    warp = udiv(thread, _32);\n+    warpMN = udiv(warp, i32_val(wpt[0]));\n+    warpM = urem(warp, i32_val(wpt[0]));\n+    warpN = urem(warpMN, i32_val(wpt[1]));\n+\n+    aElemBytes = aTensorTy.getElementTypeBitWidth() / 8;\n+    bElemBytes = bTensorTy.getElementTypeBitWidth() / 8;\n+  }\n+\n+  // Loading $a from smem to registers, returns a LLVM::Struct.\n+  Value loadA() {\n+    ValueTable ha;\n+    std::function<void(int, int)> loadFn;\n+    if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n+      // load from smem\n+      loadFn = getLoadMatrixFn(\n+          A, adapter.a() /*llTensor*/, mmaLayout.getWarpsPerCTA()[0] /*wpt*/,\n+          1 /*kOrder*/, {mmaInstrM, mmaInstrK} /*instrShpae*/,\n+          {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, ha /*vals*/);\n+    } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n+      // load from registers, used in gemm fuse\n+      // TODO(Superjomn) Port the logic.\n+      assert(false && \"Loading A from register is not supported yet.\");\n+    } else {\n+      assert(false && \"A's layout is not supported.\");\n+    }\n \n-  auto wpt = mmaLayout.getWarpsPerCTA();\n+    // step1. Preform loading.\n+    for (unsigned m = 0; m < numRepM; ++m)\n+      for (unsigned k = 0; k < numRepK; ++k)\n+        loadFn(2 * m, 2 * k);\n \n-  // TODO(Superjomn) Process C->is_trans_a() logic\n+    // step2. Format the values to LLVM::Struct to passing to mma codegen.\n+    Value result = composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK);\n \n-  DotOpConversionHelper helper(op);\n+    // TODO[Superjomn]: Replace the convert_layout op with the result once the\n+    // DotOperandEncodingAttr is ready.\n+    return result;\n+  }\n \n-  int NK = aShape[1];\n+  // Loading $b from smem to registers, returns a LLVM::Struct.\n+  Value loadB() {\n+    ValueTable hb;\n+    auto loadFn = getLoadMatrixFn(\n+        B, adapter.b() /*llTensor*/, mmaLayout.getWarpsPerCTA()[1] /*wpt*/,\n+        0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShpae*/,\n+        {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/);\n \n-  auto mmaInstrShape = helper.getMmaInstrShape();\n-  const int mmaInstrM = mmaInstrShape[0];\n-  const int mmaInstrN = mmaInstrShape[1];\n-  const int mmaInstrK = mmaInstrShape[2];\n+    for (unsigned n = 0; n < std::max(numRepN / 2, 1); ++n) {\n+      for (unsigned k = 0; k < numRepK; ++k)\n+        loadFn(2 * n, 2 * k);\n+    }\n \n-  auto matShape = helper.getMmaMatShape();\n-  const int matShapeM = matShape[0];\n-  const int matShapeN = matShape[1];\n-  const int matShapeK = matShape[2];\n+    Value result = composeValuesToDotOperandLayoutStruct(\n+        hb, std::max(numRepN / 2, 1), numRepK);\n+    return result;\n+  }\n \n-  // shape / shape_per_cta\n-  const int numRepM = std::max<int>(dShape[0] / (wpt[0] * mmaInstrM), 1);\n-  const int numRepN = std::max<int>(dShape[1] / (wpt[1] * mmaInstrN), 1);\n-  const int numRepK = std::max<int>(NK / mmaInstrK, 1);\n+  // Loading $c from smem(?) to registers, returns a Value.\n+  // NOTE Only SplatLike tensor is supported now.\n+  Value loadC() {\n+    // Currently, we only support a SplatLike C. For the other cases, e.g., C in\n+    // shared layout or blocked layout, we will support them by expanding\n+    // convert_layout.\n+    auto hc = helper.loadSplatLikeC(C, loc, rewriter);\n+    assert(hc.size() == 4UL && \"Only splat-like C is supported now\");\n+    return hc[0];\n+  }\n \n-  Value _32 = i32_val(32);\n-  Value thread = getThreadId(rewriter, loc);\n-  Value lane = urem(thread, _32);\n-  Value warp = udiv(thread, _32);\n-  Value warpMN = udiv(warp, i32_val(wpt[0]));\n-  Value warpM = urem(warp, i32_val(wpt[0]));\n-  Value warpN = urem(warpMN, i32_val(wpt[1]));\n+  // Conduct the Dot conversion.\n+  // Input the \\param a, \\param b, \\param c, all of them are result of loading.\n+  LogicalResult convertDot(Value a, Value b, Value c) {\n+    ValueTable ha = getValuesFromDotOperandLayoutStruct(a, numRepM, numRepK);\n+    ValueTable hb = getValuesFromDotOperandLayoutStruct(\n+        b, std::max(numRepN / 2, 1), numRepK);\n \n-  size_t aElemBytes = aTensorTy.getElementTypeBitWidth() / 8;\n-  size_t bElemBytes = bTensorTy.getElementTypeBitWidth() / 8;\n+    const int fcSize = 4 * numRepM * numRepN;\n+    SmallVector<Value> fc(fcSize);\n+    for (int i = 0; i < fc.size(); i++)\n+      fc[i] = c;\n \n-  std::map<std::pair<unsigned, unsigned>, Value> ha;\n-  std::map<std::pair<unsigned, unsigned>, Value> hb;\n+    auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n+      unsigned colsPerThread = numRepN * 2;\n+      PTXBuilder builder;\n+      auto &mma = *builder.create(helper.getMmaInstr().str());\n+      auto retArgs = builder.newListOperand(4, \"=r\");\n+      auto aArgs = builder.newListOperand({\n+          {ha[{m, k}], \"r\"},\n+          {ha[{m + 1, k}], \"r\"},\n+          {ha[{m, k + 1}], \"r\"},\n+          {ha[{m + 1, k + 1}], \"r\"},\n+      });\n+      auto bArgs =\n+          builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n+      auto cArgs = builder.newListOperand();\n+      for (int i = 0; i < 4; ++i) {\n+        cArgs->listAppend(builder.newOperand(fc[m * colsPerThread + 4 * n + i],\n+                                             std::to_string(i)));\n+        // reuse the output registers\n+      }\n+      mma(retArgs, aArgs, bArgs, cArgs);\n+      Value mmaOut = builder.launch(rewriter, loc, helper.getMmaRetType());\n \n-  // the original register_lds2, but discard the prefetch logic.\n-  auto ld2 = [](decltype(ha) &vals, int mn, int k, Value val) {\n-    vals[{mn, k}] = val;\n-  };\n+      auto getIntAttr = [&](int v) {\n+        return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n+      };\n+\n+      for (int i = 0; i < 4; i++)\n+        fc[m * colsPerThread + 4 * n + i] =\n+            extract_val(type::f32Ty(ctx), mmaOut, getIntAttr(i));\n+    };\n+\n+    for (unsigned k = 0; k < numRepK; ++k)\n+      for (unsigned m = 0; m < numRepM; ++m)\n+        for (unsigned n = 0; n < numRepN; ++n)\n+          callMma(2 * m, n, 2 * k);\n+\n+    // replace with new packed result\n+    Type structTy = LLVM::LLVMStructType::getLiteral(\n+        ctx, SmallVector<Type>(fc.size(), type::f32Ty(ctx)));\n+    Value res = getStructFromElements(loc, fc, rewriter, structTy);\n+    rewriter.replaceOp(op, res);\n+\n+    return success();\n+  }\n+\n+private:\n+  std::function<void(int, int)>\n+  getLoadMatrixFn(Value tensor, Value llTensor, int wpt, int kOrder,\n+                  ArrayRef<int> instrShape, ArrayRef<int> matShape,\n+                  Value warpId, ValueTable &vals) {\n \n-  // Load A or B matrix.\n-  auto getLoadMatrixFn =\n-      [&](Value tensor, Value llTensor, int wpt, int kOrder,\n-          ArrayRef<int> instrShape, ArrayRef<int> matShape, Value warpId,\n-          decltype(ha) &vals) -> std::function<void(int, int)> {\n     auto tensorTy = tensor.getType().cast<RankedTensorType>();\n     // We assumes that the input operand of Dot should be from shared layout.\n     // TODO(Superjomn) Consider other layouts if needed later.\n@@ -2739,25 +2872,31 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n     const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n     auto order = sharedLayout.getOrder();\n \n-    MMA16816SmemLoader loader(wpt, sharedLayout.getOrder(), kOrder,\n-                              tensorTy.getShape() /*tileShape*/, instrShape,\n-                              matShape, perPhase, maxPhase, elemBytes, rewriter,\n-                              typeConverter, loc);\n-    SmallVector<Value> offs = loader.computeOffsets(warpId, lane);\n+    bool needTrans = kOrder != order[0];\n \n-    const int numPtrs = loader.getNumPtr();\n-    SmallVector<Value> ptrs(numPtrs);\n+    // the original register_lds2, but discard the prefetch logic.\n+    auto ld2 = [](ValueTable &vals, int mn, int k, Value val) {\n+      vals[{mn, k}] = val;\n+    };\n \n-    Type smemPtrTy = helper.getShemPtrTy();\n-    for (int i = 0; i < numPtrs; ++i) {\n-      ptrs[i] =\n-          bitcast(smemPtrTy, gep(smemPtrTy, llTensor, ValueRange({offs[i]})));\n-    }\n+    // (a, b) is the coordinate.\n+    auto load = [=, &vals, &ld2](int a, int b) {\n+      MMA16816SmemLoader loader(wpt, sharedLayout.getOrder(), kOrder,\n+                                tensorTy.getShape() /*tileShape*/, instrShape,\n+                                matShape, perPhase, maxPhase, elemBytes,\n+                                rewriter, typeConverter, loc);\n+      SmallVector<Value> offs = loader.computeOffsets(warpId, lane);\n \n-    bool needTrans = kOrder != order[0];\n+      const int numPtrs = loader.getNumPtr();\n+\n+      SmallVector<Value> ptrs(numPtrs);\n+\n+      Type smemPtrTy = helper.getShemPtrTy();\n+      for (int i = 0; i < numPtrs; ++i) {\n+        ptrs[i] =\n+            bitcast(smemPtrTy, gep(smemPtrTy, llTensor, ValueRange({offs[i]})));\n+      }\n \n-    // (a, b) is the coordinate.\n-    auto load = [=, &vals, &helper, &ld2](int a, int b) {\n       auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n           (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n           ptrs, helper.getMatType(), helper.getShemPtrTy());\n@@ -2775,89 +2914,74 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n     };\n \n     return load;\n-  };\n+  }\n \n-  std::function<void(int, int)> loadA;\n-  if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n-    // load from smem\n-    loadA = getLoadMatrixFn(\n-        A, adapter.a() /*llTensor*/, mmaLayout.getWarpsPerCTA()[0] /*wpt*/,\n-        1 /*kOrder*/, {mmaInstrM, mmaInstrK} /*instrShpae*/,\n-        {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, ha /*vals*/);\n-  } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n-    // load from registers, used in gemm fuse\n-    // TODO(Superjomn) Port the logic.\n-    assert(false && \"Loading A from register is not supported yet.\");\n-  } else {\n-    assert(false && \"A's layout is not supported.\");\n-  }\n-\n-  std::function<void(int, int)> loadB = getLoadMatrixFn(\n-      B, adapter.b() /*llTensor*/, mmaLayout.getWarpsPerCTA()[1] /*wpt*/,\n-      0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShpae*/,\n-      {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/);\n-\n-  const int fcSize = 4 * numRepM * numRepN;\n-  SmallVector<Value> fc(fcSize);\n-\n-  // Currently, we only support a SplatLike C. For the other cases, e.g., C in\n-  // shared layout or blocked layout, we will support them by expanding\n-  // convert_layout.\n-  auto hc = helper.loadSplatLikeC(C, loc, rewriter);\n-  assert(hc.size() == 4UL && \"Only splat-like C is supported now\");\n-  for (int i = 0; i < fc.size(); i++)\n-    fc[i] = hc[0];\n-\n-  auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n-    unsigned colsPerThread = numRepN * 2;\n-    PTXBuilder builder;\n-    auto &mma = *builder.create(helper.getMmaInstr().str());\n-    auto retArgs = builder.newListOperand(4, \"=r\");\n-    auto aArgs = builder.newListOperand({\n-        {ha[{m, k}], \"r\"},\n-        {ha[{m + 1, k}], \"r\"},\n-        {ha[{m, k + 1}], \"r\"},\n-        {ha[{m + 1, k + 1}], \"r\"},\n-    });\n-    auto bArgs =\n-        builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n-    auto cArgs = builder.newListOperand();\n-    for (int i = 0; i < 4; ++i) {\n-      cArgs->listAppend(builder.newOperand(fc[m * colsPerThread + 4 * n + i],\n-                                           std::to_string(i)));\n-      // reuse the output registers\n-    }\n-    mma(retArgs, aArgs, bArgs, cArgs);\n-    Value mmaOut = builder.launch(rewriter, loc, helper.getMmaRetType());\n+  // Compose a map of Values to a LLVM::Struct.\n+  // The layout is a list of Value with coordinate of (i,j), the order is as\n+  // the follows:\n+  // [\n+  //  (0,0), (0,1), (1,0), (1,1), # i=0, j=0\n+  //  (0,2), (0,3), (1,2), (1,3), # i=0, j=1\n+  //  (0,4), (0,5), (1,4), (1,5), # i=0, j=2\n+  //  ...\n+  //  (2,0), (2,1), (3,0), (3,1), # i=1, j=0\n+  //  (2,2), (2,3), (3,2), (3,3), # i=1, j=1\n+  //  (2,4), (2,5), (2,4), (2,5), # i=1, j=2\n+  //  ...\n+  // ]\n+  // i \\in [0, n0) and j \\in [0, n1)\n+  // There should be \\param n0 * \\param n1 elements in the output Struct.\n+  Value composeValuesToDotOperandLayoutStruct(const ValueTable &vals, int n0,\n+                                              int n1) {\n+    std::vector<Value> elems;\n+    for (unsigned m = 0; m < n0; ++m)\n+      for (unsigned k = 0; k < n1; ++k) {\n+        elems.push_back(vals.at({2 * m, 2 * k}));\n+        elems.push_back(vals.at({2 * m, 2 * k + 1}));\n+        elems.push_back(vals.at({2 * m + 1, 2 * k}));\n+        elems.push_back(vals.at({2 * m + 1, 2 * k + 1}));\n+      }\n \n-    auto getIntAttr = [&](int v) {\n-      return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n-    };\n+    assert(!elems.empty());\n \n-    for (int i = 0; i < 4; i++)\n-      fc[m * colsPerThread + 4 * n + i] =\n-          extract_val(type::f32Ty(ctx), mmaOut, getIntAttr(i));\n-  };\n+    Type fp16Ty = aTensorTy.getElementType();\n+    Type fp16x2Ty = vec_ty(fp16Ty, 2);\n+    Type structTy = LLVM::LLVMStructType::getLiteral(\n+        ctx, SmallVector<Type>(elems.size(), fp16x2Ty));\n+    auto result = getStructFromElements(loc, elems, rewriter, structTy);\n+    return result;\n+  }\n \n-  // Main program\n-  for (unsigned k = 0; k < numRepK; ++k) {\n-    for (unsigned m = 0; m < numRepM; ++m)\n-      loadA(2 * m, 2 * k);\n-    for (unsigned n = 0; n < numRepN; n += 2)\n-      loadB(n, 2 * k);\n-    for (unsigned m = 0; m < numRepM; ++m)\n-      for (unsigned n = 0; n < numRepN; ++n) {\n-        callMma(2 * m, n, 2 * k);\n+  ValueTable getValuesFromDotOperandLayoutStruct(Value value, int n0, int n1) {\n+    auto elems = ConvertTritonGPUOpToLLVMPatternBase::getElementsFromStruct(\n+        loc, value, 2 * n0 * 2 * n1, rewriter);\n+\n+    int offset{};\n+    ValueTable vals;\n+    for (int i = 0; i < n0; i++) {\n+      for (int j = 0; j < n1; j++) {\n+        vals[{2 * i, 2 * j}] = elems[offset++];\n+        vals[{2 * i, 2 * j + 1}] = elems[offset++];\n+        vals[{2 * i + 1, 2 * j}] = elems[offset++];\n+        vals[{2 * i + 1, 2 * j + 1}] = elems[offset++];\n       }\n+    }\n+    return vals;\n   }\n+};\n \n-  // replace with new packed result\n-  Type structTy = LLVM::LLVMStructType::getLiteral(\n-      ctx, SmallVector<Type>(fc.size(), type::f32Ty(ctx)));\n-  Value res = getStructFromElements(loc, fc, rewriter, structTy);\n-  rewriter.replaceOp(op, res);\n+LogicalResult\n+DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n+                                 ConversionPatternRewriter &rewriter) const {\n+  auto loc = op.getLoc();\n+  MMA16816ConversionHelper mmaHelper(op, getThreadId(rewriter, loc), adapter,\n+                                     rewriter, getTypeConverter(), loc);\n \n-  return success();\n+  auto A = mmaHelper.loadA();\n+  auto B = mmaHelper.loadB();\n+  auto C = mmaHelper.loadC();\n+\n+  return mmaHelper.convertDot(A, B, C);\n }\n \n /// ====================== mma codegen end ============================\n@@ -3012,9 +3136,9 @@ struct InsertSliceAsyncOpConversion\n \n     auto inOrder = srcBlockedLayout.getOrder();\n     auto outOrder = resSharedLayout.getOrder();\n-    // If perPhase * maxPhase > threadsPerCTA, we need to swizzle over elements\n-    // across phases.\n-    // If perPhase * maxPhase == threadsPerCTA, swizzle is not allowd\n+    // If perPhase * maxPhase > threadsPerCTA, we need to swizzle over\n+    // elements across phases. If perPhase * maxPhase == threadsPerCTA,\n+    // swizzle is not allowd\n     auto numSwizzleRows = std::max<unsigned>(\n         (perPhase * maxPhase) / threadsPerCTA[inOrder[1]], 1);\n     // A sharedLayout encoding has a \"vec\" parameter."}, {"filename": "python/setup.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -107,6 +107,7 @@ def build_extension(self, ext):\n             \"-DPYTHON_INCLUDE_DIRS=\" + \";\".join(python_include_dirs),\n             \"-DLLVM_EXTERNAL_LIT=\" + lit_dir\n         ]\n+        print('\\n'.join(cmake_args))\n         # configuration\n         cfg = \"Debug\" if self.debug else \"Release\"\n         build_args = [\"--config\", cfg]"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -50,3 +50,10 @@ def test_gemm_impl(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n     golden = torch.matmul(a, b)\n     torch.set_printoptions(profile=\"full\")\n     assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)\n+\n+\n+if __name__ == '__main__':\n+    #test_gemm_impl(128, 256, 32, 4)\n+    #test_gemm_impl(256, 128, 16, 4)\n+    test_gemm_impl(128, 16, 32, 4)\n+    #test_gemm_impl(32, 128, 64, 4)"}]