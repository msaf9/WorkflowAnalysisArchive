[{"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "@@ -27,6 +27,21 @@ struct SplatOpConversion\n     auto srcType = typeConverter->convertType(tensorTy);\n     if (auto structTy = dyn_cast<LLVM::LLVMStructType>(srcType))\n       srcType = structTy.getBody()[0];\n+    // If the type sizes don't match we need to pack constants.\n+    if (constVal.getType().getIntOrFloatBitWidth() !=\n+        srcType.getIntOrFloatBitWidth()) {\n+      unsigned cstBitWidth = constVal.getType().getIntOrFloatBitWidth();\n+      unsigned srcBitWidth = srcType.getIntOrFloatBitWidth();\n+      assert(cstBitWidth <= srcBitWidth && srcBitWidth % cstBitWidth == 0);\n+      unsigned ratio = srcBitWidth / cstBitWidth;\n+      Type intTy = IntegerType::get(elemType.getContext(), cstBitWidth);\n+      VectorType vecType = VectorType::get(ratio, intTy);\n+      Value intCst = bitcast(constVal, intTy);\n+      Value vec = undef(vecType);\n+      for (unsigned i = 0; i < ratio; ++i)\n+        vec = insert_element(vecType, vec, intCst, int_val(32, i));\n+      constVal = vec;\n+    }\n     auto llSrc = bitcast(constVal, srcType);\n     size_t elemsPerThread = getTotalElemsPerThread(tensorTy);\n     llvm::SmallVector<Value> elems(elemsPerThread, llSrc);"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 17, "deletions": 18, "changes": 35, "file_content_changes": "@@ -2195,24 +2195,23 @@ def kernel(out_ptr):\n     h = kernel_patched[(1,)](out)\n     assert re.search(r\"arith.constant .* : \" + dtype_str, h.asm[\"ttir\"]) is not None\n \n-# TODO: uncomment once DotOperandEncoding::getElemsPerThread is implemented\n-# @pytest.mark.parametrize(\"dtype_str\", ['float32', 'float16'])\n-# def test_dot_without_load(dtype_str, device=device):\n-#     @triton.jit\n-#     def _kernel(out):\n-#         a = GENERATE_TEST_HERE\n-#         b = GENERATE_TEST_HERE\n-#         c = tl.dot(a, b)\n-#         out_ptr = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n-#         tl.store(out_ptr, c)\n-\n-#     kernel = patch_kernel(_kernel, {'GENERATE_TEST_HERE': f\"tl.full((32, 32), 1.0, tl.{dtype_str})\"})\n-#     a = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=device)\n-#     b = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=device)\n-#     out_ref = torch.matmul(a, b)\n-#     out = torch.zeros((32, 32), dtype=getattr(torch, dtype_str), device=device)\n-#     kernel[(1,)](out)\n-#     assert torch.all(out == out_ref)\n+\n+@pytest.mark.parametrize(\"dtype_str\", ['float32', 'float16'])\n+def test_dot_without_load(dtype_str, device):\n+    @triton.jit\n+    def _kernel(out):\n+        a = GENERATE_TEST_HERE\n+        b = GENERATE_TEST_HERE\n+        c = tl.dot(a, b)\n+        out_ptr = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n+        tl.store(out_ptr, c)\n+    kernel = patch_kernel(_kernel, {'GENERATE_TEST_HERE': f\"tl.full((32, 32), 1.0, tl.{dtype_str})\"})\n+    a = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=device)\n+    b = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=device)\n+    out_ref = torch.matmul(a, b)\n+    out = torch.zeros((32, 32), dtype=getattr(torch, dtype_str), device=device)\n+    kernel[(1,)](out)\n+    assert torch.all(out == out_ref)\n \n # ---------------\n # test arange"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 41, "deletions": 2, "changes": 43, "file_content_changes": "@@ -1168,8 +1168,8 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma, kWidth=1}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma, kWidth=1}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  // CHECK-LABEL: matmul_tf32dot_cst_b\n-  tt.func @matmul_tf32dot_cst_b(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  // CHECK-LABEL: matmul_tf32_cst_b\n+  tt.func @matmul_tf32_cst_b(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %a: tensor<32x16xf32, #dot_operand_a>, %c: tensor<32x32xf32, #mma>) {\n   // CHECK: %[[CST:.+]] = llvm.mlir.constant(1.000000e+00 : f32) : f32\n   // CHECK: %[[BC:.+]] = llvm.bitcast %[[CST]] : f32 to i32\n@@ -1184,3 +1184,42 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     tt.return\n   }\n }\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [2, 2]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  // CHECK-LABEL: matmul_f16_cst_operands\n+  tt.func public @matmul_f16_cst_operands(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n+  // CHECK: %[[C1f:.+]] = llvm.mlir.constant(1.000000e+00 : f16) : f16\n+  // CHECK: %[[Ci16:.+]] = llvm.bitcast %[[C1f]] : f16 to i16\n+  // CHECK: %[[U:.+]] = llvm.mlir.undef : vector<2xi16>\n+  // CHECK: %[[C0:.+]] = llvm.mlir.constant(0 : i32) : i32\n+  // CHECK: %[[V0:.+]] = llvm.insertelement %[[Ci16]], %[[U]][%[[C0]] : i32] : vector<2xi16>\n+  // CHECK: %[[C1:.+]] = llvm.mlir.constant(1 : i32) : i32\n+  // CHECK: %[[V1:.+]] = llvm.insertelement %[[Ci16]], %[[V0]][%[[C1]] : i32] : vector<2xi16>\n+  // CHECK: %[[BC:.+]] = llvm.bitcast %[[V1]] : vector<2xi16> to i32\n+  // CHECK: %[[SU:.+]] = llvm.mlir.undef : !llvm.struct<(i32, i32, i32, i32, i32, i32, i32, i32)>\n+  // CHECK: llvm.insertvalue %[[BC]], %[[SU]][0] : !llvm.struct<(i32, i32, i32, i32, i32, i32, i32, i32)>\n+    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>>\n+    %cst_1 = arith.constant dense<1.000000e+00> : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 2}>>\n+    %cst_2 = arith.constant dense<32> : tensor<32x1xi32, #blocked>\n+    %0 = tt.dot %cst_0, %cst_1, %cst {allowTF32 = true} : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> * tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 2}>> -> tensor<32x32xf32, #mma>\n+    %1 = triton_gpu.convert_layout %0 : (tensor<32x32xf32, #mma>) -> tensor<32x32xf32, #blocked>\n+    %2 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+    %3 = tt.expand_dims %2 {axis = 1 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<32x1xi32, #blocked>\n+    %4 = arith.muli %3, %cst_2 : tensor<32x1xi32, #blocked>\n+    %5 = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<32x1x!tt.ptr<f16>, #blocked>\n+    %6 = tt.addptr %5, %4 : tensor<32x1x!tt.ptr<f16>, #blocked>, tensor<32x1xi32, #blocked>\n+    %7 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+    %8 = tt.expand_dims %7 {axis = 0 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x32xi32, #blocked>\n+    %9 = tt.broadcast %6 : (tensor<32x1x!tt.ptr<f16>, #blocked>) -> tensor<32x32x!tt.ptr<f16>, #blocked>\n+    %10 = tt.broadcast %8 : (tensor<1x32xi32, #blocked>) -> tensor<32x32xi32, #blocked>\n+    %11 = tt.addptr %9, %10 : tensor<32x32x!tt.ptr<f16>, #blocked>, tensor<32x32xi32, #blocked>\n+    %12 = arith.truncf %1 : tensor<32x32xf32, #blocked> to tensor<32x32xf16, #blocked>\n+    tt.store %11, %12 {cache = 1 : i32, evict = 1 : i32} : tensor<32x32xf16, #blocked>\n+    tt.return\n+  }\n+}"}]