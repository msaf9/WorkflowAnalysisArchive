[{"filename": "python/tests/test_core.py", "status": "modified", "additions": 30, "deletions": 1, "changes": 31, "file_content_changes": "@@ -1544,7 +1544,7 @@ def _kernel(dst):\n                          [('int32', 'libdevice.ffs', ''),\n                           ('float32', 'libdevice.pow', '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'),\n                           ('float64', 'libdevice.norm4d', '')])\n-def test_libdevice(dtype_str, expr, lib_path):\n+def test_libdevice_tensor(dtype_str, expr, lib_path):\n \n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n@@ -1580,3 +1580,32 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n         np.testing.assert_equal(y_ref, to_numpy(y_tri))\n     else:\n         np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n+\n+\n+@pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n+                         [('float32', 'libdevice.pow', '')])\n+def test_libdevice_scalar(dtype_str, expr, lib_path):\n+\n+    @triton.jit\n+    def kernel(X, Y, BLOCK: tl.constexpr):\n+        x = X\n+        y = GENERATE_TEST_HERE\n+        tl.store(Y + tl.arange(0, BLOCK), y)\n+\n+    shape = (128, )\n+    rs = RandomState(17)\n+    # limit the range of integers so that the sum does not overflow\n+    x = numpy_random((1,), dtype_str=dtype_str, rs=rs)\n+    y_ref = np.zeros(shape, dtype=x.dtype)\n+\n+    # numpy does not allow negative factors in power, so we use abs()\n+    x = np.abs(x)\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.pow(x, x)'})\n+    y_ref[:] = np.power(x, x)\n+\n+    # triton result\n+    x_tri = to_triton(x)[0].item()\n+    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n+    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n+    # compare\n+    np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)"}, {"filename": "python/triton/language/extern.py", "status": "modified", "additions": 29, "deletions": 23, "changes": 52, "file_content_changes": "@@ -56,29 +56,35 @@ def elementwise(lib_name: str, lib_path: str, args: list, arg_type_symbol_dict:\n         :return: the return value of the function\n     '''\n     dispatch_args = args.copy()\n-    if len(args) == 1:\n-        dispatch_args[0] = core._to_tensor(dispatch_args[0], _builder)\n-        ret_shape = dispatch_args[0].shape\n-    elif len(args) == 2:\n-        dispatch_args[0] = core._to_tensor(dispatch_args[0], _builder)\n-        dispatch_args[1] = core._to_tensor(dispatch_args[1], _builder)\n-        dispatch_args[0], dispatch_args[1] = semantic.binary_op_type_checking_impl(\n-            dispatch_args[0], dispatch_args[1], _builder)\n-        ret_shape = dispatch_args[0].shape\n-    else:\n-        for i in range(len(dispatch_args)):\n-            dispatch_args[i] = core._to_tensor(dispatch_args[i], _builder)\n-        broadcast_arg = dispatch_args[0]\n-        # Get the broadcast shape over all the arguments\n-        for i in range(len(dispatch_args)):\n-            _, broadcast_arg = semantic.binary_op_type_checking_impl(\n-                dispatch_args[i], broadcast_arg, _builder)\n-        # Change the shape of each argument based on the broadcast shape\n-        for i in range(len(dispatch_args)):\n-            dispatch_args[i], _ = semantic.binary_op_type_checking_impl(\n-                dispatch_args[i], broadcast_arg, _builder)\n-        ret_shape = broadcast_arg.shape\n-    func = getattr(_builder, \"create_external_elementwise\")\n+    all_scalar = True\n+    ret_shape = None\n+    for dispatch_arg in dispatch_args:\n+        if dispatch_arg.type.is_block():\n+            all_scalar = False\n+    if not all_scalar:\n+        if len(args) == 1:\n+            dispatch_args[0] = core._to_tensor(dispatch_args[0], _builder)\n+            ret_shape = dispatch_args[0].shape\n+        elif len(args) == 2:\n+            dispatch_args[0] = core._to_tensor(dispatch_args[0], _builder)\n+            dispatch_args[1] = core._to_tensor(dispatch_args[1], _builder)\n+            dispatch_args[0], dispatch_args[1] = semantic.binary_op_type_checking_impl(\n+                dispatch_args[0], dispatch_args[1], _builder)\n+            ret_shape = dispatch_args[0].shape\n+        else:\n+            for i in range(len(dispatch_args)):\n+                dispatch_args[i] = core._to_tensor(dispatch_args[i], _builder)\n+            broadcast_arg = dispatch_args[0]\n+            # Get the broadcast shape over all the arguments\n+            for i in range(len(dispatch_args)):\n+                _, broadcast_arg = semantic.binary_op_type_checking_impl(\n+                    dispatch_args[i], broadcast_arg, _builder)\n+            # Change the shape of each argument based on the broadcast shape\n+            for i in range(len(dispatch_args)):\n+                dispatch_args[i], _ = semantic.binary_op_type_checking_impl(\n+                    dispatch_args[i], broadcast_arg, _builder)\n+            ret_shape = broadcast_arg.shape\n+    func = getattr(_builder, \"create_extern_elementwise\")\n     return dispatch(func, lib_name, lib_path, dispatch_args, arg_type_symbol_dict, ret_shape, _builder)\n \n "}]