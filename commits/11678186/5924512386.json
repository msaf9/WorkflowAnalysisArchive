[{"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -514,7 +514,7 @@ def TT_ElementwiseInlineAsmOp : TT_Op<\"elementwise_inline_asm\", [Elementwise,\n   }];\n \n   let arguments = (ins StrAttr:$asm_string, StrAttr:$constraints, BoolAttr:$pure, I32Attr:$packed_element, Variadic<AnyTypeOf<[TT_Type]>>:$args);\n-  let results = (outs TT_Tensor:$result);\n+  let results = (outs TT_Type:$result);\n \n \n   let assemblyFormat = [{"}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 0, "deletions": 13, "changes": 13, "file_content_changes": "@@ -43,17 +43,6 @@ static int getMMAVersionSafe(int computeCapability, tt::DotOp op) {\n   return 0;\n }\n \n-SmallVector<int64_t, 2> mmaVersionToShapePerWarp(int version) {\n-  if (version == 1)\n-    return {16, 16};\n-  else if (version == 2)\n-    return {16, 8};\n-  else {\n-    assert(false && \"version not supported\");\n-    return {0, 0};\n-  }\n-}\n-\n SmallVector<unsigned, 2>\n warpsPerTileV2(tt::DotOp dotOp, const ArrayRef<int64_t> shape, int numWarps) {\n   auto filter = [&dotOp](Operation *op) {\n@@ -66,13 +55,11 @@ warpsPerTileV2(tt::DotOp dotOp, const ArrayRef<int64_t> shape, int numWarps) {\n \n   SmallVector<unsigned, 2> ret = {1, 1};\n   SmallVector<int64_t, 2> shapePerWarp = {16, 8};\n-  bool changed = false;\n   // TODO (@daadaada): double-check.\n   // original logic in\n   // https://github.com/openai/triton/blob/master/lib/codegen/analysis/layout.cc#L252\n   // seems buggy for shape = [32, 16] ?\n   do {\n-    changed = false;\n     if (ret[0] * ret[1] >= numWarps)\n       break;\n     if (shape[0] / shapePerWarp[0] / ret[0] >="}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/WSPipeline.cpp", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -173,6 +173,10 @@ scf::ForOp appendPipelineIdxToLoopArgs(scf::ForOp forOp, int numStages,\n     initValue = parentForOp.getBody()->getArguments().back();\n     Value numSteps = builder.createWithAgentIds<arith::SubIOp>(\n         loc, forOp.getUpperBound(), forOp.getLowerBound());\n+    auto one = builder.createWithAgentIds<arith::ConstantIntOp>(loc, 1, 32);\n+    numSteps = builder.createWithAgentIds<arith::AddIOp>(loc, numSteps,\n+                                                         forOp.getStep());\n+    numSteps = builder.createWithAgentIds<arith::SubIOp>(loc, numSteps, one);\n     numSteps = builder.createWithAgentIds<arith::DivUIOp>(loc, numSteps,\n                                                           forOp.getStep());\n     initValue ="}, {"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "modified", "additions": 0, "deletions": 18, "changes": 18, "file_content_changes": "@@ -43,25 +43,7 @@ static bool findAndReplace(std::string &str, const std::string &begin,\n   return true;\n }\n \n-static void linkExternal(llvm::Module &module) {\n-  namespace fs = std::filesystem;\n-\n-  // TODO: enable generating bc file from clang.\n-  static const auto this_file_path = std::filesystem::path(__FILE__);\n-  static const auto path =\n-      this_file_path.parent_path().parent_path().parent_path().parent_path() /\n-      \"python\" / \"triton\" / \"hopper_lib\" / \"libhopper_helpers.bc\";\n-\n-  // static const std::filesystem::path path =\n-  //     std::filesystem::path(__BUILD_DIR__) / \"lib\" / \"Hopper\" /\n-  //     \"libhopper_helpers.bc\";\n-  if (mlir::triton::linkExternLib(module, \"libhopper_helpers\", path.string(),\n-                                  mlir::triton::Target::NVVM))\n-    llvm::errs() << \"Link failed for: libhopper_helpers.bc\";\n-}\n-\n std::string translateLLVMIRToPTX(llvm::Module &module, int cc, int version) {\n-  linkExternal(module);\n   // LLVM version in use may not officially support target hardware.\n   // Supported versions for LLVM 14 are here:\n   // https://github.com/llvm/llvm-project/blob/f28c006a5895fc0e329fe15fead81e37457cb1d1/clang/include/clang/Basic/BuiltinsNVPTX.def"}, {"filename": "python/src/extra/cuda.ll", "status": "removed", "additions": 0, "deletions": 17, "changes": 17, "file_content_changes": "@@ -1,17 +0,0 @@\n-; ~/.triton/llvm/llvm+mlir-17.0.0-x86_64-linux-gnu-ubuntu-18.04-release/bin/llvm-as ./src/extra/cuda.ll -o ./triton/language/extra/cuda.bc\n-\n-target datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128\"\n-target triple = \"nvptx64-nvidia-cuda\"\n-\n-\n-define i64 @globaltimer() #0 {\n-  %1 = call i64 asm sideeffect \"mov.u64 $0, %globaltimer;\", \"=l\"() nounwind\n-  ret i64 %1\n-}\n-\n-define i32 @smid() #0 {\n-  %1 = call i32 asm \"mov.u32 $0, %smid;\", \"=r\"() nounwind\n-  ret i32 %1\n-}\n-\n-attributes #0 = { alwaysinline nounwind }"}, {"filename": "python/test/unit/hopper/test_persistent_warp_specialized_gemm.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -701,6 +701,7 @@ def full_static_persistent_matmul_kernel(\n                                  # bad from cublas-important-layers\n                                  [4096, 1, 1024, False, False],\n                                  [2048, 204, 1000, True, False],\n+                                 [16, 524288, 32, False, True],\n                              ]\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 30, "deletions": 17, "changes": 47, "file_content_changes": "@@ -2151,7 +2151,7 @@ def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, o\n     if capability[0] < 8:\n         if in_dtype == 'int8':\n             pytest.skip(\"Only test int8 on devices with sm >= 80\")\n-        elif in_dtype == 'float32' and allow_tf32:\n+        elif allow_tf32:\n             pytest.skip(\"Only test tf32 on devices with sm >= 80\")\n     if capability[0] == 7:\n         if (M, N, K, num_warps) == (128, 256, 32, 8):\n@@ -2205,7 +2205,7 @@ def kernel(X, stride_xm, stride_xk,\n             z = num / den[:, None]\n         if CHAIN_DOT:\n             w = tl.load(Ws)\n-            z = tl.dot(z.to(w.dtype), w, out_dtype=out_dtype)\n+            z = tl.dot(z.to(w.dtype), w, allow_tf32=ALLOW_TF32, out_dtype=out_dtype)\n         tl.store(Zs, z)\n     # input\n     rs = RandomState(17)\n@@ -2312,16 +2312,25 @@ def kernel(X, stride_xm, stride_xk,\n     if in_dtype == 'float32' and allow_tf32:\n         assert re.search(r'[mma|wgmma.mma_async].sync.aligned.m\\d+n\\d+k8(?:.row.col)?.f32.tf32.tf32', ptx)\n     elif in_dtype == 'float16' and out_dtype == tl.float32:\n-        assert re.search(r'[mma|wgmma.mma_async].sync.aligned.m\\d+n\\d+k16(?:.row.col)?.f32.f16.f16', ptx)\n+        if capability[0] == 7 and capability[1] == 5:  # Turing\n+          assert re.search(r'mma.sync.aligned.m\\d+n\\d+k8(?:.row.col)?.f32.f16.f16', ptx)\n+        else:\n+          assert re.search(r'[mma|wgmma.mma_async].sync.aligned.m\\d+n\\d+k16(?:.row.col)?.f32.f16.f16', ptx)\n     elif in_dtype == 'float16' and out_dtype == tl.float16:\n-        assert re.search(r'[mma|wgmma.mma_async].sync.aligned.m\\d+n\\d+k16(?:.row.col)?.f16.f16.f16', ptx)\n+        if capability[0] == 7 and capability[1] == 5:  # Turing\n+            assert re.search(r'mma.sync.aligned.m\\d+n\\d+k8(?:.row.col)?.f16.f16.f16', ptx)\n+        else:\n+            assert re.search(r'[mma|wgmma.mma_async].sync.aligned.m\\d+n\\d+k16(?:.row.col)?.f16.f16.f16', ptx)\n     elif in_dtype == 'int8':\n         assert 'wgmma.mma_async.sync.aligned' in ptx or\\\n             'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n \n \n @pytest.mark.parametrize('in_dtype', ['float32'])\n def test_dot_mulbroadcastred(in_dtype, device):\n+    capability = torch.cuda.get_device_capability()\n+    if capability[0] < 8:\n+        pytest.skip(\"Requires sm >= 80 to run\")\n     @triton.jit\n     def kernel(Z, X, Y,\n                M: tl.constexpr, N: tl.constexpr, K: tl.constexpr,\n@@ -2419,19 +2428,21 @@ def kernel(out_ptr):\n \n @pytest.mark.parametrize(\"dtype_str\", ['float32', 'float16'])\n def test_dot_without_load(dtype_str, device):\n+    capability = torch.cuda.get_device_capability()\n+    allow_tf32 = capability[0] > 7\n     @triton.jit\n-    def _kernel(out):\n+    def _kernel(out, ALLOW_TF32: tl.constexpr):\n         a = GENERATE_TEST_HERE\n         b = GENERATE_TEST_HERE\n-        c = tl.dot(a, b)\n+        c = tl.dot(a, b, allow_tf32=ALLOW_TF32)\n         out_ptr = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n         tl.store(out_ptr, c)\n     kernel = patch_kernel(_kernel, {'GENERATE_TEST_HERE': f\"tl.full((32, 32), 1.0, tl.{dtype_str})\"})\n     a = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=device)\n     b = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=device)\n     out_ref = torch.matmul(a, b)\n     out = torch.zeros((32, 32), dtype=getattr(torch, dtype_str), device=device)\n-    kernel[(1,)](out)\n+    kernel[(1,)](out, ALLOW_TF32=allow_tf32)\n     assert torch.all(out == out_ref)\n \n # ---------------\n@@ -2888,8 +2899,8 @@ def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n # test if\n # -------------\n \n-\n-@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and_dynamic\", \"if_and_static\"])\n+# TODO(Keren): if_exp_dynamic\n+@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_and_dynamic\", \"if_exp_static\", \"if_and_static\"])\n def test_if(if_type, device):\n \n     @triton.jit\n@@ -2901,8 +2912,10 @@ def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr\n                 tl.store(Ret, tl.load(XTrue))\n             else:\n                 tl.store(Ret, tl.load(XFalse))\n-        elif IfType == \"if_exp\":\n-            tl.store(Ret, tl.load(XTrue)) if pid % 2 else tl.store(Ret, tl.load(XFalse))\n+        elif IfType == \"if_exp_dynamic\":\n+            tl.store(Ret, tl.load(XTrue)) if pid % 2 == 0 else tl.store(Ret, tl.load(XFalse))\n+        elif IfType == \"if_exp_static\":\n+            tl.store(Ret, tl.load(XTrue)) if BoolVar else tl.store(Ret, tl.load(XFalse))\n         elif IfType == \"if_and_dynamic\":\n             if BoolVar and pid % 2 == 0:\n                 tl.store(Ret, tl.load(XTrue))\n@@ -2917,7 +2930,7 @@ def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr\n     cond = torch.ones(1, dtype=torch.int32, device=device)\n     x_true = torch.tensor([3.14], dtype=torch.float32, device=device)\n     x_false = torch.tensor([1.51], dtype=torch.float32, device=device)\n-    ret = torch.empty(1, dtype=torch.float32, device=device)\n+    ret = torch.zeros(1, dtype=torch.float32, device=device)\n \n     kernel[(1,)](cond, x_true, x_false, ret, if_type, True, 1)\n     assert torch.equal(ret, x_true)\n@@ -3198,8 +3211,9 @@ def add_fn_static_cond(x, cond: tl.constexpr):\n         return x + 1\n \n \n+# TODO(Keren): if_exp\n @pytest.mark.parametrize(\"call_type\", [\"attribute\", \"attribute_jit\",\n-                                       \"jit\", \"jit_if\", \"jit_ifexp\", \"jit_expr\",\n+                                       \"jit\", \"jit_if\", \"jit_expr\",\n                                        \"jit_static_cond\", \"jit_noinline\", \"jit_extern\"])\n def test_if_call(call_type, device):\n     @triton.jit\n@@ -3230,7 +3244,7 @@ def kernel(Out, call_type: tl.constexpr):\n                 a = o\n                 a = add_fn_return(a, pid)\n                 o = a\n-        elif call_type == \"jit_ifexp\":\n+        elif call_type == \"jit_if_exp\":\n             # ifexp expression\n             if pid == 0:\n                 a = o\n@@ -3389,8 +3403,7 @@ def kernel(Out1, Out2):\n     out2 = to_triton(np.zeros((1,), dtype=np.int64), device=device)\n     h = kernel[(1,)](out1, out2)\n     assert out2[0] > 0\n-    # 2 inlined globaltimers + one extra in the wrapper extern function\n-    assert h.asm[\"ptx\"].count(\"%globaltimer\") == 3\n+    assert h.asm[\"ptx\"].count(\"%globaltimer\") == 2\n \n \n def test_smid(device):\n@@ -3403,7 +3416,7 @@ def kernel(Out):\n     out = to_triton(np.zeros((1024,), dtype=np.int32), device=device)\n     h = kernel[(out.shape[0],)](out)\n     assert out.sort()[0].unique().shape[0] > 0\n-    assert h.asm[\"ptx\"].count(\"%smid\") == 2\n+    assert h.asm[\"ptx\"].count(\"%smid\") == 1\n \n # -----------------------\n # test layout conversions"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 21, "deletions": 6, "changes": 27, "file_content_changes": "@@ -64,6 +64,10 @@ def _is_triton_scalar(o: Any) -> bool:\n     return _is_triton_tensor(o) and (not o.type.is_block() or o.type.numel == 1)\n \n \n+def _is_list_like(o: Any) -> bool:\n+    return isinstance(o, (list, tuple))\n+\n+\n def _unwrap_if_constexpr(o: Any):\n     return o.value if isinstance(o, constexpr) else o\n \n@@ -284,6 +288,9 @@ def _set_insertion_point_and_loc(self, ip, loc):\n     # AST visitor\n     #\n     def visit_compound_statement(self, stmts):\n+        # Ensure that stmts is iterable\n+        if not _is_list_like(stmts):\n+            stmts = [stmts]\n         for stmt in stmts:\n             ret_type = self.visit(stmt)\n             if ret_type is not None and isinstance(stmt, ast.Return):\n@@ -413,9 +420,9 @@ def visit_Assign(self, node):\n             raise UnsupportedLanguageConstruct(None, node, \"simultaneous multiple assignment is not supported.\")\n         names = _names[0]\n         values = self.visit(node.value)\n-        if not isinstance(names, tuple):\n+        if not _is_list_like(names):\n             names = [names]\n-        if not isinstance(values, tuple):\n+        if not _is_list_like(values):\n             values = [values]\n         native_nontensor_types = (language.dtype, )\n         for name, value in zip(names, values):\n@@ -619,11 +626,19 @@ def visit_If(self, node):\n     def visit_IfExp(self, node):\n         cond = self.visit(node.test)\n         if _is_triton_tensor(cond):\n-            cond = cond.to(language.int1, _builder=self.builder)\n-        if _unwrap_if_constexpr(cond):\n-            return self.visit(node.body)\n+            raise UnsupportedLanguageConstruct(\n+                None, node,\n+                \"Triton does not support `if` expressions (ternary operators) with dynamic conditions, use `if` statements instead\")\n         else:\n-            return self.visit(node.orelse)\n+            cond = _unwrap_if_constexpr(cond)\n+            if type(cond) not in _condition_types:  # not isinstance - we insist the real thing, no subclasses and no ducks\n+                raise UnsupportedLanguageConstruct(\n+                    None, node, \"`if` conditionals can only accept values of type {{{}}}, not objects of type {}\".format(\n+                        ', '.join(_.__name__ for _ in _condition_types), type(cond).__name__))\n+            if cond:\n+                return self.visit(node.body)\n+            else:\n+                return self.visit(node.orelse)\n \n     def visit_Pass(self, node):\n         pass"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -1638,6 +1638,7 @@ def inline_asm_elementwise(asm: str, constraints: str, args: list, dtype, is_pur\n     is_pure = _constexpr_to_value(is_pure)\n     ret_shape = None\n     arg_types = []\n+    res_ty = dtype\n     for i in range(len(dispatch_args)):\n         dispatch_args[i] = _to_tensor(dispatch_args[i], _builder)\n         arg_types.append(dispatch_args[i].dtype)\n@@ -1652,10 +1653,10 @@ def inline_asm_elementwise(asm: str, constraints: str, args: list, dtype, is_pur\n         for i in range(len(dispatch_args)):\n             dispatch_args[i], _ = semantic.binary_op_type_checking_impl(\n                 dispatch_args[i], broadcast_arg, _builder, arithmetic_check=False)\n-    ret_shape = broadcast_arg.shape\n-    res_ty = block_type(dtype, ret_shape).to_ir(_builder)\n-    call = _builder.create_inline_asm(asm, constraints, [t.handle for t in args], res_ty, is_pure, pack)\n-    return tensor(call, block_type(dtype, ret_shape))\n+        ret_shape = broadcast_arg.shape\n+        res_ty = block_type(dtype, ret_shape)\n+    call = _builder.create_inline_asm(asm, constraints, [t.handle for t in args], res_ty.to_ir(_builder), is_pure, pack)\n+    return tensor(call, res_ty)\n \n \n # -----------------------"}, {"filename": "python/triton/language/extra/cuda.bc", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/triton/language/extra/cuda.py", "status": "modified", "additions": 6, "deletions": 10, "changes": 16, "file_content_changes": "@@ -1,19 +1,15 @@\n-import os\n-\n from .. import core\n \n-__path__ = os.path.dirname(os.path.abspath(__file__))\n-\n \n @core.extern\n def globaltimer(_builder=None):\n-    return core.extern_elementwise(\"cuda\", os.path.join(__path__, \"cuda.bc\"), [],\n-                                   {tuple(): (\"globaltimer\", core.dtype(\"int64\")),\n-                                    }, is_pure=False, _builder=_builder)\n+    return core.inline_asm_elementwise(\"mov.u64 $0, %globaltimer;\", \"=l\", [],\n+                                       dtype=core.int64, is_pure=False,\n+                                       pack=1, _builder=_builder)\n \n \n @core.extern\n def smid(_builder=None):\n-    return core.extern_elementwise(\"cuda\", os.path.join(__path__, \"cuda.bc\"), [],\n-                                   {tuple(): (\"smid\", core.dtype(\"int32\")),\n-                                    }, is_pure=True, _builder=_builder)\n+    return core.inline_asm_elementwise(\"mov.u32 $0, %smid;\", \"=r\", [],\n+                                       dtype=core.int32, is_pure=True,\n+                                       pack=1, _builder=_builder)"}]