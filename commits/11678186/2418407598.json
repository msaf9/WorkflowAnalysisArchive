[{"filename": "python/tutorials/01-vector-add.py", "status": "modified", "additions": 12, "deletions": 13, "changes": 25, "file_content_changes": "@@ -34,17 +34,17 @@ def add_kernel(\n     # for instance, if you had a vector of length 256 and block_size of 64, the programs\n     # would each access the elements [0:64, 64:128, 128:192, 192:256].\n     # Note that offsets is a list of pointers\n-    for block_start in range(pid * BLOCK_SIZE, n_elements, tl.num_programs(0) * BLOCK_SIZE):\n-        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n-        # Create a mask to guard memory operations against out-of-bounds accesses\n-        mask = offsets < n_elements\n-        # Load x and y from DRAM, masking out any extra elements in case the input is not a\n-        # multiple of the block size\n-        x = tl.load(x_ptr + offsets, mask=mask)\n-        y = tl.load(y_ptr + offsets, mask=mask)\n-        output = x + y\n-        # Write x + y back to DRAM\n-        tl.store(output_ptr + offsets, output, mask=mask)\n+    block_start = pid * BLOCK_SIZE\n+    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+    # Create a mask to guard memory operations against out-of-bounds accesses\n+    mask = offsets < n_elements\n+    # Load x and y from DRAM, masking out any extra elements in case the input is not a\n+    # multiple of the block size\n+    x = tl.load(x_ptr + offsets, mask=mask)\n+    y = tl.load(y_ptr + offsets, mask=mask)\n+    output = x + y\n+    # Write x + y back to DRAM\n+    tl.store(output_ptr + offsets, output, mask=mask)\n \n \n # %%\n@@ -60,8 +60,7 @@ def add(x: torch.Tensor, y: torch.Tensor):\n     # The SPMD launch grid denotes the number of kernel instances that run in parallel.\n     # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int]\n     # In this case, we use a 1D grid where the size is the number of blocks\n-    # grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n-    grid = 128,\n+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n     # NOTE:\n     #  - each torch.tensor object is implicitly converted into a pointer to its first element.\n     #  - `triton.jit`'ed functions can be index with a launch grid to obtain a callable GPU kernel"}]