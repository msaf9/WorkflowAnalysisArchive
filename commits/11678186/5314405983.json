[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -25,7 +25,7 @@ jobs:\n         id: set-matrix\n         run: |\n           if [ x\"${{ github.repository }}\" == x\"openai/triton\" ]; then\n-            echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"V100\"], [\"self-hosted\", \"gfx908\"], [\"self-hosted\", \"arc770\"]]'\n+            echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"V100\"], [\"self-hosted\", \"H100\"], [\"self-hosted\", \"gfx908\"], [\"self-hosted\", \"arc770\"]]'\n           else\n             echo '::set-output name=matrix::[\"ubuntu-latest\"]'\n           fi\n@@ -44,7 +44,7 @@ jobs:\n         uses: actions/checkout@v2\n \n       - name: Set CUDA ENV\n-        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100')}}\n+        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n         run: |\n           echo \"BACKEND=CUDA\" >> \"${GITHUB_ENV}\"\n \n@@ -131,13 +131,13 @@ jobs:\n           python3 -m pytest\n \n       - name: Create artifacts archive\n-        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100')}}\n+        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n         run: |\n           cd ~/.triton\n           tar -czvf artifacts.tar.gz cache\n \n       - name: Upload artifacts archive\n-        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100')}}\n+        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n         uses: actions/upload-artifact@v2\n         with:\n           name: artifacts"}, {"filename": "docs/index.rst", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -1,7 +1,7 @@\n Welcome to Triton's documentation!\n ==================================\n \n-Triton is a language and compiler for parallel programming. It aims to provide a Python-based programming environment for productively writing custom DNN compute kernels capable of running at maximal throughput on modern GPU hardware.\n+Triton_ is a language and compiler for parallel programming. It aims to provide a Python-based programming environment for productively writing custom DNN compute kernels capable of running at maximal throughput on modern GPU hardware.\n \n \n Getting Started\n@@ -52,3 +52,5 @@ Check out the following documents to learn more about Triton and how it compares\n \n    programming-guide/chapter-1/introduction\n    programming-guide/chapter-2/related-work\n+\n+.. _Triton: https://github.com/openai/triton"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 8, "deletions": 1, "changes": 9, "file_content_changes": "@@ -352,7 +352,14 @@ struct StoreOpConversion\n           ptxBuilder.newAddrOperand(ptrElems[vecStart], \"l\", in_off);\n \n       auto &ptxStoreInstr =\n-          ptxBuilder.create<>(\"st\")->global().v(nWords).b(width);\n+          ptxBuilder.create<>(\"st\")\n+              ->global()\n+              .o(\"L1::evict_first\",\n+                 op.getEvict() == triton::EvictionPolicy::EVICT_FIRST)\n+              .o(\"L1::evict_last\",\n+                 op.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n+              .v(nWords)\n+              .b(width);\n       ptxStoreInstr(asmAddr, asmArgList).predicate(maskVal, \"b\");\n \n       Type boolTy = getTypeConverter()->convertType(rewriter.getIntegerType(1));"}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -47,7 +47,7 @@ SmallVector<unsigned, 2> warpsPerTileV2(triton::DotOp dotOp,\n   auto filter = [&dotOp](Operation *op) {\n     return op->getParentRegion() == dotOp->getParentRegion();\n   };\n-  auto slices = mlir::getSlice(dotOp, filter);\n+  auto slices = mlir::getSlice(dotOp, {filter});\n   for (Operation *op : slices)\n     if (isa<triton::DotOp>(op) && (op != dotOp))\n       return {(unsigned)numWarps, 1};\n@@ -113,8 +113,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n     if (versionMajor == 1) {\n       SetVector<Operation *> aBwdSlices, bBwdSlices;\n       auto isCvt = [](Operation *op) { return isa<ConvertLayoutOp>(op); };\n-      getBackwardSlice(a, &aBwdSlices, isCvt);\n-      getBackwardSlice(b, &bBwdSlices, isCvt);\n+      getBackwardSlice(a, &aBwdSlices, {isCvt});\n+      getBackwardSlice(b, &bBwdSlices, {isCvt});\n       // get the source of the first conversion found in slices\n       auto getCvtArgOrder = [](Operation *op) {\n         return cast<ConvertLayoutOp>(op)"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "file_content_changes": "@@ -74,9 +74,8 @@ class DecomposeDotOperand : public mlir::RewritePattern {\n           dstType.getShape(), dstType.getElementType(), dstParentMma);\n       auto tmp = rewriter.create<triton::gpu::ConvertLayoutOp>(\n           convert.getLoc(), tmpType, convert.getOperand());\n-      auto newConvert = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-          convert.getLoc(), dstType, tmp);\n-      rewriter.replaceOp(op, {newConvert});\n+      rewriter.replaceOpWithNewOp<triton::gpu::ConvertLayoutOp>(op, dstType,\n+                                                                tmp);\n       return mlir::success();\n     }\n     return mlir::failure();\n@@ -353,7 +352,7 @@ class RematerializeForward : public mlir::RewritePattern {\n              !(isa<triton::ReduceOp>(op) &&\n                !op->getResult(0).getType().isa<RankedTensorType>());\n     };\n-    mlir::getForwardSlice(cvt.getResult(), &cvtSlices, filter);\n+    mlir::getForwardSlice(cvt.getResult(), &cvtSlices, {filter});\n     if (cvtSlices.empty())\n       return failure();\n "}, {"filename": "python/setup.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -265,7 +265,7 @@ def build_extension(self, ext):\n         \"triton/_C\",\n         \"triton/common\",\n         \"triton/compiler\",\n-        \"triton/debugger\",\n+        \"triton/interpreter\",\n         \"triton/language\",\n         \"triton/language/extra\",\n         \"triton/ops\","}, {"filename": "python/test/unit/interpreter/test_interpreter.py", "status": "renamed", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -4,7 +4,7 @@\n \n import triton\n import triton.language as tl\n-from triton.debugger.debugger import program_ids_from_grid\n+from triton.interpreter.interpreter import program_ids_from_grid\n \n \n def test_addition():"}, {"filename": "python/test/unit/language/assert_helper.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -51,7 +51,7 @@ def test_assert(func: str):\n         kernel_device_assert[(1,)](x, y, BLOCK=shape[0])\n         kernel_device_assert_scalar[(1,)](x, y, BLOCK=shape[0])\n     elif func == \"no_debug\":\n-        # TRITON_DEBUG=True can override the debug flag\n+        # TRITON_DEBUG=1 can override the debug flag\n         kernel_device_assert_no_debug[(1,)](x, y, BLOCK=shape[0])\n     elif func == \"assert\":\n         kernel_assert[(1,)](x, y, BLOCK=shape[0])"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 11, "deletions": 6, "changes": 17, "file_content_changes": "@@ -1226,8 +1226,8 @@ def convert_float_to_float32(fp: torch.tensor, dtype=None):\n     # special cases, exp is 0b11..1\n     if dtype == tl.float8e4:\n         # float8e4m3 does not have infinities\n-        output[fp == torch.tensor(0b01111111, dtype=torch.int8)] = torch.nan\n-        output[fp == torch.tensor(0b11111111, dtype=torch.int8)] = torch.nan\n+        output[fp == 0b01111111] = torch.nan\n+        output[fp == 0b11111111] = torch.nan\n     else:\n         output = torch.where(exp == (1 << exp_width) - 1,\n                              ((sign << (tl.float32.primitive_bitwidth - 1)) | extended_exp | (frac << (tl.float32.fp_mantissa_width - dtype.fp_mantissa_width))).view(torch.float32),\n@@ -2484,11 +2484,11 @@ def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n # -------------\n \n \n-@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and\"])\n+@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and_dynamic\", \"if_and_static\"])\n def test_if(if_type):\n \n     @triton.jit\n-    def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr):\n+    def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr, StaticVaue: tl.constexpr):\n         pid = tl.program_id(0)\n         cond = tl.load(Cond)\n         if IfType == \"if\":\n@@ -2498,17 +2498,22 @@ def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr\n                 tl.store(Ret, tl.load(XFalse))\n         elif IfType == \"if_exp\":\n             tl.store(Ret, tl.load(XTrue)) if pid % 2 else tl.store(Ret, tl.load(XFalse))\n-        elif IfType == \"if_and\":\n+        elif IfType == \"if_and_dynamic\":\n             if BoolVar and pid % 2 == 0:\n                 tl.store(Ret, tl.load(XTrue))\n             else:\n                 tl.store(Ret, tl.load(XFalse))\n+        elif IfType == \"if_and_static\":\n+            if StaticVaue != 0 and StaticVaue != 0:\n+                tl.store(Ret, tl.load(XTrue))\n+            else:\n+                tl.store(Ret, tl.load(XFalse))\n \n     cond = torch.ones(1, dtype=torch.int32, device='cuda')\n     x_true = torch.tensor([3.14], dtype=torch.float32, device='cuda')\n     x_false = torch.tensor([1.51], dtype=torch.float32, device='cuda')\n     ret = torch.empty(1, dtype=torch.float32, device='cuda')\n-    kernel[(1,)](cond, x_true, x_false, ret, if_type, True)\n+    kernel[(1,)](cond, x_true, x_false, ret, if_type, True, 1)\n     assert torch.equal(ret, x_true)\n \n "}, {"filename": "python/test/unit/runtime/test_launch.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -38,7 +38,7 @@ def kernel(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):\n             kernel[(10,)](inp, out, 10, XBLOCK=16)\n         gc.collect()\n         end, _ = tracemalloc.get_traced_memory()\n-        assert end - begin < 1000\n+        assert end - begin < 5000\n     finally:\n         tracemalloc.stop()\n "}, {"filename": "python/triton/__init__.py", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -18,7 +18,6 @@\n )\n from .runtime.jit import jit\n from .compiler import compile, CompilationError\n-from .debugger.debugger import program_ids_from_grid\n \n from . import language\n from . import testing\n@@ -43,7 +42,6 @@\n     \"runtime\",\n     \"TensorWrapper\",\n     \"testing\",\n-    \"program_ids_from_grid\",\n ]\n \n "}, {"filename": "python/triton/common/backend.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -3,7 +3,7 @@\n import importlib.util\n from typing import Dict\n \n-from triton.runtime.driver import DriverBase\n+from ..runtime.driver import DriverBase\n \n \n class BaseBackend:"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 8, "deletions": 6, "changes": 14, "file_content_changes": "@@ -6,12 +6,12 @@\n from typing import Any, Callable, Dict, Optional, Tuple, Type, Union\n \n from .. import language\n+from .._C.libtriton.triton import ir\n from ..language import constexpr, tensor\n # ideally we wouldn't need any runtime component\n from ..runtime import JITFunction\n from .errors import (CompilationError, CompileTimeAssertionFailure,\n                      UnsupportedLanguageConstruct)\n-from triton._C.libtriton.triton import ir\n \n \n def mangle_ty(ty):\n@@ -595,12 +595,14 @@ def visit_Pass(self, node):\n     def visit_Compare(self, node):\n         if not (len(node.comparators) == 1 and len(node.ops) == 1):\n             raise UnsupportedLanguageConstruct(None, node, \"simultaneous multiple comparison is not supported\")\n-        lhs = _unwrap_if_constexpr(self.visit(node.left))\n-        rhs = _unwrap_if_constexpr(self.visit(node.comparators[0]))\n+        lhs = self.visit(node.left)\n+        rhs = self.visit(node.comparators[0])\n+        lhs_value = _unwrap_if_constexpr(lhs)\n+        rhs_value = _unwrap_if_constexpr(rhs)\n         if type(node.ops[0]) == ast.Is:\n-            return constexpr(lhs is rhs)\n+            return constexpr(lhs_value is rhs_value)\n         if type(node.ops[0]) == ast.IsNot:\n-            return constexpr(lhs is not rhs)\n+            return constexpr(lhs_value is not rhs_value)\n         method_name = self._method_name_for_comp_op.get(type(node.ops[0]))\n         if method_name is None:\n             raise UnsupportedLanguageConstruct(None, node, \"AST comparison operator '{}' is not (currently) implemented.\".format(node.ops[0].__name__))\n@@ -988,7 +990,7 @@ def execute_static_assert(self, node: ast.Call) -> None:\n         if not (0 < arg_count <= 2) or len(node.keywords):\n             raise TypeError(\"`static_assert` requires one or two positional arguments only\")\n \n-        passed = self.visit(node.args[0])\n+        passed = _unwrap_if_constexpr(self.visit(node.args[0]))\n         if not isinstance(passed, bool):\n             raise NotImplementedError(\"Assertion condition could not be determined at compile-time. Make sure that it depends only on `constexpr` values\")\n         if not passed:"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 52, "deletions": 45, "changes": 97, "file_content_changes": "@@ -11,20 +11,26 @@\n from pathlib import Path\n from typing import Any, Tuple\n \n-import triton\n-import triton._C.libtriton.triton as _triton\n+# import triton\n+from .._C.libtriton.triton import (add_external_libs, compile_ptx_to_cubin,\n+                                   get_shared_memory_size, ir,\n+                                   translate_llvmir_to_hsaco, translate_llvmir_to_ptx,\n+                                   translate_triton_gpu_to_llvmir)\n from ..common.backend import get_backend\n-from ..runtime import driver\n+# from ..runtime import driver, jit, JITFunction\n # TODO: runtime.errors\n from ..runtime.autotuner import OutOfResources\n from ..runtime.cache import get_cache_manager\n+from ..runtime.driver import driver\n+from ..runtime.jit import (JITFunction, get_cuda_stream, get_current_device,\n+                           get_device_capability, version_key)\n from ..tools.disasm import extract\n from .code_generator import ast_to_ttir\n from .make_launcher import make_stub\n \n \n def inline_triton_ir(mod):\n-    pm = _triton.ir.pass_manager(mod.context)\n+    pm = ir.pass_manager(mod.context)\n     pm.enable_debug()\n     pm.add_inliner_pass()\n     pm.run(mod)\n@@ -34,7 +40,7 @@ def inline_triton_ir(mod):\n def ttir_compute_capability_rewrite(mod, arch):\n     # For hardware without support, we must rewrite all load/store\n     # with block (tensor) pointers into tensors of pointers\n-    pm = _triton.ir.pass_manager(mod.context)\n+    pm = ir.pass_manager(mod.context)\n     pm.enable_debug()\n     if _is_cuda(arch):\n         pm.add_rewrite_tensor_pointer_pass(arch)\n@@ -45,7 +51,7 @@ def ttir_compute_capability_rewrite(mod, arch):\n def optimize_ttir(mod, arch):\n     mod = inline_triton_ir(mod)\n     mod = ttir_compute_capability_rewrite(mod, arch)\n-    pm = _triton.ir.pass_manager(mod.context)\n+    pm = ir.pass_manager(mod.context)\n     pm.enable_debug()\n     pm.add_inliner_pass()\n     pm.add_triton_combine_pass()\n@@ -58,14 +64,14 @@ def optimize_ttir(mod, arch):\n \n \n def ttir_to_ttgir(mod, num_warps):\n-    pm = _triton.ir.pass_manager(mod.context)\n+    pm = ir.pass_manager(mod.context)\n     pm.add_convert_triton_to_tritongpu_pass(num_warps)\n     pm.run(mod)\n     return mod\n \n \n def optimize_ttgir(mod, num_stages, arch):\n-    pm = _triton.ir.pass_manager(mod.context)\n+    pm = ir.pass_manager(mod.context)\n     pm.enable_debug()\n     pm.add_tritongpu_coalesce_pass()\n     pm.add_tritongpu_remove_layout_conversions_pass()\n@@ -89,17 +95,17 @@ def _add_external_libs(mod, libs):\n     for name, path in libs.items():\n         if len(name) == 0 or len(path) == 0:\n             return\n-    _triton.add_external_libs(mod, list(libs.keys()), list(libs.values()))\n+    add_external_libs(mod, list(libs.keys()), list(libs.values()))\n \n \n def ttgir_to_llir(mod, extern_libs, arch):\n     if extern_libs:\n         _add_external_libs(mod, extern_libs)\n     # TODO: separate tritongpu_to_llvmir for different backends\n     if _is_cuda(arch):\n-        return _triton.translate_triton_gpu_to_llvmir(mod, arch, False)\n+        return translate_triton_gpu_to_llvmir(mod, arch, False)\n     else:\n-        return _triton.translate_triton_gpu_to_llvmir(mod, 0, True)\n+        return translate_triton_gpu_to_llvmir(mod, 0, True)\n \n \n # PTX translation\n@@ -129,8 +135,9 @@ def path_to_ptxas():\n     ]\n \n     for ptxas in paths:\n-        if os.path.exists(ptxas) and os.path.isfile(ptxas):\n-            result = subprocess.check_output([ptxas, \"--version\"], stderr=subprocess.STDOUT)\n+        ptxas_bin = ptxas.split(\" \")[0]\n+        if os.path.exists(ptxas_bin) and os.path.isfile(ptxas_bin):\n+            result = subprocess.check_output([ptxas_bin, \"--version\"], stderr=subprocess.STDOUT)\n             if result is not None:\n                 version = re.search(r\".*release (\\d+\\.\\d+).*\", result.decode(\"utf-8\"), flags=re.MULTILINE)\n                 if version is not None:\n@@ -147,7 +154,7 @@ def llir_to_ptx(mod: Any, arch: int, ptx_version: int = None) -> str:\n     if ptx_version is None:\n         _, cuda_version = path_to_ptxas()\n         ptx_version = ptx_get_version(cuda_version)\n-    return _triton.translate_llvmir_to_ptx(mod, arch, ptx_version)\n+    return translate_llvmir_to_ptx(mod, arch, ptx_version)\n \n \n def ptx_to_cubin(ptx: str, arch: int):\n@@ -158,7 +165,7 @@ def ptx_to_cubin(ptx: str, arch: int):\n     :return: str\n     '''\n     ptxas, _ = path_to_ptxas()\n-    return _triton.compile_ptx_to_cubin(ptx, ptxas, arch)\n+    return compile_ptx_to_cubin(ptx, ptxas, arch)\n \n \n # AMDGCN translation\n@@ -224,7 +231,7 @@ def llir_to_amdgcn_and_hsaco(mod: Any, gfx_arch: str, gfx_triple: str, gfx_featu\n         - AMDGCN code\n         - Path to HSACO object\n     '''\n-    return _triton.translate_llvmir_to_hsaco(mod, gfx_arch, gfx_triple, gfx_features)\n+    return translate_llvmir_to_hsaco(mod, gfx_arch, gfx_triple, gfx_features)\n \n \n # ------------------------------------------------------------------------------\n@@ -251,7 +258,7 @@ def convert_type_repr(x):\n \n \n def make_hash(fn, arch, **kwargs):\n-    if isinstance(fn, triton.runtime.JITFunction):\n+    if isinstance(fn, JITFunction):\n         configs = kwargs[\"configs\"]\n         signature = kwargs[\"signature\"]\n         constants = kwargs.get(\"constants\", dict())\n@@ -264,7 +271,7 @@ def make_hash(fn, arch, **kwargs):\n         key = f\"{fn.cache_key}-{''.join(signature.values())}-{configs_key}-{constants}-{num_warps}-{num_stages}-{debug}-{arch}\"\n         return hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n     assert isinstance(fn, str)\n-    return hashlib.md5((Path(fn).read_text() + triton.runtime.jit.version_key()).encode(\"utf-8\")).hexdigest()\n+    return hashlib.md5((Path(fn).read_text() + version_key()).encode(\"utf-8\")).hexdigest()\n \n \n # - ^\\s*tt\\.func\\s+ : match the start of the string, any leading whitespace, the keyword func,\n@@ -308,7 +315,7 @@ def _is_jsonable(x):\n \n \n def parse_mlir_module(path, context):\n-    module = _triton.ir.parse_mlir_module(path, context)\n+    module = ir.parse_mlir_module(path, context)\n     # module takes ownership of the context\n     module.context = context\n     return module\n@@ -329,8 +336,8 @@ def get_architecture_descriptor(capability):\n         raise ImportError(\"Triton requires PyTorch to be installed\")\n     if capability is None:\n         if torch.version.hip is None:\n-            device = triton.runtime.jit.get_current_device()\n-            capability = triton.runtime.jit.get_device_capability(device)\n+            device = get_current_device()\n+            capability = get_device_capability(device)\n             capability = capability[0] * 10 + capability[1]\n         else:\n             capability = get_amdgpu_arch_fulldetails()\n@@ -376,7 +383,7 @@ def compile(fn, **kwargs):\n \n     is_cuda = device_type == \"cuda\" and _is_cuda(arch)\n     is_hip = device_type in [\"cuda\", \"hip\"] and not is_cuda\n-    context = _triton.ir.context()\n+    context = ir.context()\n     asm = dict()\n     constants = kwargs.get(\"constants\", dict())\n     num_warps = kwargs.get(\"num_warps\", 4)\n@@ -403,7 +410,7 @@ def compile(fn, **kwargs):\n         _device_backend.add_stages(arch, extern_libs, stages)\n \n     # find out the signature of the function\n-    if isinstance(fn, triton.runtime.JITFunction):\n+    if isinstance(fn, JITFunction):\n         configs = kwargs.get(\"configs\", None)\n         signature = kwargs[\"signature\"]\n         if configs is None:\n@@ -417,20 +424,20 @@ def compile(fn, **kwargs):\n         kwargs[\"signature\"] = signature\n     else:\n         assert isinstance(fn, str)\n-        _, ir = os.path.basename(fn).split(\".\")\n+        _, ir_name = os.path.basename(fn).split(\".\")\n         src = Path(fn).read_text()\n         import re\n-        match = re.search(prototype_pattern[ir], src, re.MULTILINE)\n+        match = re.search(prototype_pattern[ir_name], src, re.MULTILINE)\n         name, signature = match.group(1), match.group(2)\n-        types = re.findall(arg_type_pattern[ir], signature)\n-        if ir == 'ttgir':\n+        types = re.findall(arg_type_pattern[ir_name], signature)\n+        if ir_name == 'ttgir':\n             num_warps_matches = re.findall(ttgir_num_warps_pattern, src)\n             assert len(num_warps_matches) == 1, \"Expected exactly one match for num_warps\"\n             assert \"num_warps\" not in kwargs or int(num_warps_matches[0]) == num_warps, \"num_warps in ttgir does not match num_warps in compile\"\n             num_warps = int(num_warps_matches[0])\n         param_tys = [convert_type_repr(ty) for ty in types]\n         signature = {k: v for k, v in enumerate(param_tys)}\n-        first_stage = list(stages.keys()).index(ir)\n+        first_stage = list(stages.keys()).index(ir_name)\n \n     # cache manager\n     if is_cuda or is_hip:\n@@ -441,7 +448,7 @@ def compile(fn, **kwargs):\n     # create cache manager\n     fn_cache_manager = get_cache_manager(make_hash(fn, arch, **kwargs))\n     # determine name and extension type of provided function\n-    if isinstance(fn, triton.runtime.JITFunction):\n+    if isinstance(fn, JITFunction):\n         name, ext = fn.__name__, \"ast\"\n     else:\n         name, ext = os.path.basename(fn).split(\".\")\n@@ -477,10 +484,10 @@ def compile(fn, **kwargs):\n     asm = dict()\n     module = fn\n     # run compilation pipeline  and populate metadata\n-    for ir, (parse, compile_kernel) in list(stages.items())[first_stage:]:\n-        ir_filename = f\"{name}.{ir}\"\n+    for ir_name, (parse, compile_kernel) in list(stages.items())[first_stage:]:\n+        ir_filename = f\"{name}.{ir_name}\"\n \n-        if ir == ext:\n+        if ir_name == ext:\n             next_module = parse(fn)\n         else:\n             path = metadata_group.get(ir_filename)\n@@ -494,29 +501,29 @@ def compile(fn, **kwargs):\n                     metadata_group[ir_filename] = fn_cache_manager.put(next_module, ir_filename)\n                     fn_cache_manager.put(next_module, ir_filename)\n             else:\n-                if ir == \"amdgcn\":\n+                if ir_name == \"amdgcn\":\n                     extra_file_name = f\"{name}.hsaco_path\"\n                     hasco_path = metadata_group.get(extra_file_name)\n                     assert hasco_path is not None, \"Expected to have hsaco_path in metadata when we have the amdgcn\"\n                     next_module = (parse(path), parse(hasco_path))\n                 else:\n                     next_module = parse(path)\n \n-        if ir == \"cubin\":\n-            asm[ir] = next_module\n-        elif ir == \"amdgcn\":\n-            asm[ir] = str(next_module[0])\n+        if ir_name == \"cubin\":\n+            asm[ir_name] = next_module\n+        elif ir_name == \"amdgcn\":\n+            asm[ir_name] = str(next_module[0])\n         else:\n-            asm[ir] = str(next_module)\n-        if ir == \"llir\" and \"shared\" not in metadata:\n-            metadata[\"shared\"] = _triton.get_shared_memory_size(module)\n-        if ir == \"ptx\":\n+            asm[ir_name] = str(next_module)\n+        if ir_name == \"llir\" and \"shared\" not in metadata:\n+            metadata[\"shared\"] = get_shared_memory_size(module)\n+        if ir_name == \"ptx\":\n             metadata[\"name\"] = get_kernel_name(next_module, pattern='// .globl')\n-        if ir == \"amdgcn\":\n+        if ir_name == \"amdgcn\":\n             metadata[\"name\"] = get_kernel_name(next_module[0], pattern='.globl')\n             asm[\"hsaco_path\"] = next_module[1]\n         if not is_cuda and not is_hip:\n-            _device_backend.add_meta_info(ir, module, next_module, metadata, asm)\n+            _device_backend.add_meta_info(ir_name, module, next_module, metadata, asm)\n         module = next_module\n     # write-back metadata, if it didn't come from the cache\n     if metadata_path is None:\n@@ -562,7 +569,7 @@ def _init_handles(self):\n             return\n \n         if self.device_type in [\"cuda\", \"hip\"]:\n-            device = triton.runtime.jit.get_current_device()\n+            device = get_current_device()\n             bin_path = {\n                 driver.HIP: \"hsaco_path\",\n                 driver.CUDA: \"cubin\"\n@@ -597,7 +604,7 @@ def __getitem__(self, grid):\n         def runner(*args, stream=None):\n             if stream is None:\n                 if self.device_type in [\"cuda\", \"rocm\"]:\n-                    stream = triton.runtime.jit.get_cuda_stream()\n+                    stream = get_cuda_stream()\n                 else:\n                     stream = get_backend(self.device_type).get_stream(None)\n             self.c_wrapper(grid[0], grid[1], grid[2], self.num_warps, self.shared, stream, self.cu_function,"}, {"filename": "python/triton/interpreter/__init__.py", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "python/triton/debugger/__init__.py"}, {"filename": "python/triton/interpreter/core.py", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "python/triton/debugger/core.py"}, {"filename": "python/triton/interpreter/interpreter.py", "status": "renamed", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -2,13 +2,14 @@\n import random\n from typing import Tuple\n \n-import triton\n-import triton.language as tl\n+from .. import language as tl\n+# import .language.core as lcore\n+from ..language import core as lcore\n+from . import torch_wrapper\n from .core import ExecutionContext\n from .memory_map import MemoryMap\n from .tl_lang import (TritonLangProxy, WrappedTensor, _primitive_to_tensor,\n                       debugger_constexpr)\n-from triton.debugger import torch_wrapper\n \n torch = torch_wrapper.torch\n tl_method_backup = {}\n@@ -59,12 +60,12 @@ def __init__(self, func, grid=(1,)):\n         self.grid = grid\n \n     def _is_constexpr(self, name):\n-        return name in self.func.__annotations__ and self.func.__annotations__[name] is triton.language.core.constexpr\n+        return name in self.func.__annotations__ and self.func.__annotations__[name] is lcore.constexpr\n \n     def _get_constexpr(self):\n         result = []\n         for name, annotation in self.func.__annotations__.items():\n-            if annotation is triton.language.core.constexpr:\n+            if annotation is lcore.constexpr:\n                 result.append(name)\n         return result\n "}, {"filename": "python/triton/interpreter/memory_map.py", "status": "renamed", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -2,7 +2,7 @@\n \n import dataclasses\n \n-from triton.debugger import torch_wrapper\n+from triton.interpreter import torch_wrapper\n \n torch = torch_wrapper.torch\n "}, {"filename": "python/triton/interpreter/tl_lang.py", "status": "renamed", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -1,9 +1,10 @@\n from __future__ import annotations\n \n-import triton\n+# import triton\n+from ..language import core as lcore\n+from . import torch_wrapper\n from .core import ExecutionContext\n from .memory_map import MemoryMap\n-from triton.debugger import torch_wrapper\n \n torch = torch_wrapper.torch\n \n@@ -389,7 +390,7 @@ def zeros(self, shape, dtype):\n             if not isinstance(d.value, int):\n                 raise TypeError(f\"Shape element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n         shape = [x.value for x in shape]\n-        if isinstance(dtype, triton.language.core.dtype):\n+        if isinstance(dtype, lcore.dtype):\n             if dtype.is_fp32():\n                 dtype = torch.float32\n             elif dtype.is_fp16():"}, {"filename": "python/triton/interpreter/torch_wrapper.py", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "python/triton/debugger/torch_wrapper.py"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -77,7 +77,7 @@\n     static_range,\n     tensor,\n     trans,\n-    triton,\n+    # triton,\n     uint16,\n     uint32,\n     uint64,"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 16, "deletions": 15, "changes": 31, "file_content_changes": "@@ -5,9 +5,10 @@\n from functools import wraps\n from typing import Callable, List, Sequence, TypeVar\n \n-import triton\n+from .._C.libtriton.triton import ir\n+# import triton\n+from ..runtime.jit import jit\n from . import semantic\n-from triton._C.libtriton.triton import ir\n \n T = TypeVar('T')\n \n@@ -1344,7 +1345,7 @@ def _reduce_with_indices(input, axis, combine_fn, _builder=None, _generator=None\n     return rvalue, rindices\n \n \n-@triton.jit\n+@jit\n def minimum(x, y):\n     \"\"\"\n     Computes the element-wise minimum of :code:`x` and :code:`y`.\n@@ -1357,7 +1358,7 @@ def minimum(x, y):\n     return where(x < y, x, y)\n \n \n-@triton.jit\n+@jit\n def maximum(x, y):\n     \"\"\"\n     Computes the element-wise maximum of :code:`x` and :code:`y`.\n@@ -1372,20 +1373,20 @@ def maximum(x, y):\n # max and argmax\n \n \n-@triton.jit\n+@jit\n def _max_combine(a, b):\n     return maximum(a, b)\n \n \n-@triton.jit\n+@jit\n def _argmax_combine(value1, index1, value2, index2):\n     gt = value1 > value2\n     value_ret = where(gt, value1, value2)\n     index_ret = where(gt, index1, index2)\n     return value_ret, index_ret\n \n \n-@triton.jit\n+@jit\n @_add_reduction_docstr(\"maximum\")\n def max(input, axis=None, return_indices=False):\n     input = _promote_reduction_input(input)\n@@ -1395,7 +1396,7 @@ def max(input, axis=None, return_indices=False):\n         return reduce(input, axis, _max_combine)\n \n \n-@triton.jit\n+@jit\n @_add_reduction_docstr(\"maximum index\")\n def argmax(input, axis):\n     (_, ret) = max(input, axis, return_indices=True)\n@@ -1404,21 +1405,21 @@ def argmax(input, axis):\n # min and argmin\n \n \n-@triton.jit\n+@jit\n def _min_combine(a, b):\n     # TODO: minimum/maximum doesn't get lowered to fmin/fmax...\n     return minimum(a, b)\n \n \n-@triton.jit\n+@jit\n def _argmin_combine(value1, index1, value2, index2):\n     lt = value1 < value2\n     value_ret = where(lt, value1, value2)\n     index_ret = where(lt, index1, index2)\n     return value_ret, index_ret\n \n \n-@triton.jit\n+@jit\n @_add_reduction_docstr(\"minimum\")\n def min(input, axis=None, return_indices=False):\n     input = _promote_reduction_input(input)\n@@ -1428,28 +1429,28 @@ def min(input, axis=None, return_indices=False):\n         return reduce(input, axis, _min_combine)\n \n \n-@triton.jit\n+@jit\n @_add_reduction_docstr(\"minimum index\")\n def argmin(input, axis):\n     _, ret = min(input, axis, return_indices=True)\n     return ret\n \n \n-@triton.jit\n+@jit\n def _sum_combine(a, b):\n     return a + b\n \n # sum\n \n \n-@triton.jit\n+@jit\n @_add_reduction_docstr(\"sum\")\n def sum(input, axis=None):\n     input = _promote_reduction_input(input)\n     return reduce(input, axis, _sum_combine)\n \n \n-@triton.jit\n+@jit\n def _xor_combine(a, b):\n     return a ^ b\n "}, {"filename": "python/triton/language/random.py", "status": "modified", "additions": 12, "deletions": 12, "changes": 24, "file_content_changes": "@@ -1,4 +1,4 @@\n-import triton\n+from ..runtime.jit import jit\n from . import core as tl\n \n PHILOX_KEY_A: tl.constexpr = 0x9E3779B9\n@@ -12,7 +12,7 @@\n # -------------------\n \n \n-@triton.jit\n+@jit\n def philox_impl(c0, c1, c2, c3, k0, k1, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     \"\"\"\n     Run `n_rounds` rounds of Philox for state (c0, c1, c2, c3) and key (k0, k1).\n@@ -33,7 +33,7 @@ def philox_impl(c0, c1, c2, c3, k0, k1, n_rounds: tl.constexpr = N_ROUNDS_DEFAUL\n     return c0, c1, c2, c3\n \n \n-@triton.jit\n+@jit\n def philox(seed, c0, c1, c2, c3, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     seed = seed.to(tl.uint64)\n     seed_hi = ((seed >> 32) & 0xffffffff).to(tl.uint32)\n@@ -45,7 +45,7 @@ def philox(seed, c0, c1, c2, c3, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     return philox_impl(c0, c1, c2, c3, seed_lo, seed_hi, n_rounds)\n \n \n-@triton.jit\n+@jit\n def randint(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     \"\"\"\n     Given a :code:`seed` scalar and an :code:`offset` block, returns a single\n@@ -61,7 +61,7 @@ def randint(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     return ret\n \n \n-@triton.jit\n+@jit\n def randint4x(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     \"\"\"\n     Given a :code:`seed` scalar and an :code:`offset` block, returns four\n@@ -82,15 +82,15 @@ def randint4x(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n # rand\n # -------------------\n \n-# @triton.jit\n+# @jit\n # def uint32_to_uniform_float(x):\n #     \"\"\"\n #     Numerically stable function to convert a random uint32 into a random float uniformly sampled in [0, 1).\n #     \"\"\"\n #     two_to_the_minus_32: tl.constexpr = 2.328306e-10\n #     return x * two_to_the_minus_32\n \n-@triton.jit\n+@jit\n def uint32_to_uniform_float(x):\n     \"\"\"\n     Numerically stable function to convert a random uint32 into a random float uniformly sampled in [0, 1).\n@@ -102,7 +102,7 @@ def uint32_to_uniform_float(x):\n     return x * scale\n \n \n-@triton.jit\n+@jit\n def rand(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     \"\"\"\n     Given a :code:`seed` scalar and an :code:`offset` block,\n@@ -116,7 +116,7 @@ def rand(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     return uint32_to_uniform_float(source)\n \n \n-@triton.jit\n+@jit\n def rand4x(seed, offsets, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     \"\"\"\n     Given a :code:`seed` scalar and an :code:`offsets` block,\n@@ -138,7 +138,7 @@ def rand4x(seed, offsets, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n # -------------------\n \n \n-@triton.jit\n+@jit\n def pair_uniform_to_normal(u1, u2):\n     \"\"\"Box-Muller transform\"\"\"\n     u1 = tl.maximum(1.0e-7, u1)\n@@ -147,7 +147,7 @@ def pair_uniform_to_normal(u1, u2):\n     return r * tl.cos(th), r * tl.sin(th)\n \n \n-@triton.jit\n+@jit\n def randn(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     \"\"\"\n     Given a :code:`seed` scalar and an :code:`offset` block,\n@@ -163,7 +163,7 @@ def randn(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     return n1\n \n \n-@triton.jit\n+@jit\n def randn4x(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     \"\"\"\n     Given a :code:`seed` scalar and an :code:`offset` block,"}, {"filename": "python/triton/ops/blocksparse/matmul.py", "status": "modified", "additions": 9, "deletions": 6, "changes": 15, "file_content_changes": "@@ -1,7 +1,10 @@\n import torch\n \n-import triton\n-import triton.language as tl\n+from ... import cdiv, heuristics, jit\n+from ... import language as tl\n+\n+# import triton\n+# import language as tl\n \n # ********************************************************\n # --------------------------------------------------------\n@@ -13,10 +16,10 @@\n # ********************************************************\n \n \n-@triton.heuristics({\n+@heuristics({\n     'EVEN_K': lambda nargs: nargs['K'] % nargs['TILE_K'] == 0,\n })\n-@triton.jit\n+@jit\n def _sdd_kernel(\n     A, B, C,\n     stride_za, stride_ha, stride_ma, stride_ak,\n@@ -127,7 +130,7 @@ def sdd_lut(layout, block, device):\n # -----------------------------\n \n \n-@triton.jit\n+@jit\n def _dsd_kernel(\n     A, B, C,\n     stride_az, stride_ha, stride_am, stride_ak,\n@@ -227,7 +230,7 @@ def dsd_matmul(a, b, trans_a, trans_b, trans_c, spdims, block, lut, width, out=N\n     # meta-parameter heuristics\n     TILE_N = 128\n     # compute output\n-    grid = lambda meta: [triton.cdiv(BS3, meta['TILE_N']), width, BS0]\n+    grid = lambda meta: [cdiv(BS3, meta['TILE_N']), width, BS0]\n     _dsd_kernel[grid](\n         a, b, c,\n         a.stride(0), a.stride(1), a.stride(3 if trans_a else 2), a.stride(2 if trans_a else 3),"}, {"filename": "python/triton/ops/blocksparse/softmax.py", "status": "modified", "additions": 9, "deletions": 6, "changes": 15, "file_content_changes": "@@ -1,7 +1,10 @@\n import torch\n \n-import triton\n-import triton.language as tl\n+# import triton\n+# import language as tl\n+from ... import jit\n+from ... import language as tl\n+from ... import next_power_of_2\n \n \n def num_warps(n):\n@@ -16,7 +19,7 @@ def num_warps(n):\n     return 16\n \n \n-@triton.jit\n+@jit\n def _blocksparse_softmax_fwd(\n     Out, A, stride_xz, LUT,\n     R, extent, stride_zr, stride_hr,  # relative attention\n@@ -71,7 +74,7 @@ def _blocksparse_softmax_fwd(\n     tl.store(Out + off_a + lane_n, out, mask=mask)\n \n \n-@triton.jit\n+@jit\n def _blocksparse_softmax_bwd(\n     DA, stride_zdx,\n     DOut, stride_zdout,\n@@ -169,7 +172,7 @@ def forward(\n             scale,\n             is_causal,\n             BLOCK_SIZE=block,\n-            ROW_SIZE=triton.next_power_of_2(maxlut),\n+            ROW_SIZE=next_power_of_2(maxlut),\n             IS_DENSE=is_dense,\n             num_warps=num_warps(maxlut)\n         )\n@@ -208,7 +211,7 @@ def backward(ctx, dout):\n             dr, ctx.rel_shape[-1], ctx.rel_strides[0], ctx.rel_strides[1], ctx.rel_strides[2],\n             ctx.is_causal,\n             BLOCK_SIZE=ctx.block,\n-            ROW_SIZE=triton.next_power_of_2(ctx.maxlut),\n+            ROW_SIZE=next_power_of_2(ctx.maxlut),\n             IS_DENSE=ctx.is_dense,\n             num_warps=num_warps(ctx.maxlut)\n         )"}, {"filename": "python/triton/ops/cross_entropy.py", "status": "modified", "additions": 11, "deletions": 8, "changes": 19, "file_content_changes": "@@ -1,7 +1,10 @@\n import torch\n \n-import triton\n-import triton.language as tl\n+# import triton\n+# import language as tl\n+from .. import heuristics, jit\n+from .. import language as tl\n+from .. import next_power_of_2\n \n \n def num_warps(N):\n@@ -12,9 +15,9 @@ def num_warps(N):\n     return 16\n \n \n-@triton.heuristics({'num_warps': lambda nargs: num_warps(nargs['N'])})\n-@triton.heuristics({'BLOCK': lambda nargs: triton.next_power_of_2(nargs['N'])})\n-@triton.jit\n+@heuristics({'num_warps': lambda nargs: num_warps(nargs['N'])})\n+@heuristics({'BLOCK': lambda nargs: next_power_of_2(nargs['N'])})\n+@jit\n def _forward(LOGITS, PROBS, IDX, LOSS, N, BLOCK: tl.constexpr):\n     row = tl.program_id(0)\n     cols = tl.arange(0, BLOCK)\n@@ -37,9 +40,9 @@ def _forward(LOGITS, PROBS, IDX, LOSS, N, BLOCK: tl.constexpr):\n     tl.store(LOSS + row, probs)\n \n \n-@triton.heuristics({'num_warps': lambda nargs: num_warps(nargs['N'])})\n-@triton.heuristics({'BLOCK': lambda nargs: triton.next_power_of_2(nargs['N'])})\n-@triton.jit\n+@heuristics({'num_warps': lambda nargs: num_warps(nargs['N'])})\n+@heuristics({'BLOCK': lambda nargs: next_power_of_2(nargs['N'])})\n+@jit\n def _backward(PROBS, IDX, DPROBS, N, BLOCK: tl.constexpr):\n     row = tl.program_id(0)\n     cols = tl.arange(0, BLOCK)"}, {"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 9, "deletions": 6, "changes": 15, "file_content_changes": "@@ -7,11 +7,14 @@\n \n import torch\n \n-import triton\n-import triton.language as tl\n+from .. import cdiv, jit\n+from .. import language as tl\n \n+# import triton\n+# import language as tl\n \n-@triton.jit\n+\n+@jit\n def _fwd_kernel(\n     Q, K, V, sm_scale,\n     L, M,\n@@ -87,7 +90,7 @@ def _fwd_kernel(\n     tl.store(out_ptrs, acc)\n \n \n-@triton.jit\n+@jit\n def _bwd_preprocess(\n     Out, DO, L,\n     NewDO, Delta,\n@@ -107,7 +110,7 @@ def _bwd_preprocess(\n     tl.store(Delta + off_m, delta)\n \n \n-@triton.jit\n+@jit\n def _bwd_kernel(\n     Q, K, V, sm_scale, Out, DO,\n     DQ, DK, DV,\n@@ -205,7 +208,7 @@ def forward(ctx, q, k, v, sm_scale):\n         assert Lq == Lk and Lk == Lv\n         assert Lk in {16, 32, 64, 128}\n         o = torch.empty_like(q)\n-        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n+        grid = (cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         num_warps = 4 if Lk <= 64 else 8"}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 32, "deletions": 28, "changes": 60, "file_content_changes": "@@ -1,9 +1,13 @@\n import torch\n \n-import triton\n-import triton.language as tl\n+from .. import Config, autotune, cdiv, heuristics, jit\n+from .. import language as tl\n from .matmul_perf_model import early_config_prune, estimate_matmul_time\n \n+# import triton\n+# import language as tl\n+\n+\n _ordered_datatypes = [torch.float16, torch.bfloat16, torch.float32]\n \n \n@@ -33,37 +37,37 @@ def get_configs_io_bound():\n                 for block_n in [32, 64, 128, 256]:\n                     num_warps = 2 if block_n <= 64 else 4\n                     configs.append(\n-                        triton.Config({'BLOCK_M': block_m, 'BLOCK_N': block_n, 'BLOCK_K': block_k, 'SPLIT_K': 1},\n-                                      num_stages=num_stages, num_warps=num_warps))\n+                        Config({'BLOCK_M': block_m, 'BLOCK_N': block_n, 'BLOCK_K': block_k, 'SPLIT_K': 1},\n+                               num_stages=num_stages, num_warps=num_warps))\n                     # split_k\n                     for split_k in [2, 4, 8, 16]:\n-                        configs.append(triton.Config({'BLOCK_M': block_m, 'BLOCK_N': block_n, 'BLOCK_K': block_k, 'SPLIT_K': split_k},\n-                                                     num_stages=num_stages, num_warps=num_warps, pre_hook=init_to_zero('C')))\n+                        configs.append(Config({'BLOCK_M': block_m, 'BLOCK_N': block_n, 'BLOCK_K': block_k, 'SPLIT_K': split_k},\n+                                              num_stages=num_stages, num_warps=num_warps, pre_hook=init_to_zero('C')))\n     return configs\n \n \n-@triton.autotune(\n+@autotune(\n     configs=[\n         # basic configs for compute-bound matmuls\n-        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8),\n-        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8),\n-        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=5, num_warps=2),\n+        Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8),\n+        Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8),\n+        Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=5, num_warps=2),\n         # good for int8\n-        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8),\n-        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8),\n-        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=5, num_warps=2),\n+        Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8),\n+        Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8),\n+        Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=5, num_warps=2),\n     ] + get_configs_io_bound(),\n     key=['M', 'N', 'K'],\n     prune_configs_by={\n@@ -72,10 +76,10 @@ def get_configs_io_bound():\n         'top_k': 10\n     },\n )\n-@triton.heuristics({\n+@heuristics({\n     'EVEN_K': lambda args: args['K'] % (args['BLOCK_K'] * args['SPLIT_K']) == 0,\n })\n-@triton.jit\n+@jit\n def _kernel(A, B, C, M, N, K,\n             stride_am, stride_ak,\n             stride_bk, stride_bn,\n@@ -165,7 +169,7 @@ def _call(a, b, dot_out_dtype):\n             else:\n                 dot_out_dtype = tl.int32\n         # launch kernel\n-        grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']), META['SPLIT_K'])\n+        grid = lambda META: (cdiv(M, META['BLOCK_M']) * cdiv(N, META['BLOCK_N']), META['SPLIT_K'])\n         _kernel[grid](a, b, c, M, N, K,\n                       a.stride(0), a.stride(1),\n                       b.stride(0), b.stride(1),"}, {"filename": "python/triton/ops/matmul_perf_model.py", "status": "modified", "additions": 8, "deletions": 7, "changes": 15, "file_content_changes": "@@ -2,10 +2,11 @@\n \n import torch\n \n-import triton\n-import triton._C.libtriton.triton as _triton\n-from triton.runtime import driver\n-from triton.testing import get_dram_gbps, get_max_simd_tflops, get_max_tensorcore_tflops\n+# import triton\n+from .. import cdiv\n+from .._C.libtriton.triton import runtime\n+from ..runtime import driver\n+from ..testing import get_dram_gbps, get_max_simd_tflops, get_max_tensorcore_tflops\n \n \n def get_tensorcore_tflops(backend, device, num_ctas, num_warps, dtype):\n@@ -41,13 +42,13 @@ def estimate_matmul_time(\n ):\n     ''' return estimated running time in ms\n           = max(compute, loading) + store '''\n-    backend = _triton.runtime.backend.CUDA\n+    backend = runtime.backend.CUDA\n     device = torch.cuda.current_device()\n     dtype = A.dtype\n     dtsize = A.element_size()\n \n-    num_cta_m = triton.cdiv(M, BLOCK_M)\n-    num_cta_n = triton.cdiv(N, BLOCK_N)\n+    num_cta_m = cdiv(M, BLOCK_M)\n+    num_cta_n = cdiv(N, BLOCK_N)\n     num_cta_k = SPLIT_K\n     num_ctas = num_cta_m * num_cta_n * num_cta_k\n "}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 17, "deletions": 13, "changes": 30, "file_content_changes": "@@ -11,10 +11,12 @@\n from typing import (Callable, Generic, Iterable, List, Optional, TypeVar, Union, cast,\n                     overload)\n \n-import torch\n+# import triton\n+# from .. import compile, CompiledKernel\n+from ..common.backend import get_backend\n \n-import triton\n-from triton.common.backend import get_backend\n+TRITON_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n+TRITON_VERSION = \"2.1.0\"\n \n \n def get_cuda_stream(idx=None):\n@@ -69,7 +71,7 @@ def visit_Attribute(self, node):\n         lhs = self.visit(node.value)\n         while isinstance(lhs, ast.Attribute):\n             lhs = self.visit(lhs.value)\n-        if lhs is None or lhs is triton:\n+        if lhs is None or lhs.__name__ == \"triton\":\n             return None\n         return getattr(lhs, node.attr)\n \n@@ -104,15 +106,15 @@ def version_key():\n     with open(__file__, \"rb\") as f:\n         contents += [hashlib.md5(f.read()).hexdigest()]\n     # compiler\n-    compiler_path = os.path.join(*triton.__path__, 'compiler')\n+    compiler_path = os.path.join(TRITON_PATH, 'compiler')\n     for lib in pkgutil.iter_modules([compiler_path]):\n         with open(lib.module_finder.find_spec(lib.name).origin, \"rb\") as f:\n             contents += [hashlib.md5(f.read()).hexdigest()]\n     # backend\n-    with open(triton._C.libtriton.__file__, \"rb\") as f:\n+    with open(os.path.join(TRITON_PATH, \"_C/libtriton.so\"), \"rb\") as f:\n         contents += [hashlib.md5(f.read()).hexdigest()]\n     # language\n-    language_path = os.path.join(*triton.__path__, 'language')\n+    language_path = os.path.join(TRITON_PATH, 'language')\n     for lib in pkgutil.iter_modules([language_path]):\n         with open(lib.module_finder.find_spec(lib.name).origin, \"rb\") as f:\n             contents += [hashlib.md5(f.read()).hexdigest()]\n@@ -121,7 +123,7 @@ def version_key():\n         ptxas_version = hashlib.md5(subprocess.check_output([\"ptxas\", \"--version\"])).hexdigest()\n     except Exception:\n         ptxas_version = ''\n-    return '-'.join(triton.__version__) + '-' + ptxas_version + '-' + '-'.join(contents)\n+    return '-'.join(TRITON_VERSION) + '-' + ptxas_version + '-' + '-'.join(contents)\n \n \n class KernelInterface(Generic[T]):\n@@ -285,6 +287,7 @@ def _conclude_device_type(self, device_types: List[str], pinned_memory_flags: Li\n         device_types = [device_type for device_type in device_types if device_type != '']\n         # Return cuda if one of the input tensors is cuda\n         if 'cuda' in device_types:\n+            import torch\n             return 'hip' if torch.version.hip else 'cuda'\n \n         is_cpu = all(device_type == 'cpu' for device_type in device_types)\n@@ -317,6 +320,7 @@ def _make_launcher(self):\n \n         src = f\"\"\"\n def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False, device=None, device_type=None):\n+    from ..compiler import compile, CompiledKernel\n     sig_key =  {sig_keys},\n     constexpr_key = {f'{constexpr_keys},' if len(constexpr_keys) > 0 else ()}\n     spec_key = {f'{spec_keys},' if len(spec_keys) > 0 else ()}\n@@ -357,7 +361,7 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n     bin = cache[device].get(key, None)\n     if bin is not None:\n       if not warmup:\n-          bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.shared, stream, bin.cu_function, triton.compiler.CompiledKernel.launch_enter_hook, triton.compiler.CompiledKernel.launch_exit_hook, bin, {args})\n+          bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.shared, stream, bin.cu_function, CompiledKernel.launch_enter_hook, CompiledKernel.launch_exit_hook, bin, {args})\n       return bin\n     # kernel not cached -- compile\n     else:\n@@ -375,9 +379,9 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n         if callable(arg):\n           raise TypeError(f\"Callable constexpr at index {{i}} is not supported\")\n       if not self._call_hook(key, signature, device, constants, num_warps, num_stages, extern_libs, configs):\n-        bin = triton.compile(self, signature=signature, device=device, constants=constants, num_warps=num_warps, num_stages=num_stages, extern_libs=extern_libs, configs=configs, debug=self.debug, device_type=device_type)\n+        bin = compile(self, signature=signature, device=device, constants=constants, num_warps=num_warps, num_stages=num_stages, extern_libs=extern_libs, configs=configs, debug=self.debug, device_type=device_type)\n         if not warmup:\n-            bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.shared, stream, bin.cu_function, triton.compiler.CompiledKernel.launch_enter_hook, triton.compiler.CompiledKernel.launch_exit_hook, bin, *args)\n+            bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.shared, stream, bin.cu_function, CompiledKernel.launch_enter_hook, CompiledKernel.launch_exit_hook, bin, *args)\n         self.cache[device][key] = bin\n         return bin\n       return None\n@@ -390,7 +394,7 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n                  \"_device_of\": self._device_of,\n                  \"_pinned_memory_of\": self._pinned_memory_of,\n                  \"cache\": self.cache,\n-                 \"triton\": triton,\n+                 \"__spec__\": __spec__,\n                  \"get_backend\": get_backend,\n                  \"get_current_device\": get_current_device,\n                  \"set_current_device\": set_current_device}\n@@ -524,7 +528,7 @@ def jit(\n     def decorator(fn: T) -> JITFunction[T]:\n         assert callable(fn)\n         if interpret:\n-            from ..debugger.debugger import GridSelector\n+            from ..interpreter.interpreter import GridSelector\n             return GridSelector(fn)\n         else:\n             return JITFunction("}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -4,7 +4,7 @@\n import sys\n from contextlib import contextmanager\n \n-import triton._C.libtriton.triton as _triton\n+from ._C.libtriton.triton import runtime\n \n \n def nvsmi(attrs):\n@@ -281,7 +281,7 @@ def get_dram_gbps(backend=None, device=None):\n \n     from .runtime import driver\n     if not backend:\n-        backend = _triton.runtime.backend.CUDA\n+        backend = runtime.backend.CUDA\n     if not device:\n         device = torch.cuda.current_device()\n     mem_clock_khz = driver.utils.get_device_properties(device)[\"mem_clock_rate\"]  # in kHz\n@@ -295,7 +295,7 @@ def get_max_tensorcore_tflops(dtype, backend=None, device=None, clock_rate=None)\n \n     from .runtime import driver\n     if not backend:\n-        backend = _triton.runtime.backend.CUDA\n+        backend = runtime.backend.CUDA\n     if not device:\n         device = torch.cuda.current_device()\n \n@@ -398,7 +398,7 @@ def get_max_simd_tflops(dtype, backend=None, device=None):\n \n     from .runtime import driver\n     if not backend:\n-        backend = _triton.runtime.backend.CUDA\n+        backend = runtime.backend.CUDA\n     if not device:\n         device = torch.cuda.current_device()\n "}, {"filename": "python/triton/tools/aot.py", "status": "modified", "additions": 18, "deletions": 15, "changes": 33, "file_content_changes": "@@ -1,8 +1,11 @@\n import argparse\n import sys\n \n-import triton._C.libtriton.triton as libtriton\n-import triton.compiler.compiler as tc\n+from .._C.libtriton.triton import ir\n+# import triton.compiler.compiler as tc\n+from ..compiler.compiler import (get_amdgpu_arch_fulldetails, llir_to_amdgcn_and_hsaco,\n+                                 llir_to_ptx, optimize_ttgir, optimize_ttir,\n+                                 ttgir_to_llir, ttir_to_ttgir)\n \n if __name__ == '__main__':\n \n@@ -32,12 +35,12 @@\n         sys.exit(0)\n \n     # parse source file to MLIR module\n-    context = libtriton.ir.context()\n-    module = libtriton.ir.parse_mlir_module(args.src, context)\n+    context = ir.context()\n+    module = ir.parse_mlir_module(args.src, context)\n     module.context = context\n \n     # optimizer triton-ir\n-    module = tc.optimize_ttir(module, arch=args.sm)\n+    module = optimize_ttir(module, arch=args.sm)\n     if args.target == 'triton-ir':\n         print(module.str())\n         sys.exit(0)\n@@ -49,7 +52,7 @@\n     if args.target == 'amdgcn':\n         # auto detect available architecture and features\n         # if nothing detected, set with default values\n-        arch_details = tc.get_amdgpu_arch_fulldetails()\n+        arch_details = get_amdgpu_arch_fulldetails()\n         if not arch_details:\n             arch_name = \"\"\n             arch_triple = \"amdgcn-amd-amdhsa\"\n@@ -71,13 +74,13 @@\n \n         # triton-ir -> triton-gpu-ir\n         # use compute_capability == 80\n-        module = tc.ttir_to_ttgir(module, num_warps=args.num_warps)  # num_stages=3, compute_capability=80)\n-        module = tc.optimize_ttgir(module, num_stages=3, arch=80)\n+        module = ttir_to_ttgir(module, num_warps=args.num_warps)  # num_stages=3, compute_capability=80)\n+        module = optimize_ttgir(module, num_stages=3, arch=80)\n         # triton-gpu-ir -> llvm-ir\n         # use compute_capability == 80\n-        module = tc.ttgir_to_llir(module, extern_libs=None, arch=80)\n+        module = ttgir_to_llir(module, extern_libs=None, arch=80)\n         # llvm-ir -> amdgcn asm, hsaco binary\n-        module, hsaco_path = tc.llir_to_amdgcn_and_hsaco(module, arch_name, arch_triple, arch_features)\n+        module, hsaco_path = llir_to_amdgcn_and_hsaco(module, arch_name, arch_triple, arch_features)\n \n         print(hsaco_path)\n         print(module)\n@@ -87,14 +90,14 @@\n         raise argparse.ArgumentError(None, \"Must specify --sm for PTX compilation\")\n \n     # triton-ir -> triton-gpu-ir\n-    module = tc.ttir_to_ttgir(module, num_warps=args.num_warps)\n-    module = tc.optimize_ttgir(module, num_stages=3, arch=args.sm)\n+    module = ttir_to_ttgir(module, num_warps=args.num_warps)\n+    module = optimize_ttgir(module, num_stages=3, arch=args.sm)\n     if args.target == 'triton-gpu-ir':\n         print(module.str())\n         sys.exit(0)\n \n     # triton-gpu-ir -> llvm-ir\n-    module = tc.ttgir_to_llir(module, extern_libs=None, arch=args.sm)\n+    module = ttgir_to_llir(module, extern_libs=None, arch=args.sm)\n     if args.target == 'llvm-ir':\n         print(module)\n         sys.exit(0)\n@@ -103,12 +106,12 @@\n     if args.target == 'ptx':\n         if not args.ptx_version:\n             raise argparse.ArgumentError(None, \"Must specify --ptx-version for PTX compilation\")\n-        module = tc.llir_to_ptx(module, arch=args.sm, ptx_version=args.ptx_version)\n+        module = llir_to_ptx(module, arch=args.sm, ptx_version=args.ptx_version)\n \n     # llvm-ir -> amdgcn\n     if args.target == 'amdgcn':\n         if not args.gfx:\n             raise argparse.ArgumentError(None, \"Must specify --gfx for AMDGCN compilation\")\n-        module, hsaco_path = tc.llir_to_amdgcn_and_hsaco(module, args.gfx)\n+        module, hsaco_path = llir_to_amdgcn_and_hsaco(module, args.gfx)\n \n     print(module)"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "@@ -81,6 +81,21 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // -----\n \n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: store_with_cache_attr\n+  tt.func @store_with_cache_attr(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>, %cst_0 : tensor<256xf32, #blocked0>) {\n+    //      CHECK: llvm.inline_asm\n+    // CHECK-SAME: st.global.L1::evict_last.b32\n+    //      CHECK: llvm.inline_asm\n+    // CHECK-SAME: st.global.L1::evict_last.b32\n+    tt.store %a_ptr_init, %cst_0, %cst {cache = 1 : i32, evict = 3 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n   // CHECK-LABEL: global_load_store_no_vec"}]