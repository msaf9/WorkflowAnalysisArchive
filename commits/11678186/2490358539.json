[{"filename": "include/triton/ir/instructions.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -913,7 +913,7 @@ class sqrt_inst: public builtin_inst {\n class reduce_inst: public builtin_inst {\n public:\n   enum op_t{\n-    ADD, SUB, MAX, MIN,\n+    ADD, SUB, MAX, MIN, UMAX, UMIN,\n     FADD, FSUB, FMAX, FMIN,\n     XOR\n   };"}, {"filename": "lib/codegen/selection/generator.cc", "status": "modified", "additions": 9, "deletions": 3, "changes": 12, "file_content_changes": "@@ -119,6 +119,8 @@ Value* geper::operator()(Value *ptr, Value* off, const std::string& name){\n #define icmp_eq(...)         builder_->CreateICmpEQ(__VA_ARGS__)\n #define icmp_sge(...)        builder_->CreateICmpSGE(__VA_ARGS__)\n #define icmp_sle(...)        builder_->CreateICmpSLE(__VA_ARGS__)\n+#define icmp_uge(...)        builder_->CreateICmpUGE(__VA_ARGS__)\n+#define icmp_ule(...)        builder_->CreateICmpULE(__VA_ARGS__)\n #define icmp_ult(...)        builder_->CreateICmpULT(__VA_ARGS__)\n #define insert_elt(...)      builder_->CreateInsertElement(__VA_ARGS__)\n #define intrinsic(...)       builder_->CreateIntrinsic(__VA_ARGS__)\n@@ -2498,6 +2500,8 @@ void generator::visit_reduce_inst(ir::reduce_inst* x) {\n     case ir::reduce_inst::SUB: return sub(x, y);\n     case ir::reduce_inst::MAX: return select(icmp_sge(x, y), x, y);\n     case ir::reduce_inst::MIN: return select(icmp_sle(x, y), x, y);\n+    case ir::reduce_inst::UMAX: return select(icmp_uge(x, y), x, y);\n+    case ir::reduce_inst::UMIN: return select(icmp_ule(x, y), x, y);\n     case ir::reduce_inst::FADD: return fadd(x, y);\n     case ir::reduce_inst::FSUB: return fsub(x, y);\n     case ir::reduce_inst::FMAX: return max_num(x, y);\n@@ -2510,9 +2514,11 @@ void generator::visit_reduce_inst(ir::reduce_inst* x) {\n   Value *neutral;\n   switch(op) {\n     case ir::reduce_inst::ADD: neutral = ConstantInt::get(ty, 0); break;\n-    case ir::reduce_inst::SUB:  neutral = ConstantInt::get(ty, 0); break;\n-    case ir::reduce_inst::MAX:  neutral = ConstantInt::get(ty, INT32_MIN); break;\n-    case ir::reduce_inst::MIN:  neutral = ConstantInt::get(ty, INT32_MAX); break;\n+    case ir::reduce_inst::SUB: neutral = ConstantInt::get(ty, 0); break;\n+    case ir::reduce_inst::MAX: neutral = ConstantInt::get(ty, INT32_MIN); break;\n+    case ir::reduce_inst::MIN: neutral = ConstantInt::get(ty, INT32_MAX); break;\n+    case ir::reduce_inst::UMAX: neutral = ConstantInt::get(ty, 0); break;\n+    case ir::reduce_inst::UMIN: neutral = ConstantInt::get(ty, UINT32_MAX); break;\n     case ir::reduce_inst::FADD: neutral = ConstantFP::get(ty, 0); break;\n     case ir::reduce_inst::FSUB: neutral = ConstantFP::get(ty, 0); break;\n     case ir::reduce_inst::FMAX: neutral = ConstantFP::get(ty, -INFINITY); break;"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -571,6 +571,8 @@ void init_triton_ir(py::module &&m) {\n       .value(\"FADD\", ir::reduce_inst::FADD)\n       .value(\"MIN\", ir::reduce_inst::MIN)\n       .value(\"MAX\", ir::reduce_inst::MAX)\n+      .value(\"UMIN\", ir::reduce_inst::UMIN)\n+      .value(\"UMAX\", ir::reduce_inst::UMAX)\n       .value(\"FMIN\", ir::reduce_inst::FMIN)\n       .value(\"FMAX\", ir::reduce_inst::FMAX)\n       .value(\"XOR\", ir::reduce_inst::XOR);"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 38, "deletions": 20, "changes": 58, "file_content_changes": "@@ -684,60 +684,78 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n # ---------------\n \n \n-@pytest.mark.parametrize(\"dtype_str, shape\",\n-                         [(dtype, shape)\n+@pytest.mark.parametrize(\"op, dtype_str, shape\",\n+                         [(op, dtype, shape)\n+                          for op in ['min', 'max', 'sum']\n                           for dtype in dtypes\n                           for shape in [32, 64, 128, 512]])\n-def test_reduce1d(dtype_str, shape, device='cuda'):\n+def test_reduce1d(op, dtype_str, shape, device='cuda'):\n \n     # triton kernel\n     @triton.jit\n     def kernel(X, Z, BLOCK: tl.constexpr):\n         x = tl.load(X + tl.arange(0, BLOCK))\n-        tl.store(Z, tl.sum(x, axis=0))\n+        tl.store(Z, GENERATE_TEST_HERE)\n \n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=0)'})\n+    # input\n     rs = RandomState(17)\n+    # limit the range of integers so that the sum does not overflow\n     x = numpy_random((shape,), dtype_str=dtype_str, rs=rs)\n-    x[:] = 1\n+    x_tri = to_triton(x, device=device)\n+    numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min}[op]\n     # numpy result\n-    z_ref = np.sum(x).astype(getattr(np, dtype_str))\n+    z_ref = numpy_op(x).astype(getattr(np, dtype_str))\n     # triton result\n-    x_tri = to_triton(x, device=device)\n     z_tri = to_triton(numpy_random((1,), dtype_str=dtype_str, rs=rs), device=device)\n     kernel[(1,)](x_tri, z_tri, BLOCK=shape)\n     # compare\n-    np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+    if op == 'sum':\n+        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+    else:\n+        np.testing.assert_equal(z_ref, to_numpy(z_tri))\n \n \n reduce_configs1 = [\n-    (dtype, (1, 1024), axis) for dtype in ['float32', 'uint32']\n+    (op, dtype, (1, 1024), axis) for dtype in dtypes\n+    for op in ['min', 'max', 'sum']\n     for axis in [1]\n ]\n reduce_configs2 = [\n-    ('float32', shape, 1) for shape in [(2, 32), (4, 128), (32, 64), (64, 128), (128, 256), (32, 1024)]\n+    (op, 'float32', shape, 1)\n+    for op in ['min', 'max', 'sum']\n+    for shape in [(2, 32), (4, 32), (4, 128), (32, 64), (64, 128), (128, 256), (32, 1024)]\n ]\n \n \n-@pytest.mark.parametrize(\"dtype_str, shape, axis\", reduce_configs1 + reduce_configs2)\n-def test_reduce2d(dtype_str, shape, axis, device='cuda'):\n+@pytest.mark.parametrize(\"op, dtype_str, shape, axis\", reduce_configs1 + reduce_configs2)\n+def test_reduce2d(op, dtype_str, shape, axis, device='cuda'):\n     # triton kernel\n     @triton.jit\n     def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexpr):\n         range_m = tl.arange(0, BLOCK_M)\n         range_n = tl.arange(0, BLOCK_N)\n         x = tl.load(X + range_m[:, None] * BLOCK_N + range_n[None, :])\n-        z = tl.sum(x, axis=AXIS)\n+        z = GENERATE_TEST_HERE\n         tl.store(Z + range_m, z)\n+\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=AXIS)'})\n     # input\n-    x = numpy_random(shape, dtype_str=dtype_str)\n-    # triton result\n+    rs = RandomState(17)\n+    # limit the range of integers so that the sum does not overflow\n+    x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n     x_tri = to_triton(x)\n-    z_tri = to_triton(np.empty((shape[0],), dtype=getattr(np, dtype_str)), device=device)\n-    kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis)\n-    # numpy reference result\n-    z_ref = np.sum(x, axis=axis).astype(x.dtype)\n+    numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min}[op]\n+    # numpy result\n+    z_ref = numpy_op(x, axis=axis).astype(getattr(np, dtype_str))\n+    # triton result\n+    z_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device=device)\n+    binary = kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis)\n     # compare\n-    np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+    if op == 'sum':\n+        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+    else:\n+        np.testing.assert_equal(z_ref, to_numpy(z_tri))\n \n # ---------------\n # test permute"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -136,6 +136,9 @@ def is_floating(self):\n     def is_int_signed(self):\n         return self.name in dtype.SINT_TYPES\n \n+    def is_int_unsigned(self):\n+        return self.name in dtype.UINT_TYPES\n+\n     def is_int(self):\n         return self.name in dtype.SINT_TYPES + dtype.UINT_TYPES\n "}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -962,6 +962,13 @@ def reduce_impl(input: tl.tensor, axis: int, builder: ir.builder, name: str,\n     if scalar_ty.is_int() and scalar_ty.int_bitwidth <= 32:\n         input = cast(input, tl.int32, builder)\n \n+    # choose the right unsigned operation\n+    if scalar_ty.is_int_unsigned():\n+        if INT_OP is ir.REDUCE_OP.MIN:\n+            INT_OP = ir.REDUCE_OP.UMIN\n+        elif INT_OP is ir.REDUCE_OP.MAX:\n+            INT_OP = ir.REDUCE_OP.UMAX\n+\n     # get result type\n     shape = input.type.shape\n     ret_shape = []"}]