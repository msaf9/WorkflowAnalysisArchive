[{"filename": "python/tests/test_core.py", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -495,7 +495,11 @@ def make_ptr_str(name, shape):\n # TODO: handle `%4 = triton_gpu.convert_layout %3 : (tensor<32xi32, #blocked0>) -> tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>``\n @pytest.mark.parametrize(\"expr, dtype_str\", [\n     (f'x[{s}]', d)\n-    for s in ['None, :', ':, None', 'None, :, :', ':, :, None']\n+    for s in ['None, :', ':, None', \n+              # TODO: 3D\n+              #  'None, :, :', \n+              #  ':, :, None'\n+              ]\n     for d in ['int32', 'uint32', 'uint16']\n ])\n def test_index1d(expr, dtype_str, device='cuda'):"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -996,6 +996,7 @@ def _compile(fn, signature: str, device: int = -1, constants=dict(), specializat\n     if output == \"ttgir\":\n         return module.str()\n \n+    print(module.str())\n     if extern_libs:\n         add_external_libs(module, extern_libs)\n "}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 26, "deletions": 0, "changes": 26, "file_content_changes": "@@ -715,6 +715,32 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   }\n }\n \n+// -----\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: convert_blocked1d_to_slice0\n+  func @convert_blocked1d_to_slice0(%src:tensor<32xi32, #blocked0>) {\n+    // CHECK-COUNT-4: llvm.load [[.*]] : !llvm.ptr<vector<1xi32>, 3>llvm.load\n+    %cvt = triton_gpu.convert_layout %src : (tensor<32xi32, #blocked0>) -> tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>\n+    return\n+  }\n+}\n+\n+\n+// -----\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: convert_blocked1d_to_slice1\n+  func @convert_blocked1d_to_slice1(%src:tensor<32xi32, #blocked0>) {\n+    // CHECK-COUNT-32: llvm.load [[.*]] : !llvm.ptr<vector<1xi32>, 3>llvm.load\n+    %cvt = triton_gpu.convert_layout %src : (tensor<32xi32, #blocked0>) -> tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+    return\n+  }\n+}\n+\n+\n // -----\n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n #shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>"}]