[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -244,6 +244,7 @@ jobs:\n             mv cache reference\n           else\n             echo \"No artifact found with the name: $ARTIFACT_NAME\"\n+            exit 1\n           fi\n       - name: Download current job artifacts\n         uses: actions/download-artifact@v2\n@@ -294,6 +295,7 @@ jobs:\n         if: ${{ env.COMPARISON_RESULT == 'false' }}\n         uses: actions/github-script@v5\n         with:\n+          github-token: ${{ secrets.CI_ACCESS_TOKEN }}\n           script: |\n             const run_id = ${{ env.RUN_ID }};\n             const issue_number = context.payload.pull_request.number;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 12, "deletions": 11, "changes": 23, "file_content_changes": "@@ -89,13 +89,14 @@ struct ReduceOpConversion\n   void getWriteIndexBasic(ConversionPatternRewriter &rewriter, Location loc,\n                           Attribute layout, SmallVector<Value> &index,\n                           SmallVector<Value> &writeIdx,\n-                          std::map<int, Value> &ints, unsigned axis) const {\n+                          std::map<int, Value> &ints, unsigned originalAxis,\n+                          unsigned axis) const {\n     if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n-      auto dim = sliceLayout.getDim();\n-      assert(dim != axis && \"Reduction axis cannot be sliced\");\n+      // Recover the axis in the parent layout\n+      auto parentAxis = axis < sliceLayout.getDim() ? axis : axis + 1;\n       auto parentLayout = sliceLayout.getParent();\n       getWriteIndexBasic(rewriter, loc, parentLayout, index, writeIdx, ints,\n-                         axis);\n+                         originalAxis, parentAxis);\n       return;\n     }\n \n@@ -110,21 +111,21 @@ struct ReduceOpConversion\n       // we would have a single accumulation every `axisSizePerThread`\n       // contiguous values in the original tensor, so we would need\n       // to map every `axisSizePerThread` to 1 value in smem as:\n-      // writeIdx[axis] = index[axis] / axisSizePerThread\n-      writeIdx[axis] = udiv(index[axis], axisSizePerThread);\n+      // writeIdx[originalAxis] = index[originalAxis] / axisSizePerThread\n+      writeIdx[originalAxis] = udiv(index[originalAxis], axisSizePerThread);\n     } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n       if (!mmaLayout.isAmpere()) {\n         llvm::report_fatal_error(\"Unsupported layout\");\n       }\n-      if (axis == 0) {\n+      if (originalAxis == 0) {\n         // Because warpTileSize = [16, 8] and threadsPerWarp = [8, 4], each 8\n         // rows in smem would correspond to a warp. The mapping\n         // is: (warp_index) x 8 + (row index within warp)\n-        writeIdx[axis] =\n-            add(mul(udiv(index[axis], _16), _8), urem(index[axis], _8));\n+        writeIdx[originalAxis] = add(mul(udiv(index[originalAxis], _16), _8),\n+                                     urem(index[originalAxis], _8));\n       } else {\n         // Same as BlockedEncodingAttr case\n-        writeIdx[axis] = udiv(index[axis], axisSizePerThread);\n+        writeIdx[originalAxis] = udiv(index[originalAxis], axisSizePerThread);\n       }\n     } else {\n       llvm::report_fatal_error(\"Unsupported layout\");\n@@ -214,7 +215,7 @@ struct ReduceOpConversion\n       // get the writeIdx at which to write in smem\n       SmallVector<Value> writeIdx;\n       getWriteIndexBasic(rewriter, loc, srcLayout, indices[key], writeIdx, ints,\n-                         axis);\n+                         axis, axis);\n \n       // calculate the offset in smem for that writeIdx\n       Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape, srcOrd);"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.cpp", "status": "modified", "additions": 88, "deletions": 0, "changes": 88, "file_content_changes": "@@ -101,6 +101,93 @@ class CombineSelectMaskedLoadPattern : public mlir::RewritePattern {\n   }\n };\n \n+// sum(x[:, :, None] * y[None, :, :], 1)\n+// -> dot(x, y)\n+class CombineBroadcastMulReducePattern : public mlir::RewritePattern {\n+private:\n+  static bool isAddF32(const Operation *op) {\n+    if (auto addf = dyn_cast_or_null<arith::AddFOp>(op))\n+      return addf.getType().getIntOrFloatBitWidth() <= 32;\n+    return false;\n+  }\n+\n+  static SmallVector<int> getEqualIndices(ArrayRef<int64_t> x,\n+                                          ArrayRef<int64_t> y) {\n+    SmallVector<int> res;\n+    for (int i = 0; i < x.size(); ++i)\n+      if (x[i] == y[i])\n+        res.push_back(i);\n+    return res;\n+  }\n+\n+public:\n+  CombineBroadcastMulReducePattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(triton::ReduceOp::getOperationName(), 1, context) {\n+  }\n+\n+  mlir::LogicalResult matchAndRewrite(mlir::Operation *op,\n+                                      mlir::PatternRewriter &rewriter) const {\n+    auto reduceOp = llvm::dyn_cast<triton::ReduceOp>(op);\n+    if (!reduceOp)\n+      return mlir::failure();\n+    // only support reduce with simple addition\n+    Region &combineOp = reduceOp.getCombineOp();\n+    bool isReduceAdd = combineOp.hasOneBlock() &&\n+                       combineOp.front().getOperations().size() == 2 &&\n+                       isAddF32(&*combineOp.front().getOperations().begin());\n+    if (!isReduceAdd)\n+      return mlir::failure();\n+    // operand of reduce has to be mul\n+    auto mulOp = llvm::dyn_cast_or_null<arith::MulFOp>(\n+        reduceOp.getOperand(0).getDefiningOp());\n+    if (!mulOp)\n+      return mlir::failure();\n+    // mul operand has to be broadcast\n+    auto broadcastLhsOp = llvm::dyn_cast_or_null<triton::BroadcastOp>(\n+        mulOp.getOperand(0).getDefiningOp());\n+    if (!broadcastLhsOp)\n+      return mlir::failure();\n+    auto broadcastRhsOp = llvm::dyn_cast_or_null<triton::BroadcastOp>(\n+        mulOp.getOperand(1).getDefiningOp());\n+    if (!broadcastRhsOp)\n+      return mlir::failure();\n+    // broadcast operand is expand dims\n+    auto expandLhsOp = llvm::dyn_cast_or_null<triton::ExpandDimsOp>(\n+        broadcastLhsOp.getOperand().getDefiningOp());\n+    if (!expandLhsOp)\n+      return mlir::failure();\n+    auto expandRhsOp = llvm::dyn_cast_or_null<triton::ExpandDimsOp>(\n+        broadcastRhsOp.getOperand().getDefiningOp());\n+    if (!expandRhsOp)\n+      return mlir::failure();\n+    // get not-broadcast dimensions\n+    int expandLhsAxis = expandLhsOp.getAxis();\n+    int expandRhsAxis = expandRhsOp.getAxis();\n+    if (expandLhsAxis != 2 || expandRhsAxis != 0)\n+      return mlir::failure();\n+    auto broadcastLhsShape =\n+        broadcastLhsOp.getType().cast<ShapedType>().getShape();\n+    auto broadcastRhsShape =\n+        broadcastLhsOp.getType().cast<ShapedType>().getShape();\n+    if (broadcastLhsShape[2] < 16 || broadcastRhsShape[0] < 16)\n+      return mlir::failure();\n+    Type newAccType =\n+        RankedTensorType::get({broadcastLhsShape[0], broadcastRhsShape[2]},\n+                              broadcastLhsOp.getOperand()\n+                                  .getType()\n+                                  .cast<ShapedType>()\n+                                  .getElementType());\n+    rewriter.setInsertionPoint(op);\n+    auto newAcc = rewriter.create<triton::SplatOp>(\n+        op->getLoc(), newAccType,\n+        rewriter.create<arith::ConstantOp>(op->getLoc(),\n+                                           rewriter.getF32FloatAttr(0)));\n+    rewriter.replaceOpWithNewOp<triton::DotOp>(\n+        op, expandLhsOp.getOperand(), expandRhsOp.getOperand(), newAcc, true);\n+    return mlir::success();\n+  }\n+};\n+\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/Triton/Transforms/Passes.h.inc\"\n \n@@ -120,6 +207,7 @@ class CombineOpsPass : public TritonCombineOpsBase<CombineOpsPass> {\n     patterns.add<CombineSelectMaskedLoadPattern>(context);\n     // patterns.add<CombineAddPtrPattern>(context);\n     patterns.add<CombineBroadcastConstantPattern>(context);\n+    patterns.add<CombineBroadcastMulReducePattern>(context);\n \n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n       signalPassFailure();"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 52, "deletions": 7, "changes": 59, "file_content_changes": "@@ -1830,7 +1830,8 @@ def _welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n @pytest.mark.parametrize(\"M, N\", [[128, 128], [256, 128], [256, 256], [128, 256]])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n @pytest.mark.parametrize(\"op\", [\"sum\", \"max\"])\n-def test_chain_reduce(M, N, src_layout, op, device):\n+@pytest.mark.parametrize(\"first_axis\", [0, 1])\n+def test_chain_reduce(M, N, src_layout, op, device, first_axis):\n     op_str = \"\"\n     if op == \"sum\":\n         op_str = f\"\"\"\n@@ -1860,11 +1861,11 @@ def test_chain_reduce(M, N, src_layout, op, device):\n         %11 = \"tt.reduce\"(%10) ({{\n         ^bb0(%arg2: i32, %arg3: i32):\n         {op_str}\n-        }}) {{axis = 1 : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n+        }}) {{axis = {first_axis} : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{M if first_axis == 1 else N}xi32, #triton_gpu.slice<{{dim = {first_axis}, parent = #src}}>>\n         %12 = \"tt.reduce\"(%11) ({{\n         ^bb0(%arg2: i32, %arg3: i32):\n         {op_str}\n-        }}) {{axis = 0 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> i32\n+        }}) {{axis = 0 : i32}} : (tensor<{M if first_axis == 1 else N}xi32, #triton_gpu.slice<{{dim = {first_axis}, parent = #src}}>>) -> i32\n         tt.store %arg1, %12 {{cache = 1 : i32, evict = 1 : i32}} : i32\n         tt.return\n     }}\n@@ -2160,6 +2161,46 @@ def kernel(X, stride_xm, stride_xk,\n         assert 'mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16' in ptx\n \n \n+@pytest.mark.parametrize('in_dtype', ['float32'])\n+def test_dot_mulbroadcastred(in_dtype, device):\n+    @triton.jit\n+    def kernel(Z, X, Y,\n+               M: tl.constexpr, N: tl.constexpr, K: tl.constexpr,\n+               BM: tl.constexpr, BN: tl.constexpr, BK: tl.constexpr):\n+        pidn = tl.program_id(1)\n+        pidm = tl.program_id(0)\n+        offm = tl.arange(0, BM)[:, None]\n+        offn = tl.arange(0, BN)[None, :]\n+        offak = tl.arange(0, BK)[None, :]\n+        offbk = tl.arange(0, BK)[:, None]\n+        acc = tl.full((BM, BN), 0.0, tl.float32)\n+        for ridx5 in range(0, K // BK):\n+            x = tl.load(X + ((pidm * K * BM) + (offm * K) + (ridx5 * BK) + offak))\n+            y = tl.load(Y + ((pidn * BN) + (offbk * N) + (ridx5 * N * BK) + offn))\n+            x = tl.expand_dims(x, axis=2)\n+            y = tl.expand_dims(y, axis=0)\n+            t = tl.sum(x * y, axis=1)\n+            acc = t + acc\n+        tl.store(Z + ((pidm * BM * N) + (pidn * BN) + (offm * N) + offn), acc)\n+    M, N, K = 256, 192, 160\n+    BM, BN, BK = 128, 32, 32\n+    rs = RandomState(17)\n+    x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)\n+    y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)\n+    x = x * 0.1\n+    y = y * 0.1\n+    z = numpy_random((M, N), dtype_str=in_dtype, rs=rs)\n+    x_tri = to_triton(x, device=device)\n+    y_tri = to_triton(y, device=device)\n+    z_tri = to_triton(z, device=device)\n+    grid = M // BM, N // BN\n+    h = kernel[grid](z_tri, x_tri, y_tri, M, N, K, BM, BN, BK)\n+    z_ref = np.matmul(x, y)\n+    np.testing.assert_allclose(z_ref, to_numpy(z_tri), atol=0.01)\n+    assert \"tt.dot\" in h.asm['ttir']\n+    assert \"triton_gpu.async_wait {num = 2 : i32}\" in h.asm['ttgir']\n+\n+\n @pytest.mark.parametrize(\"dtype_str\", int_dtypes + float_dtypes + ['bfloat16'])\n def test_full(dtype_str, device):\n     dtype = getattr(torch, dtype_str)\n@@ -3029,24 +3070,28 @@ def kernel(Cond1, Cond2, Cond3, Val1, Val2, Val3, Out):\n def test_while(device):\n \n     @triton.jit\n-    def kernel(InitI, Bound, CutOff, OutI, OutJ):\n+    def kernel(InitI, Bound, CutOff, OutI, OutInitI, OutJ):\n         init_i = tl.load(InitI)\n         curr_i = init_i\n         j = 0\n-        while curr_i == init_i and j < tl.load(Bound):\n+        # Check that init_i is not updated by the loop\n+        while j < tl.load(Bound):\n             curr_i = curr_i + (j == tl.load(CutOff))\n             j += 1\n+            tl.store(OutInitI, init_i)\n         tl.store(OutI, curr_i)\n         tl.store(OutJ, j)\n \n     out_i = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n     out_j = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n     init_i = to_triton(np.full((1,), 1, dtype=np.int32), device=device)\n+    out_init_i = to_triton(np.full((1,), 0, dtype=np.int32), device=device)\n     bound = to_triton(np.full((1,), 10, dtype=np.int32), device=device)\n     cut_off = to_triton(np.full((1,), 5, dtype=np.int32), device=device)\n-    kernel[(1,)](init_i, bound, cut_off, out_i, out_j)\n+    kernel[(1,)](init_i, bound, cut_off, out_i, out_init_i, out_j)\n+    assert out_init_i[0] == init_i[0]\n     assert out_i[0] == init_i[0] + 1\n-    assert out_j[0] == cut_off[0] + 1\n+    assert out_j[0] == bound[0]\n \n # def test_for_if(device):\n "}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 5, "deletions": 6, "changes": 11, "file_content_changes": "@@ -563,8 +563,11 @@ def visit_If(self, node):\n             cond = cond.to(language.int1, _builder=self.builder)\n             contains_return = ContainsReturnChecker(self.gscope).visit(node)\n             if self.scf_stack and contains_return:\n-                raise UnsupportedLanguageConstruct(None, node,\n-                                                   \"Cannot have `return` statements inside `while` or `for` statements in triton\")\n+                raise UnsupportedLanguageConstruct(\n+                    None, node,\n+                    \"Cannot have `return` statements inside `while` or `for` statements in triton \"\n+                    \"(note that this also applies to `return` statements that are inside functions \"\n+                    \"transitively called from within `while`/`for` statements)\")\n             elif self.scf_stack or not contains_return:\n                 self.visit_if_scf(cond, node)\n             else:\n@@ -682,10 +685,6 @@ def visit_While(self, node):\n                     yields.append(loop_defs[name])\n             self.builder.create_yield_op([y.handle for y in yields])\n \n-        # update global uses in while_op\n-        for i, name in enumerate(names):\n-            after_block.replace_use_in_block_with(init_args[i].handle, after_block.arg(i))\n-\n         # WhileOp defines new values, update the symbol table (lscope, local_defs)\n         for i, name in enumerate(names):\n             new_def = language.core.tensor(while_op.get_result(i), ret_types[i])"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -13,6 +13,7 @@\n     zeros_like,\n )\n from .core import (\n+    TRITON_MAX_TENSOR_NUMEL,\n     abs,\n     advance,\n     arange,\n@@ -105,6 +106,7 @@\n \n \n __all__ = [\n+    \"TRITON_MAX_TENSOR_NUMEL\",\n     \"abs\",\n     \"advance\",\n     \"arange\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 10, "deletions": 5, "changes": 15, "file_content_changes": "@@ -1615,15 +1615,15 @@ def max_contiguous(input, values, _builder=None):\n @builtin\n def static_print(*values, sep: str = \" \", end: str = \"\\n\", file=None, flush=False, _builder=None):\n     '''\n-    Print the values at compile time. The parameters are the same as the Python builtin :code:`print`.\n+    Print the values at compile time.  The parameters are the same as the builtin :code:`print`.\n \n-    Calling the Python builtin :code:`print` inside your kernel is the same as calling this.\n+    NOTE: Calling the Python builtin :code:`print` is not the same as calling this, it instead maps to :code:`device_print`,\n+    which has special requirements for the arguments.\n \n     .. highlight:: python\n     .. code-block:: python\n \n         tl.static_print(f\"{BLOCK_SIZE=}\")\n-        print(f\"{BLOCK_SIZE=}\")\n     '''\n     pass\n \n@@ -1645,13 +1645,18 @@ def static_assert(cond, msg=\"\", _builder=None):\n @builtin\n def device_print(prefix, *args, _builder=None):\n     '''\n-    Print the values at runtime from the device.  String formatting does not work, so you should\n-    provide the values you want to print as arguments.\n+    Print the values at runtime from the device.  String formatting does not work for runtime values, so you should\n+    provide the values you want to print as arguments.  The first value must be a string, all following values must\n+    be scalars or tensors.\n+\n+    Calling the Python builtin :code:`print` is the same as calling this function, and the requirements for the arguments will match\n+    this function (not the normal requirements for :code:`print`).\n \n     .. highlight:: python\n     .. code-block:: python\n \n         tl.device_print(\"pid\", pid)\n+        print(\"pid\", pid)\n \n     :param prefix: a prefix to print before the values. This is required to be a string literal.\n     :param args: the values to print. They can be any tensor or scalar."}]