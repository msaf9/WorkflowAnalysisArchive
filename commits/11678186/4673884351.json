[{"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -97,7 +97,7 @@ def test_matmul(M, N, K, dtype_str):\n     ms = triton.testing.do_bench(fn, warmup=100, rep=300)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n \n #######################\n@@ -153,7 +153,7 @@ def test_elementwise(N):\n     ms = triton.testing.do_bench(fn, warmup=100, rep=500)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n #######################\n # Flash-Attention\n@@ -201,4 +201,4 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n     max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, mode, dtype_str)]\n-    torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)"}]