[{"filename": ".github/workflows/compare-artifacts.yml", "status": "added", "additions": 52, "deletions": 0, "changes": 52, "file_content_changes": "@@ -0,0 +1,52 @@\n+name: Compare Artifacts\n+on:\n+  workflow_run:\n+    workflows:\n+      - Integration Tests\n+    types:\n+      - completed\n+\n+jobs:\n+  Compare-artifacts:\n+    runs-on: ubuntu-latest\n+    if: ${{ github.event.workflow_run.conclusion == 'success' }}\n+\n+    steps:\n+      - name: 'Download artifact'\n+        uses: actions/github-script@v6\n+        with:\n+          script: |\n+            let allArtifacts = await github.rest.actions.listWorkflowRunArtifacts({\n+               owner: context.repo.owner,\n+               repo: context.repo.repo,\n+               run_id: context.payload.workflow_run.id,\n+            });\n+            let matchArtifact = allArtifacts.data.artifacts.filter((artifact) => {\n+              return artifact.name == \"pr_number\"\n+            })[0];\n+            let download = await github.rest.actions.downloadArtifact({\n+               owner: context.repo.owner,\n+               repo: context.repo.repo,\n+               artifact_id: matchArtifact.id,\n+               archive_format: 'zip',\n+            });\n+            let fs = require('fs');\n+            fs.writeFileSync(`${process.env.GITHUB_WORKSPACE}/pr_number.zip`, Buffer.from(download.data));\n+\n+      - name: 'Unzip artifact'\n+        run: unzip pr_number.zip\n+\n+      - name: 'Comment on PR'\n+        uses: actions/github-script@v6\n+        with:\n+          github-token: ${{ secrets.GITHUB_TOKEN }}\n+          script: |\n+            let fs = require('fs');\n+            let issue_number = Number(fs.readFileSync('./pr_number'));\n+            echo $issue_number\n+            await github.rest.issues.createComment({\n+              owner: context.repo.owner,\n+              repo: context.repo.repo,\n+              issue_number: issue_number,\n+              body: 'Ignore this message. This is to test another workflow posting comment on PR number ${issue_number}.'\n+            });"}, {"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 31, "deletions": 5, "changes": 36, "file_content_changes": "@@ -92,7 +92,7 @@ jobs:\n         if: ${{ env.BACKEND == 'CUDA' && env.ENABLE_TMA == '1' && env.ENABLE_MMA_V3 == '1'}}\n         run: |\n           cd python/test/unit\n-          python3 -m pytest -n 8 --ignore=runtime --ignore=language/test_line_info.py\n+          python3 -m pytest -n 8 --ignore=runtime --ignore=operators --ignore=language/test_line_info.py\n           # run runtime tests serially to avoid race condition with cache handling.\n           python3 -m pytest runtime/\n           # run test_line_info.py separately with TRITON_DISABLE_LINE_INFO=0\n@@ -102,17 +102,33 @@ jobs:\n         if: ${{ env.BACKEND == 'CUDA' && env.ENABLE_TMA == '0' && env.ENABLE_MMA_V3 == '0'}}\n         run: |\n           cd python/test/unit\n-          python3 -m pytest -n 8 --ignore=runtime --ignore=hopper --ignore=language/test_line_info.py\n+          python3 -m pytest -n 8 --ignore=runtime --ignore=hopper --ignore=operators --ignore=language/test_line_info.py\n           # run runtime tests serially to avoid race condition with cache handling.\n           python3 -m pytest runtime/\n           # run test_line_info.py separately with TRITON_DISABLE_LINE_INFO=0\n           TRITON_DISABLE_LINE_INFO=0 python3 -m pytest language/test_line_info.py\n \n+      - name: Clear cache\n+        run: |\n+          rm -rf ~/.triton\n+\n+      - name: Run partial tests on CUDA with ENABLE_TMA=1 and ENABLE_MMA_V3=1\n+        if: ${{ env.BACKEND == 'CUDA' && env.ENABLE_TMA == '1' && env.ENABLE_MMA_V3 == '1'}}\n+        run: |\n+          cd python/test/unit\n+          python3 -m pytest -n 8 operators\n+\n+      - name: Run partial tests on CUDA with ENABLE_TMA=0 and ENABLE_MMA_V3=0\n+        if: ${{ env.BACKEND == 'CUDA' && env.ENABLE_TMA == '0' && env.ENABLE_MMA_V3 == '0'}}\n+        run: |\n+          cd python/test/unit\n+          python3 -m pytest -n 8 operators\n+\n       - name: Create artifacts archive\n         if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n         run: |\n           cd ~/.triton\n-          tar -czvf artifacts.tar.gz cache\n+          tar -czf artifacts.tar.gz cache\n \n       - name: Upload artifacts archive\n         if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n@@ -137,9 +153,20 @@ jobs:\n           sudo nvidia-smi -i 0 --lock-gpu-clocks=1280,1280\n           python3 -m pytest -vs . --reruns 10\n           sudo nvidia-smi -i 0 -rgc\n+      - name: Save PR number\n+        env:\n+          PR_NUMBER: ${{ github.event.number }}\n+        run: |\n+          mkdir -p ./pr\n+          echo $PR_NUMBER > ./pr/pr_number\n+      - uses: actions/upload-artifact@v3\n+        with:\n+          name: pr_number\n+          path: pr/\n \n   Integration-Tests-Third-Party:\n     needs: Runner-Preparation\n+    if: false\n \n     runs-on: ${{ matrix.runner }}\n \n@@ -320,7 +347,7 @@ jobs:\n       - name: Compare artifacts\n         run: |\n           set +e\n-          python3 python/test/tools/compare_files.py --path1 reference --path2 current --kernels python/test/kernel_comparison/kernels.yml\n+          python3 python/test/tools/compare_files.py --path1 reference --path2 current\n           exit_code=$?\n           set -e\n           echo $exit_code\n@@ -334,7 +361,6 @@ jobs:\n             echo \"Error while comparing artifacts\"\n             echo \"COMPARISON_RESULT=error\" >> $GITHUB_ENV\n           fi\n-          echo \"COMPARISON_RESULT=env.COMPARISON_RESULT\"\n       - name: Check exit code and handle failure\n         if: ${{ env.COMPARISON_RESULT == 'error' }}\n         run: |"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -143,11 +143,11 @@ compared to 1*64 when the hasLeadingOffset is false.\n \n         // ---- begin Ampere ----\n         if (mmaEnc.isAmpere()) {\n-          int perPhase = 128 / (shapePerCTA[order[0]] * 4 / dotOpEnc.getMMAv2kWidth());\n+          int perPhase = 128 / (shapePerCTA[order[0]] * 4 / dotOpEnc.getKWidth());\n           perPhase = std::max<int>(perPhase, 1);\n-          std::vector<size_t> matShape = {8, 8, 4 * dotOpEnc.getMMAv2kWidth()};\n+          std::vector<size_t> matShape = {8, 8, 4 * dotOpEnc.getKWidth()};\n           // for now, disable swizzle when using transposed int8 tensor cores\n-          if ((32 / typeWidthInBit != dotOpEnc.getMMAv2kWidth()) && order[0] == inner)\n+          if ((32 / typeWidthInBit != dotOpEnc.getKWidth()) && order[0] == inner)\n             return get(context, 1, 1, 1, order, CTALayout);\n \n           // --- handle A operand ---\n@@ -655,7 +655,7 @@ section 9.7.13.4.1 for more details.\n     ins\n     \"unsigned\":$opIdx,\n     \"Attribute\":$parent,\n-    \"unsigned\":$MMAv2kWidth\n+    \"unsigned\":$kWidth\n   );\n \n   let builders = ["}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -582,7 +582,7 @@ Value loadArg(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n   int matShapeM = 8, matShapeN = 8, matShapeK = 2 * 64 / bitwidth;\n \n   auto numRep = encoding.getMMAv2Rep(shapePerCTA, bitwidth);\n-  int kWidth = encoding.getMMAv2kWidth();\n+  int kWidth = encoding.getKWidth();\n \n   auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n   auto order = triton::gpu::getOrder(mmaLayout);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 46, "deletions": 64, "changes": 110, "file_content_changes": "@@ -10,13 +10,11 @@ using ::mlir::triton::gpu::getTotalElemsPerThread;\n const std::string Fp16_to_Fp8E5M2 =\n     \"{                            \\n\"\n     \".reg .b32 a<2>;              \\n\"\n-    \"and.b32 a0, $1, 0x7fff7fff;  \\n\"           // a0 &= 0x7fff7fff\n-    \"and.b32 a1, $2, 0x7fff7fff;  \\n\"           // (strip sign)\n-    \"add.u32 a0, a0, 0x00800080;  \\n\"           // a0 += 0x00800080\n-    \"add.u32 a1, a1, 0x00800080;  \\n\"           // (round to nearest)\n-    \"lop3.b32 a0, $1, 0x80008000, a0, 0xea; \\n\" // a0 = a0|(0x80008000&in0)\n-    \"lop3.b32 a1, $2, 0x80008000, a1, 0xea; \\n\" // (restore sign)\n-    \"prmt.b32 $0, a0, a1, 0x7531; \\n\\t\"         // output = a1a0\n+    \"and.b32 a0, $1, 0xfffefffe;  \\n\"   // a0 &= 0xfffefffe\n+    \"and.b32 a1, $2, 0xfffefffe;  \\n\"   // (strip lowest bit)\n+    \"add.u32 a0, a0, 0x00800080;  \\n\"   // a0 += 0x00800080\n+    \"add.u32 a1, a1, 0x00800080;  \\n\"   // (round to nearest)\n+    \"prmt.b32 $0, a0, a1, 0x7531; \\n\\t\" // output = a1a0\n     \"}\";\n \n const std::string Fp8E5M2_to_Fp16 = \"{                           \\n\"\n@@ -93,32 +91,27 @@ const std::string Bf16_to_Fp8E5M2 =\n const std::string Fp8E4M3B15_to_Fp16 =\n     \"{                                      \\n\"\n     \".reg .b32 a<2>, b<2>;                  \\n\"\n-    \"prmt.b32 a0, 0, $2, 0x5040;            \\n\"\n-    \"prmt.b32 a1, 0, $2, 0x7060;            \\n\"\n-    \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n-    \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n+    \"prmt.b32 a0, 0, $2, 0x5746;            \\n\"\n+    \"and.b32 b0, a0, 0x7f007f00;            \\n\"\n+    \"and.b32 b1, a0, 0x00ff00ff;            \\n\"\n+    \"and.b32 a1, a0, 0x00800080;            \\n\"\n     \"shr.b32  b0, b0, 1;                    \\n\"\n-    \"shr.b32  b1, b1, 1;                    \\n\"\n+    \"add.u32 b1, b1, a1;                    \\n\"\n     \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\"\n-    \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\"\n+    \"shl.b32 $1, b1, 7;                     \\n\"\n     \"}                                      \\n\";\n \n const std::string Fp16_to_Fp8E4M3B15 =\n     \"{                                      \\n\"\n     \".reg .b32 a<2>, b<2>;                  \\n\"\n-    \".reg .b32 min_val, max_val;            \\n\"\n-    \"mov.b32 min_val, 0xBF80BF80;           \\n\"\n+    \".reg .b32 max_val;                     \\n\"\n     \"mov.b32 max_val, 0x3F803F80;           \\n\"\n-    \"max.f16x2 $1, $1, min_val;             \\n\"\n-    \"min.f16x2 $1, $1, max_val;             \\n\"\n-    \"max.f16x2 $2, $2, min_val;             \\n\"\n-    \"min.f16x2 $2, $2, max_val;             \\n\"\n-    \"shl.b32 a0, $1, 1;                     \\n\"\n-    \"shl.b32 a1, $2, 1;                     \\n\"\n-    \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n-    \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n-    \"add.u32 a0, a0, 0x00800080;            \\n\"\n-    \"add.u32 a1, a1, 0x00800080;            \\n\"\n+    \"and.b32 a0, $1, 0x7fff7fff;            \\n\"\n+    \"and.b32 a1, $2, 0x7fff7fff;            \\n\"\n+    \"min.f16x2 a0, a0, max_val;             \\n\"\n+    \"min.f16x2 a1, a1, max_val;             \\n\"\n+    \"mad.lo.u32 a0, a0, 2, 0x00800080;      \\n\"\n+    \"mad.lo.u32 a1, a1, 2, 0x00800080;      \\n\"\n     \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\"\n     \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\"\n     \"prmt.b32 $0, b0, b1, 0x7531;           \\n\"\n@@ -137,12 +130,11 @@ const std::string Fp16_to_Fp8E4M3B15 =\n const std::string Fp8E4M3B15x4_to_Fp16 =\n     \"{                                      \\n\"\n     \".reg .b32 a<2>;                        \\n\"\n-    \"shl.b32 a0, $2, 1;                     \\n\"\n+    \"add.u32 a0, $2, $2;                    \\n\"\n     \"shl.b32 a1, $2, 7;                     \\n\"\n     \"and.b32  $0, a0, 0x80008000;           \\n\"\n     \"lop3.b32 $0, $0, a1, 0x3f803f80, 0xf8; \\n\"\n-    \"and.b32  $1, $2, 0x80008000;           \\n\"\n-    \"lop3.b32 $1, $1, $2, 0x3f803f80, 0xf8; \\n\"\n+    \"and.b32  $1, $2, 0xbf80bf80;           \\n\"\n     \"}\";\n \n // Fp16 -> Fp8E4M3B15 (packed)\n@@ -159,8 +151,7 @@ const std::string Fp16_to_Fp8E4M3B15x4 =\n     \"shr.b32  a1, $1, 7;                     \\n\"\n     \"and.b32  $0,     a0, 0x40004000;        \\n\"\n     \"lop3.b32 $0, $0, a1, 0x007f007f, 0xf8;  \\n\"\n-    \"lop3.b32 $0, $0, $2, 0x80008000, 0xf8;  \\n\"\n-    \"lop3.b32 $0, $0, $2, 0x3f803f80, 0xf8;  \\n\"\n+    \"lop3.b32 $0, $0, $2, 0xbf80bf80, 0xf8;  \\n\"\n     \"}\";\n \n /* ----- FP8E4M3 ------ */\n@@ -172,53 +163,44 @@ const std::string Fp16_to_Fp8E4M3B15x4 =\n const std::string Fp8E4M3_to_Fp16 =\n     \"{                                      \\n\"\n     \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n-    \"prmt.b32 a0, 0, $2, 0x5040;            \\n\" // a0 = 0xf300f400\n-    \"prmt.b32 a1, 0, $2, 0x7060;            \\n\" // a1 = 0xf100f200\n-    \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // b0 = a0 & 0x7fff7fff\n-    \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n-    \"shr.b32  b0, b0, 1;                    \\n\" // b0 >>= 1\n-    \"shr.b32  b1, b1, 1;                    \\n\" // shift into fp16 position\n-    \"add.u32  b0, b0, 0x20002000;           \\n\" // b0.exp += 2**4-2**3\n-                                                // exponent compensate = 8\n-    \"add.u32  b1, b1, 0x20002000;           \\n\" // b1 += 8<<10 | 8<<10<<16\n-    \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n-    \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n+    \"prmt.b32 a0, 0, $2, 0x0504;            \\n\" // a0 = 0x00f300f4\n+    \"prmt.b32 a1, 0, $2, 0x0706;            \\n\" // a1 = 0x00f100f2\n+    \"and.b32  b0, a0, 0x00800080;           \\n\" // b0 = a0 & 0x00800080\n+    \"and.b32  b1, a1, 0x00800080;           \\n\" // (extract sign)\n+    \"add.u32  b0, b0, a0;                   \\n\" // b0 = b0 + a0\n+    \"add.u32  b1, b1, a1;                   \\n\" // (move sign to the left)\n+    \"mad.lo.u32 $0, b0, 128, 0x20002000;    \\n\" // out0 = (b0 << 7) + 0x20002000\n+    \"mad.lo.u32 $1, b1, 128, 0x20002000;    \\n\" // (shift into position and bias\n+                                                // exponent)\n     \"}\";\n \n // Fp16 -> Fp8E4M3 (packed)\n const std::string Fp16_to_Fp8E4M3 =\n     \"{                                      \\n\"\n     \".reg .b32 a<2>, b<2>;                  \\n\" // see Fp8E4M3x4ToFp16x4\n-    \"sub.u32 a0, $1, 0x20002000;            \\n\" // a0 = input0 - 0x20002000\n-                                                // (compensate offset)\n-    \"sub.u32 a1, $2, 0x20002000;            \\n\" // a1 = input1 - 0x20002000\n-                                                // (8 << 10 | 8 << 10 << 16)\n-    \"shl.b32 a0, a0, 1;                     \\n\" // a0 <<= 1\n-    \"shl.b32 a1, a1, 1;                     \\n\" // shift into fp8e4 position\n-    \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // a0 &= 0x7fff7fff\n-    \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n-    \"add.u32 a0, a0, 0x00800080;            \\n\" // a0 += 0x00800080\n-    \"add.u32 a1, a1, 0x00800080;            \\n\" // (round to nearest)\n-    \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\" // b0 = a0|(0x80008000&in0)\n-    \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\" // (restore sign)\n+    \"and.b32 a0, $1, 0x7fff7fff;            \\n\" // a0 = input0 & 0x7fff7fff\n+    \"and.b32 a1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n+    \"mad.lo.u32 a0, a0, 2, 0x40804080;      \\n\" // shift exponent (<< 1),\n+    \"mad.lo.u32 a1, a1, 2, 0x40804080;      \\n\" // correct bias (0x40004000),\n+                                                // and round to nearest\n+    \"lop3.b32 b0, $1, 0x80008000, a0, 0xe2; \\n\" // b0 = 0x80008000 ? in0 : a0\n+    \"lop3.b32 b1, $2, 0x80008000, a1, 0xe2; \\n\" // (restore sign)\n     \"prmt.b32 $0, b0, b1, 0x7531;           \\n\" // output = b1b0\n     \"}\";\n \n // WARN: subnormal (0bs0000xxx) are not handled\n const std::string Fp8E4M3_to_Bf16 =\n     \"{                                      \\n\"\n     \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n-    \"prmt.b32 a0, 0, $2, 0x5040;            \\n\" // a0 = 0xf300f400\n-    \"prmt.b32 a1, 0, $2, 0x7060;            \\n\" // a1 = 0xf100f200\n-    \"and.b32 b0, a0, 0x7fff7fff;            \\n\" // b0 = a0 & 0x7fff7fff\n-    \"and.b32 b1, a1, 0x7fff7fff;            \\n\" // (strip sign)\n-    \"shr.b32 b0, b0, 4;                     \\n\" // b0 >>= 4\n-    \"shr.b32 b1, b1, 4;                     \\n\" // shift into fp16 position\n-    \"add.u32 b0, b0, 0x3c003c00;            \\n\" // b0.exp += 2**7-2**3\n-                                                // exponent compensate = 120\n-    \"add.u32 b1, b1, 0x3c003c00;            \\n\" // b1 += 120<<7 | 120<<7<<16\n-    \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n-    \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n+    \"prmt.b32 a0, 0, $2, 0x0504;            \\n\" // a0 = 0x00f300f4\n+    \"prmt.b32 a1, 0, $2, 0x0706;            \\n\" // a1 = 0x00f100f2\n+    \"and.b32  b0, a0, 0x00800080;           \\n\" // b0 = a0 & 0x00800080\n+    \"and.b32  b1, a1, 0x00800080;           \\n\" // (extract sign)\n+    \"mad.lo.u32 b0, b0, 15, a0;             \\n\" // b0 = b0 * 15 + a0\n+    \"mad.lo.u32 b1, b1, 15, a1;             \\n\" // (move sign to the left)\n+    \"mad.lo.u32 $0, b0, 16, 0x3c003c00;     \\n\" // out0 = (b0 << 4) + 0x3c003c00\n+    \"mad.lo.u32 $1, b1, 16, 0x3c003c00;     \\n\" // (shift into position and bias\n+                                                // exponent)\n     \"}\";\n \n const std::string Bf16_to_Fp8E4M3 ="}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 17, "deletions": 4, "changes": 21, "file_content_changes": "@@ -307,8 +307,10 @@ struct ReduceOpConversion\n     Operation *yield = block->getTerminator();\n     Operation *reduceOp = yield->getOperand(0).getDefiningOp();\n     if (!reduceOp || reduceOp->getNumOperands() != 2 ||\n-        reduceOp->getNumResults() != 1 ||\n-        !reduceOp->getResultTypes()[0].isInteger(32))\n+        reduceOp->getNumResults() != 1)\n+      return std::nullopt;\n+    auto intType = reduceOp->getResultTypes()[0].dyn_cast<IntegerType>();\n+    if (!intType || intType.getWidth() > 32)\n       return std::nullopt;\n     if (reduceOp->getOperand(0) != block->getArgument(0) ||\n         reduceOp->getOperand(1) != block->getArgument(1))\n@@ -382,8 +384,19 @@ struct ReduceOpConversion\n           mask = shl(i32_val(bitmask),\n                      and_(laneId, i32_val(~(numLaneToReduce - 1))));\n         }\n-        acc[0] = rewriter.create<NVVM::ReduxOp>(loc, acc[0].getType(), acc[0],\n-                                                *kind, mask);\n+        for (unsigned i = 0; i < acc.size(); ++i) {\n+          unsigned bitwidth = acc[i].getType().cast<IntegerType>().getWidth();\n+          if (bitwidth < 32) {\n+            if (*kind == NVVM::ReduxKind::MIN || *kind == NVVM::ReduxKind::MAX)\n+              acc[i] = sext(i32_ty, acc[i]);\n+            else\n+              acc[i] = zext(i32_ty, acc[i]);\n+          }\n+          acc[i] = rewriter.create<NVVM::ReduxOp>(loc, acc[i].getType(), acc[0],\n+                                                  *kind, mask);\n+          if (bitwidth < 32)\n+            acc[i] = trunc(int_ty(bitwidth), acc[i]);\n+        }\n         return;\n       }\n     }"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1258,7 +1258,7 @@ void DotOperandEncodingAttr::print(mlir::AsmPrinter &printer) const {\n   printer << \"<{\"\n           << \"opIdx = \" << getOpIdx() << \", parent = \" << getParent();\n   if (mmaParent && mmaParent.isAmpere())\n-    printer << \", kWidth = \" << getMMAv2kWidth();\n+    printer << \", kWidth = \" << getKWidth();\n   printer << \"}>\";\n }\n \n@@ -1496,7 +1496,7 @@ struct TritonGPUInferLayoutInterface\n     // Verify that the encodings are valid.\n     if (!aEncoding || !bEncoding)\n       return op->emitError(\"mismatching encoding between A and B operands\");\n-    if (aEncoding.getMMAv2kWidth() != bEncoding.getMMAv2kWidth())\n+    if (aEncoding.getKWidth() != bEncoding.getKWidth())\n       return op->emitError(\"mismatching kWidth between A and B operands\");\n     return success();\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -203,8 +203,8 @@ LogicalResult Prefetcher::initialize() {\n     auto bType = dot.getB().getType().cast<RankedTensorType>();\n     auto aEnc = aType.getEncoding().cast<triton::gpu::DotOperandEncodingAttr>();\n     auto bEnc = bType.getEncoding().cast<triton::gpu::DotOperandEncodingAttr>();\n-    int aKWidth = aEnc.getMMAv2kWidth();\n-    int bKWidth = bEnc.getMMAv2kWidth();\n+    int aKWidth = aEnc.getKWidth();\n+    int bKWidth = bEnc.getKWidth();\n     assert(aKWidth == bKWidth);\n \n     auto kSize = aType.getShape()[1];"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 56, "deletions": 0, "changes": 56, "file_content_changes": "@@ -225,3 +225,59 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, seq_par, causal, mode, dtype_str):\n     ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, seq_par, causal, mode, dtype_str)]\n     print_perf(ms, cur_gpu_util, ref_gpu_util)\n     triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.02, rtol=0.01)\n+\n+\n+#######################\n+# Reduction\n+#######################\n+\n+\n+@triton.jit\n+def _sum(x_ptr, y_ptr, output_ptr, n_elements,\n+         BLOCK_SIZE: tl.constexpr):\n+    pid = tl.program_id(axis=0)\n+    block_start = pid * BLOCK_SIZE\n+    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+    mask = offsets < n_elements\n+    x = tl.load(x_ptr + offsets, mask=mask)\n+    y = tl.load(y_ptr + offsets, mask=mask)\n+    # run in a loop to only to make it compute bound.\n+    for i in range(100):\n+        x = tl.sum(x, axis=0) + y\n+\n+    tl.store(output_ptr + offsets, x, mask=mask)\n+\n+\n+reduction_data = {\n+    'a100': {\n+        1024 * 16384: {'float16': 0.016, 'float32': 0.031, 'int16': 0.015, 'int32': 0.031},\n+        1024 * 65536: {'float16': 0.016, 'float32': 0.032, 'int16': 0.015, 'int32': 0.032},\n+    }\n+}\n+\n+\n+@pytest.mark.parametrize('N', reduction_data[DEVICE_NAME].keys())\n+@pytest.mark.parametrize(\"dtype_str\", ['float16', 'float32', 'int16', 'int32'])\n+def test_reductions(N, dtype_str):\n+    stream = torch.cuda.Stream()\n+    torch.cuda.set_stream(stream)\n+    torch.manual_seed(0)\n+    dtype = {'float16': torch.float16, 'float32': torch.float32, 'int16': torch.int16, 'int32': torch.int32}[dtype_str]\n+    ref_gpu_util = reduction_data[DEVICE_NAME][N][dtype_str]\n+    cur_sm_clock = nvsmi(['clocks.current.sm'])[0]\n+    max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n+    z = torch.empty((N, ), dtype=dtype, device='cuda')\n+    if dtype == torch.float16 or dtype == torch.float32:\n+        x = torch.randn_like(z)\n+        y = torch.randn_like(z)\n+    else:\n+        info = torch.iinfo(dtype)\n+        x = torch.randint(info.min, info.max, (N,), dtype=dtype, device='cuda')\n+        y = torch.randint(info.min, info.max, (N,), dtype=dtype, device='cuda')\n+    grid = lambda args: (triton.cdiv(N, args['BLOCK_SIZE']), )\n+    fn = lambda: _sum[grid](x, y, z, N, BLOCK_SIZE=1024)\n+    ms = triton.testing.do_bench_cudagraph(fn)\n+    cur_gpu_perf = 100. * 2. * N / ms * 1e-9\n+    cur_gpu_util = cur_gpu_perf / max_gpu_perf\n+    print_perf(ms, cur_gpu_util, ref_gpu_util)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.02, rtol=0.01)"}, {"filename": "python/test/tools/compare_files.py", "status": "modified", "additions": 11, "deletions": 24, "changes": 35, "file_content_changes": "@@ -9,9 +9,8 @@\n \n \n class ComparisonResult:\n-    def __init__(self, name: str, extension: str, numComparisons: int, diffs: List[str] = None, errors: List[str] = None):\n+    def __init__(self, name: str, numComparisons: int, diffs: List[str] = None, errors: List[str] = None):\n         self.name = name\n-        self.extension = extension\n         self.numComparisons = numComparisons\n         self.diffs = [] if diffs is None else diffs\n         self.errors = [] if errors is None else errors\n@@ -20,7 +19,7 @@ def isSuccess(self) -> bool:\n         return len(self.diffs) == 0 and len(self.errors) == 0\n \n     def __str__(self) -> str:\n-        return f\"name={self.name}, extension={self.extension}, numComparisons={self.numComparisons}, success={self.isSuccess()}\"\n+        return f\"name={self.name}, numComparisons={self.numComparisons}, success={self.isSuccess()}\"\n \n \n def listFilesWithExtension(path: str, extension: str) -> List[str]:\n@@ -143,9 +142,9 @@ def doFilesMatch(path1: str, path2: str) -> bool:\n     return True\n \n \n-def compareMatchingFiles(name: str, extension: str, nameToHashes1: Dict[str, List[str]], nameToHashes2: Dict[str, List[str]], args) -> ComparisonResult:\n+def compareMatchingFiles(name: str, nameToHashes1: Dict[str, List[str]], nameToHashes2: Dict[str, List[str]], args) -> ComparisonResult:\n     \"\"\"\n-        Compare files with the given name/extension in all hashes in both paths\n+        Compare files with the given name in all hashes in both paths\n         Return the first mismatching files as a tuple (file1, file2), otherwise, return an empty tuple\n     \"\"\"\n     hashes1 = nameToHashes1.get(name, [])\n@@ -164,14 +163,14 @@ def compareMatchingFiles(name: str, extension: str, nameToHashes1: Dict[str, Lis\n             if not doFilesMatch(path1, path2):\n                 continue\n             numComparisons += 1\n-            extFile1 = listFilesWithExtension(path1, extension)[0]\n-            extFile2 = listFilesWithExtension(path2, extension)[0]\n+            extFile1 = listFilesWithExtension(path1, \"ptx\")[0]\n+            extFile2 = listFilesWithExtension(path2, \"ptx\")[0]\n             diff = diffFiles(extFile1, extFile2)\n             if len(diff) > 0:\n                 diffs.append(diffFiles(extFile2, extFile1))\n     if numComparisons == 0:\n         errors.append(f\"Did not find any matching files for {name}\")\n-    return ComparisonResult(name=name, extension=extension, numComparisons=numComparisons, diffs=diffs, errors=errors)\n+    return ComparisonResult(name=name, numComparisons=numComparisons, diffs=diffs, errors=errors)\n \n \n def dumpResults(results: List[ComparisonResult], fileName: str):\n@@ -203,20 +202,15 @@ def main(args) -> bool:\n     nameToHashes1 = getNameToHashesDict(args.path1)\n     nameToHashes2 = getNameToHashesDict(args.path2)\n \n-    yamlFilePath = args.kernels\n-    if not os.path.exists(yamlFilePath):\n-        print(f\"Path {yamlFilePath} does not exist!\")\n-        sys.exit(2)\n-    nameAndExtension = loadYamlFile(yamlFilePath)[\"name_and_extension\"]\n+    # Get all kernels that need to be checked\n+    kernelNames = set(nameToHashes1.keys()).union(set(nameToHashes2.keys()))\n \n     results = []\n     # iterate over the kernels that need to be checked\n-    for d in nameAndExtension:\n-        name = d[\"name\"]  # kernel name\n-        extension = d[\"extension\"]  # extension of the file to be compared (e.g. ptx)\n+    for name in kernelNames:\n         # Compare all hashes on path 1 with all hashes on path 2\n         # result is either the mismatching (file1, file2) with \"extension\" or empty tuple if no mismatch\n-        result = compareMatchingFiles(name, extension, nameToHashes1, nameToHashes2, args)\n+        result = compareMatchingFiles(name, nameToHashes1, nameToHashes2, args)\n         print(result)\n         # Otherwise, add it to the mismatches\n         results.append(result)\n@@ -250,12 +244,5 @@ def main(args) -> bool:\n         required=True,\n         help=(\"Path to second cache directory\"),\n     )\n-    parser.add_argument(\n-        \"--kernels\",\n-        type=str,\n-        default=None,\n-        required=True,\n-        help=(\"Path to kernels yaml file\"),\n-    )\n     args = parser.parse_args()\n     main(args)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 23, "deletions": 2, "changes": 25, "file_content_changes": "@@ -352,6 +352,29 @@ def test_bin_op(dtype_x, dtype_y, op, num_ctas, device):\n             num_ctas=num_ctas)\n \n \n+@pytest.mark.parametrize(\"dtype, order\", [(dtype, order) for dtype in dtypes_with_bfloat16 for order in [0, 1]])\n+def test_addptr(dtype, order, device):\n+    check_type_supported(dtype, device)\n+\n+    @triton.jit\n+    def kernel(x, y, ORDER: tl.constexpr, SIZE: tl.constexpr):\n+        offs = tl.arange(0, SIZE)\n+        if ORDER == 0:\n+            tl.store(y + offs, tl.load(x + offs))\n+        else:\n+            tl.store(offs + y, tl.load(offs + x))\n+\n+    SIZE = 1024\n+    rs = RandomState(17)\n+    x = numpy_random(SIZE, dtype_str=dtype, rs=rs)\n+    y = numpy_random(SIZE, dtype_str=dtype, rs=rs)\n+    x_tri = to_triton(x, dst_type=dtype, device=device)\n+    y_tri = to_triton(y, dst_type=dtype, device=device)\n+    y = x\n+    kernel[1,](x_tri, y_tri, order, SIZE)\n+    np.testing.assert_allclose(y, to_numpy(y_tri))\n+\n+\n @pytest.mark.parametrize(\"dtype_x, dtype_y\",\n                          [(dtype_x, dtype_y) for dtype_x in int_dtypes for dtype_y in int_dtypes] +\n                          [(dtype_x, dtype_y) for dtype_x in uint_dtypes for dtype_y in uint_dtypes]\n@@ -1462,8 +1485,6 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n def get_reduced_dtype(dtype_str, op):\n     if op in ('argmin', 'argmax'):\n         return 'int32'\n-    if dtype_str in ['int8', 'uint8', 'int16', 'uint16']:\n-        return 'int32'\n     if dtype_str == 'bfloat16':\n         return 'float32'\n     return dtype_str"}, {"filename": "python/triton/common/build.py", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -18,7 +18,7 @@ def is_hip():\n \n @functools.lru_cache()\n def libcuda_dirs():\n-    libs = subprocess.check_output([\"ldconfig\", \"-p\"]).decode()\n+    libs = subprocess.check_output([\"/sbin/ldconfig\", \"-p\"]).decode()\n     # each line looks like the following:\n     # libcuda.so.1 (libc6,x86-64) => /lib/x86_64-linux-gnu/libcuda.so.1\n     locs = [line.split()[-1] for line in libs.splitlines() if \"libcuda.so\" in line]\n@@ -27,6 +27,9 @@ def libcuda_dirs():\n     if locs:\n         msg += 'Possible files are located at %s.' % str(locs)\n         msg += 'Please create a symlink of libcuda.so to any of the file.'\n+    else:\n+        msg += 'Please make sure GPU is setup and then run \"/sbin/ldconfig\"'\n+        msg += ' (requires sudo) to refresh the linker cache.'\n     assert any(os.path.exists(os.path.join(path, 'libcuda.so')) for path in dirs), msg\n     return dirs\n "}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 0, "deletions": 5, "changes": 5, "file_content_changes": "@@ -1351,11 +1351,6 @@ def make_combine_region(reduce_op):\n @builtin\n def _promote_reduction_input(t, _builder=None):\n     scalar_ty = t.type.scalar\n-    # input is extended to 32-bits if necessary\n-    # this increases numerical accuracy and can be done pretty much for free\n-    # on GPUs\n-    if scalar_ty.is_int() and scalar_ty.int_bitwidth < 32:\n-        return t.to(int32, _builder=_builder)\n \n     # hardware doesn't support FMAX, FMIN, CMP for bfloat16\n     if scalar_ty is bfloat16:"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -133,6 +133,8 @@ def add(input: tl.tensor,\n     # ptr + offset\n     if other_scalar_ty.is_ptr() and not input_scalar_ty.is_ptr():\n         input, other = other, input\n+        input_scalar_ty = input.type.scalar\n+        other_scalar_ty = other.type.scalar\n     if input_scalar_ty.is_ptr():\n         return tl.tensor(builder.create_addptr(input.handle, other.handle), input.type)\n     # float + float"}, {"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -342,7 +342,7 @@ def backward(ctx, do):\n         dk = torch.empty_like(k)\n         dv = torch.empty_like(v)\n         delta = torch.empty_like(L)\n-        _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n+        _bwd_preprocess[(cdiv(q.shape[2], BLOCK) * ctx.grid[1], )](\n             o, do,\n             delta,\n             BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -377,9 +377,9 @@ def get_max_tensorcore_tflops(dtype, backend=None, device=None, clock_rate=None)\n         assert dtype == torch.float16\n         ops_per_sub_core = 256  # 2 4x4x4 Tensor Cores\n     else:\n-        if dtype == torch.float32:\n+        if dtype in [torch.float32, torch.int32]:\n             ops_per_sub_core = 256\n-        elif dtype in [torch.float16, torch.bfloat16]:\n+        elif dtype in [torch.float16, torch.bfloat16, torch.int16]:\n             ops_per_sub_core = 512\n         elif dtype in [torch.int8, tl.float8e4, tl.float8e4b15, tl.float8e5]:\n             ops_per_sub_core = 1024"}]