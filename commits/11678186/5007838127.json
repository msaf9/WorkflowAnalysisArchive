[{"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 175, "deletions": 83, "changes": 258, "file_content_changes": "@@ -1,5 +1,6 @@\n #include \"mlir/IR/Matchers.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n \n #include \"ConvertLayoutOpToLLVM.h\"\n #include \"LoadStoreOpToLLVM.h\"\n@@ -454,10 +455,12 @@ struct AtomicRMWOpConversion\n   AtomicRMWOpConversion(TritonGPUToLLVMTypeConverter &converter,\n                         ModuleAllocation &allocation,\n                         ModuleAxisInfoAnalysis &axisAnalysisPass,\n-                        PatternBenefit benefit)\n+                        PatternBenefit benefit,\n+                        int computeCapability)\n       : ConvertTritonGPUOpToLLVMPattern<triton::AtomicRMWOp>(\n             converter, allocation, benefit),\n-        LoadStoreConversionBase(axisAnalysisPass) {}\n+        LoadStoreConversionBase(axisAnalysisPass),\n+        computeCapability(computeCapability) {}\n \n   LogicalResult\n   matchAndRewrite(triton::AtomicRMWOp op, OpAdaptor adaptor,\n@@ -496,7 +499,7 @@ struct AtomicRMWOpConversion\n     // tensor\n     if (tensorTy) {\n       auto valTy = val.getType().cast<RankedTensorType>();\n-      vec = std::min<unsigned>(vec, valTy.getElementType().isF16() ? 2 : 1);\n+      vec = std::min<unsigned>(vec, (valTy.getElementType().isF16() || valTy.getElementType().isBF16()) ? 2 : 1);\n       // mask\n       numElems = tensorTy.getNumElements();\n     }\n@@ -514,88 +517,173 @@ struct AtomicRMWOpConversion\n \n       Value rmwPtr = ptrElements[i];\n       Value rmwMask = llMask ? and_(mask, maskElements[i]) : mask;\n-      std::string sTy;\n-      PTXBuilder ptxBuilderAtomicRMW;\n-      std::string tyId = valueElemNBits * vec == 64\n-                             ? \"l\"\n-                             : (valueElemNBits * vec == 32 ? \"r\" : \"h\");\n-      auto *dstOpr = ptxBuilderAtomicRMW.newOperand(\"=\" + tyId, /*init=*/true);\n-      auto *ptrOpr = ptxBuilderAtomicRMW.newAddrOperand(rmwPtr, \"l\");\n-      auto *valOpr = ptxBuilderAtomicRMW.newOperand(rmwVal, tyId);\n-\n-      auto &atom = ptxBuilderAtomicRMW.create<>(\"atom\")->global().o(\"gpu\");\n-      auto rmwOp = stringifyRMWOp(atomicRmwAttr).str();\n-      auto sBits = std::to_string(valueElemNBits);\n-      switch (atomicRmwAttr) {\n-      case RMWOp::AND:\n-        sTy = \"b\" + sBits;\n-        break;\n-      case RMWOp::OR:\n-        sTy = \"b\" + sBits;\n-        break;\n-      case RMWOp::XOR:\n-        sTy = \"b\" + sBits;\n-        break;\n-      case RMWOp::ADD:\n-        sTy = \"s\" + sBits;\n-        break;\n-      case RMWOp::FADD:\n-        rmwOp = \"add\";\n-        rmwOp += (valueElemNBits == 16 ? \".noftz\" : \"\");\n-        sTy = \"f\" + sBits;\n-        sTy += (vec == 2 && valueElemNBits == 16) ? \"x2\" : \"\";\n-        break;\n-      case RMWOp::MAX:\n-        sTy = \"s\" + sBits;\n-        break;\n-      case RMWOp::MIN:\n-        sTy = \"s\" + sBits;\n-        break;\n-      case RMWOp::UMAX:\n-        rmwOp = \"max\";\n-        sTy = \"u\" + sBits;\n-        break;\n-      case RMWOp::UMIN:\n-        rmwOp = \"min\";\n-        sTy = \"u\" + sBits;\n-        break;\n-      case RMWOp::XCHG:\n-        sTy = \"b\" + sBits;\n-        break;\n-      default:\n-        return failure();\n-      }\n-      atom.o(rmwOp).o(sTy);\n-      if (tensorTy) {\n-        atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n+\n+      if ((computeCapability >= 80 && computeCapability < 90) &&\n+          atomicRmwAttr == RMWOp::FADD && tensorTy.getElementType().isBF16()) {\n+\n+        assert(vec == 2);\n         auto retType = vec == 1 ? valueElemTy : vecTy;\n-        auto ret = ptxBuilderAtomicRMW.launch(rewriter, loc, retType);\n+        Type proxyType = rewriter.getI32Type();\n+        rmwVal = rewriter.create<LLVM::BitcastOp>(loc, proxyType, rmwVal);\n+        // atomic_add for bf16 (and other types)\n+        // ignore masks (for now)\n+        PTXBuilder ldBuilder;\n+        auto &ld = ldBuilder.create<>(\"ld\")\n+                    ->global()\n+                    .b(vec * 16);\n+        auto *loadedOpr = ldBuilder.newOperand(\"=r\");\n+        auto *ldPtrOpr = ldBuilder.newAddrOperand(rmwPtr, \"l\");\n+        ld(loadedOpr, ldPtrOpr);\n+        Value loaded = ldBuilder.launch(rewriter, loc, proxyType);\n+\n+        // xx => 16 or 32 depending on vec\n+        // ^bb0\n+        //   %init = ld.global.bxx \n+        //   br ^bb1(%init)\n+        // ^bb1(%assume)\n+        //   %sum = add %val, %assume\n+        //   %old = atom.global.cas.bxx [%ptr], %assume, %sum\n+        //   setp.ne.sxx %cond, %assume, %old\n+        //   cond_br %cond, ^bb1(%old), ^bb2(%old)\n+        // ^bb2(%atom_val)\n+        //   use(%atom_val)\n+        Region *region = op->getParentRegion();\n+        Block *currentBlock = op->getBlock();\n+\n+        Block *loop = rewriter.splitBlock(currentBlock, rewriter.getInsertionPoint());\n+        Block *continuation = rewriter.splitBlock(loop, loop->begin());\n+        loop->addArgument(proxyType, loc);\n+        continuation->addArgument(proxyType, loc);\n+        Value assume = loop->getArgument(0);\n+        rewriter.setInsertionPointToEnd(loop);\n+        // a + b\n+        const char *bf162AddAsm =\n+          \"{.reg .b32 c; \\n\"\n+          \" mov.b32 c, 0x3f803f80U; \\n\"\n+          \" fma.rn.bf16x2 $0, $1, c, $2; }\\n\";\n+        PTXBuilder ptxBuilderBf16Add;\n+        auto &bf16add = *ptxBuilderBf16Add.create<PTXInstr>(bf162AddAsm);\n+        auto o = ptxBuilderBf16Add.newOperand(\"=r\");\n+        auto i0 = ptxBuilderBf16Add.newOperand(rmwVal, \"r\");\n+        auto i1 = ptxBuilderBf16Add.newOperand(assume, \"r\");\n+        bf16add({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n+        Value sum = ptxBuilderBf16Add.launch(rewriter, loc, proxyType, /*hasSideEffect*/false);\n+        \n+        PTXBuilder ptxBuilderAtomicCAS;\n+        auto *dstOpr = ptxBuilderAtomicCAS.newOperand(\"=r\");\n+        auto *ptrOpr = ptxBuilderAtomicCAS.newAddrOperand(rmwPtr, \"l\");\n+        auto *cmpOpr = ptxBuilderAtomicCAS.newOperand(assume, \"r\");\n+        auto *valOpr = ptxBuilderAtomicCAS.newOperand(sum, \"r\");\n+        auto &atom = *ptxBuilderAtomicCAS.create<PTXInstr>(\"atom\");\n+        atom.global().o(\"cas\").o(\"b32\");\n+        atom(dstOpr, ptrOpr, cmpOpr, valOpr);\n+        Value old = ptxBuilderAtomicCAS.launch(rewriter, loc, proxyType);\n+\n+        // compare as integers\n+        Value assumeI32 = rewriter.create<LLVM::BitcastOp>(loc, rewriter.getI32Type(), assume);\n+        Value oldI32 = rewriter.create<LLVM::BitcastOp>(loc, rewriter.getI32Type(), old);\n+        Value cond = rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::ne, assumeI32, oldI32);\n+\n+        // Block *continuation = rewriter.createBlock(region, region->end(), {retType}, loc);\n+        rewriter.setInsertionPointToEnd(currentBlock);\n+        // It's fine to use cf::BranchOp here. We have cf => llvm conversion\n+        // after this.\n+        rewriter.create<cf::BranchOp>(loc, ValueRange{loaded}, loop);\n+        rewriter.setInsertionPointToEnd(loop);\n+        rewriter.create<cf::CondBranchOp>(loc, cond, loop, ValueRange{old}, continuation, ValueRange{old});\n+        rewriter.setInsertionPointToStart(continuation);\n+\n+        // extract results\n+        Value ret = continuation->getArgument(0);\n+        ret = rewriter.create<LLVM::BitcastOp>(loc, vecTy, ret);\n         for (int ii = 0; ii < vec; ++ii) {\n-          resultVals[i + ii] =\n-              vec == 1 ? ret : extract_element(valueElemTy, ret, i32_val(ii));\n+            resultVals[i + ii] =\n+                vec == 1 ? ret : extract_element(valueElemTy, ret, i32_val(ii));\n         }\n       } else {\n-        PTXBuilder ptxBuilderMemfence;\n-        auto memfenc = ptxBuilderMemfence.create<PTXInstr>(\"membar\")->o(\"gl\");\n-        memfenc();\n-        auto ASMReturnTy = void_ty(ctx);\n-        ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n-        atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n-        auto old = ptxBuilderAtomicRMW.launch(rewriter, loc, valueElemTy);\n-        Value atomPtr = getSharedMemoryBase(loc, rewriter, op.getOperation());\n-        atomPtr = bitcast(atomPtr, ptr_ty(valueElemTy, 3));\n-        // Only threads with rmwMask = True store the result\n-        PTXBuilder ptxBuilderStore;\n-        auto &storeShared =\n-            ptxBuilderStore.create<>(\"st\")->shared().o(\"b\" + sBits);\n-        auto *ptrOpr = ptxBuilderStore.newAddrOperand(atomPtr, \"r\");\n-        auto *valOpr = ptxBuilderStore.newOperand(old, tyId);\n-        storeShared(ptrOpr, valOpr).predicate(rmwMask);\n-        ptxBuilderStore.launch(rewriter, loc, void_ty(ctx));\n-        barrier();\n-        Value ret = load(atomPtr);\n-        barrier();\n-        rewriter.replaceOp(op, {ret});\n+        std::string sTy;\n+        PTXBuilder ptxBuilderAtomicRMW;\n+        std::string tyId = valueElemNBits * vec == 64\n+                              ? \"l\"\n+                              : (valueElemNBits * vec == 32 ? \"r\" : \"h\");\n+        auto *dstOpr = ptxBuilderAtomicRMW.newOperand(\"=\" + tyId, /*init=*/true);\n+        auto *ptrOpr = ptxBuilderAtomicRMW.newAddrOperand(rmwPtr, \"l\");\n+        auto *valOpr = ptxBuilderAtomicRMW.newOperand(rmwVal, tyId);\n+\n+        auto &atom = ptxBuilderAtomicRMW.create<>(\"atom\")->global().o(\"gpu\");\n+        auto rmwOp = stringifyRMWOp(atomicRmwAttr).str();\n+        auto sBits = std::to_string(valueElemNBits);\n+        switch (atomicRmwAttr) {\n+        case RMWOp::AND:\n+          sTy = \"b\" + sBits;\n+          break;\n+        case RMWOp::OR:\n+          sTy = \"b\" + sBits;\n+          break;\n+        case RMWOp::XOR:\n+          sTy = \"b\" + sBits;\n+          break;\n+        case RMWOp::ADD:\n+          sTy = \"s\" + sBits;\n+          break;\n+        case RMWOp::FADD:\n+          rmwOp = \"add\";\n+          rmwOp += (valueElemNBits == 16 ? \".noftz\" : \"\");\n+          sTy = \"f\" + sBits;\n+          sTy += (vec == 2 && valueElemNBits == 16) ? \"x2\" : \"\";\n+          break;\n+        case RMWOp::MAX:\n+          sTy = \"s\" + sBits;\n+          break;\n+        case RMWOp::MIN:\n+          sTy = \"s\" + sBits;\n+          break;\n+        case RMWOp::UMAX:\n+          rmwOp = \"max\";\n+          sTy = \"u\" + sBits;\n+          break;\n+        case RMWOp::UMIN:\n+          rmwOp = \"min\";\n+          sTy = \"u\" + sBits;\n+          break;\n+        case RMWOp::XCHG:\n+          sTy = \"b\" + sBits;\n+          break;\n+        default:\n+          return failure();\n+        }\n+        atom.o(rmwOp).o(sTy);\n+        if (tensorTy) {\n+          atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n+          auto retType = vec == 1 ? valueElemTy : vecTy;\n+          auto ret = ptxBuilderAtomicRMW.launch(rewriter, loc, retType);\n+          for (int ii = 0; ii < vec; ++ii) {\n+            resultVals[i + ii] =\n+                vec == 1 ? ret : extract_element(valueElemTy, ret, i32_val(ii));\n+          }\n+        } else {\n+          PTXBuilder ptxBuilderMemfence;\n+          auto memfenc = ptxBuilderMemfence.create<PTXInstr>(\"membar\")->o(\"gl\");\n+          memfenc();\n+          auto ASMReturnTy = void_ty(ctx);\n+          ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n+          atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n+          auto old = ptxBuilderAtomicRMW.launch(rewriter, loc, valueElemTy);\n+          Value atomPtr = getSharedMemoryBase(loc, rewriter, op.getOperation());\n+          atomPtr = bitcast(atomPtr, ptr_ty(valueElemTy, 3));\n+          // Only threads with rmwMask = True store the result\n+          PTXBuilder ptxBuilderStore;\n+          auto &storeShared =\n+              ptxBuilderStore.create<>(\"st\")->shared().o(\"b\" + sBits);\n+          auto *ptrOpr = ptxBuilderStore.newAddrOperand(atomPtr, \"r\");\n+          auto *valOpr = ptxBuilderStore.newOperand(old, tyId);\n+          storeShared(ptrOpr, valOpr).predicate(rmwMask);\n+          ptxBuilderStore.launch(rewriter, loc, void_ty(ctx));\n+          barrier();\n+          Value ret = load(atomPtr);\n+          barrier();\n+          rewriter.replaceOp(op, {ret});\n+        }\n       }\n     }\n     if (tensorTy) {\n@@ -606,6 +694,9 @@ struct AtomicRMWOpConversion\n     }\n     return success();\n   }\n+\n+private:\n+  int computeCapability{};\n };\n \n struct InsertSliceOpConversion\n@@ -844,13 +935,14 @@ void populateLoadStoreOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     ModuleAxisInfoAnalysis &axisInfoAnalysis, ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n-    PatternBenefit benefit) {\n+    PatternBenefit benefit, int computeCapability) {\n   patterns.add<LoadOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n   patterns.add<StoreOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n   patterns.add<AtomicCASOpConversion>(typeConverter, allocation,\n                                       axisInfoAnalysis, benefit);\n   patterns.add<AtomicRMWOpConversion>(typeConverter, allocation,\n-                                      axisInfoAnalysis, benefit);\n+                                      axisInfoAnalysis, benefit,\n+                                      computeCapability);\n   patterns.add<InsertSliceOpConversion>(typeConverter, allocation,\n                                         indexCacheInfo, benefit);\n   patterns.add<InsertSliceAsyncOpConversion>("}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -10,6 +10,6 @@ void populateLoadStoreOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     ModuleAxisInfoAnalysis &axisInfoAnalysis, ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n-    PatternBenefit benefit);\n+    PatternBenefit benefit, int computeCapability);\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -369,7 +369,7 @@ class ConvertTritonGPUToLLVM\n     populateElementwiseOpToLLVMPatterns(typeConverter, patterns, /*benefit=*/1);\n     populateLoadStoreOpToLLVMPatterns(typeConverter, patterns, axisInfoAnalysis,\n                                       allocation, indexCacheInfo,\n-                                      /*benefit=*/1);\n+                                      /*benefit=*/1, computeCapability);\n     populateReduceOpToLLVMPatterns(typeConverter, patterns, allocation,\n                                    indexCacheInfo, /*benefit=*/1);\n     populateViewOpToLLVMPatterns(typeConverter, patterns, /*benefit=*/1);"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1039,7 +1039,7 @@ def atom_red_typechecking_impl(ptr: tl.tensor,\n     element_ty = ptr.type.scalar.element_ty\n     if element_ty is tl.float16 and op != 'add':\n         raise ValueError(\"atomic_\" + op + \" does not support fp16\")\n-    if element_ty in [tl.int1, tl.int8, tl.int16, tl.bfloat16]:\n+    if element_ty in [tl.int1, tl.int8, tl.int16]:\n         raise ValueError(\"atomic_\" + op + \" does not support \" + str(element_ty))\n     if ptr.type.is_block():\n         if mask:"}]