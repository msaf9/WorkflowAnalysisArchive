[{"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "file_content_changes": "@@ -27,6 +27,9 @@ namespace mlir {\n //===----------------------------------------------------------------------===//\n namespace triton {\n \n+// Bitwidth of pointers\n+constexpr int kPtrBitWidth = 64; \n+\n static std::pair<SmallVector<unsigned>, SmallVector<unsigned>>\n getCvtOrder(const Attribute &srcLayout, const Attribute &dstLayout) {\n   auto srcBlockedLayout = srcLayout.dyn_cast<BlockedEncodingAttr>();\n@@ -193,7 +196,9 @@ class AllocationAnalysis {\n       auto smemShape = getScratchConfigForCvtLayout(cvtLayout, inVec, outVec);\n       unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n                                        std::multiplies{});\n-      auto bytes = elems * srcTy.getElementTypeBitWidth() / 8;\n+      auto bytes = srcTy.getElementType().isa<triton::PointerType>()? \n+                   elems * kPtrBitWidth / 8 :\n+                   elems * srcTy.getElementTypeBitWidth() / 8;\n       allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     }\n   }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 21, "deletions": 9, "changes": 30, "file_content_changes": "@@ -93,6 +93,8 @@ void llPrintf(StringRef msg, ValueRange args,\n               ConversionPatternRewriter &rewriter);\n \n // Shortcuts for some commonly used LLVM ops to keep code simple and intuitive\n+#define inttoptr(...) rewriter.create<LLVM::IntToPtrOp>(loc, __VA_ARGS__)\n+#define ptrtoint(...) rewriter.create<LLVM::PtrToIntOp>(loc, __VA_ARGS__)\n #define zext(...) rewriter.create<LLVM::ZExtOp>(loc, __VA_ARGS__)\n #define udiv(...) rewriter.create<LLVM::UDivOp>(loc, __VA_ARGS__)\n #define urem(...) rewriter.create<LLVM::URemOp>(loc, __VA_ARGS__)\n@@ -2929,8 +2931,13 @@ void ConvertLayoutOpConversion::processReplica(\n   }\n   auto elemTy = type.getElementType();\n   bool isInt1 = elemTy.isInteger(1);\n+  bool isPtr = elemTy.isa<triton::PointerType>();\n+  auto llvmElemTyOrig = getTypeConverter()->convertType(elemTy);\n   if (isInt1)\n     elemTy = IntegerType::get(elemTy.getContext(), 8);\n+  else if (isPtr)\n+    elemTy = IntegerType::get(elemTy.getContext(), 64);\n+\n   auto llvmElemTy = getTypeConverter()->convertType(elemTy);\n \n   for (unsigned ctaId = 0; ctaId < accumNumCTAsEachRep; ++ctaId) {\n@@ -2963,6 +2970,8 @@ void ConvertLayoutOpConversion::processReplica(\n           auto currVal = vals[elemId + linearCTAId * accumSizePerThread + v];\n           if (isInt1)\n             currVal = zext(llvmElemTy, currVal);\n+          else if (isPtr)\n+            currVal = ptrtoint(llvmElemTy, currVal);\n \n           valVec = insert_element(vecTy, valVec, currVal, idx_val(v));\n         }\n@@ -2975,6 +2984,8 @@ void ConvertLayoutOpConversion::processReplica(\n             currVal =\n                 icmp_ne(currVal, rewriter.create<LLVM::ConstantOp>(\n                                      loc, i8_ty, rewriter.getI8IntegerAttr(0)));\n+          else if (isPtr)\n+            currVal = inttoptr(llvmElemTyOrig, currVal);\n           vals[elemId + linearCTAId * accumSizePerThread + v] = currVal;\n         }\n       }\n@@ -5933,19 +5944,19 @@ struct AtomicRMWOpConversion\n     auto elemsPerThread = getElemsPerThread(val.getType());\n     SmallVector<Value> resultVals(elemsPerThread);\n     for (size_t i = 0; i < elemsPerThread; i += vec) {\n-      Value rmvVal = undef(vecTy);\n+      Value rmwVal = undef(vecTy);\n       for (int ii = 0; ii < vec; ++ii) {\n         Value iiVal = createIndexAttrConstant(\n             rewriter, loc, getTypeConverter()->getIndexType(), ii);\n-        rmvVal = insert_element(vecTy, rmvVal, valElements[i], iiVal);\n+        rmwVal = insert_element(vecTy, rmwVal, valElements[i + ii], iiVal);\n       }\n-      Value rmwPtr = bitcast(ptrElements[i], ptr_ty(valTy.getElementType()));\n+      Value rmwPtr = ptrElements[i];\n       std::string sTy;\n       PTXBuilder ptxBuilder;\n \n       auto *dstOpr = ptxBuilder.newOperand(\"=r\");\n-      auto *ptrOpr = ptxBuilder.newAddrOperand(rmwPtr, \"r\");\n-      auto *valOpr = ptxBuilder.newOperand(rmvVal, \"r\");\n+      auto *ptrOpr = ptxBuilder.newAddrOperand(rmwPtr, \"l\");\n+      auto *valOpr = ptxBuilder.newOperand(rmwVal, \"r\");\n \n       auto &atom = ptxBuilder.create<>(\"atom\")->global().o(\"gpu\");\n       auto rmwOp = stringifyRMWOp(atomicRmwAttr).str();\n@@ -5987,12 +5998,13 @@ struct AtomicRMWOpConversion\n         return failure();\n       }\n       atom.o(rmwOp).o(sTy);\n-\n-      atom(dstOpr, ptrOpr, valOpr);\n-      auto ret = ptxBuilder.launch(rewriter, loc, valueElemTy, false);\n+      //TODO:[dongdongl] actual mask support\n+      Value pred = int_val(1, 1);\n+      atom(dstOpr, ptrOpr, valOpr).predicate(pred);\n+      auto ret = ptxBuilder.launch(rewriter, loc, valueElemTy);\n       for (int ii = 0; ii < vec; ++ii) {\n         resultVals[i * vec + ii] =\n-            vec == 1 ? ret : extract_element(vecTy, ret, idx_val(ii));\n+            vec == 1 ? ret : extract_element(valueElemTy, ret, idx_val(ii));\n       }\n     }\n     Type structTy = getTypeConverter()->convertType(valueTy);"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -378,7 +378,7 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n       TritonGenericPattern<triton::AddPtrOp>, TritonReducePattern,\n       TritonExpandDimsPattern, TritonMakeRangePattern, TritonDotPattern,\n       TritonLoadPattern, TritonStorePattern, TritonExtElemwisePattern,\n-      TritonPrintfPattern>(typeConverter, context);\n+      TritonPrintfPattern, TritonAtomicRMWPattern>(typeConverter, context);\n }\n \n //"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -1086,9 +1086,7 @@ void init_triton_ir(py::module &&m) {\n               mlir::Value &ptr, mlir::Value &val,\n               mlir::Value &mask) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n-             auto ptrType = mlir::getElementTypeOrSelf(ptr)\n-                                .cast<mlir::triton::PointerType>();\n-             mlir::Type dstType = ptrType.getPointeeType();\n+             mlir::Type dstType = val.getType();\n              return self.create<mlir::triton::AtomicRMWOp>(loc, dstType, rmwOp,\n                                                            ptr, val, mask);\n            })"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 19, "deletions": 0, "changes": 19, "file_content_changes": "@@ -671,6 +671,25 @@ def without_fn(X, Y, A, B, C):\n #     kernel[(1,)](z_tri, x_tri, axis, shape0, shape1)\n #     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n \n+def test_tensor_atomic_rmw_add_elementwise(device=\"cuda\"):\n+    shape0, shape1 = 16, 16\n+    @triton.jit\n+    def kernel(Z, X, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n+        off0 = tl.arange(0, SHAPE0)\n+        off1 = tl.arange(0, SHAPE1)\n+        x = tl.load(X + off0[:, None] * SHAPE1 + off1[None, :])\n+        tl.atomic_add(Z + off0[:, None] * SHAPE1 + off1[None, :], x)\n+\n+    rs = RandomState(17)\n+    x = numpy_random((shape0, shape1), dtype_str=\"float32\", rs=rs)\n+    z = numpy_random((shape0, shape1), dtype_str=\"float32\", rs=rs)\n+    # reference\n+    z_ref = z + x\n+    # triton result\n+    x_tri = torch.from_numpy(x).to(device=device)\n+    z_tri = torch.from_numpy(z).to(device=device)\n+    kernel[(1,)](z_tri, x_tri, shape0, shape1)\n+    np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n \n # def test_atomic_cas():\n #     # 1. make sure that atomic_cas changes the original value (Lock)"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 20, "deletions": 1, "changes": 21, "file_content_changes": "@@ -742,6 +742,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n }\n \n // -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n@@ -753,8 +754,8 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   }\n }\n \n-\n // -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n@@ -766,8 +767,25 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   }\n }\n \n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: convert_blocked_to_blocked_ptr\n+  func @convert_blocked_to_blocked_ptr(%src:tensor<32x!tt.ptr<f32>, #blocked0>) {\n+    // CHECK: llvm.ptrtoint\n+    // CHECK: llvm.store\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.inttoptr\n+    // CHECK-COUNT-4: llvm.insertvalue\n+    %cvt = triton_gpu.convert_layout %src : (tensor<32x!tt.ptr<f32>, #blocked0>) -> tensor<32x!tt.ptr<f32>, #blocked1>\n+    return\n+  }\n+}\n \n // -----\n+\n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n #shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n #mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n@@ -839,6 +857,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n }\n \n // -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: atomic_add_f32"}]