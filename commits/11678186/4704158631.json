[{"filename": ".pre-commit-config.yaml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -24,7 +24,7 @@ repos:\n     rev: v1.6.0\n     hooks:\n       - id: autopep8\n-        args: [\"-a\", \"-i\", \"--max-line-length\", \"88\"]\n+        args: [\"-i\"]\n         stages: [commit, push, manual]\n   - repo: https://github.com/pycqa/flake8\n     rev: 6.0.0"}, {"filename": "docs/python-api/triton.language.rst", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -96,9 +96,13 @@ Reduction Ops\n     :toctree: generated\n     :nosignatures:\n \n+    argmax\n+    argmin\n     max\n     min\n+    reduce\n     sum\n+    xor_sum\n \n \n Atomic Ops"}, {"filename": "python/pyproject.toml", "status": "added", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -0,0 +1,8 @@\n+\n+[build-system]\n+requires = [\"setuptools>=40.8.0\", \"wheel\", \"cmake>=3.18\"]\n+\n+[tool.autopep8]\n+aggressive = 1\n+ignore = \"E501,E701,E731,W690\"\n+max_line_length = 88"}, {"filename": "python/setup.cfg", "status": "removed", "additions": 0, "deletions": 8, "changes": 8, "file_content_changes": "@@ -1,8 +0,0 @@\n-[metadata]\n-description_file = README.md\n-\n-[pycodestyle]\n-ignore = E501,E701,E731\n-\n-[flake8]\n-ignore = E501,E701,E731"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 37, "deletions": 0, "changes": 37, "file_content_changes": "@@ -1334,6 +1334,43 @@ def test_reduce_layouts(M, N, src_layout, axis, device='cuda'):\n     np.testing.assert_allclose(z_ref, z_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n \n \n+@triton.jit\n+def _welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n+    delta = mean_2 - mean_1\n+    new_weight = weight_1 + weight_2\n+    w2_over_w = weight_2 / new_weight\n+    return (\n+        mean_1 + delta * w2_over_w,\n+        m2_1 + m2_2 + delta * delta * weight_1 * w2_over_w,\n+        new_weight,\n+    )\n+\n+\n+def test_generic_reduction(device='cuda'):\n+\n+    @triton.jit\n+    def var_mean_kernel(X, out_mean, out_var, BLOCK: tl.constexpr):\n+        xindex = tl.arange(0, BLOCK)\n+        x = tl.load(X + xindex)\n+        mean = x\n+        m2 = tl.zeros_like(x)\n+        weight = tl.full(x.shape, 1, x.dtype)\n+        (mean, m2, weight) = tl.reduce((mean, m2, weight), 0, _welford_combine)\n+        tl.store(out_mean, mean)\n+        tl.store(out_var, m2 / weight)\n+\n+    SIZE = 512\n+    x = torch.rand(SIZE, device=device)\n+    out_mean = torch.empty((), device=device)\n+    out_var = torch.empty((), device=device)\n+\n+    var_mean_kernel[(1,)](x, out_mean, out_var, BLOCK=SIZE)\n+\n+    expect_var, expect_mean = torch.var_mean(x, dim=0, correction=0)\n+    torch.testing.assert_close(out_mean, expect_mean)\n+    torch.testing.assert_close(out_var, expect_var)\n+\n+\n # ---------------\n # test permute\n # ---------------"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -121,6 +121,7 @@ def ptx_get_version(cuda_version) -> int:\n     raise RuntimeError(\"Triton only support CUDA 10.0 or higher\")\n \n \n+@functools.lru_cache\n def path_to_ptxas():\n     base_dir = os.path.join(os.path.dirname(__file__), os.pardir)\n     paths = ["}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -65,6 +65,7 @@\n     pi32_t,\n     pointer_type,\n     program_id,\n+    reduce,\n     reshape,\n     sin,\n     sqrt,\n@@ -168,6 +169,7 @@\n     \"randn\",\n     \"randn4x\",\n     \"ravel\",\n+    \"reduce\",\n     \"reshape\",\n     \"sigmoid\",\n     \"sin\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -1224,7 +1224,7 @@ def _insertion_guard(builder):\n \n \n @builtin\n-def reduction(input, axis, combine_fn, _builder=None, _generator=None):\n+def reduce(input, axis, combine_fn, _builder=None, _generator=None):\n     \"\"\"Applies the combine_fn to all elements in :code:`input` tensors along the provided :code:`axis`\n \n     :param input: the input tensor, or tuple of tensors\n@@ -1233,8 +1233,8 @@ def reduction(input, axis, combine_fn, _builder=None, _generator=None):\n \n     \"\"\"\n     if isinstance(input, tensor):\n-        return reduction((input,), axis, combine_fn,\n-                         _builder=_builder, _generator=_generator)[0]\n+        return reduce((input,), axis, combine_fn,\n+                      _builder=_builder, _generator=_generator)[0]\n \n     def make_combine_region(reduce_op):\n         in_scalar_tys = [t.type.scalar for t in input]\n@@ -1286,8 +1286,8 @@ def _argreduce(input, axis, combine_fn, _builder=None, _generator=None):\n         index = index.__getitem__(expand_dims_index, _builder=_builder)\n         index = broadcast_to(index, input.shape, _builder=_builder)\n \n-    rvalue, rindices = reduction((input, index), axis, combine_fn,\n-                                 _builder=_builder, _generator=_generator)\n+    rvalue, rindices = reduce((input, index), axis, combine_fn,\n+                              _builder=_builder, _generator=_generator)\n     return rindices\n \n \n@@ -1326,7 +1326,7 @@ def _max_combine(a, b):\n @_add_reduction_docstr(\"maximum\")\n def max(input, axis):\n     input = _promote_reduction_input(input)\n-    return reduction(input, axis, _max_combine)\n+    return reduce(input, axis, _max_combine)\n \n \n @triton.jit\n@@ -1356,7 +1356,7 @@ def _min_combine(a, b):\n @_add_reduction_docstr(\"minimum\")\n def min(input, axis):\n     input = _promote_reduction_input(input)\n-    return reduction(input, axis, _min_combine)\n+    return reduce(input, axis, _min_combine)\n \n \n @triton.jit\n@@ -1385,7 +1385,7 @@ def _sum_combine(a, b):\n @_add_reduction_docstr(\"sum\")\n def sum(input, axis):\n     input = _promote_reduction_input(input)\n-    return reduction(input, axis, _sum_combine)\n+    return reduce(input, axis, _sum_combine)\n \n \n @triton.jit\n@@ -1401,8 +1401,8 @@ def xor_sum(input, axis, _builder=None, _generator=None):\n         raise ValueError(\"xor_sum only supported for integers\")\n \n     input = _promote_reduction_input(input, _builder=_builder)\n-    return reduction(input, axis, _xor_combine,\n-                     _builder=_builder, _generator=_generator)\n+    return reduce(input, axis, _xor_combine,\n+                  _builder=_builder, _generator=_generator)\n \n \n # -----------------------"}]