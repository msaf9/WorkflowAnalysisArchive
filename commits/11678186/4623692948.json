[{"filename": "include/triton/Dialect/Triton/IR/Dialect.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -37,14 +37,14 @@ class DialectInferLayoutInterface\n   virtual LogicalResult\n   inferExpandDimsOpEncoding(Attribute operandEncoding, unsigned axis,\n                             Attribute &resultEncoding,\n-                            Optional<Location> location) const = 0;\n+                            std::optional<Location> location) const = 0;\n \n   // Note: this function only verify operand encoding but doesn't infer result\n   // encoding\n   virtual LogicalResult\n   inferDotOpEncoding(Attribute operandEncoding, unsigned opIdx,\n                      Attribute retEncoding,\n-                     Optional<Location> location) const = 0;\n+                     std::optional<Location> location) const = 0;\n };\n \n } // namespace triton"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 0, "deletions": 151, "changes": 151, "file_content_changes": "@@ -1122,154 +1122,3 @@ void populateElementwiseOpToLLVMPatterns(\n   // __nv_expf for higher-precision calculation\n   patterns.add<ExpOpConversionApprox>(typeConverter, benefit);\n }\n-\n-struct FPExtOpConversion\n-    : ElementwiseOpConversionBase<LLVM::FPExtOp, FPExtOpConversion> {\n-  using Base = ElementwiseOpConversionBase<LLVM::FPExtOp, FPExtOpConversion>;\n-  using Base::Base;\n-  using Adaptor = typename Base::OpAdaptor;\n-\n-  static bool isLegalOp(LLVM::FPExtOp op) {\n-    auto retTy = op.getResult().getType();\n-    auto srcTy = op.getOperand().getType();\n-    if (retTy.isF32() && srcTy.isF16()) {\n-      return false;\n-    }\n-    return true;\n-  }\n-\n-  Value createDestOp(LLVM::FPExtOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n-    return FpToFpOpConversion::convertFp16ToFp32(loc, rewriter, operands[0]);\n-  }\n-};\n-\n-struct FPTruncOpConversion\n-    : ElementwiseOpConversionBase<LLVM::FPTruncOp, FPTruncOpConversion> {\n-  using Base =\n-      ElementwiseOpConversionBase<LLVM::FPTruncOp, FPTruncOpConversion>;\n-  using Base::Base;\n-  using Adaptor = typename Base::OpAdaptor;\n-\n-  static bool isLegalOp(LLVM::FPTruncOp op) {\n-    auto retTy = op.getResult().getType();\n-    auto srcTy = op.getOperand().getType();\n-    if (retTy.isF16() && srcTy.isF32()) {\n-      return false;\n-    }\n-    return true;\n-  }\n-\n-  Value createDestOp(LLVM::FPTruncOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n-    return FpToFpOpConversion::convertFp32ToFp16(loc, rewriter, operands[0]);\n-  }\n-};\n-\n-struct TruncOpConversion\n-    : ElementwiseOpConversionBase<LLVM::TruncOp, TruncOpConversion> {\n-  using Base = ElementwiseOpConversionBase<LLVM::TruncOp, TruncOpConversion>;\n-  using Base::Base;\n-  using Adaptor = typename Base::OpAdaptor;\n-\n-  static bool isLegalOp(LLVM::TruncOp op) {\n-    auto retTy = op.getResult().getType();\n-    auto srcTy = op.getOperand().getType();\n-    if (retTy.isInteger(16) && srcTy.isInteger(32)) {\n-      return false;\n-    }\n-    return true;\n-  }\n-\n-  Value createDestOp(LLVM::TruncOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n-    PTXBuilder builder;\n-    auto &cvt = *builder.create(\"cvt.u16.u32\");\n-    auto res = builder.newOperand(\"=h\");\n-    auto operand = builder.newOperand(operands[0], \"r\");\n-    cvt(res, operand);\n-    return builder.launch(rewriter, loc, i16_ty, false);\n-  }\n-};\n-\n-struct SExtOpConversion\n-    : ElementwiseOpConversionBase<LLVM::SExtOp, SExtOpConversion> {\n-  using Base = ElementwiseOpConversionBase<LLVM::SExtOp, SExtOpConversion>;\n-  using Base::Base;\n-  using Adaptor = typename Base::OpAdaptor;\n-\n-  static bool isLegalOp(LLVM::SExtOp op) {\n-    auto retTy = op.getResult().getType();\n-    auto srcTy = op.getOperand().getType();\n-    if (retTy.isInteger(32) && srcTy.isInteger(16)) {\n-      return false;\n-    }\n-    return true;\n-  }\n-\n-  Value createDestOp(LLVM::SExtOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n-    PTXBuilder builder;\n-    auto &cvt = *builder.create(\"cvt.s32.s16\");\n-    auto res = builder.newOperand(\"=r\");\n-    auto operand = builder.newOperand(operands[0], \"h\");\n-    cvt(res, operand);\n-    return builder.launch(rewriter, loc, i32_ty, false);\n-  }\n-};\n-\n-struct ZExtOpConversion\n-    : ElementwiseOpConversionBase<LLVM::ZExtOp, ZExtOpConversion> {\n-  using Base = ElementwiseOpConversionBase<LLVM::ZExtOp, ZExtOpConversion>;\n-  using Base::Base;\n-  using Adaptor = typename Base::OpAdaptor;\n-\n-  static bool isLegalOp(LLVM::ZExtOp op) {\n-    auto retTy = op.getResult().getType();\n-    auto srcTy = op.getOperand().getType();\n-    if (retTy.isInteger(32) && srcTy.isInteger(16)) {\n-      return false;\n-    }\n-    return true;\n-  }\n-\n-  Value createDestOp(LLVM::ZExtOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n-    PTXBuilder builder;\n-    auto &cvt = *builder.create(\"cvt.u32.u16\");\n-    auto res = builder.newOperand(\"=r\");\n-    auto operand = builder.newOperand(operands[0], \"h\");\n-    cvt(res, operand);\n-    return builder.launch(rewriter, loc, i32_ty, false);\n-  }\n-};\n-\n-bool isLegalElementwiseOp(Operation *op) {\n-  if (isa<LLVM::FPExtOp>(op)) {\n-    return FPExtOpConversion::isLegalOp(cast<LLVM::FPExtOp>(op));\n-  } else if (isa<LLVM::FPTruncOp>(op)) {\n-    return FPTruncOpConversion::isLegalOp(cast<LLVM::FPTruncOp>(op));\n-  } else if (isa<LLVM::TruncOp>(op)) {\n-    return TruncOpConversion::isLegalOp(cast<LLVM::TruncOp>(op));\n-  } else if (isa<LLVM::SExtOp>(op)) {\n-    return SExtOpConversion::isLegalOp(cast<LLVM::SExtOp>(op));\n-  } else if (isa<LLVM::ZExtOp>(op)) {\n-    return ZExtOpConversion::isLegalOp(cast<LLVM::ZExtOp>(op));\n-  }\n-  return true;\n-}\n-\n-void populateElementwiseOpToPTXPatterns(\n-    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    PatternBenefit benefit) {\n-  patterns.add<FPExtOpConversion>(typeConverter, benefit);\n-  patterns.add<FPTruncOpConversion>(typeConverter, benefit);\n-  patterns.add<TruncOpConversion>(typeConverter, benefit);\n-  patterns.add<SExtOpConversion>(typeConverter, benefit);\n-  patterns.add<ZExtOpConversion>(typeConverter, benefit);\n-}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.h", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "file_content_changes": "@@ -13,8 +13,4 @@ void populateElementwiseOpToLLVMPatterns(\n \n bool isLegalElementwiseOp(Operation *op);\n \n-void populateElementwiseOpToPTXPatterns(\n-    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    PatternBenefit benefit);\n-\n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 0, "deletions": 45, "changes": 45, "file_content_changes": "@@ -56,28 +56,6 @@ class TritonLLVMFunctionConversionTarget : public ConversionTarget {\n   }\n };\n \n-class TritonPTXConversionTarget : public ConversionTarget {\n-public:\n-  explicit TritonPTXConversionTarget(MLIRContext &ctx) : ConversionTarget(ctx) {\n-    addDynamicallyLegalDialect<LLVM::LLVMDialect>(\n-        [&](Operation *op) { return isLegalElementwiseOp(op); });\n-\n-    addLegalDialect<NVVM::NVVMDialect>();\n-    addLegalOp<mlir::UnrealizedConversionCastOp>();\n-  }\n-};\n-\n-class TritonGCNConversionTarget : public ConversionTarget {\n-public:\n-  explicit TritonGCNConversionTarget(MLIRContext &ctx) : ConversionTarget(ctx) {\n-    addDynamicallyLegalDialect<LLVM::LLVMDialect>(\n-        [&](Operation *op) { return isLegalElementwiseOp(op); });\n-\n-    addLegalDialect<ROCDL::ROCDLDialect>();\n-    addLegalOp<mlir::UnrealizedConversionCastOp>();\n-  }\n-};\n-\n struct ReturnOpConversion : public ConvertOpToLLVMPattern<func::ReturnOp> {\n   using ConvertOpToLLVMPattern<func::ReturnOp>::ConvertOpToLLVMPattern;\n \n@@ -236,29 +214,6 @@ class ConvertTritonGPUToLLVM\n                                                           patterns);\n     if (failed(applyPartialConversion(mod, target, std::move(patterns))))\n       return signalPassFailure();\n-\n-    if (isROCM) {\n-      TritonGCNConversionTarget gcnTarget(*context);\n-      RewritePatternSet gcnPatterns(context);\n-      populateElementwiseOpToPTXPatterns(typeConverter, gcnPatterns,\n-                                         /*benefits=*/10);\n-      if (failed(\n-              applyPartialConversion(mod, gcnTarget, std::move(gcnPatterns))))\n-        return signalPassFailure();\n-    } else {\n-      // Use our custom converters to convert some operations to PTX to avoid\n-      // using NVPTX for two reasons:\n-      // 1. NVPTX backend is flaky on data types like float16 and bfloat16\n-      // 2. In some cases, we may generate faster PTX code than NVPTX backend\n-      TritonPTXConversionTarget ptxTarget(*context);\n-      RewritePatternSet ptxPatterns(context);\n-      // Add patterns to convert LLVM to PTX\n-      populateElementwiseOpToPTXPatterns(typeConverter, ptxPatterns,\n-                                         /*benefits=*/10);\n-      if (failed(\n-              applyPartialConversion(mod, ptxTarget, std::move(ptxPatterns))))\n-        return signalPassFailure();\n-    }\n   }\n \n private:"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.cpp", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -17,21 +17,21 @@ TritonGPUToLLVMTypeConverter::TritonGPUToLLVMTypeConverter(\n     MLIRContext *ctx, LowerToLLVMOptions &option,\n     const DataLayoutAnalysis *analysis)\n     : LLVMTypeConverter(ctx, option, analysis) {\n-  addConversion([&](triton::PointerType type) -> llvm::Optional<Type> {\n+  addConversion([&](triton::PointerType type) -> std::optional<Type> {\n     return convertTritonPointerType(type);\n   });\n-  addConversion([&](RankedTensorType type) -> llvm::Optional<Type> {\n+  addConversion([&](RankedTensorType type) -> std::optional<Type> {\n     return convertTritonTensorType(type);\n   });\n   // Internally store float8 as int8\n-  addConversion([&](mlir::Float8E4M3FNType type) -> llvm::Optional<Type> {\n+  addConversion([&](mlir::Float8E4M3FNType type) -> std::optional<Type> {\n     return IntegerType::get(type.getContext(), 8);\n   });\n-  addConversion([&](mlir::Float8E5M2Type type) -> llvm::Optional<Type> {\n+  addConversion([&](mlir::Float8E5M2Type type) -> std::optional<Type> {\n     return IntegerType::get(type.getContext(), 8);\n   });\n   // Internally store bfloat16 as int16\n-  addConversion([&](BFloat16Type type) -> llvm::Optional<Type> {\n+  addConversion([&](BFloat16Type type) -> std::optional<Type> {\n     return IntegerType::get(type.getContext(), 16);\n   });\n }"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -257,7 +257,7 @@ void StoreOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n \n //-- TransOp --\n mlir::LogicalResult mlir::triton::TransOp::inferReturnTypes(\n-    MLIRContext *context, Optional<Location> location, ValueRange operands,\n+    MLIRContext *context, std::optional<Location> location, ValueRange operands,\n     DictionaryAttr attributes, RegionRange regions,\n     SmallVectorImpl<Type> &inferredReturnTypes) {\n   // type is the same as the input\n@@ -284,7 +284,7 @@ mlir::LogicalResult mlir::triton::TransOp::inferReturnTypes(\n \n //-- DotOp --\n mlir::LogicalResult mlir::triton::DotOp::inferReturnTypes(\n-    MLIRContext *context, Optional<Location> location, ValueRange operands,\n+    MLIRContext *context, std::optional<Location> location, ValueRange operands,\n     DictionaryAttr attributes, RegionRange regions,\n     SmallVectorImpl<Type> &inferredReturnTypes) {\n   // type is the same as the accumulator\n@@ -309,7 +309,7 @@ mlir::LogicalResult mlir::triton::DotOp::inferReturnTypes(\n \n //-- ReduceOp --\n mlir::LogicalResult mlir::triton::ReduceOp::inferReturnTypes(\n-    MLIRContext *context, Optional<Location> location, ValueRange operands,\n+    MLIRContext *context, std::optional<Location> location, ValueRange operands,\n     DictionaryAttr attributes, RegionRange regions,\n     SmallVectorImpl<Type> &inferredReturnTypes) {\n   // infer shape\n@@ -371,7 +371,7 @@ OpFoldResult SplatOp::fold(FoldAdaptor adaptor) {\n \n //-- ExpandDimsOp --\n mlir::LogicalResult mlir::triton::ExpandDimsOp::inferReturnTypes(\n-    MLIRContext *context, Optional<Location> loc, ValueRange operands,\n+    MLIRContext *context, std::optional<Location> loc, ValueRange operands,\n     DictionaryAttr attributes, RegionRange regions,\n     SmallVectorImpl<Type> &inferredReturnTypes) {\n   // infer shape"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -897,7 +897,7 @@ struct TritonGPUInferLayoutInterface\n   LogicalResult\n   inferExpandDimsOpEncoding(Attribute operandEncoding, unsigned axis,\n                             Attribute &resultEncoding,\n-                            Optional<Location> location) const override {\n+                            std::optional<Location> location) const override {\n     auto sliceEncoding = operandEncoding.dyn_cast<SliceEncodingAttr>();\n     if (!sliceEncoding)\n       return emitOptionalError(\n@@ -909,9 +909,10 @@ struct TritonGPUInferLayoutInterface\n     return success();\n   }\n \n-  LogicalResult inferDotOpEncoding(Attribute operandEncoding, unsigned opIdx,\n-                                   Attribute retEncoding,\n-                                   Optional<Location> location) const override {\n+  LogicalResult\n+  inferDotOpEncoding(Attribute operandEncoding, unsigned opIdx,\n+                     Attribute retEncoding,\n+                     std::optional<Location> location) const override {\n     if (auto dotOpEnc = operandEncoding.dyn_cast<DotOperandEncodingAttr>()) {\n       if (opIdx != dotOpEnc.getOpIdx())\n         return emitOptionalError(location, \"Wrong opIdx\");"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -63,8 +63,8 @@ class Prefetcher {\n \n   Value generatePrefetch(Value v, unsigned opIdx, bool isPrologue,\n                          Attribute dotEncoding, OpBuilder &builder,\n-                         llvm::Optional<int64_t> offsetK = std::nullopt,\n-                         llvm::Optional<int64_t> shapeK = std::nullopt);\n+                         std::optional<int64_t> offsetK = std::nullopt,\n+                         std::optional<int64_t> shapeK = std::nullopt);\n \n public:\n   Prefetcher() = delete;\n@@ -82,8 +82,8 @@ class Prefetcher {\n \n Value Prefetcher::generatePrefetch(Value v, unsigned opIdx, bool isPrologue,\n                                    Attribute dotEncoding, OpBuilder &builder,\n-                                   llvm::Optional<int64_t> offsetK,\n-                                   llvm::Optional<int64_t> shapeK) {\n+                                   std::optional<int64_t> offsetK,\n+                                   std::optional<int64_t> shapeK) {\n   // opIdx: 0 => a, 1 => b\n   auto type = v.getType().cast<RankedTensorType>();\n   SmallVector<int64_t> shape{type.getShape().begin(), type.getShape().end()};"}, {"filename": "lib/Dialect/TritonGPU/Transforms/TritonGPUConversion.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -40,7 +40,7 @@ TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n   // This will create newArg, and map(origArg, newArg)\n   addArgumentMaterialization([&](OpBuilder &builder,\n                                  RankedTensorType tensorType, ValueRange inputs,\n-                                 Location loc) -> llvm::Optional<Value> {\n+                                 Location loc) -> std::optional<Value> {\n     llvm_unreachable(\"Argument rematerialization should not happen in Triton \"\n                      \"-> TritonGPU conversion\");\n     return std::nullopt;\n@@ -50,7 +50,7 @@ TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n   // convert origValue to newValue\n   addSourceMaterialization([&](OpBuilder &builder, RankedTensorType tensorType,\n                                ValueRange inputs,\n-                               Location loc) -> llvm::Optional<Value> {\n+                               Location loc) -> std::optional<Value> {\n     llvm_unreachable(\"Source rematerialization should not happen in Triton -> \"\n                      \"TritonGPU Conversion\");\n     return std::nullopt;\n@@ -63,7 +63,7 @@ TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n                                ValueRange inputs, Location loc) {\n     auto cast =\n         builder.create<triton::gpu::ConvertLayoutOp>(loc, tensorType, inputs);\n-    return Optional<Value>(cast.getResult());\n+    return std::optional<Value>(cast.getResult());\n   });\n }\n "}, {"filename": "python/MANIFEST.in", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1 +1,2 @@\n graft src\n+graft triton/third_party"}, {"filename": "python/setup.py", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -221,16 +221,18 @@ def build_extension(self, ext):\n     packages=[\n         \"triton\",\n         \"triton/_C\",\n-        \"triton/language\",\n-        \"triton/tools\",\n         \"triton/common\",\n+        \"triton/compiler\",\n+        \"triton/language\",\n         \"triton/ops\",\n+        \"triton/ops/blocksparse\",\n         \"triton/runtime\",\n-        \"triton/ops/blocksparse\"],\n+        \"triton/runtime/driver\",\n+        \"triton/tools\",\n+    ],\n     install_requires=[\n         \"filelock\",\n     ],\n-    package_data={\"triton\": [\"third_party/**/*\"]},\n     include_package_data=True,\n     ext_modules=[CMakeExtension(\"triton\", \"triton/_C/\")],\n     cmdclass={\"build_ext\": CMakeBuild},"}, {"filename": "python/triton/runtime/__init__.py", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -1,5 +1,6 @@\n from . import driver\n-from .autotuner import Config, Heuristics, OutOfResources, autotune, heuristics\n+from .autotuner import (Autotuner, Config, Heuristics, OutOfResources, autotune,\n+                        heuristics)\n from .jit import (JITFunction, KernelInterface, MockTensor, TensorWrapper, reinterpret,\n                   version_key)\n \n@@ -16,4 +17,5 @@\n     \"TensorWrapper\",\n     \"OutOfResources\",\n     \"MockTensor\",\n+    \"Autotuner\",\n ]"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 35, "deletions": 0, "changes": 35, "file_content_changes": "@@ -89,6 +89,41 @@ def do_bench(fn, warmup=25, rep=100, grad_to_none=None,\n         return torch.mean(times).item()\n \n \n+def assert_close(x, y, atol=None, rtol=None, err_msg=''):\n+    import numpy as np\n+    import torch\n+\n+    # absolute tolerance hook\n+    def default_atol(dtype):\n+        return 1e-2\n+    if atol is None:\n+        atol = default_atol\n+    atol = atol(x.dtype) if callable(atol) else atol\n+    # relative tolerance hook\n+\n+    def default_rtol(dtype):\n+        return 0.\n+    if rtol is None:\n+        rtol = default_rtol\n+    rtol = rtol(x.dtype) if callable(rtol) else rtol\n+    # we use numpy instead of pytorch\n+    # as it seems more memory efficient\n+    # pytorch tends to oom on large tensors\n+    if isinstance(x, torch.Tensor):\n+        if x.dtype == torch.bfloat16:\n+            x = x.float()\n+        x = x.cpu().detach().numpy()\n+    if isinstance(y, torch.Tensor):\n+        if y.dtype == torch.bfloat16:\n+            y = y.float()\n+        y = y.cpu().detach().numpy()\n+    if x.size > 1 or y.size > 1:\n+        np.testing.assert_allclose(x, y, atol=atol, rtol=rtol, equal_nan=True)\n+        return\n+    if not np.allclose(x, y, atol=atol, rtol=rtol):\n+        raise AssertionError(f'{err_msg} {x} is not close to {y} (atol={atol}, rtol={rtol})')\n+\n+\n class Benchmark:\n     \"\"\"\n     This class is used by the :code:`perf_report` function to generate line plots with a concise API."}]