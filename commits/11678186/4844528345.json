[{"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -642,19 +642,18 @@ struct ConvertLayoutOpConversion\n     auto loc = op.getLoc();\n     Value src = op.getSrc();\n     Value dst = op.getResult();\n-    bool isHMMA = supportMMA(dst, mmaLayout.getVersionMajor());\n \n     auto smemObj =\n         getSharedMemoryObjectFromStruct(loc, adaptor.getSrc(), rewriter);\n     Value res;\n \n-    if (!isOuter && mmaLayout.isAmpere() && isHMMA) { // tensor core v2\n+    if (!isOuter && mmaLayout.isAmpere()) { // tensor core v2\n \n       res = SharedToDotOperandMMAv2::convertLayout(\n           dotOperandLayout.getOpIdx(), rewriter, loc, src, dotOperandLayout,\n           smemObj, getTypeConverter(), tid_val());\n \n-    } else if (!isOuter && mmaLayout.isVolta() && isHMMA) { // tensor core v1\n+    } else if (!isOuter && mmaLayout.isVolta()) { // tensor core v1\n       bool isMMAv1Row = dotOperandLayout.getMMAv1IsRow();\n       auto srcSharedLayout = src.getType()\n                                  .cast<RankedTensorType>()"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -419,7 +419,7 @@ Type getShemPtrTy(Type argType) {\n     return ptr_ty(type::i16Ty(ctx), 3);\n   else if (argType.isF32())\n     return ptr_ty(type::f32Ty(ctx), 3);\n-  else if (argType.isInteger(8))\n+  else if (argType.getIntOrFloatBitWidth() == 8)\n     return ptr_ty(type::i8Ty(ctx), 3);\n   else\n     llvm::report_fatal_error(\"mma16816 data type not supported\");"}]