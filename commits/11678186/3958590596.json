[{"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 5, "deletions": 7, "changes": 12, "file_content_changes": "@@ -128,7 +128,7 @@ struct ConvertLayoutOpConversion\n         mmaColIdx[0] = add(mmaThreadIdInGrpM2, colWarpOffset);\n         mmaColIdx[1] = add(mmaThreadIdInGrpM2P1, colWarpOffset);\n       } else if (mmaLayout.isVolta()) {\n-        // Volta doesn't follow the pattern here.\n+        llvm_unreachable(\"Volta doesn't follow the pattern here.\");\n       } else {\n         llvm_unreachable(\"Unexpected MMALayout version\");\n       }\n@@ -379,17 +379,15 @@ struct ConvertLayoutOpConversion\n       isSrcMmaV1 = mmaLayout.isVolta();\n     }\n     if (auto sliceLayout = srcLayout.dyn_cast<SliceEncodingAttr>()) {\n-      isSrcMmaV1 =\n-          sliceLayout.getParent().isa<MmaEncodingAttr>() &&\n-          sliceLayout.getParent().dyn_cast<MmaEncodingAttr>().isVolta();\n+      isSrcMmaV1 = sliceLayout.getParent().isa<MmaEncodingAttr>() &&\n+                   sliceLayout.getParent().cast<MmaEncodingAttr>().isVolta();\n     }\n     if (auto mmaLayout = dstLayout.dyn_cast<MmaEncodingAttr>()) {\n       isDstMmaV1 = mmaLayout.isVolta();\n     }\n     if (auto sliceLayout = dstLayout.dyn_cast<SliceEncodingAttr>()) {\n-      isDstMmaV1 =\n-          sliceLayout.getParent().isa<MmaEncodingAttr>() &&\n-          sliceLayout.getParent().dyn_cast<MmaEncodingAttr>().isVolta();\n+      isDstMmaV1 = sliceLayout.getParent().isa<MmaEncodingAttr>() &&\n+                   sliceLayout.getParent().cast<MmaEncodingAttr>().isVolta();\n     }\n \n     for (unsigned d = 0; d < rank; ++d) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -74,8 +74,9 @@ struct BroadcastOpConversion\n       // NOTE: This is just an naive fix, but for MMA layout, and 2-d fix should\n       // be all right.\n       // TODO[Superjomn]: Replace this with a generic implementation.\n-      if (srcMma.isVolta() &&\n-          (srcTy.getElementType().isF16() || srcTy.getElementType().isBF16())) {\n+      if (srcMma.isVolta()) {\n+        assert(srcTy.getElementType().isF16() ||\n+               srcTy.getElementType().isBF16());\n         int numElemsPerThread = srcMma.getElemsPerThread(resultTy.getShape());\n         int srcUniqElems = srcVals.size() / 2;\n         int dup = numElemsPerThread / srcUniqElems;"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "@@ -172,7 +172,7 @@ SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout) {\n }\n \n SmallVector<unsigned> getShapePerCTA(const Attribute &layout,\n-                                     ArrayRef<int64_t> blockShape) {\n+                                     ArrayRef<int64_t> tensorShape) {\n   SmallVector<unsigned> shape;\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     for (unsigned d = 0, n = blockedLayout.getOrder().size(); d < n; ++d)\n@@ -185,19 +185,19 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout,\n     for (unsigned d = 0, n = getOrder(parent).size(); d < n; ++d) {\n       if (d == dim)\n         continue;\n-      shape.push_back(getShapePerCTA(parent, blockShape)[d]);\n+      shape.push_back(getShapePerCTA(parent, tensorShape)[d]);\n     }\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     if (mmaLayout.isAmpere())\n       return {16 * mmaLayout.getWarpsPerCTA()[0],\n               8 * mmaLayout.getWarpsPerCTA()[1]};\n     if (mmaLayout.isVolta()) {\n-      assert(!blockShape.empty() && \"Volta needs the blockShape\");\n-      if (blockShape.size() == 1) // must be SliceEncoding\n-        return {static_cast<unsigned>(blockShape[0]),\n-                static_cast<unsigned>(blockShape[0])};\n-      return {static_cast<unsigned>(blockShape[0]),\n-              static_cast<unsigned>(blockShape[1])};\n+      assert(!tensorShape.empty() && \"Volta needs the tensorShape\");\n+      if (tensorShape.size() == 1) // must be SliceEncoding\n+        return {static_cast<unsigned>(tensorShape[0]),\n+                static_cast<unsigned>(tensorShape[0])};\n+      return {static_cast<unsigned>(tensorShape[0]),\n+              static_cast<unsigned>(tensorShape[1])};\n     }\n     assert(0 && \"Unexpected MMA layout version found\");\n   } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n@@ -206,7 +206,7 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout,\n     if (auto parentMmaLayout = parentLayout.dyn_cast<MmaEncodingAttr>()) {\n       assert(parentMmaLayout.isAmpere() &&\n              \"mmaLayout version = 1 is not implemented yet\");\n-      auto parentShapePerCTA = getShapePerCTA(parentLayout, blockShape);\n+      auto parentShapePerCTA = getShapePerCTA(parentLayout, tensorShape);\n       auto opIdx = dotLayout.getOpIdx();\n       if (opIdx == 0) {\n         return {parentShapePerCTA[0], 16};"}, {"filename": "lib/Dialect/TritonGPU/Transforms/UpdateMmaForVolta.cpp", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -91,8 +91,6 @@ class CollectMmaToUpdateForVolta : public mlir::RewritePattern {\n     return failure();\n   }\n \n-  void collectDot(mlir::Operation *op) const {}\n-\n   // Get the wpt for MMAv1 using more information.\n   // Reference the original logic here\n   // https://github.com/openai/triton/blob/0e4691e6dd91e001a8d33b71badf8b3314325459/lib/codegen/analysis/layout.cc#L223"}]