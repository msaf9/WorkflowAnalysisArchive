[{"filename": "include/triton/ir/builder.h", "status": "modified", "additions": 20, "deletions": 11, "changes": 31, "file_content_changes": "@@ -38,10 +38,8 @@ class builder{\n   iterator get_insert_point() { return insert_point_;}\n   // Constants\n   value *get_int1(bool val);\n-  value *get_int32(int32_t val);\n-  value *get_int64(int64_t val);\n-  value *get_uint32(uint32_t val);\n-  value *get_uint64(uint64_t val);\n+  value *get_int32(uint32_t val);\n+  value *get_int64(uint64_t val);\n   value *get_float16(float val);\n   value *get_float32(float val);\n   value *get_range(int32_t lo, int32_t hi);\n@@ -52,11 +50,9 @@ class builder{\n   type *get_int16_ty();\n   type *get_int32_ty();\n   type *get_int64_ty();\n-  type *get_uint8_ty();\n-  type *get_uint16_ty();\n-  type *get_uint32_ty();\n-  type *get_uint64_ty();\n+  type *get_fp8_ty();\n   type *get_half_ty();\n+  type *get_bf16_ty();\n   type *get_float_ty();\n   type *get_double_ty();\n   // Insert\n@@ -74,7 +70,9 @@ class builder{\n   value* create_cond_br(value *cond, basic_block* if_dest, basic_block* else_dest);\n   value* create_ret_void();\n   // Cast instructions\n+  value* create_bitcast(value *src, type *dest_ty);\n   value *create_cast(cast_op_t op, value *v, type *dst_ty);\n+  value* create_int_to_ptr(value *src, type *dst_ty);\n   value* create_ptr_to_int(value *src, type *dst_ty);\n   value* create_si_to_fp(value *src, type *dst_ty);\n   value* create_ui_to_fp(value *src, type *dst_ty);\n@@ -93,11 +91,11 @@ class builder{\n   value *create_frem(value *lhs, value *rhs);\n   value *create_fadd(value *lhs, value *rhs);\n   value *create_fsub(value *lhs, value *rhs);\n-  value *create_mul(value *lhs, value *rhs, bool has_nuw = false, bool has_nsw = false);\n   value *create_sdiv(value *lhs, value *rhs);\n   value *create_udiv(value *lhs, value *rhs);\n   value *create_srem(value *lhs, value *rhs);\n   value *create_urem(value *lhs, value *rhs);\n+  value *create_mul(value *lhs, value *rhs, bool has_nuw = false, bool has_nsw = false);\n   value *create_add(value *lhs, value *rhs, bool has_nuw = false, bool has_nsw = false);\n   value *create_sub(value *lhs, value *rhs, bool has_nuw = false, bool has_nsw = false);\n   value *create_shl(value *lhs, value *rhs, bool has_nuw = false, bool has_nsw = false);\n@@ -145,11 +143,22 @@ class builder{\n   value *create_reshape(value *arg, const type::block_shapes_t &shapes);\n   value *create_cat(value *lhs, value *rhs);\n   value *create_broadcast(value *arg, const type::block_shapes_t &shapes);\n+  // Atomic instruction\n+  value *create_atomic_cas(value *ptr, value *cmp, value *val);\n+  value *create_atomic_rmw(atomic_rmw_op_t op, value *ptr, value *val, value *msk);\n+  value *create_atomic_max(value *ptr, value *val, value *msk);\n+  value *create_atomic_umax(value *ptr, value *val, value *msk);\n+  value *create_atomic_min(value *ptr, value *val, value *msk);\n+  value *create_atomic_umin(value *ptr, value *val, value *msk);\n+  value *create_atomic_fadd(value *ptr, value *val, value *msk);\n+  value *create_atomic_add(value *ptr, value *val, value *msk);\n+  value *create_atomic_and(value *ptr, value *val, value *msk);\n+  value *create_atomic_or(value *ptr, value *val, value *msk);\n+  value *create_atomic_xor(value *ptr, value *val, value *msk);\n+  value *create_atomic_xchg(value *ptr, value *val, value *msk);\n   // Built-in instruction\n   value *create_get_program_id(unsigned axis);\n   value *create_get_num_programs(unsigned axis);\n-  value *create_atomic_cas(value *ptr, value *cmp, value *val);\n-  value *create_atomic_rmw(ir::atomic_rmw_op_t op, value *ptr, value *val, value *msk);\n   value *create_exp(value* arg);\n   value *create_cos(value* arg);\n   value *create_sin(value* arg);"}, {"filename": "include/triton/ir/context_impl.h", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -26,7 +26,6 @@ class context_impl {\n   type fp8_ty, fp16_ty, bf16_ty, fp32_ty, fp64_ty;\n   // integer types\n   integer_type int1_ty, int8_ty, int16_ty, int32_ty, int64_ty, int128_ty;\n-  integer_type uint8_ty, uint16_ty, uint32_ty, uint64_ty;\n   // Pointer types\n   std::map<std::pair<type*, unsigned>, std::unique_ptr<pointer_type>> ptr_tys;\n   // Block types"}, {"filename": "include/triton/ir/dispatch.h", "status": "removed", "additions": 0, "deletions": 113, "changes": 113, "file_content_changes": "@@ -1,113 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_IR_DISPATCH_H_\n-#define _TRITON_IR_DISPATCH_H_\n-\n-#include \"triton/ir/builder.h\"\n-#include <stdexcept>\n-\n-namespace triton{\n-namespace ir{\n-\n-\n-/*----------------------------------------------\n- higher level functions that follow the likely\n- semantics of most expected frontends\n- ----------------------------------------------*/\n-\n-struct semantic_error: public std::runtime_error {\n-  semantic_error(const std::string& msg):\n-    std::runtime_error(msg) { }\n-};\n-\n-struct dispatch{\n-  typedef ir::type::block_shapes_t shape_t;\n-\n-\n-  // programming model\n-  static ir::value *program_id(int axis, ir::builder *builder);\n-  static ir::value *num_programs(int axis, ir::builder *builder);\n-\n-  // binary operators\n-  static ir::value *add(ir::value *input, ir::value *other, ir::builder *builder);\n-  static ir::value *sub(ir::value *input, ir::value *other, ir::builder *builder);\n-  static ir::value *mul(ir::value *input, ir::value *other, ir::builder *builder);\n-  static ir::value *truediv(ir::value *input, ir::value *other, ir::builder *builder);\n-  static ir::value *floordiv(ir::value *input, ir::value *other, ir::builder *builder);\n-  static ir::value *fdiv(ir::value *input, ir::value *other, ir::constant_int* ieee_rounding, ir::builder *builder);\n-  static ir::value *mod(ir::value *input, ir::value *other, ir::builder *builder);\n-  static ir::value *and_(ir::value *input, ir::value *other, ir::builder *builder);\n-  static ir::value *or_(ir::value *input, ir::value *other, ir::builder *builder);\n-  static ir::value *xor_(ir::value *input, ir::value *other, ir::builder *builder);\n-  static ir::value *lshr(ir::value *input, ir::value *other, ir::builder *builder);\n-  static ir::value *shl(ir::value *input, ir::value *other, ir::builder *builder);\n-\n-  // unary operators\n-  static ir::value *plus(ir::value *input, ir::builder *builder);\n-  static ir::value *minus(ir::value *input, ir::builder *builder);\n-  static ir::value *invert(ir::value *input, ir::builder *builder);\n-\n-  // comparison operators\n-  static ir::value *greater_than(ir::value *input, ir::value *other, ir::builder *builder);\n-  static ir::value *greater_equal(ir::value *input, ir::value *other, ir::builder *builder);\n-  static ir::value *less_than(ir::value *input, ir::value *other, ir::builder *builder);\n-  static ir::value *less_equal(ir::value *input, ir::value *other, ir::builder *builder);\n-  static ir::value *equal(ir::value *input, ir::value *other, ir::builder *builder);\n-  static ir::value *not_equal(ir::value *input, ir::value *other, ir::builder *builder);\n-\n-  // block creation\n-  static ir::value* arange(int start, int end, ir::builder *builder);\n-  static ir::value* zeros(shape_t shape, ir::type *dtype, ir::builder *builder);\n-\n-\n-  // casting ops\n-  static ir::value *reshape(ir::value *input, shape_t shape, ir::builder *builder);\n-  static ir::value *cat(ir::value *lhs, ir::value *rhs, ir::builder *builder);\n-  static ir::value *broadcast(ir::value *input, shape_t shape, ir::builder *builder);\n-  static std::tuple<ir::value*, ir::value*> broadcast(ir::value *lhs, ir::value* rhs, ir::builder *builder);\n-  static ir::value *bitcast(ir::value *input, ir::type *type, ir::builder *builder);\n-  static ir::value *cast(ir::value *input, ir::type *type, ir::builder *builder);\n-\n-  // memory operators\n-  static ir::value *load(ir::value* ptr, ir::value* mask, ir::value* other, const std::string &cache,\n-                         const std::string& eviction_policy, int is_volatile, ir::builder *builder);\n-  static ir::value *store(ir::value* ptr, ir::value *value, ir::value *mask, ir::builder *builder);\n-  static ir::value *atomic_cas(ir::value* ptr, ir::value *cmp, ir::value *val, ir::builder *builder);\n-  static ir::value *atomic_add(ir::value* ptr, ir::value *val, ir::value *msk, ir::builder *builder);\n-  static ir::value *atomic_max(ir::value* ptr, ir::value *val, ir::value *msk, ir::builder *builder);\n-  static ir::value *atomic_min(ir::value* ptr, ir::value *val, ir::value *msk, ir::builder *builder);\n-  static ir::value *atomic_and(ir::value* ptr, ir::value *val, ir::value *msk, ir::builder *builder);\n-  static ir::value *atomic_or(ir::value* ptr, ir::value *val, ir::value *msk, ir::builder *builder);\n-  static ir::value *atomic_xor(ir::value* ptr, ir::value *val, ir::value *msk, ir::builder *builder);\n-  static ir::value *atomic_xchg(ir::value* ptr, ir::value *val, ir::value *msk, ir::builder *builder);\n-\n-  // linear algebra\n-  static ir::value *dot(ir::value *lhs, ir::value *rhs, ir::constant_int *allow_tf32, ir::builder *builder);\n-\n-  // indexing\n-  static ir::value *where(ir::value* condition, ir::value *x, ir::value *y, ir::builder *builder);\n-\n-  // reduction\n-  static ir::value *min(ir::value *input, unsigned int axis, ir::builder *builder);\n-  static ir::value *max(ir::value *input, unsigned int axis, ir::builder *builder);\n-  static ir::value *sum(ir::value *input, unsigned int axis, ir::builder *builder);\n-  static ir::value *xor_sum(ir::value *input, unsigned axis, ir::builder *builder);\n-\n-  // math\n-  static ir::value *umulhi(ir::value *x, ir::value *y, ir::builder *builder);\n-  static ir::value *exp(ir::value *x, ir::builder *builder);\n-  static ir::value *log(ir::value *x, ir::builder *builder);\n-  static ir::value *cos(ir::value *x, ir::builder *builder);\n-  static ir::value *sin(ir::value *x, ir::builder *builder);\n-  static ir::value *sqrt(ir::value *x, ir::builder *builder);\n-\n-  // internal (debug/optimization)\n-  static ir::value *multiple_of(ir::value *x, int value, ir::builder *builder);\n-  static ir::value *max_contiguous(ir::value *x, int value, ir::builder *builder);\n-  static ir::value *debug_barrier(ir::builder *builder);\n-};\n-\n-}\n-}\n-\n-#endif"}, {"filename": "include/triton/ir/module.h", "status": "modified", "additions": 6, "deletions": 29, "changes": 35, "file_content_changes": "@@ -57,26 +57,10 @@ class module {\n   void push_function(function *fn) { functions_.push_back(fn); }\n \n public:\n-  module(const std::string &name, builder& builder);\n-  builder& get_builder();\n-  // Setters\n-  void set_value(const std::string& name, basic_block* block, value *x);\n-  void set_value(const std::string& name, value* x);\n-  void set_const(const std::string& name);\n-  void set_continue_fn(std::function<ir::value*()> fn);\n-  // Getters\n-  const std::map<val_key_t, value*>& get_values() { return values_; }\n-  const std::map<std::string, type*>& get_types() { return types_; }\n-  void set_values(const std::map<val_key_t, value*>& values) { values_ = values; }\n-  void set_types(const std::map<std::string, type*>& types) { types_ = types; }\n-\n-  value *get_value(const std::string& name, basic_block* block);\n-  value *get_value(const std::string& name);\n-  void set_type(const std::string& name, ir::type* ty) { types_[name] = ty; }\n-  const std::string& get_name();\n-  std::function<ir::value*()> get_continue_fn();\n-  // Seal block -- no more predecessors will be added\n-  void seal_block(basic_block *block);\n+  module(const std::string &name, builder &builder): name_(name), builder_(builder) {}\n+  builder &get_builder() { return builder_; };\n+  const std::string& get_name() { return name_; };\n+\n   // Functions\n   const functions_list_t &get_function_list() const { return functions_; }\n   functions_list_t &get_function_list()             { return functions_; }\n@@ -89,21 +73,14 @@ class module {\n   const std::map<std::string, ir::value*>& globals() const    { return globals_; }\n   // Metadata\n   void add_metadata(const std::string &name, md_pair_t x)     { metadatas_[name] = x; }\n-\n+  const std::map<std::string, md_pair_t> &get_metadatas() const { return metadatas_; }\n   void print(std::ostream &os);\n \n private:\n   std::string name_;\n-  builder& builder_;\n-  std::map<val_key_t, value*> values_;\n-  std::map<std::string, type*> types_;\n-  std::set<std::string> const_;\n-  std::set<basic_block*> sealed_blocks_;\n-  std::map<basic_block*, std::map<std::string, phi_node*>> incomplete_phis_;\n+  builder &builder_;\n   functions_list_t functions_;\n   symbols_map_t symbols_;\n-  std::function<ir::value*()> continue_fn_;\n-  std::map<value*, value**> current_phi_;\n   std::vector<ir::alloc_const*> allocs_;\n   std::map<std::string, ir::value*> globals_;\n   std::map<std::string, md_pair_t> metadatas_;"}, {"filename": "include/triton/ir/type.h", "status": "modified", "additions": 3, "deletions": 17, "changes": 20, "file_content_changes": "@@ -16,8 +16,6 @@ class value;\n class integer_type;\n class constant_int;\n \n-enum class signedness { SIGNED, UNSIGNED };\n-\n /* Type */\n class type {\n public:\n@@ -61,8 +59,6 @@ class type {\n   // type attributes\n   unsigned get_fp_mantissa_width() const;\n   unsigned get_integer_bitwidth() const;\n-  signedness get_integer_signedness() const;\n-  bool is_integer_signed() const;\n   unsigned get_tile_bitwidth() const;\n   unsigned get_primitive_size_in_bits() const;\n   type *get_scalar_ty() const;\n@@ -85,9 +81,6 @@ class type {\n   bool is_metadata_ty() const           { return id_ == MetadataTyID; }\n   bool is_token_ty() const              { return id_ == TokenTyID; }\n   bool is_integer_ty() const            { return id_ == IntegerTyID; }\n-  bool is_integer_ty(unsigned bitwidth, signedness sn) {\n-    return is_integer_ty() && get_integer_bitwidth() == bitwidth && get_integer_signedness() == sn;\n-  }\n   bool is_bool_ty() const               { return is_integer_ty(1); }\n   bool is_pointer_ty() const            { return id_ == PointerTyID; }\n   bool is_block_ty() const               { return id_ == BlockTyID; }\n@@ -115,10 +108,6 @@ class type {\n   static integer_type *get_int32_ty(context &ctx);\n   static integer_type *get_int64_ty(context &ctx);\n   static integer_type *get_int128_ty(context &ctx);\n-  static integer_type *get_uint8_ty(context &ctx);\n-  static integer_type *get_uint16_ty(context &ctx);\n-  static integer_type *get_uint32_ty(context &ctx);\n-  static integer_type *get_uint64_ty(context &ctx);\n \n   // repr\n   std::string tile_repr() const {\n@@ -145,7 +134,7 @@ class type {\n       case LabelTyID: return \"label\";\n       case MetadataTyID: return \"md\";\n       case TokenTyID: return \"tok\";\n-      case IntegerTyID: return (is_integer_signed() ? \"i\" : \"u\") + std::to_string(get_integer_bitwidth());\n+      case IntegerTyID: return (\"i\") + std::to_string(get_integer_bitwidth());\n       case FunctionTyID: return \"fn\";\n       case PointerTyID: return get_pointer_element_ty()->repr() + \"*\";\n       case StructTyID: return \"struct\";\n@@ -168,21 +157,18 @@ class integer_type: public type {\n \n private:\n   // constructors\n-  integer_type(context &ctx, unsigned bitwidth, signedness sn)\n-    : type(ctx, IntegerTyID), bitwidth_(bitwidth), signedness_(sn){ }\n+  integer_type(context &ctx, unsigned bitwidth)\n+    : type(ctx, IntegerTyID), bitwidth_(bitwidth) {}\n \n public:\n   // accessors\n   unsigned get_bitwidth() const { return bitwidth_; }\n \n-  signedness get_signedness() const { return signedness_; }\n-\n   // factory methods\n   static integer_type* get(context &ctx, unsigned width);\n \n private:\n   unsigned bitwidth_;\n-  signedness signedness_;\n };\n \n class composite_type: public type{"}, {"filename": "lib/ir/builder.cc", "status": "modified", "additions": 31, "deletions": 22, "changes": 53, "file_content_changes": "@@ -48,18 +48,12 @@ void builder::set_insert_point(basic_block *block){\n value *builder::get_int1(bool val)\n { return constant_int::get(type::get_int1_ty(ctx_), val); }\n \n-value *builder::get_int32(int32_t val)\n+value *builder::get_int32(uint32_t val)\n { return constant_int::get(type::get_int32_ty(ctx_), val);}\n \n-value *builder::get_uint32(uint32_t val)\n-{ return constant_int::get(type::get_uint32_ty(ctx_), val);}\n-\n-value *builder::get_int64(int64_t val)\n+value *builder::get_int64(uint64_t val)\n { return constant_int::get(type::get_int64_ty(ctx_), val);}\n \n-value *builder::get_uint64(uint64_t val)\n-{ return constant_int::get(type::get_uint64_ty(ctx_), val);}\n-\n value *builder::get_float16(float val)\n { return constant_fp::get(type::get_fp16_ty(ctx_), val); }\n \n@@ -90,21 +84,15 @@ type *builder::get_int32_ty()\n type *builder::get_int64_ty()\n { return type::get_int64_ty(ctx_); }\n \n-type *builder::get_uint8_ty()\n-{ return type::get_uint8_ty(ctx_); }\n-\n-type *builder::get_uint16_ty()\n-{ return type::get_uint16_ty(ctx_); }\n-\n-type *builder::get_uint32_ty()\n-{ return type::get_uint32_ty(ctx_); }\n-\n-type *builder::get_uint64_ty()\n-{ return type::get_uint64_ty(ctx_); }\n+type *builder::get_fp8_ty()\n+{ return type::get_fp8_ty(ctx_); }\n \n type *builder::get_half_ty()\n { return type::get_fp16_ty(ctx_); }\n \n+type *builder::get_bf16_ty()\n+{ return type::get_bf16_ty(ctx_); }\n+\n type *builder::get_float_ty()\n { return type::get_fp32_ty(ctx_); }\n \n@@ -139,6 +127,8 @@ value *builder::create_ret_void() {\n     return create_cast(OPCODE, src, dst_ty);\\\n   }\n \n+DEFINE_CAST_INSTR(bitcast, cast_op_t::BitCast)\n+DEFINE_CAST_INSTR(int_to_ptr, cast_op_t::IntToPtr)\n DEFINE_CAST_INSTR(ptr_to_int, cast_op_t::PtrToInt)\n DEFINE_CAST_INSTR(si_to_fp, cast_op_t::SIToFP)\n DEFINE_CAST_INSTR(ui_to_fp, cast_op_t::UIToFP)\n@@ -331,6 +321,28 @@ value *builder::create_downcast(value *arg) {\n   return insert(downcast_inst::create(arg));\n }\n \n+//\n+\n+value *builder::create_atomic_rmw(ir::atomic_rmw_op_t op, value *ptr, value *val, value *msk){\n+  return insert(atomic_rmw_inst::create(op, ptr, val, msk));\n+}\n+\n+#define DEFINE_ATOMIC_RMW_INSTR(SUFFIX, OPCODE)\\\n+  value *builder::create_ ## SUFFIX(value *ptr, value *val, value *mask){\\\n+    return create_atomic_rmw(OPCODE, ptr, val, mask);\\\n+  }\n+\n+DEFINE_ATOMIC_RMW_INSTR(atomic_max, ir::atomic_rmw_op_t::Max)\n+DEFINE_ATOMIC_RMW_INSTR(atomic_umax, ir::atomic_rmw_op_t::UMax)\n+DEFINE_ATOMIC_RMW_INSTR(atomic_min, ir::atomic_rmw_op_t::Min)\n+DEFINE_ATOMIC_RMW_INSTR(atomic_umin, ir::atomic_rmw_op_t::UMin)\n+DEFINE_ATOMIC_RMW_INSTR(atomic_fadd, ir::atomic_rmw_op_t::FAdd)\n+DEFINE_ATOMIC_RMW_INSTR(atomic_add, ir::atomic_rmw_op_t::Add)\n+DEFINE_ATOMIC_RMW_INSTR(atomic_and, ir::atomic_rmw_op_t::And)\n+DEFINE_ATOMIC_RMW_INSTR(atomic_or, ir::atomic_rmw_op_t::Or)\n+DEFINE_ATOMIC_RMW_INSTR(atomic_xor, ir::atomic_rmw_op_t::Xor)\n+DEFINE_ATOMIC_RMW_INSTR(atomic_xchg, ir::atomic_rmw_op_t::Xchg)\n+\n //===----------------------------------------------------------------------===//\n //                               built-in instructions\n //===----------------------------------------------------------------------===//\n@@ -347,9 +359,6 @@ value *builder::create_atomic_cas(value *ptr, value *cmp, value *val){\n   return insert(atomic_cas_inst::create(ptr, cmp, val));\n }\n \n-value *builder::create_atomic_rmw(ir::atomic_rmw_op_t op, value *ptr, value *val, value *msk){\n-  return insert(atomic_rmw_inst::create(op, ptr, val, msk));\n-}\n \n value *builder::create_exp(value *arg){\n   return insert(exp_inst::create(arg));"}, {"filename": "lib/ir/context.cc", "status": "modified", "additions": 6, "deletions": 12, "changes": 18, "file_content_changes": "@@ -19,18 +19,12 @@ context_impl::context_impl(context &ctx)\n       fp32_ty(ctx, type::FP32TyID),\n       fp64_ty(ctx, type::FP64TyID),\n       // integers\n-      int1_ty(ctx, 1, signedness::SIGNED),\n-      int8_ty(ctx, 8, signedness::SIGNED),\n-      int16_ty(ctx, 16, signedness::SIGNED),\n-      int32_ty(ctx, 32, signedness::SIGNED),\n-      int64_ty(ctx, 64, signedness::SIGNED),\n-      int128_ty(ctx, 128, signedness::SIGNED),\n-      uint8_ty(ctx, 8, signedness::UNSIGNED),\n-      uint16_ty(ctx, 16, signedness::UNSIGNED),\n-      uint32_ty(ctx, 32, signedness::UNSIGNED),\n-      uint64_ty(ctx, 64, signedness::UNSIGNED){\n-\n-}\n+      int1_ty(ctx, 1),\n+      int8_ty(ctx, 8),\n+      int16_ty(ctx, 16),\n+      int32_ty(ctx, 32),\n+      int64_ty(ctx, 64),\n+      int128_ty(ctx, 128) {}\n \n //===----------------------------------------------------------------------===//\n //                                    context"}, {"filename": "lib/ir/dispatch.cc", "status": "removed", "additions": 0, "deletions": 882, "changes": 882, "file_content_changes": "@@ -1,882 +0,0 @@\n-#include \"triton/ir/dispatch.h\"\n-\n-namespace triton {\n-namespace ir {\n-\n-\n-[[ noreturn ]] void throw_unreachable(std::string key) {\n-  throw std::runtime_error(\"Encountered unimplemented code path in `\" + key + \"`. \"\n-                           \"This is likely a bug on our side.\");\n-}\n-\n-//===----------------------------------------------------------------------===//\n-//                              Programming Model\n-//===----------------------------------------------------------------------===//\n-\n-ir::value *dispatch::program_id(int axis, ir::builder *builder) {\n-  return builder->create_get_program_id(axis);\n-}\n-\n-ir::value *dispatch::num_programs(int axis, ir::builder *builder) {\n-  return builder->create_get_num_programs(axis);\n-}\n-\n-//===----------------------------------------------------------------------===//\n-//                               Implicit Casting Utilities\n-//===----------------------------------------------------------------------===//\n-\n-ir::type *integer_promote(ir::type* a_ty, ir::type* b_ty){\n-  int a_rank = a_ty->get_integer_bitwidth();\n-  int b_rank = b_ty->get_integer_bitwidth();\n-  auto a_sn = a_ty->get_integer_signedness();\n-  auto b_sn = b_ty->get_integer_signedness();\n-  // Rules for signedness taken from \"Usual arithmetic conversions\" on\n-  // https://en.cppreference.com/w/c/language/conversion.\n-  if (a_sn == b_sn) {\n-    return a_rank > b_rank ? a_ty : b_ty;\n-  } else if (a_sn == signedness::UNSIGNED) {\n-    return a_rank >= b_rank ? a_ty : b_ty;\n-  } else if (b_sn == signedness::UNSIGNED) {\n-    return b_rank >= a_rank ? b_ty : a_ty;\n-  } else {\n-    throw_unreachable(\"integer_promote\");\n-  }\n-}\n-\n-enum class DivOrMod { NO, YES };\n-\n-ir::type *computation_type(ir::type* a_ty, ir::type* b_ty, DivOrMod div_or_mod) {\n-  context &ctx = a_ty->get_context();\n-  // 1) if one operand is double, the other is implicitly\n-  //    converted to double\n-  if (a_ty->is_fp64_ty() || b_ty->is_fp64_ty())\n-    return type::get_fp64_ty(ctx);\n-  // 2) if one operand is float, the other is implicitly\n-  //    converted to float\n-  if (a_ty->is_fp32_ty() || b_ty->is_fp32_ty())\n-    return type::get_fp32_ty(ctx);\n-  // 3 ) if one operand is half, the other is implicitly converted to half\n-  //     unless we're doing / or %, which do not exist natively in PTX for fp16.\n-  if (a_ty->is_fp16_ty() || b_ty->is_fp16_ty()) {\n-    if (div_or_mod == DivOrMod::YES) {\n-      return type::get_fp32_ty(ctx);\n-    } else {\n-      return type::get_fp16_ty(ctx);\n-    }\n-  }\n-  if (!a_ty->is_integer_ty() || !b_ty->is_integer_ty())\n-    throw_unreachable(\"computation_type\");\n-  // 4 ) both operands are integer and undergo\n-  //    integer promotion\n-  if (div_or_mod == DivOrMod::YES && a_ty->get_integer_signedness() != b_ty->get_integer_signedness()) {\n-    throw semantic_error(\"Cannot use /, //, or % with \" + a_ty->repr() + \" and \" + b_ty->repr() + \" because they have different signedness; this is unlikely to result in a useful answer. Cast them to the same signedness.\");\n-  }\n-  return integer_promote(a_ty, b_ty);\n-}\n-\n-//===----------------------------------------------------------------------===//\n-//                               Binary Operators\n-//===----------------------------------------------------------------------===//\n-\n-void throw_incompatible_types(ir::type* type_a, ir::type* type_b) {\n-  throw semantic_error(\"invalid operands of type \" + type_a->repr() + \" and \" + type_b->repr());\n-}\n-\n-void check_ptr_type(ir::type* type_a, ir::type* type_b, bool allow_ptr_a){\n-\n-  if(type_a->is_pointer_ty()){\n-    if(!allow_ptr_a)\n-      throw_incompatible_types(type_a, type_b);\n-    // T* + U* with T != U\n-    if(type_b->is_pointer_ty() && (type_a != type_b))\n-      throw_incompatible_types(type_a, type_b);\n-    // T* + float\n-    if(type_b->is_floating_point_ty())\n-      throw_incompatible_types(type_a, type_b);\n-  }\n-}\n-\n-void binary_op_type_checking(ir::value*& lhs, ir::value*& rhs, ir::builder* builder,\n-                             bool allow_lhs_ptr = false, bool allow_rhs_ptr = false,\n-                             bool arithmetic_check = true, DivOrMod div_or_mod = DivOrMod::NO) {\n-  // implicit broadcasting\n-  std::tie(lhs, rhs) = dispatch::broadcast(lhs, rhs, builder);\n-  // implicit typecasting\n-  ir::type *lhs_sca_ty = lhs->get_type()->get_scalar_ty();\n-  ir::type *rhs_sca_ty = rhs->get_type()->get_scalar_ty();\n-  check_ptr_type(lhs_sca_ty, rhs_sca_ty, allow_lhs_ptr);\n-  check_ptr_type(rhs_sca_ty, lhs_sca_ty, allow_rhs_ptr);\n-  if (arithmetic_check && !lhs_sca_ty->is_pointer_ty() && !rhs_sca_ty->is_pointer_ty()) {\n-    ir::type *ret_sca_ty = computation_type(lhs_sca_ty, rhs_sca_ty, div_or_mod);\n-    lhs = dispatch::cast(lhs, ret_sca_ty, builder);\n-    rhs = dispatch::cast(rhs, ret_sca_ty, builder);\n-  }\n-}\n-\n-ir::value *dispatch::add(ir::value *input, ir::value *other, ir::builder *builder) {\n-  binary_op_type_checking(input, other, builder, true, true);\n-  ir::type *input_scalar_ty = input->get_type()->get_scalar_ty();\n-  ir::type *other_scalar_ty = other->get_type()->get_scalar_ty();\n-  // offset + ptr\n-  // ptr + offset\n-  if(other_scalar_ty->is_pointer_ty() && !input_scalar_ty->is_pointer_ty())\n-    std::swap(input, other);\n-  if (input_scalar_ty->is_pointer_ty())\n-    return builder->create_gep(input, {other});\n-  // float + float\n-  else if (input_scalar_ty->is_floating_point_ty())\n-    return builder->create_fadd(input, other);\n-  // int + int\n-  else if (input_scalar_ty->is_integer_ty())\n-    return builder->create_add(input, other);\n-  throw_unreachable(\"add\");\n-}\n-\n-ir::value *dispatch::sub(ir::value *input, ir::value *other, ir::builder *builder) {\n-  binary_op_type_checking(input, other, builder, true, false);\n-  ir::type *scalar_ty = input->get_type()->get_scalar_ty();\n-  // ptr - offset\n-  if (scalar_ty->is_pointer_ty())\n-    return builder->create_gep(input, {dispatch::minus(other, builder)});\n-  // float + float\n-  if (scalar_ty->is_floating_point_ty())\n-    return builder->create_fsub(input, other);\n-  // int + int\n-  else if (scalar_ty->is_integer_ty())\n-    return builder->create_sub(input, other);\n-  throw_unreachable(\"sub\");\n-}\n-\n-ir::value *dispatch::mul(ir::value *input, ir::value *other, ir::builder *builder) {\n-  binary_op_type_checking(input, other, builder);\n-  ir::type *scalar_ty = input->get_type()->get_scalar_ty();\n-  // float * float\n-  if (scalar_ty->is_floating_point_ty())\n-    return builder->create_fmul(input, other);\n-  // int * int\n-  else if (scalar_ty->is_integer_ty())\n-    return builder->create_mul(input, other);\n-  throw_unreachable(\"mul\");\n-}\n-\n-ir::value *dispatch::truediv(ir::value *input, ir::value *other, ir::builder *builder) {\n-  binary_op_type_checking(input, other, builder, false, false, true, DivOrMod::YES);\n-  ir::type *input_scalar_ty = input->get_type()->get_scalar_ty();\n-  ir::type *other_scalar_ty = other->get_type()->get_scalar_ty();\n-  // float / int\n-  if(input_scalar_ty->is_floating_point_ty() && other_scalar_ty->is_integer_ty())\n-    other = cast(other, input_scalar_ty, builder);\n-  // int / float\n-  else if(input_scalar_ty->is_integer_ty() && other_scalar_ty->is_floating_point_ty())\n-    input = cast(input, other_scalar_ty, builder);\n-  // int / int (cast to float32)\n-  else if(input_scalar_ty->is_integer_ty() && other_scalar_ty->is_integer_ty()){\n-    input = cast(input, builder->get_float_ty(), builder);\n-    other = cast(other, builder->get_float_ty(), builder);\n-  }\n-  // float / float (cast to highest exponent type)\n-  else if(input_scalar_ty->is_floating_point_ty() && other_scalar_ty->is_floating_point_ty()){\n-    if(input_scalar_ty->get_fp_mantissa_width() > other_scalar_ty->get_fp_mantissa_width())\n-      other = cast(other, input_scalar_ty, builder);\n-    else\n-      input = cast(input, other_scalar_ty, builder);\n-  }\n-  // unreachable\n-  else\n-    throw_unreachable(\"div\");\n-  return builder->create_fdiv(input, other);\n-}\n-\n-ir::value *dispatch::floordiv(ir::value *input, ir::value *other, ir::builder *builder){\n-  binary_op_type_checking(input, other, builder, false, false, true, DivOrMod::YES);\n-  ir::type *input_scalar_ty = input->get_type()->get_scalar_ty();\n-  ir::type *other_scalar_ty = other->get_type()->get_scalar_ty();\n-  if(input_scalar_ty->is_integer_ty() && other_scalar_ty->is_integer_ty()){\n-    ir::type *ret_ty = integer_promote(input_scalar_ty, other_scalar_ty);\n-    input = dispatch::cast(input, ret_ty, builder);\n-    other = dispatch::cast(other, ret_ty, builder);\n-    if (ret_ty->is_integer_signed()) {\n-      return builder->create_sdiv(input, other);\n-    } else {\n-      return builder->create_udiv(input, other);\n-    }\n-  }\n-  throw_unreachable(\"floordiv\");\n-}\n-\n-ir::value *dispatch::fdiv(ir::value *input, ir::value *other, constant_int *ieee_rounding, ir::builder *builder){\n-  ir::type *input_scalar_ty = input->get_type()->get_scalar_ty();\n-  ir::type *other_scalar_ty = other->get_type()->get_scalar_ty();\n-  if(!input_scalar_ty->is_floating_point_ty() || !other_scalar_ty->is_floating_point_ty())\n-    throw semantic_error(\"both operands of fdiv must have floating point scalar type\");\n-  binary_op_type_checking(input, other, builder, false, false, false, DivOrMod::YES);\n-  ir::value* ret = builder->create_fdiv(input, other);\n-  if(ir::binary_operator* binop = dynamic_cast<ir::binary_operator*>(ret))\n-    binop->set_fdiv_ieee_rounding(ieee_rounding->get_value());\n-  return ret;\n-}\n-\n-ir::value *dispatch::mod(ir::value *input, ir::value *other, ir::builder *builder) {\n-  binary_op_type_checking(input, other, builder, false, false, true, DivOrMod::YES);\n-  ir::type *scalar_ty = input->get_type()->get_scalar_ty();\n-  ir::type *other_scalar_ty = other->get_type()->get_scalar_ty();\n-  // float % int\n-  if (scalar_ty->is_floating_point_ty())\n-    return builder->create_frem(input, other);\n-  // int % int\n-  else if (scalar_ty->is_integer_ty()) {\n-    if (scalar_ty->get_integer_signedness() != other_scalar_ty->get_integer_signedness()) {\n-      throw semantic_error(\"Cannot mod \" + scalar_ty->repr() + \" by \" + other_scalar_ty->repr() + \" because they have different signedness; this is unlikely to result in a useful answer. Cast them to the same signedness.\");\n-    }\n-    if (scalar_ty->is_integer_signed()) {\n-      return builder->create_srem(input, other);\n-    } else {\n-      return builder->create_urem(input, other);\n-    }\n-  }\n-  throw_unreachable(\"mod\");\n-}\n-\n-\n-void bitwise_op_type_checking(ir::value *&input, ir::value *&other, ir::builder *builder) {\n-  binary_op_type_checking(input, other, builder, false, false, false);\n-  ir::type *input_sca_ty = input->get_type()->get_scalar_ty();\n-  ir::type *other_sca_ty = other->get_type()->get_scalar_ty();\n-  if(!input_sca_ty->is_integer_ty() || !other_sca_ty->is_integer_ty())\n-    throw_incompatible_types(input_sca_ty, other_sca_ty);\n-  ir::type *ret_sca_ty = integer_promote(input_sca_ty, other_sca_ty);\n-  if (ret_sca_ty != input_sca_ty)\n-    input = dispatch::cast(input, ret_sca_ty, builder);\n-  if (ret_sca_ty != other_sca_ty)\n-    other = dispatch::cast(other, ret_sca_ty, builder);\n-}\n-\n-ir::value *dispatch::and_(ir::value *input, ir::value *other, ir::builder *builder) {\n-  bitwise_op_type_checking(input, other, builder);\n-  return builder->create_and(input, other);\n-}\n-\n-ir::value *dispatch::or_(ir::value *input, ir::value *other, ir::builder *builder) {\n-  bitwise_op_type_checking(input, other, builder);\n-  return builder->create_or(input, other);\n-}\n-\n-\n-ir::value *dispatch::xor_(ir::value *input, ir::value *other, ir::builder *builder) {\n-  bitwise_op_type_checking(input, other, builder);\n-  return builder->create_xor(input, other);\n-}\n-\n-\n-ir::value *dispatch::lshr(ir::value *input, ir::value *other, ir::builder *builder) {\n-  bitwise_op_type_checking(input, other, builder);\n-  return builder->create_lshr(input, other);\n-}\n-\n-\n-ir::value *dispatch::shl(ir::value *input, ir::value *other, ir::builder *builder) {\n-  bitwise_op_type_checking(input, other, builder);\n-  return builder->create_shl(input, other);\n-}\n-\n-//===----------------------------------------------------------------------===//\n-//                               Unary Operators\n-//===----------------------------------------------------------------------===//\n-\n-ir::value *dispatch::plus(ir::value *input, ir::builder *) {\n-  return input;\n-}\n-\n-ir::value *dispatch::minus(ir::value *input, ir::builder *builder) {\n-  ir::type* input_sca_ty = input->get_type()->get_scalar_ty();\n-  if(input_sca_ty->is_pointer_ty())\n-    throw semantic_error(\"wrong type argument to unary minus (\" + input_sca_ty->repr() + \")\");\n-  ir::value *_0 = ir::constant::get_null_value(input_sca_ty);\n-  return dispatch::sub(_0, input, builder);\n-}\n-\n-ir::value *dispatch::invert(ir::value *input, ir::builder *builder) {\n-  ir::type* input_sca_ty = input->get_type()->get_scalar_ty();\n-  if(input_sca_ty->is_pointer_ty() || input_sca_ty->is_floating_point_ty())\n-    throw semantic_error(\"wrong type argument to unary invert (\" + input_sca_ty->repr() + \")\");\n-  ir::value *_1 = ir::constant::get_all_ones_value(input_sca_ty);\n-  return dispatch::xor_(input, _1, builder);\n-}\n-\n-\n-//===----------------------------------------------------------------------===//\n-//                               Comparison Operators\n-//===----------------------------------------------------------------------===//\n-\n-ir::value *dispatch::greater_than(ir::value *input, ir::value *other, ir::builder *builder) {\n-  binary_op_type_checking(input, other, builder);\n-  ir::type *scalar_ty = input->get_type()->get_scalar_ty();\n-  // float > float\n-  if (scalar_ty->is_floating_point_ty())\n-    return builder->create_fcmpOGT(input, other);\n-  // int > int\n-  else if (scalar_ty->is_integer_ty()) {\n-    if (scalar_ty->is_integer_signed()) {\n-      return builder->create_icmpSGT(input, other);\n-    } else {\n-      return builder->create_icmpUGT(input, other);\n-    }\n-  }\n-  throw_unreachable(\"greater_than\");\n-}\n-\n-ir::value *dispatch::greater_equal(ir::value *input, ir::value *other, ir::builder *builder) {\n-  binary_op_type_checking(input, other, builder);\n-  ir::type *scalar_ty = input->get_type()->get_scalar_ty();\n-  // float >= float\n-  if (scalar_ty->is_floating_point_ty())\n-    return builder->create_fcmpOGE(input, other);\n-  // int >= int\n-  else if (scalar_ty->is_integer_ty()) {\n-    if (scalar_ty->is_integer_signed()) {\n-      return builder->create_icmpSGE(input, other);\n-    } else {\n-      return builder->create_icmpUGE(input, other);\n-    }\n-  }\n-  throw_unreachable(\"greater_equal\");\n-}\n-\n-ir::value *dispatch::less_than(ir::value *input, ir::value *other, ir::builder *builder) {\n-  binary_op_type_checking(input, other, builder);\n-  ir::type *scalar_ty = input->get_type()->get_scalar_ty();\n-  // float < float\n-  if (scalar_ty->is_floating_point_ty())\n-    return builder->create_fcmpOLT(input, other);\n-  // int < int\n-  else if (scalar_ty->is_integer_ty()) {\n-    if (scalar_ty->is_integer_signed()) {\n-      return builder->create_icmpSLT(input, other);\n-    } else {\n-      return builder->create_icmpULT(input, other);\n-    }\n-  }\n-  throw_unreachable(\"less_than\");\n-}\n-\n-ir::value *dispatch::less_equal(ir::value *input, ir::value *other, ir::builder *builder) {\n-  binary_op_type_checking(input, other, builder);\n-  ir::type *scalar_ty = input->get_type()->get_scalar_ty();\n-  // float < float\n-  if (scalar_ty->is_floating_point_ty())\n-    return builder->create_fcmpOLE(input, other);\n-  // int < int\n-  else if (scalar_ty->is_integer_ty()) {\n-    if (scalar_ty->is_integer_signed()) {\n-      return builder->create_icmpSLE(input, other);\n-    } else {\n-      return builder->create_icmpULE(input, other);\n-    }\n-  }\n-  throw_unreachable(\"less_equal\");\n-}\n-\n-ir::value *dispatch::equal(ir::value *input, ir::value *other, ir::builder *builder) {\n-  binary_op_type_checking(input, other, builder);\n-  ir::type *scalar_ty = input->get_type()->get_scalar_ty();\n-  // float == float\n-  if (scalar_ty->is_floating_point_ty())\n-    return builder->create_fcmpOEQ(input, other);\n-  // int == int\n-  else if (scalar_ty->is_integer_ty())\n-    return builder->create_icmpEQ(input, other);\n-  throw_unreachable(\"equal\");\n-}\n-\n-ir::value *dispatch::not_equal(ir::value *input, ir::value *other, ir::builder *builder) {\n-  binary_op_type_checking(input, other, builder);\n-  ir::type *scalar_ty = input->get_type()->get_scalar_ty();\n-  // float == float\n-  if (scalar_ty->is_floating_point_ty())\n-    return builder->create_fcmpUNE(input, other);\n-  // int == int\n-  else if (scalar_ty->is_integer_ty())\n-    return builder->create_icmpNE(input, other);\n-  throw_unreachable(\"equal\");\n-}\n-\n-//===----------------------------------------------------------------------===//\n-//                               Block Creation\n-//===----------------------------------------------------------------------===//\n-\n-ir::value* dispatch::arange(int start, int end, ir::builder *builder) {\n-  return builder->get_range(start, end);\n-}\n-\n-ir::value* dispatch::zeros(shape_t shape, ir::type *dtype, ir::builder *builder) {\n-  ir::value *_0 = ir::constant::get_null_value(dtype);\n-  return builder->create_splat(_0, shape);\n-}\n-\n-//===----------------------------------------------------------------------===//\n-//                               Shape Manipulation\n-//===----------------------------------------------------------------------===//\n-\n-\n-ir::value *dispatch::reshape(ir::value *input, shape_t dst_shape, ir::builder *builder) {\n-  unsigned numel = 1;\n-  for(unsigned s: dst_shape) numel *= s;\n-  if(input->get_type()->get_tile_num_elements() != numel)\n-    throw semantic_error(\"cannot reshape block of different shape\");\n-  return builder->create_reshape(input, dst_shape);\n-}\n-\n-ir::value *dispatch::cat(ir::value *lhs, ir::value *rhs, ir::builder *builder) {\n-  return builder->create_cat(lhs, rhs);\n-}\n-\n-ir::value *dispatch::broadcast(ir::value *input, shape_t shape, ir::builder *builder) {\n-  if (!input->get_type()->is_block_ty())\n-    return builder->create_splat(input, shape);\n-  auto src_shape = input->get_type()->get_block_shapes();\n-  if (src_shape.size() != shape.size())\n-    throw std::runtime_error(\"Cannot broadcast\");\n-  if(shape == src_shape)\n-    return input;\n-  return builder->create_broadcast(input, shape);\n-}\n-\n-std::tuple<ir::value*, ir::value*> dispatch::broadcast(ir::value *lhs, ir::value* rhs, ir::builder *builder) {\n-  ir::type *lhs_ty = lhs->get_type();\n-  ir::type *rhs_ty = rhs->get_type();\n-\n-  // make_shape_compatible(block, scalar)\n-  if (lhs_ty->is_block_ty() && !rhs_ty->is_block_ty())\n-    rhs = builder->create_splat(rhs, lhs_ty->get_block_shapes());\n-  // make_shape_compatible(scalar, block)\n-  else if (!lhs_ty->is_block_ty() && rhs_ty->is_block_ty())\n-    lhs = builder->create_splat(lhs, rhs_ty->get_block_shapes());\n-  // make_shape_compatible(block, block)\n-  else if (lhs_ty->is_block_ty() && rhs_ty->is_block_ty()) {\n-    auto lhs_shape = lhs_ty->get_block_shapes();\n-    auto rhs_shape = rhs_ty->get_block_shapes();\n-    if (lhs_shape.size() != rhs_shape.size())\n-      throw std::runtime_error(\"Cannot make_shape_compatible: blocks must have the same rank\");\n-    ir::type::block_shapes_t ret_shape;\n-    for (size_t i = 0; i < lhs_shape.size(); ++i) {\n-      unsigned left = lhs_shape[i];\n-      unsigned right = rhs_shape[i];\n-      if (left == 1)\n-        ret_shape.push_back(right);\n-      else if (right == 1)\n-        ret_shape.push_back(left);\n-      else if (left == right)\n-        ret_shape.push_back(left);\n-      else\n-        throw std::runtime_error(\"Cannot make_shape_compatible: incompatible dimensions at index \" + std::to_string(i) +\n-                                 \": \" + std::to_string(left) + \" and \" + std::to_string(right));\n-    }\n-    if (lhs_shape != ret_shape)\n-      lhs = builder->create_broadcast(lhs, ret_shape);\n-    if (rhs_shape != ret_shape)\n-      rhs = builder->create_broadcast(rhs, ret_shape);\n-  }\n-  return std::make_tuple(lhs, rhs);\n-}\n-\n-ir::value *dispatch::bitcast(ir::value *input, ir::type *dst_ty, ir::builder *builder){\n-  ir::type *src_ty = input->get_type();\n-  if (src_ty->is_block_ty())\n-    dst_ty = ir::block_type::get(dst_ty, input->get_type()->get_block_shapes());\n-  if(src_ty == dst_ty)\n-    return input;\n-  ir::type *src_sca_ty = src_ty->get_scalar_ty();\n-  ir::type *dst_sca_ty = dst_ty->get_scalar_ty();\n-  if(src_sca_ty->is_pointer_ty() || dst_sca_ty->is_pointer_ty())\n-    return cast(input, dst_ty, builder);\n-  // Bitcast\n-  int src_bits = src_sca_ty->get_primitive_size_in_bits();\n-  int dst_bits = dst_sca_ty->get_primitive_size_in_bits();\n-  if( src_bits!= dst_bits)\n-    throw std::runtime_error(\"Cannot bitcast data-type of size \" + std::to_string(src_bits) +\n-                             \"to data-type of size \" + std::to_string(dst_bits));\n-  return builder->create_cast(ir::BitCast, input, dst_ty);\n-}\n-\n-ir::value *dispatch::cast(ir::value *input, ir::type *dst_ty, ir::builder *builder) {\n-  ir::type *src_ty = input->get_type();\n-  if (src_ty->is_block_ty())\n-    dst_ty = ir::block_type::get(dst_ty, input->get_type()->get_block_shapes());\n-  if(src_ty == dst_ty)\n-    return input;\n-  ir::type *src_sca_ty = src_ty->get_scalar_ty();\n-  ir::type *dst_sca_ty = dst_ty->get_scalar_ty();\n-  //\n-  if((src_sca_ty->is_bf16_ty() && !dst_sca_ty->is_fp32_ty()) ||\n-     (dst_sca_ty->is_bf16_ty() && !src_sca_ty->is_fp32_ty())){\n-    return cast(cast(input, builder->get_float_ty(), builder), dst_sca_ty, builder);\n-  }\n-  // FP Truncation\n-  bool truncate_fp = src_sca_ty->is_floating_point_ty() &&\n-                     dst_sca_ty->is_floating_point_ty() &&\n-                     src_sca_ty->get_fp_mantissa_width() > dst_sca_ty->get_fp_mantissa_width();\n-  if (truncate_fp)\n-    return builder->create_fp_trunc(input, dst_ty);\n-  // FP Extension\n-  bool ext_fp = src_sca_ty->is_floating_point_ty() &&\n-                dst_sca_ty->is_floating_point_ty() &&\n-                src_sca_ty->get_fp_mantissa_width() < dst_sca_ty->get_fp_mantissa_width();\n-  if (ext_fp)\n-    return builder->create_fp_ext(input, dst_ty);\n-  // Int cast\n-  if (src_sca_ty->is_integer_ty() && dst_sca_ty->is_integer_ty() &&\n-      (src_sca_ty->get_integer_bitwidth() != dst_sca_ty->get_integer_bitwidth() ||\n-       src_sca_ty->get_integer_signedness() != dst_sca_ty->get_integer_signedness())) {\n-    bool sign_extend = src_sca_ty->is_integer_signed() && src_sca_ty != builder->get_int1_ty();\n-    return builder->create_int_cast(input, dst_ty, sign_extend);\n-  }\n-  // Float -> Int\n-  if (src_sca_ty->is_floating_point_ty() && dst_sca_ty->is_integer_ty()){\n-    if(dst_sca_ty->is_bool_ty())\n-      return builder->create_fp_to_ui(input, dst_ty);\n-    else\n-      return builder->create_fp_to_si(input, dst_ty);\n-  }\n-  // int -> Float\n-  if (src_sca_ty->is_integer_ty() && dst_sca_ty->is_floating_point_ty()){\n-    if (src_sca_ty->is_bool_ty() || !src_sca_ty->is_integer_signed())\n-      return builder->create_ui_to_fp(input, dst_ty);\n-    else\n-      return builder->create_si_to_fp(input, dst_ty);\n-  }\n-  if (src_sca_ty->is_pointer_ty() && dst_sca_ty->is_integer_ty()){\n-    int bitwidth = dst_sca_ty->get_integer_bitwidth();\n-    if(bitwidth == 64)\n-      return builder->create_cast(ir::PtrToInt, input, dst_ty);\n-    if(bitwidth == 1)\n-      return dispatch::not_equal(dispatch::cast(input, builder->get_int64_ty(), builder),\n-                                 builder->get_int64(0),\n-                                 builder);\n-  }\n-  if (!src_sca_ty->is_pointer_ty() && dst_sca_ty->is_pointer_ty())\n-    return builder->create_cast(ir::IntToPtr, input, dst_ty);\n-  // Ptr -> Ptr\n-  if (src_sca_ty->is_pointer_ty() && dst_sca_ty->is_pointer_ty())\n-    return builder->create_cast(ir::BitCast, input, dst_ty);\n-  // * -> Bool\n-  if (dst_sca_ty->is_bool_ty()) {\n-    if (src_sca_ty->is_pointer_ty())\n-      input = cast(input, builder->get_int64_ty(), builder);\n-    ir::value *other = builder->get_int64(0);\n-    if (src_ty->is_bool_ty())\n-      other = builder->create_splat(other, src_ty->get_block_shapes());\n-    return builder->create_icmpNE(input, other);\n-  }\n-  throw_unreachable(\"casting from \" + src_sca_ty->repr() + \" to \" + dst_sca_ty->repr());\n-}\n-\n-//===----------------------------------------------------------------------===//\n-//                               Memory Operators\n-//===----------------------------------------------------------------------===//\n-\n-ir::value *dispatch::load(ir::value* ptr, ir::value* mask, ir::value* other, const std::string &cache_modifier, const std::string& eviction_policy, int is_volatile, ir::builder* builder) {\n-  if(!ptr->get_type()->get_scalar_ty()->is_pointer_ty())\n-    throw semantic_error(\"Pointer argument of load instruction is \" + ptr->get_type()->repr());\n-  if(ptr->get_type()->is_block_ty()){\n-    if(mask)\n-      mask = dispatch::broadcast(mask, ptr->get_type()->get_block_shapes(), builder);\n-    if(other)\n-      other = dispatch::broadcast(other, ptr->get_type()->get_block_shapes(), builder);\n-  }\n-  if(other)\n-    other = dispatch::cast(other, ptr->get_type()->get_scalar_ty()->get_pointer_element_ty(), builder);\n-  ir::type *ptr_ty = ptr->get_type()->get_scalar_ty();\n-  ir::type *elt_ty = ptr_ty->get_pointer_element_ty();\n-  // treat bool* as int8*\n-  if(elt_ty == builder->get_int1_ty()){\n-    elt_ty = builder->get_int8_ty();\n-    ptr_ty = pointer_type::get(elt_ty, ptr_ty->get_pointer_address_space());\n-    ptr = dispatch::cast(ptr, ptr_ty, builder);\n-  }\n-  // cache modifier\n-  load_inst::CACHE_MODIFIER cache = load_inst::NONE; // default\n-  if (!cache_modifier.empty()) {\n-    if (cache_modifier == \".ca\")\n-      cache = load_inst::CA;\n-    else if (cache_modifier == \".cg\")\n-      cache = load_inst::CG;\n-    else\n-      throw std::runtime_error(std::string(\"Cache modifier \") + cache_modifier + \" not supported\");\n-  }\n-  // eviction policy\n-  load_inst::EVICTION_POLICY eviction = load_inst::NORMAL; //default\n-  if(!eviction_policy.empty()){\n-    if (eviction_policy == \"evict_last\")\n-        eviction = load_inst::EVICT_LAST;\n-    else if(eviction_policy == \"evict_first\")\n-        eviction = load_inst::EVICT_FIRST;\n-    else\n-        throw std::runtime_error(std::string(\"Eviction policy\") + eviction_policy + \" not supported\");\n-  }\n-\n-\n-  if (!mask && !other)\n-    return builder->create_load(ptr, cache, eviction, is_volatile);\n-  if (!mask)\n-    throw std::runtime_error(\"`other` cannot be provided without `mask`\");\n-  auto shape = ptr->get_type()->get_block_shapes();\n-  if(!other){\n-    other = ir::undef_value::get(elt_ty);\n-    if(ptr->get_type()->is_block_ty())\n-      other = builder->create_splat(other, ptr->get_type()->get_block_shapes());\n-  }\n-  return builder->create_masked_load(ptr, mask, other, cache, eviction, is_volatile);\n-}\n-\n-ir::value *dispatch::store(ir::value* ptr, ir::value *val, ir::value* mask, ir::builder *builder) {\n-  if(!ptr->get_type()->get_scalar_ty()->is_pointer_ty())\n-    throw semantic_error(\"Pointer argument of store instruction is \" + ptr->get_type()->repr());\n-  if(ptr->get_type()->is_block_ty())\n-    val = dispatch::broadcast(val, ptr->get_type()->get_block_shapes(), builder);\n-  if(mask)\n-    mask = dispatch::broadcast(mask, ptr->get_type()->get_block_shapes(), builder);\n-  ir::type *ptr_ty = ptr->get_type()->get_scalar_ty();\n-  ir::type *elt_ty = ptr_ty->get_pointer_element_ty();\n-  // treat bool* as int8*\n-  if(elt_ty == builder->get_int1_ty()){\n-    elt_ty = builder->get_int8_ty();\n-    ptr_ty = pointer_type::get(elt_ty, ptr_ty->get_pointer_address_space());\n-    ptr = dispatch::cast(ptr, ptr_ty, builder);\n-  }\n-  // cast to target data-type\n-  val = dispatch::cast(val, elt_ty, builder);\n-  if (!mask)\n-    return builder->create_store(ptr, val);\n-  if(!mask->get_type()->get_scalar_ty()->is_bool_ty())\n-    throw semantic_error(\"Mask must have boolean scalar type\");\n-  return builder->create_masked_store(ptr, val, mask);\n-}\n-\n-ir::value *dispatch::atomic_cas(ir::value* ptr, ir::value *cmp, ir::value *val, ir::builder *builder){\n-  return builder->create_atomic_cas(ptr, cmp, val);\n-}\n-\n-void atom_red_typechecking(ir::value*& ptr, ir::value *&val, ir::value *&mask, ir::builder *builder){\n-  if(!ptr->get_type()->get_scalar_ty()->is_pointer_ty())\n-    throw semantic_error(\"Pointer argument of store instruction is \" + ptr->get_type()->repr());\n-  if(ptr->get_type()->is_block_ty()){\n-    if(mask){\n-      mask = dispatch::broadcast(mask, ptr->get_type()->get_block_shapes(), builder);\n-    }\n-    if(val){\n-      val = dispatch::broadcast(val, ptr->get_type()->get_block_shapes(), builder);\n-    }\n-  }\n-  val = dispatch::cast(val, ptr->get_type()->get_scalar_ty()->get_pointer_element_ty(), builder);\n-  if(!mask){\n-    mask = builder->get_int1(true);\n-    if(ptr->get_type()->is_block_ty())\n-      mask = builder->create_splat(mask, ptr->get_type()->get_block_shapes());\n-  }\n-}\n-\n-ir::value *dispatch::atomic_max(ir::value* ptr, ir::value *val, ir::value *mask, ir::builder *builder){\n-  atom_red_typechecking(ptr, val, mask, builder);\n-  ir::type* sca_ty = val->get_type()->get_scalar_ty();\n-  // direct call to atomic_max for integers\n-  if(sca_ty->is_integer_ty()) {\n-    if (sca_ty->is_integer_signed()) {\n-      return builder->create_atomic_rmw(ir::atomic_rmw_op_t::Max, ptr, val, mask);\n-    } else {\n-      return builder->create_atomic_rmw(ir::atomic_rmw_op_t::UMax, ptr, val, mask);\n-    }\n-  }\n-  // for float\n-  // return atomic_smax(i_ptr, i_val) if val >= 0\n-  // return atomic_umin(i_ptr, i_val) if val < 0\n-  ir::value* i_val = bitcast(val, builder->get_int32_ty(), builder);\n-  ir::value* i_ptr = bitcast(ptr, pointer_type::get(builder->get_int32_ty(), 1), builder);\n-  ir::value* pos = greater_equal(val, constant_fp::get(sca_ty, 0), builder);\n-  ir::value* neg = less_than(val, constant_fp::get(sca_ty, 0), builder);\n-  ir::value* pos_ret = builder->create_atomic_rmw(ir::atomic_rmw_op_t::Max, i_ptr, i_val, and_(mask, pos, builder));\n-  ir::value* neg_ret = builder->create_atomic_rmw(ir::atomic_rmw_op_t::UMin, i_ptr, i_val, and_(mask, neg, builder));\n-  return where(pos, pos_ret, neg_ret, builder);\n-}\n-\n-ir::value *dispatch::atomic_min(ir::value* ptr, ir::value *val, ir::value *mask, ir::builder *builder){\n-  atom_red_typechecking(ptr, val, mask, builder);\n-  ir::type* sca_ty = val->get_type()->get_scalar_ty();\n-  // direct call to atomic_min for integers\n-  if(sca_ty->is_integer_ty()) {\n-    if (sca_ty->is_integer_signed()) {\n-      return builder->create_atomic_rmw(ir::atomic_rmw_op_t::Min, ptr, val, mask);\n-    } else {\n-      return builder->create_atomic_rmw(ir::atomic_rmw_op_t::UMin, ptr, val, mask);\n-    }\n-  }\n-  // for float\n-  // return atomic_smin(i_ptr, i_val) if val >= 0\n-  // return atomic_umax(i_ptr, i_val) if val < 0\n-  ir::value* i_val = bitcast(val, builder->get_int32_ty(), builder);\n-  ir::value* i_ptr = bitcast(ptr, pointer_type::get(builder->get_int32_ty(), 1), builder);\n-  ir::value* pos = greater_equal(val, constant_fp::get(sca_ty, 0), builder);\n-  ir::value* neg = less_than(val, constant_fp::get(sca_ty, 0), builder);\n-  ir::value* pos_ret = builder->create_atomic_rmw(ir::atomic_rmw_op_t::Min, i_ptr, i_val, and_(mask, pos, builder));\n-  ir::value* neg_ret = builder->create_atomic_rmw(ir::atomic_rmw_op_t::UMax, i_ptr, i_val, and_(mask, neg, builder));\n-  return where(pos, pos_ret, neg_ret, builder);\n-}\n-\n-ir::value *dispatch::atomic_add(ir::value* ptr, ir::value *val, ir::value *mask, ir::builder *builder){\n-  atom_red_typechecking(ptr, val, mask, builder);\n-  ir::type* sca_ty = val->get_type()->get_scalar_ty();\n-  auto op = sca_ty->is_floating_point_ty() ? ir::atomic_rmw_op_t::FAdd : ir::atomic_rmw_op_t::Add;\n-  return builder->create_atomic_rmw(op, ptr, val, mask);\n-}\n-\n-ir::value *dispatch::atomic_and(ir::value* ptr, ir::value *val, ir::value *mask, ir::builder *builder){\n-  atom_red_typechecking(ptr, val, mask, builder);\n-  return builder->create_atomic_rmw(ir::atomic_rmw_op_t::And, ptr, val, mask);\n-}\n-\n-ir::value *dispatch::atomic_or(ir::value* ptr, ir::value *val, ir::value *mask, ir::builder *builder){\n-  atom_red_typechecking(ptr, val, mask, builder);\n-  return builder->create_atomic_rmw(ir::atomic_rmw_op_t::Or, ptr, val, mask);\n-}\n-\n-ir::value *dispatch::atomic_xor(ir::value* ptr, ir::value *val, ir::value *mask, ir::builder *builder){\n-  atom_red_typechecking(ptr, val, mask, builder);\n-  return builder->create_atomic_rmw(ir::atomic_rmw_op_t::Xor, ptr, val, mask);\n-}\n-\n-ir::value *dispatch::atomic_xchg(ir::value* ptr, ir::value *val, ir::value *mask, ir::builder *builder){\n-  atom_red_typechecking(ptr, val, mask, builder);\n-  ir::type* sca_ty = val->get_type()->get_scalar_ty();\n-  return builder->create_atomic_rmw(ir::atomic_rmw_op_t::Xchg, ptr, val, mask);\n-}\n-\n-//===----------------------------------------------------------------------===//\n-//                               Linear Algebra\n-//===----------------------------------------------------------------------===//\n-\n-ir::value *dispatch::dot(ir::value *lhs, ir::value *rhs, ir::constant_int *allow_tf32, ir::builder *builder) {\n-  ir::value *_0 = nullptr;\n-  if (lhs->get_type()->is_int_or_tileint_ty())\n-    _0 = builder->get_int32(0);\n-  else\n-    _0 = builder->get_float32(0);\n-  unsigned M = lhs->get_type()->get_block_shapes()[0];\n-  unsigned N = rhs->get_type()->get_block_shapes()[1];\n-  _0 = builder->create_splat(_0, {M, N});\n-  bool _allow_tf32 = allow_tf32->get_value() != 0;\n-  return builder->create_dot(lhs, rhs, _0, _allow_tf32);\n-}\n-\n-\n-//===----------------------------------------------------------------------===//\n-//                               Indexing\n-//===----------------------------------------------------------------------===//\n-\n-ir::value *dispatch::where(ir::value* condition, ir::value *x, ir::value *y, ir::builder *builder){\n-  condition = dispatch::cast(condition, builder->get_int1_ty(), builder);\n-  if(condition->get_type()->is_block_ty()){\n-    x = dispatch::broadcast(x, condition->get_type()->get_block_shapes(), builder);\n-    y = dispatch::broadcast(y, condition->get_type()->get_block_shapes(), builder);\n-  }\n-  ir::type* x_ty = x->get_type()->get_scalar_ty();\n-  ir::type* y_ty = y->get_type()->get_scalar_ty();\n-  ir::type* ty = computation_type(x_ty, y_ty, DivOrMod::NO);\n-  x = dispatch::cast(x, ty, builder);\n-  y = dispatch::cast(y, ty, builder);\n-  return builder->create_select(condition, x, y);\n-}\n-\n-\n-//===----------------------------------------------------------------------===//\n-//                               Reductions\n-//===----------------------------------------------------------------------===//\n-\n-ir::value *reduce_impl(ir::value *input, unsigned int axis, ir::builder *builder, const std::string &name,\n-                       ir::reduce_inst::op_t FLOAT_OP, ir::reduce_inst::op_t INT_OP) {\n-  ir::type *scalar_ty = input->get_type()->get_scalar_ty();\n-  // input is extended to 32-bits if necessary\n-  // this increases numerical accuracy and can be done pretty much for free\n-  // on GPUs\n-  if(scalar_ty->is_integer_ty() && scalar_ty->get_integer_bitwidth() <= 32)\n-    input = dispatch::cast(input, type::get_int32_ty(scalar_ty->get_context()), builder);\n-  if (scalar_ty->is_floating_point_ty())\n-    return builder->create_reduce(input, FLOAT_OP, axis);\n-  else if (scalar_ty->is_integer_ty())\n-    return builder->create_reduce(input, INT_OP, axis);\n-  throw_unreachable(name);\n-}\n-\n-ir::value *dispatch::min(ir::value *input, unsigned int axis, ir::builder *builder) {\n-  return reduce_impl(input, axis, builder, \"min\", ir::reduce_inst::FMIN, ir::reduce_inst::MIN);\n-}\n-\n-ir::value *dispatch::max(ir::value *input, unsigned int axis, ir::builder *builder) {\n-  return reduce_impl(input, axis, builder, \"max\", ir::reduce_inst::FMAX, ir::reduce_inst::MAX);\n-}\n-\n-ir::value *dispatch::sum(ir::value *input, unsigned int axis, ir::builder *builder) {\n-  return reduce_impl(input, axis, builder, \"sum\", ir::reduce_inst::FADD, ir::reduce_inst::ADD);\n-}\n-\n-ir::value *dispatch::xor_sum(ir::value *input, unsigned int axis, ir::builder *builder) {\n-  ir::type *scalar_ty = input->get_type()->get_scalar_ty();\n-  if (!scalar_ty->is_integer_ty())\n-    throw semantic_error(\"xor_sum only supported for integers\");\n-  return reduce_impl(input, axis, builder, \"sum\", ir::reduce_inst::XOR, ir::reduce_inst::XOR);\n-}\n-\n-\n-//===----------------------------------------------------------------------===//\n-//                               Math\n-//===----------------------------------------------------------------------===//\n-\n-ir::value *dispatch::umulhi(ir::value *x, ir::value* y, ir::builder *builder) {\n-  binary_op_type_checking(x, y, builder);\n-  return builder->insert(umulhi_inst::create(x, y));\n-}\n-\n-ir::value *dispatch::exp(ir::value *x, ir::builder *builder) {\n-  return builder->create_exp(x);\n-}\n-\n-ir::value *dispatch::log(ir::value *x, ir::builder *builder) {\n-  return builder->create_log(x);\n-}\n-\n-ir::value *dispatch::cos(ir::value *x, ir::builder *builder) {\n-  return builder->create_cos(x);\n-}\n-\n-ir::value *dispatch::sin(ir::value *x, ir::builder *builder) {\n-  return builder->create_sin(x);\n-}\n-\n-ir::value *dispatch::sqrt(ir::value *x, ir::builder *builder) {\n-  return builder->create_sqrt(x);\n-}\n-\n-\n-//\n-\n-ir::value *dispatch::multiple_of(ir::value *x, int value, ir::builder *){\n-  ir::instruction* i = dynamic_cast<ir::instruction*>(x);\n-  if(!i)\n-    throw_unreachable(\"multiple_of\");\n-  i->set_metadata(ir::metadata::multiple_of, value);\n-  return i;\n-}\n-\n-ir::value *dispatch::max_contiguous(ir::value *x, int value, ir::builder *){\n-  ir::instruction* i = dynamic_cast<ir::instruction*>(x);\n-  if(!i)\n-    throw_unreachable(\"max_contiguous\");\n-  i->set_metadata(ir::metadata::max_contiguous, value);\n-  return i;\n-}\n-\n-ir::value *dispatch::debug_barrier(ir::builder *builder) {\n-  return builder->create_barrier();\n-}\n-\n-\n-}\n-}"}, {"filename": "lib/ir/instructions.cc", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -312,8 +312,8 @@ cast_inst *cast_inst::create_integer_cast(value *arg, type *ty, bool is_signed,\n   unsigned arg_bits = arg_ty->get_scalar_ty()->get_integer_bitwidth();\n   unsigned dst_bits = ty->get_scalar_ty()->get_integer_bitwidth();\n   cast_op_t op = (arg_bits == dst_bits ? cast_op_t::BitCast :\n-            (arg_bits > dst_bits  ? cast_op_t::Trunc :\n-            (is_signed            ? cast_op_t::SExt : cast_op_t::ZExt)));\n+                   (arg_bits > dst_bits  ? cast_op_t::Trunc :\n+                   (is_signed            ? cast_op_t::SExt : cast_op_t::ZExt)));\n   return create(op, arg, ty, name, next);\n }\n "}, {"filename": "lib/ir/module.cc", "status": "modified", "additions": 0, "deletions": 140, "changes": 140, "file_content_changes": "@@ -9,146 +9,6 @@\n namespace triton{\n namespace ir{\n \n-/* Module */\n-module::module(const std::string &name, builder &builder)\n-  : name_(name), builder_(builder) {\n-  sealed_blocks_.insert(nullptr);\n-}\n-\n-ir::builder& module::get_builder() {\n-  return builder_;\n-}\n-\n-void module::set_value(const std::string& name, ir::basic_block *block, ir::value *value){\n-  values_[val_key_t{name, block}] = value;\n-  auto it = metadatas_.find(name);\n-  if(auto *x = dynamic_cast<ir::instruction*>(value))\n-  if(it != metadatas_.end()){\n-    x->set_metadata(it->second.first, it->second.second);\n-  }\n-//  value->set_name(name);\n-}\n-\n-void module::set_value(const std::string& name, ir::value *value){\n-  return set_value(name, builder_.get_insert_block(), value);\n-}\n-\n-void module::set_const(const std::string& name){\n-  const_.insert(name);\n-}\n-\n-void module::set_continue_fn(std::function<ir::value*()> fn) {\n-  continue_fn_ = fn;\n-}\n-\n-std::function<ir::value*()> module::get_continue_fn() {\n-  return continue_fn_;\n-}\n-\n-ir::phi_node* module::make_phi(ir::type *ty, unsigned num_values, ir::basic_block *block){\n-  basic_block::iterator insert = block->get_first_non_phi();\n-  if(insert != block->end()){\n-    builder_.set_insert_point(insert);\n-  }\n-  ir::phi_node *res = builder_.create_phi(ty, num_values);\n-  if(insert != block->end())\n-    builder_.set_insert_point(block);\n-  return res;\n-}\n-\n-ir::value *module::try_remove_trivial_phis(ir::phi_node *&phi){\n-  // find non-self references\n-  std::set<ir::value*> non_self_ref;\n-  std::copy_if(phi->ops().begin(), phi->ops().end(), std::inserter(non_self_ref, non_self_ref.begin()),\n-               [phi](ir::value* op){ return  op != phi && op; });\n-  // non-trivial\n-  if(non_self_ref.size() != 1)\n-    return phi;\n-  // unique value or self-reference\n-  ir::value *same = *non_self_ref.begin();\n-  assert(same != nullptr);\n-  phi->replace_all_uses_with(same);\n-  phi->erase_from_parent();\n-  std::set<ir::user*> users = phi->get_users();\n-  for(ir::user* u: users)\n-  if(auto *uphi = dynamic_cast<ir::phi_node*>(u))\n-    if(uphi != phi)\n-      try_remove_trivial_phis(uphi);\n-  return same;\n-}\n-\n-\n-ir::value *module::add_phi_operands(const std::string& name, ir::phi_node *&phi){\n-  // already initialized\n-  if(phi->get_num_operands())\n-    return phi;\n-  ir::basic_block *block = phi->get_parent();\n-  for(ir::basic_block *pred: block->get_predecessors()){\n-    ir::value *value = get_value(name, pred);\n-    phi->add_incoming(value, pred);\n-  }\n-  return phi;\n-}\n-\n-ir::value *module::get_value_recursive(const std::string& name, ir::basic_block *block) {\n-  ir::value *result;\n-  bool is_const = const_.find(name) != const_.end();\n-  auto &preds = block->get_predecessors();\n-  ir::type *ty = types_.at(name);\n-  if(block && !is_const && sealed_blocks_.find(block) == sealed_blocks_.end()){\n-    incomplete_phis_[block][name] = make_phi(ty, 1, block);\n-    result = (ir::value*)incomplete_phis_[block][name];\n-  }\n-  else if(preds.size() <= 1){\n-    bool has_pred = preds.size();\n-    result = get_value(name, has_pred?preds.front():nullptr);\n-  }\n-  else{\n-    ir::phi_node* phi = make_phi(ty, 1, block);\n-    set_value(name, block, phi);\n-    result = add_phi_operands(name, phi);\n-    if(auto *phi = dynamic_cast<ir::phi_node*>(result))\n-      result = try_remove_trivial_phis(phi);\n-  }\n-  if(auto *phi = dynamic_cast<ir::phi_node*>(result)){\n-    result = try_remove_trivial_phis(phi);\n-  }\n-  set_value(name, block, result);\n-  return result;\n-}\n-\n-ir::value *module::get_value(const std::string& name, ir::basic_block *block) {\n-  ir::basic_block* save_block = builder_.get_insert_block();\n-  ir::basic_block::iterator save_pt = builder_.get_insert_point();\n-  val_key_t key(name, block);\n-  if(values_.find(key) != values_.end()){\n-    return values_.at(key);\n-  }\n-  ir::value *result = get_value_recursive(name, block);\n-  builder_.set_insert_point(save_block);\n-  if(save_pt != save_block->end())\n-    builder_.set_insert_point(save_pt);\n-  return result;\n-}\n-\n-ir::value *module::get_value(const std::string& name) {\n-  return get_value(name, builder_.get_insert_block());\n-}\n-\n-const std::string& module::get_name() {\n-  return name_;\n-}\n-\n-void module::seal_block(ir::basic_block *block){\n-  for(auto &x: incomplete_phis_[block]){\n-    add_phi_operands(x.first, x.second);\n-    if(get_value(x.first) == x.second)\n-      set_value(x.first, try_remove_trivial_phis(x.second));\n-  }\n-  sealed_blocks_.insert(block);\n-  incomplete_phis_[block].clear();\n-}\n-\n /* functions */\n function *module::get_or_insert_function(const std::string &name, function_type *ty) {\n   function *&fn = (function*&)symbols_[name];"}, {"filename": "lib/ir/type.cc", "status": "modified", "additions": 0, "deletions": 14, "changes": 14, "file_content_changes": "@@ -36,16 +36,6 @@ unsigned type::get_primitive_size_in_bits() const {\n unsigned type::get_integer_bitwidth() const\n { assert(id_ == IntegerTyID); return ((integer_type*)(this))->get_bitwidth(); }\n \n-signedness type::get_integer_signedness() const\n-{ assert(id_ == IntegerTyID); return ((integer_type*)(this))->get_signedness(); }\n-\n-bool type::is_integer_signed() const {\n-  if (id_ != IntegerTyID) {\n-    throw std::logic_error(\"type is \" + repr() + \", not integer\");\n-  }\n-  return ((integer_type*)(this))->get_signedness() == signedness::SIGNED;\n-}\n-\n unsigned type::get_tile_bitwidth() const\n { return ((block_type*)(this))->get_bitwidth(); }\n \n@@ -145,10 +135,6 @@ integer_type *type::get_int16_ty(context &ctx) { return &ctx.p_impl->int16_ty; }\n integer_type *type::get_int32_ty(context &ctx) { return &ctx.p_impl->int32_ty; }\n integer_type *type::get_int64_ty(context &ctx) { return &ctx.p_impl->int64_ty; }\n integer_type *type::get_int128_ty(context &ctx) { return &ctx.p_impl->int128_ty; }\n-integer_type *type::get_uint8_ty(context &ctx) { return &ctx.p_impl->uint8_ty; }\n-integer_type *type::get_uint16_ty(context &ctx) { return &ctx.p_impl->uint16_ty; }\n-integer_type *type::get_uint32_ty(context &ctx) { return &ctx.p_impl->uint32_ty; }\n-integer_type *type::get_uint64_ty(context &ctx) { return &ctx.p_impl->uint64_ty; }\n \n \n "}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 277, "deletions": 124, "changes": 401, "file_content_changes": "@@ -3,7 +3,6 @@\n #include \"triton/driver/error.h\"\n #include \"triton/driver/llvm.h\"\n #include \"triton/ir/builder.h\"\n-#include \"triton/ir/dispatch.h\"\n #include \"triton/ir/enums.h\"\n #include \"triton/ir/function.h\"\n #include \"triton/ir/module.h\"\n@@ -12,10 +11,12 @@\n #include <pybind11/buffer_info.h>\n #include <pybind11/functional.h>\n #include <pybind11/pybind11.h>\n+#include <pybind11/stl_bind.h>\n #include <pybind11/stl.h>\n #include \"Python.h\"\n #include <regex>\n #include <sstream>\n+#include <stdexcept>\n #include <string>\n #include \"llvm/IR/Module.h\"\n #include \"llvm/IR/LegacyPassManager.h\"\n@@ -299,8 +300,12 @@ void init_triton_runtime(py::module &&m) {\n \n     // get cached binary\n     py::str key(cache_key);\n-    if(!bin_cache.contains(key))\n-      add_to_cache(key, args, device, num_warps, num_stages);\n+    py::bool_ noop = false;\n+    if(!bin_cache.contains(key)) {\n+      noop = add_to_cache(key, args, device, num_warps, num_stages);\n+    }\n+    if (noop)\n+      return (py::object)py::none();\n     py::object bin = bin_cache[key];\n \n     // get grid\n@@ -529,91 +534,14 @@ void init_triton_codegen(py::module &&m) {\n           return hip_compile_ttir(name, ir, device, num_warps, num_stages, asm_map);\n       }, py::return_value_policy::take_ownership);\n   m.def(\"load_binary\", [](backend_t backend, const std::string& name, asm_map_t &asm_map, size_t n_shared_bytes, uint64_t dev){\n+\tpy::gil_scoped_release allow_threads;\n         if(backend == CUDA)\n           return cu_load_binary(name, asm_map, n_shared_bytes, dev);\n         if(backend == ROCM)\n           return hip_load_binary(name, asm_map, n_shared_bytes, dev);\n       }, py::return_value_policy::take_ownership);\n }\n \n-/*****************************************************************************/\n-/* User-facing language features                                             */\n-/*****************************************************************************/\n-\n-void init_triton_frontend(py::module &&m) {\n-  using ret = py::return_value_policy;\n-\n-  // programming model\n-  m.def(\"program_id\", &ir::dispatch::program_id, ret::reference);\n-  m.def(\"num_programs\", &ir::dispatch::num_programs, ret::reference);\n-  // binary\n-  m.def(\"add\", &ir::dispatch::add, ret::reference);\n-  m.def(\"sub\", &ir::dispatch::sub, ret::reference);\n-  m.def(\"mul\", &ir::dispatch::mul, ret::reference);\n-  m.def(\"truediv\", &ir::dispatch::truediv, ret::reference);\n-  m.def(\"floordiv\", &ir::dispatch::floordiv, ret::reference);\n-  m.def(\"fdiv\", &ir::dispatch::fdiv, ret::reference);\n-  m.def(\"mod\", &ir::dispatch::mod, ret::reference);\n-  m.def(\"and_\", &ir::dispatch::and_, ret::reference);\n-  m.def(\"or_\", &ir::dispatch::or_, ret::reference);\n-  m.def(\"xor_\", &ir::dispatch::xor_, ret::reference);\n-  m.def(\"lshr\", &ir::dispatch::lshr, ret::reference);\n-  m.def(\"shl\", &ir::dispatch::shl, ret::reference);\n-  // unary\n-  m.def(\"plus\", &ir::dispatch::plus, ret::reference);\n-  m.def(\"minus\", &ir::dispatch::minus, ret::reference);\n-  m.def(\"invert\", &ir::dispatch::invert, ret::reference);\n-  // comparison\n-  m.def(\"greater_than\", &ir::dispatch::greater_than, ret::reference);\n-  m.def(\"greater_equal\", &ir::dispatch::greater_equal, ret::reference);\n-  m.def(\"less_than\", &ir::dispatch::less_than, ret::reference);\n-  m.def(\"less_equal\", &ir::dispatch::less_equal, ret::reference);\n-  m.def(\"equal\", &ir::dispatch::equal, ret::reference);\n-  m.def(\"not_equal\", &ir::dispatch::not_equal, ret::reference);\n-  // block creation\n-  m.def(\"arange\", &ir::dispatch::arange, ret::reference);\n-  m.def(\"zeros\", &ir::dispatch::zeros, ret::reference);\n-  // type manipuatation\n-  m.def(\"cat\", &ir::dispatch::cat, ret::reference);\n-  m.def(\"reshape\", &ir::dispatch::reshape, ret::reference);\n-  typedef std::tuple<ir::value *, ir::value *> (*broadcast_ty)(ir::value *, ir::value *, ir::builder *);\n-  typedef ir::value *(*broadcast_to_ty)(ir::value *, ir::type::block_shapes_t, ir::builder *);\n-  m.def(\"broadcast\", (broadcast_ty)(&ir::dispatch::broadcast), ret::reference);\n-  m.def(\"broadcast_to\", (broadcast_to_ty)(&ir::dispatch::broadcast), ret::reference);\n-  m.def(\"bitcast\", &ir::dispatch::bitcast, ret::reference);\n-  m.def(\"cast\", &ir::dispatch::cast, ret::reference);\n-  // memory\n-  m.def(\"load\", &ir::dispatch::load, ret::reference);\n-  m.def(\"store\", &ir::dispatch::store, ret::reference);\n-  m.def(\"atomic_cas\", &ir::dispatch::atomic_cas, ret::reference);\n-  m.def(\"atomic_xchg\", &ir::dispatch::atomic_xchg, ret::reference);\n-  m.def(\"atomic_add\", &ir::dispatch::atomic_add, ret::reference);\n-  m.def(\"atomic_max\", &ir::dispatch::atomic_max, ret::reference);\n-  m.def(\"atomic_min\", &ir::dispatch::atomic_min, ret::reference);\n-  m.def(\"atomic_and\", &ir::dispatch::atomic_and, ret::reference);\n-  m.def(\"atomic_or\", &ir::dispatch::atomic_or, ret::reference);\n-  m.def(\"atomic_xor\", &ir::dispatch::atomic_xor, ret::reference);\n-  // linear algebra\n-  m.def(\"dot\", &ir::dispatch::dot, ret::reference);\n-  // indexing\n-  m.def(\"where\", &ir::dispatch::where, ret::reference);\n-  // reduction\n-  m.def(\"min\", &ir::dispatch::min, ret::reference);\n-  m.def(\"max\", &ir::dispatch::max, ret::reference);\n-  m.def(\"sum\", &ir::dispatch::sum, ret::reference);\n-  m.def(\"xor_sum\", &ir::dispatch::xor_sum, ret::reference);\n-  // math\n-  m.def(\"umulhi\", &ir::dispatch::umulhi, ret::reference);\n-  m.def(\"exp\", &ir::dispatch::exp, ret::reference);\n-  m.def(\"log\", &ir::dispatch::log, ret::reference);\n-  m.def(\"cos\", &ir::dispatch::cos, ret::reference);\n-  m.def(\"sin\", &ir::dispatch::sin, ret::reference);\n-  m.def(\"sqrt\", &ir::dispatch::sqrt, ret::reference);\n-  // internal (debugging only)\n-  m.def(\"multiple_of\", &ir::dispatch::multiple_of, ret::reference);\n-  m.def(\"max_contiguous\", &ir::dispatch::max_contiguous, ret::reference);\n-  m.def(\"debug_barrier\", &ir::dispatch::debug_barrier, ret::reference);\n-}\n \n /*****************************************************************************/\n /* Python bindings for triton::ir                                            */\n@@ -623,16 +551,86 @@ void init_triton_ir(py::module &&m) {\n   using ret = py::return_value_policy;\n   using namespace pybind11::literals;\n \n+  py::enum_<ir::load_inst::CACHE_MODIFIER>(m, \"CACHE_MODIFIER\")\n+      .value(\"NONE\", ir::load_inst::NONE)\n+      .value(\"CA\", ir::load_inst::CA)\n+      .value(\"CG\", ir::load_inst::CG)\n+      .export_values();\n+  \n+  py::enum_<ir::load_inst::EVICTION_POLICY>(m, \"EVICTION_POLICY\")\n+      .value(\"NORMAL\", ir::load_inst::NORMAL)\n+      .value(\"EVICT_FIRST\", ir::load_inst::EVICT_FIRST)\n+      .value(\"EVICT_LAST\", ir::load_inst::EVICT_LAST)\n+      .export_values();\n+  \n+  py::enum_<ir::reduce_inst::op_t>(m, \"REDUCE_OP\")\n+      .value(\"ADD\", ir::reduce_inst::ADD)\n+      .value(\"FADD\", ir::reduce_inst::FADD)\n+      .value(\"MIN\", ir::reduce_inst::MIN)\n+      .value(\"MAX\", ir::reduce_inst::MAX)\n+      .value(\"FMIN\", ir::reduce_inst::FMIN)\n+      .value(\"FMAX\", ir::reduce_inst::FMAX)\n+      .value(\"XOR\", ir::reduce_inst::XOR);\n+  \n+  py::enum_<ir::atomic_rmw_op_t>(m, \"ATOMIC_OP\")\n+      .value(\"ADD\", ir::atomic_rmw_op_t::Add)\n+      .value(\"FADD\", ir::atomic_rmw_op_t::FAdd)\n+      .value(\"AND\", ir::atomic_rmw_op_t::And)\n+      .value(\"OR\", ir::atomic_rmw_op_t::Or)\n+      .value(\"XOR\", ir::atomic_rmw_op_t::Xor)\n+      .value(\"XCHG\", ir::atomic_rmw_op_t::Xchg)\n+      .value(\"MAX\", ir::atomic_rmw_op_t::Max)\n+      .value(\"MIN\", ir::atomic_rmw_op_t::Min)\n+      .value(\"UMIN\", ir::atomic_rmw_op_t::UMin)\n+      .value(\"UMAX\", ir::atomic_rmw_op_t::UMax);\n+\n   py::class_<ir::context>(m, \"context\")\n       .def(py::init<>());\n \n-  auto value = py::class_<ir::value>(m, \"value\");\n-  value.def_property(\"name\", &ir::value::get_name, &ir::value::set_name);\n-  value.def_property_readonly(\"type\", &ir::value::get_type);\n+  py::class_<ir::value>(m, \"value\")\n+      .def(\"multiple_of\", [](ir::value *self, int val) {\n+        if (auto *instr = dynamic_cast<ir::instruction*>(self)) {\n+          instr->set_metadata(ir::metadata::multiple_of, val);\n+        } else\n+          throw std::runtime_error(\"multiple_of\");\n+      })\n+      .def(\"max_contiguous\", [](ir::value *self, int val) {\n+        if (auto *instr = dynamic_cast<ir::instruction*>(self)) {\n+          instr->set_metadata(ir::metadata::max_contiguous, val);\n+        } else\n+          throw std::runtime_error(\"max_contiguous\");\n+      })\n+      .def(\"set_fdiv_ieee_rounding\", [](ir::value *self, bool val) {\n+        if (auto *instr = dynamic_cast<ir::binary_operator*>(self))\n+          instr->set_fdiv_ieee_rounding(val);\n+        else\n+          throw std::runtime_error(\"set_fdiv_ieee_rounding\");\n+      })\n+      .def(\"is_phi\", [](ir::value *self) {\n+        if (auto *pn = dynamic_cast<ir::phi_node*>(self))\n+          return true;\n+        return false;\n+      })\n+      .def(\"ops\", [](ir::value *self) {\n+        if (auto *instr = dynamic_cast<ir::instruction*>(self)) {\n+          return instr->ops();\n+        }\n+        throw std::runtime_error(\"cannot use ops()\");\n+      })\n+      .def(\"replace_all_uses_with\", &ir::value::replace_all_uses_with)\n+      .def(\"erase_from_parent\", [](ir::value *self) {\n+        if (auto *instr = dynamic_cast<ir::instruction*>(self))\n+          return instr->erase_from_parent();\n+        throw std::runtime_error(\"cannot use erase_from_parent\");\n+      })\n+      .def_property(\"name\", &ir::value::get_name, &ir::value::set_name)\n+      .def_property_readonly(\"type\", &ir::value::get_type);\n \n   py::class_<ir::user, ir::value>(m, \"user\");\n \n-  py::class_<ir::constant, ir::user>(m, \"constant\");\n+  py::class_<ir::constant, ir::user>(m, \"constant\")\n+      .def(\"get_null_value\", &ir::constant::get_null_value, ret::reference)\n+      .def(\"get_all_ones_value\", &ir::constant::get_all_ones_value, ret::reference);\n \n   py::class_<ir::undef_value, ir::constant>(m, \"undef\")\n       .def(\"get\", &ir::undef_value::get, ret::reference);\n@@ -643,16 +641,17 @@ void init_triton_ir(py::module &&m) {\n       .def(\"__bool__\", [](ir::constant_int *self) { return self->get_value(); });\n \n   py::class_<ir::constant_fp, ir::constant>(m, \"constant_float\")\n-      .def_property_readonly(\"value\", &ir::constant_fp::get_value);\n+      .def_property_readonly(\"value\", &ir::constant_fp::get_value)\n+      .def(\"get\", [](ir::type* ty, double val) { return ir::constant_fp::get(ty, val); }, ret::reference);\n \n-  py::class_<ir::instruction, ir::user>(m, \"instruction\");\n-  py::class_<ir::phi_node, ir::user>(m, \"phi_node\");\n+  py::class_<ir::instruction, ir::user>(m, \"instruction\")\n+      .def(\"get_parent\", [](ir::instruction *self) {\n+        return self->get_parent();\n+      }, ret::reference);\n+  py::class_<ir::phi_node, ir::instruction>(m, \"phi_node\")\n+      .def(\"add_incoming\", &ir::phi_node::add_incoming);\n \n   py::class_<ir::type>(m, \"type\")\n-      .def(\"is_ptr\", &ir::type::is_pointer_ty)\n-      .def(\"is_int\", static_cast<bool (ir::type::*)() const>(&ir::type::is_integer_ty))\n-      .def(\"is_floating\", &ir::type::is_floating_point_ty)\n-      .def(\"is_block\", &ir::type::is_block_ty)\n       .def(\"make_ptr\", &ir::pointer_type::get, ret::reference)\n       .def(\"make_function\", &ir::function_type::get, ret::reference)\n       .def(\"make_block\", &ir::block_type::get, ret::reference)\n@@ -667,34 +666,38 @@ void init_triton_ir(py::module &&m) {\n       .def(\"get_int16\", &ir::type::get_int16_ty, ret::reference)\n       .def(\"get_int32\", &ir::type::get_int32_ty, ret::reference)\n       .def(\"get_int64\", &ir::type::get_int64_ty, ret::reference)\n-      .def(\"get_uint8\", &ir::type::get_uint8_ty, ret::reference)\n-      .def(\"get_uint16\", &ir::type::get_uint16_ty, ret::reference)\n-      .def(\"get_uint32\", &ir::type::get_uint32_ty, ret::reference)\n-      .def(\"get_uint64\", &ir::type::get_uint64_ty, ret::reference)\n+      .def(\"get_fp_mantissa_width\", &ir::type::get_fp_mantissa_width, ret::reference)\n+\n+      .def(\"get_block_shapes\", &ir::type::get_block_shapes)\n \n+      .def(\"is_ptr\", &ir::type::is_pointer_ty)\n+      .def(\"is_int\", static_cast<bool (ir::type::*)() const>(&ir::type::is_integer_ty))\n+      .def(\"is_floating\", &ir::type::is_floating_point_ty)\n+      .def(\"is_block\", &ir::type::is_block_ty)\n       .def(\"is_void\", &ir::type::is_void_ty)\n+      .def(\"is_bool\", &ir::type::is_bool_ty)\n       .def(\"is_fp8\", &ir::type::is_fp8_ty)\n       .def(\"is_fp16\", &ir::type::is_fp16_ty)\n       .def(\"is_bf16\", &ir::type::is_bf16_ty)\n       .def(\"is_fp32\", &ir::type::is_fp32_ty)\n       .def(\"is_fp64\", &ir::type::is_fp64_ty)\n-      .def(\"is_int1\", [](ir::type *self) { return self->is_integer_ty(1, ir::signedness::SIGNED); })\n-      .def(\"is_int8\", [](ir::type *self) { return self->is_integer_ty(8, ir::signedness::SIGNED); })\n-      .def(\"is_int16\", [](ir::type *self) { return self->is_integer_ty(16, ir::signedness::SIGNED); })\n-      .def(\"is_int32\", [](ir::type *self) { return self->is_integer_ty(32, ir::signedness::SIGNED); })\n-      .def(\"is_int64\", [](ir::type *self) { return self->is_integer_ty(64, ir::signedness::SIGNED); })\n-      .def(\"is_uint8\", [](ir::type *self) { return self->is_integer_ty(8, ir::signedness::UNSIGNED); })\n-      .def(\"is_uint16\", [](ir::type *self) { return self->is_integer_ty(16, ir::signedness::UNSIGNED); })\n-      .def(\"is_uint32\", [](ir::type *self) { return self->is_integer_ty(32, ir::signedness::UNSIGNED); })\n-      .def(\"is_uint64\", [](ir::type *self) { return self->is_integer_ty(64, ir::signedness::UNSIGNED); })\n+      .def(\"is_int1\", [](ir::type *self) { return self->is_integer_ty(1); })\n+      .def(\"is_int8\", [](ir::type *self) { return self->is_integer_ty(8); })\n+      .def(\"is_int16\", [](ir::type *self) { return self->is_integer_ty(16); })\n+      .def(\"is_int32\", [](ir::type *self) { return self->is_integer_ty(32); })\n+      .def(\"is_int64\", [](ir::type *self) { return self->is_integer_ty(64); })\n+      .def(\"is_int_or_tileint\", &ir::type::is_int_or_tileint_ty)\n \n       .def(\"repr\", &ir::type::repr)\n       .def_property_readonly(\"fp_mantissa_width\", &ir::type::get_fp_mantissa_width)\n       .def_property_readonly(\"scalar\", &ir::type::get_scalar_ty)\n-      .def_property_readonly(\"context\", &ir::type::get_context, ret::reference);\n+      .def_property_readonly(\"context\", &ir::type::get_context, ret::reference)\n+      .def_property_readonly(\"int_bitwidth\", &ir::type::get_integer_bitwidth)\n+      .def_property_readonly(\"primitive_bitwidth\", &ir::type::get_primitive_size_in_bits);\n \n   py::class_<ir::pointer_type, ir::type>(m, \"pointer_type\")\n-      .def_property_readonly(\"element\", &ir::pointer_type::get_element_ty, ret::reference);\n+      .def_property_readonly(\"element\", &ir::pointer_type::get_element_ty, ret::reference)\n+      .def_property_readonly(\"address_space\", &ir::pointer_type::get_pointer_address_space, ret::reference);\n \n   py::class_<ir::function_type, ir::type>(m, \"function_type\");\n   py::class_<ir::integer_type, ir::type>(m, \"integer_type\");\n@@ -704,16 +707,15 @@ void init_triton_ir(py::module &&m) {\n \n   py::class_<ir::module>(m, \"module\")\n       .def(py::init<std::string, ir::builder &>())\n-      .def(\"get_or_insert_function\", &ir::module::get_or_insert_function, ret::reference)\n-      .def(\"seal_block\", &ir::module::seal_block)\n-      .def(\"set_value\", (void (ir::module::*)(const std::string &, ir::value *)) & ir::module::set_value)\n-      .def(\"set_type\", &ir::module::set_type)\n-      .def(\"get_value\", (ir::value * (ir::module::*)(const std::string &)) & ir::module::get_value, ret::reference)\n-      .def(\"get_values\", &ir::module::get_values, ret::reference)\n-      .def(\"set_values\", &ir::module::set_values)\n-      .def(\"get_types\", &ir::module::get_types, ret::reference)\n-      .def(\"set_types\", &ir::module::set_types)\n-      .def_property_readonly(\"builder\", &ir::module::get_builder, ret::reference);\n+      .def(\"set_instr_metadata\", [](ir::module *self, const std::string &name, ir::value *value) {\n+        const auto metadatas = self->get_metadatas();\n+        auto it = metadatas.find(name);\n+        if (it != metadatas.end())\n+          if (auto *instr = dynamic_cast<ir::instruction*>(value)) {\n+            instr->set_metadata(it->second.first, it->second.second);\n+          }\n+      })\n+      .def(\"get_or_insert_function\", &ir::module::get_or_insert_function, ret::reference);\n \n   using eattr = ir::attribute_kind_t;\n   py::enum_<eattr>(m, \"attribute_kind\")\n@@ -737,6 +739,13 @@ void init_triton_ir(py::module &&m) {\n \n   py::class_<ir::basic_block, ir::value>(m, \"basic_block\")\n       .def(\"create\", &ir::basic_block::create, ret::reference)\n+      .def(\"get_predecessors\", &ir::basic_block::get_predecessors, ret::reference)\n+      .def(\"get_first_non_phi\", [](ir::basic_block *self) -> ir::instruction* {\n+        ir::basic_block::iterator it = self->get_first_non_phi();\n+        if (it == self->end())\n+          return nullptr;\n+        return *it;\n+      }, ret::reference)\n       .def_property_readonly(\"parent\", &ir::basic_block::get_parent, ret::reference);\n \n   py::class_<ir::builder>(m, \"builder\", py::dynamic_attr())\n@@ -747,23 +756,167 @@ void init_triton_ir(py::module &&m) {\n       .def(\"br\", &ir::builder::create_br, ret::reference)\n       .def(\"cond_br\", &ir::builder::create_cond_br, ret::reference)\n       .def(\"ret_void\", &ir::builder::create_ret_void, ret::reference)\n+      // insertion block/point, insert points are represented as (*bb, *instr)\n       .def(\"get_insert_block\", &ir::builder::get_insert_block, ret::reference)\n       .def(\"set_insert_block\", (void (ir::builder::*)(ir::basic_block *)) & ir::builder::set_insert_point)\n-      // constants\n+      .def(\"get_insert_point\", [](ir::builder *self) {\n+        ir::basic_block *bb = self->get_insert_block();\n+        ir::basic_block::iterator it = self->get_insert_point();\n+        ir::instruction *instr = it == bb->end() ? nullptr : *it;\n+        return std::make_pair(bb, instr);\n+      }, ret::reference)\n+      .def(\"set_insert_point\", [](ir::builder *self, std::pair<ir::basic_block*, ir::instruction*> pt) {\n+        ir::basic_block *bb = pt.first;\n+        ir::instruction *instr = pt.second;\n+        if (instr) {\n+          if (bb != instr->get_parent())\n+            throw std::runtime_error(\"invalid insertion point, instr not in bb\");\n+          self->set_insert_point(instr);\n+        } else {\n+          assert(bb);\n+          self->set_insert_point(bb);\n+        }\n+      })\n+      // Constants\n       .def(\"get_int1\", &ir::builder::get_int1, ret::reference)\n-      .def(\"get_int32\", &ir::builder::get_int32, ret::reference)\n-      .def(\"get_int64\", &ir::builder::get_int64, ret::reference)\n-      .def(\"get_uint32\", &ir::builder::get_uint32, ret::reference)\n-      .def(\"get_uint64\", &ir::builder::get_uint64, ret::reference)\n+      .def(\"get_int32\", [](ir::builder *self, int32_t v) { return self->get_int32((uint32_t)v); }, ret::reference)\n+      .def(\"get_uint32\", &ir::builder::get_int32, ret::reference)\n+      .def(\"get_int64\", [](ir::builder *self, int64_t v) { return self->get_int64((uint64_t)v); }, ret::reference)\n+      .def(\"get_uint64\", &ir::builder::get_int64, ret::reference)\n       .def(\"get_float16\", &ir::builder::get_float16, ret::reference)\n       .def(\"get_float32\", &ir::builder::get_float32, ret::reference)\n-      .def(\"get_range\", &ir::builder::get_range, ret::reference);\n+      .def(\"get_range\", &ir::builder::get_range, ret::reference)\n+      // Types\n+      .def(\"get_void_ty\", &ir::builder::get_void_ty, ret::reference)\n+      .def(\"get_int1_ty\", &ir::builder::get_int1_ty, ret::reference)\n+      .def(\"get_int8_ty\", &ir::builder::get_int8_ty, ret::reference)\n+      .def(\"get_int16_ty\", &ir::builder::get_int16_ty, ret::reference)\n+      .def(\"get_int32_ty\", &ir::builder::get_int32_ty, ret::reference)\n+      .def(\"get_int64_ty\", &ir::builder::get_int64_ty, ret::reference)\n+      .def(\"get_fp8_ty\", &ir::builder::get_fp8_ty, ret::reference)\n+      .def(\"get_half_ty\", &ir::builder::get_half_ty, ret::reference)\n+      .def(\"get_bf16_ty\", &ir::builder::get_bf16_ty, ret::reference)\n+      .def(\"get_float_ty\", &ir::builder::get_float_ty, ret::reference)\n+      .def(\"get_double_ty\", &ir::builder::get_double_ty, ret::reference)\n+      // terminator instructions\n+      .def(\"create_br\", &ir::builder::create_br, ret::reference)\n+      .def(\"create_cond_br\", &ir::builder::create_cond_br, ret::reference)\n+      .def(\"create_ret_void\", &ir::builder::create_ret_void, ret::reference)\n+      // Cast instructions\n+      .def(\"create_bitcast\", &ir::builder::create_bitcast, ret::reference)\n+      .def(\"create_cast\", &ir::builder::create_cast, ret::reference)\n+      .def(\"create_ptr_to_int\", &ir::builder::create_ptr_to_int, ret::reference)\n+      .def(\"create_si_to_fp\", &ir::builder::create_si_to_fp, ret::reference)\n+      .def(\"create_ui_to_fp\", &ir::builder::create_ui_to_fp, ret::reference)\n+      .def(\"create_fp_to_si\", &ir::builder::create_fp_to_si, ret::reference)\n+      .def(\"create_fp_to_ui\", &ir::builder::create_fp_to_ui, ret::reference)\n+      .def(\"create_fp_ext\", &ir::builder::create_fp_ext, ret::reference)\n+      .def(\"create_fp_trunc\", &ir::builder::create_fp_trunc, ret::reference)\n+      .def(\"create_int_cast\", &ir::builder::create_int_cast, ret::reference)\n+      .def(\"create_downcast\", &ir::builder::create_downcast, ret::reference)\n+      // phi\n+      .def(\"create_phi\", &ir::builder::create_phi, ret::reference)\n+      // Binary instructions\n+      .def(\"create_insert_nuwnswb_binop\", &ir::builder::create_insert_nuwnswb_binop, ret::reference)\n+      .def(\"create_fmul\", &ir::builder::create_fmul, ret::reference)\n+      .def(\"create_fdiv\", &ir::builder::create_fdiv, ret::reference)\n+      .def(\"create_frem\", &ir::builder::create_frem, ret::reference)\n+      .def(\"create_fadd\", &ir::builder::create_fadd, ret::reference)\n+      .def(\"create_fsub\", &ir::builder::create_fsub, ret::reference)\n+      .def(\"create_mul\", &ir::builder::create_mul, ret::reference, \n+                          py::arg(\"lhs\"), py::arg(\"rhs\"), \n+                          py::arg(\"has_nuw\")=false, py::arg(\"has_nsw\")=false)\n+      .def(\"create_sdiv\", &ir::builder::create_sdiv, ret::reference)\n+      .def(\"create_udiv\", &ir::builder::create_udiv, ret::reference)\n+      .def(\"create_srem\", &ir::builder::create_srem, ret::reference)\n+      .def(\"create_urem\", &ir::builder::create_urem, ret::reference)\n+      .def(\"create_add\", &ir::builder::create_add, ret::reference, \n+                          py::arg(\"lhs\"), py::arg(\"rhs\"), \n+                          py::arg(\"has_nuw\")=false, py::arg(\"has_nsw\")=false)\n+      .def(\"create_sub\", &ir::builder::create_sub, ret::reference,\n+                          py::arg(\"lhs\"), py::arg(\"rhs\"), \n+                          py::arg(\"has_nuw\")=false, py::arg(\"has_nsw\")=false)\n+      .def(\"create_shl\", &ir::builder::create_shl, ret::reference,\n+                          py::arg(\"lhs\"), py::arg(\"rhs\"), \n+                          py::arg(\"has_nuw\")=false, py::arg(\"has_nsw\")=false)\n+      .def(\"create_lshr\", &ir::builder::create_lshr, ret::reference,\n+                          py::arg(\"lhs\"), py::arg(\"rhs\"), \n+                          py::arg(\"has_nuw\")=false, py::arg(\"has_nsw\")=false)\n+      .def(\"create_ashr\", &ir::builder::create_ashr, ret::reference,\n+                          py::arg(\"lhs\"), py::arg(\"rhs\"), \n+                          py::arg(\"has_nuw\")=false, py::arg(\"has_nsw\")=false)\n+      // GEP\n+      .def(\"create_gep\", &ir::builder::create_gep, ret::reference)\n+      // Comparison (int)\n+      .def(\"create_icmp\", &ir::builder::create_icmp, ret::reference)\n+      .def(\"create_icmpSLE\", &ir::builder::create_icmpSLE, ret::reference)\n+      .def(\"create_icmpSLT\", &ir::builder::create_icmpSLT, ret::reference)\n+      .def(\"create_icmpSGE\", &ir::builder::create_icmpSGE, ret::reference)\n+      .def(\"create_icmpSGT\", &ir::builder::create_icmpSGT, ret::reference)\n+      .def(\"create_icmpULE\", &ir::builder::create_icmpULE, ret::reference)\n+      .def(\"create_icmpULT\", &ir::builder::create_icmpULT, ret::reference)\n+      .def(\"create_icmpUGE\", &ir::builder::create_icmpUGE, ret::reference)\n+      .def(\"create_icmpUGT\", &ir::builder::create_icmpUGT, ret::reference)\n+      .def(\"create_icmpEQ\", &ir::builder::create_icmpEQ, ret::reference)\n+      .def(\"create_icmpNE\", &ir::builder::create_icmpNE, ret::reference)\n+      // Comparison (float)\n+      .def(\"create_fcmp\", &ir::builder::create_fcmp, ret::reference)\n+      .def(\"create_fcmpOLT\", &ir::builder::create_fcmpOLT, ret::reference)\n+      .def(\"create_fcmpOGT\", &ir::builder::create_fcmpOGT, ret::reference)\n+      .def(\"create_fcmpOLE\", &ir::builder::create_fcmpOLE, ret::reference)\n+      .def(\"create_fcmpOGE\", &ir::builder::create_fcmpOGE, ret::reference)\n+      .def(\"create_fcmpOEQ\", &ir::builder::create_fcmpOEQ, ret::reference)\n+      .def(\"create_fcmpONE\", &ir::builder::create_fcmpONE, ret::reference)\n+      .def(\"create_fcmpULT\", &ir::builder::create_fcmpULT, ret::reference)\n+      .def(\"create_fcmpUGT\", &ir::builder::create_fcmpUGT, ret::reference)\n+      .def(\"create_fcmpULE\", &ir::builder::create_fcmpULE, ret::reference)\n+      .def(\"create_fcmpUGE\", &ir::builder::create_fcmpUGE, ret::reference)\n+      .def(\"create_fcmpUEQ\", &ir::builder::create_fcmpUEQ, ret::reference)\n+      .def(\"create_fcmpUNE\", &ir::builder::create_fcmpUNE, ret::reference)\n+      // Logical\n+      .def(\"create_and\", &ir::builder::create_and, ret::reference)\n+      .def(\"create_xor\", &ir::builder::create_xor, ret::reference)\n+      .def(\"create_or\", &ir::builder::create_or, ret::reference)\n+      // Input/Output\n+      .def(\"create_load\", &ir::builder::create_load, ret::reference)\n+      .def(\"create_store\", &ir::builder::create_store, ret::reference)\n+      .def(\"create_masked_load\", &ir::builder::create_masked_load, ret::reference)\n+      .def(\"create_masked_store\", &ir::builder::create_masked_store, ret::reference)\n+      // Block instruction\n+      .def(\"create_splat\", &ir::builder::create_splat, ret::reference)\n+      .def(\"create_reshape\", &ir::builder::create_reshape, ret::reference)\n+      .def(\"create_cat\", &ir::builder::create_cat, ret::reference)\n+      .def(\"create_broadcast\", &ir::builder::create_broadcast, ret::reference)\n+      // atomic\n+      .def(\"create_atomic_cas\", &ir::builder::create_atomic_cas, ret::reference)\n+      .def(\"create_atomic_rmw\", &ir::builder::create_atomic_rmw, ret::reference)\n+\n+      // Built-in instruction\n+      .def(\"create_get_program_id\", &ir::builder::create_get_program_id, ret::reference)\n+      .def(\"create_get_num_programs\", &ir::builder::create_get_num_programs, ret::reference)\n+      .def(\"create_exp\", &ir::builder::create_exp, ret::reference)\n+      .def(\"create_cos\", &ir::builder::create_cos, ret::reference)\n+      .def(\"create_sin\", &ir::builder::create_sin, ret::reference)\n+      .def(\"create_log\", &ir::builder::create_log, ret::reference)\n+      .def(\"create_dot\", &ir::builder::create_dot, ret::reference)\n+      .def(\"create_trans\", &ir::builder::create_trans, ret::reference)\n+      .def(\"create_sqrt\", &ir::builder::create_sqrt, ret::reference)\n+      .def(\"create_reduce\", &ir::builder::create_reduce, ret::reference)\n+      .def(\"create_select\", &ir::builder::create_select, ret::reference)\n+      // Intrinsics\n+      // These have no place in the IR, and hopefully they can be removed at some point\n+      .def(\"create_umulhi\", &ir::builder::create_umulhi, ret::reference)\n+      .def(\"create_copy_to_shared\", &ir::builder::create_copy_to_shared, ret::reference)\n+      .def(\"create_masked_load_async\", &ir::builder::create_masked_load_async, ret::reference)\n+      .def(\"create_copy_from_shared\", &ir::builder::create_copy_from_shared, ret::reference)\n+      .def(\"create_barrier\", &ir::builder::create_barrier, ret::reference)\n+      .def(\"create_async_wait\", &ir::builder::create_async_wait, ret::reference)\n+      .def(\"create_prefetch_s\", &ir::builder::create_prefetch_s, ret::reference);\n }\n \n void init_triton(py::module &m) {\n   py::module subm = m.def_submodule(\"triton\");\n   init_triton_codegen(std::move(subm.def_submodule(\"code_gen\")));\n   init_triton_runtime(std::move(subm.def_submodule(\"runtime\")));\n   init_triton_ir(std::move(subm.def_submodule(\"ir\")));\n-  init_triton_frontend(std::move(subm.def_submodule(\"frontend\")));\n }"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -37,7 +37,7 @@ def nvsmi(attrs):\n         (256, 256, 256): {'float16': 0.027},\n         (512, 512, 512): {'float16': 0.158},\n         (1024, 1024, 1024): {'float16': 0.466},\n-        (2048, 2048, 2048): {'float16': 0.680},\n+        (2048, 2048, 2048): {'float16': 0.695},\n         (4096, 4096, 4096): {'float16': 0.831},\n         (8192, 8192, 8192): {'float16': 0.849},\n         # tall-skinny"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 0, "deletions": 23, "changes": 23, "file_content_changes": "@@ -1,5 +1,4 @@\n # flake8: noqa: F821,F841\n-import copy\n import itertools\n import re\n from typing import Optional, Union\n@@ -585,7 +584,6 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n \n     f8_output_tensor = torch.empty_like(f16, dtype=torch.int8)\n     f8_output = triton.reinterpret(f8_output_tensor, tl.float8)\n-    print(f16.dtype, f8_output.dtype)\n     copy_kernel[grid](f16, f8_output, n_elements, BLOCK_SIZE=1024)\n \n     assert torch.all(f8_tensor == f8_output_tensor)\n@@ -993,27 +991,6 @@ def kernel(x):\n     kernel[(1, )](x)\n \n \n-@pytest.mark.parametrize(\"value, value_type\", [\n-    (-1, 'i32'), (0, 'i32'), (1, None), (-2**31, 'i32'), (2**31 - 1, 'i32'),\n-    (2**31, 'u32'), (2**32 - 1, 'u32'), (2**32, 'i64'), (2**63 - 1, 'i64'),\n-    (-2**63, 'i64'), (2**63, 'u64'), (2**64 - 1, 'u64')\n-])\n-def test_value_specialization(value: int, value_type: str, device='cuda') -> None:\n-\n-    @triton.jit\n-    def kernel(VALUE, X):\n-        pass\n-\n-    x = torch.tensor([3.14159], device='cuda')\n-    pgm = kernel[(1, )](value, x)\n-\n-    # Parse out the type of the 'VALUE' parameter from the Triton IR.\n-    triton_ir = pgm.asm['ttir']\n-    ir_value_match = re.match(r'\\s*def void kernel\\((\\w+) VALUE ', triton_ir)\n-    ir_value_type = None if ir_value_match is None else ir_value_match.group(1)\n-    assert ir_value_type == value_type\n-\n-\n @pytest.mark.parametrize(\n     \"value, overflow\",\n     [(2**64 - 1, False), (2**64, True), (-2**63, False), (-2**63 - 1, True)]"}, {"filename": "python/test/unit/runtime/test_cache.py", "status": "modified", "additions": 30, "deletions": 2, "changes": 32, "file_content_changes": "@@ -1,4 +1,5 @@\n import os\n+import re\n import shutil\n \n import pytest\n@@ -76,7 +77,7 @@ def reset_tmp_dir():\n def test_reuse():\n     counter = 0\n \n-    def inc_counter(key, binary, repr):\n+    def inc_counter(*args, **kwargs):\n         nonlocal counter\n         counter += 1\n     JITFunction.cache_hook = inc_counter\n@@ -91,7 +92,7 @@ def inc_counter(key, binary, repr):\n def test_specialize(mode):\n     counter = 0\n \n-    def inc_counter(key, binary, repr):\n+    def inc_counter(*args, **kwargs):\n         nonlocal counter\n         counter += 1\n     JITFunction.cache_hook = inc_counter\n@@ -102,3 +103,30 @@ def inc_counter(key, binary, repr):\n     for i in [1, 2, 4, 8, 16, 32]:\n         function[(1,)](x, i, BLOCK=512)\n     assert counter == target\n+\n+\n+@pytest.mark.parametrize(\"value, value_type\", [\n+    (-1, 'int32'), (0, 'int32'), (1, None), (-2**31, 'int32'), (2**31 - 1, 'int32'),\n+    (2**32, 'int64'), (2**63 - 1, 'int64'), (-2**63, 'int64'),\n+    (2**31, 'uint32'), (2**32 - 1, 'uint32'), (2**63, 'uint64'), (2**64 - 1, 'uint64')\n+])\n+def test_value_specialization(value: int, value_type: str, device='cuda') -> None:\n+\n+    @triton.jit\n+    def kernel(VALUE, X):\n+        pass\n+\n+    cache_str = None\n+\n+    def get_cache_str(*args, **kwargs):\n+        nonlocal cache_str\n+        cache_str = kwargs['key'].split('-')\n+    triton.code_gen.JITFunction.cache_hook = get_cache_str\n+    reset_tmp_dir()\n+    x = torch.tensor([3.14159], device='cuda')\n+    kernel[(1, )](value, x)\n+    triton.code_gen.JITFunction.cache_hook = None\n+\n+    cache_str_match = re.match(r'_(\\w+)\\[multipleof\\(\\d+\\)]_float32\\*\\[multipleof\\(16\\)\\]', cache_str[-1])\n+    spec_type = None if cache_str_match is None else cache_str_match.group(1)\n+    assert spec_type == value_type"}, {"filename": "python/triton/__init__.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -6,7 +6,8 @@\n # or pybind11 shows `munmap_chunk(): invalid pointer`\n import torch\n # submodules\n-from .code_gen import cdiv, next_power_of_2, jit, autotune, heuristics, JITFunction, Config, Autotuner, reinterpret\n+from .code_gen import cdiv, next_power_of_2, jit, autotune, heuristics, \\\n+    JITFunction, Config, Autotuner, reinterpret\n from . import language\n from . import code_gen\n from . import testing"}, {"filename": "python/triton/code_gen.py", "status": "modified", "additions": 308, "deletions": 173, "changes": 481, "file_content_changes": "@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n import ast\n import builtins\n import functools\n@@ -11,7 +13,7 @@\n import textwrap\n import time\n import warnings\n-from typing import Dict\n+from typing import Dict, Optional, Set, Tuple, Union\n \n import torch\n from filelock import FileLock\n@@ -22,7 +24,39 @@\n \n \n class CodeGenerator(ast.NodeVisitor):\n+    def __init__(self, context, prototype, gscope, attributes, constants, kwargs):\n+        self.builder = _triton.ir.builder(context)\n+        self.module = _triton.ir.module('', self.builder)\n+        self.prototype = prototype\n+        self.gscope = gscope\n+        self.lscope = dict()\n+        self.is_arg_lscope = dict()  # name => is_arg: {str: bool}\n+        self.attributes = attributes\n+        self.constants = constants\n+        self.kwargs = kwargs\n+        self.last_node = None\n+        self.builtins = {\n+            'range': range,\n+            'min': triton.language.minimum,\n+            'float': float,\n+            'int': int,\n+            'print': print,\n+            'isinstance': isinstance,\n+            'getattr': getattr,\n+        }\n+        # SSA-construction\n+        # [name, bb] => triton.language.tensor\n+        self.lvalues: Dict[Tuple[str, _triton.ir.basic_block], triton.language.tensor] = {}\n+        # bb => {name => phi}\n+        self.incomplete_phis = {}\n+        self.sealed_blocks: Set[_triton.ir.basic_block] = set()\n+\n     def get_value(self, name):\n+        ''' This function:\n+        1. make sure `name` is defined\n+        2. if `name` is triton.language.tensor, get stored tensor by calling\n+           `self._get_tensor()`\n+        '''\n         # search node.id in local scope\n         ret = None\n         if name in self.lscope:\n@@ -35,49 +69,122 @@ def get_value(self, name):\n             ret = self.builtins[name]\n         else:\n             raise ValueError(f'{name} is not defined')\n-        if isinstance(ret, triton.language.block):\n-            handle = self.module.get_value(name)\n-            return triton.language.block(handle)\n+        if self.is_triton_tensor(ret) and not self.is_arg_lscope[name]:\n+            return self._get_tensor(name)\n         return ret\n \n-    def set_value(self, name, value):\n-        if isinstance(value, _triton.ir.value):\n-            value = triton.language.block(value)\n-        if isinstance(value, triton.language.block):\n-            self.module.set_value(name, value.handle)\n-            self.module.set_type(name, value.handle.type)\n+    def set_value(self, name: str,\n+                  value: Union[triton.language.tensor, triton.language.constexpr],\n+                  is_arg: bool = False) -> None:\n+        ''' This function:\n+          called by visit_Assign() & visit_FuncDef() to store left value (lvalue)\n+        1. record local defined name (FIXME: should consider control flow)\n+        2. store tensor in self.lvalue\n+        '''\n         self.lscope[name] = value\n-\n-    def is_triton_object(self, value):\n-        return isinstance(value, triton.language.block)\n-\n+        # if this value is an argument, we don't need to create phis for it\n+        self.is_arg_lscope[name] = is_arg\n+        if isinstance(value, triton.language.tensor) and not is_arg:\n+            self._set_value(name, self.builder.get_insert_block(), value)\n+\n+    #\n+    # SSA-construction\n+    #\n+    def _get_tensor(self, name: str, bb: Optional[_triton.ir.basic_block] = None) -> triton.language.tensor:\n+        if not bb:\n+            bb = self.builder.get_insert_block()\n+        # local value numbering\n+        if (name, bb) in self.lvalues:\n+            return self.lvalues[(name, bb)]\n+        # global value numbering\n+        saved_insert_point = self.builder.get_insert_point()\n+        result = self._get_tensor_recursive(name, bb)\n+        self.builder.set_insert_point(saved_insert_point)\n+        return result\n+\n+    def _get_tensor_recursive(self, name: str, bb: _triton.ir.basic_block) -> triton.language.tensor:\n+        preds = bb.get_predecessors()\n+        type = self.lscope[name].type\n+        # some preds haven't been filled, create a phi as a proxy of the value\n+        if bb not in self.sealed_blocks:\n+            result = self._make_phi(type, len(preds), bb)\n+            if bb in self.incomplete_phis:\n+                self.incomplete_phis[bb][name] = result\n+            else:\n+                self.incomplete_phis[bb] = {name: result}\n+        elif len(preds) == 1:\n+            # one predecessor: no phi needed, try get value from pred\n+            result = self._get_tensor(name, preds[0])\n+        else:  # multiple preds\n+            assert len(preds) > 1, f'{name} is an undefined name (cannot find in the entry block)'\n+            phi = self._make_phi(type, len(preds), bb)\n+            self._set_value(name, bb, phi)\n+            result = self._add_phi_operands(name, phi)\n+        self._set_value(name, bb, result)\n+        return result\n+\n+    # returns a new phi tensor, which encausulate an ir.phi_node\n+    def _make_phi(self,\n+                  type: triton.language.dtype,\n+                  num_values: int,\n+                  bb: _triton.ir.basic_block) -> triton.language.tensor:\n+        instr = bb.get_first_non_phi()\n+        self.builder.set_insert_point((bb, instr))\n+        ir_phi = self.builder.create_phi(type.to_ir(self.builder), num_values)\n+        if instr:\n+            self.builder.set_insert_block(bb)\n+        return triton.language.tensor(ir_phi, type)\n+\n+    # complete a phi node. (TODO: rename this as _complete_phis?)\n+    # Note: since we try to remove tryival phi, the return tensor might not be a phi\n+    def _add_phi_operands(self, name: str,\n+                          phi: triton.language.tensor) -> triton.language.tensor:\n+        bb = phi.handle.get_parent()\n+        for pred in bb.get_predecessors():\n+            v = self._get_tensor(name, pred)\n+            phi.handle.add_incoming(v.handle, pred)\n+        phi = self._try_remove_trivial_phi(phi)\n+        return phi\n+\n+    def _set_value(self, name: str, bb: _triton.ir.basic_block, value: triton.language.tensor) -> None:\n+        self.lvalues[(name, bb)] = value\n+        # TODO: why we need this?\n+        self.module.set_instr_metadata(name, value.handle)\n+\n+    def _seal_block(self, bb: _triton.ir.basic_block):\n+        # complete all incomplete phis\n+        if bb in self.incomplete_phis:\n+            for name, phi in self.incomplete_phis[bb].items():\n+                result = self._add_phi_operands(name, phi)\n+                # it's possible that this phi is trivial\n+                if self._get_tensor(name, bb).handle == phi.handle:\n+                    self._set_value(name, bb, result)\n+            del self.incomplete_phis[bb]\n+        self.sealed_blocks.add(bb)\n+\n+    def _try_remove_trivial_phi(self, phi: triton.language.tensor) -> triton.language.tensor:\n+        unique_handles = {op for op in phi.handle.ops() if op != phi.handle}\n+        if len(unique_handles) != 1:  # non-trivial phi\n+            return phi\n+        v = unique_handles.pop()\n+        phi.handle.replace_all_uses_with(v)\n+        phi.handle.erase_from_parent()\n+        # TODO: remove trivial phis recursively\n+        return triton.language.tensor(v, phi.type)\n+\n+    def is_triton_tensor(self, value):\n+        return isinstance(value, triton.language.tensor)\n+\n+    #\n+    # AST visitor\n+    #\n     def visit_compound_statement(self, stmts):\n         for stmt in stmts:\n             self.last_ret = self.visit(stmt)\n             if isinstance(stmt, ast.Return):\n                 break\n         return stmts and isinstance(stmt, ast.Return)\n \n-    def __init__(self, context, prototype, gscope, attributes, constants, kwargs):\n-        self.builder = _triton.ir.builder(context)\n-        self.module = _triton.ir.module('', self.builder)\n-        self.prototype = prototype\n-        self.gscope = gscope\n-        self.lscope = dict()\n-        self.attributes = attributes\n-        self.constants = constants\n-        self.kwargs = kwargs\n-        self.last_node = None\n-        self.builtins = {\n-            'range': range,\n-            'min': triton.language.minimum,\n-            'float': float,\n-            'int': int,\n-            'print': print,\n-            'isinstance': isinstance,\n-            'getattr': getattr,\n-        }\n-\n     def visit_Module(self, node):\n         ast.NodeVisitor.generic_visit(self, node)\n \n@@ -113,7 +220,7 @@ def visit_FunctionDef(self, node, inline=False, arg_values=None):\n         if inline:\n             pass\n         else:\n-            fn = self.module.get_or_insert_function(node.name, self.prototype)\n+            fn = self.module.get_or_insert_function(node.name, self.prototype.to_ir(self.builder))\n             arg_values = []\n             idx = 0\n             for i, arg_name in enumerate(arg_names):\n@@ -130,17 +237,17 @@ def visit_FunctionDef(self, node, inline=False, arg_values=None):\n                         attr = _triton.ir.attribute(attr, self.attributes[i])\n                         fn.add_attr(idx + 1, attr)\n                     fn.args[idx].name = arg_name\n-                    arg_values.append(fn.args[idx])\n+                    arg_values.append(triton.language.tensor(fn.args[idx], self.prototype.param_types[idx]))\n                     idx += 1\n \n         for arg_name, arg_value in zip(arg_names, arg_values):\n-            self.set_value(arg_name, arg_value)\n+            self.set_value(arg_name, arg_value, is_arg=True)\n         if inline:\n             self.visit_compound_statement(node.body)\n             return self.last_ret\n         else:\n             entry = _triton.ir.basic_block.create(self.builder.context, \"entry\", fn)\n-            self.module.seal_block(entry)\n+            self._seal_block(entry)\n             self.builder.set_insert_block(entry)\n             # visit function body\n             self.visit_compound_statement(node.body)\n@@ -187,11 +294,12 @@ def visit_Assign(self, node):\n         if not isinstance(values, tuple):\n             values = [values]\n         for name, value in zip(names, values):\n+            # TODO: can we store constexpr here to support constant folding?\n             # by default, constexpr are assigned into python variable\n             if isinstance(value, triton.language.constexpr):\n                 value = value.value\n-            if not isinstance(value, triton.language.block):\n-                value = triton.language.core._to_ir(value, self.builder)\n+            if not isinstance(value, triton.language.tensor):\n+                value = triton.language.core._to_tensor(value, self.builder)\n             self.set_value(name, value)\n \n     def visit_AugAssign(self, node):\n@@ -220,9 +328,9 @@ def visit_Tuple(self, node):\n     def visit_BinOp(self, node):\n         lhs = self.visit(node.left)\n         rhs = self.visit(node.right)\n-        if isinstance(lhs, triton.language.core.constexpr):\n+        if isinstance(lhs, triton.language.constexpr):\n             lhs = lhs.value\n-        if isinstance(rhs, triton.language.core.constexpr):\n+        if isinstance(rhs, triton.language.constexpr):\n             rhs = rhs.value\n         fn = {\n             ast.Add: '__add__',\n@@ -238,25 +346,25 @@ def visit_BinOp(self, node):\n             ast.BitOr: '__or__',\n             ast.BitXor: '__xor__',\n         }[type(node.op)]\n-        if self.is_triton_object(lhs):\n+        if self.is_triton_tensor(lhs):\n             return getattr(lhs, fn)(rhs, _builder=self.builder)\n-        elif self.is_triton_object(rhs):\n+        elif self.is_triton_tensor(rhs):\n             fn = fn[:2] + 'r' + fn[2:]\n             return getattr(rhs, fn)(lhs, _builder=self.builder)\n         else:\n             return getattr(lhs, fn)(rhs)\n \n     def visit_If(self, node):\n         cond = self.visit(node.test)\n-        if isinstance(cond, triton.language.block):\n+        if isinstance(cond, triton.language.tensor):\n             cond = cond.to(triton.language.int1, _builder=self.builder)\n             current_bb = self.builder.get_insert_block()\n             then_bb = _triton.ir.basic_block.create(self.builder.context, \"then\", current_bb.parent)\n             else_bb = _triton.ir.basic_block.create(self.builder.context, \"else\", current_bb.parent) if node.orelse else None\n             endif_bb = _triton.ir.basic_block.create(self.builder.context, \"endif\", current_bb.parent)\n-            self.module.seal_block(then_bb)\n+            self._seal_block(then_bb)\n             if else_bb:\n-                self.module.seal_block(else_bb)\n+                self._seal_block(else_bb)\n                 self.builder.cond_br(cond.handle, then_bb, else_bb)\n             else:\n                 self.builder.cond_br(cond.handle, then_bb, endif_bb)\n@@ -271,7 +379,7 @@ def visit_If(self, node):\n                 # TODO: last statement is a terminator?\n                 if not is_terminator:\n                     self.builder.br(endif_bb)\n-            self.module.seal_block(endif_bb)\n+            self._seal_block(endif_bb)\n             self.builder.set_insert_block(endif_bb)\n         else:\n             if isinstance(cond, triton.language.constexpr):\n@@ -296,9 +404,9 @@ def visit_Compare(self, node):\n         assert len(node.ops) == 1\n         lhs = self.visit(node.left)\n         rhs = self.visit(node.comparators[0])\n-        if isinstance(lhs, triton.language.core.constexpr):\n+        if isinstance(lhs, triton.language.constexpr):\n             lhs = lhs.value\n-        if isinstance(rhs, triton.language.core.constexpr):\n+        if isinstance(rhs, triton.language.constexpr):\n             rhs = rhs.value\n         if type(node.ops[0]) == ast.Is:\n             return triton.language.constexpr(lhs is rhs)\n@@ -312,9 +420,9 @@ def visit_Compare(self, node):\n             ast.Gt: '__gt__',\n             ast.GtE: '__ge__',\n         }[type(node.ops[0])]\n-        if self.is_triton_object(lhs):\n+        if self.is_triton_tensor(lhs):\n             return getattr(lhs, fn)(rhs, _builder=self.builder)\n-        elif self.is_triton_object(rhs):\n+        elif self.is_triton_tensor(rhs):\n             fn = fn[:2] + 'r' + fn[2:]\n             return getattr(rhs, fn)(lhs, _builder=self.builder)\n         else:\n@@ -325,21 +433,21 @@ def visit_UnaryOp(self, node):\n         if type(node.op) == ast.Not:\n             assert isinstance(op, triton.language.constexpr), \"`not` only supported for constexpr at the moment\"\n             return triton.language.constexpr(not op)\n-        if isinstance(op, triton.language.core.constexpr):\n+        if isinstance(op, triton.language.constexpr):\n             op = op.value\n         fn = {\n             ast.USub: '__neg__',\n             ast.UAdd: '__pos__',\n             ast.Invert: '__invert__',\n         }[type(node.op)]\n-        if self.is_triton_object(op):\n+        if self.is_triton_tensor(op):\n             return getattr(op, fn)(_builder=self.builder)\n         return getattr(op, fn)()\n \n     def visit_While(self, node):\n         current_bb = self.builder.get_insert_block()\n-        loop_bb = _triton.ir.basic_block.create(self.module.builder.context, \"loop\", current_bb.parent)\n-        next_bb = _triton.ir.basic_block.create(self.module.builder.context, \"postloop\", current_bb.parent)\n+        loop_bb = _triton.ir.basic_block.create(self.builder.context, \"loop\", current_bb.parent)\n+        next_bb = _triton.ir.basic_block.create(self.builder.context, \"postloop\", current_bb.parent)\n \n         def continue_fn():\n             cond = self.visit(node.test)\n@@ -350,9 +458,9 @@ def continue_fn():\n         self.visit_compound_statement(node.body)\n         continue_fn()\n         stop_bb = self.builder.get_insert_block()\n-        self.module.seal_block(stop_bb)\n-        self.module.seal_block(loop_bb)\n-        self.module.seal_block(next_bb)\n+        self._seal_block(stop_bb)\n+        self._seal_block(loop_bb)\n+        self._seal_block(next_bb)\n         self.builder.set_insert_block(next_bb)\n \n         for stmt in node.orelse:\n@@ -362,7 +470,7 @@ def visit_Subscript(self, node):\n         assert node.ctx.__class__.__name__ == \"Load\"\n         lhs = self.visit(node.value)\n         slices = self.visit(node.slice)\n-        if self.is_triton_object(lhs):\n+        if self.is_triton_tensor(lhs):\n             return lhs.__getitem__(slices, _builder=self.builder)\n         return lhs[slices]\n \n@@ -405,8 +513,8 @@ def visit_For(self, node):\n         step_node = ast.AugAssign(target=st_target, op=ast.Add(), value=arg_2)\n         # code generation\n         current_bb = self.builder.get_insert_block()\n-        loop_bb = _triton.ir.basic_block.create(self.module.builder.context, \"loop\", current_bb.parent)\n-        next_bb = _triton.ir.basic_block.create(self.module.builder.context, \"postloop\", current_bb.parent)\n+        loop_bb = _triton.ir.basic_block.create(self.builder.context, \"loop\", current_bb.parent)\n+        next_bb = _triton.ir.basic_block.create(self.builder.context, \"postloop\", current_bb.parent)\n \n         def continue_fn():\n             self.visit(step_node)\n@@ -421,9 +529,9 @@ def continue_fn():\n         # TODO: handle case where body breaks control flow\n         continue_fn()\n         stop_bb = self.builder.get_insert_block()\n-        self.module.seal_block(stop_bb)\n-        self.module.seal_block(loop_bb)\n-        self.module.seal_block(next_bb)\n+        self._seal_block(stop_bb)\n+        self._seal_block(loop_bb)\n+        self._seal_block(next_bb)\n         self.builder.set_insert_block(next_bb)\n \n         for stmt in node.orelse:\n@@ -451,7 +559,7 @@ def visit_Call(self, node):\n         args = [self.visit(arg) for arg in node.args]\n         if isinstance(fn, JITFunction):\n             return fn(*args, generator=self, **kws)\n-        if hasattr(fn, '__self__') and self.is_triton_object(fn.__self__) or \\\n+        if hasattr(fn, '__self__') and self.is_triton_tensor(fn.__self__) or \\\n                 sys.modules[fn.__module__] is triton.language.core:\n             return fn(*args, _builder=self.builder, **kws)\n         if fn in self.builtins.values():\n@@ -591,7 +699,7 @@ def _type_name(obj):\n         }\n         if hasattr(obj, 'data_ptr'):\n             return type_names[obj.dtype]\n-        if isinstance(obj, triton.language.core.constexpr):\n+        if isinstance(obj, triton.language.constexpr):\n             obj = obj.value\n         if isinstance(obj, int):\n             if -2**31 <= obj < 2**31:\n@@ -613,35 +721,44 @@ def _type_name(obj):\n         raise NotImplementedError(f'could not compute type name for {obj}')\n \n     @staticmethod\n-    def _to_triton_ir(context, obj):\n-        type_map = {\n-            'I': _triton.ir.type.get_int32,\n-            'L': _triton.ir.type.get_int64,\n-            'f': _triton.ir.type.get_fp32,\n-            'B': _triton.ir.type.get_int1,\n-            'f8': _triton.ir.type.get_fp8,\n-            'f16': _triton.ir.type.get_fp16,\n-            'bf16': _triton.ir.type.get_bf16,\n-            'f32': _triton.ir.type.get_fp32,\n-            'f64': _triton.ir.type.get_fp64,\n-            'i1': _triton.ir.type.get_int1,\n-            'i8': _triton.ir.type.get_int8,\n-            'i16': _triton.ir.type.get_int16,\n-            'i32': _triton.ir.type.get_int32,\n-            'i64': _triton.ir.type.get_int64,\n-            'u8': _triton.ir.type.get_uint8,\n-            'u16': _triton.ir.type.get_uint16,\n-            'u32': _triton.ir.type.get_uint32,\n-            'u64': _triton.ir.type.get_uint64,\n-        }\n+    def _to_python_ir(obj):\n         # convert torch.Tensor to Triton IR pointers\n         if hasattr(obj, 'data_ptr'):\n             name = Kernel._type_name(obj)\n-            elt_ty = type_map[name](context)\n-            return _triton.ir.type.make_ptr(elt_ty, 1)\n+            return 'ptr', name\n         # default path returns triton.ir.type directly\n         name = Kernel._type_name(obj)\n-        return type_map[name](context)\n+        return 'scalar', name\n+\n+    @staticmethod\n+    def _to_triton_ir(obj):\n+        which, name = obj\n+        type_map = {\n+            'I': triton.language.int32,\n+            'L': triton.language.int64,\n+            'f': triton.language.float32,\n+            'B': triton.language.int1,\n+            'f8': triton.language.float8,\n+            'f16': triton.language.float16,\n+            'bf16': triton.language.bfloat16,\n+            'f32': triton.language.float32,\n+            'f64': triton.language.float64,\n+            'i1': triton.language.int1,\n+            'i8': triton.language.int8,\n+            'i16': triton.language.int16,\n+            'i32': triton.language.int32,\n+            'i64': triton.language.int64,\n+            'u8': triton.language.uint8,\n+            'u16': triton.language.uint16,\n+            'u32': triton.language.uint32,\n+            'u64': triton.language.uint64,\n+        }\n+        # convert torch.Tensor to Triton IR pointers\n+        if which == 'ptr':\n+            elt_ty = type_map[name]\n+            return triton.language.pointer_type(elt_ty, 1)\n+        # default path returns triton.ir.type directly\n+        return type_map[name]\n \n     @staticmethod\n     def pow2_divisor(N):\n@@ -658,36 +775,6 @@ def pow2_divisor(N):\n     def __init__(self, fn):\n         self.fn = fn\n \n-    def _compile(self, *wargs, device, attributes, constants, num_warps, num_stages):\n-        # create IR module\n-        context = _triton.ir.context()\n-        # get just-in-time proto-type of kernel\n-        fn_args = [arg for i, arg in enumerate(wargs) if i not in constants]\n-        arg_types = [Kernel._to_triton_ir(context, arg) for arg in fn_args]\n-        ret_type = _triton.ir.type.get_void(context)\n-        prototype = _triton.ir.type.make_function(ret_type, arg_types)\n-        # generate Triton-IR\n-        # export symbols visible from self.fn into code-generator object\n-        gscope = self.fn.__globals__\n-        generator = CodeGenerator(context, prototype, gscope=gscope, attributes=attributes, constants=constants, kwargs=dict())\n-        try:\n-            generator.visit(self.fn.parse())\n-        except Exception as e:\n-            node = generator.last_node\n-            if node is None or isinstance(e, (NotImplementedError, CompilationError)):\n-                raise e\n-            raise CompilationError(self.fn.src, node) from e\n-        # Compile to machine code\n-        if torch.version.hip is None:\n-            backend = _triton.runtime.backend.CUDA\n-        else:\n-            backend = _triton.runtime.backend.ROCM\n-        name, asm, shared_mem = _triton.code_gen.compile_ttir(backend, generator.module, device, num_warps, num_stages)\n-        max_shared_memory = _triton.runtime.max_shared_memory(backend, device)\n-        if shared_mem > max_shared_memory:\n-            raise OutOfResources(shared_mem, max_shared_memory, \"shared memory\")\n-        return Binary(backend, name, asm, shared_mem, num_warps)\n-\n     def add_to_cache(self, key, wargs, device_idx, num_warps, num_stages):\n         tensor_idxs = [i for i, arg in enumerate(wargs) if hasattr(arg, 'data_ptr')]\n         # attributes\n@@ -702,57 +789,12 @@ def add_to_cache(self, key, wargs, device_idx, num_warps, num_stages):\n                 range_size = _triton.runtime.get_pointer_range_size(addr)\n                 attributes[i] = min(Kernel.pow2_divisor(addr),\n                                     Kernel.pow2_divisor(range_size))\n-\n         # transforms ints whose value is one into constants for just-in-time compilation\n         constants = {i: arg for i, arg in enumerate(wargs) if isinstance(arg, int) and arg == 1 and i not in self.fn.do_not_specialize}\n         constants.update({i: arg.value for i, arg in enumerate(wargs) if isinstance(arg, triton.language.constexpr)})\n         constants.update({i: None for i, arg in enumerate(wargs) if arg is None})\n-        hashed_key = hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n-\n-        # create cache directory\n-        cache_dir = os.environ.get('TRITON_CACHE_DIR', '/tmp/triton/')\n-        if cache_dir and not os.path.exists(cache_dir):\n-            os.makedirs(cache_dir, exist_ok=True)\n-\n-        if cache_dir:\n-            bin_cache_path = os.path.join(cache_dir, hashed_key)\n-            bin_lock_path = bin_cache_path + \".lock\"\n-        else:\n-            bin_cache_path = None\n-            bin_lock_path = None\n-\n-        binary = None\n-        if bin_cache_path and os.path.exists(bin_cache_path):\n-            assert bin_lock_path is not None\n-            with FileLock(bin_lock_path):\n-                with open(bin_cache_path, 'rb') as f:\n-                    binary = pickle.load(f)[\"binary\"]\n-        if binary is None:\n-            binary = self._compile(\n-                *wargs, device=device_idx, attributes=attributes,\n-                num_warps=num_warps, num_stages=num_stages,\n-                constants=constants,\n-            )\n-            if bin_cache_path:\n-                assert bin_lock_path is not None\n-                with FileLock(bin_lock_path):\n-                    with open(bin_cache_path + \".tmp\", \"wb\") as f:\n-                        pickle.dump({\"binary\": binary, \"key\": key}, f)\n-                    os.rename(bin_cache_path + \".tmp\", bin_cache_path)\n-                    if JITFunction.cache_hook is not None:\n-                        name = self.fn.__name__\n-                        info = key.split('-')[-3:]\n-                        num_warps, num_stages, sig = info[0], info[1], info[2].split('_')[1:]\n-                        # make signature human-readable\n-                        arg_reprs = []\n-                        for arg_name, arg_sig in zip(self.fn.arg_names, sig):\n-                            arg_reprs.append(f'{arg_name}: {arg_sig}')\n-                        # assemble the repr\n-                        arg_reprs = \", \".join(arg_reprs)\n-                        repr = f\"{name}[num_warps={num_warps}, num_stages={num_stages}]({arg_reprs})\"\n-                        JITFunction.cache_hook(key=key, binary=binary, repr=repr)\n-\n-        self.fn.bin_cache[key] = LoadedBinary(device_idx, binary)\n+        arg_types = [Kernel._to_python_ir(arg) for i, arg in enumerate(wargs) if i not in constants]\n+        return self.fn._warmup(key, arg_types=arg_types, device=device_idx, attributes=attributes, constants=constants, num_warps=num_warps, num_stages=num_stages, is_manual_warmup=False)\n \n     def __call__(self, *wargs, grid, num_warps=4, num_stages=2, **kwargs):\n         # handle arguments passed by name\n@@ -765,6 +807,10 @@ def __call__(self, *wargs, grid, num_warps=4, num_stages=2, **kwargs):\n         # handle annotations\n         for pos, _type in self.fn.annotations.items():\n             wargs[pos] = _type(wargs[pos])\n+        # check that tensors are on GPU.\n+        for arg in wargs:\n+            if hasattr(arg, 'data_ptr'):\n+                assert arg.is_cuda, \"All tensors must be on GPU!\"\n         # query device index and cuda stream\n         device = torch.cuda.current_device()\n         torch.cuda.set_device(device)\n@@ -992,25 +1038,31 @@ def parse(self):\n         assert isinstance(tree.body[0], ast.FunctionDef)\n         return tree\n \n+    # Called by CodeGenerator.visit_Call()\n     def __call__(self, *args, generator: CodeGenerator, **kwargs):\n         try:\n             from inspect import getcallargs\n             arg_values = getcallargs(self.fn, *args, **kwargs)\n             arg_values = [arg_values[name] for name in self.arg_names]\n-            arg_values = [arg if isinstance(arg, triton.language.block)\n+            arg_values = [arg if isinstance(arg, triton.language.tensor)\n                           else triton.language.constexpr(arg) for arg in arg_values]\n \n+            # Record values in the caller (parent scope)\n             gscope = generator.gscope.copy()\n             lscope = generator.lscope.copy()\n-            values = generator.module.get_values().copy()\n-            types = generator.module.get_types().copy()\n+\n+            # TODO: clear values other than args\n+            lvalues = generator.lvalues.copy()\n+            # types = generator.module.get_types().copy()\n             generator.gscope = sys.modules[self.fn.__module__].__dict__\n             generator.lscope = dict()\n             ret = generator.visit_FunctionDef(self.parse().body[0], inline=True, arg_values=arg_values)\n             generator.gscope = gscope\n             generator.lscope = lscope\n-            generator.module.set_values(values)\n-            generator.module.set_types(types)\n+\n+            generator.lvalues = lvalues\n+            # generator.module.set_types(types)\n+\n             return ret\n         except Exception as e:\n             node = generator.last_node\n@@ -1037,6 +1089,89 @@ def _init_kernel(self):\n                 self.kernel = decorator(self.kernel)\n         return self.kernel\n \n+    def warmup(self, compile):\n+        return self._warmup(**compile, is_manual_warmup=True)\n+\n+    def _warmup(self, key, arg_types, device, attributes, constants, num_warps, num_stages, is_manual_warmup):\n+        hashed_key = hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n+\n+        # create cache directory\n+        cache_dir = os.environ.get('TRITON_CACHE_DIR', '/tmp/triton/')\n+        if cache_dir:\n+            os.makedirs(cache_dir, exist_ok=True)\n+\n+        if cache_dir:\n+            bin_cache_path = os.path.join(cache_dir, hashed_key)\n+            bin_lock_path = bin_cache_path + \".lock\"\n+        else:\n+            bin_cache_path = None\n+            bin_lock_path = None\n+\n+        binary = None\n+        if bin_cache_path and os.path.exists(bin_cache_path):\n+            assert bin_lock_path is not None\n+            with FileLock(bin_lock_path):\n+                with open(bin_cache_path, 'rb') as f:\n+                    binary = pickle.load(f)[\"binary\"]\n+\n+        compile = dict(arg_types=arg_types, device=device, attributes=attributes, constants=constants, num_warps=num_warps, num_stages=num_stages)\n+        if JITFunction.cache_hook is not None:\n+            name = self.__name__\n+            info = key.split('-')[-3:]\n+            num_warps, num_stages, sig = info[0], info[1], info[2].split('_')[1:]\n+            # make signature human-readable\n+            arg_reprs = []\n+            for arg_name, arg_sig in zip(self.arg_names, sig):\n+                arg_reprs.append(f'{arg_name}: {arg_sig}')\n+            # assemble the repr\n+            arg_reprs = \", \".join(arg_reprs)\n+            repr = f\"{name}[num_warps={num_warps}, num_stages={num_stages}]({arg_reprs})\"\n+            noop = JITFunction.cache_hook(key=key, repr=repr, fn=self, compile={\"key\": key, **compile}, is_manual_warmup=is_manual_warmup, already_compiled=binary is not None)\n+            if noop:\n+                return True\n+\n+        if binary is None:\n+            binary = self._compile(**compile)\n+\n+        if bin_cache_path:\n+            assert bin_lock_path is not None\n+            with FileLock(bin_lock_path):\n+                with open(bin_cache_path + \".tmp\", \"wb\") as f:\n+                    pickle.dump({\"binary\": binary, \"key\": key}, f)\n+                os.rename(bin_cache_path + \".tmp\", bin_cache_path)\n+\n+        self.bin_cache[key] = LoadedBinary(device, binary)\n+        return False\n+\n+    def _compile(self, arg_types, device, attributes, constants, num_warps, num_stages):\n+        # create IR module\n+        context = _triton.ir.context()\n+        # get just-in-time proto-type of kernel\n+        arg_types = [Kernel._to_triton_ir(arg) for arg in arg_types]\n+        ret_type = triton.language.void\n+        prototype = triton.language.function_type(ret_type, arg_types)\n+        # generate Triton-IR\n+        # export symbols visible from self into code-generator object\n+        gscope = self.__globals__\n+        generator = CodeGenerator(context, prototype, gscope=gscope, attributes=attributes, constants=constants, kwargs=dict())\n+        try:\n+            generator.visit(self.parse())\n+        except Exception as e:\n+            node = generator.last_node\n+            if node is None or isinstance(e, (NotImplementedError, CompilationError)):\n+                raise e\n+            raise CompilationError(self.src, node) from e\n+        # Compile to machine code\n+        if torch.version.hip is None:\n+            backend = _triton.runtime.backend.CUDA\n+        else:\n+            backend = _triton.runtime.backend.ROCM\n+        name, asm, shared_mem = _triton.code_gen.compile_ttir(backend, generator.module, device, num_warps, num_stages)\n+        max_shared_memory = _triton.runtime.max_shared_memory(backend, device)\n+        if shared_mem > max_shared_memory:\n+            raise OutOfResources(shared_mem, max_shared_memory, \"shared memory\")\n+        return Binary(backend, name, asm, shared_mem, num_warps)\n+\n     def __getitem__(self, grid):\n         return Launcher(self._init_kernel(), grid)\n "}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 426, "deletions": 182, "changes": 608, "file_content_changes": "@@ -1,63 +1,36 @@\n+from __future__ import annotations\n+\n+from enum import Enum\n from functools import wraps\n+from typing import List\n \n import triton\n-from triton._C.libtriton.triton import frontend, ir\n+from . import semantic\n+from triton._C.libtriton.triton import ir\n \n \n-# convert block/dtype to ir values\n-def _to_ir(x, builder):\n+def _to_tensor(x, builder):\n     if isinstance(x, bool):\n-        return builder.get_int1(x)\n+        return tensor(builder.get_int1(x), int1)\n+    # Note: compile-time const integers are represented by unsigned values\n     elif isinstance(x, int):\n         if -2**31 <= x < 2**31:\n-            return builder.get_int32(x)\n+            return tensor(builder.get_int32(x), int32)\n         elif 2**31 <= x < 2**32:\n-            return builder.get_uint32(x)\n+            return tensor(builder.get_uint32(x), uint32)\n         elif -2**63 <= x < 2**63:\n-            return builder.get_int64(x)\n+            return tensor(builder.get_int64(x), int64)\n         elif 2**63 <= x < 2**64:\n-            return builder.get_uint64(x)\n+            return tensor(builder.get_uint64(x), uint64)\n         else:\n             raise RuntimeError(f'Nonrepresentable integer {x}.')\n     elif isinstance(x, float):\n-        return builder.get_float32(x)\n+        return tensor(builder.get_float32(x), float32)\n     elif isinstance(x, constexpr):\n-        return _to_ir(x.value, builder)\n-    elif isinstance(x, block):\n-        return x.handle\n-    elif isinstance(x, dtype):\n-        return x.handle(builder)\n-    return x\n-\n-\n-def _patch(fn):\n-    def _from_ir(x):\n-        if isinstance(x, ir.value):\n-            if x.type.is_void():\n-                return None\n-            return block(x)\n+        return _to_tensor(x.value, builder)\n+    elif isinstance(x, tensor):\n         return x\n-\n-    def wrapper(*args, **kwargs):\n-        builder = args[-1]\n-        assert isinstance(builder, ir.builder)\n-        args = [_to_ir(x, builder) for x in args]\n-        # for i, arg in enumerate(args):\n-        #     if arg is None:\n-        #         raise ValueError(f\"Unexpected `None` at position {i} for function {fn.__name__}\")\n-        kwargs = {k: _to_ir(v, builder) for k, v in kwargs.items()}\n-        ret = fn(*args, **kwargs)\n-        if isinstance(ret, tuple):\n-            return map(_from_ir, ret)\n-        return _from_ir(ret)\n-\n-    return wrapper\n-\n-\n-for name in dir(frontend):\n-    fn = getattr(frontend, name)\n-    if callable(fn):\n-        setattr(frontend, name, _patch(fn))\n+    assert False, f'cannot convert {x} to tensor'\n \n \n def builtin(fn):\n@@ -72,20 +45,147 @@ def wrapper(*args, **kwargs):\n \n \n class dtype:\n-    def __init__(self, init):\n-        self.init = init\n+    SINT_TYPES = ['int1', 'int8', 'int16', 'int32', 'int64']\n+    UINT_TYPES = ['uint8', 'uint16', 'uint32', 'uint64']\n+    FP_TYPES = ['fp8', 'fp16', 'bf16', 'fp32', 'fp64']\n+    OTHER_TYPES = ['void']\n+\n+    class SIGNEDNESS(Enum):\n+        SIGNED = 0\n+        UNSIGNED = 1\n+\n+    def __init__(self, name):\n+        self.name = name\n+        assert name in dtype.SINT_TYPES + dtype.UINT_TYPES + dtype.FP_TYPES + dtype.OTHER_TYPES, name\n+        if name in dtype.SINT_TYPES:\n+            self.int_signedness = dtype.SIGNEDNESS.SIGNED\n+            self.int_bitwidth = int(name.split('int')[-1])\n+            self.primitive_bitwidth = self.int_bitwidth\n+        elif name in dtype.UINT_TYPES:\n+            self.int_signedness = dtype.SIGNEDNESS.UNSIGNED\n+            self.int_bitwidth = int(name.split('int')[-1])\n+            self.primitive_bitwidth = self.int_bitwidth\n+        elif name in dtype.FP_TYPES:\n+            if name == 'fp8':\n+                self.fp_mantissa_width = 3\n+                self.primitive_bitwidth = 8\n+            elif name == 'fp16':\n+                self.fp_mantissa_width = 10\n+                self.primitive_bitwidth = 16\n+            elif name == 'bf16':\n+                self.fp_mantissa_width = 7\n+                self.primitive_bitwidth = 16\n+            elif name == 'fp32':\n+                self.fp_mantissa_width = 23\n+                self.primitive_bitwidth = 32\n+            elif name == 'fp64':\n+                self.fp_mantissa_width = 53\n+                self.primitive_bitwidth = 64\n+        elif name == 'void':\n+            self.primitive_bitwidth = 0\n+\n+    def is_fp8(self):\n+        return self.name == 'fp8'\n+\n+    def is_fp16(self):\n+        return self.name == 'fp16'\n+\n+    def is_bf16(self):\n+        return self.name == 'bf16'\n+\n+    def is_fp32(self):\n+        return self.name == 'fp32'\n+\n+    def is_fp64(self):\n+        return self.name == 'fp64'\n+\n+    def is_int1(self):\n+        return self.name == 'int1'\n+\n+    def is_int8(self):\n+        return self.name == 'int8'\n+\n+    def is_int16(self):\n+        return self.name == 'int16'\n+\n+    def is_int32(self):\n+        return self.name == 'int32'\n+\n+    def is_int64(self):\n+        return self.name == 'int64'\n+\n+    def is_uint8(self):\n+        return self.name == 'uint8'\n+\n+    def is_uint16(self):\n+        return self.name == 'uint16'\n+\n+    def is_uint32(self):\n+        return self.name == 'uint32'\n+\n+    def is_uint64(self):\n+        return self.name == 'uint64'\n+\n+    def is_floating(self):\n+        return self.name in dtype.FP_TYPES\n+\n+    def is_int_signed(self):\n+        return self.name in dtype.SINT_TYPES\n+\n+    def is_int(self):\n+        return self.name in dtype.SINT_TYPES + dtype.UINT_TYPES\n+\n+    def is_bool(self):\n+        return self.is_int1()\n+\n+    def is_void(self):\n+        raise RuntimeError(\"Not implemented\")\n+\n+    def is_block(self):\n+        return False\n+\n+    def is_ptr(self):\n+        return False\n+\n+    def __eq__(self, other: dtype):\n+        if not isinstance(other, dtype):\n+            return False\n+        return self.name == other.name\n \n-    @property\n-    def name(self) -> str:\n-        # The init functions are named something like 'get_int8'. Strip the prefix.\n-        nom = self.init.__name__\n-        prefix = 'get_'\n-        assert nom.startswith(prefix)\n-        return nom[len(prefix):]\n+    def __ne__(self, other: dtype):\n+        return not self.__eq__(other)\n+\n+    def __hash__(self):\n+        return hash((self.name,))\n \n-    def handle(self, builder):\n-        ctx = builder.context\n-        return self.init(ctx)\n+    @property\n+    def scalar(self):\n+        return self\n+\n+    def to_ir(self, builder: ir.builder) -> ir.type:\n+        if self.name == 'void':\n+            return builder.get_void_ty()\n+        elif self.name == 'int1':\n+            return builder.get_int1_ty()\n+        elif self.name == 'int8' or self.name == 'uint8':\n+            return builder.get_int8_ty()\n+        elif self.name == 'int16' or self.name == 'uint16':\n+            return builder.get_int16_ty()\n+        elif self.name == 'int32' or self.name == 'uint32':\n+            return builder.get_int32_ty()\n+        elif self.name == 'int64' or self.name == 'uint64':\n+            return builder.get_int64_ty()\n+        elif self.name == 'fp8':\n+            return builder.get_fp8_ty()\n+        elif self.name == 'fp16':\n+            return builder.get_half_ty()\n+        elif self.name == 'bf16':\n+            return builder.get_bf16_ty()\n+        elif self.name == 'fp32':\n+            return builder.get_float_ty()\n+        elif self.name == 'fp64':\n+            return builder.get_double_ty()\n+        raise ValueError(f'fail to covert {self} to ir type')\n \n     def __str__(self):\n         return self.name\n@@ -99,36 +199,112 @@ def __repr__(self):\n         return f'triton.language.{self.name}'\n \n \n-class pointer_dtype:\n-    def __init__(self, element_ty):\n+class pointer_type(dtype):\n+    def __init__(self, element_ty: dtype, address_space: int = 1):\n         if not isinstance(element_ty, dtype):\n             raise TypeError('element_ty is a {type(element_ty).__name__}.')\n         self.element_ty = element_ty\n+        self.address_space = address_space\n \n-    def handle(self, builder):\n-        return ir.type.make_ptr(self.element_ty.handle(builder), 1)\n+        self.name = self.__str__()\n+\n+    def to_ir(self, builder: ir.builder) -> ir.pointer_type:\n+        return ir.type.make_ptr(self.element_ty.to_ir(builder), 1)\n \n     def __str__(self):\n         return f'pointer<{self.element_ty}>'\n \n+    def __repr__(self):\n+        return self.__str__()\n+\n+    def is_ptr(self):\n+        return True\n+\n+    def __eq__(self, other: pointer_type) -> bool:\n+        if not isinstance(other, pointer_type):\n+            return False\n+        return self.element_ty == other.element_ty and self.address_space == other.address_space\n+\n+    def __ne__(self, other: pointer_type) -> bool:\n+        return not self.__eq__(other)\n+\n+    @property\n+    def scalar(self):\n+        return self\n+\n+\n+class block_type(dtype):\n+    def __init__(self, element_ty: dtype, shape: List[int]):\n+        self.element_ty = element_ty\n+        # FIXME:\n+        # block_type's shape is a list of int\n+        # while tensor's shape is a list of constexpr\n+        self.shape = shape\n+        self.numel = 1\n+        for s in self.shape:\n+            self.numel *= s\n+\n+        self.name = self.__str__()\n+\n+    def to_ir(self, builder: ir.builder) -> ir.block_type:\n+        return ir.type.make_block(self.element_ty.to_ir(builder), self.shape)\n+\n+    def __str__(self):\n+        return f'<{self.shape}, {self.element_ty}>'\n+\n+    def __repr__(self):\n+        return self.__str__()\n+\n+    def is_block(self):\n+        return True\n+\n+    def get_block_shapes(self) -> List[int]:\n+        return self.shape\n+\n+    def __eq__(self, other: block_type) -> bool:\n+        if not isinstance(other, block_type):\n+            return False\n+        return self.element_ty == other.element_ty and self.shape == other.shape\n+\n+    def __ne__(self, other: block_type) -> bool:\n+        return not self.__eq__(other)\n+\n+    @property\n+    def scalar(self):\n+        return self.element_ty\n+\n+\n+class function_type(dtype):\n+    def __init__(self, ret_type: dtype, param_types: List[dtype]) -> None:\n+        self.ret_type = ret_type\n+        self.param_types = param_types\n+\n+    def __str__(self):\n+        return f'fn ({self.param_types}) -> {self.ret_type}'\n+\n+    def to_ir(self, builder: ir.builder):\n+        ir_param_types = [ty.to_ir(builder) for ty in self.param_types]\n+        return ir.type.make_function(self.ret_type.to_ir(builder), ir_param_types)\n+\n \n # scalar types\n-int1 = dtype(ir.type.get_int1)\n-int8 = dtype(ir.type.get_int8)\n-int16 = dtype(ir.type.get_int16)\n-int32 = dtype(ir.type.get_int32)\n-int64 = dtype(ir.type.get_int64)\n-uint8 = dtype(ir.type.get_uint8)\n-uint16 = dtype(ir.type.get_uint16)\n-uint32 = dtype(ir.type.get_uint32)\n-uint64 = dtype(ir.type.get_uint64)\n-float8 = dtype(ir.type.get_fp8)\n-float16 = dtype(ir.type.get_fp16)\n-bfloat16 = dtype(ir.type.get_bf16)\n-float32 = dtype(ir.type.get_fp32)\n-float64 = dtype(ir.type.get_fp64)\n+void = dtype('void')\n+int1 = dtype('int1')\n+int8 = dtype('int8')\n+int16 = dtype('int16')\n+int32 = dtype('int32')\n+int64 = dtype('int64')\n+uint8 = dtype('uint8')\n+uint16 = dtype('uint16')\n+uint32 = dtype('uint32')\n+uint64 = dtype('uint64')\n+float8 = dtype('fp8')\n+float16 = dtype('fp16')\n+bfloat16 = dtype('bf16')\n+float32 = dtype('fp32')\n+float64 = dtype('fp64')\n # pointer types\n-pi32_t = pointer_dtype(int32)\n+pi32_t = pointer_type(int32)\n \n # -----------------------\n # constexpr\n@@ -149,7 +325,6 @@ def __init__(self, value):\n     def __repr__(self) -> str:\n         return f\"constexpr[{self.value}]\"\n \n-    #\n     def __add__(self, other):\n         return self.value + other.value\n \n@@ -219,31 +394,33 @@ def __call__(self, *args, **kwds):\n         return self.value(*args, **kwds)\n \n \n-class block:\n+class tensor:\n+    # infer dtype from ir type\n     @staticmethod\n-    def _init_dtype(ir_type):\n+    def _to_dtype(ir_type):\n+        # block type\n+        if ir_type.is_block():\n+            scalar_ty = tensor._to_dtype(ir_type.scalar)\n+            return block_type(scalar_ty, ir_type.get_block_shapes())\n+        # pointer type\n+        if ir_type.is_ptr():\n+            element_ty = tensor._to_dtype(ir_type.element)\n+            return pointer_type(element_ty)\n         # primitive type\n+        if ir_type.is_void(): return void\n         if ir_type.is_int1(): return int1\n         if ir_type.is_int8(): return int8\n         if ir_type.is_int16(): return int16\n         if ir_type.is_int32(): return int32\n         if ir_type.is_int64(): return int64\n-        if ir_type.is_uint8(): return uint8\n-        if ir_type.is_uint16(): return uint16\n-        if ir_type.is_uint32(): return uint32\n-        if ir_type.is_uint64(): return uint64\n         if ir_type.is_fp8(): return float8\n         if ir_type.is_fp16(): return float16\n         if ir_type.is_bf16(): return bfloat16\n         if ir_type.is_fp32(): return float32\n         if ir_type.is_fp64(): return float64\n-        # pointer type\n-        if ir_type.is_ptr():\n-            element_ty = block._init_dtype(ir_type.element)\n-            return pointer_dtype(element_ty)\n-        raise ValueError(f\"Unsupported type {ir_type}\")\n+        raise ValueError(f\"Unsupported type {ir_type.repr()}\")\n \n-    def __init__(self, handle):\n+    def __init__(self, handle, type: dtype):\n         # IR handle\n         self.handle = handle\n         # Block shape\n@@ -254,9 +431,9 @@ def __init__(self, handle):\n         for s in self.shape:\n             self.numel *= s\n         self.numel = constexpr(self.numel)\n-        # Data-type wrapper\n-        self.dtype = block._init_dtype(self.handle.type.scalar)\n-        # Shape is a constexpr\n+        self.type = type  # Tensor type (can be block_type)\n+        # Following the practice in pytorch, dtype is scalar type\n+        self.dtype = type.scalar\n         self.shape = [constexpr(s) for s in self.shape]\n \n     def __str__(self) -> str:\n@@ -265,116 +442,139 @@ def __str__(self) -> str:\n \n     @builtin\n     def __add__(self, other, _builder=None):\n-        return frontend.add(self, other, _builder)\n+        other = _to_tensor(other, _builder)\n+        return semantic.add(self, other, _builder)\n \n     def __radd__(self, other, _builder=None):\n         return self.__add__(other, _builder=_builder)\n \n     @builtin\n     def __sub__(self, other, _builder=None):\n-        return frontend.sub(self, other, _builder)\n+        other = _to_tensor(other, _builder)\n+        return semantic.sub(self, other, _builder)\n \n     def __rsub__(self, other, _builder=None):\n-        return frontend.sub(other, self, _builder)\n+        other = _to_tensor(other, _builder)\n+        return semantic.sub(other, self, _builder)\n \n     @builtin\n     def __mul__(self, other, _builder=None):\n-        return frontend.mul(self, other, _builder)\n+        other = _to_tensor(other, _builder)\n+        return semantic.mul(self, other, _builder)\n \n     def __rmul__(self, other, _builder=None):\n         return self.__mul__(other, _builder=_builder)\n \n     @builtin\n     def __truediv__(self, other, _builder=None):\n-        return frontend.truediv(self, other, _builder)\n+        other = _to_tensor(other, _builder)\n+        return semantic.truediv(self, other, _builder)\n \n     def __rtruediv__(self, other, _builder=None):\n-        return frontend.truediv(other, self, _builder)\n+        other = _to_tensor(other, _builder)\n+        return semantic.truediv(other, self, _builder)\n \n     @builtin\n     def __floordiv__(self, other, _builder=None):\n-        return frontend.floordiv(self, other, _builder)\n+        other = _to_tensor(other, _builder)\n+        return semantic.floordiv(self, other, _builder)\n \n     @builtin\n     def __mod__(self, other, _builder=None):\n-        return frontend.mod(self, other, _builder)\n+        other = _to_tensor(other, _builder)\n+        return semantic.mod(self, other, _builder)\n \n     # unary operators\n     @builtin\n     def __neg__(self, _builder=None):\n-        return frontend.minus(self, _builder)\n+        return semantic.minus(self, _builder)\n \n     @builtin\n     def __invert__(self, _builder=None):\n-        return frontend.invert(self, _builder)\n+        return semantic.invert(self, _builder)\n \n     # bitwise operators\n \n     @builtin\n     def __and__(self, other, _builder=None):\n-        return frontend.and_(self, other, _builder)\n+        other = _to_tensor(other, _builder)\n+        return semantic.and_(self, other, _builder)\n \n     @builtin\n     def __or__(self, other, _builder=None):\n-        return frontend.or_(self, other, _builder)\n+        other = _to_tensor(other, _builder)\n+        return semantic.or_(self, other, _builder)\n \n     @builtin\n     def __xor__(self, other, _builder=None):\n-        return frontend.xor_(self, other, _builder)\n+        other = _to_tensor(other, _builder)\n+        return semantic.xor_(self, other, _builder)\n \n     @builtin\n     def __lshift__(self, other, _builder=None):\n-        return frontend.shl(self, other, _builder)\n+        other = _to_tensor(other, _builder)\n+        return semantic.shl(self, other, _builder)\n \n     @builtin\n     def __rshift__(self, other, _builder=None):\n-        return frontend.lshr(self, other, _builder)\n+        other = _to_tensor(other, _builder)\n+        return semantic.lshr(self, other, _builder)\n \n     # comparison operators\n \n     # >\n     @builtin\n     def __gt__(self, other, _builder=None):\n-        return frontend.greater_than(self, other, _builder)\n+        other = _to_tensor(other, _builder)\n+        return semantic.greater_than(self, other, _builder)\n \n     @builtin\n     def __rgt__(self, other, _builder=None):\n-        return frontend.greater_than(other, self, _builder)\n+        other = _to_tensor(other, _builder)\n+        return semantic.greater_than(other, self, _builder)\n \n     # >=\n     @builtin\n     def __ge__(self, other, _builder=None):\n-        return frontend.greater_equal(self, other, _builder)\n+        other = _to_tensor(other, _builder)\n+        return semantic.greater_equal(self, other, _builder)\n \n+    @builtin\n     def __rge__(self, other, _builder=None):\n-        return frontend.greater_equal(other, self, _builder)\n+        other = _to_tensor(other, _builder)\n+        return semantic.greater_equal(other, self, _builder)\n \n     # <\n     @builtin\n     def __lt__(self, other, _builder=None):\n-        return frontend.less_than(self, other, _builder)\n+        other = _to_tensor(other, _builder)\n+        return semantic.less_than(self, other, _builder)\n \n     @builtin\n     def __rlt__(self, other, _builder=None):\n-        return frontend.less_than(other, self, _builder)\n+        return semantic.less_than(other, self, _builder)\n \n     # <=\n     @builtin\n     def __le__(self, other, _builder=None):\n-        return frontend.less_equal(self, other, _builder)\n+        other = _to_tensor(other, _builder)\n+        return semantic.less_equal(self, other, _builder)\n \n     @builtin\n     def __rle__(self, other, _builder=None):\n-        return frontend.less_equal(other, self, _builder)\n+        other = _to_tensor(other, _builder)\n+        return semantic.less_equal(other, self, _builder)\n \n     # ==\n     @builtin\n     def __eq__(self, other, _builder=None):\n-        return frontend.equal(self, other, _builder)\n+        other = _to_tensor(other, _builder)\n+        return semantic.equal(self, other, _builder)\n \n     @builtin\n     def __ne__(self, other, _builder=None):\n-        return frontend.not_equal(self, other, _builder)\n+        other = _to_tensor(other, _builder)\n+        return semantic.not_equal(self, other, _builder)\n \n     @builtin\n     def __getitem__(self, slices, _builder=None):\n@@ -389,20 +589,25 @@ def __getitem__(self, slices, _builder=None):\n             elif sl == slice(None, None, None):\n                 dst_shape.append(src_shape[curr].value)\n                 curr += 1\n-        ret = frontend.reshape(self, dst_shape, _builder)\n+        ret = semantic.reshape(self, dst_shape, _builder)\n         return ret\n \n     @builtin\n     def to(self, dtype, bitcast=False, _builder=None):\n-        dtype = dtype.handle(_builder)\n+        if isinstance(bitcast, constexpr):\n+            bitcast = bitcast.value\n         if bitcast:\n-            return frontend.bitcast(self, dtype, _builder)\n-        return frontend.cast(self, dtype, _builder)\n+            return semantic.bitcast(self, dtype, _builder)\n+        return semantic.cast(self, dtype, _builder)\n \n \n # -----------------------\n # SPMD Programming Model\n # -----------------------\n+def _constexpr_to_value(v):\n+    if isinstance(v, constexpr):\n+        return v.value\n+    return v\n \n \n @builtin\n@@ -414,13 +619,14 @@ def program_id(axis, _builder=None):\n     :type axis: int\n     \"\"\"\n     # if axis == -1:\n-    #     pid0 = frontend.program_id(0, _builder)\n-    #     pid1 = frontend.program_id(1, _builder)\n-    #     pid2 = frontend.program_id(2, _builder)\n-    #     npg0 = frontend.num_programs(0, _builder)\n-    #     npg1 = frontend.num_programs(0, _builder)\n+    #     pid0 = program_id(0, _builder)\n+    #     pid1 = program_id(1, _builder)\n+    #     pid2 = program_id(2, _builder)\n+    #     npg0 = num_programs(0, _builder)\n+    #     npg1 = num_programs(0, _builder)\n     #     return pid0 + pid1*npg0 + pid2*npg0*npg1\n-    return frontend.program_id(axis, _builder)\n+    axis = _constexpr_to_value(axis)\n+    return semantic.program_id(axis, _builder)\n \n \n @builtin\n@@ -431,7 +637,8 @@ def num_programs(axis, _builder=None):\n     :param axis: The axis of the 3D launch grid. Has to be either 0, 1 or 2.\n     :type axis: int\n     \"\"\"\n-    return frontend.num_programs(axis, _builder)\n+    axis = _constexpr_to_value(axis)\n+    return semantic.num_programs(axis, _builder)\n \n \n # -----------------------\n@@ -449,13 +656,15 @@ def arange(start, end, _builder=None):\n     :param stop: End of the interval. Must be a power of two >= start.\n     :type stop: int\n     \"\"\"\n-    return frontend.arange(start, end, _builder)\n+    start = _constexpr_to_value(start)\n+    end = _constexpr_to_value(end)\n+    return semantic.arange(start, end, _builder)\n \n \n @builtin\n def zeros(shape, dtype, _builder=None):\n     \"\"\"\n-    Returns a block filled with the scalar value 0 for the given :code:`shape` and :code:`dtype`.\n+    Returns a tensor filled with the scalar value 0 for the given :code:`shape` and :code:`dtype`.\n \n     :param shape: Shape of the new array, e.g., (8, 16) or (8, )\n     :type shape: tuple of ints\n@@ -468,7 +677,8 @@ def zeros(shape, dtype, _builder=None):\n         if not isinstance(d.value, int):\n             raise TypeError(f\"Shape element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n     shape = [x.value for x in shape]\n-    return frontend.zeros(shape, dtype, _builder)\n+    dtype = _constexpr_to_value(dtype)\n+    return semantic.zeros(shape, dtype, _builder)\n \n \n # -----------------------\n@@ -481,53 +691,53 @@ def broadcast(input, other, _builder=None):\n     \"\"\"\n     Tries to broadcast the two given blocks to a common compatible shape.\n \n-    :param input: The first input block.\n+    :param input: The first input tensor.\n     :type input: Block\n-    :param other: The second input block.\n+    :param other: The second input tensor.\n     :type other: Block\n     \"\"\"\n-    return frontend.broadcast(input, other, _builder)\n+    return semantic.broadcast_impl_value(input, other, _builder)\n \n \n @builtin\n def broadcast_to(input, shape, _builder=None):\n     \"\"\"\n-    Tries to broadcast the given block to a new :code:`shape`.\n+    Tries to broadcast the given tensor to a new :code:`shape`.\n \n-    :param input: The input block.\n+    :param input: The input tensor.\n     :type input: Block\n     :param shape: The desired shape.\n     :type shape: Tuple[int]\n     \"\"\"\n-    return frontend.broadcast_to(input, shape, _builder)\n+    return semantic.broadcast_impl_shape(input, shape, _builder)\n \n \n @builtin\n def cat(input, other, _builder=None):\n     \"\"\"\n     Concatenate the given blocks\n \n-    :param input: The first input block.\n+    :param input: The first input tensor.\n     :type input:\n-    :param other: The second input block.\n+    :param other: The second input tensor.\n     :type other:\n     \"\"\"\n-    return frontend.cat(input, other, _builder)\n+    return semantic.cat(input, other, _builder)\n \n \n @builtin\n def reshape(input, shape, _builder=None):\n     \"\"\"\n-    Tries to reshape the given block to a new shape.\n+    Tries to reshape the given tensor to a new shape.\n \n-    :param input: The input block.\n+    :param input: The input tensor.\n     :type input:\n     :param shape: The desired shape.\n     :type shape: Tuple[int]\n \n     \"\"\"\n     shape = [x.value for x in shape]\n-    return frontend.reshape(input, shape, _builder)\n+    return semantic.reshape(input, shape, _builder)\n \n \n # -----------------------\n@@ -542,12 +752,13 @@ def dot(input, other, allow_tf32=True, _builder=None):\n \n     The two blocks must be two dimensionals and have compatible inner dimensions.\n \n-    :param input: The first block to be multiplied.\n-    :type input: 2D block of scalar-type in {:code:`float16`, :code:`bfloat16`, :code:`float32`}\n-    :param other: The second block to be multiplied.\n-    :type other: 2D block of scalar-type in {:code:`float16`, :code:`bfloat16`, :code:`float32`}\n+    :param input: The first tensor to be multiplied.\n+    :type input: 2D tensor of scalar-type in {:code:`float16`, :code:`bfloat16`, :code:`float32`}\n+    :param other: The second tensor to be multiplied.\n+    :type other: 2D tensor of scalar-type in {:code:`float16`, :code:`bfloat16`, :code:`float32`}\n     \"\"\"\n-    return frontend.dot(input, other, allow_tf32, _builder)\n+    allow_tf32 = _constexpr_to_value(allow_tf32)\n+    return semantic.dot(input, other, allow_tf32, _builder)\n \n \n # -----------------------\n@@ -558,7 +769,7 @@ def dot(input, other, allow_tf32=True, _builder=None):\n @builtin\n def load(pointer, mask=None, other=None, cache_modifier=\"\", eviction_policy=\"\", volatile=False, _builder=None):\n     \"\"\"\n-    Return a block of data whose values are, elementwise, loaded from memory at location defined by :code:`pointer`.\n+    Return a tensor of data whose values are, elementwise, loaded from memory at location defined by :code:`pointer`.\n \n     :code:`mask` and :code:`other` are implicitly broadcast to :code:`pointer.shape`.\n \n@@ -573,24 +784,36 @@ def load(pointer, mask=None, other=None, cache_modifier=\"\", eviction_policy=\"\",\n     :param cache_modifier: changes cache option in nvidia ptx\n     'type cache_modifier: str, optional\n     \"\"\"\n-    return frontend.load(pointer, mask, other, cache_modifier, eviction_policy, volatile, _builder)\n+    # mask, other can be constexpr\n+    if mask is not None:\n+        mask = _to_tensor(mask, _builder)\n+    if other is not None:\n+        other = _to_tensor(other, _builder)\n+    cache_modifier = _constexpr_to_value(cache_modifier)\n+    eviction_policy = _constexpr_to_value(eviction_policy)\n+    volatile = _constexpr_to_value(volatile)\n+    return semantic.load(pointer, mask, other, cache_modifier, eviction_policy, volatile, _builder)\n \n \n @builtin\n def store(pointer, value, mask=None, _builder=None):\n     \"\"\"\n-    Stores :code:`value` block of elements in memory, element-wise, at the memory locations specified by :code:`pointer`.\n+    Stores :code:`value` tensor of elements in memory, element-wise, at the memory locations specified by :code:`pointer`.\n \n     :code:`value` is implicitly broadcast to :code:`pointer.shape` and typecast to :code:`pointer.dtype.element_ty`.\n \n     :param pointer: The memory locations where the elements of :code:`value` are stored.\n     :type pointer: Block of dtype=triton.PointerDType\n-    :param value: The block of elements to be stored.\n+    :param value: The tensor of elements to be stored.\n     :type value: Block\n     :param mask: If mask[idx] is false, do not store :code:`value[idx]` at :code:`pointer[idx]`.\n     :type mask: Block of triton.int1, optional\n     \"\"\"\n-    return frontend.store(pointer, value, mask, _builder)\n+    # value can be constexpr\n+    value = _to_tensor(value, _builder)\n+    if mask is not None:\n+        mask = _to_tensor(mask, _builder)\n+    return semantic.store(pointer, value, mask, _builder)\n \n \n # -----------------------\n@@ -621,49 +844,58 @@ def _decorator(func):\n @builtin\n @_add_atomic_docstr(\"compare-and-swap\")\n def atomic_cas(pointer, cmp, val, _builder=None):\n-    return frontend.atomic_cas(pointer, cmp, val, _builder)\n+    cmp = _to_tensor(cmp, _builder)\n+    val = _to_tensor(cmp, _builder)\n+    return semantic.atomic_cas(pointer, cmp, val, _builder)\n \n \n @builtin\n @_add_atomic_docstr(\"exchange\")\n def atomic_xchg(pointer, val, mask=None, _builder=None):\n-    return frontend.atomic_xchg(pointer, val, mask, _builder)\n+    val = _to_tensor(val, _builder)\n+    return semantic.atomic_xchg(pointer, val, mask, _builder)\n \n \n @builtin\n @_add_atomic_docstr(\"add\")\n def atomic_add(pointer, val, mask=None, _builder=None):\n-    return frontend.atomic_add(pointer, val, mask, _builder)\n+    val = _to_tensor(val, _builder)\n+    return semantic.atomic_add(pointer, val, mask, _builder)\n \n \n @builtin\n @_add_atomic_docstr(\"max\")\n def atomic_max(pointer, val, mask=None, _builder=None):\n-    return frontend.atomic_max(pointer, val, mask, _builder)\n+    val = _to_tensor(val, _builder)\n+    return semantic.atomic_max(pointer, val, mask, _builder)\n \n \n @builtin\n @_add_atomic_docstr(\"min\")\n def atomic_min(pointer, val, mask=None, _builder=None):\n-    return frontend.atomic_min(pointer, val, mask, _builder)\n+    val = _to_tensor(val, _builder)\n+    return semantic.atomic_min(pointer, val, mask, _builder)\n \n \n @builtin\n @_add_atomic_docstr(\"logical and\")\n def atomic_and(pointer, val, mask=None, _builder=None):\n-    return frontend.atomic_and(pointer, val, mask, _builder)\n+    val = _to_tensor(val, _builder)\n+    return semantic.atomic_and(pointer, val, mask, _builder)\n \n \n @builtin\n @_add_atomic_docstr(\"logical or\")\n def atomic_or(pointer, val, mask=None, _builder=None):\n-    return frontend.atomic_or(pointer, val, mask, _builder)\n+    val = _to_tensor(val, _builder)\n+    return semantic.atomic_or(pointer, val, mask, _builder)\n \n \n @builtin\n @_add_atomic_docstr(\"logical xor\")\n def atomic_xor(pointer, val, mask=None, _builder=None):\n-    return frontend.atomic_xor(pointer, val, mask, _builder)\n+    val = _to_tensor(val, _builder)\n+    return semantic.atomic_xor(pointer, val, mask, _builder)\n \n \n # -----------------------\n@@ -674,7 +906,7 @@ def atomic_xor(pointer, val, mask=None, _builder=None):\n @builtin\n def where(condition, x, y, _builder=None):\n     \"\"\"\n-    Returns a block of elements from either :code:`x` or :code:`y`, depending on :code:`condition`.\n+    Returns a tensor of elements from either :code:`x` or :code:`y`, depending on :code:`condition`.\n \n     Note that :code:`x` and :code:`y` are always evaluated regardless of the value of :code:`condition`.\n \n@@ -688,7 +920,10 @@ def where(condition, x, y, _builder=None):\n     :param x: values selected at indices where condition is True.\n     :param y: values selected at indices where condition is False.\n     \"\"\"\n-    return frontend.where(condition, x, y, _builder)\n+    condition = _to_tensor(condition, _builder)\n+    x = _to_tensor(x, _builder)\n+    y = _to_tensor(y, _builder)\n+    return semantic.where(condition, x, y, _builder)\n \n \n # -----------------------\n@@ -697,12 +932,15 @@ def where(condition, x, y, _builder=None):\n \n @builtin\n def umulhi(x, y, _builder=None):\n-    return frontend.umulhi(x, y, _builder)\n+    x = _to_tensor(x, _builder)\n+    y = _to_tensor(y, _builder)\n+    return semantic.umulhi(x, y, _builder)\n \n \n @builtin\n def fdiv(x, y, ieee_rounding=False, _builder=None):\n-    return frontend.fdiv(x, y, ieee_rounding, _builder)\n+    ieee_rounding = _constexpr_to_value(ieee_rounding)\n+    return semantic.fdiv(x, y, ieee_rounding, _builder)\n \n \n def _add_math_1arg_docstr(name):\n@@ -723,31 +961,31 @@ def _decorator(func):\n @builtin\n @_add_math_1arg_docstr(\"exponential\")\n def exp(x, _builder=None):\n-    return frontend.exp(x, _builder)\n+    return semantic.exp(x, _builder)\n \n \n @builtin\n @_add_math_1arg_docstr(\"natural logarithm\")\n def log(x, _builder=None):\n-    return frontend.log(x, _builder)\n+    return semantic.log(x, _builder)\n \n \n @builtin\n @_add_math_1arg_docstr(\"cosine\")\n def cos(x, _builder=None):\n-    return frontend.cos(x, _builder)\n+    return semantic.cos(x, _builder)\n \n \n @builtin\n @_add_math_1arg_docstr(\"sine\")\n def sin(x, _builder=None):\n-    return frontend.sin(x, _builder)\n+    return semantic.sin(x, _builder)\n \n \n @builtin\n @_add_math_1arg_docstr(\"square root\")\n def sqrt(x, _builder=None):\n-    return frontend.sqrt(x, _builder)\n+    return semantic.sqrt(x, _builder)\n \n \n # -----------------------\n@@ -758,7 +996,7 @@ def _add_reduction_docstr(name):\n \n     def _decorator(func):\n         docstr = \"\"\"\n-    Returns the {name} of all elements in the :code:`input` block along the provided :code:`axis`\n+    Returns the {name} of all elements in the :code:`input` tensor along the provided :code:`axis`\n \n     :param input: the input values\n     :param axis: the dimension along which the reduction should be done\n@@ -772,25 +1010,29 @@ def _decorator(func):\n @builtin\n @_add_reduction_docstr(\"maximum\")\n def max(input, axis, _builder=None):\n-    return frontend.max(input, axis, _builder)\n+    axis = _constexpr_to_value(axis)\n+    return semantic.max(input, axis, _builder)\n \n \n @builtin\n @_add_reduction_docstr(\"minimum\")\n def min(input, axis, _builder=None):\n-    return frontend.min(input, axis, _builder)\n+    axis = _constexpr_to_value(axis)\n+    return semantic.min(input, axis, _builder)\n \n \n @builtin\n @_add_reduction_docstr(\"sum\")\n def sum(input, axis, _builder=None):\n-    return frontend.sum(input, axis, _builder)\n+    axis = _constexpr_to_value(axis)\n+    return semantic.sum(input, axis, _builder)\n \n \n @builtin\n @_add_reduction_docstr(\"xor sum\")\n def xor_sum(input, axis, _builder=None):\n-    return frontend.xor_sum(input, axis, _builder)\n+    axis = _constexpr_to_value(axis)\n+    return semantic.xor_sum(input, axis, _builder)\n \n \n # -----------------------\n@@ -800,23 +1042,25 @@ def xor_sum(input, axis, _builder=None):\n \n @builtin\n def debug_barrier(_builder=None):\n-    return frontend.debug_barrier(_builder)\n+    return semantic.debug_barrier(_builder)\n \n \n @builtin\n def multiple_of(input, value, _builder=None):\n     \"\"\"\n     Let the compiler knows that the values in :code:`input` are all multiples of :code:`value`.\n     \"\"\"\n-    return frontend.multiple_of(input, value, _builder)\n+    value = _constexpr_to_value(value)\n+    return semantic.multiple_of(input, value)\n \n \n @builtin\n def max_contiguous(input, value, _builder=None):\n     \"\"\"\n     Let the compiler knows that the `value` first values in :code:`input` are contiguous.\n     \"\"\"\n-    return frontend.max_contiguous(input, value, _builder)\n+    value = _constexpr_to_value(value)\n+    return semantic.max_contiguous(input, value)\n \n \n # -----------------------\n@@ -846,9 +1090,9 @@ def minimum(x, y):\n     \"\"\"\n     Computes the element-wise minimum of :code:`x` and :code:`y`.\n \n-    :param input: the first input block\n+    :param input: the first input tensor\n     :type input: Block\n-    :param other: the second input block\n+    :param other: the second input tensor\n     :type other: Block\n     \"\"\"\n     return triton.language.where(x < y, x, y)\n@@ -859,9 +1103,9 @@ def maximum(x, y):\n     \"\"\"\n     Computes the element-wise maximum of :code:`x` and :code:`y`.\n \n-    :param input: the first input block\n+    :param input: the first input tensor\n     :type input: Block\n-    :param other: the second input block\n+    :param other: the second input tensor\n     :type other: Block\n     \"\"\"\n     return triton.language.where(x > y, x, y)\n@@ -887,7 +1131,7 @@ def ravel(x):\n     \"\"\"\n     Returns a contiguous flattened view of :code:`x`\n \n-    :param x: the input block\n+    :param x: the input tensor\n     :type x: Block\n     \"\"\"\n     return triton.language.reshape(x, [x.numel])"}, {"filename": "python/triton/language/semantic.py", "status": "added", "additions": 1037, "deletions": 0, "changes": 1037, "file_content_changes": "@@ -0,0 +1,1037 @@\n+from __future__ import annotations  # remove after python 3.11\n+\n+from typing import List, Optional, Tuple\n+\n+from . import core as tl\n+from triton._C.libtriton.triton import ir\n+\n+\n+# Create custom exception that prints message \"hello\"\n+class IncompatibleTypeErrorimpl(Exception):\n+    def __init__(self, type_a, type_b):\n+        self.type_a = type_a\n+        self.type_b = type_b\n+        self.message = \"invalid operands of type \" + self.type_a.__repr__() + \" and \" + self.type_b.__repr__()\n+        super(IncompatibleTypeErrorimpl, self).__init__(self.message)\n+\n+\n+# ===----------------------------------------------------------------------===##\n+# Programming Model\n+# ===----------------------------------------------------------------------===##\n+\n+def program_id(axis: int, builder: ir.builder) -> tl.tensor:\n+    return tl.tensor(builder.create_get_program_id(axis), tl.int32)\n+\n+\n+def num_programs(axis: int, builder: ir.builder) -> tl.tensor:\n+    return tl.tensor(builder.create_get_num_programs(axis), tl.int32)\n+\n+# ===----------------------------------------------------------------------===//\n+#                               Implicit Casting Utilities\n+# ===----------------------------------------------------------------------===//\n+\n+\n+def integer_promote_impl(a_ty: tl.dtype, b_ty: tl.dtype) -> tl.dtype:\n+    a_rank = a_ty.int_bitwidth\n+    b_rank = b_ty.int_bitwidth\n+    a_sn = a_ty.int_signedness\n+    b_sn = b_ty.int_signedness\n+    # Rules for signedness taken from \"Usual arithmetic conversions\" on\n+    # https://en.cppreference.com/w/c/language/conversion.\n+    if a_sn == b_sn:\n+        return a_ty if a_rank > b_rank else b_ty\n+    elif a_sn == tl.dtype.SIGNEDNESS.UNSIGNED:\n+        return a_ty if a_rank >= b_rank else b_ty\n+    elif b_sn == tl.dtype.SIGNEDNESS.UNSIGNED:\n+        return b_ty if b_rank >= a_rank else a_ty\n+    assert False\n+\n+\n+def computation_type_impl(a_ty: tl.dtype, b_ty: tl.dtype, div_or_mod: bool) -> tl.dtype:\n+    # 1) if one operand is double, the other is implicitly\n+    #    converted to double\n+    if a_ty.is_fp64() or b_ty.is_fp64():\n+        return tl.float64\n+    # 2) if one operand is float, the other is implicitly\n+    #    converted to float\n+    if a_ty.is_fp32() or b_ty.is_fp32():\n+        return tl.float32\n+    # 3 ) if one operand is half, the other is implicitly converted to half\n+    #     unless we're doing / or %, which do not exist natively in PTX for fp16.\n+    if a_ty.is_fp16() or b_ty.is_fp16():\n+        if div_or_mod:\n+            return tl.float32\n+        else:\n+            return tl.float16\n+    if not a_ty.is_int() or not b_ty.is_int():\n+        assert False\n+    # 4 ) both operands are integer and undergo\n+    #    integer promotion\n+    if div_or_mod and a_ty.int_signedness != b_ty.int_signedness:\n+        raise ValueError(\"Cannot use /, #, or % with \" + a_ty.__repr__() + \" and \" + b_ty.__repr__() + \" because they have different signedness;\"\n+                         \"this is unlikely to result in a useful answer. Cast them to the same signedness.\")\n+    return integer_promote_impl(a_ty, b_ty)\n+\n+# ===----------------------------------------------------------------------===//\n+#                               Binary Operators\n+# ===----------------------------------------------------------------------===//\n+\n+\n+def check_ptr_type_impl(type_a: tl.dtype, type_b: tl.dtype, allow_ptr_a: bool) -> None:\n+    if type_a.is_ptr():\n+        if not allow_ptr_a:\n+            raise IncompatibleTypeErrorimpl(type_a, type_b)\n+        # T* + U* with T != U\n+        if type_b.is_ptr() and (type_a != type_b):\n+            raise IncompatibleTypeErrorimpl(type_a, type_b)\n+        # T* + float\n+        if type_b.is_floating():\n+            raise IncompatibleTypeErrorimpl(type_a, type_b)\n+\n+\n+def binary_op_type_checking_impl(lhs: tl.tensor,\n+                                 rhs: tl.tensor,\n+                                 builder: ir.builder,\n+                                 allow_lhs_ptr=False, allow_rhs_ptr=False,\n+                                 arithmetic_check=True, div_or_mod=False\n+                                 ) -> Tuple[tl.tensor, tl.tensor]:\n+    # implicit broadcasting\n+    lhs, rhs = broadcast_impl_value(lhs, rhs, builder)\n+    # implicit typecasting\n+    lhs_sca_ty = lhs.type.scalar\n+    rhs_sca_ty = rhs.type.scalar\n+    check_ptr_type_impl(lhs_sca_ty, rhs_sca_ty, allow_lhs_ptr)\n+    check_ptr_type_impl(rhs_sca_ty, lhs_sca_ty, allow_rhs_ptr)\n+    if arithmetic_check and not lhs_sca_ty.is_ptr() and not rhs_sca_ty.is_ptr():\n+        ret_sca_ty = computation_type_impl(lhs_sca_ty, rhs_sca_ty, div_or_mod)\n+        lhs = cast(lhs, ret_sca_ty, builder)\n+        rhs = cast(rhs, ret_sca_ty, builder)\n+    return lhs, rhs\n+\n+\n+def add(input: tl.tensor,\n+        other: tl.tensor,\n+        builder: ir.builder) -> tl.tensor:\n+    input, other = binary_op_type_checking_impl(input, other, builder, True, True)\n+    input_scalar_ty = input.type.scalar\n+    other_scalar_ty = other.type.scalar\n+\n+    # offset + ptr\n+    # ptr + offset\n+    if other_scalar_ty.is_ptr() and not input_scalar_ty.is_ptr():\n+        input, other = other, input\n+    if input_scalar_ty.is_ptr():\n+        return tl.tensor(builder.create_gep(input.handle, [other.handle]), input.type)\n+    # float + float\n+    elif input_scalar_ty.is_floating():\n+        return tl.tensor(builder.create_fadd(input.handle, other.handle), input.type)\n+    # int + int\n+    elif input_scalar_ty.is_int():\n+        return tl.tensor(builder.create_add(input.handle, other.handle), input.type)\n+    assert False\n+\n+\n+def sub(input: tl.tensor,\n+        other: tl.tensor,\n+        builder: ir.builder) -> tl.tensor:\n+    input, other = binary_op_type_checking_impl(input, other, builder, True, False)\n+    scalar_ty = input.type.scalar\n+    # ptr - offset\n+    if scalar_ty.is_ptr():\n+        return tl.tensor(builder.create_gep(input.handle, [minus(other, builder).handle]),\n+                         input.type)\n+    # float - float\n+    if scalar_ty.is_floating():\n+        return tl.tensor(builder.create_fsub(input.handle, other.handle), input.type)\n+    # int - int\n+    elif scalar_ty.is_int():\n+        return tl.tensor(builder.create_sub(input.handle, other.handle), input.type)\n+    assert False\n+\n+\n+def mul(input: tl.tensor,\n+        other: tl.tensor,\n+        builder: ir.builder) -> tl.tensor:\n+    input, other = binary_op_type_checking_impl(input, other, builder)\n+    scalar_ty = input.type.scalar\n+    # float * float\n+    if scalar_ty.is_floating():\n+        return tl.tensor(builder.create_fmul(input.handle, other.handle), input.type)\n+    # * int\n+    elif scalar_ty.is_int():\n+        return tl.tensor(builder.create_mul(input.handle, other.handle), input.type)\n+    assert False\n+\n+\n+def truediv(input: tl.tensor,\n+            other: tl.tensor,\n+            builder: ir.builder) -> tl.tensor:\n+    input, other = binary_op_type_checking_impl(input, other, builder, False, False, True, True)\n+    input_scalar_ty = input.type.scalar\n+    other_scalar_ty = other.type.scalar\n+    # float / int\n+    if input_scalar_ty.is_floating() and other_scalar_ty.is_int():\n+        other = cast(other, input_scalar_ty, builder)\n+    # int / float\n+    elif input_scalar_ty.is_int() and other_scalar_ty.is_floating():\n+        input = cast(input, other_scalar_ty, builder)\n+    # int / int (cast to tl.float32)\n+    elif input_scalar_ty.is_int() and other_scalar_ty.is_int():\n+        input = cast(input, tl.float32, builder)\n+        other = cast(other, tl.float32, builder)\n+    # float / float (cast to highest exponent type)\n+    elif input_scalar_ty.is_floating() and other_scalar_ty.is_floating():\n+        if input_scalar_ty.fp_mantissa_width > other_scalar_ty.fp_mantissa_width:\n+            other = cast(other, input_scalar_ty, builder)\n+        else:\n+            input = cast(input, other_scalar_ty, builder)\n+    # unreachable\n+    else:\n+        assert False\n+    return tl.tensor(builder.create_fdiv(input.handle, other.handle), input.type)\n+\n+\n+def floordiv(input: tl.tensor,\n+             other: tl.tensor,\n+             builder: ir.builder) -> tl.tensor:\n+    input, other = binary_op_type_checking_impl(input, other, builder, False, False, True, True)\n+    input_scalar_ty = input.type.scalar\n+    other_scalar_ty = other.type.scalar\n+    if input_scalar_ty.is_int() and other_scalar_ty.is_int():\n+        ret_ty = integer_promote_impl(input_scalar_ty, other_scalar_ty)\n+        input = cast(input, ret_ty, builder)\n+        other = cast(other, ret_ty, builder)\n+        if ret_ty.is_int_signed():\n+            return tl.tensor(builder.create_sdiv(input.handle, other.handle), input.type)\n+        else:\n+            return tl.tensor(builder.create_udiv(input.handle, other.handle), input.type)\n+    assert False\n+\n+\n+def fdiv(input: tl.tensor,\n+         other: tl.tensor,\n+         ieee_rounding: bool,\n+         builder: ir.builder) -> tl.tensor:\n+    input_scalar_ty = input.type.scalar\n+    other_scalar_ty = other.type.scalar\n+    if not input_scalar_ty.is_floating() or not other_scalar_ty.is_floating():\n+        raise ValueError(\"both operands of fdiv must have floating poscalar type\")\n+    input, other = binary_op_type_checking_impl(input, other, builder, False, False, False, True)\n+    ret = builder.create_fdiv(input.handle, other.handle)\n+    ret.set_fdiv_ieee_rounding(ieee_rounding)\n+    return tl.tensor(ret, input.type)\n+\n+\n+def mod(input: tl.tensor,\n+        other: tl.tensor,\n+        builder: ir.builder) -> tl.tensor:\n+    input, other = binary_op_type_checking_impl(input, other, builder, False, False, True, True)\n+    scalar_ty = input.type.scalar\n+    other_scalar_ty = other.type.scalar\n+    # float % float\n+    if scalar_ty.is_floating():\n+        return tl.tensor(builder.create_frem(input.handle, other.handle), input.type)\n+    # % int\n+    elif scalar_ty.is_int():\n+        if scalar_ty.int_signedness != other_scalar_ty.int_signedness:\n+            raise ValueError(\"Cannot mod \" + scalar_ty.__repr__() + \" by \" + other_scalar_ty.__repr__() + \" \"\n+                             \"because they have different signedness;\"\n+                             \"this is unlikely to result in a useful answer. Cast them to the same signedness.\")\n+        if scalar_ty.is_int_signed():\n+            return tl.tensor(builder.create_srem(input.handle, other.handle), input.type)\n+        else:\n+            return tl.tensor(builder.create_urem(input.handle, other.handle), input.type)\n+    assert False\n+\n+##############\n+# bitwise ops\n+##############\n+\n+\n+def bitwise_op_type_checking_impl(input: tl.tensor,\n+                                  other: tl.tensor,\n+                                  builder: ir.builder) -> Tuple[tl.tensor, tl.tensor]:\n+    input, other = binary_op_type_checking_impl(input, other, builder, False, False, False)\n+    input_sca_ty = input.type.scalar\n+    other_sca_ty = other.type.scalar\n+    if not input_sca_ty.is_int() or not other_sca_ty.is_int():\n+        raise IncompatibleTypeErrorimpl(input_sca_ty, other_sca_ty)\n+    ret_sca_ty = integer_promote_impl(input_sca_ty, other_sca_ty)\n+    if ret_sca_ty != input_sca_ty:\n+        input = cast(input, ret_sca_ty, builder)\n+    if ret_sca_ty != other_sca_ty:\n+        other = cast(other, ret_sca_ty, builder)\n+    return input, other\n+\n+\n+def and_(input: tl.tensor,\n+         other: tl.tensor,\n+         builder: ir.builder) -> tl.tensor:\n+    input, other = bitwise_op_type_checking_impl(input, other, builder)\n+    return tl.tensor(builder.create_and(input.handle, other.handle), input.type)\n+\n+\n+def or_(input: tl.tensor,\n+        other: tl.tensor,\n+        builder: ir.builder) -> tl.tensor:\n+    input, other = bitwise_op_type_checking_impl(input, other, builder)\n+    return tl.tensor(builder.create_or(input.handle, other.handle), input.type)\n+\n+\n+def xor_(input: tl.tensor,\n+         other: tl.tensor,\n+         builder: ir.builder) -> tl.tensor:\n+    input, other = bitwise_op_type_checking_impl(input, other, builder)\n+    return tl.tensor(builder.create_xor(input.handle, other.handle), input.type)\n+\n+\n+def lshr(input: tl.tensor,\n+         other: tl.tensor,\n+         builder: ir.builder) -> tl.tensor:\n+    input, other = bitwise_op_type_checking_impl(input, other, builder)\n+    return tl.tensor(builder.create_lshr(input.handle, other.handle), input.type)\n+\n+\n+def shl(input: tl.tensor,\n+        other: tl.tensor,\n+        builder: ir.builder) -> tl.tensor:\n+    input, other = bitwise_op_type_checking_impl(input, other, builder)\n+    return tl.tensor(builder.create_shl(input.handle, other.handle), input.type)\n+\n+# ===----------------------------------------------------------------------===//\n+#                               Unary Operators\n+# ===----------------------------------------------------------------------===//\n+\n+\n+def plus(input: tl.tensor) -> tl.tensor:\n+    return input\n+\n+\n+def minus(input: tl.tensor,\n+          builder: ir.builder) -> tl.tensor:\n+    input_sca_ty = input.type.scalar\n+    if input_sca_ty.is_ptr():\n+        raise ValueError(\"wrong type argument to unary minus (\" + input_sca_ty.__repr__() + \")\")\n+    _0 = tl.tensor(ir.constant.get_null_value(input_sca_ty.to_ir(builder)), input_sca_ty)\n+    return sub(_0, input, builder)\n+\n+\n+def invert(input: tl.tensor,\n+           builder: tl.tensor) -> tl.tensor:\n+    input_sca_ty = input.type.scalar\n+    if input_sca_ty.is_ptr() or input_sca_ty.is_floating():\n+        raise ValueError(\"wrong type argument to unary invert (\" + input_sca_ty.__repr__() + \")\")\n+    _1 = tl.tensor(ir.constant.get_all_ones_value(input_sca_ty.to_ir(builder)), input_sca_ty)\n+    return xor_(input, _1, builder)\n+\n+\n+# ===----------------------------------------------------------------------===//\n+#                               Comparison Operators\n+# ===----------------------------------------------------------------------===//\n+def _bool_like(v: tl.tensor) -> tl.block_type:\n+    if not v.type.is_block():\n+        return tl.int1\n+    shape = v.type.shape\n+    return tl.block_type(tl.int1, shape)\n+\n+\n+def greater_than(input: tl.tensor,\n+                 other: tl.tensor,\n+                 builder: ir.builder) -> tl.tensor:\n+    input, other = binary_op_type_checking_impl(input, other, builder)\n+    scalar_ty = input.type.scalar\n+    # float > float\n+    if scalar_ty.is_floating():\n+        return tl.tensor(builder.create_fcmpOGT(input.handle, other.handle), _bool_like(input))\n+    # > int\n+    elif scalar_ty.is_int():\n+        if scalar_ty.is_int_signed():\n+            return tl.tensor(builder.create_icmpSGT(input.handle, other.handle), _bool_like(input))\n+        else:\n+            return tl.tensor(builder.create_icmpUGT(input.handle, other.handle), _bool_like(input))\n+    assert False\n+\n+\n+def greater_equal(input: tl.tensor,\n+                  other: tl.tensor,\n+                  builder: ir.builder) -> tl.tensor:\n+    input, other = binary_op_type_checking_impl(input, other, builder)\n+    scalar_ty = input.type.scalar\n+    # float >= float\n+    if scalar_ty.is_floating():\n+        return tl.tensor(builder.create_fcmpOGE(input.handle, other.handle), _bool_like(input))\n+    # >= int\n+    elif scalar_ty.is_int():\n+        if scalar_ty.is_int_signed():\n+            return tl.tensor(builder.create_icmpSGE(input.handle, other.handle), _bool_like(input))\n+        else:\n+            return tl.tensor(builder.create_icmpUGE(input.handle, other.handle), _bool_like(input))\n+    assert False\n+\n+\n+def less_than(input: tl.tensor,\n+              other: tl.tensor,\n+              builder: ir.builder) -> tl.tensor:\n+    input, other = binary_op_type_checking_impl(input, other, builder)\n+    scalar_ty = input.type.scalar\n+    # float < float\n+    if scalar_ty.is_floating():\n+        return tl.tensor(builder.create_fcmpOLT(input.handle, other.handle), _bool_like(input))\n+    # < int\n+    elif scalar_ty.is_int():\n+        if scalar_ty.is_int_signed():\n+            return tl.tensor(builder.create_icmpSLT(input.handle, other.handle), _bool_like(input))\n+        else:\n+            return tl.tensor(builder.create_icmpULT(input.handle, other.handle), _bool_like(input))\n+    assert False\n+\n+\n+def less_equal(input: tl.tensor,\n+               other: tl.tensor,\n+               builder: ir.builder) -> tl.tensor:\n+    input, other = binary_op_type_checking_impl(input, other, builder)\n+    scalar_ty = input.type.scalar\n+    # float < float\n+    if scalar_ty.is_floating():\n+        return tl.tensor(builder.create_fcmpOLE(input.handle, other.handle), _bool_like(input))\n+    # < int\n+    elif scalar_ty.is_int():\n+        if scalar_ty.is_int_signed():\n+            return tl.tensor(builder.create_icmpSLE(input.handle, other.handle), _bool_like(input))\n+        else:\n+            return tl.tensor(builder.create_icmpULE(input.handle, other.handle), _bool_like(input))\n+    assert False\n+\n+\n+def equal(input: tl.tensor,\n+          other: tl.tensor,\n+          builder: ir.builder) -> tl.tensor:\n+    input, other = binary_op_type_checking_impl(input, other, builder)\n+    scalar_ty = input.type.scalar\n+    # float == float\n+    if scalar_ty.is_floating():\n+        return tl.tensor(builder.create_fcmpOEQ(input.handle, other.handle), _bool_like(input))\n+    # == int\n+    elif scalar_ty.is_int():\n+        return tl.tensor(builder.create_icmpEQ(input.handle, other.handle), _bool_like(input))\n+    assert False\n+\n+\n+def not_equal(input: tl.tensor,\n+              other: tl.tensor,\n+              builder: ir.builder) -> tl.tensor:\n+    input, other = binary_op_type_checking_impl(input, other, builder)\n+    scalar_ty = input.type.scalar\n+    # float == float\n+    if scalar_ty.is_floating():\n+        return tl.tensor(builder.create_fcmpUNE(input.handle, other.handle), _bool_like(input))\n+    # == int\n+    elif scalar_ty.is_int():\n+        return tl.tensor(builder.create_icmpNE(input.handle, other.handle), _bool_like(input))\n+    assert False\n+\n+# ===----------------------------------------------------------------------===//\n+#                               Block Creation\n+# ===----------------------------------------------------------------------===//\n+\n+\n+def arange(start: int, end: int, builder: ir.builder) -> tl.tensor:\n+    shape = [end - start]\n+    ret_ty = tl.block_type(tl.int32, shape)\n+    return tl.tensor(builder.get_range(start, end), ret_ty)\n+\n+\n+def zeros(shape: List[int], dtype: tl.dtype, builder: ir.builder) -> tl.tensor:\n+    _0 = ir.constant.get_null_value(dtype.to_ir(builder))\n+    ret_ty = tl.block_type(dtype, shape)\n+    return tl.tensor(builder.create_splat(_0, shape), ret_ty)\n+\n+# ===----------------------------------------------------------------------===//\n+#                               Shape Manipulation\n+# ===----------------------------------------------------------------------===//\n+\n+\n+def reshape(input: tl.tensor,\n+            dst_shape: List[int],\n+            builder: ir.builder) -> tl.tensor:\n+    numel = 1\n+    for s in dst_shape:\n+        numel *= s\n+    if input.type.numel != numel:\n+        raise ValueError(\"cannot reshape block of different shape\")\n+    ret_ty = tl.block_type(input.type.scalar, dst_shape)\n+    return tl.tensor(builder.create_reshape(input.handle, dst_shape), ret_ty)\n+\n+\n+def cat(lhs: tl.tensor, rhs: tl.tensor, builder: ir.builder) -> tl.tensor:\n+    # TODO: check types\n+    return tl.tensor(builder.create_cat(lhs.handle, rhs.handle), lhs.type)\n+\n+\n+def broadcast_impl_shape(input: tl.tensor,\n+                         shape: List[int],\n+                         builder: ir.builder) -> tl.tensor:\n+    if not input.type.is_block():\n+        ret_ty = tl.block_type(input.type, shape)\n+        return tl.tensor(builder.create_splat(input.handle, shape), ret_ty)\n+    src_shape = input.type.get_block_shapes()\n+    if len(src_shape) != len(shape):\n+        raise ValueError(f\"Cannot broadcast, rank mismatch: {src_shape}, {shape}\")\n+    if shape == src_shape:\n+        return input\n+    ret_ty = tl.block_type(input.type.scalar, shape)\n+    return tl.tensor(builder.create_broadcast(input.handle, shape), ret_ty)\n+\n+\n+def broadcast_impl_value(lhs: tl.tensor,\n+                         rhs: tl.tensor,\n+                         builder: ir.builder) -> tl.tensor:\n+    lhs_ty = lhs.type\n+    rhs_ty = rhs.type\n+\n+    # make_shape_compatible(block, scalar)\n+    if lhs_ty.is_block() and not rhs_ty.is_block():\n+        rhs_ty = tl.block_type(rhs_ty.scalar, lhs_ty.shape)\n+        rhs = tl.tensor(builder.create_splat(rhs.handle, lhs_ty.get_block_shapes()), rhs_ty)\n+    # make_shape_compatible(scalar, block)\n+    elif not lhs_ty.is_block() and rhs_ty.is_block():\n+        lhs_ty = tl.block_type(lhs_ty.scalar, rhs_ty.shape)\n+        lhs = tl.tensor(builder.create_splat(lhs.handle, rhs_ty.get_block_shapes()), lhs_ty)\n+    # make_shape_compatible(block, block)\n+    elif lhs_ty.is_block() and rhs_ty.is_block():\n+        lhs_shape = lhs_ty.get_block_shapes()\n+        rhs_shape = rhs_ty.get_block_shapes()\n+        if len(lhs_shape) != len(rhs_shape):\n+            raise ValueError(\"Cannot make_shape_compatible: blocks must have the same rank\")\n+        ret_shape = []\n+        for i in range(len(lhs_shape)):\n+            left = lhs_shape[i]\n+            right = rhs_shape[i]\n+            if left == 1:\n+                ret_shape.append(right)\n+            elif right == 1:\n+                ret_shape.append(left)\n+            elif left == right:\n+                ret_shape.append(left)\n+            else:\n+                raise ValueError(\"Cannot make_shape_compatible: incompatible dimensions \"\n+                                 \"at index \" + str(i) + \": \" + str(left) + \" and \" + str(right))\n+        if lhs_shape != ret_shape:\n+            ret_ty = tl.block_type(lhs_ty.scalar, ret_shape)\n+            lhs = tl.tensor(builder.create_broadcast(lhs.handle, ret_shape), ret_ty)\n+        if rhs_shape != ret_shape:\n+            ret_ty = tl.block_type(rhs_ty.scalar, ret_shape)\n+            rhs = tl.tensor(builder.create_broadcast(rhs.handle, ret_shape), ret_ty)\n+    # (scalar, scalar) => returns original blocks\n+    return lhs, rhs\n+\n+#######\n+# cast\n+#######\n+\n+\n+def bitcast(input: tl.tensor,\n+            dst_ty: tl.dtype,\n+            builder: ir.builder) -> tl.tensor:\n+    src_ty = input.type\n+    if src_ty.is_block():\n+        dst_ty = tl.block_type(dst_ty, input.type.get_block_shapes())\n+    if src_ty == dst_ty:\n+        return input\n+    src_sca_ty = src_ty.scalar\n+    dst_sca_ty = dst_ty.scalar\n+    if src_sca_ty.is_ptr() or dst_sca_ty.is_ptr():\n+        return cast(input, dst_ty, builder)\n+    # Bitcast\n+    src_bits = src_sca_ty.primitive_bitwidth\n+    dst_bits = dst_sca_ty.primitive_bitwidth\n+    if src_bits != dst_bits:\n+        raise ValueError(\"Cannot bitcast data-type of size \" + str(src_bits) + \"to \"\n+                         \"data-type of size \" + str(dst_bits))\n+    return tl.tensor(builder.create_bitcast(input.handle, dst_ty.to_ir(builder)),\n+                     dst_ty)\n+\n+\n+def cast(input: tl.tensor,\n+         dst_ty: tl.dtype,\n+         builder: ir.builder) -> tl.tensor:\n+    src_ty = input.type\n+    if src_ty.is_block():\n+        dst_ty = tl.block_type(dst_ty, input.type.get_block_shapes())\n+    if src_ty == dst_ty:\n+        return input\n+    src_sca_ty = src_ty.scalar\n+    dst_sca_ty = dst_ty.scalar\n+\n+    # bf16 <=> (not fp32)\n+    if (src_sca_ty.is_bf16() and not dst_sca_ty.is_fp32()) or \\\n+       (dst_sca_ty.is_bf16() and not src_sca_ty.is_fp32()):\n+        return cast(cast(input, tl.float32, builder), dst_sca_ty, builder)\n+\n+    # FP Truncation\n+    truncate_fp = src_sca_ty.is_floating() and \\\n+        dst_sca_ty.is_floating() and \\\n+        src_sca_ty.fp_mantissa_width > dst_sca_ty.fp_mantissa_width\n+    if truncate_fp:\n+        return tl.tensor(builder.create_fp_trunc(input.handle,\n+                                                 dst_ty.to_ir(builder)),\n+                         dst_ty)\n+\n+    # FP Extension\n+    ext_fp = src_sca_ty.is_floating() and \\\n+        dst_sca_ty.is_floating() and \\\n+        src_sca_ty.fp_mantissa_width < dst_sca_ty.fp_mantissa_width\n+    if ext_fp:\n+        return tl.tensor(builder.create_fp_ext(input.handle,\n+                                               dst_ty.to_ir(builder)),\n+                         dst_ty)\n+\n+    # Int cast\n+    if src_sca_ty.is_int() and dst_sca_ty.is_int() and \\\n+       (src_sca_ty.int_bitwidth != dst_sca_ty.int_bitwidth or src_sca_ty.int_signedness != dst_sca_ty.int_signedness):\n+        sign_extend = src_sca_ty.is_int_signed() and not src_sca_ty.is_bool()\n+        return tl.tensor(builder.create_int_cast(input.handle,\n+                                                 dst_ty.to_ir(builder), sign_extend),\n+                         dst_ty)\n+\n+    # Float to Int\n+    if src_sca_ty.is_floating() and dst_sca_ty.is_int():\n+        # TODO: is this correct?\n+        if dst_sca_ty.is_bool():\n+            return tl.tensor(builder.create_fp_to_ui(input.handle,\n+                                                     dst_ty.to_ir(builder)),\n+                             dst_ty)\n+        else:\n+            return tl.tensor(builder.create_fp_to_si(input.handle,\n+                                                     dst_ty.to_ir(builder)),\n+                             dst_ty)\n+\n+    # int => float\n+    if src_sca_ty.is_int() and dst_sca_ty.is_floating():\n+        if src_sca_ty.is_bool() or not src_sca_ty.is_int_signed():\n+            return tl.tensor(builder.create_ui_to_fp(input.handle,\n+                                                     dst_ty.to_ir(builder)),\n+                             dst_ty)\n+        else:\n+            return tl.tensor(builder.create_si_to_fp(input.handle,\n+                                                     dst_ty.to_ir(builder)),\n+                             dst_ty)\n+\n+    # ptr => int\n+    if src_sca_ty.is_ptr() and dst_sca_ty.is_int():\n+        bitwidth = dst_sca_ty.int_bitwidth\n+        if bitwidth == 64:\n+            return tl.tensor(builder.create_cast(ir.PtrToInt, input.handle, dst_ty.to_ir(builder)),\n+                             dst_ty)\n+        if bitwidth == 1:\n+            return not_equal(cast(input, tl.int64, builder),\n+                             tl.tensor(builder.get_int64(0), tl.int64),\n+                             builder)\n+\n+    if not src_sca_ty.is_ptr() and dst_sca_ty.is_ptr():\n+        return tl.tensor(builder.create_int_to_ptr(input.handle, dst_ty.to_ir(builder)), dst_ty)\n+    # Ptr . Ptr\n+    if src_sca_ty.is_ptr() and dst_sca_ty.is_ptr():\n+        return tl.tensor(builder.create_bitcast(input.handle, dst_ty.to_ir(builder)), dst_ty)\n+    # * . Bool\n+    if dst_sca_ty.is_bool():\n+        if src_sca_ty.is_ptr():\n+            input = cast(input, tl.int64, builder)\n+        other = builder.get_int64(0)\n+        if src_ty.is_bool():\n+            other = builder.create_splat(other, src_ty.get_block_shapes())\n+        return tl.tensor(builder.create_icmpNE(input.handle, other), dst_ty)\n+    assert False, f'cannot cast {input} to {dst_ty}'\n+\n+# ===----------------------------------------------------------------------===//\n+#                               Memory Operators\n+# ===----------------------------------------------------------------------===//\n+\n+\n+def load(ptr: tl.tensor,\n+         mask: Optional[tl.tensor],\n+         other: Optional[tl.tensor],\n+         cache_modifier: str,\n+         eviction_policy: str,\n+         is_volatile: bool,\n+         builder: ir.builder) -> tl.tensor:\n+    if not ptr.type.scalar.is_ptr():\n+        raise ValueError(\"Pointer argument of load instruction is \" + ptr.type.__repr__())\n+    if ptr.type.is_block():\n+        if mask:\n+            mask = broadcast_impl_shape(mask, ptr.type.get_block_shapes(), builder)\n+        if other:\n+            other = broadcast_impl_shape(other, ptr.type.get_block_shapes(), builder)\n+\n+    if other:\n+        other = cast(other, ptr.type.scalar.element_ty, builder)\n+    ptr_ty = ptr.type.scalar\n+    elt_ty = ptr_ty.element_ty\n+    # treat bool* as tl.int8*\n+    if elt_ty == tl.int1:\n+        elt_ty = tl.int8\n+        ptr_ty = tl.pointer_type(elt_ty, ptr_ty.address_space)\n+        ptr = cast(ptr, ptr_ty, builder)\n+\n+    # cache modifier\n+    cache = ir.CACHE_MODIFIER.NONE  # default\n+    if cache_modifier:\n+        if cache_modifier == \".ca\":\n+            cache = ir.CACHE_MODIFIER.CA\n+        elif cache_modifier == \".cg\":\n+            cache = ir.CACHE_MODIFIER.CG\n+        else:\n+            raise ValueError(f\"Cache modifier {cache_modifier} not supported\")\n+\n+    # eviction policy\n+    eviction = ir.EVICTION_POLICY.NORMAL  # default\n+    if eviction_policy:\n+        if eviction_policy == \"evict_last\":\n+            eviction = ir.EVICTION_POLICY.EVICT_LAST\n+        elif eviction_policy == \"evict_first\":\n+            eviction = ir.EVICTION_POLICY.EVICT_FIRST\n+        else:\n+            raise ValueError(f\"Eviction policy {eviction_policy} not supported\")\n+\n+    if ptr.type.is_block():\n+        shape = ptr.type.get_block_shapes()\n+        dst_ty = tl.block_type(elt_ty, shape)\n+    else:\n+        dst_ty = elt_ty\n+\n+    if not mask and not other:\n+        return tl.tensor(builder.create_load(ptr.handle, cache, eviction, is_volatile),\n+                         dst_ty)\n+    if not mask:\n+        raise ValueError(\"`other` cannot be provided without `mask`\")\n+\n+    if not other:\n+        other_ir = ir.undef.get(elt_ty.to_ir(builder))\n+        if ptr.type.is_block():\n+            other_ir = builder.create_splat(other_ir, ptr.type.get_block_shapes())\n+        other = tl.tensor(other_ir, dst_ty)\n+\n+    return tl.tensor(builder.create_masked_load(ptr.handle,\n+                                                mask.handle,\n+                                                other.handle,\n+                                                cache, eviction, is_volatile),\n+                     dst_ty)\n+\n+\n+def store(ptr: tl.tensor,\n+          val: tl.tensor,\n+          mask: Optional[tl.tensor],\n+          builder: ir.builder) -> tl.tensor:\n+    if not ptr.type.scalar.is_ptr():\n+        raise ValueError(\"Pointer argument of store instruction is \" + ptr.type.__repr__())\n+    if ptr.type.is_block():\n+        val = broadcast_impl_shape(val, ptr.type.get_block_shapes(), builder)\n+    if mask:\n+        mask = broadcast_impl_shape(mask, ptr.type.get_block_shapes(), builder)\n+    ptr_ty = ptr.type.scalar\n+    elt_ty = ptr_ty.element_ty\n+    # treat bool* as tl.int8*\n+    if elt_ty == tl.int1:\n+        elt_ty = tl.int8\n+        ptr_ty = tl.pointer_type(elt_ty, ptr_ty.address_space)\n+        ptr = cast(ptr, ptr_ty, builder)\n+\n+    # cast to target data-type\n+    val = cast(val, elt_ty, builder)\n+    if not mask:\n+        return tl.tensor(builder.create_store(ptr.handle, val.handle), tl.void)\n+    if not mask.type.scalar.is_bool():\n+        raise ValueError(\"Mask must have boolean scalar type\")\n+    return tl.tensor(builder.create_masked_store(ptr.handle, val.handle, mask.handle), tl.void)\n+\n+#########\n+# atomic\n+#########\n+\n+\n+def atomic_cas(ptr: tl.tensor,\n+               cmp: tl.tensor,\n+               val: tl.tensor,\n+               builder: ir.builder) -> tl.tensor:\n+    # TODO: type checking\n+    return tl.tensor(builder.create_atomic_cas(ptr.handle, cmp.handle, val.handle), val.type)\n+\n+\n+def atom_red_typechecking_impl(ptr: tl.tensor,\n+                               val: tl.tensor,\n+                               mask: tl.tensor,\n+                               builder: ir.builder) -> Tuple[tl.tensor, tl.tensor, tl.tensor]:\n+    if not ptr.type.scalar.is_ptr():\n+        raise ValueError(\"Pointer argument of store instruction is \" + ptr.type.__repr__())\n+    if ptr.type.is_block():\n+        if mask:\n+            mask = broadcast_impl_shape(mask, ptr.type.get_block_shapes(), builder)\n+        if val:\n+            val = broadcast_impl_shape(val, ptr.type.get_block_shapes(), builder)\n+    val = cast(val, ptr.type.scalar.element_ty, builder)\n+    if not mask:\n+        mask_ir = builder.get_int1(True)\n+        mask_ty = tl.int1\n+        if ptr.type.is_block():\n+            mask_ir = builder.create_splat(mask_ir, ptr.type.get_block_shapes())\n+            mask_ty = tl.block_type(tl.int1, ptr.type.get_block_shapes())\n+        mask = tl.tensor(mask_ir, mask_ty)\n+    return ptr, val, mask\n+\n+\n+def atomic_max(ptr: tl.tensor,\n+               val: tl.tensor,\n+               mask: tl.tensor,\n+               builder: ir.builder) -> tl.tensor:\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    sca_ty = val.type.scalar\n+    # direct call to atomic_max for integers\n+    if sca_ty.is_int():\n+        if sca_ty.is_int_signed():\n+            return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MAX,\n+                                                       ptr.handle,\n+                                                       val.handle,\n+                                                       mask.handle),\n+                             val.type)\n+        else:\n+            return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMAX,\n+                                                       ptr.handle,\n+                                                       val.handle,\n+                                                       mask.handle),\n+                             val.type)\n+    # for float\n+    # return atomic_smax(i_ptr, i_val) if val >= 0\n+    # return atomic_umin(i_ptr, i_val) if val < 0\n+    i_val = bitcast(val, tl.int32, builder)\n+    i_ptr = bitcast(ptr, tl.pointer_type(tl.int32, 1), builder)\n+    pos = greater_equal(val, tl.tensor(ir.constant_float.get(sca_ty.to_ir(builder), 0), sca_ty), builder)\n+    neg = less_than(val, tl.tensor(ir.constant_float.get(sca_ty.to_ir(builder), 0), sca_ty), builder)\n+    pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MAX, i_ptr.handle, i_val.handle, and_(mask, pos, builder).handle), i_val.type)\n+    neg_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMIN, i_ptr.handle, i_val.handle, and_(mask, neg, builder).handle), i_val.type)\n+    return where(pos, pos_ret, neg_ret, builder)\n+\n+\n+def atomic_min(ptr: tl.tensor,\n+               val: tl.tensor,\n+               mask: tl.tensor,\n+               builder: ir.builder) -> tl.tensor:\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    sca_ty = val.type.scalar\n+    # direct call to atomic_min for integers\n+    if sca_ty.is_int():\n+        if sca_ty.is_int_signed():\n+            return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MIN,\n+                                                       ptr.handle,\n+                                                       val.handle,\n+                                                       mask.handle),\n+                             val.type)\n+        else:\n+            return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMIN,\n+                                                       ptr.handle,\n+                                                       val.handle,\n+                                                       mask.handle),\n+                             val.type)\n+    # for float\n+    # return atomic_smin(i_ptr, i_val) if val >= 0\n+    # return atomic_umax(i_ptr, i_val) if val < 0\n+    i_val = bitcast(val, tl.int32, builder)\n+    i_ptr = bitcast(ptr, tl.pointer_type(tl.int32, 1), builder)\n+    pos = greater_equal(val, tl.tensor(ir.constant_float.get(sca_ty.to_ir(builder), 0), sca_ty), builder)\n+    neg = less_than(val, tl.tensor(ir.constant_float.get(sca_ty.to_ir(builder), 0), sca_ty), builder)\n+    pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MIN,\n+                                                  i_ptr.handle,\n+                                                  i_val.handle,\n+                                                  and_(mask, pos, builder).handle),\n+                        i_val.type)\n+    neg_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMAX,\n+                                                  i_ptr.handle,\n+                                                  i_val.handle,\n+                                                  and_(mask, neg, builder).handle),\n+                        i_val.type)\n+    return where(pos, pos_ret, neg_ret, builder)\n+\n+\n+def atomic_add(ptr: tl.tensor,\n+               val: tl.tensor,\n+               mask: tl.tensor,\n+               builder: ir.builder) -> tl.tensor:\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    sca_ty = val.type.scalar\n+    op = ir.ATOMIC_OP.FADD if sca_ty.is_floating() else ir.ATOMIC_OP.ADD\n+    return tl.tensor(builder.create_atomic_rmw(op, ptr.handle, val.handle, mask.handle), val.type)\n+\n+\n+def atomic_and(ptr: tl.tensor,\n+               val: tl.tensor,\n+               mask: tl.tensor,\n+               builder: ir.builder) -> tl.tensor:\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.AND, ptr.handle, val.handle, mask.handle), val.type)\n+\n+\n+def atomic_or(ptr: tl.tensor,\n+              val: tl.tensor,\n+              mask: tl.tensor,\n+              builder: ir.builder) -> tl.tensor:\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.OR, ptr.handle, val.handle, mask.handle), val.type)\n+\n+\n+def atomic_xor(ptr: tl.tensor,\n+               val: tl.tensor,\n+               mask: tl.tensor,\n+               builder: ir.builder) -> tl.tensor:\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XOR, ptr.handle, val.handle, mask.handle), val.type)\n+\n+\n+def atomic_xchg(ptr: tl.tensor,\n+                val: tl.tensor,\n+                mask: tl.tensor,\n+                builder: ir.builder) -> tl.tensor:\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XCHG, ptr.handle, val.handle, mask.handle), val.type)\n+\n+# ===----------------------------------------------------------------------===//\n+#                               Linear Algebra\n+# ===----------------------------------------------------------------------===//\n+\n+\n+def dot(lhs: tl.tensor,\n+        rhs: tl.tensor,\n+        allow_tf32: bool,\n+        builder: ir.builder) -> tl.tensor:\n+    assert lhs.type.is_block() and rhs.type.is_block()\n+    if lhs.type.scalar.is_int():\n+        _0 = builder.get_int32(0)\n+        ret_scalar_ty = tl.int32\n+    else:\n+        _0 = builder.get_float32(0)\n+        ret_scalar_ty = tl.float32\n+    M = lhs.type.shape[0]\n+    N = rhs.type.shape[1]\n+    _0 = builder.create_splat(_0, [M, N])\n+    ret_ty = tl.block_type(ret_scalar_ty, [M, N])\n+    return tl.tensor(builder.create_dot(lhs.handle, rhs.handle, _0, allow_tf32),\n+                     ret_ty)\n+\n+\n+# ===----------------------------------------------------------------------===//\n+#                               Indexing\n+# ===----------------------------------------------------------------------===//\n+\n+def where(condition: tl.tensor,\n+          x: tl.tensor,\n+          y: tl.tensor,\n+          builder: ir.builder) -> tl.tensor:\n+    condition = cast(condition, tl.int1, builder)\n+    if condition.type.is_block():\n+        x = broadcast_impl_shape(x, condition.type.get_block_shapes(), builder)\n+        y = broadcast_impl_shape(y, condition.type.get_block_shapes(), builder)\n+\n+    # TODO: we need to check x's and y's shape?\n+    x_ty = x.type.scalar\n+    y_ty = y.type.scalar\n+    ty = computation_type_impl(x_ty, y_ty, div_or_mod=False)\n+    x = cast(x, ty, builder)\n+    y = cast(y, ty, builder)\n+    if x.type.is_block():\n+        ret_ty = tl.block_type(ty, x.type.shape)\n+    else:\n+        ret_ty = ty\n+    return tl.tensor(builder.create_select(condition.handle, x.handle, y.handle), ret_ty)\n+\n+\n+# ===----------------------------------------------------------------------===//\n+#                               Reductions\n+# ===----------------------------------------------------------------------===\n+\n+def reduce_impl(input: tl.tensor, axis: int, builder: ir.builder, name: str,\n+                FLOAT_OP: ir.REDUCE_OP, INT_OP: ir.REDUCE_OP) -> tl.tensor:\n+    scalar_ty = input.type.scalar\n+    # input is extended to 32-bits if necessary\n+    # this increases numerical accuracy and can be done pretty much for free\n+    # on GPUs\n+    if scalar_ty.is_int() and scalar_ty.int_bitwidth <= 32:\n+        input = cast(input, tl.int32, builder)\n+\n+    # get result type\n+    shape = input.type.shape\n+    ret_shape = []\n+    for i, s in enumerate(shape):\n+        if i != axis:\n+            ret_shape.append(s)\n+    if len(ret_shape) == 0:\n+        res_ty = scalar_ty\n+    else:\n+        res_ty = tl.block_type(scalar_ty, ret_shape)\n+\n+    if scalar_ty.is_floating():\n+        return tl.tensor(builder.create_reduce(input.handle, FLOAT_OP, axis), res_ty)\n+    elif scalar_ty.is_int():\n+        return tl.tensor(builder.create_reduce(input.handle, INT_OP, axis), res_ty)\n+    assert False\n+\n+\n+def min(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n+    return reduce_impl(input, axis, builder, \"min\", ir.REDUCE_OP.FMIN, ir.REDUCE_OP.MIN)\n+\n+\n+def max(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n+    return reduce_impl(input, axis, builder, \"max\", ir.REDUCE_OP.FMAX, ir.REDUCE_OP.MAX)\n+\n+\n+def sum(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n+    return reduce_impl(input, axis, builder, \"sum\", ir.REDUCE_OP.FADD, ir.REDUCE_OP.ADD)\n+\n+\n+def xor_sum(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n+    scalar_ty = input.type.scalar\n+    if not scalar_ty.is_int():\n+        raise ValueError(\"xor_sum only supported for integers\")\n+    return reduce_impl(input, axis, builder, \"sum\", ir.REDUCE_OP.XOR, ir.REDUCE_OP.XOR)\n+\n+\n+# ===----------------------------------------------------------------------===\n+#                               Math\n+# ===----------------------------------------------------------------------===\n+\n+def umulhi(x: tl.tensor, y: tl.tensor, builder: ir.builder) -> tl.tensor:\n+    x, y = binary_op_type_checking_impl(x, y, builder)\n+    return tl.tensor(builder.create_umulhi(x.handle, y.handle), x.type)\n+\n+\n+def exp(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n+    return tl.tensor(builder.create_exp(x.handle), x.type)\n+\n+\n+def log(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n+    return tl.tensor(builder.create_log(x.handle), x.type)\n+\n+\n+def cos(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n+    return tl.tensor(builder.create_cos(x.handle), x.type)\n+\n+\n+def sin(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n+    return tl.tensor(builder.create_sin(x.handle), x.type)\n+\n+\n+def sqrt(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n+    return tl.tensor(builder.create_sqrt(x.handle), x.type)\n+\n+\n+##\n+\n+def multiple_of(x: tl.tensor, value: int) -> tl.tensor:\n+    x.handle.multiple_of(value)\n+    return x\n+\n+\n+def max_contiguous(x: tl.tensor, value: int) -> tl.tensor:\n+    x.handle.max_contiguous(value)\n+    return x\n+\n+\n+def debug_barrier(builder: ir.builder) -> tl.tensor:\n+    return tl.tensor(builder.create_barrier(''), tl.void)"}]