[{"filename": "python/src/triton.cc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -755,7 +755,7 @@ void init_triton_ir(py::module &&m) {\n            [](TritonOpBuilder &self) -> mlir::Type {\n              return self.getBuilder().getI64Type();\n            })\n-      .def(\"get_fp8e4_ty\",\n+      .def(\"get_fp8e4nv_ty\",\n            [](TritonOpBuilder &self) -> mlir::Type {\n              return self.getBuilder().getType<mlir::Float8E4M3FNUZType>();\n            })"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -123,7 +123,7 @@ def check_type_supported(dtype, device):\n         cc = torch.cuda.get_device_capability()\n         if cc[0] < 8 and (dtype is tl.bfloat16 or dtype == \"bfloat16\" or dtype is torch.bfloat16):\n             pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n-        if cc[0] < 9 and (dtype is tl.float8e4 or dtype == \"float8e4\"):\n+        if cc[0] < 9 and (dtype is tl.float8e4nv or dtype == \"float8e4\"):\n             pytest.skip(\"float8e4 is only supported on NVGPU with cc >= 90\")\n \n \n@@ -826,7 +826,7 @@ def test_abs(dtype_x, device):\n     _test_unary(dtype_x, 'tl.abs(x)', 'np.abs(x) ', device=device)\n \n \n-@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4, tl.float8e5])\n+@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4nv, tl.float8e5])\n def test_abs_fp8(in_dtype, device):\n \n     @triton.jit\n@@ -1358,7 +1358,7 @@ def convert_float_to_float32(fp: torch.tensor, dtype=None):\n \n     extended_exp = ((1 << (tl.float32.primitive_bitwidth - tl.float32.fp_mantissa_width - 1)) - 1) << tl.float32.fp_mantissa_width\n     # special cases, exp is 0b11..1\n-    if dtype in [tl.float8e4, tl.float8e4b15]:\n+    if dtype in [tl.float8e4nv, tl.float8e4b15]:\n         # float8e4m3 does not have infinities\n         output[fp == 0b01111111] = torch.nan\n         output[fp == 0b11111111] = torch.nan\n@@ -1418,7 +1418,7 @@ def deserialize_fp8(np_data, in_dtype):\n         return np_data\n \n \n-@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4b15x4, tl.float8e4, tl.float8e5])\n+@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4b15x4, tl.float8e4nv, tl.float8e5])\n @pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.float32])\n def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n     \"\"\""}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1062,7 +1062,7 @@ def str_to_ty(name):\n         ty = str_to_ty(name[1:])\n         return language.pointer_type(ty)\n     tys = {\n-        \"fp8e4\": language.float8e4,\n+        \"fp8e4nv\": language.float8e4nv,\n         \"fp8e5\": language.float8e5,\n         \"fp8e4b15\": language.float8e4b15,\n         \"fp8e4b15x4\": language.float8e4b15x4,"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -51,7 +51,7 @@\n     float64,\n     float8e4b15,\n     float8e4b15x4,\n-    float8e4,\n+    float8e4nv,\n     float8e5,\n     function_type,\n     int1,\n@@ -150,7 +150,7 @@\n     \"float64\",\n     \"float8e4b15\",\n     \"float8e4b15x4\",\n-    \"float8e4\",\n+    \"float8e4nv\",\n     \"float8e5\",\n     \"full\",\n     \"function_type\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -76,7 +76,7 @@ def _to_tensor(x, builder):\n class dtype:\n     SINT_TYPES = ['int8', 'int16', 'int32', 'int64']\n     UINT_TYPES = ['int1', 'uint8', 'uint16', 'uint32', 'uint64']\n-    FP_TYPES = ['fp8e4b15', 'fp8e4b15x4', 'fp8e4', 'fp8e5', 'fp16', 'bf16', 'fp32', 'fp64']\n+    FP_TYPES = ['fp8e4b15', 'fp8e4b15x4', 'fp8e4nv', 'fp8e5', 'fp16', 'bf16', 'fp32', 'fp64']\n     STANDARD_FP_TYPES = ['fp16', 'bf16', 'fp32', 'fp64']\n     OTHER_TYPES = ['void']\n \n@@ -104,7 +104,7 @@ def __init__(self, name):\n                 self.fp_mantissa_width = 3\n                 self.primitive_bitwidth = 8\n                 self.exponent_bias = 15\n-            elif name == 'fp8e4':\n+            elif name == 'fp8e4nv':\n                 self.fp_mantissa_width = 3\n                 self.primitive_bitwidth = 8\n                 self.exponent_bias = 7\n@@ -136,8 +136,8 @@ def __init__(self, name):\n     def is_fp8(self):\n         return 'fp8' in self.name\n \n-    def is_fp8e4(self):\n-        return self.name == 'fp8e4'\n+    def is_fp8e4nv(self):\n+        return self.name == 'fp8e4nv'\n \n     def is_fp8e4b15(self):\n         return self.name == 'fp8e4b15'\n@@ -244,8 +244,8 @@ def to_ir(self, builder: ir.builder) -> ir.type:\n             return builder.get_int64_ty()\n         elif self.name == 'fp8e5':\n             return builder.get_fp8e5_ty()\n-        elif self.name == 'fp8e4':\n-            return builder.get_fp8e4_ty()\n+        elif self.name == 'fp8e4nv':\n+            return builder.get_fp8e4nv_ty()\n         elif self.name == 'fp8e4b15':\n             return builder.get_fp8e4b15_ty()\n         elif self.name == 'fp8e4b15x4':\n@@ -382,7 +382,7 @@ def to_ir(self, builder: ir.builder):\n uint32 = dtype('uint32')\n uint64 = dtype('uint64')\n float8e5 = dtype('fp8e5')\n-float8e4 = dtype('fp8e4')\n+float8e4nv = dtype('fp8e4nv')\n float8e4b15 = dtype('fp8e4b15')\n float8e4b15x4 = dtype('fp8e4b15x4')\n float16 = dtype('fp16')"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -684,7 +684,7 @@ def cast(input: tl.tensor,\n \n     if _is_cuda(builder.arch) and builder.arch < 89 and \\\n        (src_sca_ty.is_fp8e4() or dst_sca_ty.is_fp8e4()):\n-        warnings.warn(\"Standard tl.float8e4 format will be deprecated on SM < 89. \"\n+        warnings.warn(\"Standard tl.float8e4nv format will be deprecated on SM < 89. \"\n                       \"Please use tl.float8e4b15.\", DeprecationWarning)\n \n     # Casting with customized floating types involved: fp8 <=> bf16, fp16, fp32, fp64"}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -151,8 +151,8 @@ def _call(a, b, dot_out_dtype):\n         M, K = a.shape\n         _, N = b.shape\n         # allocates output\n-        if a.dtype in [tl.float8e4, tl.float8e4b15, tl.float8e5] or\\\n-           b.dtype in [tl.float8e4, tl.float8e4b15, tl.float8e5]:\n+        if a.dtype in [tl.float8e4nv, tl.float8e4b15, tl.float8e5] or\\\n+           b.dtype in [tl.float8e4nv, tl.float8e4b15, tl.float8e5]:\n             c_dtype = torch.float16\n         else:\n             c_dtype = get_higher_dtype(a.dtype, b.dtype)"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -245,7 +245,7 @@ def _type_of(key):\n         dtype_str = str(key).split(\".\")[-1]\n         tys = {\n             \"bool\": \"i1\",\n-            \"float8e4\": \"fp8e4\",\n+            \"float8e4nv\": \"fp8e4nv\",\n             \"float8e5\": \"fp8e5\",\n             \"float8e4b15\": \"fp8e4b15\",\n             \"float8e4b15x4\": \"fp8e4b15x4\","}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -381,7 +381,7 @@ def get_max_tensorcore_tflops(dtype, backend=None, device=None, clock_rate=None)\n             ops_per_sub_core = 256\n         elif dtype in [torch.float16, torch.bfloat16]:\n             ops_per_sub_core = 512\n-        elif dtype in [torch.int8, tl.float8e4, tl.float8e4b15, tl.float8e5]:\n+        elif dtype in [torch.int8, tl.float8e4nv, tl.float8e4b15, tl.float8e5]:\n             ops_per_sub_core = 1024\n         else:\n             raise RuntimeError(\"dtype not supported\")"}]