[{"filename": "README.md", "status": "modified", "additions": 25, "deletions": 0, "changes": 25, "file_content_changes": "@@ -9,6 +9,31 @@\n ------------------- |\n [![Documentation](https://github.com/openai/triton/actions/workflows/documentation.yml/badge.svg)](https://triton-lang.org/)\n \n+# Triton Developer Conference Registration Open\n+The Triton Developer Conference will be held in a hybrid mode at the Microsoft Silicon Valley Campus in Mountain View, California. The conference will be held on September 20th from 10am to 4pm, followed by a reception till 5:30 pm. Please use the link below to register to attend either in-person or virtually online.\n+\n+Registration Link for Triton Developer Conference is [here](https://forms.office.com/r/m4jQXShDts)\n+\n+Tentative Agenda for the conference (subject to change):\n+\n+|Time    |Title  |Speaker\n+|--------|-------|-------|\n+|10:00 AM|Welcome|Microsoft|\n+|10:20 AM|The Triton Compiler: Past, Present and Future|Phil Tillet (OpenAI)|\n+|11:00 AM|**Break**||\n+|11:20 AM|Hopper support in Triton|Gustav Zhu (Nvidia)|\n+|11:40 AM|Bringing Triton to AMD GPUs|Jason Furmanek, Lixun Zhang (AMD)|\n+|12:00 PM|Intel XPU Backend for Triton|Eikan Wang (Intel)|\n+|12:20 PM|Vectorization of Triton Kernels for Qualcomm Hexagon Backend|Javed Absar (Qualcomm)|\n+|12:30 PM|**Lunch**||\n+|1:40 PM |Triton for MTIA|Roman Levenstein et al, (Meta)|\n+|2:00 PM |Using Triton IR for high-performance fusions in XLA|George Karpenkov (Google)|\n+|2:20 PM |Triton for All: Triton as a device-independent language|Ian Bearman (Microsoft)|\n+|2:40 PM|**Break**||\n+|3:00 PM|PyTorch 2.0 and TorchInductor|Jason Ansel, Horace He (Meta)|\n+|3:20 PM|Pallas: A JAX Kernel Language|Sharad Vikram (Google)|\n+|3:40 PM|Writing Grouped GEMMs in Triton|Vinod Grover (Nvidia)|\n+|4:00 PM|**Reception**||\n \n # Triton\n "}, {"filename": "docs/meetups/08-22-2023.md", "status": "modified", "additions": 28, "deletions": 0, "changes": 28, "file_content_changes": "@@ -11,3 +11,31 @@\n 5. Intel working on the CPU backend for Triton.\n 6. AMD updates\n 7. Open discussion\n+\n+##### Minutes:\n+Recording link [here](https://drive.google.com/file/d/19Nnc0i7zUyn-ni2RSFHbPHHiPkYU96Mz/view)\n+\n+1. H100 updates:\n+   - Preliminary support is merged, disabled by default, can be enabled with env variables\n+   - Supports latest tensor cores, FP8s. Support for Flash Attention on the main branch coming soon.\n+   - Performance is very good on Matmuls, 80-90% of cublas on large Matmuls right now, will eventually reach parity with cublas. Above 600 teraflops on fp16 on xxm card, cublas is 670 on random input data. FP8 is twice that, around 1.2 petaflops.\n+   - Hopper support includes the full FP8 support for compute.\n+2. Triton release plan update\n+   - No specific dates for now, plan is to release before end of 2023.\n+   - Will move to 3.0 release due to minor backward compatibility breaking changes. For eg. Will move compiler options in the indexing operators as hardcoded operators in the kernel, will bump the major version.\n+   - Functionally the main goal will be to have 3rd party plugins for Intel and AMD gpus.\n+   - May synchronise with a PyTorch release so that PyTorch can benefit from the latest features, however continuous integration workflow is the default release cadence expected.\n+   - Will switch the default behavior to optimized mode for the release, needs more discussion with Nvidia.\n+   - Will expose flags for a user to enable kernel selection themselves.\n+   - Open question: Pytorch hasn\u2019t rebased to latest triton, it is close to PyTorch code freeze \u2013 will PyTorch still sync with Triton 2.0? Will we have another release to support triton 2.0?\n+   - Community can start with the latest stable branch and rebase 3rd party plugin on top of that. OAI has no resources to commit to, but community can contribute.\n+3. Linalg updates\n+   - Discussion on Github for Linalg as a middle layer between the language and target hardware. Includes support for block pointers and modulo operators.\n+   - Please join the conversation [here](https://github.com/openai/triton/discussions/1842)\n+   - Branch pushed is behind the tip, will work on getting it caught up on the tip.\n+4. Intel GPU Backend status update.\n+   - Please refer to slides [here](https://github.com/openai/triton/blob/main/docs/meetups/Intel%20XPU%20Backend%20for%20Triton%20-%20Update%20-%200823.pptx)\n+5. Intel working on the CPU backend for Triton.\n+   - Please refer to slides [here](https://github.com/openai/triton/blob/main/docs/meetups/Intel%20XPU%20Backend%20for%20Triton%20-%20Update%20-%200823.pptx)\n+6. AMD updates\n+   - Please refer to slides [here](https://github.com/openai/triton/blob/main/docs/meetups/Triton_AMD_update_0823.pdf)."}, {"filename": "docs/meetups/Intel XPU Backend for Triton - Update - 0823.pptx", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "docs/meetups/Triton_AMD_update_0823.pdf", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -137,7 +137,7 @@ def TT_LoadOp : TT_Op<\"load\",\n                       [SameLoadStoreOperandsAndResultShape,\n                        SameLoadStoreOperandsAndResultEncoding,\n                        AttrSizedOperandSegments,\n-                       MemoryEffects<[MemRead]>,\n+                       DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,\n                        TypesMatchWith<\"infer ptr type from result type\",\n                                       \"result\", \"ptr\", \"$_self\",\n                                       \"mlir::OpTrait::impl::verifyLoadStorePointerAndValueType\">,"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "file_content_changes": "@@ -80,6 +80,16 @@ void LoadOp::print(OpAsmPrinter &printer) {\n   printer.printStrippedAttrOrType(getResult().getType());\n }\n \n+void LoadOp::getEffects(\n+    SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>>\n+        &effects) {\n+  effects.emplace_back(MemoryEffects::Read::get(), getPtr(),\n+                       SideEffects::DefaultResource::get());\n+  if (getIsVolatile())\n+    effects.emplace_back(MemoryEffects::Write::get(),\n+                         SideEffects::DefaultResource::get());\n+}\n+\n ParseResult StoreOp::parse(OpAsmParser &parser, OperationState &result) {\n   // Parse operands\n   SmallVector<OpAsmParser::UnresolvedOperand, 4> allOperands;"}, {"filename": "python/test/unit/tools/test_aot.py", "status": "modified", "additions": 138, "deletions": 37, "changes": 175, "file_content_changes": "@@ -23,26 +23,43 @@ def mul(x, y):\n import kernel_utils\n \n @triton.jit\n-def kernel(C, A, B,\n+def kernel(C, A, B, M, N, K,\n           stride_cm, stride_cn,\n           stride_am, stride_ak,\n           stride_bk, stride_bn,\n           BLOCK_M: tl.constexpr,\n           BLOCK_N: tl.constexpr,\n           BLOCK_K: tl.constexpr):\n-  ms = tl.arange(0, BLOCK_M)\n-  ns = tl.arange(0, BLOCK_N)\n-  ks = tl.arange(0, BLOCK_K)\n-  a = tl.load(A + ms[:, None] * stride_am + ks[None, :] * stride_ak)\n-  b = tl.load(B + ks[:, None] * stride_bk + ns[None, :] * stride_bn)\n-  c = tl.dot(a, b)\n-  c = kernel_utils.mul(c, c)\n-  tl.store(C + ms[:, None] * stride_cm + ns[None, :] * stride_cn, c)\n+  pid_m = tl.program_id(0)\n+  pid_n = tl.program_id(1)\n+\n+  offs_am = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\n+  offs_bn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\n+  offs_k = tl.arange(0, BLOCK_K)\n+  a_ptrs = A + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n+  b_ptrs = B + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n+\n+  accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n+  for k in range(0, tl.cdiv(K, BLOCK_K)):\n+      # Load the next block of A and B, generate a mask by checking the K dimension.\n+      # If it is out of bounds, set it to 0.\n+      a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_K, other=0.0)\n+      b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_K, other=0.0)\n+      # We accumulate along the K dimension.\n+      accumulator += tl.dot(a, b)\n+      # Advance the ptrs to the next K block.\n+      a_ptrs += BLOCK_K * stride_ak\n+      b_ptrs += BLOCK_K * stride_bk\n+\n+  c = kernel_utils.mul(accumulator, accumulator)\n+  # Write back the block of the output matrix C with masks.\n+  offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+  offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n+  c_ptrs = C + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n+  tl.store(c_ptrs, c)\n \"\"\"\n \n-\n-def gen_test_bin(dir, M, N, K, BM, BN, BK):\n-    test_src = '''\n+test_utils_src = '''\n #include <cuda.h>\n #include <stdio.h>\n #include <stdint.h>\n@@ -78,10 +95,23 @@ def gen_test_bin(dir, M, N, K, BM, BN, BK):\n     fclose(file);\n }'''\n \n-    test_src += f'''\n+\n+def gen_kernel_library(dir, libname):\n+    c_files = glob.glob(os.path.join(dir, \"*.c\"))\n+    subprocess.run([\"gcc\"] + c_files + [\"-I\", cuda_include_dir(),\n+                                        \"-c\", \"-fPIC\"],\n+                   check=True, cwd=dir)\n+    o_files = glob.glob(os.path.join(dir, \"*.o\"))\n+    subprocess.run([\"gcc\"] + o_files + [\"-shared\",\n+                                        \"-o\", libname,\n+                                        \"-L\", libcuda_dirs()[0]],\n+                   check=True, cwd=dir)\n+\n+\n+def gen_test_bin(dir, M, N, K, exe=\"test\", algo_id=0):\n+    test_src = f'''\n int main(int argc, char **argv) {{\n   int M = {M}, N = {N}, K = {K};\n-  int BM = {M}, BN = {N}, BK = {K};\n \n   // initialize CUDA handles\n   CUdevice dev;\n@@ -96,7 +126,7 @@ def gen_test_bin(dir, M, N, K, BM, BN, BK):\n   cuMemAlloc(&B, K * N * 2);\n   cuMemAlloc(&C, M * N * 4);\n   cuStreamCreate(&stream, 0);\n-  load_matmul_fp16xfp16_16x16x16();\n+  load_matmul_fp16();\n \n   // initialize input data\n   int16_t hA[M*K];\n@@ -110,7 +140,13 @@ def gen_test_bin(dir, M, N, K, BM, BN, BK):\n \n   // launch kernel\n   cuStreamSynchronize(stream);\n-  CUresult ret = matmul_fp16xfp16_16x16x16(stream, M/BM, N/BN, 1, C, A, B, N, 1, K, 1, N, 1);\n+  CUresult ret;\n+  int algo_id = {algo_id};\n+  if (algo_id == 0) {{\n+    ret = matmul_fp16_default(stream, C, A, B, M, N, K, N, 1, K, 1, N, 1);\n+  }} else {{\n+    ret = matmul_fp16(stream, C, A, B, M, N, K, N, 1, K, 1, N, 1, {algo_id});\n+  }}\n   if (ret != 0) fprintf(stderr, \"kernel launch failed\\\\n\");\n   assert(ret == 0);\n \n@@ -123,41 +159,51 @@ def gen_test_bin(dir, M, N, K, BM, BN, BK):\n   write_buffer_to_csv(argv[3], hC, M*N);\n \n   // free cuda handles\n-  unload_matmul_fp16xfp16_16x16x16();\n+  unload_matmul_fp16();\n   cuMemFree(A);\n   cuMemFree(B);\n   cuMemFree(C);\n   cuCtxDestroy(ctx);\n }}\n '''\n-\n+    src = test_utils_src + test_src\n     with open(os.path.join(dir, \"test.c\"), \"w\") as file:\n-        file.write(test_src)\n-    c_files = glob.glob(os.path.join(dir, \"*.c\"))\n-    subprocess.run([\"gcc\"] + c_files + [\"-I\", cuda_include_dir(),\n-                                        \"-L\", libcuda_dirs()[0],\n-                                        \"-l\", \"cuda\",\n-                                        \"-o\", \"test\"], check=True, cwd=dir)\n+        file.write(src)\n+    subprocess.run([\"gcc\"] + [\"test.c\",\n+                              \"-I\", cuda_include_dir(),\n+                              \"-L\", libcuda_dirs()[0],\n+                              \"-l\", \"cuda\",\n+                              \"-L\", dir,\n+                              \"-l\", \"kernel\",\n+                              \"-o\", exe], check=True, cwd=dir)\n \n \n-def generate_matmul_launcher(dir, dtype, BM, BN, BK, ha_hb_hints):\n+def write_triton_kernels(dir, src, util_src):\n     kernel_path = os.path.join(dir, \"kernel.py\")\n     with open(kernel_path, \"w\") as file:\n-        file.write(kernel_src)\n+        file.write(src)\n \n     kernel_utils_path = os.path.join(dir, \"kernel_utils.py\")\n     with open(kernel_utils_path, \"w\") as file:\n-        file.write(kernel_utils_src)\n+        file.write(util_src)\n \n+    return kernel_path\n+\n+\n+def compile_aot_kernels(dir, kernel_path, dtype, BM, BN, BK, ha_hb_hints):\n     compiler_path = os.path.join(triton.tools.__path__[0], \"compile.py\")\n-    linker_path = os.path.join(triton.tools.__path__[0], \"link.py\")\n \n     # compile all desired configs\n     for ha in ha_hb_hints:\n         for hb in ha_hb_hints:\n-            sig = f'*fp32:16, *{dtype}:16, *{dtype}:16, i32{ha}, i32:1, i32{hb}, i32:1, i32:16, i32:1, {BM}, {BN}, {BK}'\n-            name = f\"matmul_{dtype}x{dtype}_{BM}x{BN}x{BK}\"\n-            subprocess.run([sys.executable, compiler_path, \"-n\", \"kernel\", \"--signature\", sig, \"--out-name\", name, \"-o\", name, \"-w\", \"1\", kernel_path], check=True, cwd=dir)\n+            sig = f'*fp32:16, *{dtype}:16, *{dtype}:16, i32, i32, i32, i32{ha}, i32:1, i32{hb}, i32:1, i32:16, i32:1, {BM}, {BN}, {BK}'\n+            name = f\"matmul_{dtype}\"\n+            grid = f'M/{BM}, N/{BN}, 1'\n+            subprocess.run([sys.executable, compiler_path, \"-n\", \"kernel\", \"--signature\", sig, \"--out-name\", name, \"-o\", name, \"-w\", \"1\", \"-g\", grid, kernel_path], check=True, cwd=dir)\n+\n+\n+def link_aot_kernels(dir):\n+    linker_path = os.path.join(triton.tools.__path__[0], \"link.py\")\n \n     # link all desired configs\n     h_files = glob.glob(os.path.join(dir, \"*.h\"))\n@@ -183,17 +229,22 @@ def test_compile_link_matmul():\n         dtype = \"fp16\"\n         BM, BN, BK = 16, 16, 16\n \n-        generate_matmul_launcher(tmp_dir, dtype, BM, BN, BK, ha_hb_hints=[\"\", \":16\"])\n+        kernel_path = write_triton_kernels(tmp_dir, kernel_src, kernel_utils_src)\n+        compile_aot_kernels(tmp_dir, kernel_path, dtype, BM, BN, BK, ha_hb_hints=[\"\", \":16\"])\n+        link_aot_kernels(tmp_dir)\n \n         # compile test case\n         M, N, K = 16, 16, 16\n-        gen_test_bin(tmp_dir, M, N, K, BM, BN, BK)\n+        gen_kernel_library(tmp_dir, \"libkernel.so\")\n+        gen_test_bin(tmp_dir, M, N, K)\n \n         # initialize test data\n         a, b, a_path, b_path, c_path = generate_matmul_test_data(tmp_dir, M, N, K)\n \n         # run test case\n-        subprocess.run([\"./test\", a_path, b_path, c_path], check=True, cwd=tmp_dir)\n+        env = os.environ.copy()\n+        env[\"LD_LIBRARY_PATH\"] = tmp_dir\n+        subprocess.run([\"./test\", a_path, b_path, c_path], env=env, check=True, cwd=tmp_dir)\n \n         # read data and compare against reference\n         c = np.genfromtxt(c_path, delimiter=\",\", dtype=np.int32)\n@@ -209,23 +260,73 @@ def test_launcher_has_no_available_kernel():\n         dtype = \"fp16\"\n         BM, BN, BK = 16, 16, 16\n \n-        generate_matmul_launcher(tmp_dir, dtype, BM, BN, BK, ha_hb_hints=[\":1\"])\n+        kernel_path = write_triton_kernels(tmp_dir, kernel_src, kernel_utils_src)\n+        compile_aot_kernels(tmp_dir, kernel_path, dtype, BM, BN, BK, ha_hb_hints=[\":1\"])\n+        link_aot_kernels(tmp_dir)\n \n         # compile test case\n         M, N, K = 16, 16, 16\n-        gen_test_bin(tmp_dir, M, N, K, BM, BN, BK)\n+        gen_kernel_library(tmp_dir, \"libkernel.so\")\n+        gen_test_bin(tmp_dir, M, N, K)\n \n         # initialize test data\n         a, b, a_path, b_path, c_path = generate_matmul_test_data(tmp_dir, M, N, K)\n \n         # run test case\n-        result = subprocess.run([\"./test\", a_path, b_path, c_path], cwd=tmp_dir, capture_output=True, text=True)\n+        env = os.environ.copy()\n+        env[\"LD_LIBRARY_PATH\"] = tmp_dir\n+        result = subprocess.run([\"./test\", a_path, b_path, c_path], env=env, cwd=tmp_dir, capture_output=True, text=True)\n \n         # It should fail since the launcher requires all the strides be 1 while they are not.\n         assert result.returncode == -6\n         assert \"kernel launch failed\" in result.stderr\n \n \n+def test_compile_link_autotune_matmul():\n+    np.random.seed(3)\n+\n+    with tempfile.TemporaryDirectory() as tmp_dir:\n+\n+        dtype = \"fp16\"\n+\n+        kernel_path = write_triton_kernels(tmp_dir, kernel_src, kernel_utils_src)\n+\n+        tile_sizes = [\n+            [16, 16, 16],\n+            [32, 32, 16],\n+            [32, 32, 32],\n+            [64, 64, 32],\n+        ]\n+\n+        for ts in tile_sizes:\n+            BM, BN, BK = ts[0], ts[1], ts[2]\n+            compile_aot_kernels(tmp_dir, kernel_path, dtype, BM, BN, BK, ha_hb_hints=[\"\", \":16\"])\n+\n+        link_aot_kernels(tmp_dir)\n+\n+        gen_kernel_library(tmp_dir, \"libkernel.so\")\n+\n+        # compile test case\n+        M, N, K = 64, 64, 64\n+        # initialize test data\n+        a, b, a_path, b_path, c_path = generate_matmul_test_data(tmp_dir, M, N, K)\n+        c_ref = np.matmul(a.astype(np.float32), b.astype(np.float32))\n+\n+        for algo_id in range(len(tile_sizes)):\n+            # generate and run test case\n+            test_name = f\"test_{algo_id}\"\n+            gen_test_bin(tmp_dir, M, N, K, exe=test_name, algo_id=algo_id)\n+\n+            env = os.environ.copy()\n+            env[\"LD_LIBRARY_PATH\"] = tmp_dir\n+            subprocess.run([f\"./{test_name}\", a_path, b_path, c_path], check=True, cwd=tmp_dir, env=env)\n+\n+            # read data and compare against reference\n+            c = np.genfromtxt(c_path, delimiter=\",\", dtype=np.int32)\n+            c_tri = c.reshape((M, N)).view(np.float32)\n+            np.testing.assert_allclose(c_tri, c_ref * c_ref, atol=1e-4, rtol=1e-4)\n+\n+\n def test_ttgir_to_ptx():\n     src = \"\"\"\n module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32, \"triton_gpu.num-ctas\" = 1 : i32} {"}, {"filename": "python/triton/tools/compile.c", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -54,9 +54,12 @@ void load_{kernel_name}() {{\n /*\n {kernel_docstring}\n */\n-CUresult {kernel_name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {signature}) {{\n+CUresult {kernel_name}(CUstream stream, {signature}) {{\n     if ({kernel_name}_func == NULL)\n        load_{kernel_name}();\n+    unsigned int gX = {gridX};\n+    unsigned int gY = {gridY};\n+    unsigned int gZ = {gridZ};\n     void *args[{num_args}] = {{ {arg_pointers} }};\n     // TODO: shared memory\n     if(gX * gY * gZ > 0)"}, {"filename": "python/triton/tools/compile.h", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "file_content_changes": "@@ -10,7 +10,5 @@\n \n void unload_{kernel_name}(void);\n void load_{kernel_name}(void);\n-// tt-linker: {kernel_name}:{full_signature}\n-CUresult{_placeholder} {kernel_name}(CUstream stream, unsigned int gX,\n-                                     unsigned int gY, unsigned int gZ,\n-                                     {signature});\n+// tt-linker: {kernel_name}:{full_signature}:{algo_info}\n+CUresult{_placeholder} {kernel_name}(CUstream stream, {signature});"}, {"filename": "python/triton/tools/compile.py", "status": "modified", "additions": 14, "deletions": 3, "changes": 17, "file_content_changes": "@@ -43,10 +43,11 @@\n     parser.add_argument(\"path\", help=\"Path to Python source containing desired kernel in its scope. File will be executed.\")\n     parser.add_argument(\"--kernel-name\", \"-n\", type=str, default=\"\", help=\"Name of the kernel to compile\", required=True)\n     parser.add_argument(\"--num-warps\", \"-w\", type=int, default=1, help=\"Number of warps to launch the kernel\")\n-    parser.add_argument(\"--num-stages\", \"-ns\", type=int, default=3, help=\"Number of stages meta-parameter for the kernel\")\n+    parser.add_argument(\"--num-stages\", \"-ns\", type=int, default=3, help=\"Number of stages (meta-parameter of the kernel)\")\n     parser.add_argument(\"--out-name\", \"-on\", type=str, default=None, help=\"Out name for the compiled kernel\")\n     parser.add_argument(\"--out-path\", \"-o\", type=Path, default=None, help=\"Out filename\")\n     parser.add_argument(\"--signature\", \"-s\", type=str, help=\"Signature of the kernel\", required=True)\n+    parser.add_argument(\"--grid\", \"-g\", type=str, help=\"Launch grid of the kernel\", required=True)\n     args = parser.parse_args()\n \n     out_name = args.out_name if args.out_name else args.kernel_name\n@@ -59,6 +60,8 @@\n     mod = importlib.util.module_from_spec(spec)\n     spec.loader.exec_module(mod)\n     kernel = getattr(mod, args.kernel_name)\n+    grid = args.grid.split(\",\")\n+    assert len(grid) == 3\n \n     # validate and parse signature\n     signature = list(map(lambda s: s.strip(\" \"), args.signature.split(\",\")))\n@@ -68,7 +71,8 @@ def hash_signature(signature: List[str]):\n         m.update(\" \".join(signature).encode())\n         return m.hexdigest()[:8]\n \n-    sig_hash = hash_signature(signature)\n+    meta_sig = f\"warps{args.num_warps}xstages{args.num_stages}\"\n+    sig_hash = hash_signature(signature + [meta_sig])\n \n     def constexpr(s):\n         try:\n@@ -88,6 +92,9 @@ def constexpr(s):\n     constexprs = {i: constexpr(s) for i, s in enumerate(signature)}\n     constexprs = {k: v for k, v in constexprs.items() if v is not None}\n     signature = {i: s.split(\":\")[0] for i, s in enumerate(signature) if i not in constexprs}\n+    const_sig = 'x'.join([str(v) for v in constexprs.values()])\n+    doc_string = [f\"{kernel.arg_names[i]}={constexprs[i]}\" for i in constexprs.keys()]\n+    doc_string += [f\"num_warps={args.num_warps}\", f\"num_stages={args.num_stages}\"]\n \n     # compile ast into cubin\n     for h in hints.values():\n@@ -119,9 +126,13 @@ def constexpr(s):\n         \"full_signature\": \", \".join([f\"{ty_to_cpp(signature[i])} {kernel.arg_names[i]}\" for i in signature.keys()]),\n         \"arg_pointers\": \", \".join([f\"&{arg}\" for arg in arg_names]),\n         \"num_args\": len(arg_names),\n-        \"kernel_docstring\": \"\",\n+        \"kernel_docstring\": doc_string,\n         \"shared\": ccinfo.shared,\n         \"num_warps\": args.num_warps,\n+        \"algo_info\": '_'.join([const_sig, meta_sig]),\n+        \"gridX\": grid[0],\n+        \"gridY\": grid[1],\n+        \"gridZ\": grid[2],\n         \"_placeholder\": \"\",\n     }\n     for ext in ['h', 'c']:"}, {"filename": "python/triton/tools/link.py", "status": "modified", "additions": 109, "deletions": 15, "changes": 124, "file_content_changes": "@@ -15,10 +15,12 @@ class LinkerError(Exception):\n \n @dataclass\n class KernelLinkerMeta:\n+    orig_kernel_name: str\n     arg_names: Sequence[str]\n     arg_ctypes: Sequence[str]\n     sizes: Sequence[Union[int, None]]\n     sig_hash: str\n+    triton_suffix: str\n     suffix: str\n     num_specs: int\n     \"\"\" number of specialized arguments \"\"\"\n@@ -29,8 +31,8 @@ def __init__(self) -> None:\n         import re\n \n         # [kernel_name, c signature]\n-        self.linker_directives = re.compile(\"//[\\\\s]*tt-linker:[\\\\s]*([\\\\w]+):(.+)\")\n-        # [name, suffix]\n+        self.linker_directives = re.compile(\"//[\\\\s]*tt-linker:[\\\\s]*([\\\\w]+):(.+):(.+)\")\n+        # [name, hash, suffix]\n         self.kernel_name = re.compile(\"^([\\\\w]+)_([\\\\w]+)_([\\\\w]+)$\")\n         # [(type, name)]\n         self.c_sig = re.compile(\"[\\\\s]*(\\\\w+)\\\\s(\\\\w+)[,]?\")\n@@ -45,17 +47,19 @@ def extract_linker_meta(self, header: str):\n             if ln.startswith(\"//\"):\n                 m = self.linker_directives.match(ln)\n                 if _exists(m):\n-                    ker_name, c_sig = m.group(1), m.group(2)\n+                    ker_name, c_sig, algo_info = m.group(1), m.group(2), m.group(3)\n                     name, sig_hash, suffix = self._match_name(ker_name)\n                     c_types, arg_names = self._match_c_sig(c_sig)\n                     num_specs, sizes = self._match_suffix(suffix, c_sig)\n                     self._add_kernel(\n-                        name,\n+                        \"_\".join([name, algo_info]),\n                         KernelLinkerMeta(\n+                            orig_kernel_name=name,\n                             arg_names=arg_names,\n                             arg_ctypes=c_types,\n                             sizes=sizes,\n                             sig_hash=sig_hash,\n+                            triton_suffix=suffix,\n                             suffix=suffix,\n                             num_specs=num_specs,\n                         ),\n@@ -126,44 +130,107 @@ def gen_signature(m):\n     return sig\n \n \n-def make_decls(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n+# generate declarations of kernels with meta-parameter and constant values\n+def make_algo_decls(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n     return f\"\"\"\n-CUresult {name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {gen_signature_with_full_args(metas[-1])});\n+CUresult {name}(CUstream stream, {gen_signature_with_full_args(metas[-1])});\n void load_{name}();\n void unload_{name}();\n     \"\"\"\n \n \n-def make_kernel_dispatcher(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n+# generate declarations of kernels with meta-parameter and constant values\n+def make_global_decl(meta: KernelLinkerMeta) -> str:\n+    return f\"\"\"\n+CUresult {meta.orig_kernel_name}_default(CUstream stream, {gen_signature_with_full_args(meta)});\n+CUresult {meta.orig_kernel_name}(CUstream stream, {gen_signature_with_full_args(meta)}, int algo_id);\n+void load_{meta.orig_kernel_name}();\n+void unload_{meta.orig_kernel_name}();\n+    \"\"\"\n+\n+\n+# generate dispatcher function for kernels with different meta-parameter and constant values\n+def make_default_algo_kernel(meta: KernelLinkerMeta) -> str:\n+    src = f\"CUresult {meta.orig_kernel_name}_default(CUstream stream, {gen_signature_with_full_args(meta)}){{\\n\"\n+    src += f\"  return {meta.orig_kernel_name}(stream, {', '.join(meta.arg_names)}, 0);\\n\"\n+    src += \"}\\n\"\n+    return src\n+\n+\n+# generate dispatcher function for kernels with different integer value hints\n+def make_kernel_hints_dispatcher(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n     src = f\"// launcher for: {name}\\n\"\n     for meta in sorted(metas, key=lambda m: -m.num_specs):\n-        src += f\"CUresult {name}_{meta.sig_hash}_{meta.suffix}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {gen_signature(meta)});\\n\"\n+        src += f\"CUresult {meta.orig_kernel_name}_{meta.sig_hash}_{meta.suffix}(CUstream stream, {gen_signature(meta)});\\n\"\n     src += \"\\n\"\n \n-    src += f\"CUresult {name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {gen_signature_with_full_args(metas[-1])}){{\"\n+    src += f\"CUresult {name}(CUstream stream, {gen_signature_with_full_args(metas[-1])}){{\"\n     src += \"\\n\"\n     for meta in sorted(metas, key=lambda m: -m.num_specs):\n         cond_fn = lambda val, hint: f\"({val} % {hint} == 0)\" if hint == 16 else f\"({val} == {hint})\" if hint == 1 else None\n         conds = \" && \".join([cond_fn(val, hint) for val, hint in zip(meta.arg_names, meta.sizes) if hint is not None])\n         src += f\"  if ({conds})\\n\"\n         arg_names = [arg for arg, hint in zip(meta.arg_names, meta.sizes) if hint != 1]\n-        src += f\"    return {name}_{meta.sig_hash}_{meta.suffix}(stream, gX, gY, gZ, {', '.join(arg_names)});\\n\"\n+        src += f\"    return {meta.orig_kernel_name}_{meta.sig_hash}_{meta.suffix}(stream, {', '.join(arg_names)});\\n\"\n     src += \"\\n\"\n     src += \"  return CUDA_ERROR_INVALID_VALUE;\\n\"\n     src += \"}\\n\"\n \n     for mode in [\"load\", \"unload\"]:\n         src += f\"\\n// {mode} for: {name}\\n\"\n         for meta in sorted(metas, key=lambda m: -m.num_specs):\n-            src += f\"void {mode}_{name}_{meta.sig_hash}_{meta.suffix}();\\n\"\n+            src += f\"void {mode}_{meta.orig_kernel_name}_{meta.sig_hash}_{meta.suffix}();\\n\"\n         src += f\"void {mode}_{name}() {{\"\n         src += \"\\n\"\n         for meta in sorted(metas, key=lambda m: -m.num_specs):\n-            src += f\"  {mode}_{name}_{meta.sig_hash}_{meta.suffix}();\\n\"\n+            src += f\"  {mode}_{meta.orig_kernel_name}_{meta.sig_hash}_{meta.suffix}();\\n\"\n         src += \"}\\n\"\n     return src\n \n \n+# generate dispatcher function for kernels with different meta-parameter and constant values\n+def make_kernel_meta_const_dispatcher(meta: KernelLinkerMeta) -> str:\n+    src = f\"CUresult {meta.orig_kernel_name}(CUstream stream, {gen_signature_with_full_args(meta)}, int algo_id){{\\n\"\n+    src += f\"  assert (algo_id < (int)sizeof({meta.orig_kernel_name}_kernels));\\n\"\n+    src += f\"  return {meta.orig_kernel_name}_kernels[algo_id](stream, {', '.join(meta.arg_names)});\\n\"\n+    src += \"}\\n\"\n+    return src\n+\n+\n+# generate definition of function pointers of kernel dispatchers based on meta-parameter and constant values\n+def make_func_pointers(names: str, meta: KernelLinkerMeta) -> str:\n+    # the table of hint dispatchers\n+    src = f\"typedef CUresult (*kernel_func_t)(CUstream stream, {gen_signature_with_full_args(meta)});\\n\"\n+    src += f\"kernel_func_t {meta.orig_kernel_name}_kernels[] = {{\\n\"\n+    for name in names:\n+        src += f\"  {name},\\n\"\n+    src += \"};\\n\"\n+    return src\n+\n+\n+# generate definition for load/unload functions for kernels with different meta-parameter and constant values\n+def make_kernel_load_def(names: str, meta: KernelLinkerMeta) -> str:\n+    src = \"\"\n+    for mode in [\"load\", \"unload\"]:\n+        src += f\"void {mode}_{meta.orig_kernel_name}(void){{\\n\"\n+        for name in names:\n+            src += f\"  {mode}_{name}();\\n\"\n+        src += \"}\\n\\n\"\n+    return src\n+\n+\n+def make_get_num_algos_decl(meta: KernelLinkerMeta) -> str:\n+    src = f\"int {meta.orig_kernel_name}_get_num_algos(void);\"\n+    return src\n+\n+\n+def make_get_num_algos_def(meta: KernelLinkerMeta) -> str:\n+    src = f\"int {meta.orig_kernel_name}_get_num_algos(void){{\\n\"\n+    src += f\"  return (int)sizeof({meta.orig_kernel_name}_kernels);\\n\"\n+    src += \"}\\n\"\n+    return src\n+\n+\n desc = \"\"\"\n Triton ahead-of-time linker:\n \n@@ -198,16 +265,43 @@ def make_kernel_dispatcher(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n         parser.extract_linker_meta(h_str)\n \n     # generate headers\n-    decls = [make_decls(name, meta) for name, meta in parser.kernels.items()]\n+    algo_decls = [make_algo_decls(name, meta) for name, meta in parser.kernels.items()]\n+    meta_lists = [meta for name, meta in parser.kernels.items()]\n+    meta = meta_lists[0][0]\n+    get_num_algos_decl = make_get_num_algos_decl(meta)\n+    global_decl = make_global_decl(meta)\n     with args.out.with_suffix(\".h\").open(\"w\") as fp:\n-        fp.write(\"#include <cuda.h>\\n\" + \"\\n\".join(decls))\n+        out = \"#include <cuda.h>\\n\"\n+        out += \"\\n\".join(algo_decls)\n+        out += \"\\n\"\n+        out += get_num_algos_decl\n+        out += \"\\n\"\n+        out += global_decl\n+        fp.write(out)\n \n     # generate source\n-    defs = [make_kernel_dispatcher(name, meta) for name, meta in parser.kernels.items()]\n+    defs = [make_kernel_hints_dispatcher(name, meta) for name, meta in parser.kernels.items()]\n+    names = [name for name in parser.kernels.keys()]\n+    func_pointers_def = make_func_pointers(names, meta)\n+    meta_const_def = make_kernel_meta_const_dispatcher(meta)\n+    load_unload_def = make_kernel_load_def(names, meta)\n+    get_num_algos_def = make_get_num_algos_def(meta)\n+    default_algo_kernel = make_default_algo_kernel(meta)\n     with args.out.with_suffix(\".c\").open(\"w\") as fp:\n         out = \"\"\n         out += \"#include <cuda.h>\\n\"\n         out += \"#include <stdint.h>\\n\"\n+        out += \"#include <assert.h>\\n\"\n         out += \"\\n\"\n         out += \"\\n\".join(defs)\n+        out += \"\\n\"\n+        out += func_pointers_def\n+        out += \"\\n\"\n+        out += get_num_algos_def\n+        out += \"\\n\"\n+        out += meta_const_def\n+        out += \"\\n\"\n+        out += load_unload_def\n+        out += \"\\n\"\n+        out += default_algo_kernel\n         fp.write(out)"}, {"filename": "test/Triton/canonicalize.mlir", "status": "added", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -0,0 +1,12 @@\n+// RUN: triton-opt %s -split-input-file -canonicalize | FileCheck %s\n+\n+// CHECK-LABEL: dead_load\n+tt.func @dead_load(%ptr: tensor<32x128x!tt.ptr<f16>>) {\n+  %mask = arith.constant dense<true> : tensor<32x128xi1>\n+  %other = arith.constant dense<0.00e+00> : tensor<32x128xf16>\n+  // CHECK-NOT: tt.load {{.*}} isVolatile = false\n+  //     CHECK: tt.load {{.*}} isVolatile = true\n+  %a = tt.load %ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16>\n+  %b = tt.load %ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = true} : tensor<32x128xf16>\n+  tt.return\n+}"}]