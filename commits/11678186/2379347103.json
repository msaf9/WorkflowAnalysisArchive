[{"filename": "include/triton/codegen/analysis/liveness.h", "status": "modified", "additions": 7, "deletions": 5, "changes": 12, "file_content_changes": "@@ -1,12 +1,14 @@\n #ifndef TDL_INCLUDE_IR_CODEGEN_LIVENESS_H\n #define TDL_INCLUDE_IR_CODEGEN_LIVENESS_H\n \n-#include <map>\n-#include <set>\n-#include <vector>\n #include \"triton/codegen/analysis/layout.h\"\n #include \"triton/tools/graph.h\"\n \n+#include \"llvm/ADT/MapVector.h\"\n+\n+#include <set>\n+#include <vector>\n+\n namespace triton{\n \n namespace ir{\n@@ -42,14 +44,14 @@ struct segment {\n \n class liveness {\n private:\n-  typedef std::map<shared_layout*, segment>    intervals_map_t;\n+  typedef llvm::MapVector<shared_layout*, segment>    intervals_map_t;\n \n public:\n   // constructor\n   liveness(layouts *l): layouts_(l){ }\n   // accessors\n   const intervals_map_t& get()  const { return intervals_; }\n-  segment get(shared_layout* v)  const { return intervals_.at(v); }\n+  segment get(shared_layout* v)  const { return intervals_.lookup(v); }\n   // run\n   void run(ir::module &mod);\n "}, {"filename": "include/triton/codegen/selection/generator.h", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -116,7 +116,7 @@ class generator: public ir::visitor, public analysis::layout_visitor {\n private:\n   Type *cvt(ir::type *ty);\n   llvm::Attribute cvt(ir::attribute attr);\n-  llvm::StructType* packed_type(ir::value* i);\n+  void packed_type(ir::value* i);\n   void forward_declare(ir::function* fn);\n \n public:\n@@ -177,6 +177,7 @@ class generator: public ir::visitor, public analysis::layout_visitor {\n   void visit_sqrt_inst(ir::sqrt_inst*);\n   Value* shfl_sync(Value* acc, int32_t i);\n   void visit_reduce1d_inst(ir::reduce_inst*, std::function<Value*(Value*,Value*)>, Value*);\n+  void visit_reducend_inst_fast(ir::reduce_inst* x, std::function<Value*(Value*,Value*)> do_acc, Value *neutral);\n   void visit_reducend_inst(ir::reduce_inst*, std::function<Value*(Value*,Value*)>, Value*);\n   void visit_reduce_inst(ir::reduce_inst*);\n   void visit_select_inst(ir::select_inst*);"}, {"filename": "include/triton/ir/instructions.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -475,6 +475,7 @@ class load_inst: public io_inst {\n   std::string get_eviction_policy_repr() const {\n     if (eviction_ == EVICT_FIRST) return \".L1::evict_first\";\n     if (eviction_ == EVICT_LAST) return \".L2::evict_last\";\n+    return \"\";\n   }\n   EVICTION_POLICY eviction_;\n   CACHE_MODIFIER cache_;"}, {"filename": "include/triton/ir/type.h", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -21,7 +21,6 @@ class type {\n public:\n   typedef std::vector<unsigned>\t         block_shapes_t;\n \n-protected:\n   typedef std::vector<type*>                  contained_tys_vec_t;\n   typedef contained_tys_vec_t::iterator       ty_iterator;\n   typedef contained_tys_vec_t::const_iterator const_ty_iterator;"}, {"filename": "include/triton/tools/graph.h", "status": "modified", "additions": 8, "deletions": 7, "changes": 15, "file_content_changes": "@@ -3,8 +3,9 @@\n #ifndef _TRITON_TOOLS_THREAD_GRAPH_H_\n #define _TRITON_TOOLS_THREAD_GRAPH_H_\n \n+#include \"llvm/ADT/SetVector.h\"\n+\n #include <map>\n-#include <set>\n #include <vector>\n #include <iostream>\n \n@@ -13,21 +14,21 @@ namespace tools{\n \n template<class node_t>\n class graph {\n-  typedef std::map<node_t, std::set<node_t>> edges_t;\n+  typedef std::map<node_t, llvm::SetVector<node_t>> edges_t;\n \n public:\n   typedef std::map<size_t, std::vector<node_t>> cmap_t;\n   typedef std::map<node_t, size_t> nmap_t;\n \n private:\n-  void connected_components_impl(node_t x, std::set<node_t> &nodes,\n+  void connected_components_impl(node_t x, llvm::SetVector<node_t> &nodes,\n                                  nmap_t* nmap, cmap_t* cmap, int id) const {\n     if(nmap)\n       (*nmap)[x] = id;\n     if(cmap)\n       (*cmap)[id].push_back(x);\n-    if(nodes.find(x) != nodes.end()) {\n-      nodes.erase(x);\n+    if (nodes.count(x)) {\n+      nodes.remove(x);\n       for(const node_t &y: edges_.at(x))\n         connected_components_impl(y, nodes, nmap, cmap, id);\n     }\n@@ -39,7 +40,7 @@ class graph {\n       cmap->clear();\n     if(nmap)\n       nmap->clear();\n-    std::set<node_t> nodes = nodes_;\n+    llvm::SetVector<node_t> nodes = nodes_;\n     unsigned id = 0;\n     while(!nodes.empty()){\n       connected_components_impl(*nodes.begin(), nodes, nmap, cmap, id++);\n@@ -59,7 +60,7 @@ class graph {\n   }\n \n private:\n-  std::set<node_t> nodes_;\n+  llvm::SetVector<node_t> nodes_;\n   edges_t edges_;\n };\n "}, {"filename": "lib/codegen/selection/generator.cc", "status": "modified", "additions": 57, "deletions": 47, "changes": 104, "file_content_changes": "@@ -2311,60 +2311,69 @@ inline Value* generator::shfl_sync(Value* acc, int32_t i){\n }\n \n /**\n- * \\brief Code Generation for `reduce` (1D case)\n+ * \\brief Code Generation for `reduce` (ND case)\n  */\n-void generator::visit_reduce1d_inst(ir::reduce_inst* x, std::function<Value*(Value*,Value*)> do_acc, Value *neutral) {\n-  std::map<indices_t, Value*> partial;\n+void generator::visit_reducend_inst_fast(ir::reduce_inst* x, std::function<Value*(Value*,Value*)> do_acc, Value *neutral){\n+  //\n   ir::value *arg = x->get_operand(0);\n+  analysis::scanline_layout* layout = layouts_->get(arg)->to_scanline();\n+  std::vector<unsigned> shapes = layout->get_shape();\n+  std::vector<int> order = layout->get_order();\n+  unsigned mts = layout->mts(order[0]);\n+  unsigned nts = layout->nts(order[0]);\n+  unsigned col_per_thread = shapes[order[0]] / mts;\n+  auto idxs = idxs_.at(arg);\n+  size_t n_elts = idxs.size();\n+  //\n   Type *ret_ty = cvt(x->get_type()->get_scalar_ty());\n-  Value *acc = nullptr;\n-\n-  // reduce within thread\n-  for(indices_t idx: idxs_.at(arg)){\n-    Value *val = vals_[arg][idx];\n-    acc = !acc ? val : do_acc(acc, val);\n-  }\n-  // reduce within wrap\n-  for(int i = 16; i > 0; i >>= 1)\n-    acc = do_acc(acc, shfl_sync(acc, i));\n-  // pointers\n   unsigned addr_space = shmem_->getType()->getPointerAddressSpace();\n   Value *base = bit_cast(shmem_, ptr_ty(ret_ty, addr_space));\n   Value* thread = tgt_->get_local_id(mod_, *builder_, 0);\n   Value* warp = udiv(thread, i32(32));\n   Value* lane = urem(thread, i32(32));\n-  // store warp result in shared memory\n-  add_barrier();\n-  store(neutral, gep(base, lane));\n-  add_barrier();\n-  store(acc, gep(base, warp));\n-  add_barrier();\n-\n-  // reduce across warps\n-  Value *cond = icmp_eq(warp, i32(0));\n-  Instruction *barrier = add_barrier();\n-  builder_->SetInsertPoint(barrier->getParent());\n-  Instruction* dummy = builder_->CreateRet(nullptr);\n-  Instruction *term = llvm::SplitBlockAndInsertIfThen(cond, barrier, false);\n-  dummy->removeFromParent();\n-  builder_->SetInsertPoint(term);\n-  Value* ret = load(gep(base, thread));\n-  for(int i = (num_warps_+1)/2; i > 0; i >>= 1){\n-    Value *current = shfl_sync(ret, i);\n-    ret = do_acc(ret, current);\n-  }\n-  store(ret, gep(base, thread));\n-\n-  // store first warp done\n-  builder_->SetInsertPoint(barrier->getParent());\n-  ret = load(base);\n-  for(indices_t idx: idxs_.at(x))\n-    vals_[x][idx] = ret;\n+  size_t warps_per_inner = std::max<int>(mts/32, 1);\n+  Value* warp_i = udiv(warp, i32(warps_per_inner));\n+  unsigned row_per_thread = std::max<int>(32/mts, 1);\n+\n+  for(size_t i = 0; i < n_elts/col_per_thread; i++){\n+    Value* acc;\n+    // reduce within thread\n+    for(size_t j = 0; j < col_per_thread; j++){\n+      Value* val = vals_[arg][idxs[i*col_per_thread + j]];\n+      acc = (j == 0) ? val : do_acc(acc, val);\n+    }\n+    // reduce within warp\n+    for(int k = std::min<int>(mts, 32)/2 ; k > 0; k >>= 1)\n+      acc = do_acc(acc, shfl_sync(acc, k));\n+    // store warp result in shared memory\n+    Value* ret = acc;\n+    if(mts >= 32){\n+      add_barrier();\n+      store(neutral, gep(base, lane));\n+      add_barrier();\n+      store(acc, gep(base, warp));\n+      add_barrier();\n+      // reduce across warps\n+      Value *cond = icmp_eq(warp, i32(0));\n+      Instruction *barrier = add_barrier();\n+      builder_->SetInsertPoint(barrier->getParent());\n+      Instruction* dummy = builder_->CreateRet(nullptr);\n+      Instruction *term = llvm::SplitBlockAndInsertIfThen(cond, barrier, false);\n+      dummy->removeFromParent();\n+      builder_->SetInsertPoint(term);\n+      ret = load(gep(base, thread));\n+      for(int k = (mts/32)/2; k > 0; k >>= 1){\n+        Value *current = shfl_sync(ret, k);\n+        ret = do_acc(ret, current);\n+      }\n+      store(ret, gep(base, thread));\n+      builder_->SetInsertPoint(barrier->getParent());\n+      ret = load(gep(base, warp));\n+    }\n+    vals_[x][idxs_[x][i]] = ret;\n+  }\n }\n \n-/**\n- * \\brief Code Generation for `reduce` (ND case)\n- */\n void generator::visit_reducend_inst(ir::reduce_inst* x, std::function<Value*(Value*,Value*)> do_acc, Value *neutral) {\n   ir::value *arg = x->get_operand(0);\n   Type *ty = cvt(x->get_type()->get_scalar_ty());\n@@ -2462,8 +2471,9 @@ void generator::visit_reduce_inst(ir::reduce_inst* x) {\n     default: throw std::runtime_error(\"unreachable\");\n   }\n   ir::value *arg = x->get_operand(0);\n-  if(arg->get_type()->get_tile_rank() == 1)\n-    visit_reduce1d_inst(x, do_acc, neutral);\n+  analysis::scanline_layout* scanline = layouts_->get(x->get_operand(0))->to_scanline();\n+  if(scanline && scanline->get_order()[0] == x->get_axis())\n+    visit_reducend_inst_fast(x, do_acc, neutral);\n   else\n     visit_reducend_inst(x, do_acc, neutral);\n }\n@@ -3326,7 +3336,7 @@ void generator::finalize_phi_node(ir::phi_node *x) {\n   }\n }\n \n-StructType* generator::packed_type(ir::value* i){\n+void generator::packed_type(ir::value* i){\n   Type* dtype = cvt(i->get_type()->get_tile_element_ty());\n   auto* layout = dynamic_cast<analysis::scanline_layout*>(layouts_->get(i));\n   assert(layout);"}, {"filename": "python/bench/bench_blocksparse.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -40,7 +40,7 @@ def bench_matmul(M, N, K, block, layout_mode, op_mode, AT, BT, dtype, provider,\n     # create op\n     tflops = lambda ms: num_flops / ms * 1e3\n     if provider == 'triton':\n-        op = triton.ops.blocksparse.matmul(layout, block, op_mode, trans_a=AT, trans_b=BT)\n+        op = triton.ops.blocksparse.matmul(layout, block, op_mode, device=\"cuda\", trans_a=AT, trans_b=BT)\n         # inputs\n         a = triton.testing.sparsify_tensor(a, layout, block) if op_mode == 'dsd' else a\n         b = triton.testing.sparsify_tensor(b, layout, block) if op_mode == 'dds' else b\n@@ -83,7 +83,7 @@ def bench_softmax(M, N, block, layout_mode, dtype, provider, warmup=10, rep=50):\n     a = torch.randn((Z, H, M, N), dtype=dtype, device='cuda')\n     if provider == 'triton':\n         a = triton.testing.sparsify_tensor(a, layout, block)\n-        op = triton.ops.blocksparse.softmax(layout, block)\n+        op = triton.ops.blocksparse.softmax(layout, block, device=\"cuda\")\n         gbps = lambda ms: (2 * a.numel() * a.element_size() * 1e-9) / (ms * 1e-3)\n         mean_ms, min_ms, max_ms = triton.testing.do_bench(lambda: op(a), warmup=warmup, rep=rep)\n         return gbps(mean_ms), gbps(min_ms), gbps(max_ms)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 43, "deletions": 4, "changes": 47, "file_content_changes": "@@ -514,9 +514,41 @@ def kernel(X, Z):\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n \n \n+def test_atomic_cas():\n+    # 1. make sure that atomic_cas changes the original value (Lock)\n+    @triton.jit\n+    def change_value(Lock):\n+        tl.atomic_cas(Lock, 0, 1)\n+\n+    Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n+    change_value[(1,)](Lock)\n+\n+    assert(Lock[0] == 1)\n+\n+    # 2. only one block enters the critical section\n+    @triton.jit\n+    def serialized_add(data, Lock):\n+        ptrs = data + tl.arange(0, 128)\n+        while tl.atomic_cas(Lock, 0, 1) == 1:\n+            pass\n+\n+        tl.store(ptrs, tl.load(ptrs) + 1.0)\n+\n+        # release lock\n+        tl.atomic_xchg(Lock, 0)\n+\n+    Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n+    data = torch.zeros((128,), device='cuda', dtype=torch.float32)\n+    ref = torch.full((128,), 64.0)\n+    serialized_add[(64,)](data, Lock)\n+    triton.testing.assert_almost_equal(data, ref)\n+\n+\n # ---------------\n # test cast\n # ---------------\n+\n+\n @pytest.mark.parametrize(\"dtype_x, dtype_z, bitcast\", [\n     (dtype_x, dtype_z, False)\n     for dtype_x in dtypes\n@@ -644,7 +676,7 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     )\n     assert torch.all(\n         torch.logical_not(mismatch)\n-    ), f\"{f16_input[mismatch]=} {f16_output[mismatch]=} {abs_error[mismatch]=} {min_error[mismatch]=}\"\n+    ), f\"f16_input[mismatch]={f16_input[mismatch]} f16_output[mismatch]={f16_output[mismatch]} abs_error[mismatch]={abs_error[mismatch]} min_error[mismatch]={min_error[mismatch]}\"\n \n \n # ---------------\n@@ -676,9 +708,16 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n \n \n-@pytest.mark.parametrize(\"dtype_str, shape, axis\", [\n-    (dtype, (1, 1024), 1) for dtype in ['float32', 'uint32']\n-])\n+reduce_configs1 = [\n+    (dtype, (1, 1024), axis) for dtype in ['float32', 'uint32']\n+    for axis in [1]\n+]\n+reduce_configs2 = [\n+    ('float32', shape, 1) for shape in [(2, 32), (4, 128), (32, 64), (64, 128), (128, 256), (32, 1024)]\n+]\n+\n+\n+@pytest.mark.parametrize(\"dtype_str, shape, axis\", reduce_configs1 + reduce_configs2)\n def test_reduce2d(dtype_str, shape, axis, device='cuda'):\n     # triton kernel\n     @triton.jit"}, {"filename": "python/triton/code_gen.py", "status": "modified", "additions": 34, "deletions": 19, "changes": 53, "file_content_changes": "@@ -11,6 +11,7 @@\n import sys\n import tempfile\n import textwrap\n+import threading\n import time\n import warnings\n from typing import Dict, Set, Tuple, Union\n@@ -830,6 +831,7 @@ def _type_name(obj):\n             torch.float32: 'f32',\n             torch.float64: 'f64',\n             torch.bool: 'i1',\n+            torch.uint8: 'u8',\n             torch.int8: 'i8',\n             torch.int16: 'i16',\n             torch.int32: 'i32',\n@@ -1066,27 +1068,40 @@ def __call__(self, *args, **kwargs):\n         return self.kernel(*args, num_warps=config.num_warps, num_stages=config.num_stages, **kwargs, **config.kwargs)\n \n \n-@functools.lru_cache()\n+_version_key_lock = threading.Lock()\n+_version_key = None\n+\n+\n def version_key():\n-    import pkgutil\n-    contents = []\n-    # frontend\n-    with open(triton.code_gen.__file__, \"rb\") as f:\n-        contents += [hashlib.md5(f.read()).hexdigest()]\n-    # backend\n-    with open(triton._C.libtriton.__file__, \"rb\") as f:\n-        contents += [hashlib.md5(f.read()).hexdigest()]\n-    # language\n-    language_path = os.path.join(*triton.__path__, 'language')\n-    for lib in pkgutil.iter_modules([language_path]):\n-        with open(lib.module_finder.find_spec(lib.name).origin, \"rb\") as f:\n+    global _version_key\n+\n+    if _version_key is not None:\n+        return _version_key\n+\n+    with _version_key_lock:\n+        if _version_key is not None:\n+            return _version_key\n+\n+        import pkgutil\n+        contents = []\n+        # frontend\n+        with open(triton.code_gen.__file__, \"rb\") as f:\n+            contents += [hashlib.md5(f.read()).hexdigest()]\n+        # backend\n+        with open(triton._C.libtriton.__file__, \"rb\") as f:\n             contents += [hashlib.md5(f.read()).hexdigest()]\n-    # ptxas version\n-    try:\n-        ptxas_version = hashlib.md5(subprocess.check_output([\"ptxas\", \"--version\"])).hexdigest()\n-    except Exception:\n-        ptxas_version = ''\n-    return '-'.join(triton.__version__) + '-' + ptxas_version + '-' + '-'.join(contents)\n+        # language\n+        language_path = os.path.join(*triton.__path__, 'language')\n+        for lib in pkgutil.iter_modules([language_path]):\n+            with open(lib.module_finder.find_spec(lib.name).origin, \"rb\") as f:\n+                contents += [hashlib.md5(f.read()).hexdigest()]\n+        # ptxas version\n+        try:\n+            ptxas_version = hashlib.md5(subprocess.check_output([\"ptxas\", \"--version\"])).hexdigest()\n+        except Exception:\n+            ptxas_version = ''\n+        _version_key = '-'.join(triton.__version__) + '-' + ptxas_version + '-' + '-'.join(contents)\n+        return _version_key\n \n \n class DependenciesFinder(ast.NodeVisitor):"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -806,7 +806,7 @@ def _decorator(func):\n @_add_atomic_docstr(\"compare-and-swap\")\n def atomic_cas(pointer, cmp, val, _builder=None):\n     cmp = _to_tensor(cmp, _builder)\n-    val = _to_tensor(cmp, _builder)\n+    val = _to_tensor(val, _builder)\n     return semantic.atomic_cas(pointer, cmp, val, _builder)\n \n "}, {"filename": "python/triton/tools/compare_asm.py", "status": "added", "additions": 76, "deletions": 0, "changes": 76, "file_content_changes": "@@ -0,0 +1,76 @@\n+'''\n+Compare cached triton kernels in 2 directories.\n+\n+example:\n+python compare_asm.py --dir0=triton-works/ --dir1=triton-fails/ --asm=ttir \\\n+       --diff-out0=diff-works.ll --diff-out1=diff-fails.ll\n+'''\n+import argparse\n+import os\n+import pickle\n+\n+parser = argparse.ArgumentParser(description=\"unpickle\")\n+parser.add_argument('--dir0', dest='dir0', required=True,\n+                    help=\"Triton cache dir 0\")\n+parser.add_argument('--dir1', dest='dir1', required=True,\n+                    help=\"Triton cache dir 1\")\n+parser.add_argument('--asm', dest='asm',\n+                    choices=['ttir', 'llir', 'ptx', 'cubin'], required=True)\n+parser.add_argument('--early-stop', dest='early_stop', action='store_true',\n+                    help=\"Stop after first diff\")\n+parser.set_defaults(early_stop=True)\n+parser.add_argument('--diff-out0', dest='diff_out0', required=True,\n+                    help=\"output file path for kernels in dir0\")\n+parser.add_argument('--diff-out1', dest='diff_out1', required=True,\n+                    help=\"output file path for kernels in dir1\")\n+args = parser.parse_args()\n+dir0 = args.dir0\n+dir1 = args.dir1\n+asm = args.asm\n+\n+dir0_files = {}\n+dir1_files = {}\n+for root, _, files in os.walk(dir0):\n+    for file in files:\n+        if not file.endswith('.lock'):\n+            path = os.path.join(root, file)\n+            with open(path, 'rb') as f:\n+                loaded_file = pickle.load(f)\n+                bin = loaded_file['binary']\n+                key = loaded_file['key']\n+                info = key.split('-')[-3:]  # num_warps, num_stages, signature\n+                dict_key = bin.name + '-'.join(info)\n+                dir0_files[dict_key] = bin.asm\n+\n+for root, _, files in os.walk(dir1):\n+    for file in files:\n+        if not file.endswith('.lock'):\n+            path = os.path.join(root, file)\n+            with open(path, 'rb') as f:\n+                loaded_file = pickle.load(f)\n+                bin = loaded_file['binary']\n+                key = loaded_file['key']\n+                info = key.split('-')[-3:]  # num_warps, num_stages, signature\n+                dict_key = bin.name + '-'.join(info)\n+                dir1_files[dict_key] = bin.asm\n+\n+diff_keys = []\n+for key in dir0_files:\n+    asm0 = dir0_files[key]\n+    if key not in dir1_files:\n+        continue\n+    asm1 = dir1_files[key]\n+    if asm0[asm] != asm1[asm]:\n+        diff_keys.append(key)\n+\n+if args.early_stops:\n+    diff_keys = diff_keys[:1]\n+if diff_keys:\n+    with open(args.diff_out0, 'w') as f0, open(args.diff_out1, 'w') as f1:\n+        for key in diff_keys:\n+            f0.write(f'{asm} mismatch at {key}')\n+            f0.write(dir0_files[key][asm])\n+            f0.write('\\n')\n+            f1.write(f'{asm} mismatch at {key}')\n+            f1.write(dir1_files[key][asm])\n+            f1.write('\\n')"}, {"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 175, "deletions": 125, "changes": 300, "file_content_changes": "@@ -17,98 +17,113 @@\n     HAS_APEX = False\n \n \n-# Forward Pass\n @triton.jit\n-def _layer_norm_fwd_fused(X, Y, W, B, M, V, stride, N, eps,\n-                          BLOCK_SIZE: tl.constexpr):\n+def _layer_norm_fwd_fused(\n+    Out,\n+    A,\n+    Weight,\n+    Bias,\n+    Mean, Rstd,\n+    stride, N, eps,\n+    BLOCK_SIZE: tl.constexpr,\n+):\n     # position of elements processed by this program\n     row = tl.program_id(0)\n-    cols = tl.arange(0, BLOCK_SIZE)\n-    mask = cols < N\n-    # offset data pointers to start at the row of interest\n-    X += row * stride\n-    Y += row * stride\n-    # load data and cast to float32\n-    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n+    Out += row * stride\n+    A += row * stride\n     # compute mean\n-    mean = tl.sum(x, axis=0) / N\n-    # compute std\n-    xmean = tl.where(mask, x - mean, 0.)\n-    var = tl.sum(xmean * xmean, axis=0) / N\n+    mean = 0\n+    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n+    for off in range(0, N, BLOCK_SIZE):\n+        cols = off + tl.arange(0, BLOCK_SIZE)\n+        a = tl.load(A + cols, mask=cols < N, other=0., eviction_policy=\"evict_last\").to(tl.float32)\n+        _mean += a\n+    mean = tl.sum(_mean, axis=0) / N\n+    # compute variance\n+    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n+    for off in range(0, N, BLOCK_SIZE):\n+        cols = off + tl.arange(0, BLOCK_SIZE)\n+        a = tl.load(A + cols, mask=cols < N, other=0., eviction_policy=\"evict_last\").to(tl.float32)\n+        a = tl.where(cols < N, a - mean, 0.)\n+        _var += a * a\n+    var = tl.sum(_var, axis=0) / N\n     rstd = 1 / tl.sqrt(var + eps)\n-    xhat = xmean * rstd\n     # write-back mean/rstd\n-    tl.store(M + row, mean)\n-    tl.store(V + row, rstd)\n+    tl.store(Mean + row, mean)\n+    tl.store(Rstd + row, rstd)\n     # multiply by weight and add bias\n-    w = tl.load(W + cols, mask=mask)\n-    b = tl.load(B + cols, mask=mask)\n-    y = xhat * w + b\n-    # write-back\n-    tl.store(Y + cols, y, mask=mask)\n+    for off in range(0, N, BLOCK_SIZE):\n+        cols = off + tl.arange(0, BLOCK_SIZE)\n+        mask = cols < N\n+        weight = tl.load(Weight + cols, mask=mask)\n+        bias = tl.load(Bias + cols, mask=mask)\n+        a = tl.load(A + cols, mask=mask, other=0., eviction_policy=\"evict_first\").to(tl.float32)\n+        a_hat = (a - mean) * rstd\n+        out = a_hat * weight + bias\n+        # # write-back\n+        tl.store(Out + cols, out, mask=mask)\n+\n+# Backward pass (DA + partial DW + partial DB)\n \n \n-# Backward pass (DX + partial DW + partial DB)\n @triton.jit\n-def _layer_norm_bwd_dx_fused(DX, DY, DW, DB, X, W, B, M, V, Lock, stride, N, eps,\n-                             GROUP_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):\n+def _layer_norm_bwd_dx_fused(\n+    _DA,\n+    _DOut,\n+    _A,\n+    Weight,\n+    Mean, Rstd,\n+    stride, NumRows, NumCols, eps,\n+    BLOCK_SIZE_N: tl.constexpr,\n+):\n     # position of elements processed by this program\n-    row = tl.program_id(0)\n-    cols = tl.arange(0, BLOCK_SIZE_N)\n-    mask = cols < N\n-    # offset data pointers to start at the row of interest\n-    X += row * stride\n-    DY += row * stride\n-    DX += row * stride\n-    # offset locks and weight/bias gradient pointer\n-    # each kernel instance accumulates partial sums for\n-    # DW and DB into one of GROUP_SIZE_M independent buffers\n-    # these buffers stay in the L2, which allow this kernel\n-    # to be fast\n-    lock_id = row % GROUP_SIZE_M\n-    Lock += lock_id\n-    Count = Lock + GROUP_SIZE_M\n-    DW = DW + lock_id * N + cols\n-    DB = DB + lock_id * N + cols\n+    pid = tl.program_id(0)\n+    row = pid\n+    A = _A + row * stride\n+    DOut = _DOut + row * stride\n+    DA = _DA + row * stride\n+    mean = tl.load(Mean + row)\n+    rstd = tl.load(Rstd + row)\n     # load data to SRAM\n-    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n-    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n-    w = tl.load(W + cols, mask=mask).to(tl.float32)\n-    mean = tl.load(M + row)\n-    rstd = tl.load(V + row)\n-    # compute dx\n-    xhat = (x - mean) * rstd\n-    wdy = w * dy\n-    xhat = tl.where(mask, xhat, 0.)\n-    wdy = tl.where(mask, wdy, 0.)\n-    mean1 = tl.sum(xhat * wdy, axis=0) / N\n-    mean2 = tl.sum(wdy, axis=0) / N\n-    dx = (wdy - (xhat * mean1 + mean2)) * rstd\n-    # write-back dx\n-    tl.store(DX + cols, dx, mask=mask)\n-    # accumulate partial sums for dw/db\n-    partial_dw = (dy * xhat).to(w.dtype)\n-    partial_db = (dy).to(w.dtype)\n-    while tl.atomic_cas(Lock, 0, 1) == 1:\n-        pass\n-    count = tl.load(Count)\n-    # first store doesn't accumulate\n-    if count == 0:\n-        tl.atomic_xchg(Count, 1)\n-    else:\n-        partial_dw += tl.load(DW, mask=mask)\n-        partial_db += tl.load(DB, mask=mask)\n-    tl.store(DW, partial_dw, mask=mask)\n-    tl.store(DB, partial_db, mask=mask)\n-    # release lock\n-    tl.atomic_xchg(Lock, 0)\n-\n-# Backward pass (total DW + total DB)\n+    _mean1 = tl.zeros([BLOCK_SIZE_N], dtype=tl.float32)\n+    _mean2 = tl.zeros([BLOCK_SIZE_N], dtype=tl.float32)\n+    for off in range(0, NumCols, BLOCK_SIZE_N):\n+        cols = off + tl.arange(0, BLOCK_SIZE_N)\n+        mask = cols < NumCols\n+        a = tl.load(A + cols, mask=mask, other=0).to(tl.float32)\n+        dout = tl.load(DOut + cols, mask=mask, other=0).to(tl.float32)\n+        weight = tl.load(Weight + cols, mask=mask, other=0).to(tl.float32)\n+        a_hat = (a - mean) * rstd\n+        wdout = weight * dout\n+        _mean1 += a_hat * wdout\n+        _mean2 += wdout\n+    mean1 = tl.sum(_mean1, axis=0) / NumCols\n+    mean2 = 0.\n+    mean2 = tl.sum(_mean2, axis=0) / NumCols\n+    for off in range(0, NumCols, BLOCK_SIZE_N):\n+        cols = off + tl.arange(0, BLOCK_SIZE_N)\n+        mask = cols < NumCols\n+        a = tl.load(A + cols, mask=mask, other=0).to(tl.float32)\n+        dout = tl.load(DOut + cols, mask=mask, other=0).to(tl.float32)\n+        weight = tl.load(Weight + cols, mask=mask, other=0).to(tl.float32)\n+        a_hat = (a - mean) * rstd\n+        wdout = weight * dout\n+        da = (wdout - (a_hat * mean1 + mean2)) * rstd\n+        # write-back dx\n+        tl.store(DA + cols, da, mask=mask)\n \n \n+# Backward pass (total DW + total DB)\n @triton.jit\n-def _layer_norm_bwd_dwdb(DW, DB, FINAL_DW, FINAL_DB, M, N,\n-                         BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):\n+def _layer_norm_bwd_dwdb(\n+    A, DOut,\n+    Mean, Var,\n+    DW,\n+    DB,\n+    M, N,\n+    BLOCK_SIZE_M: tl.constexpr,\n+    BLOCK_SIZE_N: tl.constexpr,\n+):\n     pid = tl.program_id(0)\n     cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n     dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n@@ -117,79 +132,113 @@ def _layer_norm_bwd_dwdb(DW, DB, FINAL_DW, FINAL_DB, M, N,\n         rows = i + tl.arange(0, BLOCK_SIZE_M)\n         mask = (rows[:, None] < M) & (cols[None, :] < N)\n         offs = rows[:, None] * N + cols[None, :]\n-        dw += tl.load(DW + offs, mask=mask, other=0.)\n-        db += tl.load(DB + offs, mask=mask, other=0.)\n+        a = tl.load(A + offs, mask=mask, other=0.).to(tl.float32)\n+        dout = tl.load(DOut + offs, mask=mask, other=0.).to(tl.float32)\n+        mean = tl.load(Mean + rows, mask=rows < M, other=0.)\n+        rstd = tl.load(Var + rows, mask=rows < M, other=0.)\n+        a_hat = (a - mean[:, None]) * rstd[:, None]\n+        dw += dout * a_hat\n+        db += dout\n     sum_dw = tl.sum(dw, axis=0)\n     sum_db = tl.sum(db, axis=0)\n-    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n-    tl.store(FINAL_DB + cols, sum_db, mask=cols < N)\n+    tl.store(DW + cols, sum_dw, mask=cols < N)\n+    tl.store(DB + cols, sum_db, mask=cols < N)\n \n \n class LayerNorm(torch.autograd.Function):\n-\n     @staticmethod\n-    def forward(ctx, x, normalized_shape, weight, bias, eps):\n+    def forward(ctx, a, normalized_shape, weight, bias, eps):\n         # allocate output\n-        y = torch.empty_like(x)\n+        out = torch.empty_like(a)\n         # reshape input data into 2D tensor\n-        x_arg = x.reshape(-1, x.shape[-1])\n-        M, N = x_arg.shape\n-        mean = torch.empty((M, ), dtype=torch.float32, device='cuda')\n-        rstd = torch.empty((M, ), dtype=torch.float32, device='cuda')\n+        a_arg = a.reshape(-1, a.shape[-1])\n+        M, N = a_arg.shape\n+        mean = torch.empty((M,), dtype=torch.float32, device=\"cuda\")\n+        rstd = torch.empty((M,), dtype=torch.float32, device=\"cuda\")\n         # Less than 64KB per feature: enqueue fused kernel\n-        MAX_FUSED_SIZE = 65536 // x.element_size()\n+        MAX_FUSED_SIZE = 65536 // a.element_size()\n         BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n-        if N > BLOCK_SIZE:\n-            raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n+        BLOCK_SIZE = max(BLOCK_SIZE, 128)\n+        BLOCK_SIZE = min(BLOCK_SIZE, 4096)\n         # heuristics for number of warps\n         num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\n-        # enqueue kernel\n-        _layer_norm_fwd_fused[(M,)](x_arg, y, weight, bias, mean, rstd,\n-                                    x_arg.stride(0), N, eps,\n-                                    BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)\n-        ctx.save_for_backward(x, weight, bias, mean, rstd)\n+        _layer_norm_fwd_fused[(M,)](\n+            out,\n+            a_arg,\n+            weight,\n+            bias,\n+            mean, rstd,\n+            a_arg.stride(0), N, eps,\n+            BLOCK_SIZE=BLOCK_SIZE,\n+            num_warps=num_warps,\n+        )\n+        ctx.save_for_backward(\n+            a, weight, bias, mean, rstd,\n+        )\n         ctx.BLOCK_SIZE = BLOCK_SIZE\n         ctx.num_warps = num_warps\n         ctx.eps = eps\n-        return y\n+        if hasattr(bias, \"config\"):\n+            assert bias.config.grad_scale_name == weight.config.grad_scale_name\n+            grad_scale_name = bias.config.grad_scale_name\n+        else:\n+            grad_scale_name = None\n+        ctx.grad_scale_gain_bias_name = grad_scale_name\n+        return out\n \n     @staticmethod\n-    def backward(ctx, dy):\n-        x, w, b, m, v = ctx.saved_tensors\n+    def backward(ctx, dout):\n+        assert dout.is_contiguous()\n+        a, weight, bias, mean, var = ctx.saved_tensors\n         # heuristics for amount of parallel reduction stream for DG/DB\n-        N = w.shape[0]\n-        GROUP_SIZE_M = 64\n-        if N <= 8192: GROUP_SIZE_M = 96\n-        if N <= 4096: GROUP_SIZE_M = 128\n-        if N <= 1024: GROUP_SIZE_M = 256\n+        N = weight.shape[0]\n         # allocate output\n-        locks = torch.zeros(2 * GROUP_SIZE_M, dtype=torch.int32, device='cuda')\n-        _dw = torch.empty((GROUP_SIZE_M, w.shape[0]), dtype=x.dtype, device=w.device)\n-        _db = torch.empty((GROUP_SIZE_M, w.shape[0]), dtype=x.dtype, device=w.device)\n-        dw = torch.empty((w.shape[0],), dtype=w.dtype, device=w.device)\n-        db = torch.empty((w.shape[0],), dtype=w.dtype, device=w.device)\n-        dx = torch.empty_like(dy)\n+        da = torch.empty_like(dout)\n         # enqueue kernel using forward pass heuristics\n         # also compute partial sums for DW and DB\n-        x_arg = x.reshape(-1, x.shape[-1])\n+        x_arg = a.reshape(-1, a.shape[-1])\n         M, N = x_arg.shape\n-        _layer_norm_bwd_dx_fused[(M,)](dx, dy, _dw, _db, x, w, b, m, v, locks,\n-                                       x_arg.stride(0), N, ctx.eps,\n-                                       BLOCK_SIZE_N=ctx.BLOCK_SIZE,\n-                                       GROUP_SIZE_M=GROUP_SIZE_M,\n-                                       num_warps=ctx.num_warps)\n-        grid = lambda meta: [triton.cdiv(N, meta['BLOCK_SIZE_N'])]\n+        dweight = torch.empty((weight.shape[0],), dtype=weight.dtype, device=weight.device)\n+        dbias = torch.empty((weight.shape[0],), dtype=weight.dtype, device=weight.device)\n+        _layer_norm_bwd_dx_fused[(M,)](\n+            da,\n+            dout,\n+            a,\n+            weight,\n+            mean, var,\n+            x_arg.stride(0), M, N,\n+            ctx.eps,\n+            BLOCK_SIZE_N=ctx.BLOCK_SIZE,\n+            num_warps=ctx.num_warps,\n+        )\n         # accumulate partial sums in separate kernel\n-        _layer_norm_bwd_dwdb[grid](_dw, _db, dw, db, GROUP_SIZE_M, N,\n-                                   BLOCK_SIZE_M=32,\n-                                   BLOCK_SIZE_N=128)\n-        return dx, None, dw, db, None\n+        grid = lambda meta: [triton.cdiv(N, meta[\"BLOCK_SIZE_N\"])]\n+        _layer_norm_bwd_dwdb[grid](\n+            a, dout,\n+            mean, var,\n+            dweight,\n+            dbias,\n+            M,\n+            N,\n+            BLOCK_SIZE_M=32,\n+            BLOCK_SIZE_N=128,\n+        )\n+        return (da, None, dweight, dbias, None, None,\n+                None, None, None, None,\n+                None,\n+                None, None, None,\n+                None,\n+                None, None, None,\n+                None, None, None,\n+                None, None, None)\n \n \n-layer_norm = LayerNorm.apply\n+def layer_norm(a, normalized_shape, weight, bias, eps):\n+    return LayerNorm.apply(a, normalized_shape, weight, bias, eps)\n \n \n def test_layer_norm(M, N, dtype, eps=1e-5, device='cuda'):\n+    torch.manual_seed(0)\n     # create data\n     x_shape = (M, N)\n     w_shape = (x_shape[-1], )\n@@ -224,11 +273,11 @@ def test_layer_norm(M, N, dtype, eps=1e-5, device='cuda'):\n         line_names=['Triton', 'Torch'] + (['Apex'] if HAS_APEX else []),\n         styles=[('blue', '-'), ('green', '-'), ('orange', '-')],\n         ylabel='GB/s',\n-        plot_name='layer-norm-backward',\n-        args={'M': 4096, 'dtype': torch.float16, 'mode': 'backward'}\n+        plot_name='layer-norm',\n+        args={'M': 4096, 'dtype': torch.float16, 'mode': 'forward'}\n     )\n )\n-def bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='cuda'):\n+def bench_layer_norm(M, N, dtype, provider, mode, eps=1e-5, device='cuda'):\n     # create data\n     x_shape = (M, N)\n     w_shape = (x_shape[-1], )\n@@ -258,4 +307,5 @@ def bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='c\n     return gbps(ms), gbps(max_ms), gbps(min_ms)\n \n \n+# test_layer_norm(1151, 8192, torch.float16)\n bench_layer_norm.run(save_path='.', print_data=True)"}]