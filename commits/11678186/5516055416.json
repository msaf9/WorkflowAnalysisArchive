[{"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 18, "deletions": 57, "changes": 75, "file_content_changes": "@@ -38,50 +38,24 @@ def nvsmi(attrs):\n mem_clocks = {'v100': 877, 'a100': 1215}\n \n matmul_data = {\n-    'v100': {\n-        # square\n-        (512, 512, 512): {'float16': 0.158},\n-        (1024, 1024, 1024): {'float16': 0.466},\n-        (2048, 2048, 2048): {'float16': 0.695},\n-        (4096, 4096, 4096): {'float16': 0.831},\n-        (8192, 8192, 8192): {'float16': 0.849},\n-        # tall-skinny\n-        (16, 1024, 1024): {'float16': 0.0128},\n-        (16, 4096, 4096): {'float16': 0.0883},\n-        (16, 8192, 8192): {'float16': 0.101},\n-        (64, 1024, 1024): {'float16': 0.073},\n-        (64, 4096, 4096): {'float16': 0.270},\n-        (64, 8192, 8192): {'float16': 0.459},\n-        (1024, 64, 1024): {'float16': 0.0692},\n-        (4096, 64, 4096): {'float16': 0.264},\n-        (8192, 64, 8192): {'float16': 0.452},\n-        # Non pow 2 shapes\n-        (1000, 200, 100): {'float16': 0.084},\n-        (1000, 200, 700): {'float16': 0.084},\n-        (994, 136, 402): {'float16': 0.084},\n-        (995, 135, 409): {'float16': 0.084},\n-        (99, 1357, 409): {'float16': 0.084},\n-    },\n     # NOTE:\n-    # A100 in the CI server is slow-ish for some reason.\n-    # On some other servers, we are getting about 90% peak for 8kx8x8k float16\n     'a100': {\n         # square\n-        (512, 512, 512): {'float16': 0.059, 'float32': 0.094, 'int8': 0.05},\n-        (1024, 1024, 1024): {'float16': 0.281, 'float32': 0.316, 'int8': 0.169},\n-        (2048, 2048, 2048): {'float16': 0.598, 'float32': 0.534, 'int8': 0.34},\n-        (4096, 4096, 4096): {'float16': 0.718, 'float32': 0.698, 'int8': 0.46},\n-        (8192, 8192, 8192): {'float16': 0.752, 'float32': 0.725, 'int8': 0.51},\n+        (512, 512, 512): {'float16': 0.061, 'float32': 0.097, 'int8': 0.05},\n+        (1024, 1024, 1024): {'float16': 0.283, 'float32': 0.313, 'int8': 0.169},\n+        (2048, 2048, 2048): {'float16': 0.618, 'float32': 0.532, 'int8': 0.34},\n+        (4096, 4096, 4096): {'float16': 0.751, 'float32': 0.726, 'int8': 0.46},\n+        (8192, 8192, 8192): {'float16': 0.786, 'float32': 0.754, 'int8': 0.51},\n         # tall-skinny\n         (16, 1024, 1024): {'float16': 0.006, 'float32': 0.009, 'int8': 0.005},\n-        (16, 4096, 4096): {'float16': 0.055, 'float32': 0.045, 'int8': 0.026},\n-        (16, 8192, 8192): {'float16': 0.074, 'float32': 0.075, 'int8': 0.043},\n+        (16, 4096, 4096): {'float16': 0.057, 'float32': 0.051, 'int8': 0.026},\n+        (16, 8192, 8192): {'float16': 0.077, 'float32': 0.077, 'int8': 0.043},\n         (64, 1024, 1024): {'float16': 0.018, 'float32': 0.023, 'int8': 0.017},\n-        (64, 4096, 4096): {'float16': 0.153, 'float32': 0.000, 'int8': 0.097},\n-        (64, 8192, 8192): {'float16': 0.324, 'float32': 0.000, 'int8': 0.174},\n-        (1024, 64, 1024): {'float16': 0.027, 'float32': 0.044, 'int8': 0.017},\n-        (4096, 64, 4096): {'float16': 0.166, 'float32': 0.134, 'int8': 0.102},\n-        (8192, 64, 8192): {'float16': 0.270, 'float32': 0.000, 'int8': 0.177},\n+        (64, 4096, 4096): {'float16': 0.150, 'float32': 0.000, 'int8': 0.097},\n+        (64, 8192, 8192): {'float16': 0.338, 'float32': 0.000, 'int8': 0.174},\n+        (1024, 64, 1024): {'float16': 0.029, 'float32': 0.046, 'int8': 0.017},\n+        (4096, 64, 4096): {'float16': 0.179, 'float32': 0.214, 'int8': 0.102},\n+        (8192, 64, 8192): {'float16': 0.278, 'float32': 0.000, 'int8': 0.177},\n     }\n }\n \n@@ -136,30 +110,17 @@ def _add(x_ptr, y_ptr, output_ptr, n_elements,\n \n \n elementwise_data = {\n-    'v100': {\n-        1024 * 16: {'float16': 0.0219, 'float32': 0.010},\n-        1024 * 64: {'float16': 0.0791, 'float32': 0.010},\n-        1024 * 256: {'float16': 0.243, 'float32': 0.010},\n-        1024 * 1024: {'float16': 0.530, 'float32': 0.010},\n-        1024 * 4096: {'float16': 0.796, 'float32': 0.010},\n-        1024 * 16384: {'float16': 0.905, 'float32': 0.010},\n-        1024 * 65536: {'float16': 0.939, 'float32': 0.010},\n-        # Non pow 2\n-        1020 * 100: {'float16': 0.010, 'float32': 0.010},\n-        995 * 125: {'float16': 0.010, 'float32': 0.010},\n-        10003 * 7007: {'float16': 0.010, 'float32': 0.010},\n-    },\n     'a100': {\n         1024 * 16: {'float16': 0.003, 'float32': 0.007},\n         1024 * 64: {'float16': 0.013, 'float32': 0.026},\n-        1024 * 256: {'float16': 0.052, 'float32': 0.105},\n-        1024 * 1024: {'float16': 0.210, 'float32': 0.404},\n-        1024 * 4096: {'float16': 0.770, 'float32': 0.672},\n-        1024 * 16384: {'float16': 0.757, 'float32': 0.813},\n-        1024 * 65536: {'float16': 0.847, 'float32': 0.866},\n+        1024 * 256: {'float16': 0.053, 'float32': 0.105},\n+        1024 * 1024: {'float16': 0.212, 'float32': 0.420},\n+        1024 * 4096: {'float16': 0.791, 'float32': 0.668},\n+        1024 * 16384: {'float16': 0.762, 'float32': 0.812},\n+        1024 * 65536: {'float16': 0.846, 'float32': 0.869},\n         # Non pow 2\n         1020 * 100: {'float16': 0.020, 'float32': 0.041},\n-        10003 * 7007: {'float16': 0.539, 'float32': 0.861},\n+        10003 * 7007: {'float16': 0.513, 'float32': 0.861},\n     }\n }\n "}]