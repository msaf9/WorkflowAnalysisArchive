[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 65, "deletions": 106, "changes": 171, "file_content_changes": "@@ -51,8 +51,7 @@ static StringRef getStructAttrsAttrName() { return \"llvm.struct_attrs\"; }\n namespace {\n \n // Create a 32-bit integer constant.\n-static Value createConstantI32(Location loc, PatternRewriter &rewriter,\n-                               int32_t v) {\n+Value createConstantI32(Location loc, PatternRewriter &rewriter, int32_t v) {\n   auto i32ty = rewriter.getIntegerType(32);\n   return rewriter.create<LLVM::ConstantOp>(loc, i32ty,\n                                            IntegerAttr::get(i32ty, v));\n@@ -71,16 +70,16 @@ Value createConstantF64(Location loc, PatternRewriter &rewriter, float v) {\n }\n \n // Create an index type constant.\n-static Value createIndexConstant(OpBuilder &builder, Location loc,\n-                                 TypeConverter *converter, int64_t value) {\n+Value createIndexConstant(OpBuilder &builder, Location loc,\n+                          TypeConverter *converter, int64_t value) {\n   Type ty = converter->convertType(builder.getIndexType());\n   return builder.create<LLVM::ConstantOp>(loc, ty,\n                                           builder.getIntegerAttr(ty, value));\n }\n \n // Create an integer constant of \\param width bits.\n-static Value createLLVMIntegerConstant(OpBuilder &builder, Location loc,\n-                                       short width, int64_t value) {\n+Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n+                                int64_t value) {\n   Type ty = builder.getIntegerType(width);\n   return builder.create<LLVM::ConstantOp>(loc, ty,\n                                           builder.getIntegerAttr(ty, value));\n@@ -187,9 +186,8 @@ template <typename T> void printScalar(const T &e, const std::string &info) {\n /// Only retain those attributes that are not constructed by\n /// `LLVMFuncOp::build`. If `filterArgAttrs` is set, also filter out argument\n /// attributes.\n-static void filterFuncAttributes(ArrayRef<NamedAttribute> attrs,\n-                                 bool filterArgAttrs,\n-                                 SmallVectorImpl<NamedAttribute> &result) {\n+void filterFuncAttributes(ArrayRef<NamedAttribute> attrs, bool filterArgAttrs,\n+                          SmallVectorImpl<NamedAttribute> &result) {\n   for (const auto &attr : attrs) {\n     if (attr.getName() == SymbolTable::getSymbolAttrName() ||\n         attr.getName() == FunctionOpInterface::getTypeAttrName() ||\n@@ -202,7 +200,7 @@ static void filterFuncAttributes(ArrayRef<NamedAttribute> attrs,\n }\n \n /// Helper function for wrapping all attributes into a single DictionaryAttr\n-static auto wrapAsStructAttrs(OpBuilder &b, ArrayAttr attrs) {\n+auto wrapAsStructAttrs(OpBuilder &b, ArrayAttr attrs) {\n   return DictionaryAttr::get(\n       b.getContext(), b.getNamedAttr(LLVM::getStructAttrsAttrName(), attrs));\n }\n@@ -359,7 +357,7 @@ Value getStructFromElements(Location loc, ValueRange resultVals,\n \n // delinearize supposing order is [0, 1, .. , n]\n template <typename T>\n-static SmallVector<T> getMultiDimIndexImpl(T linearIndex, ArrayRef<T> shape) {\n+SmallVector<T> getMultiDimIndexImpl(T linearIndex, ArrayRef<T> shape) {\n   // shape: {a, b, c, d}  ->  accMul: {1, a, a*b, a*b*c}\n   size_t rank = shape.size();\n   T accMul = product(shape.drop_back());\n@@ -376,8 +374,8 @@ static SmallVector<T> getMultiDimIndexImpl(T linearIndex, ArrayRef<T> shape) {\n }\n \n template <typename T>\n-static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape,\n-                                       ArrayRef<unsigned> order) {\n+SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape,\n+                                ArrayRef<unsigned> order) {\n   size_t rank = shape.size();\n   assert(rank == order.size());\n   auto reordered = reorder(shape, order);\n@@ -391,7 +389,7 @@ static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape,\n \n // linearize supposing order is [0, 1, .. , n]\n template <typename T>\n-static T getLinearIndexImpl(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n+T getLinearIndexImpl(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n   assert(multiDimIndex.size() == shape.size());\n   // shape: {a, b, c, d}  ->  accMul: {1, a, a*b, a*b*c}\n   size_t rank = shape.size();\n@@ -407,15 +405,15 @@ static T getLinearIndexImpl(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n }\n \n template <typename T>\n-static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape,\n-                        ArrayRef<unsigned> order) {\n+T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape,\n+                 ArrayRef<unsigned> order) {\n   assert(shape.size() == order.size());\n   return getLinearIndexImpl<T>(reorder(multiDimIndex, order),\n                                reorder(shape, order));\n }\n \n-static Value storeShared(ConversionPatternRewriter &rewriter, Location loc,\n-                         Value ptr, Value val, Value pred) {\n+Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n+                  Value val, Value pred) {\n   MLIRContext *ctx = rewriter.getContext();\n   unsigned bits = val.getType().getIntOrFloatBitWidth();\n   const char *c = bits == 64 ? \"l\" : (bits == 16 ? \"h\" : \"r\");\n@@ -517,10 +515,9 @@ struct ConvertTritonGPUOpToLLVMPatternBase {\n                                   ConversionPatternRewriter &rewriter) {\n     auto elems = getElementsFromStruct(loc, llvmStruct, rewriter);\n     auto rank = (elems.size() - 1) / 2;\n-    return SharedMemoryObject(\n-        /*base=*/elems[0],\n-        /*strides=*/{elems.begin() + 1, elems.begin() + 1 + rank},\n-        /*offsets=*/{elems.begin() + 1 + rank, elems.end()});\n+    return {/*base=*/elems[0],\n+            /*strides=*/{elems.begin() + 1, elems.begin() + 1 + rank},\n+            /*offsets=*/{elems.begin() + 1 + rank, elems.end()}};\n   }\n \n   static Value\n@@ -1018,13 +1015,13 @@ struct ArithConstantSplatOpConversion\n \n // Contains some helper functions for both Load and Store conversions.\n struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n-  LoadStoreConversionBase(AxisInfoAnalysis &axisAnalysisPass)\n+  explicit LoadStoreConversionBase(AxisInfoAnalysis &axisAnalysisPass)\n       : AxisAnalysisPass(axisAnalysisPass) {}\n \n   // Get corresponding LLVM element values of \\param value.\n-  SmallVector<Value> getLLVMElems(Value value, Value llValue,\n-                                  ConversionPatternRewriter &rewriter,\n-                                  Location loc) const {\n+  static SmallVector<Value> getLLVMElems(Value value, Value llValue,\n+                                         ConversionPatternRewriter &rewriter,\n+                                         Location loc) {\n     if (!value)\n       return {};\n     if (!llValue.getType().isa<LLVM::LLVMStructType>())\n@@ -1601,7 +1598,7 @@ void ReduceOpConversion::accumulate(ConversionPatternRewriter &rewriter,\n   default:\n     llvm::report_fatal_error(\"Unsupported reduce op\");\n   }\n-};\n+}\n \n Value ReduceOpConversion::shflSync(ConversionPatternRewriter &rewriter,\n                                    Location loc, Value val, int i) const {\n@@ -1946,7 +1943,7 @@ struct PrintfOpConversion\n     std::string formatStr;\n     llvm::raw_string_ostream os(formatStr);\n     os << op.prefix();\n-    if (operands.size() > 0) {\n+    if (!operands.empty()) {\n       os << getFormatSubstr(operands[0]);\n     }\n \n@@ -2130,7 +2127,7 @@ struct MakeRangeOpConversion\n     auto idxs = emitIndices(loc, rewriter, layout, shape);\n     unsigned elems = idxs.size();\n     SmallVector<Value> retVals(elems);\n-    for (auto multiDim : llvm::enumerate(idxs)) {\n+    for (const auto &multiDim : llvm::enumerate(idxs)) {\n       assert(multiDim.value().size() == 1);\n       retVals[multiDim.index()] = add(multiDim.value()[0], start);\n     }\n@@ -2633,7 +2630,7 @@ struct FpToFpOpConversion\n };\n \n // A CRTP style of base class.\n-template <typename SourceOp, typename DestOp, typename ConcreteT>\n+template <typename SourceOp, typename ConcreteT>\n class ElementwiseOpConversionBase\n     : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n public:\n@@ -2688,16 +2685,16 @@ class ElementwiseOpConversionBase\n template <typename SourceOp, typename DestOp>\n struct ElementwiseOpConversion\n     : public ElementwiseOpConversionBase<\n-          SourceOp, DestOp, ElementwiseOpConversion<SourceOp, DestOp>> {\n+          SourceOp, ElementwiseOpConversion<SourceOp, DestOp>> {\n   using Base =\n-      ElementwiseOpConversionBase<SourceOp, DestOp,\n+      ElementwiseOpConversionBase<SourceOp,\n                                   ElementwiseOpConversion<SourceOp, DestOp>>;\n   using Base::Base;\n   using OpAdaptor = typename Base::OpAdaptor;\n \n   explicit ElementwiseOpConversion(LLVMTypeConverter &typeConverter,\n                                    PatternBenefit benefit = 1)\n-      : ElementwiseOpConversionBase<SourceOp, DestOp, ElementwiseOpConversion>(\n+      : ElementwiseOpConversionBase<SourceOp, ElementwiseOpConversion>(\n             typeConverter, benefit) {}\n \n   // An interface to support variant DestOp builder.\n@@ -2714,10 +2711,10 @@ struct ElementwiseOpConversion\n //\n \n struct CmpIOpConversion\n-    : public ElementwiseOpConversionBase<triton::gpu::CmpIOp, LLVM::ICmpOp,\n+    : public ElementwiseOpConversionBase<triton::gpu::CmpIOp,\n                                          CmpIOpConversion> {\n-  using Base = ElementwiseOpConversionBase<triton::gpu::CmpIOp, LLVM::ICmpOp,\n-                                           CmpIOpConversion>;\n+  using Base =\n+      ElementwiseOpConversionBase<triton::gpu::CmpIOp, CmpIOpConversion>;\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n@@ -2755,17 +2752,18 @@ struct CmpIOpConversion\n };\n \n struct CmpFOpConversion\n-    : public ElementwiseOpConversionBase<triton::gpu::CmpFOp, LLVM::FCmpOp,\n+    : public ElementwiseOpConversionBase<triton::gpu::CmpFOp,\n                                          CmpFOpConversion> {\n-  using Base = ElementwiseOpConversionBase<triton::gpu::CmpFOp, LLVM::FCmpOp,\n-                                           CmpFOpConversion>;\n+  using Base =\n+      ElementwiseOpConversionBase<triton::gpu::CmpFOp, CmpFOpConversion>;\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n   // An interface to support variant DestOp builder.\n-  LLVM::FCmpOp createDestOp(triton::gpu::CmpFOp op, OpAdaptor adaptor,\n-                            ConversionPatternRewriter &rewriter, Type elemTy,\n-                            ValueRange operands, Location loc) const {\n+  static LLVM::FCmpOp createDestOp(triton::gpu::CmpFOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, ValueRange operands,\n+                                   Location loc) {\n     return rewriter.create<LLVM::FCmpOp>(\n         loc, elemTy, ArithCmpFPredicteToLLVM(op.predicate()), operands[0],\n         operands[1]);\n@@ -2945,13 +2943,6 @@ struct ConvertLayoutOpConversion\n       triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n       ConversionPatternRewriter &rewriter, const MmaEncodingAttr &mmaLayout,\n       const DotOperandEncodingAttr &dotOperandLayout, bool isOuter) const;\n-\n-  // shared -> dot_operand if the result layout is blocked\n-  Value lowerSharedToDotOperandBlocked(\n-      triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n-      ConversionPatternRewriter &rewriter,\n-      const BlockedEncodingAttr &blockedLayout,\n-      const DotOperandEncodingAttr &dotOperandLayout, bool isOuter) const;\n };\n \n void ConvertLayoutOpConversion::processReplica(\n@@ -2960,7 +2951,7 @@ void ConvertLayoutOpConversion::processReplica(\n     ArrayRef<unsigned> multiDimRepId, unsigned vec,\n     ArrayRef<unsigned> paddedRepShape, ArrayRef<unsigned> outOrd,\n     SmallVector<Value> &vals, Value smemBase) const {\n-  unsigned accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n+  auto accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n   auto layout = type.getEncoding();\n   auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>();\n   auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>();\n@@ -2989,13 +2980,12 @@ void ConvertLayoutOpConversion::processReplica(\n     auto multiDimCTAInRepId =\n         getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep, order);\n     SmallVector<unsigned> multiDimCTAId(rank);\n-    for (auto it : llvm::enumerate(multiDimCTAInRepId)) {\n+    for (const auto &it : llvm::enumerate(multiDimCTAInRepId)) {\n       auto d = it.index();\n       multiDimCTAId[d] = multiDimRepId[d] * numCTAsEachRep[d] + it.value();\n     }\n \n-    unsigned linearCTAId =\n-        getLinearIndex<unsigned>(multiDimCTAId, numCTAs, order);\n+    auto linearCTAId = getLinearIndex<unsigned>(multiDimCTAId, numCTAs, order);\n     // TODO: This is actually redundant index calculation, we should\n     //       consider of caching the index calculation result in case\n     //       of performance issue observed.\n@@ -3073,7 +3063,7 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n     outNumCTAs[d] = ceil<unsigned>(shape[d], outPerCTA);\n   }\n   // Potentially we need to store for multiple CTAs in this replication\n-  unsigned accumNumReplicates = product<unsigned>(numReplicates);\n+  auto accumNumReplicates = product<unsigned>(numReplicates);\n   // unsigned elems = getElemsPerThread(srcTy);\n   auto vals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n   unsigned inVec = 0;\n@@ -3119,7 +3109,7 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n   rewriter.replaceOp(op, result);\n \n   return success();\n-};\n+}\n \n LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n     triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n@@ -3145,7 +3135,7 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n   unsigned maxPhase = dstSharedLayout.getMaxPhase();\n   unsigned numElems = getElemsPerThread(srcTy);\n   auto inVals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n-  unsigned srcAccumSizeInThreads =\n+  auto srcAccumSizeInThreads =\n       product<unsigned>(srcBlockedLayout.getSizePerThread());\n   auto elemTy = srcTy.getElementType();\n   auto wordTy = vec_ty(elemTy, minVec);\n@@ -3178,7 +3168,7 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n   smemBase = bitcast(smemBase, elemPtrTy);\n   auto smemObj = SharedMemoryObject(smemBase, dstShape, outOrd, loc, rewriter);\n   auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n-  unsigned numWordsEachRep = product<unsigned>(wordsInEachRep);\n+  auto numWordsEachRep = product<unsigned>(wordsInEachRep);\n   SmallVector<Value> wordVecs(numWordsEachRep);\n   for (unsigned i = 0; i < numElems; ++i) {\n     if (i % srcAccumSizeInThreads == 0) {\n@@ -3192,7 +3182,7 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n         linearIdxInNanoTile, srcBlockedLayout.getSizePerThread(), inOrd);\n     unsigned pos = multiDimIdxInNanoTile[inOrd[0]] % minVec;\n     multiDimIdxInNanoTile[inOrd[0]] /= minVec;\n-    unsigned wordVecIdx =\n+    auto wordVecIdx =\n         getLinearIndex<unsigned>(multiDimIdxInNanoTile, wordsInEachRep, inOrd);\n     wordVecs[wordVecIdx] =\n         insert_element(wordTy, wordVecs[wordVecIdx], inVals[i], idx_val(pos));\n@@ -3261,7 +3251,6 @@ class MMA16816SmemLoader {\n     cMatShape = matShape[order[0]];\n     sMatShape = matShape[order[1]];\n \n-    cStride = smemStrides[order[0]];\n     sStride = smemStrides[order[1]];\n \n     // rule: k must be the fast-changing axis.\n@@ -3632,7 +3621,6 @@ class MMA16816SmemLoader {\n   int cMatShape;\n   int sMatShape;\n \n-  Value cStride;\n   Value sStride;\n \n   bool needTrans;\n@@ -3647,13 +3635,6 @@ class MMA16816SmemLoader {\n   int warpOffStride;\n };\n \n-bool isSplatLike(Value value) {\n-  if (auto constv = dyn_cast<arith::ConstantOp>(value.getDefiningOp()))\n-    if (auto attr = constv.getValue().dyn_cast<SplatElementsAttr>())\n-      return attr.isSplat();\n-  return false;\n-}\n-\n struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n   enum class TensorCoreType : uint8_t {\n     // floating-point tensor core instr\n@@ -3786,7 +3767,6 @@ struct DotOpMmaV1ConversionHelper {\n   int getRepN(int N) const {\n     return std::max<int>(N / (wpt[1] * instrShape[1]), 1);\n   }\n-  int getRepK(int K) const { return std::max<int>(K / instrShape[2], 1); }\n \n   static ArrayRef<unsigned> getMmaInstrShape() { return instrShape; }\n \n@@ -3853,9 +3833,6 @@ struct DotOpMmaV1ConversionHelper {\n   Value loadB(Value B, const SharedMemoryObject &smemObj, Value thread,\n               Location loc, ConversionPatternRewriter &rewriter) const;\n \n-  // Loading $c to registers, returns a LLVM::Struct.\n-  Value loadC(Value C, Value llC, ConversionPatternRewriter &rewriter) const;\n-\n   static ArrayRef<unsigned> getOrder() { return mmaOrder; }\n \n   // Compute the offset of the matrix to load.\n@@ -3896,13 +3873,6 @@ struct DotOpMmaV2ConversionHelper {\n     mmaType = getTensorCoreTypeFromOperand(operandTy);\n   }\n \n-  // Get the M and N of mat instruction shape.\n-  static std::tuple<int, int> getMatShapeMN() {\n-    // According to DotOpMmaV2ConversionHelper::mmaMatShape, all the matrix\n-    // shape's M,N are {8,8}\n-    return {8, 8};\n-  }\n-\n   // Get the M and N of mma instruction shape.\n   static std::tuple<int, int> getInstrShapeMN() {\n     // According to DotOpConversionHelper::mmaInstrShape, all the M,N are\n@@ -4557,7 +4527,7 @@ struct DotOpFMAConversionHelper {\n                                      ConversionPatternRewriter &rewriter,\n                                      Location loc) const;\n \n-  Value getStructFromValueTable(ValueTable vals,\n+  Value getStructFromValueTable(const ValueTable &vals,\n                                 ConversionPatternRewriter &rewriter,\n                                 Location loc) const {\n     SmallVector<Type> elemTypes(vals.size(), f32_ty);\n@@ -4834,7 +4804,7 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n     auto mma = builder.create(\"mma.sync.aligned.m8n8k4\")\n                    ->o(isARow ? \"row\" : \"col\")\n                    .o(isBRow ? \"row\" : \"col\")\n-                   .o(\".f32.f16.f16.f32\");\n+                   .o(\"f32.f16.f16.f32\");\n \n     mma(resOprs, AOprs, BOprs, COprs);\n \n@@ -5091,11 +5061,6 @@ Value DotOpMmaV1ConversionHelper::loadB(\n   return res;\n }\n \n-Value DotOpMmaV1ConversionHelper::loadC(\n-    Value tensor, Value llTensor, ConversionPatternRewriter &rewriter) const {\n-  return llTensor;\n-}\n-\n std::tuple<Value, Value, Value, Value>\n DotOpMmaV1ConversionHelper::computeOffsets(Value threadId, bool isARow,\n                                            bool isBRow, ArrayRef<int> fpw,\n@@ -5843,11 +5808,10 @@ struct InsertSliceAsyncOpConversion\n };\n \n struct ExtElemwiseOpConversion\n-    : public ElementwiseOpConversionBase<\n-          triton::ExtElemwiseOp, LLVM::LLVMFuncOp, ExtElemwiseOpConversion> {\n-  using Base =\n-      ElementwiseOpConversionBase<triton::ExtElemwiseOp, LLVM::LLVMFuncOp,\n-                                  ExtElemwiseOpConversion>;\n+    : public ElementwiseOpConversionBase<triton::ExtElemwiseOp,\n+                                         ExtElemwiseOpConversion> {\n+  using Base = ElementwiseOpConversionBase<triton::ExtElemwiseOp,\n+                                           ExtElemwiseOpConversion>;\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n@@ -5891,10 +5855,9 @@ struct ExtElemwiseOpConversion\n };\n \n struct FDivOpConversion\n-    : ElementwiseOpConversionBase<mlir::arith::DivFOp, LLVM::InlineAsmOp,\n-                                  FDivOpConversion> {\n-  using Base = ElementwiseOpConversionBase<mlir::arith::DivFOp,\n-                                           LLVM::InlineAsmOp, FDivOpConversion>;\n+    : ElementwiseOpConversionBase<mlir::arith::DivFOp, FDivOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<mlir::arith::DivFOp, FDivOpConversion>;\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n@@ -5907,30 +5870,26 @@ struct FDivOpConversion\n     unsigned bitwidth = elemTy.getIntOrFloatBitWidth();\n     if (32 == bitwidth) {\n       fdiv.o(\"full\").o(\"f32\");\n-      auto res = ptxBuilder.newOperand(\"=r\");\n-      auto lhs = ptxBuilder.newOperand(operands[0], \"r\");\n-      auto rhs = ptxBuilder.newOperand(operands[1], \"r\");\n-      fdiv(res, lhs, rhs);\n     } else if (64 == bitwidth) {\n       fdiv.o(\"rn\").o(\"f64\");\n-      auto res = ptxBuilder.newOperand(\"=l\");\n-      auto lhs = ptxBuilder.newOperand(operands[0], \"l\");\n-      auto rhs = ptxBuilder.newOperand(operands[1], \"l\");\n-      fdiv(res, lhs, rhs);\n     } else {\n       assert(0 && bitwidth && \"not supported\");\n     }\n \n+    auto res = ptxBuilder.newOperand(bitwidth == 32 ? \"=r\" : \"=l\");\n+    auto lhs = ptxBuilder.newOperand(operands[0], bitwidth == 32 ? \"r\" : \"l\");\n+    auto rhs = ptxBuilder.newOperand(operands[1], bitwidth == 32 ? \"r\" : \"l\");\n+    fdiv(res, lhs, rhs);\n+\n     Value ret = ptxBuilder.launch(rewriter, loc, elemTy, false);\n     return ret;\n   }\n };\n \n struct ExpOpConversionApprox\n-    : ElementwiseOpConversionBase<mlir::math::ExpOp, LLVM::InlineAsmOp,\n-                                  ExpOpConversionApprox> {\n-  using Base = ElementwiseOpConversionBase<mlir::math::ExpOp, LLVM::InlineAsmOp,\n-                                           ExpOpConversionApprox>;\n+    : ElementwiseOpConversionBase<mlir::math::ExpOp, ExpOpConversionApprox> {\n+  using Base =\n+      ElementwiseOpConversionBase<mlir::math::ExpOp, ExpOpConversionApprox>;\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n "}]