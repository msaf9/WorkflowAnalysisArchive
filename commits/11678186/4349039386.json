[{"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 36, "deletions": 28, "changes": 64, "file_content_changes": "@@ -258,7 +258,6 @@ struct StoreOpConversion\n   matchAndRewrite(triton::StoreOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Value ptr = op.getPtr();\n-    Value mask = op.getMask();\n     Value value = op.getValue();\n \n     Value llPtr = adaptor.getPtr();\n@@ -273,7 +272,7 @@ struct StoreOpConversion\n         typeConverter->convertType(getElementTypeOrSelf(valueTy));\n \n     unsigned vec = getVectorSize(ptr);\n-    unsigned numElems = getElemsPerThread(ptr.getType());\n+    unsigned elemsPerThread = getElemsPerThread(ptr.getType());\n \n     auto ptrElems = getTypeConverter()->unpackLLElements(loc, llPtr, rewriter,\n                                                          ptr.getType());\n@@ -284,6 +283,7 @@ struct StoreOpConversion\n     // Determine the vectorization size\n     SmallVector<Value> maskElems;\n     if (llMask) {\n+      Value mask = op.getMask();\n       maskElems = getTypeConverter()->unpackLLElements(loc, llMask, rewriter,\n                                                        mask.getType());\n       assert(valueElems.size() == maskElems.size());\n@@ -292,12 +292,20 @@ struct StoreOpConversion\n       vec = std::min(vec, maskAlign);\n     }\n \n+    // numElements = 1 for scalar\n+    auto tensorTy = valueTy.dyn_cast<RankedTensorType>();\n+    auto numElems = tensorTy ? tensorTy.getNumElements() : 1;\n+    Value mask = int_val(1, 1);\n+    auto tid = tid_val();\n+    mask = and_(mask,\n+                icmp_slt(mul(tid, i32_val(elemsPerThread)), i32_val(numElems)));\n+\n     const size_t dtsize =\n         std::max<int>(1, valueElemTy.getIntOrFloatBitWidth() / 8);\n     const size_t valueElemNbits = dtsize * 8;\n \n-    const int numVecs = numElems / vec;\n-    for (size_t vecStart = 0; vecStart < numElems; vecStart += vec) {\n+    const int numVecs = elemsPerThread / vec;\n+    for (size_t vecStart = 0; vecStart < elemsPerThread; vecStart += vec) {\n       // TODO: optimization when ptr is AddPtr with constant offset\n       size_t in_off = 0;\n \n@@ -306,7 +314,7 @@ struct StoreOpConversion\n       const size_t width = std::min(totalWidth, maxWordWidth);\n       const size_t nWords = std::max<size_t>(1, totalWidth / width);\n       const size_t wordNElems = width / valueElemNbits;\n-      assert(wordNElems * nWords * numVecs == numElems);\n+      assert(wordNElems * nWords * numVecs == elemsPerThread);\n \n       // TODO(Superjomn) Add cache policy fields to StoreOp.\n       // TODO(Superjomn) Deal with cache policy here.\n@@ -339,7 +347,7 @@ struct StoreOpConversion\n       PTXBuilder ptxBuilder;\n       auto *asmArgList = ptxBuilder.newListOperand(asmArgs);\n \n-      Value maskVal = llMask ? maskElems[vecStart] : int_val(1, 1);\n+      Value maskVal = llMask ? and_(mask, maskElems[vecStart]) : mask;\n \n       auto *asmAddr =\n           ptxBuilder.newAddrOperand(ptrElems[vecStart], \"l\", in_off);\n@@ -392,10 +400,10 @@ struct AtomicCASOpConversion\n     auto valElements = getTypeConverter()->unpackLLElements(\n         loc, llVal, rewriter, op.getVal().getType());\n \n-    auto valueTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n+    auto TensorTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n     Type valueElemTy =\n-        valueTy ? getTypeConverter()->convertType(valueTy.getElementType())\n-                : op.getResult().getType();\n+        TensorTy ? getTypeConverter()->convertType(TensorTy.getElementType())\n+                 : op.getResult().getType();\n     auto tid = tid_val();\n     Value pred = icmp_eq(tid, i32_val(0));\n     PTXBuilder ptxBuilderMemfence;\n@@ -462,7 +470,6 @@ struct AtomicRMWOpConversion\n \n     Value val = op.getVal();\n     Value ptr = op.getPtr();\n-    Value _mask = op.getMask();\n \n     Value llPtr = adaptor.getPtr();\n     Value llVal = adaptor.getVal();\n@@ -472,29 +479,31 @@ struct AtomicRMWOpConversion\n         loc, llVal, rewriter, val.getType());\n     auto ptrElements = getTypeConverter()->unpackLLElements(\n         loc, llPtr, rewriter, ptr.getType());\n-    auto maskElements = getTypeConverter()->unpackLLElements(\n-        loc, llMask, rewriter, _mask.getType());\n+    SmallVector<Value> maskElements;\n+    if (llMask)\n+      maskElements = getTypeConverter()->unpackLLElements(\n+          loc, llMask, rewriter, op.getMask().getType());\n \n-    auto valueTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n+    auto tensorTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n     Type valueElemTy =\n-        valueTy ? getTypeConverter()->convertType(valueTy.getElementType())\n-                : op.getResult().getType();\n+        tensorTy ? getTypeConverter()->convertType(tensorTy.getElementType())\n+                 : op.getResult().getType();\n     const size_t valueElemNbits = valueElemTy.getIntOrFloatBitWidth();\n     auto elemsPerThread = getElemsPerThread(val.getType());\n-    // vec = 1 for scalar\n+    // vec = 1, numElements = 1 for scalar\n     auto vec = getVectorSize(ptr);\n-    Value mask = int_val(1, 1);\n-    auto tid = tid_val();\n+    int numElems = 1;\n     // tensor\n-    if (valueTy) {\n+    if (tensorTy) {\n       auto valTy = val.getType().cast<RankedTensorType>();\n       vec = std::min<unsigned>(vec, valTy.getElementType().isF16() ? 2 : 1);\n       // mask\n-      auto shape = valueTy.getShape();\n-      auto numElements = product(shape);\n-      mask = and_(mask, icmp_slt(mul(tid, i32_val(elemsPerThread)),\n-                                 i32_val(numElements)));\n+      numElems = tensorTy.getNumElements();\n     }\n+    Value mask = int_val(1, 1);\n+    auto tid = tid_val();\n+    mask = and_(mask,\n+                icmp_slt(mul(tid, i32_val(elemsPerThread)), i32_val(numElems)));\n \n     auto vecTy = vec_ty(valueElemTy, vec);\n     SmallVector<Value> resultVals(elemsPerThread);\n@@ -507,8 +516,7 @@ struct AtomicRMWOpConversion\n       }\n \n       Value rmwPtr = ptrElements[i];\n-      Value rmwMask = maskElements[i];\n-      rmwMask = and_(rmwMask, mask);\n+      Value rmwMask = llMask ? and_(mask, maskElements[i]) : mask;\n       std::string sTy;\n       PTXBuilder ptxBuilderAtomicRMW;\n       std::string tyId = valueElemNbits * vec == 64\n@@ -561,7 +569,7 @@ struct AtomicRMWOpConversion\n         return failure();\n       }\n       atom.o(rmwOp).o(sTy);\n-      if (valueTy) {\n+      if (tensorTy) {\n         atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n         auto retType = vec == 1 ? valueElemTy : vecTy;\n         auto ret = ptxBuilderAtomicRMW.launch(rewriter, loc, retType);\n@@ -587,8 +595,8 @@ struct AtomicRMWOpConversion\n         rewriter.replaceOp(op, {ret});\n       }\n     }\n-    if (valueTy) {\n-      Type structTy = getTypeConverter()->convertType(valueTy);\n+    if (tensorTy) {\n+      Type structTy = getTypeConverter()->convertType(tensorTy);\n       Value resultStruct = getTypeConverter()->packLLElements(\n           loc, resultVals, rewriter, structTy);\n       rewriter.replaceOp(op, {resultStruct});"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -798,6 +798,20 @@ def kernel(output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     assert torch.all(output == ref)\n \n \n+def test_load_store_same_ptr():\n+    @triton.jit()\n+    def kernel(in_out_ptr):\n+        pid = tl.program_id(axis=0)\n+        x = tl.load(in_out_ptr + pid)\n+        out = x * 2\n+        tl.store(in_out_ptr + pid, out)\n+\n+    for _ in range(1000):\n+        x = torch.ones((65536,), device=\"cuda\", dtype=torch.float32)\n+        kernel[(65536,)](x, num_warps=32)\n+        assert torch.all(x == 2)\n+\n+\n @pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16])\n def test_f8_xf16_roundtrip(dtype):\n     \"\"\"Tests that converting an f8 to f16 and back to f8 doesn't change its value\"\"\""}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1027,7 +1027,7 @@ def where(condition, x, y, _builder=None):\n     If you want to avoid unintended memory operations, use the :code:`mask` arguments in `triton.load` and `triton.store` instead.\n \n     The shape of :code:`x` and :code:`y` are both broadcast to the shape of :code:`condition`.\n-    :code:`x` and :code:`y` must have the data type.\n+    :code:`x` and :code:`y` must have the same data type.\n \n     :param condition: When True (nonzero), yield x, otherwise yield y.\n     :type condition: Block of triton.bool"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 47, "deletions": 1, "changes": 48, "file_content_changes": "@@ -1001,14 +1001,60 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: atomic_add_f32\n   func.func @atomic_add_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xi1, #blocked0>, %arg2 : tensor<256xf32, #blocked0>) {\n+    // CHECK: llvm.icmp \"slt\"\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: atom.global.gpu.add.f32\n+    // CHECK-SAME: @$3 atom.global.gpu.add.f32\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: @$3 atom.global.gpu.add.f32\n     %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32} : (tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xf32, #blocked0>, tensor<256xi1, #blocked0>) -> tensor<256xf32, #blocked0>\n     return\n   }\n }\n \n // -----\n+\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: atomic_add_f32_scalar\n+  func.func @atomic_add_f32_scalar(%arg0 : !tt.ptr<f32>, %arg1 : i1, %arg2 : f32) {\n+    // CHECK: llvm.icmp \"eq\"\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: @$3 atom.global.gpu.add.f32\n+    %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32} : (!tt.ptr<f32>, f32, i1) -> f32\n+    return\n+  }\n+}\n+\n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: store_f32\n+  func.func @store_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xf32, #blocked0>) {\n+    // CHECK: llvm.icmp \"slt\"\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: @$2 st.global.b32\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: @$2 st.global.b32\n+    tt.store %arg0, %arg1 : tensor<256xf32, #blocked0>\n+    return\n+  }\n+}\n+\n+// -----\n+\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: store_f32_scalar\n+  func.func @store_f32_scalar(%arg0 : !tt.ptr<f32>, %arg1 : f32) {\n+    // CHECK: llvm.icmp \"slt\"\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: @$2 st.global.b32\n+    tt.store %arg0, %arg1 : f32\n+    return\n+  }\n+}\n+\n+// -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n "}]