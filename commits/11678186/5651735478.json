[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -170,6 +170,7 @@ jobs:\n           python3 -m pip install --upgrade pip\n           python3 -m pip install cmake==3.24\n           python3 -m pip install torch==1.13.1 --index-url https://download.pytorch.org/whl/rocm5.2\n+          export TRITON_CODEGEN_AMD_HIP_BACKEND=1\n           python3 -m pip install --no-build-isolation -vvv '.[tests]'\n \n       - name: Install Triton on XPU\n@@ -191,7 +192,7 @@ jobs:\n         if: ${{ env.BACKEND == 'ROCM'}}\n         run: |\n           cd python/test/unit/language\n-          python3 -m pytest --capture=tee-sys -rfs --verbose \"test_core.py::test_empty_kernel\"\n+          python3 -m pytest --capture=tee-sys -rfs --verbose \"test_core.py\"\n \n       - name: Run python tests on XPU\n         if: ${{ env.BACKEND == 'XPU'}}"}, {"filename": ".gitignore", "status": "modified", "additions": 0, "deletions": 9, "changes": 9, "file_content_changes": "@@ -24,12 +24,3 @@ venv.bak/\n # JetBrains project files\n .idea\n cmake-build-*\n-\n-# rocm\n-log*\n-python/triton/third_party/cuda/bin/ptxas\n-core\n-python/triton/_C/*.so\n-\n-\n-*.log\n\\ No newline at end of file"}, {"filename": "CMakeLists.txt", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -22,7 +22,7 @@ endif()\n # Options\n option(TRITON_BUILD_TUTORIALS \"Build C++ Triton tutorials\" ON)\n option(TRITON_BUILD_PYTHON_MODULE \"Build Python Triton bindings\" OFF)\n-set(TRITON_CODEGEN_BACKENDS \"amd_hip_backend\" CACHE STRING \"Enable different codegen backends\")\n+set(TRITON_CODEGEN_BACKENDS \"\" CACHE STRING \"Enable different codegen backends\")\n \n # Ensure Python3 vars are set correctly\n # used conditionally in this file and by lit tests\n@@ -194,7 +194,7 @@ include_directories(${PROJECT_BINARY_DIR}/include) # Tablegen'd files\n # link_directories(${LLVM_LIBRARY_DIR})\n add_subdirectory(include)\n add_subdirectory(lib)\n-# add_subdirectory(bin)\n+add_subdirectory(bin)\n \n # find_package(PythonLibs REQUIRED)\n set(TRITON_SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}\")\n@@ -271,6 +271,6 @@ if (${CODEGEN_BACKENDS_LEN} GREATER 0)\n   endforeach()\n endif()\n \n-# add_subdirectory(test)\n+add_subdirectory(test)\n \n-# add_subdirectory(unittest)\n+add_subdirectory(unittest)"}, {"filename": "bin/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -50,7 +50,6 @@ llvm_update_compile_flags(triton-translate)\n          TritonGPUTransforms\n          TritonLLVMIR\n          TritonPTX\n-         TritonHSACO\n          ${dialect_libs}\n          ${conversion_libs}\n          # tests"}, {"filename": "bin/triton-translate.cpp", "status": "modified", "additions": 2, "deletions": 8, "changes": 10, "file_content_changes": "@@ -14,7 +14,6 @@\n #include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n-#include \"triton/Target/HSACO/HSACOTranslation.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n #include \"triton/Target/PTX/PTXTranslation.h\"\n #include \"llvm/IR/LLVMContext.h\"\n@@ -127,16 +126,11 @@ LogicalResult tritonTranslateMain(int argc, char **argv,\n     llvm::errs() << \"Translate to LLVM IR failed\";\n   }\n \n-  if (targetKind == \"llvmir\")\n+  if (targetKind == \"llvmir\") {\n     llvm::outs() << *llvmir << '\\n';\n-  else if (targetKind == \"ptx\")\n+  } else if (targetKind == \"ptx\") {\n     llvm::outs() << ::triton::translateLLVMIRToPTX(*llvmir, SMArch.getValue(),\n                                                    ptxVersion.getValue());\n-  else if (targetKind == \"hsaco\") {\n-    auto [module, hsaco] = ::triton::translateLLVMIRToHSACO(\n-        *llvmir, GCNArch.getValue(), GCNTriple.getValue(),\n-        GCNFeatures.getValue());\n-    llvm::outs() << hsaco;\n   } else {\n     llvm::errs() << \"Error: Unknown target specified: \" << targetKind << \"\\n\";\n     return failure();"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 12, "deletions": 11, "changes": 23, "file_content_changes": "@@ -1063,8 +1063,8 @@ void init_triton_ir(py::module &&m) {\n              unsigned typeWidth = elementType.getIntOrFloatBitWidth();\n              auto constValue = builder.create<mlir::arith::ConstantIntOp>(\n                  loc, typeWidth, elementType);\n-             auto zeroConst =\n-                 builder.create<mlir::arith::ConstantIntOp>(loc, 0, elementType);\n+             auto zeroConst = builder.create<mlir::arith::ConstantIntOp>(\n+                 loc, 0, elementType);\n              if (lhs.getType().isIntOrIndex()) {\n                auto cmpValue = builder.create<mlir::arith::CmpIOp>(\n                    loc, mlir::arith::CmpIPredicate::ult, rhs, constValue);\n@@ -1095,8 +1095,8 @@ void init_triton_ir(py::module &&m) {\n              unsigned typeWidth = elementType.getIntOrFloatBitWidth();\n              auto constValue = builder.create<mlir::arith::ConstantIntOp>(\n                  loc, typeWidth, elementType);\n-             auto zeroConst =\n-                 builder.create<mlir::arith::ConstantIntOp>(loc, 0, elementType);\n+             auto zeroConst = builder.create<mlir::arith::ConstantIntOp>(\n+                 loc, 0, elementType);\n              if (lhs.getType().isIntOrIndex()) {\n                auto cmpValue = builder.create<mlir::arith::CmpIOp>(\n                    loc, mlir::arith::CmpIPredicate::ult, rhs, constValue);\n@@ -1127,16 +1127,17 @@ void init_triton_ir(py::module &&m) {\n              unsigned typeWidth = elementType.getIntOrFloatBitWidth();\n              auto constValue = builder.create<mlir::arith::ConstantIntOp>(\n                  loc, typeWidth, elementType);\n-             auto zeroConst =\n-                 builder.create<mlir::arith::ConstantIntOp>(loc, 0, elementType);\n+             auto zeroConst = builder.create<mlir::arith::ConstantIntOp>(\n+                 loc, 0, elementType);\n              uint64_t ones_val = 0xFFFFFFFFFFFFFFFF;\n              auto onesConst = builder.create<mlir::arith::ConstantIntOp>(\n                  loc, ones_val, elementType);\n              if (lhs.getType().isIntOrIndex()) {\n                auto negativeCmpValue = builder.create<mlir::arith::CmpIOp>(\n                    loc, mlir::arith::CmpIPredicate::slt, lhs, zeroConst);\n-               auto otherValue = mlir::Value(builder.create<mlir::arith::SelectOp>(\n-                   loc, negativeCmpValue, onesConst, zeroConst));\n+               auto otherValue =\n+                   mlir::Value(builder.create<mlir::arith::SelectOp>(\n+                       loc, negativeCmpValue, onesConst, zeroConst));\n                auto cmpValue = builder.create<mlir::arith::CmpIOp>(\n                    loc, mlir::arith::CmpIPredicate::ult, rhs, constValue);\n                auto shiftValue =\n@@ -1152,8 +1153,9 @@ void init_triton_ir(py::module &&m) {\n                    loc, lhs.getType(), onesConst);\n                auto negativeCmpValue = builder.create<mlir::arith::CmpIOp>(\n                    loc, mlir::arith::CmpIPredicate::slt, lhs, zeroValue);\n-               auto otherValue = mlir::Value(builder.create<mlir::arith::SelectOp>(\n-                   loc, negativeCmpValue, onesValue, zeroValue));\n+               auto otherValue =\n+                   mlir::Value(builder.create<mlir::arith::SelectOp>(\n+                       loc, negativeCmpValue, onesValue, zeroValue));\n                auto cmpValue = builder.create<mlir::arith::CmpIOp>(\n                    loc, mlir::arith::CmpIPredicate::ult, rhs, splatValue);\n                auto shiftValue =\n@@ -1857,7 +1859,6 @@ void init_triton_translation(py::module &m) {\n            const std::vector<std::string> &paths) {\n           ::mlir::triton::addExternalLibs(op, names, paths);\n         });\n-\n }\n \n void init_triton(py::module &m) {"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 18, "deletions": 18, "changes": 36, "file_content_changes": "@@ -12,8 +12,8 @@\n import triton\n import triton._C.libtriton.triton as _triton\n import triton.language as tl\n-from triton.runtime.jit import JITFunction, TensorWrapper, reinterpret\n from triton.common.build import is_hip\n+from triton.runtime.jit import JITFunction, TensorWrapper, reinterpret\n \n int_dtypes = ['int8', 'int16', 'int32', 'int64']\n uint_dtypes = ['uint8', 'uint16', 'uint32', 'uint64']\n@@ -930,7 +930,7 @@ def noinline_multi_values_fn(x, y, Z):\n def test_noinline(mode, device):\n     if is_hip() and mode == \"shared\":\n         pytest.skip('test_noinline[\"shared\"] not supported on HIP.')\n-    \n+\n     @triton.jit\n     def kernel(X, Y, Z):\n         x = tl.load(X)\n@@ -1018,7 +1018,7 @@ def kernel(X, Z):\n     sem_str = \"acq_rel\" if sem is None else sem\n     if is_hip():\n         return\n-    \n+\n     assert f\"atom.global.gpu.{sem_str}\" in h.asm[\"ptx\"]\n \n \n@@ -1140,7 +1140,6 @@ def test_cast(dtype_x, dtype_z, bitcast, device):\n     if is_hip() and (dtype_z == \"bfloat16\"):\n         pytest.skip(f'test_cast{(dtype_x, dtype_z)} cast to bfloat16 not supported on HIP.')\n \n-\n     size = 1024\n     # This is tricky because numpy doesn't have bfloat, and torch doesn't have uints.\n     if dtype_x.startswith('bfloat'):\n@@ -1238,7 +1237,7 @@ def kernel(in_out_ptr):\n     for _ in range(1000):\n         x = torch.ones((65536,), device=device, dtype=torch.float32)\n         if is_hip():\n-            kernel[(65536,)](x, num_warps=16) # threads per Warp for ROCM is 64\n+            kernel[(65536,)](x, num_warps=16)  # threads per Warp for ROCM is 64\n         else:\n             kernel[(65536,)](x, num_warps=32)\n         assert torch.all(x == 2)\n@@ -1610,7 +1609,7 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n def test_scan_layouts(M, N, src_layout, axis, device):\n     if is_hip():\n         pytest.skip(\"test_scan_layouts is not supported in HIP\")\n-        \n+\n     ir = f\"\"\"\n     #blocked = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32}} {{\n@@ -1676,9 +1675,9 @@ def test_scan_layouts(M, N, src_layout, axis, device):\n def test_reduce_layouts(M, N, src_layout, axis, device):\n     if is_hip():\n         pytest.skip(\"test_reduce_layouts is not supported in HIP\")\n-        gpu_dialect =\"triton_gpu_rocm\"\n+        gpu_dialect = \"triton_gpu_rocm\"\n     else:\n-        gpu_dialect =\"triton_gpu\"\n+        gpu_dialect = \"triton_gpu\"\n \n     rdims_2d = f\"1x{N}\" if axis == 0 else f\"{M}x1\"\n     rdims_1d = f\"{N}\" if axis == 0 else f\"{M}\"\n@@ -1751,9 +1750,9 @@ def test_reduce_layouts(M, N, src_layout, axis, device):\n @pytest.mark.parametrize(\"src_layout\", layouts)\n def test_store_op(M, src_layout, device):\n     if is_hip():\n-        gpu_dialect =\"triton_gpu_rocm\"\n+        gpu_dialect = \"triton_gpu_rocm\"\n     else:\n-        gpu_dialect =\"triton_gpu\"\n+        gpu_dialect = \"triton_gpu\"\n \n     ir = f\"\"\"\n     #src = {src_layout}\n@@ -1806,9 +1805,9 @@ def test_store_op(M, src_layout, device):\n def test_convert1d(M, src_layout, dst_layout, src_dim, dst_dim, device):\n     if is_hip():\n         pytest.skip(\"test_reduce_layouts is not supported in HIP\")\n-        gpu_dialect =\"triton_gpu_rocm\"\n+        gpu_dialect = \"triton_gpu_rocm\"\n     else:\n-        gpu_dialect =\"triton_gpu\"\n+        gpu_dialect = \"triton_gpu\"\n     ir = f\"\"\"\n     #dst = {dst_layout}\n     #src = {src_layout}\n@@ -1869,10 +1868,10 @@ def _welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n @pytest.mark.parametrize(\"first_axis\", [0, 1])\n def test_chain_reduce(M, N, src_layout, op, device, first_axis):\n     if is_hip():\n-        gpu_dialect =\"triton_gpu_rocm\"\n+        gpu_dialect = \"triton_gpu_rocm\"\n         pytest.skip(\"test_chain_reduce is not supported in HIP\")\n     else:\n-        gpu_dialect =\"triton_gpu\"\n+        gpu_dialect = \"triton_gpu\"\n \n     op_str = \"\"\n     if op == \"sum\":\n@@ -1975,7 +1974,7 @@ def var_mean_kernel(X, out_mean, out_var, BLOCK: tl.constexpr):\n def test_permute(dtype_str, shape, perm, device):\n     check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n     if is_hip():\n-        if dtype_str == \"float32\" and shape == (128,128):\n+        if dtype_str == \"float32\" and shape == (128, 128):\n             pytest.skip(f\"test_permute{(dtype_str, shape)} Not supported: memory out of resource on HIP.\")\n \n     # triton kernel\n@@ -2341,6 +2340,7 @@ def kernel(out_ptr):\n def test_dot_without_load(dtype_str, device):\n     if is_hip() and dtype_str == \"float16\":\n         pytest.skip(\"test_dot_without_load[float16] not supported in HIP\")\n+\n     @triton.jit\n     def _kernel(out):\n         a = GENERATE_TEST_HERE\n@@ -2558,7 +2558,7 @@ def _kernel(dst, src, CACHE: tl.constexpr):\n         tl.store(dst + offsets, x, cache_modifier=CACHE)\n \n     if is_hip():\n-        return \n+        return\n     pgm = _kernel[(1,)](dst, src, CACHE=cache)\n     ptx = pgm.asm['ptx']\n     if cache == '':\n@@ -3320,9 +3320,9 @@ def kernel(Out):\n def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device):\n     if is_hip():\n         pytest.skip(\"test_convert2d is not supported in HIP\")\n-        gpu_dialect =\"triton_gpu_rocm\"\n+        gpu_dialect = \"triton_gpu_rocm\"\n     else:\n-        gpu_dialect =\"triton_gpu\"\n+        gpu_dialect = \"triton_gpu\"\n     if str(src_layout) == str(dst_layout):\n         pytest.skip()\n     if 'mma' in str(src_layout) and 'mma' in str(dst_layout):"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 16, "deletions": 25, "changes": 41, "file_content_changes": "@@ -2,20 +2,20 @@\n \n import functools\n import hashlib\n+import importlib\n import json\n import os\n import re\n-import subprocess\n import tempfile\n from collections import namedtuple\n from pathlib import Path\n-from typing import Any, Tuple\n+from typing import Any\n \n from .._C.libtriton.triton import (add_external_libs, compile_ptx_to_cubin,\n-                                   get_shared_memory_size, ir,\n-                                   translate_llvmir_to_ptx,\n+                                   get_shared_memory_size, ir, translate_llvmir_to_ptx,\n                                    translate_triton_gpu_to_llvmir)\n from ..common.backend import get_backend, path_to_ptxas, register_backend\n+from ..common.build import is_hip\n # from ..runtime import driver, jit, JITFunction\n # TODO: runtime.errors\n from ..runtime.autotuner import OutOfResources\n@@ -27,8 +27,8 @@\n from .code_generator import ast_to_ttir\n from .make_launcher import make_stub\n \n-import importlib\n-spec = importlib.util.spec_from_file_location(\"rocm\", \"/dockerx/triton/third_party/amd_hip_backend/python/rocm.py\")\n+rocm_module_path = os.path.join(Path(__file__).parents[3], \"third_party\", \"amd_hip_backend\", \"python\", \"rocm.py\")\n+spec = importlib.util.spec_from_file_location(\"rocm\", rocm_module_path)\n rocm = importlib.util.module_from_spec(spec)\n spec.loader.exec_module(rocm)\n \n@@ -155,9 +155,6 @@ def ptx_to_cubin(ptx: str, arch: int):\n     return compile_ptx_to_cubin(ptx, ptxas, arch)\n \n \n-\n-\n-\n # ------------------------------------------------------------------------------\n # compiler\n # ------------------------------------------------------------------------------\n@@ -220,7 +217,7 @@ def make_hash(fn, arch, **kwargs):\n     \"ttgir\": mlir_arg_type_pattern,\n     \"ptx\": ptx_arg_type_pattern,\n }\n-if True:\n+if is_hip():\n     ttgir_num_warps_pattern = r'\"triton_gpu_rocm.num-warps\"\\s?=\\s?(\\d+)\\s?:'\n else:\n     ttgir_num_warps_pattern = r'\"triton_gpu.num-warps\"\\s?=\\s?(\\d+)\\s?:'\n@@ -256,17 +253,10 @@ def _is_cuda(arch):\n \n \n def get_architecture_descriptor(capability):\n-    try:\n-        import torch\n-    except ImportError:\n-        raise ImportError(\"Triton requires PyTorch to be installed\")\n     if capability is None:\n-        if torch.version.hip is None:\n-            device = get_current_device()\n-            capability = get_device_capability(device)\n-            capability = capability[0] * 10 + capability[1]\n-        else:\n-            capability = get_amdgpu_arch_fulldetails()\n+        device = get_current_device()\n+        capability = get_device_capability(device)\n+        capability = capability[0] * 10 + capability[1]\n     return capability\n \n \n@@ -303,8 +293,9 @@ def compile(fn, **kwargs):\n         assert _device_backend\n         arch = _device_backend.get_architecture_descriptor(**kwargs)\n \n-    is_cuda = False\n-    is_hip = True\n+    is_cuda = device_type == \"cuda\" and _is_cuda(arch)\n+    if is_hip():\n+        is_cuda = False\n     context = ir.context()\n     constants = kwargs.get(\"constants\", dict())\n     num_warps = kwargs.get(\"num_warps\", 4)\n@@ -321,7 +312,7 @@ def compile(fn, **kwargs):\n                       lambda src: optimize_ttir(ast_to_ttir(src, signature, configs[0], constants, debug=debug, arch=arch), arch))\n     if is_cuda:\n         add_cuda_stages(arch, context, num_warps, num_stages, extern_libs, stages)\n-    elif is_hip:\n+    elif is_hip():\n         _device_backend.add_stages(arch, extern_libs, stages, context=context, num_warps=num_warps, num_stages=num_stages)\n     else:\n         _device_backend.add_stages(arch, extern_libs, stages)\n@@ -433,7 +424,7 @@ def compile(fn, **kwargs):\n         else:\n             asm[ir_name] = str(next_module)\n         if ir_name == \"llir\" and \"shared\" not in metadata:\n-            if is_hip:\n+            if is_hip():\n                 metadata[\"shared\"] = _device_backend.get_shared_memory_size(module)\n             else:\n                 metadata[\"shared\"] = get_shared_memory_size(module)\n@@ -442,7 +433,7 @@ def compile(fn, **kwargs):\n         if ir_name == \"amdgcn\":\n             metadata[\"name\"] = get_kernel_name(next_module[0], pattern='.globl')\n             asm[\"hsaco_path\"] = next_module[1]\n-        if not is_cuda and not is_hip:\n+        if not is_cuda and not is_hip():\n             _device_backend.add_meta_info(ir_name, module, next_module, metadata, asm)\n         module = next_module\n     # write-back metadata, if it didn't come from the cache"}, {"filename": "python/triton/compiler/make_launcher.py", "status": "modified", "additions": 1, "deletions": 15, "changes": 16, "file_content_changes": "@@ -3,15 +3,10 @@\n import tempfile\n \n from ..common import _build\n+from ..common.build import is_hip\n from ..runtime.cache import get_cache_manager\n from ..runtime.jit import version_key\n \n-\n-def is_hip():\n-    import torch\n-    return torch.version.hip is not None\n-\n-\n # ----- stub --------\n \n \n@@ -181,15 +176,7 @@ def format_of(ty):\n       return ptr_info;\n     }}\n \n-    static PyObject* launch_counter(PyObject* self, PyObject* args) {{\n-      static int64_t launch_counter = 0;\n-      launch_counter += 1;\n-      return PyLong_FromLong(launch_counter);\n-    }}\n-\n     static PyObject* launch(PyObject* self, PyObject* args) {{\n-    \n-      launch_counter(self, args);\n \n       int gridX, gridY, gridZ;\n       uint64_t _stream;\n@@ -226,7 +213,6 @@ def format_of(ty):\n \n     static PyMethodDef ModuleMethods[] = {{\n       {{\"launch\", launch, METH_VARARGS, \"Entry point for all kernels with this signature\"}},\n-      {{\"launch_counter\", launch_counter, METH_VARARGS, \"Entry point to get launch counter\"}},\n       {{NULL, NULL, 0, NULL}} // sentinel\n     }};\n "}, {"filename": "python/triton/language/math.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -1,20 +1,20 @@\n import functools\n import os\n+from pathlib import Path\n \n-from . import core\n from ..common.build import is_hip\n-from pathlib import Path\n+from . import core\n \n \n @functools.lru_cache()\n def libdevice_path():\n     if is_hip():\n-        third_party_dir = os.path.join(Path(__file__).parents[3],\"third_party\",\"amd_hip_backend\",\"python\",\"triton\",\"third_party\")\n+        third_party_dir = os.path.join(Path(__file__).parents[3], \"third_party\", \"amd_hip_backend\", \"python\", \"triton\", \"third_party\")\n         default = os.path.join(third_party_dir, \"rocm\", \"lib\", \"bitcode\", \"cuda2gcn.bc\")\n     else:\n         third_party_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"third_party\")\n         default = os.path.join(third_party_dir, \"cuda\", \"lib\", \"libdevice.10.bc\")\n-       \n+\n     return os.getenv(\"TRITON_LIBDEVICE_PATH\", default)\n \n "}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 13, "deletions": 19, "changes": 32, "file_content_changes": "@@ -1,17 +1,20 @@\n from __future__ import annotations  # remove after python 3.11\n \n-import re\n+import importlib\n+import os\n import warnings\n from functools import wraps\n+from pathlib import Path\n from typing import List, Optional, Sequence, Tuple, TypeVar\n \n from .._C.libtriton.triton import ir\n+from ..common.build import is_hip\n from . import core as tl\n \n-import triton._C.libtriton.triton as _triton\n-import triton._C.librocm_backend_for_triton.triton_rocm as _triton_rocm\n-\n-from ..common.build import is_hip\n+rocm_module_path = os.path.join(Path(__file__).parents[3], \"third_party\", \"amd_hip_backend\", \"python\", \"rocm.py\")\n+spec = importlib.util.spec_from_file_location(\"rocm\", rocm_module_path)\n+rocm = importlib.util.module_from_spec(spec)\n+spec.loader.exec_module(rocm)\n \n T = TypeVar('T')\n \n@@ -529,7 +532,6 @@ def full(shape: List[int], value, dtype: tl.dtype, builder: ir.builder) -> tl.te\n         return tl.tensor(builder.create_splat(value, shape), ret_ty)\n \n \n-\n def ones(shape: List[int], dtype: tl.dtype, builder: ir.builder) -> tl.tensor:\n     _1 = builder.get_one_value(dtype.to_ir(builder))\n     ret_ty = tl.block_type(dtype, shape)\n@@ -1259,30 +1261,22 @@ def atomic_xchg(ptr: tl.tensor,\n #                               Linear Algebra\n # ===----------------------------------------------------------------------===//\n \n-def is_hip():\n-    try:\n-        import torch\n-    except ImportError:\n-        raise ImportError(\"Triton requires PyTorch to be installed\")\n-    return torch.version.hip is not None\n \n def gpu_has_mfma() -> bool:\n     if not is_hip():\n         return False\n-    return True\n-    # arch_info = _triton_rocm.get_arch_info()\n-    # gfx_arch_details = re.search('amd.*', arch_info)\n-    # if gfx_arch_details is None:\n-    #     return False\n-    # gfx_arch_details = gfx_arch_details.group(0).strip().split('--')\n-    # return gfx_arch_details[1].split(':')[0] in ['gfx908', 'gfx90a']\n+    arch_info = rocm.get_amdgpu_arch_fulldetails()\n+    gfx_arch = arch_info[1]\n+    return gfx_arch in ['gfx908', 'gfx90a']\n+\n \n def mfma_supported(M, N, K, allow_tf32, ret_scalar_ty) -> bool:\n     if not gpu_has_mfma():\n         return False\n     # TODO: Add check for configurations and types.\n     return True\n \n+\n def dot(lhs: tl.tensor,\n         rhs: tl.tensor,\n         allow_tf32: bool,"}, {"filename": "scripts/backtrace.sh", "status": "removed", "additions": 0, "deletions": 20, "changes": 20, "file_content_changes": "@@ -1,20 +0,0 @@\n-sudo apt install gdb -y\n-\n-# export AMD_OCL_WAIT_COMMAND=1\n-# export AMD_LOG_LEVEL=3\n-# export HIP_LAUNCH_BLOCKING=1\n-\n-ROOT_DIR=$(pwd)\n-LOG_DIR=$ROOT_DIR/log\n-rm -rf $LOG_DIR\n-mkdir -p $LOG_DIR\n-chmod -R 777 $LOG_DIR\n-\n-COMMAND=\"python/tests/test_elementwise.py::test_single_input[log-float64-float64]\"\n-gdb -ex \"set pagination off\" \\\n-    -ex \"file python\" \\\n-    -ex \"run -m pytest --capture=tee-sys --verbose $COMMAND\" \\\n-    -ex \"backtrace\" \\\n-    -ex \"set confirm off\" \\\n-    -ex \"q\" \\\n-    2>&1 | tee $LOG_DIR/gdb_backtrace.log"}, {"filename": "scripts/build.sh", "status": "removed", "additions": 0, "deletions": 11, "changes": 11, "file_content_changes": "@@ -1,11 +0,0 @@\n-set -x\n-\n-cd python\n-pip uninstall -y triton\n-\n-bash scripts/clean.sh\n-\n-export TRITON_USE_ROCM=ON\n-\n-# pip install -U matplotlib pandas filelock tabulate\n-pip install --verbose -e ."}, {"filename": "scripts/build_dbg_llvm.sh", "status": "removed", "additions": 0, "deletions": 34, "changes": 34, "file_content_changes": "@@ -1,34 +0,0 @@\n-#!/bin/bash\n-\n-# The script gets latest LLVM 14 release, builds it and\n-# installs to the `build_dbg/install` folder. After that\n-# one can use the installation with `LLVM_SYSPATH` env\n-# variable to override default paths used in\n-# `triton/python/setup.py`\n-\n-# Notes:\n-# 1. The whole folder will temporary get ~95GB disk space.\n-# After the last clean command it will be schrinked to ~38GB.\n-#\n-# 2. Default ld linker gets a lot of memory for debug build,\n-# so number of parallel linker jobs reduced to 4 with\n-# `LLVM_PARALLEL_LINK_JOBS` option. It works with ninja\n-# build system only. Additionally `LLVM_PARALLEL_COMPILE_JOBS`\n-# can be used to reduce parallel compilers processes.\n-\n-git clone https://github.com/llvm/llvm-project.git llvm14\n-cd llvm14\n-git checkout llvmorg-14.0.6\n-mkdir build_dbg\n-cd build_dbg\n-cmake ../llvm -G Ninja \\\n-   -DCMAKE_BUILD_TYPE=Debug \\\n-   -DCMAKE_INSTALL_PREFIX=${PWD}/install \\\n-   -DLLVM_ENABLE_PROJECTS=mlir \\\n-   -DLLVM_BUILD_RUNTIME=OFF \\\n-   -DLLVM_PARALLEL_LINK_JOBS=4 \\\n-   -DLLVM_TARGETS_TO_BUILD=\"X86;NVPTX;AMDGPU\"\n-\n-cmake --build .\n-cmake --install .\n-cmake --build . --target clean"}, {"filename": "scripts/cache_print.sh", "status": "removed", "additions": 0, "deletions": 28, "changes": 28, "file_content_changes": "@@ -1,28 +0,0 @@\n-#!/bin/bash\n-\n-# set -x\n-\n-LOG_DIR=${1:-\"$(pwd)/log_\"}\n-TRITON_CACHE=$LOG_DIR/cache\n-CACHED_FILES=$(find /root/.triton/cache/ -type f -name \"*.*\")\n-\n-rm -rf $TRITON_CACHE\n-mkdir -p $TRITON_CACHE\n-\n-for file in ${CACHED_FILES[@]}; do\n-\t# echo \"$file\"\n-\tif [[ $file == *.so ]]; then\n-\t\ttrue\n-\t\t# echo \"Skipping printing .so file\"\n-\telif [[ $file == *.cubin ]]; then\n-\t\ttrue\n-\t\t# echo \"Skipping printing .cubin file\"\n-\telse\n-\t\tsed -i -e '$a\\' $file\n-\t\tcat $file\n-\t\tcp $file $TRITON_CACHE\n-\tfi\n-done\n-\n-cp -rf /tmp/* $TRITON_CACHE\n-chmod -R 777 $TRITON_CACHE"}, {"filename": "scripts/check_llvm_src.sh", "status": "removed", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -1,3 +0,0 @@\n-shopt -s extglob\n-${ROCM_PATH}/llvm/bin/llc -mcpu=gfx908 triton_rocm_kernels/*+([0-9]).ll\n-# ${ROCM_PATH}/llvm/bin/llc -mcpu=gfx908 triton_rocm_kernels/*_before_verify.ll"}, {"filename": "scripts/clean.sh", "status": "removed", "additions": 0, "deletions": 42, "changes": 42, "file_content_changes": "@@ -1,42 +0,0 @@\n-set -e\n-# set -x\n-\n-LOG_CLEAN_LIST=(\n-    log_*\n-)\n-\n-CACHE_CLEAN_LIST=(\n-    /root/.triton/cache\n-    /tmp/*\n-    triton_cache\n-    *.log\n-)\n-\n-BUILD_CLEAN_LIST=(\n-    python/triton/_C/*.so\n-    python/triton.egg-info\n-    python/.pytest_cache\n-    python/tests/__pycache__\n-    python/build\n-)\n-\n-\n-# clean up files\n-if [ \"$1\" == \"cache\" ]; then\n-    for f in ${CACHE_CLEAN_LIST[*]}; do\n-        rm -rf $f\n-    done\n-elif [ \"$1\" == \"build\" ]; then\n-    for f in ${BUILD_CLEAN_LIST[*]}; do\n-        rm -rf $f\n-    done\n-elif [ \"$1\" == \"log\" ]; then\n-    for f in ${LOG_CLEAN_LIST[*]}; do\n-        rm -rf $f\n-    done\n-else\n-    CLEAN_LIST=(\"${CACHE_CLEAN_LIST[@]}\" \"${BUILD_CLEAN_LIST[@]}\")\n-    for f in ${CLEAN_LIST[*]}; do\n-        rm -rf $f\n-    done\n-fi"}, {"filename": "scripts/collect_rocm_kernels.sh", "status": "removed", "additions": 0, "deletions": 11, "changes": 11, "file_content_changes": "@@ -1,11 +0,0 @@\n-# COPY kernels\n-DIRNAME=triton_rocm_kernels\n-rm -rf $DIRNAME\n-mkdir $DIRNAME\n-mv /tmp/*.ttir $DIRNAME\n-mv /tmp/*.ll $DIRNAME\n-mv /tmp/*.gcn $DIRNAME\n-mv /tmp/*.o $DIRNAME\n-mv /tmp/*.hsaco $DIRNAME\n-mv /tmp/*.s $DIRNAME\n-chmod -R 777 $DIRNAME"}, {"filename": "scripts/create_lib.sh", "status": "removed", "additions": 0, "deletions": 58, "changes": 58, "file_content_changes": "@@ -1,58 +0,0 @@\n-#!/bin/bash\n-\n-# set -x\n-\n-# upstream lib\n-UPSTREAM_LIB=\"libtriton_rocm\"\n-rm -rf ${UPSTREAM_LIB}*\n-\n-# check lib\n-if [[ -z $1 ]]; then\n-    TRITON_LIB=\"python/triton/_C/libtriton.so\"\n-    cp $TRITON_LIB ${UPSTREAM_LIB}.so\n-else\n-    # create dir for object files\n-    OBJ_FOLDER=\".obj_folder\"\n-    rm -rf $OBJ_FOLDER\n-    mkdir $OBJ_FOLDER\n-    OBJ_FOLDER=$(realpath $OBJ_FOLDER)\n-\n-    # libs\n-    LIB_PATHS=(\n-        \"python/build/cmake*/lib/Rocm/libTritonROCM.a\"\n-        \"python/build/cmake*/lib/Analysis/libTritonAnalysis.a\"\n-        \"python/build/cmake*/lib/Conversion/TritonGPUToLLVM/libTritonGPUToLLVM.a\"\n-        \"python/build/cmake*/lib/Conversion/TritonToTritonGPU/libTritonToTritonGPU.a\"\n-        \"python/build/cmake*/lib/Dialect/Triton/IR/libTritonIR.a\"\n-        \"python/build/cmake*/lib/Dialect/Triton/Transforms/libTritonTransforms.a\"\n-        \"python/build/cmake*/lib/Dialect/TritonGPU/IR/libTritonGPUIR.a\"\n-        \"python/build/cmake*/lib/Dialect/TritonGPU/Transforms/libTritonGPUTransforms.a\"\n-    )\n-\n-    # extract object files\n-    for lib_path in ${LIB_PATHS[*]}; do\n-        DIRNAME=$(dirname $lib_path)\n-        DIRNAME_NO_PREFIX=${DIRNAME##*lib/}\n-        mkdir -p $OBJ_FOLDER/$DIRNAME_NO_PREFIX\n-        ar -x $lib_path --output $OBJ_FOLDER/$DIRNAME_NO_PREFIX\n-    done\n-\n-    OBJ_FILES=$(find $OBJ_FOLDER -type f -name \"*.o\")\n-    # echo $OBJ_FILES\n-    if [ \"$1\" == \"shared\" ]; then\n-        # create dynamic lib\n-        gcc -shared -o ${UPSTREAM_LIB}_custom.so $OBJ_FILES\n-    elif [ \"$1\" == \"static\" ]; then\n-        # create static lib\n-        ar -rcs ${UPSTREAM_LIB}_custom.a $OBJ_FILES\n-        # ranlib $UPSTREAM_LIB\n-    else\n-        echo \"Unknown arg: $1\"\n-    fi\n-fi\n-\n-# check symbol exists and is dynamic\n-# echo \"Checking if Symbol Exists\"\n-# objdump -t ${UPSTREAM_LIB}* | grep translateTritonIRToHSACO\n-echo \"Checking if Symbol Exists and is Dynamic\"\n-objdump -T ${UPSTREAM_LIB}* | grep translateTritonIRToHSACO"}, {"filename": "scripts/debug.sh", "status": "removed", "additions": 0, "deletions": 15, "changes": 15, "file_content_changes": "@@ -1,15 +0,0 @@\n-sudo apt install gdb -y\n-\n-# export AMD_OCL_WAIT_COMMAND=1\n-# export AMD_LOG_LEVEL=3\n-# export HIP_LAUNCH_BLOCKING=1\n-\n-gdb -ex \"file python\" \\\n-    -ex 'run -m pytest --capture=tee-sys --verbose \"python/test/unit/language/test_core.py::test_empty_kernel[float32]\"' \\\n-    -ex \"set pagination off\" \\\n-    -ex \"set confirm off\" \\\n-    -ex \"break _exit\" \\\n-    -ex \"commands\"\n-    -ex \"run\"\n-    -ex 'end' \\\n-    2>&1 | tee /dockerx/pytorch/test_core_gdb.log"}, {"filename": "scripts/delete_hip_files.sh", "status": "removed", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -1,2 +0,0 @@\n-# find . -name '*hip.h' -delete\n-find . -name '*_hip.*' -delete\n\\ No newline at end of file"}, {"filename": "scripts/deps.sh", "status": "removed", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -1,3 +0,0 @@\n-sudo apt update\n-sudo apt install libtinfo-dev gdb \n-# sudo apt install llvm-11 # install on cuda\n\\ No newline at end of file"}, {"filename": "scripts/docker_build.sh", "status": "removed", "additions": 0, "deletions": 16, "changes": 16, "file_content_changes": "@@ -1,16 +0,0 @@\n-# print every command\n-set -o xtrace\n-\n-# set path\n-# DOCKERFILE_PATH=scripts/docker/Dockerfile.triton_rocm\n-# DOCKERFILE_PATH=scripts/docker/Dockerfile.triton_cuda\n-# DOCKERFILE_PATH=triton_rocm_all_archs.Dockerfile\n-DOCKERFILE_PATH=triton_rocm_20-52.Dockerfile\n-\n-# get tag\n-DOCKERFILE_NAME=$(basename $DOCKERFILE_PATH)\n-DOCKERIMAGE_NAME=$(echo \"$DOCKERFILE_NAME\" | cut -f -1 -d '.')\n-echo $DOCKERIMAGE_NAME\n-\n-# build docker\n-docker build --build-arg CACHEBUST=$(date +%s) -f $DOCKERFILE_PATH -t $DOCKERIMAGE_NAME ."}, {"filename": "scripts/docker_run.sh", "status": "removed", "additions": 0, "deletions": 30, "changes": 30, "file_content_changes": "@@ -1,30 +0,0 @@\n-set -o xtrace\n-\n-DRUN='sudo docker run -it --rm --network=host --user root --group-add video --cap-add=SYS_PTRACE --security-opt seccomp=unconfined'\n-\n-# DEVICES=\"--gpus all\"\n-DEVICES=\"--device=/dev/kfd --device=/dev/dri\"\n-\n-MEMORY=\"--ipc=host --shm-size 16G\"\n-\n-VOLUMES=\"-v $HOME/dockerx:/dockerx -v /data:/data\"\n-\n-# WORK_DIR='/root/$(basename $(pwd))'\n-WORK_DIR=\"/dockerx/$(basename $(pwd))\"\n-\n-IMAGE_NAME=rocm/pytorch-nightly:latest\n-# IMAGE_NAME=rocm/pytorch:latest\n-# IMAGE_NAME=nvcr.io/nvidia/pytorch\n-\n-CONTAINER_NAME=triton\n-\n-# start new container\n-docker stop $CONTAINER_NAME\n-docker rm $CONTAINER_NAME\n-CONTAINER_ID=$($DRUN -d -w $WORK_DIR --name $CONTAINER_NAME $MEMORY $VOLUMES $DEVICES $IMAGE_NAME)\n-echo \"CONTAINER_ID: $CONTAINER_ID\"\n-# docker cp . $CONTAINER_ID:$WORK_DIR\n-# docker exec $CONTAINER_ID bash -c \"bash scripts/run.sh\"\n-docker attach $CONTAINER_ID\n-docker stop $CONTAINER_ID\n-docker rm $CONTAINER_ID"}, {"filename": "scripts/find_lib.sh", "status": "removed", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -1,2 +0,0 @@\n-LIB_NAME=libtinfow\n-ldconfig -p | grep $LIB_NAME\n\\ No newline at end of file"}, {"filename": "scripts/git_checkout_all.sh", "status": "removed", "additions": 0, "deletions": 11, "changes": 11, "file_content_changes": "@@ -1,11 +0,0 @@\n-#!/bin/bash\n-set -x\n-\n-CUR_BRANCH=$(git rev-parse --abbrev-ref HEAD)\n-REF_BRANCH=$1\n-\n-DIFF_FILES=$(git diff --name-only $REF_BRANCH $CUR_BRANCH)\n-for file in $DIFF_FILES; do\n-\techo \"$file\"\n-\tgit checkout $REF_BRANCH $file\n-done"}, {"filename": "scripts/git_commit_amend.sh", "status": "removed", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -1,2 +0,0 @@\n-git commit --amend --no-edit\n-git push --force\n\\ No newline at end of file"}, {"filename": "scripts/git_config_user.sh", "status": "removed", "additions": 0, "deletions": 7, "changes": 7, "file_content_changes": "@@ -1,7 +0,0 @@\n-# use --global flag if you want to set it for whole machine\n-git config user.name \"Michael Melesse\"\n-git config user.email \"micmelesse@gmail.com\"\n-\n-# unset with\n-# git config --global --unset-all user.name\n-# git config --global --unset-all user.email\n\\ No newline at end of file"}, {"filename": "scripts/git_submodule_add.sh", "status": "removed", "additions": 0, "deletions": 5, "changes": 5, "file_content_changes": "@@ -1,5 +0,0 @@\n-REPO=https://github.com/ROCmSoftwarePlatform/triton\n-BRANCH=third_party_backend_2\n-DIR=third_party/amd_hip_backend\n-\n-git submodule add -b $BRANCH $REPO $DIR\n\\ No newline at end of file"}, {"filename": "scripts/git_submodule_rm.sh", "status": "removed", "additions": 0, "deletions": 10, "changes": 10, "file_content_changes": "@@ -1,10 +0,0 @@\n-SUBMODULE=third_party/hipify-torch\n-\n-# Remove the submodule entry from .git/config\n-git submodule deinit -f $SUBMODULE\n-\n-# Remove the submodule directory from the superproject's .git/modules directory\n-rm -rf .git/modules/$SUBMODULE\n-\n-# Remove the entry in .gitmodules and remove the submodule directory located at path/to/submodule\n-git rm -f $SUBMODULE\n\\ No newline at end of file"}, {"filename": "scripts/git_submodule_update.sh", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-# if you are updating an existing checkout\n-git submodule sync\n-git submodule update --init --recursive\n-\n-# if you want to push every to tip\n-# git submodule update --init --recursive --remote"}, {"filename": "scripts/grep_for_symbol.sh", "status": "removed", "additions": 0, "deletions": 18, "changes": 18, "file_content_changes": "@@ -1,18 +0,0 @@\n-# SYMBOL=_ZN4llvm11PassBuilder17OptimizationLevel2O0E\n-# SYMBOL=_ZN4llvm11DDGAnalysis3KeyE\n-# SYMBOL=_ZN4llvm26UnifyFunctionExitNodesPass3runERNS_8FunctionERNS_15AnalysisManagerIS1_JEEE\n-# SYMBOL=_ZN4llvm12LoopFusePass3runERNS_8FunctionERNS_15AnalysisManagerIS1_JEEE\n-# SYMBOL=_ZN4llvm30moveInstructionsToTheBeginningERNS_10BasicBlockES1_RNS_13DominatorTreeERKNS_17PostDominatorTreeERNS_14DependenceInfoE\n-# SYMBOL=_ZN4llvm17LoopExtractorPass3runERNS_6ModuleERNS_15AnalysisManagerIS1_JEEE\n-# SYMBOL=_ZN4llvm17ObjCARCExpandPass3runERNS_8FunctionERNS_15AnalysisManagerIS1_JEEE\n-# SYMBOL=_ZN4llvm13CoroSplitPass3runERNS_13LazyCallGraph3SCCERNS_15AnalysisManagerIS2_JRS1_EEES5_RNS_17CGSCCUpdateResultE\n-SYMBOL=_ZN4llvm20SyntheticCountsUtilsIPKNS_9CallGraphEE9propagateERKS3_NS_12function_refIFNS_8OptionalINS_12ScaledNumberImEEEEPKNS_13CallGraphNodeERKSt4pairINS8_INS_14WeakTrackingVHEEEPSC_EEEENS7_IFvSE_SA_EEE\n-for lib in $(find /tmp/clang+llvm-13.0.0-x86_64-linux-gnu-ubuntu-16.04/ -name \\*.a); do\n-    symbols=$(nm $lib | grep $SYMBOL | grep -v \" U \")\n-    \n-    if [ \"${#symbols}\" -gt \"0\" ]; then\n-        echo $lib\n-        echo $symbols\n-    fi\n-\n-done"}, {"filename": "scripts/hip_kernel.cpp", "status": "removed", "additions": 0, "deletions": 53, "changes": 53, "file_content_changes": "@@ -1,53 +0,0 @@\n-#include <hip/hip_fp16.h>\n-#include <hip/hip_runtime.h>\n-\n-__global__ void div_kernel(float *in_1, float *in_2, float *out) {\n-  int i = threadIdx.x;\n-  out[i] = in_1[i] / in_2[i];\n-}\n-\n-int main() {\n-  // kernel info\n-#define nBlocks 1\n-#define nThreads 2\n-\n-  // vector size\n-  size_t size = nThreads * sizeof(float);\n-\n-  // Allocate input vectors h_A and h_B in host memory\n-  float h_A[nThreads] = {4, 4};\n-  float h_B[nThreads] = {2, 2};\n-  float h_C[nThreads] = {};\n-\n-  // show data\n-  printf(\"Input Data\\n\");\n-  for (int i = 0; i < nThreads; i++) {\n-    printf(\"%f/%f = %f\\n\", h_A[i], h_B[i], h_C[i]);\n-  }\n-\n-  // Allocate vectors in device memory\n-  float *d_A;\n-  hipMalloc(&d_A, size);\n-  float *d_B;\n-  hipMalloc(&d_B, size);\n-  float *d_C;\n-  hipMalloc(&d_C, size);\n-\n-  // Copy vectors from host memory to device memory\n-  hipMemcpyHtoD(d_A, h_A, size);\n-  hipMemcpyHtoD(d_B, h_B, size);\n-\n-  // launch kernel\n-  div_kernel<<<nBlocks, nThreads>>>(d_A, d_B, d_C);\n-  hipDeviceSynchronize(); // wait for kernel before printting\n-\n-  // check kernel output\n-  bool pass = true;\n-  printf(\"Output Data\\n\");\n-  for (int i = 0; i < nThreads; i++) {\n-    if (d_A[i] / d_B[i] != d_C[i])\n-      pass = false;\n-    printf(\"%f/%f = %f\\n\", d_A[i], d_B[i], d_C[i]);\n-  }\n-  printf(\"Test %s\\n\", pass ? \"PASS\" : \"FAIL\");\n-}\n\\ No newline at end of file"}, {"filename": "scripts/hipify.sh", "status": "removed", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1 +0,0 @@\n-PYTHONDONTWRITEBYTECODE=1 python3 third_party/hipify-torch/hipify_cli.py --project-directory ."}, {"filename": "scripts/lit.sh", "status": "removed", "additions": 0, "deletions": 21, "changes": 21, "file_content_changes": "@@ -1,21 +0,0 @@\n-python3 -m pip install lit\n-cd python\n-LIT_TEST_DIR=\"build/$(ls build | grep -i cmake)/test\"\n-if [ ! -d \"${LIT_TEST_DIR}\" ]; then\n-    echo \"Could not find '${LIT_TEST_DIR}'\"\n-    exit -1\n-fi\n-\n-lit -v \"$LIT_TEST_DIR\"\n-# lit -v \"$LIT_TEST_DIR/Conversion/AMDGPU/load_store.mlir\"\n-# lit -v \"$LIT_TEST_DIR/Conversion/tritongpu_to_llvm.mlir\"\n-# lit -v \"$LIT_TEST_DIR/Target/mlir_to_amdgcn_float16_vectorized_load_store.mlir\"\n-# lit -v \"$LIT_TEST_DIR/Target/tritongpu_to_llvmir.mlir\"\n-# lit -v \"$LIT_TEST_DIR/Target/mlir_to_amdgcn_float16_vectorized_load_store.mlir\"\n-# lit -v \"$LIT_TEST_DIR/Target/mlir_to_amdgcn_int16_vectorized_load_store.mlir\"\n-# lit -v \"$LIT_TEST_DIR/Target/tritongpu_to_amdgcn.mlir\"\n-# lit -v \"$LIT_TEST_DIR/Target/tritongpu_to_hsaco.mlir\"\n-# lit -v \"$LIT_TEST_DIR/Target/tritongpu_to_llvmir.mlir\"\n-# lit -v \"$LIT_TEST_DIR/Target/tritongpu_to_ptx.mlir?\n-\n-# triton-opt %s -split-input-file --convert-triton-gpu-to-llvm | FileCheck --check-prefixes=CHECK,GCN %s"}, {"filename": "scripts/lld.sh", "status": "removed", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1 +0,0 @@\n-${ROCM_PATH}/llvm/bin/ld.lld -flavor gnu -shared _empty.o -o _empty.hsaco"}, {"filename": "scripts/post.sh", "status": "removed", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -1,2 +0,0 @@\n-bash scripts/collect_rocm_kernels.sh\n-bash scripts/check_llvm_src.sh\n\\ No newline at end of file"}, {"filename": "scripts/pytorch.sh", "status": "removed", "additions": 0, "deletions": 8, "changes": 8, "file_content_changes": "@@ -1,8 +0,0 @@\n-# pip install transformers\n-# pip install --upgrade diffusers[torch]\n-# cd ../stuff/stable_diff\n-# python run.py\n-\n-cd ../pytorch_rocm/\n-# TORCHINDUCTOR_COMPILE_THREADS=1 pytest test/inductor/test_torchinductor.py -k \"test_views4_cuda\"\n-pytest test/inductor/test_torchinductor.py -k \"test_views4_cuda\""}, {"filename": "scripts/run.sh", "status": "removed", "additions": 0, "deletions": 20, "changes": 20, "file_content_changes": "@@ -1,20 +0,0 @@\n-clear\n-\n-set -x\n-\n-\n-ROOT_DIR=$(pwd)\n-LOG_DIR=$ROOT_DIR/log\n-rm -rf $LOG_DIR\n-mkdir -p $LOG_DIR\n-chmod -R 777 $LOG_DIR\n-\n-\n-# bash scripts/clean.sh 2>&1 |tee $LOG_DIR/clean.log\n-bash scripts/build.sh 2>&1 |tee $LOG_DIR/build.log\n-bash scripts/test.sh 2>&1 |tee $LOG_DIR/test.log\n-# bash scripts/test.sh backtrace 2>&1 |tee $LOG_DIR/backtrace.log\n-# bash scripts/create_lib.sh 2>&1 |tee $LOG_DIR/create_lib.log\n-# bash scripts/pytorch.sh 2>&1 |tee $LOG_DIR/test.log\n-# bash scripts/lit.sh 2>&1 |tee $LOG_DIR/lit.log\n-# bash scripts/cache_print.sh  2>&1 |tee $LOG_DIR/cache.log\n\\ No newline at end of file"}, {"filename": "scripts/test.sh", "status": "removed", "additions": 0, "deletions": 142, "changes": 142, "file_content_changes": "@@ -1,142 +0,0 @@\n-#!/bin/bash\n-\n-# clear\n-set -x\n-\n-\n-# log dir\n-ROOT_DIR=$(pwd)\n-LOG_DIR=$ROOT_DIR/log\n-# rm -rf $LOG_DIR\n-mkdir -p $LOG_DIR\n-chmod -R 777 $LOG_DIR\n-\n-# export MLIR_ENABLE_DUMP=1\n-# export LLVM_IR_ENABLE_DUMP=1\n-# export AMDGCN_ENABLE_DUMP=1\n-\n-# clean to avoid kernel conflicts\n-bash scripts/clean.sh cache\n-\n-UNIT_TEST=\"python/test/unit/language/test_core.py\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_dot_without_load\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_noinline\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_noinline[\"shared\"]\"  # fails on rocm fork\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_noinline[\"dynamic\"]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_empty_kernel[float32]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_bin_op\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_bitwise_op\" \n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_bitwise_op[uint8-int8-&1]\" \n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_bitwise_op[int8-int8-&1]\" #TODO compare IR\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_bin_op[int32-int32-+]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_bin_op[float32-float32-+]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_chain_reduce\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_convert1d\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_store_cache_modifier\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_convert1d[0-0-dst_layout0-src_layout2-64]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_convert2d\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_convert2d[dst_layout0-None-src_layout2-float16-shape0]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_convert1d[0-0-dst_layout0-src_layout0-64]\" \n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_scan_layouts\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_scan_layouts[0-src_layout0-32-32]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_reduce_layouts\" \n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_reduce_layouts[0-src_layout0-128-16]\" \n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_dot\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_dot_mulbroadcastred\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_dot[64-128-128-4-True-True-none-True-float32-float32]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_dot[128-128-64-4-True-True-none-True-float32-float32]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_dot[128-256-32-8-True-True-none-True-float32-float32]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_dot[16-16-16-4-False-False-none-True-float32-float32]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_dot[64-64-64-4-False-False-none-True-float32-float32]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_dot[64-64-64--False-False-none-True-float32-float32]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_dot[64-64-64-4-False-False-none-False-float16-float16]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_dot[16-16-16-4-False-False-none-False-float32-float32]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_dot[64-64-64-4-False-False-none-False-float32-float32]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_vectorization\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_vectorization_hints\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_where\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_where[int32]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_where[float32]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_where[int64]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_where[float64]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_load_store_same_ptr\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_index1d\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_globaltimer\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_smid\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_cast\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_masked_load_shared_memory\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_pointer_arguments\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_cat\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_generic_reduction\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_atomic_cas\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_permute\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_load_cache_modifier\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_load_store_same_ptr\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_atomic_rmw\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_atomic_rmw[add-float32-all_neg-None]\"\n-#  UNIT_TEST=\"python/test/unit/language/test_core.py::test_atomic_rmw[add-float16-all_pos-None]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_math_tensor\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_math_scalar\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_bin_op_constexpr\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_f8_xf16_roundtrip\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_math_tensor[float32-math.scalbn-]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_bin_op[float32-float32-+]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_reduce1d\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_reduce1d[min-int32-2]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_reduce1d[argmin-int32-32]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_reduce2d\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_reduce2d[argmax-int32-shape14-1]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_convert1d\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_store_op\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_store_op[src_layout0-32]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_index1d\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_index1d[x[None, :]-int32]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_shift_op\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_shift_op[int8-int8-<<]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_shift_op[int32-int32-<<]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_cat\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_tensor_atomic_rmw\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_tensor_atomic_rmw[shape0-0]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_empty_kernel\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_empty_kernel[float32]\"\n-# UNIT_TEST=\"python/test/unit/language/test_device_backend.py::test_empty_kernel_hip\"\n-# UNIT_TEST=\"python/test/unit/language/test_device_backend.py::test_empty_kernel_hip[float32]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_bin_op\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_bin_op[float32-float32-+]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_bin_op[int8-float16-%]\"\n-# UNIT_TEST=\"python/test/unit/runtime/test_cache.py::test_compile_in_subproc\"\n-# UNIT_TEST=\"python/test/unit/language/test_core_amd.py::test_shift_op[int8-int8-<<]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core_amd.py::test_shift_op[int32-int32->>]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_bin_op\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_bin_op[float32-float32-+]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_bin_op[int8-float16-%]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core_amd.py::test_masked_load_shared_memory[dtype0]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core_amd.py::test_masked_load_shared_memory[dtype1]\"\n-# UNIT_TEST=\"python/test/unit/language/test_elementwise.py\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_bin_op[int8-float16-%]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_tensor_atomic_rmw[shape0-0]\"\n-# UNIT_TEST=\"python/test/unit/language/test_core.py::test_cat\"\n-\n-OUTPUT_MODE=\"--capture=tee-sys -vv\"\n-# OUTPUT_MODE=\"--capture=tee-sys -v\"\n-\n-# FILTER='-k uint8-uint8-'\n-\n-# check for backtrace\n-if [ \"$1\" == \"backtrace\" ]; then\n-\tsudo apt install gdb -y\n-\n-\tCOMMAND=\"-m pytest $OUTPUT_MODE $UNIT_TEST\"\n-\tgdb python \\\n-\t\t-ex \"set pagination off\" \\\n-\t\t-ex \"run $COMMAND\" \\\n-\t\t-ex \"backtrace\" \\\n-\t\t-ex \"set confirm off\" \\\n-\t\t-ex \"q\" \\\n-\t\t2>&1 | tee $LOG_DIR/backtrace.log\n-\n-else\n-\tpytest $OUTPUT_MODE -rfs --verbose \"$UNIT_TEST\" $FILTER 2>&1 | tee $LOG_DIR/unit_test.log\n-fi\n-\n-bash scripts/cache_print.sh $LOG_DIR > $LOG_DIR/cache.log"}, {"filename": "scripts/test_fptrunc.py", "status": "removed", "additions": 0, "deletions": 62, "changes": 62, "file_content_changes": "@@ -1,62 +0,0 @@\n-import torch\n-import triton\n-import triton.language as tl\n-import pytest\n-\n-cvt = {\n-    'bool': torch.bool,\n-    'int8': torch.int8,\n-    'int16': torch.int16,\n-    'int32': torch.int32,\n-    'int64': torch.int64,\n-    'bfloat16': torch.bfloat16,\n-    'float16': torch.float16,\n-    'float32': torch.float32,\n-    'float64': torch.float64,\n-}\n-\n-int_dtypes = ['int8', 'int16', 'int32', 'int64']\n-float_dtypes = ['float16', 'float32', 'float64']\n-dtypes = int_dtypes + float_dtypes\n-\n-\n-@pytest.mark.parametrize(\"dtype_x, dtype_z, bitcast\", [\n-    (dtype_x, dtype_z, False)\n-    for dtype_x in dtypes\n-    for dtype_z in dtypes\n-])\n-def test_fptrunc(dtype_x, dtype_z, bitcast, device='cuda'):\n-    SIZE = 256\n-    # define the kernel / launch-grid\n-    @triton.jit\n-    def kernel(Z, X, **meta):\n-        off = tl.arange(0, meta['SIZE'])\n-        x = tl.load(X + off)\n-        tl.store(Z + off, x)\n-    # inputs\n-    x = triton.testing.random(SIZE, dtype=cvt[dtype_x], device=device)\n-\n-    # reference result\n-    z_ref = x.type(dtype=cvt[dtype_z])\n-\n-    # triton result\n-    z_tri = torch.zeros_like(x, dtype=cvt[dtype_z])\n-\n-    # triton.testing.assert_almost_equal(z_ref, z_tri)\n-\n-    print(\"before kernel\")\n-    # run load and store kernel\n-    kernel[(1, )](z_tri, x, SIZE=SIZE, num_warps=1)\n-    print(\"after kernel\")\n-\n-    # print(\"x:\", x)\n-    # print(\"z_ref:\", z_ref)\n-    # print(\"z_tri:\", z_tri)\n-    # compare\n-    print(\"before compare\")\n-    triton.testing.assert_almost_equal(z_ref, z_tri)\n-    print(\"after compare\")\n-\n-\n-if __name__ == '__main__':\n-    test_fptrunc()"}, {"filename": "scripts/test_hip_kernel.sh", "status": "removed", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -1,6 +0,0 @@\n-rm -rf ./scripts/hip_kernel.out\n-rm -rf ./scripts/temps\n-mkdir ./scripts/temps\n-# hipcc -save-temps=./scripts/temps scripts/hip_kernel.cpp -o scripts/hip_kernel.out\n-hipcc -ffast-math -save-temps=./scripts/temps scripts/hip_kernel.cpp -o scripts/hip_kernel.out\n-./scripts/hip_kernel.out\n\\ No newline at end of file"}, {"filename": "third_party/amd_hip_backend", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1 +1 @@\n-Subproject commit 3b1b8cc902be11035f68c21c8b83741c536415a8\n+Subproject commit 1d792ee1c6dec37a28ac7572734f357516996850"}]