[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 126, "deletions": 1, "changes": 127, "file_content_changes": "@@ -4729,6 +4729,131 @@ struct PrintfOpConversion\n   }\n };\n \n+/// ====================== atomic_rmw codegen begin ==========================\n+struct AtomicRMWOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::AtomicRMWOp>,\n+      public LoadStoreConversionBase {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::AtomicRMWOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  AtomicRMWOpConversion(LLVMTypeConverter &converter,\n+                        AxisInfoAnalysis &axisAnalysisPass,\n+                        PatternBenefit benefit)\n+      : ConvertTritonGPUOpToLLVMPattern<triton::AtomicRMWOp>(converter,\n+                                                             benefit),\n+        LoadStoreConversionBase(axisAnalysisPass) {}\n+\n+  LogicalResult\n+  matchAndRewrite(triton::AtomicRMWOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+\n+    auto loc = op.getLoc();\n+    MLIRContext *ctx = rewriter.getContext();\n+\n+    auto atomicRmwAttr = op.atomic_rmw_op();\n+    Value ptr = op.ptr();\n+    Value val = op.val();\n+    Value mask = op.mask();\n+\n+    auto llAtomicRmwAttr = adaptor.atomic_rmw_op();\n+    Value llPtr = adaptor.ptr();\n+    Value llVal = adaptor.val();\n+    Value llMask = adaptor.mask();\n+\n+    auto valElements = getElementsFromStruct(loc, llVal, rewriter);\n+    auto ptrElements = getElementsFromStruct(loc, llPtr, rewriter);\n+    auto maskElements = getElementsFromStruct(loc, llMask, rewriter);\n+\n+    auto valueTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n+    if (!valueTy)\n+      return failure();\n+    Type valueElemTy =\n+        getTypeConverter()->convertType(valueTy.getElementType());\n+\n+    auto valTy = val.getType().cast<RankedTensorType>();\n+    const size_t valueElemNbits = valueElemTy.getIntOrFloatBitWidth();\n+    auto vec = getVectorSize(ptr);\n+    vec = std::min<unsigned>(vec, valTy.getElementType().isF16() ? 2 : 1);\n+\n+    auto vecTy = LLVM::getFixedVectorType(valueElemTy, vec);\n+    auto elemsPerThread = getElemsPerThread(val.getType());\n+    SmallVector<Value> resultVals(elemsPerThread);\n+    for (size_t i = 0; i < elemsPerThread; i += vec) {\n+      Value rmw_val = undef(vecTy);\n+      for (int ii = 0; ii < vec; ii++) {\n+        Value iiVal = createIndexAttrConstant(\n+            rewriter, loc, getTypeConverter()->getIndexType(), ii);\n+        rmw_val = insert_element(vecTy, rmw_val, valElements[i], iiVal);\n+      }\n+      Value rmw_ptr = bitcast(ptrElements[i], ptr_ty(valTy.getElementType()));\n+      std::string s_ty{\"\"};\n+      PTXBuilder ptxBuilder;\n+      auto &atom = *ptxBuilder.create<>(\"atom\");\n+      atom.o(\"global\").o(\"gpu\");\n+      auto rmw_op = stringifyRMWOp(atomicRmwAttr).str();\n+      auto sBits = std::to_string(valueElemNbits);\n+      switch (atomicRmwAttr) {\n+      case RMWOp::AND:\n+        s_ty = \"b\" + sBits;\n+        break;\n+      case RMWOp::OR:\n+        s_ty = \"b\" + sBits;\n+        break;\n+      case RMWOp::XOR:\n+        s_ty = \"b\" + sBits;\n+        break;\n+      case RMWOp::ADD:\n+        s_ty = \"s\" + sBits;\n+        break;\n+      case RMWOp::FADD:\n+        rmw_op = \"add\";\n+        rmw_op += (valueElemNbits == 16 ? \".noftz\" : \"\");\n+        s_ty = \"f\" + sBits;\n+        s_ty += (vec == 2 && valueElemNbits == 16) ? \"x2\" : \"\";\n+        break;\n+      case RMWOp::MAX:\n+        s_ty = \"s\" + sBits;\n+        break;\n+      case RMWOp::MIN:\n+        s_ty = \"s\" + sBits;\n+        break;\n+      case RMWOp::UMAX:\n+        rmw_op = \"max\";\n+        s_ty = \"u\" + sBits;\n+        break;\n+      case RMWOp::UMIN:\n+        rmw_op = \"min\";\n+        s_ty = \"u\" + sBits;\n+        break;\n+      default:\n+        return failure();\n+      }\n+      atom.o(rmw_op).o(s_ty);\n+      auto width = vec * valueElemNbits;\n+\n+      auto *dstOpr = ptxBuilder.newOperand(\"=r\");\n+      auto *ptrOpr = ptxBuilder.newAddrOperand(rmw_ptr, \"r\");\n+      auto *valOpr = ptxBuilder.newOperand(rmw_val, \"r\");\n+\n+      atom(dstOpr, ptrOpr, valOpr);\n+\n+      auto ret =\n+          ptxBuilder.launch(rewriter, loc, valueElemTy,\n+                            false);\n+      for (int ii = 0; ii < vec; ii++) {\n+        resultVals[i * vec + ii] =\n+            vec == 1 ? ret : extract_element(vecTy, ret, idx_val(ii));\n+      }\n+    }\n+    Type llvmResultStructTy = getTypeConverter()->convertType(valueTy);\n+    Value resultStruct =\n+        getStructFromElements(loc, resultVals, rewriter, llvmResultStructTy);\n+    rewriter.replaceOp(op, {resultStruct});\n+    return success();\n+  }\n+};\n+/// ====================== atomic_rmw codegen end ==========================\n+\n void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                   RewritePatternSet &patterns, int numWarps,\n                                   AxisInfoAnalysis &axisInfoAnalysis,\n@@ -4800,7 +4925,7 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<ReduceOpConversion>(typeConverter, allocation, smem, benefit);\n   patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n                                           benefit);\n-\n+  patterns.add<AtomicRMWOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n   patterns.add<ExtractSliceOpConversion>(typeConverter, allocation, smem,\n                                          benefit);\n   patterns.add<GetProgramIdOpConversion>(typeConverter, benefit);"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "file_content_changes": "@@ -789,3 +789,13 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     return\n   }\n }\n+\n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  func @atomic_add_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xi1, #blocked0>, %arg2 : tensor<256xf32, #blocked0>) {\n+    %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32} : (tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xf32, #blocked0>, tensor<256xi1, #blocked0>) -> tensor<256xf32, #blocked0>\n+    return\n+  }\n+}"}]