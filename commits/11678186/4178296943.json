[{"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "file_content_changes": "@@ -257,8 +257,7 @@ def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [NoSideEffect,\n }\n \n def TT_ViewOp : TT_Op<\"view\", [NoSideEffect,\n-                               SameOperandsAndResultElementType,\n-                               SameOperandsAndResultEncoding]> {\n+                               SameOperandsAndResultElementType]> {\n     let summary = \"view\";\n \n     let arguments = (ins TT_Tensor:$src);\n@@ -284,8 +283,7 @@ def TT_BroadcastOp : TT_Op<\"broadcast\", [NoSideEffect,\n }\n \n def TT_CatOp : TT_Op<\"cat\", [NoSideEffect,\n-                             SameOperandsAndResultElementType,\n-                             SameOperandsAndResultEncoding]> {\n+                             SameOperandsAndResultElementType]> {\n     let summary = \"concatenate 2 tensors\";\n \n     let arguments = (ins TT_Tensor:$lhs, TT_Tensor:$rhs);"}, {"filename": "lib/Dialect/Triton/IR/Traits.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -3,7 +3,7 @@\n static mlir::LogicalResult verifySameEncoding(mlir::Type tyA, mlir::Type tyB) {\n   using namespace mlir;\n   auto encA = tyA.dyn_cast<RankedTensorType>();\n-  auto encB = tyA.dyn_cast<RankedTensorType>();\n+  auto encB = tyB.dyn_cast<RankedTensorType>();\n   if (!encA || !encB)\n     return success();\n   return encA.getEncoding() == encB.getEncoding() ? success() : failure();"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -282,6 +282,8 @@ LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n         targetEncoding.dyn_cast<triton::gpu::SliceEncodingAttr>();\n     if (!sliceEncoding)\n       return failure();\n+    if (sliceEncoding.getDim() != reduce.axis())\n+      return failure();\n     ret = sliceEncoding.getParent();\n   }\n   if (auto view = dyn_cast<triton::ViewOp>(op)) {"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 26, "deletions": 4, "changes": 30, "file_content_changes": "@@ -211,6 +211,20 @@ LogicalResult LoopPipeliner::initialize() {\n     if (isCandidate && loadOp.getResult().hasOneUse()) {\n       isCandidate = false;\n       Operation *use = *loadOp.getResult().getUsers().begin();\n+\n+      // advance to the first conversion as long\n+      // as the use resides in shared memory and it has\n+      // a single use itself\n+      while (use) {\n+        if (use->getNumResults() != 1 || !use->getResult(0).hasOneUse())\n+          break;\n+        auto tensorType =\n+            use->getResult(0).getType().dyn_cast<RankedTensorType>();\n+        if (!tensorType.getEncoding().isa<ttg::SharedEncodingAttr>())\n+          break;\n+        use = *use->getResult(0).getUsers().begin();\n+      }\n+\n       if (auto convertLayout = llvm::dyn_cast<ttg::ConvertLayoutOp>(use)) {\n         if (auto tensorType = convertLayout.getResult()\n                                   .getType()\n@@ -368,10 +382,14 @@ void LoopPipeliner::emitPrologue() {\n                                    loads.size() * (numStages - 2));\n   loopIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n   for (Value loadOp : loads) {\n+    auto bufferType = loadStageBuffer[loadOp][numStages - 1]\n+                          .getType()\n+                          .cast<RankedTensorType>();\n+    auto bufferShape = bufferType.getShape();\n     auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n-    sliceType =\n-        RankedTensorType::get(sliceType.getShape(), sliceType.getElementType(),\n-                              loadsBufferType[loadOp].getEncoding());\n+    sliceType = RankedTensorType::get({bufferShape[1], bufferShape[2]},\n+                                      sliceType.getElementType(),\n+                                      loadsBufferType[loadOp].getEncoding());\n     Value extractSlice = builder.create<tensor::ExtractSliceOp>(\n         loadOp.getLoc(), sliceType, loadStageBuffer[loadOp][numStages - 1],\n         SmallVector<OpFoldResult>{int_attr(0), int_attr(0), int_attr(0)},\n@@ -561,10 +579,14 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n           loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n       builder.create<triton::gpu::AsyncCommitGroupOp>(op->getLoc());\n       nextBuffers.push_back(insertAsyncOp);\n+      // ExtractSlice\n+      auto bufferType = insertAsyncOp.getType().cast<RankedTensorType>();\n+      auto bufferShape = bufferType.getShape();\n       auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n-      sliceType = RankedTensorType::get(sliceType.getShape(),\n+      sliceType = RankedTensorType::get({bufferShape[1], bufferShape[2]},\n                                         sliceType.getElementType(),\n                                         loadsBufferType[loadOp].getEncoding());\n+\n       nextOp = builder.create<tensor::ExtractSliceOp>(\n           op->getLoc(), sliceType, insertAsyncOp,\n           SmallVector<OpFoldResult>{extractSliceIndex, int_attr(0),"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -32,7 +32,7 @@ def _fwd_kernel(\n     offs_n = tl.arange(0, BLOCK_N)\n     offs_d = tl.arange(0, BLOCK_DMODEL)\n     off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n+    off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :] * stride_kk\n     off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n     # Initialize pointers to Q, K, V\n     q_ptrs = Q + off_q\n@@ -49,7 +49,7 @@ def _fwd_kernel(\n         # -- compute qk ----\n         k = tl.load(k_ptrs)\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-        qk += tl.dot(q, k)\n+        qk += tl.dot(q, tl.trans(k))\n         qk *= sm_scale\n         qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n         # compute new m"}]