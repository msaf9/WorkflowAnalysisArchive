[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "@@ -95,6 +95,15 @@ jobs:\n           fi\n           lit -v \"${LIT_TEST_DIR}\"\n \n+      - name: Regression tests\n+        if: ${{ contains(matrix.runner, 'A100') }}\n+        run: |\n+          cd python/test/regression\n+          sudo nvidia-smi -i 0 -pm 1\n+          sudo nvidia-smi -i 0 --lock-gpu-clocks=1350,1350\n+          python3 -m pytest -vs .\n+          sudo nvidia-smi -i 0 -rgc\n+\n       - name: Run python tests on CUDA\n         if: ${{ env.BACKEND == 'CUDA'}}\n         run: |\n@@ -113,12 +122,3 @@ jobs:\n           cd python\n           cd \"build/$(ls build | grep -i temp)\"\n           ctest\n-\n-      - name: Regression tests\n-        if: ${{ contains(matrix.runner, 'A100') }}\n-        run: |\n-          cd python/test/regression\n-          sudo nvidia-smi -i 0 -pm 1\n-          sudo nvidia-smi -i 0 --lock-gpu-clocks=1350,1350\n-          python3 -m pytest -vs .\n-          sudo nvidia-smi -i 0 -rgc"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -97,7 +97,7 @@ def test_matmul(M, N, K, dtype_str):\n     ms = triton.testing.do_bench(fn, warmup=100, rep=300)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n \n #######################\n@@ -153,7 +153,7 @@ def test_elementwise(N):\n     ms = triton.testing.do_bench(fn, warmup=100, rep=500)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n #######################\n # Flash-Attention\n@@ -201,4 +201,4 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n     max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, mode, dtype_str)]\n-    torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)"}]