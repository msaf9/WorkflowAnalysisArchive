[{"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -161,9 +161,16 @@ class AllocationAnalysis {\n       // TODO(Keren): Reduce with index is not supported yet.\n       auto value = op->getOperand(0);\n       if (auto tensorType = value.getType().dyn_cast<RankedTensorType>()) {\n+        auto srcLayout = tensorType.getEncoding();\n+        bool fastReduce = reduceOp.axis() == getOrder(srcLayout)[0];\n         auto smemShape = getScratchConfigForReduce(reduceOp);\n         unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n                                          std::multiplies{});\n+        if (fastReduce) {\n+          auto mod = op->getParentOfType<ModuleOp>();\n+          unsigned numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+          elems = std::max<unsigned>(elems, numWarps * 32);\n+        }\n         auto bytes = elems * tensorType.getElementTypeBitWidth() / 8;\n         allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n       }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 117, "deletions": 19, "changes": 136, "file_content_changes": "@@ -64,6 +64,12 @@ Value createConstantF32(Location loc, PatternRewriter &rewriter, float v) {\n                                            rewriter.getF32FloatAttr(v));\n }\n \n+Value createConstantF64(Location loc, PatternRewriter &rewriter, float v) {\n+  auto type = type::f64Ty(rewriter.getContext());\n+  return rewriter.create<LLVM::ConstantOp>(loc, type,\n+                                           rewriter.getF64FloatAttr(v));\n+}\n+\n // Create an index type constant.\n static Value createIndexConstant(OpBuilder &builder, Location loc,\n                                  TypeConverter *converter, int64_t value) {\n@@ -133,6 +139,7 @@ void llPrintf(StringRef msg, ValueRange args,\n #define f64_ty rewriter.getF64Type()\n #define vec_ty(type, num) VectorType::get(num, type)\n #define f32_val(...) LLVM::createConstantF32(loc, rewriter, __VA_ARGS__)\n+#define f64_val(...) LLVM::createConstantF64(loc, rewriter, __VA_ARGS__)\n #define void_ty(ctx) LLVM::LLVMVoidType::get(ctx)\n #define struct_ty(...) LLVM::LLVMStructType::getLiteral(ctx, __VA_ARGS__)\n \n@@ -348,7 +355,8 @@ Value getStructFromElements(Location loc, ValueRange resultVals,\n   return llvmStruct;\n }\n \n-// Delinearize on compile-time consts, assuming the order is [n, .. 2, 1, 0]\n+// TODO[goostavz]: to be deprecated\n+// delinearize supposing order is [n, .. , 2, 1, 0]\n template <typename T>\n static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape) {\n   // shape: {a, b, c, d}  ->  accMul: {b*c*d, c*d, d, 1}\n@@ -366,7 +374,40 @@ static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape) {\n   return multiDimIndex;\n }\n \n-// Linearize on compile-time consts, assuming the order is [n, .. 2, 1, 0]\n+// delinearize supposing order is [0, 1, .. , n]\n+template <typename T>\n+static SmallVector<T> getMultiDimIndexImpl(T linearIndex, ArrayRef<T> shape) {\n+  // shape: {a, b, c, d}  ->  accMul: {1, a, a*b, a*b*c}\n+  size_t rank = shape.size();\n+  T accMul = product(shape.drop_back());\n+  T linearRemain = linearIndex;\n+  SmallVector<T> multiDimIndex(rank);\n+  for (int i = rank - 1; i >= 0; --i) {\n+    multiDimIndex[i] = linearRemain / accMul;\n+    linearRemain = linearRemain % accMul;\n+    if (i != 0) {\n+      accMul = accMul / shape[i - 1];\n+    }\n+  }\n+  return multiDimIndex;\n+}\n+\n+template <typename T>\n+static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape,\n+                                       ArrayRef<unsigned> order) {\n+  size_t rank = shape.size();\n+  assert(rank == order.size());\n+  auto reordered = reorder(shape, order);\n+  auto reorderedMultiDim = getMultiDimIndexImpl<T>(linearIndex, reordered);\n+  SmallVector<T> multiDim(rank);\n+  for (unsigned i = 0; i < rank; ++i) {\n+    multiDim[order[i]] = reorderedMultiDim[i];\n+  }\n+  return multiDim;\n+}\n+\n+// TODO[goostavz]: to be deprecated\n+// linearize supposing order is [n, .. , 2, 1, 0]\n template <typename T>\n static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n   assert(multiDimIndex.size() == shape.size());\n@@ -383,6 +424,30 @@ static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n   return linearIndex;\n }\n \n+template <typename T>\n+static T getLinearIndexImpl(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n+  assert(multiDimIndex.size() == shape.size());\n+  // shape: {a, b, c, d}  ->  accMul: {1, a, a*b, a*b*c}\n+  size_t rank = shape.size();\n+  T accMul = product(shape.drop_back());\n+  T linearIndex = 0;\n+  for (int i = rank - 1; i >= 0; --i) {\n+    linearIndex += multiDimIndex[i] * accMul;\n+    if (i != 0) {\n+      accMul = accMul / shape[i - 1];\n+    }\n+  }\n+  return linearIndex;\n+}\n+\n+template <typename T>\n+static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape,\n+                        ArrayRef<unsigned> order) {\n+  assert(shape.size() == order.size());\n+  return getLinearIndexImpl<T>(reorder(multiDimIndex, order),\n+                               reorder(shape, order));\n+}\n+\n static Value storeShared(ConversionPatternRewriter &rewriter, Location loc,\n                          Value ptr, Value val, Value pred) {\n   MLIRContext *ctx = rewriter.getContext();\n@@ -673,6 +738,7 @@ class ConvertTritonGPUOpToLLVMPattern\n     auto sizePerThread = blockedLayout.getSizePerThread();\n     auto threadsPerWarp = blockedLayout.getThreadsPerWarp();\n     auto warpsPerCTA = blockedLayout.getWarpsPerCTA();\n+    auto order = blockedLayout.getOrder();\n \n     unsigned rank = shape.size();\n     SmallVector<unsigned> shapePerCTA = getShapePerCTA(blockedLayout);\n@@ -704,9 +770,9 @@ class ConvertTritonGPUOpToLLVMPattern\n       unsigned linearNanoTileId = n / totalSizePerThread;\n       unsigned linearNanoTileElemId = n % totalSizePerThread;\n       SmallVector<unsigned> multiDimNanoTileId =\n-          getMultiDimIndex<unsigned>(linearNanoTileId, tilesPerDim);\n-      SmallVector<unsigned> multiDimNanoTileElemId =\n-          getMultiDimIndex<unsigned>(linearNanoTileElemId, sizePerThread);\n+          getMultiDimIndex<unsigned>(linearNanoTileId, tilesPerDim, order);\n+      SmallVector<unsigned> multiDimNanoTileElemId = getMultiDimIndex<unsigned>(\n+          linearNanoTileElemId, sizePerThread, order);\n       for (unsigned k = 0; k < rank; ++k) {\n         unsigned reorderedMultiDimId =\n             multiDimNanoTileId[k] *\n@@ -1922,8 +1988,6 @@ struct PrintfOpConversion\n   // currently support pointer, i8, i16, i32, i64, f16, bf16, f32, f64\n   std::string getFormatSubstr(Value value) const {\n     Type type = value.getType();\n-    unsigned width = type.getIntOrFloatBitWidth();\n-\n     if (type.isa<LLVM::LLVMPointerType>()) {\n       return \"%p\";\n     } else if (type.isBF16() || type.isF16() || type.isF32() || type.isF64()) {\n@@ -1965,13 +2029,11 @@ struct PrintfOpConversion\n   promoteValue(ConversionPatternRewriter &rewriter, Value value) {\n     auto *context = rewriter.getContext();\n     auto type = value.getType();\n-    type.dump();\n-    unsigned width = type.getIntOrFloatBitWidth();\n     Value newOp = value;\n     Type newType = type;\n \n     bool bUnsigned = type.isUnsignedInteger();\n-    if (type.isIntOrIndex() && width < 32) {\n+    if (type.isIntOrIndex() && type.getIntOrFloatBitWidth() < 32) {\n       if (bUnsigned) {\n         newType = ui32_ty;\n         newOp = rewriter.create<LLVM::ZExtOp>(UnknownLoc::get(context), newType,\n@@ -2581,6 +2643,8 @@ class ElementwiseOpConversionBase\n     for (unsigned i = 0; i < elems; ++i) {\n       resultVals[i] = concreteThis->createDestOp(op, adaptor, rewriter, elemTy,\n                                                  operands[i], loc);\n+      if (!bool(resultVals[i]))\n+        return failure();\n     }\n     Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n     rewriter.replaceOp(op, view);\n@@ -3101,23 +3165,24 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n     }\n     unsigned linearIdxInNanoTile = i % srcAccumSizeInThreads;\n     auto multiDimIdxInNanoTile = getMultiDimIndex<unsigned>(\n-        linearIdxInNanoTile, srcBlockedLayout.getSizePerThread());\n+        linearIdxInNanoTile, srcBlockedLayout.getSizePerThread(), inOrd);\n     unsigned pos = multiDimIdxInNanoTile[inOrd[0]] % minVec;\n     multiDimIdxInNanoTile[inOrd[0]] /= minVec;\n     unsigned wordVecIdx =\n-        getLinearIndex<unsigned>(multiDimIdxInNanoTile, wordsInEachRep);\n+        getLinearIndex<unsigned>(multiDimIdxInNanoTile, wordsInEachRep, inOrd);\n     wordVecs[wordVecIdx] =\n         insert_element(wordTy, wordVecs[wordVecIdx], inVals[i], idx_val(pos));\n \n     if (i % srcAccumSizeInThreads == srcAccumSizeInThreads - 1) {\n       // end of replication, store the vectors into shared memory\n       unsigned linearRepIdx = i / srcAccumSizeInThreads;\n-      auto multiDimRepIdx = getMultiDimIndex<unsigned>(linearRepIdx, reps);\n+      auto multiDimRepIdx =\n+          getMultiDimIndex<unsigned>(linearRepIdx, reps, inOrd);\n       for (unsigned linearWordIdx = 0; linearWordIdx < numWordsEachRep;\n            ++linearWordIdx) {\n         // step 1: recover the multidim_index from the index of input_elements\n         auto multiDimWordIdx =\n-            getMultiDimIndex<unsigned>(linearWordIdx, wordsInEachRep);\n+            getMultiDimIndex<unsigned>(linearWordIdx, wordsInEachRep, inOrd);\n         SmallVector<Value> multiDimIdx(2);\n         auto wordOffset0 = multiDimRepIdx[0] * srcShapePerCTA[0] +\n                            multiDimWordIdx[0] * (inOrd[0] == 0 ? minVec : 1);\n@@ -3127,12 +3192,12 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n         multiDimIdx[1] = add(multiDimOffsetFirstElem[1], idx_val(wordOffset1));\n \n         // step 2: do swizzling\n-        Value remained = urem(multiDimIdx[inOrd[0]], outVecVal);\n-        multiDimIdx[inOrd[0]] = udiv(multiDimIdx[inOrd[0]], outVecVal);\n-        Value off_1 = mul(multiDimIdx[inOrd[1]], idx_val(srcShape[inOrd[0]]));\n-        Value phaseId = udiv(multiDimIdx[inOrd[1]], idx_val(perPhase));\n+        Value remained = urem(multiDimIdx[outOrd[0]], outVecVal);\n+        multiDimIdx[outOrd[0]] = udiv(multiDimIdx[outOrd[0]], outVecVal);\n+        Value off_1 = mul(multiDimIdx[outOrd[1]], idx_val(srcShape[outOrd[0]]));\n+        Value phaseId = udiv(multiDimIdx[outOrd[1]], idx_val(perPhase));\n         phaseId = urem(phaseId, idx_val(maxPhase));\n-        Value off_0 = xor_(multiDimIdx[inOrd[0]], phaseId);\n+        Value off_0 = xor_(multiDimIdx[outOrd[0]], phaseId);\n         off_0 = mul(off_0, outVecVal);\n         remained = udiv(remained, minVecVal);\n         off_0 = add(off_0, mul(remained, minVecVal));\n@@ -5836,6 +5901,32 @@ struct FDivOpConversion\n   }\n };\n \n+struct ExpOpConversionApprox\n+    : ElementwiseOpConversionBase<mlir::math::ExpOp, LLVM::InlineAsmOp,\n+                                  ExpOpConversionApprox> {\n+  using Base = ElementwiseOpConversionBase<mlir::math::ExpOp, LLVM::InlineAsmOp,\n+                                           ExpOpConversionApprox>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::math::ExpOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    // For FP64 input, call __nv_expf for higher-precision calculation\n+    if (elemTy.getIntOrFloatBitWidth() == 64)\n+      return {};\n+    const double log2e = 1.4426950408889634;\n+    Value prod =\n+        rewriter.create<LLVM::FMulOp>(loc, f32_ty, operands[0], f32_val(log2e));\n+    PTXBuilder ptxBuilder;\n+    auto &exp2 = ptxBuilder.create<PTXInstr>(\"ex2\")->o(\"approx\").o(\"f32\");\n+    auto output = ptxBuilder.newOperand(\"=f\");\n+    auto input = ptxBuilder.newOperand(prod, \"f\");\n+    exp2(output, input);\n+    return ptxBuilder.launch(rewriter, loc, f32_ty, false);\n+  }\n+};\n+\n /// ====================== atomic_rmw codegen begin ==========================\n struct AtomicRMWOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::AtomicRMWOp>,\n@@ -5996,6 +6087,13 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n \n   patterns.add<CmpIOpConversion>(typeConverter, benefit);\n   patterns.add<CmpFOpConversion>(typeConverter, benefit);\n+\n+  // ExpOpConversionApprox will try using ex2.approx if the input type is FP32.\n+  // For FP64 input type, ExpOpConversionApprox will return failure and\n+  // ElementwiseOpConversion<math::ExpOp, math::ExpOp> defined below will call\n+  // __nv_expf for higher-precision calculation\n+  patterns.add<ExpOpConversionApprox>(typeConverter, benefit);\n+\n #define POPULATE_UNARY_OP(SRC_OP, DST_OP)                                      \\\n   patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n "}, {"filename": "python/tests/test_elementwise.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -61,6 +61,7 @@ def get_tensor(shape, data_type, b_positive=False):\n                              ('sqrt', 'float64', 'float64'),\n                              ('abs', 'float32', 'float32'),\n                              ('exp', 'float32', 'float32'),\n+                             ('exp', 'float64', 'float64'),\n                              ('sigmoid', 'float32', 'float32'),\n                           ])\n def test_single_input(expr, output_type, input0_type):"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 80, "deletions": 39, "changes": 119, "file_content_changes": "@@ -30,18 +30,32 @@ def matmul_no_scf_kernel(\n # TODO: num_warps could only be 4 for now\n \n \n-@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS', [\n-    [128, 256, 32, 4],\n-    [256, 128, 16, 4],\n-    [128, 16, 32, 4],\n-    [32, 128, 64, 4],\n-    [128, 128, 64, 4],\n-    [64, 128, 128, 4],\n-    [64, 128, 128, 2],\n+@pytest.mark.parametrize('SHAPE,NUM_WARPS,TRANS_A,TRANS_B', [\n+    (shape, num_warps, trans_a, trans_b)\n+    for shape in [\n+        [128, 256, 32],\n+        [256, 128, 16],\n+        [128, 16, 32],\n+        [32, 128, 64],\n+        [128, 128, 64],\n+        [64, 128, 128],\n+    ]\n+    for num_warps in [2, 4]\n+    for trans_a in [False, True]\n+    for trans_b in [False, True]\n ])\n-def test_gemm_no_scf(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n-    a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n-    b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+def test_gemm_no_scf(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n+    SIZE_M, SIZE_N, SIZE_K = SHAPE\n+    if (TRANS_A):\n+        a = torch.randn((SIZE_K, SIZE_M), device='cuda', dtype=torch.float16).T\n+    else:\n+        a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n+\n+    if (TRANS_B):\n+        b = torch.randn((SIZE_N, SIZE_K), device='cuda', dtype=torch.float16).T\n+    else:\n+        b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+\n     c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n     grid = lambda META: (1, )\n     matmul_no_scf_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n@@ -55,16 +69,32 @@ def test_gemm_no_scf(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n     assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)\n \n \n-@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS', [\n-    [64, 128, 128, 1],\n-    [128, 128, 128, 4],\n-    [16, 8, 32, 1],\n-    [32, 16, 64, 2],\n-    [32, 16, 64, 4],\n+@pytest.mark.parametrize('SHAPE,NUM_WARPS,TRANS_A,TRANS_B', [\n+    (shape, num_warps, trans_a, trans_b)\n+    for shape in [\n+        [64, 128, 128],\n+        [128, 128, 128],\n+        [16, 8, 32],\n+        [32, 16, 64],\n+        [32, 16, 64],\n+    ]\n+    for num_warps in [1, 2, 4]\n+    for trans_a in [False, True]\n+    for trans_b in [False, True]\n ])\n-def test_gemm_no_scf_int8(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n-    a = torch.randint(-5, 5, (SIZE_M, SIZE_K), device='cuda', dtype=torch.int8)\n-    b = torch.randint(-5, 5, (SIZE_K, SIZE_N), device='cuda', dtype=torch.int8)\n+def test_gemm_no_scf_int8(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n+    SIZE_M, SIZE_N, SIZE_K = SHAPE\n+\n+    if (TRANS_A):\n+        a = torch.randint(-5, 5, (SIZE_K, SIZE_M), device='cuda', dtype=torch.int8).T\n+    else:\n+        a = torch.randint(-5, 5, (SIZE_M, SIZE_K), device='cuda', dtype=torch.int8)\n+\n+    if (TRANS_B):\n+        b = torch.randint(-5, 5, (SIZE_N, SIZE_K), device='cuda', dtype=torch.int8).T\n+    else:\n+        b = torch.randint(-5, 5, (SIZE_K, SIZE_N), device='cuda', dtype=torch.int8)\n+\n     c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.int32)\n \n     grid = lambda META: (1, )\n@@ -125,28 +155,39 @@ def get_variant_golden(a, b):\n     return c_padded[:SIZE_M, :SIZE_N]\n \n \n-@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K', [\n+@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,TRANS_A,TRANS_B', [\n     # Non-forloop\n-    [64, 32, 64, 4, 64, 32, 64],\n-    [128, 64, 128, 4, 128, 64, 128],\n+    [64, 32, 64, 4, 64, 32, 64, False, False],\n+    [128, 64, 128, 4, 128, 64, 128, False, False],\n     # K-Forloop\n-    [64, 32, 128, 4, 64, 32, 64],\n-    [128, 16, 128, 4, 128, 16, 32],\n-    [32, 16, 128, 4, 32, 16, 32],\n-    [32, 64, 128, 4, 32, 64, 32],\n-    [32, 128, 256, 4, 32, 128, 64],\n-    [64, 128, 64, 4, 64, 128, 32],\n-    [64, 64, 128, 4, 64, 64, 32],\n-    [128, 128, 64, 4, 128, 128, 32],\n-    [128, 128, 128, 4, 128, 128, 32],\n-    [128, 128, 256, 4, 128, 128, 64],\n-    [128, 256, 128, 4, 128, 256, 32],\n-    [256, 128, 64, 4, 256, 128, 16],\n-    [128, 64, 128, 4, 128, 64, 32],\n+    [64, 32, 128, 4, 64, 32, 64, False, False],\n+    [128, 16, 128, 4, 128, 16, 32, False, False],\n+    [32, 16, 128, 4, 32, 16, 32, False, False],\n+    [32, 64, 128, 4, 32, 64, 32, False, False],\n+    [32, 128, 256, 4, 32, 128, 64, False, False],\n+    [64, 128, 64, 4, 64, 128, 32, False, False],\n+    [64, 64, 128, 4, 64, 64, 32, False, False],\n+    [128, 128, 64, 4, 128, 128, 32, False, False],\n+    [128, 128, 128, 4, 128, 128, 32, False, False],\n+    [128, 128, 256, 4, 128, 128, 64, False, False],\n+    [128, 256, 128, 4, 128, 256, 32, False, False],\n+    [256, 128, 64, 4, 256, 128, 16, False, False],\n+    [128, 64, 128, 4, 128, 64, 32, False, False],\n+    # TODO[goostavz]: fix these cases\n+    #[128, 64, 128, 4, 128, 64, 32, True, False],\n+    #[128, 64, 128, 4, 128, 64, 32, False, True],\n ])\n-def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K):\n-    a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n-    b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B):\n+    if (TRANS_A):\n+        a = torch.randn((SIZE_K, SIZE_M), device='cuda', dtype=torch.float16).T\n+    else:\n+        a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n+\n+    if (TRANS_B):\n+        b = torch.randn((SIZE_N, SIZE_K), device='cuda', dtype=torch.float16).T\n+    else:\n+        b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+\n     c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n     grid = lambda META: (1, )\n     matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -9,6 +9,8 @@\n #A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n #B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n \n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n // CHECK-LABEL: matmul_loop\n func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n@@ -313,3 +315,5 @@ func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n   return\n   // CHECK-NEXT: size = 40960\n }\n+\n+}"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -9,6 +9,8 @@\n #A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n #B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n \n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n // CHECK-LABEL: matmul_loop\n // There shouldn't be any membar with the dot op encoding.\n func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n@@ -250,3 +252,5 @@ func @for_alias(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B :\n   %cst3 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n   return\n }\n+\n+}"}]