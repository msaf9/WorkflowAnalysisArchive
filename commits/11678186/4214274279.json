[{"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -28,9 +28,8 @@ void MembarAnalysis::resolve(Operation *operation, OpBuilder *builder) {\n         return;\n       }\n     }\n-    // Have to visit every block because the entry block\n-    // might not have any dangling read/write\n-    blockList.emplace_back(block);\n+    if (block->isEntryBlock())\n+      blockList.emplace_back(block);\n   });\n \n   // A fixed point algorithm\n@@ -48,9 +47,10 @@ void MembarAnalysis::resolve(Operation *operation, OpBuilder *builder) {\n       }\n     }\n     // Get the reference because we want to update if it changed\n-    if (inputBlockInfo == outputBlockInfoMap[block]) {\n-      // If the inputBlockInfo is the same as the outputBlockInfo, we\n-      // skip the successors\n+    if (outputBlockInfoMap.count(block) &&\n+        inputBlockInfo == outputBlockInfoMap[block]) {\n+      // If we have seen the block before and the inputBlockInfo is the same as\n+      // the outputBlockInfo, we skip the successors\n       continue;\n     }\n     // Update the current block"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -179,8 +179,8 @@ func.func @for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n   return\n }\n \n-// CHECK-LABEL: for_if_for\n-func.func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+// CHECK-LABEL: for_for_if\n+func.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: %cst -> %cst\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -315,8 +315,8 @@ func.func @for_use_ancestor(%lb : index, %ub : index, %step : index, %A : !tt.pt\n \n // a_shared_init, b_shared_init, and c_shared_init's liveness ranges are span over the entire function before cst2.\n // So they cannot be reused by cst0 and cst1, but can be reused by cst2.\n-// CHECK-LABEL: for_if_for\n-func.func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+// CHECK-LABEL: for_for_if\n+func.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: offset = 0, size = 8192\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 8192, size = 8192"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 78, "deletions": 0, "changes": 78, "file_content_changes": "@@ -232,6 +232,28 @@ func.func @multi_blocks_yield(%i1 : i1) {\n   return\n }\n \n+// Even though the entry block doesn't have a barrier, the successors should have barriers\n+// CHECK-LABEL: multi_blocks_entry_no_shared\n+func.func @multi_blocks_entry_no_shared(%i1 : i1) {\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n+  %a = scf.if %i1 -> (tensor<32x16xf16, #A_SHARED>) {\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+    %0 = tt.cat %cst1, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    scf.yield %0 : tensor<32x16xf16, #A_SHARED>\n+  } else {\n+    // CHECK-NOT: gpu.barrier\n+    // CHECK: arith.constant\n+    %cst1 = arith.constant dense<0.000000e+00> : tensor<32x16xf16, #A_SHARED>\n+    scf.yield %cst1 : tensor<32x16xf16, #A_SHARED>\n+  }\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %1 = tt.cat %a, %a {axis = 0} : (tensor<32x16xf16, #A_SHARED>, tensor<32x16xf16, #A_SHARED>) -> tensor<64x16xf16, #A_SHARED>\n+  return\n+}\n+\n // Conservatively add a barrier as if the branch (%i1) is never taken\n // CHECK-LABEL: multi_blocks_noelse\n func.func @multi_blocks_noelse(%i1 : i1) {\n@@ -362,6 +384,62 @@ func.func @for_reuse_nested(%lb : index, %ub : index, %step : index, %A : !tt.pt\n   return\n }\n \n+// repeatedly write to the same shared memory addresses\n+// CHECK-LABEL: for_for_if\n+func.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    %c_shared_next = scf.for %jv = %lb to %ub step %step iter_args(%c_shared_next = %c_shared) -> (tensor<128x32xf16, #A_SHARED>) {\n+      %c_shared_next_next = scf.if %i1 -> tensor<128x32xf16, #A_SHARED> {\n+        // CHECK: gpu.barrier\n+        // CHECK-NEXT: arith.constant\n+        %cst0 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+        scf.yield %cst0 : tensor<128x32xf16, #A_SHARED>\n+      } else {\n+        // CHECK: gpu.barrier\n+        // CHECK-NEXT: arith.constant\n+        %cst0 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+        scf.yield %cst0 : tensor<128x32xf16, #A_SHARED>\n+      }\n+      scf.yield %c_shared_next_next : tensor<128x32xf16, #A_SHARED>\n+    }\n+    scf.yield %a_shared, %b_shared, %c_shared_next : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n+  }\n+  return\n+}\n+\n+// c_block_next can either be converted from c_shared_init or c_shared_next_next\n+// CHECK-LABEL: for_if_for\n+func.func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  %c_blocked = triton_gpu.convert_layout %c_shared_init : (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #AL> \n+\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    %c_shared_next_next = scf.if %i1 -> tensor<128x32xf16, #A_SHARED> {\n+      // CHECK: gpu.barrier\n+      // CHECK-NEXT: arith.constant\n+      %cst0 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+      scf.yield %cst0 : tensor<128x32xf16, #A_SHARED>\n+    } else {\n+      %c_shared_ = scf.for %jv = %lb to %ub step %step iter_args(%c_shared_next = %c_shared) -> (tensor<128x32xf16, #A_SHARED>) {\n+        // CHECK: gpu.barrier\n+        // CHECK-NEXT: triton_gpu.convert_layout\n+        %c_blocked_next = triton_gpu.convert_layout %c_shared_next : (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #AL> \n+        scf.yield %c_shared : tensor<128x32xf16, #A_SHARED>\n+      }\n+      scf.yield %c_shared_ : tensor<128x32xf16, #A_SHARED>\n+    }\n+    // CHECK-NOT: gpu.barrier\n+    %b_blocked_next = triton_gpu.convert_layout %b_shared: (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #AL> \n+    scf.yield %a_shared, %b_shared, %c_shared_next_next : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n+  }\n+  return\n+}\n \n // CHECK-LABEL: cf_if\n func.func @cf_if(%i1 : i1) {"}]