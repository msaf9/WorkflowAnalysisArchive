[{"filename": ".github/CODEOWNERS", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -27,9 +27,9 @@ lib/Analysis/Utility.cpp @Jokeren\n # Dialects\n # ----------\n # Pipeline pass\n-lib/Dialect/TritonGPU/Transforms/Pipeline.cpp @daadaada\n+lib/Dialect/TritonGPU/Transforms/Pipeline.cpp @ptillet\n # Prefetch pass\n-lib/Dialect/TritonGPU/Transforms/Prefetch.cpp @daadaada\n+lib/Dialect/TritonGPU/Transforms/Prefetch.cpp @ptillet\n # Coalesce pass\n lib/Dialect/TritonGPU/Transforms/Coalesce.cpp @ptillet\n # Layout simplification pass\n@@ -42,8 +42,8 @@ lib/Dialect/TritonGPU/Transforms/Combine.cpp @ptillet\n include/triton/Conversion/TritonGPUToLLVM/ @goostavz @Superjomn\n lib/Conversions/TritonGPUToLLVM @goostavz @Superjomn\n # TritonToTritonGPU\n-include/triton/Conversion/TritonToTritonGPU/ @daadaada\n-lib/Dialect/TritonGPU/Transforms/TritonGPUConversion.cpp @daadaada\n+include/triton/Conversion/TritonToTritonGPU/ @ptillet\n+lib/Dialect/TritonGPU/Transforms/TritonGPUConversion.cpp @ptillet\n \n \n # -------"}, {"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 45, "deletions": 32, "changes": 77, "file_content_changes": "@@ -110,10 +110,11 @@ jobs:\n       - name: Regression tests\n         if: ${{ contains(matrix.runner, 'A100') }}\n         run: |\n+          python3 -m pip install pytest-rerunfailures\n           cd python/test/regression\n           sudo nvidia-smi -i 0 -pm 1\n-          sudo nvidia-smi -i 0 --lock-gpu-clocks=1350,1350\n-          python3 -m pytest -vs .\n+          sudo nvidia-smi -i 0 --lock-gpu-clocks=1280,1280\n+          python3 -m pytest -vs . --reruns 10\n           sudo nvidia-smi -i 0 -rgc\n \n   Integration-Tests-Third-Party:\n@@ -224,40 +225,52 @@ jobs:\n           GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n         run: |\n           OWNER_REPO=\"${{ github.repository }}\"\n-          PR_NUMBER=$(gh api --method GET repos/$OWNER_REPO/pulls -f state=closed | jq \".[] | select(.merged_at != null) | .number\" | head -1)\n-          echo \"Last merged PR number: $PR_NUMBER\"\n-\n-          BRANCH_NAME=$(gh api repos/$OWNER_REPO/pulls/$PR_NUMBER --jq '.head.ref')\n-          echo \"BRANCH_NAME: $BRANCH_NAME\"\n-\n-          page=1\n-          while true; do\n-            run_id=$(gh api --method GET \"repos/$OWNER_REPO/actions/runs?page=$page&per_page=100\" | jq --arg branch_name \"$BRANCH_NAME\" '.workflow_runs[] | select(.head_branch == $branch_name)' | jq '.id' | head -1)\n-            if [ \"$run_id\" != \"\" ]; then\n-              echo \"First run ID on branch $BRANCH_NAME is: $run_id\"\n-              WORKFLOW_RUN_ID=$run_id\n+          echo \"OWNER_REPO: $OWNER_REPO\"\n+          PR_NUMBERS=($(gh api --method GET repos/$OWNER_REPO/pulls -f state=closed | jq -r \".[] | select(.merged_at != null) | .number\"))\n+\n+          # Not all PRs go through integration tests\n+          success=0\n+          for PR_NUMBER in \"${PR_NUMBERS[@]}\"\n+          do\n+            echo \"Last merged PR number: $PR_NUMBER\"\n+            BRANCH_NAME=$(gh api repos/$OWNER_REPO/pulls/$PR_NUMBER --jq '.head.ref')\n+            echo \"BRANCH_NAME: $BRANCH_NAME\"\n+            USER_ID=$(gh api repos/$OWNER_REPO/pulls/$PR_NUMBER --jq '.user.id')\n+            echo \"USER_ID: $USER_ID\"\n+\n+            page=1\n+            while true; do\n+              run_id=$(gh api --method GET \"repos/$OWNER_REPO/actions/runs?page=$page&per_page=100\" | jq --arg branch_name \"$BRANCH_NAME\" --arg run_name \"Integration Tests\" --arg user_id \"$USER_ID\" '.workflow_runs[] | select(.head_branch == $branch_name and .name == $run_name and .actor.id == ($user_id | tonumber))' | jq '.id' | head -1)\n+              if [ \"$run_id\" != \"\" ]; then\n+                echo \"First run ID on branch $BRANCH_NAME is: $run_id\"\n+                WORKFLOW_RUN_ID=$run_id\n+                break\n+              fi\n+\n+              ((page++))\n+            done\n+\n+            echo \"WORKFLOW_RUN_ID: $WORKFLOW_RUN_ID\"\n+            ARTIFACT_URL=$(gh api repos/$OWNER_REPO/actions/runs/$WORKFLOW_RUN_ID/artifacts | jq --arg artifact_name \"$ARTIFACT_NAME\" '.artifacts[] | select(.name == $artifact_name).archive_download_url' --raw-output)\n+            echo \"ARTIFACT_URL: $ARTIFACT_URL\"\n+\n+            if [ -n \"$ARTIFACT_URL\" ]; then\n+              echo \"Downloading artifact: $ARTIFACT_URL\"\n+              curl --location --remote-header-name -H \"Authorization: token $GH_TOKEN\" -o reference.zip \"$ARTIFACT_URL\"\n+              # Print the size of the downloaded artifact\n+              echo \"Artifact size (stat): $(stat --printf=\"%s bytes\" reference.zip)\"\n+              echo \"Artifact size (du): $(du -sh reference.zip)\"\n+              unzip reference.zip\n+              tar -xzf artifacts.tar.gz\n+              rm reference.zip\n+              rm artifacts.tar.gz\n+              mv cache reference\n+              success=1\n               break\n             fi\n-\n-            ((page++))\n           done\n \n-          echo \"WORKFLOW_RUN_ID: $WORKFLOW_RUN_ID\"\n-          ARTIFACT_URL=$(gh api repos/$OWNER_REPO/actions/runs/$WORKFLOW_RUN_ID/artifacts | jq --arg artifact_name \"$ARTIFACT_NAME\" '.artifacts[] | select(.name == $artifact_name).archive_download_url' --raw-output)\n-          echo \"ARTIFACT_URL: $ARTIFACT_URL\"\n-\n-          if [ -n \"$ARTIFACT_URL\" ]; then\n-            echo \"Downloading artifact: $ARTIFACT_URL\"\n-            curl --location --remote-header-name -H \"Authorization: token $GH_TOKEN\" -o reference.zip \"$ARTIFACT_URL\"\n-            # Print the size of the downloaded artifact\n-            echo \"Artifact size (stat): $(stat --printf=\"%s bytes\" reference.zip)\"\n-            echo \"Artifact size (du): $(du -sh reference.zip)\"\n-            unzip reference.zip\n-            tar -xzf artifacts.tar.gz\n-            rm reference.zip\n-            rm artifacts.tar.gz\n-            mv cache reference\n-          else\n+          if [ $success -eq 0 ]; then\n             echo \"No artifact found with the name: $ARTIFACT_NAME\"\n             exit 1\n           fi"}, {"filename": "CONTRIBUTING.md", "status": "modified", "additions": 69, "deletions": 0, "changes": 69, "file_content_changes": "@@ -51,3 +51,72 @@ A well-structured RFC should include:\n ## New backends\n \n Due to limited resources, we need to prioritize the number of targets we support. We are committed to providing upstream support for Nvidia and AMD GPUs. However, if you wish to contribute support for other backends, please start your project in a fork. If your backend proves to be useful and meets our performance requirements, we will discuss the possibility of upstreaming it.\n+\n+\n+## Project Structure\n+```\n+triton\n+\u251c\u2500\u2500 lib : C++ code for python library\n+\u2502   \u251c\u2500\u2500Analysis\n+\u2502   \u2502\tMemory barrier analysis\n+\u2502   \u2502\tclass to extract axis information from MLIR ops\n+\u2502   \u2502\timplementation of the shared memory allocation analysis for Triton dialect\n+\u2502   \u2502\n+\u2502   \u251c\u2500\u2500Conversion\n+\u2502   \u2502\t\u251c\u2500\u2500TritonGPUToLLVM:  Transforms TritonGPU  to LLVM;\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u251c\u2500\u2500TritonToTritonGPU: Transforms ops to TritonGPU ops; loading, storing, arithmetic, casting, and tensor operations.\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u2502\n+\u2502   \u251c\u2500\u2500Dialect\n+\u2502   \u2502\t\u251c\u2500\u2500Triton\n+\u2502   \u2502\t\u2502\tDefines core IR for Triton compiler\n+\u2502   \u2502\t\u251c\u2500\u2500TritonGPU\n+\u2502   \u2502\t    Defines TritonGPU operation for IR\n+\u2502   \u2502\n+\u2502   \u251c\u2500\u2500Target: contains Triton targets for converting to PTX, LLVMIR and HSACO IR targets\n+\u2502   \u2502\n+\u251c\u2500\u2500 bin\n+\u251c\u2500\u2500 cmake\n+\u251c\u2500\u2500 docs \u251c\u2500\u2500 Documentation regarding using triton\n+\u251c\u2500\u2500 include\n+\u2502   CMakelists.txt\n+\u2502   \u251c\u2500\u2500triton\n+\u2502   \u2502   \u251c\u2500\u2500\n+\u251c\u2500\u2500 python\n+\u2502   \u251c\u2500\u2500\n+\u2502   \u251c\u2500\u2500 MANIFEST.in\n+\u2502   \u251c\u2500\u2500 README.md\n+\u2502   \u251c\u2500\u2500 build\n+\u2502   \u251c\u2500\u2500 examples\n+\u2502   \u251c\u2500\u2500 pyproject.toml\n+\u2502   \u251c\u2500\u2500 setup.py: pip install for python package\n+\u2502   \u251c\u2500\u2500 src\n+\u2502   \u251c\u2500\u2500 test\n+\u2502   \u251c\u2500\u2500 triton\n+\u2502   \u2502\t\u251c\u2500\u2500 _C: Includes header files and compiled .so file for C library\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u251c\u2500\u2500common: Has interface for CUDA hardware backend\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u251c\u2500\u2500compiler: contains code for compiling source code to IR and lauching GPU kernels\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u251c\u2500\u2500interpreter: memory-map for tensors, converting primitives to tensors, and arethmetic ops for tensors\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u251c\u2500\u2500language: core of triton language, load tensors to SRAM, language logic, etc.\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u251c\u2500\u2500ops: contains functions for flash-attn, softmax, cross-entropy and other torch.nn.F functions\n+\u2502   \u2502\t\u251c\u2500\u2500runtime: contains impl jit compilation, autotuning, backend drivers,cahcing, error handles, etc.\n+\u2502   \u2502\t\u251c\u2500\u2500third_party\n+\u2502   \u2502\t\u251c\u2500\u2500tools\n+\u2502   \u251c\u2500\u2500 triton.egg-info\n+\u2502   \u251c\u2500\u2500 tutorials: contains tutorials for various use-cases\n+\u251c\u2500\u2500 test\n+\u2502   \u251c\u2500\u2500Analysis\n+\u2502   \u251c\u2500\u2500Conversion\n+\u2502   \u251c\u2500\u2500Dialect\n+\u2502   \u251c\u2500\u2500Target\n+\u251c\u2500\u2500 third_party\n+\u251c\u2500\u2500 unittest\n+\u2514\u2500\u2500 utils\n+```"}, {"filename": "README.md", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -42,6 +42,8 @@ pip install cmake; # build-time dependency\n pip install -e .\n ```\n \n+\n+\n # Changelog\n \n Version 2.0 is out! New features include:\n@@ -56,6 +58,9 @@ Community contributions are more than welcome, whether it be to fix bugs or to a\n \n If you\u2019re interested in joining our team and working on Triton & GPU kernels, [we\u2019re hiring](https://openai.com/jobs/#acceleration)!\n \n+\n+\n+\n # Compatibility\n \n Supported Platforms:"}, {"filename": "docs/meetups/07-18-2023.md", "status": "renamed", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -4,8 +4,9 @@\n 3. Mechanisms for smaller technical discussions: Slack channel per topic? Dedicated meetings for some topics?\n 4. Stability, testing, regressions: Improving CI and conformance/testing for validating new back-ends.\n 5. Language improvements/pain points\n-6. Discussion of known/anticipated design changes for H100\n-7. Some specific more tactical areas:\n+6. Windows Support\n+7. Discussion of known/anticipated design changes for H100\n+8. Some specific more tactical areas:\n    - int8.\n    - A low hanging fruit is to let tl.dot take int8 and leverage mma.\n    - Sm75."}, {"filename": "include/triton/Dialect/Triton/IR/Dialect.h", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -45,6 +45,12 @@ class DialectInferLayoutInterface\n   inferDotOpEncoding(Attribute operandEncoding, unsigned opIdx,\n                      Attribute retEncoding,\n                      std::optional<Location> location) const = 0;\n+\n+  // Verify that the encoding are compatible to be used together in a dot\n+  // operation\n+  virtual LogicalResult\n+  verifyDotOpEncodingCompatibility(Operation *op, Attribute operandEncodingA,\n+                                   Attribute operandEncodingB) const = 0;\n };\n \n } // namespace triton"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 10, "deletions": 4, "changes": 14, "file_content_changes": "@@ -31,7 +31,8 @@ class TT_Op<string mnemonic, list<Trait> traits = []> :\n //   fptoui, fptosi, uitofp, sitofp,\n //   extf, tructf,\n //   extui, extsi, tructi\n-def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [SameOperandsAndResultShape,\n+def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [Elementwise,\n+                                         SameOperandsAndResultShape,\n                                          SameOperandsAndResultEncoding,\n                                          Pure,\n                                          /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n@@ -44,7 +45,8 @@ def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [SameOperandsAndResultShape,\n     let assemblyFormat = \"$from attr-dict `:` type($from) `->` type($result)\";\n }\n \n-def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape,\n+def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [Elementwise,\n+                                         SameOperandsAndResultShape,\n                                          SameOperandsAndResultEncoding,\n                                          Pure,\n                                          /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n@@ -58,7 +60,8 @@ def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape,\n }\n \n // arith.bitcast doesn't support pointers\n-def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape,\n+def TT_BitcastOp : TT_Op<\"bitcast\", [Elementwise,\n+                                     SameOperandsAndResultShape,\n                                      SameOperandsAndResultEncoding,\n                                      Pure,\n                                      /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n@@ -73,6 +76,7 @@ def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape,\n     // TODO: Add verifier\n }\n \n+// FIXME: Not elementwise because scalars are not supported\n def TT_FpToFpOp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n                                      SameOperandsAndResultEncoding,\n                                      Pure,\n@@ -99,6 +103,7 @@ def TT_FpToFpOp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n //\n def TT_AddPtrOp : TT_Op<\"addptr\",\n                      [Pure,\n+                      Elementwise,\n                       SameOperandsAndResultShape,\n                       SameOperandsAndResultEncoding,\n                       TypesMatchWith<\"result type matches ptr type\",\n@@ -458,7 +463,8 @@ def TT_ScanReturnOp: TT_Op<\"scan.return\",\n //\n class TT_ExternElementwiseOpBase<string mnemonic, list<Trait> traits = []> :\n     TT_Op<mnemonic,\n-         traits # [SameOperandsAndResultEncoding,\n+         traits # [Elementwise,\n+                   SameOperandsAndResultEncoding,\n                    SameVariadicOperandSize]> {\n \n     let description = [{"}, {"filename": "include/triton/Dialect/Triton/Transforms/Passes.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -8,6 +8,8 @@ namespace triton {\n \n std::unique_ptr<Pass> createCombineOpsPass();\n \n+std::unique_ptr<Pass> createReorderBroadcastPass();\n+\n std::unique_ptr<Pass>\n createRewriteTensorPointerPass(int computeCapability = 80);\n "}, {"filename": "include/triton/Dialect/Triton/Transforms/Passes.td", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -19,6 +19,15 @@ def TritonCombineOps : Pass</*cli-arg*/\"triton-combine\", /*Op*/\"mlir::ModuleOp\">\n   let dependentDialects = [\"mlir::arith::ArithDialect\"];\n }\n \n+def TritonReorderBroadcast : Pass</*cli-arg*/\"triton-reorder-broadcast\", /*Op*/\"mlir::ModuleOp\"> {\n+  let summary = \"Moves broadcast and splat after elementwise operations\";\n+  let description = [{\n+    elementwise(splat(a), splat(b), ...) => splat(elementwise(a, b, ...))\n+  }];\n+  let constructor = \"mlir::triton::createReorderBroadcastPass()\";\n+  let dependentDialects = [\"mlir::triton::TritonDialect\"];\n+}\n+\n def TritonRewriteTensorPointer : Pass</*cli-arg*/\"triton-rewrite-tensor-pointer\", /*Op*/\"mlir::ModuleOp\"> {\n   let summary = \"Rewrite load/stores with tensor pointers into legacy load/stores\";\n   let description = [{"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -49,7 +49,8 @@ SmallVector<unsigned> getContigPerThread(Attribute layout);\n // for thread 0 would be [A_{0, 0}, A_{0, 0}, A_{0, 0}, A_{0, 0}], returns [1,\n // 1]. Whereas for a tensor shape [128, 128], the elements for thread 0 would be\n // [A_{0, 0}, A_{0, 1}, A_{0, 2}, A_{0, 3}], returns [1, 4].\n-SmallVector<unsigned> getUniqueContigPerThread(Type type);\n+SmallVector<unsigned> getUniqueContigPerThread(Attribute layout,\n+                                               ArrayRef<int64_t> tensorShape);\n \n // Returns the number of threads per warp that have access to non-replicated\n // elements of the tensor. E.g. for a blocked layout with sizePerThread = [1,"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 30, "deletions": 10, "changes": 40, "file_content_changes": "@@ -162,16 +162,22 @@ class AllocationAnalysis {\n     }\n   }\n \n+  template <BufferT::BufferKind T>\n+  void maybeAddScratchBuffer(Operation *op, unsigned bytes) {\n+    if (bytes > 0)\n+      allocation->addBuffer<T>(op, bytes);\n+  }\n+\n   /// Initializes temporary shared memory for a given operation.\n   void getScratchValueSize(Operation *op) {\n     if (auto reduceOp = dyn_cast<triton::ReduceOp>(op)) {\n       ReduceOpHelper helper(reduceOp);\n       unsigned bytes = helper.getScratchSizeInBytes();\n-      allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+      maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     } else if (auto scanOp = dyn_cast<triton::ScanOp>(op)) {\n       ScanLoweringHelper helper(scanOp);\n       unsigned bytes = helper.getScratchSizeInBytes();\n-      allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+      maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     } else if (auto cvtLayout = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n       auto srcTy = cvtLayout.getSrc().getType().cast<RankedTensorType>();\n       auto dstTy = cvtLayout.getResult().getType().cast<RankedTensorType>();\n@@ -195,7 +201,7 @@ class AllocationAnalysis {\n           srcTy.getElementType().isa<triton::PointerType>()\n               ? elems * kPtrBitWidth / 8\n               : elems * std::max<int>(8, srcTy.getElementTypeBitWidth()) / 8;\n-      allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+      maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     } else if (auto atomicRMWOp = dyn_cast<triton::AtomicRMWOp>(op)) {\n       auto value = op->getOperand(0);\n       // only scalar requires scratch memory\n@@ -212,7 +218,7 @@ class AllocationAnalysis {\n             elemTy.isa<triton::PointerType>()\n                 ? elems * kPtrBitWidth / 8\n                 : elems * std::max<int>(8, elemTy.getIntOrFloatBitWidth()) / 8;\n-        allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+        maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n       }\n     } else if (auto atomicCASOp = dyn_cast<triton::AtomicCASOp>(op)) {\n       auto value = op->getOperand(0);\n@@ -224,13 +230,13 @@ class AllocationAnalysis {\n       auto bytes = elemTy.isa<triton::PointerType>()\n                        ? elems * kPtrBitWidth / 8\n                        : elems * elemTy.getIntOrFloatBitWidth() / 8;\n-      allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+      maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     } else if (auto callOp = dyn_cast<CallOpInterface>(op)) {\n       auto callable = callOp.resolveCallable();\n       auto funcOp = dyn_cast<FunctionOpInterface>(callable);\n       auto *funcAlloc = &(*funcAllocMap)[funcOp];\n       auto bytes = funcAlloc->getSharedMemorySize();\n-      allocation->addBuffer<BufferT::BufferKind::Virtual>(op, bytes);\n+      maybeAddScratchBuffer<BufferT::BufferKind::Virtual>(op, bytes);\n     }\n   }\n \n@@ -382,10 +388,19 @@ class AllocationAnalysis {\n     DenseMap<BufferT *, size_t> bufferStart;\n     calculateStarts(buffers, bufferStart);\n \n+    // NOTE: The original paper doesn't consider interference between\n+    // the bumped ranges. Buffers that previously do not interfere with\n+    // could interfere after offset bumping if their liveness ranges overlap.\n+    // Therefore, we rerun the interference graph algorithm after bumping so\n+    // that we regroup the buffers and color them again. Since we always\n+    // increase the buffer offset and keep reducing conflicts, we will\n+    // eventually reach a fixed point.\n     GraphT interference;\n     buildInterferenceGraph(buffers, bufferStart, interference);\n-\n-    allocate(buffers, bufferStart, interference);\n+    do {\n+      allocate(buffers, interference, bufferStart);\n+      buildInterferenceGraph(buffers, bufferStart, interference);\n+    } while (!interference.empty());\n   }\n \n   /// Computes the initial shared memory offsets.\n@@ -451,6 +466,8 @@ class AllocationAnalysis {\n   void buildInterferenceGraph(const SmallVector<BufferT *> &buffers,\n                               const DenseMap<BufferT *, size_t> &bufferStart,\n                               GraphT &interference) {\n+    // Reset interference graph\n+    interference.clear();\n     for (auto x : buffers) {\n       for (auto y : buffers) {\n         if (x == y)\n@@ -473,8 +490,10 @@ class AllocationAnalysis {\n \n   /// Finalizes shared memory offsets considering interference.\n   void allocate(const SmallVector<BufferT *> &buffers,\n-                const DenseMap<BufferT *, size_t> &bufferStart,\n-                const GraphT &interference) {\n+                const GraphT &interference,\n+                DenseMap<BufferT *, size_t> &bufferStart) {\n+    // Reset shared memory size\n+    allocation->sharedMemorySize = 0;\n     // First-fit graph coloring\n     // Neighbors are nodes that interfere with each other.\n     // We color a node by finding the index of the first available\n@@ -508,6 +527,7 @@ class AllocationAnalysis {\n         adj = std::max(adj, bufferStart.lookup(y) + y->size);\n       }\n       x->offset = bufferStart.lookup(x) + colors.lookup(x) * adj;\n+      bufferStart[x] = x->offset;\n       allocation->sharedMemorySize =\n           std::max(allocation->sharedMemorySize, x->offset + x->size);\n     }"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -918,7 +918,8 @@ unsigned ModuleAxisInfoAnalysis::getPtrContiguity(Value ptr) {\n   auto order = triton::gpu::getOrder(layout);\n   unsigned align = getPtrAlignment(ptr);\n \n-  auto uniqueContigPerThread = triton::gpu::getUniqueContigPerThread(tensorTy);\n+  auto uniqueContigPerThread =\n+      triton::gpu::getUniqueContigPerThread(layout, tensorTy.getShape());\n   assert(order[0] < uniqueContigPerThread.size() &&\n          \"Unxpected uniqueContigPerThread size\");\n   unsigned contiguity = uniqueContigPerThread[order[0]];"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 7, "deletions": 4, "changes": 11, "file_content_changes": "@@ -60,7 +60,9 @@ unsigned ReduceOpHelper::getInterWarpSizeWithUniqueData() {\n \n unsigned ReduceOpHelper::getIntraWarpSizeWithUniqueData() {\n   auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n-  return std::min(srcReduceDimSize,\n+  unsigned elementPerThreads = triton::gpu::getUniqueContigPerThread(\n+      getSrcLayout(), getSrcShape())[axis];\n+  return std::min(srcReduceDimSize / elementPerThreads,\n                   triton::gpu::getThreadsPerWarpWithUniqueData(\n                       getSrcLayout(), getSrcShape())[axis]);\n }\n@@ -84,9 +86,10 @@ SmallVector<SmallVector<unsigned>> ReduceOpHelper::getScratchConfigsFast() {\n \n   auto argLayout = getSrcLayout();\n   auto argLayoutMma = argLayout.dyn_cast<triton::gpu::MmaEncodingAttr>();\n-  // if (argLayoutMma && argLayoutMma.getVersionMajor() == 2 &&\n-  //     triton::gpu::getWarpsPerCTA(argLayout)[axis] == 1)\n-  //   return {{1, 1}, {1, 1}};\n+\n+  // that case doesn't need inter-warp communication\n+  if (isFastReduction() && triton::gpu::getWarpsPerCTA(argLayout)[axis] == 1)\n+    return {{0, 0}, {0, 0}};\n \n   /// shared memory block0\n   smemShapes[0] = convertType<unsigned>(getSrcShape());"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 323, "deletions": 272, "changes": 595, "file_content_changes": "@@ -225,6 +225,20 @@ const std::string Bf16_to_Fp8E4M3 =\n     \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n     \"}\";\n \n+/* ----- Packed integer to BF16 ------ */\n+const std::string S8_to_Bf16 =\n+    \"{                                           \\n\"\n+    \".reg .s8 s<4>;                              \\n\"\n+    \".reg .f32 f<4>;                             \\n\"\n+    \"mov.b32 {s0, s1, s2, s3}, $2;               \\n\" // unpack\n+    \"cvt.rn.f32.s8 f0, s0;                       \\n\" // no s8->bf16 pre-Hopper\n+    \"cvt.rn.f32.s8 f1, s1;                       \\n\" // fi[0:15] is always 0\n+    \"cvt.rn.f32.s8 f2, s2;                       \\n\" //\n+    \"cvt.rn.f32.s8 f3, s3;                       \\n\" //\n+    \"prmt.b32 $0, f0, f1, 0x7632;                \\n\" // f32->bf16 + pack\n+    \"prmt.b32 $1, f2, f3, 0x7632;                \\n\" //\n+    \"}\";\n+\n static SmallVector<Value> reorderValues(const SmallVector<Value> &values,\n                                         Type inType, Type ouType) {\n   auto inTensorTy = inType.dyn_cast<RankedTensorType>();\n@@ -350,71 +364,174 @@ inline SmallVector<Value> packI32(const SmallVector<Value> &inValues,\n   return outValues;\n }\n \n-struct FpToFpOpConversion\n-    : public ConvertTritonGPUOpToLLVMPattern<triton::FpToFpOp> {\n-  using ConvertTritonGPUOpToLLVMPattern<\n-      triton::FpToFpOp>::ConvertTritonGPUOpToLLVMPattern;\n-\n-  typedef std::function<SmallVector<Value>(\n-      Location, ConversionPatternRewriter &, const Value &, const Value &,\n-      const Value &, const Value &)>\n-      ConvertorT;\n-  /* ------------------ */\n-  // FP8 -> FP16\n-  /* ------------------ */\n-\n-  static ConvertorT makeConverterFromPtx(const std::string &ptxAsm, Type inType,\n-                                         Type outType) {\n-\n-    ConvertorT converter =\n-        [ptxAsm, inType,\n-         outType](Location loc, ConversionPatternRewriter &rewriter,\n-                  const Value &v0, const Value &v1, const Value &v2,\n-                  const Value &v3) -> SmallVector<Value> {\n-      SmallVector<Value> v = {v0, v1, v2, v3};\n-      auto ctx = rewriter.getContext();\n-      int inBitwidth = inType.getIntOrFloatBitWidth();\n-      int outBitwidth = outType.getIntOrFloatBitWidth();\n-      // first, we pack `v` into 32-bit ints\n-      int inVecWidth = 32 / inBitwidth;\n-      auto inVecTy = vec_ty(inType, inVecWidth);\n-      SmallVector<Value> inPacked(4 / inVecWidth, undef(inVecTy));\n-      for (size_t i = 0; i < 4; i++)\n-        inPacked[i / inVecWidth] = insert_element(\n-            inVecTy, inPacked[i / inVecWidth], v[i], i32_val(i % inVecWidth));\n-      for (size_t i = 0; i < inPacked.size(); i++)\n-        inPacked[i] = bitcast(inPacked[i], i32_ty);\n-\n-      // then, we run the provided inline PTX\n-      int outVecWidth = 32 / outBitwidth;\n-      int outNums = 4 / outVecWidth;\n-      PTXBuilder builder;\n-      SmallVector<PTXBuilder::Operand *> operands;\n+typedef std::function<SmallVector<Value>(Location, ConversionPatternRewriter &,\n+                                         const Value &, const Value &,\n+                                         const Value &, const Value &)>\n+    ConverterT;\n+\n+static ConverterT makeConverterFromPtx(const std::string &ptxAsm, Type inType,\n+                                       Type outType) {\n+\n+  ConverterT converter = [ptxAsm, inType, outType](\n+                             Location loc, ConversionPatternRewriter &rewriter,\n+                             const Value &v0, const Value &v1, const Value &v2,\n+                             const Value &v3) -> SmallVector<Value> {\n+    SmallVector<Value> v = {v0, v1, v2, v3};\n+    auto ctx = rewriter.getContext();\n+    int inBitwidth = inType.getIntOrFloatBitWidth();\n+    int outBitwidth = outType.getIntOrFloatBitWidth();\n+    // first, we pack `v` into 32-bit ints\n+    int inVecWidth = 32 / inBitwidth;\n+    auto inVecTy = vec_ty(inType, inVecWidth);\n+    SmallVector<Value> inPacked(4 / inVecWidth, undef(inVecTy));\n+    for (size_t i = 0; i < 4; i++)\n+      inPacked[i / inVecWidth] = insert_element(\n+          inVecTy, inPacked[i / inVecWidth], v[i], i32_val(i % inVecWidth));\n+    for (size_t i = 0; i < inPacked.size(); i++)\n+      inPacked[i] = bitcast(inPacked[i], i32_ty);\n+\n+    // then, we run the provided inline PTX\n+    int outVecWidth = 32 / outBitwidth;\n+    int outNums = 4 / outVecWidth;\n+    PTXBuilder builder;\n+    SmallVector<PTXBuilder::Operand *> operands;\n+    for (int i = 0; i < outNums; i++)\n+      operands.push_back(builder.newOperand(\"=r\"));\n+    for (Value inVal : inPacked)\n+      operands.push_back(builder.newOperand(inVal, \"r\"));\n+    auto &ptxOp = *builder.create(ptxAsm);\n+    ptxOp(operands, /*onlyAttachMLIRArgs=*/true);\n+    auto outVecTy = vec_ty(outType, outVecWidth);\n+    SmallVector<Value> outPacked;\n+    if (outNums == 1)\n+      outPacked.push_back(builder.launch(rewriter, loc, outVecTy, false));\n+    else {\n+      auto outStructTy = struct_ty(SmallVector<Type>(outNums, outVecTy));\n+      auto outStruct = builder.launch(rewriter, loc, outStructTy, false);\n       for (int i = 0; i < outNums; i++)\n-        operands.push_back(builder.newOperand(\"=r\"));\n-      for (Value inVal : inPacked)\n-        operands.push_back(builder.newOperand(inVal, \"r\"));\n-      auto &ptxOp = *builder.create(ptxAsm);\n-      ptxOp(operands, /*onlyAttachMLIRArgs=*/true);\n-      auto outVecTy = vec_ty(outType, outVecWidth);\n-      SmallVector<Value> outPacked;\n-      if (outNums == 1)\n-        outPacked.push_back(builder.launch(rewriter, loc, outVecTy, false));\n-      else {\n-        auto outStructTy = struct_ty(SmallVector<Type>(outNums, outVecTy));\n-        auto outStruct = builder.launch(rewriter, loc, outStructTy, false);\n-        for (int i = 0; i < outNums; i++)\n-          outPacked.push_back(extract_val(outVecTy, outStruct, i));\n+        outPacked.push_back(extract_val(outVecTy, outStruct, i));\n+    }\n+    // unpack the output\n+    SmallVector<Value> ret;\n+    for (size_t i = 0; i < 4; i++)\n+      ret.push_back(extract_element(outType, outPacked[i / outVecWidth],\n+                                    i32_val(i % outVecWidth)));\n+    return ret;\n+  };\n+  return converter;\n+}\n+\n+class MultipleOperandsRange\n+    : public iterator_range<SmallVector<SmallVector<Value>>::iterator> {\n+  using ContainerT = SmallVector<SmallVector<Value>>;\n+\n+public:\n+  using iterator_range<ContainerT::iterator>::iterator_range;\n+  ContainerT::reference operator[](ContainerT::size_type idx) {\n+    return begin()[idx];\n+  }\n+  ContainerT::const_reference operator[](ContainerT::size_type idx) const {\n+    return begin()[idx];\n+  }\n+  ContainerT::size_type size() const { return end() - begin(); }\n+};\n+\n+// Base pattern for elementwise conversion using ConcreteT. Unpacks individual\n+// elements from a `!llvm.struct` via `llvm.extactvalue`, calls\n+// ConcreteT::createDestOps on each element, and packs them back into an\n+// `!llvm.struct` using `llvm.insertvalue`.\n+//\n+// Also supports processing the inputs in a vectorized form by consuming and\n+// producing multiple operand sets in ConcreteT::createDestOps.\n+template <typename SourceOp, typename ConcreteT>\n+class ElementwiseOpConversionBase\n+    : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n+public:\n+  using OpAdaptor = typename SourceOp::Adaptor;\n+\n+  explicit ElementwiseOpConversionBase(\n+      TritonGPUToLLVMTypeConverter &typeConverter, PatternBenefit benefit = 1)\n+      : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n+\n+  LogicalResult\n+  matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto resultTy = op.getType();\n+    Location loc = op->getLoc();\n+    // element type\n+    auto resultElementTy = getElementTypeOrSelf(resultTy);\n+    Type elemTy = this->getTypeConverter()->convertType(resultElementTy);\n+    SmallVector<SmallVector<Value>> allOperands;\n+    for (auto operand : adaptor.getOperands()) {\n+      auto argTy = op->getOperand(0).getType();\n+      auto subOperands = this->getTypeConverter()->unpackLLElements(\n+          loc, operand, rewriter, argTy);\n+      subOperands = unpackI32(subOperands, argTy, rewriter, loc,\n+                              this->getTypeConverter());\n+      allOperands.resize(subOperands.size());\n+      for (auto v : llvm::enumerate(subOperands))\n+        allOperands[v.index()].push_back(v.value());\n+    }\n+    if (allOperands.size() == 0)\n+      allOperands.push_back({});\n+\n+    SmallVector<Value> resultVals;\n+    for (auto it = allOperands.begin(), end = allOperands.end(); it != end;) {\n+      auto curr = static_cast<const ConcreteT *>(this)->createDestOps(\n+          op, adaptor, rewriter, elemTy, MultipleOperandsRange(it, end), loc);\n+      if (curr.size() == 0)\n+        return failure();\n+      for (auto v : curr) {\n+        if (!static_cast<bool>(v))\n+          return failure();\n+        resultVals.push_back(v);\n       }\n-      // unpack the output\n-      SmallVector<Value> ret;\n-      for (size_t i = 0; i < 4; i++)\n-        ret.push_back(extract_element(outType, outPacked[i / outVecWidth],\n-                                      i32_val(i % outVecWidth)));\n-      return ret;\n-    };\n-    return converter;\n+      it += curr.size();\n+    }\n+    if (op->getNumOperands() > 0) {\n+      auto argTy = op->getOperand(0).getType();\n+      resultVals = reorderValues(resultVals, argTy, resultTy);\n+    }\n+    resultVals =\n+        packI32(resultVals, resultTy, rewriter, loc, this->getTypeConverter());\n+    Value view = this->getTypeConverter()->packLLElements(loc, resultVals,\n+                                                          rewriter, resultTy);\n+    rewriter.replaceOp(op, view);\n+\n+    return success();\n   }\n+};\n+\n+template <typename SourceOp, typename DestOp>\n+struct ElementwiseOpConversion\n+    : public ElementwiseOpConversionBase<\n+          SourceOp, ElementwiseOpConversion<SourceOp, DestOp>> {\n+  using Base =\n+      ElementwiseOpConversionBase<SourceOp,\n+                                  ElementwiseOpConversion<SourceOp, DestOp>>;\n+  using Base::Base;\n+  using OpAdaptor = typename Base::OpAdaptor;\n+\n+  explicit ElementwiseOpConversion(LLVMTypeConverter &typeConverter,\n+                                   PatternBenefit benefit = 1)\n+      : ElementwiseOpConversionBase<SourceOp, ElementwiseOpConversion>(\n+            typeConverter, benefit) {}\n+\n+  // An interface to support variant DestOp builder.\n+  SmallVector<DestOp> createDestOps(SourceOp op, OpAdaptor adaptor,\n+                                    ConversionPatternRewriter &rewriter,\n+                                    Type elemTy, MultipleOperandsRange operands,\n+                                    Location loc) const {\n+    return {rewriter.create<DestOp>(loc, elemTy, operands[0],\n+                                    adaptor.getAttributes().getValue())};\n+  }\n+};\n+\n+// Attempts to use vectorized conversions via inline PTX when possible.\n+struct FpToFpOpConversion\n+    : public ElementwiseOpConversionBase<triton::FpToFpOp, FpToFpOpConversion> {\n+  using ElementwiseOpConversionBase<\n+      triton::FpToFpOp, FpToFpOpConversion>::ElementwiseOpConversionBase;\n \n   static Value convertBf16ToFp32(Location loc,\n                                  ConversionPatternRewriter &rewriter,\n@@ -462,7 +579,7 @@ struct FpToFpOpConversion\n     return builder.launch(rewriter, loc, f16_ty, false);\n   }\n \n-  ConvertorT getConversionFunc(Type srcTy, Type dstTy) const {\n+  ConverterT getConversionFunc(Type srcTy, Type dstTy) const {\n     auto F8E4M3B15TyID = TypeID::get<mlir::Float8E4M3B11FNUZType>();\n     auto F8E4M3TyID = TypeID::get<mlir::Float8E4M3FNUZType>();\n     auto F8E5M2TyID = TypeID::get<mlir::Float8E5M2Type>();\n@@ -498,127 +615,31 @@ struct FpToFpOpConversion\n                                 getTypeConverter()->convertType(dstTy));\n   }\n \n-  LogicalResult\n-  matchAndRewrite(triton::FpToFpOp op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    auto srcTensorType = op.getFrom().getType().cast<mlir::RankedTensorType>();\n-    auto dstTensorType =\n-        op.getResult().getType().cast<mlir::RankedTensorType>();\n-    auto srcElementType = srcTensorType.getElementType();\n-    auto dstElementType = dstTensorType.getElementType();\n-    auto loc = op->getLoc();\n-    // check that the number of elements is divisible by 4\n-    // Unpack value\n-    auto inVals = getTypeConverter()->unpackLLElements(loc, adaptor.getFrom(),\n-                                                       rewriter, srcTensorType);\n-    inVals =\n-        unpackI32(inVals, srcTensorType, rewriter, loc, getTypeConverter());\n-    // Cast\n-    SmallVector<Value> outVals;\n-    auto elems = inVals.size();\n-    assert(elems % 4 == 0 &&\n+  SmallVector<Value> createDestOps(triton::FpToFpOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n+    assert(operands.size() % 4 == 0 &&\n            \"FP8 casting only support tensors with 4-aligned sizes\");\n-    bool isFP32src = srcElementType.isF32();\n-    bool isFP32dst = dstElementType.isF32();\n-    auto cvtFunc = getConversionFunc(isFP32src ? f16_ty : srcElementType,\n-                                     isFP32dst ? f16_ty : dstElementType);\n-    if (isFP32src)\n+    auto srcElementType = getElementType(op.getFrom());\n+    auto dstElementType = getElementType(op.getResult());\n+    bool isSrcFP32 = srcElementType.isF32();\n+    bool isDstFP32 = dstElementType.isF32();\n+    auto cvtFunc = getConversionFunc(isSrcFP32 ? f16_ty : srcElementType,\n+                                     isDstFP32 ? f16_ty : dstElementType);\n+    SmallVector<Value> inVals = {operands[0][0], operands[1][0], operands[2][0],\n+                                 operands[3][0]};\n+    if (isSrcFP32)\n       for (Value &v : inVals)\n         v = convertFp32ToFp16(loc, rewriter, v);\n-    for (size_t i = 0; i < elems; i += 4)\n-      outVals.append(cvtFunc(loc, rewriter, inVals[i], inVals[i + 1],\n-                             inVals[i + 2], inVals[i + 3]));\n-    if (isFP32dst)\n+    SmallVector<Value> outVals =\n+        cvtFunc(loc, rewriter, inVals[0], inVals[1], inVals[2], inVals[3]);\n+    assert(outVals.size() == inVals.size());\n+    if (isDstFP32)\n       for (Value &v : outVals)\n         v = convertFp16ToFp32(loc, rewriter, v);\n     // Pack values\n-    assert(outVals.size() == elems);\n-    outVals = reorderValues(outVals, srcTensorType, dstTensorType);\n-    outVals =\n-        packI32(outVals, dstTensorType, rewriter, loc, getTypeConverter());\n-    auto result = getTypeConverter()->packLLElements(loc, outVals, rewriter,\n-                                                     dstTensorType);\n-    rewriter.replaceOp(op, result);\n-    return success();\n-  }\n-};\n-\n-template <typename SourceOp, typename ConcreteT>\n-class ElementwiseOpConversionBase\n-    : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n-public:\n-  using OpAdaptor = typename SourceOp::Adaptor;\n-\n-  explicit ElementwiseOpConversionBase(\n-      TritonGPUToLLVMTypeConverter &typeConverter, PatternBenefit benefit = 1)\n-      : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n-\n-  LogicalResult\n-  matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    auto resultTy = op.getType();\n-    Location loc = op->getLoc();\n-    // element type\n-    auto resultElementTy = getElementTypeOrSelf(resultTy);\n-    Type elemTy = this->getTypeConverter()->convertType(resultElementTy);\n-    SmallVector<Value> resultVals;\n-    //\n-    SmallVector<SmallVector<Value>> allOperands;\n-    for (auto operand : adaptor.getOperands()) {\n-      auto argTy = op->getOperand(0).getType();\n-      auto sub_operands = this->getTypeConverter()->unpackLLElements(\n-          loc, operand, rewriter, argTy);\n-      sub_operands = unpackI32(sub_operands, argTy, rewriter, loc,\n-                               this->getTypeConverter());\n-      allOperands.resize(sub_operands.size());\n-      for (auto v : llvm::enumerate(sub_operands))\n-        allOperands[v.index()].push_back(v.value());\n-    }\n-    if (allOperands.size() == 0)\n-      allOperands.push_back({});\n-    for (const SmallVector<Value> &operands : allOperands) {\n-      Value curr =\n-          ((ConcreteT *)(this))\n-              ->createDestOp(op, adaptor, rewriter, elemTy, operands, loc);\n-      if (!bool(curr))\n-        return failure();\n-      resultVals.push_back(curr);\n-    }\n-    if (op->getNumOperands() > 0) {\n-      auto argTy = op->getOperand(0).getType();\n-      resultVals = reorderValues(resultVals, argTy, resultTy);\n-    }\n-    resultVals =\n-        packI32(resultVals, resultTy, rewriter, loc, this->getTypeConverter());\n-    Value view = this->getTypeConverter()->packLLElements(loc, resultVals,\n-                                                          rewriter, resultTy);\n-    rewriter.replaceOp(op, view);\n-\n-    return success();\n-  }\n-};\n-\n-template <typename SourceOp, typename DestOp>\n-struct ElementwiseOpConversion\n-    : public ElementwiseOpConversionBase<\n-          SourceOp, ElementwiseOpConversion<SourceOp, DestOp>> {\n-  using Base =\n-      ElementwiseOpConversionBase<SourceOp,\n-                                  ElementwiseOpConversion<SourceOp, DestOp>>;\n-  using Base::Base;\n-  using OpAdaptor = typename Base::OpAdaptor;\n-\n-  explicit ElementwiseOpConversion(LLVMTypeConverter &typeConverter,\n-                                   PatternBenefit benefit = 1)\n-      : ElementwiseOpConversionBase<SourceOp, ElementwiseOpConversion>(\n-            typeConverter, benefit) {}\n-\n-  // An interface to support variant DestOp builder.\n-  DestOp createDestOp(SourceOp op, OpAdaptor adaptor,\n-                      ConversionPatternRewriter &rewriter, Type elemTy,\n-                      ValueRange operands, Location loc) const {\n-    return rewriter.create<DestOp>(loc, elemTy, operands,\n-                                   adaptor.getAttributes().getValue());\n+    return outVals;\n   }\n };\n \n@@ -631,12 +652,13 @@ struct CmpIOpConversion\n   using Adaptor = typename Base::OpAdaptor;\n \n   // An interface to support variant DestOp builder.\n-  LLVM::ICmpOp createDestOp(triton::gpu::CmpIOp op, OpAdaptor adaptor,\n-                            ConversionPatternRewriter &rewriter, Type elemTy,\n-                            ValueRange operands, Location loc) const {\n-    return rewriter.create<LLVM::ICmpOp>(\n-        loc, elemTy, ArithCmpIPredicateToLLVM(op.getPredicate()), operands[0],\n-        operands[1]);\n+  SmallVector<LLVM::ICmpOp>\n+  createDestOps(triton::gpu::CmpIOp op, OpAdaptor adaptor,\n+                ConversionPatternRewriter &rewriter, Type elemTy,\n+                MultipleOperandsRange operands, Location loc) const {\n+    return {rewriter.create<LLVM::ICmpOp>(\n+        loc, elemTy, ArithCmpIPredicateToLLVM(op.getPredicate()),\n+        operands[0][0], operands[0][1])};\n   }\n \n   static LLVM::ICmpPredicate\n@@ -672,13 +694,13 @@ struct CmpFOpConversion\n   using Adaptor = typename Base::OpAdaptor;\n \n   // An interface to support variant DestOp builder.\n-  static LLVM::FCmpOp createDestOp(triton::gpu::CmpFOp op, OpAdaptor adaptor,\n-                                   ConversionPatternRewriter &rewriter,\n-                                   Type elemTy, ValueRange operands,\n-                                   Location loc) {\n-    return rewriter.create<LLVM::FCmpOp>(\n-        loc, elemTy, ArithCmpFPredicateToLLVM(op.getPredicate()), operands[0],\n-        operands[1]);\n+  static SmallVector<LLVM::FCmpOp>\n+  createDestOps(triton::gpu::CmpFOp op, OpAdaptor adaptor,\n+                ConversionPatternRewriter &rewriter, Type elemTy,\n+                MultipleOperandsRange operands, Location loc) {\n+    return {rewriter.create<LLVM::FCmpOp>(\n+        loc, elemTy, ArithCmpFPredicateToLLVM(op.getPredicate()),\n+        operands[0][0], operands[0][1])};\n   }\n \n   static LLVM::FCmpPredicate\n@@ -719,17 +741,19 @@ struct ExternElementwiseOpConversion\n   using Adaptor = typename Base::OpAdaptor;\n   typedef typename Base::OpAdaptor OpAdaptor;\n \n-  Value createDestOp(T op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(T op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     StringRef funcName = op.getSymbol();\n     if (funcName.empty())\n       llvm::errs() << \"ExternElementwiseOpConversion\";\n \n-    Type funcType = getFunctionType(elemTy, operands);\n+    Type funcType = getFunctionType(elemTy, operands[0]);\n     LLVM::LLVMFuncOp funcOp =\n         appendOrGetFuncOp(rewriter, op, funcName, funcType);\n-    return rewriter.create<LLVM::CallOp>(loc, funcOp, operands).getResult();\n+    return {\n+        rewriter.create<LLVM::CallOp>(loc, funcOp, operands[0]).getResult()};\n   }\n \n private:\n@@ -765,9 +789,10 @@ struct FDivOpConversion\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(mlir::arith::DivFOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(mlir::arith::DivFOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     PTXBuilder ptxBuilder;\n     auto &fdiv = *ptxBuilder.create<PTXInstr>(\"div\");\n     unsigned bitwidth = elemTy.getIntOrFloatBitWidth();\n@@ -780,12 +805,14 @@ struct FDivOpConversion\n     }\n \n     auto res = ptxBuilder.newOperand(bitwidth == 32 ? \"=r\" : \"=l\");\n-    auto lhs = ptxBuilder.newOperand(operands[0], bitwidth == 32 ? \"r\" : \"l\");\n-    auto rhs = ptxBuilder.newOperand(operands[1], bitwidth == 32 ? \"r\" : \"l\");\n+    auto lhs =\n+        ptxBuilder.newOperand(operands[0][0], bitwidth == 32 ? \"r\" : \"l\");\n+    auto rhs =\n+        ptxBuilder.newOperand(operands[0][1], bitwidth == 32 ? \"r\" : \"l\");\n     fdiv(res, lhs, rhs);\n \n     Value ret = ptxBuilder.launch(rewriter, loc, elemTy, false);\n-    return ret;\n+    return {ret};\n   }\n };\n \n@@ -796,9 +823,10 @@ struct FMulOpConversion\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(mlir::arith::MulFOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(mlir::arith::MulFOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     auto lhsElemTy = getElementType(op.getLhs());\n     auto rhsElemTy = getElementType(op.getRhs());\n     if (lhsElemTy.isBF16() && rhsElemTy.isBF16()) {\n@@ -808,13 +836,13 @@ struct FMulOpConversion\n                     \"    fma.rn.bf16 $0, $1, $2, c; } \\n\";\n       auto &fMul = *builder.create<PTXInstr>(ptxAsm);\n       auto res = builder.newOperand(\"=h\");\n-      auto lhs = builder.newOperand(operands[0], \"h\");\n-      auto rhs = builder.newOperand(operands[1], \"h\");\n+      auto lhs = builder.newOperand(operands[0][0], \"h\");\n+      auto rhs = builder.newOperand(operands[0][1], \"h\");\n       fMul({res, lhs, rhs}, /*onlyAttachMLIRArgs=*/true);\n-      return builder.launch(rewriter, loc, i16_ty, false);\n+      return {builder.launch(rewriter, loc, i16_ty, false)};\n     } else {\n-      return rewriter.create<LLVM::FMulOp>(loc, elemTy, operands[0],\n-                                           operands[1]);\n+      return {rewriter.create<LLVM::FMulOp>(loc, elemTy, operands[0][0],\n+                                            operands[0][1])};\n     }\n   }\n };\n@@ -826,9 +854,10 @@ struct FAddOpConversion\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(mlir::arith::AddFOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(mlir::arith::AddFOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     auto lhsElemTy = getElementType(op.getLhs());\n     auto rhsElemTy = getElementType(op.getRhs());\n     if (lhsElemTy.isBF16() && rhsElemTy.isBF16()) {\n@@ -838,13 +867,13 @@ struct FAddOpConversion\n                     \"   fma.rn.bf16 $0, $1, c, $2; } \\n\";\n       auto &fAdd = *builder.create<PTXInstr>(ptxAsm);\n       auto res = builder.newOperand(\"=h\");\n-      auto lhs = builder.newOperand(operands[0], \"h\");\n-      auto rhs = builder.newOperand(operands[1], \"h\");\n+      auto lhs = builder.newOperand(operands[0][0], \"h\");\n+      auto rhs = builder.newOperand(operands[0][1], \"h\");\n       fAdd({res, lhs, rhs}, /*onlyAttachMLIRArgs=*/true);\n-      return builder.launch(rewriter, loc, i16_ty, false);\n+      return {builder.launch(rewriter, loc, i16_ty, false)};\n     } else {\n-      return rewriter.create<LLVM::FAddOp>(loc, elemTy, operands[0],\n-                                           operands[1]);\n+      return {rewriter.create<LLVM::FAddOp>(loc, elemTy, operands[0][0],\n+                                            operands[0][1])};\n     }\n   }\n };\n@@ -856,9 +885,10 @@ struct FSubOpConversion\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(mlir::arith::SubFOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(mlir::arith::SubFOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     auto lhsElemTy = getElementType(op.getLhs());\n     auto rhsElemTy = getElementType(op.getRhs());\n     if (lhsElemTy.isBF16() && rhsElemTy.isBF16()) {\n@@ -868,33 +898,44 @@ struct FSubOpConversion\n                     \"    fma.rn.bf16 $0, $2, c, $1;} \\n\";\n       auto &fSub = *builder.create<PTXInstr>(ptxAsm);\n       auto res = builder.newOperand(\"=h\");\n-      auto lhs = builder.newOperand(operands[0], \"h\");\n-      auto rhs = builder.newOperand(operands[1], \"h\");\n+      auto lhs = builder.newOperand(operands[0][0], \"h\");\n+      auto rhs = builder.newOperand(operands[0][1], \"h\");\n       fSub({res, lhs, rhs}, /*onlyAttachMLIRArgs=*/true);\n-      return builder.launch(rewriter, loc, i16_ty, false);\n+      return {builder.launch(rewriter, loc, i16_ty, false)};\n     } else {\n-      return rewriter.create<LLVM::FSubOp>(loc, elemTy, operands[0],\n-                                           operands[1]);\n+      return {rewriter.create<LLVM::FSubOp>(loc, elemTy, operands[0][0],\n+                                            operands[0][1])};\n     }\n   }\n };\n \n+// Uses inline ptx to convert s8/u8 to bf16, since the\n struct SIToFPOpConversion\n     : ElementwiseOpConversionBase<mlir::arith::SIToFPOp, SIToFPOpConversion> {\n   using Base =\n       ElementwiseOpConversionBase<mlir::arith::SIToFPOp, SIToFPOpConversion>;\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(mlir::arith::SIToFPOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n-    auto outElemTy = getElementType(op.getOut());\n-    if (outElemTy.isBF16()) {\n-      auto value = rewriter.create<LLVM::SIToFPOp>(loc, f32_ty, operands[0]);\n-      return FpToFpOpConversion::convertFp32ToBf16(loc, rewriter, value);\n+  SmallVector<Value> createDestOps(mlir::arith::SIToFPOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n+    Type inElemTy = getElementType(op.getIn());\n+    Type outElemTy = getElementType(op.getOut());\n+    if (outElemTy.isBF16() && inElemTy.isInteger(8) && operands.size() >= 4) {\n+      auto cvtFunc = makeConverterFromPtx(\n+          S8_to_Bf16, getTypeConverter()->convertType(inElemTy),\n+          getTypeConverter()->convertType(outElemTy));\n+      auto outVals = cvtFunc(loc, rewriter, operands[0][0], operands[1][0],\n+                             operands[2][0], operands[3][0]);\n+      assert(outVals.size() == 4);\n+      return outVals;\n+    } else if (outElemTy.isBF16()) {\n+      auto value = rewriter.create<LLVM::SIToFPOp>(loc, f32_ty, operands[0][0]);\n+      return {FpToFpOpConversion::convertFp32ToBf16(loc, rewriter, value)};\n     } else {\n-      return rewriter.create<LLVM::SIToFPOp>(loc, elemTy, operands[0]);\n+      return {rewriter.create<LLVM::SIToFPOp>(loc, elemTy, operands[0][0])};\n     }\n   }\n };\n@@ -906,16 +947,17 @@ struct FPToSIOpConversion\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(mlir::arith::FPToSIOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(mlir::arith::FPToSIOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     auto inElemTy = getElementType(op.getIn());\n     if (inElemTy.isBF16()) {\n       auto value =\n-          FpToFpOpConversion::convertBf16ToFp32(loc, rewriter, operands[0]);\n-      return rewriter.create<LLVM::FPToSIOp>(loc, elemTy, value);\n+          FpToFpOpConversion::convertBf16ToFp32(loc, rewriter, operands[0][0]);\n+      return {rewriter.create<LLVM::FPToSIOp>(loc, elemTy, value)};\n     } else {\n-      return rewriter.create<LLVM::FPToSIOp>(loc, elemTy, operands[0]);\n+      return {rewriter.create<LLVM::FPToSIOp>(loc, elemTy, operands[0][0])};\n     }\n   }\n };\n@@ -927,16 +969,18 @@ struct ExtFOpConversion\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(mlir::arith::ExtFOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(mlir::arith::ExtFOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     auto inElemTy = getElementType(op.getIn());\n     if (inElemTy.isBF16()) {\n       auto outElemTy = getElementType(op.getOut());\n       assert(outElemTy.isF32() && \"unsupported conversion\");\n-      return FpToFpOpConversion::convertBf16ToFp32(loc, rewriter, operands[0]);\n+      return {\n+          FpToFpOpConversion::convertBf16ToFp32(loc, rewriter, operands[0][0])};\n     } else {\n-      return rewriter.create<LLVM::FPExtOp>(loc, elemTy, operands[0]);\n+      return {rewriter.create<LLVM::FPExtOp>(loc, elemTy, operands[0][0])};\n     }\n   }\n };\n@@ -948,16 +992,18 @@ struct TruncFOpConversion\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(mlir::arith::TruncFOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(mlir::arith::TruncFOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     auto outElemTy = getElementType(op.getOut());\n     if (outElemTy.isBF16()) {\n       auto inElemTy = getElementType(op.getIn());\n       assert(inElemTy.isF32() && \"unsupported conversion\");\n-      return FpToFpOpConversion::convertFp32ToBf16(loc, rewriter, operands[0]);\n+      return {\n+          FpToFpOpConversion::convertFp32ToBf16(loc, rewriter, operands[0][0])};\n     } else {\n-      return rewriter.create<LLVM::FPTruncOp>(loc, elemTy, operands[0]);\n+      return {rewriter.create<LLVM::FPTruncOp>(loc, elemTy, operands[0][0])};\n     }\n   }\n };\n@@ -969,22 +1015,23 @@ struct ExpOpConversionApprox\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(mlir::math::ExpOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(mlir::math::ExpOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     // For non-FP32 input, call __nv_expf for higher-precision calculation\n     if (elemTy.getIntOrFloatBitWidth() != 32)\n       return {};\n \n     const double log2e = 1.4426950408889634;\n-    Value prod = fmul(f32_ty, operands[0], f32_val(log2e));\n+    Value prod = fmul(f32_ty, operands[0][0], f32_val(log2e));\n \n     PTXBuilder ptxBuilder;\n     auto &exp2 = ptxBuilder.create<PTXInstr>(\"ex2\")->o(\"approx\").o(\"f32\");\n     auto output = ptxBuilder.newOperand(\"=f\");\n     auto input = ptxBuilder.newOperand(prod, \"f\");\n     exp2(output, input);\n-    return ptxBuilder.launch(rewriter, loc, f32_ty, false);\n+    return {ptxBuilder.launch(rewriter, loc, f32_ty, false)};\n   }\n };\n \n@@ -995,13 +1042,14 @@ struct AbsIOpConversion\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(mlir::math::AbsIOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(mlir::math::AbsIOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     auto boolFalse = rewriter.getBoolAttr(false);\n     auto constFalse = rewriter.create<LLVM::ConstantOp>(loc, boolFalse);\n-    return rewriter.create<LLVM::AbsOp>(loc, elemTy, operands[0],\n-                                        /*is_int_min_poison=*/constFalse);\n+    return {rewriter.create<LLVM::AbsOp>(loc, elemTy, operands[0][0],\n+                                         /*is_int_min_poison=*/constFalse)};\n   }\n };\n \n@@ -1012,9 +1060,10 @@ struct AbsFOpConversion\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(mlir::math::AbsFOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(mlir::math::AbsFOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     if (llvm::isa<IntegerType>(elemTy)) {\n       // Mask out the sign bit\n       auto num_bits =\n@@ -1023,10 +1072,10 @@ struct AbsFOpConversion\n       auto mask = (1u << (num_bits - 1u)) - 1u;\n       auto maskAttr = rewriter.getIntegerAttr(elemTy, mask);\n       auto maskConst = rewriter.create<LLVM::ConstantOp>(loc, maskAttr);\n-      return and_(operands[0], maskConst);\n+      return {and_(operands[0][0], maskConst)};\n     }\n \n-    return rewriter.create<LLVM::FAbsOp>(loc, elemTy, operands[0]);\n+    return {rewriter.create<LLVM::FAbsOp>(loc, elemTy, operands[0][0])};\n   }\n };\n \n@@ -1042,20 +1091,22 @@ struct IndexCastOpLowering\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(arith::IndexCastOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(arith::IndexCastOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     auto inElemTy =\n         this->getTypeConverter()->convertType(getElementType(op.getIn()));\n     unsigned targetBits = elemTy.getIntOrFloatBitWidth();\n     unsigned sourceBits = inElemTy.getIntOrFloatBitWidth();\n \n     if (targetBits == sourceBits)\n-      return operands[0];\n+      return {operands[0][0]};\n     if (targetBits < sourceBits)\n-      return rewriter.replaceOpWithNewOp<LLVM::TruncOp>(op, elemTy,\n-                                                        operands[0]);\n-    return rewriter.replaceOpWithNewOp<LLVM::SExtOp>(op, elemTy, operands[0]);\n+      return {rewriter.replaceOpWithNewOp<LLVM::TruncOp>(op, elemTy,\n+                                                         operands[0][0])};\n+    return {\n+        rewriter.replaceOpWithNewOp<LLVM::SExtOp>(op, elemTy, operands[0][0])};\n   }\n };\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 42, "deletions": 13, "changes": 55, "file_content_changes": "@@ -331,18 +331,20 @@ struct ReduceOpConversion\n     unsigned elems = product<unsigned>(smemShapes[0]);\n     unsigned maxElems = std::max(elems, product<unsigned>(smemShapes[1]));\n \n-    SmallVector<Value> smemBases(op.getNumOperands());\n-    smemBases[0] = bitcast(\n-        getSharedMemoryBase(loc, rewriter, op.getOperation()), elemPtrTys[0]);\n-    for (unsigned i = 1; i < op.getNumOperands(); ++i) {\n-      smemBases[i] =\n-          bitcast(gep(elemPtrTys[i - 1], smemBases[i - 1], i32_val(maxElems)),\n-                  elemPtrTys[i]);\n-    }\n-\n     unsigned sizeIntraWarps = helper.getIntraWarpSizeWithUniqueData();\n     unsigned sizeInterWarps = helper.getInterWarpSizeWithUniqueData();\n \n+    SmallVector<Value> smemBases(op.getNumOperands());\n+    if (sizeInterWarps > 1) {\n+      smemBases[0] = bitcast(\n+          getSharedMemoryBase(loc, rewriter, op.getOperation()), elemPtrTys[0]);\n+      for (unsigned i = 1; i < op.getNumOperands(); ++i) {\n+        smemBases[i] =\n+            bitcast(gep(elemPtrTys[i - 1], smemBases[i - 1], i32_val(maxElems)),\n+                    elemPtrTys[i]);\n+      }\n+    }\n+\n     unsigned srcElems = getTotalElemsPerThread(srcTys[0]);\n     auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTys[0]);\n     auto srcValues = unpackInputs(loc, op, adaptor, rewriter);\n@@ -387,6 +389,7 @@ struct ReduceOpConversion\n     Value zero = i32_val(0);\n     Value laneZero = icmp_eq(laneIdAxis, zero);\n \n+    std::map<SmallVector<unsigned>, SmallVector<Value>> finalAccs;\n     for (auto it : accs) {\n       const SmallVector<unsigned> &key = it.first;\n       SmallVector<Value> acc = it.second;\n@@ -400,8 +403,13 @@ struct ReduceOpConversion\n         accumulate(rewriter, *combineOp, acc, shfl, false);\n       }\n \n+      if (sizeInterWarps == 1) {\n+        finalAccs[key] = acc;\n+        continue;\n+      }\n+\n       SmallVector<Value> writeIdx = indices[key];\n-      writeIdx[axis] = (sizeInterWarps == 1) ? zero : warpIdAxis;\n+      writeIdx[axis] = warpIdAxis;\n       Value writeOffset =\n           linearize(rewriter, loc, writeIdx, smemShapes[0], order);\n       for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n@@ -410,6 +418,30 @@ struct ReduceOpConversion\n       }\n     }\n \n+    if (sizeInterWarps == 1) {\n+      SmallVector<Value> results(op.getNumOperands());\n+      for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+        if (auto resultTy =\n+                op.getResult()[i].getType().dyn_cast<RankedTensorType>()) {\n+          auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n+          unsigned resultElems = getTotalElemsPerThread(resultTy);\n+          SmallVector<SmallVector<unsigned>> resultOffset =\n+              emitOffsetForLayout(resultLayout, resultTy);\n+          SmallVector<Value> resultVals;\n+          for (int j = 0; j < resultElems; j++) {\n+            auto key = resultOffset[j];\n+            key.insert(key.begin() + axis, 0);\n+            resultVals.push_back(finalAccs[key][i]);\n+          }\n+          results[i] = getTypeConverter()->packLLElements(loc, resultVals,\n+                                                          rewriter, resultTy);\n+        } else\n+          results[i] = finalAccs.begin()->second[i];\n+      }\n+      rewriter.replaceOp(op, results);\n+      return success();\n+    }\n+\n     barrier();\n \n     // The second round of shuffle reduction\n@@ -463,9 +495,6 @@ struct ReduceOpConversion\n       }\n     }\n \n-    // We could avoid this barrier in some of the layouts, however this is not\n-    // the general case.\n-    // TODO: optimize the barrier in case the layouts are accepted.\n     barrier();\n \n     // set output values"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 11, "deletions": 5, "changes": 16, "file_content_changes": "@@ -212,17 +212,23 @@ struct CallOpConversion : public ConvertOpToLLVMPattern<triton::CallOp> {\n     // of shared memory and append it to the operands of the callOp.\n     auto loc = callOp.getLoc();\n     auto caller = callOp->getParentOfType<FunctionOpInterface>();\n-    auto base = allocation.getFunctionSharedMemoryBase(caller);\n-    auto *funcAllocation = allocation.getFuncData(caller);\n-    auto bufferId = funcAllocation->getBufferId(callOp);\n-    auto offset = funcAllocation->getOffset(bufferId);\n     auto ptrTy = LLVM::LLVMPointerType::get(\n         this->getTypeConverter()->convertType(rewriter.getI8Type()),\n         NVVM::kSharedMemorySpace);\n-    auto offsetValue = gep(ptrTy, base, i32_val(offset));\n     auto promotedOperands = this->getTypeConverter()->promoteOperands(\n         callOp.getLoc(), /*opOperands=*/callOp->getOperands(),\n         adaptor.getOperands(), rewriter);\n+    auto base = allocation.getFunctionSharedMemoryBase(caller);\n+    auto *funcAllocation = allocation.getFuncData(caller);\n+    auto bufferId = funcAllocation->getBufferId(callOp);\n+    // function doesn't have a shared mem buffer\n+    if (bufferId == (size_t)-1) {\n+      promotedOperands.push_back(base);\n+      return promotedOperands;\n+    }\n+    // function has a shared mem buffer\n+    auto offset = funcAllocation->getOffset(bufferId);\n+    auto offsetValue = gep(ptrTy, base, i32_val(offset));\n     promotedOperands.push_back(offsetValue);\n     return promotedOperands;\n   }"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 31, "deletions": 8, "changes": 39, "file_content_changes": "@@ -6,7 +6,6 @@\n #include \"mlir/IR/OperationSupport.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/Triton/IR/Types.h\"\n-#include \"triton/Dialect/TritonGPU/IR/Attributes.h\"\n \n namespace mlir {\n namespace triton {\n@@ -404,18 +403,17 @@ LogicalResult mlir::triton::DotOp::verify() {\n   auto bTy = getOperand(1).getType().cast<RankedTensorType>();\n   if (aTy.getElementType() != bTy.getElementType())\n     return emitError(\"element types of operands A and B must match\");\n-  auto aEncoding =\n-      aTy.getEncoding().dyn_cast_or_null<triton::gpu::DotOperandEncodingAttr>();\n-  auto bEncoding =\n-      bTy.getEncoding().dyn_cast_or_null<triton::gpu::DotOperandEncodingAttr>();\n+  auto aEncoding = aTy.getEncoding();\n+  auto bEncoding = bTy.getEncoding();\n   if (!aEncoding && !bEncoding)\n     return mlir::success();\n   // Verify that the encodings are valid.\n   if (!aEncoding || !bEncoding)\n     return emitError(\"mismatching encoding between A and B operands\");\n-  if (aEncoding.getMMAv2kWidth() != bEncoding.getMMAv2kWidth())\n-    return emitError(\"mismatching kWidth between A and B operands\");\n-  return mlir::success();\n+  Dialect &dialect = aEncoding.getDialect();\n+  auto interface = cast<DialectInferLayoutInterface>(&dialect);\n+  return interface->verifyDotOpEncodingCompatibility(getOperation(), aEncoding,\n+                                                     bEncoding);\n }\n \n //-- ReduceOp --\n@@ -641,6 +639,31 @@ LogicalResult ExpandDimsOp::canonicalize(ExpandDimsOp op,\n                                                  splat.getOperand());\n     return mlir::success();\n   }\n+  // expand_dims(broadcast) -> broadcast(expand_dims)\n+  //\n+  // On it's own this doesn't do much, but consider\n+  //    broadcast(expand_dims(broadcast))\n+  // -> broadcast(broadcast(expand_dims))\n+  // -> broadcast(expand_dims)\n+  if (auto broadcast = dyn_cast<triton::BroadcastOp>(definingOp)) {\n+    auto src = broadcast.getSrc();\n+    auto srcTy = src.getType().dyn_cast<RankedTensorType>();\n+    auto elemTy = srcTy.getElementType();\n+    auto srcShape = srcTy.getShape();\n+\n+    llvm::SmallVector<int64_t, 4> newExpandShape(srcShape.begin(),\n+                                                 srcShape.end());\n+    newExpandShape.insert(newExpandShape.begin() + op.getAxis(), 1);\n+    auto newExpandTy = RankedTensorType::get(newExpandShape, elemTy);\n+\n+    auto newExpand = rewriter.create<triton::ExpandDimsOp>(\n+        op.getLoc(), newExpandTy, src, op.getAxis());\n+    auto newBroadcast = rewriter.create<triton::BroadcastOp>(\n+        broadcast.getLoc(), op.getType(), newExpand.getResult());\n+    rewriter.replaceOp(op, {newBroadcast.getResult()});\n+    return mlir::success();\n+  }\n+\n   return mlir::failure();\n }\n "}, {"filename": "lib/Dialect/Triton/Transforms/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -4,6 +4,7 @@ add_public_tablegen_target(TritonCombineIncGen)\n \n add_mlir_dialect_library(TritonTransforms\n   Combine.cpp\n+  ReorderBroadcast.cpp\n   RewriteTensorPointer.cpp\n \n   DEPENDS"}, {"filename": "lib/Dialect/Triton/Transforms/ReorderBroadcast.cpp", "status": "added", "additions": 243, "deletions": 0, "changes": 243, "file_content_changes": "@@ -0,0 +1,243 @@\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LogicalResult.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/Triton/Transforms/Passes.h\"\n+\n+#include <memory>\n+\n+namespace mlir {\n+#define GEN_PASS_DEF_TRITONREORDERBROADCAST\n+#include \"triton/Dialect/Triton/Transforms/Passes.h.inc\"\n+} // namespace mlir\n+\n+using namespace mlir;\n+\n+namespace {\n+\n+Operation *cloneWithNewArgsAndResultTypes(PatternRewriter &rewriter,\n+                                          Operation *op, ValueRange newOperands,\n+                                          TypeRange newTypes) {\n+  OperationState newElementwiseState(op->getLoc(), op->getName());\n+  newElementwiseState.addOperands(newOperands);\n+  newElementwiseState.addTypes(newTypes);\n+  newElementwiseState.addAttributes(op->getAttrs());\n+  return rewriter.create(newElementwiseState);\n+}\n+\n+bool isSplat(Operation *op) {\n+  if (auto splatOp = llvm::dyn_cast<triton::SplatOp>(op)) {\n+    return true;\n+  }\n+  DenseElementsAttr constAttr;\n+  return (matchPattern(op, m_Constant(&constAttr)) && constAttr.isSplat());\n+}\n+\n+// elementwise(splat(a), splat(b), ...) => splat(elementwise(a, b, ...))\n+struct MoveSplatAfterElementwisePattern\n+    : public mlir::OpTraitRewritePattern<mlir::OpTrait::Elementwise> {\n+\n+  MoveSplatAfterElementwisePattern(mlir::MLIRContext *context)\n+      : OpTraitRewritePattern(context) {}\n+\n+  mlir::LogicalResult match(Operation *op) const override {\n+    if (!isMemoryEffectFree(op)) {\n+      return mlir::failure();\n+    }\n+\n+    for (auto operand : op->getOperands()) {\n+      auto definingOp = operand.getDefiningOp();\n+      if (!definingOp)\n+        return mlir::failure();\n+\n+      if (!isSplat(definingOp)) {\n+        return mlir::failure();\n+      }\n+    }\n+    return mlir::success(op->getNumOperands() > 0);\n+  }\n+\n+  void rewrite(Operation *op, mlir::PatternRewriter &rewriter) const override {\n+    auto loc = op->getLoc();\n+    auto operands = op->getOperands();\n+\n+    llvm::SmallVector<Value, 4> scalarOperands(operands.size());\n+    for (unsigned iOp = 0; iOp < operands.size(); ++iOp) {\n+      auto definingOp = operands[iOp].getDefiningOp();\n+\n+      DenseElementsAttr constAttr;\n+      if (auto splatOp = llvm::dyn_cast<triton::SplatOp>(definingOp)) {\n+        scalarOperands[iOp] = splatOp.getSrc();\n+      } else if (matchPattern(definingOp, m_Constant(&constAttr)) &&\n+                 constAttr.isSplat()) {\n+        auto value = constAttr.getSplatValue<Attribute>();\n+        scalarOperands[iOp] = arith::ConstantOp::materialize(\n+            rewriter, value, constAttr.getElementType(), loc);\n+      } else {\n+        llvm_unreachable(\"Expected a splat\");\n+      }\n+    }\n+\n+    auto resultTypes = op->getResultTypes();\n+    llvm::SmallVector<Type, 4> scalarResultTys;\n+    for (auto resultTy : resultTypes) {\n+      auto elemTy = resultTy.dyn_cast<TensorType>().getElementType();\n+      scalarResultTys.push_back(elemTy);\n+    }\n+\n+    auto newOp = cloneWithNewArgsAndResultTypes(rewriter, op, scalarOperands,\n+                                                scalarResultTys);\n+\n+    for (unsigned iRes = 0; iRes < resultTypes.size(); ++iRes) {\n+      auto newResult = rewriter.create<triton::SplatOp>(loc, resultTypes[iRes],\n+                                                        newOp->getResult(iRes));\n+      rewriter.replaceAllUsesWith(op->getResult(iRes), newResult);\n+    }\n+  }\n+};\n+\n+// elementwise(broadcast(a)) => broadcast(elementwise(a))\n+// This also generalizes to multiple arguments when the rest are splat-like\n+// Not handled: multiple broadcasted arguments\n+struct MoveBroadcastAfterElementwisePattern\n+    : public mlir::OpTraitRewritePattern<mlir::OpTrait::Elementwise> {\n+\n+  MoveBroadcastAfterElementwisePattern(mlir::MLIRContext *context)\n+      : OpTraitRewritePattern(context) {}\n+\n+  mlir::LogicalResult match(Operation *op) const override {\n+    if (!isMemoryEffectFree(op)) {\n+      return mlir::failure();\n+    }\n+\n+    auto operands = op->getOperands();\n+    bool seenBroadcast = false;\n+    for (auto operand : operands) {\n+      auto definingOp = operand.getDefiningOp();\n+      if (!definingOp) {\n+        return mlir::failure();\n+      }\n+\n+      if (auto broadcastOp = llvm::dyn_cast<triton::BroadcastOp>(definingOp)) {\n+        if (seenBroadcast) {\n+          // Only support one broadcasted argument for now\n+          return mlir::failure();\n+        }\n+        seenBroadcast = true;\n+      } else if (!isSplat(definingOp)) {\n+        // Not splat or broadcast\n+        return mlir::failure();\n+      }\n+    }\n+    return mlir::success(seenBroadcast);\n+  }\n+\n+  void rewrite(Operation *op, mlir::PatternRewriter &rewriter) const override {\n+    auto loc = op->getLoc();\n+\n+    // Find broadcast op\n+    auto operands = op->getOperands();\n+    triton::BroadcastOp broadcastOp;\n+    for (auto operand : operands) {\n+      broadcastOp = operand.getDefiningOp<triton::BroadcastOp>();\n+      if (broadcastOp) {\n+        break;\n+      }\n+    }\n+\n+    auto src = broadcastOp.getSrc();\n+    auto srcTy = src.getType().dyn_cast<RankedTensorType>();\n+    auto srcShape = srcTy.getShape();\n+    auto srcEncoding = srcTy.getEncoding();\n+\n+    // Reshape operands to match srcShape\n+    llvm::SmallVector<Value, 4> newOperands;\n+    for (auto operand : operands) {\n+      auto definingOp = operand.getDefiningOp();\n+      if (llvm::isa<triton::BroadcastOp>(definingOp)) {\n+        newOperands.push_back(src);\n+        continue;\n+      }\n+      auto elemTy =\n+          operand.getType().dyn_cast<RankedTensorType>().getElementType();\n+      auto newTy = RankedTensorType::get(srcShape, elemTy, srcEncoding);\n+      if (auto splatOp = llvm::dyn_cast<triton::SplatOp>(definingOp)) {\n+        auto newSplat =\n+            rewriter.create<triton::SplatOp>(loc, newTy, splatOp.getSrc());\n+        newOperands.push_back(newSplat);\n+        continue;\n+      }\n+      DenseElementsAttr constAttr;\n+      if (matchPattern(definingOp, m_Constant(&constAttr)) &&\n+          constAttr.isSplat()) {\n+        auto scalarValue = constAttr.getSplatValue<Attribute>();\n+        auto splatValue = SplatElementsAttr::get(newTy, scalarValue);\n+        auto newConstant =\n+            rewriter.create<arith::ConstantOp>(loc, newTy, splatValue);\n+        newOperands.push_back(newConstant);\n+        continue;\n+      }\n+      llvm_unreachable(\"Expected broadcast or splat\");\n+    }\n+\n+    // Reshape results to match srcShape\n+    llvm::SmallVector<Type, 4> newResultTypes;\n+    auto resultTypes = op->getResultTypes();\n+    for (auto resultTy : resultTypes) {\n+      auto elemTy = resultTy.dyn_cast<RankedTensorType>().getElementType();\n+      newResultTypes.push_back(\n+          RankedTensorType::get(srcShape, elemTy, srcEncoding));\n+    }\n+\n+    // Create new op and broadcast results\n+    auto newOp = cloneWithNewArgsAndResultTypes(rewriter, op, newOperands,\n+                                                newResultTypes);\n+    for (unsigned iRes = 0; iRes < newResultTypes.size(); ++iRes) {\n+      auto newResult = rewriter.create<triton::BroadcastOp>(\n+          loc, resultTypes[iRes], newOp->getResult(iRes));\n+      rewriter.replaceAllUsesWith(op->getResult(iRes), newResult);\n+    }\n+  }\n+};\n+\n+template <typename OpType>\n+class CanonicalizePattern : public mlir::OpRewritePattern<OpType> {\n+public:\n+  explicit CanonicalizePattern(mlir::MLIRContext *context)\n+      : mlir::OpRewritePattern<OpType>(context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(OpType op, mlir::PatternRewriter &rewriter) const override {\n+    return OpType::canonicalize(op, rewriter);\n+  }\n+};\n+\n+class ReorderBroadcastPass\n+    : public mlir::impl::TritonReorderBroadcastBase<ReorderBroadcastPass> {\n+public:\n+  void runOnOperation() override {\n+    mlir::MLIRContext *context = &getContext();\n+    mlir::RewritePatternSet patterns(context);\n+    mlir::ModuleOp m = getOperation();\n+\n+    patterns.add<CanonicalizePattern<triton::BroadcastOp>>(context);\n+    patterns.add<CanonicalizePattern<triton::ExpandDimsOp>>(context);\n+    // elementwise(broadcast(a)) => broadcast(elementwise(a))\n+    patterns.add<MoveBroadcastAfterElementwisePattern>(context);\n+    // elementwise(splat(a), splat(b), ...) => splat(elementwise(a, b, ...))\n+    patterns.add<MoveSplatAfterElementwisePattern>(context);\n+\n+    if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n+      signalPassFailure();\n+  }\n+};\n+\n+} // namespace\n+\n+std::unique_ptr<mlir::Pass> mlir::triton::createReorderBroadcastPass() {\n+  return std::make_unique<ReorderBroadcastPass>();\n+}"}, {"filename": "lib/Dialect/Triton/Transforms/RewriteTensorPointer.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -470,8 +470,8 @@ class RewriteTensorPointerPass\n \n   void runOnOperation() override {\n     // Only rewrite if the hardware does not support\n-    if (computeCapability >= 90)\n-      return;\n+    // if (computeCapability >= 90)\n+    //   return;\n \n     // NOTES(Chenggang): we don't use `ConversionPatternRewriter`, because\n     // MLIR does not support one-multiple value mapping. For example, if we use"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 23, "deletions": 11, "changes": 34, "file_content_changes": "@@ -221,28 +221,23 @@ SmallVector<unsigned> getContigPerThread(Attribute layout) {\n   }\n }\n \n-SmallVector<unsigned> getUniqueContigPerThread(Type type) {\n-  if (type.isIntOrIndexOrFloat() || type.isa<triton::PointerType>())\n-    return SmallVector<unsigned>(1, 1);\n-  auto tensorType = type.cast<RankedTensorType>();\n-  auto shape = tensorType.getShape();\n+SmallVector<unsigned> getUniqueContigPerThread(Attribute layout,\n+                                               ArrayRef<int64_t> shape) {\n   // If slice layout, call recursively on parent layout, and drop\n   // sliced dim\n-  if (auto sliceLayout =\n-          tensorType.getEncoding().dyn_cast<SliceEncodingAttr>()) {\n+  if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     auto parentLayout = sliceLayout.getParent();\n     auto parentShape = sliceLayout.paddedShape(shape);\n-    auto parentTy = RankedTensorType::get(\n-        parentShape, tensorType.getElementType(), parentLayout);\n-    auto parentUniqueContigPerThread = getUniqueContigPerThread(parentTy);\n+    auto parentUniqueContigPerThread =\n+        getUniqueContigPerThread(parentLayout, parentShape);\n     parentUniqueContigPerThread.erase(parentUniqueContigPerThread.begin() +\n                                       sliceLayout.getDim());\n     return parentUniqueContigPerThread;\n   }\n   // Base case\n   auto rank = shape.size();\n   SmallVector<unsigned> ret(rank);\n-  auto contigPerThread = getContigPerThread(tensorType.getEncoding());\n+  auto contigPerThread = getContigPerThread(layout);\n   assert(contigPerThread.size() == rank && \"Unexpected contigPerThread size\");\n   for (int d = 0; d < rank; ++d) {\n     ret[d] = std::min<unsigned>(shape[d], contigPerThread[d]);\n@@ -1111,6 +1106,23 @@ struct TritonGPUInferLayoutInterface\n           location, \"Dot's a/b's encoding should be of DotOperandEncodingAttr\");\n     return success();\n   }\n+\n+  LogicalResult\n+  verifyDotOpEncodingCompatibility(Operation *op, Attribute operandEncodingA,\n+                                   Attribute operandEncodingB) const override {\n+    auto aEncoding =\n+        operandEncodingA.dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n+    auto bEncoding =\n+        operandEncodingB.dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n+    if (!aEncoding && !bEncoding)\n+      return mlir::success();\n+    // Verify that the encodings are valid.\n+    if (!aEncoding || !bEncoding)\n+      return op->emitError(\"mismatching encoding between A and B operands\");\n+    if (aEncoding.getMMAv2kWidth() != bEncoding.getMMAv2kWidth())\n+      return op->emitError(\"mismatching kWidth between A and B operands\");\n+    return success();\n+  }\n };\n \n //===----------------------------------------------------------------------===//"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 21, "deletions": 19, "changes": 40, "file_content_changes": "@@ -761,27 +761,29 @@ scf::ForOp LoopPipeliner::cloneForOp(ArrayRef<Value> newLoopArgs,\n     mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n   mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n \n-  // Clone the loop body, replace original args with args of the new ForOp\n-  // Insert async wait if necessary.\n+  // Clone the loop body, replace original args with args of the new ForOp.\n+  // We want to find cvt ops that match the following pattern:\n+  // %0 = load %ptr\n+  // %1 (dotOperand) = cvt %0\n   for (Operation &op : forOp.getBody()->without_terminator()) {\n-    // is modified\n-    auto it = std::find(validLoads.begin(), validLoads.end(), op.getOperand(0));\n-    if (it == validLoads.end()) {\n-      Operation *newOp = cloneWithInferType(builder, &op, mapping);\n-      continue;\n-    }\n-\n-    // we replace the use new load use with a convert layout\n-    size_t i = std::distance(validLoads.begin(), it);\n-    auto cvtDstTy = op.getResult(0).getType().cast<RankedTensorType>();\n-    if (!cvtDstTy.getEncoding().isa<ttg::DotOperandEncodingAttr>()) {\n-      builder.clone(op, mapping);\n-      continue;\n+    if (auto cvtOp = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n+      auto result = op.getResult(0);\n+      auto cvtDstTy = result.getType().cast<RankedTensorType>();\n+      if (cvtDstTy.getEncoding().isa<ttg::DotOperandEncodingAttr>()) {\n+        auto it =\n+            std::find(validLoads.begin(), validLoads.end(), op.getOperand(0));\n+        if (it != validLoads.end()) {\n+          // We replace the use new load use with a convert layout\n+          auto loadArgIdx = std::distance(validLoads.begin(), it);\n+          auto cvt = builder.create<ttg::ConvertLayoutOp>(\n+              result.getLoc(), cvtDstTy,\n+              newForOp.getRegionIterArgs()[loadIdx + loadArgIdx]);\n+          mapping.map(result, cvt.getResult());\n+          continue;\n+        }\n+      }\n     }\n-    auto cvt = builder.create<ttg::ConvertLayoutOp>(\n-        op.getResult(0).getLoc(), cvtDstTy,\n-        newForOp.getRegionIterArgs()[loadIdx + i]);\n-    mapping.map(op.getResult(0), cvt.getResult());\n+    cloneWithInferType(builder, &op, mapping);\n   }\n \n   return newForOp;"}, {"filename": "python/setup.py", "status": "modified", "additions": 9, "deletions": 4, "changes": 13, "file_content_changes": "@@ -66,10 +66,14 @@ def get_pybind11_package_info():\n \n \n def get_llvm_package_info():\n-    # download if nothing is installed\n+    # added statement for Apple Silicon\n     system = platform.system()\n+    arch = 'x86_64'\n     if system == \"Darwin\":\n         system_suffix = \"apple-darwin\"\n+        cpu_type = os.popen('sysctl machdep.cpu.brand_string').read()\n+        if \"apple\" in cpu_type.lower():\n+            arch = 'arm64'\n     elif system == \"Linux\":\n         vglibc = tuple(map(int, platform.libc_ver()[1].split('.')))\n         vglibc = vglibc[0] * 100 + vglibc[1]\n@@ -79,7 +83,7 @@ def get_llvm_package_info():\n         return Package(\"llvm\", \"LLVM-C.lib\", \"\", \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n     use_assert_enabled_llvm = check_env_flag(\"TRITON_USE_ASSERT_ENABLED_LLVM\", \"False\")\n     release_suffix = \"assert\" if use_assert_enabled_llvm else \"release\"\n-    name = f'llvm+mlir-17.0.0-x86_64-{system_suffix}-{release_suffix}'\n+    name = f'llvm+mlir-17.0.0-{arch}-{system_suffix}-{release_suffix}'\n     version = \"llvm-17.0.0-c5dede880d17\"\n     url = f\"https://github.com/ptillet/triton-llvm-releases/releases/download/{version}/{name}.tar.xz\"\n     return Package(\"llvm\", name, url, \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n@@ -118,10 +122,11 @@ def get_thirdparty_packages(triton_cache_path):\n \n \n def download_and_copy_ptxas():\n+\n     base_dir = os.path.dirname(__file__)\n     src_path = \"bin/ptxas\"\n-    version = \"12.1.105\"\n-    url = f\"https://conda.anaconda.org/nvidia/label/cuda-12.1.1/linux-64/cuda-nvcc-{version}-0.tar.bz2\"\n+    version = \"12.2.91\"\n+    url = f\"https://conda.anaconda.org/nvidia/label/cuda-12.2.0/linux-64/cuda-nvcc-{version}-0.tar.bz2\"\n     dst_prefix = os.path.join(base_dir, \"triton\")\n     dst_suffix = os.path.join(\"third_party\", \"cuda\", src_path)\n     dst_path = os.path.join(dst_prefix, dst_suffix)"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 76, "deletions": 37, "changes": 113, "file_content_changes": "@@ -65,7 +65,7 @@ enum backend_t {\n \n void init_triton_runtime(py::module &&m) {\n   // wrap backend_t\n-  py::enum_<backend_t>(m, \"backend\")\n+  py::enum_<backend_t>(m, \"backend\", py::module_local())\n       .value(\"HOST\", HOST)\n       .value(\"CUDA\", CUDA)\n       .value(\"ROCM\", ROCM)\n@@ -164,12 +164,14 @@ void init_triton_ir(py::module &&m) {\n   using ret = py::return_value_policy;\n   using namespace pybind11::literals;\n \n-  py::enum_<mlir::triton::PaddingOption>(m, \"PADDING_OPTION\")\n+  py::enum_<mlir::triton::PaddingOption>(m, \"PADDING_OPTION\",\n+                                         py::module_local())\n       .value(\"PAD_ZERO\", mlir::triton::PaddingOption::PAD_ZERO)\n       .value(\"PAD_NAN\", mlir::triton::PaddingOption::PAD_NAN)\n       .export_values();\n \n-  py::enum_<mlir::triton::CacheModifier>(m, \"CACHE_MODIFIER\")\n+  py::enum_<mlir::triton::CacheModifier>(m, \"CACHE_MODIFIER\",\n+                                         py::module_local())\n       .value(\"NONE\", mlir::triton::CacheModifier::NONE)\n       .value(\"CA\", mlir::triton::CacheModifier::CA)\n       .value(\"CG\", mlir::triton::CacheModifier::CG)\n@@ -178,20 +180,21 @@ void init_triton_ir(py::module &&m) {\n       .value(\"WT\", mlir::triton::CacheModifier::WT)\n       .export_values();\n \n-  py::enum_<mlir::triton::MemSemantic>(m, \"MEM_SEMANTIC\")\n+  py::enum_<mlir::triton::MemSemantic>(m, \"MEM_SEMANTIC\", py::module_local())\n       .value(\"ACQUIRE_RELEASE\", mlir::triton::MemSemantic::ACQUIRE_RELEASE)\n       .value(\"ACQUIRE\", mlir::triton::MemSemantic::ACQUIRE)\n       .value(\"RELEASE\", mlir::triton::MemSemantic::RELEASE)\n       .value(\"RELAXED\", mlir::triton::MemSemantic::RELAXED)\n       .export_values();\n \n-  py::enum_<mlir::triton::EvictionPolicy>(m, \"EVICTION_POLICY\")\n+  py::enum_<mlir::triton::EvictionPolicy>(m, \"EVICTION_POLICY\",\n+                                          py::module_local())\n       .value(\"NORMAL\", mlir::triton::EvictionPolicy::NORMAL)\n       .value(\"EVICT_FIRST\", mlir::triton::EvictionPolicy::EVICT_FIRST)\n       .value(\"EVICT_LAST\", mlir::triton::EvictionPolicy::EVICT_LAST)\n       .export_values();\n \n-  py::enum_<mlir::triton::RMWOp>(m, \"ATOMIC_OP\")\n+  py::enum_<mlir::triton::RMWOp>(m, \"ATOMIC_OP\", py::module_local())\n       .value(\"ADD\", mlir::triton::RMWOp::ADD)\n       .value(\"FADD\", mlir::triton::RMWOp::FADD)\n       .value(\"AND\", mlir::triton::RMWOp::AND)\n@@ -203,7 +206,7 @@ void init_triton_ir(py::module &&m) {\n       .value(\"UMIN\", mlir::triton::RMWOp::UMIN)\n       .value(\"UMAX\", mlir::triton::RMWOp::UMAX);\n \n-  py::class_<mlir::MLIRContext>(m, \"context\")\n+  py::class_<mlir::MLIRContext>(m, \"context\", py::module_local())\n       .def(py::init<>())\n       .def(\"load_triton\", [](mlir::MLIRContext &self) {\n         self.getOrLoadDialect<mlir::triton::TritonDialect>();\n@@ -259,7 +262,7 @@ void init_triton_ir(py::module &&m) {\n   // // py::class_<ir::undef_value, ir::constant>(m, \"undef\")\n   // //     .def(\"get\", &ir::undef_value::get, ret::reference);\n \n-  py::class_<mlir::Type>(m, \"type\")\n+  py::class_<mlir::Type>(m, \"type\", py::module_local())\n       .def(\"is_integer\", &mlir::Type::isInteger)\n       .def(\"is_fp16\", &mlir::Type::isF16)\n       .def(\"__str__\", [](mlir::Type &self) {\n@@ -269,21 +272,21 @@ void init_triton_ir(py::module &&m) {\n         return os.str();\n       });\n \n-  py::class_<mlir::FunctionType>(m, \"function_type\")\n+  py::class_<mlir::FunctionType>(m, \"function_type\", py::module_local())\n       .def(\"param_types\", [](mlir::FunctionType &self) {\n         return std::vector<mlir::Type>(self.getInputs().begin(),\n                                        self.getInputs().end());\n       });\n \n-  py::class_<mlir::Location>(m, \"location\")\n+  py::class_<mlir::Location>(m, \"location\", py::module_local())\n       .def(\"__str__\", [](mlir::Location &self) {\n         std::string str;\n         llvm::raw_string_ostream os(str);\n         self.print(os);\n         return os.str();\n       });\n \n-  py::class_<mlir::Value>(m, \"value\")\n+  py::class_<mlir::Value>(m, \"value\", py::module_local())\n       .def(\"set_attr\",\n            [](mlir::Value &self, std::string &name,\n               mlir::Attribute &attr) -> void {\n@@ -307,14 +310,15 @@ void init_triton_ir(py::module &&m) {\n            })\n       .def(\"get_type\", &mlir::Value::getType);\n \n-  py::class_<mlir::BlockArgument, mlir::Value>(m, \"block_argument\");\n+  py::class_<mlir::BlockArgument, mlir::Value>(m, \"block_argument\",\n+                                               py::module_local());\n \n-  py::class_<mlir::Region>(m, \"region\")\n+  py::class_<mlir::Region>(m, \"region\", py::module_local())\n       .def(\"get_parent_region\", &mlir::Region::getParentRegion, ret::reference)\n       .def(\"size\", [](mlir::Region &self) { return self.getBlocks().size(); })\n       .def(\"empty\", &mlir::Region::empty);\n \n-  py::class_<mlir::Block>(m, \"block\")\n+  py::class_<mlir::Block>(m, \"block\", py::module_local())\n       .def(\"arg\",\n            [](mlir::Block &self, int index) -> mlir::BlockArgument {\n              return self.getArgument(index);\n@@ -383,12 +387,14 @@ void init_triton_ir(py::module &&m) {\n   //     .value(\"retune\", eattr::retune)\n   //     .value(\"not_implemented\", eattr::not_implemented);\n \n-  py::class_<mlir::Attribute>(m, \"attribute\");\n-  py::class_<mlir::IntegerAttr, mlir::Attribute>(m, \"integer_attr\");\n-  py::class_<mlir::BoolAttr, mlir::Attribute>(m, \"bool_attr\");\n+  py::class_<mlir::Attribute>(m, \"attribute\", py::module_local());\n+  py::class_<mlir::IntegerAttr, mlir::Attribute>(m, \"integer_attr\",\n+                                                 py::module_local());\n+  py::class_<mlir::BoolAttr, mlir::Attribute>(m, \"bool_attr\",\n+                                              py::module_local());\n \n   // Ops\n-  py::class_<mlir::OpState>(m, \"OpState\")\n+  py::class_<mlir::OpState>(m, \"OpState\", py::module_local())\n       .def(\"set_attr\",\n            [](mlir::OpState &self, std::string &name,\n               mlir::Attribute &attr) -> void { self->setAttr(name, attr); })\n@@ -427,23 +433,27 @@ void init_triton_ir(py::module &&m) {\n         return mlir::succeeded(mlir::verify(self.getOperation()));\n       });\n   // scf Ops\n-  py::class_<mlir::scf::ForOp, mlir::OpState>(m, \"ForOp\")\n+  py::class_<mlir::scf::ForOp, mlir::OpState>(m, \"ForOp\", py::module_local())\n       .def(\"get_induction_var\", &mlir::scf::ForOp::getInductionVar);\n \n-  py::class_<mlir::scf::IfOp, mlir::OpState>(m, \"IfOp\")\n+  py::class_<mlir::scf::IfOp, mlir::OpState>(m, \"IfOp\", py::module_local())\n       .def(\"get_then_block\", &mlir::scf::IfOp::thenBlock, ret::reference)\n       .def(\"get_else_block\", &mlir::scf::IfOp::elseBlock, ret::reference)\n       .def(\"get_then_yield\", &mlir::scf::IfOp::thenYield)\n       .def(\"get_else_yield\", &mlir::scf::IfOp::elseYield);\n-  py::class_<mlir::scf::YieldOp, mlir::OpState>(m, \"YieldOp\");\n-  py::class_<mlir::scf::WhileOp, mlir::OpState>(m, \"WhileOp\")\n+  py::class_<mlir::scf::YieldOp, mlir::OpState>(m, \"YieldOp\",\n+                                                py::module_local());\n+  py::class_<mlir::scf::WhileOp, mlir::OpState>(m, \"WhileOp\",\n+                                                py::module_local())\n       .def(\"get_before\", &mlir::scf::WhileOp::getBefore, ret::reference)\n       .def(\"get_after\", &mlir::scf::WhileOp::getAfter, ret::reference);\n-  py::class_<mlir::scf::ConditionOp, mlir::OpState>(m, \"ConditionOp\");\n+  py::class_<mlir::scf::ConditionOp, mlir::OpState>(m, \"ConditionOp\",\n+                                                    py::module_local());\n \n   // dynamic_attr is used to transfer ownership of the MLIR context to the\n   // module\n-  py::class_<mlir::ModuleOp, mlir::OpState>(m, \"module\", py::dynamic_attr())\n+  py::class_<mlir::ModuleOp, mlir::OpState>(m, \"module\", py::module_local(),\n+                                            py::dynamic_attr())\n       .def(\"dump\", &mlir::ModuleOp::dump)\n       .def(\"str\",\n            [](mlir::ModuleOp &self) -> std::string {\n@@ -523,7 +533,8 @@ void init_triton_ir(py::module &&m) {\n       },\n       ret::take_ownership);\n \n-  py::class_<mlir::triton::FuncOp, mlir::OpState>(m, \"function\")\n+  py::class_<mlir::triton::FuncOp, mlir::OpState>(m, \"function\",\n+                                                  py::module_local())\n       // .def_property_readonly(\"attrs\", &ir::function::attrs)\n       // .def(\"add_attr\", &ir::function::add_attr);\n       .def(\"args\",\n@@ -571,9 +582,11 @@ void init_triton_ir(py::module &&m) {\n       .def_property_readonly(\"type\", &mlir::triton::FuncOp::getFunctionType)\n       .def(\"reset_type\", &mlir::triton::FuncOp::setType);\n \n-  py::class_<mlir::OpBuilder::InsertPoint>(m, \"InsertPoint\");\n+  py::class_<mlir::OpBuilder::InsertPoint>(m, \"InsertPoint\",\n+                                           py::module_local());\n \n-  py::class_<TritonOpBuilder>(m, \"builder\", py::dynamic_attr())\n+  py::class_<TritonOpBuilder>(m, \"builder\", py::module_local(),\n+                              py::dynamic_attr())\n       .def(py::init<mlir::MLIRContext *>())\n       // getters\n       .def(\"create_module\",\n@@ -643,6 +656,26 @@ void init_triton_ir(py::module &&m) {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n                  v, self.getBuilder().getI64Type()));\n            })\n+      .def(\"get_uint8\",\n+           [](TritonOpBuilder &self, uint64_t v) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n+                 v, self.getBuilder().getI8Type()));\n+           })\n+      .def(\"get_uint16\",\n+           [](TritonOpBuilder &self, uint64_t v) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n+                 v, self.getBuilder().getI16Type()));\n+           })\n+      .def(\"get_uint32\",\n+           [](TritonOpBuilder &self, uint64_t v) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n+                 v, self.getBuilder().getI32Type()));\n+           })\n+      .def(\"get_uint64\",\n+           [](TritonOpBuilder &self, uint64_t v) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n+                 v, self.getBuilder().getI64Type()));\n+           })\n       .def(\"get_bf16\",\n            [](TritonOpBuilder &self, float v) -> mlir::Value {\n              auto type = self.getBuilder().getBF16Type();\n@@ -1487,23 +1520,25 @@ void init_triton_ir(py::module &&m) {\n                                                          offsets);\n            });\n \n-  py::class_<mlir::PassManager>(m, \"pass_manager\")\n+  py::class_<mlir::PassManager>(m, \"pass_manager\", py::module_local())\n       .def(py::init<mlir::MLIRContext *>())\n       .def(\"enable_debug\",\n            [](mlir::PassManager &self) {\n+             if (!::triton::tools::getBoolEnv(\"MLIR_ENABLE_DUMP\"))\n+               return;\n+             self.getContext()->disableMultithreading();\n              auto printingFlags = mlir::OpPrintingFlags();\n              printingFlags.elideLargeElementsAttrs(16);\n              printingFlags.enableDebugInfo();\n+             auto print_always = [](mlir::Pass *, mlir::Operation *) {\n+               return true;\n+             };\n              self.enableIRPrinting(\n-                 /*shouldPrintBeforePass=*/nullptr,\n-                 /*shouldPrintAfterPass=*/\n-                 [](mlir::Pass *pass, mlir::Operation *) {\n-                   return ::triton::tools::getBoolEnv(\"MLIR_ENABLE_DUMP\");\n-                 },\n-                 /*printModuleScope=*/false,\n-                 /*printAfterOnlyOnChange=*/true,\n-                 /*printAfterOnlyOnFailure*/ false, llvm::dbgs(),\n-                 printingFlags);\n+                 /*shouldPrintBeforePass=*/print_always,\n+                 /*shouldPrintAfterPass=*/print_always,\n+                 /*printModuleScope=*/true,\n+                 /*printAfterOnlyOnChange=*/false,\n+                 /*printAfterOnlyOnFailure*/ true, llvm::dbgs(), printingFlags);\n            })\n       .def(\"run\",\n            [](mlir::PassManager &self, mlir::ModuleOp &mod) {\n@@ -1541,6 +1576,10 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::PassManager &self) {\n              self.addPass(mlir::triton::createCombineOpsPass());\n            })\n+      .def(\"add_reorder_broadcast_pass\",\n+           [](mlir::PassManager &self) {\n+             self.addPass(mlir::triton::createReorderBroadcastPass());\n+           })\n       .def(\"add_rewrite_tensor_pointer_pass\",\n            [](mlir::PassManager &self, int computeCapability) {\n              self.addPass(mlir::triton::createRewriteTensorPointerPass("}, {"filename": "python/test/kernel_comparison/kernels.yml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -23,7 +23,7 @@ name_and_extension:\n     extension: ptx\n   - name: _kernel_0d1d2d345d6d7c89c1011c\n     extension: ptx\n-  - name: _bwd_kernel_0d1d2d34d5d6d7d8d9d10d11d12d13d14d15c16d17d18d19c20d21d22d23c2425d26d27\n+  - name: _bwd_kernel_0d1d2d34d5d6d7d8d9d10d11d12d13d14d15d16c17d18d19d20c21d22d23d24c2526d27d\n     extension: ptx\n   - name: _fwd_kernel_0d1d2d34d5d6d7d8d9d10c11d12d13d14c15d16d17d18c19d20d21d22c2324d25d\n     extension: ptx"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 83, "deletions": 68, "changes": 151, "file_content_changes": "@@ -38,54 +38,40 @@ def nvsmi(attrs):\n mem_clocks = {'v100': 877, 'a100': 1215}\n \n matmul_data = {\n-    'v100': {\n-        # square\n-        (512, 512, 512): {'float16': 0.158},\n-        (1024, 1024, 1024): {'float16': 0.466},\n-        (2048, 2048, 2048): {'float16': 0.695},\n-        (4096, 4096, 4096): {'float16': 0.831},\n-        (8192, 8192, 8192): {'float16': 0.849},\n-        # tall-skinny\n-        (16, 1024, 1024): {'float16': 0.0128},\n-        (16, 4096, 4096): {'float16': 0.0883},\n-        (16, 8192, 8192): {'float16': 0.101},\n-        (64, 1024, 1024): {'float16': 0.073},\n-        (64, 4096, 4096): {'float16': 0.270},\n-        (64, 8192, 8192): {'float16': 0.459},\n-        (1024, 64, 1024): {'float16': 0.0692},\n-        (4096, 64, 4096): {'float16': 0.264},\n-        (8192, 64, 8192): {'float16': 0.452},\n-    },\n     # NOTE:\n-    # A100 in the CI server is slow-ish for some reason.\n-    # On some other servers, we are getting about 90% peak for 8kx8x8k float16\n     'a100': {\n-        (512, 512, 512): {'float16': 0.084, 'float32': 0.13, 'int8': 0.05},\n-        (1024, 1024, 1024): {'float16': 0.332, 'float32': 0.35, 'int8': 0.169},\n-        (2048, 2048, 2048): {'float16': 0.641, 'float32': 0.57, 'int8': 0.34},\n-        (4096, 4096, 4096): {'float16': 0.785, 'float32': 0.75, 'int8': 0.46},\n-        (8192, 8192, 8192): {'float16': 0.805, 'float32': 0.85, 'int8': 0.51},\n+        # square\n+        (512, 512, 512): {'float16': 0.061, 'float32': 0.097, 'int8': 0.05},\n+        (1024, 1024, 1024): {'float16': 0.283, 'float32': 0.313, 'int8': 0.169},\n+        (2048, 2048, 2048): {'float16': 0.618, 'float32': 0.532, 'int8': 0.34},\n+        (8192, 8192, 8192): {'float16': 0.786, 'float32': 0.754, 'int8': 0.51},\n         # tall-skinny\n-        (16, 1024, 1024): {'float16': 0.0077, 'float32': 0.0127, 'int8': 0.005},\n-        (16, 4096, 4096): {'float16': 0.044, 'float32': 0.0457, 'int8': 0.0259},\n-        (16, 8192, 8192): {'float16': 0.07, 'float32': 0.0648, 'int8': 0.0431},\n-        (64, 1024, 1024): {'float16': 0.028, 'float32': 0.0509, 'int8': 0.0169},\n-        (64, 4096, 4096): {'float16': 0.163, 'float32': 0.162, 'int8': 0.097},\n-        (64, 8192, 8192): {'float16': 0.285, 'float32': 0.257, 'int8': 0.174},\n-        (1024, 64, 1024): {'float16': 0.033, 'float32': 0.0458, 'int8': 0.017},\n-        (4096, 64, 4096): {'float16': 0.16, 'float32': 0.177, 'int8': 0.102},\n-        (8192, 64, 8192): {'float16': 0.254, 'float32': 0.230, 'int8': 0.177},\n+        (16, 1024, 1024): {'float16': 0.006, 'float32': 0.009, 'int8': 0.005},\n+        (16, 4096, 4096): {'float16': 0.057, 'float32': 0.051, 'int8': 0.026},\n+        (16, 8192, 8192): {'float16': 0.077, 'float32': 0.077, 'int8': 0.043},\n+        (64, 1024, 1024): {'float16': 0.018, 'float32': 0.023, 'int8': 0.017},\n+        (64, 4096, 4096): {'float16': 0.150, 'float32': 0.000, 'int8': 0.097},\n+        (64, 8192, 8192): {'float16': 0.338, 'float32': 0.000, 'int8': 0.174},\n+        (1024, 64, 1024): {'float16': 0.029, 'float32': 0.046, 'int8': 0.017},\n+        (4096, 64, 4096): {'float16': 0.179, 'float32': 0.214, 'int8': 0.102},\n+        (8192, 64, 8192): {'float16': 0.278, 'float32': 0.000, 'int8': 0.177},\n+        # test EVEN_K==False\n+        (8192, 8192, 8176): {'float16': 0.786, 'float32': 0.696, 'int8': 0.51},\n     }\n }\n \n \n @pytest.mark.parametrize('M, N, K, dtype_str',\n                          [(M, N, K, dtype_str)\n                           for M, N, K in matmul_data[DEVICE_NAME].keys()\n-                          for dtype_str in ['float16']])\n+                          for dtype_str in ['float16', 'float32']])\n def test_matmul(M, N, K, dtype_str):\n+    stream = torch.cuda.Stream()\n+    torch.cuda.set_stream(stream)\n     if dtype_str in ['float32', 'int8'] and DEVICE_NAME != 'a100':\n         pytest.skip('Only test float32 & int8 on a100')\n+    if (M, N, K) in [(64, 4096, 4096), (64, 8192, 8192), (8192, 64, 8192)] and dtype_str == 'float32':\n+        pytest.skip('Out of shared memory in float32')\n     dtype = {'float16': torch.float16, 'float32': torch.float32, 'int8': torch.int8}[dtype_str]\n     torch.manual_seed(0)\n     ref_gpu_util = matmul_data[DEVICE_NAME][(M, N, K)][dtype_str]\n@@ -99,11 +85,11 @@ def test_matmul(M, N, K, dtype_str):\n         a = torch.randn((M, K), dtype=dtype, device='cuda')\n         b = torch.randn((K, N), dtype=dtype, device='cuda')\n     fn = lambda: triton.ops.matmul(a, b)\n-    ms = triton.testing.do_bench(fn, return_mode=\"min\", warmup=100, rep=300)\n+    ms = triton.testing.do_bench_cudagraph(fn)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     print_perf(ms, cur_gpu_util, ref_gpu_util)\n-    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.02, rtol=0.01)\n \n \n #######################\n@@ -125,42 +111,42 @@ def _add(x_ptr, y_ptr, output_ptr, n_elements,\n \n \n elementwise_data = {\n-    'v100': {\n-        1024 * 16: 0.0219,\n-        1024 * 64: 0.0791,\n-        1024 * 256: 0.243,\n-        1024 * 1024: 0.530,\n-        1024 * 4096: 0.796,\n-        1024 * 16384: 0.905,\n-        1024 * 65536: 0.939,\n-    },\n     'a100': {\n-        1024 * 16: 0.010,\n-        1024 * 64: 0.040,\n-        1024 * 256: 0.132,\n-        1024 * 1024: 0.353,\n-        1024 * 4096: 0.605,\n-        1024 * 16384: 0.758,\n-        1024 * 65536: 0.850,\n+        1024 * 16: {'float16': 0.003, 'float32': 0.007},\n+        1024 * 64: {'float16': 0.013, 'float32': 0.026},\n+        1024 * 256: {'float16': 0.053, 'float32': 0.105},\n+        1024 * 1024: {'float16': 0.212, 'float32': 0.420},\n+        1024 * 16384: {'float16': 0.762, 'float32': 0.812},\n+        1024 * 65536: {'float16': 0.846, 'float32': 0.869},\n+        # Non pow 2\n+        1020 * 100: {'float16': 0.020, 'float32': 0.041},\n+        10003 * 7007: {'float16': 0.513, 'float32': 0.861},\n     }\n }\n \n \n @pytest.mark.parametrize('N', elementwise_data[DEVICE_NAME].keys())\n-def test_elementwise(N):\n+@pytest.mark.parametrize(\"dtype_str\", ['float16', 'bfloat16', 'float32'])\n+def test_elementwise(N, dtype_str):\n+    stream = torch.cuda.Stream()\n+    torch.cuda.set_stream(stream)\n     torch.manual_seed(0)\n-    ref_gpu_util = elementwise_data[DEVICE_NAME][N]\n+    if dtype_str in ['bfloat16'] and DEVICE_NAME != 'a100':\n+        pytest.skip('Only test bfloat16 on a100')\n+    dtype = {'float16': torch.float16, 'bfloat16': torch.bfloat16, 'float32': torch.float32}[dtype_str]\n+    ref_dtype_str = 'float16' if dtype_str == 'bfloat16' else dtype_str\n+    ref_gpu_util = elementwise_data[DEVICE_NAME][N][ref_dtype_str]\n     max_gpu_perf = get_dram_gbps()\n-    z = torch.empty((N, ), dtype=torch.float16, device='cuda')\n+    z = torch.empty((N, ), dtype=dtype, device='cuda')\n     x = torch.randn_like(z)\n     y = torch.randn_like(z)\n     grid = lambda args: (triton.cdiv(N, args['BLOCK_SIZE']), )\n     fn = lambda: _add[grid](x, y, z, N, BLOCK_SIZE=1024)\n-    ms = triton.testing.do_bench(fn, return_mode=\"min\", warmup=100, rep=500)\n+    ms = triton.testing.do_bench_cudagraph(fn)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     print_perf(ms, cur_gpu_util, ref_gpu_util)\n-    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.02, rtol=0.01)\n \n #######################\n # Flash-Attention\n@@ -169,34 +155,63 @@ def test_elementwise(N):\n \n flash_attention_data = {\n     \"a100\": {\n-        (4, 48, 4096, 64, 'forward', 'float16'): 0.37,\n-        (4, 48, 4096, 64, 'backward', 'float16'): 0.25,\n+        (4, 48, 4096, 64, True, True, 'forward', 'float16'): 0.433,\n+        (4, 48, 4096, 64, True, True, 'forward', 'bfloat16'): 0.392,\n+        (4, 48, 1024, 16, True, True, 'forward', 'float32'): 0.106,\n+        (4, 48, 4096, 64, True, True, 'backward', 'float16'): 0.204,\n+        (4, 48, 4096, 64, True, True, 'backward', 'bfloat16'): 0.202,\n+        (4, 48, 1024, 16, True, True, 'backward', 'float32'): 0.089,\n+        (4, 48, 4096, 64, True, False, 'forward', 'float16'): 0.242,\n+        (4, 48, 4096, 64, True, False, 'forward', 'bfloat16'): 0.220,\n+        (4, 48, 1024, 16, True, False, 'forward', 'float32'): 0.069,\n+        (4, 48, 4096, 64, True, False, 'backward', 'float16'): 0.136,\n+        (4, 48, 4096, 64, True, False, 'backward', 'bfloat16'): 0.135,\n+        (4, 48, 1024, 16, True, False, 'backward', 'float32'): 0.052,\n+        (4, 48, 4096, 64, False, True, 'forward', 'float16'): 0.432,\n+        (4, 48, 4096, 64, False, True, 'forward', 'bfloat16'): 0.392,\n+        (4, 48, 1024, 16, False, True, 'forward', 'float32'): 0.107,\n+        (4, 48, 4096, 64, False, True, 'backward', 'float16'): 0.265,\n+        (4, 48, 4096, 64, False, True, 'backward', 'bfloat16'): 0.257,\n+        (4, 48, 1024, 16, False, True, 'backward', 'float32'): 0.128,\n+        (4, 48, 4096, 64, False, False, 'forward', 'float16'): 0.251,\n+        (4, 48, 4096, 64, False, False, 'forward', 'bfloat16'): 0.220,\n+        (4, 48, 1024, 16, False, False, 'forward', 'float32'): 0.069,\n+        (4, 48, 4096, 64, False, False, 'backward', 'float16'): 0.159,\n+        (4, 48, 4096, 64, False, False, 'backward', 'bfloat16'): 0.138,\n+        (4, 48, 1024, 16, False, False, 'backward', 'float32'): 0.076,\n     }\n }\n \n \n-@pytest.mark.parametrize(\"Z, H, N_CTX, D_HEAD\", [[4, 48, 4096, 64]])\n+@pytest.mark.parametrize(\"dtype_str\", ['float16', 'bfloat16', 'float32'])\n @pytest.mark.parametrize(\"mode\", ['forward', 'backward'])\n-@pytest.mark.parametrize(\"dtype_str\", ['float16'])\n-def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n+@pytest.mark.parametrize(\"causal\", [True, False])\n+@pytest.mark.parametrize(\"seq_par\", [True, False])\n+@pytest.mark.parametrize(\"Z, H, N_CTX, D_HEAD\", [[4, 48, 4096, 64]])\n+def test_flash_attention(Z, H, N_CTX, D_HEAD, seq_par, causal, mode, dtype_str):\n+    stream = torch.cuda.Stream()\n+    torch.cuda.set_stream(stream)\n     is_backward = mode == 'backward'\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 8:\n         pytest.skip(\"Flash attention only supported for compute capability < 80\")\n     torch.manual_seed(20)\n-    dtype = {'float16': torch.float16, 'float32': torch.float32, 'int8': torch.int8}[dtype_str]\n+    dtype = {'float16': torch.float16, 'bfloat16': torch.bfloat16, 'float32': torch.float32}[dtype_str]\n     # init data\n+    if dtype_str == 'float32':\n+        N_CTX = 1024\n+        D_HEAD = 16\n     q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n     k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n     v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n     sm_scale = 0.2\n     # benchmark\n-    fn = lambda: triton.ops.attention(q, k, v, sm_scale)\n+    fn = lambda: triton.ops.attention(q, k, v, causal, sm_scale, seq_par)\n     if is_backward:\n         o = fn()\n         do = torch.randn_like(o)\n         fn = lambda: o.backward(do, retain_graph=True)\n-    ms = triton.testing.do_bench(fn, return_mode=\"min\", warmup=100, rep=500)\n+    ms = triton.testing.do_bench_cudagraph(fn)\n     # compute flops\n     flops_per_matmul = 2. * Z * H * N_CTX * N_CTX * D_HEAD * 0.5\n     total_flops = 2 * flops_per_matmul\n@@ -207,6 +222,6 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n     cur_sm_clock = nvsmi(['clocks.current.sm'])[0]\n     max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, mode, dtype_str)]\n+    ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, seq_par, causal, mode, dtype_str)]\n     print_perf(ms, cur_gpu_util, ref_gpu_util)\n-    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.02, rtol=0.01)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 81, "deletions": 47, "changes": 128, "file_content_changes": "@@ -119,18 +119,18 @@ def check_type_supported(dtype, device):\n class MmaLayout:\n     def __init__(self, version, warps_per_cta):\n         self.version = version\n-        self.warps_per_cta = str(warps_per_cta)\n+        self.warps_per_cta = warps_per_cta\n \n     def __str__(self):\n         return f\"#triton_gpu.mma<{{versionMajor={self.version[0]}, versionMinor={self.version[1]}, warpsPerCTA={self.warps_per_cta}}}>\"\n \n \n class BlockedLayout:\n     def __init__(self, size_per_thread, threads_per_warp, warps_per_cta, order):\n-        self.sz_per_thread = str(size_per_thread)\n-        self.threads_per_warp = str(threads_per_warp)\n-        self.warps_per_cta = str(warps_per_cta)\n-        self.order = str(order)\n+        self.sz_per_thread = size_per_thread\n+        self.threads_per_warp = threads_per_warp\n+        self.warps_per_cta = warps_per_cta\n+        self.order = order\n \n     def __str__(self):\n         return f\"#triton_gpu.blocked<{{sizePerThread={self.sz_per_thread}, threadsPerWarp={self.threads_per_warp}, warpsPerCTA={self.warps_per_cta}, order={self.order}}}>\"\n@@ -1101,6 +1101,7 @@ def serialized_add(data, Lock, SEM: tl.constexpr):\n     ('bfloat16', 'float32', False),\n     ('float32', 'int32', True),\n     ('float32', 'int1', False),\n+    ('int8', 'bfloat16', False),\n ] + [\n     (f'uint{x}', f'int{x}', True) for x in [8, 16, 32, 64]\n ] + [\n@@ -1111,43 +1112,46 @@ def test_cast(dtype_x, dtype_z, bitcast, device):\n     check_type_supported(dtype_x, device)\n     check_type_supported(dtype_z, device)\n \n+    size = 1024\n     # This is tricky because numpy doesn't have bfloat, and torch doesn't have uints.\n-    x0 = 43 if dtype_x in int_dtypes else 43.5\n-    if dtype_x in float_dtypes and dtype_z == 'int1':\n-        x0 = 0.5\n     if dtype_x.startswith('bfloat'):\n-        x_tri = torch.tensor([x0], dtype=getattr(torch, dtype_x), device=device)\n+        x_tri = torch.randn(size, dtype=getattr(torch, dtype_x), device=device)\n     else:\n-        x = np.array([x0], dtype=getattr(np, dtype_x))\n+        x = numpy_random(size, dtype_str=dtype_x, low=-10, high=10) * 10\n+        # Triton clamps negative values to zero, while numpy wraps around\n+        # intmax, so avoid negatives for now.\n+        # TODO: figure out which one should actually be happening, and test it\n+        if dtype_z in uint_dtypes:\n+            x = np.absolute(x)\n         x_tri = to_triton(x, device=device)\n \n     # triton kernel\n     @triton.jit\n-    def kernel(X, Z, BITCAST: tl.constexpr):\n-        x_ptr = X + tl.arange(0, 1)\n-        z_ptr = Z + tl.arange(0, 1)\n+    def kernel(X, Z, BITCAST: tl.constexpr, SIZE: tl.constexpr):\n+        x_ptr = X + tl.arange(0, SIZE)\n+        z_ptr = Z + tl.arange(0, SIZE)\n         x = tl.load(x_ptr)\n         z = x.to(Z.dtype.element_ty, bitcast=BITCAST)\n         tl.store(z_ptr, z)\n \n     dtype_z_np = dtype_z if dtype_z != 'int1' else 'bool_'\n     # triton result\n     if dtype_z.startswith('bfloat'):\n-        z_tri = torch.empty((1,), dtype=getattr(torch, dtype_z), device=device)\n+        z_tri = torch.empty((size,), dtype=getattr(torch, dtype_z), device=device)\n     else:\n-        z_tri = to_triton(np.empty((1, ), dtype=getattr(np, dtype_z_np)), device=device)\n-    kernel[(1, )](x_tri, z_tri, BITCAST=bitcast)\n+        z_tri = to_triton(np.empty((size, ), dtype=getattr(np, dtype_z_np)), device=device)\n+    kernel[(1, )](x_tri, z_tri, BITCAST=bitcast, SIZE=size, num_warps=1)\n     # torch result\n     if dtype_z.startswith('bfloat') or dtype_x.startswith('bfloat'):\n         assert bitcast is False\n         z_ref = x_tri.to(z_tri.dtype)\n-        assert z_tri == z_ref\n+        torch.testing.assert_close(z_ref, z_tri, rtol=0, atol=0)\n     else:\n         if bitcast:\n             z_ref = x.view(getattr(np, dtype_z_np))\n         else:\n             z_ref = x.astype(getattr(np, dtype_z_np))\n-        assert to_numpy(z_tri) == z_ref\n+        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0, atol=0)\n \n \n @pytest.mark.parametrize(\"dtype_str, num_warps\", [(dtype_str, num_warps) for dtype_str in int_dtypes + float_dtypes for num_warps in [4, 8]])\n@@ -1614,12 +1618,13 @@ def test_scan_layouts(M, N, src_layout, axis, device):\n layouts = [\n     BlockedLayout([1, 4], [8, 4], [4, 1], [1, 0]),\n     BlockedLayout([1, 4], [8, 4], [4, 1], [0, 1]),\n+    BlockedLayout([4, 4], [2, 16], [4, 1], [1, 0]),\n     MmaLayout(version=(2, 0), warps_per_cta=[4, 1]),\n     MmaLayout(version=(2, 0), warps_per_cta=[2, 2])\n ]\n \n \n-@pytest.mark.parametrize(\"M, N\", [[128, 16], [128, 128], [32, 128]])\n+@pytest.mark.parametrize(\"M, N\", [[128, 16], [128, 128], [32, 128], [32, 32]])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n @pytest.mark.parametrize(\"axis\", [0, 1])\n def test_reduce_layouts(M, N, src_layout, axis, device):\n@@ -1630,31 +1635,30 @@ def test_reduce_layouts(M, N, src_layout, axis, device):\n     #blocked = #triton_gpu.blocked<{{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}}>\n     #src = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n-    tt.func public @kernel_0d1d2c3d4c(%arg0: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}, %arg1: i32 {{tt.divisibility = 16 : i32}}, %arg2: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}) {{\n+    tt.func public @kernel_0d1d2c3d4c(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: i32 {{tt.divisibility = 16 : i32}}, %arg2: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n         %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>\n         %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>) -> tensor<{M}x1xi32, #blocked>\n         %2 = tt.splat %arg1 : (i32) -> tensor<{M}x1xi32, #blocked>\n         %3 = arith.muli %1, %2 : tensor<{M}x1xi32, #blocked>\n-        %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<{M}x1x!tt.ptr<f32>, #blocked>\n-        %5 = tt.addptr %4, %3 : tensor<{M}x1x!tt.ptr<f32>, #blocked>, tensor<{M}x1xi32, #blocked>\n+        %4 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x1x!tt.ptr<i32>, #blocked>\n+        %5 = tt.addptr %4, %3 : tensor<{M}x1x!tt.ptr<i32>, #blocked>, tensor<{M}x1xi32, #blocked>\n         %6 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>\n         %7 = tt.expand_dims %6 {{axis = 0 : i32}} : (tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>) -> tensor<1x{N}xi32, #blocked>\n-        %8 = tt.broadcast %5 : (tensor<{M}x1x!tt.ptr<f32>, #blocked>) -> tensor<{M}x{N}x!tt.ptr<f32>, #blocked>\n+        %8 = tt.broadcast %5 : (tensor<{M}x1x!tt.ptr<i32>, #blocked>) -> tensor<{M}x{N}x!tt.ptr<i32>, #blocked>\n         %9 = tt.broadcast %7 : (tensor<1x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #blocked>\n-        %10 = tt.addptr %8, %9 : tensor<{M}x{N}x!tt.ptr<f32>, #blocked>, tensor<{M}x{N}xi32, #blocked>\n-        %11 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<{rdims_2d}x!tt.ptr<f32>, #blocked>\n-        %12 = tt.addptr %11, {store_range} : tensor<{rdims_2d}x!tt.ptr<f32>, #blocked>, tensor<{rdims_2d}xi32, #blocked>\n-        %13 = tt.load %10 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xf32, #blocked>\n-        %14 = triton_gpu.convert_layout %13 : (tensor<{M}x{N}xf32, #blocked>) -> tensor<{M}x{N}xf32, #src>\n+        %10 = tt.addptr %8, %9 : tensor<{M}x{N}x!tt.ptr<i32>, #blocked>, tensor<{M}x{N}xi32, #blocked>\n+        %11 = tt.splat %arg2 : (!tt.ptr<i32>) -> tensor<{rdims_2d}x!tt.ptr<i32>, #blocked>\n+        %12 = tt.addptr %11, {store_range} : tensor<{rdims_2d}x!tt.ptr<i32>, #blocked>, tensor<{rdims_2d}xi32, #blocked>\n+        %13 = tt.load %10 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xi32, #blocked>\n+        %14 = triton_gpu.convert_layout %13 : (tensor<{M}x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #src>\n         %15 = \"tt.reduce\"(%14) ({{\n-        ^bb0(%arg3: f32, %arg4: f32):\n-          %16 = \"triton_gpu.cmpf\"(%arg3, %arg4) {{predicate = 2 : i64}} : (f32, f32) -> i1\n-          %17 = arith.select %16, %arg3, %arg4 : f32\n-          tt.reduce.return %17 : f32\n-        }}) {{axis = {axis} : i32}} : (tensor<{M}x{N}xf32, #src>) -> tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>\n-        %18 = triton_gpu.convert_layout %15 : (tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>) -> tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>\n-        %19 = tt.expand_dims %18 {{axis = {axis} : i32}} : (tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>) -> tensor<{rdims_2d}xf32, #blocked>\n-        tt.store %12, %19 {{cache = 1 : i32, evict = 1 : i32}} : tensor<{rdims_2d}xf32, #blocked>\n+        ^bb0(%arg3: i32, %arg4: i32):\n+          %17 = arith.addi %arg3, %arg4 : i32\n+          tt.reduce.return %17 : i32\n+        }}) {{axis = {axis} : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>\n+        %18 = triton_gpu.convert_layout %15 : (tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>) -> tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>\n+        %19 = tt.expand_dims %18 {{axis = {axis} : i32}} : (tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>) -> tensor<{rdims_2d}xi32, #blocked>\n+        tt.store %12, %19 {{cache = 1 : i32, evict = 1 : i32}} : tensor<{rdims_2d}xi32, #blocked>\n         tt.return\n     }}\n     }}\n@@ -1667,22 +1671,21 @@ def test_reduce_layouts(M, N, src_layout, axis, device):\n         kernel = triton.compile(f.name)\n \n     rs = RandomState(17)\n-    x = rs.randint(0, 4, (M, N)).astype('float32')\n-    x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+    x = rs.randint(0, 20, (M, N)).astype('int32')\n \n     if axis == 0:\n-        z = np.zeros((1, N)).astype('float32')\n+        z = np.zeros((1, N)).astype('int32')\n     else:\n-        z = np.zeros((M, 1)).astype('float32')\n+        z = np.zeros((M, 1)).astype('int32')\n \n     x_tri = torch.tensor(x, device=device)\n     z_tri = torch.tensor(z, device=device)\n \n     pgm = kernel[(1, 1, 4)](x_tri, x_tri.stride(0), z_tri)\n \n-    z_ref = np.max(x, axis=axis, keepdims=True)\n+    z_ref = np.sum(x, axis=axis, keepdims=True)\n \n-    np.testing.assert_allclose(z_ref, z_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n+    np.testing.assert_equal(z_ref, z_tri.cpu().numpy())\n \n \n layouts = [\n@@ -2073,7 +2076,6 @@ def kernel(X, stride_xm, stride_xk,\n         out_dtype = tl.float16\n     else:\n         out_dtype = tl.float32\n-\n     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n                          y_tri, y_tri.stride(0), y_tri.stride(1),\n                          w_tri, w_tri.stride(0), w_tri.stride(1),\n@@ -2088,6 +2090,14 @@ def kernel(X, stride_xm, stride_xk,\n                          CHAIN_DOT=epilogue == 'chain-dot',\n                          ALLOW_TF32=allow_tf32,\n                          num_warps=num_warps)\n+    if epilogue == 'softmax' and (in_dtype != 'float32' or allow_tf32):\n+        ptx = pgm.asm[\"ptx\"]\n+        start = ptx.find(\"shfl.sync\")\n+        end = ptx.find(\"cvt.rn.f16.f32\")\n+        red_code = ptx[start:end]\n+        assert len(red_code) > 0\n+        assert \"shared\" not in red_code\n+        assert \"bar.sync\" not in red_code\n     # torch result\n     if in_dtype == 'int8':\n         z_ref = np.matmul(x.astype(np.float32),\n@@ -2172,9 +2182,13 @@ def kernel(Z, X, Y,\n     assert \"triton_gpu.async_wait {num = 2 : i32}\" in h.asm['ttgir']\n \n \n-@pytest.mark.parametrize(\"dtype_str\", int_dtypes + float_dtypes + ['bfloat16'])\n+@pytest.mark.parametrize(\"dtype_str\", int_dtypes + uint_dtypes + float_dtypes + ['bfloat16'])\n def test_full(dtype_str, device):\n-    dtype = getattr(torch, dtype_str)\n+    if dtype_str in uint_dtypes and not hasattr(torch, dtype_str):\n+        # PyTorch only has unsigned 8, but not 16, 32, or 64\n+        dtype = getattr(torch, dtype_str[1:])  # uintx -> intx\n+    else:\n+        dtype = getattr(torch, dtype_str)\n     check_type_supported(dtype, device)  # bfloat16 on cc < 80 will not be tested\n \n     @triton.jit\n@@ -2479,14 +2493,18 @@ def test_default(device):\n     ret1 = torch.zeros(1, dtype=torch.int32, device=device)\n \n     @triton.jit\n-    def _kernel(ret0, ret1, value):\n+    def _kernel(ret0, ret1, value=3):\n         tl.store(ret0, _impl())\n         tl.store(ret1, _impl(value))\n \n     _kernel[(1,)](ret0, ret1, value)\n     assert ret0.item() == 10\n     assert ret1.item() == value\n \n+    _kernel[(1,)](ret0, ret1)\n+    assert ret0.item() == 10\n+    assert ret1.item() == 3\n+\n # ---------------\n # test noop\n # ----------------\n@@ -2677,7 +2695,7 @@ def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n # -------------\n \n \n-@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and\"])\n+@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and_dynamic\", \"if_and_static\"])\n def test_if(if_type, device):\n \n     @triton.jit\n@@ -3064,6 +3082,22 @@ def kernel(InitI, Bound, CutOff, OutI, OutInitI, OutJ):\n     assert out_i[0] == init_i[0] + 1\n     assert out_j[0] == bound[0]\n \n+\n+def test_while(device):\n+    @triton.jit\n+    def nested_while(data, countPtr):\n+        for i in range(10):\n+            count = tl.load(countPtr)\n+            while count > 0:\n+                tl.store(data, tl.load(data) + 1.0)\n+                count = count - 2\n+\n+    counter = torch.tensor([8], dtype=torch.int32, device=device)\n+    data = torch.zeros((1,), device=device, dtype=torch.float32)\n+    nested_while[(1,)](data, counter)\n+    assert data[0] == 40\n+\n+\n # def test_for_if(device):\n \n #     @triton.jit"}, {"filename": "python/test/unit/operators/test_flash_attention.py", "status": "modified", "additions": 10, "deletions": 9, "changes": 19, "file_content_changes": "@@ -10,22 +10,23 @@\n                                                  (4, 48, 1024, 64),\n                                                  (4, 48, 1024, 128)])\n @pytest.mark.parametrize('dtype', [torch.float16, torch.bfloat16])\n-def test_op(Z, H, N_CTX, D_HEAD, dtype):\n+@pytest.mark.parametrize('causal', [True, False])\n+@pytest.mark.parametrize('seq_par', [True, False])\n+def test_op(Z, H, N_CTX, D_HEAD, dtype, causal, seq_par):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 8:\n         pytest.skip(\"Flash attention only supported for compute capability < 80\")\n     torch.manual_seed(20)\n-    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n-    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n-    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n-    sm_scale = 0.2\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    sm_scale = 0.5\n     dout = torch.randn_like(q)\n     # reference implementation\n     M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n     p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n-    for z in range(Z):\n-        for h in range(H):\n-            p[:, :, M == 0] = float(\"-inf\")\n+    if causal:\n+        p[:, :, M == 0] = float(\"-inf\")\n     p = torch.softmax(p.float(), dim=-1).to(dtype)\n     # p = torch.exp(p)\n     ref_out = torch.matmul(p, v)\n@@ -34,7 +35,7 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype):\n     ref_dk, k.grad = k.grad.clone(), None\n     ref_dq, q.grad = q.grad.clone(), None\n     # # triton implementation\n-    tri_out = triton.ops.attention(q, k, v, sm_scale)\n+    tri_out = triton.ops.attention(q, k, v, causal, sm_scale, seq_par)\n     # print(ref_out)\n     # print(tri_out)\n     tri_out.backward(dout)"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 38, "deletions": 18, "changes": 56, "file_content_changes": "@@ -8,20 +8,20 @@\n import triton.ops\n \n \n-def f8_to_f16(x):\n+def f8_to_f16(x, dtype):\n \n     @triton.jit\n     def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n         pid = tl.program_id(0)\n         offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n         mask = offs < N\n         x = tl.load(X + offs, mask=mask)\n-        y = x.to(tl.float8e5)\n-        tl.store(Y + offs, y, mask=mask)\n+        tl.store(Y + offs, x, mask=mask)\n \n     ret = torch.empty(x.shape, dtype=torch.float16, device=x.device)\n     grid = lambda META: (triton.cdiv(x.numel(), META['BLOCK_SIZE']),)\n-    kernel[grid](ret, triton.reinterpret(x, tl.float8e5), ret.numel(), BLOCK_SIZE=1024)\n+    dtype = getattr(tl, dtype)\n+    kernel[grid](ret, triton.reinterpret(x, dtype), ret.numel(), BLOCK_SIZE=1024)\n     return ret\n \n \n@@ -85,13 +85,17 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n         # mixed-precision\n         *[\n             [\n-                (16, 16, 16, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n-                (128, 32, 32, 1, 2, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n-                (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n-                (32, 64, 16, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n+                (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n+                (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n+                (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n                 (128, 128, 32, 8, 4, 2, 256, 256, 128, AT, BT, ADTYPE, BDTYPE),\n-            ] for ADTYPE, BDTYPE in [(\"float8\", \"float16\"), (\"float16\", \"float32\"), (\"float32\", \"float16\"),\n-                                     (\"bfloat16\", \"float32\"), (\"float32\", \"bfloat16\")] for AT in [False, True] for BT in [False, True]\n+            ] for ADTYPE, BDTYPE in [(\"float8e4b15\", \"float8e5\"),\n+                                     (\"float8e4\", \"float16\"),\n+                                     (\"float16\", \"float8e5\"),\n+                                     (\"float16\", \"float32\"),\n+                                     (\"float32\", \"float16\"),\n+                                     (\"bfloat16\", \"float32\"),\n+                                     (\"float32\", \"bfloat16\")] for AT in [False, True] for BT in [False, True]\n         ]\n     ),\n )\n@@ -116,22 +120,38 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT,\n     M = BLOCK_M if M is None else M\n     N = BLOCK_N if N is None else N\n     K = BLOCK_K * SPLIT_K if K is None else K\n+    a_fp8 = \"float8\" in ADTYPE\n+    b_fp8 = \"float8\" in BDTYPE\n \n-    def get_input(n, m, t, dtype):\n+    def maybe_upcast(x, dtype, is_float8):\n+        if is_float8:\n+            return f8_to_f16(x, dtype)\n+        return x\n+\n+    def init_input(n, m, t, dtype, is_float8):\n         if t:\n-            return get_input(m, n, False, dtype).t()\n-        if dtype == \"float8\":\n-            x = torch.randint(10, 50, (n, m), device=\"cuda\", dtype=torch.int8)\n-            return f8_to_f16(x)\n+            return init_input(m, n, False, dtype, is_float8).t()\n+        if is_float8:\n+            return torch.randint(20, 60, (n, m), device=\"cuda\", dtype=torch.int8)\n         dtype = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[dtype]\n         return .1 * torch.randn((n, m), device=\"cuda\", dtype=dtype)\n \n     # allocate/transpose inputs\n-    a = get_input(M, K, AT, ADTYPE)\n-    b = get_input(K, N, BT, BDTYPE)\n+    a = init_input(M, K, AT, ADTYPE, a_fp8)\n+    b = init_input(K, N, BT, BDTYPE, b_fp8)\n     # run test\n-    th_c = torch.matmul(a.to(torch.float32), b.to(torch.float32))\n+    th_a = maybe_upcast(a, ADTYPE, a_fp8).to(torch.float32)\n+    if AT and a_fp8:\n+        th_a = th_a.view(th_a.shape[::-1]).T\n+    th_b = maybe_upcast(b, BDTYPE, b_fp8).to(torch.float32)\n+    if BT and b_fp8:\n+        th_b = th_b.view(th_b.shape[::-1]).T\n+    th_c = torch.matmul(th_a, th_b)\n     try:\n+        if a_fp8:\n+            a = triton.reinterpret(a, getattr(tl, ADTYPE))\n+        if b_fp8:\n+            b = triton.reinterpret(b, getattr(tl, BDTYPE))\n         tt_c = triton.ops.matmul(a, b)\n         atol, rtol = 1e-2, 0\n         if ADTYPE == torch.bfloat16 or BDTYPE == torch.bfloat16:"}, {"filename": "python/test/unit/tools/test_aot.py", "status": "modified", "additions": 4, "deletions": 5, "changes": 9, "file_content_changes": "@@ -92,7 +92,7 @@ def kernel(C, A, B,\n   cuMemAlloc(&B, K * N * 2);\n   cuMemAlloc(&C, M * N * 4);\n   cuStreamCreate(&stream, 0);\n-  load_kernel();\n+  load_matmul_fp16xfp16_16x16x16();\n \n   // initialize input data\n   int16_t hA[M*K];\n@@ -105,10 +105,9 @@ def kernel(C, A, B,\n   cuMemcpyHtoD(B, hB, K*N*2);\n \n   // launch kernel\n-  int numWarps = 1;\n   int gX = 1, gY = 1, gZ = 1;\n   cuStreamSynchronize(stream);\n-  kernel(stream, M/BM, N/BN, 1, numWarps, C, A, B, N, K, N);\n+  matmul_fp16xfp16_16x16x16(stream, M/BM, N/BN, 1, C, A, B, N, K, N);\n   cuStreamSynchronize(stream);\n \n   // read data\n@@ -119,7 +118,7 @@ def kernel(C, A, B,\n \n \n   // free cuda handles\n-  unload_kernel();\n+  unload_matmul_fp16xfp16_16x16x16();\n   cuMemFree(A);\n   cuMemFree(B);\n   cuMemFree(C);\n@@ -153,7 +152,7 @@ def test_compile_link_matmul():\n             for hb in hints:\n                 sig = f'*fp32:16, *{dtype}:16, *{dtype}:16, i32{ha}, 1, i32{hb}, 1, i32:16, 1, {BM}, {BN}, {BK}'\n                 name = f\"matmul_{dtype}x{dtype}_{BM}x{BN}x{BK}\"\n-                subprocess.run([sys.executable, compiler_path, \"-n\", \"kernel\", \"--signature\", sig, \"--out-name\", name, \"-o\", name, kernel_path], check=True, cwd=tmp_dir)\n+                subprocess.run([sys.executable, compiler_path, \"-n\", \"kernel\", \"--signature\", sig, \"--out-name\", name, \"-o\", name, \"-w\", \"1\", kernel_path], check=True, cwd=tmp_dir)\n \n         # link all desired configs\n         h_files = glob.glob(os.path.join(tmp_dir, \"*.h\"))"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -659,6 +659,7 @@ def visit_UnaryOp(self, node):\n     def visit_While(self, node):\n         with enter_sub_region(self) as sr:\n             liveins, insert_block = sr\n+            ip, last_loc = self._get_insertion_point_and_loc()\n \n             # loop body (the after region)\n             # loop_block = self.builder.create_block()\n@@ -668,6 +669,7 @@ def visit_While(self, node):\n             self.visit_compound_statement(node.body)\n             self.scf_stack.pop()\n             loop_defs = self.local_defs\n+            dummy.erase()\n \n             # collect loop-carried values\n             names = []\n@@ -684,7 +686,7 @@ def visit_While(self, node):\n                     ret_types.append(loop_defs[name].type)\n                     init_args.append(liveins[name])\n \n-            self.builder.set_insertion_point_to_end(insert_block)\n+            self._set_insertion_point_and_loc(ip, last_loc)\n             while_op = self.builder.create_while_op([ty.to_ir(self.builder) for ty in ret_types],\n                                                     [arg.handle for arg in init_args])\n             # merge the condition region"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -55,6 +55,7 @@ def optimize_ttir(mod, arch):\n     pm.add_inliner_pass()\n     pm.add_triton_combine_pass()\n     pm.add_canonicalizer_pass()\n+    pm.add_reorder_broadcast_pass()\n     pm.add_cse_pass()\n     pm.add_licm_pass()\n     pm.add_symbol_dce_pass()"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 28, "deletions": 3, "changes": 31, "file_content_changes": "@@ -7,7 +7,7 @@\n \n from .._C.libtriton.triton import ir\n from ..runtime.jit import jit\n-from . import semantic\n+from . import math, semantic\n \n T = TypeVar('T')\n \n@@ -401,6 +401,9 @@ def __init__(self, value):\n     def __repr__(self) -> str:\n         return f\"constexpr[{self.value}]\"\n \n+    def __index__(self):\n+        return self.value\n+\n     def __add__(self, other):\n         return constexpr(self.value + other.value)\n \n@@ -1419,6 +1422,11 @@ def _argmax_combine_tie_break_fast(value1, index1, value2, index2):\n     return _argmax_combine(value1, index1, value2, index2, False)\n \n \n+@jit\n+def _fast_max(x, y):\n+    return math.max(x, y)\n+\n+\n @jit\n @_add_reduction_docstr(\"maximum\",\n                        return_indices_arg=\"return_indices\",\n@@ -1431,7 +1439,13 @@ def max(input, axis=None, return_indices=False, return_indices_tie_break_left=Tr\n         else:\n             return _reduce_with_indices(input, axis, _argmax_combine_tie_break_fast)\n     else:\n-        return reduce(input, axis, maximum)\n+        if constexpr(input.dtype.primitive_bitwidth) < 32:\n+            if constexpr(input.dtype.is_floating()):\n+                input = input.to(float32)\n+            else:\n+                assert input.dtype.is_integer_type()\n+                input = input.to(int32)\n+        return reduce(input, axis, _fast_max)\n \n \n @jit\n@@ -1465,6 +1479,11 @@ def _argmin_combine_tie_break_fast(value1, index1, value2, index2):\n     return _argmin_combine(value1, index1, value2, index2, False)\n \n \n+@jit\n+def _fast_min(x, y):\n+    return math.min(x, y)\n+\n+\n @jit\n @_add_reduction_docstr(\"minimum\",\n                        return_indices_arg=\"return_indices\",\n@@ -1477,7 +1496,13 @@ def min(input, axis=None, return_indices=False, return_indices_tie_break_left=Tr\n         else:\n             return _reduce_with_indices(input, axis, _argmin_combine_tie_break_fast)\n     else:\n-        return reduce(input, axis, minimum)\n+        if constexpr(input.dtype.primitive_bitwidth) < 32:\n+            if constexpr(input.dtype.is_floating()):\n+                input = input.to(float32)\n+            else:\n+                assert input.dtype.is_integer_type()\n+                input = input.to(int32)\n+        return reduce(input, axis, _fast_min)\n \n \n @jit"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "file_content_changes": "@@ -663,6 +663,11 @@ def bitcast(input: tl.tensor,\n                      dst_ty)\n \n \n+# TODO: architecture descriptor class\n+def _is_cuda(arch):\n+    return isinstance(arch, int)\n+\n+\n def cast(input: tl.tensor,\n          dst_ty: tl.dtype,\n          builder: ir.builder) -> tl.tensor:\n@@ -677,7 +682,7 @@ def cast(input: tl.tensor,\n     src_sca_ty = src_ty.scalar\n     dst_sca_ty = dst_ty.scalar\n \n-    if builder.arch < 89 and \\\n+    if _is_cuda(builder.arch) and builder.arch < 89 and \\\n        (src_sca_ty.is_fp8e4() or dst_sca_ty.is_fp8e4()):\n         warnings.warn(\"Standard tl.float8e4 format will be deprecated on SM < 89. \"\n                       \"Please use tl.float8e4b15.\", DeprecationWarning)"}, {"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 279, "deletions": 129, "changes": 408, "file_content_changes": "@@ -3,6 +3,9 @@\n ===============\n This is a Triton implementation of the Flash Attention algorithm\n (see: Dao et al., https://arxiv.org/pdf/2205.14135v2.pdf; Rabe and Staats https://arxiv.org/pdf/2112.05682v2.pdf)\n+\n+Sequence Parallel implementation inspired by HazyResearch\n+(see https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attn_triton.py)\n \"\"\"\n \n import torch\n@@ -23,68 +26,119 @@ def _fwd_kernel(\n     Z, H, N_CTX,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n+    MODE: tl.constexpr,\n ):\n     start_m = tl.program_id(0)\n     off_hz = tl.program_id(1)\n+    qvk_offset = off_hz * stride_qh\n+    Q_block_ptr = tl.make_block_ptr(\n+        base=Q + qvk_offset,\n+        shape=(N_CTX, BLOCK_DMODEL),\n+        strides=(stride_qm, stride_qk),\n+        offsets=(start_m * BLOCK_M, 0),\n+        block_shape=(BLOCK_M, BLOCK_DMODEL),\n+        order=(1, 0)\n+    )\n+    K_block_ptr = tl.make_block_ptr(\n+        base=K + qvk_offset,\n+        shape=(BLOCK_DMODEL, N_CTX),\n+        strides=(stride_kk, stride_kn),\n+        offsets=(0, 0),\n+        block_shape=(BLOCK_DMODEL, BLOCK_N),\n+        order=(0, 1)\n+    )\n+    V_block_ptr = tl.make_block_ptr(\n+        base=V + qvk_offset,\n+        shape=(N_CTX, BLOCK_DMODEL),\n+        strides=(stride_vk, stride_vn),\n+        offsets=(0, 0),\n+        block_shape=(BLOCK_N, BLOCK_DMODEL),\n+        order=(1, 0)\n+    )\n+    O_block_ptr = tl.make_block_ptr(\n+        base=Out + qvk_offset,\n+        shape=(N_CTX, BLOCK_DMODEL),\n+        strides=(stride_om, stride_on),\n+        offsets=(start_m * BLOCK_M, 0),\n+        block_shape=(BLOCK_M, BLOCK_DMODEL),\n+        order=(1, 0)\n+    )\n     # initialize offsets\n     offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n     offs_n = tl.arange(0, BLOCK_N)\n-    offs_d = tl.arange(0, BLOCK_DMODEL)\n-    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n-    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    # Initialize pointers to Q, K, V\n-    q_ptrs = Q + off_q\n-    k_ptrs = K + off_k\n-    v_ptrs = V + off_v\n     # initialize pointer to m and l\n-    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n-    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n+    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n     acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+    # causal check on every loop iteration can be expensive\n+    # and peeling the last iteration of the loop does not work well with ptxas\n+    # so we have a mode to do the causal check in a separate kernel entirely\n+    if MODE == 0:  # entire non-causal attention\n+        lo, hi = 0, N_CTX\n+    if MODE == 1:  # entire causal attention\n+        lo, hi = 0, (start_m + 1) * BLOCK_M\n+    if MODE == 2:  # off band-diagonal\n+        lo, hi = 0, start_m * BLOCK_M\n+    if MODE == 3:  # on band-diagonal\n+        l_ptrs = L + off_hz * N_CTX + offs_m\n+        m_ptrs = M + off_hz * N_CTX + offs_m\n+        m_i = tl.load(m_ptrs)\n+        l_i = tl.load(l_ptrs)\n+        acc += tl.load(O_block_ptr).to(tl.float32)\n+        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n+    # credits to: Adam P. Goucher (https://github.com/apgoucher):\n+    # scale sm_scale by 1/log_2(e) and use\n+    # 2^x instead of exp in the loop because CSE and LICM\n+    # don't work as expected with `exp` in the loop\n+    qk_scale = sm_scale * 1.44269504\n     # load q: it will stay in SRAM throughout\n-    q = tl.load(q_ptrs)\n+    q = tl.load(Q_block_ptr)\n+    q = (q * qk_scale).to(K.dtype.element_ty)\n+    # advance block pointers to first iteration of the loop\n+    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n+    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n     # loop over k, v and update accumulator\n-    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n+    for start_n in range(lo, hi, BLOCK_N):\n+        start_n = tl.multiple_of(start_n, BLOCK_N)\n         # -- compute qk ----\n-        k = tl.load(k_ptrs)\n+        k = tl.load(K_block_ptr)\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-        qk += tl.dot(q, k)\n-        qk *= sm_scale\n-        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n-        # compute new m\n-        m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n-        # correct old l\n-        l_prev *= tl.exp(m_prev - m_curr)\n-        # attention weights\n-        p = tl.exp(qk - m_curr[:, None])\n-        l_curr = tl.sum(p, 1) + l_prev\n-        # rescale operands of matmuls\n-        l_rcp = 1. / l_curr\n-        p *= l_rcp[:, None]\n-        acc *= (l_prev * l_rcp)[:, None]\n+        qk += tl.dot(q, k, allow_tf32=True)\n+        if MODE == 1 or MODE == 3:\n+            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n+        # -- compute m_ij, p, l_ij\n+        m_ij = tl.max(qk, 1)\n+        p = tl.math.exp2(qk - m_ij[:, None])\n+        l_ij = tl.sum(p, 1)\n+        # -- update m_i and l_i\n+        m_i_new = tl.maximum(m_i, m_ij)\n+        alpha = tl.math.exp2(m_i - m_i_new)\n+        beta = tl.math.exp2(m_ij - m_i_new)\n+        l_i *= alpha\n+        l_i_new = l_i + beta * l_ij\n+        # scale p\n+        p_scale = beta / l_i_new\n+        p = p * p_scale[:, None]\n+        # scale acc\n+        acc_scale = l_i / l_i_new\n+        acc = acc * acc_scale[:, None]\n         # update acc\n-        p = p.to(Q.dtype.element_ty)\n-        v = tl.load(v_ptrs)\n-        acc += tl.dot(p, v)\n+        v = tl.load(V_block_ptr)\n+        p = p.to(V.dtype.element_ty)\n+        acc += tl.dot(p, v, allow_tf32=True)\n         # update m_i and l_i\n-        l_prev = l_curr\n-        m_prev = m_curr\n+        l_i = l_i_new\n+        m_i = m_i_new\n         # update pointers\n-        k_ptrs += BLOCK_N * stride_kn\n-        v_ptrs += BLOCK_N * stride_vk\n-    # rematerialize offsets to save registers\n-    start_m = tl.program_id(0)\n-    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n+        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n     # write back l and m\n     l_ptrs = L + off_hz * N_CTX + offs_m\n     m_ptrs = M + off_hz * N_CTX + offs_m\n-    tl.store(l_ptrs, l_prev)\n-    tl.store(m_ptrs, m_prev)\n-    # initialize pointers to output\n-    offs_n = tl.arange(0, BLOCK_DMODEL)\n-    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n-    out_ptrs = Out + off_o\n-    tl.store(out_ptrs, acc)\n+    tl.store(l_ptrs, l_i)\n+    tl.store(m_ptrs, m_i)\n+    # write back O\n+    tl.store(O_block_ptr, acc.to(K.dtype.element_ty))\n \n \n @jit\n@@ -107,94 +161,168 @@ def _bwd_preprocess(\n     tl.store(Delta + off_m, delta)\n \n \n+@jit\n+def _bwd_kernel_one_col_block(\n+    Q, K, V, sm_scale, qk_scale,\n+    Out, DO,\n+    DQ, DK, DV,\n+    L, M,\n+    D,\n+    stride_dqa, stride_qz, stride_qh, stride_qm, stride_qk,\n+    stride_kz, stride_kh, stride_kn, stride_kk,\n+    stride_vz, stride_vh, stride_vk, stride_vn,\n+    Z, H, N_CTX,\n+    off_hz, start_n, num_block,\n+    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n+    BLOCK_N: tl.constexpr,\n+    SEQUENCE_PARALLEL: tl.constexpr,\n+    MODE: tl.constexpr,\n+):\n+    if SEQUENCE_PARALLEL:\n+        DQ += stride_dqa.to(tl.int64) * start_n\n+    if MODE == 0:\n+        lo = 0\n+    else:\n+        lo = start_n * BLOCK_M\n+    # initialize row/col offsets\n+    offs_qm = lo + tl.arange(0, BLOCK_M)\n+    offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n+    offs_m = tl.arange(0, BLOCK_N)\n+    offs_k = tl.arange(0, BLOCK_DMODEL)\n+    # initialize pointers to value-like data\n+    q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n+    v_ptrs = V + (offs_n[:, None] * stride_vk + offs_k[None, :] * stride_vn)\n+    do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+    dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+    # pointer to row-wise quantities in value-like data\n+    D_ptrs = D + off_hz * N_CTX\n+    m_ptrs = M + off_hz * N_CTX\n+    # initialize dv amd dk\n+    dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+    dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+    # k and v stay in SRAM throughout\n+    k = tl.load(k_ptrs)\n+    v = tl.load(v_ptrs)\n+    # loop over rows\n+    for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n+        offs_m_curr = start_m + offs_m\n+        # load q, k, v, do on-chip\n+        q = tl.load(q_ptrs)\n+        # recompute p = softmax(qk, dim=-1).T\n+        # NOTE: `do` is pre-divided by `l`; no normalization here\n+        if MODE == 1:\n+            qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\"-inf\"))\n+        else:\n+            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n+        qk += tl.dot(q, tl.trans(k))\n+        qk *= qk_scale\n+        m = tl.load(m_ptrs + offs_m_curr)\n+        p = tl.math.exp2(qk - m[:, None])\n+        # compute dv\n+        do = tl.load(do_ptrs)\n+        dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do, allow_tf32=True)\n+        # compute dp = dot(v, do)\n+        Di = tl.load(D_ptrs + offs_m_curr)\n+        # dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n+        dp = tl.dot(do, tl.trans(v), allow_tf32=True)\n+        # compute ds = p * (dp - delta[:, None])\n+        ds = (p * (dp - Di[:, None]) * sm_scale).to(Q.dtype.element_ty)\n+        # compute dk = dot(ds.T, q)\n+        dk += tl.dot(tl.trans(ds), q, allow_tf32=True)\n+        # compute dq\n+        if not SEQUENCE_PARALLEL:\n+            dq = tl.load(dq_ptrs)\n+            dq += tl.dot(ds, k, allow_tf32=True)\n+            tl.store(dq_ptrs, dq)\n+        elif SEQUENCE_PARALLEL:\n+            # dq = tl.dot(ds, k, allow_tf32=True)\n+            dq = tl.trans(tl.dot(tl.trans(k), tl.trans(ds), allow_tf32=True))\n+            tl.store(dq_ptrs, dq)\n+\n+        # increment pointers\n+        dq_ptrs += BLOCK_M * stride_qm\n+        q_ptrs += BLOCK_M * stride_qm\n+        do_ptrs += BLOCK_M * stride_qm\n+    # write-back\n+    dv_ptrs = DV + (offs_n[:, None] * stride_vk + offs_k[None, :] * stride_vn)\n+    dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n+    tl.store(dv_ptrs, dv)\n+    tl.store(dk_ptrs, dk)\n+\n+\n @jit\n def _bwd_kernel(\n-    Q, K, V, sm_scale, Out, DO,\n+    # fmt: off\n+    Q, K, V, sm_scale,\n+    Out, DO,\n     DQ, DK, DV,\n     L, M,\n     D,\n-    stride_qz, stride_qh, stride_qm, stride_qk,\n+    stride_dqa, stride_qz, stride_qh, stride_qm, stride_qk,\n     stride_kz, stride_kh, stride_kn, stride_kk,\n     stride_vz, stride_vh, stride_vk, stride_vn,\n     Z, H, N_CTX,\n-    num_block,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n+    SEQUENCE_PARALLEL: tl.constexpr,\n+    MODE: tl.constexpr,\n+    # fmt: on\n ):\n+    qk_scale = sm_scale * 1.44269504\n     off_hz = tl.program_id(0)\n     off_z = off_hz // H\n     off_h = off_hz % H\n     # offset pointers for batch/head\n     Q += off_z * stride_qz + off_h * stride_qh\n-    K += off_z * stride_qz + off_h * stride_qh\n-    V += off_z * stride_qz + off_h * stride_qh\n+    K += off_z * stride_kz + off_h * stride_kh\n+    V += off_z * stride_vz + off_h * stride_vh\n     DO += off_z * stride_qz + off_h * stride_qh\n     DQ += off_z * stride_qz + off_h * stride_qh\n-    DK += off_z * stride_qz + off_h * stride_qh\n-    DV += off_z * stride_qz + off_h * stride_qh\n-    for start_n in range(0, num_block):\n-        lo = start_n * BLOCK_M\n-        # initialize row/col offsets\n-        offs_qm = lo + tl.arange(0, BLOCK_M)\n-        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n-        offs_m = tl.arange(0, BLOCK_N)\n-        offs_k = tl.arange(0, BLOCK_DMODEL)\n-        # initialize pointers to value-like data\n-        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n-        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n-        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n-        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n-        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n-        # pointer to row-wise quantities in value-like data\n-        D_ptrs = D + off_hz * N_CTX\n-        m_ptrs = M + off_hz * N_CTX\n-        # initialize dv amd dk\n-        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n-        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n-        # k and v stay in SRAM throughout\n-        k = tl.load(k_ptrs)\n-        v = tl.load(v_ptrs)\n-        # loop over rows\n-        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n-            offs_m_curr = start_m + offs_m\n-            # load q, k, v, do on-chip\n-            q = tl.load(q_ptrs)\n-            # recompute p = softmax(qk, dim=-1).T\n-            # NOTE: `do` is pre-divided by `l`; no normalization here\n-            qk = tl.dot(q, tl.trans(k))\n-            qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\"-inf\"))\n-            m = tl.load(m_ptrs + offs_m_curr)\n-            p = tl.exp(qk * sm_scale - m[:, None])\n-            # compute dv\n-            do = tl.load(do_ptrs)\n-            dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\n-            # compute dp = dot(v, do)\n-            Di = tl.load(D_ptrs + offs_m_curr)\n-            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n-            dp += tl.dot(do, tl.trans(v))\n-            # compute ds = p * (dp - delta[:, None])\n-            ds = p * dp * sm_scale\n-            # compute dk = dot(ds.T, q)\n-            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)\n-            # compute dq\n-            dq = tl.load(dq_ptrs)\n-            dq += tl.dot(ds.to(Q.dtype.element_ty), k)\n-            tl.store(dq_ptrs, dq)\n-            # increment pointers\n-            dq_ptrs += BLOCK_M * stride_qm\n-            q_ptrs += BLOCK_M * stride_qm\n-            do_ptrs += BLOCK_M * stride_qm\n-        # write-back\n-        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n-        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n-        tl.store(dv_ptrs, dv)\n-        tl.store(dk_ptrs, dk)\n+    DK += off_z * stride_kz + off_h * stride_kh\n+    DV += off_z * stride_vz + off_h * stride_vh\n+\n+    num_block_n = tl.cdiv(N_CTX, BLOCK_N)\n+    if not SEQUENCE_PARALLEL:\n+        for start_n in range(0, num_block_n):\n+            _bwd_kernel_one_col_block(\n+                Q, K, V, sm_scale, qk_scale, Out, DO,\n+                DQ, DK, DV,\n+                L, M,\n+                D,\n+                stride_dqa, stride_qz, stride_qh, stride_qm, stride_qk,\n+                stride_kz, stride_kh, stride_kn, stride_kk,\n+                stride_vz, stride_vh, stride_vk, stride_vn,\n+                Z, H, N_CTX,\n+                off_hz, start_n, num_block_n,\n+                BLOCK_M=BLOCK_M, BLOCK_DMODEL=BLOCK_DMODEL,\n+                BLOCK_N=BLOCK_N,\n+                SEQUENCE_PARALLEL=SEQUENCE_PARALLEL,\n+                MODE=MODE,\n+            )\n+    else:\n+        start_n = tl.program_id(1)\n+        _bwd_kernel_one_col_block(\n+            Q, K, V, sm_scale, qk_scale, Out, DO,\n+            DQ, DK, DV,\n+            L, M,\n+            D,\n+            stride_dqa, stride_qz, stride_qh, stride_qm, stride_qk,\n+            stride_kz, stride_kh, stride_kn, stride_kk,\n+            stride_vz, stride_vh, stride_vk, stride_vn,\n+            Z, H, N_CTX,\n+            off_hz, start_n, num_block_n,\n+            BLOCK_M=BLOCK_M, BLOCK_DMODEL=BLOCK_DMODEL,\n+            BLOCK_N=BLOCK_N,\n+            SEQUENCE_PARALLEL=SEQUENCE_PARALLEL,\n+            MODE=MODE,\n+        )\n \n \n class _attention(torch.autograd.Function):\n \n     @staticmethod\n-    def forward(ctx, q, k, v, sm_scale):\n+    def forward(ctx, q, k, v, causal, sm_scale, sequence_parallel=False):\n         # only support for Ampere now\n         capability = torch.cuda.get_device_capability()\n         if capability[0] < 8:\n@@ -209,58 +337,80 @@ def forward(ctx, q, k, v, sm_scale):\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         num_warps = 4 if Lk <= 64 else 8\n-\n-        _fwd_kernel[grid](\n-            q, k, v, sm_scale,\n-            L, m,\n-            o,\n-            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n-            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n-            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n-            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n-            q.shape[0], q.shape[1], q.shape[2],\n-            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n-            BLOCK_DMODEL=Lk, num_warps=num_warps,\n-            num_stages=2,\n-        )\n+        if causal:\n+            modes = [1] if q.shape[2] <= 2048 else [2, 3]\n+        else:\n+            modes = [0]\n+        for mode in modes:\n+            _fwd_kernel[grid](\n+                q, k, v, sm_scale,\n+                L, m,\n+                o,\n+                q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+                k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+                v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+                o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n+                q.shape[0], q.shape[1], q.shape[2],\n+                BLOCK_M=128, BLOCK_N=BLOCK, BLOCK_DMODEL=Lk,\n+                MODE=mode,\n+                num_warps=num_warps,\n+                num_stages=2)\n \n         ctx.save_for_backward(q, k, v, o, L, m)\n         ctx.grid = grid\n         ctx.sm_scale = sm_scale\n         ctx.BLOCK_DMODEL = Lk\n+        ctx.causal = causal\n+        ctx.sequence_parallel = sequence_parallel\n         return o\n \n     @staticmethod\n     def backward(ctx, do):\n         BLOCK = 128\n         q, k, v, o, l, m = ctx.saved_tensors\n+        sequence_parallel = ctx.sequence_parallel\n+        seq_len_kv = k.shape[2]\n         do = do.contiguous()\n-        dq = torch.zeros_like(q, dtype=torch.float32)\n+        if sequence_parallel:\n+            replicas = cdiv(seq_len_kv, BLOCK)\n+            new_dq_shape = (replicas,) + q.shape\n+            dq = torch.zeros(new_dq_shape, device=q.device, dtype=q.dtype)\n+        else:\n+            dq = torch.zeros_like(q, dtype=torch.float32)\n         dk = torch.empty_like(k)\n         dv = torch.empty_like(v)\n         do_scaled = torch.empty_like(do)\n         delta = torch.empty_like(l)\n+        if ctx.causal:\n+            mode = 1\n+        else:\n+            mode = 0\n         _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n             o, do, l,\n             do_scaled, delta,\n             BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n         )\n-        _bwd_kernel[(ctx.grid[1],)](\n+        _bwd_kernel[(ctx.grid[1], cdiv(seq_len_kv, BLOCK) if sequence_parallel else 1)](\n             q, k, v, ctx.sm_scale,\n             o, do_scaled,\n             dq, dk, dv,\n             l, m,\n             delta,\n-            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+            o.numel(), q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n             k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n             v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n             q.shape[0], q.shape[1], q.shape[2],\n-            ctx.grid[0],\n             BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n-            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n+            BLOCK_DMODEL=ctx.BLOCK_DMODEL,\n+            SEQUENCE_PARALLEL=sequence_parallel,\n+            MODE=mode,\n+            num_warps=8,\n             num_stages=1,\n         )\n-        return dq, dk, dv, None\n+\n+        if len(dq.shape) == 5:\n+            dq = dq.sum(dim=0)\n+        return dq, dk, dv, None, None, None\n \n \n attention = _attention.apply"}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 8, "deletions": 3, "changes": 11, "file_content_changes": "@@ -111,8 +111,9 @@ def _kernel(A, B, C, M, N, K,\n             b = tl.load(B)\n         else:\n             k_remaining = K - k * (BLOCK_K * SPLIT_K)\n-            a = tl.load(A, mask=rk[None, :] < k_remaining, other=0.)\n-            b = tl.load(B, mask=rk[:, None] < k_remaining, other=0.)\n+            _0 = tl.zeros((1, 1), dtype=C.dtype.element_ty)\n+            a = tl.load(A, mask=rk[None, :] < k_remaining, other=_0)\n+            b = tl.load(B, mask=rk[:, None] < k_remaining, other=_0)\n         a = a.to(C.dtype.element_ty)\n         b = b.to(C.dtype.element_ty)\n         acc += tl.dot(a, b, out_dtype=dot_out_dtype)\n@@ -149,7 +150,11 @@ def _call(a, b, dot_out_dtype):\n         M, K = a.shape\n         _, N = b.shape\n         # allocates output\n-        c_dtype = get_higher_dtype(a.dtype, b.dtype)\n+        if a.dtype in [tl.float8e4, tl.float8e4b15, tl.float8e5] or\\\n+           b.dtype in [tl.float8e4, tl.float8e4b15, tl.float8e5]:\n+            c_dtype = torch.float16\n+        else:\n+            c_dtype = get_higher_dtype(a.dtype, b.dtype)\n         c = torch.empty((M, N), device=device, dtype=c_dtype)\n         if dot_out_dtype is None:\n             if c_dtype in [torch.float16, torch.float32, torch.bfloat16]:"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 9, "deletions": 2, "changes": 11, "file_content_changes": "@@ -316,9 +316,10 @@ def _make_launcher(self):\n \n         spec_keys = ', '.join(specializations)\n         grid_args = ','.join([f'\"{arg}\": {arg}' for arg in self.arg_names])\n+        args_signature = ', '.join(name if dflt == inspect._empty else f'{name} = {dflt}' for name, dflt in zip(self.arg_names, self.arg_defaults))\n \n         src = f\"\"\"\n-def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False, device=None, device_type=None):\n+def {self.fn.__name__}({args_signature}, grid=None, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False, device=None, device_type=None):\n     from ..compiler import compile, CompiledKernel\n     sig_key =  {sig_keys},\n     constexpr_key = {f'{constexpr_keys},' if len(constexpr_keys) > 0 else ()}\n@@ -327,6 +328,7 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n     if not extern_libs is None:\n       key = (key, tuple(extern_libs.items()))\n     assert num_warps > 0 and (num_warps & (num_warps - 1)) == 0, \"num_warps must be a power of 2\"\n+    assert grid is not None\n     if callable(grid):\n         grid = grid({{{grid_args}}})\n     grid_size = len(grid)\n@@ -407,7 +409,8 @@ def __init__(self, fn, version=None, do_not_specialize=None, debug=None, noinlin\n         # function signature information\n         signature = inspect.signature(fn)\n         self.arg_names = [v.name for v in signature.parameters.values()]\n-        self.has_defaults = any(v.default != inspect._empty for v in signature.parameters.values())\n+        self.arg_defaults = [v.default for v in signature.parameters.values()]\n+        self.has_defaults = any(v != inspect._empty for v in self.arg_defaults)\n         # specialization hints\n         self.do_not_specialize = [] if do_not_specialize is None else do_not_specialize\n         self.do_not_specialize = {self.arg_names.index(arg) if isinstance(arg, str) else arg for arg in self.do_not_specialize}\n@@ -574,10 +577,14 @@ def __init__(self, base, dtype):\n         self.base = base\n         self.is_cuda = base.is_cuda\n         self.device = base.device\n+        self.shape = self.base.shape\n \n     def data_ptr(self):\n         return self.base.data_ptr()\n \n+    def stride(self, i):\n+        return self.base.stride(i)\n+\n     def __str__(self) -> str:\n         return f'TensorWrapper[{self.dtype}]({self.base})'\n "}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 61, "deletions": 0, "changes": 61, "file_content_changes": "@@ -16,6 +16,67 @@ def nvsmi(attrs):\n     return ret\n \n \n+def do_bench_cudagraph(fn, rep=20, grad_to_none=None):\n+    import torch\n+    \"\"\"\n+    Benchmark the runtime of the provided function.\n+\n+    :param fn: Function to benchmark\n+    :type fn: Callable\n+    :param rep: Repetition time (in ms)\n+    :type rep: int\n+    :param grad_to_none: Reset the gradient of the provided tensor to None\n+    :type grad_to_none: torch.tensor, optional\n+    \"\"\"\n+    if torch.cuda.current_stream() == torch.cuda.default_stream():\n+        raise RuntimeError(\"Cannot capture graph in default stream. Please use side stream in benchmark code.\")\n+    # record CUDAGraph\n+    fn()\n+    if grad_to_none is not None:\n+        for x in grad_to_none:\n+            x.detach_()\n+            x.requires_grad_(True)\n+            x.grad = None\n+    g = torch.cuda.CUDAGraph()\n+    with torch.cuda.graph(g):\n+        fn()\n+    torch.cuda.synchronize()\n+    fn = lambda: g.replay()\n+    # Estimate the runtime of the function\n+    start_event = torch.cuda.Event(enable_timing=True)\n+    end_event = torch.cuda.Event(enable_timing=True)\n+    start_event.record()\n+    fn()\n+    end_event.record()\n+    torch.cuda.synchronize()\n+    estimate_ms = start_event.elapsed_time(end_event)\n+    # compute number of repetition to last `rep` ms\n+    n_repeat = max(1, int(rep / estimate_ms))\n+    # compute number of repetition to last `rep` ms\n+    start_event = [torch.cuda.Event(enable_timing=True) for i in range(n_repeat)]\n+    end_event = [torch.cuda.Event(enable_timing=True) for i in range(n_repeat)]\n+    ret = []\n+    n_retries = 50\n+    for _ in range(n_retries):\n+        # Benchmark\n+        torch.cuda.synchronize()\n+        for i in range(n_repeat):\n+            # we don't want `fn` to accumulate gradient values\n+            # if it contains a backward pass. So we clear the\n+            # provided gradients\n+            if grad_to_none is not None:\n+                for x in grad_to_none:\n+                    x.grad = None\n+            # record time of `fn`\n+            start_event[i].record()\n+            fn()\n+            end_event[i].record()\n+        torch.cuda.synchronize()\n+        times = torch.tensor([s.elapsed_time(e) for s, e in zip(start_event, end_event)])\n+        ret.append(torch.min(times))\n+    return torch.mean(torch.tensor(ret)).item()\n+\n+\n def do_bench(fn, warmup=25, rep=100, grad_to_none=None,\n              quantiles=None,\n              fast_flush=True,"}, {"filename": "python/triton/third_party/cuda/bin/ptxas", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/triton/tools/compile.c", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -41,7 +41,7 @@ void load_{kernel_name}() {{\n     void *bin = (void *)&CUBIN_NAME;\n     int shared = {shared};\n     CUDA_CHECK(cuModuleLoadData(&{kernel_name}_mod, bin));\n-    CUDA_CHECK(cuModuleGetFunction(&{kernel_name}_func, {kernel_name}_mod, \"{kernel_name}\"));\n+    CUDA_CHECK(cuModuleGetFunction(&{kernel_name}_func, {kernel_name}_mod, \"{triton_kernel_name}\"));\n     // set dynamic shared memory if necessary\n     int shared_optin;\n     CUDA_CHECK(cuDeviceGetAttribute(&shared_optin, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN, dev));\n@@ -54,11 +54,11 @@ void load_{kernel_name}() {{\n /*\n {kernel_docstring}\n */\n-CUresult {kernel_name}(CUstream stream, unsigned int gX,unsigned int gY,unsigned int gZ,unsigned int numWarps, {signature}) {{\n+CUresult {kernel_name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {signature}) {{\n     if ({kernel_name}_func == NULL)\n        load_{kernel_name}();\n     void *args[{num_args}] = {{ {arg_pointers} }};\n     // TODO: shared memory\n     if(gX * gY * gZ > 0)\n-      return cuLaunchKernel({kernel_name}_func, gX, gY, gZ, numWarps * 32, 1, 1, {shared}, stream, args, NULL);\n+      return cuLaunchKernel({kernel_name}_func, gX, gY, gZ, {num_warps} * 32, 1, 1, {shared}, stream, args, NULL);\n }}"}, {"filename": "python/triton/tools/compile.h", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -11,5 +11,6 @@\n void unload_{kernel_name}(void);\n void load_{kernel_name}(void);\n // tt-linker: {kernel_name}:{signature}\n-CUresult{kernel_name}(CUstream stream, unsigned int gX, unsigned int gY,\n-                      unsigned int gZ, unsigned int numWarps, {signature});\n+CUresult{_placeholder} {kernel_name}(CUstream stream, unsigned int gX,\n+                                     unsigned int gY, unsigned int gZ,\n+                                     {signature});"}, {"filename": "python/triton/tools/compile.py", "status": "modified", "additions": 25, "deletions": 7, "changes": 32, "file_content_changes": "@@ -1,8 +1,10 @@\n import binascii\n+import hashlib\n import importlib.util\n import sys\n from argparse import ArgumentParser\n from pathlib import Path\n+from typing import List\n \n import triton\n from triton.compiler.code_generator import kernel_suffix\n@@ -26,7 +28,7 @@\n \n The resulting entry point will have signature\n \n-CUresult kernel_{specialization_suffix}(CUstream stream, unsigned gX, unsigned gY, unsigned gZ, unsigned numWarps, float* arg0, int32_t arg1, int32_t arg2)\n+CUresult kernel_{specialization_suffix}(CUstream stream, unsigned gX, unsigned gY, unsigned gZ, float* arg0, int32_t arg1, int32_t arg2)\n \n Different such specialized entry points can be combined using the `linker.py` script.\n \n@@ -39,12 +41,16 @@\n     # command-line arguments\n     parser = ArgumentParser(description=desc)\n     parser.add_argument(\"path\", help=\"Path to Python source containing desired kernel in its scope. File will be executed.\")\n-    parser.add_argument(\"--kernel-name\", \"-n\", type=str, default=\"\")\n-    parser.add_argument(\"--out-path\", \"-o\", type=Path, help=\"Out filename\")\n+    parser.add_argument(\"--kernel-name\", \"-n\", type=str, default=\"\", help=\"Name of the kernel to compile\", required=True)\n+    parser.add_argument(\"--num-warps\", \"-w\", type=int, default=1, help=\"Number of warps to launch the kernel\")\n     parser.add_argument(\"--out-name\", \"-on\", type=str, default=None, help=\"Out name for the compiled kernel\")\n-    parser.add_argument(\"--signature\", \"-s\", type=str, help=\"Signature of the kernel\")\n+    parser.add_argument(\"--out-path\", \"-o\", type=Path, default=None, help=\"Out filename\")\n+    parser.add_argument(\"--signature\", \"-s\", type=str, help=\"Signature of the kernel\", required=True)\n     args = parser.parse_args()\n \n+    out_name = args.out_name if args.out_name else args.kernel_name\n+    out_path = args.out_path if args.out_path else out_name\n+\n     # execute python sources and extract functions wrapped in JITFunction\n     arg_path = Path(args.path)\n     sys.path.insert(0, str(arg_path.parent))\n@@ -56,6 +62,13 @@\n     # validate and parse signature\n     signature = list(map(lambda s: s.strip(\" \"), args.signature.split(\",\")))\n \n+    def hash_signature(signature: List[str]):\n+        m = hashlib.sha256()\n+        m.update(\" \".join(signature).encode())\n+        return m.hexdigest()[:8]\n+\n+    sig_hash = hash_signature(signature)\n+\n     def constexpr(s):\n         try:\n             ret = int(s)\n@@ -68,6 +81,7 @@ def constexpr(s):\n         except ValueError:\n             pass\n         return None\n+\n     hints = {i: constexpr(s.split(\":\")[1]) for i, s in enumerate(signature) if \":\" in s}\n     hints = {k: v for k, v in hints.items() if v is not None}\n     constexprs = {i: constexpr(s) for i, s in enumerate(signature)}\n@@ -80,24 +94,28 @@ def constexpr(s):\n     divisible_by_16 = [i for i, h in hints.items() if h == 16]\n     equal_to_1 = [i for i, h in hints.items() if h == 1]\n     config = triton.compiler.instance_descriptor(divisible_by_16=divisible_by_16, equal_to_1=equal_to_1)\n-    ccinfo = triton.compile(kernel, signature=signature, constants=constexprs, configs=[config], num_warps=1)\n+    ccinfo = triton.compile(kernel, signature=signature, constants=constexprs, configs=[config], num_warps=args.num_warps)\n     arg_names = [kernel.arg_names[i] for i in signature.keys()]\n \n     # dump C stub code\n     suffix = kernel_suffix(signature.values(), config)\n-    func_name = '_'.join([kernel.__name__, suffix])\n+    func_name = '_'.join([out_name, sig_hash, suffix])\n+    triton_kernel_name = '_'.join([args.kernel_name, suffix])\n     hex_ = str(binascii.hexlify(ccinfo.asm[\"cubin\"]))[2:-1]\n     params = {\n         \"kernel_name\": func_name,\n+        \"triton_kernel_name\": triton_kernel_name,\n         \"bin_size\": len(hex_),\n         \"bin_data\": \", \".join([f\"0x{x}{y}\" for x, y in zip(hex_[::2], hex_[1::2])]),\n         \"signature\": \", \".join([f\"{ty_to_cpp(ty)} {name}\" for name, ty in zip(arg_names, signature.values())]),\n         \"arg_pointers\": \", \".join([f\"&{arg}\" for arg in arg_names]),\n         \"num_args\": len(arg_names),\n         \"kernel_docstring\": \"\",\n         \"shared\": ccinfo.shared,\n+        \"num_warps\": args.num_warps,\n+        \"_placeholder\": \"\",\n     }\n     for ext in ['h', 'c']:\n         template_path = Path(__file__).parent / f\"compile.{ext}\"\n-        with args.out_path.with_suffix(f\".{suffix}.{ext}\").open(\"w\") as fp:\n+        with out_path.with_suffix(f\".{sig_hash}_{suffix}.{ext}\").open(\"w\") as fp:\n             fp.write(Path(template_path).read_text().format(**params))"}, {"filename": "python/triton/tools/link.py", "status": "modified", "additions": 12, "deletions": 10, "changes": 22, "file_content_changes": "@@ -18,6 +18,7 @@ class KernelLinkerMeta:\n     arg_names: Sequence[str]\n     arg_ctypes: Sequence[str]\n     sizes: Sequence[Union[int, None]]\n+    sig_hash: str\n     suffix: str\n     num_specs: int\n     \"\"\" number of specialized arguments \"\"\"\n@@ -30,7 +31,7 @@ def __init__(self) -> None:\n         # [kernel_name, c signature]\n         self.linker_directives = re.compile(\"//[\\\\s]*tt-linker:[\\\\s]*([\\\\w]+):(.+)\")\n         # [name, suffix]\n-        self.kernel_name = re.compile(\"([\\\\w]+)_([\\\\w]+)\")\n+        self.kernel_name = re.compile(\"^([\\\\w]+)_([\\\\w]+)_([\\\\w]+)$\")\n         # [(argnum, d|c)]\n         self.kernel_suffix = re.compile(\"([0-9]+)([c,d])\")\n         # [(type, name)]\n@@ -45,7 +46,7 @@ def extract_linker_meta(self, header: str):\n                 m = self.linker_directives.match(ln)\n                 if _exists(m):\n                     ker_name, c_sig = m.group(1), m.group(2)\n-                    name, suffix = self._match_name(ker_name)\n+                    name, sig_hash, suffix = self._match_name(ker_name)\n                     c_types, arg_names = self._match_c_sig(c_sig)\n                     num_specs, sizes = self._match_suffix(suffix)\n                     self._add_kernel(\n@@ -54,6 +55,7 @@ def extract_linker_meta(self, header: str):\n                             arg_names=arg_names,\n                             arg_ctypes=c_types,\n                             sizes=sizes,\n+                            sig_hash=sig_hash,\n                             suffix=suffix,\n                             num_specs=num_specs,\n                         ),\n@@ -62,8 +64,8 @@ def extract_linker_meta(self, header: str):\n     def _match_name(self, ker_name: str):\n         m = self.kernel_name.match(ker_name)\n         if _exists(m):\n-            name, suffix = m.group(1), m.group(2)\n-            return name, suffix\n+            name, sig_hash, suffix = m.group(1), m.group(2), m.group(3)\n+            return name, sig_hash, suffix\n         raise LinkerError(f\"{ker_name} is not a valid kernel name\")\n \n     def _match_c_sig(self, c_sig: str):\n@@ -110,7 +112,7 @@ def gen_signature(m):\n \n def make_decls(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n     return f\"\"\"\n-CUresult {name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, unsigned int numWarps, {gen_signature(metas[-1])});\n+CUresult {name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {gen_signature(metas[-1])});\n void load_{name}();\n void unload_{name}();\n     \"\"\"\n@@ -119,26 +121,26 @@ def make_decls(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n def make_kernel_dispatcher(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n     src = f\"// launcher for: {name}\\n\"\n     for meta in sorted(metas, key=lambda m: -m.num_specs):\n-        src += f\"CUresult {name}_{meta.suffix}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, unsigned int numWarps, {gen_signature(meta)});\\n\"\n+        src += f\"CUresult {name}_{meta.sig_hash}_{meta.suffix}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {gen_signature(meta)});\\n\"\n     src += \"\\n\"\n \n-    src += f\"CUresult {name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, unsigned int numWarps, {gen_signature(metas[-1])}){{\"\n+    src += f\"CUresult {name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {gen_signature(metas[-1])}){{\"\n     src += \"\\n\"\n     for meta in sorted(metas, key=lambda m: -m.num_specs):\n         cond_fn = lambda val, hint: f\"({val} % {hint} == 0)\" if hint == 16 else f\"({val} == {hint})\" if hint == 1 else None\n         conds = \" && \".join([cond_fn(val, hint) for val, hint in zip(meta.arg_names, meta.sizes) if hint is not None])\n         src += f\"  if ({conds})\\n\"\n-        src += f\"    return {name}_{meta.suffix}(stream, gX, gY, gZ, numWarps, {', '.join(meta.arg_names)});\\n\"\n+        src += f\"    return {name}_{meta.sig_hash}_{meta.suffix}(stream, gX, gY, gZ, {', '.join(meta.arg_names)});\\n\"\n     src += \"}\\n\"\n \n     for mode in [\"load\", \"unload\"]:\n         src += f\"\\n// {mode} for: {name}\\n\"\n         for meta in sorted(metas, key=lambda m: -m.num_specs):\n-            src += f\"void {mode}_{name}_{meta.suffix}();\\n\"\n+            src += f\"void {mode}_{name}_{meta.sig_hash}_{meta.suffix}();\\n\"\n         src += f\"void {mode}_{name}() {{\"\n         src += \"\\n\"\n         for meta in sorted(metas, key=lambda m: -m.num_specs):\n-            src += f\"  {mode}_{name}_{meta.suffix}();\\n\"\n+            src += f\"  {mode}_{name}_{meta.sig_hash}_{meta.suffix}();\\n\"\n         src += \"}\\n\"\n     return src\n "}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 164, "deletions": 86, "changes": 250, "file_content_changes": "@@ -13,6 +13,11 @@\n import triton.language as tl\n \n \n+@triton.jit\n+def max_fn(x, y):\n+    return tl.math.max(x, y)\n+\n+\n @triton.jit\n def _fwd_kernel(\n     Q, K, V, sm_scale,\n@@ -25,68 +30,119 @@ def _fwd_kernel(\n     Z, H, N_CTX,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n+    MODE: tl.constexpr,\n ):\n     start_m = tl.program_id(0)\n     off_hz = tl.program_id(1)\n+    qvk_offset = off_hz * stride_qh\n+    Q_block_ptr = tl.make_block_ptr(\n+        base=Q + qvk_offset,\n+        shape=(N_CTX, BLOCK_DMODEL),\n+        strides=(stride_qm, stride_qk),\n+        offsets=(start_m * BLOCK_M, 0),\n+        block_shape=(BLOCK_M, BLOCK_DMODEL),\n+        order=(1, 0)\n+    )\n+    K_block_ptr = tl.make_block_ptr(\n+        base=K + qvk_offset,\n+        shape=(BLOCK_DMODEL, N_CTX),\n+        strides=(stride_kk, stride_kn),\n+        offsets=(0, 0),\n+        block_shape=(BLOCK_DMODEL, BLOCK_N),\n+        order=(0, 1)\n+    )\n+    V_block_ptr = tl.make_block_ptr(\n+        base=V + qvk_offset,\n+        shape=(N_CTX, BLOCK_DMODEL),\n+        strides=(stride_vk, stride_vn),\n+        offsets=(0, 0),\n+        block_shape=(BLOCK_N, BLOCK_DMODEL),\n+        order=(1, 0)\n+    )\n+    O_block_ptr = tl.make_block_ptr(\n+        base=Out + qvk_offset,\n+        shape=(N_CTX, BLOCK_DMODEL),\n+        strides=(stride_om, stride_on),\n+        offsets=(start_m * BLOCK_M, 0),\n+        block_shape=(BLOCK_M, BLOCK_DMODEL),\n+        order=(1, 0)\n+    )\n     # initialize offsets\n     offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n     offs_n = tl.arange(0, BLOCK_N)\n-    offs_d = tl.arange(0, BLOCK_DMODEL)\n-    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n-    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    # Initialize pointers to Q, K, V\n-    q_ptrs = Q + off_q\n-    k_ptrs = K + off_k\n-    v_ptrs = V + off_v\n     # initialize pointer to m and l\n-    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n-    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n+    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n     acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+    # causal check on every loop iteration can be expensive\n+    # and peeling the last iteration of the loop does not work well with ptxas\n+    # so we have a mode to do the causal check in a separate kernel entirely\n+    if MODE == 0:  # entire non-causal attention\n+        lo, hi = 0, N_CTX\n+    if MODE == 1:  # entire causal attention\n+        lo, hi = 0, (start_m + 1) * BLOCK_M\n+    if MODE == 2:  # off band-diagonal\n+        lo, hi = 0, start_m * BLOCK_M\n+    if MODE == 3:  # on band-diagonal\n+        l_ptrs = L + off_hz * N_CTX + offs_m\n+        m_ptrs = M + off_hz * N_CTX + offs_m\n+        m_i = tl.load(m_ptrs)\n+        l_i = tl.load(l_ptrs)\n+        acc += tl.load(O_block_ptr).to(tl.float32)\n+        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n+    # credits to: Adam P. Goucher (https://github.com/apgoucher):\n+    # scale sm_scale by 1/log_2(e) and use\n+    # 2^x instead of exp in the loop because CSE and LICM\n+    # don't work as expected with `exp` in the loop\n+    qk_scale = sm_scale * 1.44269504\n     # load q: it will stay in SRAM throughout\n-    q = tl.load(q_ptrs)\n+    q = tl.load(Q_block_ptr)\n+    q = (q * qk_scale).to(tl.float16)\n+    # advance block pointers to first iteration of the loop\n+    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n+    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n     # loop over k, v and update accumulator\n-    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n+    for start_n in range(lo, hi, BLOCK_N):\n+        start_n = tl.multiple_of(start_n, BLOCK_N)\n         # -- compute qk ----\n-        k = tl.load(k_ptrs)\n+        k = tl.load(K_block_ptr)\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n         qk += tl.dot(q, k)\n-        qk *= sm_scale\n-        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n-        # compute new m\n-        m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n-        # correct old l\n-        l_prev *= tl.exp(m_prev - m_curr)\n-        # attention weights\n-        p = tl.exp(qk - m_curr[:, None])\n-        l_curr = tl.sum(p, 1) + l_prev\n-        # rescale operands of matmuls\n-        l_rcp = 1. / l_curr\n-        p *= l_rcp[:, None]\n-        acc *= (l_prev * l_rcp)[:, None]\n+        if MODE == 1 or MODE == 3:\n+            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n+        # -- compute m_ij, p, l_ij\n+        m_ij = tl.max(qk, 1)\n+        p = tl.math.exp2(qk - m_ij[:, None])\n+        l_ij = tl.sum(p, 1)\n+        # -- update m_i and l_i\n+        m_i_new = tl.maximum(m_i, m_ij)\n+        alpha = tl.math.exp2(m_i - m_i_new)\n+        beta = tl.math.exp2(m_ij - m_i_new)\n+        l_i *= alpha\n+        l_i_new = l_i + beta * l_ij\n+        # scale p\n+        p_scale = beta / l_i_new\n+        p = p * p_scale[:, None]\n+        # scale acc\n+        acc_scale = l_i / l_i_new\n+        acc = acc * acc_scale[:, None]\n         # update acc\n-        p = p.to(Q.dtype.element_ty)\n-        v = tl.load(v_ptrs)\n+        v = tl.load(V_block_ptr)\n+        p = p.to(tl.float16)\n         acc += tl.dot(p, v)\n         # update m_i and l_i\n-        l_prev = l_curr\n-        m_prev = m_curr\n+        l_i = l_i_new\n+        m_i = m_i_new\n         # update pointers\n-        k_ptrs += BLOCK_N * stride_kn\n-        v_ptrs += BLOCK_N * stride_vk\n-    # rematerialize offsets to save registers\n-    start_m = tl.program_id(0)\n-    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n+        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n     # write back l and m\n     l_ptrs = L + off_hz * N_CTX + offs_m\n     m_ptrs = M + off_hz * N_CTX + offs_m\n-    tl.store(l_ptrs, l_prev)\n-    tl.store(m_ptrs, m_prev)\n-    # initialize pointers to output\n-    offs_n = tl.arange(0, BLOCK_DMODEL)\n-    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n-    out_ptrs = Out + off_o\n-    tl.store(out_ptrs, acc)\n+    tl.store(l_ptrs, l_i)\n+    tl.store(m_ptrs, m_i)\n+    # write back O\n+    tl.store(O_block_ptr, acc.to(tl.float16))\n \n \n @triton.jit\n@@ -122,10 +178,12 @@ def _bwd_kernel(\n     num_block,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n+    MODE: tl.constexpr,\n ):\n     off_hz = tl.program_id(0)\n     off_z = off_hz // H\n     off_h = off_hz % H\n+    qk_scale = sm_scale * 1.44269504\n     # offset pointers for batch/head\n     Q += off_z * stride_qz + off_h * stride_qh\n     K += off_z * stride_qz + off_h * stride_qh\n@@ -135,7 +193,10 @@ def _bwd_kernel(\n     DK += off_z * stride_qz + off_h * stride_qh\n     DV += off_z * stride_qz + off_h * stride_qh\n     for start_n in range(0, num_block):\n-        lo = start_n * BLOCK_M\n+        if MODE == 0:\n+            lo = 0\n+        else:\n+            lo = start_n * BLOCK_M\n         # initialize row/col offsets\n         offs_qm = lo + tl.arange(0, BLOCK_M)\n         offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n@@ -163,10 +224,15 @@ def _bwd_kernel(\n             q = tl.load(q_ptrs)\n             # recompute p = softmax(qk, dim=-1).T\n             # NOTE: `do` is pre-divided by `l`; no normalization here\n-            qk = tl.dot(q, tl.trans(k))\n-            qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\"-inf\"))\n+            # if MODE == 1:\n+            if MODE == 1:\n+                qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\"-inf\"))\n+            else:\n+                qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n+            qk += tl.dot(q, tl.trans(k))\n+            qk *= qk_scale\n             m = tl.load(m_ptrs + offs_m_curr)\n-            p = tl.exp(qk * sm_scale - m[:, None])\n+            p = tl.math.exp2(qk - m[:, None])\n             # compute dv\n             do = tl.load(do_ptrs)\n             dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\n@@ -199,37 +265,42 @@ def _bwd_kernel(\n class _attention(torch.autograd.Function):\n \n     @staticmethod\n-    def forward(ctx, q, k, v, sm_scale):\n+    def forward(ctx, q, k, v, causal, sm_scale):\n         BLOCK = 128\n         # shape constraints\n         Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n         assert Lq == Lk and Lk == Lv\n         assert Lk in {16, 32, 64, 128}\n         o = torch.empty_like(q)\n-        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n+        grid = (triton.cdiv(q.shape[2], 128), q.shape[0] * q.shape[1], 1)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n-        num_warps = 4 if Lk <= 64 else 8\n \n-        _fwd_kernel[grid](\n-            q, k, v, sm_scale,\n-            L, m,\n-            o,\n-            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n-            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n-            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n-            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n-            q.shape[0], q.shape[1], q.shape[2],\n-            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n-            BLOCK_DMODEL=Lk, num_warps=num_warps,\n-            num_stages=2,\n-        )\n-        # print(h.asm[\"ttgir\"])\n+        num_warps = 4 if Lk <= 64 else 8\n+        if causal:\n+            modes = [1] if q.shape[2] <= 2048 else [2, 3]\n+        else:\n+            modes = [0]\n+        for mode in modes:\n+            _fwd_kernel[grid](\n+                q, k, v, sm_scale,\n+                L, m,\n+                o,\n+                q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+                k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+                v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+                o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n+                q.shape[0], q.shape[1], q.shape[2],\n+                BLOCK_M=128, BLOCK_N=BLOCK, BLOCK_DMODEL=Lk,\n+                MODE=mode,\n+                num_warps=num_warps,\n+                num_stages=2)\n \n         ctx.save_for_backward(q, k, v, o, L, m)\n         ctx.grid = grid\n         ctx.sm_scale = sm_scale\n         ctx.BLOCK_DMODEL = Lk\n+        ctx.causal = causal\n         return o\n \n     @staticmethod\n@@ -242,6 +313,10 @@ def backward(ctx, do):\n         dv = torch.empty_like(v)\n         do_scaled = torch.empty_like(do)\n         delta = torch.empty_like(l)\n+        if ctx.causal:\n+            mode = 1\n+        else:\n+            mode = 0\n         _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n             o, do, l,\n             do_scaled, delta,\n@@ -260,40 +335,38 @@ def backward(ctx, do):\n             ctx.grid[0],\n             BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n             BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n+            MODE=mode,\n             num_stages=1,\n         )\n-        # print(h.asm[\"ttgir\"])\n-        return dq, dk, dv, None\n+        return dq, dk, dv, None, None\n \n \n attention = _attention.apply\n \n \n-@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n-def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n+@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(6, 9, 1024, 64)])\n+@pytest.mark.parametrize('causal', [False, True])\n+def test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\n     torch.manual_seed(20)\n-    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n-    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n-    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n-    sm_scale = 0.2\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    sm_scale = 0.5\n     dout = torch.randn_like(q)\n     # reference implementation\n     M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n     p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n-    for z in range(Z):\n-        for h in range(H):\n-            p[:, :, M == 0] = float(\"-inf\")\n+    if causal:\n+        p[:, :, M == 0] = float(\"-inf\")\n     p = torch.softmax(p.float(), dim=-1).half()\n     # p = torch.exp(p)\n     ref_out = torch.matmul(p, v)\n     ref_out.backward(dout)\n     ref_dv, v.grad = v.grad.clone(), None\n     ref_dk, k.grad = k.grad.clone(), None\n     ref_dq, q.grad = q.grad.clone(), None\n-    # # triton implementation\n-    tri_out = attention(q, k, v, sm_scale)\n-    # print(ref_out)\n-    # print(tri_out)\n+    # triton implementation\n+    tri_out = attention(q, k, v, causal, sm_scale).half()\n     tri_out.backward(dout)\n     tri_dv, v.grad = v.grad.clone(), None\n     tri_dk, k.grad = k.grad.clone(), None\n@@ -315,19 +388,19 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n # vary seq length for fixed head and batch=4\n configs = [triton.testing.Benchmark(\n     x_names=['N_CTX'],\n-    x_vals=[2**i for i in range(10, 14)],\n+    x_vals=[2**i for i in range(10, 15)],\n     line_arg='provider',\n     line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n     line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\n     styles=[('red', '-'), ('blue', '-')],\n     ylabel='ms',\n     plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n-    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode}\n-) for mode in ['fwd', 'bwd']]\n+    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode, 'causal': causal}\n+) for mode in ['fwd', 'bwd'] for causal in [False, True]]\n \n \n @triton.testing.perf_report(configs)\n-def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.float16, device=\"cuda\"):\n+def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, causal, mode, provider, dtype=torch.float16, device=\"cuda\"):\n     assert mode in ['fwd', 'bwd']\n     warmup = 25\n     rep = 100\n@@ -336,25 +409,30 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.f\n         k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n         v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n         sm_scale = 1.3\n-        fn = lambda: attention(q, k, v, sm_scale)\n+        fn = lambda: attention(q, k, v, causal, sm_scale)\n         if mode == 'bwd':\n             o = fn()\n             do = torch.randn_like(o)\n             fn = lambda: o.backward(do, retain_graph=True)\n         ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n-        return ms\n     if provider == \"flash\":\n         lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n         cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n         cu_seqlens[1:] = lengths.cumsum(0)\n         qkv = torch.randn((BATCH * N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\n-        fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=True)\n+        fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=causal)\n         if mode == 'bwd':\n             o = fn()\n             do = torch.randn_like(o)\n             fn = lambda: o.backward(do, retain_graph=True)\n         ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n-        return ms\n+    flops_per_matmul = 2. * BATCH * H * N_CTX * N_CTX * D_HEAD\n+    total_flops = 2 * flops_per_matmul\n+    if causal:\n+        total_flops *= 0.5\n+    if mode == 'bwd':\n+        total_flops *= 2.5  # 2.0(bwd) + 0.5(recompute)\n+    return total_flops / ms * 1e-9\n \n \n # only works on post-Ampere GPUs right now"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 28, "deletions": 0, "changes": 28, "file_content_changes": "@@ -202,6 +202,34 @@ tt.func @multi_color(%A : !tt.ptr<f16>) {\n   tt.return\n }\n \n+// This example triggers graph coloring with multiple rounds\n+// CHECK-LABEL: multi_color_multi_rounds\n+tt.func @multi_color_multi_rounds(%arg0: !tt.ptr<f16>) {\n+  // CHECK: offset = 0, size = 32\n+  %cst = arith.constant dense<0.000000e+00> : tensor<4x4xf16, #A_SHARED>\n+  // CHECK-NEXT: offset = 1184, size = 128\n+  %cst_0 = arith.constant dense<0.000000e+00> : tensor<16x4xf16, #A_SHARED>\n+  // CHECK-NEXT: offset = 1312, size = 8192\n+  %cst_1 = arith.constant dense<0.000000e+00> : tensor<1024x4xf16, #A_SHARED>\n+  %cst_2 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n+  // CHECK-NEXT: scratch offset = 32, size = 1152\n+  %0 = triton_gpu.convert_layout %cst_2 : (tensor<16x32xf16, #AL>) -> tensor<16x32xf16, #AL>\n+  %1 = triton_gpu.convert_layout %cst : (tensor<4x4xf16, #A_SHARED>) -> tensor<4x4xf16, #AL>\n+  // CHECK-NEXT: offset = 11968, size = 128\n+  %cst_3 = arith.constant dense<0.000000e+00> : tensor<2x32xf16, #A_SHARED>\n+  %2 = triton_gpu.convert_layout %cst : (tensor<4x4xf16, #A_SHARED>) -> tensor<4x4xf16, #AL>\n+  // CHECK-NEXT: offset = 0, size = 512\n+  %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  %3 = triton_gpu.convert_layout %cst_0 : (tensor<16x4xf16, #A_SHARED>) -> tensor<16x4xf16, #AL>\n+  %4 = triton_gpu.convert_layout %cst_1 : (tensor<1024x4xf16, #A_SHARED>) -> tensor<1024x4xf16, #AL>\n+  // CHECK-NEXT: scratch offset = 0, size = 1152\n+  %5 = triton_gpu.convert_layout %cst_2 : (tensor<16x32xf16, #AL>) -> tensor<16x32xf16, #AL>\n+  %6 = triton_gpu.convert_layout %cst_3 : (tensor<2x32xf16, #A_SHARED>) -> tensor<2x32xf16, #AL>\n+  // CHECK-NEXT: size = 12096\n+  tt.return\n+}\n+\n+\n // CHECK-LABEL: alloc\n tt.func @alloc(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 30, "deletions": 0, "changes": 30, "file_content_changes": "@@ -1223,3 +1223,33 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-war\n     tt.return\n   }\n }\n+\n+// -----\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: test_s8_to_bf16_conversion\n+  tt.func @test_s8_to_bf16_conversion(%in: tensor<32xi8, #blocked>) {\n+    // We can't vectorize if we only process\n+    // CHECK-NOT: llvm.inline_asm\n+    // CHECK: llvm.sitofp\n+    // CHECK-NOT: llvm.sitofp\n+    %out = arith.sitofp %in : tensor<32xi8, #blocked> to tensor<32xbf16, #blocked>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 1]}>\n+#dot = #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: test_s8_to_bf16_vectorized_conversion\n+  tt.func @test_s8_to_bf16_vectorized_conversion(%in: tensor<16x16xi8, #mma>) {\n+    // CHECK-NOT: llvm.sitofp\n+    // 8 elements per thread => we should process 2 vectors of 4\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n+    // CHECK-NOT: llvm.inline_asm\n+    %out = arith.sitofp %in : tensor<16x16xi8, #mma> to tensor<16x16xbf16, #mma>\n+    tt.return\n+  }\n+}"}, {"filename": "test/Triton/combine.mlir", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "file_content_changes": "@@ -152,12 +152,18 @@ tt.func @test_canonicalize_masked_store_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>\n }\n \n // CHECK-LABEL: @test_canonicalize_expand_dims\n-tt.func @test_canonicalize_expand_dims(%arg0: tensor<f32>) -> (tensor<1x8xf32>) {\n+tt.func @test_canonicalize_expand_dims(%arg0: tensor<f32>, %arg1: tensor<1xf32>) -> (tensor<1x8xf32>, tensor<8x8xf32>) {\n     %splat = tt.splat %arg0 : (tensor<f32>) -> tensor<8xf32>\n     // CHECK: %{{.*}} = tt.splat %arg0 : (tensor<f32>) -> tensor<1x8xf32>\n     %ed = tt.expand_dims %splat {axis = 0 : i32} : (tensor<8xf32>) -> tensor<1x8xf32>\n \n-    tt.return %ed : tensor<1x8xf32>\n+    // CHECK-NEXT: %[[ed2:.*]] = tt.expand_dims %arg1 {axis = 0 : i32} : (tensor<1xf32>) -> tensor<1x1xf32>\n+    // CHECK-NEXT: %{{.*}} = tt.broadcast %[[ed2]] : (tensor<1x1xf32>) -> tensor<8x8xf32>\n+    %bc = tt.broadcast %arg1 : (tensor<1xf32>) -> tensor<8xf32>\n+    %ed2 = tt.expand_dims %bc {axis = 0 : i32} : (tensor<8xf32>) -> tensor<1x8xf32>\n+    %bc2 = tt.broadcast %ed2 : (tensor<1x8xf32>) -> tensor<8x8xf32>\n+\n+    tt.return %ed, %bc2 : tensor<1x8xf32>, tensor<8x8xf32>\n }\n \n "}, {"filename": "test/Triton/reorder-broadcast.mlir", "status": "added", "additions": 40, "deletions": 0, "changes": 40, "file_content_changes": "@@ -0,0 +1,40 @@\n+// RUN: triton-opt %s -split-input-file -triton-reorder-broadcast | FileCheck %s\n+\n+// CHECK-LABEL: @test_splat_elementwise_pattern\n+tt.func @test_splat_elementwise_pattern(%arg0: f32) -> (tensor<128x128xf32>, tensor<128x128x!tt.ptr<f32>>) {\n+    // CHECK-DAG: %[[a:.*]] = arith.constant 1.000000e+00 : f32\n+    // CHECK-DAG: %[[c1:.*]] = arith.constant 1 : i64\n+    %c1 = arith.constant 1 : i64\n+    %a = arith.constant dense<1.0> : tensor<128x128xf32>\n+\n+    // CHECK-DAG: %[[add:.*]] = arith.addf %arg0, %[[a]] : f32\n+    // CHECK-NEXT: %[[splat:.*]] = tt.splat %[[add]] : (f32) -> tensor<128x128xf32>\n+    %b = tt.splat %arg0 : (f32) -> tensor<128x128xf32>\n+    %add = arith.addf %a, %b : tensor<128x128xf32>\n+\n+\n+    // CHECK-NEXT: %[[ptr:.*]] = tt.int_to_ptr %[[c1]] : i64 -> !tt.ptr<f32>\n+    // CHECK-NEXT: %{{.*}} = tt.splat %[[ptr]] : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>\n+    %c1_t = tt.splat %c1 : (i64) -> tensor<128x128xi64>\n+    %ptr = tt.int_to_ptr %c1_t : tensor<128x128xi64> -> tensor<128x128x!tt.ptr<f32>>\n+\n+    tt.return %add, %ptr : tensor<128x128xf32>, tensor<128x128x!tt.ptr<f32>>\n+}\n+\n+// CHECK-LABEL: @test_broadcast_elementwise_pattern\n+tt.func @test_broadcast_elementwise_pattern(%arg0: tensor<128x1xf32>) -> (tensor<128x128xf32>, tensor<128x32xf32>) {\n+    // CHECK: %[[one:.*]] = arith.constant dense<1.000000e+00> : tensor<128x1xf32>\n+\n+    // CHECK-NEXT: %[[abs:.*]] = math.absf %arg0 : tensor<128x1xf32>\n+    // CHECK-NEXT: %{{.*}} = tt.broadcast %[[abs]] : (tensor<128x1xf32>) -> tensor<128x128xf32>\n+    %broadcast = tt.broadcast %arg0 : (tensor<128x1xf32>) -> tensor<128x128xf32>\n+    %abs = math.absf %broadcast : tensor<128x128xf32>\n+\n+    // CHECK-NEXT: %[[add:.*]] = arith.addf %arg0, %[[one]] : tensor<128x1xf32>\n+    // CHECK-NEXT: %{{.*}} = tt.broadcast %[[add]] : (tensor<128x1xf32>) -> tensor<128x32xf32>\n+    %broadcast2 = tt.broadcast %arg0 : (tensor<128x1xf32>) -> tensor<128x32xf32>\n+    %one = arith.constant dense<1.0> : tensor<128x32xf32>\n+    %add = arith.addf %one, %broadcast2 : tensor<128x32xf32>\n+\n+    tt.return %abs, %add : tensor<128x128xf32>, tensor<128x32xf32>\n+}"}]