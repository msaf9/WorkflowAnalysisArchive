[{"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 4, "deletions": 13, "changes": 17, "file_content_changes": "@@ -313,9 +313,9 @@ def matmul(a, b, activation=None):\n         ],  # different possible values for `x_name`\n         line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n         # possible values for `line_arg``\n-        line_vals=['cublas', 'cublas + relu', 'triton', 'triton + relu'],\n+        line_vals=['cublas', 'triton'],\n         # label name for the lines\n-        line_names=[\"cuBLAS\", \"cuBLAS (+ torch.nn.LeakyReLU)\", \"Triton\", \"Triton (+ LeakyReLU)\"],\n+        line_names=[\"cuBLAS\", \"Triton\"],\n         # line styles\n         styles=[('green', '-'), ('green', '--'), ('blue', '-'), ('blue', '--')],\n         ylabel=\"TFLOPS\",  # label name for the y-axis\n@@ -327,18 +327,9 @@ def benchmark(M, N, K, provider):\n     a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n     b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n     if provider == 'cublas':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b))\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), rep=100)\n     if provider == 'triton':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b))\n-    if provider == 'cublas + relu':\n-        torch_relu = torch.nn.ReLU(inplace=True)\n-        ms, min_ms, max_ms = triton.testing.do_bench(\n-            lambda: torch_relu(torch.matmul(a, b))\n-        )\n-    if provider == 'triton + relu':\n-        ms, min_ms, max_ms = triton.testing.do_bench(\n-            lambda: matmul(a, b, activation=leaky_relu)\n-        )\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), rep=100)\n     perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n     return perf(ms), perf(max_ms), perf(min_ms)\n "}]