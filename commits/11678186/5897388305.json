[{"filename": ".github/workflows/compare-artifacts.yml", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -13,6 +13,7 @@ on:\n jobs:\n   Compare-artifacts:\n     runs-on: ubuntu-latest\n+    if: ${{ github.event.workflow_run.conclusion == 'success' }}\n \n     steps:\n       - name: 'Download artifact'\n@@ -22,7 +23,7 @@ jobs:\n             let allArtifacts = await github.rest.actions.listWorkflowRunArtifacts({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n-               run_id: ${{ github.event.inputs.run_id }},\n+               run_id: context.payload.workflow_run.id,\n             });\n             let matchArtifact = allArtifacts.data.artifacts.filter((artifact) => {\n               return artifact.name == \"pr_number\"\n@@ -35,6 +36,7 @@ jobs:\n             });\n             let fs = require('fs');\n             fs.writeFileSync(`${process.env.GITHUB_WORKSPACE}/pr_number.zip`, Buffer.from(download.data));\n+\n       - name: 'Unzip artifact'\n         run: unzip pr_number.zip\n \n@@ -49,5 +51,5 @@ jobs:\n               owner: context.repo.owner,\n               repo: context.repo.repo,\n               issue_number: issue_number,\n-              body: 'Thank you for the PR!'\n+              body: 'Ignore this message. This is to test another workflow posting comment on PR number ${issue_number}.'\n             });"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 17, "deletions": 4, "changes": 21, "file_content_changes": "@@ -307,8 +307,10 @@ struct ReduceOpConversion\n     Operation *yield = block->getTerminator();\n     Operation *reduceOp = yield->getOperand(0).getDefiningOp();\n     if (!reduceOp || reduceOp->getNumOperands() != 2 ||\n-        reduceOp->getNumResults() != 1 ||\n-        !reduceOp->getResultTypes()[0].isInteger(32))\n+        reduceOp->getNumResults() != 1)\n+      return std::nullopt;\n+    auto intType = reduceOp->getResultTypes()[0].dyn_cast<IntegerType>();\n+    if (!intType || intType.getWidth() > 32)\n       return std::nullopt;\n     if (reduceOp->getOperand(0) != block->getArgument(0) ||\n         reduceOp->getOperand(1) != block->getArgument(1))\n@@ -382,8 +384,19 @@ struct ReduceOpConversion\n           mask = shl(i32_val(bitmask),\n                      and_(laneId, i32_val(~(numLaneToReduce - 1))));\n         }\n-        acc[0] = rewriter.create<NVVM::ReduxOp>(loc, acc[0].getType(), acc[0],\n-                                                *kind, mask);\n+        for (unsigned i = 0; i < acc.size(); ++i) {\n+          unsigned bitwidth = acc[i].getType().cast<IntegerType>().getWidth();\n+          if (bitwidth < 32) {\n+            if (*kind == NVVM::ReduxKind::MIN || *kind == NVVM::ReduxKind::MAX)\n+              acc[i] = sext(i32_ty, acc[i]);\n+            else\n+              acc[i] = zext(i32_ty, acc[i]);\n+          }\n+          acc[i] = rewriter.create<NVVM::ReduxOp>(loc, acc[i].getType(), acc[0],\n+                                                  *kind, mask);\n+          if (bitwidth < 32)\n+            acc[i] = trunc(int_ty(bitwidth), acc[i]);\n+        }\n         return;\n       }\n     }"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 56, "deletions": 0, "changes": 56, "file_content_changes": "@@ -225,3 +225,59 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, seq_par, causal, mode, dtype_str):\n     ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, seq_par, causal, mode, dtype_str)]\n     print_perf(ms, cur_gpu_util, ref_gpu_util)\n     triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.02, rtol=0.01)\n+\n+\n+#######################\n+# Reduction\n+#######################\n+\n+\n+@triton.jit\n+def _sum(x_ptr, y_ptr, output_ptr, n_elements,\n+         BLOCK_SIZE: tl.constexpr):\n+    pid = tl.program_id(axis=0)\n+    block_start = pid * BLOCK_SIZE\n+    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+    mask = offsets < n_elements\n+    x = tl.load(x_ptr + offsets, mask=mask)\n+    y = tl.load(y_ptr + offsets, mask=mask)\n+    # run in a loop to only to make it compute bound.\n+    for i in range(100):\n+        x = tl.sum(x, axis=0) + y\n+\n+    tl.store(output_ptr + offsets, x, mask=mask)\n+\n+\n+reduction_data = {\n+    'a100': {\n+        1024 * 16384: {'float16': 0.016, 'float32': 0.031, 'int16': 0.015, 'int32': 0.031},\n+        1024 * 65536: {'float16': 0.016, 'float32': 0.032, 'int16': 0.015, 'int32': 0.032},\n+    }\n+}\n+\n+\n+@pytest.mark.parametrize('N', reduction_data[DEVICE_NAME].keys())\n+@pytest.mark.parametrize(\"dtype_str\", ['float16', 'float32', 'int16', 'int32'])\n+def test_reductions(N, dtype_str):\n+    stream = torch.cuda.Stream()\n+    torch.cuda.set_stream(stream)\n+    torch.manual_seed(0)\n+    dtype = {'float16': torch.float16, 'float32': torch.float32, 'int16': torch.int16, 'int32': torch.int32}[dtype_str]\n+    ref_gpu_util = reduction_data[DEVICE_NAME][N][dtype_str]\n+    cur_sm_clock = nvsmi(['clocks.current.sm'])[0]\n+    max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n+    z = torch.empty((N, ), dtype=dtype, device='cuda')\n+    if dtype == torch.float16 or dtype == torch.float32:\n+        x = torch.randn_like(z)\n+        y = torch.randn_like(z)\n+    else:\n+        info = torch.iinfo(dtype)\n+        x = torch.randint(info.min, info.max, (N,), dtype=dtype, device='cuda')\n+        y = torch.randint(info.min, info.max, (N,), dtype=dtype, device='cuda')\n+    grid = lambda args: (triton.cdiv(N, args['BLOCK_SIZE']), )\n+    fn = lambda: _sum[grid](x, y, z, N, BLOCK_SIZE=1024)\n+    ms = triton.testing.do_bench_cudagraph(fn)\n+    cur_gpu_perf = 100. * 2. * N / ms * 1e-9\n+    cur_gpu_util = cur_gpu_perf / max_gpu_perf\n+    print_perf(ms, cur_gpu_util, ref_gpu_util)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.02, rtol=0.01)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -1485,8 +1485,6 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n def get_reduced_dtype(dtype_str, op):\n     if op in ('argmin', 'argmax'):\n         return 'int32'\n-    if dtype_str in ['int8', 'uint8', 'int16', 'uint16']:\n-        return 'int32'\n     if dtype_str == 'bfloat16':\n         return 'float32'\n     return dtype_str"}, {"filename": "python/triton/common/build.py", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -18,7 +18,7 @@ def is_hip():\n \n @functools.lru_cache()\n def libcuda_dirs():\n-    libs = subprocess.check_output([\"ldconfig\", \"-p\"]).decode()\n+    libs = subprocess.check_output([\"/sbin/ldconfig\", \"-p\"]).decode()\n     # each line looks like the following:\n     # libcuda.so.1 (libc6,x86-64) => /lib/x86_64-linux-gnu/libcuda.so.1\n     locs = [line.split()[-1] for line in libs.splitlines() if \"libcuda.so\" in line]\n@@ -27,6 +27,9 @@ def libcuda_dirs():\n     if locs:\n         msg += 'Possible files are located at %s.' % str(locs)\n         msg += 'Please create a symlink of libcuda.so to any of the file.'\n+    else:\n+        msg += 'Please make sure GPU is setup and then run \"/sbin/ldconfig\"'\n+        msg += ' (requires sudo) to refresh the linker cache.'\n     assert any(os.path.exists(os.path.join(path, 'libcuda.so')) for path in dirs), msg\n     return dirs\n "}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 0, "deletions": 5, "changes": 5, "file_content_changes": "@@ -1351,11 +1351,6 @@ def make_combine_region(reduce_op):\n @builtin\n def _promote_reduction_input(t, _builder=None):\n     scalar_ty = t.type.scalar\n-    # input is extended to 32-bits if necessary\n-    # this increases numerical accuracy and can be done pretty much for free\n-    # on GPUs\n-    if scalar_ty.is_int() and scalar_ty.int_bitwidth < 32:\n-        return t.to(int32, _builder=_builder)\n \n     # hardware doesn't support FMAX, FMIN, CMP for bfloat16\n     if scalar_ty is bfloat16:"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -377,9 +377,9 @@ def get_max_tensorcore_tflops(dtype, backend=None, device=None, clock_rate=None)\n         assert dtype == torch.float16\n         ops_per_sub_core = 256  # 2 4x4x4 Tensor Cores\n     else:\n-        if dtype == torch.float32:\n+        if dtype in [torch.float32, torch.int32]:\n             ops_per_sub_core = 256\n-        elif dtype in [torch.float16, torch.bfloat16]:\n+        elif dtype in [torch.float16, torch.bfloat16, torch.int16]:\n             ops_per_sub_core = 512\n         elif dtype in [torch.int8, tl.float8e4, tl.float8e4b15, tl.float8e5]:\n             ops_per_sub_core = 1024"}]