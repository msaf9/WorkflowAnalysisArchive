[{"filename": "include/triton/Dialect/Triton/IR/TritonInterfaces.td", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -3,4 +3,9 @@\n \n include \"mlir/IR/OpBase.td\"\n \n+def TensorSizeTrait : NativeOpTrait<\"TensorSizeTrait\">;\n+def SameOperandsAndResultEncoding : NativeOpTrait<\"SameOperandsAndResultEncoding\">;\n+def SameOperandsEncoding : NativeOpTrait<\"SameOperandsEncoding\">;\n+\n+\n #endif // TRITON_INTERFACES\n\\ No newline at end of file"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "file_content_changes": "@@ -12,10 +12,6 @@ include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n include \"mlir/Interfaces/SideEffectInterfaces.td\" // NoSideEffect\n include \"mlir/Interfaces/CastInterfaces.td\" // CastOpInterface\n \n-def TensorSizeTrait : NativeOpTrait<\"TensorSizeTrait\">;\n-def SameOperandsAndResultEncoding : NativeOpTrait<\"SameOperandsAndResultEncoding\">;\n-def SameOperandsEncoding : NativeOpTrait<\"SameOperandsEncoding\">;\n-\n //\n // Op Base\n //"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 9, "deletions": 3, "changes": 12, "file_content_changes": "@@ -44,7 +44,9 @@ def TTG_AsyncWaitOp : TTG_Op<\"async_wait\"> {\n // This is needed because these ops don't\n // handle encodings\n // e.g., https://github.com/llvm/llvm-project/blob/main/mlir/include/mlir/Dialect/Arith/IR/ArithOps.td#L111\n-def TTG_CmpIOp : TTG_Op<\"cmpi\", [NoSideEffect]> {\n+def TTG_CmpIOp : TTG_Op<\"cmpi\", [NoSideEffect, Elementwise, \n+                                 SameOperandsAndResultShape, \n+                                 SameOperandsAndResultEncoding]> {\n   let summary = \"integer comparison operation\";\n \n   let description = [{}];\n@@ -56,7 +58,9 @@ def TTG_CmpIOp : TTG_Op<\"cmpi\", [NoSideEffect]> {\n   let results = (outs TT_BoolLike:$result);\n }\n \n-def TTG_CmpFOp : TTG_Op<\"cmpf\", [NoSideEffect]> {\n+def TTG_CmpFOp : TTG_Op<\"cmpf\", [NoSideEffect, Elementwise, \n+                                 SameOperandsAndResultShape, \n+                                 SameOperandsAndResultEncoding]> {\n   let summary = \"floating-point comparison operation\";\n \n   let description = [{}];\n@@ -69,7 +73,9 @@ def TTG_CmpFOp : TTG_Op<\"cmpf\", [NoSideEffect]> {\n }\n \n // TODO: migrate to arith::SelectOp on LLVM16\n-def TTG_SelectOp : TTG_Op<\"select\", [NoSideEffect]> {\n+def TTG_SelectOp : TTG_Op<\"select\", [NoSideEffect, Elementwise, \n+                                 SameOperandsAndResultShape, \n+                                 SameOperandsAndResultEncoding]> {\n   let summary = \"select operation\";\n \n   let description = [{}];"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "removed", "additions": 0, "deletions": 1128, "changes": 1128, "file_content_changes": "@@ -1,1128 +0,0 @@\n-#include \"TritonGPUToLLVM.h\"\n-\n-using namespace mlir;\n-using namespace mlir::triton;\n-\n-LogicalResult\n-FuncOpConversion::matchAndRewrite(FuncOp funcOp, OpAdaptor adaptor,\n-                ConversionPatternRewriter &rewriter) const {\n-  auto newFuncOp = convertFuncOpToLLVMFuncOp(funcOp, rewriter);\n-  if (!newFuncOp)\n-    return failure();\n-\n-  auto ctx = funcOp->getContext();\n-\n-  // Set an attribute to indicate this function is a kernel entry.\n-  newFuncOp->setAttr(\"nvvm.kernel\",\n-                     rewriter.getIntegerAttr(type::u1Ty(ctx), 1));\n-\n-  // Set an attribute for maxntidx, it could be used in latter LLVM codegen\n-  // for `nvvm.annotation` metadata.\n-  newFuncOp->setAttr(\"nvvm.maxntid\",\n-                     rewriter.getIntegerAttr(i32_ty, 32 * numWarps));\n-\n-  rewriter.eraseOp(funcOp);\n-  return success();\n-}\n-\n-LogicalResult\n-ReturnOpConversion::matchAndRewrite(\n-    ReturnOp op, OpAdaptor adaptor,\n-    ConversionPatternRewriter &rewriter) const {\n-  unsigned numArguments = op.getNumOperands();\n-\n-  // Currently, Triton kernel function always return nothing.\n-  // TODO(Superjomn) add support for non-inline device function\n-  if (numArguments > 0) {\n-    return rewriter.notifyMatchFailure(\n-        op, \"Only kernel function with nothing returned is supported.\");\n-  }\n-\n-  rewriter.replaceOpWithNewOp<LLVM::ReturnOp>(op, TypeRange(), ValueRange(),\n-                                              op->getAttrs());\n-  return success();\n-}\n-\n-LogicalResult\n-BroadcastOpConversion::matchAndRewrite(\n-    triton::BroadcastOp op, OpAdaptor adaptor,\n-    ConversionPatternRewriter &rewriter) const {\n-  // Following the order of indices in the legacy code, a broadcast of:\n-  //   [s(0), s(1) ... s(k-1),    1, s(k+1), s(k+2) ... s(n-1)]\n-  // =>\n-  //   [s(0), s(1) ... s(k-1), s(k), s(k+1), s(k+2) ... s(n-1)]\n-  //\n-  // logically maps to a broadcast within a thread's scope:\n-  //   [cta(0)..cta(k-1),     1,cta(k+1)..cta(n-1),spt(0)..spt(k-1),\n-  //   1,spt(k+1)..spt(n-1)]\n-  // =>\n-  //   [cta(0)..cta(k-1),cta(k),cta(k+1)..cta(n-1),spt(0)..spt(k-1),spt(k),spt(k+1)..spt(n-1)]\n-  //\n-  // regardless of the order of the layout\n-  //\n-  Location loc = op->getLoc();\n-  Value src = adaptor.src();\n-  Value result = op.result();\n-  auto srcTy = op.src().getType().cast<RankedTensorType>();\n-  auto resultTy = result.getType().cast<RankedTensorType>();\n-  auto srcLayout = srcTy.getEncoding();\n-  auto resultLayout = resultTy.getEncoding();\n-  auto srcShape = srcTy.getShape();\n-  auto resultShape = resultTy.getShape();\n-  unsigned rank = srcTy.getRank();\n-  assert(rank == resultTy.getRank());\n-  auto order = triton::gpu::getOrder(srcLayout);\n-  auto srcOffsets = emitOffsetForLayout(srcLayout, srcShape);\n-  auto resultOffsets = emitOffsetForLayout(resultLayout, resultShape);\n-  SmallVector<Value> srcVals = getElementsFromStruct(loc, src, rewriter);\n-  DenseMap<SmallVector<unsigned>, Value, SmallVectorKeyInfo> srcValues;\n-  for (size_t i = 0; i < srcOffsets.size(); i++) {\n-    srcValues[srcOffsets[i]] = srcVals[i];\n-  }\n-  SmallVector<Value> resultVals;\n-  for (size_t i = 0; i < resultOffsets.size(); i++) {\n-    auto offset = resultOffsets[i];\n-    for (size_t j = 0; j < srcShape.size(); j++)\n-      if (srcShape[j] == 1)\n-        offset[j] = 0;\n-    resultVals.push_back(srcValues.lookup(offset));\n-  }\n-  auto llvmStructTy = getTypeConverter()->convertType(resultTy);\n-  Value resultStruct =\n-      getStructFromElements(loc, resultVals, rewriter, llvmStructTy);\n-  rewriter.replaceOp(op, {resultStruct});\n-  return success();\n-}\n-\n-LogicalResult\n-PrintfOpConversion::matchAndRewrite(\n-    triton::PrintfOp op, OpAdaptor adaptor,\n-    ConversionPatternRewriter &rewriter) const {\n-  auto loc = op->getLoc();\n-  SmallVector<Value, 16> operands;\n-  for (auto operand : adaptor.getOperands()) {\n-    auto sub_operands = getElementsFromStruct(loc, operand, rewriter);\n-    for (auto elem : sub_operands) {\n-      operands.push_back(elem);\n-    }\n-  }\n-  std::string formatStr;\n-  llvm::raw_string_ostream os(formatStr);\n-  os << op.prefix();\n-  if (!operands.empty()) {\n-    os << getFormatSubstr(operands[0]);\n-  }\n-\n-  for (size_t i = 1; i < operands.size(); ++i) {\n-    os << \", \" << getFormatSubstr(operands[i]);\n-  }\n-  llPrintf(formatStr, operands, rewriter);\n-  rewriter.eraseOp(op);\n-  return success();\n-}\n-\n-std::string PrintfOpConversion::getFormatSubstr(Value value) const {\n-  Type type = value.getType();\n-  if (type.isa<LLVM::LLVMPointerType>()) {\n-    return \"%p\";\n-  } else if (type.isBF16() || type.isF16() || type.isF32() || type.isF64()) {\n-    return \"%f\";\n-  } else if (type.isSignedInteger()) {\n-    return \"%i\";\n-  } else if (type.isUnsignedInteger() || type.isSignlessInteger()) {\n-    return \"%u\";\n-  }\n-  assert(false && \"not supported type\");\n-  return \"\";\n-}\n-\n-LLVM::LLVMFuncOp\n-PrintfOpConversion::getVprintfDeclaration(ConversionPatternRewriter &rewriter) {\n-  auto moduleOp =\n-      rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n-  StringRef funcName(\"vprintf\");\n-  Operation *funcOp = moduleOp.lookupSymbol(funcName);\n-  if (funcOp)\n-    return cast<LLVM::LLVMFuncOp>(*funcOp);\n-\n-  auto *context = rewriter.getContext();\n-\n-  SmallVector<Type> argsType{ptr_ty(IntegerType::get(context, 8)),\n-                             ptr_ty(IntegerType::get(context, 8))};\n-  auto funcType = LLVM::LLVMFunctionType::get(i32_ty, argsType);\n-\n-  ConversionPatternRewriter::InsertionGuard guard(rewriter);\n-  rewriter.setInsertionPointToStart(moduleOp.getBody());\n-\n-  return rewriter.create<LLVM::LLVMFuncOp>(UnknownLoc::get(context), funcName,\n-                                           funcType);\n-}\n-\n-std::pair<Type, Value> PrintfOpConversion::promoteValue(\n-    ConversionPatternRewriter &rewriter, Value value) {\n-  auto *context = rewriter.getContext();\n-  auto type = value.getType();\n-  Value newOp = value;\n-  Type newType = type;\n-\n-  bool bUnsigned = type.isUnsignedInteger();\n-  if (type.isIntOrIndex() && type.getIntOrFloatBitWidth() < 32) {\n-    if (bUnsigned) {\n-      newType = ui32_ty;\n-      newOp = rewriter.create<LLVM::ZExtOp>(UnknownLoc::get(context), newType,\n-                                            value);\n-    } else {\n-      newType = i32_ty;\n-      newOp = rewriter.create<LLVM::SExtOp>(UnknownLoc::get(context), newType,\n-                                            value);\n-    }\n-  } else if (type.isBF16() || type.isF16() || type.isF32()) {\n-    newType = f64_ty;\n-    newOp = rewriter.create<LLVM::FPExtOp>(UnknownLoc::get(context), newType,\n-                                           value);\n-  }\n-\n-  return {newType, newOp};\n-}\n-\n-void PrintfOpConversion::llPrintf(\n-    StringRef msg, ValueRange args, ConversionPatternRewriter &rewriter) {\n-  static const char formatStringPrefix[] = \"printfFormat_\";\n-  assert(!msg.empty() && \"printf with empty string not support\");\n-  Type int8Ptr = ptr_ty(i8_ty);\n-\n-  auto *context = rewriter.getContext();\n-  auto moduleOp =\n-      rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n-  auto funcOp = getVprintfDeclaration(rewriter);\n-\n-  Value one = rewriter.create<LLVM::ConstantOp>(\n-      UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(1));\n-  Value zero = rewriter.create<LLVM::ConstantOp>(\n-      UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(0));\n-\n-  unsigned stringNumber = 0;\n-  SmallString<16> stringConstName;\n-  do {\n-    stringConstName.clear();\n-    (formatStringPrefix + Twine(stringNumber++)).toStringRef(stringConstName);\n-  } while (moduleOp.lookupSymbol(stringConstName));\n-\n-  llvm::SmallString<64> formatString(msg);\n-  formatString.push_back('\\n');\n-  formatString.push_back('\\0');\n-  size_t formatStringSize = formatString.size_in_bytes();\n-  auto globalType = LLVM::LLVMArrayType::get(i8_ty, formatStringSize);\n-\n-  LLVM::GlobalOp global;\n-  {\n-    ConversionPatternRewriter::InsertionGuard guard(rewriter);\n-    rewriter.setInsertionPointToStart(moduleOp.getBody());\n-    global = rewriter.create<LLVM::GlobalOp>(\n-        UnknownLoc::get(context), globalType,\n-        /*isConstant=*/true, LLVM::Linkage::Internal, stringConstName,\n-        rewriter.getStringAttr(formatString));\n-  }\n-\n-  Value globalPtr =\n-      rewriter.create<LLVM::AddressOfOp>(UnknownLoc::get(context), global);\n-  Value stringStart = rewriter.create<LLVM::GEPOp>(\n-      UnknownLoc::get(context), int8Ptr, globalPtr,\n-      SmallVector<Value>({zero, zero}));\n-\n-  Value bufferPtr =\n-      rewriter.create<LLVM::NullOp>(UnknownLoc::get(context), int8Ptr);\n-\n-  SmallVector<Value, 16> newArgs;\n-  if (args.size() >= 1) {\n-    SmallVector<Type> argTypes;\n-    for (auto arg : args) {\n-      Type newType;\n-      Value newArg;\n-      std::tie(newType, newArg) = promoteValue(rewriter, arg);\n-      argTypes.push_back(newType);\n-      newArgs.push_back(newArg);\n-    }\n-\n-    Type structTy = LLVM::LLVMStructType::getLiteral(context, argTypes);\n-    auto allocated = rewriter.create<LLVM::AllocaOp>(UnknownLoc::get(context),\n-                                                     ptr_ty(structTy), one,\n-                                                     /*alignment=*/0);\n-\n-    for (const auto &entry : llvm::enumerate(newArgs)) {\n-      auto index = rewriter.create<LLVM::ConstantOp>(\n-          UnknownLoc::get(context), i32_ty,\n-          rewriter.getI32IntegerAttr(entry.index()));\n-      auto fieldPtr = rewriter.create<LLVM::GEPOp>(\n-          UnknownLoc::get(context), ptr_ty(argTypes[entry.index()]),\n-          allocated, ArrayRef<Value>{zero, index});\n-      rewriter.create<LLVM::StoreOp>(UnknownLoc::get(context), entry.value(),\n-                                     fieldPtr);\n-    }\n-    bufferPtr = rewriter.create<LLVM::BitcastOp>(UnknownLoc::get(context),\n-                                                 int8Ptr, allocated);\n-  }\n-\n-  SmallVector<Value> operands{stringStart, bufferPtr};\n-  rewriter.create<LLVM::CallOp>(UnknownLoc::get(context), funcOp, operands);\n-}\n-\n-LogicalResult\n-MakeRangeOpConversion::matchAndRewrite(\n-    triton::MakeRangeOp op, OpAdaptor adaptor,\n-    ConversionPatternRewriter &rewriter) const {\n-  Location loc = op->getLoc();\n-  auto rankedTy = op.result().getType().dyn_cast<RankedTensorType>();\n-  auto shape = rankedTy.getShape();\n-  auto layout = rankedTy.getEncoding();\n-\n-  auto elemTy = rankedTy.getElementType();\n-  assert(elemTy.isInteger(32));\n-  Value start = createIndexAttrConstant(rewriter, loc, elemTy, op.start());\n-  auto idxs = emitIndices(loc, rewriter, layout, shape);\n-  unsigned elems = idxs.size();\n-  SmallVector<Value> retVals(elems);\n-  // TODO: slice layout has more elements than expected.\n-  // Unexpected behavior for make range, but generally OK when followed by\n-  // expand dims + broadcast. very weird behavior otherwise potentially.\n-  for (const auto multiDim : llvm::enumerate(idxs)) {\n-    assert(multiDim.value().size() == 1);\n-    retVals[multiDim.index()] = add(multiDim.value()[0], start);\n-  }\n-  SmallVector<Type> types(elems, elemTy);\n-  Type structTy = LLVM::LLVMStructType::getLiteral(getContext(), types);\n-  Value result = getStructFromElements(loc, retVals, rewriter, structTy);\n-  rewriter.replaceOp(op, result);\n-  return success();\n-}\n-\n-LogicalResult\n-GetProgramIdOpConversion::matchAndRewrite(\n-    triton::GetProgramIdOp op, OpAdaptor adaptor,\n-    ConversionPatternRewriter &rewriter) const {\n-  Location loc = op->getLoc();\n-  assert(op.axis() < 3);\n-\n-  Value blockId = rewriter.create<::mlir::gpu::BlockIdOp>(\n-      loc, rewriter.getIndexType(), dims[op.axis()]);\n-  auto llvmIndexTy = getTypeConverter()->getIndexType();\n-  rewriter.replaceOpWithNewOp<UnrealizedConversionCastOp>(\n-      op, TypeRange{llvmIndexTy}, ValueRange{blockId});\n-  return success();\n-}\n-\n-LogicalResult\n-GetNumProgramsOpConversion::matchAndRewrite(\n-    triton::GetNumProgramsOp op, OpAdaptor adaptor,\n-    ConversionPatternRewriter &rewriter) const {\n-  Location loc = op->getLoc();\n-  assert(op.axis() < 3);\n-\n-  Value blockId = rewriter.create<::mlir::gpu::GridDimOp>(\n-      loc, rewriter.getIndexType(), dims[op.axis()]);\n-  auto llvmIndexTy = getTypeConverter()->getIndexType();\n-  rewriter.replaceOpWithNewOp<UnrealizedConversionCastOp>(\n-      op, TypeRange{llvmIndexTy}, ValueRange{blockId});\n-  return success();\n-}\n-\n-LogicalResult\n-AddPtrOpConversion::matchAndRewrite(\n-    triton::AddPtrOp op, OpAdaptor adaptor,\n-    ConversionPatternRewriter &rewriter) const {\n-  Location loc = op->getLoc();\n-  auto resultTy = op.getType();\n-  auto resultTensorTy = resultTy.dyn_cast<RankedTensorType>();\n-  if (resultTensorTy) {\n-    unsigned elems = getElemsPerThread(resultTy);\n-    Type elemTy =\n-        getTypeConverter()->convertType(resultTensorTy.getElementType());\n-    SmallVector<Type> types(elems, elemTy);\n-    Type structTy = LLVM::LLVMStructType::getLiteral(getContext(), types);\n-    auto ptrs = getElementsFromStruct(loc, adaptor.ptr(), rewriter);\n-    auto offsets = getElementsFromStruct(loc, adaptor.offset(), rewriter);\n-    SmallVector<Value> resultVals(elems);\n-    for (unsigned i = 0; i < elems; ++i) {\n-      resultVals[i] = gep(elemTy, ptrs[i], offsets[i]);\n-    }\n-    Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n-    rewriter.replaceOp(op, view);\n-  } else {\n-    assert(resultTy.isa<triton::PointerType>());\n-    Type llResultTy = getTypeConverter()->convertType(resultTy);\n-    Value result = gep(llResultTy, adaptor.ptr(), adaptor.offset());\n-    rewriter.replaceOp(op, result);\n-  }\n-  return success();\n-}\n-\n-LogicalResult\n-AllocTensorOpConversion::matchAndRewrite(\n-    triton::gpu::AllocTensorOp op, OpAdaptor adaptor,\n-    ConversionPatternRewriter &rewriter) const {\n-  Location loc = op->getLoc();\n-  Value smemBase = getSharedMemoryBase(loc, rewriter, op.getResult());\n-  auto resultTy = op.getType().dyn_cast<RankedTensorType>();\n-  auto llvmElemTy =\n-      getTypeConverter()->convertType(resultTy.getElementType());\n-  auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n-  smemBase = bitcast(smemBase, elemPtrTy);\n-  auto order = resultTy.getEncoding().cast<SharedEncodingAttr>().getOrder();\n-  // Workaround for 3D tensors\n-  // TODO: we need to modify the pipeline pass to give a proper shared\n-  // encoding to 3D tensors\n-  SmallVector<unsigned> newOrder;\n-  if (resultTy.getShape().size() == 3)\n-    newOrder = {1 + order[0], 1 + order[1], 0};\n-  else\n-    newOrder = SmallVector<unsigned>(order.begin(), order.end());\n-\n-  auto smemObj = SharedMemoryObject(smemBase, resultTy.getShape(), newOrder,\n-                                    loc, rewriter);\n-  auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n-  rewriter.replaceOp(op, retVal);\n-  return success();\n-}\n-\n-LogicalResult\n-ExtractSliceOpConversion::matchAndRewrite(\n-    tensor::ExtractSliceOp op, OpAdaptor adaptor,\n-    ConversionPatternRewriter &rewriter) const {\n-  // %dst = extract_slice %src[%offsets]\n-  Location loc = op->getLoc();\n-  auto srcTy = op.source().getType().dyn_cast<RankedTensorType>();\n-  auto srcLayout = srcTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n-  assert(srcLayout && \"Unexpected resultLayout in ExtractSliceOpConversion\");\n-  assert(op.hasUnitStride() &&\n-         \"Only unit stride supported by ExtractSliceOpConversion\");\n-\n-  // newBase = base + offset\n-  // Triton supports either static and dynamic offsets\n-  auto smemObj =\n-      getSharedMemoryObjectFromStruct(loc, adaptor.source(), rewriter);\n-  SmallVector<Value, 4> opOffsetVals;\n-  SmallVector<Value, 4> offsetVals;\n-  auto mixedOffsets = op.getMixedOffsets();\n-  for (auto i = 0; i < mixedOffsets.size(); ++i) {\n-    if (op.isDynamicOffset(i))\n-      opOffsetVals.emplace_back(adaptor.offsets()[i]);\n-    else\n-      opOffsetVals.emplace_back(i32_val(op.getStaticOffset(i)));\n-    offsetVals.emplace_back(add(smemObj.offsets[i], opOffsetVals[i]));\n-  }\n-  // Compute the offset based on the original strides of the shared memory\n-  // object\n-  auto offset = dot(rewriter, loc, opOffsetVals, smemObj.strides);\n-  // newShape = rank_reduce(shape)\n-  // Triton only supports static tensor sizes\n-  SmallVector<Value, 4> strideVals;\n-  for (auto i = 0; i < op.static_sizes().size(); ++i) {\n-    if (op.getStaticSize(i) == 1) {\n-      offsetVals.erase(offsetVals.begin() + i);\n-    } else {\n-      strideVals.emplace_back(smemObj.strides[i]);\n-    }\n-  }\n-\n-  auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n-  auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n-  auto resTy = op.getType().dyn_cast<RankedTensorType>();\n-  smemObj = SharedMemoryObject(gep(elemPtrTy, smemObj.base, offset),\n-                               strideVals, offsetVals);\n-  auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n-  rewriter.replaceOp(op, retVal);\n-  return success();\n-}\n-\n-SmallVector<Value> FpToFpOpConversion::convertFp8x4ToFp16x4(\n-    Location loc, ConversionPatternRewriter &rewriter,\n-    const Value &v0, const Value &v1, const Value &v2, const Value &v3) {\n-  auto ctx = rewriter.getContext();\n-  auto fp8x4VecTy = vec_ty(i8_ty, 4);\n-  Value fp8x4Vec = undef(fp8x4VecTy);\n-  fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v0, i32_val(0));\n-  fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v1, i32_val(1));\n-  fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v2, i32_val(2));\n-  fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v3, i32_val(3));\n-  fp8x4Vec = bitcast(fp8x4Vec, i32_ty);\n-\n-  PTXBuilder builder;\n-  auto *ptxAsm = \"{                                      \\n\"\n-                 \".reg .b32 a<2>, b<2>;                  \\n\"\n-                 \"prmt.b32 a0, 0, $2, 0x5040;            \\n\"\n-                 \"prmt.b32 a1, 0, $2, 0x7060;            \\n\"\n-                 \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                 \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                 \"shr.b32  b0, b0, 1;                    \\n\"\n-                 \"shr.b32  b1, b1, 1;                    \\n\"\n-                 \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\"\n-                 \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\"\n-                 \"}\";\n-  auto &call = *builder.create(ptxAsm);\n-\n-  auto *o0 = builder.newOperand(\"=r\");\n-  auto *o1 = builder.newOperand(\"=r\");\n-  auto *i = builder.newOperand(fp8x4Vec, \"r\");\n-  call({o0, o1, i}, /*onlyAttachMLIRArgs=*/true);\n-\n-  auto fp16x2VecTy = vec_ty(f16_ty, 2);\n-  auto fp16x2x2StructTy =\n-      struct_ty(SmallVector<Type>{fp16x2VecTy, fp16x2VecTy});\n-  auto fp16x2x2Struct =\n-      builder.launch(rewriter, loc, fp16x2x2StructTy, false);\n-  auto fp16x2Vec0 =\n-      extract_val(fp16x2VecTy, fp16x2x2Struct, rewriter.getI32ArrayAttr({0}));\n-  auto fp16x2Vec1 =\n-      extract_val(fp16x2VecTy, fp16x2x2Struct, rewriter.getI32ArrayAttr({1}));\n-  return {extract_element(f16_ty, fp16x2Vec0, i32_val(0)),\n-          extract_element(f16_ty, fp16x2Vec0, i32_val(1)),\n-          extract_element(f16_ty, fp16x2Vec1, i32_val(0)),\n-          extract_element(f16_ty, fp16x2Vec1, i32_val(1))};\n-}\n-\n-SmallVector<Value>\n-FpToFpOpConversion::convertFp16x4ToFp8x4(\n-    Location loc, ConversionPatternRewriter &rewriter,\n-    const Value &v0, const Value &v1, const Value &v2, const Value &v3) {\n-  auto ctx = rewriter.getContext();\n-  auto fp16x2VecTy = vec_ty(f16_ty, 2);\n-  Value fp16x2Vec0 = undef(fp16x2VecTy);\n-  Value fp16x2Vec1 = undef(fp16x2VecTy);\n-  fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v0, i32_val(0));\n-  fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v1, i32_val(1));\n-  fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v2, i32_val(0));\n-  fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v3, i32_val(1));\n-  fp16x2Vec0 = bitcast(fp16x2Vec0, i32_ty);\n-  fp16x2Vec1 = bitcast(fp16x2Vec1, i32_ty);\n-\n-  PTXBuilder builder;\n-  auto *ptxAsm = \"{                                      \\n\"\n-                 \".reg .b32 a<2>, b<2>;                  \\n\"\n-                 \"shl.b32 a0, $1, 1;                     \\n\"\n-                 \"shl.b32 a1, $2, 1;                     \\n\"\n-                 \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                 \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                 \"add.u32 a0, a0, 0x00800080;            \\n\"\n-                 \"add.u32 a1, a1, 0x00800080;            \\n\"\n-                 \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\"\n-                 \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\"\n-                 \"prmt.b32 $0, b0, b1, 0x7531;           \\n\"\n-                 \"}\";\n-  auto &call = *builder.create(ptxAsm);\n-\n-  auto *o = builder.newOperand(\"=r\");\n-  auto *i0 = builder.newOperand(fp16x2Vec0, \"r\");\n-  auto *i1 = builder.newOperand(fp16x2Vec1, \"r\");\n-  call({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n-\n-  auto fp8x4VecTy = vec_ty(i8_ty, 4);\n-  auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n-  return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n-          extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n-          extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n-          extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n-}\n-\n-SmallVector<Value>\n-FpToFpOpConversion::convertFp8x4ToBf16x4(\n-    Location loc, ConversionPatternRewriter &rewriter,\n-    const Value &v0, const Value &v1, const Value &v2, const Value &v3) {\n-  auto ctx = rewriter.getContext();\n-  auto fp8x4VecTy = vec_ty(i8_ty, 4);\n-  Value fp8x4Vec = undef(fp8x4VecTy);\n-  fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v0, i32_val(0));\n-  fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v1, i32_val(1));\n-  fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v2, i32_val(2));\n-  fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v3, i32_val(3));\n-  fp8x4Vec = bitcast(fp8x4Vec, i32_ty);\n-\n-  PTXBuilder builder;\n-  auto *ptxAsm = \"{                                          \\n\"\n-                 \".reg .b32 a<2>, sign<2>, nosign<2>, b<2>;  \\n\"\n-                 \"prmt.b32 a0, 0, $2, 0x5040;                \\n\"\n-                 \"prmt.b32 a1, 0, $2, 0x7060;                \\n\"\n-                 \"and.b32 sign0, a0, 0x80008000;             \\n\"\n-                 \"and.b32 sign1, a1, 0x80008000;             \\n\"\n-                 \"and.b32 nosign0, a0, 0x7fff7fff;           \\n\"\n-                 \"and.b32 nosign1, a1, 0x7fff7fff;           \\n\"\n-                 \"shr.b32 nosign0, nosign0, 4;               \\n\"\n-                 \"shr.b32 nosign1, nosign1, 4;               \\n\"\n-                 \"add.u32 nosign0, nosign0, 0x38003800;      \\n\"\n-                 \"add.u32 nosign1, nosign1, 0x38003800;      \\n\"\n-                 \"or.b32 $0, sign0, nosign0;                 \\n\"\n-                 \"or.b32 $1, sign1, nosign1;                 \\n\"\n-                 \"}\";\n-  auto &call = *builder.create(ptxAsm);\n-\n-  auto *o0 = builder.newOperand(\"=r\");\n-  auto *o1 = builder.newOperand(\"=r\");\n-  auto *i = builder.newOperand(fp8x4Vec, \"r\");\n-  call({o0, o1, i}, /* onlyAttachMLIRArgs */ true);\n-\n-  auto bf16x2VecTy = vec_ty(bf16_ty, 2);\n-  auto bf16x2x2StructTy =\n-      struct_ty(SmallVector<Type>{bf16x2VecTy, bf16x2VecTy});\n-  auto bf16x2x2Struct =\n-      builder.launch(rewriter, loc, bf16x2x2StructTy, false);\n-  auto bf16x2Vec0 =\n-      extract_val(bf16x2VecTy, bf16x2x2Struct, rewriter.getI32ArrayAttr({0}));\n-  auto bf16x2Vec1 =\n-      extract_val(bf16x2VecTy, bf16x2x2Struct, rewriter.getI32ArrayAttr({1}));\n-  return {extract_element(bf16_ty, bf16x2Vec0, i32_val(0)),\n-          extract_element(bf16_ty, bf16x2Vec0, i32_val(1)),\n-          extract_element(bf16_ty, bf16x2Vec1, i32_val(0)),\n-          extract_element(bf16_ty, bf16x2Vec1, i32_val(1))};\n-}\n-\n-SmallVector<Value>\n-FpToFpOpConversion::convertBf16x4ToFp8x4(\n-    Location loc, ConversionPatternRewriter &rewriter,\n-    const Value &v0, const Value &v1, const Value &v2, const Value &v3) {\n-  auto ctx = rewriter.getContext();\n-  auto bf16x2VecTy = vec_ty(bf16_ty, 2);\n-  Value bf16x2Vec0 = undef(bf16x2VecTy);\n-  Value bf16x2Vec1 = undef(bf16x2VecTy);\n-  bf16x2Vec0 = insert_element(bf16x2VecTy, bf16x2Vec0, v0, i32_val(0));\n-  bf16x2Vec0 = insert_element(bf16x2VecTy, bf16x2Vec0, v1, i32_val(1));\n-  bf16x2Vec1 = insert_element(bf16x2VecTy, bf16x2Vec1, v2, i32_val(0));\n-  bf16x2Vec1 = insert_element(bf16x2VecTy, bf16x2Vec1, v3, i32_val(1));\n-  bf16x2Vec0 = bitcast(bf16x2Vec0, i32_ty);\n-  bf16x2Vec1 = bitcast(bf16x2Vec1, i32_ty);\n-\n-  PTXBuilder builder;\n-  auto *ptxAsm = \"{                                            \\n\"\n-                 \".reg .u32 sign, sign<2>, nosign, nosign<2>;  \\n\"\n-                 \".reg .u32 fp8_min, fp8_max, rn_, zero;       \\n\"\n-                 \"mov.u32 fp8_min, 0x38003800;                 \\n\"\n-                 \"mov.u32 fp8_max, 0x3ff03ff0;                 \\n\"\n-                 \"mov.u32 rn_, 0x80008;                        \\n\"\n-                 \"mov.u32 zero, 0;                             \\n\"\n-                 \"and.b32 sign0, $1, 0x80008000;               \\n\"\n-                 \"and.b32 sign1, $2, 0x80008000;               \\n\"\n-                 \"prmt.b32 sign, sign0, sign1, 0x7531;         \\n\"\n-                 \"and.b32 nosign0, $1, 0x7fff7fff;             \\n\"\n-                 \"and.b32 nosign1, $2, 0x7fff7fff;             \\n\"\n-                 \".reg .u32 nosign_0_<2>, nosign_1_<2>;        \\n\"\n-                 \"and.b32 nosign_0_0, nosign0, 0xffff0000;     \\n\"\n-                 \"max.u32 nosign_0_0, nosign_0_0, 0x38000000;  \\n\"\n-                 \"min.u32 nosign_0_0, nosign_0_0, 0x3ff00000;  \\n\"\n-                 \"and.b32 nosign_0_1, nosign0, 0x0000ffff;     \\n\"\n-                 \"max.u32 nosign_0_1, nosign_0_1, 0x3800;      \\n\"\n-                 \"min.u32 nosign_0_1, nosign_0_1, 0x3ff0;      \\n\"\n-                 \"or.b32 nosign0, nosign_0_0, nosign_0_1;      \\n\"\n-                 \"and.b32 nosign_1_0, nosign1, 0xffff0000;     \\n\"\n-                 \"max.u32 nosign_1_0, nosign_1_0, 0x38000000;  \\n\"\n-                 \"min.u32 nosign_1_0, nosign_1_0, 0x3ff00000;  \\n\"\n-                 \"and.b32 nosign_1_1, nosign1, 0x0000ffff;     \\n\"\n-                 \"max.u32 nosign_1_1, nosign_1_1, 0x3800;      \\n\"\n-                 \"min.u32 nosign_1_1, nosign_1_1, 0x3ff0;      \\n\"\n-                 \"or.b32 nosign1, nosign_1_0, nosign_1_1;      \\n\"\n-                 \"add.u32 nosign0, nosign0, rn_;               \\n\"\n-                 \"add.u32 nosign1, nosign1, rn_;               \\n\"\n-                 \"sub.u32 nosign0, nosign0, 0x38003800;        \\n\"\n-                 \"sub.u32 nosign1, nosign1, 0x38003800;        \\n\"\n-                 \"shr.u32 nosign0, nosign0, 4;                 \\n\"\n-                 \"shr.u32 nosign1, nosign1, 4;                 \\n\"\n-                 \"prmt.b32 nosign, nosign0, nosign1, 0x6420;   \\n\"\n-                 \"or.b32 $0, nosign, sign;                     \\n\"\n-                 \"}\";\n-  auto &call = *builder.create(ptxAsm);\n-\n-  auto *o = builder.newOperand(\"=r\");\n-  auto *i0 = builder.newOperand(bf16x2Vec0, \"r\");\n-  auto *i1 = builder.newOperand(bf16x2Vec1, \"r\");\n-  call({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n-\n-  auto fp8x4VecTy = vec_ty(i8_ty, 4);\n-  auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n-  return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n-          extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n-          extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n-          extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n-}\n-\n-SmallVector<Value>\n-FpToFpOpConversion::convertFp8x4ToFp32x4(\n-    Location loc, ConversionPatternRewriter &rewriter,\n-    const Value &v0, const Value &v1, const Value &v2, const Value &v3) {\n-  auto fp16Values = convertFp8x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n-  return {rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[0]),\n-          rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[1]),\n-          rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[2]),\n-          rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[3])};\n-}\n-\n-SmallVector<Value>\n-FpToFpOpConversion::convertFp32x4ToFp8x4(\n-    Location loc, ConversionPatternRewriter &rewriter,\n-    const Value &v0, const Value &v1, const Value &v2, const Value &v3) {\n-  auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n-  auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n-  auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n-  auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n-  return convertFp16x4ToFp8x4(loc, rewriter, c0, c1, c2, c3);\n-}\n-\n-SmallVector<Value>\n-FpToFpOpConversion::convertFp8x4ToFp64x4(\n-    Location loc, ConversionPatternRewriter &rewriter,\n-    const Value &v0, const Value &v1, const Value &v2, const Value &v3) {\n-  auto fp16Values = convertFp8x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n-  return {rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[0]),\n-          rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[1]),\n-          rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[2]),\n-          rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[3])};\n-}\n-\n-SmallVector<Value>\n-FpToFpOpConversion::convertFp64x4ToFp8x4(\n-    Location loc, ConversionPatternRewriter &rewriter,\n-    const Value &v0, const Value &v1, const Value &v2, const Value &v3) {\n-  auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n-  auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n-  auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n-  auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n-  return convertFp16x4ToFp8x4(loc, rewriter, c0, c1, c2, c3);\n-}\n-\n-LogicalResult\n-FpToFpOpConversion::matchAndRewrite(\n-    triton::FpToFpOp op, OpAdaptor adaptor,\n-    ConversionPatternRewriter &rewriter) const {\n-  auto srcTensorType = op.from().getType().cast<mlir::RankedTensorType>();\n-  auto dstTensorType = op.result().getType().cast<mlir::RankedTensorType>();\n-  auto srcEltType = srcTensorType.getElementType();\n-  auto dstEltType = dstTensorType.getElementType();\n-  assert(srcEltType.isa<triton::Float8Type>() ||\n-         dstEltType.isa<triton::Float8Type>());\n-  auto convertedDstTensorType =\n-      this->getTypeConverter()->convertType(dstTensorType);\n-  auto convertedDstEleType =\n-      this->getTypeConverter()->convertType(dstEltType);\n-\n-  // Select convertor\n-  std::function<SmallVector<Value>(Location, ConversionPatternRewriter &,\n-                                   const Value &, const Value &,\n-                                   const Value &, const Value &)>\n-      convertor;\n-  if (srcEltType.isa<triton::Float8Type>() && dstEltType.isF16()) {\n-    convertor = convertFp8x4ToFp16x4;\n-  } else if (srcEltType.isF16() && dstEltType.isa<triton::Float8Type>()) {\n-    convertor = convertFp16x4ToFp8x4;\n-  } else if (srcEltType.isa<triton::Float8Type>() && dstEltType.isBF16()) {\n-    convertor = convertFp8x4ToBf16x4;\n-  } else if (srcEltType.isBF16() && dstEltType.isa<triton::Float8Type>()) {\n-    convertor = convertBf16x4ToFp8x4;\n-  } else if (srcEltType.isa<triton::Float8Type>() && dstEltType.isF32()) {\n-    convertor = convertFp8x4ToFp32x4;\n-  } else if (srcEltType.isF32() && dstEltType.isa<triton::Float8Type>()) {\n-    convertor = convertFp32x4ToFp8x4;\n-  } else if (srcEltType.isa<triton::Float8Type>() && dstEltType.isF64()) {\n-    convertor = convertFp8x4ToFp64x4;\n-  } else if (srcEltType.isF64() && dstEltType.isa<triton::Float8Type>()) {\n-    convertor = convertFp64x4ToFp8x4;\n-  } else {\n-    assert(false && \"unsupported type casting\");\n-  }\n-\n-  // Vectorized casting\n-  auto loc = op->getLoc();\n-  auto elems = getElemsPerThread(dstTensorType);\n-  assert(elems % 4 == 0 &&\n-         \"FP8 casting only support tensors with 4-aligned sizes\");\n-  auto elements = getElementsFromStruct(loc, adaptor.from(), rewriter);\n-  SmallVector<Value> resultVals;\n-  for (size_t i = 0; i < elems; i += 4) {\n-    auto converted = convertor(loc, rewriter, elements[i], elements[i + 1],\n-                               elements[i + 2], elements[i + 3]);\n-    resultVals.append(converted);\n-  }\n-  assert(resultVals.size() == elems);\n-  auto result = getStructFromElements(loc, resultVals, rewriter,\n-                                      convertedDstTensorType);\n-  rewriter.replaceOp(op, result);\n-  return success();\n-}\n-\n-LogicalResult InsertSliceOpConversion::matchAndRewrite(\n-    tensor::InsertSliceOp op, OpAdaptor adaptor,\n-    ConversionPatternRewriter &rewriter) const {\n-  // %dst = insert_slice %src into %dst[%offsets]\n-  Location loc = op->getLoc();\n-  Value dst = op.dest();\n-  Value src = op.source();\n-  Value res = op.result();\n-  assert(allocation->getBufferId(res) == Allocation::InvalidBufferId &&\n-         \"Only support in-place insert_slice for now\");\n-\n-  auto srcTy = src.getType().dyn_cast<RankedTensorType>();\n-  auto srcLayout = srcTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n-  auto srcShape = srcTy.getShape();\n-  assert(srcLayout && \"Unexpected srcLayout in InsertSliceOpConversion\");\n-\n-  auto dstTy = dst.getType().dyn_cast<RankedTensorType>();\n-  auto dstLayout = dstTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n-  auto llDst = adaptor.dest();\n-  assert(dstLayout && \"Unexpected dstLayout in InsertSliceOpConversion\");\n-  assert(op.hasUnitStride() &&\n-         \"Only unit stride supported by InsertSliceOpConversion\");\n-\n-  // newBase = base + offset\n-  // Triton support either static and dynamic offsets\n-  auto smemObj = getSharedMemoryObjectFromStruct(loc, llDst, rewriter);\n-  SmallVector<Value, 4> offsets;\n-  SmallVector<Value, 4> srcStrides;\n-  auto mixedOffsets = op.getMixedOffsets();\n-  for (auto i = 0; i < mixedOffsets.size(); ++i) {\n-    if (op.isDynamicOffset(i)) {\n-      offsets.emplace_back(adaptor.offsets()[i]);\n-    } else {\n-      offsets.emplace_back(i32_val(op.getStaticOffset(i)));\n-    }\n-    // Like insert_slice_async, we only support slice from one dimension,\n-    // which has a slice size of 1\n-    if (op.getStaticSize(i) != 1) {\n-      srcStrides.emplace_back(smemObj.strides[i]);\n-    }\n-  }\n-\n-  // Compute the offset based on the original strides of the shared memory\n-  // object\n-  auto offset = dot(rewriter, loc, offsets, smemObj.strides);\n-  auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n-  auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n-  auto smemBase = gep(elemPtrTy, smemObj.base, offset);\n-\n-  auto llSrc = adaptor.source();\n-  auto srcIndices =\n-      emitBaseIndexForBlockedLayout(loc, rewriter, srcLayout, srcShape);\n-  ConvertLayoutOpConversion::storeBlockedToShared(src, llSrc, srcStrides,\n-                                                  srcIndices, dst, smemBase,\n-                                                  elemPtrTy, loc, rewriter);\n-  // Barrier is not necessary.\n-  // The membar pass knows that it writes to shared memory and will handle it\n-  // properly.\n-  rewriter.replaceOp(op, llDst);\n-  return success();\n-}\n-\n-LogicalResult AsyncWaitOpConversion::matchAndRewrite(\n-    triton::gpu::AsyncWaitOp op, OpAdaptor adaptor,\n-    ConversionPatternRewriter &rewriter) const {\n-  PTXBuilder ptxBuilder;\n-  auto &asyncWaitOp = *ptxBuilder.create<>(\"cp.async.wait_group\");\n-  auto num = op->getAttrOfType<IntegerAttr>(\"num\").getInt();\n-  asyncWaitOp(ptxBuilder.newConstantOperand(num));\n-\n-  auto ctx = op.getContext();\n-  auto loc = op.getLoc();\n-  auto voidTy = void_ty(ctx);\n-  ptxBuilder.launch(rewriter, loc, voidTy);\n-\n-  // Safe to remove the op since it doesn't have any return value.\n-  rewriter.eraseOp(op);\n-  return success();\n-}\n-\n-LogicalResult InsertSliceAsyncOpConversion::matchAndRewrite(\n-    triton::gpu::InsertSliceAsyncOp op, OpAdaptor adaptor,\n-    ConversionPatternRewriter &rewriter) const {\n-  // insert_slice_async %src, %dst, %index, %mask, %other\n-  auto loc = op.getLoc();\n-  Value src = op.src();\n-  Value dst = op.dst();\n-  Value res = op.result();\n-  Value mask = op.mask();\n-  Value other = op.other();\n-  assert(allocation->getBufferId(res) == Allocation::InvalidBufferId &&\n-         \"Only support in-place insert_slice_async for now\");\n-\n-  auto srcTy = src.getType().cast<RankedTensorType>();\n-  auto resTy = dst.getType().cast<RankedTensorType>();\n-  auto resElemTy = getTypeConverter()->convertType(resTy.getElementType());\n-  auto srcBlockedLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n-  auto resSharedLayout = resTy.getEncoding().cast<SharedEncodingAttr>();\n-  auto srcShape = srcTy.getShape();\n-  assert(srcShape.size() == 2 &&\n-         \"insert_slice_async: Unexpected rank of %src\");\n-\n-  Value llDst = adaptor.dst();\n-  Value llSrc = adaptor.src();\n-  Value llMask = adaptor.mask();\n-  Value llOther = adaptor.other();\n-  Value llIndex = adaptor.index();\n-\n-  // %src\n-  auto srcElems = getLLVMElems(src, llSrc, rewriter, loc);\n-\n-  // %dst\n-  auto dstTy = dst.getType().cast<RankedTensorType>();\n-  auto dstShape = dstTy.getShape();\n-  auto smemObj = getSharedMemoryObjectFromStruct(loc, llDst, rewriter);\n-  auto axis = op->getAttrOfType<IntegerAttr>(\"axis\").getInt();\n-  SmallVector<Value, 4> offsetVals;\n-  SmallVector<Value, 4> srcStrides;\n-  for (auto i = 0; i < dstShape.size(); ++i) {\n-    if (i == axis) {\n-      offsetVals.emplace_back(llIndex);\n-    } else {\n-      offsetVals.emplace_back(i32_val(0));\n-      srcStrides.emplace_back(smemObj.strides[i]);\n-    }\n-  }\n-  // Compute the offset based on the original dimensions of the shared\n-  // memory object\n-  auto dstOffset = dot(rewriter, loc, offsetVals, smemObj.strides);\n-  auto dstPtrTy =\n-      ptr_ty(getTypeConverter()->convertType(resTy.getElementType()), 3);\n-  Value dstPtrBase = gep(dstPtrTy, smemObj.base, dstOffset);\n-\n-  // %mask\n-  SmallVector<Value> maskElems;\n-  if (llMask) {\n-    maskElems = getLLVMElems(mask, llMask, rewriter, loc);\n-    assert(srcElems.size() == maskElems.size());\n-  }\n-\n-  // %other\n-  SmallVector<Value> otherElems;\n-  if (llOther) {\n-    // FIXME(Keren): always assume other is 0 for now\n-    // It's not necessary for now because the pipeline pass will skip\n-    // generating insert_slice_async if the load op has any \"other\" tensor.\n-    // assert(false && \"insert_slice_async: Other value not supported yet\");\n-    otherElems = getLLVMElems(other, llOther, rewriter, loc);\n-    assert(srcElems.size() == otherElems.size());\n-  }\n-\n-  unsigned inVec = getVectorSize(src);\n-  unsigned outVec = resSharedLayout.getVec();\n-  unsigned minVec = std::min(outVec, inVec);\n-  unsigned numElems = getElemsPerThread(srcTy);\n-  unsigned perPhase = resSharedLayout.getPerPhase();\n-  unsigned maxPhase = resSharedLayout.getMaxPhase();\n-  auto sizePerThread = srcBlockedLayout.getSizePerThread();\n-  auto threadsPerCTA = getThreadsPerCTA(srcBlockedLayout);\n-  auto inOrder = srcBlockedLayout.getOrder();\n-\n-  // If perPhase * maxPhase > threadsPerCTA, we will have elements\n-  // that share the same tile indices. The index calculation will\n-  // be cached.\n-  auto numSwizzleRows = std::max<unsigned>(\n-      (perPhase * maxPhase) / threadsPerCTA[inOrder[1]], 1);\n-  // A sharedLayout encoding has a \"vec\" parameter.\n-  // On the column dimension, if inVec > outVec, it means we have to divide\n-  // single vector read into multiple ones\n-  auto numVecCols = std::max<unsigned>(inVec / outVec, 1);\n-\n-  auto srcIndices = emitIndices(loc, rewriter, srcBlockedLayout, srcShape);\n-  //  <<tileVecIdxRow, tileVecIdxCol>, TileOffset>\n-  DenseMap<std::pair<unsigned, unsigned>, Value> tileOffsetMap;\n-  for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n-    // minVec = 2, inVec = 4, outVec = 2\n-    //   baseOffsetCol = 0   baseOffsetCol = 0\n-    //   tileVecIdxCol = 0   tileVecIdxCol = 1\n-    //                -/\\-   -/\\-\n-    //               [|x x| |x x| x x x x x]\n-    //               [|x x| |x x| x x x x x]\n-    // baseOffsetRow [|x x| |x x| x x x x x]\n-    //               [|x x| |x x| x x x x x]\n-    auto vecIdx = elemIdx / minVec;\n-    auto vecIdxCol = vecIdx % (sizePerThread[inOrder[0]] / minVec);\n-    auto vecIdxRow = vecIdx / (sizePerThread[inOrder[0]] / minVec);\n-    auto baseOffsetCol =\n-        vecIdxCol / numVecCols * numVecCols * threadsPerCTA[inOrder[0]];\n-    auto baseOffsetRow = vecIdxRow / numSwizzleRows * numSwizzleRows *\n-                         threadsPerCTA[inOrder[1]];\n-    auto tileVecIdxCol = vecIdxCol % numVecCols;\n-    auto tileVecIdxRow = vecIdxRow % numSwizzleRows;\n-\n-    if (!tileOffsetMap.count({tileVecIdxRow, tileVecIdxCol})) {\n-      // Swizzling\n-      // Since the swizzling index is related to outVec, and we know minVec\n-      // already, inVec doesn't matter\n-      //\n-      // (Numbers represent row indices)\n-      // Example1:\n-      // outVec = 2, inVec = 2, minVec = 2\n-      // outVec = 2, inVec = 4, minVec = 2\n-      //     | [1 2] [3 4] [5 6] ... |\n-      //     | [3 4] [1 2] [7 8] ... |\n-      //     | [5 6] [7 8] [1 2] ... |\n-      // Example2:\n-      // outVec = 4, inVec = 2, minVec = 2\n-      //     | [1 2 3 4] [5 6 7 8] [9 10 11 12] ... |\n-      //     | [5 6 7 8] [1 2 3 4] [13 14 15 16] ... |\n-      //     | [9 10 11 12] [13 14 15 16] [1 2 3 4] ... |\n-      auto srcIdx = srcIndices[tileVecIdxRow * sizePerThread[inOrder[0]]];\n-      Value phase = urem(udiv(srcIdx[inOrder[1]], i32_val(perPhase)),\n-                         i32_val(maxPhase));\n-      // srcShape and smemObj.shape maybe different if smemObj is a\n-      // slice of the original shared memory object.\n-      // So we need to use the original shape to compute the offset\n-      Value rowOffset = mul(srcIdx[inOrder[1]], srcStrides[inOrder[1]]);\n-      Value colOffset =\n-          add(srcIdx[inOrder[0]], i32_val(tileVecIdxCol * minVec));\n-      Value swizzleIdx = udiv(colOffset, i32_val(outVec));\n-      Value swizzleColOffset =\n-          add(mul(xor_(swizzleIdx, phase), i32_val(outVec)),\n-              urem(colOffset, i32_val(outVec)));\n-      Value tileOffset = add(rowOffset, swizzleColOffset);\n-      tileOffsetMap[{tileVecIdxRow, tileVecIdxCol}] =\n-          gep(dstPtrTy, dstPtrBase, tileOffset);\n-    }\n-\n-    // 16 * 8 = 128bits\n-    auto maxBitWidth =\n-        std::max<unsigned>(128, resElemTy.getIntOrFloatBitWidth());\n-    auto vecBitWidth = resElemTy.getIntOrFloatBitWidth() * minVec;\n-    auto bitWidth = std::min<unsigned>(maxBitWidth, vecBitWidth);\n-    auto numWords = vecBitWidth / bitWidth;\n-    auto numWordElems = bitWidth / resElemTy.getIntOrFloatBitWidth();\n-\n-    // Tune CG and CA here.\n-    auto byteWidth = bitWidth / 8;\n-    CacheModifier srcCacheModifier =\n-        byteWidth == 16 ? CacheModifier::CG : CacheModifier::CA;\n-    assert(byteWidth == 16 || byteWidth == 8 || byteWidth == 4);\n-    auto resByteWidth = resElemTy.getIntOrFloatBitWidth() / 8;\n-\n-    Value tileOffset = tileOffsetMap[{tileVecIdxRow, tileVecIdxCol}];\n-    Value baseOffset =\n-        add(mul(i32_val(baseOffsetRow), srcStrides[inOrder[1]]),\n-            i32_val(baseOffsetCol));\n-    Value basePtr = gep(dstPtrTy, tileOffset, baseOffset);\n-    for (size_t wordIdx = 0; wordIdx < numWords; ++wordIdx) {\n-      PTXBuilder ptxBuilder;\n-      auto wordElemIdx = wordIdx * numWordElems;\n-      auto &copyAsyncOp =\n-          *ptxBuilder.create<PTXCpAsyncLoadInstr>(srcCacheModifier);\n-      auto *dstOperand =\n-          ptxBuilder.newAddrOperand(basePtr, \"r\", wordElemIdx * resByteWidth);\n-      auto *srcOperand =\n-          ptxBuilder.newAddrOperand(srcElems[elemIdx + wordElemIdx], \"l\");\n-      auto *copySize = ptxBuilder.newConstantOperand(byteWidth);\n-      auto *srcSize = copySize;\n-      if (op.mask()) {\n-        // We don't use predicate in this case, setting src-size to 0\n-        // if there's any mask. cp.async will automatically fill the\n-        // remaining slots with 0 if cp-size > src-size.\n-        // XXX(Keren): Always assume other = 0 for now.\n-        auto selectOp = select(maskElems[elemIdx + wordElemIdx],\n-                               i32_val(byteWidth), i32_val(0));\n-        srcSize = ptxBuilder.newOperand(selectOp, \"r\");\n-      }\n-      copyAsyncOp(dstOperand, srcOperand, copySize, srcSize);\n-      ptxBuilder.launch(rewriter, loc, void_ty(getContext()));\n-    }\n-  }\n-\n-  PTXBuilder ptxBuilder;\n-  ptxBuilder.create<>(\"cp.async.commit_group\")->operator()();\n-  ptxBuilder.launch(rewriter, loc, void_ty(getContext()));\n-  rewriter.replaceOp(op, llDst);\n-  return success();\n-}\n-\n-void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                  RewritePatternSet &patterns, int numWarps,\n-                                  AxisInfoAnalysis &axisInfoAnalysis,\n-                                  const Allocation *allocation, Value smem,\n-                                  PatternBenefit benefit) {\n-  patterns.add<AddPtrOpConversion>(typeConverter, benefit);\n-  patterns.add<AllocTensorOpConversion>(typeConverter, allocation, smem,\n-                                        benefit);\n-  patterns.add<ArithConstantSplatOpConversion>(typeConverter, benefit);\n-  patterns.add<AsyncWaitOpConversion>(typeConverter, benefit);\n-\n-#define POPULATE_TERNARY_OP(SRC_OP, DST_OP)                                    \\\n-  patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n-  POPULATE_TERNARY_OP(triton::gpu::SelectOp, LLVM::SelectOp)\n-#undef POPULATE_TERNARY_OP\n-\n-#define POPULATE_BINARY_OP(SRC_OP, DST_OP)                                     \\\n-  patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n-\n-  POPULATE_BINARY_OP(arith::SubIOp, LLVM::SubOp) // -\n-  POPULATE_BINARY_OP(arith::SubFOp, LLVM::FSubOp)\n-  POPULATE_BINARY_OP(arith::AddIOp, LLVM::AddOp) // +\n-  POPULATE_BINARY_OP(arith::AddFOp, LLVM::FAddOp)\n-  POPULATE_BINARY_OP(arith::MulIOp, LLVM::MulOp) // *\n-  POPULATE_BINARY_OP(arith::MulFOp, LLVM::FMulOp)\n-  POPULATE_BINARY_OP(arith::DivFOp, LLVM::FDivOp) // /\n-  POPULATE_BINARY_OP(arith::DivSIOp, LLVM::SDivOp)\n-  POPULATE_BINARY_OP(arith::DivUIOp, LLVM::UDivOp)\n-  POPULATE_BINARY_OP(arith::RemFOp, LLVM::FRemOp) // %\n-  POPULATE_BINARY_OP(arith::RemSIOp, LLVM::SRemOp)\n-  POPULATE_BINARY_OP(arith::RemUIOp, LLVM::URemOp)\n-  POPULATE_BINARY_OP(arith::AndIOp, LLVM::AndOp)   // &\n-  POPULATE_BINARY_OP(arith::OrIOp, LLVM::OrOp)     // |\n-  POPULATE_BINARY_OP(arith::XOrIOp, LLVM::XOrOp)   // ^\n-  POPULATE_BINARY_OP(arith::ShLIOp, LLVM::ShlOp)   // <<\n-  POPULATE_BINARY_OP(arith::ShRSIOp, LLVM::AShrOp) // >>\n-  POPULATE_BINARY_OP(arith::ShRUIOp, LLVM::LShrOp) // >>\n-#undef POPULATE_BINARY_OP\n-\n-  patterns.add<CmpIOpConversion>(typeConverter, benefit);\n-  patterns.add<CmpFOpConversion>(typeConverter, benefit);\n-\n-  // ExpOpConversionApprox will try using ex2.approx if the input type is FP32.\n-  // For FP64 input type, ExpOpConversionApprox will return failure and\n-  // ElementwiseOpConversion<math::ExpOp, math::ExpOp> defined below will call\n-  // __nv_expf for higher-precision calculation\n-  patterns.add<ExpOpConversionApprox>(typeConverter, benefit);\n-\n-#define POPULATE_UNARY_OP(SRC_OP, DST_OP)                                      \\\n-  patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n-\n-  POPULATE_UNARY_OP(arith::TruncIOp, LLVM::TruncOp)\n-  POPULATE_UNARY_OP(arith::TruncFOp, LLVM::FPTruncOp)\n-  POPULATE_UNARY_OP(arith::ExtSIOp, LLVM::SExtOp)\n-  POPULATE_UNARY_OP(arith::ExtUIOp, LLVM::ZExtOp)\n-  POPULATE_UNARY_OP(arith::FPToUIOp, LLVM::FPToUIOp)\n-  POPULATE_UNARY_OP(arith::FPToSIOp, LLVM::FPToSIOp)\n-  POPULATE_UNARY_OP(arith::UIToFPOp, LLVM::UIToFPOp)\n-  POPULATE_UNARY_OP(arith::SIToFPOp, LLVM::SIToFPOp)\n-  POPULATE_UNARY_OP(arith::ExtFOp, LLVM::FPExtOp)\n-  POPULATE_UNARY_OP(math::LogOp, math::LogOp)\n-  POPULATE_UNARY_OP(math::CosOp, math::CosOp)\n-  POPULATE_UNARY_OP(math::SinOp, math::SinOp)\n-  POPULATE_UNARY_OP(math::SqrtOp, math::SqrtOp)\n-  POPULATE_UNARY_OP(math::ExpOp, math::ExpOp)\n-  POPULATE_UNARY_OP(triton::BitcastOp, LLVM::BitcastOp)\n-  POPULATE_UNARY_OP(triton::IntToPtrOp, LLVM::IntToPtrOp)\n-  POPULATE_UNARY_OP(triton::PtrToIntOp, LLVM::PtrToIntOp)\n-#undef POPULATE_UNARY_OP\n-\n-  patterns.add<FpToFpOpConversion>(typeConverter, benefit);\n-  patterns.add<FDivOpConversion>(typeConverter, benefit);\n-  patterns.add<ExtElemwiseOpConversion>(typeConverter, benefit);\n-  patterns.add<BroadcastOpConversion>(typeConverter, benefit);\n-  patterns.add<ReduceOpConversion>(typeConverter, allocation, smem, benefit);\n-  patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n-                                          benefit);\n-  patterns.add<AtomicCASOpConversion>(typeConverter, allocation, smem,\n-                                      axisInfoAnalysis, benefit);\n-  patterns.add<AtomicRMWOpConversion>(typeConverter, allocation, smem,\n-                                      axisInfoAnalysis, benefit);\n-  patterns.add<ExtractSliceOpConversion>(typeConverter, allocation, smem,\n-                                         benefit);\n-  patterns.add<GetProgramIdOpConversion>(typeConverter, benefit);\n-  patterns.add<GetNumProgramsOpConversion>(typeConverter, benefit);\n-  patterns.add<InsertSliceOpConversion>(typeConverter, allocation, smem,\n-                                        benefit);\n-  patterns.add<InsertSliceAsyncOpConversion>(typeConverter, allocation, smem,\n-                                             axisInfoAnalysis, benefit);\n-  patterns.add<LoadOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n-  patterns.add<MakeRangeOpConversion>(typeConverter, benefit);\n-  patterns.add<ReturnOpConversion>(typeConverter, benefit);\n-  patterns.add<SplatOpConversion>(typeConverter, benefit);\n-  patterns.add<StoreOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n-  patterns.add<ViewLikeOpConversion<triton::ViewOp>>(typeConverter, benefit);\n-  patterns.add<ViewLikeOpConversion<triton::ExpandDimsOp>>(typeConverter,\n-                                                           benefit);\n-  patterns.add<DotOpConversion>(typeConverter, allocation, smem, benefit);\n-  patterns.add<TransOpConversion>(typeConverter, benefit);\n-  patterns.add<CatOpConversion>(typeConverter, benefit);\n-  patterns.add<PrintfOpConversion>(typeConverter, benefit);\n-}"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 271, "deletions": 1, "changes": 272, "file_content_changes": "@@ -79,6 +79,41 @@ class DecomposeDotOperand : public mlir::RewritePattern {\n   }\n };\n \n+class SimplifyReduceCvt : public mlir::RewritePattern {\n+public:\n+  explicit SimplifyReduceCvt(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(triton::ReduceOp::getOperationName(),\n+                             2, context) {}\n+  \n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto reduce = cast<triton::ReduceOp>(*op);\n+    auto reduceArg = dyn_cast<triton::gpu::ConvertLayoutOp>(reduce.getOperand().getDefiningOp());\n+    if(!reduceArg)\n+      return mlir::failure();\n+    // this may generate unsupported conversions in the LLVM codegen\n+    if(reduceArg.getOperand().getType().cast<RankedTensorType>()\n+                .getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n+      return mlir::failure();\n+    auto newReduce = rewriter.create<triton::ReduceOp>(\n+        op->getLoc(), reduce.redOp(), reduceArg.getOperand(), reduce.axis());\n+    if(isa<triton::gpu::ConvertLayoutOp>(*reduceArg.getOperand().getDefiningOp()))\n+      return mlir::failure();\n+    Value newRet = newReduce.getResult();\n+    // it's still beneficial to move the conversion\n+    // to after the reduce if necessary since it will be\n+    // done on a rank-reduced tensor hence cheaper\n+    if(newRet.getType() != reduce.getResult().getType())\n+      newRet = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+          op->getLoc(), reduce.getResult().getType(), newRet);\n+    rewriter.replaceOp(op, newRet);\n+      \n+    return success();\n+  }\n+\n+};\n+\n // Layout conversions can't deduce their return type automatically.\n // IIUC they are therefore not handled by DRR right now\n class SimplifyConversion : public mlir::RewritePattern {\n@@ -218,6 +253,7 @@ class SimplifyConversion : public mlir::RewritePattern {\n //\n // -----------------------------------------------------------------------------\n \n+// TODO: Interface\n LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n                              Attribute &ret) {\n   ret = targetEncoding;\n@@ -235,6 +271,20 @@ LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n   return success();\n }\n \n+// TODO: Interface\n+LogicalResult getForwardEncoding(Attribute sourceEncoding, Operation *op,\n+                                 Attribute &ret) {\n+  if(op->hasTrait<mlir::OpTrait::Elementwise>()){\n+    ret = sourceEncoding;\n+    return success();\n+  }\n+  if(isa<triton::ReduceOp>(op)){\n+    ret = Attribute();\n+    return success();\n+  }\n+  return failure();\n+}\n+\n inline bool expensive_to_remat(Operation *op) {\n   if (!op)\n     return true;\n@@ -247,6 +297,65 @@ inline bool expensive_to_remat(Operation *op) {\n   return false;\n }\n \n+LogicalResult simulateBackwardRematerialization(Operation* initOp,\n+                                                SetVector<Operation*>& processed, \n+                                                SetVector<Attribute>& layout,\n+                                                llvm::MapVector<Value, Attribute>& toConvert,\n+                                                Attribute targetEncoding){\n+    // DFS\n+    std::vector<std::pair<Operation *, Attribute>> queue;\n+    queue.emplace_back(initOp, targetEncoding);\n+    // We want to see the effect of converting `initOp` to a new layout\n+    // so we initialize `numCvts = 1`.\n+    int numCvts = 1;\n+    while (!queue.empty()) {\n+      Operation *currOp;\n+      Attribute currLayout;\n+      std::tie(currOp, currLayout) = queue.back();\n+      queue.pop_back();\n+      // If the current operation is expensive to rematerialize,\n+      // we stop everything\n+      if (expensive_to_remat(currOp))\n+        return mlir::failure();\n+      // we would propagate the conversion here\n+      numCvts -= 1;\n+      // check if the conversion could be folded at this operation\n+      if (isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n+              triton::MakeRangeOp, triton::SplatOp>(*currOp))\n+          continue;\n+      // done processing\n+      processed.insert(currOp);\n+      layout.insert(currLayout);\n+      // add all operands to the queue\n+      for (Value argI : currOp->getOperands()) {\n+        Attribute newEncoding;\n+        // cannot invert the current encoding for this operand\n+        // we stop everything\n+        if (failed(invertEncoding(currLayout, currOp, newEncoding))){\n+          return mlir::failure();\n+        }\n+        if (toConvert.count(argI) && toConvert[argI] != newEncoding)\n+          return mlir::failure();\n+        //\n+        Operation *opArgI = argI.getDefiningOp();\n+        toConvert.insert({argI, newEncoding});\n+        if (!opArgI || processed.contains(opArgI) ||\n+            (opArgI->getBlock() != initOp->getBlock()))\n+          continue;\n+        // we add one expensive conversion for the current operand\n+        numCvts += 1;\n+        queue.emplace_back(opArgI, newEncoding);\n+      }\n+    }\n+    // if rematerialization would add more conversions than it removes\n+    // then we don't do it\n+    if (numCvts > 0)\n+      return mlir::failure();\n+    return mlir::success();\n+}\n+\n+//\n+\n Operation *cloneWithInferType(mlir::PatternRewriter &rewriter, Operation *op,\n                               BlockAndValueMapping &mapping) {\n   Operation *newOp = rewriter.clone(*op, mapping);\n@@ -267,6 +376,164 @@ Operation *cloneWithInferType(mlir::PatternRewriter &rewriter, Operation *op,\n   return newOp;\n }\n \n+//\n+class MoveConvertOutOfIf : public mlir::RewritePattern {\n+public:\n+  explicit MoveConvertOutOfIf(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(scf::IfOp::getOperationName(),\n+                             2, context) {}\n+\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ifOp = cast<scf::IfOp>(*op);\n+    auto thenYield = ifOp.thenYield();\n+    auto elseYield = ifOp.elseYield();\n+    int numOps = thenYield.getNumOperands();\n+    SmallVector<Value> newThenYieldOps = thenYield.getOperands();\n+    SmallVector<Value> newElseYieldOps = elseYield.getOperands();\n+    SetVector<Operation*> thenCvts;\n+    SetVector<Operation*> elseCvts;\n+    SmallVector<Type> newRetTypes;\n+\n+    BlockAndValueMapping mapping;\n+    for(size_t i = 0; i < numOps ; i++){\n+      auto thenCvt = dyn_cast<triton::gpu::ConvertLayoutOp>(thenYield.getOperand(i).getDefiningOp());\n+      auto elseCvt = dyn_cast<triton::gpu::ConvertLayoutOp>(elseYield.getOperand(i).getDefiningOp());\n+      if(thenCvt && elseCvt && \n+         std::distance(thenCvt->user_begin(), thenCvt->user_end()) == 1 &&\n+         std::distance(elseCvt->user_begin(), elseCvt->user_end()) == 1 &&\n+         thenCvt.getOperand().getType() == elseCvt.getOperand().getType()){\n+        mapping.map(thenCvt.getResult(), thenCvt.getOperand());\n+        mapping.map(elseCvt.getResult(), elseCvt.getOperand());\n+        newRetTypes.push_back(thenCvt.getOperand().getType());\n+        thenCvts.insert((Operation*)thenCvt);\n+        elseCvts.insert((Operation*)elseCvt);\n+      }\n+      else\n+        newRetTypes.push_back(thenYield.getOperand(i).getType());\n+    }\n+    if(mapping.getValueMap().empty())\n+      return mlir::failure();\n+\n+    \n+\n+    \n+    rewriter.setInsertionPoint(op);\n+    auto newIfOp = rewriter.create<scf::IfOp>(ifOp.getLoc(), newRetTypes, ifOp.getCondition(), true);\n+    // rematerialize `then` block\n+    rewriter.setInsertionPointToEnd(newIfOp.thenBlock());\n+    for(Operation& op: ifOp.thenBlock()->getOperations()){\n+      if(thenCvts.contains(&op)){\n+        mapping.map(op.getResult(0), mapping.lookup(op.getOperand(0)));\n+        continue;\n+      }\n+      rewriter.clone(op, mapping);\n+    }\n+    // rematerialize `else` block \n+    rewriter.setInsertionPointToEnd(newIfOp.elseBlock());\n+    for(Operation& op: ifOp.elseBlock()->getOperations()){\n+      if(elseCvts.contains(&op)){\n+        mapping.map(op.getResult(0), mapping.lookup(op.getOperand(0)));\n+        continue;\n+      }\n+      rewriter.clone(op, mapping);\n+    }\n+    \n+    rewriter.setInsertionPointAfter(newIfOp);\n+    SmallVector<Value> newRetValues = newIfOp.getResults();\n+    for(size_t i = 0; i < numOps ; i++){\n+      if(newIfOp.getResult(i).getType() != ifOp.getResult(i).getType()){\n+        newRetValues[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(newIfOp.getLoc(), ifOp.getResult(i).getType(), newIfOp.getResult(i));\n+      }\n+    }\n+\n+    rewriter.replaceOp(op, newRetValues);\n+    return mlir::success();\n+  }\n+};\n+\n+\n+//\n+class FoldConvertAndReduce : public mlir::RewritePattern {\n+public:\n+  explicit FoldConvertAndReduce(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n+                             2, context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *cvtOp,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto cvt = dyn_cast<triton::gpu::ConvertLayoutOp>(*cvtOp);\n+    auto srcEncoding = cvt.getResult().getType().cast<RankedTensorType>().getEncoding();\n+    SetVector<Operation *> cvtSlices;\n+    auto filter = [&](Operation *op) {\n+      return op->getBlock() == cvt->getBlock() &&\n+             !(isa<triton::ReduceOp>(op) && !op->getResult(0).getType().isa<RankedTensorType>()) &&\n+             !isa<scf::YieldOp>(op);\n+    };\n+    mlir::getForwardSlice(cvt.getResult(), &cvtSlices, filter);\n+    if (cvtSlices.empty())\n+      return failure();\n+\n+    llvm::MapVector<Value, Attribute> toConvert;\n+    for (Operation *op : cvtSlices) {\n+      // don't rematerialize anything expensive\n+      if(expensive_to_remat(op))\n+        return failure();\n+      // don't rematerialize non-element-wise\n+      if(!op->hasTrait<mlir::OpTrait::Elementwise>())\n+        return failure();\n+      Attribute dstEncoding = cvt.getOperand().getType().cast<RankedTensorType>().getEncoding();\n+      // don't rematerialize if it adds an extra conversion that can't\n+      // be removed\n+      for (Value arg : op->getOperands()) {\n+        Operation *argOp = arg.getDefiningOp();\n+        SetVector<Operation *> processed;\n+        SetVector<Attribute> layout;\n+        llvm::MapVector<Value, Attribute> toConvert;\n+        if (argOp && (argOp != cvt) &&\n+            cvtSlices.count(argOp) == 0 &&\n+            failed(simulateBackwardRematerialization(\n+              argOp, processed, layout, toConvert, srcEncoding))){\n+          return failure();\n+        }\n+        \n+      }\n+    }\n+\n+    BlockAndValueMapping mapping;\n+    auto op = cvtSlices.front();\n+    for (Value arg : op->getOperands()) {\n+      if (arg.getDefiningOp() == cvt)\n+        mapping.map(arg, cvt.getOperand());\n+      else {\n+        auto cvtI = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+            arg.getLoc(), cvt.getOperand().getType(), arg);\n+        if(Operation* argOp = arg.getDefiningOp())\n+          cvtI->moveAfter(argOp);\n+        mapping.map(arg, cvtI);\n+      }\n+    }\n+    rewriter.setInsertionPoint(op);\n+    Operation *newOp = rewriter.clone(*op, mapping);\n+    auto oldType = op->getResult(0).getType().cast<RankedTensorType>();\n+    auto newType = RankedTensorType::get(\n+        oldType.getShape(), oldType.getElementType(),\n+        cvt.getOperand().getType().cast<RankedTensorType>().getEncoding());\n+\n+    newOp->getResult(0).setType(newType);\n+    auto newCvtType = RankedTensorType::get(\n+        oldType.getShape(), oldType.getElementType(),\n+        cvt.getResult().getType().cast<RankedTensorType>().getEncoding());\n+    auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+        newOp->getLoc(), newCvtType, newOp->getResult(0));\n+    rewriter.replaceOp(op, newCvt->getResults());\n+    return success();\n+  }\n+};\n+\n // Layout conversions are expensive. They require going through\n // shared memory, which is orders of magnitude slower than\n // other non-i/o operations in the dialect.\n@@ -494,7 +761,6 @@ class MoveConvertOutOfLoop : public mlir::RewritePattern {\n           continue;\n       }\n       // check\n-      // llvm::outs() << \"replacing \" << iterArg.index() << \"\\n\";\n       for (auto op : iterArg.value().getUsers()) {\n         auto cvt = dyn_cast<triton::gpu::ConvertLayoutOp>(op);\n         if (!cvt)\n@@ -944,10 +1210,13 @@ class TritonGPUCombineOpsPass\n     patterns.add<OptimizeBlockedToShared>(context);\n     patterns.add<OptimizeConvertToDotOperand>(context);\n     patterns.add<SimplifyConversion>(context);\n+    patterns.add<SimplifyReduceCvt>(context);\n+    patterns.add<FoldConvertAndReduce>(context);\n     patterns.add<DecomposeDotOperand>(context);\n     patterns.add<RematerializeBackward>(context);\n     patterns.add<RematerializeForward>(context);\n     patterns.add<MoveConvertOutOfLoop>(context);\n+    patterns.add<MoveConvertOutOfIf>(context);\n     patterns.add<BlockedToMMA>(context, computeCapability);\n \n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n@@ -959,6 +1228,7 @@ class TritonGPUCombineOpsPass\n     if (applyPatternsAndFoldGreedily(m, std::move(loopFixup)).failed()) {\n       signalPassFailure();\n     }\n+\n   }\n };\n "}]