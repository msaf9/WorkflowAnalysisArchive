[{"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 10, "deletions": 13, "changes": 23, "file_content_changes": "@@ -399,13 +399,13 @@ struct AtomicCASOpConversion\n     auto valElements = getTypeConverter()->unpackLLElements(\n         loc, llVal, rewriter, op.getVal().getType());\n \n-    auto TensorTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n+    auto valueTy = op.getResult().getType();\n+    auto TensorTy = valueTy.dyn_cast<RankedTensorType>();\n     Type valueElemTy =\n         TensorTy ? getTypeConverter()->convertType(TensorTy.getElementType())\n-                 : op.getResult().getType();\n+                 : valueTy;\n     auto valueElemNBits = valueElemTy.getIntOrFloatBitWidth();\n-    auto tid = tid_val();\n-    Value pred = icmp_eq(tid, i32_val(0));\n+    Value mask = getMask(valueTy, rewriter, loc);\n     PTXBuilder ptxBuilderMemfence;\n     auto memfence = ptxBuilderMemfence.create<PTXInstr>(\"membar\")->o(\"gl\");\n     memfence();\n@@ -425,7 +425,7 @@ struct AtomicCASOpConversion\n     auto *valOpr = ptxBuilderAtomicCAS.newOperand(casVal, \"r\");\n     auto &atom = *ptxBuilderAtomicCAS.create<PTXInstr>(\"atom\");\n     atom.global().o(\"cas\").o(\"b32\");\n-    atom(dstOpr, ptrOpr, cmpOpr, valOpr).predicate(pred);\n+    atom(dstOpr, ptrOpr, cmpOpr, valOpr).predicate(mask);\n     auto old = ptxBuilderAtomicCAS.launch(rewriter, loc, valueElemTy);\n     barrier();\n \n@@ -434,7 +434,7 @@ struct AtomicCASOpConversion\n     auto *valOprStore = ptxBuilderStore.newOperand(old, \"r\");\n     auto &st = *ptxBuilderStore.create<PTXInstr>(\"st\");\n     st.shared().o(\"b32\");\n-    st(dstOprStore, valOprStore).predicate(pred);\n+    st(dstOprStore, valOprStore).predicate(mask);\n     ptxBuilderStore.launch(rewriter, loc, ASMReturnTy);\n     ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n     barrier();\n@@ -483,10 +483,11 @@ struct AtomicRMWOpConversion\n       maskElements = getTypeConverter()->unpackLLElements(\n           loc, llMask, rewriter, op.getMask().getType());\n \n-    auto tensorTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n+    auto valueTy = op.getResult().getType();\n+    auto tensorTy = valueTy.dyn_cast<RankedTensorType>();\n     Type valueElemTy =\n         tensorTy ? getTypeConverter()->convertType(tensorTy.getElementType())\n-                 : op.getResult().getType();\n+                 : valueTy;\n     const size_t valueElemNBits = valueElemTy.getIntOrFloatBitWidth();\n     auto elemsPerThread = getTotalElemsPerThread(val.getType());\n     // vec = 1, numElements = 1 for scalar\n@@ -499,10 +500,7 @@ struct AtomicRMWOpConversion\n       // mask\n       numElems = tensorTy.getNumElements();\n     }\n-    Value mask = int_val(1, 1);\n-    auto tid = tid_val();\n-    mask = and_(mask,\n-                icmp_slt(mul(tid, i32_val(elemsPerThread)), i32_val(numElems)));\n+    Value mask = getMask(valueTy, rewriter, loc);\n \n     auto vecTy = vec_ty(valueElemTy, vec);\n     SmallVector<Value> resultVals(elemsPerThread);\n@@ -582,7 +580,6 @@ struct AtomicRMWOpConversion\n         memfenc();\n         auto ASMReturnTy = void_ty(ctx);\n         ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n-        rmwMask = and_(rmwMask, icmp_eq(tid, i32_val(0)));\n         atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n         auto old = ptxBuilderAtomicRMW.launch(rewriter, loc, valueElemTy);\n         Value atomPtr = getSharedMemoryBase(loc, rewriter, op.getOperation());"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -457,7 +457,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     } else {\n       // If the tensor is not ranked, then it is a scalar and only thread 0 can\n       // write\n-      mask = and_(mask, icmp_slt(tid, i32_val(1)));\n+      mask = and_(mask, icmp_eq(tid, i32_val(0)));\n     }\n     return mask;\n   }"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1009,7 +1009,6 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: atomic_add_f32\n   tt.func @atomic_add_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xi1, #blocked0>, %arg2 : tensor<256xf32, #blocked0>) {\n-    // CHECK: llvm.icmp \"slt\"\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$3 atom.global.gpu.add.f32\n     // CHECK: llvm.inline_asm\n@@ -1026,6 +1025,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   tt.func @atomic_add_f32_scalar(%arg0 : !tt.ptr<f32>, %arg1 : i1, %arg2 : f32) {\n     // CHECK: llvm.icmp \"eq\"\n     // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$3 atom.global.gpu.add.f32\n     %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32} : (!tt.ptr<f32>, f32, i1) -> f32\n     tt.return\n@@ -1052,7 +1052,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: store_f32_scalar\n   tt.func @store_f32_scalar(%arg0 : !tt.ptr<f32>, %arg1 : f32) {\n-    // CHECK: llvm.icmp \"slt\"\n+    // CHECK: llvm.icmp \"eq\"\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$2 st.global.b32\n     tt.store %arg0, %arg1 : f32"}]