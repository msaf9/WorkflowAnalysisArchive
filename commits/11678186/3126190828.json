[{"filename": "python/triton/compiler.py", "status": "modified", "additions": 32, "deletions": 29, "changes": 61, "file_content_changes": "@@ -1331,37 +1331,37 @@ def __new__(cls):\n         return cls.instance\n \n     def _generate_src(self):\n-        return f\"\"\"\n+        return \"\"\"\n         #include <cuda.h>\n \n         #include \\\"cuda.h\\\"\n         #include <Python.h>\n \n         static inline void gpuAssert(CUresult code, const char *file, int line)\n-        {{\n+        {\n            if (code != CUDA_SUCCESS)\n-           {{\n+           {\n               const char* prefix = \"Triton Error [CUDA]: \";\n               const char* str;\n               cuGetErrorString(code, &str);\n               char err[1024] = {0};\n               strcat(err, prefix);\n               strcat(err, str);\n               PyErr_SetString(PyExc_RuntimeError, err);\n-           }}\n-        }}\n+           }\n+        }\n \n-        #define CUDA_CHECK(ans) {{ gpuAssert((ans), __FILE__, __LINE__); }}\n+        #define CUDA_CHECK(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n \n-        static PyObject* loadBinary(PyObject* self, PyObject* args) {{\n+        static PyObject* loadBinary(PyObject* self, PyObject* args) {\n             const char* name;\n             const char* data;\n             Py_ssize_t data_size;\n             int shared;\n             int device;\n-            if(!PyArg_ParseTuple(args, \"ss#ii\", &name, &data, &data_size, &shared, &device)) {{\n+            if(!PyArg_ParseTuple(args, \"ss#ii\", &name, &data, &data_size, &shared, &device)) {\n                 return NULL;\n-            }}\n+            }\n             CUfunction fun;\n             CUmodule mod;\n             int32_t n_regs = 0;\n@@ -1377,57 +1377,60 @@ def _generate_src(self):\n             // set dynamic shared memory if necessary\n             int shared_optin;\n             CUDA_CHECK(cuDeviceGetAttribute(&shared_optin, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN, device));\n-            if (shared > 49152 && shared_optin > 49152) {{\n+            if (shared > 49152 && shared_optin > 49152) {\n               CUDA_CHECK(cuFuncSetCacheConfig(fun, CU_FUNC_CACHE_PREFER_SHARED));\n               int shared_total, shared_static;\n               CUDA_CHECK(cuDeviceGetAttribute(&shared_total, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR, device));\n               CUDA_CHECK(cuFuncGetAttribute(&shared_static, CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES, fun));\n               CUDA_CHECK(cuFuncSetAttribute(fun, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES, shared_optin - shared_static));\n-            }}\n+            }\n             Py_END_ALLOW_THREADS;\n \n-            if(PyErr_Occurred()) {{\n+            if(PyErr_Occurred()) {\n               return NULL;\n-            }}\n+            }\n             return Py_BuildValue(\"(KKii)\", (uint64_t)mod, (uint64_t)fun, n_regs, n_spills);\n-        }}\n+        }\n \n-        static PyMethodDef ModuleMethods[] = {{\n-          {{\"load_binary\", loadBinary, METH_VARARGS, \"Load provided cubin into CUDA driver\"}},\n-          {{NULL, NULL, 0, NULL}} // sentinel\n-        }};\n+        static PyMethodDef ModuleMethods[] = {\n+          {\"load_binary\", loadBinary, METH_VARARGS, \"Load provided cubin into CUDA driver\"},\n+          {NULL, NULL, 0, NULL} // sentinel\n+        };\n \n-        static struct PyModuleDef ModuleDef = {{\n+        static struct PyModuleDef ModuleDef = {\n           PyModuleDef_HEAD_INIT,\n           \\\"cuda_utils\\\",\n           NULL, //documentation\n           -1, //size\n           ModuleMethods\n-        }};\n+        };\n \n-        PyMODINIT_FUNC PyInit_cuda_utils(void) {{\n+        PyMODINIT_FUNC PyInit_cuda_utils(void) {\n           PyObject *m = PyModule_Create(&ModuleDef);\n-          if(m == NULL) {{\n+          if(m == NULL) {\n             return NULL;\n-          }}\n+          }\n           PyModule_AddFunctions(m, ModuleMethods);\n           return m;\n-        }}\n+        }\n         \"\"\"\n \n     def __init__(self):\n-        cache = CacheManager(\"cuda_utils\")\n-        if not cache.has_file(\"main.so\"):\n+        src = self._generate_src()\n+        key = hashlib.md5(src.encode(\"utf-8\")).hexdigest()\n+        cache = CacheManager(key)\n+        fname = \"cuda_utils.so\"\n+        if not cache.has_file(fname):\n+            print(\"compiling\")\n             with tempfile.TemporaryDirectory() as tmpdir:\n-                src = self._generate_src()\n                 src_path = os.path.join(tmpdir, \"main.c\")\n                 with open(src_path, \"w\") as f:\n                     f.write(src)\n                 so = _build(\"cuda_utils\", src_path, tmpdir)\n                 with open(so, \"rb\") as f:\n-                    cache.put(f.read(), \"main.so\", binary=True)\n+                    cache.put(f.read(), fname, binary=True)\n         import importlib.util\n-        spec = importlib.util.spec_from_file_location(\"cuda_utils\", cache._make_path(\"main.so\"))\n+        spec = importlib.util.spec_from_file_location(\"cuda_utils\", cache._make_path(fname))\n         mod = importlib.util.module_from_spec(spec)\n         spec.loader.exec_module(mod)\n         self.load_binary = mod.load_binary"}]