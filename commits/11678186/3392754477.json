[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 29, "deletions": 185, "changes": 214, "file_content_changes": "@@ -2603,7 +2603,7 @@ class MMA16816SmemLoader {\n \n     Value ptr = getPtr(ptrIdx);\n \n-    if (canUseLdmatrix) {\n+    if (canUseLdmatrix) { // work with fp16\n       int sOffset =\n           matIdx[order[1]] * sMatStride * sMatShape * sTileStride * elemBytes;\n       PTXBuilder builder;\n@@ -2626,12 +2626,13 @@ class MMA16816SmemLoader {\n         return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n       };\n \n-      Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n+      // The struct should have exactly the same element types.\n+      Type elemType = resV4.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n \n-      return {extract_val(fp16x2Ty, resV4, getIntAttr(0)),\n-              extract_val(fp16x2Ty, resV4, getIntAttr(1)),\n-              extract_val(fp16x2Ty, resV4, getIntAttr(2)),\n-              extract_val(fp16x2Ty, resV4, getIntAttr(3))};\n+      return {extract_val(elemType, resV4, getIntAttr(0)),\n+              extract_val(elemType, resV4, getIntAttr(1)),\n+              extract_val(elemType, resV4, getIntAttr(2)),\n+              extract_val(elemType, resV4, getIntAttr(3))};\n     } else if (elemBytes == 4 &&\n                needTrans) { // Use lds.32 to load tf32 matrices\n       Value ptr2 = getPtr(ptrIdx + 1);\n@@ -2658,9 +2659,9 @@ class MMA16816SmemLoader {\n         elems[3] =\n             load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n       }\n-\n       return {elems[0], elems[1], elems[2], elems[3]};\n-    } else if (elemBytes == 1 && needTrans) {\n+\n+    } else if (elemBytes == 1 && needTrans) { // work with int8\n       std::array<std::array<Value, 4>, 2> ptrs;\n       ptrs[0] = {\n           getPtr(ptrIdx),\n@@ -2688,17 +2689,18 @@ class MMA16816SmemLoader {\n \n       Value i8Elems[4][4];\n       Type elemTy = type::i8Ty(ctx);\n+      Type elemPtrTy = ptr_ty(elemTy);\n       if (kOrder == 1) {\n         Value offset = i32_val(sOffsetElem);\n \n         for (int i = 0; i < 2; ++i)\n           for (int j = 0; j < 4; ++j)\n-            i8Elems[i][j] = load(gep(elemTy, ptrs[i][j], offset));\n+            i8Elems[i][j] = load(gep(elemPtrTy, ptrs[i][j], offset));\n \n         offset = i32_val(sOffsetElem + sOffsetArrElem);\n         for (int i = 2; i < 4; ++i)\n           for (int j = 0; j < 4; ++j)\n-            i8Elems[i][j] = load(gep(elemTy, ptrs[i - 2][j], offset));\n+            i8Elems[i][j] = load(gep(elemPtrTy, ptrs[i - 2][j], offset));\n \n         for (int m = 0; m < 4; ++m) {\n           for (int e = 0; e < 4; ++e)\n@@ -2709,14 +2711,14 @@ class MMA16816SmemLoader {\n       } else { // k first\n         Value offset = i32_val(sOffsetElem);\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[0][j] = load(gep(elemTy, ptrs[0][j], offset));\n+          i8Elems[0][j] = load(gep(elemPtrTy, ptrs[0][j], offset));\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[2][j] = load(gep(elemTy, ptrs[1][j], offset));\n+          i8Elems[2][j] = load(gep(elemPtrTy, ptrs[1][j], offset));\n         offset = i32_val(sOffsetElem + sOffsetArrElem);\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[1][j] = load(gep(elemTy, ptrs[0][j], offset));\n+          i8Elems[1][j] = load(gep(elemPtrTy, ptrs[0][j], offset));\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[3][j] = load(gep(elemTy, ptrs[1][j], offset));\n+          i8Elems[3][j] = load(gep(elemPtrTy, ptrs[1][j], offset));\n \n         for (int m = 0; m < 4; ++m) {\n           for (int e = 0; e < 4; ++e)\n@@ -3501,19 +3503,25 @@ struct MMA16816ConversionHelper {\n         return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n       };\n \n+      Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n       for (int i = 0; i < 4; ++i)\n         fc[m * colsPerThread + 4 * n + i] =\n-            extract_val(type::f32Ty(ctx), mmaOut, getIntAttr(i));\n+            extract_val(elemTy, mmaOut, getIntAttr(i));\n     };\n \n     for (int k = 0; k < numRepK; ++k)\n       for (int m = 0; m < numRepM; ++m)\n         for (int n = 0; n < numRepN; ++n)\n           callMma(2 * m, n, 2 * k);\n \n+    // bitcast to fp32 in bulk\n+    for (auto &elem : fc) {\n+      elem = bitcast(elem, type::i32Ty(ctx));\n+    }\n+\n     // replace with new packed result\n     Type structTy = LLVM::LLVMStructType::getLiteral(\n-        ctx, SmallVector<Type>(fc.size(), type::f32Ty(ctx)));\n+        ctx, SmallVector<Type>(fc.size(), type::i32Ty(ctx)));\n     Value res = getStructFromElements(loc, fc, rewriter, structTy);\n     rewriter.replaceOp(op, res);\n \n@@ -3607,10 +3615,9 @@ struct MMA16816ConversionHelper {\n \n     assert(!elems.empty());\n \n-    Type fp16Ty = type::f16Ty(ctx);\n-    Type fp16x2Ty = vec_ty(fp16Ty, 2);\n+    Type elemTy = elems[0].getType();\n     Type structTy = LLVM::LLVMStructType::getLiteral(\n-        ctx, SmallVector<Type>(elems.size(), fp16x2Ty));\n+        ctx, SmallVector<Type>(elems.size(), elemTy));\n     auto result = getStructFromElements(loc, elems, rewriter, structTy);\n     return result;\n   }\n@@ -3634,161 +3641,6 @@ struct MMA16816ConversionHelper {\n   }\n };\n \n-// Helper for FMADot conversion.\n-class DotOpFMAConversionHelper {\n-public:\n-  MmaEncodingAttr mmaLayout;\n-  ArrayRef<unsigned> wpt;\n-\n-  using ValueTable = std::map<std::pair<int, int>, Value>;\n-\n-  explicit DotOpFMAConversionHelper(MmaEncodingAttr mmaLayout)\n-      : mmaLayout(mmaLayout), wpt(mmaLayout.getWarpsPerCTA()) {}\n-\n-  // Currently, we can tell whether to use FMAdot only from the operand type,\n-  // while in the original code, FMADot requires that both the operand and\n-  // result of dot should be fp32.\n-  // This method should be safe to use in the cases where tensor core is not\n-  // appliable.\n-  static bool useFMA(TensorType operand) {\n-    return operand.getElementType().isF32();\n-  }\n-\n-  Value loadA(Value tensor, Value llTensor, Value threadId, Location loc,\n-              Value smem, ConversionPatternRewriter &rewriter) const {\n-\n-    auto *ctx = rewriter.getContext();\n-    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-    auto aShape = tensorTy.getShape();\n-    auto aLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n-    auto aOrder = aLayout.getOrder();\n-\n-    bool isARow = aOrder[0] == 1;\n-\n-    int strideAM = isARow ? aShape[1] : 1;\n-    int strideAK = isARow ? 1 : aShape[0];\n-    int strideA0 = isARow ? strideAK : strideAM;\n-    int strideA1 = isARow ? strideAM : strideAK;\n-    int lda = isARow ? strideAM : strideAK;\n-    int aPerPhase = aLayout.getPerPhase();\n-    int aMaxPhase = aLayout.getMaxPhase();\n-    int aNumPtr = 8;\n-    int bNumPtr = 8;\n-    int aVec = 2;\n-\n-    Value _0 = i32_val(0);\n-    Value _1 = i32_val(1);\n-\n-    Value mContig = _1;\n-    Value nContig = _1;\n-\n-    Value offA0 = isARow ? _0 : mul(threadId, mContig);\n-    Value offA1 = isARow ? mul(threadId, mContig) : _0;\n-    SmallVector<Value> aOff(aNumPtr);\n-    for (int i = 0; i < aNumPtr; ++i) {\n-      aOff[i] =\n-          add(mul(offA0, i32_val(strideA0)), mul(offA1, i32_val(strideA1)));\n-    }\n-\n-    Type f32PtrTy = ptr_ty(f32_ty);\n-    SmallVector<Value> aPtrs(aNumPtr);\n-    for (int i = 0; i < aNumPtr; ++i)\n-      aPtrs[i] = gep(f32PtrTy, llTensor, aOff[i]);\n-\n-    ValueTable has;\n-\n-    auto aShapePerCTA = getShapePerCTA(aLayout);\n-    auto sizePerThread = getSizePerThread(aLayout);\n-    int M = isARow ? aShape[0] : aShape[1];\n-    int K = isARow ? aShape[1] : aShape[0];\n-\n-    for (unsigned k = 0; k < K; k++)\n-      for (unsigned m = 0; m < M; m += aShapePerCTA[aOrder[1]])\n-        for (unsigned mm = 0; mm < sizePerThread[aOrder[1]]; ++mm) {\n-          Value pa = gep(f32PtrTy, aPtrs[0],\n-                         i32_val((m + mm) * strideAM + k * strideAK));\n-          Value va = load(pa);\n-          has[{m + mm, k}] = va;\n-        }\n-\n-    SmallVector<Value> values;\n-    for (auto &item : has)\n-      values.push_back(item.second);\n-    Type structTy =\n-        struct_ty(SmallVector<Type>(values.size(), values[0].getType()));\n-\n-    return getStructFromElements(loc, values, rewriter, structTy);\n-  }\n-\n-  Value loadB(Value tensor, Value llTensor, Value threadId, Location loc,\n-              Value smem, ConversionPatternRewriter &rewriter) const {\n-\n-    auto *ctx = rewriter.getContext();\n-    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-    auto bShape = tensorTy.getShape();\n-    auto bLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n-    auto bOrder = bLayout.getOrder();\n-\n-    bool isBRow = bOrder[0] == 1;\n-\n-    int strideBN = isBRow ? 1 : bShape[0];\n-    int strideBK = isBRow ? bShape[1] : 1;\n-    int strideB0 = isBRow ? strideBN : strideBK;\n-    int strideB1 = isBRow ? strideBK : strideBN;\n-    int ldb = isBRow ? strideBK : strideBN;\n-    int bPerPhase = bLayout.getPerPhase();\n-    int bMaxPhase = bLayout.getMaxPhase();\n-    int bNumPtr = 8;\n-    int bVec = 4;\n-\n-    auto bShapePerCTA = getShapePerCTA(bLayout);\n-    auto sizePerThread = getSizePerThread(bLayout);\n-\n-    Value _0 = i32_val(0);\n-    Value _1 = i32_val(1);\n-\n-    Value mContig = _1;\n-    Value nContig = _1;\n-\n-    Value offB0 = isBRow ? mul(threadId, nContig) : _0;\n-    Value offB1 = isBRow ? _0 : mul(threadId, nContig);\n-    SmallVector<Value> bOff(bNumPtr);\n-    for (int i = 0; i < bNumPtr; ++i) {\n-      bOff[i] =\n-          add(mul(offB0, i32_val(strideB0)), mul(offB1, i32_val(strideB1)));\n-    }\n-\n-    Type f32PtrTy = ptr_ty(f32_ty);\n-    SmallVector<Value> bPtrs(bNumPtr);\n-    for (int i = 0; i < bNumPtr; ++i)\n-      bPtrs[i] = gep(f32PtrTy, llTensor, bOff[i]);\n-\n-    ValueTable hbs;\n-\n-    int K = isBRow ? bShape[0] : bShape[1];\n-    int N = isBRow ? bShape[1] : bShape[0];\n-\n-    for (int k = 0; k < K; ++k)\n-      for (unsigned n = 0; n < N; n += bShapePerCTA[bOrder[0]])\n-        for (unsigned nn = 0; nn < sizePerThread[bOrder[0]]; ++nn) {\n-          Value pb = gep(f32PtrTy, bPtrs[0],\n-                         i32_val((n + nn) * strideBN + k * strideBK));\n-          Value vb = load(pb);\n-          hbs[{n + nn, k}] = vb;\n-        }\n-\n-    SmallVector<Value> values;\n-    for (auto &item : hbs)\n-      values.push_back(item.second);\n-    Type structTy =\n-        struct_ty(SmallVector<Type>(values.size(), values[0].getType()));\n-\n-    return getStructFromElements(loc, values, rewriter, structTy);\n-  }\n-\n-  ValueTable extractLoadedOperand(Value llTensor) const { return ValueTable{}; }\n-};\n-\n LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n     triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n     ConversionPatternRewriter &rewriter) const {\n@@ -3842,15 +3694,6 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n       res = helper.loadB(src, adaptor.src(), getThreadId(rewriter, loc),\n                          adaptor.src(), loc, rewriter);\n     }\n-  } else if (DotOpFMAConversionHelper::useFMA(dstTensorTy)) { // fmadot\n-    DotOpMmaV1ConversionHelper helper(mmaLayout);\n-    if (dotOperandLayout.getOpIdx() == 0) { // operand $a\n-      res = helper.loadA(src, adaptor.src(), getThreadId(rewriter, loc),\n-                         adaptor.src(), loc, rewriter);\n-    } else if (dotOperandLayout.getOpIdx() == 1) { // operand $b\n-      res = helper.loadB(src, adaptor.src(), getThreadId(rewriter, loc),\n-                         adaptor.src(), loc, rewriter);\n-    }\n   } else {\n     assert(false && \"Unsupported mma layout found\");\n   }\n@@ -4321,6 +4164,8 @@ DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n   auto loc = op.getLoc();\n   auto threadId = getThreadId(rewriter, loc);\n \n+  using ValueTable = std::map<std::pair<int, int>, Value>;\n+\n   auto A = op.a();\n   auto B = op.b();\n   auto C = op.c();\n@@ -4400,8 +4245,7 @@ DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n   for (int i = 0; i < bNumPtr; ++i)\n     bPtrs[i] = gep(f32PtrTy, adaptor.b(), bOff[i]);\n \n-  // TODO initialize ret with $c.\n-  DotOpFMAConversionHelper::ValueTable has, hbs;\n+  ValueTable has, hbs;\n   auto cc = getElementsFromStruct(loc, adaptor.c(), rewriter);\n   SmallVector<Value> ret = cc;\n "}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 47, "deletions": 0, "changes": 47, "file_content_changes": "@@ -144,3 +144,50 @@ def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLO\n \n     torch.set_printoptions(profile=\"full\")\n     assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err), check_dtype=False)\n+\n+\n+# Precession regression for FMADot is not done yet due to some issue on the optimizer failed to give a blocked layout to dot op.\n+# TODO[Superjomn]: Uncomment this test and continue to finish precession regression latter.\n+# @pytest.mark.parametrize('M,N,K,num_warps,block_M,block_N,block_K', [\n+#     [128, 256, 128, 4, 128, 256, 32],\n+#     [256, 128, 64, 4, 256, 128, 16],\n+#     [128, 64, 128, 4, 128, 64, 32],\n+# ])\n+# def test_gemm_fmadot(M, N, K, num_warps, block_M, block_N, block_K):\n+#     @triton.jit\n+#     def matmul_kernel(\n+#         a_ptr, b_ptr, c_ptr,\n+#         stride_am, stride_ak,\n+#         stride_bk, stride_bn,\n+#         stride_cm, stride_cn,\n+#         K: tl.constexpr,\n+#         BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+#     ):\n+#         offs_m = tl.arange(0, BLOCK_SIZE_M)\n+#         offs_n = tl.arange(0, BLOCK_SIZE_N)\n+#         offs_k = tl.arange(0, BLOCK_SIZE_K)\n+#         a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n+#         b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n+#         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+#         for k in range(0, K, BLOCK_SIZE_K):\n+#             a = tl.load(a_ptrs)\n+#             b = tl.load(b_ptrs)\n+#             accumulator += tl.dot(a, b, allow_tf32=True)\n+#             a_ptrs += BLOCK_SIZE_K * stride_ak\n+#             b_ptrs += BLOCK_SIZE_K * stride_bk\n+\n+#         c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n+#         tl.store(c_ptrs, accumulator)\n+\n+#     a = torch.randn((M, K), device='cuda', dtype=torch.float32)\n+#     b = torch.randn((K, N), device='cuda', dtype=torch.float)\n+#     c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n+#     grid = lambda META: (1, )\n+#     matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n+#                         stride_am=a.stride(0), stride_ak=a.stride(1),\n+#                         stride_bk=b.stride(0), stride_bn=b.stride(1),\n+#                         stride_cm=c.stride(0), stride_cn=c.stride(1),\n+#                         K=a.shape[1], BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N,\n+#                         BLOCK_SIZE_K=block_K, num_warps=num_warps)\n+#     golden = torch.matmul(a, b)\n+#     torch.testing.assert_close(c, golden)"}]