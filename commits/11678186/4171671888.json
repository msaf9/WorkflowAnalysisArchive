[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -85,6 +85,7 @@ struct BroadcastOpConversion\n     }\n \n     auto llvmStructTy = getTypeConverter()->convertType(resultTy);\n+\n     Value resultStruct =\n         getStructFromElements(loc, resultVals, rewriter, llvmStructTy);\n     rewriter.replaceOp(op, {resultStruct});"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.cpp", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -12,6 +12,11 @@ Value getStructFromElements(Location loc, ValueRange resultVals,\n     return *resultVals.begin();\n   }\n \n+  int n0 = structType.cast<LLVM::LLVMStructType>().getBody().size();\n+  int n1 = resultVals.size();\n+  if (n0 != n1) {\n+    llvm::outs() << n0 << \" \" << n1 << \"\\n\";\n+  }\n   Value llvmStruct = rewriter.create<LLVM::UndefOp>(loc, structType);\n   for (const auto &v : llvm::enumerate(resultVals)) {\n     assert(v.value() && \"can not insert null values\");"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 10, "deletions": 118, "changes": 128, "file_content_changes": "@@ -29,124 +29,15 @@ struct SplatOpConversion\n                                   ConversionPatternRewriter &rewriter,\n                                   Location loc) {\n     auto tensorTy = resType.cast<RankedTensorType>();\n-    if (tensorTy.getEncoding().isa<BlockedEncodingAttr>() ||\n-        tensorTy.getEncoding().isa<SliceEncodingAttr>()) {\n-      auto srcType = typeConverter->convertType(elemType);\n-      auto llSrc = bitcast(constVal, srcType);\n-      size_t elemsPerThread = getElemsPerThread(tensorTy);\n-      llvm::SmallVector<Value> elems(elemsPerThread, llSrc);\n-      llvm::SmallVector<Type> elemTypes(elems.size(), srcType);\n-      auto structTy =\n-          LLVM::LLVMStructType::getLiteral(rewriter.getContext(), elemTypes);\n-\n-      return getStructFromElements(loc, elems, rewriter, structTy);\n-    } else if (auto dotLayout =\n-                   tensorTy.getEncoding()\n-                       .dyn_cast<triton::gpu::DotOperandEncodingAttr>()) {\n-      return convertSplatLikeOpWithDotOperandLayout(\n-          dotLayout, resType, elemType, constVal, typeConverter, rewriter, loc);\n-    } else if (auto mmaLayout =\n-                   tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>()) {\n-      return convertSplatLikeOpWithMmaLayout(\n-          mmaLayout, resType, elemType, constVal, typeConverter, rewriter, loc);\n-    } else\n-      assert(false && \"Unsupported layout found in ConvertSplatLikeOp\");\n-\n-    return {};\n-  }\n-\n-  static Value convertSplatLikeOpWithDotOperandLayout(\n-      const triton::gpu::DotOperandEncodingAttr &layout, Type resType,\n-      Type elemType, Value constVal, TypeConverter *typeConverter,\n-      ConversionPatternRewriter &rewriter, Location loc) {\n-    auto tensorTy = resType.cast<RankedTensorType>();\n-    auto shape = tensorTy.getShape();\n-    auto dotOperand =\n-        tensorTy.getEncoding().cast<triton::gpu::DotOperandEncodingAttr>();\n-    auto parent = layout.getParent();\n-    Value retVal = constVal;\n-    Type retTy = elemType;\n-    int numElems{};\n-    if (auto mmaLayout = parent.dyn_cast<MmaEncodingAttr>()) {\n-      Type matTy;\n-      if (mmaLayout.isAmpere()) {\n-        numElems = layout.getOpIdx() == 0\n-                       ? MMA16816ConversionHelper::getANumElemsPerThread(\n-                             tensorTy, mmaLayout.getWarpsPerCTA()[0])\n-                       : MMA16816ConversionHelper::getBNumElemsPerThread(\n-                             tensorTy, mmaLayout.getWarpsPerCTA()[1]);\n-        DotOpMmaV2ConversionHelper helper(mmaLayout);\n-        helper.deduceMmaType(tensorTy);\n-        matTy = helper.getMatType();\n-      } else if (mmaLayout.isVolta()) {\n-        DotOpMmaV1ConversionHelper helper(mmaLayout);\n-        bool isRow = layout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n-        auto [isARow, isBRow, isAVec4, isBVec4, _0] =\n-            mmaLayout.decodeVoltaLayoutStates();\n-        if (layout.getOpIdx() == 0) {\n-          DotOpMmaV1ConversionHelper::AParam aParam(isARow, isAVec4);\n-          numElems =\n-              helper.numElemsPerThreadA(shape, isARow, isAVec4, aParam.vec);\n-        } else {\n-          DotOpMmaV1ConversionHelper::BParam bParam(isBRow, isBVec4);\n-          numElems =\n-              helper.numElemsPerThreadB(shape, isBRow, isBVec4, bParam.vec);\n-        }\n-        matTy = helper.getMatType(tensorTy);\n-      }\n-\n-      auto numPackedElems = matTy.cast<LLVM::LLVMStructType>()\n-                                .getBody()[0]\n-                                .cast<VectorType>()\n-                                .getNumElements();\n-      retTy = vec_ty(elemType, numPackedElems);\n-      retVal = undef(retTy);\n-      for (auto i = 0; i < numPackedElems; ++i) {\n-        retVal = insert_element(retTy, retVal, constVal, i32_val(i));\n-      }\n-\n-    } else if (auto blockedLayout = parent.dyn_cast<BlockedEncodingAttr>()) {\n-      numElems = DotOpFMAConversionHelper::getNumElemsPerThread(shape, layout);\n-    } else {\n-      assert(false && \"Unsupported layout found\");\n-    }\n-\n-    auto structTy = LLVM::LLVMStructType::getLiteral(\n-        rewriter.getContext(), SmallVector<Type>(numElems, retTy));\n-    return getStructFromElements(loc, SmallVector<Value>(numElems, retVal),\n-                                 rewriter, structTy);\n-  }\n-\n-  static Value convertSplatLikeOpWithMmaLayout(\n-      const MmaEncodingAttr &layout, Type resType, Type elemType,\n-      Value constVal, TypeConverter *typeConverter,\n-      ConversionPatternRewriter &rewriter, Location loc) {\n-    auto tensorTy = resType.cast<RankedTensorType>();\n-    auto shape = tensorTy.getShape();\n-    if (layout.isAmpere()) {\n-      auto [repM, repN] = DotOpMmaV2ConversionHelper::getRepMN(tensorTy);\n-      size_t fcSize = 4 * repM * repN;\n-\n-      auto structTy = LLVM::LLVMStructType::getLiteral(\n-          rewriter.getContext(), SmallVector<Type>(fcSize, elemType));\n-      return getStructFromElements(loc, SmallVector<Value>(fcSize, constVal),\n-                                   rewriter, structTy);\n-    }\n-    if (layout.isVolta()) {\n-      DotOpMmaV1ConversionHelper helper(layout);\n-      int repM = helper.getRepM(shape[0]);\n-      int repN = helper.getRepN(shape[1]);\n-      // According to mma layout of v1, each thread process 8 elements.\n-      int elems = 8 * repM * repN;\n-\n-      auto structTy = LLVM::LLVMStructType::getLiteral(\n-          rewriter.getContext(), SmallVector<Type>(elems, elemType));\n-      return getStructFromElements(loc, SmallVector<Value>(elems, constVal),\n-                                   rewriter, structTy);\n-    }\n-\n-    assert(false && \"Unsupported mma layout found\");\n-    return {};\n+    auto srcType = typeConverter->convertType(elemType);\n+    auto llSrc = bitcast(constVal, srcType);\n+    size_t elemsPerThread = getElemsPerThread(tensorTy);\n+    llvm::SmallVector<Value> elems(elemsPerThread, llSrc);\n+    llvm::SmallVector<Type> elemTypes(elems.size(), srcType);\n+    auto structTy =\n+        LLVM::LLVMStructType::getLiteral(rewriter.getContext(), elemTypes);\n+\n+    return getStructFromElements(loc, elems, rewriter, structTy);\n   }\n \n   LogicalResult matchAndRewrite(triton::SplatOp op, OpAdaptor adaptor,\n@@ -254,6 +145,7 @@ struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n     SmallVector<Type> types(elems, elemTy);\n     Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n     auto vals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n+\n     Value view = getStructFromElements(loc, vals, rewriter, structTy);\n     rewriter.replaceOp(op, view);\n     return success();"}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -240,7 +240,7 @@ def matmul_kernel(\n     bias_ptrs = bias_ptr + offs_cm\n     bias = tl.load(bias_ptrs)\n     accumulator += bias[:, None]\n-    # accumulator += (offs_cm[:, None] + offs_cn[None, :])\n+    accumulator += (offs_cm[:, None] + offs_cn[None, :])\n     # write-back result\n     c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n     c = accumulator.to(tl.float16)"}]