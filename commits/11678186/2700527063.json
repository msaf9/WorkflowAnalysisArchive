[{"filename": "python/triton/code_gen.py", "status": "modified", "additions": 12, "deletions": 10, "changes": 22, "file_content_changes": "@@ -17,6 +17,7 @@\n \n import torch\n from filelock import FileLock\n+from numpy import isin\n \n import triton\n import triton._C.libtriton.triton as _triton\n@@ -964,12 +965,13 @@ def __call__(self, *wargs, grid, num_warps=4, num_stages=2, extern_libs={}, **kw\n             wargs.insert(pos + i, kwargs[pos])\n         if len(wargs) != len(self.fn.arg_names):\n             raise TypeError(f\"Function takes {len(self.fn.arg_names)} positional arguments but {len(wargs)} were given\")\n-        constexpr_str = ''\n+        jit_functions_str = \"\"\n         # handle annotations\n         for pos, _type in self.fn.annotations.items():\n             assert _type == triton.language.constexpr, \"only constexpr annotations are supported for now\"\n             wargs[pos] = _type(wargs[pos])\n-            constexpr_str = constexpr_str + f'-{wargs[pos].value}'\n+            if isinstance(wargs[pos].value, JITFunction):\n+                jit_functions_str += f'-{wargs[pos].value}'\n         # check that tensors are on GPU.\n         # for arg in wargs:\n         #     if hasattr(arg, 'data_ptr'):\n@@ -982,11 +984,11 @@ def __call__(self, *wargs, grid, num_warps=4, num_stages=2, extern_libs={}, **kw\n         torch.cuda.set_device(device)\n         if device not in self.cache_key:\n             self.cache_key[device] = {}\n-        if constexpr_str not in self.cache_key[device]:\n+        if jit_functions_str not in self.cache_key[device]:\n             cc = torch.cuda.get_device_capability(device)\n             cc = str(cc[0]) + '-' + str(cc[1])\n-            self.cache_key[device][constexpr_str] = self.fn.cache_key(wargs) + cc\n-        cache_key = self.cache_key[device][constexpr_str]\n+            self.cache_key[device][jit_functions_str] = self.fn.cache_key(wargs) + cc\n+        cache_key = self.cache_key[device][jit_functions_str]\n         stream = current_cuda_stream(device)\n         return _triton.runtime.launch(wargs, self.fn.do_not_specialize, cache_key, self.fn.arg_names,\n                                       device, stream, self.fn.bin_cache, num_warps, num_stages, extern_libs, self.add_to_cache,\n@@ -1127,13 +1129,13 @@ def version_key():\n \n class DependenciesFinder(ast.NodeVisitor):\n \n-    def __init__(self, globals, src, wargs) -> None:\n+    def __init__(self, globals, src, args) -> None:\n         super().__init__()\n         self.ret = hashlib.md5(src.encode(\"utf-8\")).hexdigest()\n         self.globals = globals\n         self.locals = {}\n-        self.constants = {i: arg.value for i, arg in enumerate(wargs) if isinstance(arg, triton.language.constexpr)}\n-        self.constants.update({i: None for i, arg in enumerate(wargs) if arg is None})\n+        self.constants = {i: arg.value for i, arg in enumerate(args) if isinstance(arg, triton.language.constexpr)}\n+        self.constants.update({i: None for i, arg in enumerate(args) if arg is None})\n \n     def visit_FunctionDef(self, node):\n         arg_names = self.visit(node.args)\n@@ -1236,10 +1238,10 @@ def __init__(self, fn, version=None, inline=True, do_not_specialize=None):\n         self.__globals__ = fn.__globals__\n         self.__module__ = fn.__module__\n \n-    def cache_key(self, wargs):\n+    def cache_key(self, args):\n         # TODO : hash should be attribute of `self`\n         if self.hash is None:\n-            dependencies_finder = DependenciesFinder(globals=self.__globals__, src=self.src, wargs=wargs)\n+            dependencies_finder = DependenciesFinder(globals=self.__globals__, src=self.src, args=args)\n             dependencies_finder.visit(self.parse())\n             self.hash = dependencies_finder.ret + version_key()\n         return self.hash"}]