[{"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -194,10 +194,13 @@ LogicalResult LoopPipeliner::initialize() {\n     if (auto loadOp = dyn_cast<triton::LoadOp>(&op)) {\n       auto ptr = loadOp.getPtr();\n       unsigned vec = axisInfoAnalysis->getPtrContiguity(ptr);\n+\n       if (auto mask = loadOp.getMask())\n         vec = std::min<unsigned>(vec, axisInfoAnalysis->getMaskAlignment(mask));\n+\n+      auto lattice = axisInfoAnalysis->getLatticeElement(ptr)->getValue();\n       auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n-      if (!tensorTy)\n+      if (!tensorTy || tensorTy.getRank() < 2)\n         continue;\n       auto ty = tensorTy.getElementType()\n                     .cast<triton::PointerType>()"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 16, "deletions": 15, "changes": 31, "file_content_changes": "@@ -6,6 +6,7 @@\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #ALs0 = #triton_gpu.slice<{parent=#AL, dim=0}>\n #BLs0 = #triton_gpu.slice<{parent=#BL, dim=0}>\n+#BLs1 = #triton_gpu.slice<{parent=#BL, dim=1}>\n #C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #A = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n #B = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n@@ -224,7 +225,7 @@ func.func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index,\n   return %loop#1 : tensor<128x128xf32, #C>\n }\n \n-// CHECK: func.func @lut_bmm_scalar\n+// CHECK: func.func @lut_bmm\n // CHECK: triton_gpu.insert_slice_async\n // CHECK: triton_gpu.insert_slice_async\n // CHECK: triton_gpu.insert_slice_async\n@@ -274,18 +275,18 @@ func.func @lut_bmm_scalar(%77: i64 {tt.divisibility=16: i32},\n // CHECK: triton_gpu.insert_slice_async\n // CHECK: triton_gpu.async_commit_group\n // CHECK: %[[LUT_BUFFER_0:.*]] = tt.load %arg15, {{.*}}\n-// CHECK: %[[LUT_BUFFER_1:.*]] = arith.muli {{.*}}, %[[LUT_BUFFER_0]]\n-// CHECK: %[[LUT_BUFFER_2:.*]] = tt.splat %[[LUT_BUFFER_1]]\n-// CHECK: %[[NEXT_BUFFER_0:.*]] = tt.addptr {{.*}}, %[[LUT_BUFFER_2]]\n+// CHECK: %[[LUT_BUFFER_1:.*]] = tt.expand_dims %[[LUT_BUFFER_0]] {axis = 1 : i32}\n+// CHECK: %[[LUT_BUFFER_2:.*]] = tt.broadcast %[[LUT_BUFFER_1]]\n+// CHECK: %[[LUT_BUFFER_3:.*]] = arith.muli {{.*}}, %[[LUT_BUFFER_2]]\n+// CHECK: %[[NEXT_BUFFER_0:.*]] = tt.addptr {{.*}}, %[[LUT_BUFFER_3]]\n // CHECK: %[[NEXT_BUFFER_1:.*]] = tt.addptr %arg14, {{.*}}\n // CHECK: triton_gpu.insert_slice_async %[[NEXT_BUFFER_1]]\n // CHECK: triton_gpu.insert_slice_async %[[NEXT_BUFFER_0]]\n // CHECK: triton_gpu.async_wait {num = 2 : i32}\n-func.func @lut_bmm_vector(%77: tensor<16xi64, #BLs0> {tt.divisibility=16: i32},\n+func.func @lut_bmm_vector(%77: tensor<16x16xi64, #BL> {tt.divisibility=16: i32, tt.constancy=16: i32},\n                    %76: index,\n                    %49: tensor<16x16x!tt.ptr<f16>, #AL> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n-                   %75: tensor<16x!tt.ptr<i64>, #BLs0>,\n-\n+                   %75: tensor<16x!tt.ptr<i64>, #BLs1>,\n                    %78: tensor<16x16xi32, #AL> {tt.constancy=16: i32, tt.divisibility=16: i32},\n                    %60: tensor<16x16x!tt.ptr<f16>, #BL> {tt.divisibility=16: i32, tt.contiguity=16 : i32}) -> tensor<16x16xf32, #C>{\n   %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #C>\n@@ -294,21 +295,21 @@ func.func @lut_bmm_vector(%77: tensor<16xi64, #BLs0> {tt.divisibility=16: i32},\n   %c0 = arith.constant 0 : index\n   %c0_i64 = arith.constant 0 : i64\n   %c1_i32 = arith.constant 1 : i32\n-  %c1_i32_splat = tt.splat %c1_i32 : (i32) -> tensor<16xi32, #BLs0>\n-  %79:3 = scf.for %arg18 = %c0 to %76 step %c1 iter_args(%arg19 = %cst, %arg20 = %49, %arg21 = %75) -> (tensor<16x16xf32, #C>, tensor<16x16x!tt.ptr<f16>, #AL>, tensor<16x!tt.ptr<i64>, #BLs0>) {\n+  %c1_i32_splat = tt.splat %c1_i32 : (i32) -> tensor<16xi32, #BLs1>\n+  %79:3 = scf.for %arg18 = %c0 to %76 step %c1 iter_args(%arg19 = %cst, %arg20 = %49, %arg21 = %75) -> (tensor<16x16xf32, #C>, tensor<16x16x!tt.ptr<f16>, #AL>, tensor<16x!tt.ptr<i64>, #BLs1>) {\n     %82 = tt.load %arg20 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #AL>\n-    %83 = tt.load %arg21 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16xi64, #BLs0>\n-    %84 = arith.muli %77, %83 : tensor<16xi64, #BLs0>\n-    %850 = tt.expand_dims %84 {axis=0: i32}: (tensor<16xi64, #BLs0>) -> tensor<1x16xi64, #BL>\n-    %85 = tt.broadcast %850 : (tensor<1x16xi64, #BL>) -> tensor<16x16xi64, #BL>\n+    %83 = tt.load %arg21 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16xi64, #BLs1>\n+    %84 = tt.expand_dims %83 {axis=1: i32}: (tensor<16xi64, #BLs1>) -> tensor<16x1xi64, #BL>\n+    %850 = tt.broadcast %84 : (tensor<16x1xi64, #BL>) -> tensor<16x16xi64, #BL>\n+    %85 = arith.muli %77, %850 : tensor<16x16xi64, #BL>\n     %86 = tt.addptr %60, %85 : tensor<16x16x!tt.ptr<f16>, #BL>, tensor<16x16xi64, #BL>\n     %87 = tt.load %86 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BL>\n     %88 = triton_gpu.convert_layout %82 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A>\n     %89 = triton_gpu.convert_layout %87 : (tensor<16x16xf16, #BL>) -> tensor<16x16xf16, #B>\n     %90 = tt.dot %88, %89, %arg19 {allowTF32 = true} : tensor<16x16xf16, #A> * tensor<16x16xf16, #B> -> tensor<16x16xf32, #C>\n     %91 = tt.addptr %arg20, %78 : tensor<16x16x!tt.ptr<f16>, #AL>, tensor<16x16xi32, #AL>\n-    %92 = tt.addptr %arg21, %c1_i32_splat : tensor<16x!tt.ptr<i64>, #BLs0>, tensor<16xi32, #BLs0>\n-    scf.yield %90, %91, %92 : tensor<16x16xf32, #C>, tensor<16x16x!tt.ptr<f16>, #AL>, tensor<16x!tt.ptr<i64>, #BLs0>\n+    %92 = tt.addptr %arg21, %c1_i32_splat : tensor<16x!tt.ptr<i64>, #BLs1>, tensor<16xi32, #BLs1>\n+    scf.yield %90, %91, %92 : tensor<16x16xf32, #C>, tensor<16x16x!tt.ptr<f16>, #AL>, tensor<16x!tt.ptr<i64>, #BLs1>\n   }\n   return %79#0 : tensor<16x16xf32, #C>\n }"}]