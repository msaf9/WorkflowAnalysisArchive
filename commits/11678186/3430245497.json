[{"filename": "CMakeLists.txt", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -15,6 +15,10 @@ endif()\n option(TRITON_BUILD_TUTORIALS \"Build C++ Triton tutorials\" ON)\n option(TRITON_BUILD_PYTHON_MODULE \"Build Python Triton bindings\" OFF)\n \n+# Ensure Python3 vars are set correctly\n+#  used conditionally in this file and by lit tests\n+find_package(Python3 REQUIRED COMPONENTS Development Interpreter)\n+\n # Default build type\n if(NOT CMAKE_BUILD_TYPE)\n   message(STATUS \"Default build type: Release\")\n@@ -133,24 +137,22 @@ endif()\n if(TRITON_BUILD_PYTHON_MODULE)\n     message(STATUS \"Adding Python module\")\n     set(PYTHON_SRC_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/src)\n+    set(PYTHON_SRC ${PYTHON_SRC_PATH}/main.cc ${PYTHON_SRC_PATH}/triton.cc)\n     include_directories(\".\" ${PYTHON_SRC_PATH})\n     if (PYTHON_INCLUDE_DIRS)\n       include_directories(${PYTHON_INCLUDE_DIRS})\n     else()\n-      find_package(Python3 REQUIRED COMPONENTS Development)\n       include_directories(${Python3_INCLUDE_DIRS})\n       link_directories(${Python3_LIBRARY_DIRS})\n       link_libraries(${Python3_LIBRARIES})\n       add_link_options(${Python3_LINK_OPTIONS})\n     endif()\n-    set(PYTHON_SRC ${PYTHON_SRC_PATH}/main.cc ${PYTHON_SRC_PATH}/triton.cc)\n endif()\n \n \n # # Triton\n # file(GLOB_RECURSE LIBTRITON_SRC lib/*.cc)\n # if (WIN32 AND TRITON_BUILD_PYTHON_MODULE)\n-#     find_package(Python3 REQUIRED COMPONENTS Development)\n #     Python3_add_library(triton SHARED ${LIBTRITON_SRC} ${PYTHON_SRC})\n #     set_target_properties(triton PROPERTIES SUFFIX \".pyd\")\n #     set_target_properties(triton PROPERTIES PREFIX \"lib\")"}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -21,11 +21,11 @@ template <typename Int> Int product(llvm::ArrayRef<Int> arr) {\n template <typename Int> Int ceil(Int m, Int n) { return (m + n - 1) / n; }\n \n // output[i] = input[order[i]]\n-template <typename T>\n-SmallVector<T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) {\n+template <typename T, typename RES_T = T>\n+SmallVector<RES_T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) {\n   size_t rank = order.size();\n   assert(input.size() == rank);\n-  SmallVector<T> result(rank);\n+  SmallVector<RES_T> result(rank);\n   for (auto it : llvm::enumerate(order)) {\n     result[it.index()] = input[it.value()];\n   }"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -320,7 +320,7 @@ struct PTXInstrExecution {\n   // Prefix a !predicate to the instruction.\n   PTXInstrExecution &predicateNot(mlir::Value value, StringRef constraint) {\n     pred = instr->builder->newOperand(value, constraint);\n-    pred->repr = [](int idx) { return \"@!%\" + std::to_string(idx); };\n+    pred->repr = [](int idx) { return \"@!$\" + std::to_string(idx); };\n     return *this;\n   }\n "}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -77,7 +77,7 @@ SmallVector<unsigned> getScratchConfigForReduce(triton::ReduceOp op) {\n   auto srcShape = srcTy.getShape();\n   auto axis = op.axis();\n \n-  bool fastReduce = axis == 1; // FIXME(Qingyi): The fastest-changing dimension\n+  bool fastReduce = axis == srcLayout.getOrder()[0];\n \n   SmallVector<unsigned> smemShape;\n   for (auto d : srcShape)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/PtxAsmFormat.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -125,7 +125,7 @@ std::string PTXBuilder::dump() const {\n     lines.push_back(exec->dump());\n   }\n \n-  return strJoin(lines, \"\\r\\n\");\n+  return strJoin(lines, \"\\n\\t\");\n }\n \n PTXInstrExecution &PTXInstrCommon::call(ArrayRef<Operand *> oprs) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 551, "deletions": 271, "changes": 822, "file_content_changes": "@@ -83,6 +83,10 @@ static Value createLLVMIntegerConstant(OpBuilder &builder, Location loc,\n \n } // namespace\n \n+// A helper function for using printf in LLVM conversion.\n+void llPrintf(StringRef msg, ValueRange args,\n+              ConversionPatternRewriter &rewriter);\n+\n // Shortcuts for some commonly used LLVM ops to keep code simple and intuitive\n #define zext(...) rewriter.create<LLVM::ZExtOp>(loc, __VA_ARGS__)\n #define udiv(...) rewriter.create<LLVM::UDivOp>(loc, __VA_ARGS__)\n@@ -139,6 +143,10 @@ static Value createLLVMIntegerConstant(OpBuilder &builder, Location loc,\n   LLVM::createIndexConstant(rewriter, loc, this->getTypeConverter(),           \\\n                             __VA_ARGS__)\n \n+// Helper function\n+#define tid_val() getThreadId(rewriter, loc)\n+#define llprintf(fmt, ...) LLVM::llPrintf(fmt, {__VA_ARGS__}, rewriter)\n+\n } // namespace LLVM\n } // namespace mlir\n \n@@ -338,6 +346,7 @@ Value getStructFromElements(Location loc, ValueRange resultVals,\n   return llvmStruct;\n }\n \n+// Delinearize on compile-time consts, assuming the order is [n, .. 2, 1, 0]\n template <typename T>\n static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape) {\n   // shape: {a, b, c, d}  ->  accMul: {b*c*d, c*d, d, 1}\n@@ -355,6 +364,7 @@ static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape) {\n   return multiDimIndex;\n }\n \n+// Linearize on compile-time consts, assuming the order is [n, .. 2, 1, 0]\n template <typename T>\n static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n   assert(multiDimIndex.size() == shape.size());\n@@ -514,12 +524,12 @@ class ConvertTritonGPUOpToLLVMPattern\n       multiDim[0] = linear;\n     } else {\n       Value remained = linear;\n-      for (auto &&en : llvm::enumerate(llvm::reverse(shape.drop_front()))) {\n+      for (auto &&en : llvm::enumerate(shape.drop_back())) {\n         Value dimSize = idx_val(en.value());\n-        multiDim[rank - 1 - en.index()] = urem(remained, dimSize);\n+        multiDim[en.index()] = urem(remained, dimSize);\n         remained = udiv(remained, dimSize);\n       }\n-      multiDim[0] = remained;\n+      multiDim[rank - 1] = remained;\n     }\n     return multiDim;\n   }\n@@ -529,9 +539,9 @@ class ConvertTritonGPUOpToLLVMPattern\n     int rank = multiDim.size();\n     Value linear = idx_val(0);\n     if (rank > 0) {\n-      linear = multiDim.front();\n+      linear = multiDim.back();\n       for (auto [dim, shape] :\n-           llvm::zip(multiDim.drop_front(), shape.drop_front())) {\n+           llvm::reverse(llvm::zip(multiDim.drop_back(), shape.drop_back()))) {\n         Value dimSize = idx_val(shape);\n         linear = add(mul(linear, dimSize), dim);\n       }\n@@ -574,6 +584,7 @@ class ConvertTritonGPUOpToLLVMPattern\n         delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n     SmallVector<Value> multiDimThreadId =\n         delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n+\n     SmallVector<Value> multiDimBase(rank);\n     for (unsigned k = 0; k < rank; ++k) {\n       // Wrap around multiDimWarpId/multiDimThreadId incase\n@@ -1129,7 +1140,7 @@ struct LoadOpConversion\n       if (other) {\n         for (size_t ii = 0; ii < nWords; ++ii) {\n           PTXInstr &mov = *ptxBuilder.create<>(\"mov\");\n-          mov.o(\"u\", width);\n+          mov.o(\"u\" + std::to_string(width));\n \n           size_t size = width / valueElemNbits;\n \n@@ -1453,7 +1464,9 @@ struct ReduceOpConversion\n LogicalResult\n ReduceOpConversion::matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n                                     ConversionPatternRewriter &rewriter) const {\n-  if (op.axis() == 1) // FIXME(Qingyi): The fastest-changing dimension\n+  auto srcTy = op.operand().getType().cast<RankedTensorType>();\n+  auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+  if (op.axis() == srcLayout.getOrder()[0])\n     return matchAndRewriteFast(op, adaptor, rewriter);\n   return matchAndRewriteBasic(op, adaptor, rewriter);\n }\n@@ -1535,6 +1548,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n \n   auto srcTy = op.operand().getType().cast<RankedTensorType>();\n   auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto srcOrd = srcLayout.getOrder();\n   auto srcShape = srcTy.getShape();\n \n   auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n@@ -1578,16 +1592,21 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n     SmallVector<Value> writeIdx = indices[key];\n \n     writeIdx[axis] = udiv(writeIdx[axis], sizePerThread);\n-    Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n+    Value writeOffset =\n+        linearize(rewriter, loc, reorder<Value>(writeIdx, srcOrd),\n+                  reorder<unsigned>(smemShape, srcOrd));\n     Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n     store(acc, writePtr);\n \n     SmallVector<Value> readIdx(writeIdx.size(), ints[0]);\n     for (int N = smemShape[axis] / 2; N > 0; N >>= 1) {\n       readIdx[axis] = ints[N];\n       Value readMask = icmp_slt(writeIdx[axis], ints[N]);\n-      Value readOffset = select(\n-          readMask, linearize(rewriter, loc, readIdx, smemShape), ints[0]);\n+      Value readOffset =\n+          select(readMask,\n+                 linearize(rewriter, loc, reorder<Value>(readIdx, srcOrd),\n+                           reorder<unsigned>(smemShape, srcOrd)),\n+                 ints[0]);\n       Value readPtr = gep(elemPtrTy, writePtr, readOffset);\n       barrier();\n       accumulate(rewriter, loc, op.redOp(), acc, load(readPtr), false);\n@@ -1610,7 +1629,9 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n     for (unsigned i = 0; i < resultElems; ++i) {\n       SmallVector<Value> readIdx = resultIndices[i];\n       readIdx.insert(readIdx.begin() + axis, ints[0]);\n-      Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n+      Value readOffset =\n+          linearize(rewriter, loc, reorder<Value>(readIdx, srcOrd),\n+                    reorder<unsigned>(smemShape, srcOrd));\n       Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n       resultVals[i] = load(readPtr);\n     }\n@@ -1639,6 +1660,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   auto srcTy = op.operand().getType().cast<RankedTensorType>();\n   auto srcLayout = srcTy.getEncoding();\n   auto srcShape = srcTy.getShape();\n+  auto srcRank = srcTy.getRank();\n \n   auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcLayout);\n   auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcLayout);\n@@ -1683,6 +1705,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n       delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n   SmallVector<Value> multiDimWarpId =\n       delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n+\n   Value laneIdAxis = multiDimLaneId[axis];\n   Value warpIdAxis = multiDimWarpId[axis];\n \n@@ -1700,56 +1723,77 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n       accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n     }\n \n-    if (sizeInterWarps == 1) {\n-      SmallVector<Value> writeIdx = indices[key];\n-      writeIdx[axis] = zero;\n-      Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n-      Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n-      storeShared(rewriter, loc, writePtr, acc, laneZero);\n-    } else {\n-      SmallVector<Value> writeIdx = indices[key];\n-      writeIdx[axis] =\n-          warpIdAxis; // axis must be the fastest-changing dimension\n-      Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n-      Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n-      storeShared(rewriter, loc, writePtr, acc, laneZero);\n-      barrier();\n+    SmallVector<Value> writeIdx = indices[key];\n+    writeIdx[axis] = (sizeInterWarps == 1) ? zero : warpIdAxis;\n+    Value writeOffset =\n+        linearize(rewriter, loc, reorder<Value>(writeIdx, order),\n+                  reorder<unsigned>(smemShape, order));\n+    Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n+    storeShared(rewriter, loc, writePtr, acc, laneZero);\n+  }\n \n-      SmallVector<Value> readIdx = writeIdx;\n-      readIdx[axis] = urem(laneId, i32_val(sizeInterWarps));\n-      Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n-      Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n-      acc = load(readPtr);\n+  barrier();\n \n-      // reduce across warps\n-      for (unsigned N = sizeInterWarps / 2; N > 0; N >>= 1) {\n-        Value shfl = shflSync(rewriter, loc, acc, N);\n-        accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n-      }\n+  // the second round of shuffle reduction\n+  //   now the problem size: sizeInterWarps, s1, s2, .. , sn  =>\n+  //                                      1, s1, s2, .. , sn\n+  //   where sizeInterWarps is 2^m\n+  //\n+  // each thread needs to process:\n+  //   elemsPerThread = sizeInterWarps * s1 * s2 .. Sn / numThreads\n+  unsigned elems = product<unsigned>(smemShape);\n+  unsigned numThreads = product<unsigned>(srcLayout.getWarpsPerCTA()) * 32;\n+  unsigned elemsPerThread = std::max<unsigned>(elems / numThreads, 1);\n+  Value readOffset = threadId;\n+  for (unsigned round = 0; round < elemsPerThread; ++round) {\n+    Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n+    Value acc = load(readPtr);\n+\n+    for (unsigned N = sizeInterWarps / 2; N > 0; N >>= 1) {\n+      Value shfl = shflSync(rewriter, loc, acc, N);\n+      accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n+    }\n \n-      writeIdx[axis] = zero;\n-      writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n-      writePtr = gep(elemPtrTy, smemBase, writeOffset);\n-      storeShared(rewriter, loc, writePtr, acc, and_(laneZero, warpZero));\n+    Value writeOffset = udiv(readOffset, i32_val(sizeInterWarps));\n+    Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n+    Value threadIsNeeded = icmp_slt(threadId, i32_val(elems));\n+    Value laneIdModSizeInterWarps = urem(laneId, i32_val(sizeInterWarps));\n+    Value laneIdModSizeInterWarpsIsZero =\n+        icmp_eq(laneIdModSizeInterWarps, zero);\n+    storeShared(rewriter, loc, writePtr, acc,\n+                and_(threadIsNeeded, laneIdModSizeInterWarpsIsZero));\n+\n+    if (round != elemsPerThread - 1) {\n+      readOffset = add(readOffset, i32_val(numThreads));\n     }\n   }\n \n+  // We could avoid this barrier in some of the layouts, however this is not\n+  // the general case. TODO: optimize the barrier incase the layouts are\n+  // accepted.\n+  barrier();\n+\n   // set output values\n   if (auto resultTy = op.getType().dyn_cast<RankedTensorType>()) {\n     // nd-tensor where n >= 1\n     auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n     auto resultShape = resultTy.getShape();\n+    SmallVector<unsigned> resultOrd;\n+    for (auto ord : order) {\n+      if (ord != 0)\n+        resultOrd.push_back(ord - 1);\n+    }\n \n     unsigned resultElems = getElemsPerThread(resultTy);\n     auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultShape);\n     assert(resultIndices.size() == resultElems);\n \n-    barrier();\n     SmallVector<Value> resultVals(resultElems);\n     for (size_t i = 0; i < resultElems; ++i) {\n       SmallVector<Value> readIdx = resultIndices[i];\n-      readIdx.insert(readIdx.begin() + axis, i32_val(0));\n-      Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n+      Value readOffset =\n+          linearize(rewriter, loc, reorder<Value>(readIdx, resultOrd),\n+                    reorder<int64_t, unsigned>(resultShape, resultOrd));\n       Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n       resultVals[i] = load(readPtr);\n     }\n@@ -1761,7 +1805,6 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n     rewriter.replaceOp(op, ret);\n   } else {\n     // 0d-tensor -> scalar\n-    barrier();\n     Value resultVal = load(smemBase);\n     rewriter.replaceOp(op, resultVal);\n   }\n@@ -1798,6 +1841,191 @@ struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n   }\n };\n \n+struct PrintfOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::PrintfOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::PrintfOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::PrintfOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto loc = op->getLoc();\n+    SmallVector<Value, 16> operands;\n+    for (auto operand : adaptor.getOperands()) {\n+      auto sub_operands = this->getElementsFromStruct(loc, operand, rewriter);\n+      for (auto elem : sub_operands) {\n+        operands.push_back(elem);\n+      }\n+    }\n+    std::string formatStr;\n+    llvm::raw_string_ostream os(formatStr);\n+    os << op.prefix();\n+    if (operands.size() > 0) {\n+      os << getFormatSubstr(operands[0]);\n+    }\n+\n+    for (size_t i = 1; i < operands.size(); ++i) {\n+      os << \", \" << getFormatSubstr(operands[i]);\n+    }\n+    llPrintf(formatStr, operands, rewriter);\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+  // get format specific for each input value\n+  // currently support pointer, i8, i16, i32, i64, f16, bf16, f32, f64\n+  std::string getFormatSubstr(Value value) const {\n+    Type type = value.getType();\n+    unsigned width = type.getIntOrFloatBitWidth();\n+\n+    if (type.isa<LLVM::LLVMPointerType>()) {\n+      return \"%p\";\n+    } else if (type.isBF16() || type.isF16() || type.isF32() || type.isF64()) {\n+      return \"%f\";\n+    } else if (type.isSignedInteger()) {\n+      return \"%i\";\n+    } else if (type.isUnsignedInteger() || type.isSignlessInteger()) {\n+      return \"%u\";\n+    }\n+    assert(false && \"not supported type\");\n+  }\n+\n+  // declare vprintf(i8*, i8*) as external function\n+  static LLVM::LLVMFuncOp\n+  getVprintfDeclaration(ConversionPatternRewriter &rewriter) {\n+    auto moduleOp =\n+        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n+    StringRef funcName(\"vprintf\");\n+    Operation *funcOp = moduleOp.lookupSymbol(funcName);\n+    if (funcOp)\n+      return cast<LLVM::LLVMFuncOp>(*funcOp);\n+\n+    auto *context = rewriter.getContext();\n+\n+    SmallVector<Type> argsType{ptr_ty(IntegerType::get(context, 8)),\n+                               ptr_ty(IntegerType::get(context, 8))};\n+    auto funcType = LLVM::LLVMFunctionType::get(i32_ty, argsType);\n+\n+    ConversionPatternRewriter::InsertionGuard guard(rewriter);\n+    rewriter.setInsertionPointToStart(moduleOp.getBody());\n+\n+    return rewriter.create<LLVM::LLVMFuncOp>(UnknownLoc::get(context), funcName,\n+                                             funcType);\n+  }\n+\n+  // extend integer to int32, extend float to float64\n+  // this comes from vprintf alignment requirements.\n+  static std::pair<Type, Value>\n+  promoteValue(ConversionPatternRewriter &rewriter, Value value) {\n+    auto *context = rewriter.getContext();\n+    auto type = value.getType();\n+    type.dump();\n+    unsigned width = type.getIntOrFloatBitWidth();\n+    Value newOp = value;\n+    Type newType = type;\n+\n+    bool bUnsigned = type.isUnsignedInteger();\n+    if (type.isIntOrIndex() && width < 32) {\n+      if (bUnsigned) {\n+        newType = ui32_ty;\n+        newOp = rewriter.create<LLVM::ZExtOp>(UnknownLoc::get(context), newType,\n+                                              value);\n+      } else {\n+        newType = i32_ty;\n+        newOp = rewriter.create<LLVM::SExtOp>(UnknownLoc::get(context), newType,\n+                                              value);\n+      }\n+    } else if (type.isBF16() || type.isF16() || type.isF32()) {\n+      newType = f64_ty;\n+      newOp = rewriter.create<LLVM::FPExtOp>(UnknownLoc::get(context), newType,\n+                                             value);\n+    }\n+\n+    return {newType, newOp};\n+  }\n+\n+  static void llPrintf(StringRef msg, ValueRange args,\n+                       ConversionPatternRewriter &rewriter) {\n+    static const char formatStringPrefix[] = \"printfFormat_\";\n+    assert(!msg.empty() && \"printf with empty string not support\");\n+    Type int8Ptr = ptr_ty(i8_ty);\n+\n+    auto *context = rewriter.getContext();\n+    auto moduleOp =\n+        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n+    auto funcOp = getVprintfDeclaration(rewriter);\n+\n+    Value one = rewriter.create<LLVM::ConstantOp>(\n+        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(1));\n+    Value zero = rewriter.create<LLVM::ConstantOp>(\n+        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(0));\n+\n+    unsigned stringNumber = 0;\n+    SmallString<16> stringConstName;\n+    do {\n+      stringConstName.clear();\n+      (formatStringPrefix + Twine(stringNumber++)).toStringRef(stringConstName);\n+    } while (moduleOp.lookupSymbol(stringConstName));\n+\n+    llvm::SmallString<64> formatString(msg);\n+    formatString.push_back('\\n');\n+    formatString.push_back('\\0');\n+    size_t formatStringSize = formatString.size_in_bytes();\n+    auto globalType = LLVM::LLVMArrayType::get(i8_ty, formatStringSize);\n+\n+    LLVM::GlobalOp global;\n+    {\n+      ConversionPatternRewriter::InsertionGuard guard(rewriter);\n+      rewriter.setInsertionPointToStart(moduleOp.getBody());\n+      global = rewriter.create<LLVM::GlobalOp>(\n+          UnknownLoc::get(context), globalType,\n+          /*isConstant=*/true, LLVM::Linkage::Internal, stringConstName,\n+          rewriter.getStringAttr(formatString));\n+    }\n+\n+    Value globalPtr =\n+        rewriter.create<LLVM::AddressOfOp>(UnknownLoc::get(context), global);\n+    Value stringStart =\n+        rewriter.create<LLVM::GEPOp>(UnknownLoc::get(context), int8Ptr,\n+                                     globalPtr, mlir::ValueRange({zero, zero}));\n+\n+    Value bufferPtr =\n+        rewriter.create<LLVM::NullOp>(UnknownLoc::get(context), int8Ptr);\n+\n+    SmallVector<Value, 16> newArgs;\n+    if (args.size() >= 1) {\n+      SmallVector<Type> argTypes;\n+      for (auto arg : args) {\n+        Type newType;\n+        Value newArg;\n+        std::tie(newType, newArg) = promoteValue(rewriter, arg);\n+        argTypes.push_back(newType);\n+        newArgs.push_back(newArg);\n+      }\n+\n+      Type structTy = LLVM::LLVMStructType::getLiteral(context, argTypes);\n+      auto allocated = rewriter.create<LLVM::AllocaOp>(UnknownLoc::get(context),\n+                                                       ptr_ty(structTy), one,\n+                                                       /*alignment=*/0);\n+\n+      for (const auto &entry : llvm::enumerate(newArgs)) {\n+        auto index = rewriter.create<LLVM::ConstantOp>(\n+            UnknownLoc::get(context), i32_ty,\n+            rewriter.getI32IntegerAttr(entry.index()));\n+        auto fieldPtr = rewriter.create<LLVM::GEPOp>(\n+            UnknownLoc::get(context), ptr_ty(argTypes[entry.index()]),\n+            allocated, ArrayRef<Value>{zero, index});\n+        rewriter.create<LLVM::StoreOp>(UnknownLoc::get(context), entry.value(),\n+                                       fieldPtr);\n+      }\n+      bufferPtr = rewriter.create<LLVM::BitcastOp>(UnknownLoc::get(context),\n+                                                   int8Ptr, allocated);\n+    }\n+\n+    ValueRange operands{stringStart, bufferPtr};\n+    rewriter.create<LLVM::CallOp>(UnknownLoc::get(context), funcOp, operands);\n+  }\n+};\n+\n struct MakeRangeOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::MakeRangeOp> {\n \n@@ -2161,6 +2389,7 @@ struct ConvertLayoutOpConversion\n   }\n \n private:\n+\n   template <typename T>\n   SmallVector<T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) const {\n     size_t rank = order.size();\n@@ -2248,6 +2477,7 @@ struct ConvertLayoutOpConversion\n     llvm_unreachable(\"unexpected layout in getMultiDimOffset\");\n   }\n \n+\n   // shared memory rd/st for blocked or mma layout with data padding\n   void processReplica(Location loc, ConversionPatternRewriter &rewriter,\n                       bool stNotRd, RankedTensorType type,\n@@ -2792,8 +3022,8 @@ class MMA16816SmemLoader {\n       Value sOffset =\n           mul(i32_val(matIdx[order[1]] * sMatStride * sMatShape), sTileStride);\n       Value sOffsetPtr = gep(shemPtrTy, ptr, sOffset);\n-      PTXBuilder builder;\n \n+      PTXBuilder builder;\n       // ldmatrix.m8n8.x4 returns 4x2xfp16(that is 4xb32) elements for a\n       // thread.\n       auto resArgs = builder.newListOperand(4, \"=r\");\n@@ -2812,12 +3042,13 @@ class MMA16816SmemLoader {\n         return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n       };\n \n-      Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n+      // The struct should have exactly the same element types.\n+      Type elemType = resV4.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n \n-      return {extract_val(fp16x2Ty, resV4, getIntAttr(0)),\n-              extract_val(fp16x2Ty, resV4, getIntAttr(1)),\n-              extract_val(fp16x2Ty, resV4, getIntAttr(2)),\n-              extract_val(fp16x2Ty, resV4, getIntAttr(3))};\n+      return {extract_val(elemType, resV4, getIntAttr(0)),\n+              extract_val(elemType, resV4, getIntAttr(1)),\n+              extract_val(elemType, resV4, getIntAttr(2)),\n+              extract_val(elemType, resV4, getIntAttr(3))};\n     } else if (elemBytes == 4 &&\n                needTrans) { // Use lds.32 to load tf32 matrices\n       Value ptr2 = getPtr(ptrIdx + 1);\n@@ -2830,20 +3061,25 @@ class MMA16816SmemLoader {\n \n       Value elems[4];\n       Type elemTy = type::f32Ty(ctx);\n+      Type elemPtrTy = ptr_ty(elemTy);\n       if (kOrder == 1) {\n-        elems[0] = load(gep(elemTy, ptr, sOffsetElemVal));\n-        elems[1] = load(gep(elemTy, ptr2, sOffsetElemVal));\n-        elems[2] = load(gep(elemTy, ptr, sOffsetArrElemVal));\n-        elems[3] = load(gep(elemTy, ptr2, sOffsetArrElemVal));\n+        elems[0] = load(gep(elemPtrTy, ptr, i32_val(sOffsetElem)));\n+        elems[1] = load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem)));\n+        elems[2] =\n+            load(gep(elemPtrTy, ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n+        elems[3] =\n+            load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n       } else {\n-        elems[0] = load(gep(elemTy, ptr, sOffsetElemVal));\n-        elems[2] = load(gep(elemTy, ptr2, sOffsetElemVal));\n-        elems[1] = load(gep(elemTy, ptr, sOffsetArrElemVal));\n-        elems[3] = load(gep(elemTy, ptr2, sOffsetArrElemVal));\n+        elems[0] = load(gep(elemPtrTy, ptr, i32_val(sOffsetElem)));\n+        elems[2] = load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem)));\n+        elems[1] =\n+            load(gep(elemPtrTy, ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n+        elems[3] =\n+            load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n       }\n-\n       return {elems[0], elems[1], elems[2], elems[3]};\n-    } else if (elemBytes == 1 && needTrans) {\n+\n+    } else if (elemBytes == 1 && needTrans) { // work with int8\n       std::array<std::array<Value, 4>, 2> ptrs;\n       ptrs[0] = {\n           getPtr(ptrIdx),\n@@ -2873,15 +3109,16 @@ class MMA16816SmemLoader {\n \n       Value i8Elems[4][4];\n       Type elemTy = type::i8Ty(ctx);\n+      Type elemPtrTy = ptr_ty(elemTy);\n       if (kOrder == 1) {\n         for (int i = 0; i < 2; ++i)\n           for (int j = 0; j < 4; ++j)\n-            i8Elems[i][j] = load(gep(elemTy, ptrs[i][j], sOffsetElemVal));\n+            i8Elems[i][j] = load(gep(elemPtrTy, ptrs[i][j], sOffsetElemVal));\n \n         for (int i = 2; i < 4; ++i)\n           for (int j = 0; j < 4; ++j)\n             i8Elems[i][j] =\n-                load(gep(elemTy, ptrs[i - 2][j], sOffsetArrElemVal));\n+                load(gep(elemPtrTy, ptrs[i - 2][j], sOffsetArrElemVal));\n \n         for (int m = 0; m < 4; ++m) {\n           for (int e = 0; e < 4; ++e)\n@@ -2891,13 +3128,13 @@ class MMA16816SmemLoader {\n         }\n       } else { // k first\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[0][j] = load(gep(elemTy, ptrs[0][j], sOffsetElemVal));\n+          i8Elems[0][j] = load(gep(elemPtrTy, ptrs[0][j], sOffsetElemVal));\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[2][j] = load(gep(elemTy, ptrs[1][j], sOffsetElemVal));\n+          i8Elems[2][j] = load(gep(elemPtrTy, ptrs[1][j], sOffsetElemVal));\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[1][j] = load(gep(elemTy, ptrs[0][j], sOffsetArrElemVal));\n+          i8Elems[1][j] = load(gep(elemPtrTy, ptrs[0][j], sOffsetArrElemVal));\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[3][j] = load(gep(elemTy, ptrs[1][j], sOffsetArrElemVal));\n+          i8Elems[3][j] = load(gep(elemPtrTy, ptrs[1][j], sOffsetArrElemVal));\n \n         for (int m = 0; m < 4; ++m) {\n           for (int e = 0; e < 4; ++e)\n@@ -2981,6 +3218,7 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n     size_t reduceAxis = 1;\n     unsigned K = AShape[reduceAxis];\n     bool isOuter = K == 1;\n+\n     bool isMMA = D.getType()\n                      .cast<RankedTensorType>()\n                      .getEncoding()\n@@ -2992,11 +3230,13 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n                       .getEncoding()\n                       .cast<MmaEncodingAttr>();\n \n-    if (!isOuter && isMMA) {\n+    bool isHMMA = isDotHMMA(op);\n+    if (!isOuter && isMMA && isHMMA) {\n       if (mmaLayout.getVersion() == 1)\n         return convertMMA884(op, adaptor, rewriter);\n       if (mmaLayout.getVersion() == 2)\n         return convertMMA16816(op, adaptor, rewriter);\n+\n       llvm::report_fatal_error(\n           \"Unsupported MMA kind found when converting DotOp to LLVM.\");\n     }\n@@ -3009,6 +3249,49 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n         \"Unsupported DotOp found when converting TritonGPU to LLVM.\");\n   }\n \n+  // Tell whether a DotOp support HMMA.\n+  // This is port from the master branch, the original logic is retained.\n+  static bool isDotHMMA(DotOp op) {\n+    auto a = op.a();\n+    auto b = op.b();\n+    auto c = op.c();\n+    auto d = op.getResult();\n+    auto aTensorTy = a.getType().cast<RankedTensorType>();\n+    auto bTensorTy = b.getType().cast<RankedTensorType>();\n+    auto cTensorTy = c.getType().cast<RankedTensorType>();\n+    auto dTensorTy = d.getType().cast<RankedTensorType>();\n+\n+    if (!dTensorTy.getEncoding().isa<MmaEncodingAttr>())\n+      return false;\n+\n+    auto mmaLayout = dTensorTy.getEncoding().cast<MmaEncodingAttr>();\n+    auto aElemTy = aTensorTy.getElementType();\n+    auto bElemTy = bTensorTy.getElementType();\n+\n+    assert((mmaLayout.getVersion() == 1 || mmaLayout.getVersion() == 2) &&\n+           \"Unexpected MMA layout version found\");\n+    // Refer to mma section for the data type supported by Volta and Hopper\n+    // Tensor Core in\n+    // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-fragment-mma-884-f16\n+    return (aElemTy.isF16() && bElemTy.isF16()) ||\n+           (aElemTy.isBF16() && bElemTy.isBF16()) ||\n+           (aElemTy.isF32() && bElemTy.isF32() && op.allowTF32() &&\n+            mmaLayout.getVersion() >= 2) ||\n+           (aElemTy.isInteger(8) && bElemTy.isInteger(8) &&\n+            mmaLayout.getVersion() >= 2);\n+  }\n+\n+  // Tell whether a DotOp support HMMA by the operand type(either $a or $b).\n+  // We cannot get both the operand types(in TypeConverter), here we assume the\n+  // types of both the operands are identical here.\n+  // TODO[Superjomn]: Find a better way to implement it.\n+  static bool isDotHMMA(TensorType operand, bool allowTF32, int mmaVersion) {\n+    auto elemTy = operand.getElementType();\n+    return elemTy.isF16() || elemTy.isBF16() ||\n+           (elemTy.isF32() && allowTF32 && mmaVersion >= 2) ||\n+           (elemTy.isInteger(8) && mmaVersion >= 2);\n+  }\n+\n private:\n   // Convert to mma.m16n8k16\n   LogicalResult convertMMA16816(triton::DotOp a, OpAdaptor adaptor,\n@@ -3018,10 +3301,7 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n                               ConversionPatternRewriter &rewriter) const;\n \n   LogicalResult convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n-                              ConversionPatternRewriter &rewriter) const {\n-    assert(false && \"Not implemented yet.\");\n-    return failure();\n-  }\n+                              ConversionPatternRewriter &rewriter) const;\n };\n \n // Helper for conversion of DotOp with mma<version=1>, that is sm<80\n@@ -3520,6 +3800,7 @@ struct MMA16816ConversionHelper {\n     std::function<void(int, int)> loadFn;\n     auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(aTensorTy);\n     auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(aTensorTy);\n+\n     int numRepM = getNumRepM(aTensorTy, shape[0]);\n     int numRepK = getNumRepK(aTensorTy, shape[1]);\n \n@@ -3635,26 +3916,34 @@ struct MMA16816ConversionHelper {\n                                              std::to_string(i)));\n         // reuse the output registers\n       }\n+\n       mma(retArgs, aArgs, bArgs, cArgs);\n       Value mmaOut = builder.launch(rewriter, loc, helper.getMmaRetType());\n \n       auto getIntAttr = [&](int v) {\n         return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n       };\n \n+      Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n       for (int i = 0; i < 4; ++i)\n         fc[m * colsPerThread + 4 * n + i] =\n-            extract_val(type::f32Ty(ctx), mmaOut, getIntAttr(i));\n+            extract_val(elemTy, mmaOut, getIntAttr(i));\n     };\n \n     for (int k = 0; k < numRepK; ++k)\n       for (int m = 0; m < numRepM; ++m)\n         for (int n = 0; n < numRepN; ++n)\n           callMma(2 * m, n, 2 * k);\n \n+    Type resElemTy = dTensorTy.getElementType();\n+\n+    for (auto &elem : fc) {\n+      elem = bitcast(elem, resElemTy);\n+    }\n+\n     // replace with new packed result\n     Type structTy = LLVM::LLVMStructType::getLiteral(\n-        ctx, SmallVector<Type>(fc.size(), type::f32Ty(ctx)));\n+        ctx, SmallVector<Type>(fc.size(), resElemTy));\n     Value res = getStructFromElements(loc, fc, rewriter, structTy);\n     rewriter.replaceOp(op, res);\n \n@@ -3690,9 +3979,7 @@ struct MMA16816ConversionHelper {\n           tensorTy.getShape() /*tileShape*/, instrShape, matShape, perPhase,\n           maxPhase, elemBytes, rewriter, typeConverter, loc);\n       SmallVector<Value> offs = loader.computeOffsets(warpId, lane);\n-\n       const int numPtrs = loader.getNumPtr();\n-\n       SmallVector<Value> ptrs(numPtrs);\n \n       Type smemPtrTy = helper.getShemPtrTy();\n@@ -3704,6 +3991,7 @@ struct MMA16816ConversionHelper {\n       auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n           (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n           ptrs, helper.getMatType(), helper.getShemPtrTy());\n+\n       if (!needTrans) {\n         ld2(vals, a, b, ha0);\n         ld2(vals, a + 1, b, ha1);\n@@ -3748,10 +4036,9 @@ struct MMA16816ConversionHelper {\n \n     assert(!elems.empty());\n \n-    Type fp16Ty = type::f16Ty(ctx);\n-    Type fp16x2Ty = vec_ty(fp16Ty, 2);\n+    Type elemTy = elems[0].getType();\n     Type structTy = LLVM::LLVMStructType::getLiteral(\n-        ctx, SmallVector<Type>(elems.size(), fp16x2Ty));\n+        ctx, SmallVector<Type>(elems.size(), elemTy));\n     auto result = getStructFromElements(loc, elems, rewriter, structTy);\n     return result;\n   }\n@@ -3790,9 +4077,25 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n       dotOperandLayout.getParent().dyn_cast_or_null<MmaEncodingAttr>();\n   assert(mmaLayout);\n \n+  bool isOuter{};\n+  {\n+    int K{};\n+    if (dotOperandLayout.getOpIdx() == 0) // $a\n+      K = dstTensorTy.getShape()[1];\n+    else // $b\n+      K = dstTensorTy.getShape()[0];\n+    isOuter = K == 1;\n+  }\n+\n+  // TODO[Superjomn]: the allowTF32 is not available in ConvertLayoutOp for it\n+  // is an attribute of DotOp.\n+  bool allowTF32 = false;\n+  bool isHMMA = DotOpConversion::isDotHMMA(dstTensorTy, allowTF32,\n+                                           mmaLayout.getVersion());\n+\n   auto smemObj = getSharedMemoryObjectFromStruct(loc, adaptor.src(), rewriter);\n   Value res;\n-  if (mmaLayout.getVersion() == 2) {\n+  if (!isOuter && mmaLayout.getVersion() == 2 && isHMMA) { // tensor core v2\n     MMA16816ConversionHelper mmaHelper(mmaLayout, getThreadId(rewriter, loc),\n                                        rewriter, getTypeConverter(),\n                                        op.getLoc());\n@@ -3804,7 +4107,8 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n       // operand $b\n       res = mmaHelper.loadB(src, smemObj);\n     }\n-  } else if (mmaLayout.getVersion() == 1) {\n+  } else if (!isOuter && mmaLayout.getVersion() == 1 &&\n+             isHMMA) { // tensor core v1\n     DotOpMmaV1ConversionHelper helper(mmaLayout);\n     if (dotOperandLayout.getOpIdx() == 0) {\n       // operand $a\n@@ -4290,6 +4594,155 @@ DotOpMmaV1ConversionHelper::extractLoadedOperand(\n   return rcds;\n }\n \n+LogicalResult\n+DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n+                               ConversionPatternRewriter &rewriter) const {\n+  auto *ctx = rewriter.getContext();\n+  auto loc = op.getLoc();\n+  auto threadId = getThreadId(rewriter, loc);\n+\n+  using ValueTable = std::map<std::pair<int, int>, Value>;\n+\n+  auto A = op.a();\n+  auto B = op.b();\n+  auto C = op.c();\n+  auto D = op.getResult();\n+\n+  auto aTensorTy = A.getType().cast<RankedTensorType>();\n+  auto bTensorTy = B.getType().cast<RankedTensorType>();\n+  auto cTensorTy = C.getType().cast<RankedTensorType>();\n+  auto dTensorTy = D.getType().cast<RankedTensorType>();\n+\n+  auto aShape = aTensorTy.getShape();\n+  auto bShape = bTensorTy.getShape();\n+  auto cShape = cTensorTy.getShape();\n+\n+  auto aLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto cLayout = cTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto dLayout = dTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n+\n+  auto aOrder = aLayout.getOrder();\n+  auto bOrder = bLayout.getOrder();\n+\n+  auto order = dLayout.getOrder();\n+\n+  bool isARow = aOrder[0] == 1;\n+  bool isBRow = bOrder[0] == 1;\n+\n+  int strideAM = isARow ? aShape[1] : 1;\n+  int strideAK = isARow ? 1 : aShape[0];\n+  int strideBN = isBRow ? 1 : bShape[0];\n+  int strideBK = isBRow ? bShape[1] : 1;\n+  int strideA0 = isARow ? strideAK : strideAM;\n+  int strideA1 = isARow ? strideAM : strideAK;\n+  int strideB0 = isBRow ? strideBN : strideBK;\n+  int strideB1 = isBRow ? strideBK : strideBN;\n+  int lda = isARow ? strideAM : strideAK;\n+  int ldb = isBRow ? strideBK : strideBN;\n+  int aPerPhase = aLayout.getPerPhase();\n+  int aMaxPhase = aLayout.getMaxPhase();\n+  int bPerPhase = bLayout.getPerPhase();\n+  int bMaxPhase = bLayout.getMaxPhase();\n+  int aNumPtr = 8;\n+  int bNumPtr = 8;\n+  int NK = aShape[1];\n+\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+\n+  auto sizePerThread = getSizePerThread(dLayout);\n+\n+  Value _0 = i32_val(0);\n+\n+  Value mContig = i32_val(sizePerThread[order[1]]);\n+  Value nContig = i32_val(sizePerThread[order[0]]);\n+\n+  // threadId in blocked layout\n+  SmallVector<Value> threadIds;\n+  {\n+    int dim = cShape.size();\n+    threadIds.resize(dim);\n+    for (unsigned k = 0; k < dim - 1; k++) {\n+      Value dimK = i32_val(shapePerCTA[order[k]]);\n+      Value rem = urem(threadId, dimK);\n+      threadId = udiv(threadId, dimK);\n+      threadIds[order[k]] = rem;\n+    }\n+    Value dimK = i32_val(shapePerCTA[order[dim - 1]]);\n+    threadIds[order[dim - 1]] = urem(threadId, dimK);\n+  }\n+\n+  Value threadIdM = threadIds[0];\n+  Value threadIdN = threadIds[1];\n+\n+  Value offA0 = isARow ? _0 : mul(threadIdM, mContig);\n+  Value offA1 = isARow ? mul(threadIdM, mContig) : _0;\n+  SmallVector<Value> aOff(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i) {\n+    aOff[i] = add(mul(offA0, i32_val(strideA0)), mul(offA1, i32_val(strideA1)));\n+  }\n+\n+  Value offB0 = isBRow ? mul(threadIdN, nContig) : _0;\n+  Value offB1 = isBRow ? _0 : mul(threadIdN, nContig);\n+  SmallVector<Value> bOff(bNumPtr);\n+  for (int i = 0; i < bNumPtr; ++i) {\n+    bOff[i] = add(mul(offB0, i32_val(strideB0)), mul(offB1, i32_val(strideB1)));\n+  }\n+\n+  auto aSmem = getSharedMemoryObjectFromStruct(loc, adaptor.a(), rewriter);\n+  auto bSmem = getSharedMemoryObjectFromStruct(loc, adaptor.b(), rewriter);\n+\n+  Type f32PtrTy = ptr_ty(f32_ty);\n+  SmallVector<Value> aPtrs(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i)\n+    aPtrs[i] = gep(f32PtrTy, aSmem.base, aOff[i]);\n+\n+  SmallVector<Value> bPtrs(bNumPtr);\n+  for (int i = 0; i < bNumPtr; ++i)\n+    bPtrs[i] = gep(f32PtrTy, bSmem.base, bOff[i]);\n+\n+  ValueTable has, hbs;\n+  auto cc = getElementsFromStruct(loc, adaptor.c(), rewriter);\n+  SmallVector<Value> ret = cc;\n+  // is this compatible with blocked layout?\n+\n+  for (unsigned k = 0; k < NK; k++) {\n+    int z = 0;\n+    for (unsigned i = 0; i < cShape[order[1]]; i += shapePerCTA[order[1]])\n+      for (unsigned j = 0; j < cShape[order[0]]; j += shapePerCTA[order[0]])\n+        for (unsigned ii = 0; ii < sizePerThread[order[1]]; ++ii)\n+          for (unsigned jj = 0; jj < sizePerThread[order[0]]; ++jj) {\n+            unsigned m = order[0] == 1 ? i : j;\n+            unsigned n = order[0] == 1 ? j : i;\n+            unsigned mm = order[0] == 1 ? ii : jj;\n+            unsigned nn = order[0] == 1 ? jj : ii;\n+            if (!has.count({m + mm, k})) {\n+              Value pa = gep(f32PtrTy, aPtrs[0],\n+                             i32_val((m + mm) * strideAM + k * strideAK));\n+              Value va = load(pa);\n+              has[{m + mm, k}] = va;\n+            }\n+            if (!hbs.count({n + nn, k})) {\n+              Value pb = gep(f32PtrTy, bPtrs[0],\n+                             i32_val((n + nn) * strideBN + k * strideBK));\n+              Value vb = load(pb);\n+              hbs[{n + nn, k}] = vb;\n+            }\n+\n+            ret[z] = rewriter.create<LLVM::FMulAddOp>(loc, has[{m + mm, k}],\n+                                                      hbs[{n + nn, k}], ret[z]);\n+            ++z;\n+          }\n+  }\n+\n+  auto res = getStructFromElements(\n+      loc, ret, rewriter,\n+      struct_ty(SmallVector<Type>(ret.size(), ret[0].getType())));\n+  rewriter.replaceOp(op, res);\n+\n+  return success();\n+}\n+\n /// ====================== mma codegen end ============================\n \n Value convertSplatLikeOpWithMmaLayout(const MmaEncodingAttr &layout,\n@@ -4559,9 +5012,9 @@ struct InsertSliceAsyncOpConversion\n     auto threadsPerCTA = getThreadsPerCTA(srcBlockedLayout);\n     auto inOrder = srcBlockedLayout.getOrder();\n \n-    // If perPhase * maxPhase > threadsPerCTA, we need to swizzle over\n-    // elements across phases. If perPhase * maxPhase <= threadsPerCTA,\n-    // swizzle is not allowd\n+    // If perPhase * maxPhase > threadsPerCTA, we will have elements\n+    // that share the same tile indices. The index calculation will\n+    // be cached.\n     auto numSwizzleRows = std::max<unsigned>(\n         (perPhase * maxPhase) / threadsPerCTA[inOrder[1]], 1);\n     // A sharedLayout encoding has a \"vec\" parameter.\n@@ -4570,7 +5023,7 @@ struct InsertSliceAsyncOpConversion\n     auto numVecCols = std::max<unsigned>(inVec / outVec, 1);\n \n     auto srcIndices = emitIndices(loc, rewriter, srcBlockedLayout, srcShape);\n-    // <<tileVecIdxRow, tileVecIdxCol>, TileOffset>\n+    //  <<tileVecIdxRow, tileVecIdxCol>, TileOffset>\n     DenseMap<std::pair<unsigned, unsigned>, Value> tileOffsetMap;\n     for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n       // minVec = 2, inVec = 4, outVec = 2\n@@ -4600,12 +5053,14 @@ struct InsertSliceAsyncOpConversion\n         // Example1:\n         // outVec = 2, inVec = 2, minVec = 2\n         // outVec = 2, inVec = 4, minVec = 2\n-        //     | [1 2] [3 4]  ... [15 16] |\n-        //     | [3 4] [5 6]  ... [1 2]   |\n+        //     | [1 2] [3 4] [5 6] ... |\n+        //     | [3 4] [1 2] [7 8] ... |\n+        //     | [5 6] [7 8] [1 2] ... |\n         // Example2:\n         // outVec = 4, inVec = 2, minVec = 2\n-        //     | [1 2 3 4] [5 6 7 8] ... [13 14 15 16] |\n-        //     | [5 6 7 8] [9 10 11 12] ... [1 2 3 4]  |\n+        //     | [1 2 3 4] [5 6 7 8] [9 10 11 12] ... |\n+        //     | [5 6 7 8] [1 2 3 4] [13 14 15 16] ... |\n+        //     | [9 10 11 12] [13 14 15 16] [1 2 3 4] ... |\n         auto srcIdx = srcIndices[tileVecIdxRow * sizePerThread[inOrder[0]]];\n         Value phase = urem(udiv(srcIdx[inOrder[1]], i32_val(perPhase)),\n                            i32_val(maxPhase));\n@@ -4761,190 +5216,6 @@ struct FDivOpConversion\n   }\n };\n \n-struct PrintfOpConversion\n-    : public ConvertTritonGPUOpToLLVMPattern<triton::PrintfOp> {\n-  using ConvertTritonGPUOpToLLVMPattern<\n-      triton::PrintfOp>::ConvertTritonGPUOpToLLVMPattern;\n-\n-  LogicalResult\n-  matchAndRewrite(triton::PrintfOp op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    auto loc = op->getLoc();\n-    SmallVector<Value, 16> operands;\n-    for (auto operand : adaptor.getOperands()) {\n-      auto sub_operands = this->getElementsFromStruct(loc, operand, rewriter);\n-      for (auto elem : sub_operands) {\n-        operands.push_back(elem);\n-      }\n-    }\n-    std::string formatStr;\n-    llvm::raw_string_ostream os(formatStr);\n-    os << op.prefix();\n-    if (operands.size() > 0) {\n-      os << getFormatSubstr(operands[0]);\n-    }\n-\n-    for (size_t i = 1; i < operands.size(); ++i) {\n-      os << \", \" << getFormatSubstr(operands[i]);\n-    }\n-    llPrintf(formatStr, operands, rewriter);\n-    rewriter.eraseOp(op);\n-    return success();\n-  }\n-  // get format specific for each input value\n-  // currently support pointer, i8, i16, i32, i64, f16, bf16, f32, f64\n-  std::string getFormatSubstr(Value value) const {\n-    Type type = value.getType();\n-    unsigned width = type.getIntOrFloatBitWidth();\n-\n-    if (type.isa<LLVM::LLVMPointerType>()) {\n-      return \"%p\";\n-    } else if (type.isBF16() || type.isF16() || type.isF32() || type.isF64()) {\n-      return \"%f\";\n-    } else if (type.isSignedInteger()) {\n-      return \"%i\";\n-    } else if (type.isUnsignedInteger() || type.isSignlessInteger()) {\n-      return \"%u\";\n-    }\n-    assert(false && \"not supported type\");\n-  }\n-\n-  // declare vprintf(i8*, i8*) as external function\n-  LLVM::LLVMFuncOp\n-  getVprintfDeclaration(ConversionPatternRewriter &rewriter) const {\n-    auto moduleOp =\n-        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n-    StringRef funcName(\"vprintf\");\n-    Operation *funcOp = moduleOp.lookupSymbol(funcName);\n-    if (funcOp)\n-      return cast<LLVM::LLVMFuncOp>(*funcOp);\n-\n-    auto *context = rewriter.getContext();\n-\n-    SmallVector<Type> argsType{ptr_ty(IntegerType::get(context, 8)),\n-                               ptr_ty(IntegerType::get(context, 8))};\n-    auto funcType = LLVM::LLVMFunctionType::get(i32_ty, argsType);\n-\n-    ConversionPatternRewriter::InsertionGuard guard(rewriter);\n-    rewriter.setInsertionPointToStart(moduleOp.getBody());\n-\n-    return rewriter.create<LLVM::LLVMFuncOp>(UnknownLoc::get(context), funcName,\n-                                             funcType);\n-  }\n-\n-  // extend integer to int32, extend float to float64\n-  // this comes from vprintf alignment requirements.\n-  std::pair<Type, Value> promoteValue(ConversionPatternRewriter &rewriter,\n-                                      Value value) const {\n-    auto *context = rewriter.getContext();\n-    auto type = value.getType();\n-    unsigned width = type.getIntOrFloatBitWidth();\n-    Value newOp = value;\n-    Type newType = type;\n-\n-    bool bUnsigned = type.isUnsignedInteger();\n-    if (type.isIntOrIndex() && width < 32) {\n-      if (bUnsigned) {\n-        newType = ui32_ty;\n-        newOp = rewriter.create<LLVM::ZExtOp>(UnknownLoc::get(context), newType,\n-                                              value);\n-      } else {\n-        newType = i32_ty;\n-        newOp = rewriter.create<LLVM::SExtOp>(UnknownLoc::get(context), newType,\n-                                              value);\n-      }\n-    } else if (type.isBF16() || type.isF16() || type.isF32()) {\n-      newType = f64_ty;\n-      newOp = rewriter.create<LLVM::FPExtOp>(UnknownLoc::get(context), newType,\n-                                             value);\n-    }\n-\n-    return {newType, newOp};\n-  }\n-\n-  void llPrintf(StringRef msg, ValueRange args,\n-                ConversionPatternRewriter &rewriter) const {\n-    static const char formatStringPrefix[] = \"printfFormat_\";\n-    assert(!msg.empty() && \"printf with empty string not support\");\n-    Type int8Ptr = ptr_ty(i8_ty);\n-\n-    auto *context = rewriter.getContext();\n-    auto moduleOp =\n-        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n-    auto funcOp = getVprintfDeclaration(rewriter);\n-\n-    Value one = rewriter.create<LLVM::ConstantOp>(\n-        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(1));\n-    Value zero = rewriter.create<LLVM::ConstantOp>(\n-        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(0));\n-\n-    unsigned stringNumber = 0;\n-    SmallString<16> stringConstName;\n-    do {\n-      stringConstName.clear();\n-      (formatStringPrefix + Twine(stringNumber++)).toStringRef(stringConstName);\n-    } while (moduleOp.lookupSymbol(stringConstName));\n-\n-    llvm::SmallString<64> formatString(msg);\n-    formatString.push_back('\\n');\n-    formatString.push_back('\\0');\n-    size_t formatStringSize = formatString.size_in_bytes();\n-    auto globalType = LLVM::LLVMArrayType::get(i8_ty, formatStringSize);\n-\n-    LLVM::GlobalOp global;\n-    {\n-      ConversionPatternRewriter::InsertionGuard guard(rewriter);\n-      rewriter.setInsertionPointToStart(moduleOp.getBody());\n-      global = rewriter.create<LLVM::GlobalOp>(\n-          UnknownLoc::get(context), globalType,\n-          /*isConstant=*/true, LLVM::Linkage::Internal, stringConstName,\n-          rewriter.getStringAttr(formatString));\n-    }\n-\n-    Value globalPtr =\n-        rewriter.create<LLVM::AddressOfOp>(UnknownLoc::get(context), global);\n-    Value stringStart =\n-        rewriter.create<LLVM::GEPOp>(UnknownLoc::get(context), int8Ptr,\n-                                     globalPtr, mlir::ValueRange({zero, zero}));\n-\n-    Value bufferPtr =\n-        rewriter.create<LLVM::NullOp>(UnknownLoc::get(context), int8Ptr);\n-\n-    SmallVector<Value, 16> newArgs;\n-    if (args.size() >= 1) {\n-      SmallVector<Type> argTypes;\n-      for (auto arg : args) {\n-        Type newType;\n-        Value newArg;\n-        std::tie(newType, newArg) = promoteValue(rewriter, arg);\n-        argTypes.push_back(newType);\n-        newArgs.push_back(newArg);\n-      }\n-\n-      Type structTy = LLVM::LLVMStructType::getLiteral(context, argTypes);\n-      auto allocated = rewriter.create<LLVM::AllocaOp>(UnknownLoc::get(context),\n-                                                       ptr_ty(structTy), one,\n-                                                       /*alignment=*/0);\n-\n-      for (const auto &entry : llvm::enumerate(newArgs)) {\n-        auto index = rewriter.create<LLVM::ConstantOp>(\n-            UnknownLoc::get(context), i32_ty,\n-            rewriter.getI32IntegerAttr(entry.index()));\n-        auto fieldPtr = rewriter.create<LLVM::GEPOp>(\n-            UnknownLoc::get(context), ptr_ty(argTypes[entry.index()]),\n-            allocated, ArrayRef<Value>{zero, index});\n-        rewriter.create<LLVM::StoreOp>(UnknownLoc::get(context), entry.value(),\n-                                       fieldPtr);\n-      }\n-      bufferPtr = rewriter.create<LLVM::BitcastOp>(UnknownLoc::get(context),\n-                                                   int8Ptr, allocated);\n-    }\n-\n-    ValueRange operands{stringStart, bufferPtr};\n-    rewriter.create<LLVM::CallOp>(UnknownLoc::get(context), funcOp, operands);\n-  }\n-};\n-\n void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                   RewritePatternSet &patterns, int numWarps,\n                                   AxisInfoAnalysis &axisInfoAnalysis,\n@@ -5149,6 +5420,15 @@ void ConvertTritonGPUToLLVM::initSharedMemory(\n \n namespace mlir {\n \n+namespace LLVM {\n+\n+void llPrintf(StringRef msg, ValueRange args,\n+              ConversionPatternRewriter &rewriter) {\n+  PrintfOpConversion::llPrintf(msg, args, rewriter);\n+}\n+\n+} // namespace LLVM\n+\n TritonLLVMConversionTarget::TritonLLVMConversionTarget(\n     MLIRContext &ctx, mlir::LLVMTypeConverter &typeConverter)\n     : ConversionTarget(ctx) {"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 7, "deletions": 4, "changes": 11, "file_content_changes": "@@ -144,10 +144,13 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n                       getThreadsPerWarp(parent)[d] * getWarpsPerCTA(parent)[d]);\n     }\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    assert(mmaLayout.getVersion() == 2 &&\n-           \"mmaLayout version = 1 is not implemented yet\");\n-    return {16 * mmaLayout.getWarpsPerCTA()[0],\n-            8 * mmaLayout.getWarpsPerCTA()[1]};\n+    if (mmaLayout.getVersion() == 2)\n+      return {16 * mmaLayout.getWarpsPerCTA()[0],\n+              8 * mmaLayout.getWarpsPerCTA()[1]};\n+    if (mmaLayout.getVersion() == 1)\n+      return {16 * mmaLayout.getWarpsPerCTA()[0],\n+              16 * mmaLayout.getWarpsPerCTA()[1]};\n+    assert(0 && \"Unexpected MMA layout version found\");\n   } else {\n     assert(0 && \"Unimplemented usage of getShapePerCTA\");\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 9, "deletions": 1, "changes": 10, "file_content_changes": "@@ -576,6 +576,14 @@ class BlockedToMMA : public mlir::RewritePattern {\n     auto oldRetType = dotOp.getResult().getType().cast<RankedTensorType>();\n     if (oldRetType.getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n       return failure();\n+\n+    auto A = dotOp.getOperand(0).getType().cast<RankedTensorType>();\n+    auto B = dotOp.getOperand(1).getType().cast<RankedTensorType>();\n+    // for FMA, should retain the blocked layout.\n+    if (A.getElementType().isF32() && B.getElementType().isF32() &&\n+        !dotOp.allowTF32())\n+      return failure();\n+\n     // get MMA encoding for the given number of warps\n     auto retShape = oldRetType.getShape();\n     auto mod = op->getParentOfType<mlir::ModuleOp>();\n@@ -627,4 +635,4 @@ class TritonGPUCombineOpsPass\n \n std::unique_ptr<Pass> mlir::createTritonGPUCombineOpsPass() {\n   return std::make_unique<TritonGPUCombineOpsPass>();\n-}\n\\ No newline at end of file\n+}"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 29, "deletions": 35, "changes": 64, "file_content_changes": "@@ -15,6 +15,14 @@ using namespace mlir;\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n \n+static Type getI1SameShape(Value v) {\n+  Type vType = v.getType();\n+  auto i1Type = IntegerType::get(vType.getContext(), 1);\n+  auto tensorType = vType.cast<RankedTensorType>();\n+  return RankedTensorType::get(tensorType.getShape(), i1Type,\n+                               tensorType.getEncoding());\n+}\n+\n namespace {\n class LoopPipeliner {\n   /// cache forOp we are working on\n@@ -262,13 +270,23 @@ void LoopPipeliner::emitPrologue() {\n           loadStageBuffer[op->getResult(0)] = {loadsBuffer[op->getResult(0)]};\n         }\n         // load => copy async\n-        // TODO: check if the hardware supports async copy\n         if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(op)) {\n+          Value mask = lookupOrDefault(loadOp.mask(), stage);\n+          Value newMask;\n+          if (mask) {\n+            Value splatCond = builder.create<triton::SplatOp>(\n+                mask.getLoc(), mask.getType(), loopCond);\n+            newMask =\n+                builder.create<arith::AndIOp>(mask.getLoc(), mask, splatCond);\n+          } else {\n+            newMask = builder.create<triton::SplatOp>(\n+                loopCond.getLoc(), getI1SameShape(loadOp), loopCond);\n+          }\n+          // TODO: check if the hardware supports async copy\n           newOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n               op->getLoc(), loadsBuffer[loadOp].getType(),\n               lookupOrDefault(loadOp.ptr(), stage),\n-              loadStageBuffer[loadOp][stage], pipelineIterIdx,\n-              lookupOrDefault(loadOp.mask(), stage),\n+              loadStageBuffer[loadOp][stage], pipelineIterIdx, newMask,\n               lookupOrDefault(loadOp.other(), stage), loadOp.cache(),\n               loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n           loadStageBuffer[loadOp].push_back(newOp->getResult(0));\n@@ -287,33 +305,6 @@ void LoopPipeliner::emitPrologue() {\n         }\n       }\n \n-      // If this is a load/async_copy, we need to update the mask\n-      if (Value mask = [&]() {\n-            if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(newOp)) {\n-              return loadOp.mask();\n-            } else if (auto insertSliceAsyncOp =\n-                           llvm::dyn_cast<triton::gpu::InsertSliceAsyncOp>(\n-                               newOp)) {\n-              return insertSliceAsyncOp.mask();\n-            } else {\n-              return mlir::Value();\n-            }\n-          }()) {\n-        // assert(I1 or TensorOf<[I1]>);\n-        OpBuilder::InsertionGuard g(builder);\n-        // TODO: move this out of the loop\n-        builder.setInsertionPoint(newOp);\n-        Value splatCond = builder.create<triton::SplatOp>(\n-            mask.getLoc(), mask.getType(), loopCond);\n-        Value newMask =\n-            builder.create<arith::AndIOp>(mask.getLoc(), mask, splatCond);\n-        // TODO: better way to do this?\n-        if (llvm::isa<triton::LoadOp>(newOp))\n-          newOp->setOperand(1, newMask);\n-        else // InsertSliceAsyncOp\n-          newOp->setOperand(3, newMask);\n-      }\n-\n       // update mapping of results\n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n         Value originalResult = op->getResult(dstIdx);\n@@ -332,7 +323,7 @@ void LoopPipeliner::emitPrologue() {\n                 newOp->getResult(dstIdx), stage + 1);\n         }\n       }\n-    }\n+    } // for (Operation *op : orderedDeps)\n \n     pipelineIterIdx = builder.create<arith::AddIOp>(\n         iv.getLoc(), pipelineIterIdx,\n@@ -490,26 +481,29 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n \n   for (Operation *op : orderedDeps) {\n     Operation *nextOp = nullptr;\n-    // TODO(da): does this work if loadOp has no mask?\n     // update loading mask\n     if (loads.contains(op->getResult(0))) {\n       auto loadOp = llvm::cast<triton::LoadOp>(op);\n       Value mask = loadOp.mask();\n+      Value newMask;\n       if (mask) {\n         Value splatCond = builder.create<triton::SplatOp>(\n             mask.getLoc(), mask.getType(), nextLoopCond);\n-        Value newMask = builder.create<arith::AndIOp>(\n+        newMask = builder.create<arith::AndIOp>(\n             mask.getLoc(), splatCond, nextMapping.lookupOrDefault(mask));\n         // if mask is defined outside the loop, don't update the map more than\n         // once\n         if (!(forOp.isDefinedOutsideOfLoop(mask) && nextMapping.contains(mask)))\n           nextMapping.map(mask, newMask);\n-      }\n+        newMask = nextMapping.lookupOrDefault(loadOp.mask());\n+      } else\n+        newMask = builder.create<triton::SplatOp>(\n+            loadOp.getLoc(), getI1SameShape(loadOp), nextLoopCond);\n       Value insertAsyncOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n           op->getLoc(), loadsBuffer[loadOp].getType(),\n           nextMapping.lookupOrDefault(loadOp.ptr()),\n           newForOp.getRegionIterArgs()[bufferIdx + nextBuffers.size()],\n-          insertSliceIndex, nextMapping.lookupOrDefault(loadOp.mask()),\n+          insertSliceIndex, newMask,\n           nextMapping.lookupOrDefault(loadOp.other()), loadOp.cache(),\n           loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n       nextBuffers.push_back(insertAsyncOp);"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 89, "deletions": 2, "changes": 91, "file_content_changes": "@@ -55,6 +55,33 @@ def test_gemm_no_scf(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n     assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)\n \n \n+@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS', [\n+    [64, 128, 128, 1],\n+    [128, 128, 128, 4],\n+    [16, 8, 32, 1],\n+    [32, 16, 64, 2],\n+    [32, 16, 64, 4],\n+])\n+def test_gemm_no_scf_int8(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n+    a = torch.randint(-5, 5, (SIZE_M, SIZE_K), device='cuda', dtype=torch.int8)\n+    b = torch.randint(-5, 5, (SIZE_K, SIZE_N), device='cuda', dtype=torch.int8)\n+    c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.int32)\n+\n+    grid = lambda META: (1, )\n+    matmul_no_scf_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n+                               stride_am=a.stride(0), stride_ak=a.stride(1),\n+                               stride_bk=b.stride(0), stride_bn=b.stride(1),\n+                               stride_cm=c.stride(0), stride_cn=c.stride(1),\n+                               M=SIZE_M, N=SIZE_N, K=SIZE_K,\n+                               num_warps=NUM_WARPS)\n+\n+    aa = a.cpu()\n+    bb = b.cpu()\n+    golden = torch.matmul(aa.float(), bb.float()).int()\n+    torch.set_printoptions(profile=\"full\")\n+    torch.testing.assert_close(c.cpu(), golden, check_dtype=False)\n+\n+\n @triton.jit\n def matmul_kernel(\n     a_ptr, b_ptr, c_ptr,\n@@ -80,8 +107,6 @@ def matmul_kernel(\n     c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n     tl.store(c_ptrs, accumulator)\n \n-# TODO: DotConversion in TritonGPUToLLVM cannot support non-splat C for the moment\n-\n \n def get_variant_golden(a, b):\n     SIZE_M = a.shape[0]\n@@ -144,3 +169,65 @@ def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLO\n \n     torch.set_printoptions(profile=\"full\")\n     assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err), check_dtype=False)\n+\n+\n+@pytest.mark.parametrize('M,N,K,num_warps,block_M,block_N,block_K', [\n+    [32, 32, 16, 4, 32, 32, 16],\n+    [32, 16, 16, 4, 32, 32, 16],\n+    [128, 8, 8, 4, 32, 32, 16],\n+    [127, 41, 43, 4, 32, 32, 16],\n+])\n+def test_gemm_fmadot(M, N, K, num_warps, block_M, block_N, block_K):\n+    @triton.jit\n+    def matmul_kernel(\n+        a_ptr, b_ptr, c_ptr,\n+        M, N, K,\n+        stride_am, stride_ak,\n+        stride_bk, stride_bn,\n+        stride_cm, stride_cn,\n+        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+    ):\n+        pid = tl.program_id(axis=0)\n+        # num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n+        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+        pid_m = pid // num_pid_n\n+        pid_n = pid % num_pid_n\n+\n+        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+        offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+        offs_k = tl.arange(0, BLOCK_SIZE_K)\n+        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n+        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n+\n+        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+        for k in range(0, K, BLOCK_SIZE_K):\n+            a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < K)\n+            b_mask = (offs_k[:, None] < K) & (offs_bn[None, :] < N)\n+            a = tl.load(a_ptrs, a_mask)\n+            b = tl.load(b_ptrs, b_mask)\n+            # NOTE the allow_tf32 should be false to force the dot op to do fmadot lowering\n+            accumulator += tl.dot(a, b, allow_tf32=False)\n+            a_ptrs += BLOCK_SIZE_K * stride_ak\n+            b_ptrs += BLOCK_SIZE_K * stride_bk\n+            offs_k += BLOCK_SIZE_K\n+\n+        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+        c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n+        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n+        tl.store(c_ptrs, accumulator, c_mask)\n+\n+    a = torch.randn((M, K), device='cuda', dtype=torch.float32)\n+    b = torch.randn((K, N), device='cuda', dtype=torch.float32)\n+    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n+\n+    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n+    matmul_kernel[grid](a, b, c,\n+                        M, N, K,\n+                        stride_am=a.stride(0), stride_ak=a.stride(1),\n+                        stride_bk=b.stride(0), stride_bn=b.stride(1),\n+                        stride_cm=c.stride(0), stride_cn=c.stride(1),\n+                        BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N, BLOCK_SIZE_K=block_K)\n+\n+    golden = torch.matmul(a, b)\n+    torch.testing.assert_close(c, golden)"}, {"filename": "python/tests/test_reduce.py", "status": "modified", "additions": 1, "deletions": 4, "changes": 5, "file_content_changes": "@@ -97,9 +97,7 @@ def test_reduce1d(op, dtype, shape):\n     (op, dtype, shape, axis)\n     for op in ['sum', 'min', 'max']\n     for dtype in dtypes\n-    for shape in [(1, 4), (1, 8), (1, 16), (1, 32), (2, 32), (4, 32)]\n-    # TODO: fix and uncomment\n-    #, (4, 128), (32, 64)]\n+    for shape in [(1, 4), (1, 8), (1, 16), (1, 32), (2, 32), (4, 32), (4, 128), (32, 64)]\n     for axis in [0, 1]\n ]\n \n@@ -128,7 +126,6 @@ def test_reduce2d(op, dtype, shape, axis):\n         golden_z = torch.min(x, dim=axis, keepdim=False)[0].to(reduced_dtype)\n     else:\n         golden_z = torch.max(x, dim=axis, keepdim=False)[0].to(reduced_dtype)\n-\n     if dtype.is_floating_point and op == 'sum':\n         if shape[axis] >= 256:\n             assert_close(z, golden_z, rtol=0.05, atol=0.1)"}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -156,7 +156,7 @@\n \n @triton.autotune(\n     configs=[\n-        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n     ],\n     key=['M', 'N', 'K'],\n )"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 20, "deletions": 1, "changes": 21, "file_content_changes": "@@ -347,7 +347,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_extract_slice\n   func @basic_extract_slice() {\n     // CHECK: llvm.mlir.addressof @global_smem\n-    // CHECK: llvm.extractvalue \n+    // CHECK: llvm.extractvalue\n     // CHECK-NEXT: llvm.extractvalue\n     // CHECK-NEXT: llvm.extractvalue\n     // CHECK-NEXT: llvm.extractvalue\n@@ -811,3 +811,22 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     return\n   }\n }\n+\n+// -----\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n+#mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  func @matmul_fmadot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  %a:tensor<32x16xf32, #shared>, %b:tensor<16x32xf32, #shared>) {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #blocked>\n+    // CHECK: llvm.intr.fmuladd\n+    %28 = tt.dot %a, %b, %cst {allowTF32 = false, transA = false, transB = false} : tensor<32x16xf32, #shared> * tensor<16x32xf32, #shared> -> tensor<32x32xf32, #blocked>\n+    %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n+    %36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x32x!tt.ptr<f32>, #blocked>\n+    tt.store %36, %28 : tensor<32x32xf32, #blocked>\n+    return\n+  }\n+}"}, {"filename": "test/Target/tritongpu_to_llvmir.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: python3 -m triton.tools.aot %s --target=llvm-ir | FileCheck %s\n+// RUN: %PYTHON -m triton.tools.aot %s --target=llvm-ir | FileCheck %s\n \n // == LLVM IR check begin ==\n // CHECK-LABEL: ; ModuleID = 'LLVMDialectModule'"}, {"filename": "test/Target/tritongpu_to_ptx.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: python3 -m triton.tools.aot %s --target=ptx --sm=80 --ptx-version=63 | FileCheck %s\n+// RUN: %PYTHON -m triton.tools.aot %s --target=ptx --sm=80 --ptx-version=63 | FileCheck %s\n // CHECK-LABEL: // Generated by LLVM NVPTX Back-End\n // CHECK: .version 6.3\n // CHECK: .target sm_80"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 12, "deletions": 5, "changes": 17, "file_content_changes": "@@ -13,12 +13,19 @@\n // CHECK-DAG: %[[CONSTANT_1:.*]] = arith.constant 1 : i32\n // CHECK-DAG: %[[CONSTANT_2:.*]] = arith.constant 2 : i32\n // CHECK-DAG: %[[CONSTANT_3:.*]] = arith.constant 3 : i32\n+// CHECK-DAG: %[[LOOP_COND_0:.*]] = arith.cmpi slt, %[[LB:.*]], %[[UB:.*]]\n // CHECK: %[[ABUFFER:.*]] = triton_gpu.alloc_tensor\n-// CHECK: %[[A0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]]\n+// CHECK-DAG: %[[LOOP_COND_0_SPLAT_A:.*]] = tt.splat %[[LOOP_COND_0]]\n+// CHECK: %[[A0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]], %[[LOOP_COND_0_SPLAT_A]]\n // CHECK: %[[BBUFFER:.*]] = triton_gpu.alloc_tensor\n-// CHECK: %[[B0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]]\n-// CHECK: %[[A1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n-// CHECK: %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n+// CHECK-DAG: %[[LOOP_COND_0_SPLAT_B:.*]] = tt.splat %[[LOOP_COND_0]]\n+// CHECK: %[[B0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]], %[[LOOP_COND_0_SPLAT_B]]\n+// CHECK-DAG: %[[IV_1:.*]] = arith.addi %[[LB]], %[[STEP:.*]]\n+// CHECK-DAG: %[[LOOP_COND_1:.*]] = arith.cmpi slt, %[[IV_1]], %[[UB]]\n+// CHECK-DAG: %[[LOOP_COND_1_SPLAT_A:.*]] = tt.splat %[[LOOP_COND_1]]\n+// CHECK: %[[A1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]], %[[LOOP_COND_1_SPLAT_A]]\n+// CHECK-DAG: %[[LOOP_COND_1_SPLAT_B:.*]] = tt.splat %[[LOOP_COND_1]]\n+// CHECK: %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]], %[[LOOP_COND_1_SPLAT_B]]\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n // CHECK: %[[A0:.*]] = tensor.extract_slice %[[A1BUFFER]][0, 0, 0]\n // CHECK: %[[B0:.*]] = tensor.extract_slice %[[B1BUFFER]][0, 0, 0]\n@@ -49,7 +56,7 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n   %b_off = arith.constant dense<4> : tensor<32x128xi32, #BL>\n \n   scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n-    %a_ = tt.load %a_ptr, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n+    %a_ = tt.load %a_ptr {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n     %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n     %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n     %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/PtxAsmFormatTest.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -112,8 +112,8 @@ TEST_F(PtxAsmFormatTest, MultiLinePTX) {\n   mov(valVal1, constVal);\n   mov(valVal1, valVal0);\n \n-  EXPECT_EQ(builder.dump(), \"mov $0, 0x1;\\r\\n\"\n-                            \"mov $1, 0x1;\\r\\n\"\n+  EXPECT_EQ(builder.dump(), \"mov $0, 0x1;\\n\\t\"\n+                            \"mov $1, 0x1;\\n\\t\"\n                             \"mov $1, $0;\");\n \n   auto values = builder.getAllMLIRArgs();"}]