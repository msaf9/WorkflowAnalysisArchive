[{"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "@@ -132,6 +132,7 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n           AxisInfo::DimVectorT(ty.getShape().begin(), ty.getShape().end()));\n     }\n   }\n+  // TODO: refactor & complete binary ops\n   // Addition\n   if (llvm::isa<arith::AddIOp, triton::AddPtrOp>(op)) {\n     auto newContiguity = [&](AxisInfo lhs, AxisInfo rhs, int d) {\n@@ -159,6 +160,20 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n     curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n                          newContiguity, newDivisibility, newConstancy);\n   }\n+  // Remainder\n+  if (llvm::isa<arith::RemSIOp, arith::RemUIOp>(op)) {\n+    auto newContiguity = [](AxisInfo lhs, AxisInfo rhs, int d) {\n+      return gcd(lhs.getContiguity(d), rhs.getDivisibility(d));\n+    };\n+    auto newDivisibility = [](AxisInfo lhs, AxisInfo rhs, int d) {\n+      return gcd(lhs.getDivisibility(d), rhs.getDivisibility(d));\n+    };\n+    auto newConstancy = [](AxisInfo lhs, AxisInfo rhs, int d) {\n+      return gcd(lhs.getConstancy(d), rhs.getConstancy(d));\n+    };\n+    curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n+                         newContiguity, newDivisibility, newConstancy);\n+  }\n   // TODO: All other binary ops\n   if (llvm::isa<arith::AndIOp, arith::OrIOp>(op)) {\n     auto newContiguity = [](AxisInfo lhs, AxisInfo rhs, int d) { return 1; };"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotHelpers.h", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -973,8 +973,9 @@ struct MMA16816ConversionHelper {\n     if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n       Value warpM = getWarpM(shape[0]);\n       // load from smem\n+      int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[0], shape[0] / matShapeM);\n       loadFn = getLoadMatrixFn(\n-          tensor, smemObj, mmaLayout, mmaLayout.getWarpsPerCTA()[0] /*wpt*/,\n+          tensor, smemObj, mmaLayout, wpt /*wpt*/,\n           1 /*kOrder*/, {mmaInstrM, mmaInstrK} /*instrShape*/,\n           {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, ha /*vals*/,\n           true /*isA*/);\n@@ -1016,8 +1017,9 @@ struct MMA16816ConversionHelper {\n     int numRepN = getNumRepN(tensorTy, shape[1]);\n \n     Value warpN = getWarpN(shape[1]);\n+    int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[1], shape[1] / matShapeN);\n     auto loadFn = getLoadMatrixFn(\n-        tensor, smemObj, mmaLayout, mmaLayout.getWarpsPerCTA()[1] /*wpt*/,\n+        tensor, smemObj, mmaLayout,  wpt /*wpt*/,\n         0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShape*/,\n         {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/,\n         false /*isA*/);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -2074,7 +2074,7 @@ struct GetNumProgramsOpConversion\n     Location loc = op->getLoc();\n     assert(op.axis() < 3);\n \n-    Value blockId = rewriter.create<::mlir::gpu::BlockDimOp>(\n+    Value blockId = rewriter.create<::mlir::gpu::GridDimOp>(\n         loc, rewriter.getIndexType(), dims[op.axis()]);\n     auto llvmIndexTy = getTypeConverter()->getIndexType();\n     rewriter.replaceOpWithNewOp<UnrealizedConversionCastOp>("}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "file_content_changes": "@@ -187,6 +187,7 @@ void init_triton_ir(py::module &&m) {\n                /* issue a warning */\n              }\n            })\n+      .def(\"get_context\", &mlir::Value::getContext)\n       .def(\"replace_all_uses_with\",\n            [](mlir::Value &self, mlir::Value &newValue) {\n              self.replaceAllUsesWith(newValue);\n@@ -335,6 +336,16 @@ void init_triton_ir(py::module &&m) {\n         return funcs[0];\n       });\n \n+   m.def(\"make_attr\",\n+        [](const std::vector<int> &values, mlir::MLIRContext &context) {\n+          return mlir::DenseIntElementsAttr::get(\n+                     mlir::RankedTensorType::get(\n+                         {static_cast<int64_t>(values.size())},\n+                         mlir::IntegerType::get(&context, 32)),\n+                     values)\n+              .cast<mlir::Attribute>();\n+        });\n+\n   m.def(\n       \"parse_mlir_module\",\n       [](const std::string &inputFilename, mlir::MLIRContext &context) {"}, {"filename": "python/tests/test_blocksparse.py", "status": "added", "additions": 188, "deletions": 0, "changes": 188, "file_content_changes": "@@ -0,0 +1,188 @@\n+import pytest\n+import torch\n+\n+import triton\n+\n+# TODO: float32 fails\n+\n+@pytest.mark.parametrize(\"MODE\", [\"sdd\", \"dds\", \"dsd\"])\n+@pytest.mark.parametrize(\"TRANS_B\", [False, True])\n+@pytest.mark.parametrize(\"TRANS_A\", [False, True])\n+@pytest.mark.parametrize(\"BLOCK\", [16, 32, 64])\n+@pytest.mark.parametrize(\"DTYPE\", [torch.float16])\n+def test_matmul(MODE, TRANS_A, TRANS_B, BLOCK, DTYPE, Z=3, H=2, M=512, N=256, K=384):\n+    seed = 0\n+    torch.manual_seed(seed)\n+    is_sdd = MODE == \"sdd\"\n+    is_dsd = MODE == \"dsd\"\n+    is_dds = MODE == \"dds\"\n+    do_sparsify = lambda x: triton.testing.sparsify_tensor(x, layout, BLOCK)\n+    do_mask = lambda x: triton.testing.mask_tensor(x, layout, BLOCK)\n+    # create inputs\n+    # create op\n+    a_shape = (Z, H, K, M) if TRANS_A else (Z, H, M, K)\n+    b_shape = (Z, H, N, K) if TRANS_B else (Z, H, K, N)\n+    c_shape = (Z, H, M, N)\n+    shape = {\n+        \"sdd\": (M, N),\n+        \"dsd\": (a_shape[2], a_shape[3]),\n+        \"dds\": (b_shape[2], b_shape[3]),\n+    }[MODE]\n+    layout = torch.randint(2, (H, shape[0] // BLOCK, shape[1] // BLOCK))\n+    layout[1, 2, :] = 0\n+    layout[1, :, 1] = 0\n+    # create data\n+    a_ref, a_tri = triton.testing.make_pair(a_shape, alpha=.1, dtype=DTYPE)\n+    b_ref, b_tri = triton.testing.make_pair(b_shape, alpha=.1, dtype=DTYPE)\n+    dc_ref, dc_tri = triton.testing.make_pair(c_shape, dtype=DTYPE)\n+    # compute [torch]\n+    dc_ref = do_mask(dc_ref) if is_sdd else dc_ref\n+    a_ref = do_mask(a_ref) if is_dsd else a_ref\n+    b_ref = do_mask(b_ref) if is_dds else b_ref\n+    a_ref.requires_grad_().retain_grad()\n+    b_ref.requires_grad_().retain_grad()\n+    c_ref = torch.matmul(a_ref.transpose(2, 3) if TRANS_A else a_ref,\n+                         b_ref.transpose(2, 3) if TRANS_B else b_ref)\n+    c_ref.backward(dc_ref)\n+    c_ref = do_sparsify(c_ref) if is_sdd else c_ref\n+    da_ref = do_sparsify(a_ref.grad) if is_dsd else a_ref.grad\n+    db_ref = do_sparsify(b_ref.grad) if is_dds else b_ref.grad\n+    # triton result\n+    dc_tri = do_sparsify(dc_tri) if is_sdd else dc_tri\n+    a_tri = do_sparsify(a_tri) if is_dsd else a_tri\n+    b_tri = do_sparsify(b_tri) if is_dds else b_tri\n+    a_tri.requires_grad_().retain_grad()\n+    b_tri.requires_grad_().retain_grad()\n+    op = triton.ops.blocksparse.matmul(layout, BLOCK, MODE, trans_a=TRANS_A, trans_b=TRANS_B, device=\"cuda\")\n+    c_tri = triton.testing.catch_oor(lambda: op(a_tri, b_tri), pytest)\n+    triton.testing.catch_oor(lambda: c_tri.backward(dc_tri), pytest)\n+    da_tri = a_tri.grad\n+    db_tri = b_tri.grad\n+    # compare\n+    triton.testing.assert_almost_equal(c_ref, c_tri)\n+    triton.testing.assert_almost_equal(da_ref, da_tri)\n+    triton.testing.assert_almost_equal(db_ref, db_tri)\n+\n+\n+configs = [\n+    (16, 256),\n+    (32, 576),\n+    (64, 1871),\n+    (128, 2511),\n+]\n+\n+\n+@pytest.mark.parametrize(\"is_dense\", [False, True])\n+@pytest.mark.parametrize(\"BLOCK, WIDTH\", configs)\n+def test_softmax(BLOCK, WIDTH, is_dense, Z=2, H=2, is_causal=True, scale=0.4):\n+    # set seed\n+    torch.random.manual_seed(0)\n+    Z, H, M, N = 2, 3, WIDTH, WIDTH\n+    # initialize layout\n+    # make sure each row has at least one non-zero element\n+    layout = torch.randint(2, (H, M // BLOCK, N // BLOCK))\n+    if is_dense:\n+        layout[:] = 1\n+    else:\n+        layout[1, 2, :] = 0\n+        layout[1, :, 1] = 0\n+    # initialize data\n+    a_shape = (Z, H, M, N)\n+    a_ref, a_tri = triton.testing.make_pair(a_shape)\n+    dout_ref, dout_tri = triton.testing.make_pair(a_shape)\n+    # compute [torch]\n+    a_ref = triton.testing.mask_tensor(a_ref, layout, BLOCK, value=float(\"-inf\"))\n+    a_ref.retain_grad()\n+    at_mask = torch.ones((M, N), device=\"cuda\")\n+    if is_causal:\n+        at_mask = torch.tril(at_mask)\n+    M = at_mask[None, None, :, :] + torch.zeros_like(a_ref)\n+    a_ref[M == 0] = float(\"-inf\")\n+    out_ref = torch.softmax(a_ref * scale, -1)\n+    out_ref.backward(dout_ref)\n+    out_ref = triton.testing.sparsify_tensor(out_ref, layout, BLOCK)\n+    da_ref = triton.testing.sparsify_tensor(a_ref.grad, layout, BLOCK)\n+    # compute [triton]\n+    a_tri = triton.testing.sparsify_tensor(a_tri, layout, BLOCK)\n+    a_tri.retain_grad()\n+    dout_tri = triton.testing.sparsify_tensor(dout_tri, layout, BLOCK)\n+    op = triton.ops.blocksparse.softmax(layout, BLOCK, device=\"cuda\", is_dense=is_dense)\n+    out_tri = op(a_tri, scale=scale, is_causal=is_causal)\n+    out_tri.backward(dout_tri)\n+    da_tri = a_tri.grad\n+    # compare\n+    triton.testing.assert_almost_equal(out_tri, out_ref)\n+    triton.testing.assert_almost_equal(da_tri, da_ref)\n+\n+\n+@pytest.mark.parametrize(\"block\", [16, 32, 64])\n+@pytest.mark.parametrize(\"dtype\", [torch.float16])\n+def test_attention_fwd_bwd(\n+    block,\n+    dtype,\n+    input_scale=1.0,\n+    scale=1 / 8.0,\n+    n_ctx=256,\n+    batch_size=2,\n+    n_heads=2,\n+):\n+    # inputs\n+    qkv_shape = (batch_size, n_heads, n_ctx, 64)\n+    qkvs = [\n+        torch.nn.Parameter(input_scale * torch.randn(qkv_shape), requires_grad=True).to(dtype).cuda() for _ in range(3)\n+    ]\n+\n+    # Triton:\n+    n_blocks = n_ctx // block\n+    layout = torch.tril(torch.ones([n_heads, n_blocks, n_blocks], dtype=torch.long))\n+    query, key, value = [x.clone() for x in qkvs]\n+    query.retain_grad()\n+    key.retain_grad()\n+    value.retain_grad()\n+    attn_out = triton_attention(layout, block, query=query, key=key, value=value, scale=scale)\n+    # ad hoc loss\n+    loss = (attn_out ** 2).mean()\n+    loss.backward()\n+    grads = [query.grad, key.grad, value.grad]\n+\n+    # Torch version:\n+    torch_q, torch_k, torch_v = [x.clone() for x in qkvs]\n+    attn_mask = torch.ones([n_ctx, n_ctx], device=\"cuda\", dtype=dtype)\n+    attn_mask = torch.tril(attn_mask, diagonal=0)\n+    attn_mask = 1e6 * (-1 + (attn_mask.reshape((1, 1, n_ctx, n_ctx)).cuda()))\n+    torch_q.retain_grad()\n+    torch_k.retain_grad()\n+    torch_v.retain_grad()\n+    scores = scale * torch.einsum(\"bhsd,bhtd->bhst\", torch_q, torch_k)\n+    scores = scores + attn_mask\n+    probs = torch.softmax(scores, dim=-1)\n+    torch_attn_out = torch.einsum(\"bhst,bhtd->bhsd\", probs, torch_v)\n+    # ad hoc loss\n+    torch_loss = (torch_attn_out ** 2).mean()\n+    torch_loss.backward()\n+    torch_grads = [torch_q.grad, torch_k.grad, torch_v.grad]\n+\n+    # comparison\n+    # print(f\"Triton loss {loss} and torch loss {torch_loss}.  Also checking grads...\")\n+    triton.testing.assert_almost_equal(loss, torch_loss)\n+    for g1, g2 in zip(grads, torch_grads):\n+        triton.testing.assert_almost_equal(g1, g2)\n+\n+\n+@pytest.mark.parametrize(\"block\", [16, 32, 64])\n+def triton_attention(\n+    layout,\n+    block: int,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    scale: float,\n+):\n+    sparse_dot_sdd_nt = triton.ops.blocksparse.matmul(layout, block, \"sdd\", trans_a=False, trans_b=True, device=value.device)\n+    sparse_dot_dsd_nn = triton.ops.blocksparse.matmul(layout, block, \"dsd\", trans_a=False, trans_b=False, device=value.device)\n+    sparse_softmax = triton.ops.blocksparse.softmax(layout, block, device=value.device)\n+\n+    w = sparse_dot_sdd_nt(query, key)\n+    w = sparse_softmax(w, scale=scale, is_causal=True)\n+    a = sparse_dot_dsd_nn(w, value)\n+    return a"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -1515,7 +1515,7 @@ def _generate_src(self):\n            }\n         }\n \n-        #define CUDA_CHECK(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n+        #define CUDA_CHECK(ans) { gpuAssert((ans), __FILE__, __LINE__); if(PyErr_Occurred()) return NULL; }\n \n         static PyObject* loadBinary(PyObject* self, PyObject* args) {\n             const char* name;\n@@ -1530,7 +1530,6 @@ def _generate_src(self):\n             CUmodule mod;\n             int32_t n_regs = 0;\n             int32_t n_spills = 0;\n-            Py_BEGIN_ALLOW_THREADS;\n             // create driver handles\n             CUDA_CHECK(cuModuleLoadData(&mod, data));\n             CUDA_CHECK(cuModuleGetFunction(&fun, mod, name));\n@@ -1548,7 +1547,6 @@ def _generate_src(self):\n               CUDA_CHECK(cuFuncGetAttribute(&shared_static, CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES, fun));\n               CUDA_CHECK(cuFuncSetAttribute(fun, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES, shared_optin - shared_static));\n             }\n-            Py_END_ALLOW_THREADS;\n \n             if(PyErr_Occurred()) {\n               return NULL;"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -1117,16 +1117,16 @@ def sqrt(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n ##\n \n def multiple_of(x: tl.tensor, values: List[int]) -> tl.tensor:\n-    if len(x.shape) != len(values):\n-        raise ValueError(\"Shape of input to multiple_of does not match the length of values\")\n-    x.handle.multiple_of(values)\n-    return x\n-\n-\n+     if len(x.shape) != len(values):\n+         raise ValueError(\"Shape of input to multiple_of does not match the length of values\")\n+     x.handle.set_attr(\"tt.divisibility\", ir.make_attr(values, x.handle.get_context()))\n+     return x\n+ \n+ \n def max_contiguous(x: tl.tensor, values: List[int]) -> tl.tensor:\n     if len(x.shape) != len(values):\n         raise ValueError(\"Shape of input to max_contiguous does not match the length of values\")\n-    x.handle.max_contiguous(values)\n+    x.handle.set_attr(\"tt.contiguity\", ir.make_attr(values, x.handle.get_context()))\n     return x\n \n "}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -34,12 +34,12 @@ def sparsify_tensor(x, mask, block):\n     return ret\n \n \n-def make_pair(shape, device=\"cuda\", alpha=1e-2, beta=0., trans=False, data=None):\n+def make_pair(shape, device=\"cuda\", alpha=1e-2, beta=0., trans=False, data=None, dtype=torch.float32):\n     if data is None:\n-        data = torch.randn(shape, dtype=torch.float32, device=device)\n+        data = torch.randn(shape, dtype=torch.float32, requires_grad=True, device=device)\n     ref_ret = data\n     ref_ret = ref_ret * alpha + beta\n-    ref_ret = ref_ret.half().float()\n+    ref_ret = ref_ret.half().to(dtype)\n     if trans:\n         ref_ret = ref_ret.t().requires_grad_()\n     ref_ret = ref_ret.detach().requires_grad_()"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -921,9 +921,9 @@ func @test_get_program_id(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n func @test_get_num_program(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n-  // CHECK: nvvm.read.ptx.sreg.ntid.x\n-  // CHECK: nvvm.read.ptx.sreg.ntid.y\n-  // CHECK: nvvm.read.ptx.sreg.ntid.z\n+  // CHECK: nvvm.read.ptx.sreg.nctaid.x\n+  // CHECK: nvvm.read.ptx.sreg.nctaid.y\n+  // CHECK: nvvm.read.ptx.sreg.nctaid.z\n   %blockdimx = tt.get_num_programs {axis=0:i32} : i32\n   %blockdimy = tt.get_num_programs {axis=1:i32} : i32\n   %blockdimz = tt.get_num_programs {axis=2:i32} : i32"}]