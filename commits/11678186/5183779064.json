[{"filename": ".github/workflows/wheels.yml", "status": "modified", "additions": 23, "deletions": 7, "changes": 30, "file_content_changes": "@@ -1,24 +1,41 @@\n name: Wheels\n on:\n   workflow_dispatch:\n-  #schedule:\n-  #  - cron: \"0 0 * * *\"\n+  schedule:\n+    - cron: \"0 2 * * *\"\n \n jobs:\n \n   Build-Wheels:\n \n     runs-on: [self-hosted, V100]\n+    permissions:\n+      id-token: write\n+      contents: read\n \n     steps:\n \n       - name: Checkout\n         uses: actions/checkout@v2\n \n+      - name: Azure login\n+        uses: azure/login@v1\n+        with:\n+          client-id: ${{ secrets.AZURE_CLIENT_ID }}\n+          tenant-id: ${{ secrets.AZURE_TENANT_ID }}\n+          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n+\n+      - id: generate-token\n+        name: Generate token\n+        run: |\n+          AZ_TOKEN=$(az account get-access-token --query accessToken)\n+          echo \"::add-mask::$AZ_TOKEN\"\n+          echo \"access_token=$AZ_TOKEN\" >> \"$GITHUB_OUTPUT\"\n+\n       - name: Patch setup.py\n         run: |\n-          #sed -i 's/name\\=\\\"triton\\\"/name=\"triton-nightly\"/g' python/setup.py\n-          export LATEST_DATE=$(TZ=UTC0 git show --quiet --date='format-local:%Y%m%d' --format=\"%cd\")\n+          sed -i 's/name\\=\\\"triton\\\"/name=\"triton-nightly\"/g' python/setup.py\n+          export LATEST_DATE=$(TZ=UTC0 git show --quiet --date='format-local:%Y%m%d%H%M%S' --format=\"%cd\")\n           sed -i -r \"s/version\\=\\\"(.*)\\\"/version=\\\"\\1-dev\"$LATEST_DATE\"\\\"/g\" python/setup.py\n           echo \"\" >> python/setup.cfg\n           echo \"[build_ext]\" >> python/setup.cfg\n@@ -33,7 +50,6 @@ jobs:\n           export CIBW_BUILD=\"{cp,pp}3*-manylinux_x86_64 cp3*-musllinux_x86_64\"\n           python3 -m cibuildwheel python --output-dir wheelhouse\n \n-\n-      - name: Upload wheels to PyPI\n+      - name: Publish wheels to Azure DevOps\n         run: |\n-          python3 -m twine upload wheelhouse/* -u __token__ -p ${{ secrets.PYPY_API_TOKEN }}\n+          python3 -m twine upload -r Triton-Nightly -u TritonArtifactsSP -p ${{ steps.generate-token.outputs.access_token }} --config-file utils/nightly.pypirc --non-interactive --verbose wheelhouse/*"}, {"filename": "include/triton/Analysis/Allocation.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -125,7 +125,7 @@ class Allocation {\n     }\n   }\n \n-  /// Returns the size of the given buffer is a virtual buffer.\n+  /// Returns if the given buffer is a virtual buffer.\n   bool isVirtualBuffer(BufferId bufferId) const {\n     return bufferSet.at(bufferId).kind == BufferT::BufferKind::Virtual;\n   }"}, {"filename": "include/triton/Conversion/TritonToTritonGPU/Passes.td", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -20,7 +20,11 @@ def ConvertTritonToTritonGPU: Pass<\"convert-triton-to-tritongpu\", \"mlir::ModuleO\n    let options = [\n        Option<\"numWarps\", \"num-warps\",\n               \"int32_t\", /*default*/\"4\",\n-              \"number of warps\">\n+              \"number of warps\">,\n+\n+       Option<\"threadsPerWarp\", \"threads-per-warp\",\n+              \"int32_t\", /*default*/\"32\",\n+              \"number of threads per warp\">,\n    ];\n }\n "}, {"filename": "include/triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -12,12 +12,14 @@ namespace triton {\n \n constexpr static char AttrNumWarpsName[] = \"triton_gpu.num-warps\";\n \n+constexpr static char AttrNumThreadsPerWarp[] = \"triton_gpu.threads-per-warp\";\n+\n // Create the pass with numWarps passed from cl::opt.\n std::unique_ptr<OperationPass<ModuleOp>> createConvertTritonToTritonGPUPass();\n \n // Create the pass with numWarps set explicitly.\n std::unique_ptr<OperationPass<ModuleOp>>\n-createConvertTritonToTritonGPUPass(int numWarps);\n+createConvertTritonToTritonGPUPass(int numWarps, int threadsPerWarp = 32);\n \n } // namespace triton\n } // namespace mlir"}, {"filename": "include/triton/Dialect/Triton/IR/TritonAttrDefs.td", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "file_content_changes": "@@ -14,6 +14,17 @@ def TT_CacheModifierAttr : I32EnumAttr<\n     let cppNamespace = \"::mlir::triton\";\n }\n \n+def TT_MemSemanticAttr : I32EnumAttr<\n+    \"MemSemantic\", \"\",\n+    [\n+      I32EnumAttrCase<\"RELAXED\", 1, \"relaxed\">,\n+      I32EnumAttrCase<\"ACQUIRE\", 2, \"acquire\">,\n+      I32EnumAttrCase<\"RELEASE\", 3, \"release\">,\n+      I32EnumAttrCase<\"ACQUIRE_RELEASE\", 4, \"acq_rel\">,\n+    ]> {\n+    let cppNamespace = \"::mlir::triton\";\n+}\n+\n def TT_EvictionPolicyAttr : I32EnumAttr<\n     \"EvictionPolicy\", \"\",\n     ["}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -234,7 +234,8 @@ def TT_AtomicRMWOp : TT_Op<\"atomic_rmw\", [SameOperandsAndResultShape,\n     }];\n \n     let arguments = (ins TT_AtomicRMWAttr:$atomic_rmw_op, TT_PtrLike:$ptr,\n-                         TT_Type:$val, Optional<TT_BoolLike>:$mask);\n+                         TT_Type:$val, Optional<TT_BoolLike>:$mask,\n+                         TT_MemSemanticAttr:$sem);\n \n     let results = (outs TT_Type:$result);\n }\n@@ -255,7 +256,8 @@ def TT_AtomicCASOp : TT_Op<\"atomic_cas\", [MemoryEffects<[MemRead]>,\n         return $old\n     }];\n \n-    let arguments = (ins TT_PtrLike:$ptr, TT_Type:$cmp, TT_Type:$val);\n+    let arguments = (ins TT_PtrLike:$ptr, TT_Type:$cmp, TT_Type:$val,\n+                     TT_MemSemanticAttr:$sem);\n \n     let results = (outs TT_Type:$result);\n }\n@@ -444,7 +446,6 @@ class TT_ExternElementwiseOpBase<string mnemonic, list<Trait> traits = []> :\n     let assemblyFormat = \"operands attr-dict `:` functional-type(operands, $result)\";\n }\n \n-\n def TT_PureExternElementwiseOp : TT_ExternElementwiseOpBase<\"pure_extern_elementwise\", [Pure, Elementwise]> {\n     let summary = \"FFI for pure element-wise extern LLVM bitcode functions\";\n }"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 11, "deletions": 10, "changes": 21, "file_content_changes": "@@ -229,31 +229,32 @@ for\n     AttrBuilder<(ins \"ArrayRef<int64_t>\":$shape,\n                      \"ArrayRef<unsigned>\":$sizePerThread,\n                      \"ArrayRef<unsigned>\":$order,\n-                     \"unsigned\":$numWarps), [{\n+                     \"unsigned\":$numWarps,\n+                     \"unsigned\":$threadsPerWarp), [{\n       int rank = sizePerThread.size();\n-      unsigned remainingLanes = 32;\n-      unsigned remainingThreads = numWarps*32;\n+      unsigned remainingLanes = threadsPerWarp;\n+      unsigned remainingThreads = numWarps*threadsPerWarp;\n       unsigned remainingWarps = numWarps;\n       unsigned prevLanes = 1;\n       unsigned prevWarps = 1;\n-      SmallVector<unsigned, 4> threadsPerWarp(rank);\n+      SmallVector<unsigned, 4> rankedThreadsPerWarp(rank);\n       SmallVector<unsigned, 4> warpsPerCTA(rank);\n       for (int _dim = 0; _dim < rank - 1; ++_dim) {\n         int i = order[_dim];\n         unsigned threadsPerCTA = std::clamp<unsigned>(remainingThreads, 1, shape[i] / sizePerThread[i]);\n-        threadsPerWarp[i] = std::clamp<unsigned>(threadsPerCTA, 1, remainingLanes);\n-        warpsPerCTA[i] = std::clamp<unsigned>(threadsPerCTA / threadsPerWarp[i], 1, remainingWarps);\n+        rankedThreadsPerWarp[i] = std::clamp<unsigned>(threadsPerCTA, 1, remainingLanes);\n+        warpsPerCTA[i] = std::clamp<unsigned>(threadsPerCTA / rankedThreadsPerWarp[i], 1, remainingWarps);\n         remainingWarps /= warpsPerCTA[i];\n-        remainingLanes /= threadsPerWarp[i];\n+        remainingLanes /= rankedThreadsPerWarp[i];\n         remainingThreads /= threadsPerCTA;\n-        prevLanes *= threadsPerWarp[i];\n+        prevLanes *= rankedThreadsPerWarp[i];\n         prevWarps *= warpsPerCTA[i];\n       }\n       // Expand the last dimension to fill the remaining lanes and warps\n-      threadsPerWarp[order[rank-1]] = 32 / prevLanes;\n+      rankedThreadsPerWarp[order[rank-1]] = threadsPerWarp / prevLanes;\n       warpsPerCTA[order[rank-1]] = numWarps / prevWarps;\n \n-      return $_get(context, sizePerThread, threadsPerWarp, warpsPerCTA, order);\n+      return $_get(context, sizePerThread, rankedThreadsPerWarp, warpsPerCTA, order);\n \n     }]>\n   ];"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUDialect.td", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "file_content_changes": "@@ -29,6 +29,16 @@ def TritonGPU_Dialect : Dialect {\n             \"TritonGPU module should contain a triton_gpu.num-warps attribute\");\n       return numWarps.cast<IntegerAttr>().getInt();\n     }\n+\n+    static std::string getThreadsPerWarpAttrName() { return \"triton_gpu.threads-per-warp\"; }\n+    static int getThreadsPerWarp(ModuleOp mod) {\n+      Attribute threadsPerWarp = mod->getDiscardableAttr(\"triton_gpu.threads-per-warp\");\n+      if(!threadsPerWarp) {\n+        return 32;\n+      }\n+      return threadsPerWarp.cast<IntegerAttr>().getInt();\n+    }\n+\n   }];\n \n   let useDefaultAttributePrinterParser = 1;"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -13,12 +13,15 @@ namespace mlir {\n \n class TritonGPUTypeConverter : public TypeConverter {\n public:\n-  TritonGPUTypeConverter(MLIRContext *context, int numWarps);\n+  TritonGPUTypeConverter(MLIRContext *context, int numWarps,\n+                         int threadsPerWarp);\n   int getNumWarps() const { return numWarps; }\n+  int getThreadsPerWarp() const { return threadsPerWarp; }\n \n private:\n   MLIRContext *context;\n   int numWarps;\n+  int threadsPerWarp;\n };\n \n class TritonGPUConversionTarget : public ConversionTarget {"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -72,7 +72,9 @@ SmallVector<SmallVector<unsigned>> ReduceOpHelper::getScratchConfigsFast() {\n   /// shared memory block1:\n   auto mod = op->getParentOfType<ModuleOp>();\n   unsigned numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n-  smemShapes[1].push_back(numWarps * 32);\n+  unsigned threadsPerWarp =\n+      triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n+  smemShapes[1].push_back(numWarps * threadsPerWarp);\n \n   return smemShapes;\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 14, "deletions": 13, "changes": 27, "file_content_changes": "@@ -406,11 +406,6 @@ struct AtomicCASOpConversion\n                  : valueTy;\n     auto valueElemNBits = valueElemTy.getIntOrFloatBitWidth();\n     Value mask = getMask(valueTy, rewriter, loc);\n-    PTXBuilder ptxBuilderMemfence;\n-    auto memfence = ptxBuilderMemfence.create<PTXInstr>(\"membar\")->o(\"gl\");\n-    memfence();\n-    auto ASMReturnTy = void_ty(ctx);\n-    ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n \n     Value atomPtr = getSharedMemoryBase(loc, rewriter, op.getOperation());\n     atomPtr = bitcast(atomPtr, ptr_ty(valueElemTy, 3));\n@@ -424,7 +419,10 @@ struct AtomicCASOpConversion\n     auto *cmpOpr = ptxBuilderAtomicCAS.newOperand(casCmp, \"r\");\n     auto *valOpr = ptxBuilderAtomicCAS.newOperand(casVal, \"r\");\n     auto &atom = *ptxBuilderAtomicCAS.create<PTXInstr>(\"atom\");\n-    atom.global().o(\"cas\").o(\"b32\");\n+    std::string semStr;\n+    llvm::raw_string_ostream os(semStr);\n+    os << op.getSem();\n+    atom.global().o(semStr).o(\"cas\").o(\"b32\");\n     atom(dstOpr, ptrOpr, cmpOpr, valOpr).predicate(mask);\n     auto old = ptxBuilderAtomicCAS.launch(rewriter, loc, valueElemTy);\n     barrier();\n@@ -435,8 +433,8 @@ struct AtomicCASOpConversion\n     auto &st = *ptxBuilderStore.create<PTXInstr>(\"st\");\n     st.shared().o(\"b32\");\n     st(dstOprStore, valOprStore).predicate(mask);\n+    auto ASMReturnTy = void_ty(ctx);\n     ptxBuilderStore.launch(rewriter, loc, ASMReturnTy);\n-    ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n     barrier();\n     Value ret = load(atomPtr);\n     barrier();\n@@ -464,7 +462,7 @@ struct AtomicRMWOpConversion\n                   ConversionPatternRewriter &rewriter) const override {\n     auto loc = op.getLoc();\n     MLIRContext *ctx = rewriter.getContext();\n-\n+    //\n     auto atomicRmwAttr = op.getAtomicRmwOp();\n \n     Value val = op.getVal();\n@@ -565,7 +563,10 @@ struct AtomicRMWOpConversion\n       default:\n         return failure();\n       }\n-      atom.o(rmwOp).o(sTy);\n+      std::string semStr;\n+      llvm::raw_string_ostream os(semStr);\n+      os << op.getSem();\n+      atom.o(semStr).o(rmwOp).o(sTy);\n       if (tensorTy) {\n         atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n         auto retType = vec == 1 ? valueElemTy : vecTy;\n@@ -575,13 +576,13 @@ struct AtomicRMWOpConversion\n               vec == 1 ? ret : extract_element(valueElemTy, ret, i32_val(ii));\n         }\n       } else {\n-        PTXBuilder ptxBuilderMemfence;\n-        auto memfenc = ptxBuilderMemfence.create<PTXInstr>(\"membar\")->o(\"gl\");\n-        memfenc();\n         auto ASMReturnTy = void_ty(ctx);\n-        ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n         atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n         auto old = ptxBuilderAtomicRMW.launch(rewriter, loc, valueElemTy);\n+        if (op->user_begin() == op->user_end()) {\n+          rewriter.replaceOp(op, {old});\n+          return success();\n+        }\n         Value atomPtr = getSharedMemoryBase(loc, rewriter, op.getOperation());\n         atomPtr = bitcast(atomPtr, ptr_ty(valueElemTy, 3));\n         // Only threads with rmwMask = True store the result"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -303,9 +303,10 @@ class ConvertTritonGPUToLLVM\n     TritonGPUToLLVMTypeConverter typeConverter(context, option);\n     TritonLLVMConversionTarget target(*context, isROCM);\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+    int threadsPerWarp = triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n \n     // Preprocess\n-    decomposeMmaToDotOperand(mod, numWarps);\n+    decomposeMmaToDotOperand(mod, numWarps, threadsPerWarp);\n     decomposeBlockedToDotOperand(mod);\n     decomposeInsertSliceAsyncOp(mod);\n \n@@ -432,7 +433,8 @@ class ConvertTritonGPUToLLVM\n                                         allocation.getSharedMemorySize()));\n   }\n \n-  void decomposeMmaToDotOperand(ModuleOp mod, int numWarps) const {\n+  void decomposeMmaToDotOperand(ModuleOp mod, int numWarps,\n+                                int threadsPerWarp) const {\n     // Replace `mma -> dot_op` with `mma -> blocked -> dot_op`\n     // unless certain conditions are met\n     mod.walk([&](triton::gpu::ConvertLayoutOp cvtOp) -> void {\n@@ -448,7 +450,7 @@ class ConvertTritonGPUToLLVM\n             dstType.getShape(), dstType.getElementType(),\n             triton::gpu::BlockedEncodingAttr::get(\n                 mod.getContext(), srcType.getShape(), getSizePerThread(srcMma),\n-                getOrder(srcMma), numWarps));\n+                getOrder(srcMma), numWarps, threadsPerWarp));\n         auto tmp = builder.create<triton::gpu::ConvertLayoutOp>(\n             cvtOp.getLoc(), tmpType, cvtOp.getOperand());\n         auto newConvert = builder.create<triton::gpu::ConvertLayoutOp>("}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 19, "deletions": 9, "changes": 28, "file_content_changes": "@@ -254,15 +254,17 @@ struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n     auto origShape = origType.getShape();\n     auto typeConverter = getTypeConverter<TritonGPUTypeConverter>();\n     int numWarps = typeConverter->getNumWarps();\n+    int threadsPerWarp = typeConverter->getThreadsPerWarp();\n \n     SmallVector<unsigned> retSizePerThread = {1, 1};\n-    if (origShape[0] * origShape[1] / (numWarps * 32) >= 4)\n+    if (origShape[0] * origShape[1] / (numWarps * threadsPerWarp) >= 4)\n       retSizePerThread = {2, 2};\n-    if (origShape[0] * origShape[1] / (numWarps * 32) >= 16)\n+    if (origShape[0] * origShape[1] / (numWarps * threadsPerWarp) >= 16)\n       retSizePerThread = {4, 4};\n     SmallVector<unsigned> retOrder = {1, 0};\n     Attribute dEncoding = triton::gpu::BlockedEncodingAttr::get(\n-        getContext(), origShape, retSizePerThread, retOrder, numWarps);\n+        getContext(), origShape, retSizePerThread, retOrder, numWarps,\n+        threadsPerWarp);\n     RankedTensorType retType =\n         RankedTensorType::get(origShape, origType.getElementType(), dEncoding);\n     // a & b must be of smem layout\n@@ -391,7 +393,8 @@ struct TritonAtomicCASPattern\n                   ConversionPatternRewriter &rewriter) const override {\n     addNamedAttrs(rewriter.replaceOpWithNewOp<triton::AtomicCASOp>(\n                       op, typeConverter->convertType(op.getType()),\n-                      adaptor.getPtr(), adaptor.getCmp(), adaptor.getVal()),\n+                      adaptor.getPtr(), adaptor.getCmp(), adaptor.getVal(),\n+                      op.getSem()),\n                   adaptor.getAttributes());\n     return success();\n   }\n@@ -407,7 +410,7 @@ struct TritonAtomicRMWPattern\n     addNamedAttrs(rewriter.replaceOpWithNewOp<triton::AtomicRMWOp>(\n                       op, typeConverter->convertType(op.getType()),\n                       adaptor.getAtomicRmwOp(), adaptor.getPtr(),\n-                      adaptor.getVal(), adaptor.getMask()),\n+                      adaptor.getVal(), adaptor.getMask(), op.getSem()),\n                   adaptor.getAttributes());\n     return success();\n   }\n@@ -806,13 +809,16 @@ class ConvertTritonToTritonGPU\n public:\n   ConvertTritonToTritonGPU() = default;\n   // constructor with some parameters set explicitly.\n-  ConvertTritonToTritonGPU(int numWarps) { this->numWarps = numWarps; }\n+  ConvertTritonToTritonGPU(int numWarps, int threadsPerWarp) {\n+    this->numWarps = numWarps;\n+    this->threadsPerWarp = threadsPerWarp;\n+  }\n \n   void runOnOperation() override {\n     MLIRContext *context = &getContext();\n     ModuleOp mod = getOperation();\n     // type converter\n-    TritonGPUTypeConverter typeConverter(context, numWarps);\n+    TritonGPUTypeConverter typeConverter(context, numWarps, threadsPerWarp);\n     TritonGPUConversionTarget target(*context, typeConverter);\n     // rewrite patterns\n     RewritePatternSet patterns(context);\n@@ -835,6 +841,9 @@ class ConvertTritonToTritonGPU\n     mod->setAttr(\n         AttrNumWarpsName,\n         IntegerAttr::get(i32_ty, llvm::APInt(32, numWarps.getValue())));\n+    mod->setAttr(\n+        AttrNumThreadsPerWarp,\n+        IntegerAttr::get(i32_ty, llvm::APInt(32, threadsPerWarp.getValue())));\n \n     // update layouts\n     //  broadcast src => multicast, dst => broadcasted\n@@ -846,8 +855,9 @@ class ConvertTritonToTritonGPU\n } // namespace\n \n std::unique_ptr<OperationPass<ModuleOp>>\n-mlir::triton::createConvertTritonToTritonGPUPass(int numWarps) {\n-  return std::make_unique<::ConvertTritonToTritonGPU>(numWarps);\n+mlir::triton::createConvertTritonToTritonGPUPass(int numWarps,\n+                                                 int threadsPerWarp) {\n+  return std::make_unique<::ConvertTritonToTritonGPU>(numWarps, threadsPerWarp);\n }\n \n std::unique_ptr<OperationPass<ModuleOp>>"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "modified", "additions": 11, "deletions": 6, "changes": 17, "file_content_changes": "@@ -23,7 +23,7 @@ typedef DenseMap<Value, std::function<Type(Type)>> LayoutMap;\n \n struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n   Attribute getCoalescedEncoding(ModuleAxisInfoAnalysis &axisInfoAnalysis,\n-                                 Value ptr, int numWarps) {\n+                                 Value ptr, int numWarps, int threadsPerWarp) {\n     auto origType = ptr.getType().cast<RankedTensorType>();\n     // Get the shape of the tensor.\n     size_t rank = origType.getRank();\n@@ -46,7 +46,7 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n         }\n       }\n     int numElems = product(origType.getShape());\n-    int numThreads = numWarps * 32;\n+    int numThreads = numWarps * threadsPerWarp;\n     int numElemsPerThread = std::max(numElems / numThreads, 1);\n     // Thread tile size depends on memory alignment\n     SmallVector<unsigned, 4> sizePerThread(rank, 1);\n@@ -68,14 +68,16 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     std::iota(dims.begin(), dims.end(), 0);\n     // create encoding\n     Attribute encoding = triton::gpu::BlockedEncodingAttr::get(\n-        &getContext(), origType.getShape(), sizePerThread, order, numWarps);\n+        &getContext(), origType.getShape(), sizePerThread, order, numWarps,\n+        threadsPerWarp);\n     return encoding;\n   }\n \n   std::function<Type(Type)>\n   getTypeConverter(ModuleAxisInfoAnalysis &axisInfoAnalysis, Value ptr,\n-                   int numWarps) {\n-    Attribute encoding = getCoalescedEncoding(axisInfoAnalysis, ptr, numWarps);\n+                   int numWarps, int threadsPerWarp) {\n+    Attribute encoding =\n+        getCoalescedEncoding(axisInfoAnalysis, ptr, numWarps, threadsPerWarp);\n     return [encoding](Type _type) {\n       RankedTensorType type = _type.cast<RankedTensorType>();\n       return RankedTensorType::get(type.getShape(), type.getElementType(),\n@@ -148,7 +150,10 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n         return;\n       auto mod = curr->getParentOfType<ModuleOp>();\n       int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n-      auto convertType = getTypeConverter(axisInfoAnalysis, ptr, numWarps);\n+      int threadsPerWarp =\n+          triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n+      auto convertType =\n+          getTypeConverter(axisInfoAnalysis, ptr, numWarps, threadsPerWarp);\n       layoutMap[ptr] = convertType;\n     });\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 481, "deletions": 379, "changes": 860, "file_content_changes": "@@ -11,10 +11,68 @@\n #include \"llvm/ADT/MapVector.h\"\n \n //===----------------------------------------------------------------------===//\n+// This file implements software pipelining for loops. The implementation here\n+// is inspired by the pipeline pass in Triton (version 2.0) and SCF's\n+// LoopPipelining.\n //\n-// This file implements loop software pipelining\n-// The implementation here is inspired by the pipeline pass in Triton (-v2.0)\n-// and SCF's LoopPipelining.\n+// We divide the loop body into the following phases:\n+// a. Pre-load operations: for instance, index computation.\n+// b. Load operations: loading from global memory to shared memory.\n+// c. Compute operations: for instance, Triton dot.\n+// d. Post-load operations: for instance, index computation.\n+//\n+// To pipeline the loop, we need to:\n+// - Hoist the pipelinable load operations for the first numStages-1 iterations\n+// to the loop pre-header\n+// - Find all the dependencies of the load operations.\n+// - Rematerialize the dependencies for their values at the first numStage-1\n+// iterations\n+// - Assemble the loop body (numStage) and prefetch (numStage + 1).\n+//\n+// In the prologue, the sequence of operations is the same as the original loop\n+// body, following the (a) -> (b) -> (c) -> (d) order. In the loop body,\n+// however, we first execute the compute operations, then pre-load operations,\n+// post-load operations, and eventually the asynchronous load operations - in\n+// the (c) -> (a) -> (d) -> (b) order. This is used to better hide the latency\n+// of the load operations. Because of this, if post-load operations have direct\n+// dependencies on the load operations, we could repeat the post-load\n+// operations. More specifically, this occurs when:\n+// 1. Any load operand has an immediate dependency argument used at numStage-1.\n+// 2. The argument is first defined at numStage-2.\n+// To avoid the repeat, we peeled off post-load operations in the prologue that\n+// satisfy the above two conditions. See the example below for the definition of\n+// immediate and non-immediate dependencies.\n+// If we have a load that immediately depends on a block argument in the\n+// current iteration, it is an immediate dependency. Otherwise, it is a\n+// non-immediate dependency, which means the load depends on a block argument\n+// in the previous iterations.\n+// For example:\n+// scf.for (%arg0, %arg1, %arg2) {\n+//   %0 = load %arg0  <--- immediate dep, this address is initialized before\n+//   numStages-1.\n+//   %1 = load %arg1\n+//   %2 = add %1, %arg2\n+//   %3 = load %2  <--- non-immediate dep, %arg1 must be an\n+//   update-to-date value.\n+// }\n+//\n+// Our pipelining pass share some common characteristics with SCF's\n+// LoopPipelining. However, it is also noteworthy that our pipelining pass has\n+// the following characteristics different from SCF's LoopPipelining:\n+// 1. It can handle loop-carried dependencies of distance greater than 1.\n+// 2. It does not have a complicated epilogue but instead uses masking to handle\n+// boundary conditions.\n+// 3. Each operation/loop-carried argument cannot provide values to both\n+// immediate and non-immediate dependencies. Otherwise, we have to rematerialize\n+// the operation and arguments, which would likely increase register pressure.\n+// For example:\n+// scf.for (%arg0, %arg1, %arg2) {\n+//  %0 = load %arg0\n+//  %1 = load %arg1, %0  <--- %0 is both a post-load op at numStages-2 and a\n+//  pre-load op at numStages-1, so that we need two versions of %0.\n+//  %2 = add %0, %arg2\n+//  scf.yield %arg0, %2, %arg2\n+//  }\n //\n //===----------------------------------------------------------------------===//\n \n@@ -25,8 +83,12 @@ namespace ttg = triton::gpu;\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n \n-// pass named attrs (e.g., tt.contiguity) from Triton to Triton\n-static void addNamedAttrs(Operation *op, DictionaryAttr dictAttrs) {\n+#define int_attr(num) builder.getI64IntegerAttr(num)\n+\n+namespace {\n+\n+// Pass named attrs (e.g., tt.contiguity) from Triton to Triton\n+void addNamedAttrs(Operation *op, DictionaryAttr dictAttrs) {\n   NamedAttrList attrs = op->getDiscardableAttrs();\n   // Collect the attributes to propagate: the ones in dictAttrs and not yet on\n   // the operation.\n@@ -42,19 +104,13 @@ static void addNamedAttrs(Operation *op, DictionaryAttr dictAttrs) {\n   }\n }\n \n-#define int_attr(num) builder.getI64IntegerAttr(num)\n-\n-namespace {\n-\n class LoopPipeliner {\n-  /// Cache forOp we are working on\n+  /// Cache of ForOp and YieldOp related to this pipeliner.\n   scf::ForOp forOp;\n-\n-  /// Cache YieldOp for this forOp\n   scf::YieldOp yieldOp;\n \n   /// Loads to be pipelined\n-  SetVector<Value> loads;\n+  SetVector<Value> validLoads;\n   /// Smallest data-type for each load (used to optimize swizzle and\n   /// (create DotOpEncoding layout)\n   DenseMap<Value, Type> loadsSmallestType;\n@@ -68,74 +124,111 @@ class LoopPipeliner {\n   DenseMap<Value, SmallVector<Value>> loadStageBuffer;\n   /// load => after extract\n   DenseMap<Value, Value> loadsExtract;\n-  ///\n+\n+  /// Iterator values\n   Value pipelineIterIdx;\n-  ///\n   Value loopIterIdx;\n+  Value nextIV;\n \n-  /// Comments on numStages:\n-  ///   [0, numStages-1) are in the prologue\n-  ///   numStages-1 is appended after the loop body\n+  /// Yield values\n+  SmallVector<Value> nextBuffers;\n+  SmallVector<Value> extractSlices;\n+  SmallVector<Value> yieldValues;\n+\n+  /// The number of stages in the pipeline.\n+  /// Stages in the range of [0, numStages-1) are in the prologue.\n+  /// numStages-1 is appended after the loop body.\n   int numStages;\n \n+  /// Arg indicies\n+  size_t bufferIdx, loadIdx, depArgsBeginIdx, ivIndex;\n+  DenseMap<BlockArgument, size_t> depArgsIdx;\n+\n   /// value (in loop) => value at stage N\n   DenseMap<Value, SmallVector<Value>> valueMapping;\n+  /// loop iter arg => value\n+  DenseMap<BlockArgument, Value> depArgsMapping;\n+  /// forOp value => newForOp value\n+  IRMapping mapping;\n+  /// forOp value => prefetch value\n+  IRMapping nextMapping;\n \n-  /// For each argument, we need to record at which stage it is defined.\n-  /// If we have a load that immediately depends on a block argument in the\n-  /// current iteration, it is an immediate dependency. Otherwise, it is a\n-  /// non-immediate dependency, which means the load depends on a block argument\n-  /// in the previous iterations.\n-  /// For example:\n-  /// scf.for (%arg0, %arg1, %arg2) {\n-  ///   %0 = load %arg0  <--- immediate dep, this address is initialized before\n-  ///   numStages-1\n-  ///   %1 = load %arg1\n-  ///   %2 = add %1, %arg2\n-  ///   %3 = load %2  <--- non-immediate dep, %arg1 must be an update-to-date\n-  ///   value\n-  /// }\n-  /// Collect values that v depends on and are defined inside the loop\n-  LogicalResult collectDeps(Value v, int stage,\n-                            MapVector<Value, int> &depStage);\n-\n-  /// Associate each variable with a unique stage. If a variable is defined\n-  /// at multiple stages, we don't pipeline it.\n-  LogicalResult addDep(Value v, int stage, MapVector<Value, int> &depStage);\n-\n-  int getArgDefStage(Value v, int stage);\n-\n-  /// Block arguments that loads depend on\n-  MapVector<BlockArgument, int> depArgUseStage;\n-\n-  /// Block arguments that loads depend on (defined in the loop body)\n-  MapVector<BlockArgument, int> depArgDefStage;\n-\n-  /// Operations (inside the loop body) that loads depend on\n-  MapVector<Operation *, int> depOpDefStage;\n-\n-  /// Operations (inside the loop body) that loads depend on (defined in the\n-  /// loop body)\n-  SetVector<BlockArgument> immediateDepArgs;\n-\n-  /// Operations (inside the loop body) that loads depend on (defined in the\n-  /// previous iterations)\n-  SetVector<BlockArgument> nonImmediateDepArgs;\n+  /// Dependency ops by program order\n+  SmallVector<Operation *> orderedDeps;\n+\n+  /// arg => source operand defined stages\n+  DenseMap<BlockArgument, DenseSet<int>> immediateArgStages;\n+\n+  /// block arguments that loads depend on\n+  SetVector<BlockArgument> depArgs;\n+\n+  /// operation => source operand defined stages\n+  DenseMap<Operation *, DenseSet<int>> immediateOpStages;\n+\n+  /// operations that loads depend on\n+  SetVector<Operation *> depOps;\n+\n+  /// Collect all pipelinable ops\n+  LogicalResult collectOps(SetVector<Operation *> &ops);\n+\n+  /// Collect values that `v` depends on and are defined inside the loop\n+  void collectValueDep(Value v, int stage, SetVector<Value> &opDeps);\n+\n+  /// Collect all op dependencies\n+  void collectDeps(SetVector<Operation *> &ops,\n+                   MapVector<Operation *, SetVector<Value>> &opDeps);\n \n+  /// Check if none of the ops has valid uses\n+  LogicalResult checkOpUses(SetVector<Operation *> &ops);\n+\n+  /// Check if ops have dependencies that are not pipelinable\n+  void checkOpDeps(SetVector<Operation *> &ops);\n+\n+  void createBufferTypes();\n+\n+  void createOrderedDeps();\n+\n+  /// Return the stage at which `v` is defined prior to `stage`\n+  int getValueDefStage(Value v, int stage);\n+\n+  /// Map `origin` to `newValue` at `stage`\n   void setValueMapping(Value origin, Value newValue, int stage);\n \n+  /// Map `origin` to `newValue` at `stage` according to the association between\n+  /// yieldOp and forOp\n+  void setValueMappingYield(Value origin, Value newValue, int stage);\n+\n+  /// Map `origin` to `newValue` at the next stage according to the association\n+  /// between yieldOp and forOp\n+  void setValueMappingYield(scf::ForOp newForOp, Value origin, Value newValue);\n+\n+  /// Return the value mapped to `origin` at `stage`, if it exists.\n   Value lookupOrDefault(Value origin, int stage);\n \n+  /// Get the load mask for `loadOp`, given the mapped mask `mappedMask` (if\n+  /// exists) and the current iteration's `loopCond`.\n   Value getLoadMask(triton::LoadOp loadOp, Value mappedMask, Value loopCond,\n                     OpBuilder &builder);\n \n-  /// Returns a empty buffer of size <numStages, ...>\n-  ttg::AllocTensorOp allocateEmptyBuffer(Operation *op, OpBuilder &builder);\n+  /// Return an empty buffer of size <numStages, ...>\n+  ttg::AllocTensorOp allocateEmptyBuffer(triton::LoadOp loadOp,\n+                                         OpBuilder &builder);\n+\n+  /// Collect all args of the new loop\n+  SmallVector<Value> collectNewLoopArgs();\n+\n+  /// Clone the forOp and return the new forOp\n+  scf::ForOp cloneForOp(ArrayRef<Value> newLoopArgs, OpBuilder &builder);\n+\n+  /// Prefetch the next iteration for `newForOp`\n+  void prefetchNextIteration(scf::ForOp newForOp, OpBuilder &builder);\n+\n+  /// Assemble `newForOp`'s yield op\n+  void finalizeYield(scf::ForOp newForOp, OpBuilder &builder);\n \n public:\n   LoopPipeliner(scf::ForOp forOp, int numStages)\n       : forOp(forOp), numStages(numStages) {\n-    // cache yieldOp\n     yieldOp = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n   }\n \n@@ -154,100 +247,14 @@ class LoopPipeliner {\n   friend struct PipelinePass;\n };\n \n-// helpers\n-void LoopPipeliner::setValueMapping(Value origin, Value newValue, int stage) {\n-  if (valueMapping.find(origin) == valueMapping.end())\n-    valueMapping[origin] = SmallVector<Value>(numStages);\n-  valueMapping[origin][stage] = newValue;\n-}\n-\n-Value LoopPipeliner::lookupOrDefault(Value origin, int stage) {\n-  if (valueMapping.find(origin) == valueMapping.end())\n-    return origin;\n-  return valueMapping[origin][stage];\n-}\n-\n-LogicalResult LoopPipeliner::addDep(Value v, int stage,\n-                                    MapVector<Value, int> &depStage) {\n-  if (!depStage.contains(v)) {\n-    depStage.insert(std::make_pair(v, stage));\n-  } else if (depStage[v] != stage)\n-    return failure();\n-  return success();\n-}\n-\n-LogicalResult LoopPipeliner::collectDeps(Value v, int stage,\n-                                         MapVector<Value, int> &depStage) {\n-  // Loop-invariant value, skip\n-  if (v.getParentRegion() != &forOp.getLoopBody())\n-    return success();\n-\n-  // Since we only need to peel the loop numStages-1 times, don't worry about\n-  // depends that are too far away\n-  if (stage < 0)\n-    return success();\n-\n-  if (auto arg = v.dyn_cast<BlockArgument>()) {\n-    // Skip the first arg (loop induction variable)\n-    // Otherwise the op idx is arg.getArgNumber()-1\n-    if (arg.getArgNumber() > 0) {\n-      // If we've found the first definition of this arg, we're done, don't\n-      // recurse\n-      if (addDep(v, stage, depStage).succeeded())\n-        if (collectDeps(yieldOp->getOperand(arg.getArgNumber() - 1), stage - 1,\n-                        depStage)\n-                .failed())\n-          return failure();\n-    }\n-  } else { // value\n-    // An operation cannot be dependent on different stages\n-    if (addDep(v, stage, depStage).failed())\n-      return failure();\n-    for (Value op : v.getDefiningOp()->getOperands())\n-      if (collectDeps(op, stage, depStage).failed())\n-        return failure();\n-  }\n-  return success();\n-}\n-\n-int LoopPipeliner::getArgDefStage(Value v, int stage) {\n-  if (stage < 0)\n-    return -1;\n-  if (auto arg = v.dyn_cast<BlockArgument>()) {\n-    if (arg.getArgNumber() > 0) {\n-      return getArgDefStage(yieldOp->getOperand(arg.getArgNumber() - 1),\n-                            stage - 1);\n-    }\n-    llvm_unreachable(\"Loop induction variable should not be a dependency\");\n-  } else\n-    return stage;\n-}\n-\n-ttg::AllocTensorOp LoopPipeliner::allocateEmptyBuffer(Operation *op,\n-                                                      OpBuilder &builder) {\n-  // Allocate a buffer for each pipelined tensor\n-  // shape: e.g. (numStages==4), <32x64xbf16> -> <4x32x64xbf16>\n-  Value convertLayout = loadsMapping[op->getResult(0)];\n-  if (auto tensorType = convertLayout.getType().dyn_cast<RankedTensorType>()) {\n-    return builder.create<ttg::AllocTensorOp>(\n-        convertLayout.getLoc(), loadsBufferType[op->getResult(0)]);\n-  }\n-  llvm_unreachable(\"Async copy's return should be of RankedTensorType\");\n-}\n-\n-/// A load instruction can be pipelined if:\n-///   - the load doesn't depend on any other loads (after loop peeling)\n-///   - (?) this load is not a loop-invariant value (we should run LICM before\n-///                                                  this pass?)\n-LogicalResult LoopPipeliner::initialize() {\n-  Block *loop = forOp.getBody();\n+/// Collect loads to pipeline. Return success if we can pipeline this loop\n+LogicalResult LoopPipeliner::collectOps(SetVector<Operation *> &ops) {\n   ModuleOp moduleOp = forOp->getParentOfType<ModuleOp>();\n   ModuleAxisInfoAnalysis axisInfoAnalysis(moduleOp);\n \n   // We cannot use forOp.walk(...) here because we only want to visit the\n   // operations in the loop body block. Nested blocks are handled separately.\n-  SmallVector<triton::LoadOp, 2> validLoads;\n-  for (Operation &op : *loop)\n+  for (Operation &op : forOp)\n     if (auto loadOp = dyn_cast<triton::LoadOp>(&op)) {\n       auto ptr = loadOp.getPtr();\n       unsigned vec = axisInfoAnalysis.getPtrContiguity(ptr);\n@@ -264,97 +271,222 @@ LogicalResult LoopPipeliner::initialize() {\n       unsigned width = vec * ty.getIntOrFloatBitWidth();\n       // We do not pipeline all loads for the following reasons:\n       // 1. On nvidia GPUs, cp.async's cp-size can only be 4, 8 and 16.\n-      // 2. It's likely that pipling small load won't offer much performance\n+      // 2. It's likely that pipling small loads won't offer much performance\n       //    improvement and may even hurt performance by increasing register\n       //    pressure.\n       if (width >= 32)\n-        validLoads.push_back(loadOp);\n+        ops.insert(loadOp);\n+    }\n+\n+  if (ops.empty())\n+    return failure();\n+  else\n+    return success();\n+}\n+\n+void LoopPipeliner::collectValueDep(Value v, int stage,\n+                                    SetVector<Value> &deps) {\n+  // Loop-invariant value, skip\n+  if (v.getParentRegion() != &forOp.getLoopBody())\n+    return;\n+\n+  // Since we only need to peel the loop numStages-1 times, don't worry\n+  // about depends that are too far away\n+  if (stage < 0)\n+    return;\n+\n+  if (auto arg = v.dyn_cast<BlockArgument>()) {\n+    if (arg.getArgNumber() > 0) {\n+      deps.insert(v);\n+      collectValueDep(yieldOp->getOperand(arg.getArgNumber() - 1), stage - 1,\n+                      deps);\n+    }\n+  } else { // value\n+    deps.insert(v);\n+    for (Value op : v.getDefiningOp()->getOperands())\n+      collectValueDep(op, stage, deps);\n+  }\n+}\n+\n+void LoopPipeliner::collectDeps(\n+    SetVector<Operation *> &ops,\n+    MapVector<Operation *, SetVector<Value>> &valueDeps) {\n+  for (auto op : ops) {\n+    for (Value v : op->getOperands()) {\n+      SetVector<Value> deps;\n+      collectValueDep(v, numStages - 1, deps);\n+      valueDeps[op] = deps;\n+    }\n+  }\n+}\n+\n+LogicalResult LoopPipeliner::checkOpUses(SetVector<Operation *> &ops) {\n+  DenseSet<Operation *> invalidOps;\n+  // Collect all ops' dependencies\n+  MapVector<Operation *, SetVector<Value>> opDeps;\n+  collectDeps(ops, opDeps);\n+\n+  for (Operation *op : ops) {\n+    if (auto loadOp = dyn_cast<triton::LoadOp>(op)) {\n+      // Don't pipeline valid loads that depend on other valid loads\n+      // (Because if a valid load depends on another valid load, this load needs\n+      // to wait on the other load in the prologue, which is against the point\n+      // of the pipeline pass)\n+      bool isCandidate = true;\n+      for (Operation *other : ops)\n+        if (isa<triton::LoadOp>(other))\n+          if (opDeps[op].contains(other->getResult(0))) {\n+            isCandidate = false;\n+            break;\n+          }\n+      // We only pipeline loads that have one covert_layout (to dot_op) use\n+      // TODO: lift this constraint in the future\n+      if (isCandidate && loadOp.getResult().hasOneUse()) {\n+        isCandidate = false;\n+        Operation *use = *loadOp.getResult().getUsers().begin();\n+\n+        // Advance to the first conversion as long as the use resides in shared\n+        // memory and it has a single use itself\n+        while (use) {\n+          if (use->getNumResults() != 1 || !use->getResult(0).hasOneUse())\n+            break;\n+          auto tensorType =\n+              use->getResult(0).getType().dyn_cast<RankedTensorType>();\n+          if (!tensorType.getEncoding().isa<ttg::SharedEncodingAttr>())\n+            break;\n+          use = *use->getResult(0).getUsers().begin();\n+        }\n+\n+        if (auto convertLayout = llvm::dyn_cast<ttg::ConvertLayoutOp>(use))\n+          if (auto tensorType = convertLayout.getResult()\n+                                    .getType()\n+                                    .dyn_cast<RankedTensorType>())\n+            if (auto dotOpEnc = tensorType.getEncoding()\n+                                    .dyn_cast<ttg::DotOperandEncodingAttr>()) {\n+              isCandidate = true;\n+              loadsMapping[loadOp] = convertLayout;\n+            }\n+      } else\n+        isCandidate = false;\n+\n+      if (!isCandidate)\n+        invalidOps.insert(loadOp);\n+      else\n+        validLoads.insert(loadOp);\n     }\n+  }\n \n-  // Early stop: no need to continue if there is no load in the loop.\n-  if (validLoads.empty())\n+  for (Operation *op : invalidOps)\n+    ops.remove(op);\n+\n+  if (ops.empty())\n     return failure();\n+  else\n+    return success();\n+}\n \n-  // load => values that it depends on\n-  // Don't pipeline if any load's operands\n-  DenseMap<Value, SetVector<Value>> loadDeps;\n-  MapVector<Value, int> depStage;\n-  for (triton::LoadOp loadOp : validLoads) {\n-    for (Value op : loadOp->getOperands()) {\n-      MapVector<Value, int> operandDepStage;\n-      if (collectDeps(op, numStages - 1, operandDepStage).failed())\n-        return failure();\n-      for (auto [v, stage] : operandDepStage) {\n-        auto immedidate = operandDepStage.front().first.isa<BlockArgument>();\n-        if (v.isa<BlockArgument>()) {\n-          auto arg = v.cast<BlockArgument>();\n-          if (immedidate)\n-            immediateDepArgs.insert(arg);\n+void LoopPipeliner::checkOpDeps(SetVector<Operation *> &ops) {\n+  SetVector<BlockArgument> nonImmediateDepArgs;\n+  SetVector<Operation *> nonImmediateOps;\n+  for (Operation *op : ops) {\n+    for (Value v : op->getOperands()) {\n+      SetVector<Value> deps;\n+      collectValueDep(v, numStages - 1, deps);\n+      int defStage = getValueDefStage(v, numStages - 1);\n+      assert(defStage >= 0 &&\n+             \"newLoopArgs has null args without a define op. Consider either \"\n+             \"rewrite the loop to reduce cross iteration dependencies or \"\n+             \"increase the num_stages value.\");\n+      for (auto dep : deps) {\n+        auto immediate = deps.front().isa<BlockArgument>();\n+        if (auto arg = dyn_cast<BlockArgument>(dep)) {\n+          depArgs.insert(arg);\n+          if (immediate)\n+            immediateArgStages[arg].insert(defStage);\n           else\n             nonImmediateDepArgs.insert(arg);\n+        } else {\n+          depOps.insert(dep.getDefiningOp());\n+          if (immediate)\n+            immediateOpStages[dep.getDefiningOp()].insert(defStage);\n+          else\n+            nonImmediateOps.insert(dep.getDefiningOp());\n         }\n-        loadDeps[loadOp].insert(v);\n-        if (addDep(v, stage, depStage).failed())\n-          return failure();\n       }\n     }\n   }\n \n-  // Don't pipeline valid loads that depend on other valid loads\n-  // (Because if a valid load depends on another valid load, this load needs to\n-  // wait on the other load in the prologue, which is against the point of the\n-  // pipeline pass)\n-  for (triton::LoadOp loadOp : validLoads) {\n-    bool isCandidate = true;\n-    for (triton::LoadOp other : validLoads) {\n-      if (loadDeps[loadOp].contains(other)) {\n-        isCandidate = false;\n-        break;\n-      }\n-    }\n+  // XXX: We could remove the following constraints if we can rematerialize in\n+  // the loop.\n+  // Check if immediateDepArgs and nonImmediateDepArgs are disjoint.\n+  for (auto &[arg, stages] : immediateArgStages) {\n+    assert(stages.size() == 1 &&\n+           \"Triton doesn't support an argument provides values for \"\n+           \"immediate operands of loads from multiple stages. Consider \"\n+           \"removing post load instructions dependency on this argument.\");\n+    assert(!(nonImmediateDepArgs.contains(arg) &&\n+             stages.contains(numStages - 2)) &&\n+           \"Loop-carried arguments provide values for both immediate and \"\n+           \"non-immediate operands of loads. Please consider removing \"\n+           \"pre/post load instructions dependency on this argument.\");\n+  }\n \n-    // We only pipeline loads that have one covert_layout (to dot_op) use\n-    // TODO: lift this constraint in the future\n-    if (isCandidate && loadOp.getResult().hasOneUse()) {\n-      isCandidate = false;\n-      Operation *use = *loadOp.getResult().getUsers().begin();\n-\n-      // advance to the first conversion as long\n-      // as the use resides in shared memory and it has\n-      // a single use itself\n-      while (use) {\n-        if (use->getNumResults() != 1 || !use->getResult(0).hasOneUse())\n-          break;\n-        auto tensorType =\n-            use->getResult(0).getType().dyn_cast<RankedTensorType>();\n-        if (!tensorType.getEncoding().isa<ttg::SharedEncodingAttr>())\n-          break;\n-        use = *use->getResult(0).getUsers().begin();\n-      }\n+  // Check if immediateOps and nonImmediateOps are disjoint.\n+  for (auto &[op, stages] : immediateOpStages) {\n+    assert(stages.size() == 1 &&\n+           \"Triton doesn't support an operation provides values for \"\n+           \"immediate operands of loads from multiple stages. Consider \"\n+           \"removing post load instructions dependency on this argument.\");\n+    assert(!(nonImmediateOps.contains(op) && stages.contains(numStages - 2)) &&\n+           \"Operations provide values for both immediate and \"\n+           \"non-immediate operands of loads.  Please consider \"\n+           \"removing pre/post load instructions dependency on this \"\n+           \"operation.\");\n+  }\n+}\n \n-      auto convertLayout = llvm::dyn_cast<ttg::ConvertLayoutOp>(use);\n-      if (!convertLayout)\n-        continue;\n-      auto tensorType =\n-          convertLayout.getResult().getType().dyn_cast<RankedTensorType>();\n-      if (!tensorType)\n-        continue;\n-      auto dotOpEnc =\n-          tensorType.getEncoding().dyn_cast<ttg::DotOperandEncodingAttr>();\n-      if (!dotOpEnc)\n-        continue;\n-      isCandidate = true;\n-      loadsMapping[loadOp] = convertLayout;\n-    } else\n-      isCandidate = false;\n+// helpers\n+void LoopPipeliner::setValueMapping(Value origin, Value newValue, int stage) {\n+  if (valueMapping.find(origin) == valueMapping.end())\n+    valueMapping[origin] = SmallVector<Value>(numStages);\n+  valueMapping[origin][stage] = newValue;\n+}\n \n-    if (isCandidate)\n-      loads.insert(loadOp);\n+void LoopPipeliner::setValueMappingYield(Value origin, Value newValue,\n+                                         int stage) {\n+  for (OpOperand &operand : origin.getUses()) {\n+    if (operand.getOwner() == yieldOp) {\n+      auto yieldIdx = operand.getOperandNumber();\n+      auto value = forOp.getRegionIterArgs()[yieldIdx];\n+      setValueMapping(value, newValue, stage);\n+    }\n   }\n+}\n+\n+void LoopPipeliner::setValueMappingYield(scf::ForOp newForOp, Value origin,\n+                                         Value newValue) {\n+  for (OpOperand &operand : origin.getUses()) {\n+    if (operand.getOwner() == yieldOp) {\n+      auto yieldIdx = operand.getOperandNumber();\n+      auto depYieldIdx = depArgsIdx[forOp.getRegionIterArgs()[yieldIdx]];\n+      auto originArg = forOp.getRegionIterArgs()[yieldIdx];\n+      nextMapping.map(originArg, newValue);\n+      auto newArg = newForOp.getRegionIterArgs()[depYieldIdx];\n+      if (!depArgsMapping.contains(newArg))\n+        depArgsMapping[newArg] = newValue;\n+    }\n+  }\n+}\n \n-  // we need to find the smallest ocmmon dtype\n-  // since this determines the layout of `mma.sync` operands\n-  // in mixed-precision mode\n+Value LoopPipeliner::lookupOrDefault(Value origin, int stage) {\n+  if (valueMapping.find(origin) == valueMapping.end())\n+    return origin;\n+  return valueMapping[origin][stage];\n+}\n+\n+void LoopPipeliner::createBufferTypes() {\n+  // We need to find the smallest common dtype since this determines the layout\n+  // of `mma.sync` operands in mixed-precision mode\n   Type smallestType;\n   for (auto loadCvt : loadsMapping) {\n     auto loadOp = loadCvt.first;\n@@ -385,27 +517,59 @@ LogicalResult LoopPipeliner::initialize() {\n     loadsBufferType[loadOp] =\n         RankedTensorType::get(bufferShape, ty.getElementType(), sharedEnc);\n   }\n+}\n \n-  // We have some loads to pipeline\n-  if (!loads.empty()) {\n-    // Update depArgs & depOps\n-    for (auto [dep, stage] : depStage) {\n-      if (auto arg = dep.dyn_cast<BlockArgument>())\n-        depArgUseStage.insert({arg, stage});\n-      else\n-        depOpDefStage.insert({dep.getDefiningOp(), stage});\n-    }\n-    return success();\n+void LoopPipeliner::createOrderedDeps() {\n+  for (Operation &op : forOp.getLoopBody().front()) {\n+    if (depOps.contains(&op))\n+      orderedDeps.push_back(&op);\n+    else if (op.getNumResults() > 0 && validLoads.contains(op.getResult(0)))\n+      orderedDeps.push_back(&op);\n   }\n+  assert(depOps.size() + validLoads.size() == orderedDeps.size() &&\n+         \"depOps contains invalid values\");\n+}\n \n-  // Check if immedidateDepArgs and nonImmedidateDepArgs are disjoint\n-  // If yes, we cannot pipeline the loop for now\n-  for (BlockArgument arg : immediateDepArgs)\n-    if (nonImmediateDepArgs.contains(arg)) {\n-      return failure();\n-    }\n+int LoopPipeliner::getValueDefStage(Value v, int stage) {\n+  if (stage < 0)\n+    return -1;\n+  if (auto arg = v.dyn_cast<BlockArgument>()) {\n+    if (arg.getArgNumber() > 0)\n+      return getValueDefStage(yieldOp->getOperand(arg.getArgNumber() - 1),\n+                              stage - 1);\n+    llvm_unreachable(\"Loop induction variable should not be a dependency\");\n+  } else\n+    return stage;\n+}\n+\n+ttg::AllocTensorOp LoopPipeliner::allocateEmptyBuffer(triton::LoadOp loadOp,\n+                                                      OpBuilder &builder) {\n+  // Allocate a buffer for each pipelined tensor\n+  // shape: e.g. (numStages==4), <32x64xbf16> -> <4x32x64xbf16>\n+  Value convertLayout = loadsMapping[loadOp];\n+  if (auto tensorType = convertLayout.getType().dyn_cast<RankedTensorType>())\n+    return builder.create<ttg::AllocTensorOp>(convertLayout.getLoc(),\n+                                              loadsBufferType[loadOp]);\n+  llvm_unreachable(\"Async copy's return should be of RankedTensorType\");\n+}\n+\n+LogicalResult LoopPipeliner::initialize() {\n+  // All ops that maybe pipelined\n+  SetVector<Operation *> ops;\n+\n+  if (collectOps(ops).failed())\n+    return failure();\n+\n+  if (checkOpUses(ops).failed())\n+    return failure();\n+\n+  checkOpDeps(ops);\n+\n+  createBufferTypes();\n+\n+  createOrderedDeps();\n \n-  return failure();\n+  return success();\n }\n \n Value LoopPipeliner::getLoadMask(triton::LoadOp loadOp, Value mappedMask,\n@@ -450,24 +614,14 @@ void LoopPipeliner::emitPrologue() {\n     // Special handling for loop condition as there is no condition in ForOp\n     Value loopCond = builder.create<arith::CmpIOp>(\n         iv.getLoc(), arith::CmpIPredicate::slt, iv, forOp.getUpperBound());\n-\n-    // Rematerialize peeled values\n-    SmallVector<Operation *> orderedDeps;\n-    for (Operation &op : forOp.getLoopBody().front()) {\n-      if (depOpDefStage.contains(&op))\n-        orderedDeps.push_back(&op);\n-      else if (op.getNumResults() > 0 && loads.contains(op.getResult(0)))\n-        orderedDeps.push_back(&op);\n-    }\n-    assert(depOpDefStage.size() + loads.size() == orderedDeps.size() &&\n-           \"depOps contains invalid values\");\n     for (Operation *op : orderedDeps) {\n       Operation *newOp = nullptr;\n-      if (loads.contains(op->getResult(0))) {\n+      if (validLoads.contains(op->getResult(0))) {\n+        auto load = cast<triton::LoadOp>(op);\n         // Allocate empty buffer\n         if (stage == 0) {\n-          loadsBuffer[op->getResult(0)] = allocateEmptyBuffer(op, builder);\n-          loadStageBuffer[op->getResult(0)] = {loadsBuffer[op->getResult(0)]};\n+          loadsBuffer[load] = allocateEmptyBuffer(load, builder);\n+          loadStageBuffer[load] = {loadsBuffer[load]};\n         }\n         // load => copy async\n         if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(op)) {\n@@ -503,53 +657,37 @@ void LoopPipeliner::emitPrologue() {\n           auto it = valueMapping.find(op->getOperand(opIdx));\n           if (it != valueMapping.end()) {\n             Value v = it->second[stage];\n-            assert(v);\n+            assert(v && \"Value not found in valueMapping\");\n             newOp->setOperand(opIdx, v);\n           } // else, op at opIdx is a loop-invariant value\n         }\n       }\n \n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n         Value originResult = op->getResult(dstIdx);\n-        // copy_async will update the value of its only use\n-        if (loads.contains(originResult))\n+        if (validLoads.contains(originResult))\n           break;\n         setValueMapping(originResult, newOp->getResult(dstIdx), stage);\n-        // update mapping for loop-carried values (args)\n-        for (OpOperand &operand : yieldOp->getOpOperands()) {\n-          if (operand.get() == op->getResult(dstIdx)) {\n-            auto yieldIdx = operand.getOperandNumber();\n-            auto value = forOp.getRegionIterArgs()[yieldIdx];\n-            setValueMapping(value, newOp->getResult(dstIdx), stage + 1);\n-          }\n-        }\n+        // Update mapping for loop-carried values (args)\n+        setValueMappingYield(op->getResult(dstIdx), newOp->getResult(dstIdx),\n+                             stage + 1);\n       }\n     } // for (Operation *op : orderedDeps)\n \n     // Update pipeline index\n     pipelineIterIdx = builder.create<arith::AddIOp>(\n         iv.getLoc(), pipelineIterIdx,\n         builder.create<arith::ConstantIntOp>(iv.getLoc(), 1, 32));\n-\n     // Some values have not been used by any ops in the loop body\n-    for (BlockArgument arg : forOp.getRegionIterArgs()) {\n-      // Check if arg has a yieldOp use\n-      for (OpOperand &operand : arg.getUses()) {\n-        if (operand.getOwner() == yieldOp) {\n-          auto yieldIdx = operand.getOperandNumber();\n-          auto value = forOp.getRegionIterArgs()[yieldIdx];\n-          if (!valueMapping[value][stage + 1])\n-            setValueMapping(value, valueMapping[arg][stage], stage + 1);\n-        }\n-      }\n-    }\n+    for (BlockArgument arg : forOp.getRegionIterArgs())\n+      setValueMappingYield(arg, valueMapping[arg][stage], stage + 1);\n   } // for (int stage = 0; stage < numStages - 1; ++stage)\n \n   // async.wait & extract_slice\n-  builder.create<ttg::AsyncWaitOp>(loads[0].getLoc(),\n-                                   loads.size() * (numStages - 2));\n+  builder.create<ttg::AsyncWaitOp>(validLoads.front().getLoc(),\n+                                   validLoads.size() * (numStages - 2));\n   loopIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n-  for (Value loadOp : loads) {\n+  for (Value loadOp : validLoads) {\n     auto bufferType = loadStageBuffer[loadOp][numStages - 1]\n                           .getType()\n                           .cast<RankedTensorType>();\n@@ -568,7 +706,7 @@ void LoopPipeliner::emitPrologue() {\n     loadsExtract[loadOp] = extractSlice;\n   }\n   // Bump up loopIterIdx, this is used for getting the correct slice for the\n-  // *next* iteration\n+  // `next` iteration\n   loopIterIdx = builder.create<arith::AddIOp>(\n       loopIterIdx.getLoc(), loopIterIdx,\n       builder.create<arith::ConstantIntOp>(loopIterIdx.getLoc(), 1, 32));\n@@ -582,78 +720,74 @@ void LoopPipeliner::emitEpilogue() {\n   builder.create<ttg::AsyncWaitOp>(forOp.getLoc(), 0);\n }\n \n-scf::ForOp LoopPipeliner::createNewForOp() {\n-  OpBuilder builder(forOp);\n-\n+SmallVector<Value> LoopPipeliner::collectNewLoopArgs() {\n   // Order of new args:\n   //   (original args)\n   //   (insertSliceAsync buffer at stage numStages - 1) for each load\n   //   (extracted tensor) for each load\n-  //   (depArgs at stage numStages - 1):\n-  //   for each dep arg that is not an immediate block argument\n-  //   (depArgs at stage numStages - 2):\n-  //   for each dep arg that is an immediate block argument\n+  //   (depArgs at stage numStages - 1)\n+  //   (depArgs at stage numStages - 2)\n+  //   ...\n   //   (iv at stage numStages - 2)\n   //   (pipeline iteration index)\n   //   (loop iteration index)\n-  SmallVector<Value> newLoopArgs;\n+\n   // We need this to update operands for yield\n   // original block arg => new arg's idx\n-  DenseMap<BlockArgument, size_t> depArgsIdx;\n+  SmallVector<Value> newLoopArgs;\n   for (auto v : forOp.getIterOperands())\n     newLoopArgs.push_back(v);\n \n-  size_t bufferIdx = newLoopArgs.size();\n-  for (Value loadOp : loads)\n+  bufferIdx = newLoopArgs.size();\n+  for (auto loadOp : validLoads)\n     newLoopArgs.push_back(loadStageBuffer[loadOp].back());\n-  size_t loadIdx = newLoopArgs.size();\n-  for (Value loadOp : loads)\n+\n+  loadIdx = newLoopArgs.size();\n+  for (auto loadOp : validLoads)\n     newLoopArgs.push_back(loadsExtract[loadOp]);\n \n-  size_t depArgsBeginIdx = newLoopArgs.size();\n-  for (auto [depArg, useStage] : depArgUseStage) {\n+  depArgsBeginIdx = newLoopArgs.size();\n+  for (auto depArg : depArgs) {\n     depArgsIdx[depArg] = newLoopArgs.size();\n-    auto defStage = getArgDefStage(depArg, useStage);\n-    assert(defStage >= 0 &&\n-           \"newLoopArgs has null args without a define op. Consider either \"\n-           \"rewrite the loop to reduce cross iteration dependencies or \"\n-           \"increase the num_stages value.\");\n-    if (immediateDepArgs.contains(depArg) && defStage == numStages - 2) {\n+    if (immediateArgStages[depArg].contains(numStages - 2))\n+      // Peel off post load ops in numStage-1\n       newLoopArgs.push_back(valueMapping[depArg][numStages - 2]);\n-    } else\n+    else\n       newLoopArgs.push_back(valueMapping[depArg][numStages - 1]);\n   }\n \n-  size_t ivIndex = newLoopArgs.size();\n+  ivIndex = newLoopArgs.size();\n   newLoopArgs.push_back(valueMapping[forOp.getInductionVar()][numStages - 2]);\n   newLoopArgs.push_back(pipelineIterIdx);\n   newLoopArgs.push_back(loopIterIdx);\n+  return newLoopArgs;\n+}\n \n-  // 1. signature of the new ForOp\n+scf::ForOp LoopPipeliner::cloneForOp(ArrayRef<Value> newLoopArgs,\n+                                     OpBuilder &builder) {\n+  // Clone the original ForOp\n   auto newForOp = builder.create<scf::ForOp>(\n       forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n       forOp.getStep(), newLoopArgs);\n \n-  // 2. body of the new ForOp\n+  // Set mapping on body of the new ForOp\n   builder.setInsertionPointToStart(newForOp.getBody());\n-  IRMapping mapping;\n   for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n     mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n   mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n \n-  // 3. clone the loop body, replace original args with args of the new ForOp\n+  // Clone the loop body, replace original args with args of the new ForOp\n   // Insert async wait if necessary.\n-  DenseSet<Value> isModified;\n   for (Operation &op : forOp.getBody()->without_terminator()) {\n     // is modified\n-    auto it = std::find(loads.begin(), loads.end(), op.getOperand(0));\n-    if (it == loads.end()) {\n+    auto it = std::find(validLoads.begin(), validLoads.end(), op.getOperand(0));\n+    if (it == validLoads.end()) {\n       Operation *newOp = cloneWithInferType(builder, &op, mapping);\n       continue;\n     }\n \n     // we replace the use new load use with a convert layout\n-    size_t i = std::distance(loads.begin(), it);\n+    size_t i = std::distance(validLoads.begin(), it);\n     auto cvtDstTy = op.getResult(0).getType().cast<RankedTensorType>();\n     auto cvtDstEnc =\n         cvtDstTy.getEncoding().dyn_cast<ttg::DotOperandEncodingAttr>();\n@@ -670,23 +804,16 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n         op.getResult(0).getLoc(), newDstTy,\n         newForOp.getRegionIterArgs()[loadIdx + i]);\n     mapping.map(op.getResult(0), cvt.getResult());\n-    isModified.insert(op.getResult(0));\n   }\n \n-  // 4. prefetch the next iteration\n-  SmallVector<Operation *> orderedDeps;\n-  for (Operation &op : forOp.getLoopBody().front()) {\n-    if (depOpDefStage.contains(&op))\n-      orderedDeps.push_back(&op);\n-    else if (op.getNumResults() > 0 && loads.contains(op.getResult(0)))\n-      orderedDeps.push_back(&op);\n-  }\n-  assert(depOpDefStage.size() + loads.size() == orderedDeps.size() &&\n-         \"depOps contains invalid values\");\n-  IRMapping nextMapping;\n-  DenseMap<BlockArgument, Value> depArgsMapping;\n+  return newForOp;\n+}\n+\n+void LoopPipeliner::prefetchNextIteration(scf::ForOp newForOp,\n+                                          OpBuilder &builder) {\n+  // Map the dep args of the next iteration to the dep args of the current\n   size_t argIdx = 0;\n-  for (auto [depArg, useStage] : depArgUseStage) {\n+  for (auto depArg : depArgs) {\n     BlockArgument nextArg =\n         newForOp.getRegionIterArgs()[argIdx + depArgsBeginIdx];\n     nextMapping.map(depArg, nextArg);\n@@ -695,16 +822,12 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n \n   // Special handling for iv & loop condition\n   Value curIV = newForOp.getRegionIterArgs()[ivIndex];\n-  Value nextIV = builder.create<arith::AddIOp>(\n-      newForOp.getInductionVar().getLoc(), curIV, newForOp.getStep());\n+  nextIV = builder.create<arith::AddIOp>(newForOp.getInductionVar().getLoc(),\n+                                         curIV, newForOp.getStep());\n   Value nextLoopCond =\n       builder.create<arith::CmpIOp>(nextIV.getLoc(), arith::CmpIPredicate::slt,\n                                     nextIV, newForOp.getUpperBound());\n \n-  // Slice index\n-  SmallVector<Value> nextBuffers;\n-  SmallVector<Value> extractSlices;\n-\n   pipelineIterIdx = newForOp.getRegionIterArgs()[ivIndex + 1];\n   Value insertSliceIndex = builder.create<arith::RemSIOp>(\n       nextIV.getLoc(), pipelineIterIdx,\n@@ -716,8 +839,9 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n \n   // Prefetch load deps\n   for (Operation *op : orderedDeps)\n-    if (!loads.contains(op->getResult(0))) {\n-      if (depOpDefStage[op] == numStages - 2)\n+    if (!validLoads.contains(op->getResult(0))) {\n+      if (immediateOpStages[op].contains(numStages - 2))\n+        // A post load op that provides values for numStage - 2\n         nextMapping.map(forOp.getInductionVar(), curIV);\n       else\n         nextMapping.map(forOp.getInductionVar(), nextIV);\n@@ -734,30 +858,19 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n             loadOp.getCache(), loadOp.getEvict(), loadOp.getIsVolatile());\n         addNamedAttrs(nextOp, op->getDiscardableAttrDictionary());\n         nextMapping.map(loadOp.getResult(), nextOp->getResult(0));\n-      } else {\n+      } else\n         nextOp = builder.clone(*op, nextMapping);\n-      }\n \n-      for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n-        for (OpOperand &operand : yieldOp->getOpOperands()) {\n-          if (operand.get() == op->getResult(dstIdx)) {\n-            size_t yieldIdx = operand.getOperandNumber();\n-            size_t depYieldIdx =\n-                depArgsIdx[forOp.getRegionIterArgs()[yieldIdx]];\n-            BlockArgument newArg = newForOp.getRegionIterArgs()[depYieldIdx];\n-            nextMapping.map(forOp.getRegionIterArgs()[yieldIdx],\n-                            nextOp->getResult(dstIdx));\n-            depArgsMapping[newArg] = nextOp->getResult(dstIdx);\n-          }\n-        }\n-      }\n+      for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults()))\n+        setValueMappingYield(newForOp, op->getResult(dstIdx),\n+                             nextOp->getResult(dstIdx));\n     }\n \n   // loads -> async loads\n   for (Operation *op : orderedDeps) {\n     Operation *nextOp = nullptr;\n     // Update loading mask\n-    if (loads.contains(op->getResult(0))) {\n+    if (validLoads.contains(op->getResult(0))) {\n       auto loadOp = llvm::cast<triton::LoadOp>(op);\n       auto mask = loadOp.getMask();\n       auto newMask =\n@@ -779,7 +892,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n           loadOp.getEvict(), loadOp.getIsVolatile(), /*axis*/ 0);\n       builder.create<ttg::AsyncCommitGroupOp>(op->getLoc());\n       nextBuffers.push_back(insertAsyncOp);\n-      // ExtractSlice\n+      // Extract slice\n       auto bufferType = insertAsyncOp.getType().cast<RankedTensorType>();\n       auto bufferShape = bufferType.getShape();\n       auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n@@ -798,40 +911,21 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n       extractSlices.push_back(nextOp->getResult(0));\n \n       // Update mapping of results\n-      for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n-        nextMapping.map(op->getResult(dstIdx), nextOp->getResult(dstIdx));\n+      for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults()))\n         // If this is a loop-carried value, update the mapping for yield\n-        for (OpOperand &operand : yieldOp->getOpOperands()) {\n-          if (operand.get() == op->getResult(dstIdx)) {\n-            auto yieldIdx = operand.getOperandNumber();\n-            auto depYieldIdx = depArgsIdx[forOp.getRegionIterArgs()[yieldIdx]];\n-            auto newArg = newForOp.getRegionIterArgs()[depYieldIdx];\n-            depArgsMapping[newArg] = nextOp->getResult(dstIdx);\n-          }\n-        }\n-      }\n+        setValueMappingYield(newForOp, op->getResult(dstIdx),\n+                             nextOp->getResult(dstIdx));\n     }\n   }\n \n   // Some values have not been used by any ops in the loop body\n-  for (BlockArgument arg : forOp.getRegionIterArgs()) {\n-    // Check if arg has a yieldOp use\n-    for (OpOperand &operand : arg.getUses()) {\n-      if (operand.getOwner() == yieldOp) {\n-        auto yieldIdx = operand.getOperandNumber();\n-        auto depYieldIdx = depArgsIdx[forOp.getRegionIterArgs()[yieldIdx]];\n-        auto newArg = newForOp.getRegionIterArgs()[depYieldIdx];\n-        if (!depArgsMapping.contains(newArg)) {\n-          auto argIdx = depArgsIdx[arg];\n-          depArgsMapping[newArg] = newForOp.getRegionIterArgs()[argIdx];\n-        }\n-      }\n-    }\n-  }\n+  for (BlockArgument arg : forOp.getRegionIterArgs())\n+    setValueMappingYield(newForOp, arg,\n+                         newForOp.getRegionIterArgs()[depArgsIdx[arg]]);\n \n   // async.wait & extract_slice\n   Operation *asyncWait = builder.create<ttg::AsyncWaitOp>(\n-      loads[0].getLoc(), loads.size() * (numStages - 2));\n+      validLoads[0].getLoc(), validLoads.size() * (numStages - 2));\n   for (auto it = extractSlices.rbegin(); it != extractSlices.rend(); ++it) {\n     // move extract_slice after asyncWait\n     it->getDefiningOp()->moveAfter(asyncWait);\n@@ -844,8 +938,9 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   loopIterIdx = builder.create<arith::AddIOp>(\n       nextIV.getLoc(), loopIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), 1, 32));\n+}\n \n-  // Finally, the YieldOp, need to sync with the order of newLoopArgs\n+void LoopPipeliner::finalizeYield(scf::ForOp newForOp, OpBuilder &builder) {\n   SmallVector<Value> yieldValues;\n   for (Value v : yieldOp->getOperands())\n     yieldValues.push_back(mapping.lookup(v));\n@@ -865,6 +960,14 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n \n   builder.setInsertionPointToEnd(newForOp.getBody());\n   builder.create<scf::YieldOp>(yieldOp->getLoc(), yieldValues);\n+}\n+\n+scf::ForOp LoopPipeliner::createNewForOp() {\n+  OpBuilder builder(forOp);\n+  auto newLoopArgs = collectNewLoopArgs();\n+  auto newForOp = cloneForOp(newLoopArgs, builder);\n+  prefetchNextIteration(newForOp, builder);\n+  finalizeYield(newForOp, builder);\n   return newForOp;\n }\n \n@@ -897,11 +1000,10 @@ struct PipelinePass : public TritonGPUPipelineBase<PipelinePass> {\n         return;\n \n       pipeliner.emitPrologue();\n-\n       scf::ForOp newForOp = pipeliner.createNewForOp();\n       pipeliner.emitEpilogue();\n \n-      // replace the original loop\n+      // Replace the original loop\n       for (unsigned i = 0; i < forOp->getNumResults(); ++i)\n         forOp->getResult(i).replaceAllUsesWith(newForOp->getResult(i));\n       forOp->erase();"}, {"filename": "lib/Dialect/TritonGPU/Transforms/TritonGPUConversion.cpp", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -12,8 +12,8 @@ using namespace mlir::triton::gpu;\n // TypeConverter\n //\n TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n-                                               int numWarps)\n-    : context(context), numWarps(numWarps) {\n+                                               int numWarps, int threadsPerWarp)\n+    : context(context), numWarps(numWarps), threadsPerWarp(threadsPerWarp) {\n   addConversion([](Type type) { return type; });\n   addConversion([this](RankedTensorType tensorType) -> RankedTensorType {\n     // types with encoding are already in the right format\n@@ -29,7 +29,8 @@ TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n     std::iota(order.begin(), order.end(), 0);\n     llvm::SmallVector<unsigned> sizePerThread(rank, 1);\n     Attribute encoding = triton::gpu::BlockedEncodingAttr::get(\n-        this->context, shape, sizePerThread, order, this->numWarps);\n+        this->context, shape, sizePerThread, order, this->numWarps,\n+        this->threadsPerWarp);\n     return RankedTensorType::get(shape, tensorType.getElementType(), encoding);\n   });\n "}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 19, "deletions": 10, "changes": 29, "file_content_changes": "@@ -91,6 +91,13 @@ void init_triton_ir(py::module &&m) {\n       .value(\"CG\", mlir::triton::CacheModifier::CG)\n       .export_values();\n \n+  py::enum_<mlir::triton::MemSemantic>(m, \"MEM_SEMANTIC\")\n+      .value(\"ACQUIRE_RELEASE\", mlir::triton::MemSemantic::ACQUIRE_RELEASE)\n+      .value(\"ACQUIRE\", mlir::triton::MemSemantic::ACQUIRE)\n+      .value(\"RELEASE\", mlir::triton::MemSemantic::RELEASE)\n+      .value(\"RELAXED\", mlir::triton::MemSemantic::RELAXED)\n+      .export_values();\n+\n   py::enum_<mlir::triton::EvictionPolicy>(m, \"EVICTION_POLICY\")\n       .value(\"NORMAL\", mlir::triton::EvictionPolicy::NORMAL)\n       .value(\"EVICT_FIRST\", mlir::triton::EvictionPolicy::EVICT_FIRST)\n@@ -1268,7 +1275,7 @@ void init_triton_ir(py::module &&m) {\n       // // atomic\n       .def(\"create_atomic_cas\",\n            [](mlir::OpBuilder &self, mlir::Value &ptr, mlir::Value &cmp,\n-              mlir::Value &val) -> mlir::Value {\n+              mlir::Value &val, mlir::triton::MemSemantic sem) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n              mlir::Type dstType;\n              if (auto srcTensorType =\n@@ -1284,12 +1291,12 @@ void init_triton_ir(py::module &&m) {\n                dstType = ptrType.getPointeeType();\n              }\n              return self.create<mlir::triton::AtomicCASOp>(loc, dstType, ptr,\n-                                                           cmp, val);\n+                                                           cmp, val, sem);\n            })\n       .def(\"create_atomic_rmw\",\n            [](mlir::OpBuilder &self, mlir::triton::RMWOp rmwOp,\n-              mlir::Value &ptr, mlir::Value &val,\n-              mlir::Value &mask) -> mlir::Value {\n+              mlir::Value &ptr, mlir::Value &val, mlir::Value &mask,\n+              mlir::triton::MemSemantic sem) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n              mlir::Type dstType;\n              if (auto srcTensorType =\n@@ -1305,7 +1312,7 @@ void init_triton_ir(py::module &&m) {\n                dstType = ptrType.getPointeeType();\n              }\n              return self.create<mlir::triton::AtomicRMWOp>(loc, dstType, rmwOp,\n-                                                           ptr, val, mask);\n+                                                           ptr, val, mask, sem);\n            })\n       // External\n       .def(\"create_extern_elementwise\",\n@@ -1532,11 +1539,13 @@ void init_triton_ir(py::module &&m) {\n              self.addPass(mlir::triton::createRewriteTensorPointerPass(\n                  computeCapability));\n            })\n-      .def(\"add_convert_triton_to_tritongpu_pass\",\n-           [](mlir::PassManager &self, int numWarps) {\n-             self.addPass(\n-                 mlir::triton::createConvertTritonToTritonGPUPass(numWarps));\n-           })\n+      .def(\n+          \"add_convert_triton_to_tritongpu_pass\",\n+          [](mlir::PassManager &self, int numWarps, int threadsPerWarp) {\n+            self.addPass(mlir::triton::createConvertTritonToTritonGPUPass(\n+                numWarps, threadsPerWarp));\n+          },\n+          py::arg(\"numWarps\") = 4, py::arg(\"threadsPerWarp\") = 32)\n       .def(\"add_tritongpu_pipeline_pass\",\n            [](mlir::PassManager &self, int numStages) {\n              self.addPass(mlir::createTritonGPUPipelinePass(numStages));"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 34, "deletions": 16, "changes": 50, "file_content_changes": "@@ -937,15 +937,16 @@ def kernel(X, Y, Z):\n # ---------------\n # test atomics\n # ---------------\n-@pytest.mark.parametrize(\"op, dtype_x_str, mode\", itertools.chain.from_iterable([\n+@pytest.mark.parametrize(\"op, dtype_x_str, mode, sem\", itertools.chain.from_iterable([\n     [\n-        ('add', 'float16', mode),\n-        ('add', 'uint32', mode), ('add', 'int32', mode), ('add', 'float32', mode),\n-        ('max', 'uint32', mode), ('max', 'int32', mode), ('max', 'float32', mode),\n-        ('min', 'uint32', mode), ('min', 'int32', mode), ('min', 'float32', mode),\n+        ('add', 'float16', mode, sem),\n+        ('add', 'uint32', mode, sem), ('add', 'int32', mode, sem), ('add', 'float32', mode, sem),\n+        ('max', 'uint32', mode, sem), ('max', 'int32', mode, sem), ('max', 'float32', mode, sem),\n+        ('min', 'uint32', mode, sem), ('min', 'int32', mode, sem), ('min', 'float32', mode, sem),\n     ]\n-    for mode in ['all_neg', 'all_pos', 'min_neg', 'max_pos']]))\n-def test_atomic_rmw(op, dtype_x_str, mode, device='cuda'):\n+    for mode in ['all_neg', 'all_pos', 'min_neg', 'max_pos']\n+    for sem in [None, 'acquire', 'release', 'acq_rel', 'relaxed']]))\n+def test_atomic_rmw(op, dtype_x_str, mode, sem, device='cuda'):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         if dtype_x_str == 'float16':\n@@ -959,7 +960,8 @@ def kernel(X, Z):\n         x = tl.load(X + pid)\n         old = GENERATE_TEST_HERE\n \n-    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.atomic_{op}(Z, x)'})\n+    sem_arg = sem if sem is None else f'\"{sem}\"'\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.atomic_{op}(Z, x, sem={sem_arg})'})\n     numpy_op = {'add': np.sum, 'max': np.max, 'min': np.min}[op]\n     max_neutral = float('-inf') if dtype_x_str in float_dtypes else np.iinfo(getattr(np, dtype_x_str)).min\n     min_neutral = float('inf') if dtype_x_str in float_dtypes else np.iinfo(getattr(np, dtype_x_str)).max\n@@ -981,7 +983,7 @@ def kernel(X, Z):\n     x_tri = to_triton(x, device=device)\n \n     z_tri = to_triton(np.array([neutral], dtype=getattr(np, dtype_x_str)), device=device)\n-    kernel[(n_programs, )](x_tri, z_tri)\n+    h = kernel[(n_programs, )](x_tri, z_tri)\n     # torch result\n     z_ref = numpy_op(x).astype(getattr(np, dtype_x_str))\n     # compare\n@@ -990,6 +992,8 @@ def kernel(X, Z):\n         assert z_ref.item() == to_numpy(z_tri).item()\n     else:\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+    sem_str = \"acq_rel\" if sem is None else sem\n+    assert f\"atom.global.gpu.{sem_str}\" in h.asm[\"ptx\"]\n \n \n def test_atomic_rmw_predicate(device=\"cuda\"):\n@@ -1047,7 +1051,8 @@ def kernel(X, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n     assert torch.min(x).item() == 0.0\n \n \n-def test_atomic_cas():\n+@pytest.mark.parametrize(\"sem\", [None, 'acquire', 'release', 'acq_rel', 'relaxed'])\n+def test_atomic_cas(sem):\n     # 1. make sure that atomic_cas changes the original value (Lock)\n     @triton.jit\n     def change_value(Lock):\n@@ -1060,9 +1065,9 @@ def change_value(Lock):\n \n     # 2. only one block enters the critical section\n     @triton.jit\n-    def serialized_add(data, Lock):\n+    def serialized_add(data, Lock, SEM: tl.constexpr):\n         ptrs = data + tl.arange(0, 128)\n-        while tl.atomic_cas(Lock, 0, 1) == 1:\n+        while tl.atomic_cas(Lock, 0, 1, SEM) == 1:\n             pass\n \n         tl.store(ptrs, tl.load(ptrs) + 1.0)\n@@ -1073,8 +1078,10 @@ def serialized_add(data, Lock):\n     Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n     data = torch.zeros((128,), device='cuda', dtype=torch.float32)\n     ref = torch.full((128,), 64.0)\n-    serialized_add[(64,)](data, Lock)\n+    h = serialized_add[(64,)](data, Lock, SEM=sem)\n+    sem_str = \"acq_rel\" if sem is None else sem\n     np.testing.assert_allclose(to_numpy(data), to_numpy(ref))\n+    assert f\"atom.global.{sem_str}\" in h.asm[\"ptx\"]\n \n \n # ---------------\n@@ -1338,7 +1345,11 @@ def get_reduced_dtype(dtype_str, op):\n \n @pytest.mark.parametrize(\"op, dtype_str, shape\",\n                          [(op, dtype, shape)\n-                          for op in ['min', 'max', 'sum', 'argmin', 'argmax']\n+                          for op in ['min', 'max',\n+                                     'min-with-indices',\n+                                     'max-with-indices',\n+                                     'argmin', 'argmax',\n+                                     'sum']\n                           for dtype in dtypes_with_bfloat16\n                           for shape in [32, 64, 128, 512]])\n def test_reduce1d(op, dtype_str, shape, device='cuda'):\n@@ -1348,15 +1359,22 @@ def test_reduce1d(op, dtype_str, shape, device='cuda'):\n     @triton.jit\n     def kernel(X, Z, BLOCK: tl.constexpr):\n         x = tl.load(X + tl.arange(0, BLOCK))\n-        tl.store(Z, GENERATE_TEST_HERE)\n+        GENERATE_TEST_HERE\n+        tl.store(Z, z)\n \n-    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=0)'})\n+    if 'with-indices' in op:\n+        patch = f'z, _ = tl.{op.split(\"-\")[0]}(x, axis=0, return_indices=True)'\n+    else:\n+        patch = f'z = tl.{op}(x, axis=0)'\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': patch})\n     # input\n     rs = RandomState(17)\n     # limit the range of integers so that the sum does not overflow\n     x = numpy_random((shape,), dtype_str=dtype_str, rs=rs)\n     x_tri = to_triton(x, device=device)\n     numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n+                'max-with-indices': np.max,\n+                'min-with-indices': np.min,\n                 'argmin': np.argmin, 'argmax': np.argmax}[op]\n     # numpy result\n     z_dtype_str = 'int32' if op in ('argmin', 'argmax') else dtype_str"}, {"filename": "python/triton/debugger/memory_map.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n import dataclasses\n \n from triton.debugger import torch_wrapper"}, {"filename": "python/triton/debugger/tl_lang.py", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n import triton\n from .core import ExecutionContext\n from .memory_map import MemoryMap\n@@ -405,7 +407,9 @@ def zeros(self, shape, dtype):\n         return torch.zeros(size=shape, dtype=dtype, device=\"cuda\")\n \n     @_tensor_operation\n-    def dequantize(self, input, scale, shift, nbit, dst_ty=torch.float16):\n+    def dequantize(self, input, scale, shift, nbit, dst_ty=None):\n+        if dst_ty is None:\n+            dst_ty = torch.float16\n         raise NotImplementedError()\n \n     @_tensor_operation"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 62, "deletions": 44, "changes": 106, "file_content_changes": "@@ -1092,59 +1092,67 @@ def _decorator(func: T) -> T:\n \n @builtin\n @_add_atomic_docstr(\"compare-and-swap\")\n-def atomic_cas(pointer, cmp, val, _builder=None):\n+def atomic_cas(pointer, cmp, val, sem=None, _builder=None):\n     cmp = _to_tensor(cmp, _builder)\n     val = _to_tensor(val, _builder)\n-    return semantic.atomic_cas(pointer, cmp, val, _builder)\n+    sem = _constexpr_to_value(sem)\n+    return semantic.atomic_cas(pointer, cmp, val, sem, _builder)\n \n \n @builtin\n @_add_atomic_docstr(\"exchange\")\n-def atomic_xchg(pointer, val, mask=None, _builder=None):\n+def atomic_xchg(pointer, val, mask=None, sem=None, _builder=None):\n     val = _to_tensor(val, _builder)\n-    return semantic.atomic_xchg(pointer, val, mask, _builder)\n+    sem = _constexpr_to_value(sem)\n+    return semantic.atomic_xchg(pointer, val, mask, sem, _builder)\n \n \n @builtin\n @_add_atomic_docstr(\"add\")\n-def atomic_add(pointer, val, mask=None, _builder=None):\n+def atomic_add(pointer, val, mask=None, sem=None, _builder=None):\n     val = _to_tensor(val, _builder)\n-    return semantic.atomic_add(pointer, val, mask, _builder)\n+    sem = _constexpr_to_value(sem)\n+    return semantic.atomic_add(pointer, val, mask, sem, _builder)\n \n \n @builtin\n @_add_atomic_docstr(\"max\")\n-def atomic_max(pointer, val, mask=None, _builder=None):\n+def atomic_max(pointer, val, mask=None, sem=None, _builder=None):\n     val = _to_tensor(val, _builder)\n-    return semantic.atomic_max(pointer, val, mask, _builder)\n+    sem = _constexpr_to_value(sem)\n+    return semantic.atomic_max(pointer, val, mask, sem, _builder)\n \n \n @builtin\n @_add_atomic_docstr(\"min\")\n-def atomic_min(pointer, val, mask=None, _builder=None):\n+def atomic_min(pointer, val, mask=None, sem=None, _builder=None):\n     val = _to_tensor(val, _builder)\n-    return semantic.atomic_min(pointer, val, mask, _builder)\n+    sem = _constexpr_to_value(sem)\n+    return semantic.atomic_min(pointer, val, mask, sem, _builder)\n \n \n @builtin\n @_add_atomic_docstr(\"logical and\")\n-def atomic_and(pointer, val, mask=None, _builder=None):\n+def atomic_and(pointer, val, mask=None, sem=None, _builder=None):\n     val = _to_tensor(val, _builder)\n-    return semantic.atomic_and(pointer, val, mask, _builder)\n+    sem = _constexpr_to_value(sem)\n+    return semantic.atomic_and(pointer, val, mask, sem, _builder)\n \n \n @builtin\n @_add_atomic_docstr(\"logical or\")\n-def atomic_or(pointer, val, mask=None, _builder=None):\n+def atomic_or(pointer, val, mask=None, sem=None, _builder=None):\n     val = _to_tensor(val, _builder)\n-    return semantic.atomic_or(pointer, val, mask, _builder)\n+    sem = _constexpr_to_value(sem)\n+    return semantic.atomic_or(pointer, val, mask, sem, _builder)\n \n \n @builtin\n @_add_atomic_docstr(\"logical xor\")\n-def atomic_xor(pointer, val, mask=None, _builder=None):\n+def atomic_xor(pointer, val, mask=None, sem=None, _builder=None):\n     val = _to_tensor(val, _builder)\n-    return semantic.atomic_xor(pointer, val, mask, _builder)\n+    sem = _constexpr_to_value(sem)\n+    return semantic.atomic_xor(pointer, val, mask, sem, _builder)\n \n \n # -----------------------\n@@ -1319,7 +1327,7 @@ def _promote_reduction_input(t, _builder=None):\n \n \n @builtin\n-def _argreduce(input, axis, combine_fn, _builder=None, _generator=None):\n+def _reduce_with_indices(input, axis, combine_fn, _builder=None, _generator=None):\n     axis = _constexpr_to_value(axis)\n     n = input.shape[axis]\n     index = arange(0, n, _builder=_builder)\n@@ -1333,7 +1341,7 @@ def _argreduce(input, axis, combine_fn, _builder=None, _generator=None):\n \n     rvalue, rindices = reduce((input, index), axis, combine_fn,\n                               _builder=_builder, _generator=_generator)\n-    return rindices\n+    return rvalue, rindices\n \n \n @triton.jit\n@@ -1361,34 +1369,39 @@ def maximum(x, y):\n     \"\"\"\n     return where(x > y, x, y)\n \n+# max and argmax\n+\n \n @triton.jit\n def _max_combine(a, b):\n     return maximum(a, b)\n \n \n-@triton.jit\n-@_add_reduction_docstr(\"maximum\")\n-def max(input, axis=None):\n-    input = _promote_reduction_input(input)\n-    return reduce(input, axis, _max_combine)\n-\n-\n @triton.jit\n def _argmax_combine(value1, index1, value2, index2):\n     gt = value1 > value2\n-    lt = value1 < value2\n-    index_min = minimum(index1, index2)\n-    index_ret = where(gt, index1, where(lt, index2, index_min))\n-    value_ret = maximum(value1, value2)\n+    value_ret = where(gt, value1, value2)\n+    index_ret = where(gt, index1, index2)\n     return value_ret, index_ret\n \n \n+@triton.jit\n+@_add_reduction_docstr(\"maximum\")\n+def max(input, axis=None, return_indices=False):\n+    input = _promote_reduction_input(input)\n+    if return_indices:\n+        return _reduce_with_indices(input, axis, _argmax_combine)\n+    else:\n+        return reduce(input, axis, _max_combine)\n+\n+\n @triton.jit\n @_add_reduction_docstr(\"maximum index\")\n def argmax(input, axis):\n-    input = _promote_reduction_input(input)\n-    return _argreduce(input, axis, _argmax_combine)\n+    (_, ret) = max(input, axis, return_indices=True)\n+    return ret\n+\n+# min and argmin\n \n \n @triton.jit\n@@ -1397,34 +1410,37 @@ def _min_combine(a, b):\n     return minimum(a, b)\n \n \n-@triton.jit\n-@_add_reduction_docstr(\"minimum\")\n-def min(input, axis=None):\n-    input = _promote_reduction_input(input)\n-    return reduce(input, axis, _min_combine)\n-\n-\n @triton.jit\n def _argmin_combine(value1, index1, value2, index2):\n     lt = value1 < value2\n-    gt = value1 > value2\n-    index_min = minimum(index1, index2)\n-    index_ret = where(lt, index1, where(gt, index2, index_min))\n-    value_ret = minimum(value1, value2)\n+    value_ret = where(lt, value1, value2)\n+    index_ret = where(lt, index1, index2)\n     return value_ret, index_ret\n \n \n+@triton.jit\n+@_add_reduction_docstr(\"minimum\")\n+def min(input, axis=None, return_indices=False):\n+    input = _promote_reduction_input(input)\n+    if return_indices:\n+        return _reduce_with_indices(input, axis, _argmin_combine)\n+    else:\n+        return reduce(input, axis, _min_combine)\n+\n+\n @triton.jit\n @_add_reduction_docstr(\"minimum index\")\n def argmin(input, axis):\n-    input = _promote_reduction_input(input)\n-    return _argreduce(input, axis, _argmin_combine)\n+    _, ret = min(input, axis, return_indices=True)\n+    return ret\n \n \n @triton.jit\n def _sum_combine(a, b):\n     return a + b\n \n+# sum\n+\n \n @triton.jit\n @_add_reduction_docstr(\"sum\")\n@@ -1438,6 +1454,8 @@ def _xor_combine(a, b):\n     return a ^ b\n \n \n+# xor sum\n+\n @builtin\n @_add_reduction_docstr(\"xor sum\")\n def xor_sum(input, axis=None, _builder=None, _generator=None):"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 52, "deletions": 15, "changes": 67, "file_content_changes": "@@ -811,6 +811,22 @@ def _str_to_padding_option(padding_option):\n     return padding\n \n \n+def _str_to_sem(sem_option):\n+    sem = ir.MEM_SEMANTIC.ACQUIRE_RELEASE\n+    if sem_option:\n+        if sem_option == \"acquire\":\n+            sem = ir.MEM_SEMANTIC.ACQUIRE\n+        elif sem_option == \"release\":\n+            sem = ir.MEM_SEMANTIC.RELEASE\n+        elif sem_option == \"acq_rel\":\n+            sem = ir.MEM_SEMANTIC.ACQUIRE_RELEASE\n+        elif sem_option == \"relaxed\":\n+            sem = ir.MEM_SEMANTIC.RELAXED\n+        else:\n+            raise ValueError(f\"Memory semantic {sem_option} not supported\")\n+    return sem\n+\n+\n def _canonicalize_boundary_check(boundary_check, block_shape):\n     if boundary_check:\n         if not hasattr(boundary_check, \"__iter__\"):\n@@ -1021,11 +1037,13 @@ def store(ptr: tl.tensor,\n def atomic_cas(ptr: tl.tensor,\n                cmp: tl.tensor,\n                val: tl.tensor,\n+               sem: str,\n                builder: ir.builder) -> tl.tensor:\n+    sem = _str_to_sem(sem)\n     element_ty = ptr.type.scalar.element_ty\n     if element_ty.primitive_bitwidth not in [16, 32, 64]:\n         raise ValueError(\"atomic_cas only supports elements with width {16, 32, 64}\")\n-    return tl.tensor(builder.create_atomic_cas(ptr.handle, cmp.handle, val.handle), val.type)\n+    return tl.tensor(builder.create_atomic_cas(ptr.handle, cmp.handle, val.handle, sem), val.type)\n \n \n def atom_red_typechecking_impl(ptr: tl.tensor,\n@@ -1035,7 +1053,6 @@ def atom_red_typechecking_impl(ptr: tl.tensor,\n                                builder: ir.builder) -> Tuple[tl.tensor, tl.tensor, tl.tensor]:\n     if not ptr.type.scalar.is_ptr():\n         raise ValueError(\"Pointer argument of store instruction is \" + ptr.type.__repr__())\n-\n     element_ty = ptr.type.scalar.element_ty\n     if element_ty is tl.float16 and op != 'add':\n         raise ValueError(\"atomic_\" + op + \" does not support fp16\")\n@@ -1060,22 +1077,26 @@ def atom_red_typechecking_impl(ptr: tl.tensor,\n def atomic_max(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n+               sem: str,\n                builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'max', builder)\n+    sem = _str_to_sem(sem)\n     sca_ty = val.type.scalar\n     # direct call to atomic_max for integers\n     if sca_ty.is_int():\n         if sca_ty.is_int_signed():\n             return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MAX,\n                                                        ptr.handle,\n                                                        val.handle,\n-                                                       mask.handle),\n+                                                       mask.handle,\n+                                                       sem),\n                              val.type)\n         else:\n             return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMAX,\n                                                        ptr.handle,\n                                                        val.handle,\n-                                                       mask.handle),\n+                                                       mask.handle,\n+                                                       sem),\n                              val.type)\n     # for float\n     # return atomic_smax(i_ptr, i_val) if val >= 0\n@@ -1084,30 +1105,34 @@ def atomic_max(ptr: tl.tensor,\n     i_ptr = bitcast(ptr, tl.pointer_type(tl.int32, 1), builder)\n     pos = greater_equal(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n     neg = less_than(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n-    pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MAX, i_ptr.handle, i_val.handle, and_(mask, pos, builder).handle), i_val.type)\n-    neg_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMIN, i_ptr.handle, i_val.handle, and_(mask, neg, builder).handle), i_val.type)\n+    pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MAX, i_ptr.handle, i_val.handle, and_(mask, pos, builder).handle, sem), i_val.type)\n+    neg_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMIN, i_ptr.handle, i_val.handle, and_(mask, neg, builder).handle, sem), i_val.type)\n     return where(pos, pos_ret, neg_ret, builder)\n \n \n def atomic_min(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n+               sem: str,\n                builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'min', builder)\n+    sem = _str_to_sem(sem)\n     sca_ty = val.type.scalar\n     # direct call to atomic_min for integers\n     if sca_ty.is_int():\n         if sca_ty.is_int_signed():\n             return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MIN,\n                                                        ptr.handle,\n                                                        val.handle,\n-                                                       mask.handle),\n+                                                       mask.handle,\n+                                                       sem),\n                              val.type)\n         else:\n             return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMIN,\n                                                        ptr.handle,\n                                                        val.handle,\n-                                                       mask.handle),\n+                                                       mask.handle,\n+                                                       sem),\n                              val.type)\n     # for float\n     # return atomic_smin(i_ptr, i_val) if val >= 0\n@@ -1119,56 +1144,68 @@ def atomic_min(ptr: tl.tensor,\n     pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MIN,\n                                                   i_ptr.handle,\n                                                   i_val.handle,\n-                                                  and_(mask, pos, builder).handle),\n+                                                  and_(mask, pos, builder).handle,\n+                                                  sem),\n                         i_val.type)\n     neg_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMAX,\n                                                   i_ptr.handle,\n                                                   i_val.handle,\n-                                                  and_(mask, neg, builder).handle),\n+                                                  and_(mask, neg, builder).handle,\n+                                                  sem),\n                         i_val.type)\n     return where(pos, pos_ret, neg_ret, builder)\n \n \n def atomic_add(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n+               sem: str,\n                builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'add', builder)\n+    sem = _str_to_sem(sem)\n     sca_ty = val.type.scalar\n     op = ir.ATOMIC_OP.FADD if sca_ty.is_floating() else ir.ATOMIC_OP.ADD\n-    return tl.tensor(builder.create_atomic_rmw(op, ptr.handle, val.handle, mask.handle), val.type)\n+    return tl.tensor(builder.create_atomic_rmw(op, ptr.handle, val.handle, mask.handle, sem), val.type)\n \n \n def atomic_and(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n+               sem: str,\n                builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'and', builder)\n-    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.AND, ptr.handle, val.handle, mask.handle), val.type)\n+    sem = _str_to_sem(sem)\n+    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.AND, ptr.handle, val.handle, mask.handle, sem), val.type)\n \n \n def atomic_or(ptr: tl.tensor,\n               val: tl.tensor,\n               mask: tl.tensor,\n+              sem: str,\n               builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'or', builder)\n-    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.OR, ptr.handle, val.handle, mask.handle), val.type)\n+    sem = _str_to_sem(sem)\n+    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.OR, ptr.handle, val.handle, mask.handle, sem), val.type)\n \n \n def atomic_xor(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n+               sem: str,\n                builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'xor', builder)\n-    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XOR, ptr.handle, val.handle, mask.handle), val.type)\n+    sem = _str_to_sem(sem)\n+    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XOR, ptr.handle, val.handle, mask.handle, sem), val.type)\n \n \n def atomic_xchg(ptr: tl.tensor,\n                 val: tl.tensor,\n                 mask: tl.tensor,\n+                sem: str,\n                 builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'xchg', builder)\n-    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XCHG, ptr.handle, val.handle, mask.handle), val.type)\n+    sem = _str_to_sem(sem)\n+    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XCHG, ptr.handle, val.handle, mask.handle, sem), val.type)\n \n # ===----------------------------------------------------------------------===//\n #                               Linear Algebra"}, {"filename": "test/Conversion/triton_to_tritongpu.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,7 +1,7 @@\n // RUN: triton-opt %s -split-input-file -convert-triton-to-tritongpu=num-warps=2 | FileCheck %s\n \n tt.func @ops() {\n-  // CHECK: module attributes {\"triton_gpu.num-warps\" = 2 : i32} {{.*}}\n+  // CHECK: module attributes {\"triton_gpu.num-warps\" = 2 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {{.*}}\n   %a = arith.constant dense<1.00e+00> : tensor<128x32xf16>\n   %b = arith.constant dense<2.00e+00> : tensor<32x128xf16>\n   %c = arith.constant dense<3.00e+00> : tensor<128x128xf32>\n@@ -36,7 +36,7 @@ tt.func @reduce_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n   // CHECK: #[[blocked0:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 8], warpsPerCTA = [1, 2], order = [0, 1]}>\n   // CHECK: #[[blocked1:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 2], order = [0, 1]}>\n   // CHECK: #[[blocked2:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [16, 2], warpsPerCTA = [1, 2], order = [0, 1]}>\n-  // CHECK: module attributes {\"triton_gpu.num-warps\" = 2 : i32} {{.*}}\n+  // CHECK: module attributes {\"triton_gpu.num-warps\" = 2 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {{.*}}\n   %c0 = arith.constant dense<1.00e+00> : tensor<4x4xf32>\n   %c1 = arith.constant dense<2.00e+00> : tensor<8x2xf32>\n   %c2 = arith.constant dense<3.00e+00> : tensor<16x16xf32>"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 5, "deletions": 6, "changes": 11, "file_content_changes": "@@ -1010,10 +1010,10 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: atomic_add_f32\n   tt.func @atomic_add_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xi1, #blocked0>, %arg2 : tensor<256xf32, #blocked0>) {\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: @$3 atom.global.gpu.add.f32\n+    // CHECK-SAME: @$3 atom.global.gpu.relaxed.add.f32\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: @$3 atom.global.gpu.add.f32\n-    %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32} : (tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xf32, #blocked0>, tensor<256xi1, #blocked0>) -> tensor<256xf32, #blocked0>\n+    // CHECK-SAME: @$3 atom.global.gpu.relaxed.add.f32\n+    %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32, sem = 1 : i32} : (tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xf32, #blocked0>, tensor<256xi1, #blocked0>) -> tensor<256xf32, #blocked0>\n     tt.return\n   }\n }\n@@ -1025,9 +1025,8 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   tt.func @atomic_add_f32_scalar(%arg0 : !tt.ptr<f32>, %arg1 : i1, %arg2 : f32) {\n     // CHECK: llvm.icmp \"eq\"\n     // CHECK: llvm.inline_asm\n-    // CHECK: llvm.inline_asm\n-    // CHECK-SAME: @$3 atom.global.gpu.add.f32\n-    %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32} : (!tt.ptr<f32>, f32, i1) -> f32\n+    // CHECK-SAME: @$3 atom.global.gpu.relaxed.add.f32\n+    %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32, sem = 1: i32} : (!tt.ptr<f32>, f32, i1) -> f32\n     tt.return\n   }\n }"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 119, "deletions": 0, "changes": 119, "file_content_changes": "@@ -313,3 +313,122 @@ tt.func @lut_bmm_vector(%77: tensor<16x16xi64, #BL> {tt.divisibility=16: i32, tt\n   }\n   tt.return %79#0 : tensor<16x16xf32, #C>\n }\n+\n+// CHECK: tt.func @post_load_inv\n+// CHECK: scf.for\n+// CHECK: arith.index_cast\n+// CHECK-DAG: %[[IV:.*]] = arith.index_cast\n+// CHECK: %[[NEXT_IV:.*]] = arith.addi %[[IV]], %c1_i32 : i32\n+// CHECK-NOT: arith.addi %[[NEXT_IV]]\n+tt.func @post_load_inv(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                       %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                       %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                       %arg3: i32 {tt.divisibility = 16 : i32},\n+                       %arg4: i32 {tt.divisibility = 16 : i32},\n+                       %arg5: i32 {tt.divisibility = 16 : i32},\n+                       %arg6: i32 {tt.divisibility = 16 : i32},\n+                       %arg7: i32 {tt.divisibility = 16 : i32},\n+                       %arg8: i32 {tt.divisibility = 16 : i32}) -> tensor<32x32xf32, #C> {\n+  %c0_index = arith.constant 0 : index\n+  %c1_index = arith.constant 1 : index\n+  %c1_i32 = arith.constant 1 : i32\n+  %c32_i32 = arith.constant 32 : i32\n+  %84 = arith.constant 900 : index\n+  %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #C>\n+  %cst_0 = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #AL>\n+  %50 = tt.splat %arg3 : (i32) -> tensor<1x32xi32, #AL>\n+  %59 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %81 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %66 = tt.splat %arg4 : (i32) -> tensor<32x1xi32, #AL>\n+  %60 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %82 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %85:3 = scf.for %arg9 = %c0_index to %84 step %c1_index iter_args(%arg10 = %cst, %arg11 = %59, %arg12 = %81) -> (tensor<32x32xf32, #C>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>)  {\n+    %130 = arith.index_cast %arg9 : index to i32\n+    %107 = arith.muli %130, %c32_i32 : i32\n+    %108 = arith.subi %arg5, %107 : i32\n+    %109 = tt.splat %108 : (i32) -> tensor<1x32xi32, #AL>\n+    %110 = \"triton_gpu.cmpi\"(%50, %109) <{predicate = 2 : i64}> : (tensor<1x32xi32, #AL>, tensor<1x32xi32, #AL>) -> tensor<1x32xi1, #AL>\n+    %111 = tt.broadcast %110 : (tensor<1x32xi1, #AL>) -> tensor<32x32xi1, #AL>\n+    %112 = tt.load %arg11, %111, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #AL>\n+    %113 = tt.splat %108 : (i32) -> tensor<32x1xi32, #AL>\n+    %114 = \"triton_gpu.cmpi\"(%66, %113) <{predicate = 2 : i64}> : (tensor<32x1xi32, #AL>, tensor<32x1xi32, #AL>) -> tensor<32x1xi1, #AL>\n+    %115 = tt.broadcast %114 : (tensor<32x1xi1, #AL>) -> tensor<32x32xi1, #AL>\n+    %116 = tt.load %arg12, %115, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #AL>\n+    %117 = triton_gpu.convert_layout %112 : (tensor<32x32xf32, #AL>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth = 1}>>\n+    %118 = triton_gpu.convert_layout %116 : (tensor<32x32xf32, #AL>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth = 1}>>\n+    %119 = tt.dot %117, %118, %arg10 {allowTF32 = true} : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth = 1}>> * tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth = 1}>> -> tensor<32x32xf32, #C>\n+    %131 = arith.index_cast %arg9 : index to i32\n+    %120 = arith.addi %131, %c1_i32 : i32\n+    %121 = arith.muli %120, %c32_i32 : i32\n+    %122 = tt.splat %121 : (i32) -> tensor<32x32xi32, #AL>\n+    %123 = tt.addptr %60, %122 : tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32xi32, #AL>\n+    %124 = arith.muli %121, %arg7 : i32\n+    %125 = tt.splat %124 : (i32) -> tensor<32x32xi32, #AL>\n+    %126 = tt.addptr %82, %125 : tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32xi32, #AL>\n+    scf.yield %119, %123, %126 : tensor<32x32xf32, #C>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>\n+  }\n+  tt.return %85#0 : tensor<32x32xf32, #C>\n+}\n+\n+// CHECK: tt.func @cross_iter_dep\n+// CHECK: triton_gpu.async_commit_group\n+// CHECK: triton_gpu.async_commit_group\n+// CHECK: triton_gpu.async_commit_group\n+// CHECK: triton_gpu.async_commit_group\n+// CHECK: %[[PTR0:.*]] = tt.addptr\n+// CHECK: %[[PTR1:.*]] = tt.addptr\n+// CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[BUF0:.*]] = %[[PTR0]], {{.*}}, %[[BUF1:.*]] = %[[PTR1]]\n+// CHECK: scf.yield\n+// CHECK-SAME: %[[BUF0]]\n+// CHECK-SAME: %[[BUF1]]\n+tt.func @cross_iter_dep(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                        %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                        %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                        %arg3: i32 {tt.divisibility = 16 : i32},\n+                        %arg4: i32 {tt.divisibility = 16 : i32},\n+                        %arg5: i32 {tt.divisibility = 16 : i32},\n+                        %arg6: i32 {tt.divisibility = 16 : i32},\n+                        %arg7: i32 {tt.divisibility = 16 : i32},\n+                        %arg8: i32 {tt.divisibility = 16 : i32}) -> tensor<32x32xf32, #C> {\n+  %c0_i32 = arith.constant 0 : index\n+  %118 = arith.constant 32 : index\n+  %c1_i32 = arith.constant 1 : index\n+  %c2_i32 = arith.constant 2 : i32\n+  %c32_i32 = arith.constant 32 : i32\n+  %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #C>\n+  %cst_1 = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #AL>\n+  %78 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %110 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %112 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %113 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %116 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %65 = tt.splat %arg3 : (i32) -> tensor<1x32xi32, #AL>\n+  %88 = tt.splat %arg4 : (i32) -> tensor<32x1xi32, #AL>\n+  %80 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %119:5 = scf.for %arg9 = %c0_i32 to %118 step %c1_i32 iter_args(%arg10 = %cst, %arg11 = %78, %arg12 = %110, %arg13 = %113, %arg14 = %116) -> (tensor<32x32xf32, #C>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>)  {\n+    %161 = arith.index_cast %arg9 : index to i32\n+    %141 = arith.muli %161, %c32_i32 : i32\n+    %142 = arith.subi %arg5, %141 : i32\n+    %143 = tt.splat %142 : (i32) -> tensor<1x32xi32, #AL>\n+    %144 = \"triton_gpu.cmpi\"(%65, %143) <{predicate = 2 : i64}> : (tensor<1x32xi32, #AL>, tensor<1x32xi32, #AL>) -> tensor<1x32xi1, #AL>\n+    %145 = tt.broadcast %144 : (tensor<1x32xi1, #AL>) -> tensor<32x32xi1, #AL>\n+    %146 = tt.load %arg11, %145, %cst_1 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #AL>\n+    %147 = tt.splat %142 : (i32) -> tensor<32x1xi32, #AL>\n+    %148 = \"triton_gpu.cmpi\"(%88, %147) <{predicate = 2 : i64}> : (tensor<32x1xi32, #AL>, tensor<32x1xi32, #AL>) -> tensor<32x1xi1, #AL>\n+    %149 = tt.broadcast %148 : (tensor<32x1xi1, #AL>) -> tensor<32x32xi1, #AL>\n+    %150 = tt.load %arg12, %149, %cst_1 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #AL>\n+    %151 = triton_gpu.convert_layout %146 : (tensor<32x32xf32, #AL>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth = 1}>>\n+    %152 = triton_gpu.convert_layout %150 : (tensor<32x32xf32, #AL>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth = 1}>>\n+    %153 = tt.dot %151, %152, %arg10 {allowTF32 = true} : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth = 1}>> * tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth = 1}>> -> tensor<32x32xf32, #C>\n+    %162 = arith.index_cast %arg9 : index to i32\n+    %154 = arith.addi %162, %c2_i32 : i32\n+    %155 = arith.muli %154, %c32_i32 : i32\n+    %156 = tt.splat %155 : (i32) -> tensor<32x32xi32, #AL>\n+    %157 = tt.addptr %80, %156 : tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32xi32, #AL>\n+    %158 = arith.muli %155, %arg7 : i32\n+    %159 = tt.splat %158 : (i32) -> tensor<32x32xi32, #AL>\n+    %160 = tt.addptr %112, %159 : tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32xi32, #AL>\n+    scf.yield %153, %arg13, %arg14, %157, %160 : tensor<32x32xf32, #C>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>\n+  }\n+  tt.return %119#0 : tensor<32x32xf32, #C>\n+}"}, {"filename": "utils/nightly.pypirc", "status": "added", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -0,0 +1,6 @@\n+[distutils]\n+Index-servers =\n+  Triton-Nightly\n+\n+[Triton-Nightly]\n+Repository = https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/upload/"}]