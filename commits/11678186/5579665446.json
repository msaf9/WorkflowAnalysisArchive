[{"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 20, "deletions": 19, "changes": 39, "file_content_changes": "@@ -2,8 +2,13 @@\n Fused Attention\n ===============\n \n-This is a Triton implementation of the Flash Attention algorithm\n-(see: Dao et al., https://arxiv.org/pdf/2205.14135v2.pdf; Rabe and Staats https://arxiv.org/pdf/2112.05682v2.pdf)\n+This is a Triton implementation of the Flash Attention v2 algorithm from Tri Dao (https://tridao.me/publications/flash2/flash2.pdf)\n+\n+Extra Credits:\n+- Original flash attention paper (https://arxiv.org/abs/2205.14135)\n+- Rabe and Staats (https://arxiv.org/pdf/2112.05682v2.pdf)\n+- Adam P. Goucher for simplified vector math\n+\n \"\"\"\n \n import pytest\n@@ -90,8 +95,7 @@ def _fwd_kernel(\n         l_i = tl.load(l_ptrs)\n         acc += tl.load(O_block_ptr).to(tl.float32)\n         lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n-    # credits to: Adam P. Goucher (https://github.com/apgoucher):\n-    # scale sm_scale by 1/log_2(e) and use\n+    # scale sm_scale by log_2(e) and use\n     # 2^x instead of exp in the loop because CSE and LICM\n     # don't work as expected with `exp` in the loop\n     qk_scale = sm_scale * 1.44269504\n@@ -111,32 +115,28 @@ def _fwd_kernel(\n         if MODE == 1 or MODE == 3:\n             qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n         # -- compute m_ij, p, l_ij\n-        m_ij = tl.max(qk, 1)\n+        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n         p = tl.math.exp2(qk - m_ij[:, None])\n         l_ij = tl.sum(p, 1)\n         # -- update m_i and l_i\n-        m_i_new = tl.maximum(m_i, m_ij)\n-        alpha = tl.math.exp2(m_i - m_i_new)\n-        beta = tl.math.exp2(m_ij - m_i_new)\n+        alpha = tl.math.exp2(m_i - m_ij)\n         l_i *= alpha\n-        l_i_new = l_i + beta * l_ij\n-        # scale p\n-        p_scale = beta / l_i_new\n-        p = p * p_scale[:, None]\n+        l_i_new = l_i + l_ij\n         # scale acc\n-        acc_scale = l_i / l_i_new\n+        acc_scale = l_i * 0 + alpha\n         acc = acc * acc_scale[:, None]\n         # update acc\n         v = tl.load(V_block_ptr)\n         p = p.to(tl.float16)\n         acc += tl.dot(p, v)\n         # update m_i and l_i\n         l_i = l_i_new\n-        m_i = m_i_new\n+        m_i = m_ij\n         # update pointers\n         K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n         V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n     # write back l and m\n+    acc = acc / l_i[:, None]\n     l_ptrs = L + off_hz * N_CTX + offs_m\n     m_ptrs = M + off_hz * N_CTX + offs_m\n     tl.store(l_ptrs, l_i)\n@@ -266,13 +266,14 @@ class _attention(torch.autograd.Function):\n \n     @staticmethod\n     def forward(ctx, q, k, v, causal, sm_scale):\n-        BLOCK = 128\n         # shape constraints\n         Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n         assert Lq == Lk and Lk == Lv\n         assert Lk in {16, 32, 64, 128}\n         o = torch.empty_like(q)\n-        grid = (triton.cdiv(q.shape[2], 128), q.shape[0] * q.shape[1], 1)\n+        BLOCK_M = 128\n+        BLOCK_N = 64\n+        grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n \n@@ -291,10 +292,10 @@ def forward(ctx, q, k, v, causal, sm_scale):\n                 v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n                 o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n                 q.shape[0], q.shape[1], q.shape[2],\n-                BLOCK_M=128, BLOCK_N=BLOCK, BLOCK_DMODEL=Lk,\n+                BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=Lk,\n                 MODE=mode,\n                 num_warps=num_warps,\n-                num_stages=2)\n+                num_stages=4)\n \n         ctx.save_for_backward(q, k, v, o, L, m)\n         ctx.grid = grid\n@@ -396,7 +397,7 @@ def test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\n     ylabel='ms',\n     plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n     args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode, 'causal': causal}\n-) for mode in ['fwd', 'bwd'] for causal in [False, True]]\n+) for mode in ['fwd'] for causal in [False]]\n \n \n @triton.testing.perf_report(configs)"}]