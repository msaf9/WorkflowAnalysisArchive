[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -56,7 +56,7 @@ jobs:\n       - name: Check cpp style\n         if: ${{ matrix.runner != 'macos-latest' }}\n         run: |\n-          sudo apt-get install -y clang-format\n+          pip install clang-format\n           find . -regex '.*\\.\\(cpp\\|hpp\\|h\\|cc\\)' -not -path \"./python/build/*\" -not -path \"./include/triton/external/*\" -print0 | xargs -0 -n1 clang-format -style=file --dry-run -Werror -i ||\n           (echo '::error title=Style issues:: Please run `find . -regex \".*\\.\\(cpp\\|hpp\\|h\\|cc\\)\" -not -path \"./python/build/*\" -not -path \"./include/triton/external/*\" -print0 | xargs -0 -n1 clang-format -style=file -i`' ; exit 1)\n "}, {"filename": "lib/Dialect/Triton/Transforms/Combine.cpp", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -71,7 +71,7 @@ class CombineSelectMaskedLoadPattern : public mlir::RewritePattern {\n     mlir::Value falseValue = selectOp.getFalseValue();\n \n     auto *loadOpCandidate = trueValue.getDefiningOp();\n-    auto loadOp = llvm::dyn_cast<triton::LoadOp>(loadOpCandidate);\n+    auto loadOp = llvm::dyn_cast_or_null<triton::LoadOp>(loadOpCandidate);\n     if (!loadOp)\n       return mlir::failure();\n \n@@ -81,7 +81,7 @@ class CombineSelectMaskedLoadPattern : public mlir::RewritePattern {\n \n     auto *broadcastOpCandidate = mask.getDefiningOp();\n     auto broadcastOp =\n-        llvm::dyn_cast<triton::BroadcastOp>(broadcastOpCandidate);\n+        llvm::dyn_cast_or_null<triton::BroadcastOp>(broadcastOpCandidate);\n     if (!broadcastOp)\n       return mlir::failure();\n \n@@ -106,7 +106,8 @@ struct CanonicalizeMaskedLoadPattern\n     if (!mask)\n       return mlir::failure();\n \n-    auto constantMask = llvm::dyn_cast<arith::ConstantOp>(mask.getDefiningOp());\n+    auto constantMask =\n+        llvm::dyn_cast_or_null<arith::ConstantOp>(mask.getDefiningOp());\n     if (!constantMask)\n       return mlir::failure();\n \n@@ -152,7 +153,8 @@ struct CanonicalizeMaskedStorePattern\n     if (!mask)\n       return mlir::failure();\n \n-    auto constantMask = llvm::dyn_cast<arith::ConstantOp>(mask.getDefiningOp());\n+    auto constantMask =\n+        llvm::dyn_cast_or_null<arith::ConstantOp>(mask.getDefiningOp());\n     if (!constantMask)\n       return mlir::failure();\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 11, "deletions": 3, "changes": 14, "file_content_changes": "@@ -301,9 +301,17 @@ void LoopPipeliner::emitPrologue() {\n       }\n \n       // If this is a load/async_copy, we need to update the mask\n-      if (llvm::isa<triton::LoadOp, triton::gpu::InsertSliceAsyncOp>(newOp)) {\n-        Value mask = llvm::isa<triton::LoadOp>(newOp) ? newOp->getOperand(1)\n-                                                      : newOp->getOperand(3);\n+      if (Value mask = [&]() {\n+            if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(newOp)) {\n+              return loadOp.mask();\n+            } else if (auto insertSliceAsyncOp =\n+                           llvm::dyn_cast<triton::gpu::InsertSliceAsyncOp>(\n+                               newOp)) {\n+              return insertSliceAsyncOp.mask();\n+            } else {\n+              return mlir::Value();\n+            }\n+          }()) {\n         // assert(I1 or TensorOf<[I1]>);\n         OpBuilder::InsertionGuard g(builder);\n         // TODO: move this out of the loop"}, {"filename": "test/Triton/combine.mlir", "status": "modified", "additions": 37, "deletions": 0, "changes": 37, "file_content_changes": "@@ -61,6 +61,22 @@ func @test_combine_select_masked_load_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %con\n     return %0, %1 : tensor<8xf32>, tensor<8xf32>\n }\n \n+// CHECK-LABEL: @test_combine_select_masked_load_fail_pattern\n+func @test_combine_select_masked_load_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %dummy_load: tensor<8xf32>, %dummy_broadcast: tensor<8xi1>, %cond: i1) -> (tensor<8xf32>, tensor<8xf32>) {\n+    %false_val = arith.constant dense<0.0> : tensor<8xf32>\n+\n+    // Case 1: value at the \"load\" position is not an \"op\".  Select should not be canonicalized.\n+    // CHECK: %{{.*}} = select %{{.*}}, %{{.*}}, %{{.*}} : tensor<8xf32>\n+    %0 = select %cond, %dummy_load, %false_val : tensor<8xf32>\n+\n+    // Case 2: value at the \"broadcast\" position is not an \"op\".  Select should not be canonicalized.\n+    %real_load = tt.load %ptr, %dummy_broadcast, %false_val {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8xf32>\n+    // CHECK: %{{.*}} = select %{{.*}}, %{{.*}}, %{{.*}} : tensor<8xf32>\n+    %1 = select %cond, %real_load, %false_val : tensor<8xf32>\n+\n+    return %0, %1 : tensor<8xf32>, tensor<8xf32>\n+}\n+\n // CHECK-LABEL: @test_combine_broadcast_constant_pattern\n func @test_combine_broadcast_constant_pattern(%cst : f32) -> tensor<8x2xf32> {\n     // CHECK: %[[cst:.*]] = arith.constant dense<1.000000e+00> : tensor<8x2xf32>\n@@ -92,6 +108,19 @@ func @test_canonicalize_masked_load_pattern(%ptr: tensor<8x!tt.ptr<f32>>) -> (te\n     return %x, %y, %z: tensor<8xf32>, tensor<8xf32>, tensor<8xf32>\n }\n \n+// CHECK-LABEL: @test_canonicalize_masked_load_fail_pattern\n+func @test_canonicalize_masked_load_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %mask: tensor<8xi1>) -> (tensor<8xf32>, tensor<8xf32>) {\n+    %other_val = arith.constant dense<0.0> : tensor<8xf32>\n+\n+    // Case: value at the \"mask\" position is not an \"op\".  Load should not be canonicalized.\n+    // CHECK: %[[res1:.*]] = tt.load %{{.*}}, %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8xf32>\n+    %x = tt.load %ptr, %mask {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8xf32>\n+    // CHECK: %[[res1:.*]] = tt.load %{{.*}}, %{{.*}}, %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8xf32>\n+    %y = tt.load %ptr, %mask, %other_val {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<8xf32>\n+\n+    return %x, %y: tensor<8xf32>, tensor<8xf32>\n+}\n+\n // CHECK-LABEL: @test_canonicalize_masked_store_pattern\n func @test_canonicalize_masked_store_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %val: tensor<8xf32>) {\n     %true_mask = arith.constant dense<true> : tensor<8xi1>\n@@ -105,3 +134,11 @@ func @test_canonicalize_masked_store_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %val:\n     tt.store %ptr, %val, %false_mask : tensor<8xf32>\n     return\n }\n+\n+// CHECK-LABEL: @test_canonicalize_masked_store_fail_pattern\n+func @test_canonicalize_masked_store_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %val: tensor<8xf32>, %mask: tensor<8xi1>) {\n+    // Case: value at the \"mask\" position is not an \"op\".  Store should not be canonicalized.\n+    // CHECK: tt.store %{{.*}}, %{{.*}}, %{{.*}} : tensor<8xf32>\n+    tt.store %ptr, %val, %mask : tensor<8xf32>\n+    return\n+}"}]