[{"filename": "include/triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -292,13 +292,11 @@ struct PTXCpAsyncWaitGroupInstr : public PTXCpAsyncInstrBase {\n \n struct PTXCpAsyncLoadInstr : public PTXCpAsyncInstrBase {\n   explicit PTXCpAsyncLoadInstr(PTXBuilder *builder,\n-                               triton::CacheModifier modifier,\n-                               triton::EvictionPolicy policy)\n+                               triton::CacheModifier modifier)\n       : PTXCpAsyncInstrBase(builder) {\n     o(triton::stringifyCacheModifier(modifier).str());\n     o(\"shared\");\n     o(\"global\");\n-    o(\"L2::\" + triton::stringifyEvictionPolicy(policy).str());\n   }\n };\n "}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 1, "deletions": 5, "changes": 6, "file_content_changes": "@@ -42,18 +42,14 @@ void MembarAnalysis::dfsOperation(Operation *operation,\n \n void MembarAnalysis::transfer(Operation *op, RegionInfo *regionInfo,\n                               OpBuilder *builder) {\n-  if (op->getNumResults() < 1)\n-    return;\n-\n   if (isa<scf::ForOp>(op) || isa<scf::IfOp>(op) || isa<scf::YieldOp>(op) ||\n       isa<triton::gpu::ExtractSliceOp>(op) ||\n-      isa<triton::gpu::InsertSliceAsyncOp>(op) ||\n       isa<triton::gpu::AllocTensorOp>(op)) {\n     // Do not insert barriers before control flow operations and\n     // alloc/extract/insert\n     // alloc is an allocation op without memory write.\n     // In contrast, arith.constant is an allocation op with memory write.\n-    // FIXME(Keren): extract and insert are always alias for now\n+    // FIXME(Keren): extract is always alias for now\n     return;\n   }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 55, "deletions": 31, "changes": 86, "file_content_changes": "@@ -6,6 +6,7 @@\n #include \"mlir/Conversion/LLVMCommon/LoweringOptions.h\"\n #include \"mlir/Conversion/LLVMCommon/Pattern.h\"\n #include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n+#include \"mlir/Conversion/SCFToStandard/SCFToStandard.h\"\n #include \"mlir/Conversion/StandardToLLVM/ConvertStandardToLLVM.h\"\n #include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"\n #include \"mlir/Dialect/GPU/GPUDialect.h\"\n@@ -14,6 +15,7 @@\n #include \"mlir/Transforms/DialectConversion.h\"\n #include \"triton/Analysis/Allocation.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n+#include \"triton/Analysis/Membar.h\"\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Conversion/MLIRTypes.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h\"\n@@ -108,7 +110,7 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n #define i32_ty rewriter.getIntegerType(32)\n #define f32_ty rewriter.getF32Type()\n #define vec_ty(type, num) VectorType::get(num, type)\n-#define void_ty LLVM::LLVMVoidType::get(ctx)\n+#define void_ty(ctx) LLVM::LLVMVoidType::get(ctx)\n #define struct_ty(...) LLVM::LLVMStructType::getLiteral(__VA_ARGS__)\n \n // Creator for constant\n@@ -360,7 +362,7 @@ Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n   auto *ptrOpr = builder.newAddrOperand(ptr, \"r\");\n   auto *valOpr = builder.newOperand(val, c);\n   st(ptrOpr, valOpr).predicate(pred, \"b\");\n-  return builder.launch(rewriter, loc, void_ty);\n+  return builder.launch(rewriter, loc, void_ty(ctx));\n }\n \n struct ConvertTritonGPUOpToLLVMPatternBase {\n@@ -1151,7 +1153,7 @@ struct StoreOpConversion\n       llvm::SmallVector<Type> argTys({boolTy, ptr.getType()});\n       argTys.insert(argTys.end(), nWords, valArgTy);\n \n-      auto ASMReturnTy = LLVM::LLVMVoidType::get(ctx);\n+      auto ASMReturnTy = void_ty(ctx);\n \n       ptxBuilder.launch(rewriter, loc, ASMReturnTy);\n     }\n@@ -3873,7 +3875,7 @@ struct AsyncWaitOpConversion\n \n     auto ctx = op.getContext();\n     auto loc = op.getLoc();\n-    auto voidTy = LLVM::LLVMVoidType::get(ctx);\n+    auto voidTy = void_ty(ctx);\n     auto ret = ptxBuilder.launch(rewriter, loc, voidTy);\n \n     // Safe to remove the op since it doesn't have any return value.\n@@ -3911,7 +3913,7 @@ struct InsertSliceAsyncOpConversion\n \n     auto srcTy = src.getType().cast<RankedTensorType>();\n     auto resTy = dst.getType().cast<RankedTensorType>();\n-    auto resElemTy = resTy.getElementType();\n+    auto resElemTy = getTypeConverter()->convertType(resTy.getElementType());\n     auto srcBlockedLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n     auto resSharedLayout = resTy.getEncoding().cast<SharedEncodingAttr>();\n     auto srcShape = srcTy.getShape();\n@@ -3932,7 +3934,7 @@ struct InsertSliceAsyncOpConversion\n     assert(axis == 0 && \"insert_slice_async: Only axis=0 is supported for now\");\n     auto dstBase = createIndexAttrConstant(rewriter, loc,\n                                            getTypeConverter()->getIndexType(),\n-                                           product<int64_t>(resTy.getShape()));\n+                                           product<int64_t>(srcTy.getShape()));\n     Value offset = mul(llIndex, dstBase);\n     auto dstPtrTy = LLVM::LLVMPointerType::get(\n         getTypeConverter()->convertType(resTy.getElementType()), 3);\n@@ -4043,40 +4045,42 @@ struct InsertSliceAsyncOpConversion\n       auto numWordElems = bitWidth / resElemTy.getIntOrFloatBitWidth();\n \n       // XXX(Keren): Tune CG and CA here.\n+      auto byteWidth = bitWidth / 8;\n       CacheModifier srcCacheModifier =\n-          bitWidth == 128 ? CacheModifier::CG : CacheModifier::CA;\n-      assert(bitWidth == 128 || bitWidth == 64 || bitWidth == 32);\n+          byteWidth == 16 ? CacheModifier::CG : CacheModifier::CA;\n+      assert(byteWidth == 16 || byteWidth == 8 || byteWidth == 4);\n+      auto resByteWidth = resElemTy.getIntOrFloatBitWidth() / 8;\n \n-      for (int wordIdx = 0; wordIdx < numWords; ++wordIdx) {\n+      auto tileOffset = tileOffsetMap[{tileVecIdxRow, tileVecIdxCol}];\n+      for (unsigned wordIdx = 0; wordIdx < numWords; ++wordIdx) {\n         PTXBuilder ptxBuilder;\n-        auto &copyAsyncOp = *ptxBuilder.create<PTXCpAsyncLoadInstr>(\n-            srcCacheModifier, op.evict());\n-\n-        auto tileOffset = tileOffsetMap[{tileVecIdxRow, tileVecIdxCol}];\n-        auto *dstOperand =\n-            ptxBuilder.newAddrOperand(tileOffset, \"r\", baseOffset);\n-        auto *srcOperand = ptxBuilder.newAddrOperand(srcElems[vecIdx], \"l\");\n-        auto *copySize = ptxBuilder.newConstantOperand(bitWidth);\n+        auto wordElemIdx = wordIdx * numWordElems;\n+        auto &copyAsyncOp =\n+            *ptxBuilder.create<PTXCpAsyncLoadInstr>(srcCacheModifier);\n+        auto *dstOperand = ptxBuilder.newAddrOperand(\n+            tileOffset, \"r\", (wordElemIdx + baseOffset) * resByteWidth);\n+        auto *srcOperand =\n+            ptxBuilder.newAddrOperand(srcElems[elemIdx + wordElemIdx], \"l\");\n+        auto *copySize = ptxBuilder.newConstantOperand(byteWidth);\n         auto *srcSize = copySize;\n         if (op.mask()) {\n           // We don't use predicate in this case, setting src-size to 0\n           // if there's any mask. cp.async will automatically fill the\n           // remaining slots with 0 if cp-size > src-size.\n           // XXX(Keren): Always assume other = 0 for now.\n-          auto selectOp = select(maskElems[vecIdx + wordIdx * numWordElems],\n-                                 i32_val(bitWidth), i32_val(0));\n+          auto selectOp = select(maskElems[elemIdx + wordElemIdx],\n+                                 i32_val(byteWidth), i32_val(0));\n           srcSize = ptxBuilder.newOperand(selectOp, \"r\");\n         }\n         copyAsyncOp(dstOperand, srcOperand, copySize, srcSize);\n-        ptxBuilder.launch(rewriter, loc, LLVM::LLVMVoidType::get(getContext()));\n+        ptxBuilder.launch(rewriter, loc, void_ty(getContext()));\n       }\n     }\n \n     PTXBuilder ptxBuilder;\n     ptxBuilder.create<PTXCpAsyncCommitGroupInstr>()->operator()();\n-    auto ret =\n-        ptxBuilder.launch(rewriter, loc, LLVM::LLVMVoidType::get(getContext()));\n-    rewriter.replaceOp(op, ret);\n+    ptxBuilder.launch(rewriter, loc, void_ty(getContext()));\n+    rewriter.replaceOp(op, llDst);\n     return success();\n   }\n };\n@@ -4270,21 +4274,39 @@ class ConvertTritonGPUToLLVM\n \n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n \n-    // step 1: Convert FuncOp to LLVMFuncOp via partial conversion\n-    // step 2: Allocate for shared memories\n-    // step 3: Convert the rest of ops via partial conversion\n-    // The reason for a seperation between 1/3 is that, step 2 is out of\n+    // step 1: Allocate shared memories and insert barriers\n+    // setp 2: Convert SCF to CFG\n+    // step 3: Convert FuncOp to LLVMFuncOp via partial conversion\n+    // step 4: Convert the rest of ops via partial conversion\n+    // The reason for putting step 1 before step 2 is that the membar analysis\n+    // currently only supports SCF but not CFG.\n+    // The reason for a seperation between 1/4 is that, step 3 is out of\n     // the scope of Dialect Conversion, thus we need to make sure the smem\n-    // is not revised during the conversion of step 3.\n+    // is not revised during the conversion of step 4.\n+    Allocation allocation(mod);\n+    MembarAnalysis membar(&allocation);\n+\n+    RewritePatternSet scf_patterns(context);\n+    mlir::populateLoopToStdConversionPatterns(scf_patterns);\n+    mlir::ConversionTarget scf_target(*context);\n+    scf_target.addIllegalOp<scf::ForOp, scf::IfOp, scf::ParallelOp,\n+                            scf::WhileOp, scf::ExecuteRegionOp>();\n+    scf_target.markUnknownOpDynamicallyLegal([](Operation *) { return true; });\n+    if (failed(\n+            applyPartialConversion(mod, scf_target, std::move(scf_patterns))))\n+      return signalPassFailure();\n+\n     RewritePatternSet func_patterns(context);\n     func_patterns.add<FuncOpConversion>(typeConverter, numWarps, 1 /*benefit*/);\n     if (failed(\n             applyPartialConversion(mod, funcTarget, std::move(func_patterns))))\n       return signalPassFailure();\n \n-    Allocation allocation(mod);\n     auto axisAnalysis = runAxisAnalysis(mod);\n     initSharedMemory(allocation.getSharedMemorySize(), typeConverter);\n+    mod->setAttr(\"triton_gpu.shared\",\n+                 mlir::IntegerAttr::get(mlir::IntegerType::get(context, 32),\n+                                        allocation.getSharedMemorySize()));\n \n     // We set a higher benefit here to ensure triton's patterns runs before\n     // arith patterns for some encoding not supported by the community\n@@ -4327,9 +4349,11 @@ void ConvertTritonGPUToLLVM::initSharedMemory(\n   OpBuilder b(mod.getBodyRegion());\n   auto loc = mod.getLoc();\n   auto elemTy = typeConverter.convertType(b.getIntegerType(8));\n-  auto arrayTy = LLVM::LLVMArrayType::get(elemTy, size);\n+  // Set array size 0 and external linkage indicates that we use dynamic shared\n+  // allocation to allow a larger shared memory size for each kernel.\n+  auto arrayTy = LLVM::LLVMArrayType::get(elemTy, 0);\n   auto global = b.create<LLVM::GlobalOp>(\n-      loc, arrayTy, /*isConstant=*/false, LLVM::Linkage::Internal,\n+      loc, arrayTy, /*isConstant=*/false, LLVM::Linkage::External,\n       \"global_smem\", /*value=*/Attribute(),\n       /*alignment=*/0, mlir::gpu::GPUDialect::getWorkgroupAddressSpace());\n   SmallVector<LLVM::LLVMFuncOp> funcs;"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 11, "deletions": 1, "changes": 12, "file_content_changes": "@@ -103,16 +103,21 @@ class SimplifyConversion : public mlir::RewritePattern {\n     if (!arg)\n       return mlir::failure();\n     // cvt(alloc_tensor(x), type2) -> alloc_tensor(x, type2)\n-    // cvt(insert_slice(x), type2) -> extract_slice(cvt(x, type2))\n     auto alloc_tensor = dyn_cast<triton::gpu::AllocTensorOp>(arg);\n     if (alloc_tensor) {\n       rewriter.replaceOpWithNewOp<triton::gpu::AllocTensorOp>(\n           op, op->getResult(0).getType());\n       return mlir::success();\n     }\n+    // cvt(insert_slice(x), type2) -> insert_slice(cvt(x, type2))\n     auto insert_slice = dyn_cast<triton::gpu::InsertSliceAsyncOp>(arg);\n     if (insert_slice) {\n       auto newType = op->getResult(0).getType();\n+      // Ensure that the new insert_slice op is placed in the same place as the\n+      // old insert_slice op. Otherwise, the new insert_slice op may be placed\n+      // after the async_wait op, which is not allowed.\n+      OpBuilder::InsertionGuard guard(rewriter);\n+      rewriter.setInsertionPoint(insert_slice);\n       auto new_arg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n           op->getLoc(), newType, insert_slice.dst());\n       rewriter.replaceOpWithNewOp<triton::gpu::InsertSliceAsyncOp>(\n@@ -126,6 +131,11 @@ class SimplifyConversion : public mlir::RewritePattern {\n     auto extract_slice = dyn_cast<triton::gpu::ExtractSliceOp>(arg);\n     if (extract_slice) {\n       auto origType = extract_slice.src().getType().cast<RankedTensorType>();\n+      // Ensure that the new extract_slice op is placed in the same place as the\n+      // old extract_slice op. Otherwise, the new extract_slice op may be placed\n+      // after the async_wait op, which is not allowed.\n+      OpBuilder::InsertionGuard guard(rewriter);\n+      rewriter.setInsertionPoint(extract_slice);\n       auto newType = RankedTensorType::get(\n           origType.getShape(), origType.getElementType(),\n           op->getResult(0).getType().cast<RankedTensorType>().getEncoding());"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 22, "deletions": 0, "changes": 22, "file_content_changes": "@@ -78,6 +78,9 @@ class LoopPipeliner {\n   /// emit pipelined loads (before loop body)\n   void emitPrologue();\n \n+  /// emit pipelined loads (after loop body)\n+  void emitEpilogue();\n+\n   /// create the new ForOp (add new args & insert prefetched ops)\n   scf::ForOp createNewForOp();\n \n@@ -362,6 +365,23 @@ void LoopPipeliner::emitPrologue() {\n         loadStageBuffer[loadOp][numStages - 1], loopIterIdx, /*axis*/ 0);\n     loadsExtract[loadOp] = extractSlice;\n   }\n+  // bump up loopIterIdx, this is used for getting the correct slice for the\n+  // *next* iteration\n+  loopIterIdx = builder.create<arith::AddIOp>(\n+      loopIterIdx.getLoc(), loopIterIdx,\n+      builder.create<arith::ConstantIntOp>(loopIterIdx.getLoc(), 1, 32));\n+}\n+\n+void LoopPipeliner::emitEpilogue() {\n+  // If there's any outstanding async copies, we need to wait for them.\n+  // TODO(Keren): We may want to completely avoid the async copies in the last\n+  // few iterations by setting is_masked attribute to true. We don't want to use\n+  // the mask operand because it's a tensor but not a scalar.\n+  OpBuilder builder(forOp);\n+  OpBuilder::InsertionGuard g(builder);\n+  builder.setInsertionPointAfter(forOp);\n+  Operation *asyncWait =\n+      builder.create<triton::gpu::AsyncWaitOp>(forOp.getLoc(), 0);\n }\n \n scf::ForOp LoopPipeliner::createNewForOp() {\n@@ -581,6 +601,8 @@ struct PipelinePass : public TritonGPUPipelineBase<PipelinePass> {\n \n       scf::ForOp newForOp = pipeliner.createNewForOp();\n \n+      pipeliner.emitEpilogue();\n+\n       // replace the original loop\n       for (unsigned i = 0; i < forOp->getNumResults(); ++i)\n         forOp->getResult(i).replaceAllUsesWith(newForOp->getResult(i));"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -139,10 +139,12 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n       /*printAfterOnlyOnChange=*/true,\n       /*printAfterOnlyOnFailure*/ false, llvm::dbgs(), printingFlags);\n \n-  pm.addPass(mlir::createLowerToCFGPass());\n   pm.addPass(createConvertTritonGPUToLLVMPass());\n   // Conanicalize to eliminate the remaining UnrealizedConversionCastOp\n   pm.addPass(mlir::createCanonicalizerPass());\n+  pm.addPass(mlir::createCSEPass()); // Simplify the IR to improve readability.\n+  pm.addPass(mlir::createSymbolDCEPass());\n+  pm.addPass(mlir::createCanonicalizerPass());\n \n   if (failed(pm.run(module))) {\n     llvm::errs() << \"Pass execution failed\";"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1257,8 +1257,8 @@ void init_triton_translation(py::module &m) {\n   using ret = py::return_value_policy;\n \n   m.def(\"get_shared_memory_size\", [](mlir::ModuleOp module) {\n-    auto pass = std::make_unique<mlir::Allocation>(module);\n-    return pass->getSharedMemorySize();\n+    return module->getAttrOfType<mlir::IntegerAttr>(\"triton_gpu.shared\")\n+        .getInt();\n   });\n \n   m.def("}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -875,7 +875,7 @@ def optimize_tritongpu_ir(mod, num_stages):\n     pm.enable_debug()\n     # Get error in backend due to wrong conversion in expanding async-related instruction.\n     # TODO[Superjomn]: Open it when fixed.\n-    # pm.add_tritongpu_pipeline_pass(num_stages)\n+    pm.add_tritongpu_pipeline_pass(num_stages)\n     pm.add_canonicalizer_pass()\n     pm.add_cse_pass()\n     pm.add_coalesce_pass()"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 23, "deletions": 23, "changes": 46, "file_content_changes": "@@ -326,7 +326,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n #shared0 = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  // CHECK: llvm.mlir.global internal @global_smem\n+  // CHECK: llvm.mlir.global external @global_smem\n   // CHECK-LABEL: basic_alloc_tensor\n   func @basic_alloc_tensor() {\n     // CHECK: llvm.mlir.addressof @global_smem\n@@ -343,7 +343,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n #shared0 = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  // CHECK: llvm.mlir.global internal @global_smem\n+  // CHECK: llvm.mlir.global external @global_smem\n   // CHECK-LABEL: basic_extract_slice\n   func @basic_extract_slice() {\n     // CHECK: %[[BASE0:.*]] = llvm.mlir.addressof @global_smem\n@@ -382,10 +382,10 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #slice2d1 = #triton_gpu.slice<{dim = 1, parent=#block2}>\n #slice3d0 = #triton_gpu.slice<{dim = 0, parent=#block3}>\n #AL = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n-#A = #triton_gpu.shared<{vec = 4, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n+#A = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_insert_slice_async_v4\n-  func @basic_insert_slice_async_v4(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}) {\n+  func @basic_insert_slice_async_v4(%arg0: !tt.ptr<f32> {tt.divisibility = 8 : i32}) {\n     %off0_ = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #slice2d1>\n     %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<64xi32, #slice3d0>\n     %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<16xi32, #slice2d1>) -> tensor<16x1xi32, #block2>\n@@ -404,9 +404,9 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %index = arith.constant 1 : i32\n \n     // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n-    // CHECK-SAME: cp.async.cg.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x80, 0x80\n+    // CHECK-SAME: cp.async.cg.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x10, 0x10\n     // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n-    // CHECK-SAME: cp.async.cg.shared.global.L2::evict_normal [ ${{.*}} + 8 ], [ ${{.*}} + 0 ], 0x80, 0x80\n+    // CHECK-SAME: cp.async.cg.shared.global [ ${{.*}} + 16 ], [ ${{.*}} + 0 ], 0x10, 0x10\n     // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x64x!tt.ptr<f32>, #AL> -> tensor<2x16x64xf32, #A>\n@@ -445,13 +445,13 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %index = arith.constant 1 : i32\n \n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x32x!tt.ptr<f32>, #AL> -> tensor<2x16x32xf32, #A>\n@@ -489,21 +489,21 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %index = arith.constant 1 : i32\n \n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 512 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 2048 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 512 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 2048 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 512 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 2048 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 512 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 2048 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32x!tt.ptr<f32>, #AL> -> tensor<2x32x32xf32, #A>\n@@ -545,7 +545,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [0, 1]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<1088 x i8>\n+  // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_blocked\n   func @convert_layout_blocked_blocked(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n@@ -593,7 +593,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [16, 2], warpsPerCTA = [1, 1], order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<1280 x i8>\n+  // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_blocked_vec\n   func @convert_layout_blocked_blocked_vec(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n@@ -617,7 +617,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<640 x i8>\n+  // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_blocked_multi_rep\n   func @convert_layout_blocked_blocked_multi_rep(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n@@ -682,7 +682,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [1, 0]}>\n #mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<2560 x i8>\n+  // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_mma_block\n   func @convert_layout_mma_blocked(%arg0: tensor<32x16xf32, #mma>) {\n     // CHECK: nvvm.barrier0\n@@ -703,7 +703,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>\n #shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<16384 x i8>\n+  // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_shared\n   func @convert_layout_blocked_shared(%arg0: tensor<128x32xf32, #blocked0>) {\n     // CHECK: llvm.store"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -22,7 +22,7 @@\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n // CHECK: %[[A0:.*]] = triton_gpu.extract_slice %[[A1BUFFER]], %[[CONSTANT_0]]\n // CHECK: %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]], %[[CONSTANT_0]]\n-// CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_0]]\n+// CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n // CHECK:   tt.dot %[[arg_a0]], %[[arg_b0]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n@@ -78,7 +78,7 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n // CHECK:   %[[A0:.*]] = triton_gpu.extract_slice %[[A1BUFFER]], %[[CONSTANT_0]]\n // CHECK:   %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]], %[[CONSTANT_0]]\n-// CHECK:   scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_0]]\n+// CHECK:   scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n // CHECK:     tt.dot %[[arg_a0]], %[[arg_b0]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n@@ -131,7 +131,7 @@ func @matmul_loop_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f\n // CHECK: %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n // CHECK: triton_gpu.async_wait {num = 1 : i32}\n // CHECK: %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]], %[[CONSTANT_0]]\n-// CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_0]]\n+// CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n // CHECK:   tt.dot {{.*}}, %[[arg_b0]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]"}]