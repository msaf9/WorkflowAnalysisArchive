[{"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 17, "deletions": 7, "changes": 24, "file_content_changes": "@@ -430,6 +430,9 @@ struct StoreAsyncOpConversion\n     auto elemTy = srcTy.getElementType();\n \n     auto rank = srcTy.getRank();\n+    // The sotre async op only supports tensor with ranke <= 5.\n+    // Reference:\n+    // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#tensor-dimension-size-and-format\n     assert(rank > 0 && rank <= 5);\n \n     auto moduleOp = op->getParentOfType<ModuleOp>();\n@@ -492,14 +495,14 @@ struct StoreAsyncOpConversion\n                            .cast<RankedTensorType>()\n                            .getShape();\n     auto shapePerCTA = getShapePerCTA(CTASplitNum, tensorShape);\n-    // magic 128 bytes\n+    const uint32_t bytesPerCacheline = 128;\n     uint32_t bytesPerElem = elemTy.getIntOrFloatBitWidth() / 8;\n     uint32_t numBox{1};\n     for (int i = 0; i < rank; ++i) {\n       auto dim = getDimOfOrder(dstOrder, i);\n       auto tNumElems = shapePerCTA[dim];\n-      if (i == 0 && tNumElems * bytesPerElem > 128) {\n-        tNumElems = 128 / bytesPerElem;\n+      if (i == 0 && tNumElems * bytesPerElem > bytesPerCacheline) {\n+        tNumElems = bytesPerCacheline / bytesPerElem;\n         numBox = (shapePerCTA[dim] + tNumElems - 1) / tNumElems;\n       }\n       boxDims.emplace_back(tNumElems);\n@@ -612,6 +615,9 @@ struct StoreAsyncOpConversion\n     auto dstElemTy = dstTensorTy.getElementType();\n \n     auto rank = srcTy.getRank();\n+    // The sotre async op only supports tensor with ranke <= 5.\n+    // Reference:\n+    // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#tensor-dimension-size-and-format\n     assert(rank > 0 && rank <= 5);\n \n     auto moduleOp = op->getParentOfType<ModuleOp>();\n@@ -683,14 +689,14 @@ struct StoreAsyncOpConversion\n         ceil<unsigned>(shapePerCTA[0], instrShape[0] * warpsPerCTA[0]);\n     uint32_t numElemsPerRep = numElems / repM;\n \n-    // magic 128 bytes\n+    const uint32_t bytesPerCacheline = 128;\n     uint32_t bytesPerElem = dstElemTy.getIntOrFloatBitWidth() / 8;\n     uint32_t numBox{1};\n     for (int i = 0; i < rank; ++i) {\n       auto dim = getDimOfOrder(dstOrder, i);\n       auto tNumElems = shapePerCTA[dim];\n-      if (i == 0 && tNumElems * bytesPerElem > 128) {\n-        tNumElems = 128 / bytesPerElem;\n+      if (i == 0 && tNumElems * bytesPerElem > bytesPerCacheline) {\n+        tNumElems = bytesPerCacheline / bytesPerElem;\n         numBox = (shapePerCTA[dim] + tNumElems - 1) / tNumElems;\n       }\n       if (i == 1) {\n@@ -757,7 +763,8 @@ struct StoreAsyncOpConversion\n \n     // rowStride in bytes\n     uint32_t rowStrideInBytes = shapePerCTA[dstOrder[0]] * bytesPerElem;\n-    uint32_t swizzlingByteWidth = std::min<uint32_t>(rowStrideInBytes, 128);\n+    uint32_t swizzlingByteWidth =\n+        std::min<uint32_t>(rowStrideInBytes, bytesPerCacheline);\n \n     unsigned numElemsPerSwizzlingRow = swizzlingByteWidth / bytesPerElem;\n     unsigned leadingDimOffset =\n@@ -1411,6 +1418,9 @@ struct InsertSliceAsyncV2OpConversion\n     auto rank = resultTy.getRank() - 1;\n \n     // TODO: support any valid rank in (3, 4, 5)\n+    // The sotre async op only supports tensor with ranke <= 5.\n+    // Reference:\n+    // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#tensor-dimension-size-and-format\n     assert(rank > 0 && rank <= 5);\n     SmallVector<unsigned> shape;\n     auto axis = op->getAttrOfType<IntegerAttr>(\"axis\").getInt();"}]