[{"filename": "docs/programming-guide/chapter-2/related-work.rst", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -168,7 +168,7 @@ Scheduling languages are, without a doubt, one of the most popular approaches fo\n Limitations\n ++++++++++++\n \n-This ease-of-development comes at a cost. First of all, existing systems that follow this paradigm tend to be noticeably slower than Triton on modern hardware when applicable (e.g., V100/A100 tensor cores w/ equal tile sizes). I do believe that this is not a fundamental issue of scheduling languages -- in the sense that it could probably be solved with more efforts -- but it could mean that these systems are harder to engineer. More importantly, existing scheduling languages generate loops whose bounds and increments cannot depend on surrounding loop indice without at least imposing severe constraints on possible schedules -- if not breaking the system entirely. This is problematic for sparse computations, whose iteration spaces may be irregular.\n+This ease-of-development comes at a cost. First of all, existing systems that follow this paradigm tend to be noticeably slower than Triton on modern hardware when applicable (e.g., V100/A100 tensor cores w/ equal tile sizes). I do believe that this is not a fundamental issue of scheduling languages -- in the sense that it could probably be solved with more efforts -- but it could mean that these systems are harder to engineer. More importantly, existing scheduling languages generate loops whose bounds and increments cannot depend on surrounding loop indices without at least imposing severe constraints on possible schedules -- if not breaking the system entirely. This is problematic for sparse computations, whose iteration spaces may be irregular.\n \n .. table::\n     :widths: 50 50"}, {"filename": "lib/Analysis/Alias.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -44,7 +44,7 @@ ChangeResult SharedMemoryAliasAnalysis::visitOperation(\n   if (pessimistic) {\n     return markAllPessimisticFixpoint(op->getResults());\n   }\n-  // Join all latice elements\n+  // Join all lattice elements\n   ChangeResult result = ChangeResult::NoChange;\n   for (Value value : op->getResults()) {\n     result |= getLatticeElement(value).join(aliasInfo);"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 14, "deletions": 2, "changes": 16, "file_content_changes": "@@ -27,6 +27,9 @@ namespace mlir {\n //===----------------------------------------------------------------------===//\n namespace triton {\n \n+// Bitwidth of pointers\n+constexpr int kPtrBitWidth = 64; \n+\n static std::pair<SmallVector<unsigned>, SmallVector<unsigned>>\n getCvtOrder(const Attribute &srcLayout, const Attribute &dstLayout) {\n   auto srcBlockedLayout = srcLayout.dyn_cast<BlockedEncodingAttr>();\n@@ -161,9 +164,16 @@ class AllocationAnalysis {\n       // TODO(Keren): Reduce with index is not supported yet.\n       auto value = op->getOperand(0);\n       if (auto tensorType = value.getType().dyn_cast<RankedTensorType>()) {\n+        auto srcLayout = tensorType.getEncoding();\n+        bool fastReduce = reduceOp.axis() == getOrder(srcLayout)[0];\n         auto smemShape = getScratchConfigForReduce(reduceOp);\n         unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n                                          std::multiplies{});\n+        if (fastReduce) {\n+          auto mod = op->getParentOfType<ModuleOp>();\n+          unsigned numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+          elems = std::max<unsigned>(elems, numWarps * 32);\n+        }\n         auto bytes = elems * tensorType.getElementTypeBitWidth() / 8;\n         allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n       }\n@@ -186,7 +196,9 @@ class AllocationAnalysis {\n       auto smemShape = getScratchConfigForCvtLayout(cvtLayout, inVec, outVec);\n       unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n                                        std::multiplies{});\n-      auto bytes = elems * srcTy.getElementTypeBitWidth() / 8;\n+      auto bytes = srcTy.getElementType().isa<triton::PointerType>()? \n+                   elems * kPtrBitWidth / 8 :\n+                   elems * srcTy.getElementTypeBitWidth() / 8;\n       allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     }\n   }\n@@ -235,7 +247,7 @@ class AllocationAnalysis {\n     }\n   }\n \n-  /// Extends the liveness range by unioning the liveness range of the aliased\n+  /// Extends the liveness range by unionizing the liveness range of the aliased\n   /// values because each allocated buffer could be an alias of others, if block\n   /// arguments are involved.\n   void resolveAliasBufferLiveness("}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -378,7 +378,7 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n       TritonGenericPattern<triton::AddPtrOp>, TritonReducePattern,\n       TritonExpandDimsPattern, TritonMakeRangePattern, TritonDotPattern,\n       TritonLoadPattern, TritonStorePattern, TritonExtElemwisePattern,\n-      TritonPrintfPattern>(typeConverter, context);\n+      TritonPrintfPattern, TritonAtomicRMWPattern>(typeConverter, context);\n }\n \n //"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -64,8 +64,7 @@ unsigned getElemsPerThread(Attribute layout, ArrayRef<int64_t> shape) {\n }\n \n unsigned getElemsPerThread(Type type) {\n-  if (type.isIntOrIndexOrFloat() ||\n-      type.isa<triton::Float8Type>() ||\n+  if (type.isIntOrIndexOrFloat() || type.isa<triton::Float8Type>() ||\n       type.isa<triton::PointerType>())\n     return 1;\n   auto tensorType = type.cast<RankedTensorType>();\n@@ -372,7 +371,10 @@ unsigned SharedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n \n unsigned\n DotOperandEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n-  assert(0 && \"DotOPerandEncodingAttr::getElemsPerThread not implemented\");\n+  if (auto blockedLayout = getParent().dyn_cast<BlockedEncodingAttr>()) {\n+    return blockedLayout.getElemsPerThread(shape);\n+  }\n+  assert(0 && \"DotOperandEncodingAttr::getElemsPerThread not implemented\");\n   return 0;\n }\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/CanonicalizeLoops.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -24,7 +24,7 @@ struct CanonicalizePass\n     // The following piece of code is a workaround to\n     // very crudely remove dead code, by making an iteration\n     // argument yield itself if it is not used to create\n-    // side-effects anywhere.\n+    // side effects anywhere.\n     getOperation()->walk([&](scf::ForOp forOp) -> void {\n       for (size_t i = 0; i < forOp.getNumResults(); ++i) {\n         // condition 1: no other iter arguments depend on it"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -29,7 +29,7 @@ namespace {\n // convert(blocked, dot_operand) ->\n // convert(blocked, mma) + convert(mma,  dot_operand)\n // if this value is itself the result of a dot operation\n-// this is a heuristic to accomodate some pattern seen in fused attention\n+// this is a heuristic to accommodate some pattern seen in fused attention\n // kernels.\n // TODO: replace this by something more generic, i.e. layout-aware CSE\n class DecomposeDotOperand : public mlir::RewritePattern {\n@@ -81,7 +81,7 @@ class SimplifyConversion : public mlir::RewritePattern {\n     auto srcType = convert.getOperand().getType().cast<RankedTensorType>();\n     auto dstType = convert.getType().cast<RankedTensorType>();\n     // we don't handle conversions to DotOperandEncodingAttr\n-    // this is a heuristics to accomodate fused attention\n+    // this is a heuristics to accommodate fused attention\n     // if (dstType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n     //   return mlir::failure();\n     // convert to the same layout -- we can delete\n@@ -265,7 +265,7 @@ class RematerializeBackward : public mlir::RewritePattern {\n         isSharedEncoding(cvt->getOperand(0)))\n       return mlir::failure();\n     // we don't handle conversions to DotOperandEncodingAttr\n-    // this is a heuristics to accomodate fused attention\n+    // this is a heuristics to accommodate fused attention\n     auto targetType = cvt->getResultTypes()[0].cast<RankedTensorType>();\n     if (targetType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n       return mlir::failure();\n@@ -285,7 +285,7 @@ class RematerializeBackward : public mlir::RewritePattern {\n       // we stop everything\n       if (expensive_to_remat(currOp))\n         break;\n-      // a conversion will be removed here (i.e. transfered to operands)\n+      // a conversion will be removed here (i.e. transferred to operands)\n       numCvts -= 1;\n       // done processing\n       processed.insert(currOp);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -110,7 +110,7 @@ Value LoopPipeliner::lookupOrDefault(Value origin, int stage) {\n }\n \n void LoopPipeliner::collectDeps(Value v, int stages, DenseSet<Value> &deps) {\n-  // Loop-invarant value. skip\n+  // Loop-invariant value, skip\n   if (v.getParentRegion() != &forOp.getLoopBody())\n     return;\n \n@@ -125,7 +125,7 @@ void LoopPipeliner::collectDeps(Value v, int stages, DenseSet<Value> &deps) {\n     collectDeps(yieldOp->getOperand(arg.getArgNumber() - 1), stages - 1, deps);\n   } else { // value\n     // v might be in deps, but we still need to visit v.\n-    // This is because v might depends on value in previous iterations\n+    // This is because v might depend on value in previous iterations\n     deps.insert(v);\n     for (Value op : v.getDefiningOp()->getOperands())\n       collectDeps(op, stages, deps);\n@@ -175,26 +175,26 @@ LogicalResult LoopPipeliner::initialize() {\n   //  other load in the prologue, which is against the point of the pipeline\n   //  pass)\n   for (triton::LoadOp loadOp : allLoads) {\n-    bool isCandiate = true;\n+    bool isCandidate = true;\n     for (triton::LoadOp other : allLoads) {\n       if (loadDeps[loadOp].contains(other)) {\n-        isCandiate = false;\n+        isCandidate = false;\n         break;\n       }\n     }\n \n     // We only pipeline loads that have one covert_layout (to dot_op) use\n     // TODO: lift this constraint in the future\n-    if (isCandiate && loadOp.getResult().hasOneUse()) {\n-      isCandiate = false;\n+    if (isCandidate && loadOp.getResult().hasOneUse()) {\n+      isCandidate = false;\n       Operation *use = *loadOp.getResult().getUsers().begin();\n       if (auto convertLayout = llvm::dyn_cast<ttg::ConvertLayoutOp>(use)) {\n         if (auto tensorType = convertLayout.getResult()\n                                   .getType()\n                                   .dyn_cast<RankedTensorType>()) {\n           if (auto dotOpEnc = tensorType.getEncoding()\n                                   .dyn_cast<ttg::DotOperandEncodingAttr>()) {\n-            isCandiate = true;\n+            isCandidate = true;\n             loadsMapping[loadOp] = convertLayout;\n             auto ty = loadOp.getType().cast<RankedTensorType>();\n             SmallVector<int64_t> bufferShape(ty.getShape().begin(),\n@@ -208,9 +208,9 @@ LogicalResult LoopPipeliner::initialize() {\n         }\n       }\n     } else\n-      isCandiate = false;\n+      isCandidate = false;\n \n-    if (isCandiate)\n+    if (isCandidate)\n       loads.insert(loadOp);\n   }\n \n@@ -317,7 +317,7 @@ void LoopPipeliner::emitPrologue() {\n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n         Value originalResult = op->getResult(dstIdx);\n         // copy_async will update the value of its only use\n-        // TODO: load should no be used in the preheader?\n+        // TODO: load should not be used in the preheader?\n         if (loads.contains(originalResult)) {\n           break;\n           // originalResult = loadsMapping[originalResult];"}, {"filename": "lib/Dialect/TritonGPU/Transforms/TritonGPUConversion.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -35,7 +35,7 @@ TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n   });\n \n   //\n-  // materailizations\n+  // Materializations\n   //\n   // This will be called when (newArgType != origArgType)\n   // This will create newArg, and map(origArg, newArg)"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 10, "deletions": 12, "changes": 22, "file_content_changes": "@@ -184,15 +184,15 @@ void init_triton_ir(py::module &&m) {\n              if (mlir::Operation *definingOp = self.getDefiningOp())\n                definingOp->setAttr(name, attr);\n              else {\n-               /* issue an warning */\n+               /* issue a warning */\n              }\n            })\n       .def(\"replace_all_uses_with\",\n            [](mlir::Value &self, mlir::Value &newValue) {\n              self.replaceAllUsesWith(newValue);\n            });\n \n-  py::class_<mlir::BlockArgument, mlir::Value>(m, \"block_arguement\");\n+  py::class_<mlir::BlockArgument, mlir::Value>(m, \"block_argument\");\n \n   py::class_<mlir::Region>(m, \"region\")\n       .def(\"get_parent_region\", &mlir::Region::getParentRegion, ret::reference)\n@@ -300,7 +300,7 @@ void init_triton_ir(py::module &&m) {\n   py::class_<mlir::scf::WhileOp, mlir::OpState>(m, \"WhileOp\")\n       .def(\"get_before\", &mlir::scf::WhileOp::getBefore, ret::reference)\n       .def(\"get_after\", &mlir::scf::WhileOp::getAfter, ret::reference);\n-  py::class_<mlir::scf::ConditionOp, mlir::OpState>(m, \"CondtionOp\");\n+  py::class_<mlir::scf::ConditionOp, mlir::OpState>(m, \"ConditionOp\");\n \n   // dynamic_attr is used to transfer ownership of the MLIR context to the\n   // module\n@@ -443,7 +443,7 @@ void init_triton_ir(py::module &&m) {\n       .def(\"get_bool_attr\", &mlir::OpBuilder::getBoolAttr)\n       .def(\"get_int32_attr\", &mlir::OpBuilder::getI32IntegerAttr)\n       // Use arith.ConstantOp to create constants\n-      // // Constants\n+      // Constants\n       .def(\"get_int1\",\n            [](mlir::OpBuilder &self, bool v) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n@@ -608,14 +608,14 @@ void init_triton_ir(py::module &&m) {\n              auto loc = self.getUnknownLoc();\n              return self.create<mlir::scf::WhileOp>(loc, retTypes, initArgs);\n            })\n-      .def(\"create_condtion_op\",\n+      .def(\"create_condition_op\",\n            [](mlir::OpBuilder &self, mlir::Value &cond,\n               std::vector<mlir::Value> &args) -> mlir::scf::ConditionOp {\n              auto loc = self.getUnknownLoc();\n              return self.create<mlir::scf::ConditionOp>(loc, cond, args);\n            })\n \n-      // miscellious\n+      // miscellaneous\n       .def(\"create_make_range\",\n            [](mlir::OpBuilder &self, int start, int end) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n@@ -996,15 +996,15 @@ void init_triton_ir(py::module &&m) {\n              auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::OrIOp>(loc, lhs, rhs);\n            })\n-      // // Input/Output\n+      // Input/Output\n       .def(\"create_load\",\n            [](mlir::OpBuilder &self, mlir::Value &ptrs,\n-              mlir::triton::CacheModifier cacheModifer,\n+              mlir::triton::CacheModifier cacheModifier,\n               mlir::triton::EvictionPolicy evictionPolicy,\n               bool isVolatile) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n              return self.create<mlir::triton::LoadOp>(\n-                 loc, ptrs, cacheModifer, evictionPolicy, isVolatile);\n+                 loc, ptrs, cacheModifier, evictionPolicy, isVolatile);\n            })\n       .def(\"create_store\",\n            [](mlir::OpBuilder &self, mlir::Value &ptrs,\n@@ -1106,9 +1106,7 @@ void init_triton_ir(py::module &&m) {\n               mlir::Value &ptr, mlir::Value &val,\n               mlir::Value &mask) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n-             auto ptrType = mlir::getElementTypeOrSelf(ptr)\n-                                .cast<mlir::triton::PointerType>();\n-             mlir::Type dstType = ptrType.getPointeeType();\n+             mlir::Type dstType = val.getType();\n              return self.create<mlir::triton::AtomicRMWOp>(loc, dstType, rmwOp,\n                                                            ptr, val, mask);\n            })"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 19, "deletions": 0, "changes": 19, "file_content_changes": "@@ -671,6 +671,25 @@ def without_fn(X, Y, A, B, C):\n #     kernel[(1,)](z_tri, x_tri, axis, shape0, shape1)\n #     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n \n+def test_tensor_atomic_rmw_add_elementwise(device=\"cuda\"):\n+    shape0, shape1 = 16, 16\n+    @triton.jit\n+    def kernel(Z, X, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n+        off0 = tl.arange(0, SHAPE0)\n+        off1 = tl.arange(0, SHAPE1)\n+        x = tl.load(X + off0[:, None] * SHAPE1 + off1[None, :])\n+        tl.atomic_add(Z + off0[:, None] * SHAPE1 + off1[None, :], x)\n+\n+    rs = RandomState(17)\n+    x = numpy_random((shape0, shape1), dtype_str=\"float32\", rs=rs)\n+    z = numpy_random((shape0, shape1), dtype_str=\"float32\", rs=rs)\n+    # reference\n+    z_ref = z + x\n+    # triton result\n+    x_tri = torch.from_numpy(x).to(device=device)\n+    z_tri = torch.from_numpy(z).to(device=device)\n+    kernel[(1,)](z_tri, x_tri, shape0, shape1)\n+    np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n \n # def test_atomic_cas():\n #     # 1. make sure that atomic_cas changes the original value (Lock)"}, {"filename": "python/tests/test_elementwise.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -61,6 +61,7 @@ def get_tensor(shape, data_type, b_positive=False):\n                              ('sqrt', 'float64', 'float64'),\n                              ('abs', 'float32', 'float32'),\n                              ('exp', 'float32', 'float32'),\n+                             ('exp', 'float64', 'float64'),\n                              ('sigmoid', 'float32', 'float32'),\n                           ])\n def test_single_input(expr, output_type, input0_type):"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 141, "deletions": 101, "changes": 242, "file_content_changes": "@@ -30,18 +30,32 @@ def matmul_no_scf_kernel(\n # TODO: num_warps could only be 4 for now\n \n \n-@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS', [\n-    [128, 256, 32, 4],\n-    [256, 128, 16, 4],\n-    [128, 16, 32, 4],\n-    [32, 128, 64, 4],\n-    [128, 128, 64, 4],\n-    [64, 128, 128, 4],\n-    [64, 128, 128, 2],\n+@pytest.mark.parametrize('SHAPE,NUM_WARPS,TRANS_A,TRANS_B', [\n+    (shape, num_warps, trans_a, trans_b)\n+    for shape in [\n+        [128, 256, 32],\n+        [256, 128, 16],\n+        [128, 16, 32],\n+        [32, 128, 64],\n+        [128, 128, 64],\n+        [64, 128, 128],\n+    ]\n+    for num_warps in [2, 4]\n+    for trans_a in [False, True]\n+    for trans_b in [False, True]\n ])\n-def test_gemm_no_scf(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n-    a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n-    b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+def test_gemm_no_scf(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n+    SIZE_M, SIZE_N, SIZE_K = SHAPE\n+    if (TRANS_A):\n+        a = torch.randn((SIZE_K, SIZE_M), device='cuda', dtype=torch.float16).T\n+    else:\n+        a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n+\n+    if (TRANS_B):\n+        b = torch.randn((SIZE_N, SIZE_K), device='cuda', dtype=torch.float16).T\n+    else:\n+        b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+\n     c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n     grid = lambda META: (1, )\n     matmul_no_scf_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n@@ -55,16 +69,32 @@ def test_gemm_no_scf(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n     assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)\n \n \n-@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS', [\n-    [64, 128, 128, 1],\n-    [128, 128, 128, 4],\n-    [16, 8, 32, 1],\n-    [32, 16, 64, 2],\n-    [32, 16, 64, 4],\n+@pytest.mark.parametrize('SHAPE,NUM_WARPS,TRANS_A,TRANS_B', [\n+    (shape, num_warps, trans_a, trans_b)\n+    for shape in [\n+        [64, 128, 128],\n+        [128, 128, 128],\n+        [16, 8, 32],\n+        [32, 16, 64],\n+        [32, 16, 64],\n+    ]\n+    for num_warps in [1, 2, 4]\n+    for trans_a in [False, True]\n+    for trans_b in [False, True]\n ])\n-def test_gemm_no_scf_int8(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n-    a = torch.randint(-5, 5, (SIZE_M, SIZE_K), device='cuda', dtype=torch.int8)\n-    b = torch.randint(-5, 5, (SIZE_K, SIZE_N), device='cuda', dtype=torch.int8)\n+def test_gemm_no_scf_int8(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n+    SIZE_M, SIZE_N, SIZE_K = SHAPE\n+\n+    if (TRANS_A):\n+        a = torch.randint(-5, 5, (SIZE_K, SIZE_M), device='cuda', dtype=torch.int8).T\n+    else:\n+        a = torch.randint(-5, 5, (SIZE_M, SIZE_K), device='cuda', dtype=torch.int8)\n+\n+    if (TRANS_B):\n+        b = torch.randint(-5, 5, (SIZE_N, SIZE_K), device='cuda', dtype=torch.int8).T\n+    else:\n+        b = torch.randint(-5, 5, (SIZE_K, SIZE_N), device='cuda', dtype=torch.int8)\n+\n     c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.int32)\n \n     grid = lambda META: (1, )\n@@ -125,28 +155,39 @@ def get_variant_golden(a, b):\n     return c_padded[:SIZE_M, :SIZE_N]\n \n \n-@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K', [\n+@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,TRANS_A,TRANS_B', [\n     # Non-forloop\n-    [64, 32, 64, 4, 64, 32, 64],\n-    [128, 64, 128, 4, 128, 64, 128],\n+    [64, 32, 64, 4, 64, 32, 64, False, False],\n+    [128, 64, 128, 4, 128, 64, 128, False, False],\n     # K-Forloop\n-    [64, 32, 128, 4, 64, 32, 64],\n-    [128, 16, 128, 4, 128, 16, 32],\n-    [32, 16, 128, 4, 32, 16, 32],\n-    [32, 64, 128, 4, 32, 64, 32],\n-    [32, 128, 256, 4, 32, 128, 64],\n-    [64, 128, 64, 4, 64, 128, 32],\n-    [64, 64, 128, 4, 64, 64, 32],\n-    [128, 128, 64, 4, 128, 128, 32],\n-    [128, 128, 128, 4, 128, 128, 32],\n-    [128, 128, 256, 4, 128, 128, 64],\n-    [128, 256, 128, 4, 128, 256, 32],\n-    [256, 128, 64, 4, 256, 128, 16],\n-    [128, 64, 128, 4, 128, 64, 32],\n+    [64, 32, 128, 4, 64, 32, 64, False, False],\n+    [128, 16, 128, 4, 128, 16, 32, False, False],\n+    [32, 16, 128, 4, 32, 16, 32, False, False],\n+    [32, 64, 128, 4, 32, 64, 32, False, False],\n+    [32, 128, 256, 4, 32, 128, 64, False, False],\n+    [64, 128, 64, 4, 64, 128, 32, False, False],\n+    [64, 64, 128, 4, 64, 64, 32, False, False],\n+    [128, 128, 64, 4, 128, 128, 32, False, False],\n+    [128, 128, 128, 4, 128, 128, 32, False, False],\n+    [128, 128, 256, 4, 128, 128, 64, False, False],\n+    [128, 256, 128, 4, 128, 256, 32, False, False],\n+    [256, 128, 64, 4, 256, 128, 16, False, False],\n+    [128, 64, 128, 4, 128, 64, 32, False, False],\n+    # TODO[goostavz]: fix these cases\n+    #[128, 64, 128, 4, 128, 64, 32, True, False],\n+    #[128, 64, 128, 4, 128, 64, 32, False, True],\n ])\n-def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K):\n-    a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n-    b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B):\n+    if (TRANS_A):\n+        a = torch.randn((SIZE_K, SIZE_M), device='cuda', dtype=torch.float16).T\n+    else:\n+        a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n+\n+    if (TRANS_B):\n+        b = torch.randn((SIZE_N, SIZE_K), device='cuda', dtype=torch.float16).T\n+    else:\n+        b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+\n     c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n     grid = lambda META: (1, )\n     matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n@@ -171,65 +212,64 @@ def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLO\n     assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err), check_dtype=False)\n \n \n-# XXX(Keren): Temporarily disable this test until we have shared -> dot conversion implemented\n-#@pytest.mark.parametrize('M,N,K,num_warps,block_M,block_N,block_K', [\n-#    [32, 32, 16, 4, 32, 32, 16],\n-#    [32, 16, 16, 4, 32, 32, 16],\n-#    [128, 8, 8, 4, 32, 32, 16],\n-#    [127, 41, 43, 4, 32, 32, 16],\n-#])\n-#def test_gemm_fmadot(M, N, K, num_warps, block_M, block_N, block_K):\n-#    @triton.jit\n-#    def matmul_kernel(\n-#        a_ptr, b_ptr, c_ptr,\n-#        M, N, K,\n-#        stride_am, stride_ak,\n-#        stride_bk, stride_bn,\n-#        stride_cm, stride_cn,\n-#        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n-#    ):\n-#        pid = tl.program_id(axis=0)\n-#        # num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n-#        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n-#        pid_m = pid // num_pid_n\n-#        pid_n = pid % num_pid_n\n-#\n-#        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n-#        offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n-#        offs_k = tl.arange(0, BLOCK_SIZE_K)\n-#        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n-#        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n-#\n-#        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-#        for k in range(0, K, BLOCK_SIZE_K):\n-#            a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < K)\n-#            b_mask = (offs_k[:, None] < K) & (offs_bn[None, :] < N)\n-#            a = tl.load(a_ptrs, a_mask)\n-#            b = tl.load(b_ptrs, b_mask)\n-#            # NOTE the allow_tf32 should be false to force the dot op to do fmadot lowering\n-#            accumulator += tl.dot(a, b, allow_tf32=False)\n-#            a_ptrs += BLOCK_SIZE_K * stride_ak\n-#            b_ptrs += BLOCK_SIZE_K * stride_bk\n-#            offs_k += BLOCK_SIZE_K\n-#\n-#        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n-#        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n-#        c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n-#        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n-#        tl.store(c_ptrs, accumulator, c_mask)\n-#\n-#    a = torch.randn((M, K), device='cuda', dtype=torch.float32)\n-#    b = torch.randn((K, N), device='cuda', dtype=torch.float32)\n-#    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n-#\n-#    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n-#    matmul_kernel[grid](a, b, c,\n-#                        M, N, K,\n-#                        stride_am=a.stride(0), stride_ak=a.stride(1),\n-#                        stride_bk=b.stride(0), stride_bn=b.stride(1),\n-#                        stride_cm=c.stride(0), stride_cn=c.stride(1),\n-#                        BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N, BLOCK_SIZE_K=block_K)\n-#\n-#    golden = torch.matmul(a, b)\n-#    torch.testing.assert_close(c, golden)\n-#\n+@pytest.mark.parametrize('M,N,K,num_warps,block_M,block_N,block_K', [\n+    [32, 32, 16, 4, 32, 32, 16],\n+    [32, 16, 16, 4, 32, 32, 16],\n+    [128, 8, 8, 4, 32, 32, 16],\n+    # TODO[Superjomn]: fix it later\n+    # [127, 41, 43, 4, 32, 32, 16],\n+])\n+def test_gemm_fmadot(M, N, K, num_warps, block_M, block_N, block_K):\n+    @triton.jit\n+    def matmul_kernel(\n+        a_ptr, b_ptr, c_ptr,\n+        M, N, K,\n+        stride_am, stride_ak,\n+        stride_bk, stride_bn,\n+        stride_cm, stride_cn,\n+        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+    ):\n+        pid = tl.program_id(axis=0)\n+        # num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n+        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+        pid_m = pid // num_pid_n\n+        pid_n = pid % num_pid_n\n+\n+        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+        offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+        offs_k = tl.arange(0, BLOCK_SIZE_K)\n+        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n+        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n+\n+        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+        for k in range(0, K, BLOCK_SIZE_K):\n+            a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < K)\n+            b_mask = (offs_k[:, None] < K) & (offs_bn[None, :] < N)\n+            a = tl.load(a_ptrs, a_mask)\n+            b = tl.load(b_ptrs, b_mask)\n+            # NOTE the allow_tf32 should be false to force the dot op to do fmadot lowering\n+            accumulator += tl.dot(a, b, allow_tf32=False)\n+            a_ptrs += BLOCK_SIZE_K * stride_ak\n+            b_ptrs += BLOCK_SIZE_K * stride_bk\n+            offs_k += BLOCK_SIZE_K\n+\n+        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+        c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n+        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n+        tl.store(c_ptrs, accumulator, c_mask)\n+\n+    a = torch.randn((M, K), device='cuda', dtype=torch.float32)\n+    b = torch.randn((K, N), device='cuda', dtype=torch.float32)\n+    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n+\n+    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n+    matmul_kernel[grid](a, b, c,\n+                        M, N, K,\n+                        stride_am=a.stride(0), stride_ak=a.stride(1),\n+                        stride_bk=b.stride(0), stride_bn=b.stride(1),\n+                        stride_cm=c.stride(0), stride_cn=c.stride(1),\n+                        BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N, BLOCK_SIZE_K=block_K)\n+\n+    golden = torch.matmul(a, b)\n+    torch.testing.assert_close(c, golden)"}, {"filename": "python/tests/test_vecadd.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -131,7 +131,7 @@ def kernel(x_ptr,\n \n def vecadd_fcmp_no_scf_tester(num_warps, block_size, shape):\n     '''\n-    vecadd tester with float comparation as load/store mask.\n+    vecadd tester with float comparison as load/store mask.\n     '''\n     @triton.jit\n     def kernel(x_ptr,"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -523,8 +523,8 @@ def visit_While(self, node):\n                                                                  [ty.to_ir(self.builder) for ty in ret_types])\n             cond_block.merge_block_before(before_block)\n             self.builder.set_insertion_point_to_end(before_block)\n-            # create CondtionOp: e.g., scf.condition(%cond) %arg0, %arg1, ...\n-            self.builder.create_condtion_op(cond.handle, [before_block.arg(i) for i in range(len(init_args))])\n+            # create ConditionOp: e.g., scf.condition(%cond) %arg0, %arg1, ...\n+            self.builder.create_condition_op(cond.handle, [before_block.arg(i) for i in range(len(init_args))])\n             # merge the loop body\n             after_block = self.builder.create_block_with_parent(while_op.get_after(),\n                                                                 [ty.to_ir(self.builder) for ty in ret_types])\n@@ -910,7 +910,7 @@ def llir_to_ptx(mod: Any, compute_capability: int = None, ptx_version: int = Non\n     :param mod: a TritonGPU dialect module\n     :return:\n         - PTX code\n-        - shared memory alloaction size\n+        - shared memory allocation size\n     '''\n     if compute_capability is None:\n         device = torch.cuda.current_device()\n@@ -1197,7 +1197,7 @@ def put(self, data, filename, binary=True):\n             os.rename(filepath + \".tmp\", filepath)\n \n \n-# utilties for generating and compiling C wrappers\n+# Utilities for generating and compiling C wrappers\n \n \n @functools.lru_cache()"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -768,7 +768,7 @@ def dot(input, other, allow_tf32=True, trans_a=False, trans_b=False, _builder=No\n     \"\"\"\n     Returns the matrix product of two blocks.\n \n-    The two blocks must be two dimensionals and have compatible inner dimensions.\n+    The two blocks must be two-dimensional and have compatible inner dimensions.\n \n     :param input: The first tensor to be multiplied.\n     :type input: 2D tensor of scalar-type in {:code:`float16`, :code:`bfloat16`, :code:`float32`}\n@@ -1172,7 +1172,7 @@ def ravel(x):\n @triton.jit\n def swizzle2d(i, j, size_i, size_j, size_g):\n     \"\"\"\n-    transformes indices of a row-major size_i*size_j matrix into those\n+    Transforms indices of a row-major size_i*size_j matrix into those\n     of one where indices are row major for each group of size_j rows.\n     For example, for size_i = size_j = 4 and size_g = 2, it will transform\n     [[0 , 1 , 2 , 3 ],"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -7,12 +7,12 @@\n \n \n # Create custom exception that prints message \"hello\"\n-class IncompatibleTypeErrorimpl(Exception):\n+class IncompatibleTypeErrorImpl(Exception):\n     def __init__(self, type_a, type_b):\n         self.type_a = type_a\n         self.type_b = type_b\n         self.message = \"invalid operands of type \" + self.type_a.__repr__() + \" and \" + self.type_b.__repr__()\n-        super(IncompatibleTypeErrorimpl, self).__init__(self.message)\n+        super(IncompatibleTypeErrorImpl, self).__init__(self.message)\n \n \n # ===----------------------------------------------------------------------===##\n@@ -88,13 +88,13 @@ def computation_type_impl(a_ty: tl.dtype, b_ty: tl.dtype, div_or_mod: bool) -> t\n def check_ptr_type_impl(type_a: tl.dtype, type_b: tl.dtype, allow_ptr_a: bool) -> None:\n     if type_a.is_ptr():\n         if not allow_ptr_a:\n-            raise IncompatibleTypeErrorimpl(type_a, type_b)\n+            raise IncompatibleTypeErrorImpl(type_a, type_b)\n         # T* + U* with T != U\n         if type_b.is_ptr() and (type_a != type_b):\n-            raise IncompatibleTypeErrorimpl(type_a, type_b)\n+            raise IncompatibleTypeErrorImpl(type_a, type_b)\n         # T* + float\n         if type_b.is_floating():\n-            raise IncompatibleTypeErrorimpl(type_a, type_b)\n+            raise IncompatibleTypeErrorImpl(type_a, type_b)\n \n \n def binary_op_type_checking_impl(lhs: tl.tensor,\n@@ -223,7 +223,7 @@ def fdiv(input: tl.tensor,\n     input_scalar_ty = input.type.scalar\n     other_scalar_ty = other.type.scalar\n     if not input_scalar_ty.is_floating() or not other_scalar_ty.is_floating():\n-        raise ValueError(\"both operands of fdiv must have floating poscalar type\")\n+        raise ValueError(\"both operands of fdiv must have floating scalar type\")\n     input, other = binary_op_type_checking_impl(input, other, builder, False, False, False, True)\n     ret = builder.create_fdiv(input.handle, other.handle)\n     return tl.tensor(ret, input.type)\n@@ -262,7 +262,7 @@ def bitwise_op_type_checking_impl(input: tl.tensor,\n     input_sca_ty = input.type.scalar\n     other_sca_ty = other.type.scalar\n     if not input_sca_ty.is_int() or not other_sca_ty.is_int():\n-        raise IncompatibleTypeErrorimpl(input_sca_ty, other_sca_ty)\n+        raise IncompatibleTypeErrorImpl(input_sca_ty, other_sca_ty)\n     ret_sca_ty = integer_promote_impl(input_sca_ty, other_sca_ty)\n     if ret_sca_ty != input_sca_ty:\n         input = cast(input, ret_sca_ty, builder)"}, {"filename": "python/tutorials/01-vector-add.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -63,7 +63,7 @@ def add(x: torch.Tensor, y: torch.Tensor):\n     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n     # NOTE:\n     #  - each torch.tensor object is implicitly converted into a pointer to its first element.\n-    #  - `triton.jit`'ed functions can be index with a launch grid to obtain a callable GPU kernel\n+    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel\n     #  - don't forget to pass meta-parameters as keywords arguments\n     add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n     # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still"}, {"filename": "python/tutorials/02-fused-softmax.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -80,7 +80,7 @@ def softmax_kernel(\n     row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n     # Subtract maximum for numerical stability\n     row_minus_max = row - tl.max(row, axis=0)\n-    # Note that exponentials in Triton are fast but approximate (i.e., think __expf in CUDA)\n+    # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n     numerator = tl.exp(row_minus_max)\n     denominator = tl.sum(numerator, axis=0)\n     softmax_output = numerator / denominator\n@@ -188,4 +188,4 @@ def benchmark(M, N, provider):\n #\n #  - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.\n #  - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.\n-#    Note however that the PyTorch `softmax` operation is more general and will works on tensors of any shape.\n+#    Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape."}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -15,7 +15,7 @@\n @triton.jit\n def _fwd_kernel(\n     Q, K, V, sm_scale,\n-    TMP, L, M,  # NOTE: TMP is a scratchpad buffer to workaround a compiler bug\n+    TMP, L, M,  # NOTE: TMP is a scratchpad buffer to work around a compiler bug\n     Out,\n     stride_qz, stride_qh, stride_qm, stride_qk,\n     stride_kz, stride_kh, stride_kn, stride_kk,"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -9,6 +9,8 @@\n #A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n #B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n \n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n // CHECK-LABEL: matmul_loop\n func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n@@ -313,3 +315,5 @@ func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n   return\n   // CHECK-NEXT: size = 40960\n }\n+\n+}"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -9,6 +9,8 @@\n #A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n #B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n \n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n // CHECK-LABEL: matmul_loop\n // There shouldn't be any membar with the dot op encoding.\n func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n@@ -250,3 +252,5 @@ func @for_alias(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B :\n   %cst3 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n   return\n }\n+\n+}"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 74, "deletions": 11, "changes": 85, "file_content_changes": "@@ -742,6 +742,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n }\n \n // -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n@@ -753,8 +754,8 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   }\n }\n \n-\n // -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n@@ -766,8 +767,25 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   }\n }\n \n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: convert_blocked_to_blocked_ptr\n+  func @convert_blocked_to_blocked_ptr(%src:tensor<32x!tt.ptr<f32>, #blocked0>) {\n+    // CHECK: llvm.ptrtoint\n+    // CHECK: llvm.store\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.inttoptr\n+    // CHECK-COUNT-4: llvm.insertvalue\n+    %cvt = triton_gpu.convert_layout %src : (tensor<32x!tt.ptr<f32>, #blocked0>) -> tensor<32x!tt.ptr<f32>, #blocked1>\n+    return\n+  }\n+}\n \n // -----\n+\n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n #shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n #mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n@@ -820,23 +838,26 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n #shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n-#mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n-#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n-#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#blocked}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#blocked}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   func @matmul_fmadot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %a:tensor<32x16xf32, #shared>, %b:tensor<16x32xf32, #shared>) {\n-    // We are going to completely depracate using shared layout for operands of dot\n-    //%cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #blocked>\n-    //%28 = tt.dot %a, %b, %cst {allowTF32 = false, transA = false, transB = false} : tensor<32x16xf32, #shared> * tensor<16x32xf32, #shared> -> tensor<32x32xf32, #blocked>\n-    //%30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n-    //%36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x32x!tt.ptr<f32>, #blocked>\n-    //tt.store %36, %28 : tensor<32x32xf32, #blocked>\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #blocked>\n+    // CHECK: llvm.intr.fmuladd\n+    %a_mat = triton_gpu.convert_layout %a : (tensor<32x16xf32, #shared>) -> tensor<32x16xf32, #dot_operand_a>\n+    %b_mat = triton_gpu.convert_layout %b : (tensor<16x32xf32, #shared>) -> tensor<16x32xf32, #dot_operand_b>\n+\n+    %28 = tt.dot %a_mat, %b_mat, %cst {allowTF32 = false, transA = false, transB = false} : tensor<32x16xf32, #dot_operand_a> * tensor<16x32xf32, #dot_operand_b> -> tensor<32x32xf32, #blocked>\n+    %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n+    %36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x32x!tt.ptr<f32>, #blocked>\n+    tt.store %36, %28 : tensor<32x32xf32, #blocked>\n     return\n   }\n }\n \n // -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: atomic_add_f32\n@@ -846,4 +867,46 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32} : (tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xf32, #blocked0>, tensor<256xi1, #blocked0>) -> tensor<256xf32, #blocked0>\n     return\n   }\n-}\n\\ No newline at end of file\n+}\n+\n+// -----\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n+func @test_get_program_id(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n+  %blockidx = tt.get_program_id {axis=0:i32} : i32\n+  %blockidy = tt.get_program_id {axis=1:i32} : i32\n+  %blockidz = tt.get_program_id {axis=2:i32} : i32\n+  // CHECK: nvvm.read.ptx.sreg.ctaid.x\n+  // CHECK: nvvm.read.ptx.sreg.ctaid.y\n+  // CHECK: nvvm.read.ptx.sreg.ctaid.z\n+  %v0 = arith.addi %blockidx, %blockidy : i32\n+  %v1 = arith.addi %v0, %blockidz : i32\n+  %0 = tt.splat %v1 : (i32) -> tensor<32xi32, #blocked0>\n+  tt.store %a, %0 : tensor<32xi32, #blocked0>\n+\n+  return\n+}\n+\n+}\n+\n+// -----\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n+func @test_get_num_program(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n+  // CHECK: nvvm.read.ptx.sreg.ntid.x\n+  // CHECK: nvvm.read.ptx.sreg.ntid.y\n+  // CHECK: nvvm.read.ptx.sreg.ntid.z\n+  %blockdimx = tt.get_num_programs {axis=0:i32} : i32\n+  %blockdimy = tt.get_num_programs {axis=1:i32} : i32\n+  %blockdimz = tt.get_num_programs {axis=2:i32} : i32\n+  %v0 = arith.addi %blockdimx, %blockdimy : i32\n+  %v1 = arith.addi %v0, %blockdimz : i32\n+  %0 = tt.splat %v1 : (i32) -> tensor<32xi32, #blocked0>\n+  tt.store %a, %0 : tensor<32xi32, #blocked0>\n+\n+  return\n+}\n+\n+}"}]