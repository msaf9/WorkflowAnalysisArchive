[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -21,7 +21,7 @@ jobs:\n \n       - name: Clear cache\n         run: |\n-          rm -r /tmp/triton/\n+          rm -r ~/.triton/\n         continue-on-error: true\n \n       - name: Install Triton"}, {"filename": "include/triton/codegen/analysis/layout.h", "status": "modified", "additions": 21, "deletions": 1, "changes": 22, "file_content_changes": "@@ -308,21 +308,40 @@ class layouts {\n \n   void create(size_t id, const std::vector<ir::value*>& values);\n \n-public:\n+  void create_tmp_layout(size_t id, data_layout* arg,\n+                         const std::vector<int>& axes,\n+                         const std::vector<unsigned>& shape,\n+                         ir::instruction* i,\n+                         bool is_index = false);\n+\n+ public:\n   // constructor\n   layouts(analysis::axes *axes, analysis::align *align, size_t num_warps, target* tgt);\n \n   // accessors\n   unsigned layout_of(ir::value *value) const                  { return groups_.at(value); }\n   bool has(ir::value* value) const { return groups_.find(value) != groups_.end(); }\n+  bool has(size_t id)                                         { return layouts_.find(id) != layouts_.end(); }\n   const std::vector<ir::value*>& values_of(unsigned id) const { return values_.at(id); }\n   size_t num_layouts() const                                  { return values_.size();}\n   data_layout* get(size_t id)                                 { return layouts_.at(id); }\n   data_layout* get(ir::value *v)                              { return get(layout_of(v));}\n   std::map<size_t, data_layout*> &get_all()                   { return layouts_; }\n   bool has_tmp(ir::value* i)                                  { return tmp_.find(i) != tmp_.end(); }\n   int tmp(ir::value* i)                                       { return tmp_.at(i);}\n+  int has_tmp_index(ir::value* i)                             { return tmp_index_.find(i) != tmp_index_.end(); }\n+  int tmp_index(ir::value* i)                                 { return tmp_index_.at(i);}\n   void copy(ir::value* dst, ir::value* src)                   { groups_[dst] = groups_[src]; }\n+\n+  // layout checkers\n+  bool is_scanline(ir::instruction* i);\n+\n+  bool is_coalesced_scanline(ir::instruction* i);\n+\n+  bool is_mma(ir::instruction* i);\n+\n+  bool is_a100_mma(ir::instruction* i);\n+\n   // execution\n   void run(ir::module &mod);\n \n@@ -336,6 +355,7 @@ class layouts {\n   std::map<size_t, std::vector<ir::value*>> values_;\n   std::map<size_t, data_layout*> layouts_;\n   std::map<ir::value*, size_t> tmp_;\n+  std::map<ir::value*, size_t> tmp_index_;\n };\n \n }"}, {"filename": "include/triton/codegen/selection/generator.h", "status": "modified", "additions": 10, "deletions": 4, "changes": 14, "file_content_changes": "@@ -118,8 +118,15 @@ class generator: public ir::visitor, public analysis::layout_visitor {\n   llvm::Attribute cvt(ir::attribute attr);\n   void packed_type(ir::value* i);\n   void forward_declare(ir::function* fn);\n+  Value *cast_shared_layout_ptr(analysis::data_layout *layout, Type *ty);\n \n-public:\n+ private:\n+  typedef std::function<void(\n+      std::pair<Value *, Value *> &acc, std::function<Value *()> load_value_fn,\n+      std::function<Value *()> load_index_fn, bool is_first)>\n+      acc_fn_t;\n+\n+ public:\n   generator(analysis::axes *a_axes,\n             analysis::layouts *layouts,\n             analysis::align *alignment,\n@@ -176,9 +183,8 @@ class generator: public ir::visitor, public analysis::layout_visitor {\n   void visit_trans_inst(ir::trans_inst*);\n   void visit_sqrt_inst(ir::sqrt_inst*);\n   Value* shfl_sync(Value* acc, int32_t i);\n-  void visit_reduce1d_inst(ir::reduce_inst*, std::function<Value*(Value*,Value*)>, Value*);\n-  void visit_reducend_inst_fast(ir::reduce_inst* x, std::function<Value*(Value*,Value*)> do_acc, Value *neutral);\n-  void visit_reducend_inst(ir::reduce_inst*, std::function<Value*(Value*,Value*)>, Value*);\n+  void visit_reducend_inst_fast(ir::reduce_inst* x, acc_fn_t do_acc, Value *neutral);\n+  void visit_reducend_inst(ir::reduce_inst* x, acc_fn_t do_acc, Value *neutral);\n   void visit_reduce_inst(ir::reduce_inst*);\n   void visit_select_inst(ir::select_inst*);\n   void visit_layout_convert(ir::value *out, ir::value *in);"}, {"filename": "include/triton/ir/instructions.h", "status": "modified", "additions": 11, "deletions": 2, "changes": 13, "file_content_changes": "@@ -914,7 +914,9 @@ class reduce_inst: public builtin_inst {\n public:\n   enum op_t{\n     ADD, SUB, MAX, MIN, UMAX, UMIN,\n+    ARGMAX, ARGMIN, ARGUMAX, ARGUMIN,\n     FADD, FSUB, FMAX, FMIN,\n+    ARGFMAX, ARGFMIN,\n     XOR\n   };\n \n@@ -932,12 +934,19 @@ class reduce_inst: public builtin_inst {\n   static instruction* create(value *arg, op_t op, unsigned axis, const std::string &name = \"\", instruction *next = nullptr);\n   unsigned get_axis() const { return axis_; }\n   op_t get_op() const { return op_; }\n+  bool with_index() const {\n+    return with_index_ops_.find(op_) != with_index_ops_.end();\n+  }\n \n private:\n-  unsigned axis_;\n-  op_t op_;\n+ const static inline std::set<op_t> with_index_ops_ = {\n+     op_t::ARGMAX,  op_t::ARGMIN,  op_t::ARGUMAX,\n+     op_t::ARGUMIN, op_t::ARGFMAX, op_t::ARGFMIN};\n+ unsigned axis_;\n+ op_t op_;\n };\n \n+\n class select_inst: public builtin_inst {\n private:\n   select_inst(value *pred, value *if_value, value *else_value, const std::string& name, instruction* next);"}, {"filename": "lib/codegen/analysis/layout.cc", "status": "modified", "additions": 54, "deletions": 12, "changes": 66, "file_content_changes": "@@ -588,6 +588,45 @@ void layouts::create(size_t id, const std::vector<ir::value*>& values) {\n   }\n }\n \n+// layout checkers\n+bool layouts::is_scanline(ir::instruction *i) {\n+  return this->get(i->get_operand(0))->to_scanline() != nullptr;\n+}\n+\n+bool layouts::is_coalesced_scanline(ir::instruction *i) {\n+  if (auto *red = dynamic_cast<ir::reduce_inst *>(i)) {\n+    auto *scanline = this->get(i->get_operand(0))->to_scanline();\n+    return scanline && scanline->get_order()[0] == red->get_axis();\n+  }\n+  return false;\n+}\n+\n+bool layouts::is_mma(ir::instruction *i) {\n+  return this->get(i->get_operand(0))->to_mma() != nullptr;\n+}\n+\n+bool layouts::is_a100_mma(ir::instruction *i) {\n+  if (auto *red = dynamic_cast<ir::reduce_inst *>(i)) {\n+    return is_mma(red) && (tgt_->as_nvidia()->sm() >= 80) &&\n+           (red->get_axis() == 1);\n+  }\n+  return false;\n+}\n+\n+void layouts::create_tmp_layout(size_t id, data_layout *arg,\n+                                const std::vector<int> &axes,\n+                                const std::vector<unsigned> &shape,\n+                                ir::instruction *i, bool is_index) {\n+  ir::type *ty = is_index ? ir::type::get_int32_ty(i->get_type()->get_context())\n+                          : i->get_type()->get_scalar_ty();\n+  layouts_[id] = new shared_layout(arg, axes, shape, {i}, ty, align_, tgt_);\n+  if (is_index) {\n+    tmp_index_[i] = id;\n+  } else {\n+    tmp_[i] = id;\n+  }\n+}\n+\n void layouts::run(ir::module &mod) {\n   // make graph\n   graph_.clear();\n@@ -612,22 +651,26 @@ void layouts::run(ir::module &mod) {\n //    std::cout << \"layout: \" << std::endl;\n //    i->print(std::cout);\n     if(auto *red = dynamic_cast<ir::reduce_inst*>(i)) {\n-      id++;\n       ir::value *arg = red->get_operand(0);\n-      unsigned axis = red->get_axis();\n+      distributed_layout *layout =\n+          dynamic_cast<analysis::distributed_layout *>(get(arg));\n       // shape\n       auto shapes = arg->get_type()->get_block_shapes();\n-      distributed_layout* layout = dynamic_cast<analysis::distributed_layout*>(get(arg));\n-      shapes[axis] = layout->shape_per_cta(axis) / layout->contig_per_thread(axis);\n-      \n+      unsigned axis = red->get_axis();\n+      shapes[axis] =\n+          layout->shape_per_cta(axis) / layout->contig_per_thread(axis);\n       // create layout\n-      layouts_[id] = new shared_layout(layout, axes_->get(arg), shapes, {red}, red->get_type()->get_scalar_ty(), align_, tgt_);\n-      tmp_[red] = id;\n+      id++;\n+      create_tmp_layout(id, layout, axes_->get(arg), shapes, red);\n+\n+      if (red->with_index()) {\n+        id++;\n+        create_tmp_layout(id, layout, axes_->get(arg), shapes, red, true);\n+      }\n     }\n     if(auto *val = dynamic_cast<ir::cvt_layout_inst*>(i)){\n       distributed_layout* out_layout = dynamic_cast<distributed_layout*>(get(val));\n       distributed_layout* in_layout = dynamic_cast<distributed_layout*>(get(i->get_operand(0)));\n-      id++;\n       size_t dim = val->get_type()->get_tile_rank();\n       ir::type::block_shapes_t shape(dim);\n       for(size_t k = 0; k < dim; k++){\n@@ -640,13 +683,12 @@ void layouts::run(ir::module &mod) {\n       int out_vec = out_layout->contig_per_thread(out_ord[0]);\n       int pad = std::max(in_vec, out_vec);\n       shape[out_ord[0]] += pad;\n-      layouts_[id] = new shared_layout(out_layout, axes_->get(val), shape, {val}, val->get_type()->get_scalar_ty(), align_, tgt_);\n-      tmp_[val] = id;\n+      id++;\n+      create_tmp_layout(id, out_layout, axes_->get(val), shape, val);\n     }\n     if(auto *atom = dynamic_cast<ir::atomic_inst*>(i)){\n       id++;\n-      layouts_[id] = new shared_layout(nullptr, {}, {1}, {atom}, atom->get_type()->get_scalar_ty(), align_, tgt_);\n-      tmp_[atom] = id;\n+      create_tmp_layout(id, nullptr, {}, {1}, atom);\n     }\n   });\n "}, {"filename": "lib/codegen/selection/generator.cc", "status": "modified", "additions": 141, "deletions": 58, "changes": 199, "file_content_changes": "@@ -112,6 +112,8 @@ Value* geper::operator()(Value *ptr, Value* off, const std::string& name){\n #define extract_val(...)     builder_->CreateExtractValue(__VA_ARGS__)\n #define fadd(...)            builder_->CreateFAdd(__VA_ARGS__)\n #define fcmp(...)            builder_->CreateFCmp(__VA_ARGS__)\n+#define fcmp_oge(...)        builder_->CreateFCmpOGE(__VA_ARGS__)\n+#define fcmp_ole(...)        builder_->CreateFCmpOLE(__VA_ARGS__)\n #define fmul(...)            builder_->CreateFMul(__VA_ARGS__)\n #define fpcast(...)          builder_->CreateFPCast(__VA_ARGS__)\n #define fsub(...)            builder_->CreateFSub(__VA_ARGS__)\n@@ -2334,15 +2336,15 @@ inline Value* generator::shfl_sync(Value* acc, int32_t i){\n /**\n  * \\brief Code Generation for `reduce` (ND case)\n  */\n-void generator::visit_reducend_inst_fast(ir::reduce_inst* x, std::function<Value*(Value*,Value*)> do_acc, Value *neutral){\n-  //\n+void generator::visit_reducend_inst_fast(ir::reduce_inst* x, acc_fn_t do_acc, Value *neutral){\n   ir::value *arg = x->get_operand(0);\n+  const auto with_index = x->with_index();\n+  unsigned axis = x->get_axis();\n   analysis::distributed_layout* layout = dynamic_cast<analysis::distributed_layout*>(layouts_->get(arg));\n-  std::vector<unsigned> shapes = layout->get_shape();\n+  const auto &shapes = layout->get_shape();\n \n   Type* sca_ty = cvt(arg->get_type()->get_scalar_ty());\n   size_t n_bits = sca_ty->getPrimitiveSizeInBits();\n-\n   std::string n_bits_str = std::to_string(n_bits);\n   std::string cst = (n_bits == 64) ? \"l\" : \"r\";\n \n@@ -2351,6 +2353,15 @@ void generator::visit_reducend_inst_fast(ir::reduce_inst* x, std::function<Value\n   FunctionType *ld_shared_ty = FunctionType::get(sca_ty, {i1_ty, ptr_ty(sca_ty, 3)}, false);\n   InlineAsm *ld_shared = InlineAsm::get(ld_shared_ty, \"@$1 ld.shared.b\" + n_bits_str + \" $0, [$2];\", \"=\" + cst + \",b,\" + cst, true);\n \n+  Type *index_ty = IntegerType::get(*ctx_, 32);\n+  FunctionType *st_shared_index_ty =\n+      FunctionType::get(void_ty, {i1_ty, ptr_ty(index_ty, 3), index_ty}, false);\n+  InlineAsm *st_shared_index = InlineAsm::get(\n+      st_shared_index_ty, \"@$0 st.shared.b32 [$1], $2;\", \"b,r,r\", true);\n+  FunctionType *ld_shared_index_ty =\n+      FunctionType::get(index_ty, {i1_ty, ptr_ty(index_ty, 3)}, false);\n+  InlineAsm *ld_shared_index = InlineAsm::get(\n+      ld_shared_index_ty, \"@$1 ld.shared.b32 $0, [$2];\", \"=r,b,r\", true);\n \n   Value* thread = tgt_->get_local_id(mod_, *builder_, 0);\n   Value* warp = udiv(thread, i32(32));\n@@ -2362,54 +2373,64 @@ void generator::visit_reducend_inst_fast(ir::reduce_inst* x, std::function<Value\n   std::vector<indices_t> arg_idxs = idxs_.at(arg);\n   size_t n_elts = arg_idxs.size();\n   unsigned col_per_thread = 0;\n-  Value* warp_i;\n-  Value* warp_j;\n-  if(analysis::scanline_layout* scanline = layout->to_scanline()){\n+  Value* warp_j = nullptr;\n+  if (analysis::scanline_layout *scanline = layout->to_scanline()) {\n     std::vector<int> order = layout->get_order();\n     unsigned mts = scanline->mts(order[0]);\n     shuffle_width = std::min<int>(mts, 32);\n-    warps_per_inner = std::max<int>(mts/32, 1);\n+    warps_per_inner = std::max<int>(mts / 32, 1);\n     col_per_thread = shapes[order[0]] / mts;\n-    warp_i = udiv(warp, i32(warps_per_inner));\n     warp_j = urem(warp, i32(warps_per_inner));\n-  }\n-  else if(layout->to_mma()){\n-    shuffle_width = 4; \n+  } else if (layout->to_mma()) {\n+    shuffle_width = 4;\n     warps_per_inner = layout->to_mma()->wpt(1);\n     col_per_thread = 16;\n-    warp_i = axes_.at(a_axes_->get(arg, 0)).thread_id;\n     warp_j = axes_.at(a_axes_->get(arg, 1)).thread_id;\n-  }\n+  } \n+  assert(warp_j != nullptr);\n \n   // unsigned col_per_thread = 2 * shapes[order[0]] / layout->shape_per_cta(order[0]);\n   //\n-  Type *ret_ty = cvt(x->get_type()->get_scalar_ty());\n-  unsigned addr_space = shmem_->getType()->getPointerAddressSpace();\n-  Value *base = bit_cast(shmem_, ptr_ty(ret_ty, addr_space));\n+  Value *base = cast_shared_layout_ptr(layouts_->get(layouts_->tmp(x)),\n+                                       cvt(x->get_type()->get_scalar_ty()));\n+  Value *index_base =\n+      with_index ? cast_shared_layout_ptr(layouts_->get(layouts_->tmp_index(x)),\n+                                          IntegerType::get(*ctx_, 32))\n+                 : nullptr;\n+\n   // preds\n   Value* is_lane0 = icmp_eq(lane, i32(0));\n   Value* is_warp0 = icmp_eq(warp, i32(0));\n   Value* is_thread0 = icmp_eq(thread, i32(0));\n   Value* lane_j = urem(lane, i32(shuffle_width));\n-  Value* first_lane_in_col = icmp_eq(lane_j, i32(0));\n   add_barrier();\n   // compute partial sum for each warp, and store to shared memory\n   for(size_t i = 0; i < n_elts/col_per_thread; i++){\n-    Value* acc;\n+    std::pair<Value*, Value*> acc;\n     // reduce within thread\n     for(size_t j = 0; j < col_per_thread; j++){\n-      Value* val = arg_vals[arg_idxs[i*col_per_thread + j]];\n-      // acc = (j == 0) ? val : do_acc(acc, val);\n-      acc = (j == 0) ? val : do_acc(acc, val);\n+      auto arg_idx = arg_idxs[i*col_per_thread + j];\n+      bool is_first = j == 0;\n+      do_acc(\n+          acc, [&]() -> Value * { return arg_vals[arg_idx]; },\n+          [&]() -> Value * { return arg_idx[axis]; }, is_first);\n     }\n+\n     // reduce within warp\n-    for(int k = shuffle_width/2 ; k > 0; k >>= 1)\n-      acc = do_acc(acc, shfl_sync(acc, k));\n+    for(int k = shuffle_width/2 ; k > 0; k >>= 1) {\n+      do_acc(\n+          acc, [&]() -> Value * { return shfl_sync(acc.first, k); },\n+          [&]() -> Value * { return shfl_sync(acc.second, k); }, false);\n+    }\n     // store partial result to shared memory\n     auto x_idxs = idxs_[x][i];\n     Value* x_idx = x_idxs.empty() ? builder_->getInt32(0) : x_idxs[0];\n     Value* st_off = add(mul(x_idx, i32(warps_per_inner)), warp_j);\n-    call(st_shared, {icmp_eq(lane_j, i32(0)), gep(base, st_off), acc});\n+    call(st_shared, {icmp_eq(lane_j, i32(0)), gep(base, st_off), acc.first});\n+    if (with_index) {\n+      call(st_shared_index,\n+           {icmp_eq(lane_j, i32(0)), gep(index_base, st_off), acc.second});\n+    }\n   }\n   add_barrier();\n   // at this point, partial accumulator synchronized in shared memory\n@@ -2418,48 +2439,66 @@ void generator::visit_reducend_inst_fast(ir::reduce_inst* x, std::function<Value\n     auto x_idxs = idxs_[x][i];\n     Value* x_idx = x_idxs.empty() ? builder_->getInt32(0) : x_idxs[0];\n     Value* ld_off = add(mul(x_idx, i32(warps_per_inner)), urem(lane_j, i32(warps_per_inner)));\n-    Value* acc = call(ld_shared, {builder_->getInt1(true), gep(base, ld_off)});\n-    for(int k = warps_per_inner/2; k > 0; k >>= 1)\n-      acc = do_acc(acc, shfl_sync(acc, k));\n-    vals_[x][idxs_[x][i]] = acc;\n+    std::pair<Value*, Value*> acc;\n+    acc.first = call(ld_shared, {builder_->getInt1(true), gep(base, ld_off)});\n+    acc.second = with_index ? call(ld_shared_index, {builder_->getInt1(true),\n+                                                     gep(index_base, ld_off)})\n+                            : nullptr;\n+    for (int k = warps_per_inner / 2; k > 0; k >>= 1) {\n+      do_acc(\n+          acc, [&]() -> Value * { return shfl_sync(acc.first, k); },\n+          [&]() -> Value * { return shfl_sync(acc.second, k); }, false);\n+    }\n+    vals_[x][idxs_[x][i]] = with_index ? acc.second : acc.first;\n   }\n   // add_barrier();\n }\n \n-void generator::visit_reducend_inst(ir::reduce_inst* x, std::function<Value*(Value*,Value*)> do_acc, Value *neutral) {\n+\n+void generator::visit_reducend_inst(ir::reduce_inst* x, acc_fn_t do_acc, Value *neutral) {\n   ir::value *arg = x->get_operand(0);\n-  Type *ty = cvt(x->get_type()->get_scalar_ty());\n   unsigned axis = x->get_axis();\n+  auto with_index = x->with_index();\n \n   // reduce within thread\n-  std::map<indices_t, Value*> accs;\n+  // index-><current reduced value, current min/max index (optional)>\n+  std::map<indices_t, std::pair<Value*, Value*>> accs;\n   for(indices_t idx: idxs_.at(arg)){\n     indices_t pidx = idx;\n     pidx[axis] = i32(0);\n-    Value *current = vals_[arg][idx];\n     bool is_first = accs.find(pidx) == accs.end();\n-    accs[pidx] = is_first ? current : do_acc(accs[pidx], current);\n+    do_acc(\n+        accs[pidx], [&]() -> Value * { return vals_[arg][idx]; },\n+        [&]() -> Value * { return idx[axis]; }, is_first);\n   };\n \n   // reduce within blocks\n-  analysis::data_layout* layout = layouts_->get(layouts_->tmp(x));\n-  Value *base = shared_ptr_.at(layout);\n-  auto shape  = layout->get_shape();\n-  auto order  = layout->get_order();\n-  int  space = base->getType()->getPointerAddressSpace();\n-  Value *ptr = bit_cast(base, ptr_ty(ty, space));\n+  auto *data_layout = layouts_->get(layouts_->tmp(x));\n+  auto *data_ptr =\n+      cast_shared_layout_ptr(data_layout, cvt(x->get_type()->get_scalar_ty()));\n+  auto *index_ptr =\n+      with_index ? cast_shared_layout_ptr(layouts_->get(layouts_->tmp_index(x)),\n+                                          IntegerType::get(*ctx_, 32))\n+                 : data_ptr;\n+\n+  auto shape  = data_layout->get_shape();\n+  auto order  = data_layout->get_order();\n   Value *lane = axes_.at(a_axes_->get(arg, axis)).thread_id;\n   for(auto& x: accs) {\n     // current element being computed\n-    Value *&acc = x.second;\n+    std::pair<Value *, Value *> acc = x.second;\n     indices_t write_idx = x.first;\n     write_idx[axis] = lane;\n     // shared memory write  pointer\n     Value *write_off = shared_off(shape, order, write_idx);\n-    Value *write_ptr = gep(ptr, write_off);\n+    Value *write_ptr = gep(data_ptr, write_off);\n+    Value *index_write_ptr = gep(index_ptr, write_off);\n     // initialize shared memory\n     add_barrier();\n-    store(acc, write_ptr);\n+    store(acc.first, write_ptr);\n+    if (with_index) {\n+      store(acc.second, index_write_ptr);\n+    }\n     // build result\n     indices_t idx(write_idx.size(), i32(0));\n     for(size_t i = shape[axis]/2; i > 0; i >>= 1){\n@@ -2468,11 +2507,17 @@ void generator::visit_reducend_inst(ir::reduce_inst* x, std::function<Value*(Val\n       Value *read_msk = icmp_ult(lane, i32(i));\n       Value *read_off = select(read_msk, shared_off(shape, order, idx), i32(0));\n       Value *read_ptr = gep(write_ptr, read_off);\n+      Value *index_read_ptr = gep(index_write_ptr, read_off);\n       add_barrier();\n       // update accumulator\n-      acc = do_acc(acc, load(read_ptr));\n+      do_acc(\n+          acc, [&]() -> Value * { return load(read_ptr); },\n+          [&]() -> Value * { return load(index_read_ptr); }, false);\n       add_barrier();\n-      store(acc, write_ptr);\n+      store(acc.first, write_ptr);\n+      if (with_index) {\n+        store(acc.second, index_write_ptr);\n+      }\n     }\n   }\n   add_barrier();\n@@ -2482,7 +2527,8 @@ void generator::visit_reducend_inst(ir::reduce_inst* x, std::function<Value*(Val\n     indices_t read_idx = idx;\n     read_idx.insert(read_idx.begin() + axis, i32(0));\n     Value *read_off = shared_off(shape, order, read_idx);\n-    Value *read_ptr = gep(ptr, read_off);\n+    Value *read_ptr =\n+        with_index ? gep(index_ptr, read_off) : gep(data_ptr, read_off);\n     vals_[x][idx] = load(read_ptr);\n   };\n }\n@@ -2494,45 +2540,75 @@ void generator::visit_reduce_inst(ir::reduce_inst* x) {\n   Type *ty = cvt(x->get_type()->get_scalar_ty());\n   // accumulation function\n   ir::reduce_inst::op_t op = x->get_op();\n-  auto do_acc = [&](Value *x, Value *y) -> Value* {\n+  auto do_acc_op = [&](Value *x, Value *y) -> Value* {\n     switch(op){\n     case ir::reduce_inst::ADD: return add(x, y);\n     case ir::reduce_inst::SUB: return sub(x, y);\n-    case ir::reduce_inst::MAX: return select(icmp_sge(x, y), x, y);\n-    case ir::reduce_inst::MIN: return select(icmp_sle(x, y), x, y);\n+    case ir::reduce_inst::ARGUMAX: return icmp_uge(x, y);\n+    case ir::reduce_inst::ARGUMIN: return icmp_ule(x, y);\n+    case ir::reduce_inst::ARGMAX: return icmp_sge(x, y);\n+    case ir::reduce_inst::ARGMIN: return icmp_sle(x, y);\n     case ir::reduce_inst::UMAX: return select(icmp_uge(x, y), x, y);\n     case ir::reduce_inst::UMIN: return select(icmp_ule(x, y), x, y);\n+    case ir::reduce_inst::MAX: return select(icmp_sge(x, y), x, y);\n+    case ir::reduce_inst::MIN: return select(icmp_sle(x, y), x, y);\n     case ir::reduce_inst::FADD: return fadd(x, y);\n     case ir::reduce_inst::FSUB: return fsub(x, y);\n+    case ir::reduce_inst::ARGFMAX: return fcmp_oge(x, y);\n+    case ir::reduce_inst::ARGFMIN: return fcmp_ole(x, y);\n     case ir::reduce_inst::FMAX: return max_num(x, y);\n     case ir::reduce_inst::FMIN: return min_num(x, y);\n     case ir::reduce_inst::XOR: return xor_(x, y);\n     default: throw std::runtime_error(\"unreachable\");\n     }\n   };\n+\n+  auto do_acc = [&](std::pair<Value *, Value *> &acc,\n+                    std::function<Value *()> load_value_fn,\n+                    std::function<Value *()> load_index_fn,\n+                    bool is_first) -> void {\n+    auto *val = load_value_fn();\n+    if (x->with_index()) {\n+      auto *index = load_index_fn();\n+      if (is_first) {\n+        acc.first = val;\n+        acc.second = index;\n+      } else {\n+        Value *ret = do_acc_op(acc.first, val);\n+        acc.first = select(ret, acc.first, val);\n+        acc.second = select(ret, acc.second, index);\n+      }\n+    } else {\n+      acc.first = is_first ? val : do_acc_op(acc.first, val);\n+    }\n+  };\n+\n   // neutral element\n   Value *neutral;\n   switch(op) {\n     case ir::reduce_inst::ADD: neutral = ConstantInt::get(ty, 0); break;\n     case ir::reduce_inst::SUB: neutral = ConstantInt::get(ty, 0); break;\n-    case ir::reduce_inst::MAX: neutral = ConstantInt::get(ty, INT32_MIN); break;\n-    case ir::reduce_inst::MIN: neutral = ConstantInt::get(ty, INT32_MAX); break;\n+    case ir::reduce_inst::ARGUMAX: neutral = ConstantInt::get(ty, INT32_MIN); break;\n+    case ir::reduce_inst::ARGUMIN: neutral = ConstantInt::get(ty, INT32_MAX); break;\n+    case ir::reduce_inst::ARGMAX: neutral = ConstantInt::get(ty, INT32_MIN); break;\n+    case ir::reduce_inst::ARGMIN: neutral = ConstantInt::get(ty, INT32_MAX); break;\n     case ir::reduce_inst::UMAX: neutral = ConstantInt::get(ty, 0); break;\n     case ir::reduce_inst::UMIN: neutral = ConstantInt::get(ty, UINT32_MAX); break;\n+    case ir::reduce_inst::MAX: neutral = ConstantInt::get(ty, INT32_MIN); break;\n+    case ir::reduce_inst::MIN: neutral = ConstantInt::get(ty, INT32_MAX); break;\n     case ir::reduce_inst::FADD: neutral = ConstantFP::get(ty, 0); break;\n     case ir::reduce_inst::FSUB: neutral = ConstantFP::get(ty, 0); break;\n+    case ir::reduce_inst::ARGFMAX: neutral = ConstantFP::get(ty, -INFINITY); break;\n+    case ir::reduce_inst::ARGFMIN: neutral = ConstantFP::get(ty, INFINITY); break;\n     case ir::reduce_inst::FMAX: neutral = ConstantFP::get(ty, -INFINITY); break;\n     case ir::reduce_inst::FMIN: neutral = ConstantFP::get(ty, INFINITY); break;\n-    case ir::reduce_inst::XOR: neutral = neutral = ConstantInt::get(ty, 0); break;\n+    case ir::reduce_inst::XOR: neutral = ConstantInt::get(ty, 0); break;\n     default: throw std::runtime_error(\"unreachable\");\n   }\n   ir::value *arg = x->get_operand(0);\n-  int cc = tgt_->as_nvidia()->sm();\n-  analysis::scanline_layout* scanline = layouts_->get(x->get_operand(0))->to_scanline();\n-  analysis::mma_layout* mma = layouts_->get(x->get_operand(0))->to_mma();\n-  bool is_coalesced_scanline = scanline && (scanline->get_order()[0] == x->get_axis());\n-  bool is_a100_mma = mma && (cc >= 80) && (x->get_axis() == 1);\n-  if(is_coalesced_scanline || is_a100_mma)\n+  bool is_coalesced_scanline = layouts_->is_coalesced_scanline(x);\n+  bool is_a100_mma = layouts_->is_a100_mma(x);\n+  if (is_coalesced_scanline || is_a100_mma)\n     visit_reducend_inst_fast(x, do_acc, neutral);\n   else\n     visit_reducend_inst(x, do_acc, neutral);\n@@ -2938,6 +3014,13 @@ void generator::forward_declare(ir::function* fn){\n   fns_[fn] = ret;\n }\n \n+Value *generator::cast_shared_layout_ptr(analysis::data_layout *layout,\n+                                         Type *ty) {\n+  unsigned addr_space = shmem_->getType()->getPointerAddressSpace();\n+  Value *base = bit_cast(shared_ptr_.at(layout), ptr_ty(ty, addr_space));\n+  return base;\n+}\n+\n void generator::visit_function(ir::function* fn) {\n   idxs_.clear();\n   vals_.clear();"}, {"filename": "lib/codegen/transform/membar.cc", "status": "modified", "additions": 8, "deletions": 1, "changes": 9, "file_content_changes": "@@ -60,15 +60,22 @@ membar::val_set_t membar::intersect_with(const val_set_t& as, const val_set_t& b\n       continue;\n     analysis::shared_layout* a_layout = layouts_->get(a)->to_shared();\n     analysis::shared_layout* a_tmp = layouts_->has_tmp(a) ? layouts_->get(layouts_->tmp(a))->to_shared() : nullptr;\n+    analysis::shared_layout* a_tmp_index = layouts_->has_tmp_index(a) ? layouts_->get(layouts_->tmp_index(a))->to_shared() : nullptr;\n     for(ir::value* b: bs){\n       if(!b->get_type()->is_block_ty())\n         continue;\n       analysis::shared_layout* b_layout = layouts_->get(b)->to_shared();\n       analysis::shared_layout* b_tmp = layouts_->has_tmp(b) ? layouts_->get(layouts_->tmp(b))->to_shared() : nullptr;\n+      analysis::shared_layout* b_tmp_index = layouts_->has_tmp_index(b) ? layouts_->get(layouts_->tmp_index(b))->to_shared() : nullptr;\n       if(intersect_with(a_layout, b_layout) ||\n          intersect_with(a_layout, b_tmp) ||\n+         intersect_with(a_layout, b_tmp_index) ||\n          intersect_with(a_tmp, b_layout) ||\n-         intersect_with(a_tmp, b_tmp))\n+         intersect_with(a_tmp, b_tmp) ||\n+         intersect_with(a_tmp, b_tmp_index) ||\n+         intersect_with(a_tmp_index, b_layout) ||\n+         intersect_with(a_tmp_index, b_tmp) ||\n+         intersect_with(a_tmp_index, b_tmp_index))\n         ret.insert(b);\n     }\n   }"}, {"filename": "python/bench/bench_matmul.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -5,7 +5,7 @@\n \n def rounded_linspace(low, high, steps, div):\n     ret = torch.linspace(low, high, steps)\n-    ret = (ret.int() + div - 1) // div * div\n+    ret = torch.div(ret.int() + div - 1, div, rounding_mode='trunc') * div\n     ret = torch.unique(ret)\n     return list(map(int, ret))\n "}, {"filename": "python/src/functions.h", "status": "modified", "additions": 23, "deletions": 3, "changes": 26, "file_content_changes": "@@ -353,9 +353,6 @@ ir::value *sqrt(ir::value *input, ir::builder *builder) {\n   return builder->create_sqrt(input);\n };\n \n-/*----------------------------------------------\n- definition of triton.min\n- ----------------------------------------------*/\n ir::value *reduce_impl(ir::value *input, unsigned int axis, ir::builder *builder, const std::string &name,\n                        ir::reduce_inst::op_t FLOAT_OP, ir::reduce_inst::op_t INT_OP) {\n   ir::type *scalar_ty = input->get_type()->get_scalar_ty();\n@@ -367,13 +364,26 @@ ir::value *reduce_impl(ir::value *input, unsigned int axis, ir::builder *builder\n     throw_not_int_or_float(name);\n }\n \n+/*----------------------------------------------\n+ definition of triton.min\n+ ----------------------------------------------*/\n std::string min_docstr = R\"pbdoc(\n     Returns the minimum value of `input`.\n  )pbdoc\";\n ir::value *min(ir::value *input, unsigned int axis, ir::builder *builder) {\n   return reduce_impl(input, axis, builder, \"min\", ir::reduce_inst::FMIN, ir::reduce_inst::MIN);\n };\n \n+/*----------------------------------------------\n+ definition of triton.arg_min\n+ ----------------------------------------------*/\n+std::string min_docstr = R\"pbdoc(\n+    Returns the minimum value's index of `input`.\n+ )pbdoc\";\n+ir::value *argmin(ir::value *input, unsigned int axis, ir::builder *builder) {\n+  return reduce_impl(input, axis, builder, \"argmin\", ir::reduce_inst::ARGFMIN, ir::reduce_inst::ARGMIN);\n+};\n+\n /*----------------------------------------------\n  definition of triton.max\n  ----------------------------------------------*/\n@@ -384,6 +394,16 @@ ir::value *max(ir::value *input, unsigned int axis, ir::builder *builder) {\n   return reduce_impl(input, axis, builder, \"max\", ir::reduce_inst::FMAX, ir::reduce_inst::MAX);\n };\n \n+/*----------------------------------------------\n+ definition of triton.arg_max\n+ ----------------------------------------------*/\n+std::string max_docstr = R\"pbdoc(\n+    Returns the maximum value's index of `input`.\n+ )pbdoc\";\n+ir::value *argmax(ir::value *input, unsigned int axis, ir::builder *builder) {\n+  return reduce_impl(input, axis, builder, \"argmax\", ir::reduce_inst::ARGFMAX, ir::reduce_inst::ARGMAX);\n+};\n+\n /*----------------------------------------------\n  definition of triton.sum\n  ----------------------------------------------*/"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -573,8 +573,14 @@ void init_triton_ir(py::module &&m) {\n       .value(\"MAX\", ir::reduce_inst::MAX)\n       .value(\"UMIN\", ir::reduce_inst::UMIN)\n       .value(\"UMAX\", ir::reduce_inst::UMAX)\n+      .value(\"ARGMIN\", ir::reduce_inst::ARGMIN)\n+      .value(\"ARGMAX\", ir::reduce_inst::ARGMAX)\n+      .value(\"ARGUMIN\", ir::reduce_inst::ARGUMIN)\n+      .value(\"ARGUMAX\", ir::reduce_inst::ARGUMAX)\n       .value(\"FMIN\", ir::reduce_inst::FMIN)\n       .value(\"FMAX\", ir::reduce_inst::FMAX)\n+      .value(\"ARGFMIN\", ir::reduce_inst::ARGFMIN)\n+      .value(\"ARGFMAX\", ir::reduce_inst::ARGFMAX)\n       .value(\"XOR\", ir::reduce_inst::XOR);\n   \n   py::enum_<ir::atomic_rmw_op_t>(m, \"ATOMIC_OP\")"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 41, "deletions": 16, "changes": 57, "file_content_changes": "@@ -690,7 +690,7 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n \n @pytest.mark.parametrize(\"op, dtype_str, shape\",\n                          [(op, dtype, shape)\n-                          for op in ['min', 'max', 'sum']\n+                          for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n                           for dtype in dtypes\n                           for shape in [32, 64, 128, 512]])\n def test_reduce1d(op, dtype_str, shape, device='cuda'):\n@@ -707,28 +707,37 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n     # limit the range of integers so that the sum does not overflow\n     x = numpy_random((shape,), dtype_str=dtype_str, rs=rs)\n     x_tri = to_triton(x, device=device)\n-    numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min}[op]\n+    numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n+                'argmin': np.argmin, 'argmax': np.argmax}[op]\n     # numpy result\n-    z_ref = numpy_op(x).astype(getattr(np, dtype_str))\n+    z_dtype_str = 'int32' if op == 'argmin' or op == 'argmax' else dtype_str\n+    z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n     # triton result\n-    z_tri = to_triton(numpy_random((1,), dtype_str=dtype_str, rs=rs), device=device)\n+    z_tri = to_triton(numpy_random((1,), dtype_str=z_dtype_str, rs=rs), device=device)\n     kernel[(1,)](x_tri, z_tri, BLOCK=shape)\n+    z_tri = to_numpy(z_tri)\n     # compare\n     if op == 'sum':\n-        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+        np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n     else:\n-        np.testing.assert_equal(z_ref, to_numpy(z_tri))\n+        if op == 'argmin' or op == 'argmax':\n+            # argmin and argmax can have multiple valid indices.\n+            # so instead we compare the values pointed by indices\n+            np.testing.assert_equal(x[z_ref], x[z_tri])\n+        else:\n+            np.testing.assert_equal(z_ref, z_tri)\n \n \n reduce_configs1 = [\n     (op, dtype, (1, 1024), axis) for dtype in dtypes\n-    for op in ['min', 'max', 'sum']\n+    for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n     for axis in [1]\n ]\n reduce_configs2 = [\n-    (op, 'float32', shape, 1)\n-    for op in ['min', 'max', 'sum']\n+    (op, 'float32', shape, axis)\n+    for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n     for shape in [(2, 32), (4, 32), (4, 128), (32, 64), (64, 128), (128, 256), (32, 1024)]\n+    for axis in [0, 1]\n ]\n \n \n@@ -741,25 +750,41 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n         range_n = tl.arange(0, BLOCK_N)\n         x = tl.load(X + range_m[:, None] * BLOCK_N + range_n[None, :])\n         z = GENERATE_TEST_HERE\n-        tl.store(Z + range_m, z)\n+        if AXIS == 1:\n+            tl.store(Z + range_m, z)\n+        else:\n+            tl.store(Z + range_n, z)\n \n     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=AXIS)'})\n     # input\n     rs = RandomState(17)\n     # limit the range of integers so that the sum does not overflow\n     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n     x_tri = to_triton(x)\n-    numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min}[op]\n+    numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n+                'argmin': np.argmin, 'argmax': np.argmax}[op]\n+    z_dtype_str = 'int32' if op == 'argmin' or op == 'argmax' else dtype_str\n     # numpy result\n-    z_ref = numpy_op(x, axis=axis).astype(getattr(np, dtype_str))\n+    z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n     # triton result\n-    z_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device=device)\n-    binary = kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis)\n+    z_tri = to_triton(numpy_random((shape[1 - axis],), dtype_str=z_dtype_str, rs=rs),\n+                      device=device)\n+    kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis)\n+    z_tri = to_numpy(z_tri)\n     # compare\n     if op == 'sum':\n-        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+        np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n     else:\n-        np.testing.assert_equal(z_ref, to_numpy(z_tri))\n+        if op == 'argmin' or op == 'argmax':\n+            # argmin and argmax can have multiple valid indices.\n+            # so instead we compare the values pointed by indices\n+            z_ref_index = np.expand_dims(z_ref, axis=axis)\n+            z_tri_index = np.expand_dims(z_tri, axis=axis)\n+            z_ref_value = np.take_along_axis(x, z_ref_index, axis=axis)\n+            z_tri_value = np.take_along_axis(x, z_tri_index, axis=axis)\n+            np.testing.assert_equal(z_ref_value, z_tri_value)\n+        else:\n+            np.testing.assert_equal(z_ref, z_tri)\n \n # ---------------\n # test permute"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -1011,13 +1011,27 @@ def max(input, axis, _builder=None):\n     return semantic.max(input, axis, _builder)\n \n \n+@builtin\n+@_add_reduction_docstr(\"maximum index\")\n+def argmax(input, axis, _builder=None):\n+    axis = _constexpr_to_value(axis)\n+    return semantic.argmax(input, axis, _builder)\n+\n+\n @builtin\n @_add_reduction_docstr(\"minimum\")\n def min(input, axis, _builder=None):\n     axis = _constexpr_to_value(axis)\n     return semantic.min(input, axis, _builder)\n \n \n+@builtin\n+@_add_reduction_docstr(\"minimum index\")\n+def argmin(input, axis, _builder=None):\n+    axis = _constexpr_to_value(axis)\n+    return semantic.argmin(input, axis, _builder)\n+\n+\n @builtin\n @_add_reduction_docstr(\"sum\")\n def sum(input, axis, _builder=None):"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 16, "deletions": 4, "changes": 20, "file_content_changes": "@@ -961,10 +961,14 @@ def reduce_impl(input: tl.tensor, axis: int, builder: ir.builder, name: str,\n \n     # choose the right unsigned operation\n     if scalar_ty.is_int_unsigned():\n-        if INT_OP is ir.REDUCE_OP.MIN:\n-            INT_OP = ir.REDUCE_OP.UMIN\n-        elif INT_OP is ir.REDUCE_OP.MAX:\n-            INT_OP = ir.REDUCE_OP.UMAX\n+        int_op_to_unit = {\n+            ir.REDUCE_OP.MIN: ir.REDUCE_OP.UMIN,\n+            ir.REDUCE_OP.MAX: ir.REDUCE_OP.UMAX,\n+            ir.REDUCE_OP.ARGMIN: ir.REDUCE_OP.ARGUMIN,\n+            ir.REDUCE_OP.ARGMAX: ir.REDUCE_OP.ARGUMAX,\n+        }\n+        if INT_OP in int_op_to_unit:\n+            INT_OP = int_op_to_unit[INT_OP]\n \n     # get result type\n     shape = input.type.shape\n@@ -988,10 +992,18 @@ def min(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n     return reduce_impl(input, axis, builder, \"min\", ir.REDUCE_OP.FMIN, ir.REDUCE_OP.MIN)\n \n \n+def argmin(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n+    return reduce_impl(input, axis, builder, \"argmin\", ir.REDUCE_OP.ARGFMIN, ir.REDUCE_OP.ARGMIN)\n+\n+\n def max(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n     return reduce_impl(input, axis, builder, \"max\", ir.REDUCE_OP.FMAX, ir.REDUCE_OP.MAX)\n \n \n+def argmax(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n+    return reduce_impl(input, axis, builder, \"argmax\", ir.REDUCE_OP.ARGFMAX, ir.REDUCE_OP.ARGMAX)\n+\n+\n def sum(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n     return reduce_impl(input, axis, builder, \"sum\", ir.REDUCE_OP.FADD, ir.REDUCE_OP.ADD)\n "}]