[{"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 15, "deletions": 27, "changes": 42, "file_content_changes": "@@ -158,59 +158,47 @@ func.func @longlive(%A : !tt.ptr<f16>) {\n // This example triggers graph coloring with > 1 colors.\n // CHECK-LABEL: multi_color\n func.func @multi_color(%A : !tt.ptr<f16>) {\n-  // 0 CHECK: offset = 0, size = 64\n+  // CHECK: offset = 0, size = 64\n   %cst = arith.constant dense<0.000000e+00> : tensor<4x8xf16, #A_SHARED>\n-  // 1 offset = 1216, size = 32\n+  // CHECK-NEXT: offset = 1216, size = 32\n   %cst_0 = arith.constant dense<0.000000e+00> : tensor<4x4xf16, #A_SHARED>\n-  // 2 offset = 1248, size = 128\n+  // CHECK-NEXT: offset = 1248, size = 128\n   %cst_1 = arith.constant dense<0.000000e+00> : tensor<16x4xf16, #A_SHARED>\n-  // 3 \n   %cst_2 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n-  // 4 scratch offset = 64, size = 1152\n+  // CHECK-NEXT: scratch offset = 64, size = 1152\n   %0 = triton_gpu.convert_layout %cst_2 : (tensor<16x32xf16, #AL>) -> tensor<16x32xf16, #AL>\n-  // 5\n   %1 = triton_gpu.convert_layout %cst : (tensor<4x8xf16, #A_SHARED>) -> tensor<4x8xf16, #AL>\n-  // 6 offset = 0, size = 128\n+  // CHECK-NEXT: offset = 0, size = 128\n   %cst_3 = arith.constant dense<0.000000e+00> : tensor<4x16xf16, #A_SHARED>\n-  // 7\n   %2 = triton_gpu.convert_layout %cst_0 : (tensor<4x4xf16, #A_SHARED>) -> tensor<4x4xf16, #AL>\n-  // 8 scratch offset = 0, size = 1152\n+  // CHECK-NEXT: scratch offset = 0, size = 1152\n   %3 = triton_gpu.convert_layout %cst_2 : (tensor<16x32xf16, #AL>) -> tensor<16x32xf16, #AL>\n-  // 9 offset = 0, size = 256\n+  // CHECK-NEXT: offset = 0, size = 256\n   %cst_4 = arith.constant dense<0.000000e+00> : tensor<4x32xf16, #A_SHARED>\n-  // 10 offset = 256, size = 64\n+  // CHECK-NEXT: offset = 256, size = 64\n   %cst_5 = arith.constant dense<0.000000e+00> : tensor<4x8xf16, #A_SHARED>\n-  // 11\n   %4 = triton_gpu.convert_layout %cst_5 : (tensor<4x8xf16, #A_SHARED>) -> tensor<4x8xf16, #AL>\n-  // 12\n   %5 = triton_gpu.convert_layout %cst_5 : (tensor<4x8xf16, #A_SHARED>) -> tensor<4x8xf16, #AL>\n-  // 13 offset = 256, size = 512\n+  // CHECK-NEXT: offset = 256, size = 512\n   %cst_6 = arith.constant dense<0.000000e+00> : tensor<8x32xf16, #A_SHARED>\n-  // 14 offset = 2528, size = 128\n+  // CHECK-NEXT: offset = 2528, size = 128\n   %cst_7 = arith.constant dense<0.000000e+00> : tensor<2x32xf16, #A_SHARED>\n-  // 15\n   %6 = triton_gpu.convert_layout %cst_0 : (tensor<4x4xf16, #A_SHARED>) -> tensor<4x4xf16, #AL>\n-  // 16 offset = 256, size = 512\n+  // CHECK-NEXT: offset = 256, size = 512\n   %cst_8 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n-  // 17 offset = 256, size = 32\n+  // CHECK-NEXT: offset = 256, size = 32\n   %cst_9 = arith.constant dense<0.000000e+00> : tensor<4x4xf16, #A_SHARED>\n-  // 18 offset = 256, size = 512\n+  // CHECK-NEXT: offset = 256, size = 512\n   %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n-  // 19\n   %7 = triton_gpu.convert_layout %cst_1 : (tensor<16x4xf16, #A_SHARED>) -> tensor<16x4xf16, #AL>\n-  // 20\n   %8 = triton_gpu.convert_layout %cst_4 : (tensor<4x32xf16, #A_SHARED>) -> tensor<4x32xf16, #AL>\n-  // 21 scratch offset = 0, size = 1152\n+  // CHECK-NEXT: scratch offset = 0, size = 1152\n   %9 = triton_gpu.convert_layout %cst_2 : (tensor<16x32xf16, #AL>) -> tensor<16x32xf16, #AL>\n-  // 22\n   %cst_11 = arith.constant dense<0.000000e+00> : tensor<4x4xf16, #AL>\n-  // 23\n   %10 = triton_gpu.convert_layout %cst_7 : (tensor<2x32xf16, #A_SHARED>) -> tensor<2x32xf16, #AL>\n-  // 24\n   %cst_12 = arith.constant dense<0.000000e+00> : tensor<4x16xf16, #AL>\n-  // 25\n   %cst_13 = arith.constant dense<0.000000e+00> : tensor<8x32xf16, #AL>\n-  // size = 2656\n+  // CHECK-NEXT: size = 2656\n   return\n }\n "}]