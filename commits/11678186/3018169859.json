[{"filename": "include/triton/codegen/analysis/layout.h", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -211,7 +211,8 @@ class mma_layout: public distributed_layout {\n   TensorCoreType tensor_core_type_ = FP32_FP16_FP16_FP32;\n };\n \n-struct scanline_layout: public distributed_layout {\n+class scanline_layout: public distributed_layout {\n+public:\n   scanline_layout(size_t num_warps,\n                     const std::vector<int>& axes,\n                     const std::vector<unsigned>& shape,\n@@ -225,7 +226,7 @@ struct scanline_layout: public distributed_layout {\n   int contig_per_thread(size_t k) { return nts_.at(k); }\n \n   int per_thread(size_t k) { return contig_per_thread(k) * shape_[k] / shape_per_cta(k);}\n-public:\n+private:\n   // micro tile size. The size of a tile held by a thread block.\n   std::vector<int> mts_;\n   // nano tile size. The size of a tile held by a thread."}, {"filename": "include/triton/codegen/selection/generator.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -148,6 +148,8 @@ class generator: public ir::visitor, public analysis::layout_visitor {\n   std::tuple<Value*, Value*, Value*, Value*> fp32x4_to_fp8x4(Value *in0, Value *in1, Value *in2, Value *in3);\n   std::tuple<Value*, Value*, Value*, Value*> fp8x4_to_fp16x4(Value *in0, Value *in1, Value *in2, Value *in3);\n   std::tuple<Value*, Value*, Value*, Value*> fp16x4_to_fp8x4(Value *in0, Value *in1, Value *in2, Value *in3);\n+  std::tuple<Value*, Value*, Value*, Value*> fp8x4_to_bf16x4(Value *in0, Value *in1, Value *in2, Value *in3);\n+  std::tuple<Value*, Value*, Value*, Value*> bf16x4_to_fp8x4(Value *in0, Value *in1, Value *in2, Value *in3);\n   Value* bf16_to_fp32(Value *in0);\n   Value* fp32_to_bf16(Value *in0);\n "}, {"filename": "include/triton/ir/instructions.h", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -59,8 +59,8 @@ class instruction: public user{\n   std::string repr() const                                    { return repr_impl(); }\n   // metadata\n   void set_metadata(ir::metadata::kind_t kind,\n-                    unsigned value)                           { metadatas_[kind] = value;}\n-  unsigned get_metadata(ir::metadata::kind_t kind)            { return metadatas_[kind];}\n+                    std::vector<unsigned> value)                           { metadatas_[kind] = value;}\n+  std::vector<unsigned> get_metadata(ir::metadata::kind_t kind)            { return metadatas_[kind];}\n   // cloning\n   ir::instruction* clone() {\n     ir::instruction* res = clone_impl();\n@@ -77,7 +77,7 @@ class instruction: public user{\n \n private:\n   basic_block *parent_;\n-  std::map<ir::metadata::kind_t, unsigned> metadatas_;\n+  std::map<ir::metadata::kind_t, std::vector<unsigned>> metadatas_;\n   value_id_t id_;\n };\n "}, {"filename": "include/triton/ir/metadata.h", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -3,6 +3,8 @@\n #ifndef _TRITON_IR_METADATA_H_\n #define _TRITON_IR_METADATA_H_\n \n+#include <vector>\n+\n namespace triton{\n namespace ir{\n \n@@ -16,14 +18,14 @@ class metadata{\n   };\n \n private:\n-  metadata(kind_t kind, unsigned value);\n+  metadata(kind_t kind, std::vector<unsigned> value);\n \n public:\n-  static metadata* get(kind_t kind, unsigned value);\n+  static metadata* get(kind_t kind, std::vector<unsigned> value);\n \n private:\n   kind_t kind_;\n-  unsigned value_;\n+  std::vector<unsigned> value_;\n };\n \n }"}, {"filename": "include/triton/ir/module.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -70,7 +70,7 @@ class value_constructor {\n \n class module {\n   typedef std::pair<std::string, basic_block*> val_key_t;\n-  typedef std::pair<ir::metadata::kind_t, unsigned> md_pair_t;\n+  typedef std::pair<ir::metadata::kind_t, std::vector<unsigned>> md_pair_t;\n   friend class function;\n \n public:"}, {"filename": "lib/codegen/analysis/align.cc", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -366,9 +366,9 @@ std::vector<unsigned> align::populate_max_contiguous(ir::value *v){\n   if(max_contiguous_.find(v) != max_contiguous_.end())\n     return max_contiguous_.at(v);\n   if(auto *x = dynamic_cast<ir::instruction*>(v)){\n-    unsigned max_contiguous = x->get_metadata(ir::metadata::max_contiguous);\n-    if(max_contiguous > 0)\n-      return add_to_cache(x, {max_contiguous}, max_contiguous_);\n+    std::vector<unsigned> max_contiguous = x->get_metadata(ir::metadata::max_contiguous);\n+    if(!max_contiguous.empty())\n+      return add_to_cache(x, max_contiguous, max_contiguous_);\n   }\n   if(auto *x = dynamic_cast<ir::cast_inst*>(v))\n     return populate_max_contiguous_cast(x);\n@@ -521,9 +521,9 @@ std::vector<unsigned> align::populate_starting_multiple(ir::value *v){\n   if(starting_multiple_.find(v) != starting_multiple_.end())\n     return starting_multiple_.at(v);\n   if(auto *x = dynamic_cast<ir::instruction*>(v)){\n-    unsigned multiple_of = x->get_metadata(ir::metadata::multiple_of);\n-    if(multiple_of > 0)\n-      return add_to_cache(x, {multiple_of}, starting_multiple_);\n+    std::vector<unsigned> multiple_of = x->get_metadata(ir::metadata::multiple_of);\n+    if(!multiple_of.empty())\n+      return add_to_cache(x, multiple_of, starting_multiple_);\n   }\n   if(auto *x = dynamic_cast<ir::cast_inst*>(v))\n     return populate_starting_multiple_cast(x);"}, {"filename": "lib/codegen/selection/generator.cc", "status": "modified", "additions": 121, "deletions": 17, "changes": 138, "file_content_changes": "@@ -569,10 +569,10 @@ std::tuple<Value*, Value*, Value*, Value*> generator::fp8x4_to_fp16x4(Value *in0\n   \"prmt.b32 a1, 0, $2, 0x7060;            \\n\\t\" // If input is 0xdcba set a1 to 0xd0c0\n   \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\\t\" // b0 = a0 & 0x7fff7fff (strip sign)\n   \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\\t\" // b1 = a1 & 0x7fff7fff (strip sign)\n-  \"shr.b32  b0, b0, 1;                    \\n\\t\" // b0 <<= 1 (shift into fp16 poistion)\n-  \"shr.b32  b1, b1, 1;                    \\n\\t\" // b1 <<= 1 (shift into fp16 position)\n-  \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\\t\" // out0 = b0 | (0x80008000 | a0) (restore sign)\n-  \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\\t\" // out1 = b1 | (0x80008000 | a1) (restore sign)\n+  \"shr.b32  b0, b0, 1;                    \\n\\t\" // b0 >>= 1 (shift into fp16 poistion)\n+  \"shr.b32  b1, b1, 1;                    \\n\\t\" // b1 >>= 1 (shift into fp16 position)\n+  \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\\t\" // out0 = b0 | (0x80008000 & a0) (restore sign)\n+  \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\\t\" // out1 = b1 | (0x80008000 & a1) (restore sign)\n   \"}\", \"=r,=r,r\", false);\n   Value *packed_in = UndefValue::get(vec_ty(i8_ty, 4));\n   packed_in = insert_elt(packed_in, in0, (uint64_t)0);\n@@ -635,6 +635,110 @@ std::tuple<Value*, Value*, Value*, Value*> generator::fp16x4_to_fp8x4(Value *in0\n   return std::make_tuple(ret0, ret1, ret2, ret3);\n }\n \n+std::tuple<Value*, Value*, Value*, Value*> generator::fp8x4_to_bf16x4(Value *in0, Value *in1, Value *in2, Value *in3) {\n+  // current exp offset: 15\n+  // Add 112 (127-15) to compensate the difference in exponent bias\n+  // bf16 = (nosign >> (8-4) + 112 << 7) | sign;\n+  // bf16 = (nosign >> 4 + 0x3800) | sign;\n+  Type *ret_ty = StructType::get(*ctx_, {vec_ty(bf16_ty, 2), vec_ty(bf16_ty, 2)});\n+  InlineAsm *ptx = InlineAsm::get(FunctionType::get(ret_ty, {i32_ty}, false),\n+  \"{\"\n+  \".reg .b32 a<2>, sign<2>, nosign<2>, b<2>; \\n\\t\"\n+  \"prmt.b32 a0, 0, $2, 0x5040; \\n\\t\" // 0xdcba => 0xb0a0\n+  \"prmt.b32 a1, 0, $2, 0x7060; \\n\\t\" // 0xdcba => 0xd0c0\n+  \"and.b32 sign0, a0, 0x80008000; \\n\\t\"\n+  \"and.b32 sign1, a1, 0x80008000; \\n\\t\"\n+  \"and.b32 nosign0, a0, 0x7fff7fff; \\n\\t\"\n+  \"and.b32 nosign1, a1, 0x7fff7fff; \\n\\t\"\n+  \"shr.b32 nosign0, nosign0, 4; \\n\\t\"\n+  \"shr.b32 nosign1, nosign1, 4; \\n\\t\"\n+  \"add.u32 nosign0, nosign0, 0x38003800; \\n\\t\"\n+  \"add.u32 nosign1, nosign1, 0x38003800; \\n\\t\"\n+  \"or.b32 $0, sign0, nosign0; \\n\\t\"\n+  \"or.b32 $1, sign1, nosign1; \\n\\t\"\n+  \"}\", \"=r,=r,r\", false);\n+  Value *packed_in = UndefValue::get(vec_ty(i8_ty, 4));\n+  packed_in = insert_elt(packed_in, in0, (uint64_t)0);\n+  packed_in = insert_elt(packed_in, in1, (uint64_t)1);\n+  packed_in = insert_elt(packed_in, in2, (uint64_t)2);\n+  packed_in = insert_elt(packed_in, in3, (uint64_t)3);\n+  Value *in = bit_cast(packed_in, i32_ty);\n+  Value *ret = call(ptx, {in});\n+  Value *packed_ret0 = extract_val(ret, {0});\n+  Value *packed_ret1 = extract_val(ret, {1});\n+  Value *ret0 = extract_elt(packed_ret0, (uint64_t)0);\n+  Value *ret1 = extract_elt(packed_ret0, (uint64_t)1);\n+  Value *ret2 = extract_elt(packed_ret1, (uint64_t)0);\n+  Value *ret3 = extract_elt(packed_ret1, (uint64_t)1);\n+  return std::make_tuple(ret0, ret1, ret2, ret3);\n+}\n+\n+std::tuple<Value*, Value*, Value*, Value*> generator::bf16x4_to_fp8x4(Value *in0, Value *in1, Value *in2, Value *in3) {\n+  /* Assuming fp8 exponent offset is 16. bf16 exponent offset is 127.\n+     Max value in fp8: 0b01111111 (0x7f),\n+                  bf16: 3ff0\n+     Min value in fp8: 0b00000000 (0x00)\n+                  bf16: 0x3c00\n+     // @note: +0x8 is for \"rounding to nearest zero\"\n+     fp8 = (nosign(bf16) - (112 << 7) + 0x8) << 4;\n+     return fp8 | sign;  // also permute bytes\n+  */\n+  InlineAsm *ptx = InlineAsm::get(FunctionType::get({vec_ty(i8_ty, 4)}, {i32_ty, i32_ty}, false),\n+  \"{\\n\\t\"\n+  \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\\t\"\n+  \".reg .u32 fp8_min, fp8_max, rn_, zero; \\n\\t\"\n+  \"mov.u32 fp8_min, 0x38003800; \\n\\t\"\n+  \"mov.u32 fp8_max, 0x3ff03ff0; \\n\\t\"\n+  \"mov.u32 rn_, 0x80008; \\n\\t\"\n+  \"mov.u32 zero, 0; \\n\\t\"\n+  \"and.b32 sign0, $1, 0x80008000;  \\n\\t\"\n+  \"and.b32 sign1, $2, 0x80008000;  \\n\\t\"\n+  \"prmt.b32 sign, sign0, sign1, 0x7531; \\n\\t\"\n+  \"and.b32 nosign0, $1, 0x7fff7fff; \\n\\t\"\n+  \"and.b32 nosign1, $2, 0x7fff7fff; \\n\\t\"\n+\n+  \".reg .u32 nosign_0_<2>, nosign_1_<2>; \\n\\t\"  // nosign = clamp(nosign, min, max)\n+  \"and.b32 nosign_0_0, nosign0, 0xffff0000; \\n\\t\"\n+  \"max.u32 nosign_0_0, nosign_0_0, 0x38000000; \\n\\t\"\n+  \"min.u32 nosign_0_0, nosign_0_0, 0x3ff00000; \\n\\t\"\n+  \"and.b32 nosign_0_1, nosign0, 0x0000ffff; \\n\\t\"\n+  \"max.u32 nosign_0_1, nosign_0_1, 0x3800; \\n\\t\"\n+  \"min.u32 nosign_0_1, nosign_0_1, 0x3ff0; \\n\\t\"\n+  \"or.b32 nosign0, nosign_0_0, nosign_0_1; \\n\\t\"\n+  \"and.b32 nosign_1_0, nosign1, 0xffff0000; \\n\\t\"\n+  \"max.u32 nosign_1_0, nosign_1_0, 0x38000000; \\n\\t\"\n+  \"min.u32 nosign_1_0, nosign_1_0, 0x3ff00000; \\n\\t\"\n+  \"and.b32 nosign_1_1, nosign1, 0x0000ffff; \\n\\t\"\n+  \"max.u32 nosign_1_1, nosign_1_1, 0x3800; \\n\\t\"\n+  \"min.u32 nosign_1_1, nosign_1_1, 0x3ff0; \\n\\t\"\n+  \"or.b32 nosign1, nosign_1_0, nosign_1_1; \\n\\t\"\n+\n+  \"add.u32 nosign0, nosign0, rn_; \\n\\t\"  // round to nearest zero\n+  \"add.u32 nosign1, nosign1, rn_; \\n\\t\"\n+  \"sub.u32 nosign0, nosign0, 0x38003800; \\n\\t\"  // compensate offset\n+  \"sub.u32 nosign1, nosign1, 0x38003800; \\n\\t\"\n+  \"shr.u32 nosign0, nosign0, 4; \\n\\t\"\n+  \"shr.u32 nosign1, nosign1, 4; \\n\\t\"\n+  \"prmt.b32 nosign, nosign0, nosign1, 0x6420; \\n\\t\"\n+  \"or.b32 $0, nosign, sign; \\n\\t\"\n+  \"\"\n+  \"}\", \"=r,r,r\", false);\n+  Value *packed_in0 = UndefValue::get(vec_ty(bf16_ty, 2));\n+  Value *packed_in1 = UndefValue::get(vec_ty(bf16_ty, 2));\n+  packed_in0 = insert_elt(packed_in0, in0, (int)0);\n+  packed_in0 = insert_elt(packed_in0, in1, (int)1);\n+  packed_in1 = insert_elt(packed_in1, in2, (int)0);\n+  packed_in1 = insert_elt(packed_in1, in3, (int)1);\n+  Value *in_arg0 = bit_cast(packed_in0, i32_ty);\n+  Value *in_arg1 = bit_cast(packed_in1, i32_ty);\n+  Value *ret = call(ptx, {in_arg0, in_arg1});\n+  Value *ret0 = extract_elt(ret, (int)0);\n+  Value *ret1 = extract_elt(ret, (int)1);\n+  Value *ret2 = extract_elt(ret, (int)2);\n+  Value *ret3 = extract_elt(ret, (int)3);\n+  return std::make_tuple(ret0, ret1, ret2, ret3);\n+}\n+\n Value* generator::bf16_to_fp32(Value *in0){\n   if (tgt_->as_nvidia()->sm() >= 80) {\n     InlineAsm *ptx = InlineAsm::get(FunctionType::get(f32_ty, {bf16_ty}, false),\n@@ -685,6 +789,11 @@ void generator::visit_cast_inst(ir::cast_inst* x) {\n         return fp8x4_to_fp16x4(a, b, c, d);\n       if(op_sca_ty->is_fp8_ty() && ret_sca_ty->is_fp32_ty())\n         return fp8x4_to_fp32x4(a, b, c, d);\n+      // fp8 <> bf16\n+      if(op_sca_ty->is_fp8_ty() && ret_sca_ty->is_bf16_ty())\n+        return fp8x4_to_bf16x4(a, b, c, d);\n+      if (op_sca_ty->is_bf16_ty() && ret_sca_ty->is_fp8_ty())\n+        return bf16x4_to_fp8x4(a, b, c, d);\n       throw std::runtime_error(\"unsupported conversion\");\n     };\n     for(size_t i = 0; i < x_idxs.size(); i+=4){\n@@ -2125,15 +2234,10 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n     ldmatrix_ty = FunctionType::get(fp16x2_pack4_ty, std::vector<llvm::Type*>{smem_ptr_ty}, false);\n     phi_ty = fp16x2_ty;    \n   } else if (A_ir_ty->is_bf16_ty() && B_ir_ty->is_bf16_ty()) {\n-    // FIXME: We should use bf16 here.\n-    mma_ty = FunctionType::get(fp32_pack4_ty, std::vector<llvm::Type*>{fp16x2_ty, fp16x2_ty, fp16x2_ty, fp16x2_ty, fp16x2_ty, fp16x2_ty, fp32_ty, fp32_ty, fp32_ty, fp32_ty}, false);\n-    smem_ptr_ty = ptr_ty(f16_ty, 3);\n-    ldmatrix_ty = FunctionType::get(fp16x2_pack4_ty, std::vector<llvm::Type*>{smem_ptr_ty}, false);\n-    phi_ty = fp16x2_ty;\n-    // mma_ty = FunctionType::get(fp32_pack4_ty, std::vector<llvm::Type*>{bf16x2_ty, bf16x2_ty, bf16x2_ty, bf16x2_ty, bf16x2_ty, bf16x2_ty, fp32_ty, fp32_ty, fp32_ty, fp32_ty}, false);\n-    // smem_ptr_ty = ptr_ty(bf16_ty, 3);\n-    // ldmatrix_ty = FunctionType::get(bf16x2_pack4_ty, std::vector<llvm::Type*>{smem_ptr_ty}, false);\n-    // phi_ty = bf16x2_ty;\n+    mma_ty = FunctionType::get(fp32_pack4_ty, std::vector<llvm::Type*>{bf16x2_ty, bf16x2_ty, bf16x2_ty, bf16x2_ty, bf16x2_ty, bf16x2_ty, fp32_ty, fp32_ty, fp32_ty, fp32_ty}, false);\n+    smem_ptr_ty = ptr_ty(bf16_ty, 3);\n+    ldmatrix_ty = FunctionType::get(bf16x2_pack4_ty, std::vector<llvm::Type*>{smem_ptr_ty}, false);\n+    phi_ty = bf16x2_ty;\n   } else if (A_ir_ty->is_fp32_ty() && B_ir_ty->is_fp32_ty()) {\n     mma_ty = FunctionType::get(fp32_pack4_ty, std::vector<llvm::Type*>{fp32_ty, fp32_ty, fp32_ty, fp32_ty, fp32_ty, fp32_ty, fp32_ty, fp32_ty, fp32_ty, fp32_ty}, false);\n     smem_ptr_ty = ptr_ty(fp32_ty, 3);\n@@ -2233,10 +2337,10 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n       // std::cout << idxs_[A].size() << std::endl;\n       // std::cout << (m+1)*ldm + k*2 + 3 << std::endl;\n       // int ldm = num_rep_k*4;\n-      Value* ha0 = UndefValue::get(fp16x2_ty);\n-      Value* ha1 = UndefValue::get(fp16x2_ty);\n-      Value* ha2 = UndefValue::get(fp16x2_ty);\n-      Value* ha3 = UndefValue::get(fp16x2_ty);\n+      Value* ha0 = UndefValue::get(phi_ty);  // e.g., fp16x2\n+      Value* ha1 = UndefValue::get(phi_ty);\n+      Value* ha2 = UndefValue::get(phi_ty);\n+      Value* ha3 = UndefValue::get(phi_ty);\n       ha0 = builder_->CreateInsertElement(ha0, vals_[A][idxs_[A][(m+0)*ldm + k*2 + 0]], i32(0));\n       ha0 = builder_->CreateInsertElement(ha0, vals_[A][idxs_[A][(m+0)*ldm + k*2 + 1]], i32(1));\n       ha1 = builder_->CreateInsertElement(ha1, vals_[A][idxs_[A][(m+1)*ldm + k*2 + 0]], i32(0));"}, {"filename": "lib/codegen/transform/membar.cc", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -36,6 +36,9 @@ int membar::group_of(ir::value* v, std::vector<ir::value*> &async_write) {\n   else{\n     if(layouts_->has_tmp(v))\n       return async_write.size() - 1;\n+    // // Ignore copy_to_shared. It won't modify async behavior.\n+    // if(dynamic_cast<ir::copy_to_shared_inst*>(v))\n+    //   return 0;\n     auto it = std::find(async_write.begin(), async_write.end(), v);\n     return std::distance(async_write.begin(), it);\n   }"}, {"filename": "lib/ir/metadata.cc", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -3,10 +3,10 @@\n namespace triton{\n namespace ir{\n \n-metadata::metadata(kind_t kind, unsigned value)\n+metadata::metadata(kind_t kind, std::vector<unsigned> value)\n   : kind_(kind), value_(value) { }\n \n-metadata* metadata::get(kind_t kind, unsigned value) {\n+metadata* metadata::get(kind_t kind, std::vector<unsigned> value) {\n   return new metadata(kind, value);\n }\n "}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -625,13 +625,13 @@ void init_triton_ir(py::module &&m) {\n       .def(py::init<>());\n \n   py::class_<ir::value>(m, \"value\")\n-      .def(\"multiple_of\", [](ir::value *self, int val) {\n+      .def(\"multiple_of\", [](ir::value *self, std::vector<unsigned> val) {\n         if (auto *instr = dynamic_cast<ir::instruction*>(self)) {\n           instr->set_metadata(ir::metadata::multiple_of, val);\n         } else\n           throw std::runtime_error(\"multiple_of\");\n       })\n-      .def(\"max_contiguous\", [](ir::value *self, int val) {\n+      .def(\"max_contiguous\", [](ir::value *self, std::vector<unsigned> val) {\n         if (auto *instr = dynamic_cast<ir::instruction*>(self)) {\n           instr->set_metadata(ir::metadata::max_contiguous, val);\n         } else"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 8, "deletions": 6, "changes": 14, "file_content_changes": "@@ -719,8 +719,11 @@ def kernel(X, Z, BITCAST: tl.constexpr):\n         assert to_numpy(z_tri) == z_ref\n \n \n-def test_f8_f16_roundtrip():\n+@pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16])\n+def test_f8_xf16_roundtrip(dtype):\n     \"\"\"Tests that converting an f8 to f16 and back to f8 doesn't change its value\"\"\"\n+    check_type_supported(dtype)\n+\n     @triton.jit\n     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n@@ -732,21 +735,20 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device='cuda')\n     f8 = triton.reinterpret(f8_tensor, tl.float8)\n     n_elements = f8_tensor.numel()\n-    f16 = torch.empty_like(f8_tensor, dtype=torch.float16)\n+    xf16 = torch.empty_like(f8_tensor, dtype=dtype)\n     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n-    copy_kernel[grid](f8, f16, n_elements, BLOCK_SIZE=1024)\n+    copy_kernel[grid](f8, xf16, n_elements, BLOCK_SIZE=1024)\n \n-    f8_output_tensor = torch.empty_like(f16, dtype=torch.int8)\n+    f8_output_tensor = torch.empty_like(xf16, dtype=torch.int8)\n     f8_output = triton.reinterpret(f8_output_tensor, tl.float8)\n-    copy_kernel[grid](f16, f8_output, n_elements, BLOCK_SIZE=1024)\n+    copy_kernel[grid](xf16, f8_output, n_elements, BLOCK_SIZE=1024)\n \n     assert torch.all(f8_tensor == f8_output_tensor)\n \n \n def test_f16_to_f8_rounding():\n     \"\"\"Takes all float16s, converts them to float8 and back to float16. Checks that the absolute\n     error is the minimum over all float8.\n-\n     Or the same explanation a bit mathier:\n     for all f16 |f16 - fromf8(tof8(f16))| == min over all f8 |f16 - fromf8(f8)|\"\"\"\n     @triton.jit"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 20, "deletions": 6, "changes": 26, "file_content_changes": "@@ -1088,21 +1088,35 @@ def debug_barrier(_builder=None):\n \n \n @builtin\n-def multiple_of(input, value, _builder=None):\n+def multiple_of(input, values, _builder=None):\n     \"\"\"\n     Let the compiler knows that the values in :code:`input` are all multiples of :code:`value`.\n     \"\"\"\n-    value = _constexpr_to_value(value)\n-    return semantic.multiple_of(input, value)\n+    if isinstance(values, constexpr):\n+        values = [values]\n+    for i, d in enumerate(values):\n+        if not isinstance(d, constexpr):\n+            raise TypeError(f\"values element {i} must have type `constexpr`\")\n+        if not isinstance(d.value, int):\n+            raise TypeError(f\"values element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n+    values = [x.value for x in values]\n+    return semantic.multiple_of(input, values)\n \n \n @builtin\n-def max_contiguous(input, value, _builder=None):\n+def max_contiguous(input, values, _builder=None):\n     \"\"\"\n     Let the compiler knows that the `value` first values in :code:`input` are contiguous.\n     \"\"\"\n-    value = _constexpr_to_value(value)\n-    return semantic.max_contiguous(input, value)\n+    if isinstance(values, constexpr):\n+        values = [values]\n+    for i, d in enumerate(values):\n+        if not isinstance(d, constexpr):\n+            raise TypeError(f\"values element {i} must have type `constexpr`\")\n+        if not isinstance(d.value, int):\n+            raise TypeError(f\"values element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n+    values = [x.value for x in values]\n+    return semantic.max_contiguous(input, values)\n \n \n # -----------------------"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 15, "deletions": 4, "changes": 19, "file_content_changes": "@@ -581,6 +581,13 @@ def cast(input: tl.tensor,\n         return input\n     src_sca_ty = src_ty.scalar\n     dst_sca_ty = dst_ty.scalar\n+    # fp8 <=> bf16/fp16\n+    if (src_sca_ty.is_bf16() or src_sca_ty.is_fp16()) and dst_sca_ty.is_fp8():\n+        return tl.tensor(builder.create_fp_trunc(input.handle, dst_ty.to_ir(builder)),\n+                         dst_ty)\n+    if src_sca_ty.is_fp8() and (dst_sca_ty.is_bf16() or dst_sca_ty.is_fp16()):\n+        return tl.tensor(builder.create_fp_ext(input.handle, dst_ty.to_ir(builder)),\n+                         dst_ty)\n     # bf16 <=> (not fp32)\n     if (src_sca_ty.is_bf16() and not dst_sca_ty.is_fp32()) or \\\n        (dst_sca_ty.is_bf16() and not src_sca_ty.is_fp32()):\n@@ -1090,13 +1097,17 @@ def sqrt(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n \n ##\n \n-def multiple_of(x: tl.tensor, value: int) -> tl.tensor:\n-    x.handle.multiple_of(value)\n+def multiple_of(x: tl.tensor, values: List[int]) -> tl.tensor:\n+    if len(x.shape) != len(values):\n+        raise ValueError(\"Shape of input to multiple_of does not match the length of values\")\n+    x.handle.multiple_of(values)\n     return x\n \n \n-def max_contiguous(x: tl.tensor, value: int) -> tl.tensor:\n-    x.handle.max_contiguous(value)\n+def max_contiguous(x: tl.tensor, values: List[int]) -> tl.tensor:\n+    if len(x.shape) != len(values):\n+        raise ValueError(\"Shape of input to max_contiguous does not match the length of values\")\n+    x.handle.max_contiguous(values)\n     return x\n \n "}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 10, "deletions": 5, "changes": 15, "file_content_changes": "@@ -204,13 +204,16 @@ class _attention(torch.autograd.Function):\n     def forward(ctx, q, k, v, sm_scale):\n         BLOCK = 128\n         # shape constraints\n-        Lq, Lk = q.shape[-1], k.shape[-1]\n-        assert Lq == Lk\n+        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n+        assert Lq == Lk and Lk == Lv\n+        assert Lk in {16, 32, 64, 128}\n         o = torch.empty_like(q)\n         grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1])\n         tmp = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        num_warps = 4 if Lk <= 64 else 8\n+\n         _fwd_kernel[grid](\n             q, k, v, sm_scale,\n             tmp, L, m,\n@@ -221,14 +224,14 @@ def forward(ctx, q, k, v, sm_scale):\n             o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n             q.shape[0], q.shape[1], q.shape[2],\n             BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n-            BLOCK_DMODEL=64, num_warps=4,\n+            BLOCK_DMODEL=Lk, num_warps=num_warps,\n             num_stages=1,\n         )\n         ctx.save_for_backward(q, k, v, o, L, m)\n         ctx.BLOCK = BLOCK\n         ctx.grid = grid\n         ctx.sm_scale = sm_scale\n-        ctx.BLOCK_DMODEL = 64\n+        ctx.BLOCK_DMODEL = Lk\n         return o\n \n     @staticmethod\n@@ -245,6 +248,8 @@ def backward(ctx, do):\n             do_scaled, delta,\n             BLOCK_M=ctx.BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n         )\n+\n+        num_warps = 4 if ctx.BLOCK_DMODEL <= 64 else 8\n         _bwd_kernel[(ctx.grid[1],)](\n             q, k, v, ctx.sm_scale,\n             o, do_scaled,\n@@ -257,7 +262,7 @@ def backward(ctx, do):\n             q.shape[0], q.shape[1], q.shape[2],\n             ctx.grid[0],\n             BLOCK_M=ctx.BLOCK, BLOCK_N=ctx.BLOCK,\n-            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n+            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=num_warps,\n             num_stages=1,\n         )\n         return dq, dk, dv, None"}]