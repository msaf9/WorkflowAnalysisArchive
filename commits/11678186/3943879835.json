[{"filename": "CMakeLists.txt", "status": "modified", "additions": 32, "deletions": 31, "changes": 63, "file_content_changes": "@@ -188,8 +188,6 @@ add_subdirectory(include)\n add_subdirectory(lib)\n add_subdirectory(bin)\n \n-add_library(triton SHARED ${PYTHON_SRC})\n-\n # find_package(PythonLibs REQUIRED)\n \n set(TRITON_SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}\")\n@@ -198,37 +196,40 @@ set(TRITON_BINARY_DIR \"${CMAKE_CURRENT_BINARY_DIR}\")\n get_property(dialect_libs GLOBAL PROPERTY MLIR_DIALECT_LIBS)\n get_property(conversion_libs GLOBAL PROPERTY MLIR_CONVERSION_LIBS)\n \n-target_link_libraries(triton\n-  TritonAnalysis\n-  TritonTransforms\n-  TritonGPUTransforms\n-  TritonLLVMIR\n-  TritonPTX\n-  ${dialect_libs}\n-  ${conversion_libs}\n-  # optimizations\n-  MLIRPass\n-  MLIRTransforms\n-  MLIRLLVMIR\n-  MLIRSupport\n-  MLIRTargetLLVMIRExport\n-  MLIRExecutionEngine\n-  MLIRMathToLLVM\n-  MLIRNVVMToLLVMIRTranslation\n-  MLIRIR\n-)\n-\n-target_link_options(triton PRIVATE ${LLVM_LDFLAGS})\n-\n-if(WIN32)\n-    target_link_libraries(triton PRIVATE ${LLVM_LIBRARIES} dl) # dl is from dlfcn-win32\n-elseif(APPLE)\n-    target_link_libraries(triton ${LLVM_LIBRARIES} z)\n-else()\n-    target_link_libraries(triton ${LLVM_LIBRARIES} z stdc++fs)\n+if(TRITON_BUILD_PYTHON_MODULE)\n+  add_library(triton SHARED ${PYTHON_SRC})\n+\n+  target_link_libraries(triton\n+    TritonAnalysis\n+    TritonTransforms\n+    TritonGPUTransforms\n+    TritonLLVMIR\n+    TritonPTX\n+    ${dialect_libs}\n+    ${conversion_libs}\n+    # optimizations\n+    MLIRPass\n+    MLIRTransforms\n+    MLIRLLVMIR\n+    MLIRSupport\n+    MLIRTargetLLVMIRExport\n+    MLIRExecutionEngine\n+    MLIRMathToLLVM\n+    MLIRNVVMToLLVMIRTranslation\n+    MLIRIR\n+  )\n+\n+  target_link_options(triton PRIVATE ${LLVM_LDFLAGS})\n+\n+  if(WIN32)\n+      target_link_libraries(triton PRIVATE ${LLVM_LIBRARIES} dl) # dl is from dlfcn-win32\n+  elseif(APPLE)\n+      target_link_libraries(triton ${LLVM_LIBRARIES} z)\n+  else()\n+      target_link_libraries(triton ${LLVM_LIBRARIES} z stdc++fs)\n+  endif()\n endif()\n \n-\n if(TRITON_BUILD_PYTHON_MODULE AND NOT WIN32)\n     set(CMAKE_SHARED_LIBRARY_SUFFIX \".so\")\n     # Check if the platform is MacOS"}, {"filename": "bin/CMakeLists.txt", "status": "modified", "additions": 33, "deletions": 32, "changes": 65, "file_content_changes": "@@ -26,35 +26,36 @@ target_link_libraries(triton-opt PRIVATE\n mlir_check_all_link_libraries(triton-opt)\n \n \n-# add_llvm_executable(triton-translate triton-translate.cpp PARTIAL_SOURCES_INTENDED)\n-#llvm_update_compile_flags(triton-translate)\n-# target_link_libraries(triton-translate PRIVATE\n-#         TritonAnalysis\n-#         TritonTransforms\n-#         TritonGPUTransforms\n-#         TritonLLVMIR\n-#         TritonDriver\n-#         ${dialect_libs}\n-#         ${conversion_libs}\n-#         # tests\n-#         TritonTestAnalysis\n-\n-#         LLVMCore\n-#         LLVMSupport\n-#         LLVMOption\n-#         LLVMCodeGen\n-#         LLVMAsmParser\n-\n-#         # MLIR core\n-#         MLIROptLib\n-#         MLIRIR\n-#         MLIRPass\n-#         MLIRSupport\n-#         MLIRTransforms\n-#         MLIRExecutionEngine\n-#         MLIRMathToLLVM\n-#         MLIRTransformUtils\n-#         MLIRLLVMToLLVMIRTranslation\n-#         MLIRNVVMToLLVMIRTranslation\n-#         )\n-# mlir_check_all_link_libraries(triton-translate)\n+add_llvm_executable(triton-translate triton-translate.cpp PARTIAL_SOURCES_INTENDED)\n+llvm_update_compile_flags(triton-translate)\n+ target_link_libraries(triton-translate PRIVATE\n+         TritonAnalysis\n+         TritonTransforms\n+         TritonGPUTransforms\n+         TritonLLVMIR\n+         TritonPTX\n+         ${dialect_libs}\n+         ${conversion_libs}\n+         # tests\n+         TritonTestAnalysis\n+\n+         LLVMCore\n+         LLVMSupport\n+         LLVMOption\n+         LLVMCodeGen\n+         LLVMAsmParser\n+\n+         # MLIR core\n+         MLIROptLib\n+         MLIRIR\n+         MLIRLLVMIR\n+         MLIRPass\n+         MLIRSupport\n+         MLIRTransforms\n+         MLIRExecutionEngine\n+         MLIRMathToLLVM\n+         MLIRTransformUtils\n+         MLIRLLVMToLLVMIRTranslation\n+         MLIRNVVMToLLVMIRTranslation\n+         )\n+mlir_check_all_link_libraries(triton-translate)"}, {"filename": "bin/triton-translate.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -15,7 +15,7 @@\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n-#include \"triton/driver/llvm.h\"\n+#include \"triton/Target/PTX/PTXTranslation.h\"\n #include \"llvm/IR/LLVMContext.h\"\n #include \"llvm/Support/CommandLine.h\"\n #include \"llvm/Support/InitLLVM.h\"\n@@ -116,8 +116,8 @@ LogicalResult tritonTranslateMain(int argc, char **argv,\n   if (targetKind == \"llvmir\")\n     llvm::outs() << *llvmir << '\\n';\n   else if (targetKind == \"ptx\")\n-    llvm::outs() << ::triton::driver::llir_to_ptx(\n-        llvmir.get(), SMArch.getValue(), ptxVersion.getValue());\n+    llvm::outs() << ::triton::translateLLVMIRToPTX(*llvmir, SMArch.getValue(),\n+                                                   ptxVersion.getValue());\n \n   return success();\n }"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 34, "deletions": 6, "changes": 40, "file_content_changes": "@@ -376,31 +376,59 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n   );\n \n   let builders = [\n-    // specific for MMAV1(Volta)\n+     // Specially for MMAV1(Volta)\n+    AttrBuilder<(ins \"int\":$versionMajor,\n+                     \"int\":$numWarps,\n+                     \"int\":$id), [{\n+      assert(versionMajor == 1 && \"This builder is specially for versionMajor==1\");\n+      SmallVector<unsigned> wpt({static_cast<unsigned>(numWarps), 1});\n+      int versionMinor = 0;\n+\n+      assert(id < (1<<numBitsToHoldMmaV1ID) && \"MMAv1 ID exceeds the maximum\");\n+      for (int i = 0; i < numBitsToHoldMmaV1ID; ++i)\n+        versionMinor |= static_cast<bool>((1<<i) & id) * (1<<(4+i));\n+\n+      return $_get(context, versionMajor, versionMinor, wpt);\n+    }]>,\n+\n+    // Specially for MMAV1(Volta)\n     AttrBuilder<(ins \"int\":$versionMajor,\n                      \"ArrayRef<unsigned>\":$warpsPerCTA,\n                      \"ArrayRef<int64_t>\":$shapeA,\n                      \"ArrayRef<int64_t>\":$shapeB,\n                      \"bool\":$isARow,\n-                     \"bool\":$isBRow), [{\n-      assert(versionMajor == 1 && \"Only MMAv1 has multiple versionMinor.\");\n+                     \"bool\":$isBRow,\n+                     \"int\":$id), [{\n+      assert(versionMajor == 1 && \"This builder is specially for versionMajor==1\");\n       bool isAVec4 = !isARow && (shapeA[isARow] <= 16);\n       bool isBVec4 = isBRow && (shapeB[isBRow] <= 16);\n       // 4-bits to encode 4 booleans: [isARow, isBRow, isAVec4, isBVec4]\n       int versionMinor = (isARow * (1<<0)) |\\\n                          (isBRow * (1<<1)) |\\\n                          (isAVec4 * (1<<2)) |\\\n                          (isBVec4 * (1<<3));\n+\n+      assert(id < (1<<numBitsToHoldMmaV1ID) && \"MMAv1 ID exceeds the maximum\");\n+      for (int i = 0; i < numBitsToHoldMmaV1ID; ++i)\n+        versionMinor |= static_cast<bool>((1<<i) & id) * (1<<(4+i));\n+\n       return $_get(context, versionMajor, versionMinor, warpsPerCTA);\n     }]>\n-\n   ];\n \n   let extraClassDeclaration = extraBaseClassDeclaration # [{\n     bool isVolta() const;\n     bool isAmpere() const;\n-    // Get [isARow, isBRow, isAVec4, isBVec4] from versionMinor\n-    std::tuple<bool, bool, bool, bool> decodeVoltaLayoutStates() const;\n+    // Get [isARow, isBRow, isAVec4, isBVec4, id] from versionMinor\n+    std::tuple<bool, bool, bool, bool, int> decodeVoltaLayoutStates() const;\n+    // Number of bits in versionMinor to hold the ID of the MMA encoding instance.\n+    // Here 5 bits can hold 32 IDs in a single module.\n+    static constexpr int numBitsToHoldMmaV1ID{5};\n+\n+    // Here is a temporary flag that indicates whether we need to update the warpsPerCTA for MMAv1, since the current backend cannot support the updated wpt.\n+    // The mmav1's wpt-related logic is separated into multiple files, so a global flag is added here for universal coordination.\n+    // TODO[Superjomn]: Remove this flag once the MMAv1 backend is ready.\n+    static constexpr bool _mmaV1UpdateWpt{false};\n   }];\n \n }"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -21,6 +21,8 @@ std::unique_ptr<Pass> createTritonGPUCombineOpsPass(int computeCapability = 80);\n \n std::unique_ptr<Pass> createTritonGPUVerifier();\n \n+std::unique_ptr<Pass> createTritonGPUUpdateMmaForVoltaPass();\n+\n /// Generate the code for registering passes.\n #define GEN_PASS_REGISTRATION\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\""}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.td", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -108,4 +108,16 @@ def TritonGPUCanonicalizeLoops: Pass<\"tritongpu-canonicalize-loops\", \"mlir::Modu\n   let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\"];\n }\n \n+def UpdateMmaForVolta : Pass<\"tritongpu-update-mma-for-volta\", \"mlir::ModuleOp\"> {\n+  let summary = \"Update mma encodings for Volta\";\n+\n+  let description = [{\n+    This helps to update the mma encodings for Volta.\n+  }];\n+\n+  let constructor = \"mlir::createTritonGPUUpdateMmaForVoltaPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\"];\n+}\n+\n #endif"}, {"filename": "lib/Analysis/CMakeLists.txt", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -6,5 +6,6 @@ add_mlir_library(TritonAnalysis\n   Utility.cpp\n \n   DEPENDS\n+  TritonTableGen\n   TritonGPUAttrDefsIncGen\n-)\n\\ No newline at end of file\n+)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.h", "status": "modified", "additions": 43, "deletions": 50, "changes": 93, "file_content_changes": "@@ -93,6 +93,13 @@ struct DotOpMmaV1ConversionHelper {\n \n   static ArrayRef<unsigned> getMmaInstrShape() { return instrShape; }\n \n+  static Type getMatType(TensorType operand) {\n+    auto *ctx = operand.getContext();\n+    Type fp16Ty = type::f16Ty(ctx);\n+    Type vecTy = vec_ty(fp16Ty, 2);\n+    return struct_ty(SmallVector<Type>{vecTy});\n+  }\n+\n   static Type getMmaRetType(TensorType operand) {\n     auto *ctx = operand.getContext();\n     Type fp32Ty = type::f32Ty(ctx);\n@@ -538,17 +545,17 @@ struct DotOpMmaV2ConversionHelper {\n \n   // The type of matrix that loaded by either a ldmatrix or composed lds.\n   Type getMatType() const {\n-    Type fp32Ty = type::f32Ty(ctx);\n+    // floating point types\n+    Type fp32x1Ty = vec_ty(type::f32Ty(ctx), 1);\n     Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n     Type i16x2Ty = vec_ty(type::i16Ty(ctx), 2);\n-    // floating point types\n     Type fp16x2Pack4Ty =\n         LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp16x2Ty));\n     // LLVM 14.0 does not support bf16 type, so we use i16 instead.\n     Type bf16x2Pack4Ty =\n         LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i16x2Ty));\n     Type fp32Pack4Ty =\n-        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n+        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32x1Ty));\n     // integer types\n     Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n     Type i8x4Pack4Ty =\n@@ -960,7 +967,7 @@ class MMA16816SmemLoader {\n   // Load 4 matrices and returns 4 vec<2> elements.\n   std::tuple<Value, Value, Value, Value>\n   loadX4(int mat0, int mat1, ArrayRef<Value> offs, ArrayRef<Value> ptrs,\n-         Type ldmatrixRetTy, Type shemPtrTy) const {\n+         Type matTy, Type shemPtrTy) const {\n     assert(mat0 % 2 == 0 && mat1 % 2 == 0 &&\n            \"smem matrix load must be aligned\");\n     int matIdx[2] = {mat0, mat1};\n@@ -983,6 +990,9 @@ class MMA16816SmemLoader {\n \n     Value ptr = getPtr(ptrIdx);\n \n+    // The struct should have exactly the same element types.\n+    Type elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n+\n     if (canUseLdmatrix) {\n       Value sOffset =\n           mul(i32_val(matIdx[order[1]] * sMatStride * sMatShape), sStride);\n@@ -1000,20 +1010,12 @@ class MMA16816SmemLoader {\n       ldmatrix(resArgs, addrArg);\n \n       // The result type is 4xi32, each i32 is composed of 2xf16\n-      // elements(adjacent two columns in a row)\n-      Value resV4 = builder.launch(rewriter, loc, ldmatrixRetTy);\n-\n-      auto getIntAttr = [&](int v) {\n-        return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n-      };\n-\n-      // The struct should have exactly the same element types.\n-      Type elemType = resV4.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n-\n-      return {extract_val(elemType, resV4, getIntAttr(0)),\n-              extract_val(elemType, resV4, getIntAttr(1)),\n-              extract_val(elemType, resV4, getIntAttr(2)),\n-              extract_val(elemType, resV4, getIntAttr(3))};\n+      // elements (adjacent two columns in a row) or a single f32 element.\n+      Value resV4 = builder.launch(rewriter, loc, matTy);\n+      return {extract_val(elemTy, resV4, i32_arr_attr(0)),\n+              extract_val(elemTy, resV4, i32_arr_attr(1)),\n+              extract_val(elemTy, resV4, i32_arr_attr(2)),\n+              extract_val(elemTy, resV4, i32_arr_attr(3))};\n     } else if (elemBytes == 4 &&\n                needTrans) { // Use lds.32 to load tf32 matrices\n       Value ptr2 = getPtr(ptrIdx + 1);\n@@ -1025,21 +1027,23 @@ class MMA16816SmemLoader {\n           add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n \n       Value elems[4];\n-      Type elemTy = type::f32Ty(ctx);\n-      Type elemPtrTy = ptr_ty(elemTy);\n       if (kOrder == 1) {\n-        elems[0] = load(gep(elemPtrTy, ptr, sOffsetElemVal));\n-        elems[1] = load(gep(elemPtrTy, ptr2, sOffsetElemVal));\n-        elems[2] = load(gep(elemPtrTy, ptr, sOffsetArrElemVal));\n-        elems[3] = load(gep(elemPtrTy, ptr2, sOffsetArrElemVal));\n+        elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n+        elems[1] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n+        elems[2] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n+        elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n       } else {\n-        elems[0] = load(gep(elemPtrTy, ptr, sOffsetElemVal));\n-        elems[2] = load(gep(elemPtrTy, ptr2, sOffsetElemVal));\n-        elems[1] = load(gep(elemPtrTy, ptr, sOffsetArrElemVal));\n-        elems[3] = load(gep(elemPtrTy, ptr2, sOffsetArrElemVal));\n+        elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n+        elems[2] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n+        elems[1] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n+        elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n       }\n-      return {elems[0], elems[1], elems[2], elems[3]};\n-\n+      std::array<Value, 4> retElems;\n+      retElems.fill(undef(elemTy));\n+      for (auto i = 0; i < 4; ++i) {\n+        retElems[i] = insert_element(elemTy, retElems[i], elems[i], i32_val(0));\n+      }\n+      return {retElems[0], retElems[1], retElems[2], retElems[3]};\n     } else if (elemBytes == 1 && needTrans) { // work with int8\n       std::array<std::array<Value, 4>, 2> ptrs;\n       ptrs[0] = {\n@@ -1064,49 +1068,42 @@ class MMA16816SmemLoader {\n           add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n \n       std::array<Value, 4> i8v4Elems;\n-      std::array<Value, 4> i32Elems;\n-      i8v4Elems.fill(\n-          rewriter.create<LLVM::UndefOp>(loc, vec_ty(type::i8Ty(ctx), 4)));\n+      i8v4Elems.fill(undef(elemTy));\n \n       Value i8Elems[4][4];\n-      Type elemTy = type::i8Ty(ctx);\n-      Type elemPtrTy = ptr_ty(elemTy);\n-      Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n       if (kOrder == 1) {\n         for (int i = 0; i < 2; ++i)\n           for (int j = 0; j < 4; ++j)\n-            i8Elems[i][j] = load(gep(elemPtrTy, ptrs[i][j], sOffsetElemVal));\n+            i8Elems[i][j] = load(gep(shemPtrTy, ptrs[i][j], sOffsetElemVal));\n \n         for (int i = 2; i < 4; ++i)\n           for (int j = 0; j < 4; ++j)\n             i8Elems[i][j] =\n-                load(gep(elemPtrTy, ptrs[i - 2][j], sOffsetArrElemVal));\n+                load(gep(shemPtrTy, ptrs[i - 2][j], sOffsetArrElemVal));\n \n         for (int m = 0; m < 4; ++m) {\n           for (int e = 0; e < 4; ++e)\n             i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n                                           i8Elems[m][e], i32_val(e));\n-          i32Elems[m] = bitcast(i8v4Elems[m], i8x4Ty);\n         }\n       } else { // k first\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[0][j] = load(gep(elemPtrTy, ptrs[0][j], sOffsetElemVal));\n+          i8Elems[0][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetElemVal));\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[2][j] = load(gep(elemPtrTy, ptrs[1][j], sOffsetElemVal));\n+          i8Elems[2][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetElemVal));\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[1][j] = load(gep(elemPtrTy, ptrs[0][j], sOffsetArrElemVal));\n+          i8Elems[1][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetArrElemVal));\n         for (int j = 0; j < 4; ++j)\n-          i8Elems[3][j] = load(gep(elemPtrTy, ptrs[1][j], sOffsetArrElemVal));\n+          i8Elems[3][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetArrElemVal));\n \n         for (int m = 0; m < 4; ++m) {\n           for (int e = 0; e < 4; ++e)\n             i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n                                           i8Elems[m][e], i32_val(e));\n-          i32Elems[m] = bitcast(i8v4Elems[m], i8x4Ty);\n         }\n       }\n \n-      return {i32Elems[0], i32Elems[1], i32Elems[2], i32Elems[3]};\n+      return {i8v4Elems[0], i8v4Elems[1], i8v4Elems[2], i8v4Elems[3]};\n     }\n \n     assert(false && \"Invalid smem load\");\n@@ -1415,14 +1412,10 @@ struct MMA16816ConversionHelper {\n       mma(retArgs, aArgs, bArgs, cArgs);\n       Value mmaOut = builder.launch(rewriter, loc, helper.getMmaRetType());\n \n-      auto getIntAttr = [&](int v) {\n-        return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n-      };\n-\n       Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n       for (int i = 0; i < 4; ++i)\n         fc[m * colsPerThread + 4 * n + i] =\n-            extract_val(elemTy, mmaOut, getIntAttr(i));\n+            extract_val(elemTy, mmaOut, i32_arr_attr(i));\n     };\n \n     for (int k = 0; k < numRepK; ++k)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -203,12 +203,8 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n       Value res =\n           builder.launch(rewriter, loc, helper.getMmaRetType(ATensorTy));\n \n-      auto getIntAttr = [&](int v) {\n-        return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n-      };\n-\n-      for (unsigned i = 0; i < 8; i++) {\n-        Value elem = extract_val(f32_ty, res, getIntAttr(i));\n+      for (auto i = 0; i < 8; i++) {\n+        Value elem = extract_val(f32_ty, res, i32_arr_attr(i));\n         acc[idx[i]] = elem;\n         resVals[(m * numN / 2 + n) * 8 + i] = elem;\n       }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 4, "deletions": 8, "changes": 12, "file_content_changes": "@@ -49,10 +49,8 @@ struct FpToFpOpConversion\n         struct_ty(SmallVector<Type>{fp16x2VecTy, fp16x2VecTy});\n     auto fp16x2x2Struct =\n         builder.launch(rewriter, loc, fp16x2x2StructTy, false);\n-    auto fp16x2Vec0 =\n-        extract_val(fp16x2VecTy, fp16x2x2Struct, rewriter.getI32ArrayAttr({0}));\n-    auto fp16x2Vec1 =\n-        extract_val(fp16x2VecTy, fp16x2x2Struct, rewriter.getI32ArrayAttr({1}));\n+    auto fp16x2Vec0 = extract_val(fp16x2VecTy, fp16x2x2Struct, i32_arr_attr(0));\n+    auto fp16x2Vec1 = extract_val(fp16x2VecTy, fp16x2x2Struct, i32_arr_attr(1));\n     return {extract_element(f16_ty, fp16x2Vec0, i32_val(0)),\n             extract_element(f16_ty, fp16x2Vec0, i32_val(1)),\n             extract_element(f16_ty, fp16x2Vec1, i32_val(0)),\n@@ -143,10 +141,8 @@ struct FpToFpOpConversion\n         struct_ty(SmallVector<Type>{bf16x2VecTy, bf16x2VecTy});\n     auto bf16x2x2Struct =\n         builder.launch(rewriter, loc, bf16x2x2StructTy, false);\n-    auto bf16x2Vec0 =\n-        extract_val(bf16x2VecTy, bf16x2x2Struct, rewriter.getI32ArrayAttr({0}));\n-    auto bf16x2Vec1 =\n-        extract_val(bf16x2VecTy, bf16x2x2Struct, rewriter.getI32ArrayAttr({1}));\n+    auto bf16x2Vec0 = extract_val(bf16x2VecTy, bf16x2x2Struct, i32_arr_attr(0));\n+    auto bf16x2Vec1 = extract_val(bf16x2VecTy, bf16x2x2Struct, i32_arr_attr(1));\n     return {extract_element(i16_ty, bf16x2Vec0, i32_val(0)),\n             extract_element(i16_ty, bf16x2Vec0, i32_val(1)),\n             extract_element(i16_ty, bf16x2Vec1, i32_val(0)),"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -214,7 +214,7 @@ struct LoadOpConversion\n         Value curr;\n         if (retTy.isa<LLVM::LLVMStructType>()) {\n           curr = extract_val(IntegerType::get(getContext(), width), ret,\n-                             rewriter.getI64ArrayAttr(ii));\n+                             i64_arr_attr(ii));\n         } else {\n           curr = ret;\n         }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.h", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -90,7 +90,7 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n         Type elemTy = convertType(type.getElementType());\n         if (mmaLayout.isAmpere()) {\n           const llvm::DenseMap<int, Type> targetTyMap = {\n-              {32, elemTy},\n+              {32, vec_ty(elemTy, 1)},\n               {16, vec_ty(elemTy, 2)},\n               {8, vec_ty(elemTy, 4)},\n           };\n@@ -103,8 +103,7 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n           if (dotOpLayout.getOpIdx() == 0) { // $a\n             auto elems =\n                 MMA16816ConversionHelper::getANumElemsPerThread(type, wpt[0]);\n-            return LLVM::LLVMStructType::getLiteral(\n-                ctx, SmallVector<Type>(elems, targetTy));\n+            return struct_ty(SmallVector<Type>(elems, targetTy));\n           }\n           if (dotOpLayout.getOpIdx() == 1) { // $b\n             auto elems ="}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -95,6 +95,10 @@\n                             __VA_ARGS__)\n #define tid_val() getThreadId(rewriter, loc)\n \n+// Attributes\n+#define i32_arr_attr(...) rewriter.getI32ArrayAttr({__VA_ARGS__})\n+#define i64_arr_attr(...) rewriter.getI64ArrayAttr({__VA_ARGS__})\n+\n namespace mlir {\n namespace triton {\n \n@@ -191,7 +195,7 @@ getElementsFromStruct(Location loc, Value llvmStruct,\n   SmallVector<Value> results(types.size());\n   for (unsigned i = 0; i < types.size(); ++i) {\n     Type type = types[i];\n-    results[i] = extract_val(type, llvmStruct, rewriter.getI64ArrayAttr(i));\n+    results[i] = extract_val(type, llvmStruct, i64_arr_attr(i));\n   }\n   return results;\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 19, "deletions": 2, "changes": 21, "file_content_changes": "@@ -62,28 +62,45 @@ struct SplatOpConversion\n     auto tensorTy = resType.cast<RankedTensorType>();\n     auto shape = tensorTy.getShape();\n     auto parent = layout.getParent();\n+    Value retVal = constVal;\n+    Type retTy = elemType;\n     int numElems{};\n     if (auto mmaLayout = parent.dyn_cast<MmaEncodingAttr>()) {\n+      Type matTy;\n       if (mmaLayout.isAmpere()) {\n         numElems = layout.getOpIdx() == 0\n                        ? MMA16816ConversionHelper::getANumElemsPerThread(\n                              tensorTy, mmaLayout.getWarpsPerCTA()[0])\n                        : MMA16816ConversionHelper::getBNumElemsPerThread(\n                              tensorTy, mmaLayout.getWarpsPerCTA()[1]);\n+        DotOpMmaV2ConversionHelper helper(mmaLayout);\n+        helper.deduceMmaType(tensorTy);\n+        matTy = helper.getMatType();\n       } else if (mmaLayout.isVolta()) {\n         DotOpMmaV1ConversionHelper helper(mmaLayout);\n         numElems = layout.getOpIdx() == 0\n                        ? helper.numElemsPerThreadA(shape, {0, 1})\n                        : helper.numElemsPerThreadB(shape, {0, 1});\n+        matTy = helper.getMatType(tensorTy);\n+      }\n+      auto numPackedElems = matTy.cast<LLVM::LLVMStructType>()\n+                                .getBody()[0]\n+                                .cast<VectorType>()\n+                                .getNumElements();\n+      retTy = vec_ty(elemType, numPackedElems);\n+      retVal = undef(retTy);\n+      for (auto i = 0; i < numPackedElems; ++i) {\n+        retVal = insert_element(retTy, retVal, constVal, i32_val(i));\n       }\n     } else if (auto blockedLayout = parent.dyn_cast<BlockedEncodingAttr>()) {\n       numElems = DotOpFMAConversionHelper::getNumElemsPerThread(shape, layout);\n     } else {\n       assert(false && \"Unsupported layout found\");\n     }\n+\n     auto structTy = LLVM::LLVMStructType::getLiteral(\n-        rewriter.getContext(), SmallVector<Type>(numElems, elemType));\n-    return getStructFromElements(loc, SmallVector<Value>(numElems, constVal),\n+        rewriter.getContext(), SmallVector<Type>(numElems, retTy));\n+    return getStructFromElements(loc, SmallVector<Value>(numElems, retVal),\n                                  rewriter, structTy);\n   }\n "}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 8, "deletions": 3, "changes": 11, "file_content_changes": "@@ -600,15 +600,20 @@ bool MmaEncodingAttr::isVolta() const { return getVersionMajor() == 1; }\n \n bool MmaEncodingAttr::isAmpere() const { return getVersionMajor() == 2; }\n \n-// Get [isARow, isBRow, isAVec4, isBVec4] from versionMinor\n-std::tuple<bool, bool, bool, bool>\n+// Get [isARow, isBRow, isAVec4, isBVec4, id] from versionMinor\n+std::tuple<bool, bool, bool, bool, int>\n MmaEncodingAttr::decodeVoltaLayoutStates() const {\n   unsigned versionMinor = getVersionMinor();\n   bool isARow = versionMinor & (1 << 0);\n   bool isBRow = versionMinor & (1 << 1);\n   bool isAVec4 = versionMinor & (1 << 2);\n   bool isBVec4 = versionMinor & (1 << 3);\n-  return std::make_tuple(isARow, isBRow, isAVec4, isBVec4);\n+\n+  int id = 0;\n+  for (int i = numBitsToHoldMmaV1ID - 1; i >= 0; --i)\n+    id = (id << 1) + static_cast<bool>(versionMinor & (1 << (4 + i)));\n+\n+  return std::make_tuple(isARow, isBRow, isAVec4, isBVec4, id);\n }\n \n //===----------------------------------------------------------------------===//"}, {"filename": "lib/Dialect/TritonGPU/Transforms/CMakeLists.txt", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -11,6 +11,8 @@ add_mlir_dialect_library(TritonGPUTransforms\n   ReorderInstructions.cpp\n   DecomposeConversions.cpp\n   TritonGPUConversion.cpp\n+  UpdateMmaForVolta.cpp\n+  Utility.cpp\n \n   DEPENDS\n   TritonGPUTransformsIncGen"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 36, "deletions": 285, "changes": 321, "file_content_changes": "@@ -1,3 +1,4 @@\n+#include \"Utility.h\"\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/SCF/SCF.h\"\n #include \"mlir/IR/BlockAndValueMapping.h\"\n@@ -26,6 +27,7 @@ using triton::DotOp;\n using triton::gpu::ConvertLayoutOp;\n using triton::gpu::DotOperandEncodingAttr;\n using triton::gpu::MmaEncodingAttr;\n+using triton::gpu::SliceEncodingAttr;\n \n // -----------------------------------------------------------------------------\n //\n@@ -885,24 +887,31 @@ SmallVector<int64_t, 2> mmaVersionToShapePerWarp(int version) {\n \n SmallVector<unsigned, 2> warpsPerTileV1(const ArrayRef<int64_t> shape,\n                                         int numWarps) {\n-  SmallVector<unsigned, 2> ret = {1, 1};\n-  SmallVector<int64_t, 2> shapePerWarp =\n-      mmaVersionToShapePerWarp(1 /*version*/);\n-  bool changed = false;\n-  do {\n-    changed = false;\n-    int pre = ret[0];\n-    if (ret[0] * ret[1] < numWarps) {\n-      ret[0] = std::clamp<unsigned>(ret[0] * 2, 1, shape[0] / shapePerWarp[0]);\n-      changed = pre != ret[0];\n-    }\n-    if (ret[0] * ret[1] < numWarps) {\n-      pre = ret[1];\n-      ret[1] = std::clamp<unsigned>(ret[1] * 2, 1, shape[1] / shapePerWarp[1]);\n-      changed = pre != ret[1];\n-    }\n-  } while (changed);\n-  return ret;\n+  if (!MmaEncodingAttr::_mmaV1UpdateWpt) {\n+    SmallVector<unsigned, 2> ret = {1, 1};\n+    SmallVector<int64_t, 2> shapePerWarp =\n+        mmaVersionToShapePerWarp(1 /*version*/);\n+    bool changed = false;\n+    do {\n+      changed = false;\n+      int pre = ret[0];\n+      if (ret[0] * ret[1] < numWarps) {\n+        ret[0] =\n+            std::clamp<unsigned>(ret[0] * 2, 1, shape[0] / shapePerWarp[0]);\n+        changed = pre != ret[0];\n+      }\n+      if (ret[0] * ret[1] < numWarps) {\n+        pre = ret[1];\n+        ret[1] =\n+            std::clamp<unsigned>(ret[1] * 2, 1, shape[1] / shapePerWarp[1]);\n+        changed = pre != ret[1];\n+      }\n+    } while (changed);\n+    return ret;\n+  } else {\n+    // Set a default value and ensure product of wpt equals numWarps\n+    return {static_cast<unsigned>(numWarps), 1};\n+  }\n }\n \n SmallVector<unsigned, 2> warpsPerTileV2(triton::DotOp dotOp,\n@@ -1039,6 +1048,7 @@ class OptimizeConvertToDotOperand : public mlir::RewritePattern {\n \n class BlockedToMMA : public mlir::RewritePattern {\n   int computeCapability;\n+  mutable int mmaV1Counter{}; // used to generate ID for MMAv1 encoding\n \n public:\n   BlockedToMMA(mlir::MLIRContext *context, int computeCapability)\n@@ -1097,13 +1107,13 @@ class BlockedToMMA : public mlir::RewritePattern {\n         getWarpsPerTile(dotOp, retShape, versionMajor, numWarps);\n     triton::gpu::MmaEncodingAttr mmaEnc;\n     if (versionMajor == 1) {\n-      auto shapeA = AType.getShape();\n-      auto shapeB = BType.getShape();\n-      bool isARow = AOrder[0] != 0;\n-      bool isBRow = BOrder[0] != 0;\n-      mmaEnc = triton::gpu::MmaEncodingAttr::get(\n-          oldRetType.getContext(), versionMajor, warpsPerTile, shapeA, shapeB,\n-          isARow, isBRow);\n+      if (MmaEncodingAttr::_mmaV1UpdateWpt)\n+        mmaEnc = triton::gpu::MmaEncodingAttr::get(\n+            oldRetType.getContext(), versionMajor, numWarps, mmaV1Counter++);\n+      else\n+        mmaEnc = triton::gpu::MmaEncodingAttr::get(\n+            dotOp->getContext(), versionMajor, 0 /*versionMinor*/,\n+            warpsPerTileV1(retShape, numWarps));\n     } else if (versionMajor == 2) {\n       mmaEnc = triton::gpu::MmaEncodingAttr::get(\n           oldRetType.getContext(), versionMajor, 0 /*versionMinor*/,\n@@ -1159,102 +1169,6 @@ class BlockedToMMA : public mlir::RewritePattern {\n   }\n };\n \n-class FixupLoop : public mlir::RewritePattern {\n-\n-public:\n-  explicit FixupLoop(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(scf::ForOp::getOperationName(), 2, context) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto forOp = cast<scf::ForOp>(op);\n-\n-    // Rewrite init argument\n-    SmallVector<Value, 4> newInitArgs = forOp.getInitArgs();\n-    bool shouldRematerialize = false;\n-    for (size_t i = 0; i < newInitArgs.size(); i++) {\n-      auto initArg = newInitArgs[i];\n-      auto regionArg = forOp.getRegionIterArgs()[i];\n-      if (newInitArgs[i].getType() != forOp.getRegionIterArgs()[i].getType() ||\n-          newInitArgs[i].getType() != forOp.getResultTypes()[i]) {\n-        shouldRematerialize = true;\n-        break;\n-      }\n-    }\n-    if (!shouldRematerialize)\n-      return failure();\n-\n-    scf::ForOp newForOp = rewriter.create<scf::ForOp>(\n-        forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n-        forOp.getStep(), newInitArgs);\n-    newForOp->moveBefore(forOp);\n-    rewriter.setInsertionPointToStart(newForOp.getBody());\n-    BlockAndValueMapping mapping;\n-    for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n-      mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n-    mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n-\n-    for (Operation &op : forOp.getBody()->getOperations()) {\n-      rewriter.clone(op, mapping);\n-    }\n-    rewriter.replaceOp(forOp, newForOp.getResults());\n-    return success();\n-  }\n-};\n-\n-// This pattern collects the wrong Mma those need to update and create the right\n-// ones for each.\n-class CollectMmaToUpdateForVolta : public mlir::RewritePattern {\n-  DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n-\n-public:\n-  CollectMmaToUpdateForVolta(\n-      mlir::MLIRContext *ctx,\n-      DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n-      : mlir::RewritePattern(triton::DotOp::getOperationName(), 1, ctx),\n-        mmaToUpdate(mmaToUpdate) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-\n-    auto dotOp = cast<triton::DotOp>(op);\n-    auto *ctx = dotOp->getContext();\n-    auto AT = dotOp.a().getType().cast<RankedTensorType>();\n-    auto BT = dotOp.b().getType().cast<RankedTensorType>();\n-    auto DT = dotOp.d().getType().cast<RankedTensorType>();\n-    if (!DT.getEncoding())\n-      return failure();\n-    auto mmaLayout = DT.getEncoding().dyn_cast<MmaEncodingAttr>();\n-    if (!(mmaLayout && mmaLayout.isVolta()))\n-      return failure();\n-\n-    // Has processed.\n-    if (mmaToUpdate.count(mmaLayout))\n-      return failure();\n-\n-    auto dotOperandA = AT.getEncoding().cast<DotOperandEncodingAttr>();\n-    auto dotOperandB = BT.getEncoding().cast<DotOperandEncodingAttr>();\n-    bool isARow = dotOperandA.getIsMMAv1Row().cast<BoolAttr>().getValue();\n-    bool isBRow = dotOperandB.getIsMMAv1Row().cast<BoolAttr>().getValue();\n-    auto [isARow_, isBRow_, isAVec4, isBVec4] =\n-        mmaLayout.decodeVoltaLayoutStates();\n-    if (isARow_ == isARow && isBRow_ == isBRow) {\n-      return failure(); // No need to update\n-    }\n-\n-    auto newMmaLayout = MmaEncodingAttr::get(\n-        ctx, mmaLayout.getVersionMajor(), mmaLayout.getWarpsPerCTA(),\n-        AT.getShape(), BT.getShape(), isARow, isBRow);\n-\n-    // Collect the wrong MMA Layouts, and mark need to update.\n-    mmaToUpdate.try_emplace(mmaLayout, newMmaLayout);\n-\n-    return failure();\n-  }\n-};\n-\n // Convert + trans + convert\n // x = convert_layout distributed -> #shared_x\n // y = trans x -> #shared_y\n@@ -1359,145 +1273,6 @@ class ConvertDotConvert : public mlir::RewritePattern {\n   }\n };\n \n-// Correct the versionMinor field in MmaEncodingAttr for Volta.\n-class UpdateMMAVersionMinorForVolta : public mlir::RewritePattern {\n-  const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n-  enum class Kind {\n-    kUnk,\n-    kCvtToMma,\n-    kCvtToDotOp,\n-    kDot,\n-    kConstant,\n-  };\n-  mutable Kind rewriteKind{Kind::kUnk};\n-\n-public:\n-  UpdateMMAVersionMinorForVolta(\n-      mlir::MLIRContext *ctx, llvm::StringRef opName,\n-      const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n-      : RewritePattern(opName, 1 /*benefit*/, ctx), mmaToUpdate(mmaToUpdate) {}\n-\n-  LogicalResult match(Operation *op) const override {\n-    MmaEncodingAttr mma;\n-    if (mmaToUpdate.empty())\n-      return failure();\n-    if (op->getNumResults() != 1)\n-      return failure();\n-    auto tensorTy = op->getResult(0).getType().dyn_cast<RankedTensorType>();\n-    if (!tensorTy)\n-      return failure();\n-\n-    // ConvertLayoutOp\n-    if (auto cvt = llvm::dyn_cast<ConvertLayoutOp>(op)) {\n-      // cvt X -> dot_operand\n-      if (auto dotOperand =\n-              tensorTy.getEncoding().dyn_cast<DotOperandEncodingAttr>()) {\n-        mma = dotOperand.getParent().dyn_cast<MmaEncodingAttr>();\n-        rewriteKind = Kind::kCvtToDotOp;\n-        if (mma && mmaToUpdate.count(mma))\n-          return success();\n-      }\n-      if ((mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>())) {\n-        // cvt X -> mma\n-        rewriteKind = Kind::kCvtToMma;\n-        if (mma && mmaToUpdate.count(mma))\n-          return success();\n-      }\n-    } else if (auto dot = llvm::dyn_cast<DotOp>(op)) {\n-      // DotOp\n-      mma = dot.d()\n-                .getType()\n-                .cast<RankedTensorType>()\n-                .getEncoding()\n-                .dyn_cast<MmaEncodingAttr>();\n-      rewriteKind = Kind::kDot;\n-    } else if (auto constant = llvm::dyn_cast<arith::ConstantOp>(op)) {\n-      // ConstantOp\n-      mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>();\n-      rewriteKind = Kind::kConstant;\n-    }\n-\n-    return success(mma && mmaToUpdate.count(mma));\n-  }\n-\n-  void rewrite(Operation *op, PatternRewriter &rewriter) const override {\n-    switch (rewriteKind) {\n-    case Kind::kDot:\n-      rewriteDot(op, rewriter);\n-      break;\n-    case Kind::kConstant:\n-      rewriteConstant(op, rewriter);\n-      break;\n-    case Kind::kCvtToDotOp:\n-      rewriteCvtDotOp(op, rewriter);\n-      break;\n-    case Kind::kCvtToMma:\n-      rewriteCvtToMma(op, rewriter);\n-      break;\n-    default:\n-      llvm::report_fatal_error(\"Not supported rewrite kind\");\n-    }\n-  }\n-\n-private:\n-  void rewriteCvtDotOp(Operation *op, PatternRewriter &rewriter) const {\n-    auto *ctx = op->getContext();\n-    auto cvt = llvm::cast<ConvertLayoutOp>(op);\n-    auto tensorTy = cvt.result().getType().cast<RankedTensorType>();\n-    auto dotOperand = tensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n-    MmaEncodingAttr newMma =\n-        mmaToUpdate.lookup(dotOperand.getParent().cast<MmaEncodingAttr>());\n-    auto newDotOperand = DotOperandEncodingAttr::get(\n-        ctx, dotOperand.getOpIdx(), newMma, dotOperand.getIsMMAv1Row());\n-    auto newTensorTy = RankedTensorType::get(\n-        tensorTy.getShape(), tensorTy.getElementType(), newDotOperand);\n-    rewriter.replaceOpWithNewOp<ConvertLayoutOp>(op, newTensorTy,\n-                                                 cvt.getOperand());\n-  }\n-\n-  void rewriteDot(Operation *op, PatternRewriter &rewriter) const {\n-    auto *ctx = op->getContext();\n-    auto dot = llvm::cast<DotOp>(op);\n-    auto tensorTy = dot.d().getType().cast<RankedTensorType>();\n-    auto mma = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n-    auto newMma = mmaToUpdate.lookup(mma);\n-    auto newTensorTy = RankedTensorType::get(tensorTy.getShape(),\n-                                             tensorTy.getElementType(), newMma);\n-    rewriter.replaceOpWithNewOp<DotOp>(op, newTensorTy, dot.a(), dot.b(),\n-                                       dot.c(), dot.allowTF32());\n-  }\n-\n-  void rewriteCvtToMma(Operation *op, PatternRewriter &rewriter) const {\n-    auto *ctx = op->getContext();\n-    auto cvt = llvm::cast<ConvertLayoutOp>(op);\n-    auto tensorTy = cvt.result().getType().cast<RankedTensorType>();\n-    auto mma = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n-    auto newMma = mmaToUpdate.lookup(mma);\n-    auto newTensorTy = RankedTensorType::get(tensorTy.getShape(),\n-                                             tensorTy.getElementType(), newMma);\n-    rewriter.replaceOpWithNewOp<ConvertLayoutOp>(op, newTensorTy,\n-                                                 cvt.getOperand());\n-  }\n-\n-  void rewriteConstant(Operation *op, PatternRewriter &rewriter) const {\n-    auto *ctx = op->getContext();\n-    auto constant = llvm::cast<arith::ConstantOp>(op);\n-    auto tensorTy = constant.getResult().getType().dyn_cast<RankedTensorType>();\n-    auto mma = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n-    auto newMma = mmaToUpdate.lookup(mma);\n-    auto newTensorTy = RankedTensorType::get(tensorTy.getShape(),\n-                                             tensorTy.getElementType(), newMma);\n-    if (auto attr = constant.getValue().dyn_cast<SplatElementsAttr>()) {\n-      auto newRet =\n-          SplatElementsAttr::get(newTensorTy, attr.getSplatValue<Attribute>());\n-      rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, newTensorTy, newRet);\n-      return;\n-    }\n-\n-    assert(false && \"Not supported ConstantOp value type\");\n-  }\n-};\n-\n } // namespace\n \n #define GEN_PASS_CLASSES\n@@ -1534,31 +1309,7 @@ class TritonGPUCombineOpsPass\n       signalPassFailure();\n     }\n \n-    llvm::DenseMap<MmaEncodingAttr, MmaEncodingAttr> mmaToUpdate;\n-    {\n-      mlir::RewritePatternSet patterns(context);\n-      patterns.add<CollectMmaToUpdateForVolta>(context, mmaToUpdate);\n-      if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n-        signalPassFailure();\n-    }\n-    {\n-      mlir::RewritePatternSet patterns(context);\n-      patterns.add<UpdateMMAVersionMinorForVolta>(\n-          context, DotOp::getOperationName(), mmaToUpdate);\n-      patterns.add<UpdateMMAVersionMinorForVolta>(\n-          context, ConvertLayoutOp::getOperationName(), mmaToUpdate);\n-      patterns.add<UpdateMMAVersionMinorForVolta>(\n-          context, arith::ConstantOp::getOperationName(), mmaToUpdate);\n-      mlir::GreedyRewriteConfig config;\n-      config.useTopDownTraversal = true;\n-\n-      if (applyPatternsAndFoldGreedily(m, std::move(patterns), config).failed())\n-        signalPassFailure();\n-    }\n-\n-    mlir::RewritePatternSet loopFixup(context);\n-    loopFixup.add<FixupLoop>(context);\n-    if (applyPatternsAndFoldGreedily(m, std::move(loopFixup)).failed()) {\n+    if (fixupLoops(m).failed()) {\n       signalPassFailure();\n     }\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/UpdateMmaForVolta.cpp", "status": "added", "additions": 355, "deletions": 0, "changes": 355, "file_content_changes": "@@ -0,0 +1,355 @@\n+#include \"Utility.h\"\n+#include \"mlir/Dialect/SCF/SCF.h\"\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+\n+namespace mlir {\n+namespace {\n+using triton::DotOp;\n+using triton::gpu::ConvertLayoutOp;\n+using triton::gpu::DotOperandEncodingAttr;\n+using triton::gpu::MmaEncodingAttr;\n+using triton::gpu::SliceEncodingAttr;\n+\n+// This pattern collects the wrong Mma those need to update and create the right\n+// ones for each.\n+// TODO[Superjomn]: RewirtePattern is not needed here, Rewrite this to a method\n+class CollectMmaToUpdateForVolta : public mlir::RewritePattern {\n+  // Holds the mapping from old(wrong) mmaEncodingAttr to the new(correct)\n+  // mmaEncodingAttr.\n+  DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n+\n+public:\n+  CollectMmaToUpdateForVolta(\n+      mlir::MLIRContext *ctx,\n+      DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n+      : mlir::RewritePattern(triton::DotOp::getOperationName(), 1, ctx),\n+        mmaToUpdate(mmaToUpdate) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+\n+    auto dotOp = cast<triton::DotOp>(op);\n+    auto *ctx = dotOp->getContext();\n+    auto AT = dotOp.a().getType().cast<RankedTensorType>();\n+    auto BT = dotOp.b().getType().cast<RankedTensorType>();\n+    auto DT = dotOp.d().getType().cast<RankedTensorType>();\n+    if (!DT.getEncoding())\n+      return failure();\n+    auto mmaLayout = DT.getEncoding().dyn_cast<MmaEncodingAttr>();\n+    if (!(mmaLayout && mmaLayout.isVolta()))\n+      return failure();\n+\n+    // Has processed.\n+    if (mmaToUpdate.count(mmaLayout))\n+      return failure();\n+\n+    auto dotOperandA = AT.getEncoding().cast<DotOperandEncodingAttr>();\n+    auto dotOperandB = BT.getEncoding().cast<DotOperandEncodingAttr>();\n+    bool isARow = dotOperandA.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    bool isBRow = dotOperandB.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    auto [isARow_, isBRow_, isAVec4, isBVec4, mmaId] =\n+        mmaLayout.decodeVoltaLayoutStates();\n+\n+    // The wpt of MMAv1 is also determined by isARow, isBRow and shape, and it\n+    // could only be set here for those states might be updated by previous\n+    // patterns in the Combine Pass.\n+    if (isARow_ == isARow && isBRow_ == isBRow) {\n+      auto tgtWpt =\n+          getWarpsPerCTA(DT.getShape(), isARow, isBRow, isAVec4, isBVec4,\n+                         product(mmaLayout.getWarpsPerCTA()));\n+      // Check if the wpt should be updated.\n+      if (tgtWpt == mmaLayout.getWarpsPerCTA() ||\n+          !MmaEncodingAttr::_mmaV1UpdateWpt)\n+        return failure();\n+    }\n+\n+    MmaEncodingAttr newMmaLayout;\n+    {\n+      // Create a temporary MMA layout to obtain the isAVec4 and isBVec4\n+      auto tmpMmaLayout = MmaEncodingAttr::get(\n+          ctx, mmaLayout.getVersionMajor(), mmaLayout.getWarpsPerCTA(),\n+          AT.getShape(), BT.getShape(), isARow, isBRow, mmaId);\n+      auto [isARow_, isBRow_, isAVec4_, isBVec4_, _] =\n+          tmpMmaLayout.decodeVoltaLayoutStates();\n+\n+      // Recalculate the wpt, for here we could get the latest information, the\n+      // wpt should be updated.\n+      auto updatedWpt =\n+          getWarpsPerCTA(DT.getShape(), isARow_, isBRow_, isAVec4_, isBVec4_,\n+                         product(mmaLayout.getWarpsPerCTA()));\n+      auto newWpt = MmaEncodingAttr::_mmaV1UpdateWpt\n+                        ? updatedWpt\n+                        : mmaLayout.getWarpsPerCTA();\n+      newMmaLayout = MmaEncodingAttr::get(ctx, mmaLayout.getVersionMajor(),\n+                                          newWpt, AT.getShape(), BT.getShape(),\n+                                          isARow, isBRow, mmaId);\n+    }\n+\n+    // Collect the wrong MMA Layouts, and mark need to update.\n+    mmaToUpdate.try_emplace(mmaLayout, newMmaLayout);\n+\n+    return failure();\n+  }\n+\n+  // Get the wpt for MMAv1 using more information.\n+  // Reference the original logic here\n+  // https://github.com/openai/triton/blob/0e4691e6dd91e001a8d33b71badf8b3314325459/lib/codegen/analysis/layout.cc#L223\n+  SmallVector<unsigned, 2> getWarpsPerCTA(ArrayRef<int64_t> shape, bool isARow,\n+                                          bool isBRow, bool isAVec4,\n+                                          bool isBVec4, int numWarps) const {\n+    // TODO[Superjomn]: Share code with\n+    // DotOpMmaV1ConversionHelper::AParam/BParam, since same code to compute the\n+    // rep,spw and fpw.\n+    SmallVector<unsigned, 2> wpt({1, 1});\n+    SmallVector<unsigned, 2> wpt_nm1;\n+\n+    SmallVector<int, 2> rep(2), spw(2);\n+    std::array<int, 3> fpw{{2, 2, 1}};\n+    int packSize0 = (isARow || isAVec4) ? 1 : 2;\n+    rep[0] = 2 * packSize0;\n+    spw[0] = fpw[0] * 4 * rep[0];\n+\n+    int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n+    rep[1] = 2 * packSize1;\n+    spw[1] = fpw[1] * 4 * rep[1];\n+\n+    do {\n+      wpt_nm1 = wpt;\n+      if (wpt[0] * wpt[1] < numWarps)\n+        wpt[0] = std::clamp<int>(wpt[0] * 2, 1, shape[0] / spw[0]);\n+      if (wpt[0] * wpt[1] < numWarps)\n+        wpt[1] = std::clamp<int>(wpt[1] * 2, 1, shape[1] / spw[1]);\n+    } while (wpt_nm1 != wpt);\n+\n+    return wpt;\n+  }\n+};\n+\n+class UpdateMMAForMMAv1 : public mlir::RewritePattern {\n+  const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n+\n+public:\n+  UpdateMMAForMMAv1(\n+      MLIRContext *context,\n+      const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n+      : RewritePattern(MatchAnyOpTypeTag{}, 1, context),\n+        mmaToUpdate(mmaToUpdate) {}\n+\n+  LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    // Nothing to update\n+    if (mmaToUpdate.empty())\n+      return failure();\n+\n+    if (auto dotOp = llvm::dyn_cast<DotOp>(op))\n+      return rewriteDotOp(op, rewriter);\n+    else if (auto cvtOp = llvm::dyn_cast<ConvertLayoutOp>(op))\n+      return rewriteCvtOp(op, rewriter);\n+    else if (auto expandDimsOp = llvm::dyn_cast<triton::ExpandDimsOp>(op))\n+      return rewriteExpandDimsOp(op, rewriter);\n+    else if (auto constOp = llvm::dyn_cast<arith::ConstantOp>(op))\n+      return rewriteConstantOp(op, rewriter);\n+    else\n+      return rewriteElementwiseOp(op, rewriter);\n+    return failure();\n+  }\n+\n+  LogicalResult rewriteDotOp(Operation *op,\n+                             mlir::PatternRewriter &rewriter) const {\n+    auto dotOp = llvm::cast<DotOp>(op);\n+    auto tensorTy = dotOp->getResult(0).getType().dyn_cast<RankedTensorType>();\n+    if (!tensorTy)\n+      return failure();\n+\n+    auto mma = dotOp.d()\n+                   .getType()\n+                   .cast<RankedTensorType>()\n+                   .getEncoding()\n+                   .dyn_cast<MmaEncodingAttr>();\n+    if (!mma || !mmaToUpdate.count(mma))\n+      return failure();\n+\n+    auto newTensorTy = getUpdatedType(tensorTy);\n+    rewriter.replaceOpWithNewOp<DotOp>(op, newTensorTy, dotOp.a(), dotOp.b(),\n+                                       dotOp.c(), dotOp.allowTF32());\n+    return success();\n+  }\n+\n+  LogicalResult rewriteCvtOp(Operation *op,\n+                             mlir::PatternRewriter &rewriter) const {\n+    auto cvt = llvm::cast<ConvertLayoutOp>(op);\n+    if (!needUpdate(cvt.getResult().getType()))\n+      return failure();\n+    auto tensorTy = cvt.result().getType().dyn_cast<RankedTensorType>();\n+\n+    auto newTensorTy = getUpdatedType(tensorTy);\n+    auto newOp = rewriter.replaceOpWithNewOp<ConvertLayoutOp>(op, newTensorTy,\n+                                                              cvt.getOperand());\n+    return success();\n+  }\n+\n+  LogicalResult rewriteExpandDimsOp(Operation *op,\n+                                    mlir::PatternRewriter &rewriter) const {\n+    auto expandDims = llvm::cast<triton::ExpandDimsOp>(op);\n+    auto srcTy = expandDims.src().getType();\n+    auto resTy = expandDims.getResult().getType();\n+\n+    // the result type need to update\n+    if (!needUpdate(srcTy) && needUpdate(resTy)) {\n+      rewriter.replaceOpWithNewOp<triton::ExpandDimsOp>(op, expandDims.src(),\n+                                                        expandDims.axis());\n+      return success();\n+    }\n+\n+    return failure();\n+  }\n+\n+  LogicalResult rewriteConstantOp(Operation *op,\n+                                  mlir::PatternRewriter &rewriter) const {\n+    auto constant = llvm::cast<arith::ConstantOp>(op);\n+    auto resTy = constant.getResult().getType();\n+    if (!needUpdate(resTy))\n+      return failure();\n+\n+    auto tensorTy = constant.getResult().getType().cast<RankedTensorType>();\n+    auto mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>();\n+    auto dot = tensorTy.getEncoding().dyn_cast<DotOperandEncodingAttr>();\n+    if (!mma && !dot)\n+      return failure();\n+\n+    auto newTensorTy = getUpdatedType(tensorTy);\n+    if (auto attr = constant.getValue().dyn_cast<SplatElementsAttr>()) {\n+      auto newRet =\n+          SplatElementsAttr::get(newTensorTy, attr.getSplatValue<Attribute>());\n+      rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, newRet);\n+      return success();\n+    }\n+\n+    return failure();\n+  }\n+\n+  LogicalResult rewriteElementwiseOp(Operation *op,\n+                                     mlir::PatternRewriter &rewriter) const {\n+    if (op->getNumOperands() != 1 || op->getNumResults() != 1)\n+      return failure();\n+\n+    auto srcTy = op->getOperand(0).getType();\n+    auto resTy = op->getResult(0).getType();\n+    if (!needUpdate(srcTy) && needUpdate(resTy)) {\n+      op->getResult(0).setType(\n+          getUpdatedType(resTy.dyn_cast<RankedTensorType>()));\n+      return success();\n+    }\n+    return failure();\n+  }\n+\n+  RankedTensorType getUpdatedType(RankedTensorType type) const {\n+    if (!needUpdate(type))\n+      return type;\n+    auto encoding = type.getEncoding();\n+    if (auto mma = encoding.dyn_cast<MmaEncodingAttr>()) {\n+      auto newMma = mmaToUpdate.lookup(mma);\n+      return RankedTensorType::get(type.getShape(), type.getElementType(),\n+                                   newMma);\n+    } else if (auto slice = encoding.dyn_cast<SliceEncodingAttr>()) {\n+      if (auto mma = slice.getParent().dyn_cast<MmaEncodingAttr>()) {\n+        auto newMma = mmaToUpdate.lookup(mma);\n+        auto newSlice =\n+            SliceEncodingAttr::get(slice.getContext(), slice.getDim(), newMma);\n+        return RankedTensorType::get(type.getShape(), type.getElementType(),\n+                                     newSlice);\n+      }\n+    } else if (auto dotOp = encoding.dyn_cast<DotOperandEncodingAttr>()) {\n+      if (auto mma = dotOp.getParent().dyn_cast<MmaEncodingAttr>()) {\n+        auto newMma = mmaToUpdate.lookup(mma);\n+        auto newDotOp =\n+            DotOperandEncodingAttr::get(dotOp.getContext(), dotOp.getOpIdx(),\n+                                        newMma, dotOp.getIsMMAv1Row());\n+        return RankedTensorType::get(type.getShape(), type.getElementType(),\n+                                     newDotOp);\n+      }\n+    }\n+    return type;\n+  }\n+\n+  // Tell if this type contains a wrong MMA encoding and need to update.\n+  bool needUpdate(Type type) const {\n+    auto tensorTy = type.dyn_cast<RankedTensorType>();\n+    if (!tensorTy)\n+      return false;\n+    return needUpdate(tensorTy);\n+  }\n+\n+  // Tell if this type contains a wrong MMA encoding and need to update.\n+  bool needUpdate(RankedTensorType type) const {\n+    auto encoding = type.getEncoding();\n+    if (!encoding)\n+      return false;\n+\n+    MmaEncodingAttr mma;\n+    if ((mma = encoding.dyn_cast<MmaEncodingAttr>())) {\n+    } else if (auto slice = encoding.dyn_cast<SliceEncodingAttr>()) {\n+      mma = slice.getParent().dyn_cast<MmaEncodingAttr>();\n+    } else if (auto dotOp = encoding.dyn_cast<DotOperandEncodingAttr>()) {\n+      mma = dotOp.getParent().dyn_cast<MmaEncodingAttr>();\n+    }\n+\n+    return mma && mmaToUpdate.count(mma);\n+  }\n+};\n+\n+} // namespace\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n+\n+class UpdateMmaForVoltaPass\n+    : public UpdateMmaForVoltaBase<UpdateMmaForVoltaPass> {\n+public:\n+  UpdateMmaForVoltaPass() = default;\n+  void runOnOperation() override {\n+    MLIRContext *context = &getContext();\n+    ModuleOp m = getOperation();\n+\n+    llvm::DenseMap<MmaEncodingAttr, MmaEncodingAttr> mmaToUpdate;\n+    {\n+      mlir::RewritePatternSet patterns(context);\n+      patterns.add<CollectMmaToUpdateForVolta>(context, mmaToUpdate);\n+\n+      GreedyRewriteConfig config;\n+      config.enableRegionSimplification =\n+          false; // The pattern doesn't modify the IR\n+      if (applyPatternsAndFoldGreedily(m, std::move(patterns), config).failed())\n+        signalPassFailure();\n+    }\n+\n+    if (!mmaToUpdate.empty()) {\n+      mlir::RewritePatternSet patterns(context);\n+      patterns.add<UpdateMMAForMMAv1>(context, mmaToUpdate);\n+\n+      mlir::GreedyRewriteConfig config;\n+      // Make sure the slice and dot_operand layouts' parent mma are updated\n+      // before updating DotOp or it will get a mismatch parent-encoding.\n+      config.useTopDownTraversal = true;\n+\n+      if (applyPatternsAndFoldGreedily(m, std::move(patterns), config).failed())\n+        signalPassFailure();\n+\n+      if (fixupLoops(m).failed())\n+        signalPassFailure();\n+    }\n+  }\n+};\n+\n+std::unique_ptr<Pass> createTritonGPUUpdateMmaForVoltaPass() {\n+  return std::make_unique<UpdateMmaForVoltaPass>();\n+}\n+\n+} // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "added", "additions": 63, "deletions": 0, "changes": 63, "file_content_changes": "@@ -0,0 +1,63 @@\n+#include \"Utility.h\"\n+#include \"mlir/Dialect/SCF/SCF.h\"\n+#include \"mlir/IR/BlockAndValueMapping.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+\n+namespace mlir {\n+\n+namespace {\n+\n+class FixupLoop : public mlir::RewritePattern {\n+\n+public:\n+  explicit FixupLoop(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(scf::ForOp::getOperationName(), 2, context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto forOp = cast<scf::ForOp>(op);\n+\n+    // Rewrite init argument\n+    SmallVector<Value, 4> newInitArgs = forOp.getInitArgs();\n+    bool shouldRematerialize = false;\n+    for (size_t i = 0; i < newInitArgs.size(); i++) {\n+      if (newInitArgs[i].getType() != forOp.getRegionIterArgs()[i].getType() ||\n+          newInitArgs[i].getType() != forOp.getResultTypes()[i]) {\n+        shouldRematerialize = true;\n+        break;\n+      }\n+    }\n+    if (!shouldRematerialize)\n+      return failure();\n+\n+    scf::ForOp newForOp = rewriter.create<scf::ForOp>(\n+        forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n+        forOp.getStep(), newInitArgs);\n+    newForOp->moveBefore(forOp);\n+    rewriter.setInsertionPointToStart(newForOp.getBody());\n+    BlockAndValueMapping mapping;\n+    for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n+      mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n+    mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n+\n+    for (Operation &op : forOp.getBody()->getOperations()) {\n+      rewriter.clone(op, mapping);\n+    }\n+    rewriter.replaceOp(forOp, newForOp.getResults());\n+    return success();\n+  }\n+};\n+\n+} // namespace\n+\n+LogicalResult fixupLoops(ModuleOp mod) {\n+  auto *ctx = mod.getContext();\n+  mlir::RewritePatternSet patterns(ctx);\n+  patterns.add<FixupLoop>(ctx);\n+  if (applyPatternsAndFoldGreedily(mod, std::move(patterns)).failed())\n+    return failure();\n+  return success();\n+}\n+\n+} // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.h", "status": "added", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -0,0 +1,12 @@\n+#ifndef TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n+#define TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+\n+namespace mlir {\n+\n+LogicalResult fixupLoops(ModuleOp mod);\n+\n+} // namespace mlir\n+\n+#endif // TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_"}, {"filename": "lib/Target/LLVMIR/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -7,6 +7,7 @@ add_mlir_translation_library(TritonLLVMIR\n         LINK_LIBS PUBLIC\n         MLIRIR\n         MLIRLLVMIR\n+        MLIRSCFToStandard\n         MLIRSupport\n         MLIRTargetLLVMIRExport\n         )"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 27, "deletions": 3, "changes": 30, "file_content_changes": "@@ -468,6 +468,12 @@ void init_triton_ir(py::module &&m) {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n                  loc, v, self.getI1Type()));\n            })\n+      .def(\"get_int8\",\n+           [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n+                 loc, v, self.getI8Type()));\n+           })\n       .def(\"get_int32\",\n            [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n@@ -480,9 +486,23 @@ void init_triton_ir(py::module &&m) {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n                  loc, v, self.getI64Type()));\n            })\n-      // .def(\"get_uint32\", &ir::builder::get_int32, ret::reference)\n-      // .def(\"get_float16\", &ir::builder::get_float16, ret::reference)\n-      .def(\"get_float32\",\n+      // bfloat16 cannot be initialized as it is treated as int16 for now\n+      //.def(\"get_bf16\",\n+      //     [](mlir::OpBuilder &self, float v) -> mlir::Value {\n+      //       auto loc = self.getUnknownLoc();\n+      //       auto type = self.getBF16Type();\n+      //       return self.create<mlir::arith::ConstantFloatOp>(\n+      //           loc,\n+      //           mlir::APFloat(type.getFloatSemantics(), std::to_string(v)),\n+      //           type);\n+      //     })\n+      .def(\"get_fp16\",\n+           [](mlir::OpBuilder &self, float v) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return self.create<mlir::arith::ConstantOp>(\n+                 loc, self.getF16FloatAttr(v));\n+           })\n+      .def(\"get_fp32\",\n            [](mlir::OpBuilder &self, float v) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::ConstantOp>(\n@@ -1347,6 +1367,10 @@ void init_triton_ir(py::module &&m) {\n              self.addPass(\n                  mlir::createTritonGPUCombineOpsPass(computeCapability));\n            })\n+      .def(\"add_tritongpu_update_mma_for_volta_pass\",\n+           [](mlir::PassManager &self) {\n+             self.addPass(mlir::createTritonGPUUpdateMmaForVoltaPass());\n+           })\n       .def(\"add_tritongpu_reorder_instructions_pass\",\n            [](mlir::PassManager &self) {\n              self.addPass(mlir::createTritonGPUReorderInstructionsPass());"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 32, "deletions": 14, "changes": 46, "file_content_changes": "@@ -885,7 +885,7 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n \n \n def get_reduced_dtype(dtype_str, op):\n-    if op == 'argmin' or op == 'argmax':\n+    if op in ('argmin', 'argmax'):\n         return 'int32'\n     if dtype_str in ['int8', 'uint8', 'int16', 'uint16']:\n         return 'int32'\n@@ -917,7 +917,7 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n     numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n                 'argmin': np.argmin, 'argmax': np.argmax}[op]\n     # numpy result\n-    z_dtype_str = 'int32' if op == 'argmin' or op == 'argmax' else dtype_str\n+    z_dtype_str = 'int32' if op in ('argmin', 'argmax') else dtype_str\n     z_tri_dtype_str = z_dtype_str\n     if op not in ['argmin', 'argmax'] and dtype_str == 'bfloat16':\n         z_dtype_str = 'float32'\n@@ -936,7 +936,7 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n     if op == 'sum':\n         np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n     else:\n-        if op == 'argmin' or op == 'argmax':\n+        if op in ('argmin', 'argmax'):\n             # argmin and argmax can have multiple valid indices.\n             # so instead we compare the values pointed by indices\n             np.testing.assert_equal(x[z_ref], x[z_tri])\n@@ -1013,7 +1013,7 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n     if op == 'sum':\n         np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n     else:\n-        if op == 'argmin' or op == 'argmax':\n+        if op in ('argmin', 'argmax'):\n             # argmin and argmax can have multiple valid indices.\n             # so instead we compare the values pointed by indices\n             z_ref_index = np.expand_dims(z_ref, axis=axis)\n@@ -1231,19 +1231,23 @@ def kernel(X, stride_xm, stride_xk,\n         assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n \n \n-def test_dot_without_load():\n+@pytest.mark.parametrize(\"dtype_str\", ['float32', 'float16'])\n+def test_dot_without_load(dtype_str):\n     @triton.jit\n-    def kernel(out):\n-        pid = tl.program_id(axis=0)\n-        a = tl.zeros((32, 32), tl.float32)\n-        b = tl.zeros((32, 32), tl.float32)\n-        c = tl.zeros((32, 32), tl.float32)\n+    def _kernel(out):\n+        a = GENERATE_TEST_HERE\n+        b = GENERATE_TEST_HERE\n         c = tl.dot(a, b)\n-        pout = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n-        tl.store(pout, c)\n-\n-    out = torch.ones((32, 32), dtype=torch.float32, device=\"cuda\")\n+        out_ptr = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n+        tl.store(out_ptr, c)\n+\n+    kernel = patch_kernel(_kernel, {'GENERATE_TEST_HERE': f\"tl.full((32, 32), 1.0, tl.{dtype_str})\"})\n+    a = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n+    b = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n+    out_ref = torch.matmul(a, b)\n+    out = torch.zeros((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n     kernel[(1,)](out)\n+    assert torch.all(out == out_ref)\n \n # ---------------\n # test arange\n@@ -1449,6 +1453,20 @@ def kernel(x):\n     kernel[(1, )](x)\n \n \n+@pytest.mark.parametrize(\"device\", ['cuda', 'cpu'])\n+def test_pointer_arguments(device):\n+    @triton.jit\n+    def kernel(x):\n+        pass\n+    x = torch.empty(1024, device=device)\n+    result = True\n+    try:\n+        kernel[(1,)](x)\n+    except ValueError:\n+        result = True if device == 'cpu' else False\n+    assert result\n+\n+\n @pytest.mark.parametrize(\"value, value_type\", [\n     (-1, 'i32'), (0, 'i32'), (-2**31, 'i32'), (2**31 - 1, 'i32'),\n     (2**31, 'u32'), (2**32 - 1, 'u32'), (2**32, 'i64'), (2**63 - 1, 'i64'),"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 37, "deletions": 7, "changes": 44, "file_content_changes": "@@ -900,6 +900,9 @@ def ttir_to_ttgir(mod, num_warps, num_stages, compute_capability):\n     pm.add_tritongpu_combine_pass(compute_capability)\n     pm.add_licm_pass()\n     pm.add_tritongpu_combine_pass(compute_capability)\n+    if compute_capability // 10 == 7:\n+        # The update_mma_for_volta pass helps to compute some information for MMA encoding specifically for MMAv1\n+        pm.add_tritongpu_update_mma_for_volta_pass()\n     pm.add_cse_pass()\n     pm.add_tritongpu_decompose_conversions_pass()\n     pm.add_cse_pass()\n@@ -1071,6 +1074,7 @@ def format_of(ty):\n     # generate glue code\n     src = f\"\"\"\n #include \\\"cuda.h\\\"\n+#include <stdbool.h>\n #include <Python.h>\n \n static inline void gpuAssert(CUresult code, const char *file, int line)\n@@ -1096,12 +1100,22 @@ def format_of(ty):\n   }}\n }}\n \n-static inline CUdeviceptr getPointer(PyObject *obj, int idx) {{\n+typedef struct _DevicePtrInfo {{\n+    CUdeviceptr dev_ptr;\n+    bool valid;\n+}} DevicePtrInfo;\n+\n+static inline DevicePtrInfo getPointer(PyObject *obj, int idx) {{\n+  DevicePtrInfo ptr_info;\n+  ptr_info.dev_ptr = 0;\n+  ptr_info.valid = true;\n   if (PyLong_Check(obj)) {{\n-    return (CUdeviceptr)PyLong_AsUnsignedLongLong(obj);\n+    ptr_info.dev_ptr = PyLong_AsUnsignedLongLong(obj);\n+    return ptr_info;\n   }}\n   if (obj == Py_None) {{\n-    return (CUdeviceptr)0;\n+    // valid nullptr\n+    return ptr_info;\n   }}\n   PyObject *ptr = PyObject_GetAttrString(obj, \"data_ptr\");\n   if(ptr){{\n@@ -1111,11 +1125,23 @@ def format_of(ty):\n     Py_DECREF(ptr);\n     if (!PyLong_Check(ret)) {{\n       PyErr_SetString(PyExc_TypeError, \"data_ptr method of Pointer object must return 64-bit int\");\n+      ptr_info.valid = false;\n+      return ptr_info;\n+    }}\n+    ptr_info.dev_ptr = PyLong_AsUnsignedLongLong(ret);\n+    unsigned attr;\n+    CUresult status =\n+        cuPointerGetAttribute(&attr, CU_POINTER_ATTRIBUTE_MEMORY_TYPE, ptr_info.dev_ptr);\n+    if (!(attr == CU_MEMORYTYPE_DEVICE || attr == CU_MEMORYTYPE_UNIFIED) ||\n+        !(status == CUDA_SUCCESS)) {{\n+        PyErr_Format(PyExc_ValueError,\n+                     \"Pointer argument (at %d) cannot be accessed from Triton (cpu tensor?)\", idx);\n+        ptr_info.valid = false;\n     }}\n-    return (CUdeviceptr)PyLong_AsUnsignedLongLong(ret);\n+    return ptr_info;\n   }}\n   PyErr_SetString(PyExc_TypeError, \"Pointer argument must be either uint64 or have data_ptr method\");\n-  return (CUdeviceptr)0;\n+  return ptr_info;\n }}\n \n static PyObject* launch(PyObject* self, PyObject* args) {{\n@@ -1139,7 +1165,10 @@ def format_of(ty):\n     Py_DECREF(new_args);\n   }}\n \n-  _launch(gridX, gridY, gridZ, num_warps, shared_memory, (CUstream)_stream, (CUfunction)_function, {', '.join(f\"getPointer(_arg{i},{i})\" if ty[0]==\"*\" else f\"_arg{i}\"for i, ty in signature.items())});\n+\n+  // raise exception asap\n+  {\"; \".join([f\"DevicePtrInfo ptr_info{i} = getPointer(_arg{i}, {i}); if (!ptr_info{i}.valid) return NULL;\" if ty[0] == \"*\" else \"\" for i, ty in signature.items()])};\n+  _launch(gridX, gridY, gridZ, num_warps, shared_memory, (CUstream)_stream, (CUfunction)_function, {', '.join(f\"ptr_info{i}.dev_ptr\" if ty[0]==\"*\" else f\"_arg{i}\"for i, ty in signature.items())});\n \n   if (launch_exit_hook != Py_None) {{\n     PyObject *new_args = NULL;\n@@ -1591,7 +1620,8 @@ def __new__(cls):\n             cls.instance = super(CudaUtils, cls).__new__(cls)\n         return cls.instance\n \n-    def _generate_src(self):\n+    @staticmethod\n+    def _generate_src():\n         return \"\"\"\n         #include <cuda.h>\n "}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -31,6 +31,7 @@\n     dot,\n     dtype,\n     exp,\n+    full,\n     fdiv,\n     float16,\n     float32,\n@@ -124,6 +125,7 @@\n     \"float32\",\n     \"float64\",\n     \"float8\",\n+    \"full\",\n     \"function_type\",\n     \"int1\",\n     \"int16\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 43, "deletions": 19, "changes": 62, "file_content_changes": "@@ -26,7 +26,7 @@ def _to_tensor(x, builder):\n         else:\n             raise RuntimeError(f'Nonrepresentable integer {x}.')\n     elif isinstance(x, float):\n-        return tensor(builder.get_float32(x), float32)\n+        return tensor(builder.get_fp32(x), float32)\n     elif isinstance(x, constexpr):\n         return _to_tensor(x.value, builder)\n     elif isinstance(x, tensor):\n@@ -139,13 +139,16 @@ def is_int(self):\n     def is_bool(self):\n         return self.is_int1()\n \n-    def is_void(self):\n+    @staticmethod\n+    def is_void():\n         raise RuntimeError(\"Not implemented\")\n \n-    def is_block(self):\n+    @staticmethod\n+    def is_block():\n         return False\n \n-    def is_ptr(self):\n+    @staticmethod\n+    def is_ptr():\n         return False\n \n     def __eq__(self, other: dtype):\n@@ -168,13 +171,13 @@ def to_ir(self, builder: ir.builder) -> ir.type:\n             return builder.get_void_ty()\n         elif self.name == 'int1':\n             return builder.get_int1_ty()\n-        elif self.name == 'int8' or self.name == 'uint8':\n+        elif self.name in ('int8', 'uint8'):\n             return builder.get_int8_ty()\n-        elif self.name == 'int16' or self.name == 'uint16':\n+        elif self.name in ('int16', 'uint16'):\n             return builder.get_int16_ty()\n-        elif self.name == 'int32' or self.name == 'uint32':\n+        elif self.name in ('int32', 'uint32'):\n             return builder.get_int32_ty()\n-        elif self.name == 'int64' or self.name == 'uint64':\n+        elif self.name in ('int64', 'uint64'):\n             return builder.get_int64_ty()\n         elif self.name == 'fp8':\n             return builder.get_fp8_ty()\n@@ -690,24 +693,31 @@ def arange(start, end, _builder=None):\n     return semantic.arange(start, end, _builder)\n \n \n+def _shape_check_impl(shape):\n+    shape = _constexpr_to_value(shape)\n+    for i, d in enumerate(shape):\n+        if not isinstance(d, constexpr):\n+            raise TypeError(f\"Shape element {i} must have type `constexpr`\")\n+        if not isinstance(d.value, int):\n+            raise TypeError(f\"Shape element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n+    return [_constexpr_to_value(x) for x in shape]\n+\n+\n @builtin\n-def zeros(shape, dtype, _builder=None):\n+def full(shape, value, dtype, _builder=None):\n     \"\"\"\n-    Returns a tensor filled with the scalar value 0 for the given :code:`shape` and :code:`dtype`.\n+    Returns a tensor filled with the scalar value for the given :code:`shape` and :code:`dtype`.\n \n     :param shape: Shape of the new array, e.g., (8, 16) or (8, )\n+    :value value: A scalar value to fill the array with\n     :type shape: tuple of ints\n     :param dtype: Data-type of the new array, e.g., :code:`tl.float16`\n     :type dtype: DType\n     \"\"\"\n-    for i, d in enumerate(shape):\n-        if not isinstance(d, constexpr):\n-            raise TypeError(f\"Shape element {i} must have type `constexpr`\")\n-        if not isinstance(d.value, int):\n-            raise TypeError(f\"Shape element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n-    shape = [x.value for x in shape]\n+    shape = _shape_check_impl(shape)\n+    value = _constexpr_to_value(value)\n     dtype = _constexpr_to_value(dtype)\n-    return semantic.zeros(shape, dtype, _builder)\n+    return semantic.full(shape, value, dtype, _builder)\n \n \n # -----------------------\n@@ -738,6 +748,7 @@ def broadcast_to(input, shape, _builder=None):\n     :param shape: The desired shape.\n     :type shape: Tuple[int]\n     \"\"\"\n+    shape = _shape_check_impl(shape)\n     return semantic.broadcast_impl_shape(input, shape, _builder)\n \n \n@@ -775,14 +786,14 @@ def view(input, shape, _builder=None):\n     :type shape: Tuple[int]\n \n     \"\"\"\n-    shape = [x.value for x in shape]\n+    shape = _shape_check_impl(shape)\n     return semantic.view(input, shape, _builder)\n \n \n @builtin\n def reshape(input, shape, _builder=None):\n     # TODO: should be more than just a view\n-    shape = [x.value for x in shape]\n+    shape = _shape_check_impl(shape)\n     return semantic.view(input, shape, _builder)\n \n # -----------------------\n@@ -1243,6 +1254,19 @@ def swizzle2d(i, j, size_i, size_j, size_g):\n     return new_i, new_j\n \n \n+@triton.jit\n+def zeros(shape, dtype):\n+    \"\"\"\n+    Returns a tensor filled with the scalar value 0 for the given :code:`shape` and :code:`dtype`.\n+\n+    :param shape: Shape of the new array, e.g., (8, 16) or (8, )\n+    :type shape: tuple of ints\n+    :param dtype: Data-type of the new array, e.g., :code:`tl.float16`\n+    :type dtype: DType\n+    \"\"\"\n+    return full(shape, 0, dtype)\n+\n+\n @triton.jit\n def zeros_like(input):\n     return zeros(input.shape, input.dtype)"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 13, "deletions": 8, "changes": 21, "file_content_changes": "@@ -471,10 +471,15 @@ def arange(start: int, end: int, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_make_range(start, end), ret_ty)\n \n \n-def zeros(shape: List[int], dtype: tl.dtype, builder: ir.builder) -> tl.tensor:\n-    _0 = builder.get_null_value(dtype.to_ir(builder))\n+def full(shape: List[int], value, dtype: tl.dtype, builder: ir.builder) -> tl.tensor:\n+    if value == 0:\n+        _value = builder.get_null_value(dtype.to_ir(builder))\n+    else:\n+        get_value_fn = getattr(builder, f\"get_{dtype.name}\")\n+        _value = get_value_fn(value)\n     ret_ty = tl.block_type(dtype, shape)\n-    return tl.tensor(builder.create_splat(_0, shape), ret_ty)\n+    return tl.tensor(builder.create_splat(_value, shape), ret_ty)\n+\n \n # ===----------------------------------------------------------------------===//\n #                               Shape Manipulation\n@@ -891,8 +896,8 @@ def atomic_max(ptr: tl.tensor,\n     # return atomic_umin(i_ptr, i_val) if val < 0\n     i_val = bitcast(val, tl.int32, builder)\n     i_ptr = bitcast(ptr, tl.pointer_type(tl.int32, 1), builder)\n-    pos = greater_equal(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n-    neg = less_than(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n+    pos = greater_equal(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n+    neg = less_than(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n     pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MAX, i_ptr.handle, i_val.handle, and_(mask, pos, builder).handle), i_val.type)\n     neg_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMIN, i_ptr.handle, i_val.handle, and_(mask, neg, builder).handle), i_val.type)\n     return where(pos, pos_ret, neg_ret, builder)\n@@ -923,8 +928,8 @@ def atomic_min(ptr: tl.tensor,\n     # return atomic_umax(i_ptr, i_val) if val < 0\n     i_val = bitcast(val, tl.int32, builder)\n     i_ptr = bitcast(ptr, tl.pointer_type(tl.int32, 1), builder)\n-    pos = greater_equal(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n-    neg = less_than(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n+    pos = greater_equal(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n+    neg = less_than(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n     pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MIN,\n                                                   i_ptr.handle,\n                                                   i_val.handle,\n@@ -998,7 +1003,7 @@ def dot(lhs: tl.tensor,\n         _0 = builder.get_int32(0)\n         ret_scalar_ty = tl.int32\n     else:\n-        _0 = builder.get_float32(0)\n+        _0 = builder.get_fp32(0)\n         ret_scalar_ty = tl.float32\n     M = lhs.type.shape[0]\n     N = rhs.type.shape[1]"}, {"filename": "python/triton/tools/aot.py", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -1,4 +1,5 @@\n import argparse\n+import sys\n \n import triton\n import triton._C.libtriton.triton as libtriton\n@@ -24,7 +25,7 @@\n     # check for validity of format arguments\n     if args.target not in VALID_FORMATS:\n         print(\"Invalid target format: \" + args.target)\n-        exit(0)\n+        sys.exit(0)\n \n     # parse source file to MLIR module\n     context = libtriton.ir.context()\n@@ -35,7 +36,7 @@\n     module = triton.compiler.optimize_triton_ir(module)\n     if args.target == 'triton-ir':\n         print(module.str())\n-        exit(0)\n+        sys.exit(0)\n \n     if not args.sm:\n         raise argparse.ArgumentError(None, \"Must specify --sm for PTX compilation\")\n@@ -44,13 +45,13 @@\n     module = triton.compiler.ttir_to_ttgir(module, num_warps=4, num_stages=3, compute_capability=args.sm)\n     if args.target == 'triton-gpu-ir':\n         print(module.str())\n-        exit(0)\n+        sys.exit(0)\n \n     # triton-gpu-ir -> llvm-ir\n     module = triton.compiler.ttgir_to_llir(module, extern_libs=None, compute_capability=args.sm)\n     if args.target == 'llvm-ir':\n         print(module)\n-        exit(0)\n+        sys.exit(0)\n \n     if not args.ptx_version:\n         raise argparse.ArgumentError(None, \"Must specify --ptx-version for PTX compilation\")"}, {"filename": "python/triton/tools/build_extern.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -157,7 +157,8 @@ def __init__(self, path) -> None:\n         super().__init__(\"libdevice\", path)\n         self._symbol_groups = {}\n \n-    def _extract_symbol(self, line) -> Optional[Symbol]:\n+    @staticmethod\n+    def _extract_symbol(line) -> Optional[Symbol]:\n         # Extract symbols from line in the following format:\n         # \"define [internal] <ret_type> @<name>(<arg_types>,)\"\n         entries = line.split(\"@\")"}, {"filename": "python/triton/utils.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -33,7 +33,8 @@ def wrap_dtype(arg):\n     def __init__(self, dtype):\n         self.dtype = dtype\n \n-    def data_ptr(self):\n+    @staticmethod\n+    def data_ptr():\n         return 0  # optimistically assumes multiple of 16\n \n "}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -926,10 +926,10 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n-    // CHECK-SAME: (f32, f32, f32, f32)\n+    // CHECK-SAME: (vector<1xf32>, vector<1xf32>, vector<1xf32>, vector<1xf32>)\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n-    // CHECK-SAME: (f32, f32, f32, f32)\n+    // CHECK-SAME: (vector<1xf32>, vector<1xf32>, vector<1xf32>, vector<1xf32>)\n     %a_mat = triton_gpu.convert_layout %a : (tensor<32x16xf32, #shared>) -> tensor<32x16xf32, #dot_operand_a>\n     %b_mat = triton_gpu.convert_layout %b : (tensor<16x32xf32, #shared>) -> tensor<16x32xf32, #dot_operand_b>\n "}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 0, "deletions": 29, "changes": 29, "file_content_changes": "@@ -183,32 +183,3 @@ func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f3\n   tt.store %21, %22 : tensor<256xf32, #layout1>\n   return\n }\n-\n-\n-// -----\n-\n-// check the UpdateMMAVersionMinorForVolta pattern\n-#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n-#shared0 = #triton_gpu.shared<{vec = 1, perPhase=2, maxPhase=8 ,order = [1, 0]}>\n-#mma0 = #triton_gpu.mma<{versionMajor=1, versionMinor=0, warpsPerCTA=[1,1]}>\n-// Here, the isMMAv1Row of a and b's dot_operands mismatch #mma0's versionMinor,\n-// and the pattern should update the versionMinor.\n-#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, isMMAv1Row=true}>\n-#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n-// It creates a new MMA layout to fit with $a and $b's dot_operand\n-// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 11, warpsPerCTA = [1, 1]}>\n-module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK-LABEL: dot_mmav1\n-  func @dot_mmav1(%A: tensor<16x16xf16, #blocked0>, %B: tensor<16x16xf16, #blocked0>) -> tensor<16x16xf32, #blocked0> {\n-    %C = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #blocked0>\n-    %AA = triton_gpu.convert_layout %A : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #dot_operand_a>\n-    %BB = triton_gpu.convert_layout %B : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #dot_operand_b>\n-    %CC = triton_gpu.convert_layout %C : (tensor<16x16xf32, #blocked0>) -> tensor<16x16xf32, #mma0>\n-\n-    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, %cst {allowTF32 = true} : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma]], isMMAv1Row = true}>> * tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma]], isMMAv1Row = true}>> -> tensor<16x16xf32, [[new_mma]]>\n-    %D = tt.dot %AA, %BB, %CC {allowTF32 = true} : tensor<16x16xf16, #dot_operand_a> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n-    %res = triton_gpu.convert_layout %D : (tensor<16x16xf32, #mma0>) -> tensor<16x16xf32, #blocked0>\n-\n-    return %res : tensor<16x16xf32, #blocked0>\n-  }\n-}"}, {"filename": "test/TritonGPU/update-mma-for-volta.mlir", "status": "added", "additions": 73, "deletions": 0, "changes": 73, "file_content_changes": "@@ -0,0 +1,73 @@\n+// RUN: triton-opt %s -split-input-file -tritongpu-combine -tritongpu-update-mma-for-volta 2>&1 | FileCheck %s\n+\n+// -----\n+\n+// check the UpdateMMAVersionMinorForVolta pattern\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [4, 4], order = [1, 0]}>\n+#shared0 = #triton_gpu.shared<{vec = 1, perPhase=2, maxPhase=8 ,order = [1, 0]}>\n+#mma0 = #triton_gpu.mma<{versionMajor=1, versionMinor=0, warpsPerCTA=[4,4]}>\n+// Here, the isMMAv1Row of a and b's dot_operands mismatch #mma0's versionMinor,\n+// and the pattern should update the versionMinor.\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, isMMAv1Row=true}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n+// It creates a new MMA layout to fit with $a and $b's dot_operand, and get the right warpsPerCTA\n+// The ID of this MMA instance should be 0.\n+// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 4]}>\n+module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n+  // CHECK-LABEL: dot_mmav1\n+  func @dot_mmav1(%A: tensor<64x64xf16, #blocked0>, %B: tensor<64x64xf16, #blocked0>) -> tensor<64x64xf32, #blocked0> {\n+    %C = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked0>\n+    %AA = triton_gpu.convert_layout %A : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_a>\n+    %BB = triton_gpu.convert_layout %B : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_b>\n+    %CC = triton_gpu.convert_layout %C : (tensor<64x64xf32, #blocked0>) -> tensor<64x64xf32, #mma0>\n+\n+    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, %cst {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[new_mma]]>\n+    %D = tt.dot %AA, %BB, %CC {allowTF32 = true} : tensor<64x64xf16, #dot_operand_a> * tensor<64x64xf16, #dot_operand_b> -> tensor<64x64xf32, #mma0>\n+    %res = triton_gpu.convert_layout %D : (tensor<64x64xf32, #mma0>) -> tensor<64x64xf32, #blocked0>\n+\n+    return %res : tensor<64x64xf32, #blocked0>\n+  }\n+}\n+\n+\n+// -----\n+// Check id in multiple MMA layout instances\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [4, 4], order = [1, 0]}>\n+#shared0 = #triton_gpu.shared<{vec = 1, perPhase=2, maxPhase=8 ,order = [1, 0]}>\n+#mma0 = #triton_gpu.mma<{versionMajor=1, versionMinor=0, warpsPerCTA=[4,4]}>\n+// mma id=1, with all other boolean flags be false, should get a versionMinor of 16(= 1 * 1<<4)\n+#mma1 = #triton_gpu.mma<{versionMajor=1, versionMinor=16, warpsPerCTA=[4,4]}>\n+\n+// Will still get two MMA layouts\n+// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 4]}>\n+// CHECK: [[new_mma1:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 19, warpsPerCTA = [4, 4]}>\n+\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, isMMAv1Row=true}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n+#dot_operand_a1 = #triton_gpu.dot_op<{opIdx=0, parent=#mma1, isMMAv1Row=true}>\n+#dot_operand_b1 = #triton_gpu.dot_op<{opIdx=1, parent=#mma1, isMMAv1Row=false}>\n+\n+module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n+  // CHECK-LABEL: dot_mmav1\n+  func @dot_mmav1(%A: tensor<64x64xf16, #blocked0>, %B: tensor<64x64xf16, #blocked0>) -> tensor<64x64xf32, #blocked0> {\n+    %C = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked0>\n+    %AA = triton_gpu.convert_layout %A : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_a>\n+    %BB = triton_gpu.convert_layout %B : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_b>\n+    %CC = triton_gpu.convert_layout %C : (tensor<64x64xf32, #blocked0>) -> tensor<64x64xf32, #mma0>\n+\n+    %AA1 = triton_gpu.convert_layout %A : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_a1>\n+    %BB1 = triton_gpu.convert_layout %B : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_b1>\n+    %CC1 = triton_gpu.convert_layout %C : (tensor<64x64xf32, #blocked0>) -> tensor<64x64xf32, #mma1>\n+\n+    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, {{.*}} {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[new_mma]]>\n+    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, {{.*}} {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma1]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma1]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[new_mma1]]>\n+    %D = tt.dot %AA, %BB, %CC {allowTF32 = true} : tensor<64x64xf16, #dot_operand_a> * tensor<64x64xf16, #dot_operand_b> -> tensor<64x64xf32, #mma0>\n+    %D1 = tt.dot %AA1, %BB1, %CC1 {allowTF32 = true} : tensor<64x64xf16, #dot_operand_a1> * tensor<64x64xf16, #dot_operand_b1> -> tensor<64x64xf32, #mma1>\n+    %res = triton_gpu.convert_layout %D : (tensor<64x64xf32, #mma0>) -> tensor<64x64xf32, #blocked0>\n+    %res1 = triton_gpu.convert_layout %D1 : (tensor<64x64xf32, #mma1>) -> tensor<64x64xf32, #blocked0>\n+    %sum = arith.addf %res, %res1 : tensor<64x64xf32, #blocked0>\n+\n+    return %sum : tensor<64x64xf32, #blocked0>\n+  }\n+}"}, {"filename": "unittest/Dialect/TritonGPU/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -1,6 +1,5 @@\n-\n add_triton_ut(\n \tNAME TestSwizzling\n \tSRCS SwizzleTest.cpp\n \tLIBS TritonGPUIR  ${dialect_libs} ${conversion_libs}\n-)\n\\ No newline at end of file\n+)"}]