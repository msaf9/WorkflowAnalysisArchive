[{"filename": "python/triton/compiler.py", "status": "modified", "additions": 76, "deletions": 65, "changes": 141, "file_content_changes": "@@ -20,6 +20,10 @@\n import triton\n import triton._C.libtriton.triton as _triton\n \n+import shutil\n+import subprocess\n+from sysconfig import get_paths\n+\n \n def str_to_ty(name):\n     if name[0] == \"*\":\n@@ -917,57 +921,6 @@ def generate_name_initializer(signature):\n         src\n \n \n-@contextlib.contextmanager\n-def quiet():\n-    old_stdout, old_stderr = sys.stdout, sys.stderr\n-    sys.stdout, sys.stderr = io.StringIO(), io.StringIO()\n-    try:\n-        yield\n-    finally:\n-        sys.stdout, sys.stderr = old_stdout, old_stderr\n-\n-\n-@functools.lru_cache()\n-def libcuda_dir():\n-    loc = subprocess.check_output([\"whereis\", \"libcuda.so\"]).decode().strip().split()[-1]\n-    return os.path.dirname(loc)\n-\n-\n-def _build(name, src, path):\n-    # add framework\n-    extra_compile_args = []\n-    library_dirs = [libcuda_dir()]\n-    include_dirs = [path, \"/usr/local/cuda/include/\"]\n-    libraries = ['cuda']\n-    # extra arguments\n-    extra_link_args = []\n-    # create extension module\n-    ext = setuptools.Extension(\n-        name=name,\n-        language='c++',\n-        sources=[src],\n-        include_dirs=include_dirs,\n-        extra_compile_args=extra_compile_args + ['-O3'],\n-        extra_link_args=extra_link_args,\n-        library_dirs=library_dirs,\n-        libraries=libraries,\n-    )\n-    # build extension module\n-    args = ['build_ext']\n-    args.append('--build-temp=' + path)\n-    args.append('--build-lib=' + path)\n-    args.append('-q')\n-    args = dict(\n-        name=name,\n-        ext_modules=[ext],\n-        script_args=args,\n-    )\n-    # with quiet():\n-    setuptools.setup(**args)\n-    suffix = sysconfig.get_config_var('EXT_SUFFIX')\n-    so = os.path.join(path, '{name}{suffix}'.format(name=name, suffix=suffix))\n-    return so\n-\n \n def binary_name_to_header_name(name):\n     if len(name) > 128:\n@@ -1030,7 +983,7 @@ def format_of(ty):\n #include \\\"cuda.h\\\"\n #include <Python.h>\n \n-inline void gpuAssert(CUresult code, const char *file, int line)\n+static inline void gpuAssert(CUresult code, const char *file, int line)\n {{\n    if (code != CUDA_SUCCESS)\n    {{\n@@ -1048,7 +1001,7 @@ def format_of(ty):\n static CUmodule module = 0;\n static CUfunction function = 0;\n \n-static void init_function(const char* name, const unsigned char* src, size_t n_shared_bytes, int64_t device){{\n+static inline void init_function(const char* name, const unsigned char* src, size_t n_shared_bytes, int64_t device){{\n   CUmodule mod;\n   CUfunction fun;\n   CUDA_CHECK(cuModuleLoadData(&mod, src));\n@@ -1070,7 +1023,7 @@ def format_of(ty):\n   function = fun;\n }}\n \n-static void init_module(CUdevice device) {{\n+static inline void init_module(CUdevice device) {{\n   {func_init}\n }}\n \n@@ -1209,16 +1162,70 @@ def make_cache_key(fn, signature, configs, constants, num_warps, num_stages):\n     key = hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n     return key\n \n+# utilties for generating and compiling C wrappers\n \n-def make_shared_object(fn, constants, signature, num_warps, binaries, tmpdir):\n-    src = generate_torch_glue(fn.__name__, constants, signature, num_warps, binaries, tmpdir)\n-    src_path = os.path.join(tmpdir, \"main.c\")\n-    with open(src_path, \"w\") as f:\n-        f.write(src)\n+@functools.lru_cache()\n+def libcuda_dir():\n+    loc = subprocess.check_output([\"whereis\", \"libcuda.so\"]).decode().strip().split()[-1]\n+    return os.path.dirname(loc)\n+\n+@contextlib.contextmanager\n+def quiet():\n+    old_stdout, old_stderr = sys.stdout, sys.stderr\n+    sys.stdout, sys.stderr = io.StringIO(), io.StringIO()\n+    try:\n+        yield\n+    finally:\n+        sys.stdout, sys.stderr = old_stdout, old_stderr\n+\n+def _build(name, src, srcdir):\n+    cuda_lib_dir = libcuda_dir()\n+    cu_include_dir = \"/usr/local/cuda/include\"\n+    suffix = sysconfig.get_config_var('EXT_SUFFIX')\n+    so = os.path.join(srcdir, '{name}{suffix}'.format(name=name, suffix=suffix))\n+    # try to avoid setuptools if possible\n+    cc = os.environ.get(\"CC\")\n+    if cc is None:\n+      # TODO: support more things here.\n+      clang = shutil.which(\"clang\")\n+      gcc = shutil.which(\"gcc\")\n+      cc = gcc if gcc is not None else clang\n+    py_include_dir = get_paths()[\"include\"]\n+    ret = subprocess.check_call([cc, src, \"-O3\", f\"-I{cu_include_dir}\", f\"-I{py_include_dir}\",  f\"-I{srcdir}\", \"-shared\", \"-fPIC\", f\"-L{cuda_lib_dir}\", f\"-lcuda\", \"-o\", so])\n+    if ret == 0:\n+      return so\n+    # fallback on setuptools\n+    extra_compile_args = []\n+    library_dirs = [cuda_lib_dir]\n+    include_dirs = [srcdir, cu_include_dir]\n+    libraries = ['cuda']\n+    # extra arguments\n+    extra_link_args = []\n+    # create extension module\n+    ext = setuptools.Extension(\n+        name=name,\n+        language='c',\n+        sources=[src],\n+        include_dirs=include_dirs,\n+        extra_compile_args=extra_compile_args + ['-O3'],\n+        extra_link_args=extra_link_args,\n+        library_dirs=library_dirs,\n+        libraries=libraries,\n+    )\n+    # build extension module\n+    args = ['build_ext']\n+    args.append('--build-temp=' + srcdir)\n+    args.append('--build-lib=' + srcdir)\n+    args.append('-q')\n+    args = dict(\n+        name=name,\n+        ext_modules=[ext],\n+        script_args=args,\n+    )\n     with quiet():\n-        bin_path = _build(fn.__name__, src_path, tmpdir)\n-    with open(bin_path, \"rb\") as f:\n-        return f.read()\n+      setuptools.setup(**args)\n+    return so\n+\n \n \n def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: int = 4, num_stages: int = 3, extern_libs=None, configs=None):\n@@ -1243,10 +1250,14 @@ def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: i\n     with tempfile.TemporaryDirectory() as tmpdir:\n         all_constants = set(constants.keys())\n         all_constants.update(configs[0].equal_to_1)\n-        so = make_shared_object(fn, all_constants, signature, num_warps, binaries, tmpdir)\n+        src = generate_torch_glue(fn.__name__, constants, signature, num_warps, binaries, tmpdir)\n+        src_path = os.path.join(tmpdir, \"main.c\")\n+        with open(src_path, \"w\") as f:\n+            f.write(src)\n+        so = _build(fn.__name__, src_path, tmpdir)\n+        with open(so, \"rb\") as f:\n+          cache_manager.put(f.read())\n \n-    # write shared object to cache\n-    cache_manager.put(so)\n     return CompiledKernel(fn.__name__, cache_manager.bin_path)\n \n "}]