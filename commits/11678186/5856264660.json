[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -99,7 +99,7 @@ jobs:\n         if: ${{ env.BACKEND == 'CUDA' && env.ENABLE_TMA == '0' && env.ENABLE_MMA_V3 == '0'}}\n         run: |\n           cd python/test/unit\n-          python3 -m pytest -n 8 --ignore=runtime\n+          python3 -m pytest -n 8 --ignore=runtime --ignore=hopper\n           # run runtime tests serially to avoid race condition with cache handling.\n           python3 -m pytest runtime/\n "}, {"filename": "python/test/unit/hopper/test_persistent_warp_specialized_gemm.py", "status": "modified", "additions": 13, "deletions": 28, "changes": 41, "file_content_changes": "@@ -302,20 +302,7 @@ def test_non_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K\n     th_c = torch.matmul(a, b)\n     torch.testing.assert_allclose(th_c, c, atol=1e-2, rtol=0)\n \n-    # # #############################################Performance Evaluation#############################################\n-    # fn = lambda: call_vintage()\n-    # ms = triton.testing.do_bench(fn, warmup=25, rep=100)\n-    # cur_gpu_perf = round(2. * M * N * K / ms * 1e-9, 2)\n-    # print(' '.join(['Performance of', str(M), str(N), str(K), ':', str(ms), 'ms, ', str(cur_gpu_perf), 'TFLOPS']))\n-\n-\n-@triton.autotune(\n-    configs=[\n-        triton.Config({}, num_stages=3, num_warps=4, enable_warp_specialization=True),\n-        # triton.Config({}, num_stages=3, num_warps=4, enable_warp_specialization=False),\n-    ],\n-    key=['M', 'N', 'K'],\n-)\n+\n @triton.jit\n def static_persistent_warp_specialized_matmul_kernel(\n     a_ptr, b_ptr, c_ptr,\n@@ -355,13 +342,6 @@ def static_persistent_warp_specialized_matmul_kernel(\n         tl.store(c_ptrs, accumulator)\n \n \n-@triton.autotune(\n-    configs=[\n-        triton.Config({}, num_stages=3, num_warps=4, enable_warp_specialization=True),\n-        # triton.Config({}, num_stages=3, num_warps=4, enable_warp_specialization=False),\n-    ],\n-    key=['M', 'N', 'K'],\n-)\n @triton.jit\n def static_persistent_tma_warp_specialized_matmul_kernel(\n     a_ptr, b_ptr, c_ptr,\n@@ -429,6 +409,12 @@ def static_persistent_tma_warp_specialized_matmul_kernel(\n                              [4096, 4096, 256, 256, 128, 64, 1, False, True],\n                              [4096, 4096, 256, 128, 256, 16, 1, False, True],\n                              [4096, 4096, 256, 128, 256, 64, 1, False, True],\n+                             # numCTAs > 1\n+                             [2048, 2048, 64, 128, 128, 64, 2, False, True],\n+                             [2048, 2048, 128, 256, 128, 64, 4, False, True],\n+                             [4096, 4096, 128, 256, 128, 64, 4, False, True],\n+                             [4096, 4096, 256, 128, 256, 64, 4, False, True],\n+                             [4096, 4096, 256, 256, 256, 64, 4, False, True],\n                          ]\n                              for use_tma in [False, True]\n                          ])\n@@ -455,23 +441,22 @@ def test_user_defined_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N\n             a.stride(0), a.stride(1),\n             b.stride(0), b.stride(1),\n             c.stride(0), c.stride(1),\n-            BLOCK_M, BLOCK_N, BLOCK_K, num_SMs)\n+            BLOCK_M, BLOCK_N, BLOCK_K, num_SMs,\n+            num_warps=4, num_ctas=NUM_CTAS,\n+            enable_warp_specialization=True)\n     else:\n         static_persistent_warp_specialized_matmul_kernel[grid](\n             a, b, c,\n             M, N, K,\n             a.stride(0), a.stride(1),\n             b.stride(0), b.stride(1),\n             c.stride(0), c.stride(1),\n-            BLOCK_M, BLOCK_N, BLOCK_K, num_SMs)\n+            BLOCK_M, BLOCK_N, BLOCK_K, num_SMs,\n+            num_warps=4, num_ctas=NUM_CTAS,\n+            enable_warp_specialization=True)\n \n     th_c = torch.matmul(a, b)\n     torch.testing.assert_allclose(th_c, c, atol=1e-2, rtol=0)\n-    # #############################################Performance Evaluation#############################################\n-    # fn = lambda: call_stylish()\n-    # ms = triton.testing.do_bench(fn, warmup=25, rep=100)\n-    # cur_gpu_perf = round(2. * M * N * K / ms * 1e-9, 2)\n-    # print(' '.join(['Performance of', str(M), str(N), str(K), ':', str(ms), 'ms, ', str(cur_gpu_perf), 'TFLOPS']))\n \n \n @triton.jit"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 26, "deletions": 28, "changes": 54, "file_content_changes": "@@ -58,14 +58,10 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 (256, 128, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 (256, 128, 32, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                # split-k\n-                (64, 64, 16, 2, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (64, 64, 16, 4, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (64, 64, 16, 8, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 # variable input\n                 (128, 128, 32, 1, 4, 2, 256, 384, 160, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 107, 233, 256, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 107, 233, 311, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 107, 233, 128, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 107, 233, 83, AT, BT, DTYPE, DTYPE),\n                 (128, 256, 64, 1, 8, 3, 256, 512, 160, AT, BT, DTYPE, DTYPE),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True]\n         ],\n@@ -77,9 +73,6 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (128, 64, 16, 1, 4, STAGES, 256, 128, 80, AT, BT, DTYPE, DTYPE),\n                 (256, 128, 32, 1, 8, STAGES, 512, 256, 160, AT, BT, DTYPE, DTYPE),\n                 (128, 128, 32, 1, 4, STAGES, 256, 256, 160, AT, BT, DTYPE, DTYPE),\n-                # split-k\n-                (64, 64, 16, 8, 4, STAGES, 128, 128, 768, AT, BT, DTYPE, DTYPE),\n-                (64, 64, 16, 8, 4, STAGES, 128, 128, 32, AT, BT, DTYPE, DTYPE),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True] for STAGES in [4]\n         ],\n         # mixed-precision\n@@ -88,7 +81,6 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n                 (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n                 (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n-                (128, 128, 32, 8, 4, 2, 256, 256, 128, AT, BT, ADTYPE, BDTYPE),\n             ] for ADTYPE, BDTYPE in [(\"float8e4\", \"float8e5\"),\n                                      (\"float8e4\", \"float16\"),\n                                      (\"float16\", \"float8e5\"),\n@@ -131,35 +123,44 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT,\n     M = BLOCK_M if M is None else M\n     N = BLOCK_N if N is None else N\n     K = BLOCK_K * SPLIT_K if K is None else K\n-    a_fp8 = \"float8\" in ADTYPE\n-    b_fp8 = \"float8\" in BDTYPE\n \n     def maybe_upcast(x, dtype, is_float8):\n         if is_float8:\n             return f8_to_f16(x, dtype)\n         return x\n \n-    def init_input(n, m, t, dtype, is_float8):\n-        if t:\n-            return init_input(m, n, False, dtype, is_float8).t()\n-        if is_float8:\n-            return torch.randint(20, 50, (n, m), device=\"cuda\", dtype=torch.int8)\n+    def init_input(m, n, dtype):\n+        if 'float8' in dtype:\n+            ewidth = {'float8e4b15': 4, 'float8e4': 4, 'float8e5': 5}[dtype]\n+            sign = torch.randint(2, size=(m, n), device=\"cuda\", dtype=torch.int8) * 128\n+            val = torch.randint(2**3 - 1, size=(m, n), device=\"cuda\", dtype=torch.int8) << 7 - ewidth\n+            return sign | val\n         if dtype == \"int8\":\n-            return torch.randint(-128, 127, (n, m), device=\"cuda\", dtype=torch.int8)\n+            return torch.randint(-128, 127, (m, n), device=\"cuda\", dtype=torch.int8)\n         dtype = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[dtype]\n-        return .1 * torch.randn((n, m), device=\"cuda\", dtype=dtype)\n+        exponents = torch.randint(-10, 0, size=(m, n))\n+        ret = (2. ** exponents).to(dtype).to(\"cuda\")\n+        return ret\n \n     # allocate/transpose inputs\n-    a = init_input(M, K, AT, ADTYPE, a_fp8)\n-    b = init_input(K, N, BT, BDTYPE, b_fp8)\n+    a = init_input(M, K, ADTYPE)\n+    b = init_input(K, N, BDTYPE)\n+    a = a if not AT else a.T.contiguous().T\n+    b = b if not BT else b.T.contiguous().T\n     # run test\n-    th_a = maybe_upcast(a, ADTYPE, a_fp8).to(torch.float32)\n+    a_fp8 = \"float8\" in ADTYPE\n+    b_fp8 = \"float8\" in BDTYPE\n+    th_a = maybe_upcast(a, ADTYPE, a_fp8)\n     if AT and a_fp8:\n         th_a = th_a.view(th_a.shape[::-1]).T\n-    th_b = maybe_upcast(b, BDTYPE, b_fp8).to(torch.float32)\n+    th_b = maybe_upcast(b, BDTYPE, b_fp8)\n     if BT and b_fp8:\n         th_b = th_b.view(th_b.shape[::-1]).T\n-    th_c = torch.matmul(th_a, th_b)\n+    if th_a.is_floating_point():\n+        ab_dtype = th_a.dtype if th_a.element_size() > th_b.element_size() else th_b.dtype\n+    else:\n+        ab_dtype = torch.float32\n+    th_c = torch.matmul(th_a.to(ab_dtype), th_b.to(ab_dtype))\n     if ADTYPE == \"int8\" or BDTYPE == \"int8\":\n         th_c = th_c.to(torch.int8)\n     try:\n@@ -168,9 +169,6 @@ def init_input(n, m, t, dtype, is_float8):\n         if b_fp8:\n             b = triton.reinterpret(b, getattr(tl, BDTYPE))\n         tt_c = triton.ops.matmul(a, b)\n-        atol, rtol = 1e-2, 0\n-        if ADTYPE == torch.bfloat16 or BDTYPE == torch.bfloat16:\n-            atol, rtol = 3.5e-2, 0\n-        torch.testing.assert_allclose(th_c, tt_c, atol=atol, rtol=rtol)\n+        torch.testing.assert_allclose(th_c, tt_c, atol=0, rtol=0)\n     except triton.OutOfResources as e:\n         pytest.skip(str(e))"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 13, "deletions": 14, "changes": 27, "file_content_changes": "@@ -11,10 +11,9 @@\n from pathlib import Path\n from typing import Any, Tuple\n \n-import triton\n-import triton._C.libtriton.triton as _triton\n-from .._C.libtriton.triton import (add_external_libs, compile_ptx_to_cubin,\n-                                   get_num_warps, get_shared_memory_size, ir,\n+from .._C.libtriton.triton import (ClusterInfo, TMAInfos, add_external_libs,\n+                                   compile_ptx_to_cubin, get_env_vars, get_num_warps,\n+                                   get_shared_memory_size, ir,\n                                    translate_llvmir_to_hsaco, translate_llvmir_to_ptx,\n                                    translate_triton_gpu_to_llvmir)\n from ..common.backend import get_backend, path_to_ptxas\n@@ -101,8 +100,8 @@ def optimize_ttgir(mod, num_stages, num_warps, num_ctas, arch,\n     if arch // 10 >= 9 and enable_warp_specialization and num_warps == 4:\n         pm.add_tritongpu_ws_feasibility_checking_pass(arch)\n         pm.run(mod)\n-        ws_enabled = _triton.ir.is_ws_supported(mod)\n-        pm = _triton.ir.pass_manager(mod.context)\n+        ws_enabled = ir.is_ws_supported(mod)\n+        pm = ir.pass_manager(mod.context)\n         pm.enable_debug()\n     if ws_enabled:\n         pm.add_tritongpu_wsdecomposing_pass(arch)\n@@ -426,12 +425,12 @@ def compile(fn, **kwargs):\n     if os.environ.get('OPTIMIZE_EPILOGUE', '') == '1':\n         optimize_epilogue = True\n     #\n-    cluster_info = _triton.ClusterInfo()\n+    cluster_info = ClusterInfo()\n     if \"clusterDims\" in kwargs:\n         cluster_info.clusterDimX = kwargs[\"clusterDims\"][0]\n         cluster_info.clusterDimY = kwargs[\"clusterDims\"][1]\n         cluster_info.clusterDimZ = kwargs[\"clusterDims\"][2]\n-    tma_infos = _triton.TMAInfos()\n+    tma_infos = TMAInfos()\n     # build compilation stages\n     stages = dict()\n     stages[\"ast\"] = (lambda path: fn, None)\n@@ -479,7 +478,7 @@ def compile(fn, **kwargs):\n         first_stage = list(stages.keys()).index(ir_name)\n \n     # create cache manager\n-    fn_cache_manager = get_cache_manager(make_hash(fn, arch, _triton.get_env_vars(), **kwargs))\n+    fn_cache_manager = get_cache_manager(make_hash(fn, arch, get_env_vars(), **kwargs))\n     # determine name and extension type of provided function\n     if isinstance(fn, JITFunction):\n         name, ext = fn.__name__, \"ast\"\n@@ -512,7 +511,7 @@ def compile(fn, **kwargs):\n                     \"constants\": _get_jsonable_constants(constants),\n                     \"debug\": debug,\n                     \"arch\": arch, }\n-        metadata.update(_triton.get_env_vars())\n+        metadata.update(get_env_vars())\n         if ext == \"ptx\":\n             assert \"shared\" in kwargs, \"ptx compilation must provide shared memory size\"\n             metadata[\"shared\"] = kwargs[\"shared\"]\n@@ -558,7 +557,7 @@ def compile(fn, **kwargs):\n         if ir_name == \"llir\" and \"shared\" not in metadata:\n             metadata[\"shared\"] = get_shared_memory_size(module)\n         if ir_name == \"ttgir\":\n-            metadata[\"enable_warp_specialization\"] = _triton.ir.is_ws_supported(next_module)\n+            metadata[\"enable_warp_specialization\"] = ir.is_ws_supported(next_module)\n             if metadata[\"enable_warp_specialization\"]:\n                 metadata[\"num_warps\"] = get_num_warps(next_module)\n         if ir_name == \"ptx\":\n@@ -570,7 +569,7 @@ def compile(fn, **kwargs):\n             _device_backend.add_meta_info(ir_name, module, next_module, metadata, asm)\n         module = next_module\n \n-    ids_of_folded_args = tuple([int(k) for k in configs[0].ids_of_folded_args]) if isinstance(fn, triton.runtime.JITFunction) else ()\n+    ids_of_folded_args = tuple([int(k) for k in configs[0].ids_of_folded_args]) if isinstance(fn, JITFunction) else ()\n     if \"clusterDims\" not in metadata:\n         metadata[\"clusterDims\"] = [\n             cluster_info.clusterDimX,\n@@ -585,10 +584,10 @@ def compile(fn, **kwargs):\n             metadata[\"tensormaps_info\"][i].ids_of_folded_args = ids_of_folded_args\n \n     ids_of_tensormaps = get_ids_of_tensormaps(metadata.get(\"tensormaps_info\", None))\n-    if isinstance(fn, triton.runtime.JITFunction) and \"tensormaps_info\" in metadata:\n+    if isinstance(fn, JITFunction) and \"tensormaps_info\" in metadata:\n         fn.tensormaps_info = metadata[\"tensormaps_info\"]\n \n-    ids_of_const_exprs = tuple(fn.constexprs) if isinstance(fn, triton.runtime.JITFunction) else ()\n+    ids_of_const_exprs = tuple(fn.constexprs) if isinstance(fn, JITFunction) else ()\n     ids = {\"ids_of_tensormaps\": ids_of_tensormaps, \"ids_of_folded_args\": ids_of_folded_args, \"ids_of_const_exprs\": ids_of_const_exprs}\n     # cache manager\n     if is_cuda or is_hip:"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -11,7 +11,7 @@\n from typing import (Callable, Generic, Iterable, List, Optional, TypeVar, Union, cast,\n                     overload)\n \n-import triton\n+from .._C.libtriton.triton import TMAInfos\n from ..common.backend import get_backend, path_to_ptxas\n \n TRITON_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n@@ -481,7 +481,7 @@ def __init__(self, fn, version=None, do_not_specialize=None, debug=None, noinlin\n         # index of constexprs\n         self.constexprs = [self.arg_names.index(name) for name, ty in self.__annotations__.items() if 'constexpr' in ty]\n         # tma info\n-        self.tensormaps_info = triton._C.libtriton.triton.TMAInfos()\n+        self.tensormaps_info = TMAInfos()\n         # launcher\n         self.run = self._make_launcher()\n         # re-use docs of wrapped function"}]