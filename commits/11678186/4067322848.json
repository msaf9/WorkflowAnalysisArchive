[{"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -294,8 +294,7 @@ def TT_CatOp : TT_Op<\"cat\", [NoSideEffect,\n \n def TT_TransOp : TT_Op<\"trans\", [NoSideEffect,\n                                  DeclareOpInterfaceMethods<InferTypeOpInterface>,\n-                                 SameOperandsAndResultElementType,\n-                                 SameOperandsAndResultEncoding]> {\n+                                 SameOperandsAndResultElementType]> {\n \n     let summary = \"transpose a tensor\";\n "}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -1433,6 +1433,7 @@ void init_triton_translation(py::module &m) {\n   m.def(\n       \"translate_triton_gpu_to_llvmir\",\n       [](mlir::ModuleOp op, int computeCapability) {\n+        py::gil_scoped_release allow_threads;\n         llvm::LLVMContext llvmContext;\n         auto llvmModule = ::mlir::triton::translateTritonGPUToLLVMIR(\n             &llvmContext, op, computeCapability);\n@@ -1450,6 +1451,7 @@ void init_triton_translation(py::module &m) {\n   m.def(\n       \"translate_llvmir_to_ptx\",\n       [](const std::string llvmIR, int capability, int version) -> std::string {\n+        py::gil_scoped_release allow_threads;\n         // create LLVM module from C++\n         llvm::LLVMContext context;\n         std::unique_ptr<llvm::MemoryBuffer> buffer ="}]