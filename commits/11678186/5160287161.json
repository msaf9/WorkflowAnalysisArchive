[{"filename": "CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -215,6 +215,7 @@ if(TRITON_BUILD_PYTHON_MODULE)\n     ${conversion_libs}\n \n     # optimizations\n+    MLIRBytecodeWriter\n     MLIRPass\n     MLIRTransforms\n     MLIRLLVMDialect"}, {"filename": "include/triton/Dialect/Triton/IR/TritonAttrDefs.td", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -52,4 +52,16 @@ def TT_AtomicRMWAttr : I32EnumAttr<\n     let cppNamespace = \"::mlir::triton\";\n }\n \n+// Program ID dimensions.\n+def TT_ProgramDim : I32EnumAttr<\n+    \"ProgramIDDim\", \"\",\n+    [\n+        I32EnumAttrCase<\"X\", 0, \"x\">,\n+        I32EnumAttrCase<\"Y\", 1, \"y\">,\n+        I32EnumAttrCase<\"Z\", 2, \"z\">,\n+    ]> {\n+    let cppNamespace = \"::mlir::triton\";\n+}\n+\n+\n #endif"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "file_content_changes": "@@ -351,11 +351,17 @@ def TT_TransOp : TT_Op<\"trans\", [Pure,\n // SPMD Ops\n //\n def TT_GetProgramIdOp : TT_Op<\"get_program_id\", [Pure]> {\n-    let arguments = (ins I32Attr:$axis);\n+    let arguments = (ins TT_ProgramDim:$axis);\n \n     let results = (outs I32:$result);\n \n-    let assemblyFormat = \"attr-dict `:` type($result)\";\n+    let assemblyFormat = \"$axis attr-dict `:` type($result)\";\n+\n+    let extraClassDeclaration = [{\n+      int32_t getAxisAsInt() {\n+        return static_cast<int32_t>(getAxis());\n+      }\n+    }];\n }\n \n def TT_GetNumProgramsOp : TT_Op<\"get_num_programs\", [Pure]> {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -384,10 +384,10 @@ struct GetProgramIdOpConversion\n   matchAndRewrite(triton::GetProgramIdOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n-    assert(op.getAxis() < 3);\n+    assert(op.getAxisAsInt() < 3);\n \n     Value blockId =\n-        rewriter.create<::mlir::gpu::BlockIdOp>(loc, dims[op.getAxis()]);\n+        rewriter.create<::mlir::gpu::BlockIdOp>(loc, dims[op.getAxisAsInt()]);\n     rewriter.replaceOpWithNewOp<arith::TruncIOp>(op, i32_ty, blockId);\n     return success();\n   }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -307,8 +307,7 @@ class ConvertTritonGPUToLLVM\n     // Preprocess\n     decomposeMmaToDotOperand(mod, numWarps);\n     decomposeBlockedToDotOperand(mod);\n-    if (failed(decomposeInsertSliceAsyncOp(mod)))\n-      return signalPassFailure();\n+    decomposeInsertSliceAsyncOp(mod);\n \n     // Allocate shared memory and set barrier\n     ModuleAllocation allocation(mod);\n@@ -487,7 +486,7 @@ class ConvertTritonGPUToLLVM\n     });\n   }\n \n-  LogicalResult decomposeInsertSliceAsyncOp(ModuleOp mod) const {\n+  void decomposeInsertSliceAsyncOp(ModuleOp mod) const {\n     ModuleAxisInfoAnalysis axisInfoAnalysis(mod);\n     // TODO(Keren): This is a hacky knob that may cause performance regression\n     // when decomposition has been performed. We should remove this knob once we\n@@ -515,6 +514,7 @@ class ConvertTritonGPUToLLVM\n       // Get the vectorized load size\n       auto src = insertSliceAsyncOp.getSrc();\n       auto dst = insertSliceAsyncOp.getDst();\n+      auto mask = insertSliceAsyncOp.getMask();\n       auto srcTy = src.getType().cast<RankedTensorType>();\n       auto dstTy = dst.getType().cast<RankedTensorType>();\n       auto srcBlocked =\n@@ -523,6 +523,9 @@ class ConvertTritonGPUToLLVM\n           dstTy.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n       auto resElemTy = dstTy.getElementType();\n       unsigned inVec = axisInfoAnalysis.getPtrContiguity(src);\n+      if (mask)\n+        inVec =\n+            std::min<unsigned>(axisInfoAnalysis.getMaskAlignment(mask), inVec);\n       unsigned outVec = resSharedLayout.getVec();\n       unsigned minVec = std::min(outVec, inVec);\n       auto maxBitWidth =\n@@ -586,7 +589,6 @@ class ConvertTritonGPUToLLVM\n         asyncWaitOp.erase();\n       }\n     });\n-    return success();\n   }\n };\n "}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -790,7 +790,6 @@ class CFCondBranchPattern : public OpConversionPattern<cf::CondBranchOp> {\n     if (failed(rewriter.convertRegionTypes(newOp.getFalseDest()->getParent(),\n                                            *converter)))\n       return failure();\n-    rewriter.eraseOp(op);\n     return success();\n   }\n };"}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "file_content_changes": "@@ -55,10 +55,6 @@ SmallVector<unsigned, 2> warpsPerTileV2(triton::DotOp dotOp,\n   SmallVector<unsigned, 2> ret = {1, 1};\n   SmallVector<int64_t, 2> shapePerWarp = {16, 8};\n   bool changed = false;\n-  // TODO (@daadaada): double-check.\n-  // original logic in\n-  // https://github.com/openai/triton/blob/master/lib/codegen/analysis/layout.cc#L252\n-  // seems buggy for shape = [32, 16] ?\n   do {\n     changed = false;\n     if (ret[0] * ret[1] >= numWarps)"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 512, "deletions": 325, "changes": 837, "file_content_changes": "@@ -8,23 +8,87 @@\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+#include \"llvm/ADT/MapVector.h\"\n \n //===----------------------------------------------------------------------===//\n+// This file implements software pipelining for loops. The implementation here\n+// is inspired by the pipeline pass in Triton (version 2.0) and SCF's\n+// LoopPipelining.\n //\n-// This file implements loop software pipelining\n-// The implementation here is inspired by the pipeline pass in Triton (-v2.0)\n-// and SCF's LoopPipelining.\n+// We divide the loop body into the following phases:\n+// a. Pre-load operations: for instance, index computation.\n+// b. Load operations: loading from global memory to shared memory.\n+// c. Compute operations: for instance, Triton dot.\n+// d. Post-load operations: for instance, index computation.\n+//\n+// To pipeline the loop, we need to:\n+// - Hoist the pipelinable load operations for the first numStages-1 iterations\n+// to the loop pre-header\n+// - Find all the dependencies of the load operations.\n+// - Rematerialize the dependencies for their values at the first numStage-1\n+// iterations\n+// - Assemble the loop body (numStage) and prefetch (numStage + 1).\n+//\n+// In the prologue, the sequence of operations is the same as the original loop\n+// body, following the (a) -> (b) -> (c) -> (d) order. In the loop body,\n+// however, we first execute the compute operations, then pre-load operations,\n+// post-load operations, and eventually the asynchronous load operations - in\n+// the (c) -> (a) -> (d) -> (b) order. This is used to better hide the latency\n+// of the load operations. Because of this, if post-load operations have direct\n+// dependencies on the load operations, we could repeat the post-load\n+// operations. More specifically, this occurs when:\n+// 1. Any load operand has an immediate dependency argument used at numStage-1.\n+// 2. The argument is first defined at numStage-2.\n+// To avoid the repeat, we peeled off post-load operations in the prologue that\n+// satisfy the above two conditions. See the example below for the definition of\n+// immediate and non-immediate dependencies.\n+// If we have a load that immediately depends on a block argument in the\n+// current iteration, it is an immediate dependency. Otherwise, it is a\n+// non-immediate dependency, which means the load depends on a block argument\n+// in the previous iterations.\n+// For example:\n+// scf.for (%arg0, %arg1, %arg2) {\n+//   %0 = load %arg0  <--- immediate dep, this address is initialized before\n+//   numStages-1.\n+//   %1 = load %arg1\n+//   %2 = add %1, %arg2\n+//   %3 = load %2  <--- non-immediate dep, %arg1 must be an\n+//   update-to-date value.\n+// }\n+//\n+// Our pipelining pass share some common characteristics with SCF's\n+// LoopPipelining. However, it is also noteworthy that our pipelining pass has\n+// the following characteristics different from SCF's LoopPipelining:\n+// 1. It can handle loop-carried dependencies of distance greater than 1.\n+// 2. It does not have a complicated epilogue but instead uses masking to handle\n+// boundary conditions.\n+// 3. Each operation/loop-carried argument cannot provide values to both\n+// immediate and non-immediate dependencies. Otherwise, we have to rematerialize\n+// the operation and arguments, which would likely increase register pressure.\n+// For example:\n+// scf.for (%arg0, %arg1, %arg2) {\n+//  %0 = load %arg0\n+//  %1 = load %arg1, %0  <--- %0 is both a post-load op at numStages-2 and a\n+//  pre-load op at numStages-1, so that we need two versions of %0.\n+//  %2 = add %0, %arg2\n+//  scf.yield %arg0, %2, %arg2\n+//  }\n //\n //===----------------------------------------------------------------------===//\n \n+using llvm::MapVector;\n using namespace mlir;\n namespace ttg = triton::gpu;\n \n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n \n-// pass named attrs (e.g., tt.contiguity) from Triton to Triton\n-static void addNamedAttrs(Operation *op, DictionaryAttr dictAttrs) {\n+#define int_attr(num) builder.getI64IntegerAttr(num)\n+\n+namespace {\n+\n+// Pass named attrs (e.g., tt.contiguity) from Triton to Triton\n+void addNamedAttrs(Operation *op, DictionaryAttr dictAttrs) {\n   NamedAttrList attrs = op->getDiscardableAttrs();\n   // Collect the attributes to propagate: the ones in dictAttrs and not yet on\n   // the operation.\n@@ -40,19 +104,13 @@ static void addNamedAttrs(Operation *op, DictionaryAttr dictAttrs) {\n   }\n }\n \n-#define int_attr(num) builder.getI64IntegerAttr(num)\n-\n-namespace {\n-\n class LoopPipeliner {\n-  /// Cache forOp we are working on\n+  /// Cache of ForOp and YieldOp related to this pipeliner.\n   scf::ForOp forOp;\n-\n-  /// Cache YieldOp for this forOp\n   scf::YieldOp yieldOp;\n \n   /// Loads to be pipelined\n-  SetVector<Value> loads;\n+  SetVector<Value> validLoads;\n   /// Smallest data-type for each load (used to optimize swizzle and\n   /// (create DotOpEncoding layout)\n   DenseMap<Value, Type> loadsSmallestType;\n@@ -66,59 +124,111 @@ class LoopPipeliner {\n   DenseMap<Value, SmallVector<Value>> loadStageBuffer;\n   /// load => after extract\n   DenseMap<Value, Value> loadsExtract;\n-  ///\n+\n+  /// Iterator values\n   Value pipelineIterIdx;\n-  ///\n   Value loopIterIdx;\n+  Value nextIV;\n+\n+  /// Yield values\n+  SmallVector<Value> nextBuffers;\n+  SmallVector<Value> extractSlices;\n+  SmallVector<Value> yieldValues;\n \n-  /// Comments on numStages:\n-  ///   [0, numStages-1) are in the prologue\n-  ///   numStages-1 is appended after the loop body\n+  /// The number of stages in the pipeline.\n+  /// Stages in the range of [0, numStages-1) are in the prologue.\n+  /// numStages-1 is appended after the loop body.\n   int numStages;\n \n+  /// Arg indicies\n+  size_t bufferIdx, loadIdx, depArgsBeginIdx, ivIndex;\n+  DenseMap<BlockArgument, size_t> depArgsIdx;\n+\n   /// value (in loop) => value at stage N\n   DenseMap<Value, SmallVector<Value>> valueMapping;\n+  /// loop iter arg => value\n+  DenseMap<BlockArgument, Value> depArgsMapping;\n+  /// forOp value => newForOp value\n+  IRMapping mapping;\n+  /// forOp value => prefetch value\n+  IRMapping nextMapping;\n+\n+  /// Dependency ops by program order\n+  SmallVector<Operation *> orderedDeps;\n+\n+  /// arg => source operand defined stages\n+  DenseMap<BlockArgument, DenseSet<int>> immediateArgStages;\n \n-  /// Block arguments that loads depend on\n+  /// block arguments that loads depend on\n   SetVector<BlockArgument> depArgs;\n \n-  /// If we have a load that immediately depends on a block argument in the\n-  /// current iteration, it is an immediate dependency. Otherwise, it is a\n-  /// non-immediate dependency, which means the load depends on a block argument\n-  /// in the previous iterations.\n-  /// For example:\n-  /// scf.for (%arg0, %arg1, %arg2) {\n-  ///   %0 = load %arg0  <--- immediate dep, this address is initialized at\n-  ///   numStages-2\n-  ///   %1 = load %arg1\n-  ///   %2 = add %1, %arg2\n-  ///   %3 = load %2  <--- non-immediate dep, %arg1 must be an update-to-date\n-  ///   value\n-  /// }\n-  SetVector<BlockArgument> immedidateDepArgs;\n-\n-  SetVector<BlockArgument> nonImmedidateDepArgs;\n-\n-  /// Operations (inside the loop body) that loads depend on\n+  /// operation => source operand defined stages\n+  DenseMap<Operation *, DenseSet<int>> immediateOpStages;\n+\n+  /// operations that loads depend on\n   SetVector<Operation *> depOps;\n \n-  /// collect values that v depends on and are defined inside the loop\n-  void collectDeps(Value v, int stages, SetVector<Value> &deps);\n+  /// Collect all pipelinable ops\n+  LogicalResult collectOps(SetVector<Operation *> &ops);\n+\n+  /// Collect values that `v` depends on and are defined inside the loop\n+  void collectValueDep(Value v, int stage, SetVector<Value> &opDeps);\n+\n+  /// Collect all op dependencies\n+  void collectDeps(SetVector<Operation *> &ops,\n+                   MapVector<Operation *, SetVector<Value>> &opDeps);\n \n+  /// Check if none of the ops has valid uses\n+  LogicalResult checkOpUses(SetVector<Operation *> &ops);\n+\n+  /// Check if ops have dependencies that are not pipelinable\n+  void checkOpDeps(SetVector<Operation *> &ops);\n+\n+  void createBufferTypes();\n+\n+  void createOrderedDeps();\n+\n+  /// Return the stage at which `v` is defined prior to `stage`\n+  int getValueDefStage(Value v, int stage);\n+\n+  /// Map `origin` to `newValue` at `stage`\n   void setValueMapping(Value origin, Value newValue, int stage);\n \n+  /// Map `origin` to `newValue` at `stage` according to the association between\n+  /// yieldOp and forOp\n+  void setValueMappingYield(Value origin, Value newValue, int stage);\n+\n+  /// Map `origin` to `newValue` at the next stage according to the association\n+  /// between yieldOp and forOp\n+  void setValueMappingYield(scf::ForOp newForOp, Value origin, Value newValue);\n+\n+  /// Return the value mapped to `origin` at `stage`, if it exists.\n   Value lookupOrDefault(Value origin, int stage);\n \n+  /// Get the load mask for `loadOp`, given the mapped mask `mappedMask` (if\n+  /// exists) and the current iteration's `loopCond`.\n   Value getLoadMask(triton::LoadOp loadOp, Value mappedMask, Value loopCond,\n                     OpBuilder &builder);\n \n-  /// Returns a empty buffer of size <numStages, ...>\n-  ttg::AllocTensorOp allocateEmptyBuffer(Operation *op, OpBuilder &builder);\n+  /// Return an empty buffer of size <numStages, ...>\n+  ttg::AllocTensorOp allocateEmptyBuffer(triton::LoadOp loadOp,\n+                                         OpBuilder &builder);\n+\n+  /// Collect all args of the new loop\n+  SmallVector<Value> collectNewLoopArgs();\n+\n+  /// Clone the forOp and return the new forOp\n+  scf::ForOp cloneForOp(ArrayRef<Value> newLoopArgs, OpBuilder &builder);\n+\n+  /// Prefetch the next iteration for `newForOp`\n+  void prefetchNextIteration(scf::ForOp newForOp, OpBuilder &builder);\n+\n+  /// Assemble `newForOp`'s yield op\n+  void finalizeYield(scf::ForOp newForOp, OpBuilder &builder);\n \n public:\n   LoopPipeliner(scf::ForOp forOp, int numStages)\n       : forOp(forOp), numStages(numStages) {\n-    // cache yieldOp\n     yieldOp = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n   }\n \n@@ -137,159 +247,246 @@ class LoopPipeliner {\n   friend struct PipelinePass;\n };\n \n-// helpers\n-void LoopPipeliner::setValueMapping(Value origin, Value newValue, int stage) {\n-  if (valueMapping.find(origin) == valueMapping.end())\n-    valueMapping[origin] = SmallVector<Value>(numStages);\n-  valueMapping[origin][stage] = newValue;\n-}\n+/// Collect loads to pipeline. Return success if we can pipeline this loop\n+LogicalResult LoopPipeliner::collectOps(SetVector<Operation *> &ops) {\n+  ModuleOp moduleOp = forOp->getParentOfType<ModuleOp>();\n+  ModuleAxisInfoAnalysis axisInfoAnalysis(moduleOp);\n \n-Value LoopPipeliner::lookupOrDefault(Value origin, int stage) {\n-  if (valueMapping.find(origin) == valueMapping.end())\n-    return origin;\n-  return valueMapping[origin][stage];\n+  // We cannot use forOp.walk(...) here because we only want to visit the\n+  // operations in the loop body block. Nested blocks are handled separately.\n+  for (Operation &op : forOp)\n+    if (auto loadOp = dyn_cast<triton::LoadOp>(&op)) {\n+      auto ptr = loadOp.getPtr();\n+      unsigned vec = axisInfoAnalysis.getPtrContiguity(ptr);\n+\n+      if (auto mask = loadOp.getMask())\n+        vec = std::min<unsigned>(vec, axisInfoAnalysis.getMaskAlignment(mask));\n+\n+      auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n+      if (!tensorTy || tensorTy.getRank() < 2)\n+        continue;\n+      auto ty = tensorTy.getElementType()\n+                    .cast<triton::PointerType>()\n+                    .getPointeeType();\n+      unsigned width = vec * ty.getIntOrFloatBitWidth();\n+      // We do not pipeline all loads for the following reasons:\n+      // 1. On nvidia GPUs, cp.async's cp-size can only be 4, 8 and 16.\n+      // 2. It's likely that pipling small loads won't offer much performance\n+      //    improvement and may even hurt performance by increasing register\n+      //    pressure.\n+      if (width >= 32)\n+        ops.insert(loadOp);\n+    }\n+\n+  if (ops.empty())\n+    return failure();\n+  else\n+    return success();\n }\n \n-void LoopPipeliner::collectDeps(Value v, int stages, SetVector<Value> &deps) {\n+void LoopPipeliner::collectValueDep(Value v, int stage,\n+                                    SetVector<Value> &deps) {\n   // Loop-invariant value, skip\n   if (v.getParentRegion() != &forOp.getLoopBody())\n     return;\n \n-  // Since we only need to peel the loop numStages-1 times, don't worry about\n-  // depends that are too far away\n-  if (stages < 0)\n+  // Since we only need to peel the loop numStages-1 times, don't worry\n+  // about depends that are too far away\n+  if (stage < 0)\n     return;\n \n   if (auto arg = v.dyn_cast<BlockArgument>()) {\n     if (arg.getArgNumber() > 0) {\n-      // Skip the first arg (loop induction variable)\n-      // Otherwise the op idx is arg.getArgNumber()-1\n       deps.insert(v);\n-      collectDeps(yieldOp->getOperand(arg.getArgNumber() - 1), stages - 1,\n-                  deps);\n+      collectValueDep(yieldOp->getOperand(arg.getArgNumber() - 1), stage - 1,\n+                      deps);\n     }\n   } else { // value\n-    // v might be in deps, but we still need to visit v.\n-    // This is because v might depend on value in previous iterations\n     deps.insert(v);\n     for (Value op : v.getDefiningOp()->getOperands())\n-      collectDeps(op, stages, deps);\n+      collectValueDep(op, stage, deps);\n   }\n }\n \n-ttg::AllocTensorOp LoopPipeliner::allocateEmptyBuffer(Operation *op,\n-                                                      OpBuilder &builder) {\n-  // Allocate a buffer for each pipelined tensor\n-  // shape: e.g. (numStages==4), <32x64xbf16> -> <4x32x64xbf16>\n-  Value convertLayout = loadsMapping[op->getResult(0)];\n-  if (auto tensorType = convertLayout.getType().dyn_cast<RankedTensorType>()) {\n-    return builder.create<ttg::AllocTensorOp>(\n-        convertLayout.getLoc(), loadsBufferType[op->getResult(0)]);\n+void LoopPipeliner::collectDeps(\n+    SetVector<Operation *> &ops,\n+    MapVector<Operation *, SetVector<Value>> &valueDeps) {\n+  for (auto op : ops) {\n+    for (Value v : op->getOperands()) {\n+      SetVector<Value> deps;\n+      collectValueDep(v, numStages - 1, deps);\n+      valueDeps[op] = deps;\n+    }\n   }\n-  llvm_unreachable(\"Async copy's return should be of RankedTensorType\");\n }\n \n-/// A load instruction can be pipelined if:\n-///   - the load doesn't depend on any other loads (after loop peeling)\n-///   - (?) this load is not a loop-invariant value (we should run LICM before\n-///                                                  this pass?)\n-LogicalResult LoopPipeliner::initialize() {\n-  Block *loop = forOp.getBody();\n-  ModuleOp moduleOp = forOp->getParentOfType<ModuleOp>();\n-  ModuleAxisInfoAnalysis axisInfoAnalysis(moduleOp);\n-\n-  // can we use forOp.walk(...) here?\n-  SmallVector<triton::LoadOp, 2> validLoads;\n-  for (Operation &op : *loop)\n-    if (auto loadOp = dyn_cast<triton::LoadOp>(&op)) {\n-      auto ptr = loadOp.getPtr();\n-      unsigned vec = axisInfoAnalysis.getPtrContiguity(ptr);\n+LogicalResult LoopPipeliner::checkOpUses(SetVector<Operation *> &ops) {\n+  DenseSet<Operation *> invalidOps;\n+  // Collect all ops' dependencies\n+  MapVector<Operation *, SetVector<Value>> opDeps;\n+  collectDeps(ops, opDeps);\n+\n+  for (Operation *op : ops) {\n+    if (auto loadOp = dyn_cast<triton::LoadOp>(op)) {\n+      // Don't pipeline valid loads that depend on other valid loads\n+      // (Because if a valid load depends on another valid load, this load needs\n+      // to wait on the other load in the prologue, which is against the point\n+      // of the pipeline pass)\n+      bool isCandidate = true;\n+      for (Operation *other : ops)\n+        if (isa<triton::LoadOp>(other))\n+          if (opDeps[op].contains(other->getResult(0))) {\n+            isCandidate = false;\n+            break;\n+          }\n+      // We only pipeline loads that have one covert_layout (to dot_op) use\n+      // TODO: lift this constraint in the future\n+      if (isCandidate && loadOp.getResult().hasOneUse()) {\n+        isCandidate = false;\n+        Operation *use = *loadOp.getResult().getUsers().begin();\n+\n+        // Advance to the first conversion as long as the use resides in shared\n+        // memory and it has a single use itself\n+        while (use) {\n+          if (use->getNumResults() != 1 || !use->getResult(0).hasOneUse())\n+            break;\n+          auto tensorType =\n+              use->getResult(0).getType().dyn_cast<RankedTensorType>();\n+          if (!tensorType.getEncoding().isa<ttg::SharedEncodingAttr>())\n+            break;\n+          use = *use->getResult(0).getUsers().begin();\n+        }\n \n-      if (auto mask = loadOp.getMask())\n-        vec = std::min<unsigned>(vec, axisInfoAnalysis.getMaskAlignment(mask));\n+        if (auto convertLayout = llvm::dyn_cast<ttg::ConvertLayoutOp>(use))\n+          if (auto tensorType = convertLayout.getResult()\n+                                    .getType()\n+                                    .dyn_cast<RankedTensorType>())\n+            if (auto dotOpEnc = tensorType.getEncoding()\n+                                    .dyn_cast<ttg::DotOperandEncodingAttr>()) {\n+              isCandidate = true;\n+              loadsMapping[loadOp] = convertLayout;\n+            }\n+      } else\n+        isCandidate = false;\n \n-      auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n-      if (!tensorTy || tensorTy.getRank() < 2)\n-        continue;\n-      auto ty = tensorTy.getElementType()\n-                    .cast<triton::PointerType>()\n-                    .getPointeeType();\n-      unsigned width = vec * ty.getIntOrFloatBitWidth();\n-      // cp.async's cp-size can only be 4, 8 and 16.\n-      if (width >= 32)\n-        validLoads.push_back(loadOp);\n+      if (!isCandidate)\n+        invalidOps.insert(loadOp);\n+      else\n+        validLoads.insert(loadOp);\n     }\n+  }\n \n-  // Early stop: no need to continue if there is no load in the loop.\n-  if (validLoads.empty())\n-    return failure();\n+  for (Operation *op : invalidOps)\n+    ops.remove(op);\n \n-  // load => values that it depends on\n-  DenseMap<Value, SetVector<Value>> loadDeps;\n-  for (triton::LoadOp loadOp : validLoads) {\n-    SetVector<Value> deps;\n-    for (Value op : loadOp->getOperands())\n-      collectDeps(op, numStages - 1, deps);\n-    loadDeps[loadOp] = deps;\n-  }\n+  if (ops.empty())\n+    return failure();\n+  else\n+    return success();\n+}\n \n-  // Don't pipeline valid loads that depend on other valid loads\n-  // (Because if a valid load depends on another valid load, this load needs to\n-  // wait on the other load in the prologue, which is against the point of the\n-  // pipeline pass)\n-  for (triton::LoadOp loadOp : validLoads) {\n-    bool isCandidate = true;\n-    for (triton::LoadOp other : validLoads) {\n-      if (loadDeps[loadOp].contains(other)) {\n-        isCandidate = false;\n-        break;\n+void LoopPipeliner::checkOpDeps(SetVector<Operation *> &ops) {\n+  SetVector<BlockArgument> nonImmediateDepArgs;\n+  SetVector<Operation *> nonImmediateOps;\n+  for (Operation *op : ops) {\n+    for (Value v : op->getOperands()) {\n+      SetVector<Value> deps;\n+      collectValueDep(v, numStages - 1, deps);\n+      int defStage = getValueDefStage(v, numStages - 1);\n+      assert(defStage >= 0 &&\n+             \"newLoopArgs has null args without a define op. Consider either \"\n+             \"rewrite the loop to reduce cross iteration dependencies or \"\n+             \"increase the num_stages value.\");\n+      for (auto dep : deps) {\n+        auto immediate = deps.front().isa<BlockArgument>();\n+        if (auto arg = dyn_cast<BlockArgument>(dep)) {\n+          depArgs.insert(arg);\n+          if (immediate)\n+            immediateArgStages[arg].insert(defStage);\n+          else\n+            nonImmediateDepArgs.insert(arg);\n+        } else {\n+          depOps.insert(dep.getDefiningOp());\n+          if (immediate)\n+            immediateOpStages[dep.getDefiningOp()].insert(defStage);\n+          else\n+            nonImmediateOps.insert(dep.getDefiningOp());\n+        }\n       }\n     }\n+  }\n \n-    // We only pipeline loads that have one covert_layout (to dot_op) use\n-    // TODO: lift this constraint in the future\n-    if (isCandidate && loadOp.getResult().hasOneUse()) {\n-      isCandidate = false;\n-      Operation *use = *loadOp.getResult().getUsers().begin();\n-\n-      // advance to the first conversion as long\n-      // as the use resides in shared memory and it has\n-      // a single use itself\n-      while (use) {\n-        if (use->getNumResults() != 1 || !use->getResult(0).hasOneUse())\n-          break;\n-        auto tensorType =\n-            use->getResult(0).getType().dyn_cast<RankedTensorType>();\n-        if (!tensorType.getEncoding().isa<ttg::SharedEncodingAttr>())\n-          break;\n-        use = *use->getResult(0).getUsers().begin();\n-      }\n+  // XXX: We could remove the following constraints if we can rematerialize in\n+  // the loop.\n+  // Check if immediateDepArgs and nonImmediateDepArgs are disjoint.\n+  for (auto &[arg, stages] : immediateArgStages) {\n+    assert(stages.size() == 1 &&\n+           \"Triton doesn't support an argument provides values for \"\n+           \"immediate operands of loads from multiple stages. Consider \"\n+           \"removing post load instructions dependency on this argument.\");\n+    assert(!(nonImmediateDepArgs.contains(arg) &&\n+             stages.contains(numStages - 2)) &&\n+           \"Loop-carried arguments provide values for both immediate and \"\n+           \"non-immediate operands of loads. Please consider removing \"\n+           \"pre/post load instructions dependency on this argument.\");\n+  }\n \n-      auto convertLayout = llvm::dyn_cast<ttg::ConvertLayoutOp>(use);\n-      if (!convertLayout)\n-        continue;\n-      auto tensorType =\n-          convertLayout.getResult().getType().dyn_cast<RankedTensorType>();\n-      if (!tensorType)\n-        continue;\n-      auto dotOpEnc =\n-          tensorType.getEncoding().dyn_cast<ttg::DotOperandEncodingAttr>();\n-      if (!dotOpEnc)\n-        continue;\n-      isCandidate = true;\n-      loadsMapping[loadOp] = convertLayout;\n-    }\n+  // Check if immediateOps and nonImmediateOps are disjoint.\n+  for (auto &[op, stages] : immediateOpStages) {\n+    assert(stages.size() == 1 &&\n+           \"Triton doesn't support an operation provides values for \"\n+           \"immediate operands of loads from multiple stages. Consider \"\n+           \"removing post load instructions dependency on this argument.\");\n+    assert(!(nonImmediateOps.contains(op) && stages.contains(numStages - 2)) &&\n+           \"Operations provide values for both immediate and \"\n+           \"non-immediate operands of loads.  Please consider \"\n+           \"removing pre/post load instructions dependency on this \"\n+           \"operation.\");\n+  }\n+}\n \n-    else\n-      isCandidate = false;\n+// helpers\n+void LoopPipeliner::setValueMapping(Value origin, Value newValue, int stage) {\n+  if (valueMapping.find(origin) == valueMapping.end())\n+    valueMapping[origin] = SmallVector<Value>(numStages);\n+  valueMapping[origin][stage] = newValue;\n+}\n \n-    if (isCandidate)\n-      loads.insert(loadOp);\n+void LoopPipeliner::setValueMappingYield(Value origin, Value newValue,\n+                                         int stage) {\n+  for (OpOperand &operand : origin.getUses()) {\n+    if (operand.getOwner() == yieldOp) {\n+      auto yieldIdx = operand.getOperandNumber();\n+      auto value = forOp.getRegionIterArgs()[yieldIdx];\n+      setValueMapping(value, newValue, stage);\n+    }\n   }\n+}\n+\n+void LoopPipeliner::setValueMappingYield(scf::ForOp newForOp, Value origin,\n+                                         Value newValue) {\n+  for (OpOperand &operand : origin.getUses()) {\n+    if (operand.getOwner() == yieldOp) {\n+      auto yieldIdx = operand.getOperandNumber();\n+      auto depYieldIdx = depArgsIdx[forOp.getRegionIterArgs()[yieldIdx]];\n+      auto originArg = forOp.getRegionIterArgs()[yieldIdx];\n+      nextMapping.map(originArg, newValue);\n+      auto newArg = newForOp.getRegionIterArgs()[depYieldIdx];\n+      if (!depArgsMapping.contains(newArg))\n+        depArgsMapping[newArg] = newValue;\n+    }\n+  }\n+}\n+\n+Value LoopPipeliner::lookupOrDefault(Value origin, int stage) {\n+  if (valueMapping.find(origin) == valueMapping.end())\n+    return origin;\n+  return valueMapping[origin][stage];\n+}\n \n-  // we need to find the smallest ocmmon dtype\n-  // since this determines the layout of `mma.sync` operands\n-  // in mixed-precision mode\n+void LoopPipeliner::createBufferTypes() {\n+  // We need to find the smallest common dtype since this determines the layout\n+  // of `mma.sync` operands in mixed-precision mode\n   Type smallestType;\n   for (auto loadCvt : loadsMapping) {\n     auto loadOp = loadCvt.first;\n@@ -316,39 +513,63 @@ LogicalResult LoopPipeliner::initialize() {\n     bufferShape.insert(bufferShape.begin(), numStages);\n     auto sharedEnc = ttg::SharedEncodingAttr::get(\n         ty.getContext(), dotOpEnc, ty.getShape(),\n-        triton::gpu::getOrder(ty.getEncoding()), loadsSmallestType[loadOp]);\n+        ttg::getOrder(ty.getEncoding()), loadsSmallestType[loadOp]);\n     loadsBufferType[loadOp] =\n         RankedTensorType::get(bufferShape, ty.getElementType(), sharedEnc);\n   }\n+}\n \n-  // We have some loads to pipeline\n-  if (!loads.empty()) {\n-    // Update depArgs & depOps\n-    for (Value loadOp : loads) {\n-      auto &deps = loadDeps[loadOp];\n-      for (auto &dep : deps) {\n-        if (auto arg = dep.dyn_cast<BlockArgument>()) {\n-          depArgs.insert(arg);\n-          if (deps.front().isa<BlockArgument>()) {\n-            immedidateDepArgs.insert(arg);\n-          } else {\n-            nonImmedidateDepArgs.insert(arg);\n-          }\n-        } else\n-          depOps.insert(dep.getDefiningOp());\n-      }\n-    }\n-    return success();\n+void LoopPipeliner::createOrderedDeps() {\n+  for (Operation &op : forOp.getLoopBody().front()) {\n+    if (depOps.contains(&op))\n+      orderedDeps.push_back(&op);\n+    else if (op.getNumResults() > 0 && validLoads.contains(op.getResult(0)))\n+      orderedDeps.push_back(&op);\n   }\n+  assert(depOps.size() + validLoads.size() == orderedDeps.size() &&\n+         \"depOps contains invalid values\");\n+}\n \n-  // Check if immedidateDepArgs and nonImmedidateDepArgs are disjoint\n-  // If yes, we cannot pipeline the loop for now\n-  for (BlockArgument arg : immedidateDepArgs)\n-    if (nonImmedidateDepArgs.contains(arg)) {\n-      return failure();\n-    }\n+int LoopPipeliner::getValueDefStage(Value v, int stage) {\n+  if (stage < 0)\n+    return -1;\n+  if (auto arg = v.dyn_cast<BlockArgument>()) {\n+    if (arg.getArgNumber() > 0)\n+      return getValueDefStage(yieldOp->getOperand(arg.getArgNumber() - 1),\n+                              stage - 1);\n+    llvm_unreachable(\"Loop induction variable should not be a dependency\");\n+  } else\n+    return stage;\n+}\n \n-  return failure();\n+ttg::AllocTensorOp LoopPipeliner::allocateEmptyBuffer(triton::LoadOp loadOp,\n+                                                      OpBuilder &builder) {\n+  // Allocate a buffer for each pipelined tensor\n+  // shape: e.g. (numStages==4), <32x64xbf16> -> <4x32x64xbf16>\n+  Value convertLayout = loadsMapping[loadOp];\n+  if (auto tensorType = convertLayout.getType().dyn_cast<RankedTensorType>())\n+    return builder.create<ttg::AllocTensorOp>(convertLayout.getLoc(),\n+                                              loadsBufferType[loadOp]);\n+  llvm_unreachable(\"Async copy's return should be of RankedTensorType\");\n+}\n+\n+LogicalResult LoopPipeliner::initialize() {\n+  // All ops that maybe pipelined\n+  SetVector<Operation *> ops;\n+\n+  if (collectOps(ops).failed())\n+    return failure();\n+\n+  if (checkOpUses(ops).failed())\n+    return failure();\n+\n+  checkOpDeps(ops);\n+\n+  createBufferTypes();\n+\n+  createOrderedDeps();\n+\n+  return success();\n }\n \n Value LoopPipeliner::getLoadMask(triton::LoadOp loadOp, Value mappedMask,\n@@ -375,12 +596,13 @@ Value LoopPipeliner::getLoadMask(triton::LoadOp loadOp, Value mappedMask,\n \n void LoopPipeliner::emitPrologue() {\n   OpBuilder builder(forOp);\n+  // Get init operands for loop carried values\n   for (BlockArgument &arg : forOp.getRegionIterArgs()) {\n     OpOperand &operand = forOp.getOpOperandForRegionIterArg(arg);\n     setValueMapping(arg, operand.get(), 0);\n   }\n \n-  // prologue from [0, numStage-1)\n+  // Emit prologue from [0, numStage-1)\n   Value iv = forOp.getLowerBound();\n   pipelineIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n   for (int stage = 0; stage < numStages - 1; ++stage) {\n@@ -392,38 +614,27 @@ void LoopPipeliner::emitPrologue() {\n     // Special handling for loop condition as there is no condition in ForOp\n     Value loopCond = builder.create<arith::CmpIOp>(\n         iv.getLoc(), arith::CmpIPredicate::slt, iv, forOp.getUpperBound());\n-\n-    // Rematerialize peeled values\n-    SmallVector<Operation *> orderedDeps;\n-    for (Operation &op : forOp.getLoopBody().front()) {\n-      if (depOps.contains(&op))\n-        orderedDeps.push_back(&op);\n-      else if (op.getNumResults() > 0 && loads.contains(op.getResult(0)))\n-        orderedDeps.push_back(&op);\n-    }\n-    assert(depOps.size() + loads.size() == orderedDeps.size() &&\n-           \"depOps contains invalid values\");\n     for (Operation *op : orderedDeps) {\n       Operation *newOp = nullptr;\n-      if (loads.contains(op->getResult(0))) {\n+      if (validLoads.contains(op->getResult(0))) {\n+        auto load = cast<triton::LoadOp>(op);\n         // Allocate empty buffer\n         if (stage == 0) {\n-          loadsBuffer[op->getResult(0)] = allocateEmptyBuffer(op, builder);\n-          loadStageBuffer[op->getResult(0)] = {loadsBuffer[op->getResult(0)]};\n+          loadsBuffer[load] = allocateEmptyBuffer(load, builder);\n+          loadStageBuffer[load] = {loadsBuffer[load]};\n         }\n         // load => copy async\n         if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(op)) {\n           Value newMask =\n               getLoadMask(loadOp, lookupOrDefault(loadOp.getMask(), stage),\n                           loopCond, builder);\n-          // TODO: check if the hardware supports async copy\n-          newOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n+          newOp = builder.create<ttg::InsertSliceAsyncOp>(\n               op->getLoc(), loadsBuffer[loadOp].getType(),\n               lookupOrDefault(loadOp.getPtr(), stage),\n               loadStageBuffer[loadOp][stage], pipelineIterIdx, newMask,\n               lookupOrDefault(loadOp.getOther(), stage), loadOp.getCache(),\n               loadOp.getEvict(), loadOp.getIsVolatile(), /*axis*/ 0);\n-          builder.create<triton::gpu::AsyncCommitGroupOp>(op->getLoc());\n+          builder.create<ttg::AsyncCommitGroupOp>(op->getLoc());\n           loadStageBuffer[loadOp].push_back(newOp->getResult(0));\n         } else\n           llvm_unreachable(\"This should be LoadOp\");\n@@ -439,53 +650,44 @@ void LoopPipeliner::emitPrologue() {\n               loadOp.getBoundaryCheckAttr(), loadOp.getPaddingAttr(),\n               loadOp.getCache(), loadOp.getEvict(), loadOp.getIsVolatile());\n           addNamedAttrs(newOp, op->getDiscardableAttrDictionary());\n-        } else {\n+        } else\n           newOp = builder.clone(*op);\n-        }\n         // Update loop-carried uses\n         for (unsigned opIdx = 0; opIdx < op->getNumOperands(); ++opIdx) {\n           auto it = valueMapping.find(op->getOperand(opIdx));\n           if (it != valueMapping.end()) {\n             Value v = it->second[stage];\n-            assert(v);\n+            assert(v && \"Value not found in valueMapping\");\n             newOp->setOperand(opIdx, v);\n           } // else, op at opIdx is a loop-invariant value\n         }\n       }\n \n-      // Update mapping of results\n-      // if (stage == numStages - 2)\n-      //   continue;\n-\n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n-        Value originalResult = op->getResult(dstIdx);\n-        // copy_async will update the value of its only use\n-        // TODO: load should not be used in the preheader?\n-        if (loads.contains(originalResult)) {\n+        Value originResult = op->getResult(dstIdx);\n+        if (validLoads.contains(originResult))\n           break;\n-          // originalResult = loadsMapping[originalResult];\n-        }\n-        setValueMapping(originalResult, newOp->getResult(dstIdx), stage);\n-        // update mapping for loop-carried values (args)\n-        for (OpOperand &operand : yieldOp->getOpOperands()) {\n-          if (operand.get() == op->getResult(dstIdx))\n-            setValueMapping(\n-                forOp.getRegionIterArgs()[operand.getOperandNumber()],\n-                newOp->getResult(dstIdx), stage + 1);\n-        }\n+        setValueMapping(originResult, newOp->getResult(dstIdx), stage);\n+        // Update mapping for loop-carried values (args)\n+        setValueMappingYield(op->getResult(dstIdx), newOp->getResult(dstIdx),\n+                             stage + 1);\n       }\n     } // for (Operation *op : orderedDeps)\n \n+    // Update pipeline index\n     pipelineIterIdx = builder.create<arith::AddIOp>(\n         iv.getLoc(), pipelineIterIdx,\n         builder.create<arith::ConstantIntOp>(iv.getLoc(), 1, 32));\n+    // Some values have not been used by any ops in the loop body\n+    for (BlockArgument arg : forOp.getRegionIterArgs())\n+      setValueMappingYield(arg, valueMapping[arg][stage], stage + 1);\n   } // for (int stage = 0; stage < numStages - 1; ++stage)\n \n   // async.wait & extract_slice\n-  builder.create<ttg::AsyncWaitOp>(loads[0].getLoc(),\n-                                   loads.size() * (numStages - 2));\n+  builder.create<ttg::AsyncWaitOp>(validLoads.front().getLoc(),\n+                                   validLoads.size() * (numStages - 2));\n   loopIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n-  for (Value loadOp : loads) {\n+  for (Value loadOp : validLoads) {\n     auto bufferType = loadStageBuffer[loadOp][numStages - 1]\n                           .getType()\n                           .cast<RankedTensorType>();\n@@ -494,7 +696,7 @@ void LoopPipeliner::emitPrologue() {\n     sliceType = RankedTensorType::get({bufferShape[1], bufferShape[2]},\n                                       sliceType.getElementType(),\n                                       loadsBufferType[loadOp].getEncoding());\n-    Value extractSlice = builder.create<triton::gpu::ExtractSliceOp>(\n+    Value extractSlice = builder.create<ttg::ExtractSliceOp>(\n         loadOp.getLoc(), sliceType, loadStageBuffer[loadOp][numStages - 1],\n         SmallVector<OpFoldResult>{int_attr(0), int_attr(0), int_attr(0)},\n         SmallVector<OpFoldResult>{int_attr(1),\n@@ -504,7 +706,7 @@ void LoopPipeliner::emitPrologue() {\n     loadsExtract[loadOp] = extractSlice;\n   }\n   // Bump up loopIterIdx, this is used for getting the correct slice for the\n-  // *next* iteration\n+  // `next` iteration\n   loopIterIdx = builder.create<arith::AddIOp>(\n       loopIterIdx.getLoc(), loopIterIdx,\n       builder.create<arith::ConstantIntOp>(loopIterIdx.getLoc(), 1, 32));\n@@ -515,79 +717,77 @@ void LoopPipeliner::emitEpilogue() {\n   OpBuilder builder(forOp);\n   OpBuilder::InsertionGuard g(builder);\n   builder.setInsertionPointAfter(forOp);\n-  builder.create<triton::gpu::AsyncWaitOp>(forOp.getLoc(), 0);\n+  builder.create<ttg::AsyncWaitOp>(forOp.getLoc(), 0);\n }\n \n-scf::ForOp LoopPipeliner::createNewForOp() {\n-  OpBuilder builder(forOp);\n-\n+SmallVector<Value> LoopPipeliner::collectNewLoopArgs() {\n   // Order of new args:\n   //   (original args)\n   //   (insertSliceAsync buffer at stage numStages - 1) for each load\n   //   (extracted tensor) for each load\n-  //   (depArgs at stage numStages - 1):\n-  //   for each dep arg that is not an immediate block argument\n-  //   (depArgs at stage numStages - 2):\n-  //   for each dep arg that is an immediate block argument\n+  //   (depArgs at stage numStages - 1)\n+  //   (depArgs at stage numStages - 2)\n+  //   ...\n   //   (iv at stage numStages - 2)\n   //   (pipeline iteration index)\n   //   (loop iteration index)\n-  SmallVector<Value> newLoopArgs;\n+\n   // We need this to update operands for yield\n   // original block arg => new arg's idx\n-  DenseMap<BlockArgument, size_t> depArgsIdx;\n+  SmallVector<Value> newLoopArgs;\n   for (auto v : forOp.getIterOperands())\n     newLoopArgs.push_back(v);\n \n-  size_t bufferIdx = newLoopArgs.size();\n-  for (Value loadOp : loads)\n+  bufferIdx = newLoopArgs.size();\n+  for (auto loadOp : validLoads)\n     newLoopArgs.push_back(loadStageBuffer[loadOp].back());\n-  size_t loadIdx = newLoopArgs.size();\n-  for (Value loadOp : loads)\n+\n+  loadIdx = newLoopArgs.size();\n+  for (auto loadOp : validLoads)\n     newLoopArgs.push_back(loadsExtract[loadOp]);\n \n-  size_t depArgsBeginIdx = newLoopArgs.size();\n-  for (BlockArgument depArg : depArgs) {\n+  depArgsBeginIdx = newLoopArgs.size();\n+  for (auto depArg : depArgs) {\n     depArgsIdx[depArg] = newLoopArgs.size();\n-    if (immedidateDepArgs.contains(depArg)) {\n+    if (immediateArgStages[depArg].contains(numStages - 2))\n+      // Peel off post load ops in numStage-1\n       newLoopArgs.push_back(valueMapping[depArg][numStages - 2]);\n-    } else\n+    else\n       newLoopArgs.push_back(valueMapping[depArg][numStages - 1]);\n   }\n \n-  size_t nextIVIdx = newLoopArgs.size();\n+  ivIndex = newLoopArgs.size();\n   newLoopArgs.push_back(valueMapping[forOp.getInductionVar()][numStages - 2]);\n   newLoopArgs.push_back(pipelineIterIdx);\n   newLoopArgs.push_back(loopIterIdx);\n+  return newLoopArgs;\n+}\n \n-  for (size_t i = 0; i < newLoopArgs.size(); ++i)\n-    assert(newLoopArgs[i]);\n-\n-  // 1. signature of the new ForOp\n+scf::ForOp LoopPipeliner::cloneForOp(ArrayRef<Value> newLoopArgs,\n+                                     OpBuilder &builder) {\n+  // Clone the original ForOp\n   auto newForOp = builder.create<scf::ForOp>(\n       forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n       forOp.getStep(), newLoopArgs);\n \n-  // 2. body of the new ForOp\n+  // Set mapping on body of the new ForOp\n   builder.setInsertionPointToStart(newForOp.getBody());\n-  IRMapping mapping;\n   for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n     mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n   mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n \n-  // 2. clone the loop body, replace original args with args of the new ForOp\n+  // Clone the loop body, replace original args with args of the new ForOp\n   // Insert async wait if necessary.\n-  DenseSet<Value> isModified;\n   for (Operation &op : forOp.getBody()->without_terminator()) {\n     // is modified\n-    auto it = std::find(loads.begin(), loads.end(), op.getOperand(0));\n-    if (it == loads.end()) {\n+    auto it = std::find(validLoads.begin(), validLoads.end(), op.getOperand(0));\n+    if (it == validLoads.end()) {\n       Operation *newOp = cloneWithInferType(builder, &op, mapping);\n       continue;\n     }\n \n     // we replace the use new load use with a convert layout\n-    size_t i = std::distance(loads.begin(), it);\n+    size_t i = std::distance(validLoads.begin(), it);\n     auto cvtDstTy = op.getResult(0).getType().cast<RankedTensorType>();\n     auto cvtDstEnc =\n         cvtDstTy.getEncoding().dyn_cast<ttg::DotOperandEncodingAttr>();\n@@ -604,53 +804,47 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n         op.getResult(0).getLoc(), newDstTy,\n         newForOp.getRegionIterArgs()[loadIdx + i]);\n     mapping.map(op.getResult(0), cvt.getResult());\n-    isModified.insert(op.getResult(0));\n   }\n \n-  // 3. prefetch the next iteration\n-  SmallVector<Operation *> orderedDeps;\n-  for (Operation &op : forOp.getLoopBody().front()) {\n-    if (depOps.contains(&op))\n-      orderedDeps.push_back(&op);\n-    else if (op.getNumResults() > 0 && loads.contains(op.getResult(0)))\n-      orderedDeps.push_back(&op);\n-  }\n-  assert(depOps.size() + loads.size() == orderedDeps.size() &&\n-         \"depOps contains invalid values\");\n-  IRMapping nextMapping;\n-  DenseMap<BlockArgument, Value> depArgsMapping;\n+  return newForOp;\n+}\n+\n+void LoopPipeliner::prefetchNextIteration(scf::ForOp newForOp,\n+                                          OpBuilder &builder) {\n+  // Map the dep args of the next iteration to the dep args of the current\n   size_t argIdx = 0;\n-  for (BlockArgument arg : depArgs) {\n+  for (auto depArg : depArgs) {\n     BlockArgument nextArg =\n         newForOp.getRegionIterArgs()[argIdx + depArgsBeginIdx];\n-    nextMapping.map(arg, nextArg);\n+    nextMapping.map(depArg, nextArg);\n     ++argIdx;\n   }\n \n   // Special handling for iv & loop condition\n-  Value nextIV = builder.create<arith::AddIOp>(\n-      newForOp.getInductionVar().getLoc(),\n-      newForOp.getRegionIterArgs()[nextIVIdx], newForOp.getStep());\n+  Value curIV = newForOp.getRegionIterArgs()[ivIndex];\n+  nextIV = builder.create<arith::AddIOp>(newForOp.getInductionVar().getLoc(),\n+                                         curIV, newForOp.getStep());\n   Value nextLoopCond =\n       builder.create<arith::CmpIOp>(nextIV.getLoc(), arith::CmpIPredicate::slt,\n                                     nextIV, newForOp.getUpperBound());\n-  nextMapping.map(forOp.getInductionVar(), nextIV);\n \n-  // Slice index\n-  SmallVector<Value> nextBuffers;\n-  SmallVector<Value> extractSlices;\n-\n-  pipelineIterIdx = newForOp.getRegionIterArgs()[nextIVIdx + 1];\n+  pipelineIterIdx = newForOp.getRegionIterArgs()[ivIndex + 1];\n   Value insertSliceIndex = builder.create<arith::RemSIOp>(\n       nextIV.getLoc(), pipelineIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n-  loopIterIdx = newForOp.getRegionIterArgs()[nextIVIdx + 2];\n+  loopIterIdx = newForOp.getRegionIterArgs()[ivIndex + 2];\n   Value extractSliceIndex = builder.create<arith::RemSIOp>(\n       nextIV.getLoc(), loopIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n \n+  // Prefetch load deps\n   for (Operation *op : orderedDeps)\n-    if (!loads.contains(op->getResult(0))) {\n+    if (!validLoads.contains(op->getResult(0))) {\n+      if (immediateOpStages[op].contains(numStages - 2))\n+        // A post load op that provides values for numStage - 2\n+        nextMapping.map(forOp.getInductionVar(), curIV);\n+      else\n+        nextMapping.map(forOp.getInductionVar(), nextIV);\n       Operation *nextOp;\n       if (auto loadOp = dyn_cast<triton::LoadOp>(op)) {\n         auto newMask =\n@@ -664,29 +858,19 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n             loadOp.getCache(), loadOp.getEvict(), loadOp.getIsVolatile());\n         addNamedAttrs(nextOp, op->getDiscardableAttrDictionary());\n         nextMapping.map(loadOp.getResult(), nextOp->getResult(0));\n-      } else {\n+      } else\n         nextOp = builder.clone(*op, nextMapping);\n-      }\n \n-      auto originYield = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n-      for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n-        for (OpOperand &operand : originYield->getOpOperands()) {\n-          if (operand.get() == op->getResult(dstIdx)) {\n-            size_t originIdx = operand.getOperandNumber();\n-            size_t newArgIdx = depArgsIdx[forOp.getRegionIterArgs()[originIdx]];\n-            BlockArgument newArg = newForOp.getRegionIterArgs()[newArgIdx];\n-            nextMapping.map(forOp.getRegionIterArgs()[originIdx],\n-                            nextOp->getResult(dstIdx));\n-            depArgsMapping[newArg] = nextOp->getResult(dstIdx);\n-          }\n-        }\n-      }\n+      for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults()))\n+        setValueMappingYield(newForOp, op->getResult(dstIdx),\n+                             nextOp->getResult(dstIdx));\n     }\n \n+  // loads -> async loads\n   for (Operation *op : orderedDeps) {\n     Operation *nextOp = nullptr;\n     // Update loading mask\n-    if (loads.contains(op->getResult(0))) {\n+    if (validLoads.contains(op->getResult(0))) {\n       auto loadOp = llvm::cast<triton::LoadOp>(op);\n       auto mask = loadOp.getMask();\n       auto newMask =\n@@ -699,24 +883,24 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n           nextMapping.map(loadOp.getMask(), newMask);\n         newMask = nextMapping.lookupOrDefault(mask);\n       }\n-      Value insertAsyncOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n+      Value insertAsyncOp = builder.create<ttg::InsertSliceAsyncOp>(\n           op->getLoc(), loadsBuffer[loadOp].getType(),\n           nextMapping.lookupOrDefault(loadOp.getPtr()),\n           newForOp.getRegionIterArgs()[bufferIdx + nextBuffers.size()],\n           insertSliceIndex, newMask,\n           nextMapping.lookupOrDefault(loadOp.getOther()), loadOp.getCache(),\n           loadOp.getEvict(), loadOp.getIsVolatile(), /*axis*/ 0);\n-      builder.create<triton::gpu::AsyncCommitGroupOp>(op->getLoc());\n+      builder.create<ttg::AsyncCommitGroupOp>(op->getLoc());\n       nextBuffers.push_back(insertAsyncOp);\n-      // ExtractSlice\n+      // Extract slice\n       auto bufferType = insertAsyncOp.getType().cast<RankedTensorType>();\n       auto bufferShape = bufferType.getShape();\n       auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n       sliceType = RankedTensorType::get({bufferShape[1], bufferShape[2]},\n                                         sliceType.getElementType(),\n                                         loadsBufferType[loadOp].getEncoding());\n \n-      nextOp = builder.create<triton::gpu::ExtractSliceOp>(\n+      nextOp = builder.create<ttg::ExtractSliceOp>(\n           op->getLoc(), sliceType, insertAsyncOp,\n           SmallVector<OpFoldResult>{extractSliceIndex, int_attr(0),\n                                     int_attr(0)},\n@@ -727,25 +911,21 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n       extractSlices.push_back(nextOp->getResult(0));\n \n       // Update mapping of results\n-      for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n-        nextMapping.map(op->getResult(dstIdx), nextOp->getResult(dstIdx));\n+      for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults()))\n         // If this is a loop-carried value, update the mapping for yield\n-        auto originYield = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n-        for (OpOperand &operand : originYield->getOpOperands()) {\n-          if (operand.get() == op->getResult(dstIdx)) {\n-            size_t originIdx = operand.getOperandNumber();\n-            size_t newArgIdx = depArgsIdx[forOp.getRegionIterArgs()[originIdx]];\n-            BlockArgument newArg = newForOp.getRegionIterArgs()[newArgIdx];\n-            depArgsMapping[newArg] = nextOp->getResult(dstIdx);\n-          }\n-        }\n-      }\n+        setValueMappingYield(newForOp, op->getResult(dstIdx),\n+                             nextOp->getResult(dstIdx));\n     }\n   }\n \n+  // Some values have not been used by any ops in the loop body\n+  for (BlockArgument arg : forOp.getRegionIterArgs())\n+    setValueMappingYield(newForOp, arg,\n+                         newForOp.getRegionIterArgs()[depArgsIdx[arg]]);\n+\n   // async.wait & extract_slice\n   Operation *asyncWait = builder.create<ttg::AsyncWaitOp>(\n-      loads[0].getLoc(), loads.size() * (numStages - 2));\n+      validLoads[0].getLoc(), validLoads.size() * (numStages - 2));\n   for (auto it = extractSlices.rbegin(); it != extractSlices.rend(); ++it) {\n     // move extract_slice after asyncWait\n     it->getDefiningOp()->moveAfter(asyncWait);\n@@ -758,17 +938,18 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   loopIterIdx = builder.create<arith::AddIOp>(\n       nextIV.getLoc(), loopIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), 1, 32));\n+}\n \n-  // Finally, the YieldOp, need to sync with the order of newLoopArgs\n+void LoopPipeliner::finalizeYield(scf::ForOp newForOp, OpBuilder &builder) {\n   SmallVector<Value> yieldValues;\n-  for (Value v : forOp.getBody()->getTerminator()->getOperands())\n+  for (Value v : yieldOp->getOperands())\n     yieldValues.push_back(mapping.lookup(v));\n   for (Value nextBuffer : nextBuffers)\n     yieldValues.push_back(nextBuffer);\n   for (Value nextSlice : extractSlices)\n     yieldValues.push_back(nextSlice);\n \n-  for (size_t i = depArgsBeginIdx; i < nextIVIdx; ++i) {\n+  for (size_t i = depArgsBeginIdx; i < ivIndex; ++i) {\n     auto arg = newForOp.getRegionIterArgs()[i];\n     assert(depArgsMapping.count(arg) && \"Missing loop-carried value\");\n     yieldValues.push_back(depArgsMapping[arg]);\n@@ -778,8 +959,15 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   yieldValues.push_back(loopIterIdx);\n \n   builder.setInsertionPointToEnd(newForOp.getBody());\n-  builder.create<scf::YieldOp>(forOp.getBody()->getTerminator()->getLoc(),\n-                               yieldValues);\n+  builder.create<scf::YieldOp>(yieldOp->getLoc(), yieldValues);\n+}\n+\n+scf::ForOp LoopPipeliner::createNewForOp() {\n+  OpBuilder builder(forOp);\n+  auto newLoopArgs = collectNewLoopArgs();\n+  auto newForOp = cloneForOp(newLoopArgs, builder);\n+  prefetchNextIteration(newForOp, builder);\n+  finalizeYield(newForOp, builder);\n   return newForOp;\n }\n \n@@ -812,11 +1000,10 @@ struct PipelinePass : public TritonGPUPipelineBase<PipelinePass> {\n         return;\n \n       pipeliner.emitPrologue();\n-\n       scf::ForOp newForOp = pipeliner.createNewForOp();\n       pipeliner.emitEpilogue();\n \n-      // replace the original loop\n+      // Replace the original loop\n       for (unsigned i = 0; i < forOp->getNumResults(); ++i)\n         forOp->getResult(i).replaceAllUsesWith(newForOp->getResult(i));\n       forOp->erase();"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 23, "deletions": 5, "changes": 28, "file_content_changes": "@@ -3,6 +3,8 @@\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/Verifier.h\"\n \n+#include \"mlir/Bytecode/BytecodeWriter.h\"\n+\n #include \"mlir/Conversion/Passes.h\"\n #include \"mlir/Pass/Pass.h\"\n #include \"mlir/Pass/PassManager.h\"\n@@ -348,6 +350,14 @@ void init_triton_ir(py::module &&m) {\n              self.print(os);\n              return str;\n            })\n+      .def(\"bytecode\",\n+           [](mlir::ModuleOp &self) -> py::bytearray {\n+             std::string bytecode;\n+             llvm::raw_string_ostream os(bytecode);\n+             if (failed(mlir::writeBytecodeToFile(self, os)))\n+               throw std::runtime_error(\"Failed to write module bytecode\");\n+             return py::bytearray(bytecode);\n+           })\n       .def(\"push_back\",\n            [](mlir::ModuleOp &self, mlir::triton::FuncOp &funcOp) -> void {\n              self.push_back(funcOp);\n@@ -439,11 +449,15 @@ void init_triton_ir(py::module &&m) {\n              // 1. Unreachable code after return\n              self.walk([&](mlir::Block *block) {\n                mlir::Operation *retOp = nullptr;\n-               block->walk([&](mlir::Operation *op) {\n+               // It's better to not use walk here because we only want to\n+               // check operations in the current block\n+               for (auto &op : block->getOperations()) {\n                  if (mlir::isa<mlir::triton::ReturnOp>(op))\n-                   if (retOp == nullptr)\n-                     retOp = op;\n-               });\n+                   if (retOp == nullptr) {\n+                     retOp = &op;\n+                     break;\n+                   }\n+               }\n                if (retOp && retOp != &block->back()) {\n                  auto pos = retOp->getIterator();\n                  pos++;\n@@ -1311,8 +1325,12 @@ void init_triton_ir(py::module &&m) {\n       .def(\"create_get_program_id\",\n            [](mlir::OpBuilder &self, int axis) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n+             if (axis < 0 || axis > 3)\n+               throw std::runtime_error(\"program_id must be in [0,3]\");\n              return self.create<mlir::triton::GetProgramIdOp>(\n-                 loc, self.getI32Type(), self.getI32IntegerAttr(axis));\n+                 loc, self.getI32Type(),\n+                 mlir::triton::ProgramIDDimAttr::get(\n+                     loc.getContext(), mlir::triton::ProgramIDDim(axis)));\n            })\n       .def(\"create_get_num_programs\",\n            [](mlir::OpBuilder &self, int axis) -> mlir::Value {"}, {"filename": "python/test/regression/test_functional_regressions.py", "status": "modified", "additions": 94, "deletions": 0, "changes": 94, "file_content_changes": "@@ -1,4 +1,5 @@\n import numpy as np\n+import pytest\n import torch\n from numpy.random import RandomState\n \n@@ -134,3 +135,96 @@ def batched_vecmat(\n     C_ref = np.sum(AB, axis=2)\n \n     np.testing.assert_allclose(C_ref, C_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n+\n+\n+@pytest.mark.parametrize(\"type\", [\"pre_load\", \"post_load\", \"post_pre_mixed\", \"post_load_two_iters\", \"post_load_three_iters\"])\n+def test_iv_dependent_matmul(type):\n+    @triton.jit\n+    def kernel(\n+        a_ptr, b_ptr, c_ptr,\n+        M, N, K,\n+        stride_am, stride_ak,\n+        stride_bk, stride_bn,\n+        stride_cm, stride_cn,\n+        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+        type: tl.constexpr\n+    ):\n+        pid = tl.program_id(axis=0)\n+        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+        pid_m = pid // num_pid_n\n+        pid_n = pid % num_pid_n\n+\n+        offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n+        offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n+        offs_k = tl.arange(0, BLOCK_SIZE_K)\n+        a_ptr = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n+        b_ptr = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n+        a_ptrs = a_ptr\n+        b_ptrs = b_ptr\n+        if type == \"post_load_two_iters\":\n+            a_ptrs_next = a_ptr + BLOCK_SIZE_K * stride_ak\n+            b_ptrs_next = b_ptr + BLOCK_SIZE_K * stride_bk\n+        elif type == \"post_load_three_iters\":\n+            a_ptrs_next = a_ptr + BLOCK_SIZE_K * stride_ak\n+            b_ptrs_next = b_ptr + BLOCK_SIZE_K * stride_bk\n+            a_ptrs_next_next = a_ptr + 2 * BLOCK_SIZE_K * stride_ak\n+            b_ptrs_next_next = b_ptr + 2 * BLOCK_SIZE_K * stride_bk\n+\n+        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n+            if type == \"pre_load\":\n+                a_ptrs = a_ptr + k * BLOCK_SIZE_K * stride_ak\n+                b_ptrs = b_ptr + k * BLOCK_SIZE_K * stride_bk\n+            elif type == \"post_pre_mixed\":\n+                a_ptrs = a_ptr + k * BLOCK_SIZE_K * stride_ak\n+            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n+            b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n+            accumulator += tl.dot(a, b)\n+            if type == \"post_load\":\n+                a_ptrs = a_ptr + (k + 1) * BLOCK_SIZE_K * stride_ak\n+                b_ptrs = b_ptr + (k + 1) * BLOCK_SIZE_K * stride_bk\n+            elif type == \"post_pre_mixed\":\n+                b_ptrs = b_ptr + (k + 1) * BLOCK_SIZE_K * stride_bk\n+            elif type == \"post_load_two_iters\":\n+                a_ptrs = a_ptrs_next\n+                b_ptrs = b_ptrs_next\n+                a_ptrs_next = a_ptr + (k + 2) * BLOCK_SIZE_K * stride_ak\n+                b_ptrs_next = b_ptr + (k + 2) * BLOCK_SIZE_K * stride_bk\n+            elif type == \"post_load_three_iters\":\n+                a_ptrs = a_ptrs_next\n+                b_ptrs = b_ptrs_next\n+                a_ptrs_next = a_ptrs_next_next\n+                b_ptrs_next = b_ptrs_next_next\n+                a_ptrs_next_next = a_ptr + (k + 3) * BLOCK_SIZE_K * stride_ak\n+                b_ptrs_next_next = b_ptr + (k + 3) * BLOCK_SIZE_K * stride_bk\n+        c = accumulator.to(tl.float16)\n+\n+        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+        c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n+        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n+        tl.store(c_ptrs, c, mask=c_mask)\n+\n+    M = 256\n+    K = 256\n+    N = 256\n+    BLOCK_SIZE_K = 32\n+    BLOCK_SIZE_N = 32\n+    BLOCK_SIZE_M = 32\n+\n+    a = torch.rand((M, K), device='cuda')\n+    b = torch.rand((K, N), device='cuda')\n+\n+    torch_output = torch.mm(a, b)\n+    triton_output = torch.empty_like(\n+        torch_output, device=torch_output.device)\n+\n+    def grid(META):\n+        return (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n+\n+    num_stages = 4 if type == \"post_load_three_iters\" else 3\n+    kernel[grid](a, b, triton_output, M, N, K, a.stride(0), a.stride(1),\n+                 b.stride(0), b.stride(1), triton_output.stride(0), triton_output.stride(1),\n+                 BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n+                 type=type, num_stages=num_stages)\n+    torch.testing.assert_allclose(torch_output, triton_output, rtol=1e-2, atol=1e-2)"}, {"filename": "python/test/unit/language/assert_helper.py", "status": "modified", "additions": 78, "deletions": 1, "changes": 79, "file_content_changes": "@@ -22,6 +22,13 @@ def kernel_device_assert_scalar(X, Y, BLOCK: tl.constexpr):\n     tl.store(Y + tl.arange(0, BLOCK), x)\n \n \n+@triton.jit(debug=False)\n+def kernel_device_assert_no_debug(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    tl.device_assert(x == 0, \"x != 0\")\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+\n @triton.jit\n def kernel_assert(X, Y, BLOCK: tl.constexpr):\n     x = tl.load(X + tl.arange(0, BLOCK))\n@@ -43,12 +50,82 @@ def test_assert(func: str):\n     if func == \"device_assert\":\n         kernel_device_assert[(1,)](x, y, BLOCK=shape[0])\n         kernel_device_assert_scalar[(1,)](x, y, BLOCK=shape[0])\n+    elif func == \"no_debug\":\n+        # TRITON_DEBUG=True can override the debug flag\n+        kernel_device_assert_no_debug[(1,)](x, y, BLOCK=shape[0])\n     elif func == \"assert\":\n         kernel_assert[(1,)](x, y, BLOCK=shape[0])\n     elif func == \"static_assert\":\n         kernel_static_assert[(1,)](x, y, BLOCK=shape[0])\n     assert_close(y, x)\n \n \n+@triton.jit\n+def jit_device_assert_none(x):\n+    tl.device_assert(x == 0, \"x != 0\")\n+\n+\n+@triton.jit(debug=True)\n+def jit_device_assert_true(x):\n+    tl.device_assert(x == 0, \"x != 0\")\n+\n+\n+@triton.jit(debug=False)\n+def jit_device_assert_false(x):\n+    tl.device_assert(x == 0, \"x != 0\")\n+\n+\n+@triton.jit\n+def kernel_device_assert_nested(X, Y, BLOCK: tl.constexpr, jit_debug: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    if jit_debug == \"true\":\n+        jit_device_assert_true(x)\n+    elif jit_debug == \"false\":\n+        jit_device_assert_false(x)\n+    else:\n+        jit_device_assert_none(x)\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+\n+@triton.jit(debug=True)\n+def kernel_device_assert_nested_true(X, Y, BLOCK: tl.constexpr, jit_debug: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    if jit_debug == \"true\":\n+        jit_device_assert_true(x)\n+    elif jit_debug == \"false\":\n+        jit_device_assert_false(x)\n+    else:\n+        jit_device_assert_none(x)\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+\n+@triton.jit(debug=False)\n+def kernel_device_assert_nested_false(X, Y, BLOCK: tl.constexpr, jit_debug: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    if jit_debug == \"true\":\n+        jit_device_assert_true(x)\n+    elif jit_debug == \"false\":\n+        jit_device_assert_false(x)\n+    else:\n+        jit_device_assert_none(x)\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+\n+def test_assert_nested(caller: str, callee: str):\n+    shape = (128, )\n+    x = torch.arange(0, shape[0], dtype=torch.int32, device='cuda')\n+    y = torch.zeros(shape, dtype=x.dtype, device=\"cuda\")\n+    if caller == \"none\":\n+        kernel_device_assert_nested[(1,)](x, y, BLOCK=shape[0], jit_debug=callee)\n+    elif caller == \"true\":\n+        kernel_device_assert_nested_true[(1,)](x, y, BLOCK=shape[0], jit_debug=callee)\n+    elif caller == \"false\":\n+        kernel_device_assert_nested_false[(1,)](x, y, BLOCK=shape[0], jit_debug=callee)\n+    assert_close(y, x)\n+\n+\n if __name__ == \"__main__\":\n-    test_assert(sys.argv[1])\n+    if len(sys.argv) == 3:\n+        test_assert_nested(sys.argv[1], sys.argv[2])\n+    else:\n+        test_assert(sys.argv[1])"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 38, "deletions": 3, "changes": 41, "file_content_changes": "@@ -467,6 +467,21 @@ def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.con\n     broadcast_kernel[(1,)](x_tri, y_tri, y_broadcasted_tri, M=M, N=N)\n     assert (y_broadcasted_np == to_numpy(y_broadcasted_tri)).all()\n \n+# ------------------\n+# test invalid slice\n+# ------------------\n+\n+\n+def test_invalid_slice():\n+    dst = torch.empty(128, device='cuda')\n+\n+    @triton.jit\n+    def _kernel(dst):\n+        dst[10:]\n+\n+    with pytest.raises(triton.CompilationError, match='unsupported tensor index'):\n+        _kernel[(1,)](dst=dst)\n+\n \n # ----------------\n # test expand_dims\n@@ -548,6 +563,20 @@ def duplicate_dim2(dummy, N: tl.constexpr):\n         duplicate_dim2[(1,)](dummy_tensor, N)\n \n \n+# ----------------------------\n+# test invalid program id axis\n+# ----------------------------\n+def test_invalid_pid_axis():\n+    dst = torch.empty(128, device='cuda')\n+\n+    @triton.jit\n+    def _kernel(dst):\n+        pid = tl.program_id(20)\n+\n+    with pytest.raises(triton.CompilationError, match=r\"program_id must be in \\[0,3\\]\"):\n+        _kernel[(1,)](dst)\n+\n+\n # ---------------\n # test where\n # ---------------\n@@ -1379,6 +1408,9 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n     for op in ['min', 'max', 'sum', 'argmin', 'argmax']\n     for shape in reduce2d_shapes\n     for axis in [0, 1]\n+] + [\n+    (op, 'float32', [16, 32], None)\n+    for op in ['min', 'max', 'sum']\n ]\n \n \n@@ -1393,7 +1425,9 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n         range_n = tl.arange(0, BLOCK_N)\n         x = tl.load(X + range_m[:, None] * BLOCK_N + range_n[None, :])\n         z = GENERATE_TEST_HERE\n-        if AXIS == 1:\n+        if AXIS is None:\n+            tl.store(Z, z)\n+        elif AXIS == 1:\n             tl.store(Z + range_m, z)\n         else:\n             tl.store(Z + range_n, z)\n@@ -1418,7 +1452,8 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n     else:\n         z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n     # triton result\n-    z_tri = to_triton(numpy_random((shape[1 - axis],), dtype_str=z_dtype_str, rs=rs),\n+    ret_numel = 1 if axis is None else shape[1 - axis]\n+    z_tri = to_triton(numpy_random((ret_numel,), dtype_str=z_dtype_str, rs=rs),\n                       device=device, dst_type=z_tri_dtype_str)\n     kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis)\n     z_tri = to_numpy(z_tri)\n@@ -2817,7 +2852,7 @@ def test_globaltimer():\n     def kernel(Out1, Out2):\n         start = tl.extra.cuda.globaltimer()\n         off = tl.arange(0, 128)\n-        for i in range(100):\n+        for i in range(10000):\n             tl.store(Out1 + off, tl.load(Out1 + off) + 1)\n         end = tl.extra.cuda.globaltimer()\n         tl.store(Out2, end - start)"}, {"filename": "python/test/unit/language/test_subprocess.py", "status": "modified", "additions": 28, "deletions": 1, "changes": 29, "file_content_changes": "@@ -9,7 +9,8 @@\n assert_path = os.path.join(dir_path, \"assert_helper.py\")\n \n # TODO: bfloat16 after LLVM-15\n-func_types = [\"device_assert\", \"assert\", \"static_assert\"]\n+func_types = [\"device_assert\", \"assert\", \"static_assert\", \"no_debug\"]\n+nested_types = [(caller, callee) for caller in [\"true\", \"false\", \"none\"] for callee in [\"true\", \"false\", \"none\"]]\n torch_types = [\"int8\", \"uint8\", \"int16\", \"int32\", \"long\", \"float16\", \"float32\", \"float64\"]\n \n \n@@ -51,3 +52,29 @@ def test_assert(func_type: str):\n         assert num_errs == 127\n     else:\n         assert num_errs == 0\n+\n+\n+@pytest.mark.parametrize(\"caller_type, callee_type\", nested_types)\n+def test_assert_nested(caller_type, callee_type):\n+    proc = subprocess.Popen([sys.executable, assert_path, caller_type, callee_type], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=False)\n+    _, errs = proc.communicate()\n+    errs = errs.splitlines()\n+    num_errs = 0\n+    for err in errs:\n+        if \"x != 0\" in err.decode(\"utf-8\"):\n+            num_errs += 1\n+    if caller_type == \"none\":\n+        if callee_type == \"true\":\n+            assert num_errs == 127\n+        else:\n+            assert num_errs == 0\n+    elif caller_type == \"true\":\n+        if callee_type == \"false\":\n+            assert num_errs == 0\n+        else:\n+            assert num_errs == 127\n+    elif caller_type == \"false\":\n+        if callee_type == \"true\":\n+            assert num_errs == 127\n+        else:\n+            assert num_errs == 0"}, {"filename": "python/test/unit/operators/test_flash_attention.py", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -5,7 +5,10 @@\n import triton.ops\n \n \n-@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n+@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 16),\n+                                                 (4, 48, 1024, 32),\n+                                                 (4, 48, 1024, 64),\n+                                                 (4, 48, 1024, 128)])\n @pytest.mark.parametrize('dtype', [torch.float16, torch.bfloat16])\n def test_op(Z, H, N_CTX, D_HEAD, dtype):\n     capability = torch.cuda.get_device_capability()"}, {"filename": "python/test/unit/runtime/test_cache.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -160,12 +160,12 @@ def kernel_add(a, b, o, N: tl.constexpr):\n     assert len(kernel_add.cache[device]) == 1\n     kernel_add.debug = False\n     kernel_add.warmup(torch.float32, torch.float32, torch.float32, 32, grid=(1,))\n-    assert len(kernel_add.cache[device]) == 1\n+    assert len(kernel_add.cache[device]) == 2\n     kernel_add.debug = True\n     kernel_add.warmup(torch.float32, torch.float32, torch.float32, 32, grid=(1,))\n-    assert len(kernel_add.cache[device]) == 2\n+    assert len(kernel_add.cache[device]) == 3\n     bins = list(kernel_add.cache[device].values())\n-    assert bins[0].asm['ttir'] != bins[1].asm['ttir']\n+    assert bins[2].asm['ttir'] != bins[1].asm['ttir']\n \n \n @triton.jit"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 10, "deletions": 3, "changes": 13, "file_content_changes": "@@ -388,7 +388,8 @@ def visit_Assign(self, node):\n         for name, value in zip(names, values):\n             # by default, constexpr are assigned into python variable\n             value = _unwrap_if_constexpr(value)\n-            if not _is_triton_tensor(value) and \\\n+            if value is not None and \\\n+               not _is_triton_tensor(value) and \\\n                not isinstance(value, native_nontensor_types):\n                 value = language.core._to_tensor(value, self.builder)\n             self.set_value(name, value)\n@@ -560,7 +561,11 @@ def visit_If(self, node):\n         cond = self.visit(node.test)\n         if _is_triton_tensor(cond):\n             cond = cond.to(language.int1, _builder=self.builder)\n-            if self.scf_stack or not ContainsReturnChecker(self.gscope).visit(node):\n+            contains_return = ContainsReturnChecker(self.gscope).visit(node)\n+            if self.scf_stack and contains_return:\n+                raise UnsupportedLanguageConstruct(None, node,\n+                                                   \"Cannot have `return` statements inside `while` or `for` statements in triton\")\n+            elif self.scf_stack or not contains_return:\n                 self.visit_if_scf(cond, node)\n             else:\n                 self.visit_if_top_level(cond, node)\n@@ -859,7 +864,9 @@ def call_JitFunction(self, fn: JITFunction, args, kwargs):\n         if not self.module.has_function(fn_name):\n             prototype = language.function_type([], arg_types)\n             gscope = sys.modules[fn.fn.__module__].__dict__\n-            generator = CodeGenerator(self.builder.context, prototype, gscope, attributes, constants, module=self.module, function_name=fn_name, function_types=self.function_ret_types, debug=fn.debug, noinline=fn.noinline)\n+            # If the callee is not set, we use the same debug setting as the caller\n+            debug = self.debug if fn.debug is None else fn.debug\n+            generator = CodeGenerator(self.builder.context, prototype, gscope, attributes, constants, module=self.module, function_name=fn_name, function_types=self.function_ret_types, debug=debug, noinline=fn.noinline)\n             generator.visit(fn.parse())\n             callee_ret_type = generator.last_ret_type\n             self.function_ret_types[fn_name] = callee_ret_type"}, {"filename": "python/triton/debugger/memory_map.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n import dataclasses\n \n from triton.debugger import torch_wrapper"}, {"filename": "python/triton/debugger/tl_lang.py", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n import triton\n from .core import ExecutionContext\n from .memory_map import MemoryMap\n@@ -405,7 +407,9 @@ def zeros(self, shape, dtype):\n         return torch.zeros(size=shape, dtype=dtype, device=\"cuda\")\n \n     @_tensor_operation\n-    def dequantize(self, input, scale, shift, nbit, dst_ty=torch.float16):\n+    def dequantize(self, input, scale, shift, nbit, dst_ty=None):\n+        if dst_ty is None:\n+            dst_ty = torch.float16\n         raise NotImplementedError()\n \n     @_tensor_operation"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -711,7 +711,7 @@ def __getitem__(self, slices, _builder=None):\n         for dim, sl in enumerate(slices):\n             if isinstance(sl, constexpr) and sl.value is None:\n                 ret = semantic.expand_dims(ret, dim, _builder)\n-            elif sl == slice(None, None, None):\n+            elif isinstance(sl, slice) and sl.start is None and sl.stop is None and sl.step is None:\n                 pass\n             else:\n                 assert False, f\"unsupported tensor index: {sl}\"\n@@ -1297,8 +1297,8 @@ def make_combine_region(reduce_op):\n             else:\n                 handles = [r.handle for r in results]\n             _builder.create_reduce_ret(*handles)\n-\n-    axis = _constexpr_to_value(axis)\n+    if axis is not None:\n+        axis = _constexpr_to_value(axis)\n     return semantic.reduction(input, axis, make_combine_region, _builder)\n \n \n@@ -1369,7 +1369,7 @@ def _max_combine(a, b):\n \n @triton.jit\n @_add_reduction_docstr(\"maximum\")\n-def max(input, axis):\n+def max(input, axis=None):\n     input = _promote_reduction_input(input)\n     return reduce(input, axis, _max_combine)\n \n@@ -1399,7 +1399,7 @@ def _min_combine(a, b):\n \n @triton.jit\n @_add_reduction_docstr(\"minimum\")\n-def min(input, axis):\n+def min(input, axis=None):\n     input = _promote_reduction_input(input)\n     return reduce(input, axis, _min_combine)\n \n@@ -1428,7 +1428,7 @@ def _sum_combine(a, b):\n \n @triton.jit\n @_add_reduction_docstr(\"sum\")\n-def sum(input, axis):\n+def sum(input, axis=None):\n     input = _promote_reduction_input(input)\n     return reduce(input, axis, _sum_combine)\n \n@@ -1440,7 +1440,7 @@ def _xor_combine(a, b):\n \n @builtin\n @_add_reduction_docstr(\"xor sum\")\n-def xor_sum(input, axis, _builder=None, _generator=None):\n+def xor_sum(input, axis=None, _builder=None, _generator=None):\n     scalar_ty = input.type.scalar\n     if not scalar_ty.is_int():\n         raise ValueError(\"xor_sum only supported for integers\")"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -1236,6 +1236,13 @@ def where(condition: tl.tensor,\n def reduction(\n     inputs: Sequence[tl.tensor], axis: int, region_builder_fn, builder: ir.builder\n ) -> Tuple[tl.tensor, ...]:\n+    if axis is None:\n+        new_inputs = []\n+        for i in range(len(inputs)):\n+            new_shape = [inputs[i].numel.value]\n+            new_inputs.append(view(inputs[i], new_shape, builder))\n+        inputs = tuple(new_inputs)\n+        axis = 0\n     # get result shape\n     shape = inputs[0].type.shape\n     ret_shape = [s for i, s in enumerate(shape) if i != axis]"}, {"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -203,8 +203,7 @@ def forward(ctx, q, k, v, sm_scale):\n         # shape constraints\n         Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n         assert Lq == Lk and Lk == Lv\n-        # assert Lk in {16, 32, 64, 128}\n-        assert Lk in {64}  # TODO: fix other cases\n+        assert Lk in {16, 32, 64, 128}\n         o = torch.empty_like(q)\n         grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -356,7 +356,7 @@ def __init__(self, fn, version=None, do_not_specialize=None, debug=None, noinlin\n         # when called with a grid using __getitem__\n         self.kernel_decorators = []\n         self.kernel = None\n-        self.debug = os.environ.get(\"TRITON_DEBUG\", \"0\") == \"1\" if debug is None else debug\n+        self.debug = True if os.environ.get(\"TRITON_DEBUG\", \"0\") == \"1\" else debug\n         self.noinline = noinline\n         # annotations\n         normalize_ty = lambda ty: ty.__name__ if isinstance(ty, type) else ty"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 12, "deletions": 8, "changes": 20, "file_content_changes": "@@ -40,29 +40,33 @@ def do_bench(fn, warmup=25, rep=100, grad_to_none=None,\n     :type fast_flush: bool\n     \"\"\"\n \n-    # Estimate the runtime of the function\n     fn()\n     torch.cuda.synchronize()\n+\n+    # We maintain a buffer of 256 MB that we clear\n+    # before each kernel call to make sure that the L2\n+    # doesn't contain any input data before the run\n+    if fast_flush:\n+        cache = torch.empty(int(256e6 // 4), dtype=torch.int, device='cuda')\n+    else:\n+        cache = torch.empty(int(256e6), dtype=torch.int8, device='cuda')\n+\n+    # Estimate the runtime of the function\n     start_event = torch.cuda.Event(enable_timing=True)\n     end_event = torch.cuda.Event(enable_timing=True)\n     start_event.record()\n     for _ in range(5):\n+        cache.zero_()\n         fn()\n     end_event.record()\n     torch.cuda.synchronize()\n     estimate_ms = start_event.elapsed_time(end_event) / 5\n+\n     # compute number of warmup and repeat\n     n_warmup = max(1, int(warmup / estimate_ms))\n     n_repeat = max(1, int(rep / estimate_ms))\n-    # We maintain a buffer of 256 MB that we clear\n-    # before each kernel call to make sure that the L2\n-    # doesn't contain any input data before the run\n     start_event = [torch.cuda.Event(enable_timing=True) for i in range(n_repeat)]\n     end_event = [torch.cuda.Event(enable_timing=True) for i in range(n_repeat)]\n-    if fast_flush:\n-        cache = torch.empty(int(256e6 // 4), dtype=torch.int, device='cuda')\n-    else:\n-        cache = torch.empty(int(256e6), dtype=torch.int8, device='cuda')\n     # Warm-up\n     for _ in range(n_warmup):\n         fn()"}, {"filename": "test/Analysis/test-alignment.mlir", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -406,7 +406,7 @@ tt.func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32\n // CHECK-LABEL: @store_constant_align\n tt.func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n: i32 {tt.divisibility = 16 : i32}) {\n   // CHECK: contiguity = [1], divisibility = [1], constancy = [1], constant_value = <none>\n-  %pid = tt.get_program_id {axis = 0 : i32} : i32\n+  %pid = tt.get_program_id x : i32\n   // CHECK-NEXT: contiguity = [1], divisibility = [128], constancy = [1], constant_value = 128\n   %c128_i32 = arith.constant 128 : i32\n   // CHECK-NEXT: contiguity = [1], divisibility = [128], constancy = [1], constant_value = <none>\n@@ -438,7 +438,7 @@ tt.func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n // CHECK-LABEL: @vecadd_mask_align_16\n tt.func @vecadd_mask_align_16(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32 {tt.divisibility = 16 : i32}) {\n   %c64_i32 = arith.constant 64 : i32\n-  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %0 = tt.get_program_id x : i32\n   %1 = arith.muli %0, %c64_i32 : i32\n   %2 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n   %3 = tt.splat %1 : (i32) -> tensor<64xi32>\n@@ -467,7 +467,7 @@ tt.func @vecadd_mask_align_16(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n // CHECK-LABEL: @vecadd_mask_align_1\n tt.func @vecadd_mask_align_1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32) {\n   %c64_i32 = arith.constant 64 : i32\n-  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %0 = tt.get_program_id x : i32\n   %1 = arith.muli %0, %c64_i32 : i32\n   %2 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n   %3 = tt.splat %1 : (i32) -> tensor<64xi32>"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "@@ -86,7 +86,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n   // CHECK-LABEL: global_load_store_no_vec\n   tt.func @global_load_store_no_vec(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n     %c256_i32 = arith.constant 256 : i32\n-    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %0 = tt.get_program_id x : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n     %2 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n     %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n@@ -138,7 +138,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n   // CHECK-LABEL: global_load_store_vec4\n   tt.func @global_load_store_vec4(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n     %c256_i32 = arith.constant 256 : i32\n-    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %0 = tt.get_program_id x : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n     %2 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n     %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n@@ -175,7 +175,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n   tt.func @vecadd_masked_vec1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32) {\n     %c64_i32 = arith.constant 64 : i32\n-    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %0 = tt.get_program_id x : i32\n     %1 = arith.muli %0, %c64_i32 : i32\n     %2 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked>\n     %3 = tt.splat %1 : (i32) -> tensor<64xi32, #blocked>\n@@ -205,7 +205,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: global_load_store_vec2\n     tt.func @global_load_store_vec2(%arg0: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg3: i32) {\n     %c256_i32 = arith.constant 256 : i32\n-    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %0 = tt.get_program_id x : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n     %2 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n     %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n@@ -250,7 +250,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: global_load_store_vec8\n     tt.func @global_load_store_vec8(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n     %c256_i32 = arith.constant 256 : i32\n-    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %0 = tt.get_program_id x : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n     %2 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n     %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n@@ -357,7 +357,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_program_id\n   tt.func @basic_program_id() {\n     // CHECK: nvvm.read.ptx.sreg.ctaid.x : i32\n-    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %0 = tt.get_program_id x : i32\n     tt.return\n   }\n }\n@@ -1066,9 +1066,9 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n tt.func @test_get_program_id(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n-  %blockidx = tt.get_program_id {axis=0:i32} : i32\n-  %blockidy = tt.get_program_id {axis=1:i32} : i32\n-  %blockidz = tt.get_program_id {axis=2:i32} : i32\n+  %blockidx = tt.get_program_id x : i32\n+  %blockidy = tt.get_program_id y : i32\n+  %blockidz = tt.get_program_id z : i32\n   // CHECK: nvvm.read.ptx.sreg.ctaid.x\n   // CHECK: nvvm.read.ptx.sreg.ctaid.y\n   // CHECK: nvvm.read.ptx.sreg.ctaid.z"}, {"filename": "test/Triton/rewrite-tensor-pointer.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -10,8 +10,8 @@ tt.func public @matmul_kernel(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32},\n   %c32_i32 = arith.constant 32 : i32\n   %c128_i32 = arith.constant 128 : i32\n   %c8_i32 = arith.constant 8 : i32\n-  %0 = tt.get_program_id {axis = 0 : i32} : i32\n-  %1 = tt.get_program_id {axis = 1 : i32} : i32\n+  %0 = tt.get_program_id x : i32\n+  %1 = tt.get_program_id y : i32\n   %2 = arith.addi %arg3, %c127_i32 : i32\n   %3 = arith.divsi %2, %c128_i32 : i32\n   %4 = arith.addi %arg4, %c31_i32 : i32"}, {"filename": "test/Triton/vecadd.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -2,7 +2,7 @@\n \n module {\n   tt.func @add_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32__(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32) {\n-    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %0 = tt.get_program_id x : i32\n     %c256_i32 = arith.constant 256 : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n     %2 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32>\n@@ -49,7 +49,7 @@ module {\n //     %c0 = arith.constant 0 : index\n //     %cst = arith.constant 0.000000e+00 : f32\n //     %c256_i32 = arith.constant 256 : i32\n-//     %0 = tt.get_program_id {axis = 0 : i32} : i32\n+//     %0 = tt.get_program_id x : i32\n //     %1 = arith.muli %0, %c256_i32 : i32\n //     %2 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %3 = tt.broadcast %1 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 11, "deletions": 11, "changes": 22, "file_content_changes": "@@ -86,7 +86,7 @@ tt.func @remat_fast_load(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n tt.func @if(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout1>\n-  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %0 = tt.get_program_id x : i32\n   %1 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout1>\n   %2 = arith.muli %1, %c32_i32 : tensor<1024xi32, #layout1>\n   %3 = arith.addi %2, %c32_i32 : tensor<1024xi32, #layout1>\n@@ -102,7 +102,7 @@ tt.func @if(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n // CHECK-LABEL: if_convert_else_not\n tt.func @if_convert_else_not(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout0>\n-  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %0 = tt.get_program_id x : i32\n   %1 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout0>\n   %9 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout1>\n   %2 = arith.muli %1, %c32_i32 : tensor<1024xi32, #layout0>\n@@ -123,7 +123,7 @@ tt.func @if_convert_else_not(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility =\n // CHECK-LABEL: if_not_else_convert\n tt.func @if_not_else_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout0>\n-  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %0 = tt.get_program_id x : i32\n   %1 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout0>\n   %9 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout1>\n   %2 = arith.muli %1, %c32_i32 : tensor<1024xi32, #layout0>\n@@ -144,7 +144,7 @@ tt.func @if_not_else_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility =\n // CHECK-LABEL: if_else_both_convert\n tt.func @if_else_both_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout0>\n-  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %0 = tt.get_program_id x : i32\n   %1 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout0>\n   %2 = arith.muli %1, %c32_i32 : tensor<1024xi32, #layout0>\n   %3 = arith.addi %2, %c32_i32 : tensor<1024xi32, #layout0>\n@@ -323,7 +323,7 @@ tt.func @loop_if(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i3\n tt.func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %c256_i32 = arith.constant 256 : i32\n-  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %0 = tt.get_program_id x : i32\n   %1 = arith.muli %0, %c256_i32 : i32\n   %2 = tt.splat %1 : (i32) -> tensor<256xi32, #layout1>\n   %3 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #layout1>\n@@ -361,7 +361,7 @@ tt.func @select(%arg0: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr\n   %c0 = arith.constant 0 : index\n   %cst_1 = arith.constant dense<2048> : tensor<1x1xi32, #blocked2>\n   %cst_2 = arith.constant dense<0.000000e+00> : tensor<1x512xf64, #blocked2>\n-  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %0 = tt.get_program_id x : i32\n   %1 = tt.make_range {end = 1 : i32, start = 0 : i32} : tensor<1xi32, #blocked0>\n   %2 = triton_gpu.convert_layout %1 : (tensor<1xi32, #blocked0>) -> tensor<1xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n   %3 = tt.expand_dims %2 {axis = 1 : i32} : (tensor<1xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<1x1xi32, #blocked1>\n@@ -422,7 +422,7 @@ tt.func public @long_func(%arg0: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg\n   %cst_12 = arith.constant dense<1> : tensor<1024xi32, #blocked0>\n   %cst_13 = arith.constant dense<0.000000e+00> : tensor<1024xf32, #blocked0>\n   %cst_14 = arith.constant dense<0> : tensor<1024xi32, #blocked0>\n-  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %0 = tt.get_program_id x : i32\n   %1 = arith.muli %0, %c1024_i32 : i32\n   %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #blocked0>\n   %3 = tt.splat %1 : (i32) -> tensor<1024xi32, #blocked0>\n@@ -809,7 +809,7 @@ tt.func public @mnist(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !\n   %cst_2 = arith.constant dense<0xFF800000> : tensor<16x16xf32, #blocked2>\n   %cst_3 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #blocked2>\n   %cst_4 = arith.constant dense<0> : tensor<16x16xi32, #blocked2>\n-  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %0 = tt.get_program_id x : i32\n   %1 = arith.muli %0, %c16_i32 : i32\n   %2 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked0>\n   %3 = triton_gpu.convert_layout %2 : (tensor<16xi32, #blocked0>) -> tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n@@ -908,7 +908,7 @@ tt.func public @cmp(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt\n   %cst_4 = arith.constant dense<2048> : tensor<64x1xi32, #blocked2>\n   %cst_5 = arith.constant dense<49152> : tensor<64x1xi32, #blocked2>\n   %cst_6 = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked2>\n-  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %0 = tt.get_program_id x : i32\n   %1 = arith.muli %0, %c64_i32 : i32\n   %2 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked0>\n   %3 = triton_gpu.convert_layout %2 : (tensor<64xi32, #blocked0>) -> tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n@@ -1044,7 +1044,7 @@ tt.func public @if_no_tensor(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %\n   %c-1_i64 = arith.constant -1 : i64\n   %cst = arith.constant 0.000000e+00 : f32\n   %c-1_i32 = arith.constant -1 : i32\n-  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %0 = tt.get_program_id x : i32\n   %1 = tt.addptr %arg3, %0 : !tt.ptr<i64>, i32\n   %2 = tt.load %1 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : i64\n   %3 = arith.cmpi eq, %2, %c-1_i64 : i64\n@@ -1127,7 +1127,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %cst_3 = arith.constant dense<196> : tensor<1x256xi32, #blocked>\n     %cst_4 = arith.constant dense<3136> : tensor<1x256xi32, #blocked>\n     %cst_5 = arith.constant dense<256> : tensor<1x1xi32, #blocked>\n-    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %0 = tt.get_program_id x : i32\n     %1 = tt.make_range {end = 1 : i32, start = 0 : i32} : tensor<1xi32, #blocked1>\n     %2 = triton_gpu.convert_layout %1 : (tensor<1xi32, #blocked1>) -> tensor<1xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n     %3 = tt.expand_dims %2 {axis = 1 : i32} : (tensor<1xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<1x1xi32, #blocked2>"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 119, "deletions": 0, "changes": 119, "file_content_changes": "@@ -313,3 +313,122 @@ tt.func @lut_bmm_vector(%77: tensor<16x16xi64, #BL> {tt.divisibility=16: i32, tt\n   }\n   tt.return %79#0 : tensor<16x16xf32, #C>\n }\n+\n+// CHECK: tt.func @post_load_inv\n+// CHECK: scf.for\n+// CHECK: arith.index_cast\n+// CHECK-DAG: %[[IV:.*]] = arith.index_cast\n+// CHECK: %[[NEXT_IV:.*]] = arith.addi %[[IV]], %c1_i32 : i32\n+// CHECK-NOT: arith.addi %[[NEXT_IV]]\n+tt.func @post_load_inv(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                       %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                       %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                       %arg3: i32 {tt.divisibility = 16 : i32},\n+                       %arg4: i32 {tt.divisibility = 16 : i32},\n+                       %arg5: i32 {tt.divisibility = 16 : i32},\n+                       %arg6: i32 {tt.divisibility = 16 : i32},\n+                       %arg7: i32 {tt.divisibility = 16 : i32},\n+                       %arg8: i32 {tt.divisibility = 16 : i32}) -> tensor<32x32xf32, #C> {\n+  %c0_index = arith.constant 0 : index\n+  %c1_index = arith.constant 1 : index\n+  %c1_i32 = arith.constant 1 : i32\n+  %c32_i32 = arith.constant 32 : i32\n+  %84 = arith.constant 900 : index\n+  %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #C>\n+  %cst_0 = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #AL>\n+  %50 = tt.splat %arg3 : (i32) -> tensor<1x32xi32, #AL>\n+  %59 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %81 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %66 = tt.splat %arg4 : (i32) -> tensor<32x1xi32, #AL>\n+  %60 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %82 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %85:3 = scf.for %arg9 = %c0_index to %84 step %c1_index iter_args(%arg10 = %cst, %arg11 = %59, %arg12 = %81) -> (tensor<32x32xf32, #C>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>)  {\n+    %130 = arith.index_cast %arg9 : index to i32\n+    %107 = arith.muli %130, %c32_i32 : i32\n+    %108 = arith.subi %arg5, %107 : i32\n+    %109 = tt.splat %108 : (i32) -> tensor<1x32xi32, #AL>\n+    %110 = \"triton_gpu.cmpi\"(%50, %109) <{predicate = 2 : i64}> : (tensor<1x32xi32, #AL>, tensor<1x32xi32, #AL>) -> tensor<1x32xi1, #AL>\n+    %111 = tt.broadcast %110 : (tensor<1x32xi1, #AL>) -> tensor<32x32xi1, #AL>\n+    %112 = tt.load %arg11, %111, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #AL>\n+    %113 = tt.splat %108 : (i32) -> tensor<32x1xi32, #AL>\n+    %114 = \"triton_gpu.cmpi\"(%66, %113) <{predicate = 2 : i64}> : (tensor<32x1xi32, #AL>, tensor<32x1xi32, #AL>) -> tensor<32x1xi1, #AL>\n+    %115 = tt.broadcast %114 : (tensor<32x1xi1, #AL>) -> tensor<32x32xi1, #AL>\n+    %116 = tt.load %arg12, %115, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #AL>\n+    %117 = triton_gpu.convert_layout %112 : (tensor<32x32xf32, #AL>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth = 1}>>\n+    %118 = triton_gpu.convert_layout %116 : (tensor<32x32xf32, #AL>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth = 1}>>\n+    %119 = tt.dot %117, %118, %arg10 {allowTF32 = true} : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth = 1}>> * tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth = 1}>> -> tensor<32x32xf32, #C>\n+    %131 = arith.index_cast %arg9 : index to i32\n+    %120 = arith.addi %131, %c1_i32 : i32\n+    %121 = arith.muli %120, %c32_i32 : i32\n+    %122 = tt.splat %121 : (i32) -> tensor<32x32xi32, #AL>\n+    %123 = tt.addptr %60, %122 : tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32xi32, #AL>\n+    %124 = arith.muli %121, %arg7 : i32\n+    %125 = tt.splat %124 : (i32) -> tensor<32x32xi32, #AL>\n+    %126 = tt.addptr %82, %125 : tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32xi32, #AL>\n+    scf.yield %119, %123, %126 : tensor<32x32xf32, #C>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>\n+  }\n+  tt.return %85#0 : tensor<32x32xf32, #C>\n+}\n+\n+// CHECK: tt.func @cross_iter_dep\n+// CHECK: triton_gpu.async_commit_group\n+// CHECK: triton_gpu.async_commit_group\n+// CHECK: triton_gpu.async_commit_group\n+// CHECK: triton_gpu.async_commit_group\n+// CHECK: %[[PTR0:.*]] = tt.addptr\n+// CHECK: %[[PTR1:.*]] = tt.addptr\n+// CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[BUF0:.*]] = %[[PTR0]], {{.*}}, %[[BUF1:.*]] = %[[PTR1]]\n+// CHECK: scf.yield\n+// CHECK-SAME: %[[BUF0]]\n+// CHECK-SAME: %[[BUF1]]\n+tt.func @cross_iter_dep(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                        %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                        %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n+                        %arg3: i32 {tt.divisibility = 16 : i32},\n+                        %arg4: i32 {tt.divisibility = 16 : i32},\n+                        %arg5: i32 {tt.divisibility = 16 : i32},\n+                        %arg6: i32 {tt.divisibility = 16 : i32},\n+                        %arg7: i32 {tt.divisibility = 16 : i32},\n+                        %arg8: i32 {tt.divisibility = 16 : i32}) -> tensor<32x32xf32, #C> {\n+  %c0_i32 = arith.constant 0 : index\n+  %118 = arith.constant 32 : index\n+  %c1_i32 = arith.constant 1 : index\n+  %c2_i32 = arith.constant 2 : i32\n+  %c32_i32 = arith.constant 32 : i32\n+  %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #C>\n+  %cst_1 = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #AL>\n+  %78 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %110 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %112 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %113 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %116 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %65 = tt.splat %arg3 : (i32) -> tensor<1x32xi32, #AL>\n+  %88 = tt.splat %arg4 : (i32) -> tensor<32x1xi32, #AL>\n+  %80 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+  %119:5 = scf.for %arg9 = %c0_i32 to %118 step %c1_i32 iter_args(%arg10 = %cst, %arg11 = %78, %arg12 = %110, %arg13 = %113, %arg14 = %116) -> (tensor<32x32xf32, #C>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>)  {\n+    %161 = arith.index_cast %arg9 : index to i32\n+    %141 = arith.muli %161, %c32_i32 : i32\n+    %142 = arith.subi %arg5, %141 : i32\n+    %143 = tt.splat %142 : (i32) -> tensor<1x32xi32, #AL>\n+    %144 = \"triton_gpu.cmpi\"(%65, %143) <{predicate = 2 : i64}> : (tensor<1x32xi32, #AL>, tensor<1x32xi32, #AL>) -> tensor<1x32xi1, #AL>\n+    %145 = tt.broadcast %144 : (tensor<1x32xi1, #AL>) -> tensor<32x32xi1, #AL>\n+    %146 = tt.load %arg11, %145, %cst_1 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #AL>\n+    %147 = tt.splat %142 : (i32) -> tensor<32x1xi32, #AL>\n+    %148 = \"triton_gpu.cmpi\"(%88, %147) <{predicate = 2 : i64}> : (tensor<32x1xi32, #AL>, tensor<32x1xi32, #AL>) -> tensor<32x1xi1, #AL>\n+    %149 = tt.broadcast %148 : (tensor<32x1xi1, #AL>) -> tensor<32x32xi1, #AL>\n+    %150 = tt.load %arg12, %149, %cst_1 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #AL>\n+    %151 = triton_gpu.convert_layout %146 : (tensor<32x32xf32, #AL>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth = 1}>>\n+    %152 = triton_gpu.convert_layout %150 : (tensor<32x32xf32, #AL>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth = 1}>>\n+    %153 = tt.dot %151, %152, %arg10 {allowTF32 = true} : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth = 1}>> * tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth = 1}>> -> tensor<32x32xf32, #C>\n+    %162 = arith.index_cast %arg9 : index to i32\n+    %154 = arith.addi %162, %c2_i32 : i32\n+    %155 = arith.muli %154, %c32_i32 : i32\n+    %156 = tt.splat %155 : (i32) -> tensor<32x32xi32, #AL>\n+    %157 = tt.addptr %80, %156 : tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32xi32, #AL>\n+    %158 = arith.muli %155, %arg7 : i32\n+    %159 = tt.splat %158 : (i32) -> tensor<32x32xi32, #AL>\n+    %160 = tt.addptr %112, %159 : tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32xi32, #AL>\n+    scf.yield %153, %arg13, %arg14, %157, %160 : tensor<32x32xf32, #C>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32x!tt.ptr<f32>, #AL>\n+  }\n+  tt.return %119#0 : tensor<32x32xf32, #C>\n+}"}, {"filename": "test/TritonGPU/matmul.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -12,7 +12,7 @@ tt.func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32__1\n     %c64_i32 = arith.constant 64 : i32\n     %c63_i32 = arith.constant 63 : i32\n     %c8_i32 = arith.constant 8 : i32\n-    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %0 = tt.get_program_id x : i32\n     %1 = arith.addi %arg3, %c63_i32 : i32\n     %2 = arith.divsi %1, %c64_i32 : i32\n     %3 = arith.addi %arg4, %c63_i32 : i32"}]