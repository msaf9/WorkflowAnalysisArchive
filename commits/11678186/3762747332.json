[{"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 39, "deletions": 0, "changes": 39, "file_content_changes": "@@ -4,11 +4,13 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n+using ::mlir::LLVM::DotOpFMAConversionHelper;\n using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n using ::mlir::LLVM::DotOpMmaV2ConversionHelper;\n using ::mlir::LLVM::getElementsFromStruct;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n using ::mlir::LLVM::getStructFromElements;\n+using ::mlir::LLVM::MMA16816ConversionHelper;\n using ::mlir::triton::gpu::getElemsPerThread;\n \n struct SplatOpConversion\n@@ -38,6 +40,11 @@ struct SplatOpConversion\n           LLVM::LLVMStructType::getLiteral(rewriter.getContext(), elemTypes);\n \n       return getStructFromElements(loc, elems, rewriter, structTy);\n+    } else if (auto dotLayout =\n+                   tensorTy.getEncoding()\n+                       .dyn_cast<triton::gpu::DotOperandEncodingAttr>()) {\n+      return convertSplatLikeOpWithDotOperandLayout(\n+          dotLayout, resType, elemType, constVal, typeConverter, rewriter, loc);\n     } else if (auto mmaLayout =\n                    tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>()) {\n       return convertSplatLikeOpWithMmaLayout(\n@@ -48,6 +55,38 @@ struct SplatOpConversion\n     return {};\n   }\n \n+  static Value convertSplatLikeOpWithDotOperandLayout(\n+      const triton::gpu::DotOperandEncodingAttr &layout, Type resType,\n+      Type elemType, Value constVal, TypeConverter *typeConverter,\n+      ConversionPatternRewriter &rewriter, Location loc) {\n+    auto tensorTy = resType.cast<RankedTensorType>();\n+    auto shape = tensorTy.getShape();\n+    auto parent = layout.getParent();\n+    int numElems{};\n+    if (auto mmaLayout = parent.dyn_cast<MmaEncodingAttr>()) {\n+      if (mmaLayout.isAmpere()) {\n+        numElems = layout.getOpIdx() == 0\n+                       ? MMA16816ConversionHelper::getANumElemsPerThread(\n+                             tensorTy, mmaLayout.getWarpsPerCTA()[0])\n+                       : MMA16816ConversionHelper::getBNumElemsPerThread(\n+                             tensorTy, mmaLayout.getWarpsPerCTA()[1]);\n+      } else if (mmaLayout.isVolta()) {\n+        DotOpMmaV1ConversionHelper helper(mmaLayout);\n+        numElems = layout.getOpIdx() == 0\n+                       ? helper.numElemsPerThreadA(shape, {0, 1})\n+                       : helper.numElemsPerThreadB(shape, {0, 1});\n+      }\n+    } else if (auto blockedLayout = parent.dyn_cast<BlockedEncodingAttr>()) {\n+      numElems = DotOpFMAConversionHelper::getNumElemsPerThread(shape, layout);\n+    } else {\n+      assert(false && \"Unsupported layout found\");\n+    }\n+    auto structTy = LLVM::LLVMStructType::getLiteral(\n+        rewriter.getContext(), SmallVector<Type>(numElems, elemType));\n+    return getStructFromElements(loc, SmallVector<Value>(numElems, constVal),\n+                                 rewriter, structTy);\n+  }\n+\n   static Value convertSplatLikeOpWithMmaLayout(\n       const MmaEncodingAttr &layout, Type resType, Type elemType,\n       Value constVal, TypeConverter *typeConverter,"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 17, "deletions": 18, "changes": 35, "file_content_changes": "@@ -491,10 +491,9 @@ def make_ptr_str(name, shape):\n # TODO: handle `%4 = triton_gpu.convert_layout %3 : (tensor<32xi32, #blocked0>) -> tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>``\n @pytest.mark.parametrize(\"expr, dtype_str\", [\n     (f'x[{s}]', d)\n-    for s in ['None, :', ':, None']\n-    # FIXME: 3d indexing doesn't work\n-    #'None, :, :',\n-    # ':, :, None']\n+    for s in ['None, :', ':, None',\n+              'None, :, :',\n+              ':, :, None']\n     for d in ['int32', 'uint32', 'uint16']\n ])\n def test_index1d(expr, dtype_str, device='cuda'):\n@@ -1228,20 +1227,20 @@ def kernel(X, stride_xm, stride_xk,\n     elif dtype == 'int8':\n         assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n \n-# FIXME: Unsupported layout found in ConvertSplatLikeOp\n-# def test_dot_without_load():\n-#    @triton.jit\n-#    def kernel(out):\n-#        pid = tl.program_id(axis=0)\n-#        a = tl.zeros((32, 32), tl.float32)\n-#        b = tl.zeros((32, 32), tl.float32)\n-#        c = tl.zeros((32, 32), tl.float32)\n-#        c = tl.dot(a, b)\n-#        pout = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n-#        tl.store(pout, c)\n-#\n-#    out = torch.ones((32, 32), dtype=torch.float32, device=\"cuda\")\n-#    kernel[(1,)](out)\n+\n+def test_dot_without_load():\n+    @triton.jit\n+    def kernel(out):\n+        pid = tl.program_id(axis=0)\n+        a = tl.zeros((32, 32), tl.float32)\n+        b = tl.zeros((32, 32), tl.float32)\n+        c = tl.zeros((32, 32), tl.float32)\n+        c = tl.dot(a, b)\n+        pout = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n+        tl.store(pout, c)\n+\n+    out = torch.ones((32, 32), dtype=torch.float32, device=\"cuda\")\n+    kernel[(1,)](out)\n \n # ---------------\n # test arange"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -20,6 +20,8 @@\n     atomic_xor,\n     bfloat16,\n     block_type,\n+    broadcast,\n+    broadcast_to,\n     cat,\n     cdiv,\n     constexpr,\n@@ -105,6 +107,8 @@\n     \"atomic_xor\",\n     \"bfloat16\",\n     \"block_type\",\n+    \"broadcast\",\n+    \"broadcast_to\",\n     \"builtin\",\n     \"cat\",\n     \"cdiv\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -596,11 +596,9 @@ def __getitem__(self, slices, _builder=None):\n         if isinstance(slices, slice):\n             slices = [slices]\n         ret = self\n-        n_inserted = 0\n         for dim, sl in enumerate(slices):\n             if isinstance(sl, constexpr) and sl.value is None:\n-                ret = semantic.expand_dims(ret, dim + n_inserted, _builder)\n-                n_inserted += 1\n+                ret = semantic.expand_dims(ret, dim, _builder)\n             elif sl == slice(None, None, None):\n                 pass\n             else:"}]