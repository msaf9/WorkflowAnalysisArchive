[{"filename": "python/tests/test_core.py", "status": "added", "additions": 1552, "deletions": 0, "changes": 1552, "file_content_changes": "@@ -0,0 +1,1552 @@\n+# flake8: noqa: F821,F841\n+import itertools\n+import re\n+from typing import Optional, Union\n+\n+import numpy as np\n+import pytest\n+import torch\n+from numpy.random import RandomState\n+\n+import triton\n+import triton._C.libtriton.triton as _triton\n+import triton.language as tl\n+from triton.runtime.jit import JITFunction, TensorWrapper, reinterpret\n+\n+int_dtypes = ['int8', 'int16', 'int32', 'int64']\n+uint_dtypes = ['uint8', 'uint16', 'uint32', 'uint64']\n+float_dtypes = ['float16', 'float32', 'float64']\n+dtypes = int_dtypes + uint_dtypes + float_dtypes\n+# TODO: handle bfloat16\n+dtypes_with_bfloat16 = dtypes # + ['bfloat16']\n+\n+\n+def _bitwidth(dtype: str) -> int:\n+    # ex.: \"int64\" -> 64\n+    return int(re.search(r'(\\d+)$', dtype).group(1))\n+\n+\n+def numpy_random(shape, dtype_str, rs: Optional[RandomState] = None, low=None, high=None):\n+    \"\"\"\n+    Override `rs` if you're calling this function twice and don't want the same\n+    result for both calls.\n+    \"\"\"\n+    if isinstance(shape, int):\n+        shape = (shape, )\n+    if rs is None:\n+        rs = RandomState(seed=17)\n+    if dtype_str in int_dtypes + uint_dtypes:\n+        iinfo = np.iinfo(getattr(np, dtype_str))\n+        low = iinfo.min if low is None else max(low, iinfo.min)\n+        high = iinfo.max if high is None else min(high, iinfo.max)\n+        dtype = getattr(np, dtype_str)\n+        x = rs.randint(low, high, shape, dtype=dtype)\n+        x[x == 0] = 1  # Hack. Never return zero so tests of division don't error out.\n+        return x\n+    elif dtype_str in float_dtypes:\n+        return rs.normal(0, 1, shape).astype(dtype_str)\n+    elif dtype_str == 'bfloat16':\n+        return (rs.normal(0, 1, shape).astype('float32').view('uint32')\n+                & np.uint32(0xffff0000)).view('float32')\n+    elif dtype_str in ['bool', 'int1', 'bool_']:\n+        return rs.normal(0, 1, shape) > 0.0\n+    else:\n+        raise RuntimeError(f'Unknown dtype {dtype_str}')\n+\n+\n+def to_triton(x: np.ndarray, device='cuda', dst_type=None) -> Union[TensorWrapper, torch.Tensor]:\n+    '''\n+    Note: We need dst_type because the type of x can be different from dst_type.\n+          For example: x is of type `float32`, dst_type is `bfloat16`.\n+          If dst_type is None, we infer dst_type from x.\n+    '''\n+    t = x.dtype.name\n+    if t in uint_dtypes:\n+        signed_type_name = t.lstrip('u')  # e.g. \"uint16\" -> \"int16\"\n+        x_signed = x.astype(getattr(np, signed_type_name))\n+        return reinterpret(torch.tensor(x_signed, device=device), getattr(tl, t))\n+    else:\n+        if t == 'float32' and dst_type == 'bfloat16':\n+            return torch.tensor(x, device=device).bfloat16()\n+        return torch.tensor(x, device=device)\n+\n+\n+def torch_dtype_name(dtype) -> str:\n+    if isinstance(dtype, triton.language.dtype):\n+        return dtype.name\n+    elif isinstance(dtype, torch.dtype):\n+        # 'torch.int64' -> 'int64'\n+        m = re.match(r'^torch\\.(\\w+)$', str(dtype))\n+        return m.group(1)\n+    else:\n+        raise TypeError(f'not a triton or torch dtype: {type(dtype)}')\n+\n+\n+def to_numpy(x):\n+    if isinstance(x, TensorWrapper):\n+        return x.base.cpu().numpy().astype(getattr(np, torch_dtype_name(x.dtype)))\n+    elif isinstance(x, torch.Tensor):\n+        if x.dtype is torch.bfloat16:\n+            return x.cpu().float().numpy()\n+        return x.cpu().numpy()\n+    else:\n+        raise ValueError(f\"Not a triton-compatible tensor: {x}\")\n+\n+\n+def patch_kernel(template, to_replace):\n+    kernel = triton.JITFunction(template.fn)\n+    for key, value in to_replace.items():\n+        kernel.src = kernel.src.replace(key, value)\n+    return kernel\n+\n+\n+def check_type_supported(dtype):\n+    '''\n+    skip test if dtype is not supported on the current device\n+    '''\n+    cc = torch.cuda.get_device_capability()\n+    if cc[0] < 8 and (dtype is tl.bfloat16 or dtype == \"bfloat16\" or dtype is torch.bfloat16):\n+        pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n+\n+\n+@pytest.mark.parametrize(\"dtype_x\", [dtype_x for dtype_x in dtypes] + [\"bfloat16\"])\n+def test_empty_kernel(dtype_x, device='cuda'):\n+    SIZE = 128\n+\n+    @triton.jit\n+    def kernel(X, SIZE: tl.constexpr):\n+        pass\n+    check_type_supported(dtype_x)\n+    x = to_triton(numpy_random(SIZE, dtype_str=dtype_x), device=device, dst_type=dtype_x)\n+    kernel[(1, )](x, SIZE=SIZE, num_warps=4)\n+\n+\n+# generic test functions\n+def _test_unary(dtype_x, expr, numpy_expr=None, device='cuda'):\n+    check_type_supported(dtype_x)  # early return if dtype_x is not supported\n+    SIZE = 128\n+    # define the kernel / launch-grid\n+\n+    @triton.jit\n+    def kernel(Z, X, SIZE: tl.constexpr):\n+        off = tl.arange(0, SIZE)\n+        x = tl.load(X + off)\n+        z = GENERATE_TEST_HERE\n+        tl.store(Z + off, z)\n+\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': expr})\n+    # inputs\n+    x = numpy_random(SIZE, dtype_str=dtype_x)\n+    if 'log' in expr:\n+        x = np.abs(x) + 0.01\n+    # reference result\n+    z_ref = eval(expr if numpy_expr is None else numpy_expr)\n+    # triton result\n+    x_tri = to_triton(x, device=device, dst_type=dtype_x)\n+    z_tri = to_triton(np.empty_like(z_ref), device=device, dst_type=dtype_x)\n+    kernel[(1, )](z_tri, x_tri, SIZE=SIZE, num_warps=4)\n+    # compare\n+    np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+\n+\n+def _binary_op_dtype_override(a: str, b: str) -> Optional[np.dtype]:\n+    \"\"\"\n+    Given two dtype strings, returns the numpy dtype Triton thinks binary\n+    operations on the two types should return. Returns None if the return value\n+    matches numpy. This is generally needed because Triton and pytorch return\n+    narrower floating point types than numpy in mixed operations, and because\n+    Triton follows C/C++ semantics around mixed signed/unsigned operations, and\n+    numpy/pytorch do not.\n+    \"\"\"\n+    overrides = {\n+        ('float16', 'int16'): np.float16,\n+        ('float16', 'int32'): np.float16,\n+        ('float16', 'int64'): np.float16,\n+        ('float16', 'uint16'): np.float16,\n+        ('float16', 'uint32'): np.float16,\n+        ('float16', 'uint64'): np.float16,\n+        ('int8', 'uint8'): np.uint8,\n+        ('int8', 'uint16'): np.uint16,\n+        ('int8', 'uint32'): np.uint32,\n+        ('int8', 'uint64'): np.uint64,\n+        ('int16', 'uint16'): np.uint16,\n+        ('int16', 'uint32'): np.uint32,\n+        ('int16', 'uint64'): np.uint64,\n+        ('int32', 'uint32'): np.uint32,\n+        ('int32', 'uint64'): np.uint64,\n+        ('int64', 'uint64'): np.uint64,\n+    }\n+    key = (a, b) if a < b else (b, a)\n+    return overrides.get(key)\n+\n+\n+def _test_binary(dtype_x, dtype_y, expr, numpy_expr=None, mode_x='real', mode_y='real', device='cuda', y_low=None, y_high=None):\n+    check_type_supported(dtype_x)  # early return if dtype_x is not supported\n+    check_type_supported(dtype_y)\n+    SIZE = 128\n+    # define the kernel / launch-grid\n+\n+    @triton.jit\n+    def kernel(Z, X, Y, SIZE: tl.constexpr):\n+        off = tl.arange(0, SIZE)\n+        x = tl.load(X + off)\n+        y = tl.load(Y + off)\n+        z = GENERATE_TEST_HERE\n+        tl.store(Z + off, z)\n+\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': expr})\n+    # inputs\n+    rs = RandomState(17)\n+    x = numpy_random(SIZE, dtype_str=dtype_x, rs=rs)\n+    y = numpy_random(SIZE, dtype_str=dtype_y, rs=rs, low=y_low, high=y_high)\n+    if mode_x == 'nan':\n+        x[:] = float('nan')\n+    if mode_y == 'nan':\n+        y[:] = float('nan')\n+    # reference result\n+    z_ref = eval(expr if numpy_expr is None else numpy_expr)\n+    dtype_z = _binary_op_dtype_override(dtype_x, dtype_y)\n+    if dtype_z is not None:\n+        z_ref = z_ref.astype(dtype_z)\n+    # triton result\n+    x_tri = to_triton(x, device=device, dst_type=dtype_x)\n+    y_tri = to_triton(y, device=device, dst_type=dtype_y)\n+    z_tri = to_triton(np.empty(SIZE, dtype=z_ref.dtype), device=device)\n+    kernel[(1, )](z_tri, x_tri, y_tri, SIZE=SIZE, num_warps=4)\n+    np.testing.assert_allclose(z_ref, to_numpy(z_tri), err_msg=expr, rtol=0.01)\n+\n+\n+def _mod_operation_ill_conditioned(dtype_x, dtype_y) -> bool:\n+    # The result of x % y is ill-conditioned if x % y is much smaller than x.\n+    # pytorch/CUDA has slightly different (probably better) rounding on\n+    # remainders than stock LLVM. We currently don't expect to match it\n+    # bit-for-bit.\n+    return (dtype_x, dtype_y) in [\n+        ('int32', 'bfloat16'),\n+        ('int32', 'float16'),\n+        ('int32', 'float32'),\n+        ('int64', 'bfloat16'),\n+        ('int64', 'float16'),\n+        ('int64', 'float32'),\n+        ('int64', 'float64'),\n+        ('uint16', 'bfloat16'),\n+        ('uint16', 'float16'),\n+        ('uint16', 'float32'),\n+        ('uint32', 'bfloat16'),\n+        ('uint32', 'float16'),\n+        ('uint32', 'float32'),\n+        ('uint64', 'bfloat16'),\n+        ('uint64', 'float16'),\n+        ('uint64', 'float32'),\n+        ('uint64', 'float64'),\n+    ]\n+\n+# ---------------\n+# test binary ops\n+# ---------------\n+\n+\n+@pytest.mark.parametrize(\"dtype_x, dtype_y, op\", [\n+    (dtype_x, dtype_y, op)\n+    for op in ['+', '-', '*', '/']#, '%'] #TODO: handle remainder\n+    for dtype_x in dtypes_with_bfloat16\n+    for dtype_y in dtypes_with_bfloat16\n+])\n+def test_bin_op(dtype_x, dtype_y, op, device='cuda'):\n+    expr = f' x {op} y'\n+    if op == '%' and dtype_x in int_dtypes + uint_dtypes and dtype_y in int_dtypes + uint_dtypes:\n+        # LLVM has 'numpy.fmod', not 'numpy.remainder', semantics on integer remainders.\n+        numpy_expr = 'np.fmod(x, y)'\n+    elif op in ('/', '%') and dtype_x in ('int16', 'float16', 'bfloat16') and dtype_y in ('int16', 'float16', 'bfloat16'):\n+        # Triton promotes 16-bit floating-point / and % to 32-bit because there\n+        # are no native div or FRem operations on float16. Since we have to\n+        # convert anyway, we may as well take the accuracy bump.\n+        numpy_expr = f'x.astype(np.float32) {op} y.astype(np.float32)'\n+    elif (dtype_x in uint_dtypes and dtype_y in int_dtypes and _bitwidth(dtype_x) >= _bitwidth(dtype_y)):\n+        numpy_expr = f'x.astype(np.{dtype_x}) {op} y.astype(np.{dtype_x})'\n+    elif (dtype_y in uint_dtypes and dtype_x in int_dtypes and _bitwidth(dtype_y) >= _bitwidth(dtype_x)):\n+        numpy_expr = f'x.astype(np.{dtype_y}) {op} y.astype(np.{dtype_y})'\n+    else:\n+        numpy_expr = None\n+    if op == '%' and _mod_operation_ill_conditioned(dtype_x, dtype_y):\n+        with pytest.raises(AssertionError, match='Not equal to tolerance'):\n+            _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n+    elif (op in ('%', '/') and\n+          ((dtype_x in int_dtypes and dtype_y in uint_dtypes) or\n+           (dtype_x in uint_dtypes and dtype_y in int_dtypes))):\n+        with pytest.raises(triton.CompilationError) as exc_info:\n+            _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n+        assert re.match('Cannot use .* because they have different signedness', str(exc_info.value.__cause__))\n+    else:\n+        _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n+\n+\n+# @pytest.mark.parametrize(\"dtype_x, dtype_y\",\n+#                          [(dtype_x, dtype_y) for dtype_x in int_dtypes for dtype_y in int_dtypes] +\n+#                          [(dtype_x, dtype_y) for dtype_x in uint_dtypes for dtype_y in uint_dtypes]\n+#                          )\n+# def test_floordiv(dtype_x, dtype_y, device='cuda'):\n+#     # Triton has IEEE, not numpy/torch, semantics for %, and those carry\n+#     # through to //, so we have to use a nonstandard expression to get a\n+#     # reference result for //.\n+#     expr = 'x // y'\n+#     numpy_expr = '((x - np.fmod(x, y)) / y)'\n+#     _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n+\n+\n+# # ---------------\n+# # test bitwise ops\n+# # ---------------\n+# @pytest.mark.parametrize(\"dtype_x, dtype_y, op\", [\n+#     (dtype_x, dtype_y, op)\n+#     for op in ['&', '|', '^']\n+#     for dtype_x in dtypes + dtypes_with_bfloat16\n+#     for dtype_y in dtypes + dtypes_with_bfloat16\n+# ])\n+# def test_bitwise_op(dtype_x, dtype_y, op, device='cuda'):\n+#     expr = f'x {op} y'\n+#     if (dtype_x in uint_dtypes and dtype_y in int_dtypes and _bitwidth(dtype_x) >= _bitwidth(dtype_y)):\n+#         numpy_expr = f'x.astype(np.{dtype_x}) {op} y.astype(np.{dtype_x})'\n+#     elif (dtype_y in uint_dtypes and dtype_x in int_dtypes and _bitwidth(dtype_y) >= _bitwidth(dtype_x)):\n+#         numpy_expr = f'x.astype(np.{dtype_y}) {op} y.astype(np.{dtype_y})'\n+#     else:\n+#         numpy_expr = None\n+#     if 'float' in dtype_x + dtype_y:\n+#         with pytest.raises(triton.CompilationError) as exc_info:\n+#             _test_binary(dtype_x, dtype_y, expr, numpy_expr='np.array([])', device=device)\n+#         # The CompilationError must have been caused by a C++ exception with this text.\n+#         assert re.match('invalid operands of type', str(exc_info.value.__cause__))\n+#     else:\n+#         _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n+\n+\n+# @pytest.mark.parametrize(\"dtype_x, dtype_y, op\", [\n+#     (dtype_x, dtype_y, op)\n+#     for op in ['<<', '>>']\n+#     for dtype_x in int_dtypes + uint_dtypes\n+#     for dtype_y in int_dtypes + uint_dtypes\n+# ])\n+# def test_shift_op(dtype_x, dtype_y, op, device='cuda'):\n+#     expr = f'x {op} y'\n+#     bw = max(_bitwidth(dtype_x), _bitwidth(dtype_y))\n+#     dtype_z = f'uint{bw}'\n+#     numpy_expr = f'x.astype(np.{dtype_z}) {op} y.astype(np.{dtype_z})'\n+#     _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device, y_low=0, y_high=65)\n+\n+\n+# # ---------------\n+# # test compare ops\n+# # ---------------\n+# ops = ['==', '!=', '>', '<', '>=', '<=']\n+\n+\n+# @pytest.mark.parametrize(\"dtype_x, dtype_y, op, mode_x, mode_y\",\n+#                          # real\n+#                          [\n+#                              (dtype_x, dtype_y, op, 'real', 'real')\n+#                              for op in ops\n+#                              for dtype_x in dtypes\n+#                              for dtype_y in dtypes\n+#                          ] +\n+#                          # NaNs\n+#                          [('float32', 'float32', op, mode_x, mode_y)\n+#                              for op in ops\n+#                              for mode_x, mode_y in [('nan', 'real'),\n+#                                                     ('real', 'nan'),\n+#                                                     ('nan', 'nan')]\n+\n+#                           ])\n+# def test_compare_op(dtype_x, dtype_y, op, mode_x, mode_y, device='cuda'):\n+#     expr = f'x {op} y'\n+#     if (dtype_x in uint_dtypes and dtype_y in int_dtypes and _bitwidth(dtype_x) >= _bitwidth(dtype_y)):\n+#         numpy_expr = f'x.astype(np.{dtype_x}) {op} y.astype(np.{dtype_x})'\n+#     elif (dtype_y in uint_dtypes and dtype_x in int_dtypes and _bitwidth(dtype_y) >= _bitwidth(dtype_x)):\n+#         numpy_expr = f'x.astype(np.{dtype_y}) {op} y.astype(np.{dtype_y})'\n+#     else:\n+#         numpy_expr = None\n+#     _test_binary(dtype_x, dtype_y, expr, numpy_expr, mode_x=mode_x, mode_y=mode_y, device=device)\n+\n+\n+# # ---------------\n+# # test where\n+# # ---------------\n+# @pytest.mark.parametrize(\"dtype\", dtypes_with_bfloat16 + [\"*int32\"])\n+# def test_where(dtype):\n+#     select_ptrs = False\n+#     if dtype == \"*int32\":\n+#         dtype = \"int64\"\n+#         select_ptrs = True\n+#     check_type_supported(dtype)\n+\n+#     @triton.jit\n+#     def where_kernel(cond_ptr, a_ptr, b_ptr, output_ptr, n_elements,\n+#                      BLOCK_SIZE: tl.constexpr,\n+#                      TEST_POINTERS: tl.constexpr):\n+#         offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+#         mask = offsets < n_elements\n+#         decide = tl.load(cond_ptr + offsets, mask=mask)\n+#         if TEST_POINTERS:\n+#             a = tl.load(a_ptr + offsets, mask=mask).to(tl.pi32_t)\n+#             b = tl.load(b_ptr + offsets, mask=mask).to(tl.pi32_t)\n+#         else:\n+#             a = tl.load(a_ptr + offsets, mask=mask)\n+#             b = tl.load(b_ptr + offsets, mask=mask)\n+#         output = tl.where(decide, a, b)\n+#         tl.store(output_ptr + offsets, output, mask=mask)\n+\n+#     SIZE = 1_000\n+#     rs = RandomState(17)\n+#     cond = numpy_random(SIZE, 'bool', rs)\n+#     x = numpy_random(SIZE, dtype_str=dtype, rs=rs)\n+#     y = numpy_random(SIZE, dtype_str=dtype, rs=rs)\n+#     z = np.where(cond, x, y)\n+\n+#     cond_tri = to_triton(cond, device='cuda')\n+#     x_tri = to_triton(x, device='cuda', dst_type=dtype)\n+#     y_tri = to_triton(y, device='cuda', dst_type=dtype)\n+#     z_tri = to_triton(np.empty(SIZE, dtype=z.dtype), device='cuda', dst_type=dtype)\n+\n+#     grid = lambda meta: (triton.cdiv(SIZE, meta['BLOCK_SIZE']),)\n+#     where_kernel[grid](cond_tri, x_tri, y_tri, z_tri, SIZE, BLOCK_SIZE=1024, TEST_POINTERS=select_ptrs)\n+#     assert (z == to_numpy(z_tri)).all()\n+\n+\n+# def test_where_broadcast():\n+#     @triton.jit\n+#     def where_kernel(cond_ptr, a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n+#         xoffsets = tl.reshape(tl.arange(0, BLOCK_SIZE), [BLOCK_SIZE, 1])\n+#         yoffsets = tl.reshape(tl.arange(0, BLOCK_SIZE), [1, BLOCK_SIZE])\n+\n+#         mask = tl.load(cond_ptr + yoffsets)\n+#         vals = tl.load(a_ptr + yoffsets + BLOCK_SIZE * xoffsets)\n+#         res = tl.where(mask, vals, 0.)\n+#         tl.store(out_ptr + yoffsets + BLOCK_SIZE * xoffsets, res)\n+\n+#     @triton.jit\n+#     def where_scalar_condition(a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n+#         xoffsets = tl.reshape(tl.arange(0, BLOCK_SIZE), [BLOCK_SIZE, 1])\n+#         yoffsets = tl.reshape(tl.arange(0, BLOCK_SIZE), [1, BLOCK_SIZE])\n+#         mask = 0\n+#         vals = tl.load(a_ptr + yoffsets + BLOCK_SIZE * xoffsets)\n+#         res = tl.where(mask, vals, 0.)\n+#         tl.store(out_ptr + yoffsets + BLOCK_SIZE * xoffsets, res)\n+\n+#     SIZE = 32\n+#     dtype = 'float32'\n+#     rs = RandomState(17)\n+#     x = numpy_random((SIZE, SIZE), dtype_str=dtype, rs=rs)\n+#     mask = numpy_random(SIZE, 'bool', rs=rs)\n+#     z = np.where(mask, x, 0)\n+#     cond_tri = to_triton(mask, device=\"cuda\")\n+#     x_tri = to_triton(x, device='cuda', dst_type=dtype)\n+#     z_tri = to_triton(np.empty((SIZE, SIZE), dtype=z.dtype), device='cuda', dst_type=dtype)\n+#     where_kernel[(1,)](cond_tri, x_tri, z_tri, SIZE)\n+#     assert (z == to_numpy(z_tri)).all()\n+#     where_scalar_condition[(1,)](x_tri, z_tri, SIZE)\n+#     z = np.where(0, x, 0)\n+#     assert (z == to_numpy(z_tri)).all()\n+\n+# # ---------------\n+# # test unary ops\n+# # ---------------\n+\n+\n+# @pytest.mark.parametrize(\"dtype_x, expr\", [\n+#     (dtype_x, ' -x') for dtype_x in dtypes_with_bfloat16\n+# ] + [\n+#     (dtype_x, ' ~x') for dtype_x in int_dtypes\n+# ])\n+# def test_unary_op(dtype_x, expr, device='cuda'):\n+#     _test_unary(dtype_x, expr, device=device)\n+\n+# # ----------------\n+# # test math ops\n+# # ----------------\n+# # @pytest.mark.parametrize(\"expr\", [\n+# #     'exp', 'log', 'cos', 'sin'\n+# # ])\n+\n+\n+# @pytest.mark.parametrize(\"expr\", [\n+#     'exp', 'log', 'cos', 'sin'\n+# ])\n+# def test_math_op(expr, device='cuda'):\n+#     _test_unary('float32', f'tl.{expr}(x)', f'np.{expr}(x) ', device=device)\n+\n+\n+# # ----------------\n+# # test indexing\n+# # ----------------\n+\n+\n+# def make_ptr_str(name, shape):\n+#     rank = len(shape)\n+#     offsets = []\n+#     stride = 1\n+#     for i in reversed(range(rank)):\n+#         idx = ', '.join([':' if ii == i else 'None' for ii in range(rank)])\n+#         offsets += [f'tl.arange(0, {shape[i]})[{idx}]*{stride}']\n+#         stride *= shape[i]\n+#     return f\"{name} + {' + '.join(offsets)}\"\n+\n+\n+# @pytest.mark.parametrize(\"expr, dtype_str\", [\n+#     (f'x[{s}]', d)\n+#     for s in ['None, :', ':, None', 'None, :, :', ':, :, None']\n+#     for d in ['int32', 'uint32', 'uint16']\n+# ])\n+# def test_index1d(expr, dtype_str, device='cuda'):\n+#     rank_x = expr.count(':')\n+#     rank_y = expr.count(',') + 1\n+#     shape_x = [32 for _ in range(rank_x)]\n+#     shape_z = [32 for _ in range(rank_y)]\n+#     shape_z_rank_mismatch = [32 for _ in range(rank_y + 1)]\n+#     shape_z_dim_mismatch = [64 for _ in range(rank_y)]\n+\n+#     # Triton kernel\n+#     @triton.jit\n+#     def kernel(Z, X, SIZE: tl.constexpr):\n+#         m = tl.arange(0, SIZE)\n+#         n = tl.arange(0, SIZE)\n+#         x = tl.load(X_PTR_EXPR)\n+#         z = GENERATE_TEST_HERE\n+#         tl.store(Z_PTR_EXPR, z)\n+\n+#     def generate_kernel(shape_x, shape_z):\n+#         to_replace = {\n+#             'X_PTR_EXPR': make_ptr_str('X', shape_x),\n+#             'Z_PTR_EXPR': make_ptr_str('Z', shape_z),\n+#             'GENERATE_TEST_HERE': expr,\n+#         }\n+#         return patch_kernel(kernel, to_replace)\n+\n+#     kernel_match = generate_kernel(shape_x, shape_z)\n+#     kernel_dim_mismatch = generate_kernel(shape_x, shape_z_dim_mismatch)\n+#     kernel_rank_mismatch = generate_kernel(shape_x, shape_z_rank_mismatch)\n+\n+#     # torch result\n+#     x = numpy_random(shape_x, dtype_str=dtype_str)\n+#     y = np.zeros(shape_z, dtype=getattr(np, dtype_str))\n+#     z_ref = eval(expr) + y\n+#     # triton result\n+#     z_tri = to_triton(np.empty_like(z_ref), device=device)\n+#     x_tri = to_triton(x)\n+#     kernel_match[(1, )](z_tri, x_tri, num_warps=1, SIZE=shape_x[0])\n+#     # compare\n+#     assert (z_ref == to_numpy(z_tri)).all()\n+\n+#     def catch_compilation_error(kernel):\n+#         try:\n+#             kernel[(1, )](z_tri, x_tri, num_warps=1, SIZE=shape_x[0])\n+#         except triton.CompilationError as e:\n+#             np.testing.assert_(True)\n+#         except BaseException:\n+#             np.testing.assert_(False)\n+\n+#     catch_compilation_error(kernel_dim_mismatch)\n+#     catch_compilation_error(kernel_rank_mismatch)\n+\n+\n+# # ---------------\n+# # test tuples\n+# # ---------------\n+\n+\n+# @triton.jit\n+# def fn(a, b):\n+#     return a + b, \\\n+#         a - b, \\\n+#         a * b\n+\n+\n+# def test_tuples():\n+#     device = 'cuda'\n+\n+#     @triton.jit\n+#     def with_fn(X, Y, A, B, C):\n+#         x = tl.load(X)\n+#         y = tl.load(Y)\n+#         a, b, c = fn(x, y)\n+#         tl.store(A, a)\n+#         tl.store(B, b)\n+#         tl.store(C, c)\n+\n+#     @triton.jit\n+#     def without_fn(X, Y, A, B, C):\n+#         x = tl.load(X)\n+#         y = tl.load(Y)\n+#         a, b, c = x + y, x - y, x * y\n+#         tl.store(A, a)\n+#         tl.store(B, b)\n+#         tl.store(C, c)\n+\n+#     x = torch.tensor([1.3], device=device, dtype=torch.float32)\n+#     y = torch.tensor([1.9], device=device, dtype=torch.float32)\n+#     a_tri = torch.tensor([0], device=device, dtype=torch.float32)\n+#     b_tri = torch.tensor([0], device=device, dtype=torch.float32)\n+#     c_tri = torch.tensor([0], device=device, dtype=torch.float32)\n+#     for kernel in [with_fn, without_fn]:\n+#         kernel[(1, )](x, y, a_tri, b_tri, c_tri, num_warps=1)\n+#         a_ref, b_ref, c_ref = x + y, x - y, x * y\n+#         assert a_tri == a_ref\n+#         assert b_tri == b_ref\n+#         assert c_tri == c_ref\n+\n+\n+# # ---------------\n+# # test atomics\n+# # ---------------\n+# @pytest.mark.parametrize(\"op, dtype_x_str, mode\", itertools.chain.from_iterable([\n+#     [\n+#         ('add', 'float16', mode),\n+#         ('add', 'uint32', mode), ('add', 'int32', mode), ('add', 'float32', mode),\n+#         ('max', 'uint32', mode), ('max', 'int32', mode), ('max', 'float32', mode),\n+#         ('min', 'uint32', mode), ('min', 'int32', mode), ('min', 'float32', mode),\n+#     ]\n+#     for mode in ['all_neg', 'all_pos', 'min_neg', 'max_pos']]))\n+# def test_atomic_rmw(op, dtype_x_str, mode, device='cuda'):\n+#     n_programs = 5\n+\n+#     # triton kernel\n+#     @triton.jit\n+#     def kernel(X, Z):\n+#         pid = tl.program_id(0)\n+#         x = tl.load(X + pid)\n+#         old = GENERATE_TEST_HERE\n+\n+#     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.atomic_{op}(Z, x)'})\n+#     numpy_op = {'add': np.sum, 'max': np.max, 'min': np.min}[op]\n+#     max_neutral = float('-inf') if dtype_x_str in float_dtypes else np.iinfo(getattr(np, dtype_x_str)).min\n+#     min_neutral = float('inf') if dtype_x_str in float_dtypes else np.iinfo(getattr(np, dtype_x_str)).max\n+#     neutral = {'add': 0, 'max': max_neutral, 'min': min_neutral}[op]\n+\n+#     # triton result\n+#     rs = RandomState(17)\n+#     x = numpy_random((n_programs, ), dtype_str=dtype_x_str, rs=rs)\n+#     if mode == 'all_neg':\n+#         x = -np.abs(x)\n+#     if mode == 'all_pos':\n+#         x = np.abs(x)\n+#     if mode == 'min_neg':\n+#         idx = rs.randint(n_programs, size=(1, )).item()\n+#         x[idx] = -np.max(np.abs(x)) - 1\n+#     if mode == 'max_pos':\n+#         idx = rs.randint(n_programs, size=(1, )).item()\n+#         x[idx] = np.max(np.abs(x)) + 1\n+#     x_tri = to_triton(x, device=device)\n+\n+#     z_tri = to_triton(np.array([neutral], dtype=getattr(np, dtype_x_str)), device=device)\n+#     kernel[(n_programs, )](x_tri, z_tri)\n+#     # torch result\n+#     z_ref = numpy_op(x).astype(getattr(np, dtype_x_str))\n+#     # compare\n+#     exact = op not in ['add']\n+#     if exact:\n+#         assert z_ref.item() == to_numpy(z_tri).item()\n+#     else:\n+#         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+\n+\n+# @pytest.mark.parametrize(\"axis\", [0, 1])\n+# def test_tensor_atomic_rmw(axis, device=\"cuda\"):\n+#     shape0, shape1 = 8, 8\n+#     # triton kernel\n+\n+#     @triton.jit\n+#     def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n+#         off0 = tl.arange(0, SHAPE0)\n+#         off1 = tl.arange(0, SHAPE1)\n+#         x = tl.load(X + off0[:, None] * SHAPE1 + off1[None, :])\n+#         z = tl.sum(x, axis=AXIS)\n+#         tl.atomic_add(Z + off0, z)\n+#     rs = RandomState(17)\n+#     x = numpy_random((shape0, shape1), dtype_str=\"float32\", rs=rs)\n+#     # reference result\n+#     z_ref = np.sum(x, axis=axis)\n+#     # triton result\n+#     x_tri = to_triton(x, device=device)\n+#     z_tri = to_triton(np.zeros((shape0,), dtype=\"float32\"), device=device)\n+#     kernel[(1,)](z_tri, x_tri, axis, shape0, shape1)\n+#     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n+\n+\n+# def test_atomic_cas():\n+#     # 1. make sure that atomic_cas changes the original value (Lock)\n+#     @triton.jit\n+#     def change_value(Lock):\n+#         tl.atomic_cas(Lock, 0, 1)\n+\n+#     Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n+#     change_value[(1,)](Lock)\n+\n+#     assert (Lock[0] == 1)\n+\n+#     # 2. only one block enters the critical section\n+#     @triton.jit\n+#     def serialized_add(data, Lock):\n+#         ptrs = data + tl.arange(0, 128)\n+#         while tl.atomic_cas(Lock, 0, 1) == 1:\n+#             pass\n+\n+#         tl.store(ptrs, tl.load(ptrs) + 1.0)\n+\n+#         # release lock\n+#         tl.atomic_xchg(Lock, 0)\n+\n+#     Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n+#     data = torch.zeros((128,), device='cuda', dtype=torch.float32)\n+#     ref = torch.full((128,), 64.0)\n+#     serialized_add[(64,)](data, Lock)\n+#     triton.testing.assert_almost_equal(data, ref)\n+\n+\n+# # ---------------\n+# # test cast\n+# # ---------------\n+\n+\n+# @pytest.mark.parametrize(\"dtype_x, dtype_z, bitcast\", [\n+#     (dtype_x, dtype_z, False)\n+#     for dtype_x in dtypes\n+#     for dtype_z in dtypes\n+# ] + [\n+#     ('float32', 'bfloat16', False),\n+#     ('bfloat16', 'float32', False),\n+#     ('float32', 'int32', True),\n+#     ('float32', 'int1', False),\n+# ] + [\n+#     (f'uint{x}', f'int{x}', True) for x in [8, 16, 32, 64]\n+# ] + [\n+#     (f'int{x}', f'uint{x}', True) for x in [8, 16, 32, 64]\n+# ])\n+# def test_cast(dtype_x, dtype_z, bitcast, device='cuda'):\n+#     # This is tricky because numpy doesn't have bfloat, and torch doesn't have uints.\n+#     x0 = 43 if dtype_x in int_dtypes else 43.5\n+#     if dtype_x in float_dtypes and dtype_z == 'int1':\n+#         x0 = 0.5\n+#     if dtype_x.startswith('bfloat'):\n+#         x_tri = torch.tensor([x0], dtype=getattr(torch, dtype_x), device=device)\n+#     else:\n+#         x = np.array([x0], dtype=getattr(np, dtype_x))\n+#         x_tri = to_triton(x)\n+\n+#     # triton kernel\n+#     @triton.jit\n+#     def kernel(X, Z, BITCAST: tl.constexpr):\n+#         x = tl.load(X)\n+#         z = x.to(Z.dtype.element_ty, bitcast=BITCAST)\n+#         tl.store(Z, z)\n+\n+#     dtype_z_np = dtype_z if dtype_z != 'int1' else 'bool_'\n+#     # triton result\n+#     if dtype_z.startswith('bfloat'):\n+#         z_tri = torch.empty((1,), dtype=getattr(torch, dtype_z), device=device)\n+#     else:\n+#         z_tri = to_triton(np.empty((1, ), dtype=getattr(np, dtype_z_np)), device=device)\n+#     kernel[(1, )](x_tri, z_tri, BITCAST=bitcast)\n+#     # torch result\n+#     if dtype_z.startswith('bfloat') or dtype_x.startswith('bfloat'):\n+#         assert bitcast is False\n+#         z_ref = x_tri.to(z_tri.dtype)\n+#         assert z_tri == z_ref\n+#     else:\n+#         if bitcast:\n+#             z_ref = x.view(getattr(np, dtype_z_np))\n+#         else:\n+#             z_ref = x.astype(getattr(np, dtype_z_np))\n+#         assert to_numpy(z_tri) == z_ref\n+\n+\n+# def test_store_bool():\n+#     \"\"\"Tests that boolean True is stored as 1\"\"\"\n+#     @triton.jit\n+#     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n+#         offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+#         mask = offsets < n_elements\n+#         input = tl.load(input_ptr + offsets, mask=mask)\n+#         output = input\n+#         tl.store(output_ptr + offsets, output, mask=mask)\n+\n+#     src = torch.tensor([True, False], dtype=torch.bool, device='cuda')\n+#     n_elements = src.numel()\n+#     dst = torch.empty_like(src)\n+#     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+#     copy_kernel[grid](src, dst, n_elements, BLOCK_SIZE=1024)\n+\n+#     assert (to_numpy(src).view('uint8') == to_numpy(dst).view('uint8')).all()\n+\n+\n+# @pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16])\n+# def test_f8_xf16_roundtrip(dtype):\n+#     \"\"\"Tests that converting an f8 to f16 and back to f8 doesn't change its value\"\"\"\n+#     check_type_supported(dtype)\n+\n+#     @triton.jit\n+#     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n+#         offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+#         mask = offsets < n_elements\n+#         input = tl.load(input_ptr + offsets, mask=mask)\n+#         output = input\n+#         tl.store(output_ptr + offsets, output, mask=mask)\n+\n+#     f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device='cuda')\n+#     f8 = triton.reinterpret(f8_tensor, tl.float8)\n+#     n_elements = f8_tensor.numel()\n+#     xf16 = torch.empty_like(f8_tensor, dtype=dtype)\n+#     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+#     copy_kernel[grid](f8, xf16, n_elements, BLOCK_SIZE=1024)\n+\n+#     f8_output_tensor = torch.empty_like(xf16, dtype=torch.int8)\n+#     f8_output = triton.reinterpret(f8_output_tensor, tl.float8)\n+#     copy_kernel[grid](xf16, f8_output, n_elements, BLOCK_SIZE=1024)\n+\n+#     assert torch.all(f8_tensor == f8_output_tensor)\n+\n+\n+# def test_f16_to_f8_rounding():\n+#     \"\"\"Takes all float16s, converts them to float8 and back to float16. Checks that the absolute\n+#     error is the minimum over all float8.\n+#     Or the same explanation a bit mathier:\n+#     for all f16 |f16 - fromf8(tof8(f16))| == min over all f8 |f16 - fromf8(f8)|\"\"\"\n+#     @triton.jit\n+#     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n+#         offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+#         mask = offsets < n_elements\n+#         input = tl.load(input_ptr + offsets, mask=mask)\n+#         output = input\n+#         tl.store(output_ptr + offsets, output, mask=mask)\n+\n+#     # torch.view with a dtype isn't supported in triton's torch yet so use numpy's view\n+#     f16_input_np = (\n+#         np.array(\n+#             range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=np.int16,\n+#         )\n+#         .view(np.float16)\n+#     )\n+#     f16_input = torch.tensor(f16_input_np, dtype=torch.float16, device='cuda')\n+#     n_elements = f16_input.numel()\n+#     f8_output_tensor = torch.empty_like(f16_input, dtype=torch.int8)\n+#     f8_output = triton.reinterpret(f8_output_tensor, tl.float8)\n+#     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+#     copy_kernel[grid](f16_input, f8_output, n_elements, BLOCK_SIZE=1024)\n+\n+#     f16_output = torch.empty_like(f16_input, dtype=torch.float16)\n+#     copy_kernel[grid](f8_output, f16_output, n_elements, BLOCK_SIZE=1024)\n+\n+#     abs_error = torch.abs(f16_input - f16_output)\n+\n+#     all_f8_vals_tensor = torch.tensor(range(2 ** 8), dtype=torch.uint8, device='cuda')\n+#     all_f8_vals = triton.reinterpret(all_f8_vals_tensor, tl.float8)\n+#     all_f8_vals_in_f16 = torch.empty_like(all_f8_vals_tensor, dtype=torch.float16)\n+#     copy_kernel[grid](all_f8_vals, all_f8_vals_in_f16, n_elements=256, BLOCK_SIZE=1024)\n+\n+#     all_finite_f8_vals_in_f16 = all_f8_vals_in_f16[\n+#         torch.isfinite(all_f8_vals_in_f16)\n+#     ]\n+\n+#     min_error = torch.min(\n+#         torch.abs(\n+#             f16_input.reshape((-1, 1))\n+#             - all_finite_f8_vals_in_f16.reshape((1, -1))\n+#         ),\n+#         dim=1,\n+#     )[0]\n+#     # 1.9375 is float8 max\n+#     mismatch = torch.logical_and(\n+#         abs_error != min_error, torch.logical_and(torch.isfinite(f16_input), torch.abs(f16_input) < 1.9375)\n+#     )\n+#     assert torch.all(\n+#         torch.logical_not(mismatch)\n+#     ), f\"f16_input[mismatch]={f16_input[mismatch]} f16_output[mismatch]={f16_output[mismatch]} abs_error[mismatch]={abs_error[mismatch]} min_error[mismatch]={min_error[mismatch]}\"\n+\n+\n+# # ---------------\n+# # test reduce\n+# # ---------------\n+\n+\n+# @pytest.mark.parametrize(\"op, dtype_str, shape\",\n+#                          [(op, dtype, shape)\n+#                           for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n+#                           for dtype in dtypes_with_bfloat16\n+#                           for shape in [32, 64, 128, 512]])\n+# def test_reduce1d(op, dtype_str, shape, device='cuda'):\n+#     check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n+\n+#     # triton kernel\n+#     @triton.jit\n+#     def kernel(X, Z, BLOCK: tl.constexpr):\n+#         x = tl.load(X + tl.arange(0, BLOCK))\n+#         tl.store(Z, GENERATE_TEST_HERE)\n+\n+#     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=0)'})\n+#     # input\n+#     rs = RandomState(17)\n+#     # limit the range of integers so that the sum does not overflow\n+#     x = numpy_random((shape,), dtype_str=dtype_str, rs=rs)\n+#     x_tri = to_triton(x, device=device)\n+#     numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n+#                 'argmin': np.argmin, 'argmax': np.argmax}[op]\n+#     # numpy result\n+#     z_dtype_str = 'int32' if op == 'argmin' or op == 'argmax' else dtype_str\n+#     z_tri_dtype_str = z_dtype_str\n+#     if op not in ['argmin', 'argmax'] and dtype_str == 'bfloat16':\n+#         z_dtype_str = 'float32'\n+#         z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n+#         # trunc mantissa for a fair comparison of accuracy\n+#         z_ref = (z_ref.view('uint32') & np.uint32(0xffff0000)).view('float32')\n+#         z_tri_dtype_str = 'bfloat16'\n+#     else:\n+#         z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n+#     # triton result\n+#     z_tri = to_triton(numpy_random((1,), dtype_str=z_dtype_str, rs=rs),\n+#                       device=device, dst_type=z_tri_dtype_str)\n+#     kernel[(1,)](x_tri, z_tri, BLOCK=shape)\n+#     z_tri = to_numpy(z_tri)\n+#     # compare\n+#     if op == 'sum':\n+#         np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n+#     else:\n+#         if op == 'argmin' or op == 'argmax':\n+#             # argmin and argmax can have multiple valid indices.\n+#             # so instead we compare the values pointed by indices\n+#             np.testing.assert_equal(x[z_ref], x[z_tri])\n+#         else:\n+#             np.testing.assert_equal(z_ref, z_tri)\n+\n+\n+# reduce_configs1 = [\n+#     (op, dtype, (1, 1024), axis) for dtype in dtypes_with_bfloat16\n+#     for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n+#     for axis in [1]\n+# ]\n+# reduce_configs2 = [\n+#     (op, 'float32', shape, axis)\n+#     for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n+#     for shape in [(2, 32), (4, 32), (4, 128), (32, 64), (64, 128), (128, 256), (32, 1024)]\n+#     for axis in [0, 1]\n+# ]\n+\n+\n+# @pytest.mark.parametrize(\"op, dtype_str, shape, axis\", reduce_configs1 + reduce_configs2)\n+# def test_reduce2d(op, dtype_str, shape, axis, device='cuda'):\n+#     # triton kernel\n+#     @triton.jit\n+#     def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexpr):\n+#         range_m = tl.arange(0, BLOCK_M)\n+#         range_n = tl.arange(0, BLOCK_N)\n+#         x = tl.load(X + range_m[:, None] * BLOCK_N + range_n[None, :])\n+#         z = GENERATE_TEST_HERE\n+#         if AXIS == 1:\n+#             tl.store(Z + range_m, z)\n+#         else:\n+#             tl.store(Z + range_n, z)\n+\n+#     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=AXIS)'})\n+#     # input\n+#     rs = RandomState(17)\n+#     # limit the range of integers so that the sum does not overflow\n+#     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n+#     x_tri = to_triton(x)\n+#     numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n+#                 'argmin': np.argmin, 'argmax': np.argmax}[op]\n+#     z_dtype_str = 'int32' if op == 'argmin' or op == 'argmax' else dtype_str\n+#     z_tri_dtype_str = z_dtype_str\n+#     # numpy result\n+#     if op not in ['argmin', 'argmax'] and dtype_str == 'bfloat16':\n+#         z_dtype_str = 'float32'\n+#         z_tri_dtype_str = 'bfloat16'\n+#         z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n+#         # trunc mantissa for a fair comparison of accuracy\n+#         z_ref = (z_ref.view('uint32') & np.uint32(0xffff0000)).view('float32')\n+#     else:\n+#         z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n+#     # triton result\n+#     z_tri = to_triton(numpy_random((shape[1 - axis],), dtype_str=z_dtype_str, rs=rs),\n+#                       device=device, dst_type=z_tri_dtype_str)\n+#     kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis)\n+#     z_tri = to_numpy(z_tri)\n+#     # compare\n+#     if op == 'sum':\n+#         np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n+#     else:\n+#         if op == 'argmin' or op == 'argmax':\n+#             # argmin and argmax can have multiple valid indices.\n+#             # so instead we compare the values pointed by indices\n+#             z_ref_index = np.expand_dims(z_ref, axis=axis)\n+#             z_tri_index = np.expand_dims(z_tri, axis=axis)\n+#             z_ref_value = np.take_along_axis(x, z_ref_index, axis=axis)\n+#             z_tri_value = np.take_along_axis(x, z_tri_index, axis=axis)\n+#             np.testing.assert_equal(z_ref_value, z_tri_value)\n+#         else:\n+#             np.testing.assert_equal(z_ref, z_tri)\n+\n+# # ---------------\n+# # test permute\n+# # ---------------\n+\n+\n+# @pytest.mark.parametrize(\"dtype_str, shape, perm\",\n+#                          [(dtype, shape, perm)\n+#                           for dtype in ['bfloat16', 'float16', 'float32']\n+#                              for shape in [(64, 64), (128, 128)]\n+#                              for perm in [(1, 0)]])\n+# def test_permute(dtype_str, shape, perm, device='cuda'):\n+#     check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n+\n+#     # triton kernel\n+#     @triton.jit\n+#     def kernel(X, stride_xm, stride_xn,\n+#                Z, stride_zm, stride_zn,\n+#                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n+#         off_m = tl.arange(0, BLOCK_M)\n+#         off_n = tl.arange(0, BLOCK_N)\n+#         Xs = X + off_m[:, None] * stride_xm + off_n[None, :] * stride_xn\n+#         Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn\n+#         tl.store(Zs, tl.load(Xs))\n+#     # input\n+#     x = numpy_random(shape, dtype_str=dtype_str)\n+#     # triton result\n+#     z_tri = to_triton(np.empty_like(x), device=device, dst_type=dtype_str)\n+#     z_tri_contiguous = to_triton(np.empty_like(x), device=device, dst_type=dtype_str)\n+#     x_tri = to_triton(x, device=device, dst_type=dtype_str)\n+#     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n+#                          z_tri, z_tri.stride(1), z_tri.stride(0),\n+#                          BLOCK_M=shape[0], BLOCK_N=shape[1])\n+#     pgm_contiguous = kernel[(1, 1)](x_tri, x_tri.stride(1), x_tri.stride(0),\n+#                                     z_tri_contiguous, z_tri_contiguous.stride(0), z_tri_contiguous.stride(1),\n+#                                     BLOCK_M=shape[0], BLOCK_N=shape[1])\n+#     # numpy result\n+#     z_ref = x.transpose(*perm)\n+#     # compare\n+#     triton.testing.assert_almost_equal(z_tri, z_ref)\n+#     triton.testing.assert_almost_equal(z_tri_contiguous, z_ref)\n+#     # parse ptx to make sure ld/st are vectorized\n+#     ptx = pgm.asm['ptx']\n+#     assert 'ld.global.v4' in ptx\n+#     assert 'st.global.v4' in ptx\n+#     ptx = pgm_contiguous.asm['ptx']\n+#     assert 'ld.global.v4' in ptx\n+#     assert 'st.global.v4' in ptx\n+\n+# # ---------------\n+# # test dot\n+# # ---------------\n+\n+\n+# @pytest.mark.parametrize(\"epilogue, allow_tf32, dtype\",\n+#                          [(epilogue, allow_tf32, dtype)\n+#                           for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols', 'softmax', 'chain-dot']\n+#                           for allow_tf32 in [True, False]\n+#                           for dtype in ['float16']\n+#                           if not (allow_tf32 and (dtype in ['float16']))])\n+# def test_dot(epilogue, allow_tf32, dtype, device='cuda'):\n+#     cc = _triton.runtime.cc(_triton.runtime.backend.CUDA, torch.cuda.current_device())\n+#     if cc < 80:\n+#         if dtype == 'int8':\n+#             pytest.skip(\"Only test int8 on devices with sm >= 80\")\n+#         elif dtype == 'float32' and allow_tf32:\n+#             pytest.skip(\"Only test tf32 on devices with sm >= 80\")\n+\n+#     M, N, K = 128, 128, 64\n+#     num_warps = 8\n+#     trans_a, trans_b = False, False\n+\n+#     # triton kernel\n+#     @triton.jit\n+#     def kernel(X, stride_xm, stride_xk,\n+#                Y, stride_yk, stride_yn,\n+#                W, stride_wn, stride_wl,\n+#                Z, stride_zm, stride_zn,\n+#                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n+#                ADD_MATRIX: tl.constexpr, ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr,\n+#                ALLOW_TF32: tl.constexpr,\n+#                DO_SOFTMAX: tl.constexpr, CHAIN_DOT: tl.constexpr,\n+#                TRANS_A: tl.constexpr, TRANS_B: tl.constexpr):\n+#         off_m = tl.arange(0, BLOCK_M)\n+#         off_n = tl.arange(0, BLOCK_N)\n+#         off_l = tl.arange(0, BLOCK_N)\n+#         off_k = tl.arange(0, BLOCK_K)\n+#         Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk\n+#         Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn\n+#         Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl\n+#         Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn\n+#         z = tl.dot(tl.load(Xs), tl.load(Ys), trans_a=TRANS_A, trans_b=TRANS_B, allow_tf32=ALLOW_TF32)\n+#         if ADD_MATRIX:\n+#             z += tl.load(Zs)\n+#         if ADD_ROWS:\n+#             ZRs = Z + off_m * stride_zm\n+#             z += tl.load(ZRs)[:, None]\n+#         if ADD_COLS:\n+#             ZCs = Z + off_n * stride_zn\n+#             z += tl.load(ZCs)[None, :]\n+#         if DO_SOFTMAX:\n+#             max = tl.max(z, 1)\n+#             z = z - max[:, None]\n+#             num = tl.exp(z)\n+#             den = tl.sum(num, 1)\n+#             z = num / den[:, None]\n+#         if CHAIN_DOT:\n+#             # tl.store(Zs, z)\n+#             # tl.debug_barrier()\n+#             z = tl.dot(z.to(tl.float16), tl.load(Ws), trans_a=TRANS_A)\n+#         tl.store(Zs, z)\n+#     # input\n+#     rs = RandomState(17)\n+#     x = numpy_random((K, M) if trans_a else (M, K), dtype_str=dtype, rs=rs) * .1\n+#     y = numpy_random((N, K) if trans_b else (K, N), dtype_str=dtype, rs=rs) * .1\n+#     w = numpy_random((N, N), dtype_str=dtype, rs=rs) * .1\n+#     if allow_tf32:\n+#         x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+#         y = (y.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+#         w = (w.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+#     x_tri = to_triton(x, device=device)\n+#     y_tri = to_triton(y, device=device)\n+#     w_tri = to_triton(w, device=device)\n+#     # triton result\n+#     z = 1 + numpy_random((M, N), dtype_str=dtype, rs=rs) * .1\n+#     z_tri = to_triton(z, device=device)\n+#     if epilogue == 'trans':\n+#         z_tri = torch.as_strided(z_tri, (M, N), z_tri.stride()[::-1])\n+#     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n+#                          y_tri, y_tri.stride(0), y_tri.stride(1),\n+#                          w_tri, w_tri.stride(0), w_tri.stride(1),\n+#                          z_tri, z_tri.stride(0), z_tri.stride(1),\n+#                          TRANS_A=trans_a, TRANS_B=trans_b,\n+#                          BLOCK_M=M, BLOCK_K=K, BLOCK_N=N,\n+#                          ADD_MATRIX=epilogue == 'add-matrix',\n+#                          ADD_ROWS=epilogue == 'add-rows',\n+#                          ADD_COLS=epilogue == 'add-cols',\n+#                          DO_SOFTMAX=epilogue == 'softmax',\n+#                          CHAIN_DOT=epilogue == 'chain-dot',\n+#                          ALLOW_TF32=allow_tf32,\n+#                          num_warps=num_warps)\n+#     # torch result\n+#     x_ref = x.T if trans_a else x\n+#     y_ref = y.T if trans_b else y\n+#     z_ref = np.matmul(x_ref, y_ref)\n+#     if epilogue == 'add-matrix':\n+#         z_ref += z\n+#     if epilogue == 'add-rows':\n+#         z_ref += z[:, 0][:, None]\n+#     if epilogue == 'add-cols':\n+#         z_ref += z[0, :][None, :]\n+#     if epilogue == 'softmax':\n+#         num = np.exp(z_ref - np.max(z_ref, axis=-1, keepdims=True))\n+#         denom = np.sum(num, axis=-1, keepdims=True)\n+#         z_ref = num / denom\n+#     if epilogue == 'chain-dot':\n+#         z_ref = np.matmul(z_ref.T if trans_a else z_ref, w)\n+#     # compare\n+#     # print(z_ref[:,0], z_tri[:,0])\n+#     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+#     # make sure ld/st are vectorized\n+#     ptx = pgm.asm['ptx']\n+#     assert 'ld.global.v4' in ptx\n+#     assert 'st.global.v4' in ptx\n+#     if allow_tf32:\n+#         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' in ptx\n+#     elif dtype == 'float32':\n+#         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' not in ptx\n+#     elif dtype == 'int8':\n+#         assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n+\n+\n+# def test_dot_without_load():\n+#     @triton.jit\n+#     def kernel(out):\n+#         pid = tl.program_id(axis=0)\n+#         a = tl.zeros((32, 32), tl.float32)\n+#         b = tl.zeros((32, 32), tl.float32)\n+#         c = tl.zeros((32, 32), tl.float32)\n+#         c = tl.dot(a, b)\n+#         pout = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n+#         tl.store(pout, c)\n+\n+#     out = torch.ones((32, 32), dtype=torch.float32, device=\"cuda\")\n+#     kernel[(1,)](out)\n+\n+# # ---------------\n+# # test arange\n+# # ---------------\n+\n+\n+# @pytest.mark.parametrize(\"start\", [0, 1, 7, 16])\n+# def test_arange(start, device='cuda'):\n+#     BLOCK = 128\n+#     z_tri = torch.empty(BLOCK, dtype=torch.int32, device=device)\n+\n+#     @triton.jit\n+#     def _kernel(z, BLOCK: tl.constexpr,\n+#                 START: tl.constexpr, END: tl.constexpr):\n+#         off = tl.arange(0, BLOCK)\n+#         val = tl.arange(START, END)\n+#         tl.store(z + off, val)\n+#     _kernel[(1,)](z_tri, START=start, END=start + BLOCK, BLOCK=BLOCK)\n+#     z_ref = torch.arange(start, BLOCK + start, dtype=torch.int32, device=device)\n+#     triton.testing.assert_almost_equal(z_tri, z_ref)\n+\n+# # ---------------\n+# # test load\n+# # ---------------\n+# # 'bfloat16': torch.bfloat16,\n+# # Testing masked loads with an intermate copy to shared memory run.\n+\n+\n+# @pytest.mark.parametrize(\"dtype\", [torch.bfloat16, torch.float16, torch.float32])\n+# def test_masked_load_shared_memory(dtype, device='cuda'):\n+#     check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n+\n+#     M = 32\n+#     N = 32\n+#     K = 16\n+\n+#     in1 = torch.rand((M, K), dtype=dtype, device=device)\n+#     in2 = torch.rand((K, N), dtype=dtype, device=device)\n+#     out = torch.zeros((M, N), dtype=dtype, device=device)\n+\n+#     @triton.jit\n+#     def _kernel(in1_ptr, in2_ptr, output_ptr,\n+#                 in_stride, in2_stride, out_stride,\n+#                 in_numel, in2_numel, out_numel,\n+#                 M: tl.constexpr, N: tl.constexpr, K: tl.constexpr):\n+\n+#         M_offsets = tl.arange(0, M)\n+#         N_offsets = tl.arange(0, N)\n+#         K_offsets = tl.arange(0, K)\n+\n+#         in_offsets = M_offsets[:, None] * in_stride + K_offsets[None, :]\n+#         in2_offsets = K_offsets[:, None] * in2_stride + N_offsets[None, :]\n+\n+#         # Load inputs.\n+#         x = tl.load(in1_ptr + in_offsets, mask=in_offsets < in_numel)\n+#         w = tl.load(in2_ptr + in2_offsets, mask=in2_offsets < in2_numel)\n+\n+#         # Without a dot product the memory doesn't get promoted to shared.\n+#         o = tl.dot(x, w)\n+\n+#         # Store output\n+#         output_offsets = M_offsets[:, None] * out_stride + N_offsets[None, :]\n+#         tl.store(output_ptr + output_offsets, o, mask=output_offsets < in2_numel)\n+\n+#     pgm = _kernel[(1,)](in1, in2, out,\n+#                         in1.stride()[0],\n+#                         in2.stride()[0],\n+#                         out.stride()[0],\n+#                         in1.numel(),\n+#                         in2.numel(),\n+#                         out.numel(),\n+#                         M=M, N=N, K=K)\n+\n+#     reference_out = torch.matmul(in1, in2)\n+#     triton.testing.allclose(out, reference_out)\n+\n+\n+# @pytest.mark.parametrize(\"cache\", [\"\", \".ca\", \".cg\"])\n+# def test_load_cache_modifier(cache):\n+#     src = torch.empty(128, device='cuda')\n+#     dst = torch.empty(128, device='cuda')\n+\n+#     @triton.jit\n+#     def _kernel(dst, src, CACHE: tl.constexpr):\n+#         offsets = tl.arange(0, 128)\n+#         x = tl.load(src + offsets, cache_modifier=CACHE)\n+#         tl.store(dst + offsets, x)\n+\n+#     pgm = _kernel[(1,)](dst, src, CACHE=cache)\n+#     ptx = pgm.asm['ptx']\n+#     if cache == '':\n+#         assert 'ld.global.ca' not in ptx\n+#         assert 'ld.global.cg' not in ptx\n+#     if cache == '.cg':\n+#         assert 'ld.global.cg' in ptx\n+#         assert 'ld.global.ca' not in ptx\n+#     if cache == '.ca':\n+#         assert 'ld.global.ca' in ptx\n+#         assert 'ld.global.cg' not in ptx\n+\n+\n+# @pytest.mark.parametrize(\"N\", [16, 10, 11, 1024])\n+# def test_vectorization(N):\n+#     src = torch.empty(1024, device='cuda')\n+#     dst = torch.empty(1024, device='cuda')\n+\n+#     @triton.jit\n+#     def _kernel(dst, src, N, BLOCK_SIZE: tl.constexpr):\n+#         offsets = tl.program_id(0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+#         x = tl.load(src + offsets, mask=offsets < N)\n+#         tl.store(dst + offsets, x, mask=offsets < N)\n+#     pgm = _kernel[(1,)](dst, src, N=N, BLOCK_SIZE=src.shape[0])\n+#     ptx = pgm.asm[\"ptx\"]\n+#     if N % 16 == 0:\n+#         assert \"ld.global.v4.b32\" in ptx\n+#     else:\n+#         assert \"ld.global.b32\" in ptx\n+#     # triton.testing.assert_almost_equal(dst, src[:N])\n+# # ---------------\n+# # test store\n+# # ---------------\n+\n+# # ---------------\n+# # test if\n+# # ---------------\n+\n+# # ---------------\n+# # test for\n+# # ---------------\n+\n+# # ---------------\n+# # test while\n+# # ---------------\n+\n+# # ---------------\n+# # test default\n+# # ---------------\n+# # TODO: can't be local to test_default\n+\n+\n+# @triton.jit\n+# def _impl(value=10):\n+#     return value\n+\n+\n+# def test_default():\n+#     value = 5\n+#     ret0 = torch.zeros(1, dtype=torch.int32, device='cuda')\n+#     ret1 = torch.zeros(1, dtype=torch.int32, device='cuda')\n+\n+#     @triton.jit\n+#     def _kernel(ret0, ret1, value):\n+#         tl.store(ret0, _impl())\n+#         tl.store(ret1, _impl(value))\n+\n+#     _kernel[(1,)](ret0, ret1, value)\n+#     assert ret0.item() == 10\n+#     assert ret1.item() == value\n+\n+# # ---------------\n+# # test noop\n+# # ----------------\n+\n+\n+# def test_noop(device='cuda'):\n+#     @triton.jit\n+#     def kernel(x):\n+#         pass\n+#     x = to_triton(numpy_random((1,), dtype_str='int32'), device=device)\n+#     kernel[(1, )](x)\n+\n+\n+# @pytest.mark.parametrize(\"value, value_type\", [\n+#     (-1, 'i32'), (0, 'i32'), (-2**31, 'i32'), (2**31 - 1, 'i32'),\n+#     (2**31, 'u32'), (2**32 - 1, 'u32'), (2**32, 'i64'), (2**63 - 1, 'i64'),\n+#     (-2**63, 'i64'), (2**63, 'u64'), (2**64 - 1, 'u64')\n+# ])\n+# def test_value_specialization(value: int, value_type: str, device='cuda') -> None:\n+#     spec_type = None\n+\n+#     def cache_hook(*args, **kwargs):\n+#         nonlocal spec_type\n+#         spec_type = kwargs[\"compile\"][\"signature\"][0]\n+#     JITFunction.cache_hook = cache_hook\n+\n+#     @triton.jit\n+#     def kernel(VALUE, X):\n+#         pass\n+\n+#     x = torch.tensor([3.14159], device='cuda')\n+#     pgm = kernel[(1, )](value, x)\n+\n+#     JITFunction.cache_hook = None\n+#     assert spec_type == value_type\n+\n+\n+# @pytest.mark.parametrize(\n+#     \"value, overflow\",\n+#     [(2**64 - 1, False), (2**64, True), (-2**63, False), (-2**63 - 1, True)]\n+# )\n+# def test_value_specialization_overflow(value: int, overflow: bool, device='cuda') -> None:\n+\n+#     @triton.jit\n+#     def kernel(VALUE, X):\n+#         pass\n+\n+#     x = torch.tensor([3.14159], device='cuda')\n+\n+#     if overflow:\n+#         with pytest.raises(OverflowError):\n+#             kernel[(1, )](value, x)\n+#     else:\n+#         kernel[(1, )](value, x)\n+\n+\n+# # ----------------\n+# # test constexpr\n+# # ----------------\n+\n+# @pytest.mark.parametrize(\"op\", ['+', '-', '*', '/', '%', '<', '>'])\n+# @pytest.mark.parametrize(\"is_lhs_constexpr\", [False, True])\n+# @pytest.mark.parametrize(\"is_rhs_constexpr\", [True, False])\n+# def test_bin_op_constexpr(op, is_lhs_constexpr, is_rhs_constexpr):\n+\n+#     @triton.jit\n+#     def kernel(Z, X, Y):\n+#         x = tl.load(X)\n+#         y = tl.load(Y)\n+#         z = GENERATE_TEST_HERE\n+#         tl.store(Z, z)\n+\n+#     x_str = \"3.14\" if is_lhs_constexpr else \"x\"\n+#     y_str = \"4.13\" if is_rhs_constexpr else \"y\"\n+#     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f\"{x_str} {op} {y_str}\"})\n+#     x = numpy_random((1,), dtype_str=\"float32\")\n+#     y = numpy_random((1,), dtype_str=\"float32\")\n+#     z = np.array(eval(f\"{x_str} {op} {y_str}\"))\n+#     x_tri = to_triton(x)\n+#     y_tri = to_triton(y)\n+#     z_tri = to_triton(np.empty((1,), dtype=z.dtype))\n+#     kernel[(1,)](z_tri, x_tri, y_tri)\n+#     np.testing.assert_allclose(z, to_numpy(z_tri))\n+\n+\n+# def test_constexpr_shape():\n+\n+#     @triton.jit\n+#     def kernel(X):\n+#         off = tl.arange(0, 128 + 128)\n+#         tl.store(X + off, off)\n+\n+#     x_tri = to_triton(np.empty((256, ), dtype=np.int32))\n+#     kernel[(1,)](x_tri)\n+#     np.testing.assert_equal(to_numpy(x_tri), np.arange(0, 256))\n+\n+\n+# def test_constexpr_scalar_shape():\n+\n+#     @triton.jit\n+#     def kernel(X, s):\n+#         off = tl.arange(0, 256)\n+#         val = off % (256 // s)\n+#         tl.store(X + off, val)\n+\n+#     x_tri = to_triton(np.empty((256, ), dtype=np.int32))\n+#     kernel[(1,)](x_tri, 32)\n+#     np.testing.assert_equal(to_numpy(x_tri), np.arange(0, 256) % 8)\n+\n+# # -------------\n+# # test call\n+# # -------------\n+\n+\n+# @triton.jit\n+# def val_multiplier(val, i):\n+#     return val * i\n+\n+\n+# @triton.jit\n+# def vecmul_kernel(ptr, n_elements, rep):\n+#     pid = tl.program_id(axis=0)\n+#     offsets = pid * 128 + tl.arange(0, 128)\n+#     mask = offsets < n_elements\n+#     vec = tl.load(ptr + offsets, mask=mask)\n+#     for i in range(1, rep):\n+#         vec = val_multiplier(vec, i)\n+#     tl.store(ptr + offsets, vec, mask=mask)\n+\n+\n+# def test_call():\n+\n+#     @triton.jit\n+#     def kernel(ptr, n_elements, num1, num2):\n+#         vecmul_kernel(ptr, n_elements, num1)\n+#         vecmul_kernel(ptr, n_elements, num2)\n+\n+#     size = 1024\n+#     rand_val = numpy_random((size,), dtype_str=\"float32\")\n+#     rand_val_tri = to_triton(rand_val, device='cuda')\n+#     kernel[(size // 128,)](rand_val_tri, size, 3, 5)\n+\n+#     ans = rand_val * 1 * 2 * 1 * 2 * 3 * 4\n+#     np.testing.assert_equal(to_numpy(rand_val_tri), ans)\n+\n+# # -------------\n+# # test if\n+# # -------------\n+\n+\n+# def test_if():\n+\n+#     @triton.jit\n+#     def kernel(Cond, XTrue, XFalse, Ret):\n+#         pid = tl.program_id(0)\n+#         cond = tl.load(Cond)\n+#         if pid % 2:\n+#             tl.store(Ret, tl.load(XTrue))\n+#         else:\n+#             tl.store(Ret, tl.load(XFalse))\n+\n+#     cond = torch.ones(1, dtype=torch.int32, device='cuda')\n+#     x_true = torch.tensor([3.14], dtype=torch.float32, device='cuda')\n+#     x_false = torch.tensor([1.51], dtype=torch.float32, device='cuda')\n+#     ret = torch.empty(1, dtype=torch.float32, device='cuda')\n+#     kernel[(1,)](cond, x_true, x_false, ret)\n+\n+\n+# def test_num_warps_pow2():\n+#     dst = torch.empty(128, device='cuda')\n+\n+#     @triton.jit\n+#     def _kernel(dst):\n+#         pass\n+\n+#     with pytest.raises(AssertionError, match='must be a power of 2'):\n+#         _kernel[(1,)](dst=dst, num_warps=3)\n+#     _kernel[(1,)](dst=dst, num_warps=1)\n+#     _kernel[(1,)](dst=dst, num_warps=2)\n+#     _kernel[(1,)](dst=dst, num_warps=4)\n+\n+# # -------------\n+# # test extern\n+# # -------------\n+\n+\n+# @pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n+#                          [('int32', 'libdevice.ffs', ''),\n+#                           ('float32', 'libdevice.pow', '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'),\n+#                           ('float64', 'libdevice.norm4d', '')])\n+# def test_libdevice(dtype_str, expr, lib_path):\n+\n+#     @triton.jit\n+#     def kernel(X, Y, BLOCK: tl.constexpr):\n+#         x = tl.load(X + tl.arange(0, BLOCK))\n+#         y = GENERATE_TEST_HERE\n+#         tl.store(Y + tl.arange(0, BLOCK), y)\n+\n+#     shape = (128, )\n+#     rs = RandomState(17)\n+#     # limit the range of integers so that the sum does not overflow\n+#     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n+\n+#     if expr == 'libdevice.ffs':\n+#         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.ffs(x)'})\n+#         y_ref = np.zeros(shape, dtype=x.dtype)\n+#         for i in range(shape[0]):\n+#             y_ref[i] = (int(x[i]) & int(-x[i])).bit_length()\n+#     elif expr == 'libdevice.pow':\n+#         # numpy does not allow negative factors in power, so we use abs()\n+#         x = np.abs(x)\n+#         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.pow(x, x)'})\n+#         y_ref = np.power(x, x)\n+#     elif expr == 'libdevice.norm4d':\n+#         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.norm4d(x, x, x, x)'})\n+#         y_ref = np.sqrt(4 * np.power(x, 2))\n+\n+#     x_tri = to_triton(x)\n+#     # triton result\n+#     y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n+#     kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n+#     # compare\n+#     if expr == 'libdevice.ffs':\n+#         np.testing.assert_equal(y_ref, to_numpy(y_tri))\n+#     else:\n+#         np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)"}]