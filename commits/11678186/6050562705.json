[{"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "modified", "additions": 21, "deletions": 7, "changes": 28, "file_content_changes": "@@ -38,7 +38,8 @@ typedef DenseMap<Value, std::function<Type(Type)>> LayoutMap;\n \n struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n   Attribute getCoalescedEncoding(ModuleAxisInfoAnalysis &axisInfoAnalysis,\n-                                 Value ptr, int numWarps, int threadsPerWarp) {\n+                                 Value ptr, Operation *op, int numWarps,\n+                                 int threadsPerWarp) {\n     auto refType = ptr.getType();\n     if (refType.isa<PointerType>())\n       refType = refType.cast<PointerType>().getPointeeType();\n@@ -147,7 +148,20 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n       unsigned currPerThread = std::min(alignment, 128 / elemNumBits);\n       perThread = std::max(perThread, currPerThread);\n     }\n-    sizePerThread[order[0]] = std::min<int>(perThread, numElemsPerThread);\n+\n+    perThread = std::min<int>(perThread, numElemsPerThread);\n+\n+    if (!dyn_cast<triton::LoadOp>(op)) {\n+      // For ops that can result in a global memory write, we should enforce\n+      // that each thread handles at most 128 bits, which is the widest\n+      // available vectorized store op; otherwise, the store will have \"gaps\"\n+      // in the memory write at the warp level, resulting in worse performance.\n+      // For loads, we can expect that the gaps won't matter due to the L1\n+      // cache.\n+      unsigned elemNumBits = getElementBitWidth(ptr);\n+      perThread = std::min<int>(perThread, 128 / elemNumBits);\n+    }\n+    sizePerThread[order[0]] = perThread;\n \n     auto CTALayout = triton::gpu::getCTALayout(refTensorType.getEncoding());\n     return triton::gpu::BlockedEncodingAttr::get(\n@@ -157,9 +171,9 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n \n   std::function<Type(Type)>\n   getTypeConverter(ModuleAxisInfoAnalysis &axisInfoAnalysis, Value ptr,\n-                   int numWarps, int threadsPerWarp) {\n-    Attribute encoding =\n-        getCoalescedEncoding(axisInfoAnalysis, ptr, numWarps, threadsPerWarp);\n+                   Operation *op, int numWarps, int threadsPerWarp) {\n+    Attribute encoding = getCoalescedEncoding(axisInfoAnalysis, ptr, op,\n+                                              numWarps, threadsPerWarp);\n     return [encoding](Type type) {\n       RankedTensorType tensorType = type.cast<RankedTensorType>();\n       return RankedTensorType::get(tensorType.getShape(),\n@@ -265,8 +279,8 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n       int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n       int threadsPerWarp =\n           triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n-      auto convertType =\n-          getTypeConverter(axisInfoAnalysis, ptr, numWarps, threadsPerWarp);\n+      auto convertType = getTypeConverter(axisInfoAnalysis, ptr, curr, numWarps,\n+                                          threadsPerWarp);\n       layoutMap[ptr] = convertType;\n     });\n "}, {"filename": "test/TritonGPU/coalesce.mlir", "status": "modified", "additions": 38, "deletions": 3, "changes": 41, "file_content_changes": "@@ -75,9 +75,9 @@ tt.func @load_tensor(%arg0: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32}, %arg1:\n #blocked = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n module attributes {\"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n \n-// CHECK-NOT: sizePerThread = [4]\n-// CHECK: #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n-// CHECK-NOT: sizePerThread = [4]\n+\n+// CHECK: [[NARROW_LAYOUT:#.*]] = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+// CHECK: [[WIDE_LAYOUT:#.*]] = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n tt.func public @load_tensors_two_types(%arg0: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32}, %arg3: i32) attributes {noinline = false} {\n     %c1024_i32 = arith.constant 1024 : i32\n     %0 = tt.get_program_id x : i32\n@@ -97,8 +97,43 @@ tt.func public @load_tensors_two_types(%arg0: !tt.ptr<f32, 1> {tt.divisibility =\n     %14 = arith.addf %9, %13 : tensor<1024xf32, #blocked>\n     %15 = tt.splat %arg2 : (!tt.ptr<f32, 1>) -> tensor<1024x!tt.ptr<f32, 1>, #blocked>\n     %16 = tt.addptr %15, %4 : tensor<1024x!tt.ptr<f32, 1>, #blocked>, tensor<1024xi32, #blocked>\n+    // CHECK: tt.store {{.*}} : tensor<1024xf32, [[WIDE_LAYOUT]]>\n     tt.store %16, %14, %6 {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf32, #blocked>\n     tt.return\n }\n \n }\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+module attributes {\"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+\n+// CHECK-NOT: sizePerThread = [4]\n+// CHECK: #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+// CHECK-NOT: sizePerThread = [4]\n+tt.func public @load_tensors_two_types(%arg0: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg3: i32) attributes {noinline = false} {\n+    %c1024_i32 = arith.constant 1024 : i32\n+    %0 = tt.get_program_id x : i32\n+    %1 = arith.muli %0, %c1024_i32 : i32\n+    %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #blocked>\n+    %3 = tt.splat %1 : (i32) -> tensor<1024xi32, #blocked>\n+    %4 = arith.addi %3, %2 : tensor<1024xi32, #blocked>\n+    %5 = tt.splat %arg3 : (i32) -> tensor<1024xi32, #blocked>\n+    %6 = \"triton_gpu.cmpi\"(%4, %5) <{predicate = 2 : i64}> : (tensor<1024xi32, #blocked>, tensor<1024xi32, #blocked>) -> tensor<1024xi1, #blocked>\n+    %7 = tt.splat %arg0 : (!tt.ptr<f32, 1>) -> tensor<1024x!tt.ptr<f32, 1>, #blocked>\n+    %8 = tt.addptr %7, %4 : tensor<1024x!tt.ptr<f32, 1>, #blocked>, tensor<1024xi32, #blocked>\n+    %9 = tt.load %8, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked>\n+    %10 = tt.splat %arg1 : (!tt.ptr<f16, 1>) -> tensor<1024x!tt.ptr<f16, 1>, #blocked>\n+    %11 = tt.addptr %10, %4 : tensor<1024x!tt.ptr<f16, 1>, #blocked>, tensor<1024xi32, #blocked>\n+    %12 = tt.load %11, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf16, #blocked>\n+    %13 = arith.extf %12 : tensor<1024xf16, #blocked> to tensor<1024xf32, #blocked>\n+    %14 = arith.addf %9, %13 : tensor<1024xf32, #blocked>\n+    %15 = tt.splat %arg2 : (!tt.ptr<f16, 1>) -> tensor<1024x!tt.ptr<f16, 1>, #blocked>\n+    %16 = tt.addptr %15, %4 : tensor<1024x!tt.ptr<f16, 1>, #blocked>, tensor<1024xi32, #blocked>\n+    %17 = arith.truncf %14 : tensor<1024xf32, #blocked> to tensor<1024xf16, #blocked>\n+    tt.store %16, %17, %6 {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf16, #blocked>\n+    tt.return\n+}\n+\n+}"}]