[{"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 186, "deletions": 0, "changes": 186, "file_content_changes": "@@ -22,6 +22,10 @@\n using namespace mlir;\n namespace {\n #include \"TritonGPUCombine.inc\"\n+using triton::DotOp;\n+using triton::gpu::ConvertLayoutOp;\n+using triton::gpu::DotOperandEncodingAttr;\n+using triton::gpu::MmaEncodingAttr;\n \n // -----------------------------------------------------------------------------\n //\n@@ -1019,6 +1023,7 @@ class OptimizeConvertToDotOperand : public mlir::RewritePattern {\n         dstDotOperandLayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n     if ((order[0] == 1 && isMMAv1Row) || (order[0] == 0 && !isMMAv1Row))\n       return failure();\n+\n     auto newIsRow = BoolAttr::get(op->getContext(), !isMMAv1Row);\n     auto newDstEncoding = triton::gpu::DotOperandEncodingAttr::get(\n         op->getContext(), dstDotOperandLayout.getOpIdx(),\n@@ -1195,6 +1200,170 @@ class FixupLoop : public mlir::RewritePattern {\n   }\n };\n \n+// This pattern collects the Mma those need to update and create the new\n+// layouts.\n+class CollectMmaToUpdateForVolta : public mlir::RewritePattern {\n+  DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n+\n+public:\n+  CollectMmaToUpdateForVolta(\n+      mlir::MLIRContext *ctx,\n+      DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n+      : mlir::RewritePattern(triton::DotOp::getOperationName(), 1, ctx),\n+        mmaToUpdate(mmaToUpdate) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+\n+    auto dotOp = cast<triton::DotOp>(op);\n+    auto *ctx = dotOp->getContext();\n+    auto AT = dotOp.a().getType().cast<RankedTensorType>();\n+    auto BT = dotOp.b().getType().cast<RankedTensorType>();\n+    auto DT = dotOp.d().getType().cast<RankedTensorType>();\n+    auto mmaLayout = DT.getEncoding().dyn_cast<MmaEncodingAttr>();\n+    if (!(mmaLayout && mmaLayout.isVolta()))\n+      return failure();\n+\n+    // Has processed.\n+    if (mmaToUpdate.count(mmaLayout))\n+      return failure();\n+\n+    // NOTE Should run after the BlockedToMMA pattern.\n+    auto dotOperandA = AT.getEncoding().cast<DotOperandEncodingAttr>();\n+    auto dotOperandB = BT.getEncoding().cast<DotOperandEncodingAttr>();\n+    // NOTE Should run after OptimizeConvertToDotOperand pattern.\n+    bool isARow = dotOperandA.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    bool isBRow = dotOperandB.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    auto [isARow_, isBRow_, isAVec4, isBVec4] =\n+        mmaLayout.decodeVoltaLayoutStates();\n+    if (isARow_ == isARow && isBRow_ == isBRow) {\n+      return failure(); // No need to update\n+    }\n+\n+    auto newMmaLayout = MmaEncodingAttr::get(\n+        ctx, mmaLayout.getVersionMajor(), mmaLayout.getWarpsPerCTA(),\n+        AT.getShape(), BT.getShape(), isARow, isBRow);\n+\n+    // need to update\n+    mmaToUpdate.try_emplace(mmaLayout, newMmaLayout);\n+\n+    return failure();\n+  }\n+};\n+\n+class UpdateMMAVersionMinorForVolta : public mlir::RewritePattern {\n+  const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n+\n+public:\n+  UpdateMMAVersionMinorForVolta(\n+      mlir::MLIRContext *ctx,\n+      const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n+      : RewritePattern(MatchAnyOpTypeTag{}, 1 /*benefit*/, ctx),\n+        mmaToUpdate(mmaToUpdate) {}\n+\n+  LogicalResult match(Operation *op) const override {\n+    if (op->getNumResults() != 1)\n+      return failure();\n+    auto tensorTy = op->getResult(0).getType().dyn_cast<RankedTensorType>();\n+    if (!tensorTy)\n+      return failure();\n+    bool hit{};\n+    if (auto cvt = llvm::dyn_cast<ConvertLayoutOp>(op)) {\n+      if (auto dotOperand =\n+              tensorTy.getEncoding().dyn_cast<DotOperandEncodingAttr>()) {\n+        if (auto mma = dotOperand.getParent().dyn_cast<MmaEncodingAttr>())\n+          hit = mmaToUpdate.count(mma);\n+      } else if (auto mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>())\n+        hit = mmaToUpdate.count(mma);\n+    } else if (auto dot = llvm::dyn_cast<DotOp>(op)) {\n+      auto mma = dot.d()\n+                     .getType()\n+                     .cast<RankedTensorType>()\n+                     .getEncoding()\n+                     .dyn_cast<MmaEncodingAttr>();\n+      if (mma)\n+        hit = mmaToUpdate.count(mma);\n+    } else if (auto constant = llvm::dyn_cast<arith::ConstantOp>(op)) {\n+      if (auto mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>())\n+        hit = mmaToUpdate.count(mma);\n+    }\n+\n+    return failure(!hit);\n+  }\n+\n+  void rewrite(Operation *op, PatternRewriter &rewriter) const override {\n+    if (auto cvt = llvm::dyn_cast<ConvertLayoutOp>(op)) {\n+      auto tensorTy = cvt.result().getType().cast<RankedTensorType>();\n+      if (tensorTy.getEncoding().dyn_cast<DotOperandEncodingAttr>())\n+        rewriteCvtDotOp(op, rewriter);\n+      else\n+        rewriteCvtMma(op, rewriter);\n+    } else if (auto dot = llvm::dyn_cast<DotOp>(op))\n+      rewriteDot(op, rewriter);\n+    else if (llvm::dyn_cast<arith::ConstantOp>(op))\n+      rewriteConstant(op, rewriter);\n+  }\n+\n+private:\n+  void rewriteCvtDotOp(Operation *op, PatternRewriter &rewriter) const {\n+    auto *ctx = op->getContext();\n+    auto cvt = llvm::cast<ConvertLayoutOp>(op);\n+    auto tensorTy = cvt.result().getType().cast<RankedTensorType>();\n+    auto dotOperand = tensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n+    MmaEncodingAttr newMma =\n+        mmaToUpdate.lookup(dotOperand.getParent().cast<MmaEncodingAttr>());\n+    auto newDotOperand = DotOperandEncodingAttr::get(\n+        ctx, dotOperand.getOpIdx(), newMma, dotOperand.getIsMMAv1Row());\n+    auto newTensorTy = RankedTensorType::get(\n+        tensorTy.getShape(), tensorTy.getElementType(), newDotOperand);\n+    rewriter.replaceOpWithNewOp<ConvertLayoutOp>(op, newTensorTy,\n+                                                 cvt.getOperand());\n+  }\n+\n+  void rewriteDot(Operation *op, PatternRewriter &rewriter) const {\n+    auto *ctx = op->getContext();\n+    auto dot = llvm::cast<DotOp>(op);\n+    auto tensorTy = dot.d().getType().cast<RankedTensorType>();\n+    auto mma = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n+    auto newMma = mmaToUpdate.lookup(mma);\n+    auto newTensorTy = RankedTensorType::get(tensorTy.getShape(),\n+                                             tensorTy.getElementType(), newMma);\n+    rewriter.replaceOpWithNewOp<DotOp>(op, newTensorTy, dot.a(), dot.b(),\n+                                       dot.c(), dot.allowTF32());\n+  }\n+\n+  void rewriteCvtMma(Operation *op, PatternRewriter &rewriter) const {\n+    auto *ctx = op->getContext();\n+    auto cvt = llvm::cast<ConvertLayoutOp>(op);\n+    auto tensorTy = cvt.result().getType().cast<RankedTensorType>();\n+    auto mma = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n+    auto newMma = mmaToUpdate.lookup(mma);\n+    auto newTensorTy = RankedTensorType::get(tensorTy.getShape(),\n+                                             tensorTy.getElementType(), newMma);\n+    rewriter.replaceOpWithNewOp<ConvertLayoutOp>(op, newTensorTy,\n+                                                 cvt.getOperand());\n+  }\n+\n+  void rewriteConstant(Operation *op, PatternRewriter &rewriter) const {\n+    auto *ctx = op->getContext();\n+    auto constant = llvm::cast<arith::ConstantOp>(op);\n+    auto tensorTy = constant.getResult().getType().dyn_cast<RankedTensorType>();\n+    auto mma = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n+    auto newMma = mmaToUpdate.lookup(mma);\n+    auto newTensorTy = RankedTensorType::get(tensorTy.getShape(),\n+                                             tensorTy.getElementType(), newMma);\n+    if (auto attr = constant.getValue().dyn_cast<SplatElementsAttr>()) {\n+      auto newRet =\n+          SplatElementsAttr::get(newTensorTy, attr.getSplatValue<Attribute>());\n+      rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, newTensorTy, newRet);\n+      return;\n+    }\n+\n+    assert(false && \"Not supported ConstantOp value type\");\n+  }\n+};\n+\n } // namespace\n \n #define GEN_PASS_CLASSES\n@@ -1229,6 +1398,23 @@ class TritonGPUCombineOpsPass\n       signalPassFailure();\n     }\n \n+    llvm::DenseMap<MmaEncodingAttr, MmaEncodingAttr> mmaToUpdate;\n+    {\n+      mlir::RewritePatternSet patterns(context);\n+      patterns.add<CollectMmaToUpdateForVolta>(context, mmaToUpdate);\n+      if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n+        signalPassFailure();\n+    }\n+    {\n+      mlir::RewritePatternSet patterns(context);\n+      patterns.add<UpdateMMAVersionMinorForVolta>(context, mmaToUpdate);\n+      mlir::GreedyRewriteConfig config;\n+      config.useTopDownTraversal = true;\n+\n+      if (applyPatternsAndFoldGreedily(m, std::move(patterns), config).failed())\n+        signalPassFailure();\n+    }\n+\n     mlir::RewritePatternSet loopFixup(context);\n     loopFixup.add<FixupLoop>(context);\n     if (applyPatternsAndFoldGreedily(m, std::move(loopFixup)).failed()) {"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 30, "deletions": 1, "changes": 31, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -tritongpu-combine 2>&1 | FileCheck %s\n+// RUN: triton-opt -split-input-file %s -tritongpu-combine 2>&1 | FileCheck %s\n \n #layout0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n #layout1 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n@@ -184,3 +184,32 @@ func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f3\n   tt.store %21, %22 : tensor<256xf32, #layout1>\n   return\n }\n+\n+\n+// -----\n+\n+// check the UpdateMMAVersionMinorForVolta pattern\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n+#shared0 = #triton_gpu.shared<{vec = 1, perPhase=2, maxPhase=8 ,order = [1, 0]}>\n+#mma0 = #triton_gpu.mma<{versionMajor=1, versionMinor=0, warpsPerCTA=[1,1]}>\n+// Here, the isMMAv1Row of a and b's dot_operands mismatch #mma0's versionMinor,\n+// and the pattern should update the versionMinor.\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, isMMAv1Row=true}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n+// It creates a new MMA layout to fit with $a and $b's dot_operand\n+// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 11, warpsPerCTA = [1, 1]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: dot_mmav1\n+  func @dot_mmav1(%A: tensor<16x16xf16, #blocked0>, %B: tensor<16x16xf16, #blocked0>) -> tensor<16x16xf32, #blocked0> {\n+    %C = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #blocked0>\n+    %AA = triton_gpu.convert_layout %A : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #dot_operand_a>\n+    %BB = triton_gpu.convert_layout %B : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #dot_operand_b>\n+    %CC = triton_gpu.convert_layout %C : (tensor<16x16xf32, #blocked0>) -> tensor<16x16xf32, #mma0>\n+\n+    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, %cst {allowTF32 = true} : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma]], isMMAv1Row = true}>> * tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma]], isMMAv1Row = true}>> -> tensor<16x16xf32, [[new_mma]]>\n+    %D = tt.dot %AA, %BB, %CC {allowTF32 = true} : tensor<16x16xf16, #dot_operand_a> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n+    %res = triton_gpu.convert_layout %D : (tensor<16x16xf32, #mma0>) -> tensor<16x16xf32, #blocked0>\n+\n+    return %res : tensor<16x16xf32, #blocked0>\n+  }\n+}"}]