[{"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -295,6 +295,18 @@ def TT_CatOp : TT_Op<\"cat\", [NoSideEffect,\n     let assemblyFormat = \"$lhs `,` $rhs attr-dict `:` functional-type(operands, results)\";\n }\n \n+def TT_TransOp : TT_Op<\"trans\", [NoSideEffect,\n+                               SameOperandsAndResultElementType]> {\n+\n+    let summary = \"transpose a tensor\";\n+\n+    let arguments = (ins TT_Tensor:$src);\n+\n+    let results = (outs TT_Tensor:$result);\n+\n+    let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n+}\n+\n //\n // SPMD Ops\n //"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 23, "deletions": 23, "changes": 46, "file_content_changes": "@@ -293,37 +293,37 @@ partitioned between warps.\n // -------------------------------- version = 1 --------------------------- //\n \n For first-gen tensor cores, the implicit warpTileSize is [16, 16].\n-Information about this layout can be found in the official PTX documentation\n+Note: the layout is different from the recommended in PTX ISA\n https://docs.nvidia.com/cuda/parallel-thread-execution/index.html\n (mma.884 section, FP32 accumulator).\n \n For example, the matrix L corresponding to blockTileSize=[32,16] is:\n \n                                warp 0\n --------------------------------/\\-------------------------------\n-[ 0   0   2   2   0   0   2   2    4   4   6   6   4   4   6   6 ]\n-[ 1   1   3   3   1   1   3   3    5   5   7   7   5   5   7   7 ]\n-[ 0   0   2   2   0   0   2   2    4   4   6   6   4   4   6   6 ]\n-[ 1   1   3   3   1   1   3   3    5   5   7   7   5   5   7   7 ]\n-[ 16  16  18  18  16  16  18  18   20  20  22  22  20  20  22  22]\n-[ 17  17  19  19  17  17  19  19   21  21  23  23  21  21  23  23]\n-[ 16  16  18  18  16  16  18  18   20  20  22  22  20  20  22  22]\n-[ 17  17  19  19  17  17  19  19   21  21  23  23  21  21  23  23]\n-[ 8   8   10  10  8   8   10  10   12  12  14  14  12  12  14  14]\n-[ 9   9   11  11  9   9   11  11   13  13  15  15  13  13  15  15]\n-[ ..............................................................\n-[ ..............................................................\n-[ 24  24  26  26  24  24  26  26   28  28  30  30  28  28  30  30]\n-[ 25  25  27  27  25  25  27  27   29  29  31  31  29  29  31  31]\n-\n-                         warp 1 = warp0 + 32\n+[ 0   0   2   2   8   8   10  10   0   0   2   2   8   8   10  10 ]\n+[ 1   1   3   3   9   9   11  11   1   1   3   3   9   9   11  11 ]\n+[ 0   0   2   2   8   8   10  10   0   0   2   2   8   8   10  10 ]\n+[ 1   1   3   3   9   9   11  11   1   1   3   3   9   9   11  11 ]\n+[ 4   4   6   6   12  12  14  14   4   4   6   6   12  12  14  14 ]\n+[ 5   5   7   7   13  13  15  15   5   5   7   7   13  13  15  15 ]\n+[ 4   4   6   6   12  12  14  14   4   4   6   6   12  12  14  14 ]\n+[ 5   5   7   7   13  13  15  15   5   5   7   7   13  13  15  15 ]\n+[ 16  16  18  18  20  20  22  22   16  16  18  18  20  20  22  22 ]\n+[ 17  17  19  19  21  21  23  23   17  17  19  19  21  21  23  23 ]\n+[ 16  16  18  18  20  20  22  22   16  16  18  18  20  20  22  22 ]\n+[ 17  17  19  19  21  21  23  23   17  17  19  19  21  21  23  23 ]\n+[ 24  24  26  26  28  28  30  30   24  24  26  26  28  28  30  30 ]\n+[ 25  25  27  27  29  29  31  31   25  25  27  27  29  29  31  31 ]\n+[ 24  24  26  26  28  28  30  30   24  24  26  26  28  28  30  30 ]\n+[ 25  25  27  27  29  29  31  31   25  25  27  27  29  29  31  31 ]\n+\n+                          warp 1 = warp0 + 32\n --------------------------------/\\-------------------------------\n-[ 32  32  34  34  32  32  34  34   36  36  38  38  36  36  38  38]\n-[ 33  33  35  35  33  33  35  35   37  37  39  39  37  37  39  39]\n-[ ..............................................................\n-[ ..............................................................\n-[ 56  56  58  58  56  56  58  58   60  60  62  62  60  60  62  62]\n-[ 57  57  59  59  57  57  59  59   61  61  63  63  61  61  63  63]\n+[ 32  32  34  34  40  40  42  42   32  32  34  34  40  40  42  42 ]\n+[ 33  33  35  35  41  41  43  43   33  33  35  35  41  41  43  43 ]\n+[ ............................................................... ]\n+\n \n // -------------------------------- version = 2 --------------------------- //\n "}, {"filename": "lib/Analysis/Alias.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -26,7 +26,7 @@ ChangeResult SharedMemoryAliasAnalysis::visitOperation(\n     // These ops may allocate a new shared memory buffer.\n     auto result = op->getResult(0);\n     // FIXME(Keren): extract and insert are always alias for now\n-    if (isa<tensor::ExtractSliceOp>(op)) {\n+    if (isa<tensor::ExtractSliceOp, triton::TransOp>(op)) {\n       // extract_slice %src\n       aliasInfo = AliasInfo(operands[0]->getValue());\n       pessimistic = false;"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -105,7 +105,7 @@ bool maybeSharedAllocationOp(Operation *op) {\n }\n \n bool maybeAliasOp(Operation *op) {\n-  return isa<tensor::ExtractSliceOp>(op) ||\n+  return isa<tensor::ExtractSliceOp>(op) || isa<triton::TransOp>(op) ||\n          isa<triton::gpu::InsertSliceAsyncOp>(op) ||\n          isa<tensor::InsertSliceOp>(op);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 58, "deletions": 19, "changes": 77, "file_content_changes": "@@ -2716,6 +2716,9 @@ struct ConvertLayoutOpConversion\n     auto dstSharedLayout = dstTy.getEncoding().cast<SharedEncodingAttr>();\n     auto inOrd = srcBlockedLayout.getOrder();\n     auto outOrd = dstSharedLayout.getOrder();\n+    if (inOrd != outOrd)\n+      llvm_unreachable(\n+          \"blocked -> shared with different order not yet implemented\");\n     unsigned inVec =\n         inOrd == outOrd ? srcBlockedLayout.getSizePerThread()[inOrd[0]] : 1;\n     unsigned outVec = dstSharedLayout.getVec();\n@@ -2775,7 +2778,8 @@ struct ConvertLayoutOpConversion\n             getMultiDimIndex<unsigned>(linearRepIdx, reps, inOrd);\n         for (unsigned linearWordIdx = 0; linearWordIdx < numWordsEachRep;\n              ++linearWordIdx) {\n-          // step 1: recover the multidim_index from the index of input_elements\n+          // step 1: recover the multidim_index from the index of\n+          // input_elements\n           auto multiDimWordIdx =\n               getMultiDimIndex<unsigned>(linearWordIdx, wordsInEachRep, inOrd);\n           SmallVector<Value> multiDimIdx(2);\n@@ -2876,20 +2880,21 @@ struct ConvertLayoutOpConversion\n       } else if (mmaLayout.getVersion() == 1) {\n         multiDimWarpId[0] = urem(multiDimWarpId[0], idx_val(shape[0] / 16));\n         multiDimWarpId[1] = urem(multiDimWarpId[1], idx_val(shape[1] / 16));\n-        Value partId = udiv(laneId, _4);\n-        Value partIdDiv4 = udiv(partId, _4);\n-        Value partIdRem4 = urem(partId, _4);\n-        Value partRowOffset = mul(udiv(partIdRem4, _2), _8);\n-        partRowOffset = add(mul(partIdDiv4, _4), partRowOffset);\n-        Value partColOffset = mul(urem(partIdRem4, _2), _8);\n-        Value colOffset = add(mul(multiDimWarpId[0], _16), partColOffset);\n-        Value rowOffset = add(mul(multiDimWarpId[1], _16), partRowOffset);\n-        mmaRowIdx[0] = add(urem(laneId, _2), rowOffset);\n+        Value laneIdDiv16 = udiv(laneId, _16);\n+        Value laneIdRem16 = urem(laneId, _16);\n+        Value laneIdRem2 = urem(laneId, _2);\n+        Value laneIdRem16Div8 = udiv(laneIdRem16, _8);\n+        Value laneIdRem16Div4 = udiv(laneIdRem16, _4);\n+        Value laneIdRem16Div4Rem2 = urem(laneIdRem16Div4, _2);\n+        Value laneIdRem4Div2 = udiv(urem(laneId, _4), _2);\n+        mmaRowIdx[0] =\n+            add(add(mul(laneIdDiv16, _8), mul(laneIdRem16Div4Rem2, _4)),\n+                laneIdRem2);\n         mmaRowIdx[1] = add(mmaRowIdx[0], _2);\n-        mmaColIdx[0] = add(mul(udiv(urem(laneId, _4), _2), _2), colOffset);\n+        mmaColIdx[0] = add(mul(laneIdRem16Div8, _4), mul(laneIdRem4Div2, _2));\n         mmaColIdx[1] = add(mmaColIdx[0], _1);\n-        mmaColIdx[2] = add(mmaColIdx[0], _4);\n-        mmaColIdx[3] = add(mmaColIdx[0], idx_val(5));\n+        mmaColIdx[2] = add(mmaColIdx[0], _8);\n+        mmaColIdx[3] = add(mmaColIdx[0], idx_val(9));\n       } else {\n         llvm_unreachable(\"Unexpected MMALayout version\");\n       }\n@@ -3530,6 +3535,8 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n \n   // initialize accumulators\n   SmallVector<Value> acc = getElementsFromStruct(loc, loadedC, rewriter);\n+  size_t resSize = acc.size();\n+  SmallVector<Value> resVals(resSize);\n \n   auto callMMA = [&](unsigned m, unsigned n, unsigned k) {\n     auto ha = has[{m, k}];\n@@ -3573,8 +3580,14 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n     auto getIntAttr = [&](int v) {\n       return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n     };\n-    for (unsigned i = 0; i < 8; i++)\n-      acc[idx[i]] = extract_val(f32_ty, res, getIntAttr(i));\n+\n+    for (unsigned i = 0; i < 8; i++) {\n+      Value elem = extract_val(f32_ty, res, getIntAttr(i));\n+      acc[idx[i]] = elem;\n+      // TODO[goostavz]: double confirm this when m/n/k = [32, 32, x] has been\n+      // verified before MMA\n+      resVals[(m * numN / 2 + n) * 8 + i] = elem;\n+    }\n   };\n \n   for (unsigned k = 0; k < NK; k += 4)\n@@ -3583,12 +3596,10 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n         callMMA(m, n, k);\n       }\n \n-  // replace with new packed result\n   Type structTy = LLVM::LLVMStructType::getLiteral(\n-      ctx, SmallVector<Type>(acc.size(), type::f32Ty(ctx)));\n-  Value res = getStructFromElements(loc, acc, rewriter, structTy);\n+      ctx, SmallVector<Type>(resSize, type::f32Ty(ctx)));\n+  Value res = getStructFromElements(loc, resVals, rewriter, structTy);\n   rewriter.replaceOp(op, res);\n-\n   return success();\n }\n \n@@ -3691,6 +3702,33 @@ DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n \n /// ====================== mma codegen end ============================\n \n+/// ====================== trans codegen begin ============================\n+\n+struct TransOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::TransOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::TransOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::TransOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Location loc = op->getLoc();\n+    auto srcSmemObj =\n+        getSharedMemoryObjectFromStruct(loc, adaptor.src(), rewriter);\n+    SmallVector<Value> dstStrides = {srcSmemObj.strides[1],\n+                                     srcSmemObj.strides[0]};\n+    SmallVector<Value> dstOffsets = {srcSmemObj.offsets[1],\n+                                     srcSmemObj.offsets[0]};\n+    auto dstSmemObj =\n+        SharedMemoryObject(srcSmemObj.base, dstStrides, dstOffsets);\n+    auto retVal = getStructFromSharedMemoryObject(loc, dstSmemObj, rewriter);\n+    rewriter.replaceOp(op, retVal);\n+    return success();\n+  }\n+};\n+\n+/// ====================== trans codegen end ============================\n+\n Value convertSplatLikeOpWithMmaLayout(const MmaEncodingAttr &layout,\n                                       Type resType, Type elemType,\n                                       Value constVal,\n@@ -4513,6 +4551,7 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<ViewLikeOpConversion<triton::ExpandDimsOp>>(typeConverter,\n                                                            benefit);\n   patterns.add<DotOpConversion>(typeConverter, allocation, smem, benefit);\n+  patterns.add<TransOpConversion>(typeConverter, benefit);\n   patterns.add<PrintfOpConversion>(typeConverter, benefit);\n }\n "}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 95, "deletions": 4, "changes": 99, "file_content_changes": "@@ -252,6 +252,51 @@ struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n   }\n };\n \n+struct TritonTransPattern : public OpConversionPattern<triton::TransOp> {\n+\n+  using OpConversionPattern<triton::TransOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::TransOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Value src = adaptor.src();\n+    auto srcType = src.getType().cast<RankedTensorType>();\n+    Attribute srcEncoding = srcType.getEncoding();\n+    if (!srcEncoding)\n+      return failure();\n+    if (!srcEncoding.isa<triton::gpu::SharedEncodingAttr>()) {\n+      // TODO: end-to-end correctness is broken if\n+      // the input is blocked and the output is shared\n+      // with different order. Maybe a backend issue in BlockedToShared?\n+      SmallVector<unsigned> order = {1, 0};\n+      if (auto srcBlockedEncoding =\n+              srcEncoding.dyn_cast<triton::gpu::BlockedEncodingAttr>())\n+        llvm::copy(srcBlockedEncoding.getOrder(), order.begin());\n+      srcEncoding =\n+          triton::gpu::SharedEncodingAttr::get(getContext(), 1, 1, 1, order);\n+      srcType = RankedTensorType::get(srcType.getShape(),\n+                                      srcType.getElementType(), srcEncoding);\n+      src = rewriter.create<triton::gpu::ConvertLayoutOp>(src.getLoc(), srcType,\n+                                                          src);\n+    }\n+    auto srcSharedEncoding =\n+        srcEncoding.cast<triton::gpu::SharedEncodingAttr>();\n+    SmallVector<unsigned> retOrder(srcSharedEncoding.getOrder().begin(),\n+                                   srcSharedEncoding.getOrder().end());\n+    SmallVector<int64_t> retShapes(srcType.getShape().begin(),\n+                                   srcType.getShape().end());\n+    std::reverse(retOrder.begin(), retOrder.end());\n+    std::reverse(retShapes.begin(), retShapes.end());\n+    auto retEncoding =\n+        triton::gpu::SharedEncodingAttr::get(getContext(), 1, 1, 1, retOrder);\n+    auto retType =\n+        RankedTensorType::get(retShapes, srcType.getElementType(), retEncoding);\n+\n+    rewriter.replaceOpWithNewOp<triton::TransOp>(op, retType, src);\n+    return success();\n+  }\n+};\n+\n struct TritonLoadPattern : public OpConversionPattern<triton::LoadOp> {\n   using OpConversionPattern<triton::LoadOp>::OpConversionPattern;\n \n@@ -390,9 +435,10 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n       TritonGenericPattern<triton::PtrToIntOp>,\n       TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n       TritonGenericPattern<triton::AddPtrOp>, TritonReducePattern,\n-      TritonExpandDimsPattern, TritonMakeRangePattern, TritonDotPattern,\n-      TritonLoadPattern, TritonStorePattern, TritonExtElemwisePattern,\n-      TritonPrintfPattern, TritonAtomicRMWPattern>(typeConverter, context);\n+      TritonTransPattern, TritonExpandDimsPattern, TritonMakeRangePattern,\n+      TritonDotPattern, TritonLoadPattern, TritonStorePattern,\n+      TritonExtElemwisePattern, TritonPrintfPattern, TritonAtomicRMWPattern>(\n+      typeConverter, context);\n }\n \n //\n@@ -456,10 +502,55 @@ struct SCFYieldPattern : public OpConversionPattern<scf::YieldOp> {\n   }\n };\n \n+// This is borrowed from ConvertFIfOpTypes in\n+//    SCF/Transforms/StructuralTypeConversions.cpp\n+class SCFIfPattern : public OpConversionPattern<scf::IfOp> {\n+public:\n+  using OpConversionPattern<scf::IfOp>::OpConversionPattern;\n+  LogicalResult\n+  matchAndRewrite(scf::IfOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    // TODO: Generalize this to any type conversion, not just 1:1.\n+    //\n+    // We need to implement something more sophisticated here that tracks which\n+    // types convert to which other types and does the appropriate\n+    // materialization logic.\n+    // For example, it's possible that one result type converts to 0 types and\n+    // another to 2 types, so newResultTypes would at least be the right size to\n+    // not crash in the llvm::zip call below, but then we would set the the\n+    // wrong type on the SSA values! These edge cases are also why we cannot\n+    // safely use the TypeConverter::convertTypes helper here.\n+    SmallVector<Type> newResultTypes;\n+    for (auto type : op.getResultTypes()) {\n+      Type newType = typeConverter->convertType(type);\n+      if (!newType)\n+        return rewriter.notifyMatchFailure(op, \"not a 1:1 type conversion\");\n+      newResultTypes.push_back(newType);\n+    }\n+\n+    // See comments in the ForOp pattern for why we clone without regions and\n+    // then inline.\n+    scf::IfOp newOp =\n+        cast<scf::IfOp>(rewriter.cloneWithoutRegions(*op.getOperation()));\n+    rewriter.inlineRegionBefore(op.getThenRegion(), newOp.getThenRegion(),\n+                                newOp.getThenRegion().end());\n+    rewriter.inlineRegionBefore(op.getElseRegion(), newOp.getElseRegion(),\n+                                newOp.getElseRegion().end());\n+\n+    // Update the operands and types.\n+    newOp->setOperands(adaptor.getOperands());\n+    for (auto t : llvm::zip(newOp.getResults(), newResultTypes))\n+      std::get<0>(t).setType(std::get<1>(t));\n+    rewriter.replaceOp(op, newOp.getResults());\n+    return success();\n+  }\n+};\n+\n void populateSCFPatterns(TritonGPUTypeConverter &typeConverter,\n                          RewritePatternSet &patterns) {\n   MLIRContext *context = patterns.getContext();\n-  patterns.add<SCFYieldPattern, SCFForPattern>(typeConverter, context);\n+  patterns.add<SCFYieldPattern, SCFForPattern, SCFIfPattern>(typeConverter,\n+                                                             context);\n }\n \n class ConvertTritonToTritonGPU"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 53, "deletions": 0, "changes": 53, "file_content_changes": "@@ -178,6 +178,10 @@ class SimplifyConversion : public mlir::RewritePattern {\n           !isSharedEncoding(convert.getResult())) {\n         return mlir::failure();\n       }\n+      if (isSharedEncoding(convert.getOperand()) &&\n+          isSharedEncoding(convert.getResult())) {\n+        return mlir::failure();\n+      }\n       auto srcType = convert.getOperand().getType().cast<RankedTensorType>();\n       auto srcShared =\n           srcType.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n@@ -661,6 +665,54 @@ SmallVector<unsigned, 2> warpsPerTileV2(triton::DotOp dotOp,\n \n } // namespace\n \n+class OptimizeBlockedToShared : public mlir::RewritePattern {\n+public:\n+  OptimizeBlockedToShared(mlir::MLIRContext *context)\n+      : RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(), 1,\n+                       context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto cvt = cast<triton::gpu::ConvertLayoutOp>(op);\n+    auto srcType = cvt.getOperand().getType().cast<RankedTensorType>();\n+    auto dstType = cvt.getResult().getType().cast<RankedTensorType>();\n+    auto srcBlockedLayout =\n+        srcType.getEncoding().dyn_cast<triton::gpu::BlockedEncodingAttr>();\n+    auto dstSharedLayout =\n+        dstType.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n+    if (!srcBlockedLayout || !dstSharedLayout)\n+      return failure();\n+    if (srcBlockedLayout.getOrder() == dstSharedLayout.getOrder())\n+      return failure();\n+    // For now only works if single use is transpose\n+    // TODO: rematerialize #shared uses\n+    auto users = op->getUsers();\n+    if (std::distance(users.begin(), users.end()) != 1 ||\n+        !isa<triton::TransOp>(*users.begin()))\n+      return failure();\n+\n+    auto tmpShared = triton::gpu::SharedEncodingAttr::get(\n+        op->getContext(), dstSharedLayout.getVec(),\n+        dstSharedLayout.getPerPhase(), dstSharedLayout.getMaxPhase(),\n+        srcBlockedLayout.getOrder());\n+    auto tmpType = RankedTensorType::get(srcType.getShape(),\n+                                         srcType.getElementType(), tmpShared);\n+    auto tmpCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+        op->getLoc(), tmpType, cvt.getOperand());\n+\n+    auto newDstType = RankedTensorType::get(\n+        users.begin()->getResultTypes()[0].cast<RankedTensorType>().getShape(),\n+        srcType.getElementType(), dstSharedLayout);\n+\n+    auto newTrans = rewriter.create<triton::TransOp>(op->getLoc(), newDstType,\n+                                                     tmpCvt.getResult());\n+\n+    rewriter.replaceOp(*users.begin(), newTrans.getResult());\n+    return success();\n+  }\n+};\n+\n class BlockedToMMA : public mlir::RewritePattern {\n   int computeCapability;\n \n@@ -755,6 +807,7 @@ class TritonGPUCombineOpsPass\n \n     mlir::RewritePatternSet patterns(context);\n \n+    patterns.add<OptimizeBlockedToShared>(context);\n     patterns.add<SimplifyConversion>(context);\n     patterns.add<DecomposeDotOperand>(context);\n     patterns.add<RematerializeBackward>(context);"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "file_content_changes": "@@ -1085,6 +1085,16 @@ void init_triton_ir(py::module &&m) {\n                  mlir::RankedTensorType::get(shape, lhsType.getElementType()),\n                  lhs, rhs);\n            })\n+      .def(\"create_trans\",\n+           [](mlir::OpBuilder &self, mlir::Value &arg) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             auto argType = arg.getType().dyn_cast<mlir::RankedTensorType>();\n+             auto argEltType = argType.getElementType();\n+             std::vector<int64_t> retShape = argType.getShape();\n+             std::reverse(retShape.begin(), retShape.end());\n+             return self.create<mlir::triton::TransOp>(\n+                 loc, mlir::RankedTensorType::get(retShape, argEltType), arg);\n+           })\n       .def(\"create_broadcast\",\n            [](mlir::OpBuilder &self, mlir::Value &arg,\n               std::vector<int64_t> &shape) -> mlir::Value {"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 120, "deletions": 117, "changes": 237, "file_content_changes": "@@ -667,7 +667,6 @@ def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr)\n             tl.atomic_add(Z + off1, z)\n     rs = RandomState(17)\n     x = numpy_random((shape0, shape1), dtype_str=\"float32\", rs=rs)\n-    print(x)\n     # reference result\n     z_ref = np.sum(x, axis=axis, keepdims=False)\n     # triton result\n@@ -1067,122 +1066,126 @@ def kernel(X, stride_xm, stride_xn,\n # # ---------------\n \n \n-# @pytest.mark.parametrize(\"epilogue, allow_tf32, dtype\",\n-#                          [(epilogue, allow_tf32, dtype)\n-#                           for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols', 'softmax', 'chain-dot']\n-#                           for allow_tf32 in [True, False]\n-#                           for dtype in ['float16']\n-#                           if not (allow_tf32 and (dtype in ['float16']))])\n-# def test_dot(epilogue, allow_tf32, dtype, device='cuda'):\n-#     cc = _triton.runtime.cc(_triton.runtime.backend.CUDA, torch.cuda.current_device())\n-#     if cc < 80:\n-#         if dtype == 'int8':\n-#             pytest.skip(\"Only test int8 on devices with sm >= 80\")\n-#         elif dtype == 'float32' and allow_tf32:\n-#             pytest.skip(\"Only test tf32 on devices with sm >= 80\")\n-\n-#     M, N, K = 128, 128, 64\n-#     num_warps = 8\n-#     trans_a, trans_b = False, False\n-\n-#     # triton kernel\n-#     @triton.jit\n-#     def kernel(X, stride_xm, stride_xk,\n-#                Y, stride_yk, stride_yn,\n-#                W, stride_wn, stride_wl,\n-#                Z, stride_zm, stride_zn,\n-#                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n-#                ADD_MATRIX: tl.constexpr, ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr,\n-#                ALLOW_TF32: tl.constexpr,\n-#                DO_SOFTMAX: tl.constexpr, CHAIN_DOT: tl.constexpr,\n-#                TRANS_A: tl.constexpr, TRANS_B: tl.constexpr):\n-#         off_m = tl.arange(0, BLOCK_M)\n-#         off_n = tl.arange(0, BLOCK_N)\n-#         off_l = tl.arange(0, BLOCK_N)\n-#         off_k = tl.arange(0, BLOCK_K)\n-#         Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk\n-#         Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn\n-#         Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl\n-#         Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn\n-#         z = tl.dot(tl.load(Xs), tl.load(Ys), trans_a=TRANS_A, trans_b=TRANS_B, allow_tf32=ALLOW_TF32)\n-#         if ADD_MATRIX:\n-#             z += tl.load(Zs)\n-#         if ADD_ROWS:\n-#             ZRs = Z + off_m * stride_zm\n-#             z += tl.load(ZRs)[:, None]\n-#         if ADD_COLS:\n-#             ZCs = Z + off_n * stride_zn\n-#             z += tl.load(ZCs)[None, :]\n-#         if DO_SOFTMAX:\n-#             max = tl.max(z, 1)\n-#             z = z - max[:, None]\n-#             num = tl.exp(z)\n-#             den = tl.sum(num, 1)\n-#             z = num / den[:, None]\n-#         if CHAIN_DOT:\n-#             # tl.store(Zs, z)\n-#             # tl.debug_barrier()\n-#             z = tl.dot(z.to(tl.float16), tl.load(Ws), trans_a=TRANS_A)\n-#         tl.store(Zs, z)\n-#     # input\n-#     rs = RandomState(17)\n-#     x = numpy_random((K, M) if trans_a else (M, K), dtype_str=dtype, rs=rs) * .1\n-#     y = numpy_random((N, K) if trans_b else (K, N), dtype_str=dtype, rs=rs) * .1\n-#     w = numpy_random((N, N), dtype_str=dtype, rs=rs) * .1\n-#     if allow_tf32:\n-#         x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n-#         y = (y.view('uint32') & np.uint32(0xffffe000)).view('float32')\n-#         w = (w.view('uint32') & np.uint32(0xffffe000)).view('float32')\n-#     x_tri = to_triton(x, device=device)\n-#     y_tri = to_triton(y, device=device)\n-#     w_tri = to_triton(w, device=device)\n-#     # triton result\n-#     z = 1 + numpy_random((M, N), dtype_str=dtype, rs=rs) * .1\n-#     z_tri = to_triton(z, device=device)\n-#     if epilogue == 'trans':\n-#         z_tri = torch.as_strided(z_tri, (M, N), z_tri.stride()[::-1])\n-#     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n-#                          y_tri, y_tri.stride(0), y_tri.stride(1),\n-#                          w_tri, w_tri.stride(0), w_tri.stride(1),\n-#                          z_tri, z_tri.stride(0), z_tri.stride(1),\n-#                          TRANS_A=trans_a, TRANS_B=trans_b,\n-#                          BLOCK_M=M, BLOCK_K=K, BLOCK_N=N,\n-#                          ADD_MATRIX=epilogue == 'add-matrix',\n-#                          ADD_ROWS=epilogue == 'add-rows',\n-#                          ADD_COLS=epilogue == 'add-cols',\n-#                          DO_SOFTMAX=epilogue == 'softmax',\n-#                          CHAIN_DOT=epilogue == 'chain-dot',\n-#                          ALLOW_TF32=allow_tf32,\n-#                          num_warps=num_warps)\n-#     # torch result\n-#     x_ref = x.T if trans_a else x\n-#     y_ref = y.T if trans_b else y\n-#     z_ref = np.matmul(x_ref, y_ref)\n-#     if epilogue == 'add-matrix':\n-#         z_ref += z\n-#     if epilogue == 'add-rows':\n-#         z_ref += z[:, 0][:, None]\n-#     if epilogue == 'add-cols':\n-#         z_ref += z[0, :][None, :]\n-#     if epilogue == 'softmax':\n-#         num = np.exp(z_ref - np.max(z_ref, axis=-1, keepdims=True))\n-#         denom = np.sum(num, axis=-1, keepdims=True)\n-#         z_ref = num / denom\n-#     if epilogue == 'chain-dot':\n-#         z_ref = np.matmul(z_ref.T if trans_a else z_ref, w)\n-#     # compare\n-#     # print(z_ref[:,0], z_tri[:,0])\n-#     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n-#     # make sure ld/st are vectorized\n-#     ptx = pgm.asm['ptx']\n-#     assert 'ld.global.v4' in ptx\n-#     assert 'st.global.v4' in ptx\n-#     if allow_tf32:\n-#         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' in ptx\n-#     elif dtype == 'float32':\n-#         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' not in ptx\n-#     elif dtype == 'int8':\n-#         assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n+@pytest.mark.parametrize(\"epilogue, allow_tf32, dtype\",\n+                         [(epilogue, allow_tf32, dtype)\n+                          for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols', 'softmax', 'chain-dot']\n+                          for allow_tf32 in [True, False]\n+                          for dtype in ['float16']\n+                          if not (allow_tf32 and (dtype in ['float16']))])\n+def test_dot(epilogue, allow_tf32, dtype, device='cuda'):\n+    capability = torch.cuda.get_device_capability()\n+    if capability[0] < 80:\n+        if dtype == 'int8':\n+            pytest.skip(\"Only test int8 on devices with sm >= 80\")\n+        elif dtype == 'float32' and allow_tf32:\n+            pytest.skip(\"Only test tf32 on devices with sm >= 80\")\n+\n+    M, N, K = 64, 64, 64\n+    num_warps = 4\n+    trans_a, trans_b = False, False\n+\n+    # triton kernel\n+    @triton.jit\n+    def kernel(X, stride_xm, stride_xk,\n+               Y, stride_yk, stride_yn,\n+               W, stride_wn, stride_wl,\n+               Z, stride_zm, stride_zn,\n+               BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n+               ADD_MATRIX: tl.constexpr, ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr,\n+               ALLOW_TF32: tl.constexpr,\n+               DO_SOFTMAX: tl.constexpr, CHAIN_DOT: tl.constexpr,\n+               TRANS_A: tl.constexpr, TRANS_B: tl.constexpr):\n+        off_m = tl.arange(0, BLOCK_M)\n+        off_n = tl.arange(0, BLOCK_N)\n+        off_l = tl.arange(0, BLOCK_N)\n+        off_k = tl.arange(0, BLOCK_K)\n+        Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk\n+        Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn\n+        Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl\n+        Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn\n+        x = tl.load(Xs)\n+        y = tl.load(Ys)\n+        x = tl.trans(x) if TRANS_A else x\n+        y = tl.trans(y) if TRANS_B else y\n+        z = tl.dot(x, y, allow_tf32=ALLOW_TF32)\n+        if ADD_MATRIX:\n+            z += tl.load(Zs)\n+        if ADD_ROWS:\n+            ZRs = Z + off_m * stride_zm\n+            z += tl.load(ZRs)[:, None]\n+        if ADD_COLS:\n+            ZCs = Z + off_n * stride_zn\n+            z += tl.load(ZCs)[None, :]\n+        if DO_SOFTMAX:\n+            max = tl.max(z, 1)\n+            z = z - max[:, None]\n+            num = tl.exp(z)\n+            den = tl.sum(num, 1)\n+            z = num / den[:, None]\n+        if CHAIN_DOT:\n+            # tl.store(Zs, z)\n+            # tl.debug_barrier()\n+            z = tl.dot(tl.trans(z.to(tl.float16)), tl.load(Ws))\n+        tl.store(Zs, z)\n+    # input\n+    rs = RandomState(17)\n+    x = numpy_random((K, M) if trans_a else (M, K), dtype_str=dtype, rs=rs) * .1\n+    y = numpy_random((N, K) if trans_b else (K, N), dtype_str=dtype, rs=rs) * .1\n+    w = numpy_random((N, N), dtype_str=dtype, rs=rs) * .1\n+    if allow_tf32:\n+        x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+        y = (y.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+        w = (w.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+    x_tri = to_triton(x, device=device)\n+    y_tri = to_triton(y, device=device)\n+    w_tri = to_triton(w, device=device)\n+    # triton result\n+    z = 1 + numpy_random((M, N), dtype_str=dtype, rs=rs) * .1\n+    z_tri = to_triton(z, device=device)\n+    if epilogue == 'trans':\n+        z_tri = torch.as_strided(z_tri, (M, N), z_tri.stride()[::-1])\n+    pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n+                         y_tri, y_tri.stride(0), y_tri.stride(1),\n+                         w_tri, w_tri.stride(0), w_tri.stride(1),\n+                         z_tri, z_tri.stride(0), z_tri.stride(1),\n+                         TRANS_A=trans_a, TRANS_B=trans_b,\n+                         BLOCK_M=M, BLOCK_K=K, BLOCK_N=N,\n+                         ADD_MATRIX=epilogue == 'add-matrix',\n+                         ADD_ROWS=epilogue == 'add-rows',\n+                         ADD_COLS=epilogue == 'add-cols',\n+                         DO_SOFTMAX=epilogue == 'softmax',\n+                         CHAIN_DOT=epilogue == 'chain-dot',\n+                         ALLOW_TF32=allow_tf32,\n+                         num_warps=num_warps)\n+    # torch result\n+    x_ref = x.T if trans_a else x\n+    y_ref = y.T if trans_b else y\n+    z_ref = np.matmul(x_ref, y_ref)\n+    if epilogue == 'add-matrix':\n+        z_ref += z\n+    if epilogue == 'add-rows':\n+        z_ref += z[:, 0][:, None]\n+    if epilogue == 'add-cols':\n+        z_ref += z[0, :][None, :]\n+    if epilogue == 'softmax':\n+        num = np.exp(z_ref - np.max(z_ref, axis=-1, keepdims=True))\n+        denom = np.sum(num, axis=-1, keepdims=True)\n+        z_ref = num / denom\n+    if epilogue == 'chain-dot':\n+        z_ref = np.matmul(z_ref.T, w)\n+    # compare\n+    # print(z_ref[:,0], z_tri[:,0])\n+    np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+    # make sure ld/st are vectorized\n+    ptx = pgm.asm['ptx']\n+    assert 'ld.global.v4' in ptx\n+    assert 'st.global.v4' in ptx\n+    if allow_tf32:\n+        assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' in ptx\n+    elif dtype == 'float32':\n+        assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' not in ptx\n+    elif dtype == 'int8':\n+        assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n \n \n # def test_dot_without_load():"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 13, "deletions": 3, "changes": 16, "file_content_changes": "@@ -359,7 +359,7 @@ def visit_If(self, node):\n             cond = cond.to(triton.language.int1, _builder=self.builder)\n             with enter_sub_region(self) as sr:\n                 liveins, ip_block = sr\n-\n+                liveins_copy = liveins.copy()\n                 then_block = self.builder.create_block()\n                 self.builder.set_insertion_point_to_start(then_block)\n                 self.visit_compound_statement(node.body)\n@@ -394,7 +394,15 @@ def visit_If(self, node):\n                             if then_defs[then_name].type == else_defs[else_name].type:\n                                 names.append(then_name)\n                                 ret_types.append(then_defs[then_name].type)\n-\n+                \n+                # defined in else block but not in then block\n+                # to find in parent scope and yield them\n+                for else_name in else_defs:\n+                    if else_name in liveins and else_name not in then_defs:\n+                        if else_defs[else_name].type == liveins[else_name].type:\n+                            names.append(else_name)\n+                            ret_types.append(else_defs[else_name].type)\n+                            then_defs[else_name] = liveins_copy[else_name]\n                 self.builder.set_insertion_point_to_end(ip_block)\n \n                 if then_defs or node.orelse:  # with else block\n@@ -1359,7 +1367,7 @@ def make_hash(fn, **kwargs):\n     \"ptx\": ptx_prototype_pattern,\n }\n \n-mlir_arg_type_pattern = r'%\\w+: ([^,\\s]+)(?: \\{\\S+ = \\S+ : \\S+\\})?,?'\n+mlir_arg_type_pattern = r'%\\w+: ([^,^\\)\\s]+)(?: \\{\\S+ = \\S+ : \\S+\\})?,?'\n ptx_arg_type_pattern = r\"\\.param\\s+\\.(\\w+)\"\n arg_type_pattern = {\n     \"ttir\": mlir_arg_type_pattern,\n@@ -1416,7 +1424,9 @@ def compile(fn, **kwargs):\n         import re\n         match = re.search(prototype_pattern[ir], src, re.MULTILINE)\n         name, signature = match.group(1), match.group(2)\n+        print(name, signature)\n         types = re.findall(arg_type_pattern[ir], signature)\n+        print(types)\n         param_tys = [convert_type_repr(ty) for ty in types]\n         signature = {k: v for k, v in enumerate(param_tys)}\n         first_stage = list(stages.keys()).index(ir)"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -614,6 +614,7 @@ def __getitem__(self, slices, _builder=None):\n                 assert False, \"unsupported\"\n         return ret\n \n+\n     # x[:, None, :, None]\n     # x = expand_dims(x, axis=1)\n     # x = expand_dims(x, axis=2)\n@@ -737,6 +738,9 @@ def broadcast_to(input, shape, _builder=None):\n     \"\"\"\n     return semantic.broadcast_impl_shape(input, shape, _builder)\n \n+@builtin\n+def trans(input, _builder=None):\n+    return semantic.trans(input, _builder)\n \n @builtin\n def cat(input, other, _builder=None):"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -502,6 +502,11 @@ def cat(lhs: tl.tensor, rhs: tl.tensor, builder: ir.builder) -> tl.tensor:\n     # TODO: check types\n     return tl.tensor(builder.create_cat(lhs.handle, rhs.handle), lhs.type)\n \n+def trans(input: tl.tensor, builder: ir.builder) -> tl.tensor:\n+    if len(input.shape) != 2:\n+        raise ValueError(\"Only 2D tensors can be transposed\")\n+    ret_type = tl.block_type(input.type.scalar, [input.shape[1], input.shape[0]])\n+    return tl.tensor(builder.create_trans(input.handle), ret_type)\n \n def broadcast_impl_shape(input: tl.tensor,\n                          shape: List[int],"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 21, "deletions": 21, "changes": 42, "file_content_changes": "@@ -32,7 +32,7 @@ def _fwd_kernel(\n     offs_n = tl.arange(0, BLOCK_N)\n     offs_d = tl.arange(0, BLOCK_DMODEL)\n     off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n+    off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :] * stride_kk\n     off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n     # Initialize pointers to Q, K, V\n     q_ptrs = Q + off_q\n@@ -50,7 +50,7 @@ def _fwd_kernel(\n         # -- compute qk ----\n         k = tl.load(k_ptrs + start_n * stride_kn)\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-        qk += tl.dot(q, k)\n+        qk += tl.dot(q, tl.trans(k))\n         qk *= sm_scale\n         qk += tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), 0, float(\"-inf\"))\n         # -- compute m_ij, p, l_ij\n@@ -165,26 +165,26 @@ def _bwd_kernel(\n             q = tl.load(q_ptrs)\n             # recompute p = softmax(qk, dim=-1).T\n             # NOTE: `do` is pre-divided by `l`; no normalization here\n-            qk = tl.dot(q, k, trans_b=True)\n+            qk = tl.dot(q, tl.trans(k))\n             qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\"-inf\"))\n             m = tl.load(m_ptrs + offs_m_curr)\n             p = tl.exp(qk * sm_scale - m[:, None])\n             # compute dv\n             do = tl.load(do_ptrs)\n-            dv += tl.dot(p.to(tl.float16), do, trans_a=True)\n+            dv += tl.dot(tl.trans(p.to(tl.float16)), do)\n             # compute dp = dot(v, do)\n             Di = tl.load(D_ptrs + offs_m_curr)\n             dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n-            dp += tl.dot(do, v, trans_b=True)\n+            dp += tl.dot(do, tl.trans(v))\n             # compute ds = p * (dp - delta[:, None])\n             ds = p * dp * sm_scale\n             # compute dk = dot(ds.T, q)\n-            dk += tl.dot(ds.to(tl.float16), q, trans_a=True)\n-            # # compute dq\n+            dk += tl.dot(tl.trans(ds.to(tl.float16)), q)\n+            # compute dq\n             dq = tl.load(dq_ptrs)\n             dq += tl.dot(ds.to(tl.float16), k)\n             tl.store(dq_ptrs, dq)\n-            # # increment pointers\n+            # increment pointers\n             dq_ptrs += BLOCK_M * stride_qm\n             q_ptrs += BLOCK_M * stride_qm\n             do_ptrs += BLOCK_M * stride_qm\n@@ -273,7 +273,7 @@ def backward(ctx, do):\n @pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     torch.manual_seed(20)\n-    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.1).requires_grad_()\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n     k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n     v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n     sm_scale = 0.2\n@@ -287,23 +287,23 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     p = torch.softmax(p.float(), dim=-1).half()\n     # p = torch.exp(p)\n     ref_out = torch.matmul(p, v)\n-    # ref_out.backward(dout)\n-    # ref_dv, v.grad = v.grad.clone(), None\n-    # ref_dk, k.grad = k.grad.clone(), None\n-    # ref_dq, q.grad = q.grad.clone(), None\n+    ref_out.backward(dout)\n+    ref_dv, v.grad = v.grad.clone(), None\n+    ref_dk, k.grad = k.grad.clone(), None\n+    ref_dq, q.grad = q.grad.clone(), None\n     # # triton implementation\n     tri_out = attention(q, k, v, sm_scale)\n     # print(ref_out)\n     # print(tri_out)\n-    # tri_out.backward(dout)\n-    # tri_dv, v.grad = v.grad.clone(), None\n-    # tri_dk, k.grad = k.grad.clone(), None\n-    # tri_dq, q.grad = q.grad.clone(), None\n+    tri_out.backward(dout)\n+    tri_dv, v.grad = v.grad.clone(), None\n+    tri_dk, k.grad = k.grad.clone(), None\n+    tri_dq, q.grad = q.grad.clone(), None\n     # compare\n     triton.testing.assert_almost_equal(ref_out, tri_out)\n-    # triton.testing.assert_almost_equal(ref_dv, tri_dv)\n-    # triton.testing.assert_almost_equal(ref_dk, tri_dk)\n-    # triton.testing.assert_almost_equal(ref_dq, tri_dq)\n+    triton.testing.assert_almost_equal(ref_dv, tri_dv)\n+    triton.testing.assert_almost_equal(ref_dk, tri_dk)\n+    triton.testing.assert_almost_equal(ref_dq, tri_dq)\n \n BATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\n # vary seq length for fixed head and batch=4\n@@ -350,4 +350,4 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.f\n         ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n         return ms\n \n-bench_flash_attention.run(save_path='.', print_data=True)\n\\ No newline at end of file\n+# bench_flash_attention.run(save_path='.', print_data=True)\n\\ No newline at end of file"}]