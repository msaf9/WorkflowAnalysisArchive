[{"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 16, "deletions": 3, "changes": 19, "file_content_changes": "@@ -125,11 +125,24 @@ class AllocationAnalysis {\n     // TODO(Keren): Add convert ops\n     if (auto reduceOp = dyn_cast<triton::ReduceOp>(op)) {\n       // TODO(Keren): Reduce with index is not supported yet.\n+      unsigned axis = reduceOp.axis();\n       auto value = op->getOperand(0);\n       if (auto tensorType = value.getType().dyn_cast<RankedTensorType>()) {\n-        auto bytes = tensorType.getNumElements() *\n-                     tensorType.getElementTypeBitWidth() / 8;\n-        allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+        if (auto layout =\n+                tensorType.getEncoding().dyn_cast<BlockedEncodingAttr>()) {\n+          auto shape = tensorType.getShape();\n+          int64_t bytes = tensorType.getElementTypeBitWidth() / 8;\n+          for (unsigned d = 0; d < shape.size(); ++d)\n+            if (d == axis)\n+              bytes *= layout.getThreadsPerWarp()[axis] *\n+                       layout.getWarpsPerCTA()[axis];\n+            else\n+              bytes *= shape[d];\n+          allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+        } else {\n+          assert(0 && \"ReduceOp with input layout other than blocked layout is \"\n+                      \"not implemented yet\");\n+        }\n       }\n     } else if (auto cvtLayout = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n       auto srcTy = cvtLayout.src().getType().cast<RankedTensorType>();"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 195, "deletions": 27, "changes": 222, "file_content_changes": "@@ -75,7 +75,14 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n #define udiv(...) rewriter.create<LLVM::UDivOp>(loc, __VA_ARGS__)\n #define urem(...) rewriter.create<LLVM::URemOp>(loc, __VA_ARGS__)\n #define add(...) rewriter.create<LLVM::AddOp>(loc, __VA_ARGS__)\n+#define fadd(...) rewriter.create<LLVM::FAddOp>(loc, __VA_ARGS__)\n #define mul(...) rewriter.create<LLVM::MulOp>(loc, __VA_ARGS__)\n+#define smax(...) rewriter.create<LLVM::SMaxOp>(loc, __VA_ARGS__)\n+#define umax(...) rewriter.create<LLVM::UMaxOp>(loc, __VA_ARGS__)\n+#define fmax(...) rewriter.create<LLVM::MaxNumOp>(loc, __VA_ARGS__)\n+#define smin(...) rewriter.create<LLVM::SMinOp>(loc, __VA_ARGS__)\n+#define umin(...) rewriter.create<LLVM::UMinOp>(loc, __VA_ARGS__)\n+#define fmin(...) rewriter.create<LLVM::MinNumOp>(loc, __VA_ARGS__)\n #define xor_(...) rewriter.create<LLVM::XOrOp>(loc, __VA_ARGS__)\n #define bitcast(...) rewriter.create<LLVM::BitcastOp>(loc, __VA_ARGS__)\n #define gep(...) rewriter.create<LLVM::GEPOp>(loc, __VA_ARGS__)\n@@ -88,9 +95,11 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n   rewriter.create<LLVM::ExtractElementOp>(loc, __VA_ARGS__)\n #define load(...) rewriter.create<LLVM::LoadOp>(loc, __VA_ARGS__)\n #define store(val, ptr) rewriter.create<LLVM::StoreOp>(loc, val, ptr)\n+#define icmp_slt(...)                                                          \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::slt, __VA_ARGS__)\n #define select(...) rewriter.create<LLVM::SelectOp>(loc, __VA_ARGS__)\n #define address_of(...) rewriter.create<LLVM::AddressOfOp>(loc, __VA_ARGS__)\n-#define barrier rewriter.create<mlir::gpu::BarrierOp>(loc)\n+#define barrier() rewriter.create<mlir::gpu::BarrierOp>(loc)\n #define undef(...) rewriter.create<LLVM::UndefOp>(loc, __VA_ARGS__)\n #define i32_ty rewriter.getIntegerType(32)\n #define vec_ty(type, num) VectorType::get(num, type)\n@@ -534,31 +543,19 @@ class ConvertTritonGPUOpToLLVMPattern\n     }\n   }\n \n-  // Emit indices calculation within each ConversionPattern, and returns a\n-  // [elemsPerThread X rank] index matrix.\n-  // TODO: [goostavz] Double confirm the redundant indices calculations will\n-  //       be eliminated in the consequent MLIR/LLVM optimization. We might\n-  //       implement a indiceCache if necessary.\n-  SmallVector<SmallVector<Value>>\n-  emitIndicesForBlockedLayout(Location loc, ConversionPatternRewriter &rewriter,\n-                              const BlockedEncodingAttr &blockedLayout,\n-                              ArrayRef<int64_t> shape) const {\n-    auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n+  SmallVector<SmallVector<unsigned>>\n+  emitOffsetForBlockedLayout(const BlockedEncodingAttr &blockedLayout,\n+                             ArrayRef<int64_t> shape) const {\n     auto sizePerThread = blockedLayout.getSizePerThread();\n     auto threadsPerWarp = blockedLayout.getThreadsPerWarp();\n     auto warpsPerCTA = blockedLayout.getWarpsPerCTA();\n+\n     unsigned rank = shape.size();\n     SmallVector<unsigned> shapePerCTA = getShapePerCTA(blockedLayout);\n     SmallVector<unsigned> tilesPerDim(rank);\n     for (unsigned k = 0; k < rank; ++k)\n       tilesPerDim[k] = ceil<unsigned>(shape[k], shapePerCTA[k]);\n \n-    // step 1, delinearize threadId to get the base index\n-    auto multiDimBase =\n-        emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n-\n-    // step 2, get offset of each element\n-    unsigned elemsPerThread = blockedLayout.getElemsPerThread(shape);\n     SmallVector<SmallVector<unsigned>> offset(rank);\n     for (unsigned k = 0; k < rank; ++k) {\n       // 1 block in minimum if shape[k] is less than shapePerCTA[k]\n@@ -575,12 +572,10 @@ class ConvertTritonGPUOpToLLVMPattern\n                                       threadsPerWarp[k] +\n                                   threadOffset * sizePerThread[k] + elemOffset);\n     }\n-    // step 3, add offset to base, and reorder the sequence of indices to\n-    // guarantee that elems in the same sizePerThread are adjacent in order\n-    SmallVector<SmallVector<Value>> multiDimIdx(elemsPerThread,\n-                                                SmallVector<Value>(rank));\n-    unsigned totalSizePerThread = product<unsigned>(sizePerThread);\n \n+    unsigned elemsPerThread = blockedLayout.getElemsPerThread(shape);\n+    unsigned totalSizePerThread = product<unsigned>(sizePerThread);\n+    SmallVector<SmallVector<unsigned>> reorderedOffset(elemsPerThread);\n     for (unsigned n = 0; n < elemsPerThread; ++n) {\n       unsigned linearNanoTileId = n / totalSizePerThread;\n       unsigned linearNanoTileElemId = n % totalSizePerThread;\n@@ -593,10 +588,38 @@ class ConvertTritonGPUOpToLLVMPattern\n             multiDimNanoTileId[k] *\n                 (sizePerThread[k] * threadsPerWarp[k] * warpsPerCTA[k]) +\n             multiDimNanoTileElemId[k];\n-        multiDimIdx[n][k] =\n-            add(multiDimBase[k], idx_val(offset[k][reorderedMultiDimId]));\n+        reorderedOffset[n].push_back(offset[k][reorderedMultiDimId]);\n       }\n     }\n+    return reorderedOffset;\n+  }\n+\n+  // Emit indices calculation within each ConversionPattern, and returns a\n+  // [elemsPerThread X rank] index matrix.\n+  // TODO: [goostavz] Double confirm the redundant indices calculations will\n+  //       be eliminated in the consequent MLIR/LLVM optimization. We might\n+  //       implement a indiceCache if necessary.\n+  SmallVector<SmallVector<Value>>\n+  emitIndicesForBlockedLayout(Location loc, ConversionPatternRewriter &rewriter,\n+                              const BlockedEncodingAttr &blockedLayout,\n+                              ArrayRef<int64_t> shape) const {\n+    // step 1, delinearize threadId to get the base index\n+    auto multiDimBase =\n+        emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n+\n+    // step 2, get offset of each element\n+    SmallVector<SmallVector<unsigned>> offset =\n+        emitOffsetForBlockedLayout(blockedLayout, shape);\n+\n+    // step 3, add offset to base, and reorder the sequence of indices to\n+    // guarantee that elems in the same sizePerThread are adjacent in order\n+    unsigned rank = shape.size();\n+    unsigned elemsPerThread = offset.size();\n+    SmallVector<SmallVector<Value>> multiDimIdx(elemsPerThread,\n+                                                SmallVector<Value>(rank));\n+    for (unsigned n = 0; n < elemsPerThread; ++n)\n+      for (unsigned k = 0; k < rank; ++k)\n+        multiDimIdx[n][k] = add(multiDimBase[k], idx_val(offset[n][k]));\n \n     return multiDimIdx;\n   }\n@@ -1004,6 +1027,150 @@ struct BroadcastOpConversion\n   }\n };\n \n+struct ReduceOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::ReduceOp> {\n+\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::ReduceOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Location loc = op->getLoc();\n+    unsigned axis = adaptor.axis();\n+\n+    auto doAcc = [&](Value &acc, Value cur, bool isFirst) -> void {\n+      if (isFirst) {\n+        acc = cur;\n+        return;\n+      }\n+      auto type = cur.getType();\n+      switch (op.redOp()) {\n+      case RedOp::ADD:\n+        acc = add(acc, cur);\n+        break;\n+      case RedOp::MAX:\n+        if (type.isUnsignedInteger())\n+          acc = umax(acc, cur);\n+        else\n+          acc = smax(acc, cur);\n+        break;\n+      case RedOp::MIN:\n+        if (type.isUnsignedInteger())\n+          acc = umin(acc, cur);\n+        else\n+          acc = smin(acc, cur);\n+        break;\n+      case RedOp::FADD:\n+        acc = fadd(acc.getType(), acc, cur);\n+        break;\n+      case RedOp::FMAX:\n+        acc = fmax(acc, cur);\n+        break;\n+      case RedOp::FMIN:\n+        acc = fmin(acc, cur);\n+        break;\n+      case RedOp::XOR:\n+        acc = xor_(acc, cur);\n+        break;\n+      default:\n+        llvm::report_fatal_error(\"Unsupported reduce op\");\n+      }\n+    };\n+\n+    auto srcTy = op.operand().getType().cast<RankedTensorType>();\n+    auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+    auto srcShape = srcTy.getShape();\n+\n+    auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n+    auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n+    Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n+    smemBase = bitcast(elemPtrTy, smemBase);\n+\n+    std::vector<unsigned> shmShape;\n+    for (auto d : srcShape)\n+      shmShape.push_back(d);\n+    shmShape[axis] =\n+        srcLayout.getThreadsPerWarp()[axis] * srcLayout.getWarpsPerCTA()[axis];\n+\n+    unsigned srcElems = getElemsPerThread(srcLayout, srcShape);\n+    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n+    auto srcValues =\n+        getElementsFromStruct(loc, adaptor.operand(), srcElems, rewriter);\n+\n+    SmallVector<SmallVector<unsigned>> offset =\n+        emitOffsetForBlockedLayout(srcLayout, srcShape);\n+\n+    std::map<SmallVector<unsigned>, Value> accs;\n+    std::map<SmallVector<unsigned>, SmallVector<Value>> indices;\n+\n+    for (int i = 0; i < srcElems; ++i) {\n+      SmallVector<unsigned> key = offset[i];\n+      key[axis] = 0;\n+      bool isFirst = accs.find(key) == accs.end();\n+      doAcc(accs[key], srcValues[i], isFirst);\n+      if (isFirst)\n+        indices[key] = srcIndices[i];\n+    }\n+\n+    // cached int32 constants\n+    std::map<int, Value> ints;\n+    ints[0] = i32_val(0);\n+    for (int N = shmShape[axis] / 2; N > 0; N >>= 1)\n+      ints[N] = i32_val(N);\n+    Value sizePerThread = i32_val(srcLayout.getSizePerThread()[axis]);\n+\n+    for (auto it : accs) {\n+      const SmallVector<unsigned> &key = it.first;\n+      Value acc = it.second;\n+      SmallVector<Value> writeIdx = indices[key];\n+\n+      writeIdx[axis] = udiv(writeIdx[axis], sizePerThread);\n+      Value writeOffset = linearize(rewriter, loc, writeIdx, shmShape);\n+      Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n+      store(acc, writePtr);\n+\n+      SmallVector<Value> readIdx(writeIdx.size(), ints[0]);\n+      for (int N = shmShape[axis] / 2; N > 0; N >>= 1) {\n+        readIdx[axis] = ints[N];\n+        Value readMask = icmp_slt(writeIdx[axis], ints[N]);\n+        Value readOffset = select(\n+            readMask, linearize(rewriter, loc, readIdx, shmShape), ints[0]);\n+        Value readPtr = gep(elemPtrTy, writePtr, readOffset);\n+        barrier();\n+        doAcc(acc, load(readPtr), false);\n+        store(acc, writePtr);\n+      }\n+    }\n+\n+    auto resultTy = op.getType().cast<RankedTensorType>();\n+    auto resultLayout = resultTy.getEncoding();\n+    auto resultShape = resultTy.getShape();\n+\n+    unsigned resultElems = getElemsPerThread(resultLayout, resultShape);\n+    auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultShape);\n+    assert(resultIndices.size() == resultElems);\n+\n+    barrier();\n+    SmallVector<Value> resultVals(resultElems);\n+    for (int i = 0; i < resultElems; i++) {\n+      SmallVector<Value> readIdx = resultIndices[i];\n+      readIdx.insert(readIdx.begin() + axis, ints[0]);\n+      Value readOffset = linearize(rewriter, loc, readIdx, shmShape);\n+      Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n+      resultVals[i] = load(readPtr);\n+    }\n+\n+    SmallVector<Type> resultTypes(resultElems, llvmElemTy);\n+    Type structTy =\n+        LLVM::LLVMStructType::getLiteral(this->getContext(), resultTypes);\n+    Value ret = getStructFromElements(loc, resultVals, rewriter, structTy);\n+    rewriter.replaceOp(op, ret);\n+\n+    return success();\n+  }\n+};\n+\n template <typename SourceOp>\n struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n   using OpAdaptor = typename SourceOp::Adaptor;\n@@ -1772,7 +1939,7 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n \n   for (unsigned repId = 0; repId < accumNumReplicates; ++repId) {\n     auto multiDimRepId = getMultiDimIndex<unsigned>(repId, numReplicates);\n-    barrier;\n+    barrier();\n     if (srcLayout.isa<BlockedEncodingAttr>() ||\n         srcLayout.isa<MmaEncodingAttr>()) {\n       processReplica(loc, rewriter, /*stNotRd*/ true, srcTy, inNumCTAsEachRep,\n@@ -1782,7 +1949,7 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n       assert(0 && \"ConvertLayout with input layout not implemented\");\n       return failure();\n     }\n-    barrier;\n+    barrier();\n     if (dstLayout.isa<BlockedEncodingAttr>() ||\n         dstLayout.isa<MmaEncodingAttr>()) {\n       processReplica(loc, rewriter, /*stNotRd*/ false, dstTy, outNumCTAsEachRep,\n@@ -1912,7 +2079,7 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n     }\n   }\n   // TODO: double confirm if the Barrier is necessary here\n-  barrier;\n+  barrier();\n   rewriter.replaceOp(op, smemBase);\n   return success();\n }\n@@ -3150,6 +3317,7 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<CmpIOpConversion>(typeConverter, benefit);\n   patterns.add<CmpFOpConversion>(typeConverter, benefit);\n   patterns.add<BroadcastOpConversion>(typeConverter, benefit);\n+  patterns.add<ReduceOpConversion>(typeConverter, allocation, smem, benefit);\n   patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n                                           benefit);\n   patterns.add<ExtractSliceOpConversion>(typeConverter, allocation, smem,"}, {"filename": "python/tests/test_reduce.py", "status": "added", "additions": 89, "deletions": 0, "changes": 89, "file_content_changes": "@@ -0,0 +1,89 @@\n+import pytest\n+import torch\n+from torch.testing import assert_close\n+\n+import triton\n+import triton.language as tl\n+\n+\n+def patch_kernel(template, to_replace):\n+    kernel = triton.JITFunction(template.fn)\n+    for key, value in to_replace.items():\n+        kernel.src = kernel.src.replace(key, value)\n+    return kernel\n+\n+\n+@triton.jit\n+def reduce1d_kernel(x_ptr, z_ptr, block: tl.constexpr):\n+    x = tl.load(x_ptr + tl.arange(0, block))\n+    tl.store(z_ptr, tl.OP(x, axis=0))\n+\n+\n+@triton.jit\n+def reduce2d_kernel(x_ptr, z_ptr, block_m: tl.constexpr, block_n: tl.constexpr):\n+    range_m = tl.arange(0, block_m)\n+    range_n = tl.arange(0, block_n)\n+    x = tl.load(x_ptr + range_m[:, None] * block_n + range_n[None, :])\n+    z = tl.OP(x, axis=AXIS)\n+    if AXIS == 0:\n+        tl.store(z_ptr + range_n, z)\n+    else:\n+        tl.store(z_ptr + range_m, z)\n+\n+\n+reduce1d_configs = [\n+    (op, shape) for op in ['sum', 'min', 'max'] for shape in [32, 64, 128, 512, 1024]\n+]\n+\n+\n+@pytest.mark.skip(reason=\"conversion from scalar to 0-d tensor not implemented\")\n+@pytest.mark.parametrize('op, shape', reduce1d_configs)\n+def test_reduce1d(op, shape):\n+    print('test_reduce1d, op = {}, shape = {}'.format(op, shape))\n+    x = torch.randn((shape,), device='cuda', dtype=torch.float32)\n+    z = torch.empty(\n+        (1,),\n+        device=x.device,\n+        dtype=torch.float32,\n+    )\n+\n+    kernel = patch_kernel(reduce1d_kernel, {'OP': op})\n+    grid = (1,)\n+    kernel[grid](x_ptr=x, z_ptr=z, block=shape)\n+\n+    if op == 'sum':\n+        golden_z = torch.sum(x)\n+    elif op == 'min':\n+        golden_z = torch.min(x)\n+    else:\n+        golden_z = torch.max(x)\n+\n+    assert_close(z, golden_z, rtol=1e-3, atol=1e-3)\n+\n+\n+reduce2d_configs = [\n+    (op, shape, axis)\n+    for op in ['sum', 'min', 'max']\n+    for shape in [(2, 32), (4, 32), (4, 128), (32, 64), (64, 128)]\n+    for axis in [0, 1]\n+]\n+\n+\n+@pytest.mark.parametrize('op, shape, axis', reduce2d_configs)\n+def test_reduce2d(op, shape, axis):\n+    x = torch.randn(shape, device='cuda', dtype=torch.float32)\n+    reduced_shape = (shape[1 - axis],)\n+    z = torch.empty(reduced_shape, device=x.device, dtype=torch.float32)\n+\n+    kernel = patch_kernel(reduce2d_kernel, {'OP': op, 'AXIS': str(axis)})\n+    grid = (1,)\n+    kernel[grid](x_ptr=x, z_ptr=z, block_m=shape[0], block_n=shape[1])\n+\n+    if op == 'sum':\n+        golden_z = torch.sum(x, dim=axis, keepdim=False)\n+    elif op == 'min':\n+        golden_z = torch.min(x, dim=axis, keepdim=False)[0]\n+    else:\n+        golden_z = torch.max(x, dim=axis, keepdim=False)[0]\n+\n+    assert_close(z, golden_z, rtol=1e-3, atol=1e-3)"}]