[{"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -67,7 +67,7 @@ def nvsmi(attrs):\n         (64, 1024, 1024): {'float16': 0.0271, 'float32': 0.0509, 'int8': 0.0169},\n         (64, 4096, 4096): {'float16': 0.16, 'float32': 0.162, 'int8': 0.097},\n         (64, 8192, 8192): {'float16': 0.30, 'float32': 0.257, 'int8': 0.174},\n-        (1024, 64, 1024): {'float16': 0.037, 'float32': 0.0458, 'int8': 0.017},\n+        (1024, 64, 1024): {'float16': 0.0263, 'float32': 0.0458, 'int8': 0.017},\n         (4096, 64, 4096): {'float16': 0.16, 'float32': 0.177, 'int8': 0.102},\n         (8192, 64, 8192): {'float16': 0.25, 'float32': 0.230, 'int8': 0.177},\n     }\n@@ -97,7 +97,7 @@ def test_matmul(M, N, K, dtype_str):\n     ms = triton.testing.do_bench(fn, warmup=100, rep=300)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n \n #######################\n@@ -131,8 +131,8 @@ def _add(x_ptr, y_ptr, output_ptr, n_elements,\n     'a100': {\n         1024 * 16: 0.008,\n         1024 * 64: 0.034,\n-        1024 * 256: 0.132,\n-        1024 * 1024: 0.352,\n+        1024 * 256: 0.114,\n+        1024 * 1024: 0.315,\n         1024 * 4096: 0.580,\n         1024 * 16384: 0.782,\n         1024 * 65536: 0.850,\n@@ -153,7 +153,7 @@ def test_elementwise(N):\n     ms = triton.testing.do_bench(fn, warmup=100, rep=500)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n #######################\n # Flash-Attention\n@@ -201,4 +201,4 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n     max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, mode, dtype_str)]\n-    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)"}]