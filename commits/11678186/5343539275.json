[{"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -85,9 +85,9 @@ class MoveOpAfterLayoutConversion : public mlir::RewritePattern {\n   isBlockedToDotOperand(mlir::Operation *op,\n                         triton::gpu::DotOperandEncodingAttr &retEncoding,\n                         triton::gpu::BlockedEncodingAttr &srcEncoding) {\n-    if (!op)\n+    auto cvt = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(op);\n+    if (!cvt)\n       return failure();\n-    auto cvt = cast<triton::gpu::ConvertLayoutOp>(op);\n     auto srcTy = cvt.getOperand().getType().cast<RankedTensorType>();\n     auto retTy = cvt.getResult().getType().dyn_cast<RankedTensorType>();\n     retEncoding ="}, {"filename": "test/TritonGPU/dot-operands.mlir", "status": "modified", "additions": 21, "deletions": 0, "changes": 21, "file_content_changes": "@@ -125,4 +125,25 @@ tt.func @push_elementwise5(\n   tt.return %newc : tensor<16x16xf32, #Cv1>\n }\n \n+// CHECK: tt.func @succeeds_if_arg_is_not_convert_layout\n+// CHECK: %[[ALOAD:.*]] = tt.load %arg0\n+// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]]\n+// CHECK: %[[AF8E5:.*]] = tt.bitcast %[[ACVT]]\n+// CHECK: %[[AF16:.*]] = tt.fp_to_fp %[[AF8E5]]\n+// CHECK: %[[C:.*]] = tt.dot %[[AF16]]\n+// CHECK: tt.return %[[C]] : tensor<16x16xf32, #mma>\n+tt.func @succeeds_if_arg_is_not_convert_layout(\n+                   %pa: tensor<16x16x!tt.ptr<i8>, #ALR> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %pb: tensor<16x16x!tt.ptr<f16>, #BLC> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %c: tensor<16x16xf32, #Cv2>) -> tensor<16x16xf32, #Cv2>{\n+  %ai8 = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xi8, #ALR>\n+  %dotai8 = triton_gpu.convert_layout %ai8 : (tensor<16x16xi8, #ALR>) -> tensor<16x16xi8, #Av2>\n+  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BLC>\n+  %dotaf8 = tt.bitcast %dotai8 : tensor<16x16xi8, #Av2> -> tensor<16x16xf8E5M2, #Av2>\n+  %dota = tt.fp_to_fp %dotaf8 : tensor<16x16xf8E5M2, #Av2> -> tensor<16x16xf16, #Av2>\n+  %dotb = triton_gpu.convert_layout %b : (tensor<16x16xf16, #BLC>) -> tensor<16x16xf16, #Bv2>\n+  %newc = tt.dot %dota, %dotb, %c {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #Av2> * tensor<16x16xf16, #Bv2> -> tensor<16x16xf32, #Cv2>\n+  tt.return %newc : tensor<16x16xf32, #Cv2>\n+}\n+\n }"}]