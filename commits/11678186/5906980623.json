[{"filename": ".github/workflows/compare-artifacts.yml", "status": "added", "additions": 51, "deletions": 0, "changes": 51, "file_content_changes": "@@ -0,0 +1,51 @@\n+name: Compare Artifacts\n+on:\n+  workflow_run:\n+    workflows:\n+      - Integration Tests\n+    types:\n+      - completed\n+\n+jobs:\n+  Compare-artifacts:\n+    runs-on: ubuntu-latest\n+    if: ${{ github.event.workflow_run.conclusion == 'success' }}\n+\n+    steps:\n+      - name: 'Download artifact'\n+        uses: actions/github-script@v6\n+        with:\n+          script: |\n+            let allArtifacts = await github.rest.actions.listWorkflowRunArtifacts({\n+               owner: context.repo.owner,\n+               repo: context.repo.repo,\n+               run_id: context.payload.workflow_run.id,\n+            });\n+            let matchArtifact = allArtifacts.data.artifacts.filter((artifact) => {\n+              return artifact.name == \"pr_number\"\n+            })[0];\n+            let download = await github.rest.actions.downloadArtifact({\n+               owner: context.repo.owner,\n+               repo: context.repo.repo,\n+               artifact_id: matchArtifact.id,\n+               archive_format: 'zip',\n+            });\n+            let fs = require('fs');\n+            fs.writeFileSync(`${process.env.GITHUB_WORKSPACE}/pr_number.zip`, Buffer.from(download.data));\n+\n+      - name: 'Unzip artifact'\n+        run: unzip pr_number.zip\n+\n+      - name: 'Comment on PR'\n+        uses: actions/github-script@v6\n+        with:\n+          github-token: ${{ secrets.GITHUB_TOKEN }}\n+          script: |\n+            let fs = require('fs');\n+            let issue_number = Number(fs.readFileSync('./pr_number'));\n+            await github.rest.issues.createComment({\n+              owner: context.repo.owner,\n+              repo: context.repo.repo,\n+              issue_number: issue_number,\n+              body: `Ignore this message. This is to test another workflow posting comment on PR number ${issue_number}.`\n+            });"}, {"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "file_content_changes": "@@ -153,6 +153,16 @@ jobs:\n           sudo nvidia-smi -i 0 --lock-gpu-clocks=1280,1280\n           python3 -m pytest -vs . --reruns 10\n           sudo nvidia-smi -i 0 -rgc\n+      - name: Save PR number\n+        env:\n+          PR_NUMBER: ${{ github.event.number }}\n+        run: |\n+          mkdir -p ./pr\n+          echo $PR_NUMBER > ./pr/pr_number\n+      - uses: actions/upload-artifact@v3\n+        with:\n+          name: pr_number\n+          path: pr/\n \n   Integration-Tests-Third-Party:\n     needs: Runner-Preparation"}, {"filename": "bin/triton-translate.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -125,7 +125,7 @@ LogicalResult tritonTranslateMain(int argc, char **argv,\n   llvm::LLVMContext llvmContext;\n   mlir::triton::gpu::TMAMetadataTy tmaInfos;\n   auto llvmir = translateTritonGPUToLLVMIR(\n-      &llvmContext, *module, SMArch.getValue(), tmaInfos, false /*isRocm*/);\n+      &llvmContext, *module, SMArch.getValue(), tmaInfos, Target::Default);\n \n   if (!llvmir) {\n     llvm::errs() << \"Translate to LLVM IR failed\";"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/Passes.td", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "file_content_changes": "@@ -30,9 +30,13 @@ def ConvertTritonGPUToLLVM : Pass<\"convert-triton-gpu-to-llvm\", \"mlir::ModuleOp\"\n         Option<\"tmaMetadata\", \"tma-metadata\",\n                \"mlir::triton::gpu::TMAMetadataTy*\", /*default*/\"nullptr\",\n                \"tma metadata to the runtime\">,\n-        Option<\"isROCM\", \"is-rocm\",\n-               \"bool\", /*default*/\"false\",\n-               \"compile for ROCM-compatible LLVM\">,\n+        Option<\"target\", \"target\", \"enum Target\", \"mlir::triton::Target::Default\",\n+               \"compile for target compatible LLVM\",\n+               \"llvm::cl::values(\"\n+               \"clEnumValN(mlir::triton::Target::NVVM, \\\"nvvm\\\", \\\"compile for \"\n+               \"NVVM-compatible LLVM\\\"), \"\n+               \"clEnumValN(mlir::triton::Target::ROCDL, \\\"rocdl\\\", \\\"compile for \"\n+               \"ROCDL-compatible LLVM\\\"))\">,\n     ];\n }\n "}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -14,6 +14,8 @@ template <typename T> class OperationPass;\n \n namespace triton {\n \n+enum Target { NVVM, ROCDL, Default = NVVM };\n+\n #define GEN_PASS_DECL\n #include \"triton/Conversion/TritonGPUToLLVM/Passes.h.inc\"\n "}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 22, "deletions": 0, "changes": 22, "file_content_changes": "@@ -510,6 +510,28 @@ def TT_MakeRangeOp : TT_Op<\"make_range\", [Pure]> {\n     let assemblyFormat = \"attr-dict `:` type($result)\";\n }\n \n+//\n+// ElementwiseInlineAsm Op\n+//\n+def TT_ElementwiseInlineAsmOp : TT_Op<\"elementwise_inline_asm\", [Elementwise,\n+                                                                 SameOperandsAndResultEncoding,\n+                                                                 DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {\n+  let summary = \"inline assembly applying elementwise operation to a group of packed element.\";\n+  let description = [{\n+   This will apply the given in inline assembly to `packed_element` number of\n+   elements of the inputs. The elements packed together is unknown and will\n+   depend on the backend implementation.\n+  }];\n+\n+  let arguments = (ins StrAttr:$asm_string, StrAttr:$constraints, BoolAttr:$pure, I32Attr:$packed_element, Variadic<AnyTypeOf<[TT_Type]>>:$args);\n+  let results = (outs TT_Tensor:$result);\n+\n+\n+  let assemblyFormat = [{\n+    $asm_string attr-dict ($args^ `:` type($args))? `->` type($result)\n+  }];\n+}\n+\n //\n // Print Op\n //"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -19,8 +19,6 @@ class SharedEncodingAttr;\n }\n } // namespace triton\n \n-LogicalResult fixupLoops(ModuleOp mod);\n-\n SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n                                                 const ArrayRef<int64_t> &shape,\n                                                 RankedTensorType type);"}, {"filename": "include/triton/Target/LLVMIR/LLVMIRTranslation.h", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -1,5 +1,6 @@\n #ifndef TRITON_TARGET_LLVM_IR_LLVM_IR_TRANSLATION_H\n #define TRITON_TARGET_LLVM_IR_LLVM_IR_TRANSLATION_H\n+#include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n #include \"triton/Target/PTX/TmaMetadata.h\"\n #include \"llvm/ADT/StringRef.h\"\n #include <memory>\n@@ -28,15 +29,15 @@ std::unique_ptr<llvm::Module>\n translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n                            mlir::ModuleOp module, int computeCapability,\n                            mlir::triton::gpu::TMAMetadataTy &tmaInfos,\n-                           bool isROCM);\n+                           Target target);\n \n // Translate mlir LLVM dialect to LLVMIR, return null if failed.\n std::unique_ptr<llvm::Module>\n translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module,\n-                      bool isROCM);\n+                      Target target);\n \n bool linkExternLib(llvm::Module &module, llvm::StringRef name,\n-                   llvm::StringRef path, bool isROCM);\n+                   llvm::StringRef path, Target target);\n \n } // namespace triton\n } // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 81, "deletions": 0, "changes": 81, "file_content_changes": "@@ -811,6 +811,86 @@ struct ExternElementwiseOpConversion\n   }\n };\n \n+struct ElementwiseInlineAsmOpConversion\n+    : public ElementwiseOpConversionBase<ElementwiseInlineAsmOp,\n+                                         ElementwiseInlineAsmOpConversion> {\n+  using Base = ElementwiseOpConversionBase<ElementwiseInlineAsmOp,\n+                                           ElementwiseInlineAsmOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+  typedef typename Base::OpAdaptor OpAdaptor;\n+\n+  // If operand size is smaller than 32bits pack by groups of 32bits.\n+  // Otherwise have separate inputs.\n+  SmallVector<Value> packOperands(ElementwiseInlineAsmOp op,\n+                                  MultipleOperandsRange operands,\n+                                  ConversionPatternRewriter &rewriter,\n+                                  Location loc) const {\n+    SmallVector<Value> packedOperands;\n+    unsigned numPackedElements = op.getPackedElement();\n+    for (int i = 0, e = op.getNumOperands(); i < e; i++) {\n+      unsigned bitWidth =\n+          getElementType(op.getOperand(i)).getIntOrFloatBitWidth();\n+      unsigned numElementPerReg = bitWidth < 32 ? 32 / bitWidth : 1;\n+      numElementPerReg = std::min(numElementPerReg, numPackedElements);\n+      for (int j = 0; j < numPackedElements; j += numElementPerReg) {\n+        if (numElementPerReg == 1) {\n+          packedOperands.push_back(operands[j][i]);\n+          continue;\n+        }\n+        Type t = vec_ty(\n+            getTypeConverter()->convertType(getElementType(op.getOperand(i))),\n+            numElementPerReg);\n+        Value packed = undef(t);\n+        for (int k = 0; k < numElementPerReg; k++) {\n+          packed = insert_element(packed, operands[j + k][i], i32_val(k));\n+        }\n+        packedOperands.push_back(packed);\n+      }\n+    }\n+    return packedOperands;\n+  }\n+\n+  SmallVector<Value> createDestOps(ElementwiseInlineAsmOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n+    int numPackedElements = op.getPackedElement();\n+    if (operands.size() % numPackedElements != 0)\n+      llvm::report_fatal_error(\"Inline asm op has more packed elements than \"\n+                               \"number of elements per thread.\");\n+    SmallVector<Value> packedOperands =\n+        packOperands(op, operands, rewriter, loc);\n+    Type dstType =\n+        getTypeConverter()->convertType(getElementType(op.getResult()));\n+    Type retType = dstType;\n+    if (numPackedElements > 1)\n+      retType = vec_ty(retType, numPackedElements);\n+    Value result = rewriter\n+                       .create<LLVM::InlineAsmOp>(\n+                           loc, retType,\n+                           packedOperands,      // operands\n+                           op.getAsmString(),   // asm_string\n+                           op.getConstraints(), // constraints\n+                           !op.getPure(),       // has_side_effects\n+                           false,               // is_align_stack\n+                           LLVM::AsmDialectAttr::get(\n+                               rewriter.getContext(),\n+                               LLVM::AsmDialect::AD_ATT), // asm_dialect\n+                           ArrayAttr()                    // operand_attrs\n+                           )\n+                       ->getResult(0);\n+    SmallVector<Value> results;\n+    if (numPackedElements > 1) {\n+      for (int i = 0; i < numPackedElements; i++)\n+        results.push_back(extract_element(result, i32_val(i)));\n+    } else {\n+      results = {result};\n+    }\n+    return results;\n+  }\n+};\n+\n struct FDivOpConversion\n     : ElementwiseOpConversionBase<mlir::arith::DivFOp, FDivOpConversion> {\n   using Base =\n@@ -1213,6 +1293,7 @@ void populateElementwiseOpToLLVMPatterns(\n   patterns\n       .add<ExternElementwiseOpConversion<triton::ImpureExternElementwiseOp>>(\n           typeConverter, benefit);\n+  patterns.add<ElementwiseInlineAsmOpConversion>(typeConverter, benefit);\n   // ExpOpConversionApprox will try using ex2.approx if the input type is\n   // FP32. For other input types, ExpOpConversionApprox will return failure and\n   // ElementwiseOpConversion<math::ExpOp, math::ExpOp> defined below will call"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 17, "deletions": 4, "changes": 21, "file_content_changes": "@@ -307,8 +307,10 @@ struct ReduceOpConversion\n     Operation *yield = block->getTerminator();\n     Operation *reduceOp = yield->getOperand(0).getDefiningOp();\n     if (!reduceOp || reduceOp->getNumOperands() != 2 ||\n-        reduceOp->getNumResults() != 1 ||\n-        !reduceOp->getResultTypes()[0].isInteger(32))\n+        reduceOp->getNumResults() != 1)\n+      return std::nullopt;\n+    auto intType = reduceOp->getResultTypes()[0].dyn_cast<IntegerType>();\n+    if (!intType || intType.getWidth() > 32)\n       return std::nullopt;\n     if (reduceOp->getOperand(0) != block->getArgument(0) ||\n         reduceOp->getOperand(1) != block->getArgument(1))\n@@ -382,8 +384,19 @@ struct ReduceOpConversion\n           mask = shl(i32_val(bitmask),\n                      and_(laneId, i32_val(~(numLaneToReduce - 1))));\n         }\n-        acc[0] = rewriter.create<NVVM::ReduxOp>(loc, acc[0].getType(), acc[0],\n-                                                *kind, mask);\n+        for (unsigned i = 0; i < acc.size(); ++i) {\n+          unsigned bitwidth = acc[i].getType().cast<IntegerType>().getWidth();\n+          if (bitwidth < 32) {\n+            if (*kind == NVVM::ReduxKind::MIN || *kind == NVVM::ReduxKind::MAX)\n+              acc[i] = sext(i32_ty, acc[i]);\n+            else\n+              acc[i] = zext(i32_ty, acc[i]);\n+          }\n+          acc[i] = rewriter.create<NVVM::ReduxOp>(loc, acc[i].getType(), acc[0],\n+                                                  *kind, mask);\n+          if (bitwidth < 32)\n+            acc[i] = trunc(int_ty(bitwidth), acc[i]);\n+        }\n         return;\n       }\n     }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 24, "deletions": 15, "changes": 39, "file_content_changes": "@@ -63,14 +63,17 @@ static void addWSNamedAttrs(Operation *op,\n \n class TritonLLVMFunctionConversionTarget : public ConversionTarget {\n public:\n-  explicit TritonLLVMFunctionConversionTarget(MLIRContext &ctx, bool isROCM)\n+  explicit TritonLLVMFunctionConversionTarget(MLIRContext &ctx, Target target)\n       : ConversionTarget(ctx) {\n     addLegalDialect<index::IndexDialect>();\n     addLegalDialect<LLVM::LLVMDialect>();\n-    if (isROCM) {\n-      addLegalDialect<ROCDL::ROCDLDialect>();\n-    } else {\n+    switch (target) {\n+    case Target::NVVM:\n       addLegalDialect<NVVM::NVVMDialect>();\n+      break;\n+    case Target::ROCDL:\n+      addLegalDialect<ROCDL::ROCDLDialect>();\n+      break;\n     }\n     addLegalOp<mlir::UnrealizedConversionCastOp>();\n   }\n@@ -359,13 +362,16 @@ struct CallOpConversion : public ConvertOpToLLVMPattern<triton::CallOp> {\n \n class TritonLLVMConversionTarget : public ConversionTarget {\n public:\n-  explicit TritonLLVMConversionTarget(MLIRContext &ctx, bool isROCM)\n+  explicit TritonLLVMConversionTarget(MLIRContext &ctx, Target target)\n       : ConversionTarget(ctx) {\n     addLegalDialect<LLVM::LLVMDialect>();\n-    if (isROCM) {\n-      addLegalDialect<ROCDL::ROCDLDialect>();\n-    } else {\n+    switch (target) {\n+    case Target::NVVM:\n       addLegalDialect<NVVM::NVVMDialect>();\n+      break;\n+    case Target::ROCDL:\n+      addLegalDialect<ROCDL::ROCDLDialect>();\n+      break;\n     }\n     addLegalDialect<mlir::triton::nvgpu::NVGPUDialect>();\n     addIllegalDialect<triton::TritonDialect>();\n@@ -387,7 +393,7 @@ struct ConvertTritonGPUToLLVM\n     mlir::LowerToLLVMOptions option(context);\n     option.overrideIndexBitwidth(32);\n     TritonGPUToLLVMTypeConverter typeConverter(context, option);\n-    TritonLLVMConversionTarget target(*context, isROCM);\n+    TritonLLVMConversionTarget convTarget(*context, target);\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n     int numCTAs = triton::gpu::TritonGPUDialect::getNumCTAs(mod);\n     int threadsPerWarp = triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n@@ -441,7 +447,7 @@ struct ConvertTritonGPUToLLVM\n     {\n       mlir::LowerToLLVMOptions option(context);\n       TritonGPUToLLVMTypeConverter typeConverter(context, option);\n-      TritonLLVMFunctionConversionTarget funcTarget(*context, isROCM);\n+      TritonLLVMFunctionConversionTarget funcTarget(*context, target);\n       RewritePatternSet funcPatterns(context);\n       funcPatterns.add<FuncOpConversion>(typeConverter, numWarps, allocation,\n                                          /*benefit=*/1);\n@@ -461,7 +467,7 @@ struct ConvertTritonGPUToLLVM\n     {\n       mlir::LowerToLLVMOptions option(context);\n       TritonGPUToLLVMTypeConverter typeConverter(context, option);\n-      TritonLLVMFunctionConversionTarget funcTarget(*context, isROCM);\n+      TritonLLVMFunctionConversionTarget funcTarget(*context, target);\n       RewritePatternSet funcPatterns(context);\n       funcPatterns.add<CallOpConversion>(typeConverter, numWarps, allocation,\n                                          /*benefit=*/1);\n@@ -539,16 +545,19 @@ struct ConvertTritonGPUToLLVM\n     mlir::populateMathToLLVMConversionPatterns(typeConverter, patterns);\n \n     // Native lowering patterns\n-    if (isROCM) {\n+    switch (target) {\n+    case Target::NVVM:\n+      mlir::populateGpuToNVVMConversionPatterns(typeConverter, patterns);\n+      break;\n+    case Target::ROCDL:\n       mlir::populateGpuToROCDLConversionPatterns(typeConverter, patterns,\n                                                  mlir::gpu::amd::HIP);\n-    } else {\n-      mlir::populateGpuToNVVMConversionPatterns(typeConverter, patterns);\n+      break;\n     }\n \n     mlir::cf::populateControlFlowToLLVMConversionPatterns(typeConverter,\n                                                           patterns);\n-    if (failed(applyPartialConversion(mod, target, std::move(patterns))))\n+    if (failed(applyPartialConversion(mod, convTarget, std::move(patterns))))\n       return signalPassFailure();\n \n     // Fold CTAId when there is only 1 CTA."}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 20, "deletions": 20, "changes": 40, "file_content_changes": "@@ -695,26 +695,26 @@ class TritonReturnOpPattern : public OpConversionPattern<ReturnOp> {\n void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n                             RewritePatternSet &patterns, unsigned numCTAs) {\n   MLIRContext *context = patterns.getContext();\n-  patterns\n-      .insert< // TODO: view should have custom pattern that views the layout\n-          TritonGenericPattern<triton::AdvanceOp>,\n-          TritonGenericPattern<triton::MakeTensorPtrOp>,\n-          TritonGenericPattern<triton::ViewOp>,\n-          TritonGenericPattern<triton::BitcastOp>,\n-          TritonGenericPattern<triton::FpToFpOp>,\n-          TritonGenericPattern<triton::IntToPtrOp>,\n-          TritonGenericPattern<triton::PtrToIntOp>,\n-          TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n-          TritonGenericPattern<triton::AddPtrOp>, TritonCatPattern,\n-          TritonReducePattern, TritonReduceReturnPattern, TritonScanPattern,\n-          TritonScanReturnPattern, TritonTransPattern, TritonExpandDimsPattern,\n-          TritonMakeRangePattern, TritonDotPattern, TritonLoadPattern,\n-          TritonStorePattern,\n-          TritonExternElementwisePattern<triton::PureExternElementwiseOp>,\n-          TritonExternElementwisePattern<triton::ImpureExternElementwiseOp>,\n-          TritonPrintPattern, TritonAssertPattern, TritonAtomicRMWPattern,\n-          TritonFuncOpPattern, TritonReturnOpPattern, TritonCallOpPattern>(\n-          typeConverter, context);\n+  patterns.insert< // TODO: view should have custom pattern that views the\n+                   // layout\n+      TritonGenericPattern<triton::AdvanceOp>,\n+      TritonGenericPattern<triton::MakeTensorPtrOp>,\n+      TritonGenericPattern<triton::ViewOp>,\n+      TritonGenericPattern<triton::BitcastOp>,\n+      TritonGenericPattern<triton::FpToFpOp>,\n+      TritonGenericPattern<triton::IntToPtrOp>,\n+      TritonGenericPattern<triton::PtrToIntOp>,\n+      TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n+      TritonGenericPattern<triton::AddPtrOp>, TritonCatPattern,\n+      TritonGenericPattern<triton::ElementwiseInlineAsmOp>, TritonReducePattern,\n+      TritonReduceReturnPattern, TritonScanPattern, TritonScanReturnPattern,\n+      TritonTransPattern, TritonExpandDimsPattern, TritonMakeRangePattern,\n+      TritonDotPattern, TritonLoadPattern, TritonStorePattern,\n+      TritonExternElementwisePattern<triton::PureExternElementwiseOp>,\n+      TritonExternElementwisePattern<triton::ImpureExternElementwiseOp>,\n+      TritonPrintPattern, TritonAssertPattern, TritonAtomicRMWPattern,\n+      TritonFuncOpPattern, TritonReturnOpPattern, TritonCallOpPattern>(\n+      typeConverter, context);\n }\n \n //"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -870,5 +870,17 @@ LogicalResult triton::ReturnOp::verify() {\n   return success();\n }\n \n+// -- ElementwiseInlineAsmOp --\n+void ElementwiseInlineAsmOp::getEffects(\n+    SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>>\n+        &effects) {\n+  if (getPure())\n+    return;\n+  effects.emplace_back(MemoryEffects::Write::get(),\n+                       SideEffects::DefaultResource::get());\n+  effects.emplace_back(MemoryEffects::Read::get(),\n+                       SideEffects::DefaultResource::get());\n+}\n+\n } // namespace triton\n } // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -248,8 +248,6 @@ class TritonGPUOptimizeDotOperandsPass\n     patterns.add<FuseTransHopper>(context);\n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n       signalPassFailure();\n-    if (fixupLoops(m).failed())\n-      signalPassFailure();\n   }\n };\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeEpilogue.cpp", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -127,9 +127,6 @@ class TritonGPUOptimizeEpiloguePass\n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n       signalPassFailure();\n     }\n-    if (fixupLoops(m).failed()) {\n-      signalPassFailure();\n-    }\n   }\n };\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 12, "deletions": 30, "changes": 42, "file_content_changes": "@@ -262,12 +262,12 @@ class MoveConvertOutOfIf : public mlir::RewritePattern {\n \n     IRMapping mapping;\n     for (size_t i = 0; i < numOps; i++) {\n-      auto thenCvt = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n-          thenYield.getOperand(i).getDefiningOp());\n+      auto thenCvt =\n+          thenYield.getOperand(i).getDefiningOp<triton::gpu::ConvertLayoutOp>();\n       if (hasElse) {\n         auto elseYield = ifOp.elseYield();\n-        auto elseCvt = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n-            elseYield.getOperand(i).getDefiningOp());\n+        auto elseCvt = elseYield.getOperand(i)\n+                           .getDefiningOp<triton::gpu::ConvertLayoutOp>();\n         if (thenCvt && elseCvt &&\n             std::distance(elseCvt->user_begin(), elseCvt->user_end()) == 1 &&\n             std::distance(thenCvt->user_begin(), thenCvt->user_end()) == 1 &&\n@@ -585,40 +585,26 @@ class ConvertDotConvert : public mlir::RewritePattern {\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n     auto dstOp = cast<triton::gpu::ConvertLayoutOp>(op);\n-    auto dotOp =\n-        dyn_cast_or_null<triton::DotOp>(dstOp.getSrc().getDefiningOp());\n+    auto dotOp = dstOp.getSrc().getDefiningOp<triton::DotOp>();\n     if (!dotOp)\n       return mlir::failure();\n     if (std::distance(dstOp->user_begin(), dstOp->user_end()) != 1 ||\n         std::distance(dotOp->user_begin(), dotOp->user_end()) != 1)\n       return mlir::failure();\n-    auto cvtOp = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n-        dotOp.getOperand(2).getDefiningOp());\n+    auto cvtOp =\n+        dotOp.getOperand(2).getDefiningOp<triton::gpu::ConvertLayoutOp>();\n     if (!cvtOp)\n       return mlir::failure();\n-    auto loadOp =\n-        dyn_cast_or_null<triton::LoadOp>(cvtOp.getSrc().getDefiningOp());\n-    if (!loadOp)\n-      return mlir::failure();\n+    if (!cvtOp.getSrc().getDefiningOp<triton::LoadOp>())\n+      return failure();\n     auto dstTy = dstOp.getResult().getType().cast<RankedTensorType>();\n     auto srcTy = cvtOp.getOperand().getType().cast<RankedTensorType>();\n     if (dstTy != srcTy)\n       return mlir::failure();\n \n-    // TODO: int tensor cores\n-    auto out_dtype = dstTy.getElementType().cast<FloatType>();\n-    APFloat value(0.0f);\n-    if (out_dtype.isBF16())\n-      value = APFloat(APFloat::IEEEhalf(), APInt(16, 0));\n-    else if (out_dtype.isF16())\n-      value = APFloat(APFloat::IEEEhalf(), APInt(16, 0));\n-    else if (out_dtype.isF32())\n-      value = APFloat(0.0f);\n-    else\n-      llvm_unreachable(\"unsupported data type\");\n-\n-    auto _0f =\n-        rewriter.create<arith::ConstantFloatOp>(op->getLoc(), value, out_dtype);\n+    auto _0f = rewriter.create<arith::ConstantOp>(\n+        op->getLoc(), dstTy.getElementType(),\n+        rewriter.getZeroAttr(dstTy.getElementType()));\n     auto _0 = rewriter.create<triton::SplatOp>(\n         op->getLoc(), dotOp.getResult().getType(), _0f);\n     auto newDot = rewriter.create<triton::DotOp>(\n@@ -660,10 +646,6 @@ class TritonGPURemoveLayoutConversionsPass\n     if (mlir::applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n       signalPassFailure();\n     }\n-\n-    if (fixupLoops(m).failed()) {\n-      signalPassFailure();\n-    }\n   }\n };\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 0, "deletions": 55, "changes": 55, "file_content_changes": "@@ -9,61 +9,6 @@\n \n namespace mlir {\n \n-namespace {\n-\n-class FixupLoop : public mlir::RewritePattern {\n-\n-public:\n-  explicit FixupLoop(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(scf::ForOp::getOperationName(), 2, context) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto forOp = cast<scf::ForOp>(op);\n-\n-    // Rewrite init argument\n-    SmallVector<Value, 4> newInitArgs = forOp.getInitArgs();\n-    bool shouldRematerialize = false;\n-    for (size_t i = 0; i < newInitArgs.size(); i++) {\n-      if (newInitArgs[i].getType() != forOp.getRegionIterArgs()[i].getType() ||\n-          newInitArgs[i].getType() != forOp.getResultTypes()[i]) {\n-        shouldRematerialize = true;\n-        break;\n-      }\n-    }\n-    if (!shouldRematerialize)\n-      return failure();\n-\n-    scf::ForOp newForOp = rewriter.create<scf::ForOp>(\n-        forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n-        forOp.getStep(), newInitArgs);\n-    newForOp->moveBefore(forOp);\n-    rewriter.setInsertionPointToStart(newForOp.getBody());\n-    IRMapping mapping;\n-    for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n-      mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n-    mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n-\n-    for (Operation &op : forOp.getBody()->getOperations()) {\n-      rewriter.clone(op, mapping);\n-    }\n-    rewriter.replaceOp(forOp, newForOp.getResults());\n-    return success();\n-  }\n-};\n-\n-} // namespace\n-\n-LogicalResult fixupLoops(ModuleOp mod) {\n-  auto *ctx = mod.getContext();\n-  mlir::RewritePatternSet patterns(ctx);\n-  patterns.add<FixupLoop>(ctx);\n-  if (applyPatternsAndFoldGreedily(mod, std::move(patterns)).failed())\n-    return failure();\n-  return success();\n-}\n-\n SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n                                                 const ArrayRef<int64_t> &shape,\n                                                 RankedTensorType type) {"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 16, "deletions": 14, "changes": 30, "file_content_changes": "@@ -55,7 +55,7 @@ struct NVVMMetadata {\n \n // Add the nvvm related metadata to LLVM IR.\n static void amendLLVMFunc(llvm::Function *func, const NVVMMetadata &metadata,\n-                          bool isROCM) {\n+                          Target target) {\n   auto *module = func->getParent();\n   auto &ctx = func->getContext();\n \n@@ -85,16 +85,19 @@ static void amendLLVMFunc(llvm::Function *func, const NVVMMetadata &metadata,\n   }\n \n   if (metadata.isKernel) {\n-    if (isROCM) {\n-      func->setCallingConv(llvm::CallingConv::AMDGPU_KERNEL);\n-      func->addFnAttr(\"amdgpu-flat-work-group-size\", \"1, 1024\");\n-    } else {\n+    switch (target) {\n+    case Target::NVVM: {\n       llvm::Metadata *mdArgs[] = {\n           llvm::ValueAsMetadata::get(func), llvm::MDString::get(ctx, \"kernel\"),\n           llvm::ValueAsMetadata::get(\n               llvm::ConstantInt::get(llvm::Type::getInt32Ty(ctx), 1))};\n       module->getOrInsertNamedMetadata(\"nvvm.annotations\")\n           ->addOperand(llvm::MDNode::get(ctx, mdArgs));\n+    } break;\n+    case Target::ROCDL: {\n+      func->setCallingConv(llvm::CallingConv::AMDGPU_KERNEL);\n+      func->addFnAttr(\"amdgpu-flat-work-group-size\", \"1, 1024\");\n+    } break;\n     }\n   }\n }\n@@ -240,7 +243,7 @@ static void linkLibdevice(llvm::Module &module) {\n }\n \n bool linkExternLib(llvm::Module &module, llvm::StringRef name,\n-                   llvm::StringRef path, bool isROCM) {\n+                   llvm::StringRef path, Target target) {\n   llvm::SMDiagnostic err;\n   auto &ctx = module.getContext();\n \n@@ -259,8 +262,7 @@ bool linkExternLib(llvm::Module &module, llvm::StringRef name,\n     return true;\n   }\n \n-  // check if ROCM\n-  if (!isROCM) {\n+  if (target == Target::NVVM) {\n     if (name == \"libdevice\") {\n       linkLibdevice(module);\n     }\n@@ -274,7 +276,7 @@ bool linkExternLib(llvm::Module &module, llvm::StringRef name,\n \n std::unique_ptr<llvm::Module>\n translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module,\n-                      bool isROCM) {\n+                      Target target) {\n   DialectRegistry registry;\n   mlir::registerBuiltinDialectTranslation(registry);\n   mlir::registerLLVMDialectTranslation(registry);\n@@ -302,7 +304,7 @@ translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module,\n   // dead code.\n   auto externLibs = getExternLibs(module);\n   for (auto &lib : externLibs) {\n-    if (linkExternLib(*llvmModule, lib.first, lib.second, isROCM))\n+    if (linkExternLib(*llvmModule, lib.first, lib.second, target))\n       return nullptr;\n   }\n \n@@ -318,7 +320,7 @@ translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module,\n   for (auto &func : llvmModule->functions()) {\n     auto it = nvvmMetadata.find(func.getName());\n     if (it != nvvmMetadata.end())\n-      amendLLVMFunc(&func, it->second, isROCM);\n+      amendLLVMFunc(&func, it->second, target);\n   }\n \n   return llvmModule;\n@@ -328,7 +330,7 @@ std::unique_ptr<llvm::Module>\n translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n                            mlir::ModuleOp module, int computeCapability,\n                            mlir::triton::gpu::TMAMetadataTy &tmaInfos,\n-                           bool isROCM) {\n+                           Target target) {\n   mlir::PassManager pm(module->getContext());\n   mlir::registerPassManagerCLOptions();\n   if (failed(applyPassManagerCLOptions(pm))) {\n@@ -351,7 +353,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n   pm.addPass(mlir::createConvertSCFToCFPass());\n   pm.addPass(mlir::createConvertIndexToLLVMPass());\n   pm.addPass(\n-      createConvertTritonGPUToLLVMPass({computeCapability, &tmaInfos, isROCM}));\n+      createConvertTritonGPUToLLVMPass({computeCapability, &tmaInfos, target}));\n   pm.addPass(createConvertNVGPUToLLVMPass());\n   pm.addPass(mlir::createArithToLLVMConversionPass());\n   pm.addPass(mlir::createCanonicalizerPass());\n@@ -366,7 +368,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n     return nullptr;\n   }\n \n-  auto llvmIR = translateLLVMToLLVMIR(llvmContext, module, isROCM);\n+  auto llvmIR = translateLLVMToLLVMIR(llvmContext, module, target);\n   if (!llvmIR) {\n     llvm::errs() << \"Translate to LLVM IR failed\";\n     return nullptr;"}, {"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -56,7 +56,7 @@ static void linkExternal(llvm::Module &module) {\n   //     std::filesystem::path(__BUILD_DIR__) / \"lib\" / \"Hopper\" /\n   //     \"libhopper_helpers.bc\";\n   if (mlir::triton::linkExternLib(module, \"libhopper_helpers\", path.string(),\n-                                  /*isROCM*/ false))\n+                                  mlir::triton::Target::NVVM))\n     llvm::errs() << \"Link failed for: libhopper_helpers.bc\";\n }\n "}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 16, "deletions": 2, "changes": 18, "file_content_changes": "@@ -81,6 +81,11 @@ void init_triton_runtime(py::module &&m) {\n       .value(\"CUDA\", CUDA)\n       .value(\"ROCM\", ROCM)\n       .export_values();\n+\n+  py::enum_<mlir::triton::Target>(m, \"TARGET\")\n+      .value(\"NVVM\", mlir::triton::NVVM)\n+      .value(\"ROCDL\", mlir::triton::ROCDL)\n+      .export_values();\n }\n \n // A custom op builder that keeps track of the last location\n@@ -1519,6 +1524,14 @@ void init_triton_ir(py::module &&m) {\n              return self.create<mlir::arith::SelectOp>(condition, trueValue,\n                                                        falseValue);\n            })\n+      .def(\"create_inline_asm\",\n+           [](TritonOpBuilder &self, const std::string &inlineAsm,\n+              const std::string &constraints,\n+              const std::vector<mlir::Value> &values, mlir::Type &type,\n+              bool isPure, int pack) -> mlir::Value {\n+             return self.create<mlir::triton::ElementwiseInlineAsmOp>(\n+                 type, inlineAsm, constraints, isPure, pack, values);\n+           })\n       .def(\"create_print\",\n            [](TritonOpBuilder &self, const std::string &prefix,\n               const std::vector<mlir::Value> &values) -> void {\n@@ -1804,11 +1817,12 @@ void init_triton_translation(py::module &m) {\n   m.def(\n       \"translate_triton_gpu_to_llvmir\",\n       [](mlir::ModuleOp op, int computeCapability,\n-         mlir::triton::gpu::TMAMetadataTy &tmaInfos, bool isROCM) {\n+         mlir::triton::gpu::TMAMetadataTy &tmaInfos,\n+         mlir::triton::Target target) {\n         py::gil_scoped_release allow_threads;\n         llvm::LLVMContext llvmContext;\n         auto llvmModule = ::mlir::triton::translateTritonGPUToLLVMIR(\n-            &llvmContext, op, computeCapability, tmaInfos, isROCM);\n+            &llvmContext, op, computeCapability, tmaInfos, target);\n         if (!llvmModule)\n           llvm::report_fatal_error(\"Failed to translate TritonGPU to LLVM IR.\");\n "}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 56, "deletions": 0, "changes": 56, "file_content_changes": "@@ -225,3 +225,59 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, seq_par, causal, mode, dtype_str):\n     ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, seq_par, causal, mode, dtype_str)]\n     print_perf(ms, cur_gpu_util, ref_gpu_util)\n     triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.02, rtol=0.01)\n+\n+\n+#######################\n+# Reduction\n+#######################\n+\n+\n+@triton.jit\n+def _sum(x_ptr, y_ptr, output_ptr, n_elements,\n+         BLOCK_SIZE: tl.constexpr):\n+    pid = tl.program_id(axis=0)\n+    block_start = pid * BLOCK_SIZE\n+    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+    mask = offsets < n_elements\n+    x = tl.load(x_ptr + offsets, mask=mask)\n+    y = tl.load(y_ptr + offsets, mask=mask)\n+    # run in a loop to only to make it compute bound.\n+    for i in range(100):\n+        x = tl.sum(x, axis=0) + y\n+\n+    tl.store(output_ptr + offsets, x, mask=mask)\n+\n+\n+reduction_data = {\n+    'a100': {\n+        1024 * 16384: {'float16': 0.016, 'float32': 0.031, 'int16': 0.015, 'int32': 0.031},\n+        1024 * 65536: {'float16': 0.016, 'float32': 0.032, 'int16': 0.015, 'int32': 0.032},\n+    }\n+}\n+\n+\n+@pytest.mark.parametrize('N', reduction_data[DEVICE_NAME].keys())\n+@pytest.mark.parametrize(\"dtype_str\", ['float16', 'float32', 'int16', 'int32'])\n+def test_reductions(N, dtype_str):\n+    stream = torch.cuda.Stream()\n+    torch.cuda.set_stream(stream)\n+    torch.manual_seed(0)\n+    dtype = {'float16': torch.float16, 'float32': torch.float32, 'int16': torch.int16, 'int32': torch.int32}[dtype_str]\n+    ref_gpu_util = reduction_data[DEVICE_NAME][N][dtype_str]\n+    cur_sm_clock = nvsmi(['clocks.current.sm'])[0]\n+    max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n+    z = torch.empty((N, ), dtype=dtype, device='cuda')\n+    if dtype == torch.float16 or dtype == torch.float32:\n+        x = torch.randn_like(z)\n+        y = torch.randn_like(z)\n+    else:\n+        info = torch.iinfo(dtype)\n+        x = torch.randint(info.min, info.max, (N,), dtype=dtype, device='cuda')\n+        y = torch.randint(info.min, info.max, (N,), dtype=dtype, device='cuda')\n+    grid = lambda args: (triton.cdiv(N, args['BLOCK_SIZE']), )\n+    fn = lambda: _sum[grid](x, y, z, N, BLOCK_SIZE=1024)\n+    ms = triton.testing.do_bench_cudagraph(fn)\n+    cur_gpu_perf = 100. * 2. * N / ms * 1e-9\n+    cur_gpu_util = cur_gpu_perf / max_gpu_perf\n+    print_perf(ms, cur_gpu_util, ref_gpu_util)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.02, rtol=0.01)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 54, "deletions": 2, "changes": 56, "file_content_changes": "@@ -1485,8 +1485,6 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n def get_reduced_dtype(dtype_str, op):\n     if op in ('argmin', 'argmax'):\n         return 'int32'\n-    if dtype_str in ['int8', 'uint8', 'int16', 'uint16']:\n-        return 'int32'\n     if dtype_str == 'bfloat16':\n         return 'float32'\n     return dtype_str\n@@ -3038,6 +3036,60 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n     # compare\n     np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n \n+\n+# -----------------------\n+# test inline asm\n+# -----------------------\n+\n+@pytest.mark.parametrize(\"num_ctas\", num_ctas_list)\n+def test_inline_asm(num_ctas, device):\n+    check_cuda_only(device)\n+\n+    @triton.jit\n+    def kernel(X, Y, Z, n: tl.constexpr, BLOCK: tl.constexpr):\n+        x = tl.load(X + tl.arange(0, BLOCK))\n+        y = tl.load(Y + tl.arange(0, BLOCK))\n+        s = tl.full([BLOCK], n, tl.int32)\n+        z = tl.inline_asm_elementwise(\"shf.l.wrap.b32 $0, $1, $2, $3;\", \"=r,r, r, r\", [x, y, s], dtype=tl.int32, is_pure=True, pack=1)\n+        tl.store(Z + tl.arange(0, BLOCK), z)\n+\n+    shape = (128, )\n+    rs = RandomState(17)\n+    x = numpy_random(shape, dtype_str='uint32', rs=rs)\n+    y = numpy_random(shape, dtype_str='uint32', rs=rs)\n+    x_tri = to_triton(x, device=device)\n+    y_tri = to_triton(y, device=device)\n+    n = 17\n+    z_tri = to_triton(numpy_random(shape, dtype_str='uint32', rs=rs), device=device)\n+    kernel[(1,)](x_tri, y_tri, z_tri, n, BLOCK=shape[0], num_ctas=num_ctas)\n+    y_ref = (y << n) | (x >> (32 - n))\n+    # compare\n+    np.testing.assert_equal(y_ref, to_numpy(z_tri))\n+\n+\n+@pytest.mark.parametrize(\"num_ctas\", num_ctas_list)\n+def test_inline_asm_packed(num_ctas, device):\n+    check_cuda_only(device)\n+    \n+    @triton.jit\n+    def kernel(X, Y, BLOCK: tl.constexpr):\n+        x = tl.load(X + tl.arange(0, BLOCK))\n+        # shift 4x8bits values together.\n+        y = tl.inline_asm_elementwise(\"and.b32 $0, $1, 0x1F1F1F1F; \\\n+                                       shl.b32 $0, $0, 3;\",\n+                                      \"=r,r\", [x,], dtype=tl.int8, is_pure=True, pack=4)\n+        tl.store(Y + tl.arange(0, BLOCK), y)\n+\n+    shape = (512, )\n+    rs = RandomState(17)\n+    x = numpy_random(shape, dtype_str='uint8', rs=rs)\n+    x_tri = to_triton(x, device=device)\n+    y_tri = to_triton(numpy_random(shape, dtype_str='uint8', rs=rs), device=device)\n+    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], num_ctas=num_ctas)\n+    y_ref = x << 3\n+    # compare\n+    np.testing.assert_equal(y_ref, to_numpy(y_tri))\n+\n # -----------------------\n # test control flow\n # -----------------------"}, {"filename": "python/triton/common/build.py", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -18,7 +18,7 @@ def is_hip():\n \n @functools.lru_cache()\n def libcuda_dirs():\n-    libs = subprocess.check_output([\"ldconfig\", \"-p\"]).decode()\n+    libs = subprocess.check_output([\"/sbin/ldconfig\", \"-p\"]).decode()\n     # each line looks like the following:\n     # libcuda.so.1 (libc6,x86-64) => /lib/x86_64-linux-gnu/libcuda.so.1\n     locs = [line.split()[-1] for line in libs.splitlines() if \"libcuda.so\" in line]\n@@ -27,6 +27,9 @@ def libcuda_dirs():\n     if locs:\n         msg += 'Possible files are located at %s.' % str(locs)\n         msg += 'Please create a symlink of libcuda.so to any of the file.'\n+    else:\n+        msg += 'Please make sure GPU is setup and then run \"/sbin/ldconfig\"'\n+        msg += ' (requires sudo) to refresh the linker cache.'\n     assert any(os.path.exists(os.path.join(path, 'libcuda.so')) for path in dirs), msg\n     return dirs\n "}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -13,7 +13,7 @@\n \n from .._C.libtriton.triton import (ClusterInfo, TMAInfos, add_external_libs,\n                                    compile_ptx_to_cubin, get_env_vars, get_num_warps,\n-                                   get_shared_memory_size, ir,\n+                                   get_shared_memory_size, ir, runtime,\n                                    translate_llvmir_to_hsaco, translate_llvmir_to_ptx,\n                                    translate_triton_gpu_to_llvmir)\n from ..common.backend import get_backend, path_to_ptxas\n@@ -142,9 +142,9 @@ def ttgir_to_llir(mod, extern_libs, arch, tma_infos):\n         _add_external_libs(mod, extern_libs)\n     # TODO: separate tritongpu_to_llvmir for different backends\n     if _is_cuda(arch):\n-        return translate_triton_gpu_to_llvmir(mod, arch, tma_infos, False)\n+        return translate_triton_gpu_to_llvmir(mod, arch, tma_infos, runtime.TARGET.NVVM)\n     else:\n-        return translate_triton_gpu_to_llvmir(mod, 0, True)\n+        return translate_triton_gpu_to_llvmir(mod, 0, TMAInfos(), runtime.TARGET.ROCDL)\n \n \n # PTX translation"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -54,6 +54,7 @@\n     float8e4,\n     float8e5,\n     function_type,\n+    inline_asm_elementwise,\n     int1,\n     int16,\n     int32,\n@@ -154,6 +155,7 @@\n     \"float8e5\",\n     \"full\",\n     \"function_type\",\n+    \"inline_asm_elementwise\",\n     \"int1\",\n     \"int16\",\n     \"int32\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 40, "deletions": 5, "changes": 45, "file_content_changes": "@@ -1351,11 +1351,6 @@ def make_combine_region(reduce_op):\n @builtin\n def _promote_reduction_input(t, _builder=None):\n     scalar_ty = t.type.scalar\n-    # input is extended to 32-bits if necessary\n-    # this increases numerical accuracy and can be done pretty much for free\n-    # on GPUs\n-    if scalar_ty.is_int() and scalar_ty.int_bitwidth < 32:\n-        return t.to(int32, _builder=_builder)\n \n     # hardware doesn't support FMAX, FMIN, CMP for bfloat16\n     if scalar_ty is bfloat16:\n@@ -1793,6 +1788,46 @@ def device_assert(cond, msg=\"\", _builder=None):\n     return semantic.device_assert(_to_tensor(cond, _builder), msg, file_name, func_name, lineno, _builder)\n \n \n+@builtin\n+def inline_asm_elementwise(asm: str, constraints: str, args: list, dtype, is_pure: bool, pack: int, _builder=None):\n+    '''\n+        Execute the inline assembly to a packed of elements of the tensor\n+        :param asm: assembly to be inlined, it has to match the target assembly format\n+        :param constraints: string representing the mapping of operands to register\n+        :param args: the arguments of the operation\n+        :param dtype: the element type of the returned variable\n+        :param is_pure: whether the operation is pure\n+        :param pack: the number of elements to be processed by one instance of inline assembly\n+        :param _builder: the builder\n+        :return: the return value of the function\n+    '''\n+    dispatch_args = args.copy()\n+    asm = _constexpr_to_value(asm)\n+    constraints = _constexpr_to_value(constraints)\n+    pack = _constexpr_to_value(pack)\n+    is_pure = _constexpr_to_value(is_pure)\n+    ret_shape = None\n+    arg_types = []\n+    for i in range(len(dispatch_args)):\n+        dispatch_args[i] = _to_tensor(dispatch_args[i], _builder)\n+        arg_types.append(dispatch_args[i].dtype)\n+    if len(arg_types) > 0:\n+        arg_types = tuple(arg_types)\n+        broadcast_arg = dispatch_args[0]\n+        # Get the broadcast shape over all the arguments\n+        for i, item in enumerate(dispatch_args):\n+            _, broadcast_arg = semantic.binary_op_type_checking_impl(\n+                item, broadcast_arg, _builder, arithmetic_check=False)\n+        # Change the shape of each argument based on the broadcast shape\n+        for i in range(len(dispatch_args)):\n+            dispatch_args[i], _ = semantic.binary_op_type_checking_impl(\n+                dispatch_args[i], broadcast_arg, _builder, arithmetic_check=False)\n+    ret_shape = broadcast_arg.shape\n+    res_ty = block_type(dtype, ret_shape).to_ir(_builder)\n+    call = _builder.create_inline_asm(asm, constraints, [t.handle for t in args], res_ty, is_pure, pack)\n+    return tensor(call, block_type(dtype, ret_shape))\n+\n+\n # -----------------------\n # Iterators\n # -----------------------"}, {"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -342,7 +342,7 @@ def backward(ctx, do):\n         dk = torch.empty_like(k)\n         dv = torch.empty_like(v)\n         delta = torch.empty_like(L)\n-        _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n+        _bwd_preprocess[(cdiv(q.shape[2], BLOCK) * ctx.grid[1], )](\n             o, do,\n             delta,\n             BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -377,9 +377,9 @@ def get_max_tensorcore_tflops(dtype, backend=None, device=None, clock_rate=None)\n         assert dtype == torch.float16\n         ops_per_sub_core = 256  # 2 4x4x4 Tensor Cores\n     else:\n-        if dtype == torch.float32:\n+        if dtype in [torch.float32, torch.int32]:\n             ops_per_sub_core = 256\n-        elif dtype in [torch.float16, torch.bfloat16]:\n+        elif dtype in [torch.float16, torch.bfloat16, torch.int16]:\n             ops_per_sub_core = 512\n         elif dtype in [torch.int8, tl.float8e4, tl.float8e4b15, tl.float8e5]:\n             ops_per_sub_core = 1024"}, {"filename": "test/Conversion/triton_ops.mlir", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -201,5 +201,12 @@ tt.func @scan_op(%ptr: tensor<1x2x4x!tt.ptr<f32>>, %v : tensor<1x2x4xf32>) {\n   }) : (tensor<1x2x4xf32>) -> tensor<1x2x4xf32>\n   tt.store %ptr, %a : tensor<1x2x4xf32>\n   tt.return\n+}\n \n+// CHECK-LABEL: inline_asm\n+// CHECK: tt.elementwise_inline_asm \"shl.b32 $0, $0, 3;\"\n+tt.func @inline_asm(%0: tensor<512xi8>) {\n+  %1 = tt.elementwise_inline_asm \"shl.b32 $0, $0, 3;\"\n+    {constraints = \"=r,r\", packed_element = 4 : i32, pure = true} %0 : tensor<512xi8> -> tensor<512xi8>\n+  tt.return\n }"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 40, "deletions": 1, "changes": 41, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -split-input-file --convert-triton-gpu-to-llvm | FileCheck %s\n+// RUN: triton-opt %s -split-input-file --convert-triton-gpu-to-llvm=\"target=nvvm\" | FileCheck %s\n \n module attributes {\"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK: llvm.func @test_empty_kernel(%arg0: i64, %arg1: !llvm.ptr<f16, 1>)\n@@ -1396,3 +1396,42 @@ module attributes {\"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 2 :\n     tt.return\n   }\n }\n+\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+module attributes {\"triton_gpu.compute-capability\" = 80 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  // CHECK-LABEL: inline_asm\n+  tt.func public @inline_asm(%arg0: !tt.ptr<i8, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8, 1> {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n+    %0 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked>\n+    %1 = tt.splat %arg0 : (!tt.ptr<i8, 1>) -> tensor<512x!tt.ptr<i8, 1>, #blocked>\n+    %2 = tt.addptr %1, %0 : tensor<512x!tt.ptr<i8, 1>, #blocked>, tensor<512xi32, #blocked>\n+    %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512xi8, #blocked>\n+// CHECK: %{{.*}} = llvm.inline_asm asm_dialect = att \"shl.b32 $0, $0, 3;\", \"=r,r\" %{{.*}} : (vector<4xi8>) -> vector<4xi8>\n+    %4 = tt.elementwise_inline_asm \"shl.b32 $0, $0, 3;\" {constraints = \"=r,r\", packed_element = 4 : i32, pure = true} %3 : tensor<512xi8, #blocked> -> tensor<512xi8, #blocked>\n+    %5 = tt.splat %arg1 : (!tt.ptr<i8, 1>) -> tensor<512x!tt.ptr<i8, 1>, #blocked>\n+    %6 = tt.addptr %5, %0 : tensor<512x!tt.ptr<i8, 1>, #blocked>, tensor<512xi32, #blocked>\n+    tt.store %6, %4 {cache = 1 : i32, evict = 1 : i32} : tensor<512xi8, #blocked>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+module attributes {\"triton_gpu.compute-capability\" = 80 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  // CHECK-LABEL: inline_asm_pack_16bit\n+  tt.func public @inline_asm_pack_16bit(%arg0: !tt.ptr<i8, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8, 1> {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n+    %0 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked>\n+    %1 = tt.splat %arg0 : (!tt.ptr<i8, 1>) -> tensor<512x!tt.ptr<i8, 1>, #blocked>\n+    %2 = tt.addptr %1, %0 : tensor<512x!tt.ptr<i8, 1>, #blocked>, tensor<512xi32, #blocked>\n+    %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512xi8, #blocked>\n+// CHECK: %{{.*}} = llvm.inline_asm asm_dialect = att \"shl.b16 $0, $0, 3;\", \"=h,h\" %{{.*}} : (vector<2xi8>) -> vector<2xi8>\n+    %4 = tt.elementwise_inline_asm \"shl.b16 $0, $0, 3;\" {constraints = \"=h,h\", packed_element = 2 : i32, pure = true} %3 : tensor<512xi8, #blocked> -> tensor<512xi8, #blocked>\n+    %5 = tt.splat %arg1 : (!tt.ptr<i8, 1>) -> tensor<512x!tt.ptr<i8, 1>, #blocked>\n+    %6 = tt.addptr %5, %0 : tensor<512x!tt.ptr<i8, 1>, #blocked>, tensor<512xi32, #blocked>\n+    tt.store %6, %4 {cache = 1 : i32, evict = 1 : i32} : tensor<512xi8, #blocked>\n+    tt.return\n+  }\n+}"}]