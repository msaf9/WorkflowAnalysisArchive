[{"filename": "include/triton/Analysis/Membar.h", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -29,7 +29,11 @@ class MembarAnalysis {\n   /// The following circumstances are not considered yet:\n   /// - Double buffers\n   /// - N buffers\n-  MembarAnalysis(Allocation *allocation) : allocation(allocation) { run(); }\n+  MembarAnalysis(Allocation *allocation) : allocation(allocation) {}\n+\n+  /// Runs the membar analysis to the given operation, inserts a barrier if\n+  /// necessary.\n+  void run();\n \n private:\n   struct RegionInfo {\n@@ -82,10 +86,6 @@ class MembarAnalysis {\n     }\n   };\n \n-  /// Runs the membar analysis to the given operation, inserts a barrier if\n-  /// necessary.\n-  void run();\n-\n   /// Applies the barrier analysis based on the SCF dialect, in which each\n   /// region has a single basic block only.\n   /// Example:"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 35, "deletions": 14, "changes": 49, "file_content_changes": "@@ -24,21 +24,43 @@ void MembarAnalysis::dfsOperation(Operation *operation,\n     // scf.if only: two regions\n     // scf.for: one region\n     RegionInfo curRegionInfo;\n-    for (auto &region : operation->getRegions()) {\n-      // Copy the parent info as the current info.\n-      RegionInfo regionInfo = *parentRegionInfo;\n-      for (auto &block : region.getBlocks()) {\n-        assert(region.getBlocks().size() == 1 &&\n-               \"Multiple blocks in a region is not supported\");\n-        for (auto &op : block.getOperations()) {\n-          // Traverse the nested operation.\n-          dfsOperation(&op, &regionInfo, builder);\n+    auto traverseRegions = [&]() -> auto{\n+      for (auto &region : operation->getRegions()) {\n+        // Copy the parent info as the current info.\n+        RegionInfo regionInfo = *parentRegionInfo;\n+        for (auto &block : region.getBlocks()) {\n+          assert(region.getBlocks().size() == 1 &&\n+                 \"Multiple blocks in a region is not supported\");\n+          for (auto &op : block.getOperations()) {\n+            // Traverse the nested operation.\n+            dfsOperation(&op, &regionInfo, builder);\n+          }\n         }\n+        curRegionInfo.join(regionInfo);\n       }\n-      curRegionInfo.join(regionInfo);\n+      // Set the parent region info as the union of the nested region info.\n+      *parentRegionInfo = curRegionInfo;\n+    };\n+\n+    traverseRegions();\n+    if (isa<scf::ForOp>(operation)) {\n+      // scf.for's can have two possible inputs: the init value and the\n+      // previous iteration's result. Although we've applied alias analysis,\n+      // there could be unsynced memory accesses on reused memories.\n+      // For example, consider the following code:\n+      // %1 = convert_layout %0: blocked -> shared\n+      // ...\n+      // gpu.barrier\n+      // ...\n+      // %5 = convert_layout %4 : shared -> dot\n+      // %6 = tt.dot %2, %5\n+      // scf.yield\n+      //\n+      // Though %5 could be released before scf.yield, it may shared the same\n+      // memory with %1. So we actually have to insert a barrier before %1 to\n+      // make sure the memory is synced.\n+      traverseRegions();\n     }\n-    // Set the parent region info as the union of the nested region info.\n-    *parentRegionInfo = curRegionInfo;\n   }\n }\n \n@@ -49,8 +71,7 @@ void MembarAnalysis::transfer(Operation *op, RegionInfo *regionInfo,\n     // Do not insert barriers before control flow operations and\n     // alloc/extract/insert\n     // alloc is an allocation op without memory write.\n-    // In contrast, arith.constant is an allocation op with memory write.\n-    // FIXME(Keren): extract is always alias for now\n+    // FIXME(Keren): extract_slice is always alias for now\n     return;\n   }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 28, "deletions": 30, "changes": 58, "file_content_changes": "@@ -644,7 +644,6 @@ class ConvertTritonGPUOpToLLVMPattern\n     return multiDimIdx;\n   }\n \n-\n   struct SmallVectorKeyInfo {\n     static unsigned getHashValue(const SmallVector<unsigned> &key) {\n       return llvm::hash_combine_range(key.begin(), key.end());\n@@ -672,7 +671,7 @@ class ConvertTritonGPUOpToLLVMPattern\n         emitIndices(loc, rewriter, parent, sliceLayout.paddedShape(shape));\n     unsigned numIndices = parentIndices.size();\n     SmallVector<SmallVector<Value>> resultIndices;\n-    for (unsigned i = 0; i < numIndices; ++i){\n+    for (unsigned i = 0; i < numIndices; ++i) {\n       SmallVector<Value> indices = parentIndices[i];\n       indices.erase(indices.begin() + dim);\n       resultIndices.push_back(indices);\n@@ -1239,14 +1238,14 @@ struct BroadcastOpConversion\n     auto resultOffsets = emitOffsetForLayout(resultLayout, resultShape);\n     SmallVector<Value> srcVals = getElementsFromStruct(loc, src, rewriter);\n     DenseMap<SmallVector<unsigned>, Value, SmallVectorKeyInfo> srcValues;\n-    for(size_t i = 0; i < srcOffsets.size(); i++){\n+    for (size_t i = 0; i < srcOffsets.size(); i++) {\n       srcValues[srcOffsets[i]] = srcVals[i];\n     }\n     SmallVector<Value> resultVals;\n-    for(size_t i = 0; i < resultOffsets.size(); i++) {\n+    for (size_t i = 0; i < resultOffsets.size(); i++) {\n       auto offset = resultOffsets[i];\n-      for(size_t j = 0; j < srcShape.size(); j++)\n-        if(srcShape[j]==1)\n+      for (size_t j = 0; j < srcShape.size(); j++)\n+        if (srcShape[j] == 1)\n           offset[j] = 0;\n       resultVals.push_back(srcValues.lookup(offset));\n     }\n@@ -1976,8 +1975,8 @@ struct MakeRangeOpConversion\n     unsigned elems = idxs.size();\n     SmallVector<Value> retVals(elems);\n     // TODO: slice layout has more elements than expected.\n-    // Unexpected behavior for make range, but genereally ok when followed by expand dims + broadcast.\n-    // very weird behavior otherwise potentially.\n+    // Unexpected behavior for make range, but genereally ok when followed by\n+    // expand dims + broadcast. very weird behavior otherwise potentially.\n     for (const auto multiDim : llvm::enumerate(idxs)) {\n       assert(multiDim.value().size() == 1);\n       retVals[multiDim.index()] = add(multiDim.value()[0], start);\n@@ -2683,13 +2682,13 @@ struct ConvertLayoutOpConversion\n     }\n     // dot_op<opIdx=0, parent=#mma> = #mma\n     // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>\n-    if(srcLayout.isa<MmaEncodingAttr>() &&\n+    if (srcLayout.isa<MmaEncodingAttr>() &&\n         dstLayout.isa<DotOperandEncodingAttr>()) {\n       auto srcMmaLayout = srcLayout.cast<MmaEncodingAttr>();\n       auto dstDotLayout = dstLayout.cast<DotOperandEncodingAttr>();\n-      if(srcMmaLayout.getWarpsPerCTA()[1] == 1 &&\n-         dstDotLayout.getOpIdx() == 0 &&\n-         dstDotLayout.getParent() == srcMmaLayout) {\n+      if (srcMmaLayout.getWarpsPerCTA()[1] == 1 &&\n+          dstDotLayout.getOpIdx() == 0 &&\n+          dstDotLayout.getParent() == srcMmaLayout) {\n         // get source values\n         Location loc = op->getLoc();\n         auto vals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n@@ -2698,35 +2697,37 @@ struct ConvertLayoutOpConversion\n             this->getTypeConverter()->convertType(srcTy.getElementType());\n         // for the destination type, we need to pack values together\n         // so they can be consumed by tensor core operations\n-        unsigned vecSize = std::max<unsigned>(32 / elemTy.getIntOrFloatBitWidth(), 1);\n+        unsigned vecSize =\n+            std::max<unsigned>(32 / elemTy.getIntOrFloatBitWidth(), 1);\n         Type vecTy = vec_ty(elemTy, vecSize);\n-        SmallVector<Type> types(elems/vecSize, vecTy);\n+        SmallVector<Type> types(elems / vecSize, vecTy);\n         SmallVector<Value> vecVals;\n-        for(unsigned i = 0; i < elems; i += vecSize) {\n+        for (unsigned i = 0; i < elems; i += vecSize) {\n           Value packed = rewriter.create<LLVM::UndefOp>(loc, vecTy);\n-          for(unsigned j = 0; j < vecSize; j++)\n-            packed = insert_element(vecTy, packed, vals[i+j], i32_val(j));\n+          for (unsigned j = 0; j < vecSize; j++)\n+            packed = insert_element(vecTy, packed, vals[i + j], i32_val(j));\n           vecVals.push_back(packed);\n         }\n-    \n+\n         // This needs to be ordered the same way that\n         // ldmatrix.x4 would order it\n         // TODO: this needs to be refactor so we don't\n         // implicitly depends on how emitOffsetsForMMAV2\n         // is implemented\n         SmallVector<Value> reorderedVals;\n-        for(unsigned i = 0; i < vecVals.size(); i += 4) {\n+        for (unsigned i = 0; i < vecVals.size(); i += 4) {\n           reorderedVals.push_back(vecVals[i]);\n-          reorderedVals.push_back(vecVals[i+2]);\n-          reorderedVals.push_back(vecVals[i+1]);\n-          reorderedVals.push_back(vecVals[i+3]);\n+          reorderedVals.push_back(vecVals[i + 2]);\n+          reorderedVals.push_back(vecVals[i + 1]);\n+          reorderedVals.push_back(vecVals[i + 3]);\n         }\n \n         // return composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK);\n \n-\n-        Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n-        Value view = getStructFromElements(loc, reorderedVals, rewriter, structTy);\n+        Type structTy =\n+            LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n+        Value view =\n+            getStructFromElements(loc, reorderedVals, rewriter, structTy);\n         rewriter.replaceOp(op, view);\n         return success();\n       }\n@@ -3174,10 +3175,6 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n     ConversionPatternRewriter &rewriter) const {\n   auto loc = op.getLoc();\n \n-  // TODO[Keren]: A temporary workaround for an issue from membar pass.\n-  // https://triton-lang.slack.com/archives/C042VBSQWNS/p1669796615860699?thread_ts=1669779203.526739&cid=C042VBSQWNS\n-  barrier();\n-\n   Value src = op.src();\n   Value dst = op.result();\n   auto srcTy = src.getType().cast<RankedTensorType>();\n@@ -4687,7 +4684,8 @@ class ConvertTritonGPUToLLVM\n     decomposeInsertSliceAsyncOp(mod, typeConverter);\n \n     Allocation allocation(mod);\n-    MembarAnalysis membar(&allocation);\n+    MembarAnalysis membarPass(&allocation);\n+    membarPass.run();\n \n     RewritePatternSet scf_patterns(context);\n     mlir::populateLoopToStdConversionPatterns(scf_patterns);"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -286,8 +286,8 @@ struct TritonAtomicCASPattern\n   matchAndRewrite(triton::AtomicCASOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     rewriter.replaceOpWithNewOp<triton::AtomicCASOp>(\n-        op, typeConverter->convertType(op.getType()), \n-        adaptor.ptr(), adaptor.cmp(), adaptor.val());\n+        op, typeConverter->convertType(op.getType()), adaptor.ptr(),\n+        adaptor.cmp(), adaptor.val());\n     return success();\n   }\n };"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -241,7 +241,8 @@ mlir::LogicalResult mlir::triton::ReduceOp::inferReturnTypes(\n   auto argTy = arg.getType().cast<RankedTensorType>();\n   auto argEltTy = argTy.getElementType();\n   auto i32Ty = IntegerType::get(argEltTy.getContext(), 32);\n-  auto redOp = attributes.get(\"redOp\").cast<mlir::triton::RedOpAttr>().getValue();\n+  auto redOp =\n+      attributes.get(\"redOp\").cast<mlir::triton::RedOpAttr>().getValue();\n   bool withIndex = mlir::triton::ReduceOp::withIndex(redOp);\n   auto retEltTy = withIndex ? i32Ty : argEltTy;\n   auto retShape = argTy.getShape().vec();"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 45, "deletions": 41, "changes": 86, "file_content_changes": "@@ -50,22 +50,25 @@ class DecomposeDotOperand : public mlir::RewritePattern {\n     auto dstType = convert.getType().cast<RankedTensorType>();\n     if (srcType.getEncoding().isa<triton::gpu::BlockedEncodingAttr>() &&\n         dstType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>()) {\n-      auto dstDotOperand = dstType.getEncoding().cast<triton::gpu::DotOperandEncodingAttr>();\n+      auto dstDotOperand =\n+          dstType.getEncoding().cast<triton::gpu::DotOperandEncodingAttr>();\n       auto dstParent = dstDotOperand.getParent();\n-      if(dstDotOperand.getOpIdx()==1 || \n-         !dstParent.isa<triton::gpu::MmaEncodingAttr>())\n+      if (dstDotOperand.getOpIdx() == 1 ||\n+          !dstParent.isa<triton::gpu::MmaEncodingAttr>())\n         return mlir::failure();\n       auto dstParentMma = dstParent.cast<triton::gpu::MmaEncodingAttr>();\n-      if(dstParentMma.getVersion() == 1 ||\n-         dstParentMma.getWarpsPerCTA()[1] > 1)\n+      if (dstParentMma.getVersion() == 1 ||\n+          dstParentMma.getWarpsPerCTA()[1] > 1)\n         return mlir::failure();\n-      SetVector<Operation*> bwdSlices;\n+      SetVector<Operation *> bwdSlices;\n       mlir::getBackwardSlice(convert.getResult(), &bwdSlices);\n-      if(llvm::find_if(bwdSlices, [](Operation *op) { return isa<triton::DotOp>(op); }) == bwdSlices.end())\n+      if (llvm::find_if(bwdSlices, [](Operation *op) {\n+            return isa<triton::DotOp>(op);\n+          }) == bwdSlices.end())\n         return mlir::failure();\n-      \n-      auto tmpType =\n-          RankedTensorType::get(dstType.getShape(), dstType.getElementType(), dstParentMma);\n+\n+      auto tmpType = RankedTensorType::get(\n+          dstType.getShape(), dstType.getElementType(), dstParentMma);\n       auto tmp = rewriter.create<triton::gpu::ConvertLayoutOp>(\n           convert.getLoc(), tmpType, convert.getOperand());\n       auto newConvert = rewriter.create<triton::gpu::ConvertLayoutOp>(\n@@ -601,10 +604,9 @@ mmaVersionToShapePerWarp(int version, const ArrayRef<int64_t> &shape,\n   }\n }\n \n-\n SmallVector<unsigned, 2> warpsPerTileV1(triton::DotOp dotOp,\n-                                         const ArrayRef<int64_t> shape,\n-                                         int numWarps) {\n+                                        const ArrayRef<int64_t> shape,\n+                                        int numWarps) {\n   SmallVector<unsigned, 2> ret = {1, 1};\n   SmallVector<int64_t, 2> shapePerWarp =\n       mmaVersionToShapePerWarp(1, shape, numWarps);\n@@ -624,35 +626,37 @@ SmallVector<unsigned, 2> warpsPerTileV1(triton::DotOp dotOp,\n }\n \n SmallVector<unsigned, 2> warpsPerTileV2(triton::DotOp dotOp,\n-                                         const ArrayRef<int64_t> shape,\n-                                         int numWarps) {\n-    SetVector<Operation*> slices;\n-    mlir::getForwardSlice(dotOp.getResult(), &slices);\n-    if(llvm::find_if(slices, [](Operation *op) { return isa<triton::DotOp>(op); }) != slices.end())\n-      return {(unsigned)numWarps, 1};\n-    \n-    SmallVector<unsigned, 2> ret = {1, 1};\n-    SmallVector<int64_t, 2> shapePerWarp = {16, 8};\n-    bool changed = false;\n-    // TODO (@daadaada): double-check.\n-    // original logic in\n-    // https://github.com/openai/triton/blob/master/lib/codegen/analysis/layout.cc#L252\n-    // seems buggy for shape = [32, 16] ?\n-    do {\n-      changed = false;\n-      if (ret[0] * ret[1] >= numWarps)\n-        break;\n-      if (shape[0] / shapePerWarp[0] / ret[0] >=\n-          shape[1] / (shapePerWarp[1] * 2) / ret[1]) {\n-        if (ret[0] < shape[0] / shapePerWarp[0]) {\n-          ret[0] *= 2;\n-        } else\n-          ret[1] *= 2;\n-      } else {\n+                                        const ArrayRef<int64_t> shape,\n+                                        int numWarps) {\n+  SetVector<Operation *> slices;\n+  mlir::getForwardSlice(dotOp.getResult(), &slices);\n+  if (llvm::find_if(slices, [](Operation *op) {\n+        return isa<triton::DotOp>(op);\n+      }) != slices.end())\n+    return {(unsigned)numWarps, 1};\n+\n+  SmallVector<unsigned, 2> ret = {1, 1};\n+  SmallVector<int64_t, 2> shapePerWarp = {16, 8};\n+  bool changed = false;\n+  // TODO (@daadaada): double-check.\n+  // original logic in\n+  // https://github.com/openai/triton/blob/master/lib/codegen/analysis/layout.cc#L252\n+  // seems buggy for shape = [32, 16] ?\n+  do {\n+    changed = false;\n+    if (ret[0] * ret[1] >= numWarps)\n+      break;\n+    if (shape[0] / shapePerWarp[0] / ret[0] >=\n+        shape[1] / (shapePerWarp[1] * 2) / ret[1]) {\n+      if (ret[0] < shape[0] / shapePerWarp[0]) {\n+        ret[0] *= 2;\n+      } else\n         ret[1] *= 2;\n-      }\n-    } while (true);\n-    return ret;\n+    } else {\n+      ret[1] *= 2;\n+    }\n+  } while (true);\n+  return ret;\n }\n \n } // namespace"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -130,10 +130,10 @@ LogicalResult Prefetcher::initialize() {\n \n   if (dotsInFor.empty())\n     return failure();\n-  \n+\n   // TODO: segfault (original for still has uses)\n   // when used in flash attention that has 2 dots in the loop\n-  if(dotsInFor.size() > 1)\n+  if (dotsInFor.size() > 1)\n     return failure();\n \n   // returns source of cvt"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "@@ -341,7 +341,7 @@ void init_triton_ir(py::module &&m) {\n         return funcs[0];\n       });\n \n-   m.def(\"make_attr\",\n+  m.def(\"make_attr\",\n         [](const std::vector<int> &values, mlir::MLIRContext &context) {\n           return mlir::DenseIntElementsAttr::get(\n                      mlir::RankedTensorType::get(\n@@ -1113,7 +1113,8 @@ void init_triton_ir(py::module &&m) {\n               mlir::Value &val) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n              mlir::Type dstType;\n-             if (auto srcTensorType = ptr.getType().dyn_cast<mlir::RankedTensorType>()) {\n+             if (auto srcTensorType =\n+                     ptr.getType().dyn_cast<mlir::RankedTensorType>()) {\n                mlir::Type dstElemType = srcTensorType.getElementType()\n                                             .cast<mlir::triton::PointerType>()\n                                             .getPointeeType();\n@@ -1250,13 +1251,12 @@ void init_triton_ir(py::module &&m) {\n                                        llvm::StringRef(prefix)),\n                  values);\n            })\n-       // Undef\n-          .def(\"create_undef\",\n-               [](mlir::OpBuilder &self, mlir::Type &type) -> mlir::Value {\n-               auto loc = self.getUnknownLoc();\n-               return self.create<::mlir::LLVM::UndefOp>(loc, type);\n-          })    \n-       ;\n+      // Undef\n+      .def(\"create_undef\",\n+           [](mlir::OpBuilder &self, mlir::Type &type) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return self.create<::mlir::LLVM::UndefOp>(loc, type);\n+           });\n \n   py::class_<mlir::PassManager>(m, \"pass_manager\")\n       .def(py::init<mlir::MLIRContext *>())"}, {"filename": "test/lib/Analysis/TestMembar.cpp", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -26,7 +26,9 @@ struct TestMembarPass\n     auto op_name = SymbolTable::getSymbolName(operation).getValue().str();\n     os << op_name << \"\\n\";\n     Allocation allocation(operation);\n-    MembarAnalysis analysis(&allocation);\n+    MembarAnalysis membarPass(&allocation);\n+    membarPass.run();\n+\n     size_t operationId = 0;\n     operation->walk<WalkOrder::PreOrder>([&](Operation *op) {\n       if (isa<gpu::BarrierOp>(op)) {"}]