[{"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -90,9 +90,9 @@ SmallVector<unsigned> getCTAOrder(Attribute layout);\n  * (3) In the implementation of emitIndices, ShapePerCTATile will\n  *     be replicated or wraped to fit ShapePerCTA.\n  */\n-SmallVector<unsigned>\n-getShapePerCTATile(Attribute layout,\n-                   ArrayRef<int64_t> tensorShape = ArrayRef<int64_t>());\n+SmallVector<unsigned> getShapePerCTATile(BlockedEncodingAttr layout);\n+SmallVector<unsigned> getShapePerCTATile(Attribute layout,\n+                                         ArrayRef<int64_t> tensorShape);\n \n SmallVector<int64_t> getShapePerCTA(ArrayRef<unsigned> CTASplitNum,\n                                     ArrayRef<int64_t> shape);"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -21,14 +21,14 @@ class SharedEncodingAttr;\n } // namespace triton\n \n SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n-                                                const ArrayRef<int64_t> &shape,\n+                                                ArrayRef<int64_t> shapePerCTA,\n                                                 RankedTensorType type);\n SmallVector<unsigned, 3>\n mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n-                       const ArrayRef<int64_t> shapePerCTA);\n+                       ArrayRef<int64_t> shapePerCTA);\n SmallVector<unsigned, 3>\n-mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma, unsigned opIdx,\n-                       RankedTensorType type);\n+mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n+                       ArrayRef<int64_t> shapePerCTA, unsigned opIdx);\n \n /// Returns true if the Load is for TMA\n bool isLoadFromTensorPtr(triton::LoadOp op);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -253,7 +253,7 @@ struct ConvertLayoutOpConversion\n     auto sizePerThread = getSizePerThread(layout, shapePerCTA);\n     auto accumSizePerThread = product<unsigned>(sizePerThread);\n     SmallVector<unsigned> numCTATiles(rank);\n-    auto shapePerCTATile = getShapePerCTATile(layout);\n+    auto shapePerCTATile = getShapePerCTATile(layout, type.getShape());\n     auto order = getOrder(layout);\n     for (unsigned d = 0; d < rank; ++d) {\n       numCTATiles[d] = ceil<unsigned>(shapePerCTA[d], shapePerCTATile[d]);\n@@ -344,8 +344,9 @@ struct ConvertLayoutOpConversion\n \n     SmallVector<unsigned> numCTAs(rank, 1);\n     SmallVector<unsigned> numCTAsEachRep(rank, 1);\n-    SmallVector<unsigned> shapePerCTATile = getShapePerCTATile(layout, shape);\n     SmallVector<int64_t> shapePerCTA = getShapePerCTA(layout, shape);\n+    SmallVector<unsigned> shapePerCTATile =\n+        getShapePerCTATile(layout, type.getShape());\n     auto elemTy = type.getElementType();\n \n     int ctaId = 0;\n@@ -534,9 +535,9 @@ struct ConvertLayoutOpConversion\n     SmallVector<unsigned> outNumCTAsEachRep(rank);\n     SmallVector<unsigned> inNumCTAs(rank);\n     SmallVector<unsigned> outNumCTAs(rank);\n+    auto shapePerCTA = getShapePerCTA(srcLayout, shape);\n     auto srcShapePerCTATile = getShapePerCTATile(srcLayout, srcTy.getShape());\n     auto dstShapePerCTATile = getShapePerCTATile(dstLayout, shape);\n-    auto shapePerCTA = getShapePerCTA(srcLayout, shape);\n \n     // For Volta, all the coords for a CTA are calculated.\n     bool isSrcMmaV1{}, isDstMmaV1{};\n@@ -758,7 +759,7 @@ struct ConvertLayoutOpConversion\n       auto ptrI8SharedTy = LLVM::LLVMPointerType::get(\n           typeConverter->convertType(rewriter.getI8Type()), 3);\n \n-      uint32_t rowsPerRep = getShapePerCTATile(mmaLayout)[0];\n+      uint32_t rowsPerRep = getShapePerCTATile(mmaLayout, srcShape)[0];\n \n       Value threadId = getThreadId(rewriter, loc);\n       Value warpId = udiv(threadId, i32_val(32));"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/WGMMA.cpp", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -161,7 +161,7 @@ DotOpMmaV3SmemLoader loadA(TritonGPUToLLVMTypeConverter *typeConverter,\n   auto aSharedLayout = aTensorTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n   assert(aSharedLayout && \"only support load dot operand from shared.\");\n   auto shapePerCTA = getShapePerCTA(aTensorTy);\n-  auto instrShape = mmaVersionToInstrShape(mmaEncoding, 0, aTensorTy);\n+  auto instrShape = mmaVersionToInstrShape(mmaEncoding, shapePerCTA, 0);\n   auto wpt = mmaEncoding.getWarpsPerCTA();\n   auto aOrd = aSharedLayout.getOrder();\n   bool transA = aOrd[0] == 0;\n@@ -191,11 +191,11 @@ DotOpMmaV3SmemLoader loadB(TritonGPUToLLVMTypeConverter *typeConverter,\n   auto bTensorTy = tensor.getType().cast<RankedTensorType>();\n   auto bSharedLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n   assert(bSharedLayout && \"only support load B from shared.\");\n-  auto instrShape = mmaVersionToInstrShape(mmaEncoding, 1, bTensorTy);\n+  auto shapePerCTA = triton::gpu::getShapePerCTA(bTensorTy);\n+  auto instrShape = mmaVersionToInstrShape(mmaEncoding, shapePerCTA, 1);\n   auto wpt = mmaEncoding.getWarpsPerCTA();\n   auto bOrd = bSharedLayout.getOrder();\n   bool transB = bOrd[0] == 1;\n-  auto shapePerCTA = triton::gpu::getShapePerCTA(bTensorTy);\n \n   int numRepK = ceil<unsigned>(shapePerCTA[0], instrShape[2]);\n   int numRepN = ceil<unsigned>(shapePerCTA[1], instrShape[1] * wpt[1]);\n@@ -284,7 +284,7 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n   int N = instrShape[1];\n   int K = instrShape[2];\n \n-  auto shapePerCTATile = getShapePerCTATile(mmaEncoding);\n+  auto shapePerCTATile = getShapePerCTATile(mmaEncoding, dTensorTy.getShape());\n   int numRepM = ceil<unsigned>(dShapePerCTA[0], shapePerCTATile[0]);\n   int numRepN = ceil<unsigned>(dShapePerCTA[1], shapePerCTATile[1]);\n   int numRepK = ceil<unsigned>(aTensorTy.getShape()[1], instrShape[2]);\n@@ -362,7 +362,7 @@ Value loadC(Value tensor, Value llTensor) {\n   auto shapePerCTA = getShapePerCTA(tensorTy);\n   auto instrShape = mmaVersionToInstrShape(mmaEncoding, shapePerCTA);\n   auto wpt = mmaEncoding.getWarpsPerCTA();\n-  auto shapePerCTATile = getShapePerCTATile(mmaEncoding);\n+  auto shapePerCTATile = getShapePerCTATile(mmaEncoding, tensorTy.getShape());\n \n   int numRepM = ceil<unsigned>(shapePerCTA[0], shapePerCTATile[0]);\n   int numRepN = ceil<unsigned>(shapePerCTA[1], shapePerCTATile[1]);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -1018,9 +1018,9 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     SmallVector<SmallVector<unsigned>> ret;\n \n     for (unsigned i = 0; i < shapePerCTA[0];\n-         i += getShapePerCTATile(mmaLayout)[0]) {\n+         i += getShapePerCTATile(mmaLayout, shape)[0]) {\n       for (unsigned j = 0; j < shapePerCTA[1];\n-           j += getShapePerCTATile(mmaLayout)[1]) {\n+           j += getShapePerCTATile(mmaLayout, shape)[1]) {\n         ret.push_back({i, j});\n         ret.push_back({i, j + 1});\n         ret.push_back({i + 8, j});\n@@ -1097,9 +1097,9 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n         mmaVersionToInstrShape(mmaLayout, shapePerCTA);\n \n     for (unsigned i = 0; i < shapePerCTA[0];\n-         i += getShapePerCTATile(mmaLayout)[0]) {\n+         i += getShapePerCTATile(mmaLayout, shape)[0]) {\n       for (unsigned j = 0; j < shapePerCTA[1];\n-           j += getShapePerCTATile(mmaLayout)[1]) {\n+           j += getShapePerCTATile(mmaLayout, shape)[1]) {\n         for (unsigned k = 0; k < instrShape[1]; k += 8) {\n           ret.push_back({i, j + k});\n           ret.push_back({i, j + k + 1});"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 47, "deletions": 25, "changes": 72, "file_content_changes": "@@ -289,45 +289,67 @@ SmallVector<unsigned> getThreadsPerCTA(Attribute layout) {\n   return threads;\n }\n \n+SmallVector<unsigned> getShapePerCTATile(BlockedEncodingAttr blockedLayout) {\n+  SmallVector<unsigned> shape;\n+  for (unsigned d = 0, n = blockedLayout.getOrder().size(); d < n; ++d)\n+    shape.push_back(blockedLayout.getSizePerThread()[d] *\n+                    blockedLayout.getThreadsPerWarp()[d] *\n+                    blockedLayout.getWarpsPerCTA()[d]);\n+\n+  return shape;\n+}\n+\n+static SmallVector<unsigned>\n+getShapePerCTATileOfMma(MmaEncodingAttr mmaLayout,\n+                        ArrayRef<int64_t> tensorShape, int opIdx) {\n+  if (mmaLayout.isAmpere())\n+    return {16 * mmaLayout.getWarpsPerCTA()[0],\n+            8 * mmaLayout.getWarpsPerCTA()[1]};\n+  if (mmaLayout.isVolta()) {\n+    assert(!tensorShape.empty() && \"Volta needs the tensorShape\");\n+    if (tensorShape.size() == 1) // must be SliceEncoding\n+      return {static_cast<unsigned>(tensorShape[0]),\n+              static_cast<unsigned>(tensorShape[0])};\n+    return {static_cast<unsigned>(tensorShape[0]),\n+            static_cast<unsigned>(tensorShape[1])};\n+  }\n+  if (mmaLayout.isHopper()) {\n+    auto shapePerCTA = getShapePerCTA(mmaLayout, tensorShape);\n+    SmallVector<unsigned, 3> instrShape;\n+    if (opIdx < 0)\n+      instrShape = mmaVersionToInstrShape(mmaLayout, shapePerCTA);\n+    else\n+      instrShape =\n+          mmaVersionToInstrShape(mmaLayout, shapePerCTA, (unsigned)opIdx);\n+\n+    return {16 * mmaLayout.getWarpsPerCTA()[0],\n+            instrShape[1] * mmaLayout.getWarpsPerCTA()[1]};\n+  }\n+\n+  assert(0 && \"Unexpected MMA layout version found\");\n+  return {};\n+}\n+\n SmallVector<unsigned> getShapePerCTATile(Attribute layout,\n                                          ArrayRef<int64_t> tensorShape) {\n   SmallVector<unsigned> shape;\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n-    for (unsigned d = 0, n = blockedLayout.getOrder().size(); d < n; ++d)\n-      shape.push_back(blockedLayout.getSizePerThread()[d] *\n-                      blockedLayout.getThreadsPerWarp()[d] *\n-                      blockedLayout.getWarpsPerCTA()[d]);\n+    return getShapePerCTATile(blockedLayout);\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n-    shape = getShapePerCTATile(sliceLayout.getParent(), tensorShape);\n+    auto parentTensorShape = sliceLayout.paddedShape(tensorShape);\n+    shape = getShapePerCTATile(sliceLayout.getParent(), parentTensorShape);\n     shape.erase(shape.begin() + sliceLayout.getDim());\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    if (mmaLayout.isAmpere())\n-      return {16 * mmaLayout.getWarpsPerCTA()[0],\n-              8 * mmaLayout.getWarpsPerCTA()[1]};\n-    if (mmaLayout.isVolta()) {\n-      assert(!tensorShape.empty() && \"Volta needs the tensorShape\");\n-      if (tensorShape.size() == 1) // must be SliceEncoding\n-        return {static_cast<unsigned>(tensorShape[0]),\n-                static_cast<unsigned>(tensorShape[0])};\n-      return {static_cast<unsigned>(tensorShape[0]),\n-              static_cast<unsigned>(tensorShape[1])};\n-    }\n-    if (mmaLayout.isHopper()) {\n-      auto shapePerCTA = getShapePerCTA(layout, tensorShape);\n-      auto instrShape = mmaVersionToInstrShape(mmaLayout, shapePerCTA);\n-      return {16 * mmaLayout.getWarpsPerCTA()[0],\n-              instrShape[1] * mmaLayout.getWarpsPerCTA()[1]};\n-    }\n-    assert(0 && \"Unexpected MMA layout version found\");\n+    return getShapePerCTATileOfMma(mmaLayout, tensorShape, -1);\n   } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n     auto parentLayout = dotLayout.getParent();\n     assert(parentLayout && \"DotOperandEncodingAttr must have a parent\");\n     if (auto parentMmaLayout = parentLayout.dyn_cast<MmaEncodingAttr>()) {\n       assert(parentMmaLayout.isAmpere() &&\n              \"mmaLayout version = 1 is not implemented yet\");\n+      int opIdx = dotLayout.getOpIdx();\n       auto parentShapePerCTATile =\n-          getShapePerCTATile(parentLayout, tensorShape);\n-      auto opIdx = dotLayout.getOpIdx();\n+          getShapePerCTATileOfMma(parentMmaLayout, tensorShape, opIdx);\n       if (opIdx == 0) {\n         return {parentShapePerCTATile[0], 16};\n       } else if (opIdx == 1) {"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 16, "deletions": 10, "changes": 26, "file_content_changes": "@@ -51,12 +51,12 @@ mmaVersionToInstrShapeImpl(int version, const ArrayRef<int64_t> &shape,\n }\n \n SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n-                                                const ArrayRef<int64_t> &shape,\n+                                                ArrayRef<int64_t> shapePerCTA,\n                                                 RankedTensorType type) {\n   auto instrShape =\n-      mmaVersionToInstrShapeImpl(version, shape, type.getElementType());\n+      mmaVersionToInstrShapeImpl(version, shapePerCTA, type.getElementType());\n   if (version == 3) {\n-    if (shape[0] % 64 != 0 || shape[1] % 8 != 0) {\n+    if (shapePerCTA[0] % 64 != 0 || shapePerCTA[1] % 8 != 0) {\n       assert(false && \"type not supported\");\n       return {0, 0, 0};\n     }\n@@ -67,7 +67,7 @@ SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n \n SmallVector<unsigned, 3>\n mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n-                       const ArrayRef<int64_t> shapePerCTA) {\n+                       ArrayRef<int64_t> shapePerCTA) {\n   Type eltType;\n   auto ctx = mma.getContext();\n   OpBuilder builder(ctx);\n@@ -80,12 +80,18 @@ mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n       RankedTensorType::get({1, 1}, eltType, mma));\n }\n SmallVector<unsigned, 3>\n-mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma, unsigned opIdx,\n-                       RankedTensorType type) {\n-  auto shapePerCTA =\n-      triton::gpu::getShapePerCTA(type.getEncoding(), type.getShape());\n-  auto instrShape = mmaVersionToInstrShapeImpl(\n-      mma.getVersionMajor(), shapePerCTA, type.getElementType());\n+mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n+                       ArrayRef<int64_t> shapePerCTA, unsigned opIdx) {\n+  Type eltType;\n+  auto ctx = mma.getContext();\n+  OpBuilder builder(ctx);\n+  if (mma.getIsInt8Input())\n+    eltType = builder.getIntegerType(8);\n+  else\n+    eltType = builder.getF16Type();\n+  auto instrShape =\n+      mmaVersionToInstrShapeImpl(mma.getVersionMajor(), shapePerCTA, eltType);\n+\n   if (opIdx == 0) {\n     instrShape[1] = 0;\n     assert(shapePerCTA[0] % 64 == 0 && \"M direction not compatible\");"}]