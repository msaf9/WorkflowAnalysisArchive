[{"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 113, "deletions": 16, "changes": 129, "file_content_changes": "@@ -175,9 +175,11 @@ class LayoutPropagation {\n   Operation *rewriteOp(Operation *op);\n   // Rewrite a for op based on the layout picked by the analysis.\n   Operation *rewriteForOp(scf::ForOp forOp);\n+  Operation *rewriteWhileOp(scf::WhileOp whileOp);\n   Operation *rewriteIfOp(scf::IfOp ifOp);\n-  Operation *rewriteYieldOp(scf::YieldOp yieldOp);\n-  Operation *rewriteReduceToScalar(Operation *reduceOp);\n+  void rewriteYieldOp(scf::YieldOp yieldOp);\n+  void rewriteConditionOp(scf::ConditionOp conditionOp);\n+  void rewriteReduceToScalar(Operation *reduceOp);\n   Operation *cloneElementwise(OpBuilder &rewriter, Operation *op,\n                               Attribute encoding);\n   // Map the original value to the rewritten one.\n@@ -291,16 +293,36 @@ SmallVector<Value> LayoutPropagation::propagateToUsers(Value value,\n       setEncoding({arg, result}, info, changed, user);\n       continue;\n     }\n+    if (auto whileOp = dyn_cast<scf::WhileOp>(user)) {\n+      Value arg = whileOp.getBeforeArguments()[use.getOperandNumber()];\n+      setEncoding({arg}, info, changed, user);\n+      continue;\n+    }\n     if (auto yieldOp = dyn_cast<scf::YieldOp>(user)) {\n       auto parent = yieldOp->getParentOp();\n-      SmallVector<Value> valuesToPropagate = {\n-          parent->getResult(use.getOperandNumber())};\n+      SmallVector<Value> valuesToPropagate;\n+      if (isa<scf::ForOp, scf::IfOp>(parent))\n+        valuesToPropagate.push_back(parent->getResult(use.getOperandNumber()));\n       if (auto forOp = dyn_cast<scf::ForOp>(parent))\n         valuesToPropagate.push_back(\n             forOp.getRegionIterArg(use.getOperandNumber()));\n-      if (isa<scf::ForOp, scf::IfOp>(parent))\n-        setEncoding({valuesToPropagate}, info, changed, user);\n-      // TODO: handle while.\n+      if (auto whileOp = dyn_cast<scf::WhileOp>(parent)) {\n+        valuesToPropagate.push_back(\n+            whileOp.getBeforeArguments()[use.getOperandNumber()]);\n+        valuesToPropagate.push_back(\n+            whileOp->getOperand(use.getOperandNumber()));\n+      }\n+      if (isa<scf::ForOp, scf::IfOp, scf::WhileOp>(parent))\n+        setEncoding(valuesToPropagate, info, changed, user);\n+      continue;\n+    }\n+    if (auto conditionOp = dyn_cast<scf::ConditionOp>(user)) {\n+      auto whileOp = cast<scf::WhileOp>(conditionOp->getParentOp());\n+      // Skip arg 0 as it is the condition.\n+      unsigned argIndex = use.getOperandNumber() - 1;\n+      Value afterArg = whileOp.getAfterArguments()[argIndex];\n+      Value result = whileOp->getResult(argIndex);\n+      setEncoding({afterArg, result}, info, changed, user);\n       continue;\n     }\n     // Workaround: don't propagate through truncI\n@@ -402,6 +424,8 @@ void LayoutPropagation::rewriteRegion(Region &region) {\n           queue.push_back(&R);\n       } else if (auto yieldOp = dyn_cast<scf::YieldOp>(&op)) {\n         rewriteYieldOp(yieldOp);\n+      } else if (auto conditionOp = dyn_cast<scf::ConditionOp>(&op)) {\n+        rewriteConditionOp(conditionOp);\n       } else if (reduceToScalar(&op)) {\n         rewriteReduceToScalar(&op);\n       } else {\n@@ -519,6 +543,66 @@ Operation *LayoutPropagation::rewriteForOp(scf::ForOp forOp) {\n   return newForOp.getOperation();\n }\n \n+Operation *LayoutPropagation::rewriteWhileOp(scf::WhileOp whileOp) {\n+  SmallVector<Value> operands;\n+  SmallVector<Type> returnTypes;\n+  OpBuilder rewriter(whileOp);\n+  for (auto [operand, arg] :\n+       llvm::zip(whileOp->getOperands(), whileOp.getBeforeArguments())) {\n+    Value convertedOperand = operand;\n+    if (layouts.count(arg))\n+      convertedOperand = getValueAs(operand, *layouts[arg].encodings.begin());\n+    operands.push_back(convertedOperand);\n+  }\n+  for (Value ret : whileOp.getResults()) {\n+    auto it = layouts.find(ret);\n+    if (it == layouts.end()) {\n+      returnTypes.push_back(ret.getType());\n+      continue;\n+    }\n+    auto origType = ret.getType().dyn_cast<RankedTensorType>();\n+    auto newType =\n+        RankedTensorType::get(origType.getShape(), origType.getElementType(),\n+                              it->second.encodings[0]);\n+    returnTypes.push_back(newType);\n+  }\n+\n+  auto newWhileOp =\n+      rewriter.create<scf::WhileOp>(whileOp.getLoc(), returnTypes, operands);\n+  SmallVector<Type> argsTypesBefore;\n+  for (Value operand : operands)\n+    argsTypesBefore.push_back(operand.getType());\n+  SmallVector<Location> bbArgLocsBefore(argsTypesBefore.size(),\n+                                        whileOp.getLoc());\n+  SmallVector<Location> bbArgLocsAfter(returnTypes.size(), whileOp.getLoc());\n+  rewriter.createBlock(&newWhileOp.getBefore(), {}, argsTypesBefore,\n+                       bbArgLocsBefore);\n+  rewriter.createBlock(&newWhileOp.getAfter(), {}, returnTypes, bbArgLocsAfter);\n+\n+  for (int i = 0; i < whileOp.getNumRegions(); ++i) {\n+    newWhileOp->getRegion(i).front().getOperations().splice(\n+        newWhileOp->getRegion(i).front().getOperations().begin(),\n+        whileOp->getRegion(i).front().getOperations());\n+  }\n+\n+  auto remapArg = [&](Value oldVal, Value newVal) {\n+    if (oldVal.getType() == newVal.getType())\n+      oldVal.replaceAllUsesWith(newVal);\n+    else\n+      map(oldVal, newVal);\n+  };\n+  for (auto [oldResult, newResult] :\n+       llvm::zip(whileOp.getResults(), newWhileOp.getResults()))\n+    remapArg(oldResult, newResult);\n+  for (auto [oldArg, newArg] :\n+       llvm::zip(whileOp.getBeforeArguments(), newWhileOp.getBeforeArguments()))\n+    remapArg(oldArg, newArg);\n+  for (auto [oldArg, newArg] :\n+       llvm::zip(whileOp.getAfterArguments(), newWhileOp.getAfterArguments()))\n+    remapArg(oldArg, newArg);\n+  return newWhileOp.getOperation();\n+}\n+\n Operation *LayoutPropagation::rewriteIfOp(scf::IfOp ifOp) {\n   SmallVector<Value> operands;\n   OpBuilder rewriter(ifOp);\n@@ -547,25 +631,37 @@ Operation *LayoutPropagation::rewriteIfOp(scf::IfOp ifOp) {\n   return newIfOp.getOperation();\n }\n \n-Operation *LayoutPropagation::rewriteYieldOp(scf::YieldOp yieldOp) {\n-  OpBuilder rewriter(yieldOp);\n-  Operation *newYield = rewriter.clone(*yieldOp.getOperation());\n+void LayoutPropagation::rewriteYieldOp(scf::YieldOp yieldOp) {\n   Operation *parentOp = yieldOp->getParentOp();\n   for (OpOperand &operand : yieldOp->getOpOperands()) {\n     Type yieldType = operand.get().getType();\n     if (isa<scf::ForOp, scf::IfOp>(parentOp))\n       yieldType = parentOp->getResult(operand.getOperandNumber()).getType();\n+    if (auto whileOp = dyn_cast<scf::WhileOp>(parentOp))\n+      yieldType =\n+          whileOp.getBeforeArguments()[operand.getOperandNumber()].getType();\n     auto tensorType = yieldType.dyn_cast<RankedTensorType>();\n     if (!tensorType)\n       continue;\n     Value newOperand = getValueAs(operand.get(), tensorType.getEncoding());\n-    newYield->setOperand(operand.getOperandNumber(), newOperand);\n+    yieldOp->setOperand(operand.getOperandNumber(), newOperand);\n   }\n-  opToDelete.push_back(yieldOp.getOperation());\n-  return newYield;\n }\n \n-Operation *LayoutPropagation::rewriteReduceToScalar(Operation *reduceOp) {\n+void LayoutPropagation::rewriteConditionOp(scf::ConditionOp conditionOp) {\n+  scf::WhileOp whileOp = cast<scf::WhileOp>(conditionOp->getParentOp());\n+  for (unsigned i = 1; i < conditionOp->getNumOperands(); ++i) {\n+    OpOperand &operand = conditionOp->getOpOperand(i);\n+    Type argType = whileOp->getResult(operand.getOperandNumber() - 1).getType();\n+    auto tensorType = argType.dyn_cast<RankedTensorType>();\n+    if (!tensorType)\n+      continue;\n+    Value newOperand = getValueAs(operand.get(), tensorType.getEncoding());\n+    conditionOp->setOperand(operand.getOperandNumber(), newOperand);\n+  }\n+}\n+\n+void LayoutPropagation::rewriteReduceToScalar(Operation *reduceOp) {\n   OpBuilder rewriter(reduceOp);\n   Attribute srcEncoding;\n   // Since all the operands need to have the same encoding pick the first one\n@@ -578,18 +674,19 @@ Operation *LayoutPropagation::rewriteReduceToScalar(Operation *reduceOp) {\n     }\n   }\n   if (!srcEncoding)\n-    return reduceOp;\n+    return;\n   for (OpOperand &operand : reduceOp->getOpOperands()) {\n     Value newOperand = getValueAs(operand.get(), srcEncoding);\n     reduceOp->setOperand(operand.getOperandNumber(), newOperand);\n   }\n-  return reduceOp;\n }\n \n Operation *LayoutPropagation::rewriteOp(Operation *op) {\n   opToDelete.push_back(op);\n   if (auto forOp = dyn_cast<scf::ForOp>(op))\n     return rewriteForOp(forOp);\n+  if (auto whileOp = dyn_cast<scf::WhileOp>(op))\n+    return rewriteWhileOp(whileOp);\n   if (auto ifOp = dyn_cast<scf::IfOp>(op))\n     return rewriteIfOp(ifOp);\n   OpBuilder rewriter(op);"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 33, "deletions": 0, "changes": 33, "file_content_changes": "@@ -1803,3 +1803,36 @@ tt.func @reduce_to_scalar(%ptr: tensor<1024x!tt.ptr<f32>, #blocked>) -> (f32, i3\n   tt.return %3#0, %3#1: f32, i32\n }\n }\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+// CHECK-LABEL: whileop\n+//       CHECK: %[[L:.+]] = tt.load %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked>\n+//       CHECK: %[[W:.+]] = scf.while (%[[I:.+]] = %[[L]], %{{.*}} = %{{.*}}) : (tensor<1024xf32, #blocked>, i1) -> tensor<1024xf32, #blocked> {\n+//       CHECK:   scf.condition(%{{.*}}) %[[I]] : tensor<1024xf32, #blocked>\n+//       CHECK: } do {\n+//       CHECK: ^bb0(%[[ARG1:.+]]: tensor<1024xf32, #blocked>):\n+//       CHECK:    %[[ADD:.+]] = arith.addf %[[ARG1]], %[[ARG1]] : tensor<1024xf32, #blocked>\n+//       CHECK:    scf.yield %[[ADD]], %{{.*}} : tensor<1024xf32, #blocked>, i1\n+//       CHECK:  }\n+//       CHECK:  tt.store %{{.*}}, %[[W]] {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf32, #blocked>\n+tt.func @whileop(%ptr: tensor<1024x!tt.ptr<f32>, #blocked>, %cond: i1) {\n+  %0 = tt.load %ptr {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked>\n+  %1 = triton_gpu.convert_layout %0 : (tensor<1024xf32, #blocked>) -> tensor<1024xf32, #blocked1>\n+  %2 = scf.while (%arg0 = %1, %arg1 = %cond) : (tensor<1024xf32, #blocked1>, i1) -> (tensor<1024xf32, #blocked1>) {\n+      scf.condition(%arg1) %arg0 : tensor<1024xf32, #blocked1>\n+    } do {\n+    ^bb0(%arg0: tensor<1024xf32, #blocked1>):\n+      %4 = triton_gpu.convert_layout %arg0 : (tensor<1024xf32, #blocked1>) -> tensor<1024xf32, #blocked>\n+      %5 = arith.addf %4, %4 : tensor<1024xf32, #blocked>\n+      %6 = triton_gpu.convert_layout %5 : (tensor<1024xf32, #blocked>) -> tensor<1024xf32, #blocked1>\n+      scf.yield %6, %cond : tensor<1024xf32, #blocked1>, i1\n+    }\n+  %3 = triton_gpu.convert_layout %2 : (tensor<1024xf32, #blocked1>) -> tensor<1024xf32, #blocked>\n+  tt.store %ptr, %3 {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf32, #blocked>\n+  tt.return\n+}\n+}"}]