[{"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -311,10 +311,10 @@ def test_layer_norm(M, N, dtype, eps=1e-5, device='cuda'):\n     y_ref.backward(dy, retain_graph=True)\n     dx_ref, dw_ref, db_ref = [_.grad.clone() for _ in [x, weight, bias]]\n     # compare\n-    triton.testing.assert_almost_equal(y_tri, y_ref)\n-    triton.testing.assert_almost_equal(dx_tri, dx_ref)\n-    triton.testing.assert_almost_equal(db_tri, db_ref, decimal=1)\n-    triton.testing.assert_almost_equal(dw_tri, dw_ref, decimal=1)\n+    assert torch.allclose(y_tri, y_ref, atol=1e-2, rtol=0)\n+    assert torch.allclose(dx_tri, dx_ref, atol=1e-2, rtol=0)\n+    assert torch.allclose(db_tri, db_ref, atol=1e-2, rtol=0)\n+    assert torch.allclose(dw_tri, dw_ref, atol=1e-2, rtol=0)\n \n \n @triton.testing.perf_report("}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -299,10 +299,10 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     tri_dk, k.grad = k.grad.clone(), None\n     tri_dq, q.grad = q.grad.clone(), None\n     # compare\n-    triton.testing.assert_almost_equal(ref_out, tri_out)\n-    triton.testing.assert_almost_equal(ref_dv, tri_dv)\n-    triton.testing.assert_almost_equal(ref_dk, tri_dk)\n-    triton.testing.assert_almost_equal(ref_dq, tri_dq)\n+    assert torch.allclose(ref_out, tri_out, atol=1e-2, rtol=0)\n+    assert torch.allclose(ref_dv, tri_dv, atol=1e-2, rtol=0)\n+    assert torch.allclose(ref_dk, tri_dk, atol=1e-2, rtol=0)\n+    assert torch.allclose(ref_dq, tri_dq, atol=1e-2, rtol=0)\n \n \n try:"}]