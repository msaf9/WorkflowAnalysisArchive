[{"filename": "include/triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -161,7 +161,7 @@ struct PTXBuilder {\n \n   std::string dump() const;\n \n-  mlir::Value launch(ConversionPatternRewriter &rewriter, Location loc,\n+  mlir::Value launch(OpBuilder &rewriter, Location loc,\n                      Type resTy, bool hasSideEffect = true,\n                      bool isAlignStack = false,\n                      ArrayRef<Attribute> attrs = {}) const;"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "file_content_changes": "@@ -40,6 +40,19 @@ def TTG_AsyncWaitOp : TTG_Op<\"async_wait\"> {\n   }];\n }\n \n+def TTG_AsyncCommitGroupOp : TTG_Op<\"async_commit_group\"> {\n+  let summary = \"async commit group\";\n+\n+  let assemblyFormat = \"attr-dict\";\n+\n+  let extraClassDeclaration = [{\n+    static bool isSupported(int computeCapability) {\n+      return computeCapability >= 80;\n+    }\n+  }];\n+}\n+\n+\n // Port Arith_CmpIOp & Arith_CmpFOp & Std_SelectOp to TritonGPU.\n // This is needed because these ops don't\n // handle encodings"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -802,9 +802,6 @@ struct InsertSliceAsyncOpConversion\n       }\n     }\n \n-    PTXBuilder ptxBuilder;\n-    ptxBuilder.create<>(\"cp.async.commit_group\")->operator()();\n-    ptxBuilder.launch(rewriter, loc, void_ty(getContext()));\n     rewriter.replaceOp(op, llDst);\n     return success();\n   }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/PTXAsmFormat.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -78,7 +78,7 @@ SmallVector<PTXBuilder::Operand *, 4> PTXBuilder::getAllArgs() const {\n   return res;\n }\n \n-mlir::Value PTXBuilder::launch(ConversionPatternRewriter &rewriter,\n+mlir::Value PTXBuilder::launch(OpBuilder &rewriter,\n                                Location loc, Type resTy, bool hasSideEffect,\n                                bool isAlignStack,\n                                ArrayRef<Attribute> attrs) const {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 19, "deletions": 0, "changes": 19, "file_content_changes": "@@ -505,6 +505,24 @@ struct AsyncWaitOpConversion\n   }\n };\n \n+struct AsyncCommitGroupOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::AsyncCommitGroupOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::gpu::AsyncCommitGroupOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::gpu::AsyncCommitGroupOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    \n+    PTXBuilder ptxBuilder;\n+    ptxBuilder.create<>(\"cp.async.commit_group\")->operator()();\n+    ptxBuilder.launch(rewriter, op.getLoc(), void_ty(op.getContext()));\n+    // Safe to remove the op since it doesn't have any return value.\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+};\n+\n void populateTritonGPUToLLVMPatterns(\n     mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n@@ -514,6 +532,7 @@ void populateTritonGPUToLLVMPatterns(\n   patterns.add<AddPtrOpConversion>(typeConverter, benefit);\n   patterns.add<AllocTensorOpConversion>(typeConverter, allocation, smem,\n                                         benefit);\n+  patterns.add<AsyncCommitGroupOpConversion>(typeConverter, benefit);\n   patterns.add<AsyncWaitOpConversion>(typeConverter, benefit);\n   patterns.add<BroadcastOpConversion>(typeConverter, benefit);\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -388,6 +388,11 @@ class ConvertTritonGPUToLLVM\n       decomposed = true;\n     });\n \n+    mod.walk([&](triton::gpu::AsyncCommitGroupOp asyncCommitGroupOp) -> void {\n+      if (!triton::gpu::AsyncCommitGroupOp::isSupported(computeCapability)) \n+        asyncCommitGroupOp.erase();\n+    });\n+\n     mod.walk([&](triton::gpu::AsyncWaitOp asyncWaitOp) -> void {\n       if (!triton::gpu::AsyncWaitOp::isSupported(computeCapability)) {\n         // async wait is supported in Ampere and later"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -302,6 +302,7 @@ void LoopPipeliner::emitPrologue() {\n               loadStageBuffer[loadOp][stage], pipelineIterIdx, newMask,\n               lookupOrDefault(loadOp.other(), stage), loadOp.cache(),\n               loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n+          builder.create<triton::gpu::AsyncCommitGroupOp>(op->getLoc());\n           loadStageBuffer[loadOp].push_back(newOp->getResult(0));\n         } else\n           llvm_unreachable(\"This should be LoadOp\");\n@@ -542,6 +543,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n           insertSliceIndex, newMask,\n           nextMapping.lookupOrDefault(loadOp.other()), loadOp.cache(),\n           loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n+      builder.create<triton::gpu::AsyncCommitGroupOp>(op->getLoc());\n       nextBuffers.push_back(insertAsyncOp);\n       auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n       sliceType = RankedTensorType::get(sliceType.getShape(),"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -258,6 +258,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n     return nullptr;\n   }\n \n+  // llvm::outs() << module << \"\\n\";\n   auto llvmIR = translateLLVMToLLVMIR(llvmContext, module);\n   if (!llvmIR) {\n     llvm::errs() << \"Translate to LLVM IR failed\";"}, {"filename": "python/matmul.ptx", "status": "removed", "additions": 0, "deletions": 2652, "changes": 2652, "file_content_changes": "N/A"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1551,7 +1551,7 @@ def _init_handles(self):\n         if self.shared > max_shared:\n             raise OutOfResources(self.shared, max_shared, \"shared memory\")\n         mod, func, n_regs, n_spills = cuda_utils.load_binary(self.metadata[\"name\"], self.asm[\"cubin\"], self.shared, device)\n-        print(self.shared, n_regs, n_spills)\n+        # print(self.shared, n_regs, n_spills)\n         # print(self.shared, n_regs, n_spills)\n         self.cu_module = mod\n         self.cu_function = func"}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 5, "deletions": 13, "changes": 18, "file_content_changes": "@@ -111,9 +111,6 @@ def _kernel(A, B, C, M, N, K,\n     else:\n         tl.atomic_add(C, acc, mask=mask)\n \n-\n-_kernel = triton.compile(\"matmul.ptx\", num_warps=4, shared=32768)\n-\n class _matmul(torch.autograd.Function):\n     kernel = _kernel\n \n@@ -137,16 +134,11 @@ def _call(a, b):\n         ACC_TYPE = tl.float32 if a.dtype in [torch.float16, torch.bfloat16, torch.float32] else tl.int32\n         # launch kernel\n         grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']), META['SPLIT_K'], 1)\n-        # _kernel[grid](a, b, c, M, N, K,\n-        #               a.stride(0), a.stride(1),\n-        #               b.stride(0), b.stride(1),\n-        #               c.stride(0), c.stride(1),\n-        #               GROUP_M=8, ACC_TYPE=ACC_TYPE)\n-        grid = grid({\"BLOCK_M\": 128, \"BLOCK_N\":128, \"BLOCK_K\":32, \"SPLIT_K\": 1})\n-        _kernel[grid](a.data_ptr(), b.data_ptr(), c.data_ptr(), M, N, K,\n-                      a.stride(1),\n-                      b.stride(0),\n-                      c.stride(0),)\n+        _kernel[grid](a, b, c, M, N, K,\n+                      a.stride(0), a.stride(1),\n+                      b.stride(0), b.stride(1),\n+                      c.stride(0), c.stride(1),\n+                      GROUP_M=8, ACC_TYPE=ACC_TYPE)\n         return c\n \n     @staticmethod"}]