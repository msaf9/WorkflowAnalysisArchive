[{"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -17,7 +17,6 @@ using ::mlir::triton::gpu::getContigPerThread;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::getShapePerCTATile;\n-using ::mlir::triton::gpu::getSizePerThread;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n using ::mlir::triton::gpu::SliceEncodingAttr;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "file_content_changes": "@@ -127,8 +127,6 @@ struct ConvertLayoutOpConversion\n     if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n       unsigned dim = sliceLayout.getDim();\n       auto parentEncoding = sliceLayout.getParent();\n-      // [benzh] how about slice of mma\n-      auto parentSizePerThread = getSizePerThread(parentEncoding, {});\n       auto parentShape = sliceLayout.paddedShape(shape);\n       auto parentTy = RankedTensorType::get(parentShape, type.getElementType(),\n                                             parentEncoding);\n@@ -249,10 +247,10 @@ struct ConvertLayoutOpConversion\n     auto accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n     auto layout = type.getEncoding();\n     auto rank = type.getRank();\n-    auto sizePerThread = getSizePerThread(layout, {});\n+    auto shapePerCTA = getShapePerCTA(layout, type.getShape());\n+    auto sizePerThread = getSizePerThread(layout, shapePerCTA);\n     auto accumSizePerThread = product<unsigned>(sizePerThread);\n     SmallVector<unsigned> numCTATiles(rank);\n-    auto shapePerCTA = getShapePerCTA(layout, type.getShape());\n     auto shapePerCTATile = getShapePerCTATile(layout, shapePerCTA);\n     auto order = getOrder(layout);\n     for (unsigned d = 0; d < rank; ++d) {\n@@ -537,7 +535,7 @@ struct ConvertLayoutOpConversion\n     SmallVector<unsigned> outNumCTAsEachRep(rank);\n     SmallVector<unsigned> inNumCTAs(rank);\n     SmallVector<unsigned> outNumCTAs(rank);\n-    // [benzh] here logic need more check\n+    // [benzh] here logic need more check: src and dst using same shape???\n     auto srcShapePerCTATile = getShapePerCTATile(srcLayout, srcTy.getShape());\n     auto dstShapePerCTATile = getShapePerCTATile(dstLayout, shape);\n     auto shapePerCTA = getShapePerCTA(srcLayout, shape);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -9,7 +9,6 @@ using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::getContigPerThread;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n-using ::mlir::triton::gpu::getSizePerThread;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n using ::mlir::triton::gpu::isaDistributedLayout;\n using ::mlir::triton::gpu::SharedEncodingAttr;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -11,7 +11,6 @@ using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::getContigPerThread;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n-using ::mlir::triton::gpu::getSizePerThread;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n using ::mlir::triton::gpu::isaDistributedLayout;\n using ::mlir::triton::gpu::SharedEncodingAttr;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -153,8 +153,9 @@ struct ReduceOpConversion\n     }\n     // The order of the axes for the the threads within the warp\n     auto srcOrd = triton::gpu::getOrder(srcLayout);\n-    auto sizePerThread = triton::gpu::getSizePerThread(srcLayout, {});\n     auto srcShape = helper.getSrcShape();\n+    auto sizePerThread = triton::gpu::getSizePerThread(\n+        srcLayout, triton::gpu::getShapePerCTA(srcLayout, srcShape));\n \n     SmallVector<Type> elemPtrTys(srcTys.size());\n     for (unsigned i = 0; i < op.getNumOperands(); ++i) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -545,11 +545,13 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       auto layout = tensorTy.getEncoding();\n       auto shape = tensorTy.getShape();\n       unsigned rank = shape.size();\n-      auto sizePerThread = triton::gpu::getSizePerThread(layout, {});\n+      auto shapePerCTA = triton::gpu::getShapePerCTA(tensorTy);\n+      auto sizePerThread = triton::gpu::getSizePerThread(layout, shapePerCTA);\n       auto threadsPerWarp = triton::gpu::getThreadsPerWarp(layout);\n       auto warpsPerCTA = triton::gpu::getWarpsPerCTA(layout);\n       auto order = triton::gpu::getOrder(layout);\n-      auto shapePerCTATile = triton::gpu::getShapePerCTATile(layout, shape);\n+      auto shapePerCTATile =\n+          triton::gpu::getShapePerCTATile(layout, shapePerCTA);\n       Value warpSize = i32_val(32);\n       Value laneId = urem(tid, warpSize);\n       Value warpId = udiv(tid, warpSize);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -655,8 +655,8 @@ class ConvertTritonGPUToLLVM\n             dstType.getShape(), dstType.getElementType(),\n             triton::gpu::BlockedEncodingAttr::get(\n                 mod.getContext(), srcType.getShape(),\n-                getSizePerThread(srcMma, {}), getOrder(srcMma), numWarps,\n-                threadsPerWarp, numCTAs));\n+                getSizePerThread(srcMma, triton::gpu::getShapePerCTA(srcType)),\n+                getOrder(srcMma), numWarps, threadsPerWarp, numCTAs));\n         auto tmp = builder.create<triton::gpu::ConvertLayoutOp>(\n             cvtOp.getLoc(), tmpType, cvtOp.getOperand());\n         addWSNamedAttrs(tmp, cvtOp->getAttrs());"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/PlanCTA.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -301,7 +301,8 @@ bool CTAPlanner::processReduce(triton::FuncOp &funcOp) {\n \n     auto rank = srcShape.size();\n     auto order = ttg::getOrder(srcLayout);\n-    auto sizePerThread = ttg::getSizePerThread(srcLayout, {});\n+    auto sizePerThread =\n+        ttg::getSizePerThread(srcLayout, ttg::getShapePerCTA(srcTy));\n     auto CTAOrder = ttg::getCTAOrder(srcLayout);\n \n     llvm::SmallVector<unsigned> CTAsPerCGA(rank, 0);"}]