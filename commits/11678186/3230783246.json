[{"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.td", "status": "modified", "additions": 0, "deletions": 11, "changes": 11, "file_content_changes": "@@ -76,15 +76,4 @@ def TritonGPUCanonicalizeLoops: Pass<\"tritongpu-canonicalize-loops\", \"mlir::Modu\n   let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\"];\n }\n \n-def TritonGPUVerifier : Pass<\"tritongpu-verifier\", \"mlir::ModuleOp\"> {\n-  let summary = \"verify TritonGPU IR\";\n-\n-  let description = [{}];\n-\n-  let constructor = \"mlir::createTritonGPUVerifier()\";\n-\n-  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n-                           \"mlir::gpu::GPUDialect\"];\n-}\n-\n #endif"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Verifier.cpp", "status": "removed", "additions": 0, "deletions": 106, "changes": 106, "file_content_changes": "@@ -1,106 +0,0 @@\n-#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n-#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n-\n-#include <memory>\n-\n-using namespace mlir;\n-\n-#define GEN_PASS_CLASSES\n-#include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n-\n-class TritonGPUVerifier : public TritonGPUVerifierBase<TritonGPUVerifier> {\n-public:\n-  void runOnOperation() override {\n-    MLIRContext *context = &getContext();\n-    ModuleOp m = getOperation();\n-\n-    // The idea is similar to mlir/lib/IR/Verifier.cpp\n-    verifyImpl(m.getOperation());\n-  }\n-\n-private:\n-  LogicalResult verifySingleOp(Operation *op) {\n-    if (auto dotOp = llvm::dyn_cast<triton::DotOp>(op)) {\n-      Type aType = dotOp.a().getType();\n-      Type bType = dotOp.b().getType();\n-      Type cType = dotOp.c().getType();\n-      Type dType = dotOp.d().getType();\n-      for (auto it : llvm::zip(llvm::SmallVector<Type>{aType, bType},\n-                               llvm::SmallVector<char>{'a', 'b'})) {\n-        Type type = std::get<0>(it);\n-        char name = std::get<1>(it);\n-        if (auto tensorType = type.dyn_cast<RankedTensorType>()) {\n-          Attribute encoding = tensorType.getEncoding();\n-          if (!encoding)\n-            return dotOp.emitError() << name << \" should have encoding\";\n-          if (!encoding.isa<triton::gpu::SharedEncodingAttr>())\n-            return dotOp.emitError() << name << \" should be of shared layout\";\n-        } else\n-          return dotOp.emitError()\n-                 << name << \"'s type should be of RankedTensorType\";\n-      }\n-\n-      Attribute cLayout;\n-      for (auto it : llvm::zip(llvm::SmallVector<Type>{cType, dType},\n-                               llvm::SmallVector<char>{'c', 'd'})) {\n-        Type type = std::get<0>(it);\n-        char name = std::get<1>(it);\n-        if (auto tensorType = type.dyn_cast<RankedTensorType>()) {\n-          Attribute encoding = tensorType.getEncoding();\n-          if (!encoding)\n-            return dotOp.emitError() << name << \" should have encoding\";\n-          if (!encoding.isa<triton::gpu::MmaEncodingAttr>() &&\n-              !encoding.isa<triton::gpu::BlockedEncodingAttr>())\n-            return dotOp.emitError()\n-                   << name << \" should be of distributed layout\";\n-          if (name == 'c')\n-            cLayout = encoding;\n-          else if (encoding != cLayout)\n-            return dotOp.emitError() << \"d & c should have the same layout\";\n-        } else\n-          return dotOp.emitError()\n-                 << name << \"'s type should be of RankedTensorType\";\n-      }\n-\n-      // signalPassFailure();\n-    }\n-    if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(op)) {\n-      // TODO: fill this\n-    }\n-    if (auto storeOp = llvm::dyn_cast<triton::StoreOp>(op)) {\n-      // TODO: fill this\n-    }\n-    if (auto addptrOp = llvm::dyn_cast<triton::AddPtrOp>(op)) {\n-      // TODO: fill this\n-    }\n-    // Triton builtin Ops\n-    if (llvm::isa<triton::GetProgramIdOp, triton::GetNumProgramsOp,\n-                  triton::MakeRangeOp>(op)) {\n-      // TODO: fill this\n-    }\n-    if (auto atomicRmw = llvm::dyn_cast<triton::AtomicRMWOp>(op)) {\n-      // TODO: fill this\n-    }\n-    if (auto atomicCas = llvm::dyn_cast<triton::AtomicCASOp>(op)) {\n-      // TODO: fill this\n-    }\n-\n-    // TODO: Arithmetic, SCF, TritonGPU ops\n-    return success();\n-  }\n-\n-  void verifyImpl(Operation *op) {\n-    if (verifySingleOp(op).failed())\n-      signalPassFailure();\n-\n-    // verify that all child regions are ok\n-    for (Region &region : op->getRegions())\n-      for (Block &block : region)\n-        for (Operation &childOp : block)\n-          verifyImpl(&childOp);\n-  }\n-};\n-\n-std::unique_ptr<Pass> mlir::createTritonGPUVerifier() {\n-  return std::make_unique<TritonGPUVerifier>();\n-}"}]