[{"filename": ".gitmodules", "status": "removed", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -1,3 +0,0 @@\n-[submodule \"deps/dlfcn-win32\"]\n-\tpath = deps/dlfcn-win32\n-\turl = https://github.com/dlfcn-win32/dlfcn-win32.git"}, {"filename": "CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 6, "changes": 6, "file_content_changes": "@@ -46,12 +46,6 @@ include_directories(${CMAKE_CURRENT_SOURCE_DIR}/include)\n # Third-party\n include_directories(${PYBIND11_INCLUDE_DIR})\n \n-if(WIN32)\n-  SET(BUILD_SHARED_LIBS OFF)\n-  find_package(dlfcn-win32 REQUIRED)\n-  set(CMAKE_DL_LIBS dlfcn-win32::dl)\n-endif()\n-\n set(CMAKE_CXX_FLAGS \"${CMAKE_C_FLAGS} -D__STDC_FORMAT_MACROS  -fPIC -std=gnu++17 -fvisibility=hidden -fvisibility-inlines-hidden\")\n \n if(APPLE)"}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -61,8 +61,6 @@ class ReduceOpHelper {\n   int axis;\n };\n \n-bool isSharedEncoding(Value value);\n-\n bool maybeSharedAllocationOp(Operation *op);\n \n bool maybeAliasOp(Operation *op);"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -601,6 +601,11 @@ def CallOp : TT_Op<\"call\", [CallOpInterface, /*MemRefsNormalizable, */DeclareOpI\n     CallInterfaceCallable getCallableForCallee() {\n       return (*this)->getAttrOfType<SymbolRefAttr>(\"callee\");\n     }\n+\n+    /// Set the callee for this operation.\n+    void setCalleeFromCallable(CallInterfaceCallable callee) {\n+      (*this)->setAttr(\"callee\", callee.get<SymbolRefAttr>());\n+    }\n   }];\n \n   let assemblyFormat = [{"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -74,6 +74,9 @@ bool isaDistributedLayout(Attribute layout);\n \n } // namespace gpu\n } // namespace triton\n+\n+bool isSharedEncoding(Value value);\n+\n } // namespace mlir\n \n #endif // TRITON_DIALECT_TRITONGPU_IR_DIALECT_H_"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -4,7 +4,6 @@\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"triton/Analysis/Alias.h\"\n-#include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"llvm/ADT/SmallVector.h\"\n "}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 0, "deletions": 9, "changes": 9, "file_content_changes": "@@ -111,15 +111,6 @@ bool ReduceOpHelper::isSupportedLayout() {\n   return false;\n }\n \n-bool isSharedEncoding(Value value) {\n-  auto type = value.getType();\n-  if (auto tensorType = type.dyn_cast<RankedTensorType>()) {\n-    auto encoding = tensorType.getEncoding();\n-    return encoding && encoding.isa<triton::gpu::SharedEncodingAttr>();\n-  }\n-  return false;\n-}\n-\n bool maybeSharedAllocationOp(Operation *op) {\n   // TODO(Keren): This function can be replaced by adding\n   // MemoryEffectOpInterface. We can then use the MemoryEffectOpInterface to"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 8, "deletions": 6, "changes": 14, "file_content_changes": "@@ -67,19 +67,21 @@ class ArithConstantPattern : public OpConversionPattern<arith::ConstantOp> {\n   matchAndRewrite(arith::ConstantOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = getTypeConverter()->convertType(op.getType());\n+    auto retShapedType = retType.cast<ShapedType>();\n     auto value = adaptor.getValue().dyn_cast<DenseElementsAttr>();\n-    if (dyn_cast<RankedTensorType>(retType)) {\n+    if (dyn_cast<RankedTensorType>(retShapedType)) {\n       assert(value);\n       if (value.getElementType().isInteger(1) && value.isSplat())\n         // Workaround until https://reviews.llvm.org/D133743 is included.\n-        value = DenseElementsAttr::get(retType, value.getSplatValue<bool>());\n+        value =\n+            DenseElementsAttr::get(retShapedType, value.getSplatValue<bool>());\n       else\n         // This is a hack. We just want to add encoding\n-        value = value.reshape(retType);\n+        value = value.reshape(retShapedType);\n     }\n-    addNamedAttrs(\n-        rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, retType, value),\n-        adaptor.getAttributes());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<arith::ConstantOp>(\n+                      op, retShapedType, value),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };"}, {"filename": "lib/Dialect/Triton/IR/Dialect.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -91,5 +91,5 @@ void TritonDialect::initialize() {\n Operation *TritonDialect::materializeConstant(OpBuilder &builder,\n                                               Attribute value, Type type,\n                                               Location loc) {\n-  return builder.create<arith::ConstantOp>(loc, type, value);\n+  return arith::ConstantOp::materialize(builder, value, type, loc);\n }"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -349,7 +349,7 @@ void triton::StoreOp::getCanonicalizationPatterns(RewritePatternSet &results,\n //-- TransOp --\n mlir::LogicalResult mlir::triton::TransOp::inferReturnTypes(\n     MLIRContext *context, std::optional<Location> location, ValueRange operands,\n-    DictionaryAttr attributes, RegionRange regions,\n+    DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,\n     SmallVectorImpl<Type> &inferredReturnTypes) {\n   // type is the same as the input\n   auto argTy = operands[0].getType().cast<RankedTensorType>();\n@@ -376,7 +376,7 @@ mlir::LogicalResult mlir::triton::TransOp::inferReturnTypes(\n //-- DotOp --\n mlir::LogicalResult mlir::triton::DotOp::inferReturnTypes(\n     MLIRContext *context, std::optional<Location> location, ValueRange operands,\n-    DictionaryAttr attributes, RegionRange regions,\n+    DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,\n     SmallVectorImpl<Type> &inferredReturnTypes) {\n   // type is the same as the accumulator\n   auto accTy = operands[2].getType().cast<RankedTensorType>();\n@@ -444,7 +444,7 @@ void ReduceOp::build(mlir::OpBuilder &builder, mlir::OperationState &state,\n \n mlir::LogicalResult mlir::triton::ReduceOp::inferReturnTypes(\n     MLIRContext *context, std::optional<Location> location, ValueRange operands,\n-    DictionaryAttr attributes, RegionRange regions,\n+    DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,\n     SmallVectorImpl<Type> &inferredReturnTypes) {\n   for (auto arg : operands) {\n     auto argTy = arg.getType().cast<RankedTensorType>();\n@@ -551,7 +551,7 @@ OpFoldResult SplatOp::fold(FoldAdaptor adaptor) {\n //-- ExpandDimsOp --\n mlir::LogicalResult mlir::triton::ExpandDimsOp::inferReturnTypes(\n     MLIRContext *context, std::optional<Location> loc, ValueRange operands,\n-    DictionaryAttr attributes, RegionRange regions,\n+    DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,\n     SmallVectorImpl<Type> &inferredReturnTypes) {\n   // infer shape\n   auto arg = operands[0];"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -37,7 +37,7 @@ bool isBroadcastConstantCombinable(Attribute value) {\n DenseElementsAttr getConstantValue(Builder &builder, Attribute value,\n                                    Value bcast_res) {\n \n-  Type resType = bcast_res.getType();\n+  auto resType = bcast_res.getType().cast<ShapedType>();\n   DenseElementsAttr res;\n   if (auto denseValue = value.dyn_cast<DenseElementsAttr>()) {\n     res ="}, {"filename": "lib/Dialect/Triton/Transforms/RewriteTensorPointer.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -167,10 +167,10 @@ struct RewritedInfo {\n     auto otherTensorType = RankedTensorType::get(tensorShape, elementType);\n \n     // Set zero padding value\n-    Attribute attr =\n+    TypedAttr attr =\n         elementType.isIntOrIndex()\n-            ? builder.getIntegerAttr(elementType, 0).cast<Attribute>()\n-            : builder.getFloatAttr(elementType, 0).cast<Attribute>();\n+            ? builder.getIntegerAttr(elementType, 0).cast<TypedAttr>()\n+            : builder.getFloatAttr(elementType, 0).cast<TypedAttr>();\n \n     // Float NaN padding case\n     if (padding.value() == triton::PaddingOption::PAD_NAN) {"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 12, "deletions": 2, "changes": 14, "file_content_changes": "@@ -356,6 +356,16 @@ bool isaDistributedLayout(Attribute layout) {\n \n } // namespace gpu\n } // namespace triton\n+\n+bool isSharedEncoding(Value value) {\n+  auto type = value.getType();\n+  if (auto tensorType = type.dyn_cast<RankedTensorType>()) {\n+    auto encoding = tensorType.getEncoding();\n+    return encoding && encoding.isa<triton::gpu::SharedEncodingAttr>();\n+  }\n+  return false;\n+}\n+\n } // namespace mlir\n \n static LogicalResult parseIntAttrValue(AsmParser &parser, Attribute attr,\n@@ -1218,8 +1228,8 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n   // cvt(type, constant) -> constant\n   if (auto cst = llvm::dyn_cast<arith::ConstantOp>(arg))\n     if (auto ret = cst.getValue().dyn_cast<SplatElementsAttr>()) {\n-      auto newRet = SplatElementsAttr::get(op->getResultTypes().front(),\n-                                           ret.getSplatValue<Attribute>());\n+      auto ty = op->getResultTypes().front().cast<ShapedType>();\n+      auto newRet = SplatElementsAttr::get(ty, ret.getSplatValue<Attribute>());\n       rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, newRet);\n       return mlir::success();\n     }"}, {"filename": "lib/Dialect/TritonGPU/IR/Traits.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,5 +1,5 @@\n #include \"triton/Dialect/TritonGPU/IR/Traits.h\"\n-#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n \n mlir::LogicalResult\n mlir::OpTrait::impl::verifyResultsAreSharedEncoding(Operation *op) {"}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 7, "deletions": 6, "changes": 13, "file_content_changes": "@@ -44,12 +44,13 @@ SmallVector<int64_t, 2> mmaVersionToShapePerWarp(int version) {\n SmallVector<unsigned, 2> warpsPerTileV2(triton::DotOp dotOp,\n                                         const ArrayRef<int64_t> shape,\n                                         int numWarps) {\n-  SetVector<Operation *> slices;\n-  mlir::getForwardSlice(dotOp.getResult(), &slices);\n-  if (llvm::find_if(slices, [](Operation *op) {\n-        return isa<triton::DotOp>(op);\n-      }) != slices.end())\n-    return {(unsigned)numWarps, 1};\n+  auto filter = [&dotOp](Operation *op) {\n+    return op->getParentRegion() == dotOp->getParentRegion();\n+  };\n+  auto slices = mlir::getSlice(dotOp, filter);\n+  for (Operation *op : slices)\n+    if (isa<triton::DotOp>(op) && (op != dotOp))\n+      return {(unsigned)numWarps, 1};\n \n   SmallVector<unsigned, 2> ret = {1, 1};\n   SmallVector<int64_t, 2> shapePerWarp = {16, 8};"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -27,7 +27,6 @@\n //===----------------------------------------------------------------------===//\n \n #include \"mlir/IR/IRMapping.h\"\n-#include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -13,7 +13,6 @@\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n #include \"mlir/Transforms/Passes.h\"\n #include \"mlir/Transforms/RegionUtils.h\"\n-#include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h\""}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 6, "deletions": 3, "changes": 9, "file_content_changes": "@@ -213,9 +213,12 @@ Operation *cloneWithInferType(mlir::OpBuilder &rewriter, Operation *op,\n     SmallVector<Type, 1> newTypes;\n     auto success = typeInfer.inferReturnTypes(\n         newOp->getContext(), newOp->getLoc(), newOp->getOperands(),\n-        newOp->getAttrDictionary(), newOp->getRegions(), newTypes);\n-    if (succeeded(success))\n-      newOp->getResult(0).setType(newTypes.front());\n+        newOp->getAttrDictionary(), newOp->getPropertiesStorage(),\n+        newOp->getRegions(), newTypes);\n+    if (succeeded(success)) {\n+      for (size_t i = 0; i < newTypes.size(); i++)\n+        newOp->getResult(i).setType(newTypes[i]);\n+    }\n   }\n   return newOp;\n }"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 34, "deletions": 8, "changes": 42, "file_content_changes": "@@ -25,10 +25,17 @@\n #include \"llvm/IRReader/IRReader.h\"\n #include \"llvm/Linker/Linker.h\"\n #include \"llvm/Support/SourceMgr.h\"\n+#ifdef _WIN32\n+#define WIN32_LEAN_AND_MEAN\n+#include <windows.h>\n+#else\n #include <dlfcn.h>\n+#endif\n #include <filesystem>\n #include <iterator>\n \n+namespace fs = std::filesystem;\n+\n namespace mlir {\n namespace triton {\n \n@@ -113,6 +120,32 @@ extractNVVMMetadata(mlir::ModuleOp module,\n   }\n }\n \n+static std::filesystem::path getThisLibraryPath() {\n+#ifdef _WIN32\n+  /* Get module of the specified address */\n+  HMODULE hModule;\n+  GetModuleHandleExA(GET_MODULE_HANDLE_EX_FLAG_FROM_ADDRESS |\n+                         GET_MODULE_HANDLE_EX_FLAG_UNCHANGED_REFCOUNT,\n+                     reinterpret_cast<LPCSTR>(&getThisLibraryPath), &hModule);\n+  if (NULL == hModule) {\n+    return std::filesystem::path();\n+  }\n+\n+  char fileName[1024]; // this is way beyond Windows MAX_PATH limit.\n+  DWORD dwSize = GetModuleFileNameA(hModule, fileName, sizeof(fileName));\n+  if (0 == dwSize || sizeof(fileName) == dwSize) {\n+    return std::filesystem::path();\n+  }\n+  return std::filesystem::path(fileName);\n+#else\n+  Dl_info fileinfo;\n+  if (dladdr(reinterpret_cast<void *>(&getThisLibraryPath), &fileinfo) == 0) {\n+    return std::filesystem::path();\n+  }\n+  return std::filesystem::path(fileinfo.dli_fname);\n+#endif\n+}\n+\n static std::map<std::string, std::string> getExternLibs(mlir::ModuleOp module) {\n   std::map<std::string, std::string> externLibs;\n   SmallVector<LLVM::LLVMFuncOp> funcs;\n@@ -152,17 +185,10 @@ static std::map<std::string, std::string> getExternLibs(mlir::ModuleOp module) {\n       externLibs.try_emplace(libdevice, env_path);\n       return externLibs;\n     }\n-    namespace fs = std::filesystem;\n     // Search for libdevice relative to its library path if used from Python\n     // Then native code is in `triton/_C/libtriton.so` and libdevice in\n     // `triton/third_party/cuda/lib/libdevice.10.bc`\n-    static const auto this_library_path = [] {\n-      Dl_info fileinfo;\n-      if (dladdr(reinterpret_cast<void *>(&getExternLibs), &fileinfo) == 0) {\n-        return std::filesystem::path();\n-      }\n-      return std::filesystem::path(fileinfo.dli_fname);\n-    }();\n+    static const auto this_library_path = getThisLibraryPath();\n     static const auto runtime_path =\n         this_library_path.parent_path().parent_path() / \"third_party\" / \"cuda\" /\n         \"lib\" / \"libdevice.10.bc\";"}, {"filename": "python/setup.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -68,7 +68,7 @@ def get_llvm_package_info():\n     use_assert_enabled_llvm = check_env_flag(\"TRITON_USE_ASSERT_ENABLED_LLVM\", \"False\")\n     release_suffix = \"assert\" if use_assert_enabled_llvm else \"release\"\n     name = f'llvm+mlir-17.0.0-x86_64-{system_suffix}-{release_suffix}'\n-    version = \"llvm-17.0.0-f733b4fb9b8b\"\n+    version = \"llvm-17.0.0-c5dede880d17\"\n     url = f\"https://github.com/ptillet/triton-llvm-releases/releases/download/{version}/{name}.tar.xz\"\n     return Package(\"llvm\", name, url, \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n "}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -700,6 +700,8 @@ def visit_For(self, node):\n         ub = language.core._to_tensor(ub, self.builder)\n         step = language.core._to_tensor(step, self.builder)\n         # induction variable type\n+        if not lb.dtype.is_int() or not ub.dtype.is_int() or not step.dtype.is_int():\n+            raise TypeError(f\"For loop bounds and step must all be ints, are ({lb.dtype}, {ub.dtype}, {step.dtype})\")\n         iv_type = language.semantic.integer_promote_impl(lb.dtype, ub.dtype)\n         iv_type = language.semantic.integer_promote_impl(iv_type, step.dtype)\n         iv_ir_type = iv_type.to_ir(self.builder)"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -349,13 +349,14 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n             stream = get_cuda_stream(device)\n         else:\n             stream = device_backend.get_stream()\n-    try:\n-      bin = cache[device][key]\n+\n+    bin = cache[device].get(key, None)\n+    if bin is not None:\n       if not warmup:\n           bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.shared, stream, bin.cu_function, triton.compiler.CompiledKernel.launch_enter_hook, triton.compiler.CompiledKernel.launch_exit_hook, bin, {args})\n       return bin\n     # kernel not cached -- compile\n-    except KeyError:\n+    else:\n       # build dict of constant values\n       args = [{args}]\n       all_args = {', '.join([f'{arg}' for arg in self.arg_names])},"}]