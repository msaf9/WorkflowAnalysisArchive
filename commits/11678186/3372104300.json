[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 5, "deletions": 9, "changes": 14, "file_content_changes": "@@ -1231,13 +1231,10 @@ struct BroadcastOpConversion\n         }\n         auto resultLinearIndex =\n             getLinearIndex<int64_t>(resultMultiDim, resultLogicalShape);\n-        // llvm::outs() << i << \" \" << j << \" \" << resultLinearIndex << \"\\n\";\n         resultVals[resultLinearIndex] = srcVals[i];\n       }\n     }\n     auto llvmStructTy = getTypeConverter()->convertType(resultTy);\n-    // for (auto value : resultVals)\n-    //   llvm::outs() << value << \" \" << srcElems << \" \" << duplicates << \"\\n\";\n \n     Value resultStruct =\n         getStructFromElements(loc, resultVals, rewriter, llvmStructTy);\n@@ -1990,7 +1987,7 @@ struct ConvertLayoutOpConversion\n                       ArrayRef<unsigned> multiDimRepId, unsigned vec,\n                       ArrayRef<unsigned> paddedRepShape,\n                       ArrayRef<unsigned> outOrd, SmallVector<Value> &vals,\n-                      unsigned valOffset, Value smemBase) const;\n+                      Value smemBase) const;\n \n   // blocked/mma -> blocked/mma.\n   // Data padding in shared memory to avoid bank confict.\n@@ -2017,7 +2014,7 @@ void ConvertLayoutOpConversion::processReplica(\n     RankedTensorType type, ArrayRef<unsigned> numCTAsEachRep,\n     ArrayRef<unsigned> multiDimRepId, unsigned vec,\n     ArrayRef<unsigned> paddedRepShape, ArrayRef<unsigned> outOrd,\n-    SmallVector<Value> &vals, unsigned valOffset, Value smemBase) const {\n+    SmallVector<Value> &vals, Value smemBase) const {\n   unsigned accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n   auto layout = type.getEncoding();\n   auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>();\n@@ -2160,8 +2157,7 @@ void ConvertLayoutOpConversion::processReplica(\n             currVal =\n                 icmp_ne(currVal, rewriter.create<LLVM::ConstantOp>(\n                                      loc, i8_ty, rewriter.getI8IntegerAttr(0)));\n-          vals[valOffset + elemId + linearCTAId * accumSizePerThread + v] =\n-              currVal;\n+          vals[elemId + linearCTAId * accumSizePerThread + v] = currVal;\n         }\n       }\n     }\n@@ -2221,7 +2217,7 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n         srcLayout.isa<SliceEncodingAttr>() ||\n         srcLayout.isa<MmaEncodingAttr>()) {\n       processReplica(loc, rewriter, /*stNotRd*/ true, srcTy, inNumCTAsEachRep,\n-                     multiDimRepId, inVec, paddedRepShape, outOrd, vals, 0,\n+                     multiDimRepId, inVec, paddedRepShape, outOrd, vals,\n                      smemBase);\n     } else {\n       assert(0 && \"ConvertLayout with input layout not implemented\");\n@@ -2232,7 +2228,7 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n         dstLayout.isa<SliceEncodingAttr>() ||\n         dstLayout.isa<MmaEncodingAttr>()) {\n       processReplica(loc, rewriter, /*stNotRd*/ false, dstTy, outNumCTAsEachRep,\n-                     multiDimRepId, outVec, paddedRepShape, outOrd, outVals, 0,\n+                     multiDimRepId, outVec, paddedRepShape, outOrd, outVals,\n                      smemBase);\n     } else {\n       assert(0 && \"ConvertLayout with output layout not implemented\");"}]