[{"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -995,8 +995,8 @@ struct ExpOpConversionApprox\n   Value createDestOp(mlir::math::ExpOp op, OpAdaptor adaptor,\n                      ConversionPatternRewriter &rewriter, Type elemTy,\n                      ValueRange operands, Location loc) const {\n-    // For FP64 input, call __nv_expf for higher-precision calculation\n-    if (elemTy.getIntOrFloatBitWidth() == 64)\n+    // For non-FP32 input, call __nv_expf for higher-precision calculation\n+    if (elemTy.getIntOrFloatBitWidth() != 32)\n       return {};\n \n     const double log2e = 1.4426950408889634;\n@@ -1117,7 +1117,7 @@ void populateElementwiseOpToLLVMPatterns(\n \n   patterns.add<ExtElemwiseOpConversion>(typeConverter, benefit);\n   // ExpOpConversionApprox will try using ex2.approx if the input type is\n-  // FP32. For FP64 input type, ExpOpConversionApprox will return failure and\n+  // FP32. For other input types, ExpOpConversionApprox will return failure and\n   // ElementwiseOpConversion<math::ExpOp, math::ExpOp> defined below will call\n   // __nv_expf for higher-precision calculation\n   patterns.add<ExpOpConversionApprox>(typeConverter, benefit);"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 6, "deletions": 7, "changes": 13, "file_content_changes": "@@ -514,11 +514,9 @@ def test_unary_op(dtype_x, expr, device='cuda'):\n # ----------------\n \n \n-@pytest.mark.parametrize(\"expr\", [\n-    'exp', 'log', 'cos', 'sin'\n-])\n-def test_math_op(expr, device='cuda'):\n-    _test_unary('float32', f'tl.{expr}(x)', f'np.{expr}(x) ', device=device)\n+@pytest.mark.parametrize(\"dtype_x, expr\", [(dtype_x, expr) for dtype_x in float_dtypes for expr in ['exp', 'log', 'cos', 'sin']])\n+def test_math_op(dtype_x, expr, device='cuda'):\n+    _test_unary(dtype_x, f'tl.{expr}(x)', f'np.{expr}(x) ', device=device)\n \n # ----------------\n # test abs\n@@ -1968,7 +1966,8 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n-                         [('float32', 'math.pow', '')])\n+                         [('float32', 'math.pow', ''),\n+                          ('float64', 'math.pow', tl.math.LIBDEVICE_PATH)])\n def test_math_scalar(dtype_str, expr, lib_path):\n \n     @triton.jit\n@@ -1991,7 +1990,7 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n     # triton result\n     x_tri = to_triton(x)[0].item()\n     y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n-    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'math': lib_path})\n+    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n     # compare\n     np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n "}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -163,8 +163,8 @@ def _define_name_lookup(self):\n         ))\n         # TODO: this needs to be moved to class scope when cyclic imports untangled and `triton.language` can be imported at module level\n         self.statically_implemented_functions.update((\n-            (triton.language.core.static_assert, self.execute_static_assert),\n-            (triton.language.core.static_print, self.execute_static_print),\n+            (triton.language.core.static_assert, CodeGenerator.execute_static_assert),\n+            (triton.language.core.static_print, CodeGenerator.execute_static_print),\n         ))\n \n         def local_lookup(name: str, absent):\n@@ -813,7 +813,7 @@ def visit_Call(self, node):\n \n         static_implementation = self.statically_implemented_functions.get(fn)\n         if static_implementation is not None:\n-            return static_implementation(node)\n+            return static_implementation(self, node)\n \n         kws = dict(self.visit(keyword) for keyword in node.keywords)\n         args = [self.visit(arg) for arg in node.args]"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 28, "deletions": 12, "changes": 40, "file_content_changes": "@@ -10,6 +10,8 @@\n from collections import defaultdict, namedtuple\n from typing import Callable, Generic, Iterable, Optional, TypeVar, Union, cast, overload\n \n+import torch\n+\n import triton\n from triton.utils import MockTensor\n \n@@ -234,30 +236,44 @@ def __init__(self, module, name):\n \n         return JITFunction.cache_hook(key=key, repr=repr, fn=LegacyCompiler(module, name), compile={\"key\": key, **kwargs}, is_manual_warmup=False, already_compiled=False)\n \n+    def _get_arg_specialization_key(self, arg) -> str:\n+        arg_annotation = self.__annotations__.get(arg, None)\n+        if not arg_annotation:\n+            return f'({arg}.data_ptr() % {JITFunction.divisibility} == 0) if hasattr({arg}, \"data_ptr\") \\\n+                        else ({arg} % {JITFunction.divisibility} == 0, {arg} == 1) if isinstance({arg}, int) \\\n+                        else (False,)'\n+        elif arg_annotation is torch.Tensor:\n+            return f'({arg}.data_ptr() % {JITFunction.divisibility} == 0)'\n+        elif arg_annotation is int:\n+            return f'({arg} % {JITFunction.divisibility} == 0, {arg} == 1)'\n+        else:\n+            return '(False,)'\n+\n+    def _get_arg_sig_key(self, arg) -> str:\n+        arg_annotation = self.__annotations__.get(arg, None)\n+        if arg_annotation is torch.Tensor:\n+            return f'{arg}.dtype'\n+        elif arg_annotation is bool:\n+            return \"i1\"\n+        elif arg_annotation is float:\n+            return 'fp32'\n+        else:\n+            return f'_key_of({arg})'\n+\n     def _make_launcher(self):\n         regular_args = [f'{arg}' for i, arg in enumerate(self.arg_names) if i not in self.constexprs]\n         constexpr_args = [f'{arg}' for i, arg in enumerate(self.arg_names) if i in self.constexprs]\n         args = ', '.join(regular_args)\n         # cache key for regular argument type\n-        sig_keys = ', '.join([f'_key_of({arg})' for arg in regular_args])\n+        sig_keys = ', '.join([self._get_arg_sig_key(arg) for arg in regular_args])\n         # cache key for constexpr argument values\n         constexpr_keys = ', '.join(constexpr_args)\n         # cache key for argument specialization\n         specializations = []\n         for i, arg in enumerate(regular_args):\n             if i in self.do_not_specialize:\n                 continue\n-            arg_annotation = self.__annotations__.get(arg, None)\n-            if not arg_annotation:\n-                specializations += [f'({arg}.data_ptr() % {JITFunction.divisibility} == 0) if hasattr({arg}, \"data_ptr\") '\n-                                    f'else ({arg} % {JITFunction.divisibility} == 0, {arg} == 1) if isinstance({arg}, int) '\n-                                    f'else (False,)']\n-            elif arg_annotation == 'torch.Tensor':\n-                specializations += [f'({arg}.data_ptr() % {JITFunction.divisibility} == 0)']\n-            elif arg_annotation == 'int':\n-                specializations += [f'({arg} % {JITFunction.divisibility} == 0, {arg} == 1)']\n-            else:\n-                specializations += ['(False,)']\n+            specializations += [self._get_arg_specialization_key(arg)]\n \n         spec_keys = ', '.join(specializations)\n         grid_args = ','.join([f'\"{arg}\": {arg}' for arg in self.arg_names])"}, {"filename": "python/tutorials/07-math-functions.py", "status": "added", "additions": 73, "deletions": 0, "changes": 73, "file_content_changes": "@@ -0,0 +1,73 @@\n+\"\"\"\n+Libdevice (`tl.math`) function\n+===============\n+Triton can invoke a custom function from an external library.\n+In this example, we will use the `libdevice` library (a.k.a `math` in triton) to apply `asin` on a tensor.\n+Please refer to https://docs.nvidia.com/cuda/libdevice-users-guide/index.html regarding the semantics of all available libdevice functions.\n+In `triton/language/math.py`, we try to aggregate functions with the same computation but different data types together.\n+For example, both `__nv_asin` and `__nvasinf` calculate the principal value of the arc sine of the input, but `__nv_asin` operates on `double` and `__nv_asinf` operates on `float`.\n+Using triton, you can simply call `tl.math.asin`.\n+Triton automatically selects the correct underlying device function to invoke based on input and output types.\n+\"\"\"\n+\n+# %%\n+#  asin Kernel\n+# --------------------------\n+\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def asin_kernel(\n+        x_ptr,\n+        y_ptr,\n+        n_elements,\n+        BLOCK_SIZE: tl.constexpr,\n+):\n+    pid = tl.program_id(axis=0)\n+    block_start = pid * BLOCK_SIZE\n+    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+    mask = offsets < n_elements\n+    x = tl.load(x_ptr + offsets, mask=mask)\n+    x = tl.math.asin(x)\n+    tl.store(y_ptr + offsets, x, mask=mask)\n+\n+# %%\n+#  Using the default libdevice library path\n+# --------------------------\n+# We can use the default libdevice library path encoded in `triton/language/math.py`\n+\n+\n+torch.manual_seed(0)\n+size = 98432\n+x = torch.rand(size, device='cuda')\n+output_triton = torch.zeros(size, device='cuda')\n+output_torch = torch.asin(x)\n+assert x.is_cuda and output_triton.is_cuda\n+n_elements = output_torch.numel()\n+grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+asin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024)\n+print(output_torch)\n+print(output_triton)\n+print(\n+    f'The maximum difference between torch and triton is '\n+    f'{torch.max(torch.abs(output_torch - output_triton))}'\n+)\n+\n+# %%\n+#  Customize the libdevice library path\n+# --------------------------\n+# We can also customize the libdevice library path by passing the path to the `libdevice` library to the `asin` kernel.\n+\n+output_triton = torch.empty_like(x)\n+asin_kernel[grid](x, output_triton, n_elements, BLOCK_SIZE=1024,\n+                  extern_libs={'libdevice': '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'})\n+print(output_torch)\n+print(output_triton)\n+print(\n+    f'The maximum difference between torch and triton is '\n+    f'{torch.max(torch.abs(output_torch - output_triton))}'\n+)"}]