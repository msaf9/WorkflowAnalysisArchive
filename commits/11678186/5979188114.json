[{"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -190,6 +190,12 @@ static SmallVector<Value> reorderValues(const SmallVector<Value> &values,\n   assert(inEncoding == ouEncoding);\n   if (!inEncoding)\n     return values;\n+  // If the parent of the dot operand is in block encoding, we don't need to\n+  // reorder elements\n+  auto parentEncoding =\n+      dyn_cast<triton::gpu::MmaEncodingAttr>(ouEncoding.getParent());\n+  if (!parentEncoding)\n+    return values;\n   size_t inBitWidth = inTensorTy.getElementType().getIntOrFloatBitWidth();\n   size_t ouBitWidth = ouTensorTy.getElementType().getIntOrFloatBitWidth();\n   auto ouEltTy = ouTensorTy.getElementType();"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 53, "deletions": 40, "changes": 93, "file_content_changes": "@@ -26,61 +26,61 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n \n \n @pytest.mark.parametrize(\n-    \"BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, ADTYPE, BDTYPE\",\n+    \"BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, ADTYPE, BDTYPE, ALLOW_TF32\",\n     itertools.chain(\n         *[\n             [\n                 # 1 warp\n-                (16, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (16, 32, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (16, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (16, 32, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (16, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (64, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (16, 64, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (16, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (16, 32, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (16, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (16, 32, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (16, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (64, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (16, 64, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n                 # 2 warp\n-                (64, 32, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 64, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (64, 32, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 64, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (128, 32, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 128, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (64, 32, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 64, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (64, 32, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 64, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (128, 32, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 128, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n                 # 4 warp\n-                (128, 64, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (64, 128, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (128, 32, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 128, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (128, 32, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 128, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (128, 64, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (64, 128, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (128, 32, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 128, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (128, 32, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 128, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n                 # 8 warp\n-                (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (256, 128, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (256, 128, 32, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (256, 128, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (256, 128, 32, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n                 # variable input\n-                (128, 128, 32, 1, 4, 2, 256, 384, 160, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 107, 233, 128, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 107, 233, 83, AT, BT, DTYPE, DTYPE),\n-                (128, 256, 64, 1, 8, 3, 256, 512, 160, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 256, 384, 160, AT, BT, DTYPE, DTYPE, True),\n+                (128, 128, 32, 1, 4, 2, 107, 233, 128, AT, BT, DTYPE, DTYPE, True),\n+                (128, 128, 32, 1, 4, 2, 107, 233, 83, AT, BT, DTYPE, DTYPE, True),\n+                (128, 256, 64, 1, 8, 3, 256, 512, 160, AT, BT, DTYPE, DTYPE, True),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True]\n         ],\n         # n-stage\n         *[\n             [\n-                (16, 16, 16, 1, 1, STAGES, 32, 32, 80, AT, BT, DTYPE, DTYPE),\n-                (64, 32, 64, 1, 2, STAGES, 128, 64, 128, AT, BT, DTYPE, DTYPE),\n-                (128, 64, 16, 1, 4, STAGES, 256, 128, 80, AT, BT, DTYPE, DTYPE),\n-                (256, 128, 32, 1, 8, STAGES, 512, 256, 160, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, STAGES, 256, 256, 160, AT, BT, DTYPE, DTYPE),\n+                (16, 16, 16, 1, 1, STAGES, 32, 32, 80, AT, BT, DTYPE, DTYPE, True),\n+                (64, 32, 64, 1, 2, STAGES, 128, 64, 128, AT, BT, DTYPE, DTYPE, True),\n+                (128, 64, 16, 1, 4, STAGES, 256, 128, 80, AT, BT, DTYPE, DTYPE, True),\n+                (256, 128, 32, 1, 8, STAGES, 512, 256, 160, AT, BT, DTYPE, DTYPE, True),\n+                (128, 128, 32, 1, 4, STAGES, 256, 256, 160, AT, BT, DTYPE, DTYPE, True),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True] for STAGES in [4]\n         ],\n         # mixed-precision\n         *[\n             [\n-                (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n-                (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n-                (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n+                (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE, True),\n+                (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE, True),\n+                (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE, True),\n             ] for ADTYPE, BDTYPE in [(\"float8e4nv\", \"float8e5\"),\n                                      (\"float8e4nv\", \"float8e4nv\"),\n                                      (\"float8e5\", \"float8e4nv\"),\n@@ -92,10 +92,23 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                                      (\"bfloat16\", \"float32\"),\n                                      (\"float32\", \"bfloat16\")] for AT in [False, True] for BT in [False, True]\n         ],\n+        # mixed-precision block layout\n+        *[\n+            [\n+                (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE, False),\n+                (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE, False),\n+                (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE, False),\n+            ] for ADTYPE, BDTYPE in [(\"float8e4nv\", \"float16\"),\n+                                     (\"float16\", \"float8e5\"),\n+                                     (\"float16\", \"float32\"),\n+                                     (\"float32\", \"float16\"),\n+                                     (\"bfloat16\", \"float32\"),\n+                                     (\"float32\", \"bfloat16\")] for AT in [False, True] for BT in [False, True]\n+        ],\n         *[\n             # float8e4b15 only supports row-col layout\n             [\n-                (128, 128, 32, 1, 4, 2, None, None, None, False, True, ADTYPE, BDTYPE),\n+                (128, 128, 32, 1, 4, 2, None, None, None, False, True, ADTYPE, BDTYPE, True),\n             ] for ADTYPE, BDTYPE in [(\"float8e4b15\", \"float8e5\"),\n                                      (\"float8e4b15\", \"float16\"),\n                                      (\"float16\", \"float8e4b15\"),\n@@ -105,7 +118,7 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n         ]\n     ),\n )\n-def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, ADTYPE, BDTYPE):\n+def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, ADTYPE, BDTYPE, ALLOW_TF32):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n@@ -173,7 +186,7 @@ def init_input(m, n, dtype):\n             a = triton.reinterpret(a, getattr(tl, ADTYPE))\n         if b_fp8:\n             b = triton.reinterpret(b, getattr(tl, BDTYPE))\n-        tt_c = triton.ops.matmul(a, b)\n+        tt_c = triton.ops.matmul(a, b, None, ALLOW_TF32)\n         torch.testing.assert_allclose(th_c, tt_c, atol=0, rtol=0)\n     except triton.OutOfResources as e:\n         pytest.skip(str(e))"}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -81,6 +81,7 @@ def _kernel(A, B, C, M, N, K,\n             stride_bk, stride_bn,\n             stride_cm, stride_cn,\n             dot_out_dtype: tl.constexpr,\n+            allow_tf32: tl.constexpr,\n             BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n             GROUP_M: tl.constexpr, SPLIT_K: tl.constexpr, EVEN_K: tl.constexpr, AB_DTYPE: tl.constexpr\n             ):\n@@ -117,7 +118,7 @@ def _kernel(A, B, C, M, N, K,\n         if AB_DTYPE:\n             a = a.to(C.dtype.element_ty)\n             b = b.to(C.dtype.element_ty)\n-        acc += tl.dot(a, b, out_dtype=dot_out_dtype)\n+        acc += tl.dot(a, b, out_dtype=dot_out_dtype, allow_tf32=allow_tf32)\n         A += BLOCK_K * SPLIT_K * stride_ak\n         B += BLOCK_K * SPLIT_K * stride_bk\n     acc = acc.to(C.dtype.element_ty)\n@@ -139,7 +140,7 @@ class _matmul(torch.autograd.Function):\n     _locks = {}\n \n     @staticmethod\n-    def _call(a, b, dot_out_dtype):\n+    def _call(a, b, dot_out_dtype, allow_tf32):\n         device = a.device\n         # handle non-contiguous inputs if necessary\n         if a.stride(0) > 1 and a.stride(1) > 1:\n@@ -180,12 +181,13 @@ def _call(a, b, dot_out_dtype):\n                       b.stride(0), b.stride(1),\n                       c.stride(0), c.stride(1),\n                       dot_out_dtype=dot_out_dtype,\n+                      allow_tf32=allow_tf32,\n                       GROUP_M=8, AB_DTYPE=ab_dtype)\n         return c\n \n     @staticmethod\n-    def forward(ctx, a, b, dot_out_dtype=None):\n-        return _matmul._call(a, b, dot_out_dtype=dot_out_dtype)\n+    def forward(ctx, a, b, dot_out_dtype=None, allow_tf32=True):\n+        return _matmul._call(a, b, dot_out_dtype=dot_out_dtype, allow_tf32=allow_tf32)\n \n \n matmul = _matmul.apply"}]