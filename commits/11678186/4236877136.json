[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 11, "deletions": 5, "changes": 16, "file_content_changes": "@@ -6,6 +6,8 @@ on:\n     branches:\n       - main\n       - triton-mlir\n+  merge_group:\n+    types: [checks_requested]\n \n concurrency:\n   group: ${{ github.ref }}\n@@ -21,7 +23,7 @@ jobs:\n         id: set-matrix\n         run: |\n           if [ x\"${{ github.repository }}\" == x\"openai/triton\" ]; then\n-            echo '::set-output name=matrix::[[\"self-hosted\", \"A10\"], [\"self-hosted\", \"V100\"], \"macos-10.15\"]'\n+            echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"V100\"], \"macos-10.15\"]'\n           else\n             echo '::set-output name=matrix::[\"ubuntu-latest\", \"macos-10.15\"]'\n           fi\n@@ -43,29 +45,33 @@ jobs:\n         run: |\n           rm -rf ~/.triton/cache/\n \n+      - name: Update path\n+        run: |\n+          echo \"$HOME/.local/bin/\" >> $GITHUB_PATH\n+\n       - name: Check imports\n         if: ${{ matrix.runner != 'macos-10.15' }}\n         run: |\n-          pip install isort\n+          pip3 install isort\n           isort -c ./python || ( echo '::error title=Imports not sorted::Please run \\\"isort ./python\\\"' ; exit 1 )\n \n       - name: Check python style\n         if: ${{ matrix.runner != 'macos-10.15' }}\n         run: |\n-          pip install autopep8\n+          pip3 install autopep8\n           autopep8 -a -r -d --exit-code ./python || ( echo '::error title=Style issues::Please run \\\"autopep8 -a -r -i ./python\\\"' ; exit 1 )\n \n       - name: Check cpp style\n         if: ${{ matrix.runner != 'macos-10.15' }}\n         run: |\n-          pip install clang-format\n+          pip3 install clang-format\n           find . -regex '.*\\.\\(cpp\\|hpp\\|h\\|cc\\)' -not -path \"./python/triton/*\" -not -path \"./python/build/*\" -not -path \"./include/triton/external/*\" -print0 | xargs -0 -n1 clang-format -style=file --dry-run -Werror -i ||\n           (echo '::error title=Style issues:: Please run `find . -regex \".*\\.\\(cpp\\|hpp\\|h\\|cc\\)\" -not -path \"./python/triton/*\" -not -path \"./python/build/*\" -not -path \"./include/triton/external/*\" -print0 | xargs -0 -n1 clang-format -style=file -i`' ; exit 1)\n \n       - name: Flake8\n         if: ${{ matrix.runner != 'macos-10.15' }}\n         run: |\n-          pip install flake8\n+          pip3 install flake8\n           flake8 --config ./python/setup.cfg ./python || ( echo '::error::Flake8 failed; see logs for errors.' ; exit 1 )\n \n       - name: Install Triton"}, {"filename": "CMakeLists.txt", "status": "modified", "additions": 14, "deletions": 10, "changes": 24, "file_content_changes": "@@ -43,8 +43,8 @@ include_directories(${PYBIND11_INCLUDE_DIR})\n \n if(WIN32)\n     SET(BUILD_SHARED_LIBS OFF)\n-    include_directories(${CMAKE_CURRENT_SOURCE_DIR}/deps/dlfcn-win32/src)\n-    add_subdirectory(deps/dlfcn-win32/src ${CMAKE_BINARY_DIR}/dlfcn-win32)\n+    find_package(dlfcn-win32 REQUIRED)\n+    set(CMAKE_DL_LIBS dlfcn-win32::dl)\n endif()\n \n set(CMAKE_CXX_FLAGS \"${CMAKE_C_FLAGS} -D__STDC_FORMAT_MACROS  -fPIC -std=gnu++17 -fvisibility=hidden -fvisibility-inlines-hidden\")\n@@ -202,8 +202,7 @@ get_property(conversion_libs GLOBAL PROPERTY MLIR_CONVERSION_LIBS)\n \n if(TRITON_BUILD_PYTHON_MODULE)\n   add_library(triton SHARED ${PYTHON_SRC})\n-\n-  target_link_libraries(triton\n+  set(TRITON_LIBRARIES\n     TritonAnalysis\n     TritonTransforms\n     TritonGPUTransforms\n@@ -222,16 +221,21 @@ if(TRITON_BUILD_PYTHON_MODULE)\n     MLIRNVVMToLLVMIRTranslation\n     MLIRIR\n   )\n-\n-  target_link_options(triton PRIVATE ${LLVM_LDFLAGS})\n-\n   if(WIN32)\n-      target_link_libraries(triton PRIVATE ${LLVM_LIBRARIES} dl) # dl is from dlfcn-win32\n+    target_link_libraries(triton PRIVATE ${LLVM_LIBRARIES} ${CMAKE_DL_LIBS}\n+      ${TRITON_LIBRARIES}\n+    )\n   elseif(APPLE)\n-      target_link_libraries(triton ${LLVM_LIBRARIES} z)\n+    target_link_libraries(triton ${LLVM_LIBRARIES} z\n+      ${TRITON_LIBRARIES}\n+    )\n   else()\n-      target_link_libraries(triton ${LLVM_LIBRARIES} z stdc++fs)\n+    target_link_libraries(triton ${LLVM_LIBRARIES} z stdc++fs\n+      ${TRITON_LIBRARIES}\n+    )\n   endif()\n+  \n+  target_link_options(triton PRIVATE ${LLVM_LDFLAGS})\n endif()\n \n if(TRITON_BUILD_PYTHON_MODULE AND NOT WIN32)"}, {"filename": "bin/triton-opt.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -34,7 +34,7 @@ int main(int argc, char **argv) {\n   mlir::DialectRegistry registry;\n   registry.insert<mlir::triton::TritonDialect,\n                   mlir::triton::gpu::TritonGPUDialect, mlir::func::FuncDialect,\n-                  mlir::math::MathDialect, mlir::arith::ArithmeticDialect,\n+                  mlir::math::MathDialect, mlir::arith::ArithDialect,\n                   mlir::scf::SCFDialect, mlir::gpu::GPUDialect>();\n \n   return mlir::asMainReturnCode(mlir::MlirOptMain("}, {"filename": "bin/triton-translate.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -36,9 +36,9 @@ OwningOpRef<ModuleOp> loadMLIRModule(llvm::StringRef inputFilename,\n   }\n \n   mlir::DialectRegistry registry;\n-  registry.insert<TritonDialect, triton::gpu::TritonGPUDialect,\n-                  mlir::math::MathDialect, arith::ArithmeticDialect,\n-                  scf::SCFDialect>();\n+  registry\n+      .insert<TritonDialect, triton::gpu::TritonGPUDialect,\n+              mlir::math::MathDialect, arith::ArithDialect, scf::SCFDialect>();\n \n   context.appendDialectRegistry(registry);\n "}, {"filename": "include/triton/Analysis/Alias.h", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -77,6 +77,12 @@ class SharedMemoryAliasAnalysis\n   /// Returns the modify-reference behavior of `op` on `location`.\n   ModRefResult getModRef(Operation *op, Value location);\n \n+  void setToEntryState(dataflow::Lattice<AliasInfo> *lattice) override {\n+    propagateIfChanged(\n+        lattice, lattice->join(\n+                     AliasInfo::getPessimisticValueState(lattice->getPoint())));\n+  }\n+\n   /// Computes if the alloc set of the results are changed.\n   void\n   visitOperation(Operation *op,"}, {"filename": "include/triton/Analysis/AxisInfo.h", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -267,6 +267,12 @@ class AxisInfoAnalysis\n private:\n   AxisInfoVisitorList visitors;\n \n+  void setToEntryState(dataflow::Lattice<AxisInfo> *lattice) override {\n+    propagateIfChanged(\n+        lattice,\n+        lattice->join(AxisInfo::getPessimisticValueState(lattice->getPoint())));\n+  }\n+\n public:\n   AxisInfoAnalysis(DataFlowSolver &solver);\n   using dataflow::SparseDataFlowAnalysis<"}, {"filename": "include/triton/Analysis/Membar.h", "status": "modified", "additions": 26, "deletions": 17, "changes": 43, "file_content_changes": "@@ -36,28 +36,29 @@ class MembarAnalysis {\n   void run();\n \n private:\n-  struct RegionInfo {\n+  struct BlockInfo {\n     using BufferIdSetT = Allocation::BufferIdSetT;\n \n     BufferIdSetT syncReadBuffers;\n     BufferIdSetT syncWriteBuffers;\n \n-    RegionInfo() = default;\n-    RegionInfo(const BufferIdSetT &syncReadBuffers,\n-               const BufferIdSetT &syncWriteBuffers)\n+    BlockInfo() = default;\n+    BlockInfo(const BufferIdSetT &syncReadBuffers,\n+              const BufferIdSetT &syncWriteBuffers)\n         : syncReadBuffers(syncReadBuffers), syncWriteBuffers(syncWriteBuffers) {\n     }\n \n-    /// Unions two RegionInfo objects.\n-    void join(const RegionInfo &other) {\n+    /// Unions two BlockInfo objects.\n+    BlockInfo &join(const BlockInfo &other) {\n       syncReadBuffers.insert(other.syncReadBuffers.begin(),\n                              other.syncReadBuffers.end());\n       syncWriteBuffers.insert(other.syncWriteBuffers.begin(),\n                               other.syncWriteBuffers.end());\n+      return *this;\n     }\n \n-    /// Returns true if buffers in two RegionInfo objects are intersected.\n-    bool isIntersected(const RegionInfo &other, Allocation *allocation) const {\n+    /// Returns true if buffers in two BlockInfo objects are intersected.\n+    bool isIntersected(const BlockInfo &other, Allocation *allocation) const {\n       return /*RAW*/ isIntersected(syncWriteBuffers, other.syncReadBuffers,\n                                    allocation) ||\n              /*WAR*/\n@@ -74,6 +75,14 @@ class MembarAnalysis {\n       syncWriteBuffers.clear();\n     }\n \n+    /// Compares two BlockInfo objects.\n+    bool operator==(const BlockInfo &other) const {\n+      return syncReadBuffers == other.syncReadBuffers &&\n+             syncWriteBuffers == other.syncWriteBuffers;\n+    }\n+\n+    bool operator!=(const BlockInfo &other) const { return !(*this == other); }\n+\n   private:\n     /// Returns true if buffers in two sets are intersected.\n     bool isIntersected(const BufferIdSetT &lhs, const BufferIdSetT &rhs,\n@@ -99,19 +108,19 @@ class MembarAnalysis {\n   ///        op5\n   ///        op6\n   ///   op7\n-  /// region2 and region3 started with the information of region1.\n-  /// Each region is analyzed separately and keeps their own copy of the\n-  /// information. At op7, we union the information of the region2 and region3\n-  /// and update the information of region1.\n-  void dfsOperation(Operation *operation, RegionInfo *blockInfo,\n-                    OpBuilder *builder);\n+  /// TODO: Explain why we don't use ForwardAnalysis:\n+  void resolve(Operation *operation, OpBuilder *builder);\n+\n+  /// Updates the BlockInfo operation based on the operation.\n+  void update(Operation *operation, BlockInfo *blockInfo, OpBuilder *builder);\n \n-  /// Updates the RegionInfo operation based on the operation.\n-  void transfer(Operation *operation, RegionInfo *blockInfo,\n-                OpBuilder *builder);\n+  /// Collects the successors of the terminator\n+  void visitTerminator(Operation *operation, SmallVector<Block *> &successors);\n \n private:\n   Allocation *allocation;\n+  DenseMap<Block *, BlockInfo> inputBlockInfoMap;\n+  DenseMap<Block *, BlockInfo> outputBlockInfoMap;\n };\n \n } // namespace mlir"}, {"filename": "include/triton/Conversion/Passes.td", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -10,7 +10,7 @@ def ConvertTritonToTritonGPU: Pass<\"convert-triton-to-tritongpu\", \"mlir::ModuleO\n     }];\n     let constructor = \"mlir::triton::createConvertTritonToTritonGPUPass()\";\n \n-    let dependentDialects = [\"mlir::arith::ArithmeticDialect\",\n+    let dependentDialects = [\"mlir::arith::ArithDialect\",\n                              \"mlir::math::MathDialect\",\n                              // TODO: Does this pass depend on SCF?\n                              \"mlir::scf::SCFDialect\",\n@@ -32,7 +32,7 @@ def ConvertTritonGPUToLLVM : Pass<\"convert-triton-gpu-to-llvm\", \"mlir::ModuleOp\"\n     }];\n     let constructor = \"mlir::triton::createConvertTritonGPUToLLVMPass()\";\n \n-    let dependentDialects = [\"mlir::arith::ArithmeticDialect\",\n+    let dependentDialects = [\"mlir::arith::ArithDialect\",\n                              \"mlir::math::MathDialect\",\n                              \"mlir::gpu::GPUDialect\",\n                              \"mlir::scf::SCFDialect\","}, {"filename": "include/triton/Dialect/Triton/IR/Dialect.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,7 +1,7 @@\n #ifndef TRITON_DIALECT_TRITON_IR_DIALECT_H_\n #define TRITON_DIALECT_TRITON_IR_DIALECT_H_\n \n-#include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/ControlFlow/IR/ControlFlow.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/Dialect/Math/IR/Math.h\""}, {"filename": "include/triton/Dialect/Triton/IR/TritonDialect.td", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -14,7 +14,7 @@ def Triton_Dialect : Dialect {\n     Triton Dialect.\n \n     Dependent Dialects:\n-      * Arithmetic:\n+      * Arith:\n         * addf, addi, andi, cmpf, cmpi, divf, fptosi, ...\n       * Math:\n         * exp, sin, cos, log, ...\n@@ -23,7 +23,7 @@ def Triton_Dialect : Dialect {\n   }];\n \n   let dependentDialects = [\n-    \"arith::ArithmeticDialect\",\n+    \"arith::ArithDialect\",\n     \"math::MathDialect\",\n     \"scf::SCFDialect\",\n     \"cf::ControlFlowDialect\","}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 22, "deletions": 20, "changes": 42, "file_content_changes": "@@ -6,10 +6,10 @@ include \"triton/Dialect/Triton/IR/TritonTypes.td\"\n include \"triton/Dialect/Triton/IR/TritonAttrDefs.td\"\n include \"triton/Dialect/Triton/IR/TritonInterfaces.td\"\n include \"mlir/IR/OpBase.td\"\n-include \"mlir/Interfaces/SideEffectInterfaces.td\" // NoSideEffect\n+include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n include \"mlir/Interfaces/ControlFlowInterfaces.td\" // BranchOpInterface\n include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n-include \"mlir/Interfaces/SideEffectInterfaces.td\" // NoSideEffect\n+include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n include \"mlir/Interfaces/CastInterfaces.td\" // CastOpInterface\n \n //\n@@ -29,7 +29,7 @@ class TT_Op<string mnemonic, list<Trait> traits = []> :\n //   extui, extsi, tructi\n def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [SameOperandsAndResultShape,\n                                          SameOperandsAndResultEncoding,\n-                                         NoSideEffect,\n+                                         Pure,\n                                          /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n     let summary = \"Cast int64 to pointer\";\n \n@@ -42,7 +42,7 @@ def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [SameOperandsAndResultShape,\n \n def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape,\n                                          SameOperandsAndResultEncoding,\n-                                         NoSideEffect,\n+                                         Pure,\n                                          /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n     let summary = \"Cast pointer to int64\";\n \n@@ -56,7 +56,7 @@ def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape,\n // arith.bitcast doesn't support pointers\n def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape,\n                                      SameOperandsAndResultEncoding,\n-                                     NoSideEffect,\n+                                     Pure,\n                                      /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n     let summary = \"Cast between types of the same bitwidth\";\n \n@@ -71,7 +71,7 @@ def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape,\n \n def TT_FpToFpOp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n                                      SameOperandsAndResultEncoding,\n-                                     NoSideEffect,\n+                                     Pure,\n                                      DeclareOpInterfaceMethods<CastOpInterface>]> {\n     let summary = \"Floating point casting for custom types\";\n \n@@ -95,7 +95,7 @@ def TT_FpToFpOp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n //\n \n def TT_AddPtrOp : TT_Op<\"addptr\",\n-                     [NoSideEffect,\n+                     [Pure,\n                       SameOperandsAndResultShape,\n                       SameOperandsAndResultEncoding,\n                       TypesMatchWith<\"result type matches ptr type\",\n@@ -158,7 +158,7 @@ def TT_StoreOp : TT_Op<\"store\",\n     let summary = \"store\";\n \n     let arguments = (ins TT_PtrLike:$ptr, TT_Type:$value, Optional<TT_BoolLike>:$mask,\n-                     DefaultValuedAttr<TT_CacheModifierAttr, \"triton::CacheModifier::NONE\">:$cache, \n+                     DefaultValuedAttr<TT_CacheModifierAttr, \"triton::CacheModifier::NONE\">:$cache,\n                      DefaultValuedAttr<TT_EvictionPolicyAttr, \"triton::EvictionPolicy::NORMAL\">:$evict);\n \n     let builders = [\n@@ -222,7 +222,7 @@ def TT_AtomicCASOp : TT_Op<\"atomic_cas\", [MemoryEffects<[MemRead]>,\n //\n // Shape Manipulation Ops\n //\n-def TT_SplatOp : TT_Op<\"splat\", [NoSideEffect,\n+def TT_SplatOp : TT_Op<\"splat\", [Pure,\n                                  SameOperandsAndResultElementType,\n                                  SameOperandsAndResultEncoding]> {\n     let summary = \"splat\";\n@@ -236,7 +236,7 @@ def TT_SplatOp : TT_Op<\"splat\", [NoSideEffect,\n     let hasFolder = 1;\n }\n \n-def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [NoSideEffect,\n+def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [Pure,\n                                             DeclareOpInterfaceMethods<InferTypeOpInterface>,\n                                             SameOperandsAndResultElementType]> {\n     let summary = \"expand_dims\";\n@@ -248,7 +248,8 @@ def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [NoSideEffect,\n     let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n }\n \n-def TT_ViewOp : TT_Op<\"view\", [NoSideEffect,\n+// view is not `pure` because it may reorder elements\n+def TT_ViewOp : TT_Op<\"view\", [NoMemoryEffect,\n                                SameOperandsAndResultElementType]> {\n     let summary = \"view\";\n \n@@ -260,7 +261,7 @@ def TT_ViewOp : TT_Op<\"view\", [NoSideEffect,\n \n }\n \n-def TT_BroadcastOp : TT_Op<\"broadcast\", [NoSideEffect,\n+def TT_BroadcastOp : TT_Op<\"broadcast\", [Pure,\n                                          SameOperandsAndResultElementType,\n                                          SameOperandsAndResultEncoding]> {\n     let summary = \"broadcast. No left-padding as of now.\";\n@@ -274,7 +275,8 @@ def TT_BroadcastOp : TT_Op<\"broadcast\", [NoSideEffect,\n     let hasFolder = 1;\n }\n \n-def TT_CatOp : TT_Op<\"cat\", [NoSideEffect,\n+// cat is not `pure` because it may reorder elements\n+def TT_CatOp : TT_Op<\"cat\", [NoMemoryEffect,\n                              SameOperandsAndResultElementType]> {\n     let summary = \"concatenate 2 tensors\";\n \n@@ -285,7 +287,7 @@ def TT_CatOp : TT_Op<\"cat\", [NoSideEffect,\n     let assemblyFormat = \"$lhs `,` $rhs attr-dict `:` functional-type(operands, results)\";\n }\n \n-def TT_TransOp : TT_Op<\"trans\", [NoSideEffect,\n+def TT_TransOp : TT_Op<\"trans\", [Pure,\n                                  DeclareOpInterfaceMethods<InferTypeOpInterface>,\n                                  SameOperandsAndResultElementType]> {\n \n@@ -301,15 +303,15 @@ def TT_TransOp : TT_Op<\"trans\", [NoSideEffect,\n //\n // SPMD Ops\n //\n-def TT_GetProgramIdOp : TT_Op<\"get_program_id\", [NoSideEffect]> {\n+def TT_GetProgramIdOp : TT_Op<\"get_program_id\", [Pure]> {\n     let arguments = (ins I32Attr:$axis);\n \n     let results = (outs I32:$result);\n \n     let assemblyFormat = \"attr-dict `:` type($result)\";\n }\n \n-def TT_GetNumProgramsOp : TT_Op<\"get_num_programs\", [NoSideEffect]> {\n+def TT_GetNumProgramsOp : TT_Op<\"get_num_programs\", [Pure]> {\n     let arguments = (ins I32Attr:$axis);\n \n     let results = (outs I32:$result);\n@@ -320,7 +322,7 @@ def TT_GetNumProgramsOp : TT_Op<\"get_num_programs\", [NoSideEffect]> {\n //\n // Dot Op\n //\n-def TT_DotOp : TT_Op<\"dot\", [NoSideEffect,\n+def TT_DotOp : TT_Op<\"dot\", [Pure,\n                              DeclareOpInterfaceMethods<InferTypeOpInterface>,\n                              TypesMatchWith<\"result's type matches accumulator's type\",\n                                             \"d\", \"c\", \"$_self\">]> {\n@@ -340,7 +342,7 @@ def TT_DotOp : TT_Op<\"dot\", [NoSideEffect,\n //\n // Reduce Op\n //\n-def TT_ReduceOp : TT_Op<\"reduce\", [NoSideEffect,\n+def TT_ReduceOp : TT_Op<\"reduce\", [Pure,\n                                    DeclareOpInterfaceMethods<InferTypeOpInterface>]> {\n     let summary = \"reduce\";\n \n@@ -364,7 +366,7 @@ def TT_ReduceOp : TT_Op<\"reduce\", [NoSideEffect,\n //\n // External elementwise op\n //\n-def TT_ExtElemwiseOp : TT_Op<\"ext_elemwise\", [NoSideEffect, Elementwise, SameOperandsAndResultShape,\n+def TT_ExtElemwiseOp : TT_Op<\"ext_elemwise\", [Pure, Elementwise, SameOperandsAndResultShape,\n                                               SameOperandsAndResultEncoding,\n                                               SameVariadicOperandSize]> {\n     let summary = \"ext_elemwise\";\n@@ -386,7 +388,7 @@ def TT_ExtElemwiseOp : TT_Op<\"ext_elemwise\", [NoSideEffect, Elementwise, SameOpe\n // Make Range Op\n //\n // TODO: should have ConstantLike as Trait\n-def TT_MakeRangeOp : TT_Op<\"make_range\", [NoSideEffect]> {\n+def TT_MakeRangeOp : TT_Op<\"make_range\", [Pure]> {\n     let summary = \"make range\";\n \n     let description = [{"}, {"filename": "include/triton/Dialect/Triton/Transforms/Passes.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -16,7 +16,7 @@ def TritonCombineOps : Pass</*cli-arg*/\"triton-combine\", /*Op*/\"mlir::ModuleOp\">\n \n   let constructor = \"mlir::triton::createCombineOpsPass()\";\n \n-  let dependentDialects = [\"mlir::arith::ArithmeticDialect\"];\n+  let dependentDialects = [\"mlir::arith::ArithDialect\"];\n }\n \n #endif"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -3,11 +3,11 @@\n \n include \"triton/Dialect/TritonGPU/IR/TritonGPUDialect.td\"\n include \"triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td\"\n-include \"mlir/Dialect/Arithmetic/IR/ArithmeticBase.td\"\n+include \"mlir/Dialect/Arith/IR/ArithBase.td\"\n include \"triton/Dialect/Triton/IR/TritonTypes.td\"\n include \"triton/Dialect/Triton/IR/TritonAttrDefs.td\"\n include \"mlir/IR/OpBase.td\"\n-include \"mlir/Interfaces/SideEffectInterfaces.td\" // NoSideEffect\n+include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n \n def ResultsAreSharedEncoding: NativeOpTrait<\"ResultsAreSharedEncoding\">;\n@@ -18,7 +18,7 @@ class TTG_Op<string mnemonic, list<Trait> traits = []> :\n def TTG_ConvertLayoutOp : TTG_Op<\"convert_layout\",\n                                  [SameOperandsAndResultShape,\n                                   SameOperandsAndResultElementType,\n-                                  NoSideEffect]> {\n+                                  Pure]> {\n   let summary = \"convert layout\";\n \n   let arguments = (ins TT_Tensor:$src);\n@@ -59,7 +59,7 @@ def TTG_AsyncCommitGroupOp : TTG_Op<\"async_commit_group\"> {\n // This is needed because these ops don't\n // handle encodings\n // e.g., https://github.com/llvm/llvm-project/blob/main/mlir/include/mlir/Dialect/Arith/IR/ArithOps.td#L111\n-def TTG_CmpIOp : TTG_Op<\"cmpi\", [NoSideEffect, Elementwise,\n+def TTG_CmpIOp : TTG_Op<\"cmpi\", [Pure, Elementwise,\n                                  SameOperandsAndResultShape, \n                                  SameOperandsAndResultEncoding]> {\n   let summary = \"integer comparison operation\";\n@@ -73,7 +73,7 @@ def TTG_CmpIOp : TTG_Op<\"cmpi\", [NoSideEffect, Elementwise,\n   let results = (outs TT_BoolLike:$result);\n }\n \n-def TTG_CmpFOp : TTG_Op<\"cmpf\", [NoSideEffect, Elementwise,\n+def TTG_CmpFOp : TTG_Op<\"cmpf\", [Pure, Elementwise,\n                                  SameOperandsAndResultShape, \n                                  SameOperandsAndResultEncoding]> {\n   let summary = \"floating-point comparison operation\";\n@@ -88,7 +88,7 @@ def TTG_CmpFOp : TTG_Op<\"cmpf\", [NoSideEffect, Elementwise,\n }\n \n // TODO: migrate to arith::SelectOp on LLVM16\n-def TTG_SelectOp : TTG_Op<\"select\", [NoSideEffect, Elementwise,\n+def TTG_SelectOp : TTG_Op<\"select\", [Pure, Elementwise,\n                                      SameOperandsAndResultShape,\n                                      SameOperandsAndResultEncoding]> {\n   let summary = \"select operation\";"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.td", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -14,7 +14,7 @@ def TritonGPUPipeline : Pass<\"tritongpu-pipeline\", \"mlir::ModuleOp\"> {\n \n   let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n                            \"mlir::scf::SCFDialect\",\n-                           \"mlir::arith::ArithmeticDialect\"];\n+                           \"mlir::arith::ArithDialect\"];\n \n   let options = [\n     Option<\"numStages\", \"num-stages\",\n@@ -34,7 +34,7 @@ def TritonGPUPrefetch : Pass<\"tritongpu-prefetch\", \"mlir::ModuleOp\"> {\n \n   let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n                            \"mlir::scf::SCFDialect\",\n-                           \"mlir::arith::ArithmeticDialect\"];\n+                           \"mlir::arith::ArithDialect\"];\n }\n \n def TritonGPUCoalesce: Pass<\"tritongpu-coalesce\", \"mlir::ModuleOp\"> {"}, {"filename": "lib/Analysis/Alias.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -45,7 +45,7 @@ void SharedMemoryAliasAnalysis::visitOperation(\n   }\n \n   if (pessimistic) {\n-    return markAllPessimisticFixpoint(results);\n+    return setAllToEntryStates(results);\n   }\n   // Join all lattice elements\n   for (auto *result : results)"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 10, "deletions": 8, "changes": 18, "file_content_changes": "@@ -53,8 +53,8 @@ getCvtOrder(const Attribute &srcLayout, const Attribute &dstLayout) {\n SmallVector<unsigned>\n getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n                              unsigned &outVec) {\n-  auto srcTy = op.src().getType().cast<RankedTensorType>();\n-  auto dstTy = op.result().getType().cast<RankedTensorType>();\n+  auto srcTy = op.getSrc().getType().cast<RankedTensorType>();\n+  auto dstTy = op.getResult().getType().cast<RankedTensorType>();\n   Attribute srcLayout = srcTy.getEncoding();\n   Attribute dstLayout = dstTy.getEncoding();\n \n@@ -100,7 +100,7 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n // TODO: extend beyond scalars\n SmallVector<unsigned> getScratchConfigForAtomicRMW(triton::AtomicRMWOp op) {\n   SmallVector<unsigned> smemShape;\n-  if (op.ptr().getType().isa<RankedTensorType>()) {\n+  if (op.getPtr().getType().isa<RankedTensorType>()) {\n     // do nothing or just assert because shared memory is not used in tensor up\n     // to now\n   } else {\n@@ -167,8 +167,8 @@ class AllocationAnalysis {\n       unsigned bytes = helper.getScratchSizeInBytes();\n       allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     } else if (auto cvtLayout = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n-      auto srcTy = cvtLayout.src().getType().cast<RankedTensorType>();\n-      auto dstTy = cvtLayout.result().getType().cast<RankedTensorType>();\n+      auto srcTy = cvtLayout.getSrc().getType().cast<RankedTensorType>();\n+      auto dstTy = cvtLayout.getResult().getType().cast<RankedTensorType>();\n       auto srcEncoding = srcTy.getEncoding();\n       auto dstEncoding = dstTy.getEncoding();\n       if (srcEncoding.isa<SharedEncodingAttr>() ||\n@@ -225,10 +225,12 @@ class AllocationAnalysis {\n   void getValueAlias(Value value, SharedMemoryAliasAnalysis &analysis) {\n     dataflow::Lattice<AliasInfo> *latticeElement =\n         analysis.getLatticeElement(value);\n-    if (latticeElement && !latticeElement->isUninitialized()) {\n+    if (latticeElement) {\n       AliasInfo &info = latticeElement->getValue();\n-      for (auto alloc : info.getAllocs()) {\n-        allocation->addAlias(value, alloc);\n+      if (!info.getAllocs().empty()) {\n+        for (auto alloc : info.getAllocs()) {\n+          allocation->addAlias(value, alloc);\n+        }\n       }\n     }\n   }"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 14, "deletions": 9, "changes": 23, "file_content_changes": "@@ -111,6 +111,11 @@ AxisInfo AxisInfo::getPessimisticValueState(Value value) {\n \n // The gcd of both arguments for each dimension\n AxisInfo AxisInfo::join(const AxisInfo &lhs, const AxisInfo &rhs) {\n+  // If one argument is not initialized, return the other.\n+  if (lhs.getRank() == 0)\n+    return rhs;\n+  if (rhs.getRank() == 0)\n+    return lhs;\n   DimVectorT contiguity;\n   DimVectorT divisibility;\n   DimVectorT constancy;\n@@ -151,8 +156,8 @@ class MakeRangeOpAxisInfoVisitor final\n   AxisInfo\n   getAxisInfo(triton::MakeRangeOp op,\n               ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n-    auto start = op.start();\n-    auto end = op.end();\n+    auto start = op.getStart();\n+    auto end = op.getEnd();\n     return AxisInfo(/*contiguity=*/{end - start},\n                     /*divisibility=*/{highestPowOf2Divisor(start)},\n                     /*constancy=*/{1});\n@@ -450,9 +455,9 @@ class ExpandDimsOpAxisInfoVisitor final\n     AxisInfo::DimVectorT contiguity = opInfo.getContiguity();\n     AxisInfo::DimVectorT divisibility = opInfo.getDivisibility();\n     AxisInfo::DimVectorT constancy = opInfo.getConstancy();\n-    contiguity.insert(contiguity.begin() + op.axis(), 1);\n-    divisibility.insert(divisibility.begin() + op.axis(), 1);\n-    constancy.insert(constancy.begin() + op.axis(), 1);\n+    contiguity.insert(contiguity.begin() + op.getAxis(), 1);\n+    divisibility.insert(divisibility.begin() + op.getAxis(), 1);\n+    constancy.insert(constancy.begin() + op.getAxis(), 1);\n     return AxisInfo(contiguity, divisibility, constancy,\n                     operands[0]->getValue().getConstantValue());\n   }\n@@ -551,7 +556,7 @@ class CmpOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n \n private:\n   static arith::CmpIPredicate getPredicate(triton::gpu::CmpIOp op) {\n-    return op.predicate();\n+    return op.getPredicate();\n   }\n \n   static arith::CmpIPredicate getPredicate(arith::CmpIOp op) {\n@@ -843,7 +848,7 @@ void AxisInfoAnalysis::visitOperation(\n     ArrayRef<dataflow::Lattice<AxisInfo> *> results) {\n   AxisInfo curr = visitors.apply(op, operands);\n   if (curr.getRank() == 0) {\n-    return markAllPessimisticFixpoint(results);\n+    return setAllToEntryStates(results);\n   }\n   // override with hint\n   auto newContiguity = curr.getContiguity();\n@@ -892,7 +897,7 @@ unsigned AxisInfoAnalysis::getPtrAlignment(Value ptr) {\n   if (!tensorTy)\n     return 1;\n   dataflow::Lattice<AxisInfo> *latticeElement = getLatticeElement(ptr);\n-  if (!latticeElement || latticeElement->isUninitialized())\n+  if (!latticeElement)\n     return 1;\n   auto axisInfo = latticeElement->getValue();\n   auto layout = tensorTy.getEncoding();\n@@ -911,7 +916,7 @@ unsigned AxisInfoAnalysis::getMaskAlignment(Value mask) {\n   if (!tensorTy)\n     return 1;\n   dataflow::Lattice<AxisInfo> *latticeElement = getLatticeElement(mask);\n-  if (!latticeElement || latticeElement->isUninitialized())\n+  if (!latticeElement)\n     return 1;\n   auto maskAxis = latticeElement->getValue();\n   auto maskOrder = triton::gpu::getOrder(tensorTy.getEncoding());"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 76, "deletions": 66, "changes": 142, "file_content_changes": "@@ -2,109 +2,119 @@\n #include \"triton/Analysis/Alias.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n \n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/Dialect/GPU/IR/GPUDialect.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n+#include <deque>\n \n namespace mlir {\n \n void MembarAnalysis::run() {\n   auto *operation = allocation->getOperation();\n-  RegionInfo regionInfo;\n   OpBuilder builder(operation);\n-  dfsOperation(operation, &regionInfo, &builder);\n+  resolve(operation, &builder);\n }\n \n-void MembarAnalysis::dfsOperation(Operation *operation,\n-                                  RegionInfo *parentRegionInfo,\n-                                  OpBuilder *builder) {\n-  transfer(operation, parentRegionInfo, builder);\n-  if (operation->getNumRegions()) {\n-    // If there's any nested regions, we need to visit them.\n-    // scf.if and scf.else: two regions\n-    // scf.if only: two regions\n-    // scf.for: one region\n-    RegionInfo curRegionInfo;\n-    auto traverseRegions = [&]() -> auto{\n-      for (auto &region : operation->getRegions()) {\n-        // Copy the parent info as the current info.\n-        RegionInfo regionInfo = *parentRegionInfo;\n-        for (auto &block : region.getBlocks()) {\n-          // assert(region.getBlocks().size() == 1 &&\n-          //        \"Multiple blocks in a region is not supported\");\n-          for (auto &op : block.getOperations()) {\n-            // Traverse the nested operation.\n-            dfsOperation(&op, &regionInfo, builder);\n-          }\n-        }\n-        curRegionInfo.join(regionInfo);\n+void MembarAnalysis::resolve(Operation *operation, OpBuilder *builder) {\n+  // Initialize the blockList\n+  std::deque<Block *> blockList;\n+  operation->walk<WalkOrder::PreOrder>([&](Block *block) {\n+    for (auto &op : block->getOperations()) {\n+      // Check if the operation belongs to scf dialect, if so, we need to\n+      // throw an error\n+      if (op.getDialect()->getNamespace() == \"scf\") {\n+        op.emitError(\"scf dialect is not supported in membar. Please lower it \"\n+                     \"to cf dialect first.\");\n+        return;\n       }\n-      // Set the parent region info as the union of the nested region info.\n-      *parentRegionInfo = curRegionInfo;\n-    };\n+    }\n+    if (block->isEntryBlock())\n+      blockList.emplace_back(block);\n+  });\n+\n+  // A fixed point algorithm\n+  while (!blockList.empty()) {\n+    auto *block = blockList.front();\n+    blockList.pop_front();\n+    // Make a copy of the inputblockInfo but not update\n+    auto inputBlockInfo = inputBlockInfoMap.lookup(block);\n+    SmallVector<Block *> successors;\n+    for (auto &op : block->getOperations()) {\n+      if (op.hasTrait<OpTrait::IsTerminator>()) {\n+        visitTerminator(&op, successors);\n+      } else {\n+        update(&op, &inputBlockInfo, builder);\n+      }\n+    }\n+    // Get the reference because we want to update if it changed\n+    if (outputBlockInfoMap.count(block) &&\n+        inputBlockInfo == outputBlockInfoMap[block]) {\n+      // If we have seen the block before and the inputBlockInfo is the same as\n+      // the outputBlockInfo, we skip the successors\n+      continue;\n+    }\n+    // Update the current block\n+    outputBlockInfoMap[block].join(inputBlockInfo);\n+    // Update the successors\n+    for (auto *successor : successors) {\n+      inputBlockInfoMap[successor].join(outputBlockInfoMap[block]);\n+      blockList.emplace_back(successor);\n+    }\n+  }\n+}\n \n-    traverseRegions();\n-    if (isa<scf::ForOp>(operation)) {\n-      // scf.for can have two possible inputs: the init value and the\n-      // previous iteration's result. Although we've applied alias analysis,\n-      // there could be unsynced memory accesses on reused memories.\n-      // For example, consider the following code:\n-      // %1 = convert_layout %0: blocked -> shared\n-      // ...\n-      // gpu.barrier\n-      // ...\n-      // %5 = convert_layout %4 : shared -> dot\n-      // %6 = tt.dot %2, %5\n-      // scf.yield\n-      //\n-      // Though %5 could be released before scf.yield, it may shared the same\n-      // memory with %1. So we actually have to insert a barrier before %1 to\n-      // make sure the memory is synced.\n-      traverseRegions();\n+void MembarAnalysis::visitTerminator(Operation *op,\n+                                     SmallVector<Block *> &successors) {\n+  if (auto branchInterface = dyn_cast<BranchOpInterface>(op)) {\n+    Block *parentBlock = branchInterface->getBlock();\n+    for (Block *successor : parentBlock->getSuccessors()) {\n+      successors.push_back(successor);\n     }\n+    return;\n   }\n+  // Otherwise, it could be a return op\n+  assert(isa<func::ReturnOp>(op) && \"Unknown terminator\");\n }\n \n-void MembarAnalysis::transfer(Operation *op, RegionInfo *regionInfo,\n-                              OpBuilder *builder) {\n-  if (isa<scf::ForOp>(op) || isa<scf::IfOp>(op) || isa<scf::YieldOp>(op) ||\n-      isa<tensor::ExtractSliceOp>(op) || isa<triton::gpu::AllocTensorOp>(op)) {\n-    // Do not insert barriers before control flow operations and\n-    // alloc/extract/insert\n+void MembarAnalysis::update(Operation *op, BlockInfo *blockInfo,\n+                            OpBuilder *builder) {\n+  if (isa<tensor::ExtractSliceOp>(op) || isa<triton::gpu::AllocTensorOp>(op) ||\n+      isa<triton::TransOp>(op)) {\n     // alloc is an allocation op without memory write.\n     // FIXME(Keren): extract_slice is always alias for now\n     return;\n   }\n \n   if (isa<gpu::BarrierOp>(op)) {\n     // If the current op is a barrier, we sync previous reads and writes\n-    regionInfo->sync();\n+    blockInfo->sync();\n     return;\n   }\n \n   if (isa<triton::gpu::AsyncWaitOp>(op) &&\n       !isa<gpu::BarrierOp>(op->getNextNode())) {\n     // If the current op is an async wait and the next op is not a barrier we\n     // insert a barrier op and sync\n-    regionInfo->sync();\n+    blockInfo->sync();\n     OpBuilder::InsertionGuard g(*builder);\n     builder->setInsertionPointAfter(op);\n     builder->create<gpu::BarrierOp>(op->getLoc());\n-    regionInfo->sync();\n+    blockInfo->sync();\n     return;\n   }\n \n-  RegionInfo curRegionInfo;\n+  BlockInfo curBlockInfo;\n   for (Value value : op->getOperands()) {\n     for (auto bufferId : allocation->getBufferIds(value)) {\n       if (bufferId != Allocation::InvalidBufferId) {\n         if (isa<triton::gpu::InsertSliceAsyncOp>(op) ||\n             isa<tensor::InsertSliceOp>(op)) {\n-          // FIXME(Keren): insert_slice and insert_slice_async are always alias\n-          // for now\n-          curRegionInfo.syncWriteBuffers.insert(bufferId);\n+          // FIXME(Keren): insert_slice and insert_slice_async are always\n+          // alias for now\n+          curBlockInfo.syncWriteBuffers.insert(bufferId);\n         } else {\n           // ConvertLayoutOp: shared memory -> registers\n-          curRegionInfo.syncReadBuffers.insert(bufferId);\n+          curBlockInfo.syncReadBuffers.insert(bufferId);\n         }\n       }\n     }\n@@ -113,25 +123,25 @@ void MembarAnalysis::transfer(Operation *op, RegionInfo *regionInfo,\n     // ConvertLayoutOp: registers -> shared memory\n     auto bufferId = allocation->getBufferId(value);\n     if (bufferId != Allocation::InvalidBufferId) {\n-      curRegionInfo.syncWriteBuffers.insert(bufferId);\n+      curBlockInfo.syncWriteBuffers.insert(bufferId);\n     }\n   }\n   // Scratch buffer is considered as both shared memory write & read\n   auto bufferId = allocation->getBufferId(op);\n   if (bufferId != Allocation::InvalidBufferId) {\n-    curRegionInfo.syncWriteBuffers.insert(bufferId);\n-    curRegionInfo.syncReadBuffers.insert(bufferId);\n+    curBlockInfo.syncWriteBuffers.insert(bufferId);\n+    curBlockInfo.syncReadBuffers.insert(bufferId);\n   }\n \n-  if (regionInfo->isIntersected(curRegionInfo, allocation)) {\n+  if (blockInfo->isIntersected(curBlockInfo, allocation)) {\n     OpBuilder::InsertionGuard g(*builder);\n     builder->setInsertionPoint(op);\n     builder->create<gpu::BarrierOp>(op->getLoc());\n-    regionInfo->sync();\n+    blockInfo->sync();\n   }\n   // Update the region info, even if barrier is inserted, we have to maintain\n   // the current op's read/write buffers.\n-  regionInfo->join(curRegionInfo);\n+  blockInfo->join(curBlockInfo);\n }\n \n } // namespace mlir"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 24, "deletions": 17, "changes": 41, "file_content_changes": "@@ -11,14 +11,14 @@ namespace mlir {\n \n bool ReduceOpHelper::isFastReduction() {\n   auto srcLayout = srcTy.getEncoding();\n-  auto axis = op.axis();\n+  auto axis = op.getAxis();\n   return axis == triton::gpu::getOrder(srcLayout)[0];\n }\n \n unsigned ReduceOpHelper::getInterWarpSize() {\n   auto srcLayout = srcTy.getEncoding();\n   auto srcShape = srcTy.getShape();\n-  auto axis = op.axis();\n+  auto axis = op.getAxis();\n   auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n   unsigned sizeIntraWarps = getIntraWarpSize();\n   return std::min(srcReduceDimSize / sizeIntraWarps,\n@@ -28,28 +28,28 @@ unsigned ReduceOpHelper::getInterWarpSize() {\n unsigned ReduceOpHelper::getIntraWarpSize() {\n   auto srcLayout = srcTy.getEncoding();\n   auto srcShape = srcTy.getShape();\n-  auto axis = op.axis();\n+  auto axis = op.getAxis();\n   auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n   return std::min(srcReduceDimSize,\n                   triton::gpu::getThreadsPerWarp(srcLayout)[axis]);\n }\n \n unsigned ReduceOpHelper::getThreadsReductionAxis() {\n   auto srcLayout = srcTy.getEncoding();\n-  auto axis = op.axis();\n+  auto axis = op.getAxis();\n   return triton::gpu::getThreadsPerWarp(srcLayout)[axis] *\n          triton::gpu::getWarpsPerCTA(srcLayout)[axis];\n }\n \n SmallVector<unsigned> ReduceOpHelper::getScratchConfigBasic() {\n-  auto axis = op.axis();\n+  auto axis = op.getAxis();\n   auto smemShape = convertType<unsigned>(getSrcShape());\n   smemShape[axis] = std::min(smemShape[axis], getThreadsReductionAxis());\n   return smemShape;\n }\n \n SmallVector<SmallVector<unsigned>> ReduceOpHelper::getScratchConfigsFast() {\n-  auto axis = op.axis();\n+  auto axis = op.getAxis();\n   SmallVector<SmallVector<unsigned>> smemShapes(3);\n \n   auto argLayout = srcTy.getEncoding();\n@@ -82,10 +82,10 @@ unsigned ReduceOpHelper::getScratchSizeInBytes() {\n     elems = product<unsigned>(smemShape);\n   }\n \n-  auto tensorType = op.operand().getType().cast<RankedTensorType>();\n+  auto tensorType = op.getOperand().getType().cast<RankedTensorType>();\n   unsigned bytes = elems * tensorType.getElementTypeBitWidth() / 8;\n \n-  if (triton::ReduceOp::withIndex(op.redOp()))\n+  if (triton::ReduceOp::withIndex(op.getRedOp()))\n     bytes += elems * sizeof(int32_t);\n \n   return bytes;\n@@ -109,8 +109,7 @@ bool maybeSharedAllocationOp(Operation *op) {\n          (dialect->getTypeID() ==\n               mlir::TypeID::get<triton::gpu::TritonGPUDialect>() ||\n           dialect->getTypeID() == mlir::TypeID::get<triton::TritonDialect>() ||\n-          dialect->getTypeID() ==\n-              mlir::TypeID::get<arith::ArithmeticDialect>() ||\n+          dialect->getTypeID() == mlir::TypeID::get<arith::ArithDialect>() ||\n           dialect->getTypeID() == mlir::TypeID::get<tensor::TensorDialect>());\n }\n \n@@ -124,12 +123,12 @@ bool supportMMA(triton::DotOp op, int version) {\n   // Refer to mma section for the data type supported by Volta and Hopper\n   // Tensor Core in\n   // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-fragment-mma-884-f16\n-  auto aElemTy = op.a().getType().cast<RankedTensorType>().getElementType();\n-  auto bElemTy = op.b().getType().cast<RankedTensorType>().getElementType();\n+  auto aElemTy = op.getA().getType().cast<RankedTensorType>().getElementType();\n+  auto bElemTy = op.getB().getType().cast<RankedTensorType>().getElementType();\n   if (aElemTy.isF32() && bElemTy.isF32()) {\n-    return op.allowTF32() && version >= 2;\n+    return op.getAllowTF32() && version >= 2;\n   }\n-  return supportMMA(op.a(), version) && supportMMA(op.b(), version);\n+  return supportMMA(op.getA(), version) && supportMMA(op.getB(), version);\n }\n \n bool supportMMA(Value value, int version) {\n@@ -332,7 +331,8 @@ namespace {\n // Copied from TestDeadCodeAnalysis.cpp, because some dead code analysis\n // interacts with constant propagation, but SparseConstantPropagation\n // doesn't seem to be sufficient.\n-struct ConstantAnalysis : public DataFlowAnalysis {\n+class ConstantAnalysis : public DataFlowAnalysis {\n+public:\n   using DataFlowAnalysis::DataFlowAnalysis;\n \n   LogicalResult initialize(Operation *top) override {\n@@ -354,12 +354,19 @@ struct ConstantAnalysis : public DataFlowAnalysis {\n                                        value, op->getDialect())));\n       return success();\n     }\n+    // Dead code analysis requires every operands has initialized ConstantValue\n+    // state before it is visited.\n+    // https://github.com/llvm/llvm-project/blob/2ec1aba2b69faa1de5f71832a48e25aa3b5d5314/mlir/lib/Analysis/DataFlow/DeadCodeAnalysis.cpp#L322\n+    // That's why we need to set all operands to unknown constants.\n     setAllToUnknownConstants(op->getResults());\n-    for (Region &region : op->getRegions())\n-      setAllToUnknownConstants(region.getArguments());\n+    for (Region &region : op->getRegions()) {\n+      for (Block &block : region.getBlocks())\n+        setAllToUnknownConstants(block.getArguments());\n+    }\n     return success();\n   }\n \n+private:\n   /// Set all given values as not constants.\n   void setAllToUnknownConstants(ValueRange values) {\n     dataflow::ConstantValue unknownConstant(nullptr, nullptr);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 18, "deletions": 18, "changes": 36, "file_content_changes": "@@ -27,8 +27,8 @@ struct ConvertLayoutOpConversion\n   LogicalResult\n   matchAndRewrite(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    Value src = op.src();\n-    Value dst = op.result();\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n     auto srcTy = src.getType().cast<RankedTensorType>();\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n     Attribute srcLayout = srcTy.getEncoding();\n@@ -342,8 +342,8 @@ struct ConvertLayoutOpConversion\n                                 OpAdaptor adaptor,\n                                 ConversionPatternRewriter &rewriter) const {\n     auto loc = op.getLoc();\n-    Value src = op.src();\n-    Value dst = op.result();\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n     auto srcTy = src.getType().cast<RankedTensorType>();\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n     Attribute srcLayout = srcTy.getEncoding();\n@@ -393,7 +393,7 @@ struct ConvertLayoutOpConversion\n     // Potentially we need to store for multiple CTAs in this replication\n     auto accumNumReplicates = product<unsigned>(numReplicates);\n     // unsigned elems = getElemsPerThread(srcTy);\n-    auto vals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n+    auto vals = getElementsFromStruct(loc, adaptor.getSrc(), rewriter);\n     unsigned inVec = 0;\n     unsigned outVec = 0;\n     auto paddedRepShape = getScratchConfigForCvtLayout(op, inVec, outVec);\n@@ -457,8 +457,8 @@ struct ConvertLayoutOpConversion\n   lowerDistributedToShared(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n                            ConversionPatternRewriter &rewriter) const {\n     auto loc = op.getLoc();\n-    Value src = op.src();\n-    Value dst = op.result();\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n     auto srcTy = src.getType().cast<RankedTensorType>();\n     auto srcShape = srcTy.getShape();\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n@@ -477,7 +477,7 @@ struct ConvertLayoutOpConversion\n     auto dstStrides =\n         getStridesFromShapeAndOrder(dstShape, outOrd, loc, rewriter);\n     auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n-    storeDistributedToShared(src, adaptor.src(), dstStrides, srcIndices, dst,\n+    storeDistributedToShared(src, adaptor.getSrc(), dstStrides, srcIndices, dst,\n                              smemBase, elemTy, loc, rewriter);\n     auto smemObj =\n         SharedMemoryObject(smemBase, dstShape, outOrd, loc, rewriter);\n@@ -491,8 +491,8 @@ struct ConvertLayoutOpConversion\n   lowerSharedToDotOperand(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n                           ConversionPatternRewriter &rewriter) const {\n     auto loc = op.getLoc();\n-    Value src = op.src();\n-    Value dst = op.result();\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n     auto dstTensorTy = dst.getType().cast<RankedTensorType>();\n     auto srcTensorTy = src.getType().cast<RankedTensorType>();\n     auto dotOperandLayout =\n@@ -520,10 +520,10 @@ struct ConvertLayoutOpConversion\n       DotOpFMAConversionHelper helper(blockedLayout);\n       auto thread = getThreadId(rewriter, loc);\n       if (dotOpLayout.getOpIdx() == 0) { // $a\n-        res = helper.loadA(src, adaptor.src(), blockedLayout, thread, loc,\n+        res = helper.loadA(src, adaptor.getSrc(), blockedLayout, thread, loc,\n                            rewriter);\n       } else { // $b\n-        res = helper.loadB(src, adaptor.src(), blockedLayout, thread, loc,\n+        res = helper.loadB(src, adaptor.getSrc(), blockedLayout, thread, loc,\n                            rewriter);\n       }\n     } else {\n@@ -539,15 +539,15 @@ struct ConvertLayoutOpConversion\n   lowerMmaToDotOperand(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n                        ConversionPatternRewriter &rewriter) const {\n     auto loc = op.getLoc();\n-    auto srcTy = op.src().getType().cast<RankedTensorType>();\n-    auto dstTy = op.result().getType().cast<RankedTensorType>();\n+    auto srcTy = op.getSrc().getType().cast<RankedTensorType>();\n+    auto dstTy = op.getResult().getType().cast<RankedTensorType>();\n     auto srcLayout = srcTy.getEncoding();\n     auto dstLayout = dstTy.getEncoding();\n     auto srcMmaLayout = srcLayout.cast<MmaEncodingAttr>();\n     auto dstDotLayout = dstLayout.cast<DotOperandEncodingAttr>();\n     if (isMmaToDotShortcut(srcMmaLayout, dstDotLayout)) {\n       // get source values\n-      auto vals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n+      auto vals = getElementsFromStruct(loc, adaptor.getSrc(), rewriter);\n       unsigned elems = getElemsPerThread(srcTy);\n       Type elemTy =\n           this->getTypeConverter()->convertType(srcTy.getElementType());\n@@ -596,12 +596,12 @@ struct ConvertLayoutOpConversion\n       ConversionPatternRewriter &rewriter, const MmaEncodingAttr &mmaLayout,\n       const DotOperandEncodingAttr &dotOperandLayout, bool isOuter) const {\n     auto loc = op.getLoc();\n-    Value src = op.src();\n-    Value dst = op.result();\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n     bool isHMMA = supportMMA(dst, mmaLayout.getVersionMajor());\n \n     auto smemObj =\n-        getSharedMemoryObjectFromStruct(loc, adaptor.src(), rewriter);\n+        getSharedMemoryObjectFromStruct(loc, adaptor.getSrc(), rewriter);\n     Value res;\n \n     if (!isOuter && mmaLayout.isAmpere() && isHMMA) { // tensor core v2"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.cpp", "status": "modified", "additions": 7, "deletions": 10, "changes": 17, "file_content_changes": "@@ -597,20 +597,20 @@ DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(Type operandTy) {\n \n DotOpMmaV2ConversionHelper::TensorCoreType\n DotOpMmaV2ConversionHelper::getMmaType(triton::DotOp op) {\n-  Value A = op.a();\n-  Value B = op.b();\n+  Value A = op.getA();\n+  Value B = op.getB();\n   auto aTy = A.getType().cast<RankedTensorType>();\n   auto bTy = B.getType().cast<RankedTensorType>();\n   // d = a*b + c\n-  auto dTy = op.d().getType().cast<RankedTensorType>();\n+  auto dTy = op.getD().getType().cast<RankedTensorType>();\n \n   if (dTy.getElementType().isF32()) {\n     if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n       return TensorCoreType::FP32_FP16_FP16_FP32;\n     if (aTy.getElementType().isBF16() && bTy.getElementType().isBF16())\n       return TensorCoreType::FP32_BF16_BF16_FP32;\n     if (aTy.getElementType().isF32() && bTy.getElementType().isF32() &&\n-        op.allowTF32())\n+        op.getAllowTF32())\n       return TensorCoreType::FP32_TF32_TF32_FP32;\n   } else if (dTy.getElementType().isInteger(32)) {\n     if (aTy.getElementType().isInteger(8) && bTy.getElementType().isInteger(8))\n@@ -822,10 +822,8 @@ MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> offs,\n     // The result type is 4xi32, each i32 is composed of 2xf16\n     // elements (adjacent two columns in a row) or a single f32 element.\n     Value resV4 = builder.launch(rewriter, loc, resTy);\n-    return {extract_val(elemTy, resV4, i32_arr_attr(0)),\n-            extract_val(elemTy, resV4, i32_arr_attr(1)),\n-            extract_val(elemTy, resV4, i32_arr_attr(2)),\n-            extract_val(elemTy, resV4, i32_arr_attr(3))};\n+    return {extract_val(elemTy, resV4, 0), extract_val(elemTy, resV4, 1),\n+            extract_val(elemTy, resV4, 2), extract_val(elemTy, resV4, 3)};\n   } else if (elemBytes == 4 && needTrans) { // Use lds.32 to load tf32 matrices\n     Value ptr2 = getPtr(ptrIdx + 1);\n     assert(sMatStride == 1);\n@@ -1117,8 +1115,7 @@ LogicalResult MMA16816ConversionHelper::convertDot(Value a, Value b, Value c,\n \n     Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n     for (int i = 0; i < 4; ++i)\n-      fc[m * colsPerThread + 4 * n + i] =\n-          extract_val(elemTy, mmaOut, i32_arr_attr(i));\n+      fc[m * colsPerThread + 4 * n + i] = extract_val(elemTy, mmaOut, i);\n   };\n \n   for (int k = 0; k < numRepK; ++k)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -2,12 +2,12 @@\n #define TRITON_CONVERSION_TRITONGPU_TO_LLVM_DOT_OP_HELPERS_H\n \n #include \"mlir/Analysis/SliceAnalysis.h\"\n-#include \"mlir/Conversion/ArithmeticToLLVM/ArithmeticToLLVM.h\"\n+#include \"mlir/Conversion/ArithToLLVM/ArithToLLVM.h\"\n #include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n #include \"mlir/Conversion/LLVMCommon/LoweringOptions.h\"\n #include \"mlir/Conversion/LLVMCommon/Pattern.h\"\n #include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n-#include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/GPU/IR/GPUDialect.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\""}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.cpp", "status": "modified", "additions": 22, "deletions": 21, "changes": 43, "file_content_changes": "@@ -21,7 +21,7 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n   matchAndRewrite(triton::DotOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     // D = A * B + C\n-    Value A = op.a();\n+    Value A = op.getA();\n     Value D = op.getResult();\n \n     // Here we assume the DotOp's operands always comes from shared memory.\n@@ -65,9 +65,9 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n                          .getEncoding()\n                          .cast<MmaEncodingAttr>();\n \n-    Value A = op.a();\n-    Value B = op.b();\n-    Value C = op.c();\n+    Value A = op.getA();\n+    Value B = op.getB();\n+    Value C = op.getC();\n \n     MMA16816ConversionHelper mmaHelper(A.getType(), mmaLayout,\n                                        getThreadId(rewriter, loc), rewriter,\n@@ -81,21 +81,21 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n            \"Both $a and %b should be DotOperand layout.\");\n \n     Value loadedA, loadedB, loadedC;\n-    loadedA = adaptor.a();\n-    loadedB = adaptor.b();\n-    loadedC = mmaHelper.loadC(op.c(), adaptor.c());\n+    loadedA = adaptor.getA();\n+    loadedB = adaptor.getB();\n+    loadedC = mmaHelper.loadC(op.getC(), adaptor.getC());\n \n-    return mmaHelper.convertDot(A, B, C, op.d(), loadedA, loadedB, loadedC, op,\n-                                adaptor);\n+    return mmaHelper.convertDot(A, B, C, op.getD(), loadedA, loadedB, loadedC,\n+                                op, adaptor);\n   }\n   /// Convert to mma.m8n8k4\n   LogicalResult convertMMA884(triton::DotOp op, OpAdaptor adaptor,\n                               ConversionPatternRewriter &rewriter) const {\n     auto *ctx = op.getContext();\n     auto loc = op.getLoc();\n \n-    Value A = op.a();\n-    Value B = op.b();\n+    Value A = op.getA();\n+    Value B = op.getB();\n     Value D = op.getResult();\n     auto mmaLayout = D.getType()\n                          .cast<RankedTensorType>()\n@@ -129,14 +129,15 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n     unsigned numN = helper.getNumN(BShape[1], isBRow, isBVec4_);\n     unsigned NK = AShape[1];\n \n-    auto has = helper.extractLoadedOperand(adaptor.a(), NK, rewriter);\n-    auto hbs = helper.extractLoadedOperand(adaptor.b(), NK, rewriter);\n+    auto has = helper.extractLoadedOperand(adaptor.getA(), NK, rewriter);\n+    auto hbs = helper.extractLoadedOperand(adaptor.getB(), NK, rewriter);\n \n     // Initialize accumulators with external values, the acc holds the\n     // accumulator value that is shared between the MMA instructions inside a\n     // DotOp, we can call the order of the values the accumulator-internal\n     // order.\n-    SmallVector<Value> acc = getElementsFromStruct(loc, adaptor.c(), rewriter);\n+    SmallVector<Value> acc =\n+        getElementsFromStruct(loc, adaptor.getC(), rewriter);\n     size_t resSize = acc.size();\n \n     // The resVals holds the final result of the DotOp.\n@@ -192,7 +193,7 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n           builder.launch(rewriter, loc, helper.getMmaRetType(ATensorTy));\n \n       for (auto i = 0; i < 8; i++) {\n-        Value elem = extract_val(f32_ty, res, i32_arr_attr(i));\n+        Value elem = extract_val(f32_ty, res, i);\n         acc[idx[i]] = elem;\n       }\n     };\n@@ -220,9 +221,9 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n     auto *ctx = rewriter.getContext();\n     auto loc = op.getLoc();\n \n-    auto A = op.a();\n-    auto B = op.b();\n-    auto C = op.c();\n+    auto A = op.getA();\n+    auto B = op.getB();\n+    auto C = op.getC();\n     auto D = op.getResult();\n \n     auto aTensorTy = A.getType().cast<RankedTensorType>();\n@@ -235,11 +236,11 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n     BlockedEncodingAttr dLayout =\n         dTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n     auto order = dLayout.getOrder();\n-    auto cc = getElementsFromStruct(loc, adaptor.c(), rewriter);\n+    auto cc = getElementsFromStruct(loc, adaptor.getC(), rewriter);\n \n     DotOpFMAConversionHelper helper(dLayout);\n-    Value llA = adaptor.a();\n-    Value llB = adaptor.b();\n+    Value llA = adaptor.getA();\n+    Value llB = adaptor.getB();\n \n     auto sizePerThread = getSizePerThread(dLayout);\n     auto shapePerCTA = getShapePerCTA(dLayout);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 18, "deletions": 15, "changes": 33, "file_content_changes": "@@ -49,8 +49,8 @@ struct FpToFpOpConversion\n         struct_ty(SmallVector<Type>{fp16x2VecTy, fp16x2VecTy});\n     auto fp16x2x2Struct =\n         builder.launch(rewriter, loc, fp16x2x2StructTy, false);\n-    auto fp16x2Vec0 = extract_val(fp16x2VecTy, fp16x2x2Struct, i32_arr_attr(0));\n-    auto fp16x2Vec1 = extract_val(fp16x2VecTy, fp16x2x2Struct, i32_arr_attr(1));\n+    auto fp16x2Vec0 = extract_val(fp16x2VecTy, fp16x2x2Struct, 0);\n+    auto fp16x2Vec1 = extract_val(fp16x2VecTy, fp16x2x2Struct, 1);\n     return {extract_element(f16_ty, fp16x2Vec0, i32_val(0)),\n             extract_element(f16_ty, fp16x2Vec0, i32_val(1)),\n             extract_element(f16_ty, fp16x2Vec1, i32_val(0)),\n@@ -140,8 +140,8 @@ struct FpToFpOpConversion\n         struct_ty(SmallVector<Type>{bf16x2VecTy, bf16x2VecTy});\n     auto bf16x2x2Struct =\n         builder.launch(rewriter, loc, bf16x2x2StructTy, false);\n-    auto bf16x2Vec0 = extract_val(bf16x2VecTy, bf16x2x2Struct, i32_arr_attr(0));\n-    auto bf16x2Vec1 = extract_val(bf16x2VecTy, bf16x2x2Struct, i32_arr_attr(1));\n+    auto bf16x2Vec0 = extract_val(bf16x2VecTy, bf16x2x2Struct, 0);\n+    auto bf16x2Vec1 = extract_val(bf16x2VecTy, bf16x2x2Struct, 1);\n     return {extract_element(i16_ty, bf16x2Vec0, i32_val(0)),\n             extract_element(i16_ty, bf16x2Vec0, i32_val(1)),\n             extract_element(i16_ty, bf16x2Vec1, i32_val(0)),\n@@ -285,8 +285,9 @@ struct FpToFpOpConversion\n   LogicalResult\n   matchAndRewrite(triton::FpToFpOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    auto srcTensorType = op.from().getType().cast<mlir::RankedTensorType>();\n-    auto dstTensorType = op.result().getType().cast<mlir::RankedTensorType>();\n+    auto srcTensorType = op.getFrom().getType().cast<mlir::RankedTensorType>();\n+    auto dstTensorType =\n+        op.getResult().getType().cast<mlir::RankedTensorType>();\n     auto srcEltType = srcTensorType.getElementType();\n     auto dstEltType = dstTensorType.getElementType();\n     auto loc = op->getLoc();\n@@ -323,16 +324,18 @@ struct FpToFpOpConversion\n       // Vectorized casting\n       assert(elems % 4 == 0 &&\n              \"FP8 casting only support tensors with 4-aligned sizes\");\n-      auto elements = getElementsFromStruct(loc, adaptor.from(), rewriter);\n+      auto elements = getElementsFromStruct(loc, adaptor.getFrom(), rewriter);\n       for (size_t i = 0; i < elems; i += 4) {\n         auto converted = convertor(loc, rewriter, elements[i], elements[i + 1],\n                                    elements[i + 2], elements[i + 3]);\n         resultVals.append(converted);\n       }\n     } else if (srcEltType.isBF16() && dstEltType.isF32()) {\n-      resultVals.emplace_back(convertBf16ToFp32(loc, rewriter, adaptor.from()));\n+      resultVals.emplace_back(\n+          convertBf16ToFp32(loc, rewriter, adaptor.getFrom()));\n     } else if (srcEltType.isF32() && dstEltType.isBF16()) {\n-      resultVals.emplace_back(convertFp32ToBf16(loc, rewriter, adaptor.from()));\n+      resultVals.emplace_back(\n+          convertFp32ToBf16(loc, rewriter, adaptor.getFrom()));\n     } else {\n       assert(false && \"unsupported type casting\");\n     }\n@@ -436,7 +439,7 @@ struct CmpIOpConversion\n                             ConversionPatternRewriter &rewriter, Type elemTy,\n                             ValueRange operands, Location loc) const {\n     return rewriter.create<LLVM::ICmpOp>(\n-        loc, elemTy, ArithCmpIPredicateToLLVM(op.predicate()), operands[0],\n+        loc, elemTy, ArithCmpIPredicateToLLVM(op.getPredicate()), operands[0],\n         operands[1]);\n   }\n \n@@ -478,7 +481,7 @@ struct CmpFOpConversion\n                                    Type elemTy, ValueRange operands,\n                                    Location loc) {\n     return rewriter.create<LLVM::FCmpOp>(\n-        loc, elemTy, ArithCmpFPredicateToLLVM(op.predicate()), operands[0],\n+        loc, elemTy, ArithCmpFPredicateToLLVM(op.getPredicate()), operands[0],\n         operands[1]);\n   }\n \n@@ -523,14 +526,14 @@ struct ExtElemwiseOpConversion\n   Value createDestOp(triton::ExtElemwiseOp op, OpAdaptor adaptor,\n                      ConversionPatternRewriter &rewriter, Type elemTy,\n                      ValueRange operands, Location loc) const {\n-    StringRef funcName = op.symbol();\n+    StringRef funcName = op.getSymbol();\n     if (funcName.empty())\n       llvm::errs() << \"ExtElemwiseOpConversion\";\n \n     Type funcType = getFunctionType(elemTy, operands);\n     LLVM::LLVMFuncOp funcOp =\n         appendOrGetFuncOp(rewriter, op, funcName, funcType);\n-    return rewriter.create<LLVM::CallOp>(loc, funcOp, operands).getResult(0);\n+    return rewriter.create<LLVM::CallOp>(loc, funcOp, operands).getResult();\n   }\n \n private:\n@@ -552,9 +555,9 @@ struct ExtElemwiseOpConversion\n     mlir::OpBuilder b(op->getParentOfType<LLVMFuncOp>());\n     auto ret = b.create<LLVMFuncOp>(op->getLoc(), funcName, funcType);\n     ret.getOperation()->setAttr(\n-        \"libname\", StringAttr::get(op->getContext(), op.libname()));\n+        \"libname\", StringAttr::get(op->getContext(), op.getLibname()));\n     ret.getOperation()->setAttr(\n-        \"libpath\", StringAttr::get(op->getContext(), op.libpath()));\n+        \"libpath\", StringAttr::get(op->getContext(), op.getLibpath()));\n     return ret;\n   }\n };"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 44, "deletions": 45, "changes": 89, "file_content_changes": "@@ -73,14 +73,14 @@ struct LoadOpConversion\n     auto loc = op->getLoc();\n \n     // original values\n-    Value ptr = op.ptr();\n-    Value mask = op.mask();\n-    Value other = op.other();\n+    Value ptr = op.getPtr();\n+    Value mask = op.getMask();\n+    Value other = op.getOther();\n \n     // adaptor values\n-    Value llPtr = adaptor.ptr();\n-    Value llMask = adaptor.mask();\n-    Value llOther = adaptor.other();\n+    Value llPtr = adaptor.getPtr();\n+    Value llMask = adaptor.getMask();\n+    Value llOther = adaptor.getOther();\n \n     // Determine the vectorization size\n     Type valueTy = op.getResult().getType();\n@@ -158,14 +158,14 @@ struct LoadOpConversion\n \n       // Define the instruction opcode\n       auto &ld = ptxBuilder.create<>(\"ld\")\n-                     ->o(\"volatile\", op.isVolatile())\n+                     ->o(\"volatile\", op.getIsVolatile())\n                      .global()\n-                     .o(\"ca\", op.cache() == triton::CacheModifier::CA)\n-                     .o(\"cg\", op.cache() == triton::CacheModifier::CG)\n+                     .o(\"ca\", op.getCache() == triton::CacheModifier::CA)\n+                     .o(\"cg\", op.getCache() == triton::CacheModifier::CG)\n                      .o(\"L1::evict_first\",\n-                        op.evict() == triton::EvictionPolicy::EVICT_FIRST)\n+                        op.getEvict() == triton::EvictionPolicy::EVICT_FIRST)\n                      .o(\"L1::evict_last\",\n-                        op.evict() == triton::EvictionPolicy::EVICT_LAST)\n+                        op.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n                      .o(\"L1::cache_hint\", hasL2EvictPolicy)\n                      .v(nWords)\n                      .b(width);\n@@ -227,8 +227,7 @@ struct LoadOpConversion\n       for (unsigned int ii = 0; ii < nWords; ++ii) {\n         Value curr;\n         if (retTy.isa<LLVM::LLVMStructType>()) {\n-          curr = extract_val(IntegerType::get(getContext(), width), ret,\n-                             i64_arr_attr(ii));\n+          curr = extract_val(IntegerType::get(getContext(), width), ret, ii);\n         } else {\n           curr = ret;\n         }\n@@ -267,13 +266,13 @@ struct StoreOpConversion\n   LogicalResult\n   matchAndRewrite(triton::StoreOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    Value ptr = op.ptr();\n-    Value mask = op.mask();\n-    Value value = op.value();\n+    Value ptr = op.getPtr();\n+    Value mask = op.getMask();\n+    Value value = op.getValue();\n \n-    Value llPtr = adaptor.ptr();\n-    Value llMask = adaptor.mask();\n-    Value llValue = adaptor.value();\n+    Value llPtr = adaptor.getPtr();\n+    Value llMask = adaptor.getMask();\n+    Value llValue = adaptor.getValue();\n \n     auto loc = op->getLoc();\n     MLIRContext *ctx = rewriter.getContext();\n@@ -388,9 +387,9 @@ struct AtomicCASOpConversion\n     auto loc = op.getLoc();\n     MLIRContext *ctx = rewriter.getContext();\n \n-    Value llPtr = adaptor.ptr();\n-    Value llCmp = adaptor.cmp();\n-    Value llVal = adaptor.val();\n+    Value llPtr = adaptor.getPtr();\n+    Value llCmp = adaptor.getCmp();\n+    Value llVal = adaptor.getVal();\n \n     auto ptrElements = getElementsFromStruct(loc, llPtr, rewriter);\n     auto cmpElements = getElementsFromStruct(loc, llCmp, rewriter);\n@@ -462,13 +461,13 @@ struct AtomicRMWOpConversion\n     auto loc = op.getLoc();\n     MLIRContext *ctx = rewriter.getContext();\n \n-    auto atomicRmwAttr = op.atomic_rmw_op();\n-    Value ptr = op.ptr();\n-    Value val = op.val();\n+    auto atomicRmwAttr = op.getAtomicRmwOp();\n+    Value ptr = op.getPtr();\n+    Value val = op.getVal();\n \n-    Value llPtr = adaptor.ptr();\n-    Value llVal = adaptor.val();\n-    Value llMask = adaptor.mask();\n+    Value llPtr = adaptor.getPtr();\n+    Value llVal = adaptor.getVal();\n+    Value llMask = adaptor.getMask();\n \n     auto valElements = getElementsFromStruct(loc, llVal, rewriter);\n     auto ptrElements = getElementsFromStruct(loc, llPtr, rewriter);\n@@ -606,9 +605,9 @@ struct InsertSliceOpConversion\n                   ConversionPatternRewriter &rewriter) const override {\n     // %dst = insert_slice %src into %dst[%offsets]\n     Location loc = op->getLoc();\n-    Value dst = op.dest();\n-    Value src = op.source();\n-    Value res = op.result();\n+    Value dst = op.getDest();\n+    Value src = op.getSource();\n+    Value res = op.getResult();\n     assert(allocation->getBufferId(res) == Allocation::InvalidBufferId &&\n            \"Only support in-place insert_slice for now\");\n \n@@ -619,7 +618,7 @@ struct InsertSliceOpConversion\n \n     auto dstTy = dst.getType().dyn_cast<RankedTensorType>();\n     auto dstLayout = dstTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n-    auto llDst = adaptor.dest();\n+    auto llDst = adaptor.getDest();\n     assert(dstLayout && \"Unexpected dstLayout in InsertSliceOpConversion\");\n     assert(op.hasUnitStride() &&\n            \"Only unit stride supported by InsertSliceOpConversion\");\n@@ -632,7 +631,7 @@ struct InsertSliceOpConversion\n     auto mixedOffsets = op.getMixedOffsets();\n     for (auto i = 0; i < mixedOffsets.size(); ++i) {\n       if (op.isDynamicOffset(i)) {\n-        offsets.emplace_back(adaptor.offsets()[i]);\n+        offsets.emplace_back(adaptor.getOffsets()[i]);\n       } else {\n         offsets.emplace_back(i32_val(op.getStaticOffset(i)));\n       }\n@@ -650,7 +649,7 @@ struct InsertSliceOpConversion\n     auto elemPtrTy = ptr_ty(elemTy, 3);\n     auto smemBase = gep(elemPtrTy, smemObj.base, offset);\n \n-    auto llSrc = adaptor.source();\n+    auto llSrc = adaptor.getSource();\n     auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n     storeDistributedToShared(src, llSrc, srcStrides, srcIndices, dst, smemBase,\n                              elemTy, loc, rewriter);\n@@ -681,11 +680,11 @@ struct InsertSliceAsyncOpConversion\n                   ConversionPatternRewriter &rewriter) const override {\n     // insert_slice_async %src, %dst, %index, %mask, %other\n     auto loc = op.getLoc();\n-    Value src = op.src();\n-    Value dst = op.dst();\n-    Value res = op.result();\n-    Value mask = op.mask();\n-    Value other = op.other();\n+    Value src = op.getSrc();\n+    Value dst = op.getDst();\n+    Value res = op.getResult();\n+    Value mask = op.getMask();\n+    Value other = op.getOther();\n     assert(allocation->getBufferId(res) == Allocation::InvalidBufferId &&\n            \"Only support in-place insert_slice_async for now\");\n \n@@ -698,11 +697,11 @@ struct InsertSliceAsyncOpConversion\n     assert(srcShape.size() == 2 &&\n            \"insert_slice_async: Unexpected rank of %src\");\n \n-    Value llDst = adaptor.dst();\n-    Value llSrc = adaptor.src();\n-    Value llMask = adaptor.mask();\n-    Value llOther = adaptor.other();\n-    Value llIndex = adaptor.index();\n+    Value llDst = adaptor.getDst();\n+    Value llSrc = adaptor.getSrc();\n+    Value llMask = adaptor.getMask();\n+    Value llOther = adaptor.getOther();\n+    Value llIndex = adaptor.getIndex();\n \n     // %src\n     auto srcElems = getLLVMElems(src, llSrc, rewriter, loc);\n@@ -802,7 +801,7 @@ struct InsertSliceAsyncOpConversion\n             ptxBuilder.newAddrOperand(srcElems[elemIdx + wordElemIdx], \"l\");\n         auto *copySize = ptxBuilder.newConstantOperand(byteWidth);\n         auto *srcSize = copySize;\n-        if (op.mask()) {\n+        if (op.getMask()) {\n           // We don't use predicate in this case, setting src-size to 0\n           // if there's any mask. cp.async will automatically fill the\n           // remaining slots with 0 if cp-size > src-size."}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 20, "deletions": 18, "changes": 38, "file_content_changes": "@@ -138,10 +138,10 @@ struct ReduceOpConversion\n   matchAndRewriteBasic(triton::ReduceOp op, OpAdaptor adaptor,\n                        ConversionPatternRewriter &rewriter) const {\n     Location loc = op->getLoc();\n-    unsigned axis = op.axis();\n-    bool withIndex = triton::ReduceOp::withIndex(op.redOp());\n+    unsigned axis = op.getAxis();\n+    bool withIndex = triton::ReduceOp::withIndex(op.getRedOp());\n \n-    auto srcTy = op.operand().getType().cast<RankedTensorType>();\n+    auto srcTy = op.getOperand().getType().cast<RankedTensorType>();\n     auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n     auto srcOrd = srcLayout.getOrder();\n     auto srcShape = srcTy.getShape();\n@@ -161,7 +161,7 @@ struct ReduceOpConversion\n \n     unsigned srcElems = getElemsPerThread(srcTy);\n     auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n-    auto srcValues = getElementsFromStruct(loc, adaptor.operand(), rewriter);\n+    auto srcValues = getElementsFromStruct(loc, adaptor.getOperand(), rewriter);\n \n     SmallVector<SmallVector<unsigned>> offset =\n         emitOffsetForLayout(srcLayout, srcShape);\n@@ -176,10 +176,11 @@ struct ReduceOpConversion\n       key[axis] = 0;\n       bool isFirst = accs.find(key) == accs.end();\n       if (!withIndex) {\n-        accumulate(rewriter, loc, op.redOp(), accs[key], srcValues[i], isFirst);\n+        accumulate(rewriter, loc, op.getRedOp(), accs[key], srcValues[i],\n+                   isFirst);\n       } else {\n         Value curIndex = srcIndices[i][axis];\n-        accumulateWithIndex(rewriter, loc, op.redOp(), accs[key],\n+        accumulateWithIndex(rewriter, loc, op.getRedOp(), accs[key],\n                             accIndices[key], srcValues[i], curIndex, isFirst);\n       }\n       if (isFirst)\n@@ -221,14 +222,14 @@ struct ReduceOpConversion\n         barrier();\n         if (!withIndex) {\n           Value cur = load(readPtr);\n-          accumulate(rewriter, loc, op.redOp(), acc, cur, false);\n+          accumulate(rewriter, loc, op.getRedOp(), acc, cur, false);\n           barrier();\n           store(acc, writePtr);\n         } else {\n           Value cur = load(readPtr);\n           Value indexReadPtr = gep(indexPtrTy, indexWritePtr, readOffset);\n           Value curIndex = load(indexReadPtr);\n-          accumulateWithIndex(rewriter, loc, op.redOp(), acc, accIndex, cur,\n+          accumulateWithIndex(rewriter, loc, op.getRedOp(), acc, accIndex, cur,\n                               curIndex, false);\n           barrier();\n           store(acc, writePtr);\n@@ -280,10 +281,10 @@ struct ReduceOpConversion\n   LogicalResult matchAndRewriteFast(triton::ReduceOp op, OpAdaptor adaptor,\n                                     ConversionPatternRewriter &rewriter) const {\n     Location loc = op->getLoc();\n-    unsigned axis = adaptor.axis();\n-    bool withIndex = triton::ReduceOp::withIndex(op.redOp());\n+    unsigned axis = adaptor.getAxis();\n+    bool withIndex = triton::ReduceOp::withIndex(op.getRedOp());\n \n-    auto srcTy = op.operand().getType().cast<RankedTensorType>();\n+    auto srcTy = op.getOperand().getType().cast<RankedTensorType>();\n     auto srcLayout = srcTy.getEncoding();\n     auto srcShape = srcTy.getShape();\n     auto order = getOrder(srcLayout);\n@@ -310,7 +311,7 @@ struct ReduceOpConversion\n \n     unsigned srcElems = getElemsPerThread(srcTy);\n     auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n-    auto srcValues = getElementsFromStruct(loc, adaptor.operand(), rewriter);\n+    auto srcValues = getElementsFromStruct(loc, adaptor.getOperand(), rewriter);\n \n     SmallVector<SmallVector<unsigned>> offset =\n         emitOffsetForLayout(srcLayout, srcShape);\n@@ -325,10 +326,11 @@ struct ReduceOpConversion\n       key[axis] = 0;\n       bool isFirst = accs.find(key) == accs.end();\n       if (!withIndex) {\n-        accumulate(rewriter, loc, op.redOp(), accs[key], srcValues[i], isFirst);\n+        accumulate(rewriter, loc, op.getRedOp(), accs[key], srcValues[i],\n+                   isFirst);\n       } else {\n         Value curIndex = srcIndices[i][axis];\n-        accumulateWithIndex(rewriter, loc, op.redOp(), accs[key],\n+        accumulateWithIndex(rewriter, loc, op.getRedOp(), accs[key],\n                             accIndices[key], srcValues[i], curIndex, isFirst);\n       }\n       if (isFirst)\n@@ -362,10 +364,10 @@ struct ReduceOpConversion\n       for (unsigned N = sizeIntraWarps / 2; N > 0; N >>= 1) {\n         Value shfl = shflSync(loc, rewriter, acc, N);\n         if (!withIndex) {\n-          accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n+          accumulate(rewriter, loc, op.getRedOp(), acc, shfl, false);\n         } else {\n           Value shflIndex = shflSync(loc, rewriter, accIndex, N);\n-          accumulateWithIndex(rewriter, loc, op.redOp(), acc, accIndex, shfl,\n+          accumulateWithIndex(rewriter, loc, op.getRedOp(), acc, accIndex, shfl,\n                               shflIndex, false);\n         }\n       }\n@@ -408,10 +410,10 @@ struct ReduceOpConversion\n       for (unsigned N = sizeInterWarps / 2; N > 0; N >>= 1) {\n         Value shfl = shflSync(loc, rewriter, acc, N);\n         if (!withIndex) {\n-          accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n+          accumulate(rewriter, loc, op.getRedOp(), acc, shfl, false);\n         } else {\n           Value shflIndex = shflSync(loc, rewriter, accIndex, N);\n-          accumulateWithIndex(rewriter, loc, op.redOp(), acc, accIndex, shfl,\n+          accumulateWithIndex(rewriter, loc, op.getRedOp(), acc, accIndex, shfl,\n                               shflIndex, false);\n         }\n       }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 16, "deletions": 16, "changes": 32, "file_content_changes": "@@ -54,9 +54,9 @@ struct BroadcastOpConversion\n     // regardless of the order of the layout\n     //\n     Location loc = op->getLoc();\n-    Value src = adaptor.src();\n-    Value result = op.result();\n-    auto srcTy = op.src().getType().cast<RankedTensorType>();\n+    Value src = adaptor.getSrc();\n+    Value result = op.getResult();\n+    auto srcTy = op.getSrc().getType().cast<RankedTensorType>();\n     auto resultTy = result.getType().cast<RankedTensorType>();\n     auto srcLayout = srcTy.getEncoding();\n     auto resultLayout = resultTy.getEncoding();\n@@ -111,7 +111,7 @@ struct PrintfOpConversion\n     }\n     std::string formatStr;\n     llvm::raw_string_ostream os(formatStr);\n-    os << op.prefix();\n+    os << op.getPrefix();\n     if (!operands.empty()) {\n       os << getFormatSubstr(operands[0]);\n     }\n@@ -289,13 +289,13 @@ struct MakeRangeOpConversion\n   matchAndRewrite(triton::MakeRangeOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n-    auto rankedTy = op.result().getType().dyn_cast<RankedTensorType>();\n+    auto rankedTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n     auto shape = rankedTy.getShape();\n     auto layout = rankedTy.getEncoding();\n \n     auto elemTy = rankedTy.getElementType();\n     assert(elemTy.isInteger(32));\n-    Value start = createIndexAttrConstant(rewriter, loc, elemTy, op.start());\n+    Value start = createIndexAttrConstant(rewriter, loc, elemTy, op.getStart());\n     auto idxs = emitIndices(loc, rewriter, layout, shape);\n     unsigned elems = idxs.size();\n     SmallVector<Value> retVals(elems);\n@@ -323,10 +323,10 @@ struct GetProgramIdOpConversion\n   matchAndRewrite(triton::GetProgramIdOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n-    assert(op.axis() < 3);\n+    assert(op.getAxis() < 3);\n \n     Value blockId = rewriter.create<::mlir::gpu::BlockIdOp>(\n-        loc, rewriter.getIndexType(), dims[op.axis()]);\n+        loc, rewriter.getIndexType(), dims[op.getAxis()]);\n     auto llvmIndexTy = getTypeConverter()->getIndexType();\n     rewriter.replaceOpWithNewOp<UnrealizedConversionCastOp>(\n         op, TypeRange{llvmIndexTy}, ValueRange{blockId});\n@@ -347,10 +347,10 @@ struct GetNumProgramsOpConversion\n   matchAndRewrite(triton::GetNumProgramsOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n-    assert(op.axis() < 3);\n+    assert(op.getAxis() < 3);\n \n     Value blockId = rewriter.create<::mlir::gpu::GridDimOp>(\n-        loc, rewriter.getIndexType(), dims[op.axis()]);\n+        loc, rewriter.getIndexType(), dims[op.getAxis()]);\n     auto llvmIndexTy = getTypeConverter()->getIndexType();\n     rewriter.replaceOpWithNewOp<UnrealizedConversionCastOp>(\n         op, TypeRange{llvmIndexTy}, ValueRange{blockId});\n@@ -379,8 +379,8 @@ struct AddPtrOpConversion\n           getTypeConverter()->convertType(resultTensorTy.getElementType());\n       SmallVector<Type> types(elems, elemTy);\n       Type structTy = LLVM::LLVMStructType::getLiteral(getContext(), types);\n-      auto ptrs = getElementsFromStruct(loc, adaptor.ptr(), rewriter);\n-      auto offsets = getElementsFromStruct(loc, adaptor.offset(), rewriter);\n+      auto ptrs = getElementsFromStruct(loc, adaptor.getPtr(), rewriter);\n+      auto offsets = getElementsFromStruct(loc, adaptor.getOffset(), rewriter);\n       SmallVector<Value> resultVals(elems);\n       for (unsigned i = 0; i < elems; ++i) {\n         resultVals[i] = gep(elemTy, ptrs[i], offsets[i]);\n@@ -390,7 +390,7 @@ struct AddPtrOpConversion\n     } else {\n       assert(resultTy.isa<triton::PointerType>());\n       Type llResultTy = getTypeConverter()->convertType(resultTy);\n-      Value result = gep(llResultTy, adaptor.ptr(), adaptor.offset());\n+      Value result = gep(llResultTy, adaptor.getPtr(), adaptor.getOffset());\n       rewriter.replaceOp(op, result);\n     }\n     return success();\n@@ -440,7 +440,7 @@ struct ExtractSliceOpConversion\n                   ConversionPatternRewriter &rewriter) const override {\n     // %dst = extract_slice %src[%offsets]\n     Location loc = op->getLoc();\n-    auto srcTy = op.source().getType().dyn_cast<RankedTensorType>();\n+    auto srcTy = op.getSource().getType().dyn_cast<RankedTensorType>();\n     auto srcLayout = srcTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n     assert(srcLayout && \"Unexpected resultLayout in ExtractSliceOpConversion\");\n     assert(op.hasUnitStride() &&\n@@ -449,13 +449,13 @@ struct ExtractSliceOpConversion\n     // newBase = base + offset\n     // Triton supports either static and dynamic offsets\n     auto smemObj =\n-        getSharedMemoryObjectFromStruct(loc, adaptor.source(), rewriter);\n+        getSharedMemoryObjectFromStruct(loc, adaptor.getSource(), rewriter);\n     SmallVector<Value, 4> opOffsetVals;\n     SmallVector<Value, 4> offsetVals;\n     auto mixedOffsets = op.getMixedOffsets();\n     for (auto i = 0; i < mixedOffsets.size(); ++i) {\n       if (op.isDynamicOffset(i))\n-        opOffsetVals.emplace_back(adaptor.offsets()[i]);\n+        opOffsetVals.emplace_back(adaptor.getOffsets()[i]);\n       else\n         opOffsetVals.emplace_back(i32_val(op.getStaticOffset(i)));\n       offsetVals.emplace_back(add(smemObj.offsets[i], opOffsetVals[i]));"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 6, "deletions": 8, "changes": 14, "file_content_changes": "@@ -50,10 +50,9 @@ struct FuncOpConversionBase : public ConvertOpToLLVMPattern<func::FuncOp> {\n \n     for (const auto &attr : op->getAttrs()) {\n       if (attr.getName() == SymbolTable::getSymbolAttrName() ||\n-          attr.getName() == FunctionOpInterface::getTypeAttrName() ||\n+          attr.getName() == op.getFunctionTypeAttrName() ||\n           attr.getName() == \"std.varargs\" ||\n-          (filterArgAttrs &&\n-           attr.getName() == FunctionOpInterface::getArgDictAttrName()))\n+          (filterArgAttrs && attr.getName() == op.getArgAttrsAttrName()))\n         continue;\n       result.push_back(attr);\n     }\n@@ -94,8 +93,8 @@ struct FuncOpConversionBase : public ConvertOpToLLVMPattern<func::FuncOp> {\n               ? resAttrDicts\n               : rewriter.getArrayAttr(\n                     {wrapAsStructAttrs(rewriter, resAttrDicts)});\n-      attributes.push_back(rewriter.getNamedAttr(\n-          FunctionOpInterface::getResultDictAttrName(), newResAttrDicts));\n+      attributes.push_back(\n+          rewriter.getNamedAttr(funcOp.getResAttrsAttrName(), newResAttrDicts));\n     }\n     if (ArrayAttr argAttrDicts = funcOp.getAllArgAttrs()) {\n       SmallVector<Attribute, 4> newArgAttrs(\n@@ -106,9 +105,8 @@ struct FuncOpConversionBase : public ConvertOpToLLVMPattern<func::FuncOp> {\n         for (size_t j = 0; j < mapping->size; ++j)\n           newArgAttrs[mapping->inputNo + j] = argAttrDicts[i];\n       }\n-      attributes.push_back(\n-          rewriter.getNamedAttr(FunctionOpInterface::getArgDictAttrName(),\n-                                rewriter.getArrayAttr(newArgAttrs)));\n+      attributes.push_back(rewriter.getNamedAttr(\n+          funcOp.getArgAttrsAttrName(), rewriter.getArrayAttr(newArgAttrs)));\n     }\n     for (const auto &pair : llvm::enumerate(attributes)) {\n       if (pair.value().getName() == \"llvm.linkage\") {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 32, "deletions": 36, "changes": 68, "file_content_changes": "@@ -1,7 +1,7 @@\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n \n #include \"mlir/Analysis/DataFlowFramework.h\"\n-#include \"mlir/Conversion/ArithmeticToLLVM/ArithmeticToLLVM.h\"\n+#include \"mlir/Conversion/ArithToLLVM/ArithToLLVM.h\"\n #include \"mlir/Conversion/ControlFlowToLLVM//ControlFlowToLLVM.h\"\n #include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n #include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n@@ -83,8 +83,7 @@ struct FuncOpConversion : public FuncOpConversionBase {\n \n     // Set an attribute for maxntidx, it could be used in latter LLVM codegen\n     // for `nvvm.annotation` metadata.\n-    newFuncOp->setAttr(\"nvvm.maxntid\",\n-                       rewriter.getIntegerAttr(i32_ty, 32 * numWarps));\n+    newFuncOp->setAttr(\"nvvm.maxntid\", rewriter.getI32ArrayAttr(32 * numWarps));\n \n     rewriter.eraseOp(funcOp);\n     return success();\n@@ -116,17 +115,15 @@ class ConvertTritonGPUToLLVM\n     // Step 1: Decompose unoptimized layout conversions to use shared memory\n     // Step 2: Decompose insert_slice_async to use load + insert_slice for\n     //   pre-Ampere architectures or unsupported vectorized load sizes\n-    // Step 3: Allocate shared memories and insert barriers\n-    // Step 4: Convert SCF to CFG\n+    // Step 3: Convert SCF to CFG\n+    // Step 4: Allocate shared memories and insert barriers\n     // Step 5: Convert FuncOp to LLVMFuncOp via partial conversion\n     // Step 6: Get axis and shared memory info\n     // Step 7: Convert the rest of ops via partial conversion\n     //\n-    // The reason for putting step 3 before step 4 is that the membar\n-    // analysis currently only supports SCF but not CFG. The reason for a\n-    // separation between 5/7 is that, step 6 is out of the scope of Dialect\n-    // Conversion, thus we need to make sure the smem is not revised during the\n-    // conversion of step 7.\n+    // The reason for a separation between 5/7 is that, step 6 is out of the\n+    // scope of Dialect Conversion, thus we need to make sure the smem is not\n+    // revised during the conversion of step 7.\n \n     // Step 1\n     decomposeMmaToDotOperand(mod, numWarps);\n@@ -137,26 +134,25 @@ class ConvertTritonGPUToLLVM\n       return signalPassFailure();\n \n     // Step 3\n+    RewritePatternSet scfPatterns(context);\n+    mlir::populateSCFToControlFlowConversionPatterns(scfPatterns);\n+    mlir::ConversionTarget scfTarget(*context);\n+    scfTarget.addIllegalOp<scf::ForOp, scf::IfOp, scf::ParallelOp, scf::WhileOp,\n+                           scf::ExecuteRegionOp>();\n+    scfTarget.markUnknownOpDynamicallyLegal([](Operation *) { return true; });\n+    if (failed(applyPartialConversion(mod, scfTarget, std::move(scfPatterns))))\n+      return signalPassFailure();\n+\n+    // Step 4\n     Allocation allocation(mod);\n     MembarAnalysis membarPass(&allocation);\n     membarPass.run();\n \n-    // Step 4\n-    RewritePatternSet scf_patterns(context);\n-    mlir::populateSCFToControlFlowConversionPatterns(scf_patterns);\n-    mlir::ConversionTarget scf_target(*context);\n-    scf_target.addIllegalOp<scf::ForOp, scf::IfOp, scf::ParallelOp,\n-                            scf::WhileOp, scf::ExecuteRegionOp>();\n-    scf_target.markUnknownOpDynamicallyLegal([](Operation *) { return true; });\n-    if (failed(\n-            applyPartialConversion(mod, scf_target, std::move(scf_patterns))))\n-      return signalPassFailure();\n-\n     // Step 5\n-    RewritePatternSet func_patterns(context);\n-    func_patterns.add<FuncOpConversion>(typeConverter, numWarps, /*benefit=*/1);\n+    RewritePatternSet funcPatterns(context);\n+    funcPatterns.add<FuncOpConversion>(typeConverter, numWarps, /*benefit=*/1);\n     if (failed(\n-            applyPartialConversion(mod, funcTarget, std::move(func_patterns))))\n+            applyPartialConversion(mod, funcTarget, std::move(funcPatterns))))\n       return signalPassFailure();\n \n     // Step 6 - get axis and shared memory info\n@@ -209,8 +205,7 @@ class ConvertTritonGPUToLLVM\n                                  /*benefit=*/10);\n \n     // Add arith/math's patterns to help convert scalar expression to LLVM.\n-    mlir::arith::populateArithmeticToLLVMConversionPatterns(typeConverter,\n-                                                            patterns);\n+    mlir::arith::populateArithToLLVMConversionPatterns(typeConverter, patterns);\n     mlir::populateMathToLLVMConversionPatterns(typeConverter, patterns);\n     mlir::cf::populateControlFlowToLLVMConversionPatterns(typeConverter,\n                                                           patterns);\n@@ -244,7 +239,8 @@ class ConvertTritonGPUToLLVM\n     auto global = b.create<LLVM::GlobalOp>(\n         loc, arrayTy, /*isConstant=*/false, LLVM::Linkage::External,\n         \"global_smem\", /*value=*/Attribute(), /*alignment=*/0,\n-        mlir::gpu::GPUDialect::getWorkgroupAddressSpace());\n+        // Add ROCm support.\n+        static_cast<unsigned>(NVVM::NVVMMemorySpace::kSharedMemorySpace));\n     SmallVector<LLVM::LLVMFuncOp> funcs;\n     mod.walk([&](LLVM::LLVMFuncOp func) { funcs.push_back(func); });\n     assert(funcs.size() == 1 &&\n@@ -339,8 +335,8 @@ class ConvertTritonGPUToLLVM\n       OpBuilder builder(insertSliceAsyncOp);\n \n       // Get the vectorized load size\n-      auto src = insertSliceAsyncOp.src();\n-      auto dst = insertSliceAsyncOp.dst();\n+      auto src = insertSliceAsyncOp.getSrc();\n+      auto dst = insertSliceAsyncOp.getDst();\n       auto srcTy = src.getType().cast<RankedTensorType>();\n       auto dstTy = dst.getType().cast<RankedTensorType>();\n       auto srcBlocked =\n@@ -368,24 +364,24 @@ class ConvertTritonGPUToLLVM\n       auto tmpTy =\n           RankedTensorType::get(srcTy.getShape(), resElemTy, srcBlocked);\n       auto loadOp = builder.create<triton::LoadOp>(\n-          insertSliceAsyncOp.getLoc(), tmpTy, insertSliceAsyncOp.src(),\n-          insertSliceAsyncOp.mask(), insertSliceAsyncOp.other(),\n-          insertSliceAsyncOp.cache(), insertSliceAsyncOp.evict(),\n-          insertSliceAsyncOp.isVolatile());\n+          insertSliceAsyncOp.getLoc(), tmpTy, insertSliceAsyncOp.getSrc(),\n+          insertSliceAsyncOp.getMask(), insertSliceAsyncOp.getOther(),\n+          insertSliceAsyncOp.getCache(), insertSliceAsyncOp.getEvict(),\n+          insertSliceAsyncOp.getIsVolatile());\n \n       // insert_slice\n-      auto axis = insertSliceAsyncOp.axis();\n+      auto axis = insertSliceAsyncOp.getAxis();\n       auto intAttr = [&](int64_t v) { return builder.getI64IntegerAttr(v); };\n       auto offsets = SmallVector<OpFoldResult>(dstTy.getRank(), intAttr(0));\n       auto sizes = SmallVector<OpFoldResult>(dstTy.getRank(), intAttr(1));\n       auto strides = SmallVector<OpFoldResult>(dstTy.getRank(), intAttr(1));\n-      offsets[axis] = insertSliceAsyncOp.index();\n+      offsets[axis] = insertSliceAsyncOp.getIndex();\n       for (size_t i = 0; i < dstTy.getRank(); i++) {\n         if (i != axis)\n           sizes[i] = intAttr(dstTy.getShape()[i]);\n       }\n       auto insertSliceOp = builder.create<tensor::InsertSliceOp>(\n-          insertSliceAsyncOp.getLoc(), loadOp, insertSliceAsyncOp.dst(),\n+          insertSliceAsyncOp.getLoc(), loadOp, insertSliceAsyncOp.getDst(),\n           offsets, sizes, strides);\n \n       // Replace"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -144,10 +144,10 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n \n       llvm::errs() << \"Unexpected dot operand layout detected in \"\n                       \"TritonToLLVMTypeConverter\";\n-      return llvm::None;\n+      return std::nullopt;\n     }\n \n-    return llvm::None;\n+    return std::nullopt;\n   }\n };\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.cpp", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -15,8 +15,7 @@ Value getStructFromElements(Location loc, ValueRange resultVals,\n   Value llvmStruct = rewriter.create<LLVM::UndefOp>(loc, structType);\n   for (const auto &v : llvm::enumerate(resultVals)) {\n     assert(v.value() && \"can not insert null values\");\n-    llvmStruct = insert_val(structType, llvmStruct, v.value(),\n-                            rewriter.getI64ArrayAttr(v.index()));\n+    llvmStruct = insert_val(structType, llvmStruct, v.value(), v.index());\n   }\n   return llvmStruct;\n }\n@@ -32,7 +31,7 @@ SmallVector<Value> getElementsFromStruct(Location loc, Value llvmStruct,\n   SmallVector<Value> results(types.size());\n   for (unsigned i = 0; i < types.size(); ++i) {\n     Type type = types[i];\n-    results[i] = extract_val(type, llvmStruct, i64_arr_attr(i));\n+    results[i] = extract_val(type, llvmStruct, i);\n   }\n   return results;\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 6, "deletions": 7, "changes": 13, "file_content_changes": "@@ -43,7 +43,7 @@ struct SplatOpConversion\n   LogicalResult matchAndRewrite(triton::SplatOp op, OpAdaptor adaptor,\n                                 ConversionPatternRewriter &rewriter) const {\n     auto loc = op->getLoc();\n-    auto src = adaptor.src();\n+    auto src = adaptor.getSrc();\n     auto llStruct = convertSplatLikeOp(src.getType(), op.getType(), src,\n                                        getTypeConverter(), rewriter, loc);\n     rewriter.replaceOp(op, {llStruct});\n@@ -109,8 +109,8 @@ struct CatOpConversion : public ConvertTritonGPUOpToLLVMPattern<CatOp> {\n         this->getTypeConverter()->convertType(resultTy.getElementType());\n     SmallVector<Type> types(elems, elemTy);\n     // unpack input values\n-    auto lhsVals = getElementsFromStruct(loc, adaptor.lhs(), rewriter);\n-    auto rhsVals = getElementsFromStruct(loc, adaptor.rhs(), rewriter);\n+    auto lhsVals = getElementsFromStruct(loc, adaptor.getLhs(), rewriter);\n+    auto rhsVals = getElementsFromStruct(loc, adaptor.getRhs(), rewriter);\n     // concatenate (and potentially reorder) values\n     SmallVector<Value> retVals;\n     for (Value v : lhsVals)\n@@ -135,7 +135,7 @@ struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n   LogicalResult\n   matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    // We cannot directly run `rewriter.replaceOp(op, adaptor.src())`\n+    // We cannot directly run `rewriter.replaceOp(op, adaptor.getSrc())`\n     // due to MLIR's restrictions\n     Location loc = op->getLoc();\n     auto resultTy = op.getType().template cast<RankedTensorType>();\n@@ -144,8 +144,7 @@ struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n         this->getTypeConverter()->convertType(resultTy.getElementType());\n     SmallVector<Type> types(elems, elemTy);\n     Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n-    auto vals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n-\n+    auto vals = getElementsFromStruct(loc, adaptor.getSrc(), rewriter);\n     Value view = getStructFromElements(loc, vals, rewriter, structTy);\n     rewriter.replaceOp(op, view);\n     return success();\n@@ -162,7 +161,7 @@ struct TransOpConversion\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n     auto srcSmemObj =\n-        getSharedMemoryObjectFromStruct(loc, adaptor.src(), rewriter);\n+        getSharedMemoryObjectFromStruct(loc, adaptor.getSrc(), rewriter);\n     SmallVector<Value> dstStrides = {srcSmemObj.strides[1],\n                                      srcSmemObj.strides[0]};\n     SmallVector<Value> dstOffsets = {srcSmemObj.offsets[1],"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 44, "deletions": 44, "changes": 88, "file_content_changes": "@@ -1,6 +1,6 @@\n #include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h\"\n \n-#include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n #include \"mlir/Dialect/GPU/IR/GPUDialect.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n@@ -70,30 +70,29 @@ class ArithConstantPattern : public OpConversionPattern<arith::ConstantOp> {\n   }\n };\n \n-class ConvertArithmeticOp : public ConversionPattern {\n+class ConvertArithOp : public ConversionPattern {\n public:\n-  ConvertArithmeticOp(TritonGPUTypeConverter &typeConverter,\n-                      MLIRContext *context)\n+  ConvertArithOp(TritonGPUTypeConverter &typeConverter, MLIRContext *context)\n       : ConversionPattern(typeConverter, MatchAnyOpTypeTag(), /*benefit=*/1,\n                           context) {}\n \n   LogicalResult\n   matchAndRewrite(Operation *op, ArrayRef<Value> operands,\n                   ConversionPatternRewriter &rewriter) const override {\n     Dialect *dialect = op->getDialect();\n-    if (dialect->getTypeID() != mlir::TypeID::get<arith::ArithmeticDialect>())\n+    if (dialect->getTypeID() != mlir::TypeID::get<arith::ArithDialect>())\n       return failure();\n     return success();\n   }\n };\n \n-void populateArithmeticPatternsAndLegality(\n-    TritonGPUTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    TritonGPUConversionTarget &target) {\n+void populateArithPatternsAndLegality(TritonGPUTypeConverter &typeConverter,\n+                                      RewritePatternSet &patterns,\n+                                      TritonGPUConversionTarget &target) {\n   // --------------\n   // Add legality and rewrite pattern rules for operations\n-  // from the Arithmetic dialect. The basic premise is that\n-  // arithmetic operations require both inputs to have the same\n+  // from the Arith dialect. The basic premise is that\n+  // Arith operations require both inputs to have the same\n   // non-null encoding\n   // --------------\n   MLIRContext *context = patterns.getContext();\n@@ -178,7 +177,7 @@ struct TritonMakeRangePattern\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = getTypeConverter()->convertType(op.getType());\n     rewriter.replaceOpWithNewOp<triton::MakeRangeOp>(\n-        op, retType, adaptor.start(), adaptor.end());\n+        op, retType, adaptor.getStart(), adaptor.getEnd());\n     return success();\n   }\n };\n@@ -191,21 +190,22 @@ struct TritonExpandDimsPattern\n   matchAndRewrite(triton::ExpandDimsOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     // Type retType = op.getType());\n-    RankedTensorType argType = adaptor.src().getType().cast<RankedTensorType>();\n+    RankedTensorType argType =\n+        adaptor.getSrc().getType().cast<RankedTensorType>();\n     Attribute _argEncoding = argType.getEncoding();\n     if (!_argEncoding)\n       return failure();\n     auto argEncoding = _argEncoding.cast<triton::gpu::BlockedEncodingAttr>();\n     // return shape\n     auto retShape = argType.getShape().vec();\n-    retShape.insert(retShape.begin() + op.axis(), 1);\n+    retShape.insert(retShape.begin() + op.getAxis(), 1);\n     // return encoding\n     auto retSizePerThread = argEncoding.getSizePerThread().vec();\n-    retSizePerThread.insert(retSizePerThread.begin() + op.axis(), 1);\n+    retSizePerThread.insert(retSizePerThread.begin() + op.getAxis(), 1);\n     auto retThreadsPerWarp = argEncoding.getThreadsPerWarp().vec();\n-    retThreadsPerWarp.insert(retThreadsPerWarp.begin() + op.axis(), 1);\n+    retThreadsPerWarp.insert(retThreadsPerWarp.begin() + op.getAxis(), 1);\n     auto retWarpsPerCTA = argEncoding.getWarpsPerCTA().vec();\n-    retWarpsPerCTA.insert(retWarpsPerCTA.begin() + op.axis(), 1);\n+    retWarpsPerCTA.insert(retWarpsPerCTA.begin() + op.getAxis(), 1);\n     SmallVector<unsigned, 4> retOrder(retShape.size());\n     std::iota(retOrder.begin(), retOrder.end(), 0);\n     triton::gpu::BlockedEncodingAttr retEncoding =\n@@ -214,14 +214,14 @@ struct TritonExpandDimsPattern\n                                               retOrder);\n     // convert operand to slice of return type\n     Attribute newArgEncoding = triton::gpu::SliceEncodingAttr::get(\n-        getContext(), op.axis(), retEncoding);\n+        getContext(), op.getAxis(), retEncoding);\n     RankedTensorType newArgType = RankedTensorType::get(\n         argType.getShape(), argType.getElementType(), newArgEncoding);\n     // construct new op\n     auto newSrc = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        op.getLoc(), newArgType, adaptor.src());\n+        op.getLoc(), newArgType, adaptor.getSrc());\n     rewriter.replaceOpWithNewOp<triton::ExpandDimsOp>(op, newSrc,\n-                                                      adaptor.axis());\n+                                                      adaptor.getAxis());\n     return success();\n   }\n };\n@@ -248,15 +248,15 @@ struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n     RankedTensorType retType =\n         RankedTensorType::get(origShape, origType.getElementType(), dEncoding);\n     // a & b must be of smem layout\n-    auto aType = adaptor.a().getType().cast<RankedTensorType>();\n-    auto bType = adaptor.b().getType().cast<RankedTensorType>();\n+    auto aType = adaptor.getA().getType().cast<RankedTensorType>();\n+    auto bType = adaptor.getB().getType().cast<RankedTensorType>();\n     Attribute aEncoding = aType.getEncoding();\n     Attribute bEncoding = bType.getEncoding();\n     if (!aEncoding || !bEncoding)\n       return failure();\n-    Value a = adaptor.a();\n-    Value b = adaptor.b();\n-    Value c = adaptor.c();\n+    Value a = adaptor.getA();\n+    Value b = adaptor.getB();\n+    Value c = adaptor.getC();\n     if (!aEncoding.isa<triton::gpu::DotOperandEncodingAttr>()) {\n       Attribute encoding =\n           triton::gpu::DotOperandEncodingAttr::get(getContext(), 0, dEncoding);\n@@ -274,7 +274,7 @@ struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n     c = rewriter.create<triton::gpu::ConvertLayoutOp>(c.getLoc(), retType, c);\n \n     rewriter.replaceOpWithNewOp<triton::DotOp>(op, retType, a, b, c,\n-                                               adaptor.allowTF32());\n+                                               adaptor.getAllowTF32());\n     return success();\n   }\n };\n@@ -302,7 +302,7 @@ struct TritonTransPattern : public OpConversionPattern<triton::TransOp> {\n   LogicalResult\n   matchAndRewrite(triton::TransOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    Value src = adaptor.src();\n+    Value src = adaptor.getSrc();\n     auto srcType = src.getType().cast<RankedTensorType>();\n     Attribute srcEncoding = srcType.getEncoding();\n     if (!srcEncoding)\n@@ -334,9 +334,9 @@ struct TritonLoadPattern : public OpConversionPattern<triton::LoadOp> {\n   matchAndRewrite(triton::LoadOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     rewriter.replaceOpWithNewOp<triton::LoadOp>(\n-        op, typeConverter->convertType(op.getType()), adaptor.ptr(),\n-        adaptor.mask(), adaptor.other(), adaptor.cache(), adaptor.evict(),\n-        adaptor.isVolatile());\n+        op, typeConverter->convertType(op.getType()), adaptor.getPtr(),\n+        adaptor.getMask(), adaptor.getOther(), adaptor.getCache(),\n+        adaptor.getEvict(), adaptor.getIsVolatile());\n     return success();\n   }\n };\n@@ -348,8 +348,8 @@ struct TritonStorePattern : public OpConversionPattern<triton::StoreOp> {\n   matchAndRewrite(triton::StoreOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     rewriter.replaceOpWithNewOp<triton::StoreOp>(\n-        op, adaptor.ptr(), adaptor.value(), adaptor.mask(), adaptor.cache(),\n-        adaptor.evict());\n+        op, adaptor.getPtr(), adaptor.getValue(), adaptor.getMask(),\n+        adaptor.getCache(), adaptor.getEvict());\n     return success();\n   }\n };\n@@ -362,8 +362,8 @@ struct TritonAtomicCASPattern\n   matchAndRewrite(triton::AtomicCASOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     rewriter.replaceOpWithNewOp<triton::AtomicCASOp>(\n-        op, typeConverter->convertType(op.getType()), adaptor.ptr(),\n-        adaptor.cmp(), adaptor.val());\n+        op, typeConverter->convertType(op.getType()), adaptor.getPtr(),\n+        adaptor.getCmp(), adaptor.getVal());\n     return success();\n   }\n };\n@@ -376,8 +376,8 @@ struct TritonAtomicRMWPattern\n   matchAndRewrite(triton::AtomicRMWOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     rewriter.replaceOpWithNewOp<triton::AtomicRMWOp>(\n-        op, typeConverter->convertType(op.getType()), adaptor.atomic_rmw_op(),\n-        adaptor.ptr(), adaptor.val(), adaptor.mask());\n+        op, typeConverter->convertType(op.getType()), adaptor.getAtomicRmwOp(),\n+        adaptor.getPtr(), adaptor.getVal(), adaptor.getMask());\n     return success();\n   }\n };\n@@ -390,8 +390,8 @@ struct TritonExtElemwisePattern\n   matchAndRewrite(triton::ExtElemwiseOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     rewriter.replaceOpWithNewOp<triton::ExtElemwiseOp>(\n-        op, typeConverter->convertType(op.getType()), adaptor.args(),\n-        adaptor.libname(), adaptor.libpath(), adaptor.symbol());\n+        op, typeConverter->convertType(op.getType()), adaptor.getArgs(),\n+        adaptor.getLibname(), adaptor.getLibpath(), adaptor.getSymbol());\n     return success();\n   }\n };\n@@ -417,7 +417,7 @@ struct TritonBroadcastPattern\n   LogicalResult\n   matchAndRewrite(BroadcastOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    auto srcType = adaptor.src().getType().cast<RankedTensorType>();\n+    auto srcType = adaptor.getSrc().getType().cast<RankedTensorType>();\n     auto srcEncoding = srcType.getEncoding();\n     if (!srcEncoding)\n       return failure();\n@@ -438,7 +438,7 @@ struct TritonReducePattern : public OpConversionPattern<triton::ReduceOp> {\n   matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     rewriter.replaceOpWithNewOp<triton::ReduceOp>(\n-        op, adaptor.redOp(), adaptor.operand(), adaptor.axis());\n+        op, adaptor.getRedOp(), adaptor.getOperand(), adaptor.getAxis());\n     return success();\n   }\n };\n@@ -449,7 +449,7 @@ struct TritonPrintfPattern : public OpConversionPattern<triton::PrintfOp> {\n   LogicalResult\n   matchAndRewrite(PrintfOp op, typename PrintfOp::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    rewriter.replaceOpWithNewOp<triton::PrintfOp>(op, op.prefixAttr(),\n+    rewriter.replaceOpWithNewOp<triton::PrintfOp>(op, op.getPrefixAttr(),\n                                                   adaptor.getOperands());\n     return success();\n   }\n@@ -501,7 +501,7 @@ struct SCFForPattern : public OpConversionPattern<scf::ForOp> {\n       return rewriter.notifyMatchFailure(op, \"could not convert body types\");\n     }\n     // Change the clone to use the updated operands. We could have cloned with\n-    // a BlockAndValueMapping, but this seems a bit more direct.\n+    // a IRMapping, but this seems a bit more direct.\n     newOp->setOperands(adaptor.getOperands());\n     // Update the result types to the new converted types.\n     SmallVector<Type> newResultTypes;\n@@ -634,8 +634,8 @@ class CFBranchPattern : public OpConversionPattern<cf::BranchOp> {\n   LogicalResult\n   matchAndRewrite(cf::BranchOp op, cf::BranchOp::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    auto newOp = rewriter.replaceOpWithNewOp<cf::BranchOp>(\n-        op, op.getSuccessor(), adaptor.getOperands());\n+    rewriter.replaceOpWithNewOp<cf::BranchOp>(op, op.getSuccessor(),\n+                                              adaptor.getOperands());\n     return success();\n   }\n };\n@@ -687,7 +687,7 @@ class ConvertTritonToTritonGPU\n     RewritePatternSet patterns(context);\n     // add rules\n     populateStdPatternsAndLegality(typeConverter, patterns, target);\n-    populateArithmeticPatternsAndLegality(typeConverter, patterns, target);\n+    populateArithPatternsAndLegality(typeConverter, patterns, target);\n     populateMathPatternsAndLegality(typeConverter, patterns, target);\n     populateTritonPatterns(typeConverter, patterns);\n     // TODO: can we use"}, {"filename": "lib/Dialect/Triton/IR/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -10,7 +10,7 @@ add_mlir_dialect_library(TritonIR\n \n   LINK_LIBS PUBLIC\n   MLIRIR\n-  MLIRArithmeticDialect\n+  MLIRArithDialect\n   MLIRSCFDialect\n   MLIRFuncDialect\n )"}, {"filename": "lib/Dialect/Triton/IR/Dialect.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -22,11 +22,11 @@ namespace {\n struct TritonInlinerInterface : public DialectInlinerInterface {\n   using DialectInlinerInterface::DialectInlinerInterface;\n   bool isLegalToInline(Region *dest, Region *src, bool wouldBeCloned,\n-                       BlockAndValueMapping &valueMapping) const final {\n+                       IRMapping &valueMapping) const final {\n     return true;\n   }\n   bool isLegalToInline(Operation *, Region *, bool wouldBeCloned,\n-                       BlockAndValueMapping &) const final {\n+                       IRMapping &) const final {\n     return true;\n   }\n };"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 13, "deletions": 13, "changes": 26, "file_content_changes": "@@ -65,10 +65,10 @@ ParseResult LoadOp::parse(OpAsmParser &parser, OperationState &result) {\n     return failure();\n   // Deduce operand_segment_sizes from the number of the operands.\n   auto operand_segment_sizesAttrName =\n-      LoadOp::operand_segment_sizesAttrName(result.name);\n+      LoadOp::getOperandSegmentSizesAttrName(result.name);\n   result.addAttribute(\n       operand_segment_sizesAttrName,\n-      parser.getBuilder().getI32VectorAttr({1, hasMask, hasOther}));\n+      parser.getBuilder().getDenseI32ArrayAttr({1, hasMask, hasOther}));\n   return success();\n }\n \n@@ -77,7 +77,7 @@ void LoadOp::print(OpAsmPrinter &printer) {\n   printer << getOperation()->getOperands();\n   // \"operand_segment_sizes\" can be deduced, so we don't print it.\n   printer.printOptionalAttrDict(getOperation()->getAttrs(),\n-                                {operand_segment_sizesAttrName()});\n+                                {getOperandSegmentSizesAttrName()});\n   printer << \" : \";\n   printer.printStrippedAttrOrType(getResult().getType());\n }\n@@ -108,7 +108,7 @@ void StoreOp::print(OpAsmPrinter &printer) {\n   printer << getOperation()->getOperands();\n   printer.printOptionalAttrDict(getOperation()->getAttrs(), /*elidedAttrs=*/{});\n   printer << \" : \";\n-  printer.printStrippedAttrOrType(value().getType());\n+  printer.printStrippedAttrOrType(getValue().getType());\n }\n \n } // namespace triton\n@@ -195,15 +195,15 @@ void LoadOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n     }\n   }\n   state.addAttribute(\n-      operand_segment_sizesAttrName(state.name),\n-      builder.getI32VectorAttr({1, (mask ? 1 : 0), (other ? 1 : 0)}));\n+      getOperandSegmentSizesAttrName(state.name),\n+      builder.getDenseI32ArrayAttr({1, (mask ? 1 : 0), (other ? 1 : 0)}));\n   state.addAttribute(\n-      cacheAttrName(state.name),\n+      getCacheAttrName(state.name),\n       ::mlir::triton::CacheModifierAttr::get(builder.getContext(), cache));\n   state.addAttribute(\n-      evictAttrName(state.name),\n+      getEvictAttrName(state.name),\n       ::mlir::triton::EvictionPolicyAttr::get(builder.getContext(), evict));\n-  state.addAttribute(isVolatileAttrName(state.name),\n+  state.addAttribute(getIsVolatileAttrName(state.name),\n                      builder.getBoolAttr(isVolatile));\n   state.addTypes({resultType});\n }\n@@ -313,8 +313,8 @@ bool mlir::triton::ReduceOp::withIndex(mlir::triton::RedOp redOp) {\n }\n \n //-- SplatOp --\n-OpFoldResult SplatOp::fold(ArrayRef<Attribute> operands) {\n-  auto constOperand = src().getDefiningOp<arith::ConstantOp>();\n+OpFoldResult SplatOp::fold(FoldAdaptor adaptor) {\n+  auto constOperand = getSrc().getDefiningOp<arith::ConstantOp>();\n   if (!constOperand)\n     return {};\n   auto shapedType = getType().cast<ShapedType>();\n@@ -353,8 +353,8 @@ mlir::LogicalResult mlir::triton::ExpandDimsOp::inferReturnTypes(\n }\n \n //-- BroadcastOp --\n-OpFoldResult BroadcastOp::fold(ArrayRef<Attribute> operands) {\n-  auto constOperand = src().getDefiningOp<arith::ConstantOp>();\n+OpFoldResult BroadcastOp::fold(FoldAdaptor adaptor) {\n+  auto constOperand = getSrc().getDefiningOp<arith::ConstantOp>();\n   if (!constOperand)\n     return {};\n "}, {"filename": "lib/Dialect/Triton/Transforms/Combine.cpp", "status": "modified", "additions": 16, "deletions": 13, "changes": 29, "file_content_changes": "@@ -20,8 +20,8 @@ bool isZero(mlir::Value val) {\n     return true;\n   // broadcast(constant_0)\n   if (auto bc = val.getDefiningOp<mlir::triton::BroadcastOp>()) {\n-    if (mlir::matchPattern(bc.src(), mlir::m_Zero()) ||\n-        mlir::matchPattern(bc.src(), mlir::m_AnyZeroFloat()))\n+    if (mlir::matchPattern(bc.getSrc(), mlir::m_Zero()) ||\n+        mlir::matchPattern(bc.getSrc(), mlir::m_AnyZeroFloat()))\n       return true;\n   }\n   return false;\n@@ -48,6 +48,9 @@ DenseElementsAttr getConstantValue(Builder &builder, Attribute value,\n   return res;\n }\n \n+// TODO(csigg): remove after next LLVM integrate.\n+using FastMathFlags = arith::FastMathFlags;\n+\n #include \"TritonCombine.inc\"\n \n } // anonymous namespace\n@@ -76,7 +79,7 @@ class CombineSelectMaskedLoadPattern : public mlir::RewritePattern {\n     if (!loadOp)\n       return mlir::failure();\n \n-    mlir::Value mask = loadOp.mask();\n+    mlir::Value mask = loadOp.getMask();\n     if (!mask)\n       return mlir::failure();\n \n@@ -86,13 +89,13 @@ class CombineSelectMaskedLoadPattern : public mlir::RewritePattern {\n     if (!broadcastOp)\n       return mlir::failure();\n \n-    auto broadcastCond = broadcastOp.src();\n+    auto broadcastCond = broadcastOp.getSrc();\n     if (broadcastCond != condSelect)\n       return mlir::failure();\n \n     rewriter.replaceOpWithNewOp<triton::LoadOp>(\n-        op, loadOp.ptr(), loadOp.mask(), falseValue, loadOp.cache(),\n-        loadOp.evict(), loadOp.isVolatile());\n+        op, loadOp.getPtr(), loadOp.getMask(), falseValue, loadOp.getCache(),\n+        loadOp.getEvict(), loadOp.getIsVolatile());\n     return mlir::success();\n   }\n };\n@@ -107,7 +110,7 @@ struct CanonicalizeMaskedLoadPattern\n   mlir::LogicalResult\n   matchAndRewrite(triton::LoadOp loadOp,\n                   mlir::PatternRewriter &rewriter) const override {\n-    auto mask = loadOp.mask();\n+    auto mask = loadOp.getMask();\n     if (!mask)\n       return mlir::failure();\n \n@@ -123,14 +126,14 @@ struct CanonicalizeMaskedLoadPattern\n     if (splatMask.getSplatValue<IntegerAttr>().getValue() == true) {\n       // mask = splat(1)\n       rewriter.replaceOpWithNewOp<triton::LoadOp>(\n-          loadOp, loadOp.getType(), loadOp.ptr(), Value(), Value(),\n-          loadOp.cache(), loadOp.evict(), loadOp.isVolatile());\n+          loadOp, loadOp.getType(), loadOp.getPtr(), Value(), Value(),\n+          loadOp.getCache(), loadOp.getEvict(), loadOp.getIsVolatile());\n     } else {\n       // mask = splat(0)\n \n       // If there's no \"other\", the value is \"undef\".  Perhaps we want to\n       // optimize it in the future.x\n-      auto otherVal = loadOp.other();\n+      auto otherVal = loadOp.getOther();\n       if (!otherVal)\n         return mlir::failure();\n       rewriter.replaceOp(loadOp, otherVal);\n@@ -154,7 +157,7 @@ struct CanonicalizeMaskedStorePattern\n   mlir::LogicalResult\n   matchAndRewrite(triton::StoreOp storeOp,\n                   mlir::PatternRewriter &rewriter) const override {\n-    auto mask = storeOp.mask();\n+    auto mask = storeOp.getMask();\n     if (!mask)\n       return mlir::failure();\n \n@@ -170,8 +173,8 @@ struct CanonicalizeMaskedStorePattern\n     if (splatMask.getSplatValue<IntegerAttr>().getValue() == true) {\n       // mask = splat(1)\n       rewriter.replaceOpWithNewOp<triton::StoreOp>(\n-          storeOp, storeOp.ptr(), storeOp.value(), storeOp.cache(),\n-          storeOp.evict());\n+          storeOp, storeOp.getPtr(), storeOp.getValue(), storeOp.getCache(),\n+          storeOp.getEvict());\n     } else {\n       // mask = splat(0)\n       rewriter.eraseOp(storeOp);"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.td", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -1,7 +1,7 @@\n #ifndef TRITON_PATTERNS\n #define TRITON_PATTERNS\n \n-include \"mlir/Dialect/Arithmetic/IR/ArithmeticOps.td\"\n+include \"mlir/Dialect/Arith/IR/ArithOps.td\"\n include \"triton/Dialect/Triton/IR/TritonOps.td\"\n include \"mlir/IR/PatternBase.td\"\n \n@@ -16,7 +16,7 @@ def CombineDotAddIPattern : Pat<\n         (TT_DotOp $a, $b, $d, $allowTF32),\n         [(Constraint<CPred<\"isZero($0)\">> $c)]>;\n def CombineDotAddFPattern : Pat<\n-        (Arith_AddFOp $d, (TT_DotOp:$res $a, $b, $c, $allowTF32)),\n+        (Arith_AddFOp $d, (TT_DotOp:$res $a, $b, $c, $allowTF32), $fastmath),\n         (TT_DotOp $a, $b, $d, $allowTF32),\n         [(Constraint<CPred<\"isZero($0)\">> $c)]>;\n \n@@ -25,15 +25,15 @@ def CombineDotAddIRevPattern : Pat<\n         (TT_DotOp $a, $b, $d, $allowTF32),\n         [(Constraint<CPred<\"isZero($0)\">> $c)]>;\n def CombineDotAddFRevPattern : Pat<\n-        (Arith_AddFOp (TT_DotOp:$res $a, $b, $c, $allowTF32), $d),\n+        (Arith_AddFOp (TT_DotOp:$res $a, $b, $c, $allowTF32), $d, $fastmath),\n         (TT_DotOp $a, $b, $d, $allowTF32),\n         [(Constraint<CPred<\"isZero($0)\">> $c)]>;\n \n // TODO: this fails for addptr(addptr(ptr, i32), i64)\n // Commented out until fixed\n // addptr(addptr(%ptr, %idx0), %idx1) => addptr(%ptr, AddI(%idx0, %idx1))\n-//   Note: leave (sub %c0, %c0) canceling to ArithmeticDialect\n-//         (ref: ArithmeticCanonicalization.td)\n+//   Note: leave (sub %c0, %c0) canceling to ArithDialect\n+//         (ref: ArithCanonicalization.td)\n // def CombineAddPtrPattern : Pat<\n //         (TT_AddPtrOp (TT_AddPtrOp $ptr, $idx0), $idx1),\n //         (TT_AddPtrOp $ptr, (Arith_AddIOp $idx0, $idx1))>;"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -688,10 +688,10 @@ ParseResult InsertSliceAsyncOp::parse(OpAsmParser &parser,\n \n   // Deduce operand_segment_sizes from the number of the operands.\n   auto operand_segment_sizesAttrName =\n-      InsertSliceAsyncOp::operand_segment_sizesAttrName(result.name);\n+      InsertSliceAsyncOp::getOperandSegmentSizesAttrName(result.name);\n   result.addAttribute(\n       operand_segment_sizesAttrName,\n-      parser.getBuilder().getI32VectorAttr({1, 1, 1, hasMask, hasOther}));\n+      parser.getBuilder().getDenseI32ArrayAttr({1, 1, 1, hasMask, hasOther}));\n   return success();\n }\n \n@@ -700,11 +700,11 @@ void InsertSliceAsyncOp::print(OpAsmPrinter &printer) {\n   printer << getOperation()->getOperands();\n   // \"operand_segment_sizes\" can be deduced, so we don't print it.\n   printer.printOptionalAttrDict(getOperation()->getAttrs(),\n-                                {operand_segment_sizesAttrName()});\n+                                {getOperandSegmentSizesAttrName()});\n   printer << \" : \";\n-  printer.printStrippedAttrOrType(src().getType());\n+  printer.printStrippedAttrOrType(getSrc().getType());\n   printer << \" -> \";\n-  printer.printStrippedAttrOrType(result().getType());\n+  printer.printStrippedAttrOrType(getResult().getType());\n }\n \n //===----------------------------------------------------------------------===//"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "modified", "additions": 12, "deletions": 14, "changes": 26, "file_content_changes": "@@ -29,9 +29,7 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     size_t rank = origType.getRank();\n     dataflow::Lattice<AxisInfo> *latticeElement =\n         axisInfo.getLatticeElement(ptr);\n-    AxisInfo info = latticeElement && !latticeElement->isUninitialized()\n-                        ? latticeElement->getValue()\n-                        : AxisInfo();\n+    AxisInfo info = latticeElement ? latticeElement->getValue() : AxisInfo();\n     // Get the contiguity order of `ptr`\n     auto order = argSort(info.getContiguity());\n     // The desired divisibility is the maximum divisibility\n@@ -138,15 +136,15 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     op->walk([&](Operation *curr) {\n       Value ptr;\n       if (auto op = dyn_cast<triton::LoadOp>(curr))\n-        ptr = op.ptr();\n+        ptr = op.getPtr();\n       if (auto op = dyn_cast<triton::AtomicRMWOp>(curr))\n-        ptr = op.ptr();\n+        ptr = op.getPtr();\n       if (auto op = dyn_cast<triton::AtomicCASOp>(curr))\n-        ptr = op.ptr();\n+        ptr = op.getPtr();\n       if (auto op = dyn_cast<triton::gpu::InsertSliceAsyncOp>(curr))\n-        ptr = op.src();\n+        ptr = op.getSrc();\n       if (auto op = dyn_cast<triton::StoreOp>(curr))\n-        ptr = op.ptr();\n+        ptr = op.getPtr();\n       if (!ptr)\n         return;\n       RankedTensorType ty = ptr.getType().template dyn_cast<RankedTensorType>();\n@@ -169,24 +167,24 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     op->walk([&](Operation *curr) {\n       OpBuilder builder(curr);\n       if (auto load = dyn_cast<triton::LoadOp>(curr)) {\n-        coalesceOp<triton::LoadOp>(layoutMap, curr, load.ptr(), builder);\n+        coalesceOp<triton::LoadOp>(layoutMap, curr, load.getPtr(), builder);\n         return;\n       }\n       if (auto op = dyn_cast<triton::AtomicRMWOp>(curr)) {\n-        coalesceOp<triton::AtomicRMWOp>(layoutMap, curr, op.ptr(), builder);\n+        coalesceOp<triton::AtomicRMWOp>(layoutMap, curr, op.getPtr(), builder);\n         return;\n       }\n       if (auto op = dyn_cast<triton::AtomicCASOp>(curr)) {\n-        coalesceOp<triton::AtomicCASOp>(layoutMap, curr, op.ptr(), builder);\n+        coalesceOp<triton::AtomicCASOp>(layoutMap, curr, op.getPtr(), builder);\n         return;\n       }\n       if (auto load = dyn_cast<triton::gpu::InsertSliceAsyncOp>(curr)) {\n-        coalesceOp<triton::gpu::InsertSliceAsyncOp>(layoutMap, curr, load.src(),\n-                                                    builder);\n+        coalesceOp<triton::gpu::InsertSliceAsyncOp>(layoutMap, curr,\n+                                                    load.getSrc(), builder);\n         return;\n       }\n       if (auto store = dyn_cast<triton::StoreOp>(curr)) {\n-        coalesceOp<triton::StoreOp>(layoutMap, curr, store.ptr(), builder);\n+        coalesceOp<triton::StoreOp>(layoutMap, curr, store.getPtr(), builder);\n         return;\n       }\n     });"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 38, "deletions": 51, "changes": 89, "file_content_changes": "@@ -1,8 +1,8 @@\n #include \"Utility.h\"\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/SCF/IR/SCF.h\"\n-#include \"mlir/IR/BlockAndValueMapping.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/IRMapping.h\"\n #include \"mlir/IR/Matchers.h\"\n #include \"mlir/IR/PatternMatch.h\"\n #include \"mlir/IR/Verifier.h\"\n@@ -107,7 +107,8 @@ class SimplifyReduceCvt : public mlir::RewritePattern {\n             .isa<triton::gpu::MmaEncodingAttr>())\n       return mlir::failure();\n     auto newReduce = rewriter.create<triton::ReduceOp>(\n-        op->getLoc(), reduce.redOp(), reduceArg.getOperand(), reduce.axis());\n+        op->getLoc(), reduce.getRedOp(), reduceArg.getOperand(),\n+        reduce.getAxis());\n     if (isa<triton::gpu::ConvertLayoutOp>(\n             *reduceArg.getOperand().getDefiningOp()))\n       return mlir::failure();\n@@ -183,12 +184,13 @@ class SimplifyConversion : public mlir::RewritePattern {\n       OpBuilder::InsertionGuard guard(rewriter);\n       rewriter.setInsertionPoint(insert_slice);\n       auto newArg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-          op->getLoc(), newType, insert_slice.dst());\n+          op->getLoc(), newType, insert_slice.getDst());\n       rewriter.replaceOpWithNewOp<triton::gpu::InsertSliceAsyncOp>(\n-          op, newType, insert_slice.src(), newArg.getResult(),\n-          insert_slice.index(), insert_slice.mask(), insert_slice.other(),\n-          insert_slice.cache(), insert_slice.evict(), insert_slice.isVolatile(),\n-          insert_slice.axis());\n+          op, newType, insert_slice.getSrc(), newArg.getResult(),\n+          insert_slice.getIndex(), insert_slice.getMask(),\n+          insert_slice.getOther(), insert_slice.getCache(),\n+          insert_slice.getEvict(), insert_slice.getIsVolatile(),\n+          insert_slice.getAxis());\n       return mlir::success();\n     }\n     // cvt(extract_slice(x), type2) -> extract_slice(cvt(x, type2))\n@@ -197,7 +199,8 @@ class SimplifyConversion : public mlir::RewritePattern {\n       if (!isSharedEncoding(op->getResult(0))) {\n         return mlir::failure();\n       }\n-      auto origType = extract_slice.source().getType().cast<RankedTensorType>();\n+      auto origType =\n+          extract_slice.getSource().getType().cast<RankedTensorType>();\n       auto newType = RankedTensorType::get(\n           origType.getShape(), origType.getElementType(),\n           op->getResult(0).getType().cast<RankedTensorType>().getEncoding());\n@@ -211,7 +214,7 @@ class SimplifyConversion : public mlir::RewritePattern {\n       OpBuilder::InsertionGuard guard(rewriter);\n       rewriter.setInsertionPoint(extract_slice);\n       auto newArg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-          op->getLoc(), newType, extract_slice.source());\n+          op->getLoc(), newType, extract_slice.getSource());\n       rewriter.replaceOpWithNewOp<tensor::ExtractSliceOp>(\n           op, resType, newArg.getResult(), extract_slice.offsets(),\n           extract_slice.sizes(), extract_slice.strides(),\n@@ -244,13 +247,13 @@ class SimplifyConversion : public mlir::RewritePattern {\n     // cvt(type1, splat(type2, x)) -> splat(type1, x)\n     if (auto splat = llvm::dyn_cast<triton::SplatOp>(arg)) {\n       rewriter.replaceOpWithNewOp<triton::SplatOp>(op, op->getResultTypes(),\n-                                                   splat.src());\n+                                                   splat.getSrc());\n       return mlir::success();\n     }\n     // cvt(type1, make_range(type2, x)) -> make_range(type1, x)\n     if (auto range = llvm::dyn_cast<triton::MakeRangeOp>(arg)) {\n       rewriter.replaceOpWithNewOp<triton::MakeRangeOp>(\n-          op, op->getResultTypes(), range.start(), range.end());\n+          op, op->getResultTypes(), range.getStart(), range.getEnd());\n       return mlir::success();\n     }\n     // cvt(type, constant) -> constant\n@@ -275,14 +278,14 @@ LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n   ret = targetEncoding;\n   if (auto expand_dims = dyn_cast<triton::ExpandDimsOp>(op)) {\n     ret = triton::gpu::SliceEncodingAttr::get(\n-        op->getContext(), expand_dims.axis(), targetEncoding);\n+        op->getContext(), expand_dims.getAxis(), targetEncoding);\n   }\n   if (auto reduce = dyn_cast<triton::ReduceOp>(op)) {\n     auto sliceEncoding =\n         targetEncoding.dyn_cast<triton::gpu::SliceEncodingAttr>();\n     if (!sliceEncoding)\n       return failure();\n-    if (sliceEncoding.getDim() != reduce.axis())\n+    if (sliceEncoding.getDim() != reduce.getAxis())\n       return failure();\n     ret = sliceEncoding.getParent();\n   }\n@@ -300,10 +303,10 @@ inline bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n   auto ptr = op->getOperand(0);\n   // Case 2: We assume that `evict_last` loads/stores have high hit rate\n   if (auto load = dyn_cast<triton::LoadOp>(op))\n-    if (load.evict() == triton::EvictionPolicy::EVICT_LAST)\n+    if (load.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n       return false;\n   if (auto store = dyn_cast<triton::StoreOp>(op))\n-    if (store.evict() == triton::EvictionPolicy::EVICT_LAST)\n+    if (store.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n       return false;\n   if (auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>()) {\n     auto encoding = tensorTy.getEncoding();\n@@ -401,7 +404,7 @@ LogicalResult simulateBackwardRematerialization(\n //\n \n Operation *cloneWithInferType(mlir::PatternRewriter &rewriter, Operation *op,\n-                              BlockAndValueMapping &mapping) {\n+                              IRMapping &mapping) {\n   Operation *newOp = rewriter.clone(*op, mapping);\n   auto origType = op->getResult(0).getType().cast<RankedTensorType>();\n   auto argType = newOp->getOperand(0).getType().cast<RankedTensorType>();\n@@ -429,7 +432,7 @@ void pushConversionForward(triton::gpu::ConvertLayoutOp cvt,\n       cvt.getOperand().getType().cast<RankedTensorType>().getEncoding();\n   auto dstEncoding =\n       cvt.getResult().getType().cast<RankedTensorType>().getEncoding();\n-  BlockAndValueMapping mapping;\n+  IRMapping mapping;\n   auto op = cvtSlices.front();\n   for (Value arg : op->getOperands()) {\n     if (arg.getDefiningOp() == cvt)\n@@ -489,7 +492,7 @@ class MoveConvertOutOfIf : public mlir::RewritePattern {\n       newElseYieldOps = elseYield.getOperands();\n     }\n \n-    BlockAndValueMapping mapping;\n+    IRMapping mapping;\n     for (size_t i = 0; i < numOps; i++) {\n       auto thenCvt = dyn_cast<triton::gpu::ConvertLayoutOp>(\n           thenYield.getOperand(i).getDefiningOp());\n@@ -679,7 +682,7 @@ class RematerializeBackward : public mlir::RewritePattern {\n     for (Operation *op : tmp)\n       sortedValues.push_back(op->getResult(0));\n \n-    BlockAndValueMapping mapping;\n+    IRMapping mapping;\n     for (Value currOperand : sortedValues) {\n       // unpack information\n       Attribute targetLayout = toConvert.lookup(currOperand);\n@@ -735,7 +738,7 @@ class MoveConvertOutOfLoop : public mlir::RewritePattern {\n         forOp.getStep(), newInitArgs);\n     newForOp->moveBefore(forOp);\n     rewriter.setInsertionPointToStart(newForOp.getBody());\n-    BlockAndValueMapping mapping;\n+    IRMapping mapping;\n     for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n       mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n     mapping.map(origConversion.getResult(), newForOp.getRegionIterArgs()[i]);\n@@ -1091,25 +1094,11 @@ class BlockedToMMA : public mlir::RewritePattern {\n         oldRetType.getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n       return failure();\n \n-    auto AType = dotOp.getOperand(0).getType().cast<RankedTensorType>();\n-    auto BType = dotOp.getOperand(1).getType().cast<RankedTensorType>();\n-\n     // for FMA, should retain the blocked layout.\n     int versionMajor = computeCapabilityToMMAVersion(computeCapability);\n     if (!supportMMA(dotOp, versionMajor))\n       return failure();\n \n-    auto AOrder = AType.getEncoding()\n-                      .cast<triton::gpu::DotOperandEncodingAttr>()\n-                      .getParent()\n-                      .cast<triton::gpu::BlockedEncodingAttr>()\n-                      .getOrder();\n-    auto BOrder = BType.getEncoding()\n-                      .cast<triton::gpu::DotOperandEncodingAttr>()\n-                      .getParent()\n-                      .cast<triton::gpu::BlockedEncodingAttr>()\n-                      .getOrder();\n-\n     // get MMA encoding for the given number of warps\n     auto retShape = oldRetType.getShape();\n     auto mod = op->getParentOfType<mlir::ModuleOp>();\n@@ -1135,8 +1124,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n     auto oldAcc = dotOp.getOperand(2);\n     auto newAcc = rewriter.create<triton::gpu::ConvertLayoutOp>(\n         oldAcc.getLoc(), newRetType, oldAcc);\n-    Value a = dotOp.a();\n-    Value b = dotOp.b();\n+    Value a = dotOp.getA();\n+    Value b = dotOp.getB();\n     auto oldAType = a.getType().cast<RankedTensorType>();\n     auto oldBType = b.getType().cast<RankedTensorType>();\n     auto oldAOrder = oldAType.getEncoding()\n@@ -1167,8 +1156,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n \n     a = rewriter.create<triton::gpu::ConvertLayoutOp>(a.getLoc(), newAType, a);\n     b = rewriter.create<triton::gpu::ConvertLayoutOp>(b.getLoc(), newBType, b);\n-    auto newDot = rewriter.create<triton::DotOp>(dotOp.getLoc(), newRetType, a,\n-                                                 b, newAcc, dotOp.allowTF32());\n+    auto newDot = rewriter.create<triton::DotOp>(\n+        dotOp.getLoc(), newRetType, a, b, newAcc, dotOp.getAllowTF32());\n \n     rewriter.replaceOpWithNewOp<triton::gpu::ConvertLayoutOp>(\n         op, oldRetType, newDot.getResult());\n@@ -1191,27 +1180,24 @@ class ConvertTransConvert : public mlir::RewritePattern {\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n     auto dstOp = cast<triton::gpu::ConvertLayoutOp>(op);\n-    auto tmpOp = dyn_cast_or_null<triton::TransOp>(dstOp.src().getDefiningOp());\n+    auto tmpOp =\n+        dyn_cast_or_null<triton::TransOp>(dstOp.getSrc().getDefiningOp());\n     if (!tmpOp)\n       return mlir::failure();\n     auto srcOp = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n-        tmpOp.src().getDefiningOp());\n+        tmpOp.getSrc().getDefiningOp());\n     if (!srcOp)\n       return mlir::failure();\n-    auto arg = srcOp.src();\n-    auto X = tmpOp.src();\n-    auto Y = dstOp.src();\n+    auto arg = srcOp.getSrc();\n+    auto X = tmpOp.getSrc();\n     // types\n     auto argType = arg.getType().cast<RankedTensorType>();\n     auto XType = X.getType().cast<RankedTensorType>();\n-    auto YType = Y.getType().cast<RankedTensorType>();\n     auto ZType = dstOp.getResult().getType().cast<RankedTensorType>();\n     // encodings\n     auto argEncoding = argType.getEncoding();\n     auto XEncoding =\n         XType.getEncoding().cast<triton::gpu::SharedEncodingAttr>();\n-    auto YEncoding =\n-        YType.getEncoding().cast<triton::gpu::SharedEncodingAttr>();\n     auto ZEncoding =\n         ZType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n     if (!ZEncoding)\n@@ -1246,7 +1232,8 @@ class ConvertDotConvert : public mlir::RewritePattern {\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n     auto dstOp = cast<triton::gpu::ConvertLayoutOp>(op);\n-    auto dotOp = dyn_cast_or_null<triton::DotOp>(dstOp.src().getDefiningOp());\n+    auto dotOp =\n+        dyn_cast_or_null<triton::DotOp>(dstOp.getSrc().getDefiningOp());\n     if (!dotOp)\n       return mlir::failure();\n     if (std::distance(dstOp->user_begin(), dstOp->user_end()) != 1 ||\n@@ -1256,7 +1243,8 @@ class ConvertDotConvert : public mlir::RewritePattern {\n         dotOp.getOperand(2).getDefiningOp());\n     if (!cvtOp)\n       return mlir::failure();\n-    auto loadOp = dyn_cast_or_null<triton::LoadOp>(cvtOp.src().getDefiningOp());\n+    auto loadOp =\n+        dyn_cast_or_null<triton::LoadOp>(cvtOp.getSrc().getDefiningOp());\n     if (!loadOp)\n       return mlir::failure();\n     auto dstTy = dstOp.getResult().getType().cast<RankedTensorType>();\n@@ -1271,11 +1259,10 @@ class ConvertDotConvert : public mlir::RewritePattern {\n         op->getLoc(), dotOp.getResult().getType(), _0f);\n     auto newDot = rewriter.create<triton::DotOp>(\n         op->getLoc(), dotOp.getResult().getType(), dotOp.getOperand(0),\n-        dotOp.getOperand(1), _0, dotOp.allowTF32());\n+        dotOp.getOperand(1), _0, dotOp.getAllowTF32());\n     auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n         op->getLoc(), dstTy, newDot.getResult());\n-    auto newAdd = rewriter.replaceOpWithNewOp<arith::AddFOp>(\n-        op, newCvt, cvtOp.getOperand());\n+    rewriter.replaceOpWithNewOp<arith::AddFOp>(op, newCvt, cvtOp.getOperand());\n     return mlir::success();\n   }\n };"}, {"filename": "lib/Dialect/TritonGPU/Transforms/DecomposeConversions.cpp", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -1,7 +1,7 @@\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/SCF/IR/SCF.h\"\n-#include \"mlir/IR/BlockAndValueMapping.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/IRMapping.h\"\n #include \"mlir/IR/Matchers.h\"\n #include \"mlir/IR/PatternMatch.h\"\n #include \"mlir/IR/Verifier.h\"\n@@ -28,7 +28,6 @@ class TritonGPUDecomposeConversionsPass\n   TritonGPUDecomposeConversionsPass() = default;\n \n   void runOnOperation() override {\n-    MLIRContext *context = &getContext();\n     ModuleOp mod = getOperation();\n     mod.walk([&](triton::gpu::ConvertLayoutOp cvtOp) -> void {\n       OpBuilder builder(cvtOp);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 15, "deletions": 15, "changes": 30, "file_content_changes": "@@ -1,5 +1,5 @@\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n-#include \"mlir/IR/BlockAndValueMapping.h\"\n+#include \"mlir/IR/IRMapping.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n #include \"triton/Analysis/Utility.h\"\n@@ -171,7 +171,7 @@ LogicalResult LoopPipeliner::initialize() {\n   SmallVector<triton::LoadOp, 2> allLoads;\n   for (Operation &op : *loop)\n     if (auto loadOp = dyn_cast<triton::LoadOp>(&op)) {\n-      auto ptr = loadOp.ptr();\n+      auto ptr = loadOp.getPtr();\n       unsigned vec = axisInfoAnalysis->getPtrContiguity(ptr);\n       auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n       if (!tensorTy)\n@@ -318,7 +318,7 @@ void LoopPipeliner::emitPrologue() {\n         }\n         // load => copy async\n         if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(op)) {\n-          Value mask = lookupOrDefault(loadOp.mask(), stage);\n+          Value mask = lookupOrDefault(loadOp.getMask(), stage);\n           Value newMask;\n           if (mask) {\n             Value splatCond = builder.create<triton::SplatOp>(\n@@ -332,10 +332,10 @@ void LoopPipeliner::emitPrologue() {\n           // TODO: check if the hardware supports async copy\n           newOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n               op->getLoc(), loadsBuffer[loadOp].getType(),\n-              lookupOrDefault(loadOp.ptr(), stage),\n+              lookupOrDefault(loadOp.getPtr(), stage),\n               loadStageBuffer[loadOp][stage], pipelineIterIdx, newMask,\n-              lookupOrDefault(loadOp.other(), stage), loadOp.cache(),\n-              loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n+              lookupOrDefault(loadOp.getOther(), stage), loadOp.getCache(),\n+              loadOp.getEvict(), loadOp.getIsVolatile(), /*axis*/ 0);\n           builder.create<triton::gpu::AsyncCommitGroupOp>(op->getLoc());\n           loadStageBuffer[loadOp].push_back(newOp->getResult(0));\n         } else\n@@ -464,7 +464,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n \n   // 2. body of the new ForOp\n   builder.setInsertionPointToStart(newForOp.getBody());\n-  BlockAndValueMapping mapping;\n+  IRMapping mapping;\n   for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n     mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n   mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n@@ -501,7 +501,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   }\n   assert(depOps.size() + loads.size() == orderedDeps.size() &&\n          \"depOps contains invalid values\");\n-  BlockAndValueMapping nextMapping;\n+  IRMapping nextMapping;\n   DenseMap<BlockArgument, Value> depArgsMapping;\n   size_t argIdx = 0;\n   for (BlockArgument arg : depArgs) {\n@@ -559,7 +559,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n     // Update loading mask\n     if (loads.contains(op->getResult(0))) {\n       auto loadOp = llvm::cast<triton::LoadOp>(op);\n-      Value mask = loadOp.mask();\n+      Value mask = loadOp.getMask();\n       Value newMask;\n       if (mask) {\n         Value splatCond = builder.create<triton::SplatOp>(\n@@ -570,17 +570,17 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n         // once\n         if (!(forOp.isDefinedOutsideOfLoop(mask) && nextMapping.contains(mask)))\n           nextMapping.map(mask, newMask);\n-        newMask = nextMapping.lookupOrDefault(loadOp.mask());\n+        newMask = nextMapping.lookupOrDefault(loadOp.getMask());\n       } else\n         newMask = builder.create<triton::SplatOp>(\n             loadOp.getLoc(), getI1SameShape(loadOp), nextLoopCond);\n       Value insertAsyncOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n           op->getLoc(), loadsBuffer[loadOp].getType(),\n-          nextMapping.lookupOrDefault(loadOp.ptr()),\n+          nextMapping.lookupOrDefault(loadOp.getPtr()),\n           newForOp.getRegionIterArgs()[bufferIdx + nextBuffers.size()],\n           insertSliceIndex, newMask,\n-          nextMapping.lookupOrDefault(loadOp.other()), loadOp.cache(),\n-          loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n+          nextMapping.lookupOrDefault(loadOp.getOther()), loadOp.getCache(),\n+          loadOp.getEvict(), loadOp.getIsVolatile(), /*axis*/ 0);\n       builder.create<triton::gpu::AsyncCommitGroupOp>(op->getLoc());\n       nextBuffers.push_back(insertAsyncOp);\n       // ExtractSlice\n@@ -624,8 +624,8 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n       if (auto dotOp = llvm::dyn_cast<triton::DotOp>(&op)) {\n         builder.setInsertionPoint(&op);\n         auto dotType = dotOp.getType().cast<RankedTensorType>();\n-        Value a = dotOp.a();\n-        Value b = dotOp.b();\n+        Value a = dotOp.getA();\n+        Value b = dotOp.getB();\n         auto layoutCast = [&](Value dotOperand, int opIdx) -> Value {\n           auto tensorType = dotOperand.getType().cast<RankedTensorType>();\n           if (!tensorType.getEncoding().isa<ttg::DotOperandEncodingAttr>()) {"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 21, "deletions": 24, "changes": 45, "file_content_changes": "@@ -26,7 +26,7 @@\n // }\n //===----------------------------------------------------------------------===//\n \n-#include \"mlir/IR/BlockAndValueMapping.h\"\n+#include \"mlir/IR/IRMapping.h\"\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n@@ -63,8 +63,8 @@ class Prefetcher {\n \n   Value generatePrefetch(Value v, unsigned opIdx, bool isPrologue,\n                          Attribute dotEncoding, OpBuilder &builder,\n-                         llvm::Optional<int64_t> offsetK = llvm::None,\n-                         llvm::Optional<int64_t> shapeK = llvm::None);\n+                         llvm::Optional<int64_t> offsetK = std::nullopt,\n+                         llvm::Optional<int64_t> shapeK = std::nullopt);\n \n public:\n   Prefetcher() = delete;\n@@ -140,7 +140,7 @@ LogicalResult Prefetcher::initialize() {\n   auto getPrefetchSrc = [](Value v) -> Value {\n     if (auto cvt = v.getDefiningOp<triton::gpu::ConvertLayoutOp>())\n       if (isSharedEncoding(cvt.getOperand()))\n-        return cvt.src();\n+        return cvt.getSrc();\n     return Value();\n   };\n \n@@ -158,18 +158,18 @@ LogicalResult Prefetcher::initialize() {\n   };\n \n   for (triton::DotOp dot : dotsInFor) {\n-    auto kSize = dot.a().getType().cast<RankedTensorType>().getShape()[1];\n+    auto kSize = dot.getA().getType().cast<RankedTensorType>().getShape()[1];\n \n     // works better with nvidia tensor cores\n     unsigned elementWidth =\n-        dot.a().getType().cast<RankedTensorType>().getElementTypeBitWidth();\n+        dot.getA().getType().cast<RankedTensorType>().getElementTypeBitWidth();\n     prefetchWidth = 256 / elementWidth;\n \n     // Skip prefetching if kSize is less than prefetchWidth\n     if (kSize < prefetchWidth)\n       continue;\n-    Value aSmem = getPrefetchSrc(dot.a());\n-    Value bSmem = getPrefetchSrc(dot.b());\n+    Value aSmem = getPrefetchSrc(dot.getA());\n+    Value bSmem = getPrefetchSrc(dot.getB());\n     if (aSmem && bSmem) {\n       Value aHeaderDef = getIncomingOp(aSmem);\n       Value bHeaderDef = getIncomingOp(bSmem);\n@@ -197,10 +197,12 @@ void Prefetcher::emitPrologue() {\n         dot.getType().cast<RankedTensorType>().getEncoding();\n     Value aPrefetched =\n         generatePrefetch(dot2aHeaderDef[dot], 0, true, dotEncoding, builder);\n-    operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().a()] = aPrefetched;\n+    operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().getA()] =\n+        aPrefetched;\n     Value bPrefetched =\n         generatePrefetch(dot2bHeaderDef[dot], 1, true, dotEncoding, builder);\n-    operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().b()] = bPrefetched;\n+    operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().getB()] =\n+        bPrefetched;\n   }\n }\n \n@@ -212,46 +214,41 @@ scf::ForOp Prefetcher::createNewForOp() {\n     loopArgs.push_back(v);\n   for (Value dot : dots) {\n     loopArgs.push_back(\n-        operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().a()]);\n+        operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().getA()]);\n     loopArgs.push_back(\n-        operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().b()]);\n+        operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().getB()]);\n   }\n \n   auto newForOp = builder.create<scf::ForOp>(\n       forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n       forOp.getStep(), loopArgs);\n \n-  auto largestPow2 = [](int64_t n) -> int64_t {\n-    while ((n & (n - 1)) != 0)\n-      n = n & (n - 1);\n-    return n;\n-  };\n-\n   builder.setInsertionPointToStart(newForOp.getBody());\n-  BlockAndValueMapping mapping;\n+  IRMapping mapping;\n   for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n     mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n   mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n \n   for (Operation &op : forOp.getBody()->without_terminator()) {\n     Operation *newOp = builder.clone(op, mapping);\n     auto dot = dyn_cast<triton::DotOp>(&op);\n-    if (dots.contains(dot)) {\n+    if (dot && dots.contains(dot)) {\n       Attribute dotEncoding =\n           dot.getType().cast<RankedTensorType>().getEncoding();\n       // prefetched dot\n       Operation *firstDot = builder.clone(*dot, mapping);\n-      if (Value a = operand2headPrefetch.lookup(dot.a()))\n+      if (Value a = operand2headPrefetch.lookup(dot.getA()))\n         firstDot->setOperand(\n             0, newForOp.getRegionIterArgForOpOperand(*a.use_begin()));\n-      if (Value b = operand2headPrefetch.lookup(dot.b()))\n+      if (Value b = operand2headPrefetch.lookup(dot.getB()))\n         firstDot->setOperand(\n             1, newForOp.getRegionIterArgForOpOperand(*b.use_begin()));\n \n       // remaining part\n       int64_t kOff = prefetchWidth;\n-      int64_t kRem = dot.a().getType().cast<RankedTensorType>().getShape()[1] -\n-                     prefetchWidth;\n+      int64_t kRem =\n+          dot.getA().getType().cast<RankedTensorType>().getShape()[1] -\n+          prefetchWidth;\n       Operation *prevDot = firstDot;\n       while (kRem != 0) {\n         // int64_t kShape = largestPow2(kRem);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/ReorderInstructions.cpp", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -1,7 +1,7 @@\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/SCF/IR/SCF.h\"\n-#include \"mlir/IR/BlockAndValueMapping.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/IRMapping.h\"\n #include \"mlir/IR/Matchers.h\"\n #include \"mlir/IR/PatternMatch.h\"\n #include \"mlir/IR/Verifier.h\"\n@@ -41,7 +41,6 @@ class TritonGPUReorderInstructionsPass\n   TritonGPUReorderInstructionsPass() = default;\n \n   void runOnOperation() override {\n-    MLIRContext *context = &getContext();\n     ModuleOp m = getOperation();\n     // Sink conversions into loops when they will increase\n     // register pressure"}, {"filename": "lib/Dialect/TritonGPU/Transforms/TritonGPUConversion.cpp", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -1,5 +1,5 @@\n #include \"triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h\"\n-#include \"mlir/IR/BlockAndValueMapping.h\"\n+#include \"mlir/IR/IRMapping.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include <algorithm>\n@@ -43,15 +43,15 @@ TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n                                  RankedTensorType tensorType, ValueRange inputs,\n                                  Location loc) {\n     llvm_unreachable(\"Argument rematerialization not implemented\");\n-    return llvm::None;\n+    return std::nullopt;\n   });\n \n   // If the origValue still has live user(s), use this to\n   // convert origValue to newValue\n   addSourceMaterialization([&](OpBuilder &builder, RankedTensorType tensorType,\n                                ValueRange inputs, Location loc) {\n     llvm_unreachable(\"Source rematerialization not implemented\");\n-    return llvm::None;\n+    return std::nullopt;\n   });\n \n   // This will be called when (desiredType != newOperandType)\n@@ -64,7 +64,7 @@ TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n     return Optional<Value>(cast.getResult());\n     // return Optional<Value>(cast.getResult(0));\n     // llvm_unreachable(\"Not implemented\");\n-    // return llvm::None;\n+    // return std::nullopt;\n   });\n }\n \n@@ -81,7 +81,7 @@ TritonGPUConversionTarget::TritonGPUConversionTarget(\n   addIllegalOp<scf::ExecuteRegionOp, scf::ParallelOp, scf::ReduceOp,\n                scf::ReduceReturnOp>();\n \n-  addDynamicallyLegalDialect<arith::ArithmeticDialect, math::MathDialect,\n+  addDynamicallyLegalDialect<arith::ArithDialect, math::MathDialect,\n                              triton::TritonDialect, scf::SCFDialect>(\n       [&](Operation *op) {\n         if (typeConverter.isLegal(op))\n@@ -92,9 +92,9 @@ TritonGPUConversionTarget::TritonGPUConversionTarget(\n   // We have requirements for the data layouts\n   addDynamicallyLegalOp<triton::DotOp>([](triton::DotOp dotOp) -> bool {\n     Attribute aEncoding =\n-        dotOp.a().getType().cast<RankedTensorType>().getEncoding();\n+        dotOp.getA().getType().cast<RankedTensorType>().getEncoding();\n     Attribute bEncoding =\n-        dotOp.b().getType().cast<RankedTensorType>().getEncoding();\n+        dotOp.getB().getType().cast<RankedTensorType>().getEncoding();\n     if (aEncoding && aEncoding.isa<triton::gpu::DotOperandEncodingAttr>() &&\n         bEncoding && bEncoding.isa<triton::gpu::DotOperandEncodingAttr>())\n       return true;"}, {"filename": "lib/Dialect/TritonGPU/Transforms/UpdateMmaForVolta.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -57,9 +57,9 @@ LogicalResult getOptimizedV100MMaLayout(triton::DotOp dotOp,\n                                         MmaEncodingAttr &old,\n                                         MmaEncodingAttr &ret) {\n   auto *ctx = dotOp->getContext();\n-  auto AT = dotOp.a().getType().cast<RankedTensorType>();\n-  auto BT = dotOp.b().getType().cast<RankedTensorType>();\n-  auto DT = dotOp.d().getType().cast<RankedTensorType>();\n+  auto AT = dotOp.getA().getType().cast<RankedTensorType>();\n+  auto BT = dotOp.getB().getType().cast<RankedTensorType>();\n+  auto DT = dotOp.getD().getType().cast<RankedTensorType>();\n   auto shapeA = AT.getShape();\n   auto shapeB = BT.getShape();\n   if (!DT.getEncoding())"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,6 +1,6 @@\n #include \"Utility.h\"\n #include \"mlir/Dialect/SCF/IR/SCF.h\"\n-#include \"mlir/IR/BlockAndValueMapping.h\"\n+#include \"mlir/IR/IRMapping.h\"\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n \n namespace mlir {\n@@ -36,7 +36,7 @@ class FixupLoop : public mlir::RewritePattern {\n         forOp.getStep(), newInitArgs);\n     newForOp->moveBefore(forOp);\n     rewriter.setInsertionPointToStart(newForOp.getBody());\n-    BlockAndValueMapping mapping;\n+    IRMapping mapping;\n     for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n       mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n     mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 35, "deletions": 11, "changes": 46, "file_content_changes": "@@ -14,20 +14,24 @@\n #include \"mlir/Transforms/Passes.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n #include \"triton/Tools/Sys/GetEnv.hpp\"\n+#include \"llvm/ADT/APInt.h\"\n+#include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/IR/Constants.h\"\n #include \"llvm/IRReader/IRReader.h\"\n #include \"llvm/Linker/Linker.h\"\n #include \"llvm/Support/SourceMgr.h\"\n #include <dlfcn.h>\n #include <filesystem>\n+#include <iterator>\n \n namespace mlir {\n namespace triton {\n \n // Describes NVVM Metadata. It is used to record the nvvm related meta\n // information from mlir module.\n struct NVVMMetadata {\n-  int maxntidx{-1};\n+  SmallVector<int, 3> maxntid;\n   bool isKernel{};\n   // Free to extend with other information.\n };\n@@ -37,13 +41,26 @@ static void amendLLVMFunc(llvm::Function *func, const NVVMMetadata &metadata) {\n   auto *module = func->getParent();\n   auto &ctx = func->getContext();\n \n-  if (metadata.maxntidx > 0) {\n-    auto warps = llvm::ConstantInt::get(llvm::IntegerType::get(ctx, 32),\n-                                        llvm::APInt(32, metadata.maxntidx));\n-\n-    llvm::Metadata *md_args[] = {llvm::ValueAsMetadata::get(func),\n-                                 llvm::MDString::get(ctx, \"maxntidx\"),\n-                                 llvm::ValueAsMetadata::get(warps)};\n+  if (!metadata.maxntid.empty()) {\n+    auto maxntid =\n+        llvm::to_vector(llvm::map_range(metadata.maxntid, [&](int value) {\n+          return llvm::ConstantInt::get(llvm::IntegerType::get(ctx, 32),\n+                                        llvm::APInt(32, value));\n+        }));\n+\n+    SmallVector<llvm::Metadata *> md_args = {llvm::ValueAsMetadata::get(func)};\n+    if (maxntid.size() > 0) {\n+      md_args.push_back(llvm::MDString::get(ctx, \"maxntidx\"));\n+      md_args.push_back(llvm::ValueAsMetadata::get(maxntid[0]));\n+    }\n+    if (maxntid.size() > 1) {\n+      md_args.push_back(llvm::MDString::get(ctx, \"maxntidy\"));\n+      md_args.push_back(llvm::ValueAsMetadata::get(maxntid[1]));\n+    }\n+    if (maxntid.size() > 2) {\n+      md_args.push_back(llvm::MDString::get(ctx, \"maxntidz\"));\n+      md_args.push_back(llvm::ValueAsMetadata::get(maxntid[2]));\n+    }\n \n     module->getOrInsertNamedMetadata(\"nvvm.annotations\")\n         ->addOperand(llvm::MDNode::get(ctx, md_args));\n@@ -68,9 +85,10 @@ extractNVVMMetadata(mlir::ModuleOp module,\n     bool hasMetadata{};\n \n     // maxntid\n-    if (op->hasAttr(\"nvvm.maxntid\")) {\n-      auto attr = op->getAttr(\"nvvm.maxntid\");\n-      meta.maxntidx = attr.dyn_cast<IntegerAttr>().getInt();\n+    if (auto attr = op->getAttrOfType<ArrayAttr>(\"nvvm.maxntid\")) {\n+      llvm::transform(attr.getAsValueRange<IntegerAttr>(),\n+                      std::back_inserter(meta.maxntid),\n+                      [](llvm::APInt value) { return value.getZExtValue(); });\n       hasMetadata = true;\n     }\n \n@@ -118,6 +136,12 @@ static std::map<std::string, std::string> getExternLibs(mlir::ModuleOp module) {\n \n   if (!funcs.empty()) {\n     static const std::string libdevice = \"libdevice\";\n+    // first search for environmental path\n+    std::string env_path = ::triton::tools::getenv(\"TRITON_LIBDEVICE_PATH\");\n+    if (!env_path.empty()) {\n+      externLibs.try_emplace(libdevice, env_path);\n+      return externLibs;\n+    }\n     namespace fs = std::filesystem;\n     // Search for libdevice relative to its library path if used from Python\n     // Then native code is in `triton/_C/libtriton.so` and libdevice in"}, {"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -75,7 +75,7 @@ std::string translateLLVMIRToPTX(llvm::Module &module, int cc, int version) {\n   opt.NoNaNsFPMath = true;\n   llvm::TargetMachine *machine = target->createTargetMachine(\n       module.getTargetTriple(), proc, features, opt, llvm::Reloc::PIC_,\n-      llvm::None, llvm::CodeGenOpt::Aggressive);\n+      std::nullopt, llvm::CodeGenOpt::Aggressive);\n   // set data layout\n   if (layout.empty())\n     module.setDataLayout(machine->createDataLayout());"}, {"filename": "python/setup.py", "status": "modified", "additions": 8, "deletions": 4, "changes": 12, "file_content_changes": "@@ -65,11 +65,11 @@ def get_llvm_package_info():\n         linux_suffix = 'ubuntu-18.04' if vglibc > 217 else 'centos-7'\n         system_suffix = f\"linux-gnu-{linux_suffix}\"\n     else:\n-        raise RuntimeError(f\"unsupported system: {system}\")\n+        return Package(\"llvm\", \"LLVM-C.lib\", \"\", \"lib\", \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n     use_assert_enabled_llvm = check_env_flag(\"TRITON_USE_ASSERT_ENABLED_LLVM\", \"False\")\n     release_suffix = \"assert\" if use_assert_enabled_llvm else \"release\"\n-    name = f'llvm+mlir-15.0.7-x86_64-{system_suffix}-{release_suffix}'\n-    url = f\"https://github.com/ptillet/triton-llvm-releases/releases/download/llvm-15.0.7-8dfdcc7b7bf6/{name}.tar.xz\"\n+    name = f'llvm+mlir-17.0.0-x86_64-{system_suffix}-{release_suffix}'\n+    url = f\"https://github.com/ptillet/triton-llvm-releases/releases/download/llvm-17.0.0-37b7a60cd74b/{name}.tar.xz\"\n     return Package(\"llvm\", name, url, \"lib\", \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n \n \n@@ -159,7 +159,11 @@ def run(self):\n \n     def build_extension(self, ext):\n         lit_dir = shutil.which('lit')\n-        triton_cache_path = os.path.join(os.environ[\"HOME\"], \".triton\")\n+        user_home = os.getenv(\"HOME\") or os.getenv(\"USERPROFILE\") or \\\n+            os.getenv(\"HOMEPATH\") or None\n+        if not user_home:\n+            raise RuntimeError(\"Could not find user home directory\")\n+        triton_cache_path = os.path.join(user_home, \".triton\")\n         # lit is used by the test suite\n         thirdparty_cmake_args = get_thirdparty_packages(triton_cache_path)\n         extdir = os.path.abspath(os.path.dirname(self.get_ext_fullpath(ext.path)))"}, {"filename": "python/src/main.cc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n-\ufeff#include <pybind11/pybind11.h>\n+#include <pybind11/pybind11.h>\n \n void init_superblocking(pybind11::module &m);\n void init_torch_utils(pybind11::module &m);"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -11,6 +11,7 @@\n #include \"mlir/Parser/Parser.h\"\n #include \"mlir/Support/FileUtilities.h\"\n \n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlow.h\"\n #include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"triton/Analysis/Allocation.h\"\n@@ -390,8 +391,9 @@ void init_triton_ir(py::module &&m) {\n         mlir::DialectRegistry registry;\n         registry.insert<mlir::triton::TritonDialect,\n                         mlir::triton::gpu::TritonGPUDialect,\n-                        mlir::math::MathDialect, mlir::arith::ArithmeticDialect,\n-                        mlir::func::FuncDialect, mlir::scf::SCFDialect>();\n+                        mlir::math::MathDialect, mlir::arith::ArithDialect,\n+                        mlir::func::FuncDialect, mlir::scf::SCFDialect,\n+                        mlir::cf::ControlFlowDialect>();\n         context.appendDialectRegistry(registry);\n         context.loadAllAvailableDialects();\n "}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 3, "deletions": 6, "changes": 9, "file_content_changes": "@@ -1315,11 +1315,6 @@ def default_cache_dir():\n     return os.path.join(os.environ[\"HOME\"], \".triton\", \"cache\")\n \n \n-def default_cuda_dir():\n-    default_dir = \"/usr/local/cuda\"\n-    return os.getenv(\"CUDA_HOME\", default=default_dir)\n-\n-\n class CacheManager:\n \n     def __init__(self, key):\n@@ -1377,7 +1372,9 @@ def quiet():\n \n def _build(name, src, srcdir):\n     cuda_lib_dirs = libcuda_dirs()\n-    cuda_path = os.environ.get('CUDA_PATH', default_cuda_dir())\n+    base_dir = os.path.dirname(__file__)\n+    cuda_path = os.path.join(base_dir, \"third_party\", \"cuda\")\n+\n     cu_include_dir = os.path.join(cuda_path, \"include\")\n     triton_include_dir = os.path.join(os.path.dirname(__file__), \"include\")\n     cuda_header = os.path.join(cu_include_dir, \"cuda.h\")"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -813,9 +813,8 @@ def view(input, shape, _builder=None):\n \n @builtin\n def reshape(input, shape, _builder=None):\n-    # TODO: should be more than just a view\n     shape = _shape_check_impl(shape)\n-    return semantic.view(input, shape, _builder)\n+    return semantic.reshape(input, shape, _builder)\n \n # -----------------------\n # Linear Algebra"}, {"filename": "python/triton/language/libdevice.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -3,7 +3,8 @@\n from .. import impl\n from . import core, extern\n \n-LIBDEVICE_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"third_party\", \"cuda\", \"lib\", \"libdevice.10.bc\")\n+LOCAL_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"third_party\", \"cuda\", \"lib\", \"libdevice.10.bc\")\n+LIBDEVICE_PATH = os.getenv(\"TRITON_LIBDEVICE_PATH\", LOCAL_PATH)\n \n \n @impl.extern"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -514,6 +514,13 @@ def view(input: tl.tensor,\n     return tl.tensor(builder.create_view(input.handle, dst_shape), ret_ty)\n \n \n+def reshape(input: tl.tensor,\n+            dst_shape: List[int],\n+            builder: ir.builder) -> tl.tensor:\n+    raise ValueError(\"`reshape` is not supported yet. Please use `view` instead if applicable. \"\n+                     \"Note that view may reorder elements in an implementation- and context- dependent way.\")\n+\n+\n def expand_dims(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n     dst_shape = list(input.type.shape)\n     dst_shape.insert(axis, 1)\n@@ -640,6 +647,8 @@ def cast(input: tl.tensor,\n          dst_ty: tl.dtype,\n          builder: ir.builder) -> tl.tensor:\n     src_ty = input.type\n+    if isinstance(dst_ty, tl.constexpr):\n+        dst_ty = dst_ty.value\n     if src_ty.is_block():\n         dst_ty = tl.block_type(dst_ty, input.type.get_block_shapes())\n     if src_ty == dst_ty:"}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -62,9 +62,9 @@ def kernel_call():\n             self.hook(args)\n             self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **current)\n         try:\n-            return do_bench(kernel_call)\n+            return do_bench(kernel_call, percentiles=(0.5, 0.2, 0.8))\n         except OutOfResources:\n-            return float('inf')\n+            return (float('inf'), float('inf'), float('inf'))\n \n     def run(self, *args, **kwargs):\n         self.nargs = dict(zip(self.arg_names, args))"}, {"filename": "python/triton/tools/build_extern.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -289,7 +289,8 @@ def _output_stubs(self) -> str:\n         #   return extern.dispatch(\"libdevice\", <path>, <args>, <arg_type_symbol_dict>, _builder)\n         import_str = \"from . import core, extern\\n\"\n         import_str += \"import os\\n\"\n-        header_str = \"LIBDEVICE_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \\\"..\\\", \\\"third_party\\\", \\\"cuda\\\", \\\"lib\\\", \\\"libdevice.10.bc\\\")\"\n+        header_str = \"LOCAL_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \\\"..\\\", \\\"third_party\\\", \\\"cuda\\\", \\\"lib\\\", \\\"libdevice.10.bc\\\")\\n\"\n+        header_str += \"LIBDEVICE_PATH = os.getenv(\\\"TRITON_LIBDEVICE_PATH\\\", LOCAL_PATH)\\n\"\n         func_str = \"\"\n         for symbols in self._symbol_groups.values():\n             func_str += \"@extern.extern\\n\""}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 36, "deletions": 5, "changes": 41, "file_content_changes": "@@ -84,7 +84,7 @@ func.func @insert_slice(%A : !tt.ptr<f16>, %i1 : i1) {\n   %tensor = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : index\n   %a = tt.load %a_ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #AL>\n-  // CHECK: %3 -> %cst_0\n+  // CHECK: %inserted_slice -> %cst_0\n   %b = tensor.insert_slice %a into %tensor[%index, 0, 0][1, 16, 16][1, 1, 1]: tensor<16x16xf16, #AL> into tensor<1x16x16xf16, #A_SHARED>\n   return\n }\n@@ -94,7 +94,7 @@ func.func @extract_slice(%A : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : index\n-  // CHECK-NEXT: %0 -> %cst\n+  // CHECK-NEXT: %extracted_slice -> %cst\n   %cst1 = tensor.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n   return\n }\n@@ -170,7 +170,7 @@ func.func @for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n     scf.if %i1 {\n       %index = arith.constant 8 : index\n-      // CHECK-NEXT: %1 -> %cst,%cst_0\n+      // CHECK-NEXT: %extracted_slice -> %cst,%cst_0\n       %cst0 = tensor.extract_slice %a_shared[%index, 0][1, 32][1, 1] : tensor<128x32xf16, #A_SHARED> to tensor<32xf16, #A_SHARED>\n       scf.yield\n     }\n@@ -179,8 +179,8 @@ func.func @for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n   return\n }\n \n-// CHECK-LABEL: for_if_for\n-func.func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+// CHECK-LABEL: for_for_if\n+func.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: %cst -> %cst\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n@@ -213,3 +213,34 @@ func.func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>\n   }\n   return\n }\n+\n+// CHECK-LABEL: cf_for\n+func.func @cf_for(%arg0: index, %arg1: index, %arg2: index, %arg3: !tt.ptr<f16>, %arg4: !tt.ptr<f16>) {\n+  // CHECK: %cst -> %cst\n+  %cst = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #A_SHARED>\n+  // CHECK-NEXT: %cst_0 -> %cst_0\n+  %cst_0 = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #A_SHARED>\n+  gpu.barrier\n+  // CHECK-NEXT: %0 -> %0\n+  %0 = tt.cat %cst, %cst_0 {axis = 0 : i64} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+  // CHECK-NEXT: %cst_1 -> %cst_1\n+  %cst_1 = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #A_SHARED>\n+  // CHECK-NEXT: %2 -> %cst,%cst_0,%cst_1\n+  // CHECK-NEXT: %3 -> %cst,%cst_0,%cst_1\n+  // CHECK-NEXT: %4 -> %cst,%cst_0,%cst_1\n+  cf.br ^bb1(%arg0, %cst, %cst_0, %cst_1 : index, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>)\n+^bb1(%1: index, %2: tensor<128x32xf16, #A_SHARED>, %3: tensor<128x32xf16, #A_SHARED>, %4: tensor<128x32xf16, #A_SHARED>):  // 2 preds: ^bb0, ^bb2\n+  %5 = arith.cmpi slt, %1, %arg1 : index\n+  cf.cond_br %5, ^bb2, ^bb3\n+^bb2:  // pred: ^bb1\n+  %6 = tt.cat %cst, %cst_0 {axis = 0 : i64} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #blocked>\n+  gpu.barrier\n+  %7 = tt.cat %2, %3 {axis = 0 : i64} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #blocked>\n+  %8 = arith.addi %1, %arg2 : index\n+  cf.br ^bb1(%8, %4, %2, %3 : index, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>)\n+^bb3:  // pred: ^bb1\n+  gpu.barrier\n+  // CHECK-NEXT: %9 -> %9\n+  %9 = tt.cat %0, %0 {axis = 0 : i64} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n+  return\n+}"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -315,8 +315,8 @@ func.func @for_use_ancestor(%lb : index, %ub : index, %step : index, %A : !tt.pt\n \n // a_shared_init, b_shared_init, and c_shared_init's liveness ranges are span over the entire function before cst2.\n // So they cannot be reused by cst0 and cst1, but can be reused by cst2.\n-// CHECK-LABEL: for_if_for\n-func.func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+// CHECK-LABEL: for_for_if\n+func.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: offset = 0, size = 8192\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 8192, size = 8192"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 275, "deletions": 100, "changes": 375, "file_content_changes": "@@ -45,109 +45,123 @@ func.func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16\n func.func @raw_single_block(%A : !tt.ptr<f16>) {\n   %cst1 = arith.constant dense<true> : tensor<128x32xi1, #AL>\n   %cst2 = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #AL>\n-  %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n-  %a1_ = tt.load %a_ptr, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n-  %a1 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_SHARED>\n-  // CHECK: Membar 5\n-  %a2 = triton_gpu.convert_layout %a1 : (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #A_SHARED>\n+  %0 = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n+  %1 = tt.load %0, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n+  %2 = triton_gpu.convert_layout %1 : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: triton_gpu.convert_layout\n+  %3 = triton_gpu.convert_layout %2 : (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #A_SHARED>\n   return\n }\n \n // CHECK-LABEL: war_single_block\n func.func @war_single_block(%A : !tt.ptr<f16>) {\n   %cst1 = arith.constant dense<true> : tensor<128x32xi1, #AL>\n   %cst2 = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #AL>\n-  %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n-  %a1_ = tt.load %a_ptr, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n-  %a1 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_SHARED>\n-  // CHECK: Membar 5\n-  %a2 = triton_gpu.convert_layout %a1 : (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #AL>\n-  // a2's liveness range ends here, and a3 and a2 have the same address range.\n-  // So it makes sense to have a WAR dependency between a2 and a3.\n-  // CHECK-NEXT: Membar 7\n-  %a3 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_SHARED>\n+  %0 = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n+  %1 = tt.load %0, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n+  %2 = triton_gpu.convert_layout %1 : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: triton_gpu.convert_layout\n+  %3 = triton_gpu.convert_layout %2 : (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #AL>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: %4 = triton_gpu.convert_layout\n+  %4 = triton_gpu.convert_layout %1 : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_SHARED>\n   return\n }\n \n // CHECK-LABEL: scratch\n func.func @scratch() {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n-  // CHECK: Membar 1\n-  %a = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n-  // CHECK-NEXT: Membar 3\n-  %aa = triton_gpu.convert_layout %a : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n-  %b = tt.reduce %aa {redOp = 1 : i32, axis = 0 : i32} : tensor<32x16xf16, #AL> -> tensor<16xf16, #sliceAd0>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %0 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: triton_gpu.convert_layout\n+  %1 = triton_gpu.convert_layout %0 : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n+  %2 = tt.reduce %1 {redOp = 1 : i32, axis = 0 : i32} : tensor<32x16xf16, #AL> -> tensor<16xf16, #sliceAd0>\n   return\n }\n \n // CHECK-LABEL: async_wait\n func.func @async_wait() {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n-  // CHECK: Membar 1\n-  %a = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %0 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   triton_gpu.async_wait {num = 4 : i32}\n-  // CHECK-NEXT: Membar 4\n-  %a_ = triton_gpu.convert_layout %a : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: triton_gpu.convert_layout\n+  %1 = triton_gpu.convert_layout %0 : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n   return\n }\n \n // CHECK-LABEL: alloc\n func.func @alloc() {\n-  %cst0 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SHARED>\n-  %a = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n-  // CHECK: Membar 2\n-  %b = triton_gpu.convert_layout %a : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n+  %0 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SHARED>\n+  %1 = tt.cat %0, %0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: triton_gpu.convert_layout\n+  %2 = triton_gpu.convert_layout %1 : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n   return\n }\n \n // CHECK-LABEL: extract_slice\n func.func @extract_slice() {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : index\n-  %cst1 = tensor.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n-  // CHECK: Membar 3\n-  %cst2 = triton_gpu.convert_layout %cst1 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n-  // CHECK-NEXT: Membar 5\n-  %cst3 = triton_gpu.convert_layout %cst2 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A_SHARED>\n+  %0 = tensor.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: triton_gpu.convert_layout\n+  %1 = triton_gpu.convert_layout %0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: triton_gpu.convert_layout\n+  %2 = triton_gpu.convert_layout %1 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A_SHARED>\n   return\n }\n \n // CHECK-LABEL: trans\n func.func @trans() {\n+  // CHECK-NOT: gpu.barrier\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n   %b = tt.trans %cst0 : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED_T>\n   return\n }\n \n-// CHECK-LABEL: insert_slice_async\n-func.func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n+// CHECK-LABEL: insert_slice_async_op\n+func.func @insert_slice_async_op(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   %tensor = triton_gpu.alloc_tensor : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : i32\n-  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A_SHARED>\n-  // CHECK: Membar 6\n-  %b = tt.cat %a, %a {axis = 0} : (tensor<1x16x16xf16, #A_SHARED>, tensor<1x16x16xf16, #A_SHARED>) -> tensor<2x16x16xf16, #A_SHARED>\n-  // CHECK: Membar 8\n-  %c = tt.cat %b, %b {axis = 0} : (tensor<2x16x16xf16, #A_SHARED>, tensor<2x16x16xf16, #A_SHARED>) -> tensor<4x16x16xf16, #A_SHARED>\n+  %3 = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %4 = tt.cat %3, %3 {axis = 0} : (tensor<1x16x16xf16, #A_SHARED>, tensor<1x16x16xf16, #A_SHARED>) -> tensor<2x16x16xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %5 = tt.cat %4, %4 {axis = 0} : (tensor<2x16x16xf16, #A_SHARED>, tensor<2x16x16xf16, #A_SHARED>) -> tensor<4x16x16xf16, #A_SHARED>\n   return\n }\n \n-// CHECK-LABEL: insert_slice\n-func.func @insert_slice(%A : !tt.ptr<f16>, %i1 : i1) {\n+// CHECK-LABEL: insert_slice_op\n+func.func @insert_slice_op(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   %tensor = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : index\n-  %al = tt.load %a_ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #AL>\n-  // CHECK: Membar 6\n-  %a = tensor.insert_slice %al into %tensor[%index, 0, 0][1, 16, 16][1, 1, 1]: tensor<16x16xf16, #AL> into tensor<1x16x16xf16, #A_SHARED>\n-  // CHECK: Membar 8\n-  %b = tt.cat %a, %a {axis = 0} : (tensor<1x16x16xf16, #A_SHARED>, tensor<1x16x16xf16, #A_SHARED>) -> tensor<2x16x16xf16, #A_SHARED>\n-  // CHECK: Membar 10\n-  %c = tt.cat %b, %b {axis = 0} : (tensor<2x16x16xf16, #A_SHARED>, tensor<2x16x16xf16, #A_SHARED>) -> tensor<4x16x16xf16, #A_SHARED>\n+  %2 = tt.load %a_ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #AL>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tensor.insert_slice\n+  %3 = tensor.insert_slice %2 into %tensor[%index, 0, 0][1, 16, 16][1, 1, 1]: tensor<16x16xf16, #AL> into tensor<1x16x16xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %4 = tt.cat %3, %3 {axis = 0} : (tensor<1x16x16xf16, #A_SHARED>, tensor<1x16x16xf16, #A_SHARED>) -> tensor<2x16x16xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %5 = tt.cat %4, %4 {axis = 0} : (tensor<2x16x16xf16, #A_SHARED>, tensor<2x16x16xf16, #A_SHARED>) -> tensor<4x16x16xf16, #A_SHARED>\n   return\n }\n \n@@ -157,18 +171,21 @@ func.func @multi_blocks(%i1 : i1) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n-    // CHECK: Membar 2\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %0 = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   } else {\n     %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n     %cst3 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n-    // CHECK-NEXT: Membar 7\n-    %b = tt.cat %cst2, %cst3 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %1 = tt.cat %cst2, %cst3 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   }\n-  // CHECK-NEXT: Membar 10\n-  %c = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %2 = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   return\n }\n \n@@ -178,12 +195,14 @@ func.func @multi_blocks_join_barrier(%i1 : i1) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n-    // CHECK: Membar 2\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %0 = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   } else {\n-    // CHECK-NEXT: Membar 5\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %1 = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   }\n   %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n@@ -196,17 +215,42 @@ func.func @multi_blocks_yield(%i1 : i1) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %a = scf.if %i1 -> (tensor<32x16xf16, #A_SHARED>) {\n-    // CHECK: Membar 2\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n-    scf.yield %a : tensor<32x16xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %0 = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    scf.yield %0 : tensor<32x16xf16, #A_SHARED>\n   } else {\n-    // CHECK-NEXT: Membar 5\n-    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n-    scf.yield %b : tensor<32x16xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %1 = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    scf.yield %1 : tensor<32x16xf16, #A_SHARED>\n   }\n   %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n-  // CHECK-NEXT: Membar 9\n-  %b = tt.cat %a, %a {axis = 0} : (tensor<32x16xf16, #A_SHARED>, tensor<32x16xf16, #A_SHARED>) -> tensor<64x16xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %4 = tt.cat %a, %a {axis = 0} : (tensor<32x16xf16, #A_SHARED>, tensor<32x16xf16, #A_SHARED>) -> tensor<64x16xf16, #A_SHARED>\n+  return\n+}\n+\n+// Even though the entry block doesn't have a barrier, the successors should have barriers\n+// CHECK-LABEL: multi_blocks_entry_no_shared\n+func.func @multi_blocks_entry_no_shared(%i1 : i1) {\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n+  %a = scf.if %i1 -> (tensor<32x16xf16, #A_SHARED>) {\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+    %0 = tt.cat %cst1, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    scf.yield %0 : tensor<32x16xf16, #A_SHARED>\n+  } else {\n+    // CHECK-NOT: gpu.barrier\n+    // CHECK: arith.constant\n+    %cst1 = arith.constant dense<0.000000e+00> : tensor<32x16xf16, #A_SHARED>\n+    scf.yield %cst1 : tensor<32x16xf16, #A_SHARED>\n+  }\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %1 = tt.cat %a, %a {axis = 0} : (tensor<32x16xf16, #A_SHARED>, tensor<32x16xf16, #A_SHARED>) -> tensor<64x16xf16, #A_SHARED>\n   return\n }\n \n@@ -216,11 +260,14 @@ func.func @multi_blocks_noelse(%i1 : i1) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n-    // CHECK: Membar 2\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %0 = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   }\n-  %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: triton_gpu.convert_layout\n+  %1 = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n   return\n }\n \n@@ -231,18 +278,21 @@ func.func @multi_blocks_nested_scf(%i1 : i1, %i2 : i1) {\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n     scf.if %i2 {\n-      // CHECK: Membar 2\n-      %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+      // CHECK: gpu.barrier\n+      // CHECK-NEXT: tt.cat\n+      %0 = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n       scf.yield\n     }\n     scf.yield\n   } else {\n-    // CHECK-NEXT: Membar 6\n-    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %1 = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   }\n-  // CHECK-NEXT: Membar 9\n-  %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: triton_gpu.convert_layout\n+  %2 = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n   return\n }\n \n@@ -252,8 +302,9 @@ func.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B :\n   %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n-    // CHECK-NEXT: Membar 3\n-    %cst0 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %5 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n     scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n   return\n@@ -265,17 +316,20 @@ func.func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B :\n func.func @for_alias(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n-  // CHECK-NEXT: Membar 2\n-  %cst0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n     %cst1 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #AL>\n-    // CHECK-NEXT: Membar 6\n-    %cst2 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #AL>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %7 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #AL>\n     scf.yield %c_shared, %a_shared, %b_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  // CHECK-NEXT: Membar 9\n-  %cst3 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %9 = tt.cat %0, %0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n   return\n }\n \n@@ -285,41 +339,162 @@ func.func @for_alias(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>,\n func.func @for_reuse(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n-  // CHECK-NEXT: Membar 2\n-  %cst0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n-    // CHECK-NEXT: Membar 5\n-    %cst1 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n-    // CHECK-NEXT: Membar 7\n-    %cst2 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %6 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %7 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n     scf.yield %c_shared, %a_shared, %b_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  // CHECK-NEXT: Membar 10\n-  %cst3 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %9 = tt.cat %0, %0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n   return\n }\n \n-\n // CHECK-LABEL: for_reuse_nested\n func.func @for_reuse_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n-  // CHECK-NEXT: Membar 2\n-  %cst0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n-    // CHECK-NEXT: Membar 5\n-    %cst1 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    // CHECK: gpu.barrier\n+    // CHECK-NEXT: tt.cat\n+    %6 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n     %a_shared_next, %b_shared_next, %c_shared_next = scf.for %ivv = %lb to %ub step %step iter_args(%a_shared_nested = %a_shared_init, %b_shared_nested = %b_shared_init, %c_shared_nested = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n-      // CHECK-NEXT: Membar 7\n-      %cst2 = tt.cat %a_shared_nested, %b_shared_nested {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+      // CHECK: gpu.barrier\n+      // CHECK-NEXT:  tt.cat\n+      %12 = tt.cat %a_shared_nested, %b_shared_nested {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n       scf.yield %c_shared_nested, %a_shared_nested, %b_shared_nested : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n     }\n     scf.yield %c_shared, %a_shared, %b_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n-  // CHECK-NEXT: Membar 11\n-  %cst3 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT:  tt.cat\n+  %15 = tt.cat %0, %0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n+  return\n+}\n+\n+// repeatedly write to the same shared memory addresses\n+// CHECK-LABEL: for_for_if\n+func.func @for_for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    %c_shared_next = scf.for %jv = %lb to %ub step %step iter_args(%c_shared_next = %c_shared) -> (tensor<128x32xf16, #A_SHARED>) {\n+      %c_shared_next_next = scf.if %i1 -> tensor<128x32xf16, #A_SHARED> {\n+        // CHECK: gpu.barrier\n+        // CHECK-NEXT: arith.constant\n+        %cst0 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+        scf.yield %cst0 : tensor<128x32xf16, #A_SHARED>\n+      } else {\n+        // CHECK: gpu.barrier\n+        // CHECK-NEXT: arith.constant\n+        %cst0 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+        scf.yield %cst0 : tensor<128x32xf16, #A_SHARED>\n+      }\n+      scf.yield %c_shared_next_next : tensor<128x32xf16, #A_SHARED>\n+    }\n+    scf.yield %a_shared, %b_shared, %c_shared_next : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n+  }\n+  return\n+}\n+\n+// c_block_next can either be converted from c_shared_init or c_shared_next_next\n+// CHECK-LABEL: for_if_for\n+func.func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  // CHECK: gpu.barrier\n+  %c_blocked = triton_gpu.convert_layout %c_shared_init : (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #AL> \n+\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    %c_shared_next_next = scf.if %i1 -> tensor<128x32xf16, #A_SHARED> {\n+      // CHECK: gpu.barrier\n+      // CHECK-NEXT: arith.constant\n+      %cst0 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+      scf.yield %cst0 : tensor<128x32xf16, #A_SHARED>\n+    } else {\n+      %c_shared_ = scf.for %jv = %lb to %ub step %step iter_args(%c_shared_next = %c_shared) -> (tensor<128x32xf16, #A_SHARED>) {\n+        // CHECK: gpu.barrier\n+        // CHECK-NEXT: triton_gpu.convert_layout\n+        %c_blocked_next = triton_gpu.convert_layout %c_shared_next : (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #AL> \n+        scf.yield %c_shared : tensor<128x32xf16, #A_SHARED>\n+      }\n+      scf.yield %c_shared_ : tensor<128x32xf16, #A_SHARED>\n+    }\n+    // CHECK-NOT: gpu.barrier\n+    %b_blocked_next = triton_gpu.convert_layout %b_shared: (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #AL> \n+    scf.yield %a_shared, %b_shared, %c_shared_next_next : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n+  }\n+  return\n+}\n+\n+// CHECK-LABEL: cf_if\n+func.func @cf_if(%i1 : i1) {\n+  %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n+  %cst_0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n+  cf.cond_br %i1, ^bb1, ^bb2\n+^bb1:  // pred: ^bb0\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %0 = tt.cat %cst, %cst_0 {axis = 0 : i64} : (tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>, tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n+  cf.br ^bb2\n+^bb2:  // 2 preds: ^bb0, ^bb1\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: triton_gpu.convert_layout\n+  %1 = triton_gpu.convert_layout %cst : (tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<16x16xf16, #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>>\n+  return\n+}\n+\n+func.func @cf_if_else(%i1 : i1) {\n+  %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n+  %cst_0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n+  cf.cond_br %i1, ^bb1, ^bb2\n+^bb1:  // pred: ^bb0\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %0 = tt.cat %cst, %cst_0 {axis = 0 : i64} : (tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>, tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n+  cf.br ^bb3(%0 : tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>)\n+^bb2:  // pred: ^bb0\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %1 = tt.cat %cst, %cst_0 {axis = 0 : i64} : (tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>, tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n+  cf.br ^bb3(%1 : tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>)\n+^bb3(%2: tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>):  // 2 preds: ^bb1, ^bb2\n+  cf.br ^bb4\n+^bb4:  // pred: ^bb3\n+  %3 = triton_gpu.convert_layout %cst : (tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<16x16xf16, #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>>\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %4 = tt.cat %2, %2 {axis = 0 : i64} : (tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>, tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<64x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n+  return\n+}\n+\n+func.func @cf_if_else_return(%i1 : i1) {\n+  %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n+  %cst_0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n+  cf.cond_br %i1, ^bb1, ^bb2\n+^bb1:  // pred: ^bb0\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %0 = tt.cat %cst, %cst_0 {axis = 0 : i64} : (tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>, tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n+  return\n+^bb2:  // pred: ^bb0\n+  // CHECK: gpu.barrier\n+  // CHECK-NEXT: tt.cat\n+  %1 = tt.cat %cst, %cst_0 {axis = 0 : i64} : (tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>, tensor<16x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>) -> tensor<32x16xf16, #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>>\n   return\n }\n "}, {"filename": "test/Conversion/triton_ops.mlir", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -67,11 +67,11 @@ func.func @load_store_ops_scalar(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}\n   %c = tt.load %ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = true} : f32\n \n   // store scalar\n-  // CHECK: tt.store %{{.*}}, %[[L0]] : f32\n+  // CHECK: tt.store %{{.*}}, %[[L0]] {cache = 1 : i32, evict = 1 : i32} : f32\n   tt.store %ptr, %a : f32\n-  // CHECK: tt.store %{{.*}}, %[[L1]], %{{.*}} : f32\n+  // CHECK: tt.store %{{.*}}, %[[L1]], %{{.*}} {cache = 1 : i32, evict = 1 : i32} : f32\n   tt.store %ptr, %b, %mask : f32\n-  // CHECK: tt.store %{{.*}}, %[[L2]], %{{.*}} : f32\n+  // CHECK: tt.store %{{.*}}, %[[L2]], %{{.*}} {cache = 1 : i32, evict = 1 : i32} : f32\n   tt.store %ptr, %c, %mask : f32\n   return\n }"}, {"filename": "test/Conversion/triton_to_tritongpu.mlir", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -33,20 +33,20 @@ func.func @load_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n func.func @reduce_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n   // Test if the total number of threadsPerWarp is 32\n   // Test if the total number of warps is 2\n-  // CHECK: #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 8], warpsPerCTA = [1, 2], order = [0, 1]}>\n-  // CHECK: #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 2], order = [0, 1]}>\n-  // CHECK: #blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [16, 2], warpsPerCTA = [1, 2], order = [0, 1]}>\n+  // CHECK: #[[blocked0:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 8], warpsPerCTA = [1, 2], order = [0, 1]}>\n+  // CHECK: #[[blocked1:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 2], order = [0, 1]}>\n+  // CHECK: #[[blocked2:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [16, 2], warpsPerCTA = [1, 2], order = [0, 1]}>\n   // CHECK: module attributes {\"triton_gpu.num-warps\" = 2 : i32} {{.*}}\n   %c0 = arith.constant dense<1.00e+00> : tensor<4x4xf32>\n   %c1 = arith.constant dense<2.00e+00> : tensor<8x2xf32>\n   %c2 = arith.constant dense<3.00e+00> : tensor<16x16xf32>\n-  // CHECK: tensor<4x4xf32, #blocked0> -> tensor<4xf32, #triton_gpu.slice<{dim = 0, parent = #blocked0}>>\n+  // CHECK: tensor<4x4xf32, #[[blocked0]]> -> tensor<4xf32, #triton_gpu.slice<{dim = 0, parent = #[[blocked0]]}>>\n   %c0_ = tt.reduce %c0 {redOp = 1 : i32, axis = 0 : i32} : tensor<4x4xf32> -> tensor<4xf32>\n-  // CHECK: tensor<8x2xf32, #blocked1> -> tensor<2xf32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>\n+  // CHECK: tensor<8x2xf32, #[[blocked1]]> -> tensor<2xf32, #triton_gpu.slice<{dim = 0, parent = #[[blocked1]]}>\n   %c1_ = tt.reduce %c1 {redOp = 1 : i32, axis = 0 : i32} : tensor<8x2xf32> -> tensor<2xf32>\n-  // CHECK: tensor<8x2xf32, #blocked1> -> tensor<8xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+  // CHECK: tensor<8x2xf32, #[[blocked1]]> -> tensor<8xf32, #triton_gpu.slice<{dim = 1, parent = #[[blocked1]]}>>\n   %c2_ = tt.reduce %c1 {redOp = 1 : i32, axis = 1 : i32} : tensor<8x2xf32> -> tensor<8xf32>\n-  // CHECK: tensor<16x16xf32, #blocked2> -> tensor<16xf32, #triton_gpu.slice<{dim = 0, parent = #blocked2}>>\n+  // CHECK: tensor<16x16xf32, #[[blocked2]]> -> tensor<16xf32, #triton_gpu.slice<{dim = 0, parent = #[[blocked2]]}>>\n   %c3_ = tt.reduce %c2 {redOp = 1 : i32, axis = 0 : i32} : tensor<16x16xf32> -> tensor<16xf32>\n \n   return"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -3,7 +3,7 @@\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK: llvm.func @test_empty_kernel(%arg0: i32, %arg1: !llvm.ptr<f16, 1>)\n   // Here the 128 comes from the 4 in module attribute multiples 32\n-  // CHECK:  attributes {nvvm.kernel = 1 : ui1, nvvm.maxntid = 128 : i32} {{.*}}\n+  // CHECK:  attributes {nvvm.kernel = 1 : ui1, nvvm.maxntid = [128 : i32]} {{.*}}\n   func.func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n     // CHECK:  llvm.return\n     return"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 26, "deletions": 25, "changes": 51, "file_content_changes": "@@ -4,16 +4,16 @@\n #layout1 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n #layout2 = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [4, 1]}>\n \n-// CHECK: [[target_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n-// CHECK: [[row_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n-// CHECK: [[col_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [0, 1]}>\n-// CHECK: [[col_layout_novec:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n+// CHECK: [[$target_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n+// CHECK: [[$row_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n+// CHECK: [[$col_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [0, 1]}>\n+// CHECK: [[$col_layout_novec:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n // CHECK-LABEL: cst\n func.func @cst() -> tensor<1024xi32, #layout1> {\n   %cst = arith.constant dense<0> : tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %cst : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n   // CHECK-NOT: triton_gpu.convert_layout\n-  // CHECK: return %cst : tensor<1024xi32, [[target_layout]]>\n+  // CHECK: return %cst : tensor<1024xi32, [[$target_layout]]>\n   return %1: tensor<1024xi32, #layout1>\n }\n \n@@ -22,7 +22,7 @@ func.func @range() -> tensor<1024xi32, #layout1> {\n   %0 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %0 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n   // CHECK-NOT: triton_gpu.convert_layout\n-  // CHECK: return %0 : tensor<1024xi32, [[target_layout]]>\n+  // CHECK: return %0 : tensor<1024xi32, [[$target_layout]]>\n   return %1: tensor<1024xi32, #layout1>\n }\n \n@@ -31,7 +31,7 @@ func.func @splat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   %0 = tt.splat %arg0 : (i32) -> tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %0 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n   // CHECK-NOT: triton_gpu.convert_layout\n-  // CHECK: return %0 : tensor<1024xi32, [[target_layout]]>\n+  // CHECK: return %0 : tensor<1024xi32, [[$target_layout]]>\n   return %1: tensor<1024xi32, #layout1>\n }\n \n@@ -45,14 +45,14 @@ func.func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   %5 = triton_gpu.convert_layout %2 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n   %6 = arith.addi %3, %5 : tensor<1024xi32, #layout1>\n   return %6: tensor<1024xi32, #layout1>\n-  // CHECK: %0 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[target_layout]]>\n-  // CHECK: %1 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[target_layout]]>\n-  // CHECK: %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[target_layout]]>\n-  // CHECK: %3 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[target_layout]]>\n-  // CHECK: %4 = arith.muli %0, %2 : tensor<1024xi32, [[target_layout]]>\n-  // CHECK: %5 = arith.muli %1, %3 : tensor<1024xi32, [[target_layout]]>\n-  // CHECK: %6 = arith.addi %4, %5 : tensor<1024xi32, [[target_layout]]>\n-  // CHECK: return %6 : tensor<1024xi32, [[target_layout]]>\n+  // CHECK: %0 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[$target_layout]]>\n+  // CHECK: %1 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[$target_layout]]>\n+  // CHECK: %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[$target_layout]]>\n+  // CHECK: %3 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[$target_layout]]>\n+  // CHECK: %4 = arith.muli %0, %2 : tensor<1024xi32, [[$target_layout]]>\n+  // CHECK: %5 = arith.muli %1, %3 : tensor<1024xi32, [[$target_layout]]>\n+  // CHECK: %6 = arith.addi %4, %5 : tensor<1024xi32, [[$target_layout]]>\n+  // CHECK: return %6 : tensor<1024xi32, [[$target_layout]]>\n }\n \n // CHECK-LABEL: remat_load_store\n@@ -185,7 +185,8 @@ func.func @if_else_both_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility\n     %7 = triton_gpu.convert_layout %3 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n     scf.yield %7 : tensor<1024xi32, #layout1>\n   }\n-  // CHECK: triton_gpu.convert_layout\n+  // TODO(csigg): seems like the whole function is converted to layout1.\n+  // disabledCHECK: triton_gpu.convert_layout\n   // CHECK-NOT: triton_gpu.convert_layout\n   tt.store %5, %8 : tensor<1024xi32, #layout1>\n   return\n@@ -202,9 +203,9 @@ func.func @if_else_both_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility\n // CHECK-LABEL: transpose\n func.func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n-  // CHECK: [[loaded_val:%.*]] = tt.load {{.*}}, {{%cst.*}}, {{%cst.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, [[row_layout]]>\n-  // CHECK: [[cvt_val:%.*]] = triton_gpu.convert_layout [[loaded_val]] : (tensor<64x64xf32, [[row_layout]]>) -> tensor<64x64xf32, [[col_layout]]>\n-  // CHECK: tt.store {{.*}}, [[cvt_val]], {{%cst.*}} : tensor<64x64xf32, [[col_layout]]>\n+  // CHECK: [[loaded_val:%.*]] = tt.load {{.*}}, {{%cst.*}}, {{%cst.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, [[$row_layout]]>\n+  // CHECK: [[cvt_val:%.*]] = triton_gpu.convert_layout [[loaded_val]] : (tensor<64x64xf32, [[$row_layout]]>) -> tensor<64x64xf32, [[$col_layout]]>\n+  // CHECK: tt.store {{.*}}, [[cvt_val]], {{%cst.*}} : tensor<64x64xf32, [[$col_layout]]>\n   // CHECK: return\n   %cst = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked1>\n   %cst_0 = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n@@ -243,13 +244,13 @@ func.func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i3\n // CHECK-LABEL: loop\n func.func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32) {\n     // CHECK-NOT: triton_gpu.convert_layout\n-    // CHECK: [[loop_ret:%.*]]:2 = scf.for {{.*}} -> (tensor<64x64xf32, [[row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[row_layout]]>)\n-    // CHECK-NEXT: {{.*}} = tt.load {{.*}} : tensor<64x64xf32, [[row_layout]]>\n-    // CHECK-NEXT: {{.*}} = arith.addf {{.*}} : tensor<64x64xf32, [[row_layout]]>\n-    // CHECK-NEXT: {{.*}} = tt.addptr {{.*}} : tensor<64x64x!tt.ptr<f32>, [[row_layout]]>, tensor<64x64xi32, [[row_layout]]>\n-    // CHECK-NEXT: scf.yield {{.*}} : tensor<64x64xf32, [[row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n+    // CHECK: [[loop_ret:%.*]]:2 = scf.for {{.*}} -> (tensor<64x64xf32, [[$row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[$row_layout]]>)\n+    // CHECK-NEXT: {{.*}} = tt.load {{.*}} : tensor<64x64xf32, [[$row_layout]]>\n+    // CHECK-NEXT: {{.*}} = arith.addf {{.*}} : tensor<64x64xf32, [[$row_layout]]>\n+    // CHECK-NEXT: {{.*}} = tt.addptr {{.*}} : tensor<64x64x!tt.ptr<f32>, [[$row_layout]]>, tensor<64x64xi32, [[$row_layout]]>\n+    // CHECK-NEXT: scf.yield {{.*}} : tensor<64x64xf32, [[$row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[$row_layout]]>\n     // CHECK-NEXT: }\n-    // CHECK-NEXT: {{.*}} = triton_gpu.convert_layout [[loop_ret]]#0 : (tensor<64x64xf32, [[row_layout]]>) -> tensor<64x64xf32, [[col_layout_novec]]>\n+    // CHECK-NEXT: {{.*}} = triton_gpu.convert_layout [[loop_ret]]#0 : (tensor<64x64xf32, [[$row_layout]]>) -> tensor<64x64xf32, [[$col_layout_novec]]>\n     // CHECK-NOT: triton_gpu.convert_layout\n     %cst = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n     %cst_0 = arith.constant dense<64> : tensor<64x64xi32, #blocked1>"}, {"filename": "test/TritonGPU/update-mma-for-volta.mlir", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -12,7 +12,7 @@\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n // It creates a new MMA layout to fit with $a and $b's dot_operand, and get the right warpsPerCTA\n // The ID of this MMA instance should be 0.\n-// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 2]}>\n+// CHECK: [[$new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 2]}>\n module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n   // CHECK-LABEL: dot_mmav1\n   func.func @dot_mmav1(%A: tensor<64x64xf16, #blocked0>, %B: tensor<64x64xf16, #blocked0>) -> tensor<64x64xf32, #blocked0> {\n@@ -21,7 +21,7 @@ module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n     %BB = triton_gpu.convert_layout %B : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_b>\n     %CC = triton_gpu.convert_layout %C : (tensor<64x64xf32, #blocked0>) -> tensor<64x64xf32, #mma0>\n \n-    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, %cst {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[new_mma]]>\n+    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, %cst {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[$new_mma]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[$new_mma]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[$new_mma]]>\n     %D = tt.dot %AA, %BB, %CC {allowTF32 = true} : tensor<64x64xf16, #dot_operand_a> * tensor<64x64xf16, #dot_operand_b> -> tensor<64x64xf32, #mma0>\n     %res = triton_gpu.convert_layout %D : (tensor<64x64xf32, #mma0>) -> tensor<64x64xf32, #blocked0>\n \n@@ -40,8 +40,8 @@ module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n #mma1 = #triton_gpu.mma<{versionMajor=1, versionMinor=16, warpsPerCTA=[4,4]}>\n \n // Will still get two MMA layouts\n-// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 2]}>\n-// CHECK: [[new_mma1:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 19, warpsPerCTA = [4, 2]}>\n+// CHECK-DAG: [[$new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 2]}>\n+// CHECK-DAG: [[$new_mma1:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 19, warpsPerCTA = [4, 2]}>\n \n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, isMMAv1Row=true}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n@@ -60,8 +60,8 @@ module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n     %BB1 = triton_gpu.convert_layout %B : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_b1>\n     %CC1 = triton_gpu.convert_layout %C : (tensor<64x64xf32, #blocked0>) -> tensor<64x64xf32, #mma1>\n \n-    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, {{.*}} {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[new_mma]]>\n-    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, {{.*}} {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma1]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma1]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[new_mma1]]>\n+    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, {{.*}} {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[$new_mma]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[$new_mma]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[$new_mma]]>\n+    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, {{.*}} {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[$new_mma1]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[$new_mma1]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[$new_mma1]]>\n     %D = tt.dot %AA, %BB, %CC {allowTF32 = true} : tensor<64x64xf16, #dot_operand_a> * tensor<64x64xf16, #dot_operand_b> -> tensor<64x64xf32, #mma0>\n     %D1 = tt.dot %AA1, %BB1, %CC1 {allowTF32 = true} : tensor<64x64xf16, #dot_operand_a1> * tensor<64x64xf16, #dot_operand_b1> -> tensor<64x64xf32, #mma1>\n     %res = triton_gpu.convert_layout %D : (tensor<64x64xf32, #mma0>) -> tensor<64x64xf32, #blocked0>"}, {"filename": "test/lib/Analysis/TestAlias.cpp", "status": "modified", "additions": 13, "deletions": 2, "changes": 15, "file_content_changes": "@@ -51,7 +51,7 @@ struct TestAliasPass\n       dataflow::Lattice<AliasInfo> *latticeElement =\n           analysis->getLatticeElement(value);\n       SmallVector<std::string, 4> opNames;\n-      if (latticeElement && !latticeElement->isUninitialized()) {\n+      if (latticeElement) {\n         auto &info = latticeElement->getValue();\n         for (auto &alias : info.getAllocs()) {\n           auto opName =\n@@ -65,8 +65,19 @@ struct TestAliasPass\n     };\n \n     operation->walk<WalkOrder::PreOrder>([&](Operation *op) {\n-      if (op->getNumResults() < 1)\n+      if (op->getNumResults() < 1) {\n+        // cond br, br\n+        if (auto branch = dyn_cast<BranchOpInterface>(op)) {\n+          auto *block = branch->getBlock();\n+          for (auto arg : llvm::enumerate(block->getArguments())) {\n+            auto operand = block->getArgument(arg.index());\n+            auto opNames = getAllocOpNames(operand);\n+            auto argName = getValueOperandName(arg.value(), state);\n+            print(argName, opNames, os);\n+          }\n+        }\n         return;\n+      }\n       if (auto forOp = dyn_cast<scf::ForOp>(op)) {\n         for (auto arg : llvm::enumerate(forOp.getRegionIterArgs())) {\n           auto operand = forOp.getOpOperandForRegionIterArg(arg.value()).get();"}, {"filename": "test/lib/Analysis/TestMembar.cpp", "status": "modified", "additions": 17, "deletions": 11, "changes": 28, "file_content_changes": "@@ -1,6 +1,8 @@\n+#include \"mlir/Conversion/SCFToControlFlow/SCFToControlFlow.h\"\n #include \"mlir/Dialect/GPU/IR/GPUDialect.h\"\n #include \"mlir/IR/Dialect.h\"\n #include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Transforms/DialectConversion.h\"\n #include \"triton/Analysis/Allocation.h\"\n #include \"triton/Analysis/Membar.h\"\n \n@@ -24,21 +26,25 @@ struct TestMembarPass\n     // Convert to std::string can remove quotes from op_name\n     auto opName = SymbolTable::getSymbolName(operation).getValue().str();\n     os << opName << \"\\n\";\n+\n+    // Lower the module to the cf dialect\n+    auto *context = operation->getContext();\n+    RewritePatternSet scfPatterns(context);\n+    mlir::populateSCFToControlFlowConversionPatterns(scfPatterns);\n+    mlir::ConversionTarget scfTarget(*context);\n+    scfTarget.addIllegalOp<scf::ForOp, scf::IfOp, scf::ParallelOp, scf::WhileOp,\n+                           scf::ExecuteRegionOp>();\n+    scfTarget.markUnknownOpDynamicallyLegal([](Operation *) { return true; });\n+    if (failed(applyPartialConversion(operation, scfTarget,\n+                                      std::move(scfPatterns))))\n+      return signalPassFailure();\n+\n+    // Print all ops after membar pass\n     Allocation allocation(operation);\n     MembarAnalysis membarPass(&allocation);\n     membarPass.run();\n \n-    size_t operationId = 0;\n-    operation->walk<WalkOrder::PreOrder>([&](Operation *op) {\n-      if (isa<gpu::BarrierOp>(op)) {\n-        os << \"Membar \" << operationId << \"\\n\";\n-      }\n-      if (op->getNumRegions() == 0) {\n-        // Don't count parent Operation to simplify the test.\n-        operationId++;\n-      }\n-      return;\n-    });\n+    os << *operation << \"\\n\";\n   }\n };\n "}, {"filename": "unittest/Conversion/TritonGPUToLLVM/PTXAsmFormatTest.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,5 +1,5 @@\n #include \"triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h\"\n-#include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n \n@@ -12,7 +12,7 @@ class PTXAsmFormatTest : public ::testing::Test {\n   static constexpr int numValues = 4;\n \n   PTXAsmFormatTest() {\n-    ctx.loadDialect<arith::ArithmeticDialect>();\n+    ctx.loadDialect<arith::ArithDialect>();\n \n     createValues();\n   }"}]