[{"filename": ".github/workflows/documentation.yml", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -24,6 +24,7 @@ jobs:\n         run: |\n           pip3 install tabulate\n           pip3 install cmake\n+          pip3 install sphinx\n \n       #- name: Fetch dependent branches\n       #  run: |\n@@ -33,7 +34,7 @@ jobs:\n         run: |\n           cd docs\n           export PATH=$(python3 -c \"import cmake; print(cmake.CMAKE_BIN_DIR)\"):$PATH\n-          python3 -m sphinx_multiversion . _build/html/\n+          python3 -m sphinx . _build/html/main\n \n       - name: Update docs\n         run: |"}, {"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -27,7 +27,7 @@ jobs:\n         run: |\n           if [ x\"${{ github.repository }}\" == x\"openai/triton\" ]; then\n             echo '::set-output name=matrix-required::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"H100\"]]'\n-            echo '::set-output name=matrix-optional::[]'\n+            echo '::set-output name=matrix-optional::[[\"self-hosted\", \"gfx908\"], [\"self-hosted\", \"arc770\"]]'\n           else\n             echo '::set-output name=matrix-required::[\"ubuntu-latest\"]'\n             echo '::set-output name=matrix-optional::[\"ubuntu-latest\"]'\n@@ -209,10 +209,12 @@ jobs:\n       - name: Install Triton on ROCM\n         if: ${{ env.BACKEND == 'ROCM'}}\n         run: |\n+          git submodule update --init --recursive\n           cd python\n           python3 -m pip install --upgrade pip\n           python3 -m pip install cmake==3.24\n           python3 -m pip install torch==1.13.1 --index-url https://download.pytorch.org/whl/rocm5.2\n+          export TRITON_CODEGEN_AMD_HIP_BACKEND=1\n           python3 -m pip install --no-build-isolation -vvv '.[tests]'\n \n       - name: Install Triton on XPU\n@@ -234,7 +236,7 @@ jobs:\n         if: ${{ env.BACKEND == 'ROCM'}}\n         run: |\n           cd python/test/unit/language\n-          python3 -m pytest --capture=tee-sys -rfs --verbose \"test_core.py::test_empty_kernel\"\n+          python3 -m pytest --capture=tee-sys -rfs --verbose \"test_core.py\"\n \n       - name: Run python tests on XPU\n         if: ${{ env.BACKEND == 'XPU'}}"}, {"filename": ".gitmodules", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -1,3 +1,7 @@\n [submodule \"third_party/intel_xpu_backend\"]\n \tpath = third_party/intel_xpu_backend\n \turl = http://github.com/intel/intel-xpu-backend-for-triton\n+[submodule \"third_party/amd_hip_backend\"]\n+\tpath = third_party/amd_hip_backend\n+\turl = https://github.com/ROCmSoftwarePlatform/triton\n+\tbranch = third_party_backend_2"}, {"filename": "CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -212,7 +212,6 @@ if(TRITON_BUILD_PYTHON_MODULE)\n     TritonNvidiaGPUTransforms\n     TritonLLVMIR\n     TritonPTX\n-    TritonHSACO\n     ${dialect_libs}\n     ${conversion_libs}\n "}, {"filename": "README.md", "status": "modified", "additions": 1, "deletions": 4, "changes": 5, "file_content_changes": "@@ -4,6 +4,7 @@\n \n [![Wheels](https://github.com/openai/triton/actions/workflows/wheels.yml/badge.svg?branch=release/2.0.x)](https://github.com/openai/triton/actions/workflows/wheels.yml)\n \n+We're hiring! If you are interested in working on Triton at OpenAI, we have roles open for [Compiler Engineers](https://openai.com/careers/software-engineer-triton-compiler) and [Kernel Engineers](https://openai.com/careers/kernel-engineer).\n \n **`Documentation`** |\n ------------------- |\n@@ -82,10 +83,6 @@ Version 2.0 is out! New features include:\n \n Community contributions are more than welcome, whether it be to fix bugs or to add new features at [github](https://github.com/openai/triton/). For more detailed instructions, please visit our [contributor's guide](CONTRIBUTING.md).\n \n-If you\u2019re interested in joining our team and working on Triton & GPU kernels, [we\u2019re hiring](https://openai.com/jobs/#acceleration)!\n-\n-\n-\n \n # Compatibility\n "}, {"filename": "bin/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -53,7 +53,6 @@ llvm_update_compile_flags(triton-translate)\n          TritonNvidiaGPUTransforms\n          TritonLLVMIR\n          TritonPTX\n-         TritonHSACO\n          ${dialect_libs}\n          ${conversion_libs}\n          # tests"}, {"filename": "bin/triton-translate.cpp", "status": "modified", "additions": 2, "deletions": 8, "changes": 10, "file_content_changes": "@@ -15,7 +15,6 @@\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n-#include \"triton/Target/HSACO/HSACOTranslation.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n #include \"triton/Target/PTX/PTXTranslation.h\"\n #include \"llvm/IR/LLVMContext.h\"\n@@ -131,16 +130,11 @@ LogicalResult tritonTranslateMain(int argc, char **argv,\n     llvm::errs() << \"Translate to LLVM IR failed\";\n   }\n \n-  if (targetKind == \"llvmir\")\n+  if (targetKind == \"llvmir\") {\n     llvm::outs() << *llvmir << '\\n';\n-  else if (targetKind == \"ptx\")\n+  } else if (targetKind == \"ptx\") {\n     llvm::outs() << ::triton::translateLLVMIRToPTX(*llvmir, SMArch.getValue(),\n                                                    ptxVersion.getValue());\n-  else if (targetKind == \"hsaco\") {\n-    auto [module, hsaco] = ::triton::translateLLVMIRToHSACO(\n-        *llvmir, GCNArch.getValue(), GCNTriple.getValue(),\n-        GCNFeatures.getValue());\n-    llvm::outs() << hsaco;\n   } else {\n     llvm::errs() << \"Error: Unknown target specified: \" << targetKind << \"\\n\";\n     return failure();"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 14, "deletions": 17, "changes": 31, "file_content_changes": "@@ -101,31 +101,28 @@ class GraphLayoutMarker : public GraphDumper {\n   std::string getColor(const Type &type) const;\n };\n \n-// TODO: Interface\n-LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n-                             Attribute &ret);\n+// Infers the encoding of the result of op given the source encoding.\n+std::optional<Attribute> inferDstEncoding(Operation *op, Attribute encoding);\n \n-bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding);\n+// Infers the encoding of the source of op given the result encoding.\n+std::optional<Attribute> inferSrcEncoding(Operation *op, Attribute encoding);\n \n-bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding);\n+bool isExpensiveLoadOrStore(Operation *op);\n \n-// skipInit is True when we only consider the operands of the initOp but\n-// not the initOp itself.\n-int simulateBackwardRematerialization(\n-    Operation *initOp, SetVector<Operation *> &processed,\n-    SetVector<Attribute> &layout, llvm::MapVector<Value, Attribute> &toConvert,\n-    Attribute targetEncoding);\n+bool canFoldIntoConversion(Operation *op, Attribute targetEncoding);\n \n Operation *cloneWithInferType(mlir::OpBuilder &rewriter, Operation *op,\n                               IRMapping &mapping);\n \n-void rematerializeConversionChain(\n-    const llvm::MapVector<Value, Attribute> &toConvert,\n-    mlir::PatternRewriter &rewriter, SetVector<Operation *> &processed,\n-    IRMapping &mapping);\n+// Get backward slice of tensor values starting from the root node along with\n+// encoding propagation.\n+LogicalResult getConvertBackwardSlice(\n+    Value root, SetVector<Value> &slice, Attribute rootEncoding,\n+    DenseMap<Value, Attribute> &layout,\n+    std::function<bool(Operation *)> stopPropagation = nullptr);\n \n-LogicalResult canMoveOutOfLoop(BlockArgument arg,\n-                               SmallVector<Operation *> &cvts);\n+// Populate pattern to remove dead cycles in ForOp.\n+void populateForOpDeadArgumentElimination(RewritePatternSet &patterns);\n \n // Convert an \\param index to a multi-dim coordinate given \\param shape and\n // \\param order."}, {"filename": "include/triton/Target/AMDGCN/AMDGCNTranslation.h", "status": "removed", "additions": 0, "deletions": 19, "changes": 19, "file_content_changes": "@@ -1,19 +0,0 @@\n-#ifndef TRITON_TARGET_AMDGCNTRANSLATION_H\n-#define TRITON_TARGET_AMDGCNTRANSLATION_H\n-\n-#include <string>\n-#include <tuple>\n-\n-namespace llvm {\n-class Module;\n-} // namespace llvm\n-\n-namespace triton {\n-\n-// Translate LLVM IR to AMDGCN code.\n-std::tuple<std::string, std::string>\n-translateLLVMIRToAMDGCN(llvm::Module &module, std::string cc);\n-\n-} // namespace triton\n-\n-#endif"}, {"filename": "include/triton/Target/HSACO/HSACOTranslation.h", "status": "removed", "additions": 0, "deletions": 21, "changes": 21, "file_content_changes": "@@ -1,21 +0,0 @@\n-#ifndef TRITON_TARGET_HSACOTRANSLATION_H\n-#define TRITON_TARGET_HSACOTRANSLATION_H\n-\n-#include <memory>\n-#include <string>\n-#include <tuple>\n-\n-namespace llvm {\n-class Module;\n-} // namespace llvm\n-\n-namespace triton {\n-\n-// Translate TritonGPU IR to HSACO code.\n-std::tuple<std::string, std::string>\n-translateLLVMIRToHSACO(llvm::Module &module, std::string gfx_arch,\n-                       std::string gfx_triple, std::string gfx_features);\n-\n-} // namespace triton\n-\n-#endif"}, {"filename": "include/triton/Tools/Sys/GetEnv.hpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -46,7 +46,7 @@ inline std::string getenv(const char *name) {\n \n inline bool getBoolEnv(const std::string &env) {\n   std::string msg = \"Environment variable \" + env + \" is not recognized\";\n-  assert(triton::ENV_VARS.find(env.c_str()) != triton::ENV_VARS.end() &&\n+  assert(::triton::ENV_VARS.find(env.c_str()) != ::triton::ENV_VARS.end() &&\n          msg.c_str());\n   const char *s = std::getenv(env.c_str());\n   std::string str(s ? s : \"\");"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 13, "deletions": 3, "changes": 16, "file_content_changes": "@@ -473,9 +473,11 @@ struct DFSState {\n   SmallVector<Operation *, 16> topologicalCounts;\n   DenseSet<Operation *> seen;\n \n-  /// We mark each op as ready if all its operands are seen. If an op is ready,\n-  /// we add it to the queue. Otherwise, we keep adding its operands to the\n-  /// ancestors set.\n+  /// We mark each op as ready if all its operands and parents ops are seen. If\n+  /// an op is ready, we add it to the queue. Otherwise, we keep adding its\n+  /// operands to the ancestors set.\n+  /// We always want an op to be scheduled after all its parents to handle\n+  /// correctly cases with scf operations.\n   void addToReadyQueue(Operation *op, DFSSubgraphState &subGraph,\n                        SmallVector<Operation *, 4> &readyQueue) {\n     bool ready = true;\n@@ -486,6 +488,14 @@ struct DFSState {\n         ready = false;\n       }\n     }\n+    Operation *parent = op->getParentOp();\n+    while (parent) {\n+      if (!seen.count(parent)) {\n+        subGraph.push_back(parent);\n+        ready = false;\n+      }\n+      parent = parent->getParentOp();\n+    }\n     if (ready)\n       readyQueue.push_back(op);\n   }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -346,7 +346,7 @@ struct ConvertLayoutOpConversion\n     SmallVector<unsigned> numCTAsEachRep(rank, 1);\n     SmallVector<unsigned> shapePerCTATile = getShapePerCTATile(layout, shape);\n     SmallVector<int64_t> shapePerCTA = getShapePerCTA(layout, shape);\n-    auto elemTy = type.getElementType();\n+    auto elemTy = getTypeConverter()->convertType(type.getElementType());\n \n     int ctaId = 0;\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -203,8 +203,8 @@ static Value loadA(Value tensor, const SharedMemoryObject &smemObj,\n   SmallVector<Value> elems;\n   elems.reserve(has.size() * 2);\n   for (auto item : has) { // has is a map, the key should be ordered.\n-    elems.push_back(item.second.first);\n-    elems.push_back(item.second.second);\n+    elems.push_back(bitcast(item.second.first, i32_ty));\n+    elems.push_back(bitcast(item.second.second, i32_ty));\n   }\n \n   Value res = typeConverter->packLLElements(loc, elems, rewriter, resultTy);\n@@ -327,8 +327,8 @@ static Value loadB(Value tensor, const SharedMemoryObject &smemObj,\n \n   SmallVector<Value> elems;\n   for (auto &item : hbs) { // has is a map, the key should be ordered.\n-    elems.push_back(item.second.first);\n-    elems.push_back(item.second.second);\n+    elems.push_back(bitcast(item.second.first, i32_ty));\n+    elems.push_back(bitcast(item.second.second, i32_ty));\n   }\n \n   Value res = typeConverter->packLLElements(loc, elems, rewriter, resultTy);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 0, "deletions": 8, "changes": 8, "file_content_changes": "@@ -511,14 +511,6 @@ std::function<void(int, int)> getLoadMatrixFn(\n   const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n   auto order = sharedLayout.getOrder();\n \n-  if (tensor.getType()\n-          .cast<RankedTensorType>()\n-          .getElementType()\n-          .isa<mlir::Float8E4M3B11FNUZType, mlir::Float8E4M3FNType>()) {\n-    bool noTrans = (isA ^ (order[0] == 0));\n-    assert(noTrans && \"float8e4b15 must have row-col layout\");\n-  }\n-\n   if (kWidth != (4 / elemBytes))\n     assert(vecPhase == 1 || vecPhase == 4 * kWidth);\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 35, "deletions": 16, "changes": 51, "file_content_changes": "@@ -101,21 +101,40 @@ const std::string Fp8E4M3B15_to_Fp16 =\n     \"shl.b32 $1, b1, 7;                     \\n\"\n     \"}                                      \\n\";\n \n-const std::string Fp16_to_Fp8E4M3B15 =\n-    \"{                                      \\n\"\n-    \".reg .b32 a<2>, b<2>;                  \\n\"\n-    \".reg .b32 max_val;                     \\n\"\n-    \"mov.b32 max_val, 0x3F803F80;           \\n\"\n-    \"and.b32 a0, $1, 0x7fff7fff;            \\n\"\n-    \"and.b32 a1, $2, 0x7fff7fff;            \\n\"\n-    \"min.f16x2 a0, a0, max_val;             \\n\"\n-    \"min.f16x2 a1, a1, max_val;             \\n\"\n-    \"mad.lo.u32 a0, a0, 2, 0x00800080;      \\n\"\n-    \"mad.lo.u32 a1, a1, 2, 0x00800080;      \\n\"\n-    \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\"\n-    \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\"\n-    \"prmt.b32 $0, b0, b1, 0x7531;           \\n\"\n-    \"}\";\n+const std::string Fp16_to_Fp8E4M3B15(bool has_minx2) {\n+  std::string ret;\n+  ret += \"{                                      \\n\"\n+         \".reg .pred p<4>;                       \\n\"\n+         \".reg .b32 a<2>, b<2>;                  \\n\"\n+         \".reg .b16 c<4>;                        \\n\"\n+         \".reg .b16 max_val_f16;                 \\n\"\n+         \".reg .b32 max_val_f16x2;               \\n\"\n+         \"mov.b16 max_val_f16,   0x3F80;         \\n\"\n+         \"mov.b32 max_val_f16x2, 0x3F803F80;     \\n\"\n+         \"and.b32 a0, $1, 0x7fff7fff;            \\n\"\n+         \"and.b32 a1, $2, 0x7fff7fff;            \\n\";\n+  if (has_minx2)\n+    ret += \"min.f16x2 a0, a0, max_val_f16x2;      \\n\"\n+           \"min.f16x2 a1, a1, max_val_f16x2;      \\n\";\n+  else\n+    ret += \"setp.lt.f16x2  p0|p1, a0, max_val_f16x2;   \\n\"\n+           \"setp.lt.f16x2  p2|p3, a1, max_val_f16x2;   \\n\"\n+           \"mov.b32 {c0, c1}, a0;                \\n\"\n+           \"mov.b32 {c2, c3}, a1;                \\n\"\n+           \"selp.b16  c0, c0, max_val_f16, p0;   \\n\"\n+           \"selp.b16  c1, c1, max_val_f16, p1;   \\n\"\n+           \"selp.b16  c2, c2, max_val_f16, p2;   \\n\"\n+           \"selp.b16  c3, c3, max_val_f16, p3;   \\n\"\n+           \"mov.b32 a0, {c0, c1};                \\n\"\n+           \"mov.b32 a1, {c2, c3};                \\n\";\n+  ret += \"mad.lo.u32 a0, a0, 2, 0x00800080;      \\n\"\n+         \"mad.lo.u32 a1, a1, 2, 0x00800080;      \\n\"\n+         \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\"\n+         \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\"\n+         \"prmt.b32 $0, b0, b1, 0x7531;           \\n\"\n+         \"}\";\n+  return ret;\n+}\n \n /* ----- FP8E4M3B15X4 ------ */\n // NOTE: NOT USED RIGHT NOW\n@@ -557,7 +576,7 @@ struct FpToFpOpConversion\n         {{F8E4M3TyID, F16TyID}, Fp8E4M3Nv_to_Fp16},\n         {{F8E5M2TyID, F16TyID}, Fp8E5M2_to_Fp16},\n         // F16 -> F8\n-        {{F16TyID, F8E4M3B15TyID}, Fp16_to_Fp8E4M3B15},\n+        {{F16TyID, F8E4M3B15TyID}, Fp16_to_Fp8E4M3B15(computeCapability >= 80)},\n         {{F16TyID, F8E4M3FNTyID}, Fp16_to_Fp8E4M3B15x4},\n         {{F16TyID, F8E4M3TyID}, Fp16_to_Fp8E4M3Nv},\n         {{F16TyID, F8E5M2TyID}, Fp16_to_Fp8E5M2},"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 26, "deletions": 6, "changes": 32, "file_content_changes": "@@ -914,6 +914,18 @@ struct StoreAsyncOpConversion\n   const TensorPtrMapT *tensorPtrMap;\n };\n \n+namespace {\n+void createBarrier(ConversionPatternRewriter &rewriter, Location loc,\n+                   int numCTAs) {\n+  if (numCTAs == 1) {\n+    barrier();\n+  } else {\n+    rewriter.create<triton::nvidia_gpu::ClusterArriveOp>(loc, false);\n+    rewriter.create<triton::nvidia_gpu::ClusterWaitOp>(loc);\n+  }\n+}\n+} // namespace\n+\n struct AtomicCASOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::AtomicCASOp>,\n       public LoadStoreConversionBase {\n@@ -934,6 +946,10 @@ struct AtomicCASOpConversion\n     auto loc = op.getLoc();\n     MLIRContext *ctx = rewriter.getContext();\n \n+    auto moduleOp = op->getParentOfType<ModuleOp>();\n+    assert(moduleOp && \"Parent ModuleOp not found for AtomicCASOp\");\n+    int numCTAs = triton::gpu::TritonGPUDialect::getNumCTAs(moduleOp);\n+\n     Value llPtr = adaptor.getPtr();\n     Value llCmp = adaptor.getCmp();\n     Value llVal = adaptor.getVal();\n@@ -971,7 +987,7 @@ struct AtomicCASOpConversion\n     atom.global().o(semStr).o(\"cas\").o(\"b32\");\n     atom(dstOpr, ptrOpr, cmpOpr, valOpr).predicate(mask);\n     auto old = ptxBuilderAtomicCAS.launch(rewriter, loc, valueElemTy);\n-    barrier();\n+    createBarrier(rewriter, loc, numCTAs);\n \n     PTXBuilder ptxBuilderStore;\n     auto *dstOprStore = ptxBuilderStore.newAddrOperand(atomPtr, \"r\");\n@@ -981,9 +997,9 @@ struct AtomicCASOpConversion\n     st(dstOprStore, valOprStore).predicate(mask);\n     auto ASMReturnTy = void_ty(ctx);\n     ptxBuilderStore.launch(rewriter, loc, ASMReturnTy);\n-    barrier();\n+    createBarrier(rewriter, loc, numCTAs);\n     Value ret = load(atomPtr);\n-    barrier();\n+    createBarrier(rewriter, loc, numCTAs);\n     rewriter.replaceOp(op, {ret});\n     return success();\n   }\n@@ -1008,7 +1024,11 @@ struct AtomicRMWOpConversion\n                   ConversionPatternRewriter &rewriter) const override {\n     auto loc = op.getLoc();\n     MLIRContext *ctx = rewriter.getContext();\n-    //\n+\n+    auto moduleOp = op->getParentOfType<ModuleOp>();\n+    assert(moduleOp && \"Parent ModuleOp not found for AtomicRMWOp\");\n+    int numCTAs = triton::gpu::TritonGPUDialect::getNumCTAs(moduleOp);\n+\n     auto atomicRmwAttr = op.getAtomicRmwOp();\n \n     Value val = op.getVal();\n@@ -1139,9 +1159,9 @@ struct AtomicRMWOpConversion\n         auto *valOpr = ptxBuilderStore.newOperand(old, tyId);\n         storeShared(ptrOpr, valOpr).predicate(rmwMask);\n         ptxBuilderStore.launch(rewriter, loc, void_ty(ctx));\n-        barrier();\n+        createBarrier(rewriter, loc, numCTAs);\n         Value ret = load(atomPtr);\n-        barrier();\n+        createBarrier(rewriter, loc, numCTAs);\n         rewriter.replaceOp(op, {ret});\n       }\n     }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -11,6 +11,7 @@ using ::mlir::LLVM::delinearize;\n using ::mlir::LLVM::linearize;\n using ::mlir::LLVM::shflSync;\n using ::mlir::LLVM::storeShared;\n+using ::mlir::triton::gpu::getCTASplitNum;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n \n@@ -28,6 +29,12 @@ struct ReduceOpConversion\n   LogicalResult\n   matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n+    // When cross-CTA reduction is implemented in the future, this assertion can\n+    // be removed\n+    assert(isReduceWithinCTA(op) &&\n+           \"Layout optimization passes such as PlanCTAPass and \"\n+           \"RemoveLayoutConversionPass should avoid cross-CTA reduction\");\n+\n     if (ReduceOpHelper(op).isFastReduction())\n       return matchAndRewriteFast(op, adaptor, rewriter);\n     return matchAndRewriteBasic(op, adaptor, rewriter);\n@@ -36,6 +43,15 @@ struct ReduceOpConversion\n private:\n   int computeCapability;\n \n+  bool isReduceWithinCTA(triton::ReduceOp op) const {\n+    auto axis = op.getAxis();\n+    ReduceOpHelper helper(op);\n+    auto srcLayout = helper.getSrcLayout();\n+    auto CTASplitNum = getCTASplitNum(srcLayout);\n+    assert(axis < CTASplitNum.size());\n+    return CTASplitNum[axis] == 1;\n+  }\n+\n   void accumulate(ConversionPatternRewriter &rewriter, Region &combineOp,\n                   SmallVector<Value> &acc, ValueRange cur, bool isFirst) const {\n     if (isFirst) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 15, "deletions": 6, "changes": 21, "file_content_changes": "@@ -541,6 +541,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto tensorTy = valueTy.dyn_cast<RankedTensorType>();\n     Value mask = int_val(1, 1);\n     auto tid = tid_val();\n+    auto clusterCTAId = getClusterCTAId(rewriter, loc);\n     if (tensorTy) {\n       auto layout = tensorTy.getEncoding();\n       auto shape = tensorTy.getShape();\n@@ -576,7 +577,6 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n         auto CTASplitNum = triton::gpu::getCTASplitNum(layout);\n         auto CTAOrder = triton::gpu::getCTAOrder(layout);\n \n-        auto clusterCTAId = getClusterCTAId(rewriter, loc);\n         auto multiDimClusterCTAId =\n             delinearize(rewriter, loc, clusterCTAId, CTAsPerCGA, CTAOrder);\n \n@@ -586,14 +586,23 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n             continue;\n           // This wrapping rule must be consistent with emitCTAOffsetForLayout\n           unsigned splitNum = std::min<unsigned>(shape[dim], CTASplitNum[dim]);\n-          multiDimClusterCTAId[dim] =\n-              urem(multiDimClusterCTAId[dim], i32_val(splitNum));\n-          mask = and_(mask, icmp_eq(multiDimClusterCTAId[dim], _0));\n+          Value repId = udiv(multiDimClusterCTAId[dim], i32_val(splitNum));\n+          // Consider the example where CTAsPerCGA = [4] and CTASplitNum = [2]:\n+          //     CTA0 and CTA2 holds data of block0,\n+          //     CTA1 and CTA3 holds data of block1.\n+          // Only CTA0 and CTA1 are expected to write while CTA2 and CTA3 should\n+          // be masked. We add the following mask:\n+          //     multiDimClusterCTAId[dim] / splitNum == 0\n+          // Actually in all existing cases of multicast, splitNum is always 1.\n+          // The mask is equivalent to:\n+          //     multiDimClusterCTAId[dim] == 0\n+          mask = and_(mask, icmp_eq(repId, _0));\n         }\n       }\n     } else {\n-      // If the tensor is not ranked, then it is a scalar and only thread 0 can\n-      // write\n+      // If the tensor is not ranked, then it is a scalar and only thread 0 of\n+      // CTA0 can write\n+      mask = and_(mask, icmp_eq(clusterCTAId, i32_val(0)));\n       mask = and_(mask, icmp_eq(tid, i32_val(0)));\n     }\n     return mask;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.cpp", "status": "modified", "additions": 3, "deletions": 8, "changes": 11, "file_content_changes": "@@ -130,14 +130,9 @@ Type TritonGPUToLLVMTypeConverter::getElementTypeForStruct(\n   auto mmaParent = dotOpLayout.getParent().dyn_cast<MmaEncodingAttr>();\n   if (!mmaParent)\n     return elemTy;\n-  if (mmaParent.isAmpere()) {\n-    int bitwidth = elemTy.getIntOrFloatBitWidth();\n-    assert(bitwidth <= 32);\n-    return IntegerType::get(ctx, 32);\n-  } else {\n-    assert(mmaParent.isVolta());\n-    return vec_ty(elemTy, 2);\n-  }\n+  int bitwidth = elemTy.getIntOrFloatBitWidth();\n+  assert(bitwidth <= 32);\n+  return IntegerType::get(ctx, 32);\n }\n \n Type TritonGPUToLLVMTypeConverter::convertTritonTensorType("}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -121,6 +121,9 @@ class MoveOpAfterLayoutConversion : public mlir::RewritePattern {\n         cvtArgOp->getDialect()->getTypeID() !=\n             mlir::TypeID::get<arith::ArithDialect>())\n       return mlir::failure();\n+    // not handled in elementwise lowering.\n+    if (isa<arith::TruncIOp, arith::TruncFOp>(cvtArgOp))\n+      return mlir::failure();\n     // only considers conversions to dot operand\n     if (!cvtTy.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n       return mlir::failure();\n@@ -244,7 +247,8 @@ class TritonGPUOptimizeDotOperandsPass\n \n     mlir::RewritePatternSet patterns(context);\n     patterns.add<ConvertTransConvert>(context);\n-    patterns.add<MoveOpAfterLayoutConversion>(context);\n+    if (triton::gpu::TritonGPUDialect::getComputeCapability(m) >= 80)\n+      patterns.add<MoveOpAfterLayoutConversion>(context);\n     patterns.add<FuseTransHopper>(context);\n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n       signalPassFailure();"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 877, "deletions": 478, "changes": 1355, "file_content_changes": "@@ -12,11 +12,11 @@\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n #include \"mlir/Transforms/Passes.h\"\n #include \"mlir/Transforms/RegionUtils.h\"\n+#include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n-\n #include <memory>\n \n using namespace mlir;\n@@ -82,542 +82,914 @@ class DecomposeDotOperand : public mlir::RewritePattern {\n   }\n };\n \n-// It's beneficial to move the conversion\n-// to after the reduce if necessary since it will be\n-// done on a rank-reduced tensor hence cheaper\n-class SimplifyReduceCvt : public mlir::RewritePattern {\n+//\n+class ConvertDotConvert : public mlir::RewritePattern {\n public:\n-  explicit SimplifyReduceCvt(mlir::MLIRContext *context)\n+  ConvertDotConvert(mlir::MLIRContext *context)\n       : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             2, context) {}\n+                             1, context) {}\n \n-  mlir::LogicalResult\n+  LogicalResult\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n-    if (!llvm::isa<triton::gpu::ConvertLayoutOp>(op))\n+    auto dstOp = cast<triton::gpu::ConvertLayoutOp>(op);\n+    auto dotOp = dstOp.getSrc().getDefiningOp<triton::DotOp>();\n+    if (!dotOp)\n       return mlir::failure();\n-    auto convert = llvm::cast<triton::gpu::ConvertLayoutOp>(op);\n-    triton::ReduceOp reduce;\n-    for (auto &use : convert.getResult().getUses()) {\n-      auto owner = llvm::dyn_cast<triton::ReduceOp>(use.getOwner());\n-      if (!owner) {\n-        continue;\n-      }\n+    if (std::distance(dstOp->user_begin(), dstOp->user_end()) != 1 ||\n+        std::distance(dotOp->user_begin(), dotOp->user_end()) != 1)\n+      return mlir::failure();\n+    auto cvtOp =\n+        dotOp.getOperand(2).getDefiningOp<triton::gpu::ConvertLayoutOp>();\n+    if (!cvtOp)\n+      return mlir::failure();\n+    if (!cvtOp.getSrc().getDefiningOp<triton::LoadOp>())\n+      return failure();\n+    auto dstTy = dstOp.getResult().getType().cast<RankedTensorType>();\n+    auto srcTy = cvtOp.getOperand().getType().cast<RankedTensorType>();\n+    if (dstTy != srcTy)\n+      return mlir::failure();\n+\n+    auto _0f = rewriter.create<arith::ConstantOp>(\n+        op->getLoc(), dstTy.getElementType(),\n+        rewriter.getZeroAttr(dstTy.getElementType()));\n+    auto _0 = rewriter.create<triton::SplatOp>(\n+        op->getLoc(), dotOp.getResult().getType(), _0f);\n+    auto newDot = rewriter.create<triton::DotOp>(\n+        op->getLoc(), dotOp.getResult().getType(), dotOp.getOperand(0),\n+        dotOp.getOperand(1), _0, dotOp.getAllowTF32());\n+    auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+        op->getLoc(), dstTy, newDot.getResult());\n+    rewriter.replaceOpWithNewOp<arith::AddFOp>(op, newCvt, cvtOp.getOperand());\n+    return mlir::success();\n+  }\n+};\n+\n+// Class to propagate layout globally within a function.\n+// The current algorithm works by analysis the IR and doing a one shot rewrite\n+// based on the analysis. The algorithm is as follows:\n+// 1. Find all the anchor ops. These are ops that have a layout we want to\n+// preserve.\n+//\n+// 2. Propagate the layout to every op reachable which is a transitive child of\n+// an anchor op until we reach a fix point.\n+// An op can have multiple transitive anchor parents therefore at this stage\n+// it may have multiple layout associated to it.\n+//\n+// 3. Resolve conflicts by deciding which of the multiple layouts the op should\n+// keep. If one of the parents has a different layout than what is picked a\n+// convert operation will be inserted. After this stage each value should have\n+// only one layout associated.\n+//\n+// 4. Rewrite the IR by walking the function following dominance order. Since we\n+// assume the IR is structured we just need to process the regions in the\n+// correct order. For each op rewrite it using the layout decided by the\n+// analysis phase.\n+class LayoutPropagation {\n+public:\n+  // Structure to keep track of the layout associated to a value.\n+  struct LayoutInfo {\n+    LayoutInfo(Attribute encoding) { encodings.insert(encoding); }\n+    LayoutInfo() {}\n+    llvm::SmallSetVector<Attribute, 8> encodings;\n+  };\n+  LayoutPropagation(triton::FuncOp F) : funcOp(F) {}\n+  // Find the anchor ops and set their layout in the data structure.\n+  void initAnchorLayout();\n+  // Recursively Propagate the layout to all the users of the anchor ops until\n+  // we reach a fix point.\n+  void propagateLayout();\n+  // Add layouts given in `Info` to the uses of `value`.\n+  SmallVector<Value> propagateToUsers(Value value, LayoutInfo &info);\n+  // Set the encoding to all the values and fill out the values with new layout\n+  // in `changed`.\n+  void setEncoding(ValueRange values, LayoutInfo &info,\n+                   SmallVector<Value> &changed, Operation *op);\n+  // Resolve cases where a value has multiple layouts associated to it.\n+  void resolveConflicts();\n+  // Rewrite the IR for the full module.\n+  void rewrite();\n+  // Rewrite the IR for a region.\n+  void rewriteRegion(Region &R);\n+  // Rewrite an op based on the layout picked by the analysis.\n+  Operation *rewriteOp(Operation *op);\n+  // Rewrite a for op based on the layout picked by the analysis.\n+  Operation *rewriteForOp(scf::ForOp forOp);\n+  Operation *rewriteWhileOp(scf::WhileOp whileOp);\n+  Operation *rewriteIfOp(scf::IfOp ifOp);\n+  void rewriteYieldOp(scf::YieldOp yieldOp);\n+  void rewriteConditionOp(scf::ConditionOp conditionOp);\n+  void rewriteReduceToScalar(Operation *reduceOp);\n+  Operation *cloneElementwise(OpBuilder &rewriter, Operation *op,\n+                              Attribute encoding);\n+  // Map the original value to the rewritten one.\n+  void map(Value old, Value newV);\n+  // Return the mapped value in the given encoding. This will insert a convert\n+  // if the encoding is different than the encoding decided at resolve time.\n+  Value getValueAs(Value value, Attribute encoding);\n+  // Dump the current stage of layout information.\n+  void dump();\n \n-      // TODO: This only moves conversions from the first argument which is\n-      // fine for argmin/argmax but may not be optimal generally\n-      if (convert.getResult() != owner.getOperands()[0]) {\n+private:\n+  // map from value to layout information.\n+  llvm::MapVector<Value, LayoutInfo> layouts;\n+  // map of the values rewrite based on their encoding.\n+  DenseMap<std::pair<Value, Attribute>, Value> rewriteMapping;\n+  std::vector<Operation *> opToDelete;\n+  triton::FuncOp funcOp;\n+};\n+\n+} // namespace\n+\n+// Look ahead to at the transitive uses and see if there is a convert to mma\n+// operations.\n+static bool hasConvertToMMATransisitiveUse(Operation *op, Attribute encoding) {\n+  SmallVector<Value> queue = {op->getResult(0)};\n+  SetVector<Operation *> forwardSlice;\n+  llvm::SmallDenseSet<Value> seen;\n+  while (!queue.empty()) {\n+    Value currentValue = queue.back();\n+    queue.pop_back();\n+    getForwardSlice(currentValue, &forwardSlice);\n+    for (Operation *op : forwardSlice) {\n+      if (auto convertOp = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n+        if (convertOp.getResult()\n+                .getType()\n+                .cast<RankedTensorType>()\n+                .getEncoding() == encoding)\n+          return true;\n+      }\n+      auto yield = dyn_cast<scf::YieldOp>(op);\n+      if (!yield)\n         continue;\n+      auto forOp = dyn_cast<scf::ForOp>(yield.getOperation()->getParentOp());\n+      if (!forOp)\n+        continue;\n+      for (OpOperand &operand : yield->getOpOperands()) {\n+        Operation *def = operand.get().getDefiningOp();\n+        if (def && forwardSlice.count(def) &&\n+            (seen.insert(operand.get()).second == true))\n+          queue.push_back(forOp.getRegionIterArg(operand.getOperandNumber()));\n       }\n-      reduce = owner;\n-      break;\n     }\n-    if (!reduce)\n-      return mlir::failure();\n+  }\n+  return false;\n+}\n \n-    SmallVector<Value> newOperands = reduce.getOperands();\n+// Return true if the op is an op with a layout we don't want to change. We will\n+// propagate the layout starting from anchor ops.\n+static bool isLayoutAnchor(Operation *op) {\n+  if (isa<triton::LoadOp, triton::StoreOp>(op))\n+    return isExpensiveLoadOrStore(op);\n+  if (isa<triton::DotOp, triton::AtomicRMWOp, triton::AtomicCASOp>(op))\n+    return true;\n+  return false;\n+}\n \n-    newOperands[0] = convert.getOperand();\n-    auto newEncoding =\n-        newOperands[0].getType().cast<RankedTensorType>().getEncoding();\n+void LayoutPropagation::initAnchorLayout() {\n+  funcOp.walk([&](Operation *op) {\n+    if (isLayoutAnchor(op)) {\n+      for (auto result : op->getResults()) {\n+        if (auto tensorType = result.getType().dyn_cast<RankedTensorType>()) {\n+          // Workaround, don't popagate MMA layout unless there is a convert\n+          // back to mma further down to avoid generating reduction with MMA\n+          // layout that may have lower performance.\n+          // This can be improved with more aggressive backward propagation.\n+          if (tensorType.getEncoding().isa<triton::gpu::MmaEncodingAttr>() &&\n+              !hasConvertToMMATransisitiveUse(op, tensorType.getEncoding()))\n+            continue;\n+          layouts.insert({result, tensorType.getEncoding()});\n+        }\n+      }\n+    }\n+  });\n+}\n \n-    // this may generate unsupported conversions in the LLVM codegen\n-    if (newEncoding.isa<triton::gpu::MmaEncodingAttr>()) {\n-      return failure();\n+void LayoutPropagation::setEncoding(ValueRange values, LayoutInfo &info,\n+                                    SmallVector<Value> &changed,\n+                                    Operation *op) {\n+  for (Value value : values) {\n+    if (!value.getType().isa<RankedTensorType>())\n+      continue;\n+    bool hasChanged = false;\n+    for (auto encoding : info.encodings) {\n+      auto dstEncoding = inferDstEncoding(op, encoding);\n+      if (dstEncoding)\n+        hasChanged |= layouts[value].encodings.insert(*dstEncoding);\n     }\n+    if (hasChanged)\n+      changed.push_back(value);\n+  }\n+}\n \n-    // ReduceOp does not support SharedLayout as its src layout, therefore\n-    // ConvertLayoutOp and ReduceOp should not be swapped when the conversion is\n-    // from SharedLayout to DistributedLayout\n-    if (newEncoding.isa<triton::gpu::SharedEncodingAttr>()) {\n-      return failure();\n+SmallVector<Value> LayoutPropagation::propagateToUsers(Value value,\n+                                                       LayoutInfo &info) {\n+  SmallVector<Value> changed;\n+  for (OpOperand &use : value.getUses()) {\n+    Operation *user = use.getOwner();\n+    if (auto forOp = dyn_cast<scf::ForOp>(user)) {\n+      Value arg = forOp.getRegionIterArgForOpOperand(use);\n+      Value result = forOp.getResultForOpOperand(use);\n+      setEncoding({arg, result}, info, changed, user);\n+      continue;\n+    }\n+    if (auto whileOp = dyn_cast<scf::WhileOp>(user)) {\n+      Value arg = whileOp.getBeforeArguments()[use.getOperandNumber()];\n+      setEncoding({arg}, info, changed, user);\n+      continue;\n+    }\n+    if (auto yieldOp = dyn_cast<scf::YieldOp>(user)) {\n+      auto parent = yieldOp->getParentOp();\n+      SmallVector<Value> valuesToPropagate;\n+      if (isa<scf::ForOp, scf::IfOp>(parent))\n+        valuesToPropagate.push_back(parent->getResult(use.getOperandNumber()));\n+      if (auto forOp = dyn_cast<scf::ForOp>(parent))\n+        valuesToPropagate.push_back(\n+            forOp.getRegionIterArg(use.getOperandNumber()));\n+      if (auto whileOp = dyn_cast<scf::WhileOp>(parent)) {\n+        valuesToPropagate.push_back(\n+            whileOp.getBeforeArguments()[use.getOperandNumber()]);\n+        valuesToPropagate.push_back(\n+            whileOp->getOperand(use.getOperandNumber()));\n+      }\n+      if (isa<scf::ForOp, scf::IfOp, scf::WhileOp>(parent))\n+        setEncoding(valuesToPropagate, info, changed, user);\n+      continue;\n+    }\n+    if (auto conditionOp = dyn_cast<scf::ConditionOp>(user)) {\n+      auto whileOp = cast<scf::WhileOp>(conditionOp->getParentOp());\n+      // Skip arg 0 as it is the condition.\n+      unsigned argIndex = use.getOperandNumber() - 1;\n+      Value afterArg = whileOp.getAfterArguments()[argIndex];\n+      Value result = whileOp->getResult(argIndex);\n+      setEncoding({afterArg, result}, info, changed, user);\n+      continue;\n+    }\n+    // Workaround: don't propagate through truncI\n+    if (isa<arith::TruncIOp>(user))\n+      continue;\n+    if (user->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() ||\n+        user->hasTrait<mlir::OpTrait::Elementwise>() ||\n+        isa<triton::ReduceOp, triton::ExpandDimsOp,\n+            triton::gpu::ConvertLayoutOp>(user)) {\n+      setEncoding(user->getResults(), info, changed, user);\n+      continue;\n     }\n+  }\n+  return changed;\n+}\n \n-    for (unsigned i = 1; i < newOperands.size(); ++i) {\n-      auto oldTy = newOperands[i].getType().cast<RankedTensorType>();\n-      RankedTensorType newTy =\n-          RankedTensorType::Builder(oldTy).setEncoding(newEncoding);\n+void LayoutPropagation::propagateLayout() {\n+  SmallVector<Value> queue;\n+  for (auto it : layouts) {\n+    queue.push_back(it.first);\n+  }\n+  while (!queue.empty()) {\n+    Value currentValue = queue.back();\n+    LayoutInfo info = layouts[currentValue];\n+    queue.pop_back();\n+    SmallVector<Value> changed = propagateToUsers(currentValue, info);\n+    queue.insert(queue.end(), changed.begin(), changed.end());\n+  }\n+}\n \n-      newOperands[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-          op->getLoc(), newTy, newOperands[i]);\n+void LayoutPropagation::resolveConflicts() {\n+  for (auto &it : layouts) {\n+    LayoutInfo &info = it.second;\n+    if (info.encodings.size() <= 1)\n+      continue;\n+    // Hacky resolve, prefer block encoding.\n+    // TODO: add a proper heuristic.\n+    Attribute encoding = *info.encodings.begin();\n+    for (Attribute e : info.encodings) {\n+      if (e.isa<triton::gpu::BlockedEncodingAttr>()) {\n+        encoding = e;\n+        break;\n+      }\n     }\n+    info.encodings.clear();\n+    info.encodings.insert(encoding);\n+  }\n+}\n \n-    rewriter.setInsertionPoint(reduce);\n-    auto newReduce = rewriter.create<triton::ReduceOp>(\n-        op->getLoc(), newOperands, reduce.getAxis());\n-    auto &newCombineOp = newReduce.getCombineOp();\n-    rewriter.cloneRegionBefore(reduce.getCombineOp(), newCombineOp,\n-                               newCombineOp.end());\n-\n-    SmallVector<Value> newRet = newReduce.getResult();\n-    auto oldTypes = reduce.getResult().getType();\n-    for (unsigned i = 0; i < reduce.getNumOperands(); ++i) {\n-      // it's still beneficial to move the conversion\n-      // to after the reduce if necessary since it will be\n-      // done on a rank-reduced tensor hence cheaper\n-      if (newRet[i].getType() != oldTypes[i])\n-        newRet[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-            op->getLoc(), oldTypes[i], newRet[i]);\n+void LayoutPropagation::dump() {\n+  for (auto it : layouts) {\n+    llvm::errs() << \"Value: \";\n+    OpPrintingFlags flags;\n+    flags.skipRegions();\n+    it.first.print(llvm::errs(), flags);\n+    llvm::errs() << \" \\n encoding:\\n\";\n+    for (auto encoding : it.second.encodings) {\n+      encoding.print(llvm::errs());\n+      llvm::errs() << \"\\n\";\n     }\n-    rewriter.replaceAllUsesWith(reduce.getResult(), newRet);\n-\n-    return success();\n+    llvm::errs() << \"--\\n\";\n   }\n-};\n+}\n \n-// Layout conversions can't deduce their return type automatically.\n-// IIUC they are therefore not handled by DRR right now\n-class SimplifyConversion : public mlir::RewritePattern {\n-public:\n-  explicit SimplifyConversion(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             4, context) {}\n+void LayoutPropagation::rewrite() { rewriteRegion(funcOp->getRegion(0)); }\n \n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    if (!llvm::isa<triton::gpu::ConvertLayoutOp>(op))\n-      return mlir::failure();\n-    auto convert = llvm::cast<triton::gpu::ConvertLayoutOp>(op);\n-    return ConvertLayoutOp::canonicalize(convert, rewriter);\n+static bool reduceToScalar(Operation *op) {\n+  // For reductions returning a scalar we can change the src encoding without\n+  // affecting the output.\n+  return isa<triton::ReduceOp>(op) &&\n+         !op->getResultTypes()[0].isa<RankedTensorType>();\n+}\n+\n+void LayoutPropagation::rewriteRegion(Region &region) {\n+  SmallVector<Region *> queue = {&region};\n+  while (!queue.empty()) {\n+    Region *currentRegion = queue.back();\n+    queue.pop_back();\n+    for (Operation &op : currentRegion->getOps()) {\n+      bool needRewrite = false;\n+      SmallVector<Value> results = op.getResults();\n+      for (Value result : results) {\n+        auto it = layouts.find(result);\n+        // If we haven't mapped this value skip.\n+        if (it == layouts.end())\n+          continue;\n+        LayoutInfo &info = it->second;\n+        assert(info.encodings.size() == 1 &&\n+               \"we should have resolved to a single encoding\");\n+        auto encoding = result.getType().cast<RankedTensorType>().getEncoding();\n+        // If the encoding is already what we want skip.\n+        if (encoding == *info.encodings.begin())\n+          continue;\n+        needRewrite = true;\n+      }\n+      if (needRewrite) {\n+        Operation *newOp = rewriteOp(&op);\n+        for (Region &R : newOp->getRegions())\n+          queue.push_back(&R);\n+      } else if (auto yieldOp = dyn_cast<scf::YieldOp>(&op)) {\n+        rewriteYieldOp(yieldOp);\n+      } else if (auto conditionOp = dyn_cast<scf::ConditionOp>(&op)) {\n+        rewriteConditionOp(conditionOp);\n+      } else if (reduceToScalar(&op)) {\n+        rewriteReduceToScalar(&op);\n+      } else {\n+        // If we don't need to rewrite the op we still need to remap the\n+        // operands.\n+        for (OpOperand &operand : op.getOpOperands()) {\n+          auto it = layouts.find(operand.get());\n+          if (it == layouts.end())\n+            continue;\n+          Attribute encoding =\n+              operand.get().getType().cast<RankedTensorType>().getEncoding();\n+          Value newOperand = getValueAs(operand.get(), encoding);\n+          op.setOperand(operand.getOperandNumber(), newOperand);\n+        }\n+        for (Region &R : op.getRegions())\n+          queue.push_back(&R);\n+      }\n+    }\n   }\n-};\n+  for (Operation *op : llvm::reverse(opToDelete))\n+    op->erase();\n+}\n \n-// -----------------------------------------------------------------------------\n-//\n-// -----------------------------------------------------------------------------\n+void LayoutPropagation::map(Value old, Value newV) {\n+  rewriteMapping[{old, newV.getType().cast<RankedTensorType>().getEncoding()}] =\n+      newV;\n+}\n \n-// op(cvt(arg_0), arg_1, ..., arg_n)\n-// -> cvt(op(arg_0, cvt(arg_1), ..., cvt(arg_n)))\n-void pushConversionForward(triton::gpu::ConvertLayoutOp cvt,\n-                           SetVector<Operation *> &cvtSlices,\n-                           mlir::PatternRewriter &rewriter) {\n-  auto srcEncoding =\n-      cvt.getOperand().getType().cast<RankedTensorType>().getEncoding();\n-  auto dstEncoding =\n-      cvt.getResult().getType().cast<RankedTensorType>().getEncoding();\n-  IRMapping mapping;\n-  auto op = cvtSlices.front();\n-  for (Value arg : op->getOperands()) {\n-    if (arg.getDefiningOp() == cvt)\n-      mapping.map(arg, cvt.getOperand());\n-    else {\n-      auto oldType = arg.getType().dyn_cast<RankedTensorType>();\n-      // TODO: we may be creating block pointer load/store with mismatching\n-      // pointer type.\n-      if (!oldType)\n-        continue;\n-      auto newType = RankedTensorType::get(\n-          oldType.getShape(), oldType.getElementType(), srcEncoding);\n-      auto cvtI = rewriter.create<triton::gpu::ConvertLayoutOp>(arg.getLoc(),\n-                                                                newType, arg);\n-      if (Operation *argOp = arg.getDefiningOp())\n-        cvtI->moveAfter(argOp);\n-      mapping.map(arg, cvtI);\n+Value LayoutPropagation::getValueAs(Value value, Attribute encoding) {\n+  if (auto tensorType = value.getType().dyn_cast<RankedTensorType>()) {\n+    Value rewrittenValue;\n+    auto layoutIt = layouts.find(value);\n+    if (layoutIt == layouts.end()) {\n+      rewrittenValue = value;\n+    } else {\n+      assert(layoutIt->second.encodings.size() == 1 &&\n+             \"we should have resolved to a single encoding\");\n+      Attribute encodingPicked = *(layoutIt->second.encodings.begin());\n+      if (encodingPicked == tensorType.getEncoding())\n+        rewrittenValue = value;\n+      else\n+        rewrittenValue = rewriteMapping[{value, encodingPicked}];\n     }\n+    assert(rewrittenValue);\n+    if (rewrittenValue.getType().cast<RankedTensorType>().getEncoding() ==\n+        encoding)\n+      return rewrittenValue;\n+    OpBuilder rewriter(value.getContext());\n+    rewriter.setInsertionPointAfterValue(rewrittenValue);\n+    auto tmpType = RankedTensorType::get(tensorType.getShape(),\n+                                         tensorType.getElementType(), encoding);\n+    Value converted = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+        value.getLoc(), tmpType, rewrittenValue);\n+    // TODO: we could cache the conversion.\n+    return converted;\n   }\n-  rewriter.setInsertionPoint(op);\n-  if (op->getNumResults() == 0) {\n-    Operation *newOp = cloneWithInferType(rewriter, op, mapping);\n-    rewriter.eraseOp(op);\n-    return;\n+  return value;\n+}\n+\n+Operation *LayoutPropagation::cloneElementwise(OpBuilder &rewriter,\n+                                               Operation *op,\n+                                               Attribute encoding) {\n+  Operation *newOp = rewriter.clone(*op);\n+  for (OpOperand &operand : op->getOpOperands())\n+    newOp->setOperand(\n+        operand.getOperandNumber(),\n+        getValueAs(operand.get(), *inferSrcEncoding(op, encoding)));\n+  for (unsigned i = 0, e = op->getNumResults(); i < e; ++i) {\n+    auto origType = op->getResult(i).getType().dyn_cast<RankedTensorType>();\n+    if (!origType)\n+      continue;\n+    auto newType = RankedTensorType::get(origType.getShape(),\n+                                         origType.getElementType(), encoding);\n+    newOp->getResult(i).setType(newType);\n   }\n-  auto *newOp = cloneWithInferType(rewriter, op, mapping);\n-  auto newType = newOp->getResult(0).getType().cast<RankedTensorType>();\n-  auto newCvtType = RankedTensorType::get(\n-      newType.getShape(), newType.getElementType(), dstEncoding);\n-  auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-      newOp->getLoc(), newCvtType, newOp->getResult(0));\n-  rewriter.replaceOp(op, newCvt->getResults());\n+  return newOp;\n }\n \n-//\n-class MoveConvertOutOfIf : public mlir::RewritePattern {\n-public:\n-  explicit MoveConvertOutOfIf(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(scf::IfOp::getOperationName(), 2, context) {}\n+Operation *LayoutPropagation::rewriteForOp(scf::ForOp forOp) {\n+  SmallVector<Value> operands;\n+  OpBuilder rewriter(forOp);\n+  for (auto [operand, result] :\n+       llvm::zip(forOp.getInitArgs(), forOp.getResults())) {\n+    Value convertedOperand = operand;\n+    if (layouts.count(result))\n+      convertedOperand =\n+          getValueAs(operand, *layouts[result].encodings.begin());\n+    operands.push_back(convertedOperand);\n+  }\n+  auto newForOp = rewriter.create<scf::ForOp>(\n+      forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n+      forOp.getStep(), operands);\n \n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto ifOp = cast<scf::IfOp>(*op);\n-    // If \u201cscf.if\u201d defines no values, \u201cscf.yield\u201d will be inserted implicitly.\n-    // However, \"scf.else\" is not required to be present, so we need to check\n-    // if it exists.\n-    auto thenYield = ifOp.thenYield();\n-    int numOps = thenYield.getNumOperands();\n-    SmallVector<Value> newThenYieldOps = thenYield.getOperands();\n-    SetVector<Operation *> thenCvts;\n-    SmallVector<Type> newRetTypes;\n-\n-    bool hasElse = !ifOp.getElseRegion().empty();\n-\n-    scf::YieldOp elseYield;\n-    SmallVector<Value> newElseYieldOps;\n-    SetVector<Operation *> elseCvts;\n-    if (hasElse) {\n-      elseYield = ifOp.elseYield();\n-      newElseYieldOps = elseYield.getOperands();\n-    }\n+  newForOp.getBody()->getOperations().splice(\n+      newForOp.getBody()->getOperations().begin(),\n+      forOp.getBody()->getOperations());\n \n-    IRMapping mapping;\n-    for (size_t i = 0; i < numOps; i++) {\n-      auto thenCvt =\n-          thenYield.getOperand(i).getDefiningOp<triton::gpu::ConvertLayoutOp>();\n-      if (hasElse) {\n-        auto elseYield = ifOp.elseYield();\n-        auto elseCvt = elseYield.getOperand(i)\n-                           .getDefiningOp<triton::gpu::ConvertLayoutOp>();\n-        if (thenCvt && elseCvt &&\n-            std::distance(elseCvt->user_begin(), elseCvt->user_end()) == 1 &&\n-            std::distance(thenCvt->user_begin(), thenCvt->user_end()) == 1 &&\n-            thenCvt.getOperand().getType() == elseCvt.getOperand().getType()) {\n-          // If thenCvt and elseCvt's type are the same, it means a single\n-          // conversion is enough to replace both of them. We can move the\n-          // conversion out of scf.if and replace both thenCvt and elseCvt with\n-          // the new conversion.\n-          mapping.map(thenCvt.getResult(), thenCvt.getOperand());\n-          thenCvts.insert((Operation *)thenCvt);\n-          newRetTypes.push_back(thenCvt.getOperand().getType());\n-          mapping.map(elseCvt.getResult(), elseCvt.getOperand());\n-          elseCvts.insert((Operation *)elseCvt);\n-        } else\n-          // Cannot move out of scf.if because thenCvt != elseCvt\n-          // Moving it out of scf.if will introduce a new conversion\n-          newRetTypes.push_back(thenYield.getOperand(i).getType());\n-      } else {\n-        if (thenCvt &&\n-            std::distance(thenCvt->user_begin(), thenCvt->user_end()) == 1) {\n-          // If there's only a single use of the conversion then we can move it\n-          mapping.map(thenCvt.getResult(), thenCvt.getOperand());\n-          thenCvts.insert((Operation *)thenCvt);\n-          newRetTypes.push_back(thenCvt.getOperand().getType());\n-        } else\n-          // Cannot move out of scf.if because either there's another use of\n-          // the conversion or there's no conversion at all\n-          newRetTypes.push_back(thenYield.getOperand(i).getType());\n-      }\n+  for (auto [oldResult, newResult] :\n+       llvm::zip(forOp.getResults(), newForOp.getResults())) {\n+    if (oldResult.getType() == newResult.getType()) {\n+      oldResult.replaceAllUsesWith(newResult);\n+      continue;\n     }\n-    if (mapping.getValueMap().empty())\n-      return mlir::failure();\n+    map(oldResult, newResult);\n+  }\n \n-    auto newIfOp = rewriter.create<scf::IfOp>(ifOp.getLoc(), newRetTypes,\n-                                              ifOp.getCondition(), hasElse);\n-    auto rematerialize = [&](Block *block, SetVector<Operation *> &cvts) {\n-      for (Operation &op : block->getOperations()) {\n-        if (cvts.contains(&op)) {\n-          if (mapping.contains(op.getOperand(0)))\n-            mapping.map(op.getResult(0), mapping.lookup(op.getOperand(0)));\n-          continue;\n-        }\n-        cloneWithInferType(rewriter, &op, mapping);\n-      }\n-    };\n-    rewriter.setInsertionPointToEnd(newIfOp.thenBlock());\n-    rematerialize(ifOp.thenBlock(), thenCvts);\n-    if (hasElse) {\n-      rewriter.setInsertionPointToEnd(newIfOp.elseBlock());\n-      rematerialize(ifOp.elseBlock(), elseCvts);\n+  for (auto [oldArg, newArg] : llvm::zip(forOp.getBody()->getArguments(),\n+                                         newForOp.getBody()->getArguments())) {\n+    if (oldArg.getType() == newArg.getType()) {\n+      oldArg.replaceAllUsesWith(newArg);\n+      continue;\n     }\n+    map(oldArg, newArg);\n+  }\n+  return newForOp.getOperation();\n+}\n \n-    rewriter.setInsertionPointAfter(newIfOp);\n-    SmallVector<Value> newRetValues = newIfOp.getResults();\n-    for (size_t i = 0; i < numOps; i++) {\n-      if (newIfOp.getResult(i).getType() != ifOp.getResult(i).getType()) {\n-        newRetValues[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-            newIfOp.getLoc(), ifOp.getResult(i).getType(),\n-            newIfOp.getResult(i));\n-      }\n+Operation *LayoutPropagation::rewriteWhileOp(scf::WhileOp whileOp) {\n+  SmallVector<Value> operands;\n+  SmallVector<Type> returnTypes;\n+  OpBuilder rewriter(whileOp);\n+  for (auto [operand, arg] :\n+       llvm::zip(whileOp->getOperands(), whileOp.getBeforeArguments())) {\n+    Value convertedOperand = operand;\n+    if (layouts.count(arg))\n+      convertedOperand = getValueAs(operand, *layouts[arg].encodings.begin());\n+    operands.push_back(convertedOperand);\n+  }\n+  for (Value ret : whileOp.getResults()) {\n+    auto it = layouts.find(ret);\n+    if (it == layouts.end()) {\n+      returnTypes.push_back(ret.getType());\n+      continue;\n     }\n-\n-    rewriter.replaceOp(op, newRetValues);\n-    return mlir::success();\n+    auto origType = ret.getType().dyn_cast<RankedTensorType>();\n+    auto newType =\n+        RankedTensorType::get(origType.getShape(), origType.getElementType(),\n+                              it->second.encodings[0]);\n+    returnTypes.push_back(newType);\n   }\n-};\n \n-//\n-class RematerializeForward : public mlir::RewritePattern {\n-public:\n-  explicit RematerializeForward(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             1, context) {}\n+  auto newWhileOp =\n+      rewriter.create<scf::WhileOp>(whileOp.getLoc(), returnTypes, operands);\n+  SmallVector<Type> argsTypesBefore;\n+  for (Value operand : operands)\n+    argsTypesBefore.push_back(operand.getType());\n+  SmallVector<Location> bbArgLocsBefore(argsTypesBefore.size(),\n+                                        whileOp.getLoc());\n+  SmallVector<Location> bbArgLocsAfter(returnTypes.size(), whileOp.getLoc());\n+  rewriter.createBlock(&newWhileOp.getBefore(), {}, argsTypesBefore,\n+                       bbArgLocsBefore);\n+  rewriter.createBlock(&newWhileOp.getAfter(), {}, returnTypes, bbArgLocsAfter);\n \n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *cvtOp,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto cvt = dyn_cast<triton::gpu::ConvertLayoutOp>(*cvtOp);\n-    auto srcEncoding =\n-        cvt.getOperand().getType().cast<RankedTensorType>().getEncoding();\n-    auto dstEncoding =\n-        cvt.getResult().getType().cast<RankedTensorType>().getEncoding();\n-    if (srcEncoding.isa<triton::gpu::SharedEncodingAttr>() ||\n-        dstEncoding.isa<triton::gpu::SharedEncodingAttr>())\n-      return failure();\n-    // heuristics for flash attention\n-    if (srcEncoding.isa<triton::gpu::SliceEncodingAttr>())\n-      return failure();\n-    // For cases like:\n-    // %0 = convert_layout %arg0\n-    // We should try to move %0 out of scf.for first, if it couldn't be moved\n-    // out additional conversions will be added to the loop body.\n-    if (!cvt.getOperand().getDefiningOp() &&\n-        isa<scf::ForOp>(cvt->getParentOp()))\n-      return failure();\n+  for (int i = 0; i < whileOp.getNumRegions(); ++i) {\n+    newWhileOp->getRegion(i).front().getOperations().splice(\n+        newWhileOp->getRegion(i).front().getOperations().begin(),\n+        whileOp->getRegion(i).front().getOperations());\n+  }\n \n-    SetVector<Operation *> cvtSlices;\n-    auto filter = [&](Operation *op) {\n-      return op->getBlock() == cvt->getBlock() &&\n-             !isa<triton::gpu::ConvertLayoutOp, scf::YieldOp>(op) &&\n-             !(isa<triton::ReduceOp>(op) &&\n-               !op->getResult(0).getType().isa<RankedTensorType>());\n-    };\n-    mlir::getForwardSlice(cvt.getResult(), &cvtSlices, {filter});\n-    if (cvtSlices.empty())\n-      return failure();\n+  auto remapArg = [&](Value oldVal, Value newVal) {\n+    if (oldVal.getType() == newVal.getType())\n+      oldVal.replaceAllUsesWith(newVal);\n+    else\n+      map(oldVal, newVal);\n+  };\n+  for (auto [oldResult, newResult] :\n+       llvm::zip(whileOp.getResults(), newWhileOp.getResults()))\n+    remapArg(oldResult, newResult);\n+  for (auto [oldArg, newArg] :\n+       llvm::zip(whileOp.getBeforeArguments(), newWhileOp.getBeforeArguments()))\n+    remapArg(oldArg, newArg);\n+  for (auto [oldArg, newArg] :\n+       llvm::zip(whileOp.getAfterArguments(), newWhileOp.getAfterArguments()))\n+    remapArg(oldArg, newArg);\n+  return newWhileOp.getOperation();\n+}\n \n-    for (Operation *op : cvtSlices) {\n-      // don't rematerialize anything expensive\n-      if (isExpensiveToRemat(op, srcEncoding))\n-        return failure();\n-      // don't rematerialize non-element-wise\n-      if (!op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n-          !op->hasTrait<mlir::OpTrait::Elementwise>() &&\n-          !isa<triton::StoreOp, triton::AssertOp, triton::PrintOp,\n-               triton::ReduceOp>(op))\n-        return failure();\n-      // don't rematerialize if it adds an extra conversion that can't\n-      // be removed\n-      for (Value arg : op->getOperands()) {\n-        Operation *argOp = arg.getDefiningOp();\n-        SetVector<Operation *> processed;\n-        SetVector<Attribute> layout;\n-        llvm::MapVector<Value, Attribute> toConvert;\n-        int numAddedConvs = simulateBackwardRematerialization(\n-            argOp, processed, layout, toConvert, srcEncoding);\n-        if (argOp && !isa<triton::gpu::ConvertLayoutOp>(argOp) &&\n-            cvtSlices.count(argOp) == 0 && numAddedConvs > 0)\n-          return failure();\n-      }\n+Operation *LayoutPropagation::rewriteIfOp(scf::IfOp ifOp) {\n+  SmallVector<Value> operands;\n+  OpBuilder rewriter(ifOp);\n+  SmallVector<Type> newResultTypes(ifOp->getResultTypes());\n+  for (unsigned i = 0, e = ifOp->getNumResults(); i < e; ++i) {\n+    auto it = layouts.find(ifOp->getResult(i));\n+    if (it == layouts.end())\n+      continue;\n+    auto origType = ifOp->getResult(i).getType().cast<RankedTensorType>();\n+    Attribute encoding = *(it->second.encodings.begin());\n+    newResultTypes[i] = RankedTensorType::get(\n+        origType.getShape(), origType.getElementType(), encoding);\n+  }\n+  auto newIfOp = rewriter.create<scf::IfOp>(ifOp.getLoc(), newResultTypes,\n+                                            ifOp.getCondition(), true, true);\n+  newIfOp.getThenRegion().takeBody(ifOp.getThenRegion());\n+  newIfOp.getElseRegion().takeBody(ifOp.getElseRegion());\n+  for (auto [oldResult, newResult] :\n+       llvm::zip(ifOp.getResults(), newIfOp.getResults())) {\n+    if (oldResult.getType() == newResult.getType()) {\n+      oldResult.replaceAllUsesWith(newResult);\n+      continue;\n     }\n+    map(oldResult, newResult);\n+  }\n+  return newIfOp.getOperation();\n+}\n \n-    // Call SimplifyReduceCvt instead of the general push conversion forward\n-    if (isa<triton::ReduceOp>(cvtSlices.front()))\n-      return failure();\n+void LayoutPropagation::rewriteYieldOp(scf::YieldOp yieldOp) {\n+  Operation *parentOp = yieldOp->getParentOp();\n+  for (OpOperand &operand : yieldOp->getOpOperands()) {\n+    Type yieldType = operand.get().getType();\n+    if (isa<scf::ForOp, scf::IfOp>(parentOp))\n+      yieldType = parentOp->getResult(operand.getOperandNumber()).getType();\n+    if (auto whileOp = dyn_cast<scf::WhileOp>(parentOp))\n+      yieldType =\n+          whileOp.getBeforeArguments()[operand.getOperandNumber()].getType();\n+    auto tensorType = yieldType.dyn_cast<RankedTensorType>();\n+    if (!tensorType)\n+      continue;\n+    Value newOperand = getValueAs(operand.get(), tensorType.getEncoding());\n+    yieldOp->setOperand(operand.getOperandNumber(), newOperand);\n+  }\n+}\n \n-    pushConversionForward(cvt, cvtSlices, rewriter);\n-    return success();\n+void LayoutPropagation::rewriteConditionOp(scf::ConditionOp conditionOp) {\n+  scf::WhileOp whileOp = cast<scf::WhileOp>(conditionOp->getParentOp());\n+  for (unsigned i = 1; i < conditionOp->getNumOperands(); ++i) {\n+    OpOperand &operand = conditionOp->getOpOperand(i);\n+    Type argType = whileOp->getResult(operand.getOperandNumber() - 1).getType();\n+    auto tensorType = argType.dyn_cast<RankedTensorType>();\n+    if (!tensorType)\n+      continue;\n+    Value newOperand = getValueAs(operand.get(), tensorType.getEncoding());\n+    conditionOp->setOperand(operand.getOperandNumber(), newOperand);\n   }\n-};\n+}\n \n-// Layout conversions are expensive. They require going through\n-// shared memory, which is orders of magnitude slower than\n-// other non-i/o operations in the dialect.\n-// It therefore makes sense to remove them whenever possible,\n-// even if it means rematerializing all values whose definitions\n-// are reachable from it without passing through any memory operation.\n-class RematerializeBackward : public mlir::RewritePattern {\n-public:\n-  explicit RematerializeBackward(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             3, context) {}\n+void LayoutPropagation::rewriteReduceToScalar(Operation *reduceOp) {\n+  OpBuilder rewriter(reduceOp);\n+  Attribute srcEncoding;\n+  // Since all the operands need to have the same encoding pick the first one\n+  // and use it for all the operands.\n+  for (Value operand : reduceOp->getOperands()) {\n+    auto it = layouts.find(operand);\n+    if (it != layouts.end()) {\n+      srcEncoding = it->second.encodings[0];\n+      break;\n+    }\n+  }\n+  if (!srcEncoding)\n+    return;\n+  for (OpOperand &operand : reduceOp->getOpOperands()) {\n+    Value newOperand = getValueAs(operand.get(), srcEncoding);\n+    reduceOp->setOperand(operand.getOperandNumber(), newOperand);\n+  }\n+}\n \n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *cvt,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    if (!llvm::isa<triton::gpu::ConvertLayoutOp>(cvt))\n-      return mlir::failure();\n-    // we don't touch block arguments\n-    Operation *op = cvt->getOperand(0).getDefiningOp();\n-    if (!op)\n-      return mlir::failure();\n-    // we don't want to rematerialize any conversion to/from shared\n-    if (triton::gpu::isSharedEncoding(cvt->getResults()[0]) ||\n-        triton::gpu::isSharedEncoding(cvt->getOperand(0)))\n-      return mlir::failure();\n-    // we don't handle conversions to DotOperandEncodingAttr\n-    // this is a heuristics to accommodate fused attention\n-    auto targetType = cvt->getResultTypes()[0].cast<RankedTensorType>();\n-    if (targetType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n-      return mlir::failure();\n-    // DFS\n-    SetVector<Operation *> processed;\n-    SetVector<Attribute> layout;\n-    llvm::MapVector<Value, Attribute> toConvert;\n-    if (simulateBackwardRematerialization(cvt, processed, layout, toConvert,\n-                                          targetType.getEncoding()) > 0)\n-      return mlir::failure();\n+Operation *LayoutPropagation::rewriteOp(Operation *op) {\n+  opToDelete.push_back(op);\n+  if (auto forOp = dyn_cast<scf::ForOp>(op))\n+    return rewriteForOp(forOp);\n+  if (auto whileOp = dyn_cast<scf::WhileOp>(op))\n+    return rewriteWhileOp(whileOp);\n+  if (auto ifOp = dyn_cast<scf::IfOp>(op))\n+    return rewriteIfOp(ifOp);\n+  OpBuilder rewriter(op);\n+  Attribute encoding = *layouts[op->getResult(0)].encodings.begin();\n+  if (auto convertOp = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n+    Attribute srcEncoding =\n+        convertOp.getOperand().getType().cast<RankedTensorType>().getEncoding();\n+    auto it = layouts.find(convertOp.getOperand());\n+    if (it != layouts.end())\n+      srcEncoding = *(it->second.encodings.begin());\n+    Value src = getValueAs(convertOp.getOperand(), srcEncoding);\n+    auto tensorType = op->getResult(0).getType().cast<RankedTensorType>();\n+    auto newType = RankedTensorType::get(tensorType.getShape(),\n+                                         tensorType.getElementType(), encoding);\n+    auto cvt = rewriter.create<triton::gpu::ConvertLayoutOp>(op->getLoc(),\n+                                                             newType, src);\n+    map(op->getResult(0), cvt.getResult());\n+    return cvt.getOperation();\n+  }\n+  if (canFoldIntoConversion(op, encoding)) {\n+    Operation *newOp = rewriter.clone(*op);\n+    auto tensorType = op->getResult(0).getType().cast<RankedTensorType>();\n+    auto newType = RankedTensorType::get(tensorType.getShape(),\n+                                         tensorType.getElementType(), encoding);\n+    auto cvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+        op->getLoc(), newType, newOp->getResult(0));\n+    map(op->getResult(0), cvt.getResult());\n+    return cvt.getOperation();\n+  }\n+  if (op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() ||\n+      op->hasTrait<mlir::OpTrait::Elementwise>() ||\n+      isa<triton::ReduceOp, triton::ExpandDimsOp, triton::gpu::ConvertLayoutOp>(\n+          op)) {\n+    Operation *newOp = cloneElementwise(rewriter, op, encoding);\n+    for (auto [oldResult, newResult] :\n+         llvm::zip(op->getResults(), newOp->getResults()))\n+      map(oldResult, newResult);\n+    return newOp;\n+  }\n+  assert(0 && \"unexpected op in rewrite\");\n+  return nullptr;\n+}\n \n-    IRMapping mapping;\n-    rematerializeConversionChain(toConvert, rewriter, processed, mapping);\n-    rewriter.replaceOp(cvt, mapping.lookup(cvt->getOperand(0)));\n+static bool canBeRemat(Operation *op) {\n+  if (isa<triton::LoadOp, triton::StoreOp>(op))\n+    return !isExpensiveLoadOrStore(op);\n+  if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n+          triton::gpu::InsertSliceAsyncOp, triton::AtomicRMWOp,\n+          triton::AtomicCASOp, triton::DotOp>(op))\n+    return false;\n+  if (isa<scf::IfOp, scf::WhileOp, scf::ConditionOp>(op))\n+    return false;\n \n-    return mlir::success();\n-  }\n-};\n+  return true;\n+}\n \n-// -----------------------------------------------------------------------------\n-//\n-// -----------------------------------------------------------------------------\n+// Replace ForOp with a new ForOp with extra operands. The YieldOp is not\n+// updated and needs to be updated separatly for the loop to be correct.\n+static scf::ForOp replaceForOpWithNewSignature(OpBuilder &rewriter,\n+                                               scf::ForOp loop,\n+                                               ValueRange newIterOperands) {\n+  OpBuilder::InsertionGuard g(rewriter);\n+  rewriter.setInsertionPoint(loop);\n \n-class MoveConvertOutOfLoop : public mlir::RewritePattern {\n-public:\n-  explicit MoveConvertOutOfLoop(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(scf::ForOp::getOperationName(), 1, context) {}\n-\n-  SmallVector<Value, 4>\n-  rematerializeForLoop(mlir::PatternRewriter &rewriter, scf::ForOp &forOp,\n-                       size_t i, RankedTensorType newType,\n-                       triton::gpu::ConvertLayoutOp origConversion) const {\n-    // Rewrite init argument\n-    auto origType = forOp.getInitArgs()[i].getType().cast<RankedTensorType>();\n-    SmallVector<Value, 4> newInitArgs = forOp.getInitArgs();\n-    newInitArgs[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        newInitArgs[i].getLoc(), newType, newInitArgs[i]);\n-    // Clone for loop\n-    auto newForOp = rewriter.create<scf::ForOp>(\n-        forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n-        forOp.getStep(), newInitArgs);\n-    newForOp->moveBefore(forOp);\n-    rewriter.setInsertionPointToStart(newForOp.getBody());\n-    IRMapping mapping;\n-    for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n-      mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n-    mapping.map(origConversion.getResult(), newForOp.getRegionIterArgs()[i]);\n-\n-    mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n-    for (Operation &op : forOp.getBody()->without_terminator()) {\n-      if (dyn_cast<triton::gpu::ConvertLayoutOp>(op) == origConversion)\n-        continue;\n+  // Create a new loop before the existing one, with the extra operands.\n+  rewriter.setInsertionPoint(loop);\n+  auto operands = llvm::to_vector<4>(loop.getIterOperands());\n+  operands.append(newIterOperands.begin(), newIterOperands.end());\n+  scf::ForOp newLoop = rewriter.create<scf::ForOp>(\n+      loop.getLoc(), loop.getLowerBound(), loop.getUpperBound(), loop.getStep(),\n+      operands);\n+  newLoop.getBody()->erase();\n \n-      bool convert = llvm::any_of(op.getOperands(), [&](auto operand) {\n-        return operand == origConversion.getOperand();\n-      });\n-      auto convertLayout = [&](Value operand, Value value, Attribute encoding) {\n-        auto tensorType = value.getType().cast<RankedTensorType>();\n-        auto cvtType = RankedTensorType::get(\n-            tensorType.getShape(), tensorType.getElementType(), encoding);\n-        auto cvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-            op.getLoc(), cvtType, value);\n-        mapping.map(operand, cvt);\n-      };\n-      DenseMap<Value, Value> cvtValues;\n-      if (convert) {\n-        for (auto operand : op.getOperands()) {\n-          if (operand == origConversion.getOperand() ||\n-              !isa<RankedTensorType>(operand.getType()))\n-            continue;\n-          auto value = mapping.lookupOrDefault(operand);\n-          // Convert to the new type\n-          convertLayout(operand, value, newType.getEncoding());\n-          // Other ops don't use the converted value and we need to restore\n-          cvtValues[operand] = value;\n+  newLoop.getLoopBody().getBlocks().splice(\n+      newLoop.getLoopBody().getBlocks().begin(),\n+      loop.getLoopBody().getBlocks());\n+  for (Value operand : newIterOperands)\n+    newLoop.getBody()->addArgument(operand.getType(), operand.getLoc());\n+\n+  for (auto it : llvm::zip(loop.getResults(), newLoop.getResults().take_front(\n+                                                  loop.getNumResults())))\n+    std::get<0>(it).replaceAllUsesWith(std::get<1>(it));\n+  return newLoop;\n+}\n+\n+static void rewriteSlice(SetVector<Value> &slice,\n+                         DenseMap<Value, Attribute> &layout,\n+                         ConvertLayoutOp convertOp, IRMapping &mapping) {\n+  SetVector<Operation *> opsToRewrite;\n+  for (Value v : slice) {\n+    if (v.getDefiningOp()) {\n+      opsToRewrite.insert(v.getDefiningOp());\n+    } else {\n+      opsToRewrite.insert(v.cast<BlockArgument>().getOwner()->getParentOp());\n+      // We also need to rewrite the yield op.\n+      opsToRewrite.insert(v.cast<BlockArgument>().getOwner()->getTerminator());\n+    }\n+  }\n+  opsToRewrite = multiRootTopologicalSort(opsToRewrite);\n+\n+  SmallVector<Operation *> deadLoops;\n+  OpBuilder builder(slice.begin()->getContext());\n+  for (Operation *op : opsToRewrite) {\n+    if (auto forOp = dyn_cast<scf::ForOp>(op)) {\n+      // Keep a mapping of the operands index to the new operands index.\n+      SmallVector<std::pair<size_t, size_t>> argMapping;\n+      SmallVector<Value> newOperands;\n+      for (auto arg : forOp.getRegionIterArgs()) {\n+        if (slice.count(arg)) {\n+          OpOperand &initVal = forOp.getOpOperandForRegionIterArg(arg);\n+          argMapping.push_back(\n+              std::make_pair(*forOp.getIterArgNumberForOpOperand(initVal),\n+                             forOp.getNumIterOperands() + newOperands.size()));\n+          newOperands.push_back(mapping.lookup(initVal.get()));\n         }\n       }\n-      auto *newOp = cloneWithInferType(rewriter, &op, mapping);\n-      if (convert) {\n-        for (auto result : op.getResults()) {\n-          if (!isa<RankedTensorType>(result.getType()))\n-            continue;\n-          auto value = mapping.lookupOrDefault(result);\n-          auto tensorType = result.getType().cast<RankedTensorType>();\n-          // Convert to the original type\n-          convertLayout(result, value, tensorType.getEncoding());\n-        }\n-        // Restore original values\n-        for (auto [operand, value] : cvtValues)\n-          mapping.map(operand, value);\n+      // Create a new for loop with the new operands.\n+      scf::ForOp newForOp =\n+          replaceForOpWithNewSignature(builder, forOp, newOperands);\n+      deadLoops.push_back(forOp.getOperation());\n+      Block &loopBody = *newForOp.getBody();\n+      for (auto m : argMapping) {\n+        mapping.map(newForOp.getResult(m.first), newForOp.getResult(m.second));\n+        int numIndVars = newForOp.getNumInductionVars();\n+        mapping.map(loopBody.getArgument(m.first + numIndVars),\n+                    loopBody.getArgument(m.second + numIndVars));\n       }\n+      continue;\n+    }\n+    builder.setInsertionPoint(op);\n+    if (auto yieldOp = dyn_cast<scf::YieldOp>(op)) {\n+      auto yieldOperands = llvm::to_vector(yieldOp.getOperands());\n+      for (Value operand : yieldOp.getOperands()) {\n+        if (slice.count(operand) == 0)\n+          continue;\n+        yieldOperands.push_back(mapping.lookup(operand));\n+      }\n+      builder.create<scf::YieldOp>(op->getLoc(), yieldOperands);\n+      op->erase();\n+      continue;\n+    }\n+    if (isa<arith::ConstantOp>(op)) {\n+      Operation *newOp = builder.clone(*op);\n+      auto tensorType = op->getResult(0).getType().cast<RankedTensorType>();\n+      auto newType = RankedTensorType::get(tensorType.getShape(),\n+                                           tensorType.getElementType(),\n+                                           layout[op->getResult(0)]);\n+      auto cvt = builder.create<triton::gpu::ConvertLayoutOp>(\n+          op->getLoc(), newType, newOp->getResult(0));\n+      mapping.map(op->getResult(0), cvt.getResult());\n+      continue;\n+    }\n+    Operation *newOp = builder.clone(*op, mapping);\n+    for (auto [old, newV] : llvm::zip(op->getResults(), newOp->getResults())) {\n+      auto it = layout.find(old);\n+      if (it == layout.end())\n+        continue;\n+      auto newType = RankedTensorType::get(\n+          old.getType().cast<RankedTensorType>().getShape(),\n+          old.getType().cast<RankedTensorType>().getElementType(), it->second);\n+      newV.setType(newType);\n     }\n-    // create yield, inserting conversions if necessary\n-    auto yieldOp = forOp.getBody()->getTerminator();\n-    SmallVector<Value, 4> newYieldArgs;\n-    // We use the new type for the result of the conversion\n-    for (Value arg : yieldOp->getOperands())\n-      newYieldArgs.push_back(mapping.lookup(arg));\n-    if (newYieldArgs[i].getType() != newType)\n-      newYieldArgs[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-          yieldOp->getLoc(), newType, newYieldArgs[i]);\n-    rewriter.create<scf::YieldOp>(forOp.getLoc(), newYieldArgs);\n-\n-    // replace\n-    SmallVector<Value, 4> newResults = newForOp->getResults();\n-    newResults[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        newForOp.getLoc(), origType, newForOp->getResult(i));\n-    newResults[i].getDefiningOp()->moveAfter(newForOp);\n-\n-    return newResults;\n   }\n+  convertOp.replaceAllUsesWith(mapping.lookup(convertOp.getOperand()));\n+  convertOp.erase();\n+  for (Operation *op : deadLoops)\n+    op->erase();\n+}\n \n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto forOp = cast<scf::ForOp>(op);\n-    auto iterArgs = forOp.getRegionIterArgs();\n-    for (const auto &iterArg : llvm::enumerate(iterArgs)) {\n-      // skip non-tensor types\n-      if (!iterArg.value().getType().isa<RankedTensorType>())\n-        continue;\n-      SmallVector<Operation *> cvts;\n-      if (canMoveOutOfLoop(iterArg.value(), cvts).failed())\n+static void rewriteSlice(SetVector<Value> &slice,\n+                         DenseMap<Value, Attribute> &layout,\n+                         ConvertLayoutOp convertOp) {\n+  IRMapping mapping;\n+  rewriteSlice(slice, layout, convertOp, mapping);\n+}\n+\n+static LogicalResult getRematerializableSlice(\n+    Value root, Attribute rootEncoding, SetVector<Value> &slice,\n+    DenseMap<Value, Attribute> &layout,\n+    std::function<bool(Operation *)> stopPropagation = nullptr) {\n+  LogicalResult result = getConvertBackwardSlice(root, slice, rootEncoding,\n+                                                 layout, stopPropagation);\n+  if (result.failed() || slice.empty())\n+    return failure();\n+\n+  // Check if all the operations in the slice can be rematerialized.\n+  for (Value v : slice) {\n+    if (Operation *op = v.getDefiningOp()) {\n+      if (!canBeRemat(op))\n+        return failure();\n+    }\n+  }\n+  return success();\n+}\n+\n+static void backwardRematerialization(ConvertLayoutOp convertOp) {\n+  // we don't want to rematerialize any conversion to/from shared\n+  if (triton::gpu::isSharedEncoding(convertOp.getResult()) ||\n+      triton::gpu::isSharedEncoding(convertOp.getOperand()))\n+    return;\n+  // we don't handle conversions to DotOperandEncodingAttr\n+  // this is a heuristics to accommodate fused attention\n+  auto targetType = convertOp->getResultTypes()[0].cast<RankedTensorType>();\n+  if (targetType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n+    return;\n+\n+  // 1. Take a backward slice of all the tensor dependencies that can be\n+  // rematerialized.\n+  SetVector<Value> slice;\n+  DenseMap<Value, Attribute> layout;\n+  LogicalResult result = getRematerializableSlice(\n+      convertOp.getOperand(), targetType.getEncoding(), slice, layout);\n+  if (result.failed())\n+    return;\n+\n+  // 2. Rewrite the slice.\n+  rewriteSlice(slice, layout, convertOp);\n+}\n+\n+// For convert left we try to hoist them above type extension to reduce the cost\n+// of the convert.\n+static void hoistConvertOnTopOfExt(ConvertLayoutOp convertOp) {\n+  // we don't want to rematerialize any conversion to/from shared\n+  if (triton::gpu::isSharedEncoding(convertOp.getResult()) ||\n+      triton::gpu::isSharedEncoding(convertOp.getOperand()))\n+    return;\n+  // we don't handle conversions to DotOperandEncodingAttr\n+  // this is a heuristics to accommodate fused attention\n+  auto targetType = convertOp->getResultTypes()[0].cast<RankedTensorType>();\n+  if (targetType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n+    return;\n+\n+  auto isExtOp = [](Operation *op) {\n+    return isa<arith::ExtSIOp, arith::ExtUIOp, arith::ExtFOp>(op);\n+  };\n+  // 1. Take a backward slice of all the tensor dependencies.\n+  SetVector<Value> slice;\n+  DenseMap<Value, Attribute> layout;\n+  LogicalResult result = getRematerializableSlice(\n+      convertOp.getOperand(), targetType.getEncoding(), slice, layout, isExtOp);\n+  if (result.failed())\n+    return;\n+\n+  Operation *extOp = nullptr;\n+  unsigned sliceSize = slice.size();\n+  for (unsigned i = 0; i < sliceSize; i++) {\n+    Value v = slice[i];\n+    Operation *op = v.getDefiningOp();\n+    if (!op)\n+      continue;\n+    if (isExtOp(op)) {\n+      SetVector<Value> tempSlice;\n+      DenseMap<Value, Attribute> tempLayout;\n+      LogicalResult result = getRematerializableSlice(\n+          op->getOperand(0), layout[v], tempSlice, tempLayout);\n+      // If we can rematerialize the rest of the ext slice we can ignore this\n+      // ext as it won't need a convert.\n+      if (result.succeeded()) {\n+        slice.insert(tempSlice.begin(), tempSlice.end());\n+        layout.insert(tempLayout.begin(), tempLayout.end());\n         continue;\n-      // check\n-      for (auto *op : cvts) {\n-        auto cvt = dyn_cast<triton::gpu::ConvertLayoutOp>(op);\n-        auto targetType = op->getResultTypes()[0].cast<RankedTensorType>();\n-        auto newFor = rematerializeForLoop(rewriter, forOp, iterArg.index(),\n-                                           targetType, cvt);\n-        rewriter.replaceOp(forOp, newFor);\n-        return success();\n       }\n+      // Only apply it if there is a single ext op otherwise we would have to\n+      // duplicate the convert.\n+      if (extOp != nullptr)\n+        return;\n+      extOp = op;\n     }\n-    return failure();\n   }\n-};\n \n-//\n-class ConvertDotConvert : public mlir::RewritePattern {\n-public:\n-  ConvertDotConvert(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             1, context) {}\n-\n-  LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto dstOp = cast<triton::gpu::ConvertLayoutOp>(op);\n-    auto dotOp = dstOp.getSrc().getDefiningOp<triton::DotOp>();\n-    if (!dotOp)\n-      return mlir::failure();\n-    if (std::distance(dstOp->user_begin(), dstOp->user_end()) != 1 ||\n-        std::distance(dotOp->user_begin(), dotOp->user_end()) != 1)\n-      return mlir::failure();\n-    auto cvtOp =\n-        dotOp.getOperand(2).getDefiningOp<triton::gpu::ConvertLayoutOp>();\n-    if (!cvtOp)\n-      return mlir::failure();\n-    if (!cvtOp.getSrc().getDefiningOp<triton::LoadOp>())\n-      return failure();\n-    auto dstTy = dstOp.getResult().getType().cast<RankedTensorType>();\n-    auto srcTy = cvtOp.getOperand().getType().cast<RankedTensorType>();\n-    if (dstTy != srcTy)\n-      return mlir::failure();\n+  if (extOp == nullptr)\n+    return;\n+  // Move the convert before the ext op and rewrite the slice.\n+  OpBuilder builder(extOp);\n+  auto tensorType = extOp->getOperand(0).getType().cast<RankedTensorType>();\n+  auto newType =\n+      RankedTensorType::get(tensorType.getShape(), tensorType.getElementType(),\n+                            layout[extOp->getResult(0)]);\n+  auto newConvertOp = builder.create<ConvertLayoutOp>(\n+      convertOp.getLoc(), newType, extOp->getOperand(0));\n+  IRMapping mapping;\n+  mapping.map(extOp->getOperand(0), newConvertOp.getResult());\n+  // 3. Rewrite the slice.\n+  rewriteSlice(slice, layout, convertOp, mapping);\n+}\n \n-    auto _0f = rewriter.create<arith::ConstantOp>(\n-        op->getLoc(), dstTy.getElementType(),\n-        rewriter.getZeroAttr(dstTy.getElementType()));\n-    auto _0 = rewriter.create<triton::SplatOp>(\n-        op->getLoc(), dotOp.getResult().getType(), _0f);\n-    auto newDot = rewriter.create<triton::DotOp>(\n-        op->getLoc(), dotOp.getResult().getType(), dotOp.getOperand(0),\n-        dotOp.getOperand(1), _0, dotOp.getAllowTF32());\n-    auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        op->getLoc(), dstTy, newDot.getResult());\n-    rewriter.replaceOpWithNewOp<arith::AddFOp>(op, newCvt, cvtOp.getOperand());\n-    return mlir::success();\n+static void backwardRematerialization(ModuleOp module) {\n+  SmallVector<ConvertLayoutOp> convertOps;\n+  module.walk(\n+      [&](ConvertLayoutOp convertOp) { convertOps.push_back(convertOp); });\n+  for (ConvertLayoutOp convertOp : convertOps) {\n+    backwardRematerialization(convertOp);\n   }\n-};\n+}\n \n-} // namespace\n+static void hoistConvert(ModuleOp module) {\n+  SmallVector<ConvertLayoutOp> convertOps;\n+  module.walk(\n+      [&](ConvertLayoutOp convertOp) { convertOps.push_back(convertOp); });\n+  for (ConvertLayoutOp convertOp : convertOps) {\n+    hoistConvertOnTopOfExt(convertOp);\n+  }\n+}\n \n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n@@ -632,18 +1004,45 @@ class TritonGPURemoveLayoutConversionsPass\n     MLIRContext *context = &getContext();\n     ModuleOp m = getOperation();\n \n-    mlir::RewritePatternSet patterns(context);\n+    // 1. Propagate layout forward starting from \"anchor\" ops.\n+    m.walk([](triton::FuncOp funcOp) {\n+      LayoutPropagation layoutPropagation(funcOp);\n+      layoutPropagation.initAnchorLayout();\n+      layoutPropagation.propagateLayout();\n+      layoutPropagation.resolveConflicts();\n+      layoutPropagation.rewrite();\n+    });\n+\n+    mlir::RewritePatternSet cleanUpPatterns(context);\n+    ConvertLayoutOp::getCanonicalizationPatterns(cleanUpPatterns, context);\n+    if (mlir::applyPatternsAndFoldGreedily(m, std::move(cleanUpPatterns))\n+            .failed()) {\n+      signalPassFailure();\n+    }\n+\n+    // 2. For convert ops left try to rematerialize the slice of producer\n+    // operation to avoid having to convert.\n+    backwardRematerialization(m);\n+    // 3. For converts left try to hoist them above cast generating larger size\n+    // types in order to reduce the cost of the convert op.\n+    hoistConvert(m);\n \n-    patterns.add<SimplifyConversion>(context);\n-    patterns.add<SimplifyReduceCvt>(context);\n-    patterns.add<RematerializeBackward>(context);\n-    patterns.add<RematerializeForward>(context);\n-    patterns.add<MoveConvertOutOfLoop>(context);\n-    patterns.add<MoveConvertOutOfIf>(context);\n-    patterns.add<DecomposeDotOperand>(context);\n-    patterns.add<ConvertDotConvert>(context);\n+    mlir::RewritePatternSet decomposePatterns(context);\n+    decomposePatterns.add<DecomposeDotOperand>(context);\n+    decomposePatterns.add<ConvertDotConvert>(context);\n+    if (mlir::applyPatternsAndFoldGreedily(m, std::move(decomposePatterns))\n+            .failed()) {\n+      signalPassFailure();\n+    }\n \n-    if (mlir::applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n+    // 4. Apply clean up patterns to remove remove dead convert and dead code\n+    // generated by the previous transformations.\n+    mlir::RewritePatternSet cleanUpPatterns2(context);\n+    populateForOpDeadArgumentElimination(cleanUpPatterns2);\n+    scf::ForOp::getCanonicalizationPatterns(cleanUpPatterns2, context);\n+    ConvertLayoutOp::getCanonicalizationPatterns(cleanUpPatterns2, context);\n+    if (mlir::applyPatternsAndFoldGreedily(m, std::move(cleanUpPatterns2))\n+            .failed()) {\n       signalPassFailure();\n     }\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 215, "deletions": 286, "changes": 501, "file_content_changes": "@@ -240,30 +240,59 @@ std::string GraphLayoutMarker::getColor(const Type &type) const {\n }\n // -------------------------------------------------------------------------- //\n \n-// TODO: Interface\n-LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n-                             Attribute &ret) {\n-  ret = targetEncoding;\n-  if (auto expand_dims = dyn_cast<triton::ExpandDimsOp>(op)) {\n-    ret = triton::gpu::SliceEncodingAttr::get(\n-        op->getContext(), expand_dims.getAxis(), targetEncoding);\n-  }\n-  if (auto reduce = dyn_cast<triton::ReduceOp>(op)) {\n-    auto sliceEncoding =\n-        targetEncoding.dyn_cast<triton::gpu::SliceEncodingAttr>();\n-    if (!sliceEncoding)\n-      return failure();\n-    if (sliceEncoding.getDim() != reduce.getAxis())\n-      return failure();\n-    ret = sliceEncoding.getParent();\n-  }\n-  if (isa<triton::ViewOp, triton::CatOp>(op)) {\n-    return failure();\n-  }\n-  return success();\n+static std::optional<Attribute> inferDstEncoding(triton::ReduceOp op,\n+                                                 Attribute encoding) {\n+  return triton::gpu::SliceEncodingAttr::get(op->getContext(), op.getAxis(),\n+                                             encoding);\n+}\n+\n+static std::optional<Attribute> inferDstEncoding(triton::ExpandDimsOp op,\n+                                                 Attribute encoding) {\n+  auto sliceEncoding = encoding.dyn_cast<triton::gpu::SliceEncodingAttr>();\n+  if (!sliceEncoding)\n+    return std::nullopt;\n+  if (op.getAxis() != sliceEncoding.getDim())\n+    return std::nullopt;\n+  return sliceEncoding.getParent();\n+}\n+\n+static std::optional<Attribute> inferSrcEncoding(triton::ReduceOp op,\n+                                                 Attribute encoding) {\n+  auto sliceEncoding = encoding.dyn_cast<triton::gpu::SliceEncodingAttr>();\n+  if (!sliceEncoding)\n+    return std::nullopt;\n+  if (op.getAxis() != sliceEncoding.getDim())\n+    return std::nullopt;\n+  return sliceEncoding.getParent();\n }\n \n-bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n+static std::optional<Attribute> inferSrcEncoding(triton::ExpandDimsOp op,\n+                                                 Attribute encoding) {\n+  return triton::gpu::SliceEncodingAttr::get(op->getContext(), op.getAxis(),\n+                                             encoding);\n+}\n+\n+std::optional<Attribute> inferSrcEncoding(Operation *op, Attribute encoding) {\n+  if (auto reduceOp = dyn_cast<triton::ReduceOp>(op))\n+    return inferSrcEncoding(reduceOp, encoding);\n+  if (auto expand = dyn_cast<triton::ExpandDimsOp>(op))\n+    return inferSrcEncoding(expand, encoding);\n+  if (isa<triton::ViewOp, triton::CatOp>(op))\n+    return std::nullopt;\n+  return encoding;\n+}\n+\n+std::optional<Attribute> inferDstEncoding(Operation *op, Attribute encoding) {\n+  if (auto reduceOp = dyn_cast<triton::ReduceOp>(op))\n+    return inferDstEncoding(reduceOp, encoding);\n+  if (auto expand = dyn_cast<triton::ExpandDimsOp>(op))\n+    return inferDstEncoding(expand, encoding);\n+  if (isa<triton::ViewOp, triton::CatOp>(op))\n+    return std::nullopt;\n+  return encoding;\n+}\n+\n+bool isExpensiveLoadOrStore(Operation *op) {\n   // Case 1: Pointer of tensor is always expensive\n   auto operandType = op->getOperand(0).getType();\n   if (triton::isTensorPointerType(operandType))\n@@ -287,7 +316,7 @@ bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding) {\n   if (!op)\n     return true;\n   if (isa<triton::LoadOp, triton::StoreOp>(op))\n-    return isExpensiveLoadOrStore(op, targetEncoding);\n+    return isExpensiveLoadOrStore(op);\n   if (isa<triton::CatOp>(op))\n     return triton::gpu::isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n   if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n@@ -300,75 +329,21 @@ bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding) {\n   return false;\n }\n \n-bool canFoldConversion(Operation *op, Attribute targetEncoding) {\n+bool canFoldIntoConversion(Operation *op, Attribute targetEncoding) {\n   if (isa<triton::CatOp>(op))\n     return !triton::gpu::isExpensiveCat(cast<triton::CatOp>(op),\n                                         targetEncoding);\n-  return isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n-             triton::MakeRangeOp, triton::SplatOp, triton::ViewOp>(op);\n-}\n-\n-int simulateBackwardRematerialization(\n-    Operation *initOp, SetVector<Operation *> &processed,\n-    SetVector<Attribute> &layout, llvm::MapVector<Value, Attribute> &toConvert,\n-    Attribute targetEncoding) {\n-  // DFS\n-  std::vector<std::pair<Operation *, Attribute>> queue;\n-  queue.emplace_back(initOp, targetEncoding);\n-  // We want to see the effect of converting `initOp` to a new layout\n-  // so we initialize `numCvts = 1`.\n-  int numCvts = 1;\n-  while (!queue.empty()) {\n-    Operation *currOp;\n-    Attribute currLayout;\n-    std::tie(currOp, currLayout) = queue.back();\n-    queue.pop_back();\n-    // If the current operation is expensive to rematerialize,\n-    // we stop everything\n-    if (isExpensiveToRemat(currOp, currLayout))\n-      break;\n-    // A conversion will be removed here (i.e. transferred to operands)\n-    numCvts -= 1;\n-    // Done processing\n-    processed.insert(currOp);\n-    layout.insert(currLayout);\n-    // Add all operands to the queue\n-    for (Value argI : currOp->getOperands()) {\n-      Attribute newEncoding;\n-      // Cannot invert the current encoding for this operand\n-      // we stop everything\n-      if (failed(invertEncoding(currLayout, currOp, newEncoding)))\n-        return INT_MAX;\n-      if (toConvert.count(argI) && toConvert[argI] != newEncoding)\n-        return INT_MAX;\n-      if (auto ptrTy = argI.getType().dyn_cast<triton::PointerType>()) {\n-        if (ptrTy.getPointeeType().isa<RankedTensorType>()) {\n-          return INT_MAX;\n-        }\n-      }\n-\n-      Operation *opArgI = argI.getDefiningOp();\n-      toConvert.insert({argI, newEncoding});\n-      // 1. Only convert RankedTensorType\n-      // 2. Skip if there's no defining op\n-      // 3. Skip if the defining op has already been processed\n-      // 4. Skip or the defining op is in a different block\n-      if (!argI.getType().isa<RankedTensorType>() || !opArgI ||\n-          processed.contains(opArgI) ||\n-          opArgI->getBlock() != currOp->getBlock())\n-        continue;\n-      // If the conversion can be folded into opArgI then\n-      // we don't count this conversion as expensive\n-      if (canFoldConversion(opArgI, newEncoding))\n-        continue;\n-\n-      // We add one expensive conversion for the current operand\n-      numCvts += 1;\n-      queue.emplace_back(opArgI, newEncoding);\n+  if (auto convert = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n+    if (targetEncoding.isa<triton::gpu::MmaEncodingAttr>()) {\n+      auto srcEncoding =\n+          convert.getOperand().getType().cast<RankedTensorType>().getEncoding();\n+      if (targetEncoding != srcEncoding)\n+        return false;\n     }\n+    return true;\n   }\n-  // return net number of conversions\n-  return numCvts;\n+  return isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n+             triton::MakeRangeOp, triton::SplatOp, triton::ViewOp>(op);\n }\n \n //\n@@ -409,213 +384,54 @@ Operation *cloneWithInferType(mlir::OpBuilder &rewriter, Operation *op,\n   return newOp;\n }\n \n-namespace {\n-\n-struct OpUseInfo {\n-  Value value;\n-  Operation *op;\n-  unsigned index;\n-};\n-\n-void getForwardSliceOpUseInfo(Operation *op,\n-                              SetVector<Operation *> *forwardSliceOps,\n-                              SmallVector<OpUseInfo> *forwardOpUseInfo) {\n-  if (!op)\n-    return;\n-\n-  for (Region &region : op->getRegions())\n-    for (Block &block : region)\n-      for (Operation &blockOp : block)\n-        if (forwardSliceOps->count(&blockOp) == 0)\n-          getForwardSliceOpUseInfo(&blockOp, forwardSliceOps, forwardOpUseInfo);\n-  for (Value result : op->getResults()) {\n-    for (OpOperand &operand : result.getUses()) {\n-      auto *blockOp = operand.getOwner();\n-      forwardOpUseInfo->push_back(\n-          {operand.get(), blockOp, operand.getOperandNumber()});\n-      if (forwardSliceOps->count(blockOp) == 0)\n-        getForwardSliceOpUseInfo(blockOp, forwardSliceOps, forwardOpUseInfo);\n-    }\n-  }\n-\n-  forwardSliceOps->insert(op);\n-}\n-} // namespace\n-\n-LogicalResult simulateForwardRematerializationInLoop(Operation *startOp,\n-                                                     BlockArgument arg,\n-                                                     Attribute targetEncoding) {\n-  // heuristics for flash attention\n-  if (targetEncoding.isa<triton::gpu::SharedEncodingAttr>())\n-    return failure();\n-  SetVector<Operation *> cvtSliceOps;\n-  SmallVector<OpUseInfo> cvtSliceOpUseInfo;\n-  getForwardSliceOpUseInfo(startOp, &cvtSliceOps, &cvtSliceOpUseInfo);\n-\n-  // Check if any additional conversion is needed along the way\n-  for (Operation *op : cvtSliceOps) {\n-    if (isa<scf::YieldOp>(op))\n+LogicalResult\n+getConvertBackwardSlice(Value root, SetVector<Value> &slice,\n+                        Attribute rootEncoding,\n+                        DenseMap<Value, Attribute> &layout,\n+                        std::function<bool(Operation *)> stopPropagation) {\n+  SmallVector<std::pair<Value, Attribute>> queue = {{root, rootEncoding}};\n+  while (!queue.empty()) {\n+    auto [currentValue, encoding] = queue.back();\n+    queue.pop_back();\n+    if (!currentValue.getType().isa<RankedTensorType>())\n       continue;\n-    // The first op doesn't push forward any conversion\n-    if (op != startOp) {\n-      if (isa<triton::ReduceOp>(op) &&\n-          !op->getResult(0).getType().isa<RankedTensorType>())\n-        return failure();\n-      // don't rematerialize anything expensive\n-      if (isExpensiveToRemat(op, targetEncoding))\n-        return failure();\n-      // don't rematerialize non-element-wise\n-      if (!op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n-          !op->hasTrait<mlir::OpTrait::Elementwise>() &&\n-          !isa<triton::StoreOp, triton::AssertOp, triton::PrintOp,\n-               triton::ReduceOp>(op))\n-        return failure();\n-    }\n-    // don't rematerialize if it adds an extra conversion that can't\n-    // be removed\n-    for (Value value : op->getOperands()) {\n-      Operation *argOp = arg.getDefiningOp();\n-      SetVector<Operation *> processed;\n-      SetVector<Attribute> layout;\n-      llvm::MapVector<Value, Attribute> toConvert;\n-      int numAddedConvs = simulateBackwardRematerialization(\n-          argOp, processed, layout, toConvert, targetEncoding);\n-      if (argOp && !isa<triton::gpu::ConvertLayoutOp>(argOp) &&\n-          cvtSliceOps.count(argOp) == 0 && numAddedConvs > 0)\n+    // Skip propagating through for op results for now.\n+    // TODO: enable this based on needs.\n+    if (currentValue.getDefiningOp<scf::ForOp>())\n+      return failure();\n+    slice.insert(currentValue);\n+    layout[currentValue] = encoding;\n+    if (auto *definingOp = currentValue.getDefiningOp()) {\n+      if (canFoldIntoConversion(definingOp, encoding))\n+        continue;\n+      if (stopPropagation && stopPropagation(definingOp))\n+        continue;\n+      if (isa<triton::CatOp>(definingOp))\n         return failure();\n-    }\n-  }\n-\n-  // We apply conservative analysis. Only when the final operand's index\n-  // matches the argument's index or their encoding match, we can rematerialize.\n-  for (auto &opUseInfo : cvtSliceOpUseInfo) {\n-    Operation *op = opUseInfo.op;\n-    if (isa<scf::YieldOp>(op)) {\n-      auto yieldIdx = opUseInfo.index;\n-      // 0 is the induction variable\n-      auto argIdx = arg.getArgNumber() - 1;\n-      if (yieldIdx != argIdx) {\n-        auto argType = arg.getType().cast<RankedTensorType>();\n-        auto yieldType =\n-            op->getOperand(yieldIdx).getType().dyn_cast<RankedTensorType>();\n-        if (!yieldType || argType.getEncoding() != yieldType.getEncoding())\n+      for (Value operand : definingOp->getOperands()) {\n+        auto srcEncoding = inferSrcEncoding(definingOp, encoding);\n+        if (!srcEncoding)\n           return failure();\n+        if (slice.count(operand) == 0)\n+          queue.push_back({operand, *srcEncoding});\n       }\n+      continue;\n     }\n-  }\n-  return success();\n-}\n-\n-void rematerializeConversionChain(\n-    const llvm::MapVector<Value, Attribute> &toConvert,\n-    mlir::PatternRewriter &rewriter, SetVector<Operation *> &processed,\n-    IRMapping &mapping) {\n-  SmallVector<Value, 4> sortedValues;\n-  SetVector<Operation *> tmp;\n-  for (auto &item : toConvert) {\n-    Value v = item.first;\n-    if (v.getDefiningOp())\n-      tmp.insert(v.getDefiningOp());\n-    else\n-      sortedValues.push_back(v);\n-  }\n-  tmp = mlir::multiRootTopologicalSort(tmp);\n-  for (Operation *op : tmp)\n-    sortedValues.push_back(op->getResult(0));\n-\n-  for (Value currOperand : sortedValues) {\n-    Value origOperand = currOperand;\n-    // unpack information\n-    Attribute targetLayout = toConvert.lookup(currOperand);\n-    // rematerialize the operand if necessary\n-    Operation *currOperation = currOperand.getDefiningOp();\n-    if (processed.contains(currOperation)) {\n-      Operation *newOperation =\n-          cloneWithInferType(rewriter, currOperation, mapping);\n-      newOperation->moveAfter(currOperation);\n-      currOperation = newOperation;\n-      currOperand = currOperation->getResult(0);\n-    }\n-    // compute target type for the layout cast\n-    auto currType = currOperand.getType().cast<RankedTensorType>();\n-    auto newType = RankedTensorType::get(\n-        currType.getShape(), currType.getElementType(), targetLayout);\n-    auto newOperand = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        currOperand.getLoc(), newType, currOperand);\n-    if (currOperation)\n-      newOperand->moveAfter(currOperation);\n-    else {\n-      Block *block = currOperand.cast<BlockArgument>().getOwner();\n-      newOperand->moveBefore(block, block->begin());\n+    auto blockArg = cast<BlockArgument>(currentValue);\n+    Block *block = blockArg.getOwner();\n+    Operation *parentOp = block->getParentOp();\n+    if (auto forOp = dyn_cast<scf::ForOp>(parentOp)) {\n+      OpOperand &initOperand = forOp.getOpOperandForRegionIterArg(blockArg);\n+      Value yieldOperand = forOp.getBody()->getTerminator()->getOperand(\n+          blockArg.getArgNumber() - forOp.getNumInductionVars());\n+      queue.push_back({initOperand.get(), encoding});\n+      queue.push_back({yieldOperand, encoding});\n+      continue;\n     }\n-    mapping.map(origOperand, newOperand);\n-  }\n-}\n-\n-LogicalResult canMoveOutOfLoop(BlockArgument arg,\n-                               SmallVector<Operation *> &cvts) {\n-  auto parentOp = arg.getOwner()->getParentOp();\n-  // Don't move if arg is defined in a while loop\n-  if (isa<scf::WhileOp>(parentOp))\n+    // TODO: add support for WhileOp and other region types.\n     return failure();\n-  // Skip if arg is not defined in scf.for\n-  if (!isa<scf::ForOp>(parentOp))\n-    return success();\n-  auto forOp = cast<scf::ForOp>(parentOp);\n-  // We only move `iterArg` out of the loop if\n-  // 1. There is no conversion\n-  // 2. There is only a single conversion\n-  // 3. Moving this conversion out of the loop will not generate any extra\n-  // non-removable conversion\n-  SetVector<RankedTensorType> cvtTypes;\n-  SetVector<Operation *> others;\n-  auto oldType = arg.getType().cast<RankedTensorType>();\n-  for (auto user : arg.getUsers()) {\n-    if (isa<triton::gpu::ConvertLayoutOp>(user)) {\n-      // Don't move if the conversion target is a dot operand or shared memory\n-      auto newType = user->getResults()[0].getType().cast<RankedTensorType>();\n-      if (oldType.getEncoding().isa<triton::gpu::SharedEncodingAttr>() &&\n-          newType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>()) {\n-        continue;\n-      }\n-      if (newType.getEncoding().isa<triton::gpu::SharedEncodingAttr>()) {\n-        if (newType.getEncoding()\n-                .cast<triton::gpu::SharedEncodingAttr>()\n-                .getVec() == 1)\n-          continue;\n-      }\n-      cvts.emplace_back(user);\n-      cvtTypes.insert(newType);\n-    } else\n-      others.insert(user);\n   }\n-  // First condition\n-  if (cvts.empty())\n-    return success();\n-  if (cvtTypes.size() == 1) {\n-    // Third condition - part 1:\n-    // If the other or the cvt is in the different block, we cannot push the\n-    // conversion forward or backward\n-    for (auto *cvt : cvts) {\n-      if (cvt->getBlock() != forOp.getBody())\n-        return failure();\n-    }\n-    auto targetEncoding = cvtTypes.front().getEncoding();\n-    for (auto *other : others) {\n-      // Third condition - part 2:\n-      // If the other non-cvt op is in the different block, we cannot push the\n-      // conversion forward or backward\n-      if (other->getBlock() != forOp.getBody())\n-        return failure();\n-      // Third condition - part 3:\n-      // Check if we can directly use arg without conversion\n-      if (simulateForwardRematerializationInLoop(other, arg, targetEncoding)\n-              .failed())\n-        return failure();\n-    }\n-    return success();\n-  }\n-  return failure();\n+  return success();\n }\n \n // TODO(thomas): this is duplicated with what is in GPUToLLVM\n@@ -700,4 +516,117 @@ void setRoleId(Operation *op, int roleId) {\n   op->setAttr(\"agent.mutex_role\", attr);\n }\n \n+namespace {\n+\n+/// Detect dead arguments in scf.for op by assuming all the values are dead and\n+/// propagate liveness property.\n+struct ForOpDeadArgElimination : public OpRewritePattern<scf::ForOp> {\n+  using OpRewritePattern<scf::ForOp>::OpRewritePattern;\n+\n+  LogicalResult matchAndRewrite(scf::ForOp forOp,\n+                                PatternRewriter &rewriter) const final {\n+    Block &block = *forOp.getBody();\n+    auto yieldOp = cast<scf::YieldOp>(block.getTerminator());\n+    // Assume that nothing is live at the beginning and mark values as live\n+    // based on uses.\n+    DenseSet<Value> aliveValues;\n+    SmallVector<Value> queue;\n+    // Helper to mark values as live and add them to the queue of value to\n+    // propagate if it is the first time we detect the value as live.\n+    auto markLive = [&](Value val) {\n+      if (!forOp->isAncestor(val.getParentRegion()->getParentOp()))\n+        return;\n+      if (aliveValues.insert(val).second)\n+        queue.push_back(val);\n+    };\n+    // Mark all yield operands as live if the associated forOp result has any\n+    // use.\n+    for (auto result : llvm::enumerate(forOp.getResults())) {\n+      if (!result.value().use_empty())\n+        markLive(yieldOp.getOperand(result.index()));\n+    }\n+    if (aliveValues.size() == forOp.getNumResults())\n+      return failure();\n+    // Operations with side-effects are always live. Mark all theirs operands as\n+    // live.\n+    block.walk([&](Operation *op) {\n+      if (!isa<scf::YieldOp, scf::ForOp>(op) && !wouldOpBeTriviallyDead(op)) {\n+        for (Value operand : op->getOperands())\n+          markLive(operand);\n+      }\n+    });\n+    // Propagate live property until reaching a fixed point.\n+    while (!queue.empty()) {\n+      Value value = queue.pop_back_val();\n+      if (auto nestedFor = value.getDefiningOp<scf::ForOp>()) {\n+        auto result = value.cast<OpResult>();\n+        OpOperand &forOperand = nestedFor.getOpOperandForResult(result);\n+        markLive(forOperand.get());\n+        auto nestedYieldOp =\n+            cast<scf::YieldOp>(nestedFor.getBody()->getTerminator());\n+        Value nestedYieldOperand =\n+            nestedYieldOp.getOperand(result.getResultNumber());\n+        markLive(nestedYieldOperand);\n+        continue;\n+      }\n+      if (auto nestedIf = value.getDefiningOp<scf::IfOp>()) {\n+        auto result = value.cast<OpResult>();\n+        for (scf::YieldOp nestedYieldOp :\n+             {nestedIf.thenYield(), nestedIf.elseYield()}) {\n+          Value nestedYieldOperand =\n+              nestedYieldOp.getOperand(result.getResultNumber());\n+          markLive(nestedYieldOperand);\n+        }\n+        continue;\n+      }\n+      if (Operation *def = value.getDefiningOp()) {\n+        // TODO: support while ops.\n+        if (isa<scf::WhileOp>(def))\n+          return failure();\n+        for (Value operand : def->getOperands())\n+          markLive(operand);\n+        continue;\n+      }\n+      // If an argument block is live then the associated yield operand and\n+      // forOp operand are live.\n+      auto arg = value.cast<BlockArgument>();\n+      if (auto forOwner = dyn_cast<scf::ForOp>(arg.getOwner()->getParentOp())) {\n+        if (arg.getArgNumber() < forOwner.getNumInductionVars())\n+          continue;\n+        unsigned iterIdx = arg.getArgNumber() - forOwner.getNumInductionVars();\n+        Value yieldOperand =\n+            forOwner.getBody()->getTerminator()->getOperand(iterIdx);\n+        markLive(yieldOperand);\n+        markLive(forOwner.getIterOperands()[iterIdx]);\n+      }\n+    }\n+    SmallVector<unsigned> deadArg;\n+    for (auto yieldOperand : llvm::enumerate(yieldOp->getOperands())) {\n+      if (aliveValues.contains(yieldOperand.value()))\n+        continue;\n+      if (yieldOperand.value() == block.getArgument(yieldOperand.index() + 1))\n+        continue;\n+      deadArg.push_back(yieldOperand.index());\n+    }\n+    if (deadArg.empty())\n+      return failure();\n+    rewriter.updateRootInPlace(forOp, [&]() {\n+      // For simplicity we just change the dead yield operand to use the\n+      // associated argument and leave the operations and argument removal to\n+      // dead code elimination.\n+      for (unsigned deadArgIdx : deadArg) {\n+        BlockArgument arg = block.getArgument(deadArgIdx + 1);\n+        yieldOp.setOperand(deadArgIdx, arg);\n+      }\n+    });\n+    return success();\n+  }\n+};\n+\n+} // namespace\n+\n+void populateForOpDeadArgumentElimination(RewritePatternSet &patterns) {\n+  patterns.add<ForOpDeadArgElimination>(patterns.getContext());\n+}\n+\n } // namespace mlir"}, {"filename": "lib/Target/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,3 +1,2 @@\n add_subdirectory(LLVMIR)\n add_subdirectory(PTX)\n-add_subdirectory(HSACO)"}, {"filename": "lib/Target/HSACO/CMakeLists.txt", "status": "removed", "additions": 0, "deletions": 9, "changes": 9, "file_content_changes": "@@ -1,9 +0,0 @@\n-add_mlir_translation_library(TritonHSACO\n-        HSACOTranslation.cpp\n-\n-        LINK_COMPONENTS\n-        Core\n-\n-        LINK_LIBS PUBLIC\n-        TritonLLVMIR\n-        )"}, {"filename": "lib/Target/HSACO/HSACOTranslation.cpp", "status": "removed", "additions": 0, "deletions": 182, "changes": 182, "file_content_changes": "@@ -1,182 +0,0 @@\n-#include \"triton/Target/HSACO/HSACOTranslation.h\"\n-#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n-#include \"mlir/ExecutionEngine/ExecutionEngine.h\"\n-#include \"mlir/ExecutionEngine/OptUtils.h\"\n-#include \"mlir/IR/BuiltinOps.h\"\n-#include \"mlir/IR/Dialect.h\"\n-#include \"mlir/Pass/Pass.h\"\n-#include \"mlir/Pass/PassManager.h\"\n-#include \"mlir/Support/LogicalResult.h\"\n-#include \"mlir/Target/LLVMIR/Dialect/LLVMIR/LLVMToLLVMIRTranslation.h\"\n-#include \"mlir/Target/LLVMIR/Export.h\"\n-#include \"mlir/Target/LLVMIR/LLVMTranslationInterface.h\"\n-#include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n-#include \"triton/Tools/Sys/GetEnv.hpp\"\n-\n-#include \"llvm/ExecutionEngine/ExecutionEngine.h\"\n-#include \"llvm/ExecutionEngine/SectionMemoryManager.h\"\n-#include \"llvm/IR/IRBuilder.h\"\n-#include \"llvm/IR/IRPrintingPasses.h\"\n-#include \"llvm/IR/LegacyPassManager.h\"\n-#include \"llvm/IR/Module.h\"\n-#include \"llvm/IR/Verifier.h\"\n-#include \"llvm/MC/TargetRegistry.h\"\n-#include \"llvm/Support/CodeGen.h\"\n-#include \"llvm/Support/CommandLine.h\"\n-#include \"llvm/Support/SourceMgr.h\"\n-#include \"llvm/Support/TargetSelect.h\"\n-#include \"llvm/Support/raw_ostream.h\"\n-#include \"llvm/Target/TargetMachine.h\"\n-#include \"llvm/Target/TargetOptions.h\"\n-#include \"llvm/Transforms/Scalar.h\"\n-#include \"llvm/Transforms/Utils/Cloning.h\"\n-#include <filesystem>\n-#include <iostream>\n-#include <memory>\n-#include <random>\n-\n-namespace {\n-\n-void init_llvm() {\n-  LLVMInitializeAMDGPUTarget();\n-  LLVMInitializeAMDGPUTargetInfo();\n-  LLVMInitializeAMDGPUTargetMC();\n-  LLVMInitializeAMDGPUAsmParser();\n-  LLVMInitializeAMDGPUAsmPrinter();\n-}\n-\n-std::unique_ptr<llvm::TargetMachine>\n-initialize_module(llvm::Module *module, const std::string &triple,\n-                  const std::string &proc, const std::string &features) {\n-  // verify and store llvm\n-  llvm::legacy::PassManager pm;\n-  pm.add(llvm::createVerifierPass());\n-  pm.run(*module);\n-\n-  module->setTargetTriple(triple);\n-\n-  std::string error;\n-  auto target =\n-      llvm::TargetRegistry::lookupTarget(module->getTargetTriple(), error);\n-  llvm::TargetOptions opt;\n-  opt.AllowFPOpFusion = llvm::FPOpFusion::Fast;\n-  opt.UnsafeFPMath = false;\n-  opt.NoInfsFPMath = false;\n-  opt.NoNaNsFPMath = true;\n-  llvm::TargetMachine *machine = target->createTargetMachine(\n-      module->getTargetTriple(), proc, features, opt, llvm::Reloc::PIC_,\n-      std::nullopt, llvm::CodeGenOpt::Aggressive);\n-\n-  module->setDataLayout(machine->createDataLayout());\n-\n-  for (llvm::Function &f : module->functions())\n-    f.addFnAttr(llvm::Attribute::AlwaysInline);\n-\n-  return std::unique_ptr<llvm::TargetMachine>(machine);\n-}\n-\n-std::string generate_amdgcn_assembly(llvm::Module *module,\n-                                     const std::string &triple,\n-                                     const std::string &proc,\n-                                     const std::string &features) {\n-  auto machine = initialize_module(module, triple, proc, features);\n-  llvm::SmallVector<char, 0> buffer;\n-  llvm::legacy::PassManager pass;\n-  llvm::raw_svector_ostream stream(buffer);\n-\n-  // emit\n-  machine->addPassesToEmitFile(pass, stream, nullptr,\n-                               llvm::CodeGenFileType::CGFT_AssemblyFile);\n-  pass.run(*module);\n-\n-  std::string amdgcn(buffer.begin(), buffer.end());\n-  if (::triton::tools::getBoolEnv(\"AMDGCN_ENABLE_DUMP\")) {\n-    std::cout << \"// -----// AMDGCN Dump //----- //\\n\" << amdgcn << std::endl;\n-  }\n-\n-  return amdgcn;\n-}\n-\n-std::string generate_hsaco(llvm::Module *module, const std::string &triple,\n-                           const std::string &proc,\n-                           const std::string &features) {\n-  auto machine = initialize_module(module, triple, proc, features);\n-\n-  // create unique dir for kernel's binary and hsaco\n-  std::error_code ec;\n-  std::string kernel_name_base = \"amd_triton_kernel\";\n-  std::filesystem::path tmp = std::filesystem::temp_directory_path();\n-  std::filesystem::path kernel_dir_base(kernel_name_base);\n-  llvm::SmallString<256> unique_dir;\n-  ec = llvm::sys::fs::createUniqueDirectory((tmp / kernel_dir_base).string(),\n-                                            unique_dir);\n-  if (ec) {\n-    std::cerr << \"Directory for \" << kernel_name_base\n-              << \" was not created. error code: \" << ec << std::endl;\n-  }\n-  std::filesystem::path kernel_dir(unique_dir.data());\n-  std::string kernel_name = kernel_dir.stem();\n-\n-  // Save GCN ISA binary.\n-  std::filesystem::path isa_binary(kernel_name + \".o\");\n-  std::string isabin_path = (kernel_dir / isa_binary).string();\n-  std::unique_ptr<llvm::raw_fd_ostream> isabin_fs(\n-      new llvm::raw_fd_ostream(isabin_path, ec, llvm::sys::fs::OF_Text));\n-  if (ec) {\n-    std::cerr << isabin_path << \" was not created. error code: \" << ec\n-              << std::endl;\n-  }\n-\n-  // emit\n-  llvm::legacy::PassManager pass;\n-  machine->addPassesToEmitFile(pass, *isabin_fs, nullptr,\n-                               llvm::CGFT_ObjectFile);\n-  pass.run(*module);\n-\n-  // generate HASCO file\n-  std::filesystem::path hsaco(kernel_name + \".hsaco\");\n-  std::string hsaco_path = (kernel_dir / hsaco).string();\n-  std::string error_message;\n-  std::string lld_path = \"/opt/rocm/llvm/bin/ld.lld\";\n-  int lld_result = llvm::sys::ExecuteAndWait(\n-      lld_path,\n-      {lld_path, \"-flavor\", \"gnu\", \"-shared\", \"-o\", hsaco_path, isabin_path},\n-      std::nullopt, {}, 0, 0, &error_message);\n-  if (lld_result) {\n-    std::cout << \"ld.lld execute fail: \" << std::endl;\n-    std::cout << error_message << std::endl;\n-    std::cout << lld_result << std::endl;\n-  }\n-\n-  return hsaco_path;\n-}\n-\n-std::tuple<std::string, std::string>\n-llir_to_amdgcn_and_hsaco(llvm::Module *module, std::string gfx_arch,\n-                         std::string gfx_triple, std::string gfx_features) {\n-\n-  init_llvm();\n-\n-  // verify and store llvm\n-  auto module_obj = llvm::CloneModule(*module);\n-  auto amdgcn =\n-      generate_amdgcn_assembly(module, gfx_triple, gfx_arch, gfx_features);\n-  auto hsaco_path =\n-      generate_hsaco(module_obj.get(), gfx_triple, gfx_arch, gfx_features);\n-\n-  return std::make_tuple(amdgcn, hsaco_path);\n-}\n-\n-} // namespace\n-\n-namespace triton {\n-\n-std::tuple<std::string, std::string>\n-translateLLVMIRToHSACO(llvm::Module &module, std::string gfx_arch,\n-                       std::string gfx_triple, std::string gfx_features) {\n-  auto hsacoCode =\n-      llir_to_amdgcn_and_hsaco(&module, gfx_arch, gfx_triple, gfx_features);\n-  return hsacoCode;\n-}\n-\n-} // namespace triton"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 1, "deletions": 19, "changes": 20, "file_content_changes": "@@ -33,7 +33,6 @@\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n-#include \"triton/Target/HSACO/HSACOTranslation.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n #include \"triton/Target/PTX/PTXTranslation.h\"\n #include \"triton/Target/PTX/TmaMetadata.h\"\n@@ -257,6 +256,7 @@ void init_triton_ir(py::module &&m) {\n         // we load LLVM because the frontend uses LLVM.undef for\n         // some placeholders\n         self.getOrLoadDialect<mlir::LLVM::LLVMDialect>();\n+        self.getOrLoadDialect<mlir::tensor::TensorDialect>();\n       });\n   // .def(py::init([](){\n   //   mlir::MLIRContext context;\n@@ -1958,24 +1958,6 @@ void init_triton_translation(py::module &m) {\n            const std::vector<std::string> &paths) {\n           ::mlir::triton::addExternalLibs(op, names, paths);\n         });\n-\n-  m.def(\n-      \"translate_llvmir_to_hsaco\",\n-      [](const std::string llvmIR, std::string gfx_arch, std::string gfx_triple,\n-         std::string gfx_features) -> std::tuple<std::string, std::string> {\n-        // create LLVM module from C++\n-        llvm::LLVMContext context;\n-        std::unique_ptr<llvm::MemoryBuffer> buffer =\n-            llvm::MemoryBuffer::getMemBuffer(llvmIR.c_str());\n-        llvm::SMDiagnostic error;\n-        std::unique_ptr<llvm::Module> module =\n-            llvm::parseIR(buffer->getMemBufferRef(), error, context);\n-        // translate module to HSACO\n-        auto hsacoCode = triton::translateLLVMIRToHSACO(\n-            *module, gfx_arch, gfx_triple, gfx_features);\n-        return hsacoCode;\n-      },\n-      ret::take_ownership);\n }\n \n void init_triton(py::module &m) {"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 178, "deletions": 54, "changes": 232, "file_content_changes": "@@ -12,6 +12,7 @@\n import triton\n import triton._C.libtriton.triton as _triton\n import triton.language as tl\n+from triton.common.build import is_hip\n from triton.runtime.jit import JITFunction, TensorWrapper, reinterpret\n \n int_dtypes = ['int8', 'int16', 'int32', 'int64']\n@@ -25,6 +26,13 @@\n # num_ctas_list = [1, 4] if torch.cuda.get_device_capability()[0] == 9 else [1]\n num_ctas_list = [1]\n \n+if is_hip():\n+    GPU_DIALECT = \"triton_gpu_rocm\"\n+    THREADS_PER_WARP = 64\n+else:\n+    GPU_DIALECT = \"triton_gpu\"\n+    THREADS_PER_WARP = 32\n+\n \n def _bitwidth(dtype: str) -> int:\n     # ex.: \"int64\" -> 64\n@@ -137,7 +145,7 @@ def __init__(self, version, warps_per_cta, ctas_per_cga, cta_split_num, cta_orde\n         self.instr_shape = str(instr_shape)\n \n     def __str__(self):\n-        return f\"#triton_gpu.mma<{{versionMajor={self.version[0]}, versionMinor={self.version[1]}, warpsPerCTA={self.warps_per_cta}, CTAsPerCGA={self.ctas_per_cga}, CTASplitNum={self.cta_split_num}, CTAOrder={self.cta_order}, instrShape={self.instr_shape}}}>\"\n+        return f\"#{GPU_DIALECT}.mma<{{versionMajor={self.version[0]}, versionMinor={self.version[1]}, warpsPerCTA={self.warps_per_cta}, CTAsPerCGA={self.ctas_per_cga}, CTASplitNum={self.cta_split_num}, CTAOrder={self.cta_order}, instrShape={self.instr_shape}}}>\"\n \n \n class BlockedLayout:\n@@ -151,7 +159,7 @@ def __init__(self, size_per_thread, threads_per_warp, warps_per_cta, order, ctas\n         self.cta_order = str(cta_order)\n \n     def __str__(self):\n-        return f\"#triton_gpu.blocked<{{sizePerThread={self.sz_per_thread}, threadsPerWarp={self.threads_per_warp}, warpsPerCTA={self.warps_per_cta}, order={self.order}, CTAsPerCGA={self.ctas_per_cga}, CTASplitNum={self.cta_split_num}, CTAOrder={self.cta_order}}}>\"\n+        return f\"#{GPU_DIALECT}.blocked<{{sizePerThread={self.sz_per_thread}, threadsPerWarp={self.threads_per_warp}, warpsPerCTA={self.warps_per_cta}, order={self.order}, CTAsPerCGA={self.ctas_per_cga}, CTASplitNum={self.cta_split_num}, CTAOrder={self.cta_order}}}>\"\n \n \n class SharedLayout:\n@@ -165,7 +173,7 @@ def __init__(self, vec, per_phase, max_phase, order, ctas_per_cga, cta_split_num\n         self.cta_order = str(cta_order)\n \n     def __str__(self):\n-        return f\"#triton_gpu.shared<{{vec={self.vec}, perPhase={self.per_phase}, maxPhase={self.max_phase}, order={self.order}, CTAsPerCGA={self.ctas_per_cga}, CTASplitNum={self.cta_split_num}, CTAOrder={self.cta_order}}}>\"\n+        return f\"#{GPU_DIALECT}.shared<{{vec={self.vec}, perPhase={self.per_phase}, maxPhase={self.max_phase}, order={self.order}, CTAsPerCGA={self.ctas_per_cga}, CTASplitNum={self.cta_split_num}, CTAOrder={self.cta_order}}}>\"\n \n \n @pytest.mark.parametrize(\"dtype_x\", list(dtypes) + [\"bfloat16\"])\n@@ -851,6 +859,8 @@ def test_abs(dtype_x, device):\n \n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4nv, tl.float8e5])\n def test_abs_fp8(in_dtype, device):\n+    if is_hip():\n+        pytest.skip('test_abs_fp8 not supported on HIP.')\n \n     @triton.jit\n     def abs_kernel(X, Z, SIZE: tl.constexpr):\n@@ -1056,6 +1066,9 @@ def noinline_multi_values_fn(x, y, Z):\n \n @pytest.mark.parametrize(\"mode\", [\"simple\", \"call_graph\", \"shared\", \"dynamic\", \"multi_values\"])\n def test_noinline(mode, device):\n+    if is_hip() and mode == \"shared\":\n+        pytest.skip('test_noinline[\"shared\"] not supported on HIP.')\n+\n     @triton.jit\n     def kernel(X, Y, Z):\n         x = tl.load(X)\n@@ -1141,6 +1154,9 @@ def kernel(X, Z):\n     else:\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n     sem_str = \"acq_rel\" if sem is None else sem\n+    if is_hip():\n+        return\n+\n     assert f\"atom.global.gpu.{sem_str}\" in h.asm[\"ptx\"]\n \n \n@@ -1232,6 +1248,8 @@ def serialized_add(data, Lock, SEM: tl.constexpr):\n     h = serialized_add[(64,)](data, Lock, SEM=sem, num_ctas=num_ctas)\n     sem_str = \"acq_rel\" if sem is None else sem\n     np.testing.assert_allclose(to_numpy(data), to_numpy(ref))\n+    if is_hip():\n+        return\n     assert f\"atom.global.{sem_str}\" in h.asm[\"ptx\"]\n \n \n@@ -1261,6 +1279,9 @@ def test_cast(dtype_x, dtype_z, bitcast, num_ctas, device):\n     check_type_supported(dtype_x, device)\n     check_type_supported(dtype_z, device)\n \n+    if is_hip() and (dtype_z == \"bfloat16\"):\n+        pytest.skip(f'test_cast{(dtype_x, dtype_z)} cast to bfloat16 not supported on HIP.')\n+\n     size = 1024\n     # This is tricky because numpy doesn't have bfloat, and torch doesn't have uints.\n     if dtype_x.startswith('bfloat'):\n@@ -1358,7 +1379,10 @@ def kernel(in_out_ptr):\n \n     for _ in range(1000):\n         x = torch.ones((65536,), device=device, dtype=torch.float32)\n-        kernel[(65536,)](x, num_warps=32)\n+        if is_hip():\n+            kernel[(65536,)](x, num_warps=16)  # threads per Warp for ROCM is 64\n+        else:\n+            kernel[(65536,)](x, num_warps=32)\n         assert torch.all(x == 2)\n \n \n@@ -1452,6 +1476,8 @@ def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n     \"\"\"\n     check_type_supported(in_dtype, device)\n     check_type_supported(out_dtype, device)\n+    if is_hip():\n+        pytest.skip('test_abs_fp8 not supported on HIP.')\n \n     @triton.jit\n     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n@@ -1507,6 +1533,9 @@ def get_reduced_dtype(dtype_str, op):\n def test_reduce1d(op, dtype_str, shape, num_ctas, device):\n     check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n \n+    if is_hip():\n+        pytest.skip(f\"test_reduce1d not supported on HIP\")\n+\n     # triton kernel\n     @triton.jit\n     def kernel(X, Z, BLOCK: tl.constexpr):\n@@ -1597,7 +1626,10 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n def test_reduce2d(op, dtype_str, shape, axis, num_ctas, device):\n     check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n \n+    if is_hip():\n+        pytest.skip(f\"test_reduce2d not supported on HIP\")\n     # triton kernel\n+\n     @triton.jit\n     def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexpr):\n         range_m = tl.arange(0, BLOCK_M)\n@@ -1667,6 +1699,8 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n \n @pytest.mark.parametrize(\"op, dtype_str, shape, axis, num_warps\", scan_configs)\n def test_scan2d(op, dtype_str, shape, axis, num_warps, device):\n+    if is_hip():\n+        pytest.skip(\"test_scan2d is not supported in HIP\")\n     check_type_supported(dtype_str, device)\n \n     # triton kernel\n@@ -1720,6 +1754,9 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n @pytest.mark.parametrize(\"src_layout\", scan_layouts)\n @pytest.mark.parametrize(\"axis\", [0, 1])\n def test_scan_layouts(M, N, src_layout, axis, device):\n+    if is_hip():\n+        pytest.skip(\"test_scan_layouts is not supported in HIP\")\n+\n     ir = f\"\"\"\n     #blocked = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32}} {{\n@@ -1783,6 +1820,9 @@ def test_scan_layouts(M, N, src_layout, axis, device):\n @pytest.mark.parametrize(\"src_layout\", layouts)\n @pytest.mark.parametrize(\"axis\", [0, 1])\n def test_reduce_layouts(M, N, src_layout, axis, device):\n+    if is_hip():\n+        pytest.skip(\"test_reduce_layouts is not supported in HIP\")\n+\n     rdims_2d = f\"1x{N}\" if axis == 0 else f\"{M}x1\"\n     rdims_1d = f\"{N}\" if axis == 0 else f\"{M}\"\n     store_range = \"%7\" if axis == 0 else \"%1\"\n@@ -1792,28 +1832,28 @@ def test_reduce_layouts(M, N, src_layout, axis, device):\n     #src = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32}} {{\n     tt.func public @kernel_0d1d2c3d4c(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: i32 {{tt.divisibility = 16 : i32}}, %arg2: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n-        %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>\n-        %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>) -> tensor<{M}x1xi32, #blocked>\n+        %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #blocked}}>>\n+        %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #blocked}}>>) -> tensor<{M}x1xi32, #blocked>\n         %2 = tt.splat %arg1 : (i32) -> tensor<{M}x1xi32, #blocked>\n         %3 = arith.muli %1, %2 : tensor<{M}x1xi32, #blocked>\n         %4 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x1x!tt.ptr<i32>, #blocked>\n         %5 = tt.addptr %4, %3 : tensor<{M}x1x!tt.ptr<i32>, #blocked>, tensor<{M}x1xi32, #blocked>\n-        %6 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>\n-        %7 = tt.expand_dims %6 {{axis = 0 : i32}} : (tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>) -> tensor<1x{N}xi32, #blocked>\n+        %6 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #{GPU_DIALECT}.slice<{{dim = 0, parent = #blocked}}>>\n+        %7 = tt.expand_dims %6 {{axis = 0 : i32}} : (tensor<{N}xi32, #{GPU_DIALECT}.slice<{{dim = 0, parent = #blocked}}>>) -> tensor<1x{N}xi32, #blocked>\n         %8 = tt.broadcast %5 : (tensor<{M}x1x!tt.ptr<i32>, #blocked>) -> tensor<{M}x{N}x!tt.ptr<i32>, #blocked>\n         %9 = tt.broadcast %7 : (tensor<1x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #blocked>\n         %10 = tt.addptr %8, %9 : tensor<{M}x{N}x!tt.ptr<i32>, #blocked>, tensor<{M}x{N}xi32, #blocked>\n         %11 = tt.splat %arg2 : (!tt.ptr<i32>) -> tensor<{rdims_2d}x!tt.ptr<i32>, #blocked>\n         %12 = tt.addptr %11, {store_range} : tensor<{rdims_2d}x!tt.ptr<i32>, #blocked>, tensor<{rdims_2d}xi32, #blocked>\n         %13 = tt.load %10 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xi32, #blocked>\n-        %14 = triton_gpu.convert_layout %13 : (tensor<{M}x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #src>\n+        %14 = {GPU_DIALECT}.convert_layout %13 : (tensor<{M}x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #src>\n         %15 = \"tt.reduce\"(%14) ({{\n         ^bb0(%arg3: i32, %arg4: i32):\n           %17 = arith.addi %arg3, %arg4 : i32\n           tt.reduce.return %17 : i32\n-        }}) {{axis = {axis} : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>\n-        %18 = triton_gpu.convert_layout %15 : (tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>) -> tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>\n-        %19 = tt.expand_dims %18 {{axis = {axis} : i32}} : (tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>) -> tensor<{rdims_2d}xi32, #blocked>\n+        }}) {{axis = {axis} : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{rdims_1d}xi32, #{GPU_DIALECT}.slice<{{dim = {axis}, parent = #src}}>>\n+        %18 = {GPU_DIALECT}.convert_layout %15 : (tensor<{rdims_1d}xi32, #{GPU_DIALECT}.slice<{{dim = {axis}, parent = #src}}>>) -> tensor<{rdims_1d}xi32, #{GPU_DIALECT}.slice<{{dim = {axis}, parent = #blocked}}>>\n+        %19 = tt.expand_dims %18 {{axis = {axis} : i32}} : (tensor<{rdims_1d}xi32, #{GPU_DIALECT}.slice<{{dim = {axis}, parent = #blocked}}>>) -> tensor<{rdims_2d}xi32, #blocked>\n         tt.store %12, %19 {{cache = 1 : i32, evict = 1 : i32}} : tensor<{rdims_2d}xi32, #blocked>\n         tt.return\n     }}\n@@ -1854,17 +1894,20 @@ def test_reduce_layouts(M, N, src_layout, axis, device):\n @pytest.mark.parametrize(\"M\", [32, 64, 128, 256])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n def test_store_op(M, src_layout, device):\n+    if is_hip():\n+        pytest.skip(\"test_convert1d is not supported yet in HIP\")\n+\n     ir = f\"\"\"\n     #src = {src_layout}\n-    module attributes {{\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32}} {{\n+    module attributes {{\"{GPU_DIALECT}.num-warps\" = 4 : i32, \"{GPU_DIALECT}.num-ctas\" = 1 : i32, \"{GPU_DIALECT}.threads-per-warp\" = {THREADS_PER_WARP} : i32}} {{\n         tt.func public @kernel(%arg0: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}) {{\n-            %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n-            %1 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<{M}x!tt.ptr<f32>, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n-            %2 = tt.addptr %1, %0 : tensor<{M}x!tt.ptr<f32>, #triton_gpu.slice<{{dim = 1, parent = #src}}>>, tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n-            %3 = tt.load %2 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}xf32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n-            %4 = tt.expand_dims %3 {{axis = 1 : i32}} : (tensor<{M}xf32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xf32, #src>\n-            %5 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n-            %6 = tt.expand_dims %5 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xi32, #src>\n+            %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>\n+            %1 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<{M}x!tt.ptr<f32>, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>\n+            %2 = tt.addptr %1, %0 : tensor<{M}x!tt.ptr<f32>, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>, tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>\n+            %3 = tt.load %2 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}xf32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>\n+            %4 = tt.expand_dims %3 {{axis = 1 : i32}} : (tensor<{M}xf32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xf32, #src>\n+            %5 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>\n+            %6 = tt.expand_dims %5 {{axis = 1 : i32}} : (tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xi32, #src>\n             %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<{M}x1x!tt.ptr<f32>, #src>\n             %8 = tt.addptr %7, %6 : tensor<{M}x1x!tt.ptr<f32>, #src>, tensor<{M}x1xi32, #src>\n             tt.store %8, %4 : tensor<{M}x1xf32, #src>\n@@ -1903,20 +1946,23 @@ def test_store_op(M, src_layout, device):\n @pytest.mark.parametrize(\"src_dim\", [0, 1])\n @pytest.mark.parametrize(\"dst_dim\", [0, 1])\n def test_convert1d(M, src_layout, dst_layout, src_dim, dst_dim, device):\n+    if is_hip():\n+        pytest.skip(\"test_convert1d is not supported in HIP\")\n+\n     ir = f\"\"\"\n     #dst = {dst_layout}\n     #src = {src_layout}\n-    module attributes {{\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32}} {{\n+    module attributes {{\"{GPU_DIALECT}.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.threads-per-warp\" = {THREADS_PER_WARP} : i32}} {{\n         tt.func public @kernel(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n-            %0 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x!tt.ptr<i32>, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>\n-            %1 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>\n-            %2 = tt.addptr %0, %1 : tensor<{M}x!tt.ptr<i32>, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>, tensor<{M}xi32, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>\n-            %3 = tt.load %2 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>\n-            %4 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<{M}x!tt.ptr<i32>, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n-            %5 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n-            %6 = tt.addptr %4, %5 : tensor<{M}x!tt.ptr<i32>, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>, tensor<{M}xi32, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n-            %7 = triton_gpu.convert_layout %3 : (tensor<{M}xi32, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>) -> tensor<{M}xi32, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n-            tt.store %6, %7 : tensor<{M}xi32, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            %0 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x!tt.ptr<i32>, #{GPU_DIALECT}.slice<{{dim = {src_dim}, parent = #src}}>>\n+            %1 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {src_dim}, parent = #src}}>>\n+            %2 = tt.addptr %0, %1 : tensor<{M}x!tt.ptr<i32>, #{GPU_DIALECT}.slice<{{dim = {src_dim}, parent = #src}}>>, tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {src_dim}, parent = #src}}>>\n+            %3 = tt.load %2 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {src_dim}, parent = #src}}>>\n+            %4 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<{M}x!tt.ptr<i32>, #{GPU_DIALECT}.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            %5 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            %6 = tt.addptr %4, %5 : tensor<{M}x!tt.ptr<i32>, #{GPU_DIALECT}.slice<{{dim = {dst_dim}, parent = #dst}}>>, tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            %7 = {GPU_DIALECT}.convert_layout %3 : (tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {src_dim}, parent = #src}}>>) -> tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            tt.store %6, %7 : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = {dst_dim}, parent = #dst}}>>\n             tt.return\n         }}\n     }}\n@@ -1962,26 +2008,29 @@ def _welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n @pytest.mark.parametrize(\"op\", [\"sum\", \"max\"])\n @pytest.mark.parametrize(\"first_axis\", [0, 1])\n def test_chain_reduce(M, N, src_layout, op, device, first_axis):\n+    if is_hip():\n+        pytest.skip(\"test_chain_reduce is not supported in HIP\")\n+\n     op_str = \"\"\n     if op == \"sum\":\n         op_str = f\"\"\"\n         %13 = arith.addi %arg2, %arg3 : i32\n         tt.reduce.return %13 : i32\"\"\"\n     elif op == \"max\":\n         op_str = f\"\"\"\n-        %13 = \"triton_gpu.cmpi\"(%arg2, %arg3) <{{predicate = 4 : i64}}> : (i32, i32) -> i1\n+        %13 = \"{GPU_DIALECT}.cmpi\"(%arg2, %arg3) <{{predicate = 4 : i64}}> : (i32, i32) -> i1\n         %14 = arith.select %13, %arg2, %arg3 : i32\n         tt.reduce.return %14 : i32\"\"\"\n     ir = f\"\"\"\n     #src = {src_layout}\n-    module attributes {{\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32}} {{\n+    module attributes {{\"{GPU_DIALECT}.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.threads-per-warp\" = {THREADS_PER_WARP} : i32}} {{\n     tt.func public @sum_kernel_0d1d(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n         %cst = arith.constant dense<{N}> : tensor<{M}x1xi32, #src>\n-        %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n-        %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xi32, #src>\n+        %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>\n+        %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xi32, #src>\n         %2 = arith.muli %1, %cst : tensor<{M}x1xi32, #src>\n-        %3 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #src}}>>\n-        %4 = tt.expand_dims %3 {{axis = 0 : i32}} : (tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #src}}>>) -> tensor<1x{N}xi32, #src>\n+        %3 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #{GPU_DIALECT}.slice<{{dim = 0, parent = #src}}>>\n+        %4 = tt.expand_dims %3 {{axis = 0 : i32}} : (tensor<{N}xi32, #{GPU_DIALECT}.slice<{{dim = 0, parent = #src}}>>) -> tensor<1x{N}xi32, #src>\n         %5 = tt.broadcast %2 : (tensor<{M}x1xi32, #src>) -> tensor<{M}x{N}xi32, #src>\n         %6 = tt.broadcast %4 : (tensor<1x{N}xi32, #src>) -> tensor<{M}x{N}xi32, #src>\n         %7 = arith.addi %5, %6 : tensor<{M}x{N}xi32, #src>\n@@ -1991,11 +2040,11 @@ def test_chain_reduce(M, N, src_layout, op, device, first_axis):\n         %11 = \"tt.reduce\"(%10) ({{\n         ^bb0(%arg2: i32, %arg3: i32):\n         {op_str}\n-        }}) {{axis = {first_axis} : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{M if first_axis == 1 else N}xi32, #triton_gpu.slice<{{dim = {first_axis}, parent = #src}}>>\n+        }}) {{axis = {first_axis} : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{M if first_axis == 1 else N}xi32, #{GPU_DIALECT}.slice<{{dim = {first_axis}, parent = #src}}>>\n         %12 = \"tt.reduce\"(%11) ({{\n         ^bb0(%arg2: i32, %arg3: i32):\n         {op_str}\n-        }}) {{axis = 0 : i32}} : (tensor<{M if first_axis == 1 else N}xi32, #triton_gpu.slice<{{dim = {first_axis}, parent = #src}}>>) -> i32\n+        }}) {{axis = 0 : i32}} : (tensor<{M if first_axis == 1 else N}xi32, #{GPU_DIALECT}.slice<{{dim = {first_axis}, parent = #src}}>>) -> i32\n         tt.store %arg1, %12 {{cache = 1 : i32, evict = 1 : i32}} : i32\n         tt.return\n     }}\n@@ -2063,6 +2112,8 @@ def var_mean_kernel(X, out_mean, out_var, BLOCK: tl.constexpr):\n @pytest.mark.parametrize(\"num_ctas\", num_ctas_list)\n def test_permute(dtype_str, shape, perm, num_ctas, device):\n     check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n+    if is_hip():\n+        pytest.skip(f\"test_permute is not supported in HIP\")\n \n     # triton kernel\n     @triton.jit\n@@ -2099,6 +2150,10 @@ def kernel(X, stride_xm, stride_xn,\n     # compare\n     np.testing.assert_allclose(to_numpy(z_tri), z_ref)\n     np.testing.assert_allclose(to_numpy(z_tri_contiguous), z_ref)\n+\n+    if is_hip():\n+        return\n+\n     # parse ptx to make sure ld/st are vectorized\n     ptx = pgm.asm['ptx']\n     assert 'ld.global.v4' in ptx\n@@ -2115,7 +2170,7 @@ def kernel(X, stride_xm, stride_xn,\n \n @pytest.mark.parametrize(\"M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype\",\n                          [(*shape, 4, False, False, epilogue, allow_tf32, in_dtype, out_dtype)\n-                          for shape in [(64, 64, 64), (16, 16, 16)]\n+                          for shape in [(64, 64, 64), (32, 32, 32), (16, 16, 16)]\n                           for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols', 'softmax', 'chain-dot']\n                           for allow_tf32 in [True, False]\n                           for in_dtype, out_dtype in [('float16', 'float16'),\n@@ -2146,6 +2201,17 @@ def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, o\n     check_cuda_only(device)\n \n     capability = torch.cuda.get_device_capability()\n+\n+    if is_hip():\n+        # set capability to large number to jump over check below\n+        # check are not relevant to amd gpu, left them for smaller diff between test_core.py and test_core_amd.py tests\n+        capability = (100, 100)\n+        if out_dtype is None:\n+            if in_dtype in float_dtypes:\n+                out_dtype = \"float32\"\n+            else:\n+                out_dtype = \"int32\"\n+\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n     if capability[0] < 8:\n@@ -2160,6 +2226,16 @@ def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, o\n             # TODO: support out_dtype=float16 for tl.dot on V100\n             pytest.skip(\"Only test out_dtype=float16 on devices with sm >=80\")\n \n+    if is_hip():\n+        if (M, N, K) in [(64, 128, 128)]:\n+            pytest.skip(f\"test_dot{(M, N, K)} not supported on HIP: memory out of resource.\")\n+        if (M, N, K, num_warps) in [(128, 256, 32, 8), (128, 128, 64, 4)]:\n+            pytest.skip(f\"test_dot{(M, N, K)} not supported on HIP. Reduce Warp to work\")\n+        if M == 16 or N == 16 or K == 16:\n+            pytest.skip(f\"test_dot{(M, N, K)} segfaults on HIP\")\n+        if epilogue == \"softmax\":\n+            pytest.skip(f\"test_dot{epilogue} segfaults on HIP\")\n+\n     torch.backends.cuda.matmul.allow_tf32 = allow_tf32\n \n     if num_ctas > 1 and in_dtype == 'int8':\n@@ -2247,6 +2323,7 @@ def kernel(X, stride_xm, stride_xk,\n         out_dtype = tl.float16\n     else:\n         out_dtype = tl.float32\n+\n     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n                          y_tri, y_tri.stride(0), y_tri.stride(1),\n                          w_tri, w_tri.stride(0), w_tri.stride(1),\n@@ -2261,20 +2338,24 @@ def kernel(X, stride_xm, stride_xk,\n                          ALLOW_TF32=allow_tf32,\n                          num_warps=num_warps, num_ctas=num_ctas,\n                          out_dtype=out_dtype)\n+\n     if epilogue == 'softmax' and (in_dtype != 'float32' or allow_tf32):\n-        ptx = pgm.asm[\"ptx\"]\n-        start = ptx.find(\"shfl.sync\")\n-        end = ptx.find(\"cvt.rn.f16.f32\")\n-        red_code = ptx[start:end]\n-        assert len(red_code) > 0\n-        import os\n-        enable_mmav3 = os.environ.get('ENABLE_MMA_V3', 'not found').lower()\n-        enable_tma = os.environ.get('ENABLE_TMA', 'not found').lower()\n-        # skip this check on hopper because there are some functions whose name contain \"shared\" in ptx.\n-        # TODO: we should eliminate these unused functions in ptx code.\n-        if not (enable_mmav3 in [\"on\", \"true\", \"1\"] and enable_tma in [\"on\", \"true\", \"1\"]):\n-            assert \"shared\" not in red_code\n-        assert \"bar.sync\" not in red_code\n+        if is_hip():\n+            pass\n+        else:\n+            ptx = pgm.asm[\"ptx\"]\n+            start = ptx.find(\"shfl.sync\")\n+            end = ptx.find(\"cvt.rn.f16.f32\")\n+            red_code = ptx[start:end]\n+            assert len(red_code) > 0\n+            import os\n+            enable_mmav3 = os.environ.get('ENABLE_MMA_V3', 'not found').lower()\n+            enable_tma = os.environ.get('ENABLE_TMA', 'not found').lower()\n+            # skip this check on hopper because there are some functions whose name contain \"shared\" in ptx.\n+            # TODO: we should eliminate these unused functions in ptx code.\n+            if not (enable_mmav3 in [\"on\", \"true\", \"1\"] and enable_tma in [\"on\", \"true\", \"1\"]):\n+                assert \"shared\" not in red_code\n+            assert \"bar.sync\" not in red_code\n     # torch result\n     if in_dtype == 'int8':\n         z_ref = np.matmul(x.astype(np.float32),\n@@ -2300,9 +2381,12 @@ def kernel(X, stride_xm, stride_xk,\n         # XXX: Somehow there's a larger difference when we use float32\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)\n     elif out_dtype == tl.float16:\n-        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)\n+        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-2)\n     else:\n-        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+        # added atol, to loose precision for float16xfloat16->float32 case\n+        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)\n+    if is_hip():\n+        return\n     # make sure ld/st are vectorized\n     ptx = pgm.asm['ptx']\n     if (K > 16 or N > 16 or M > 16) and (M * N // (num_warps * 32) >= 4):\n@@ -2366,6 +2450,9 @@ def kernel(Z, X, Y,\n     h = kernel[grid](z_tri, x_tri, y_tri, M, N, K, BM, BN, BK)\n     z_ref = np.matmul(x, y)\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), atol=0.01)\n+\n+    if is_hip():\n+        return\n     assert \"tt.dot\" in h.asm['ttir']\n     # with option ENABLE_MMA_V3 on, we will not pipeline the load op for Y\n     # as the loaded value is in rowmajor. But MMAv3 requires it's second\n@@ -2432,6 +2519,9 @@ def test_dot_without_load(dtype_str, device):\n     capability = torch.cuda.get_device_capability()\n     allow_tf32 = capability[0] > 7\n \n+    if is_hip() and dtype_str == \"float16\":\n+        pytest.skip(\"test_dot_without_load[float16] not supported in HIP\")\n+\n     @triton.jit\n     def _kernel(out, ALLOW_TF32: tl.constexpr):\n         a = GENERATE_TEST_HERE\n@@ -2512,6 +2602,9 @@ def _kernel(in_ptr, out_ptr, in_size: tl.constexpr, out_size: tl.constexpr):\n # FIXME: Shape too small for ldmatrix when num_ctas=4\n @pytest.mark.parametrize(\"dtype\", [torch.bfloat16, torch.float16, torch.float32])\n def test_masked_load_shared_memory(dtype, device):\n+    if is_hip():\n+        pytest.skip(\"test_masked_load_shared_memory is not supported in HIP\")\n+\n     check_type_supported(dtype, device)  # bfloat16 on cc < 80 will not be tested\n \n     M = 32\n@@ -2571,6 +2664,9 @@ def _kernel(dst, src, CACHE: tl.constexpr):\n         tl.store(dst + offsets, x)\n \n     pgm = _kernel[(1,)](dst, src, CACHE=cache)\n+    if is_hip():\n+        return\n+\n     ptx = pgm.asm['ptx']\n     if cache == '':\n         assert 'ld.global.ca' not in ptx\n@@ -2597,6 +2693,10 @@ def _kernel(dst, src, N, BLOCK_SIZE: tl.constexpr):\n         tl.store(dst + offsets, x, mask=offsets < N)\n     pgm = _kernel[(1,)](\n         dst, src, N=N, BLOCK_SIZE=block_size)\n+\n+    if is_hip():\n+        return\n+\n     ptx = pgm.asm[\"ptx\"]\n     if N % 16 == 0:\n         assert \"ld.global.v4.b32\" in ptx\n@@ -2620,6 +2720,9 @@ def _kernel(dst, src, off, N, BLOCK_SIZE: tl.constexpr, HINT: tl.constexpr):\n         x = tl.load(src + offsets, mask=offsets < N)\n         tl.store(dst + offsets, x, mask=offsets < N)\n     pgm = _kernel[(1,)](dst, src, off, N=1024, BLOCK_SIZE=src.shape[0], HINT=has_hints)\n+    if is_hip():\n+        return\n+\n     ptx = pgm.asm[\"ptx\"]\n     if has_hints:\n         assert \"ld.global.v4.b32\" in ptx\n@@ -2642,6 +2745,8 @@ def _kernel(dst, src, CACHE: tl.constexpr):\n         x = tl.load(src + offsets)\n         tl.store(dst + offsets, x, cache_modifier=CACHE)\n \n+    if is_hip():\n+        return\n     pgm = _kernel[(1,)](dst, src, CACHE=cache)\n     ptx = pgm.asm['ptx']\n     if cache == '':\n@@ -2793,6 +2898,9 @@ def kernel(VALUE, X):\n @pytest.mark.parametrize(\"is_lhs_constexpr\", [False, True])\n @pytest.mark.parametrize(\"is_rhs_constexpr\", [True, False])\n def test_bin_op_constexpr(op, is_lhs_constexpr, is_rhs_constexpr, device):\n+    if is_hip():\n+        if (is_rhs_constexpr, is_lhs_constexpr, op) in [(False, False, \"<<\"), (False, False, \">>\"), (False, True, \"<<\")]:\n+            pytest.skip(f\"test_bin_op_constexpr[{is_lhs_constexpr}-{is_rhs_constexpr}-{op}] is not supported in HIP\")\n \n     @triton.jit\n     def kernel(Z, X, Y):\n@@ -2968,6 +3076,9 @@ def _kernel(dst):\n @pytest.mark.parametrize(\"num_ctas\", num_ctas_list)\n def test_math_tensor(dtype_str, expr, lib_path, num_ctas, device):\n \n+    if is_hip() and expr == \"math.scalbn\":\n+        pytest.skip(\"test_math_tensor[math.scalbn] is not supported in HIP\")\n+\n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n         x = tl.load(X + tl.arange(0, BLOCK))\n@@ -3063,6 +3174,9 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n def test_inline_asm(num_ctas, device):\n     check_cuda_only(device)\n \n+    if is_hip():\n+        pytest.skip(\"test_inline_asm is not supported in HIP\")\n+\n     @triton.jit\n     def kernel(X, Y, Z, n: tl.constexpr, BLOCK: tl.constexpr):\n         x = tl.load(X + tl.arange(0, BLOCK))\n@@ -3089,6 +3203,9 @@ def kernel(X, Y, Z, n: tl.constexpr, BLOCK: tl.constexpr):\n def test_inline_asm_packed(num_ctas, device):\n     check_cuda_only(device)\n \n+    if is_hip():\n+        pytest.skip(\"test_inline_asm is not supported in HIP\")\n+\n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n         x = tl.load(X + tl.arange(0, BLOCK))\n@@ -3392,6 +3509,8 @@ def nested_while(data, countPtr):\n \n \n def test_globaltimer(device):\n+    if is_hip():\n+        pytest.skip(\"test_globaltimer is not supported in HIP\")\n     check_cuda_only(device)\n \n     @triton.jit\n@@ -3411,6 +3530,8 @@ def kernel(Out1, Out2):\n \n \n def test_smid(device):\n+    if is_hip():\n+        pytest.skip(\"test_smid is not supported in HIP\")\n     check_cuda_only(device)\n \n     @triton.jit\n@@ -3456,6 +3577,9 @@ def kernel(Out):\n @pytest.mark.parametrize(\"interm_layout\", intermediate_layouts)\n @pytest.mark.parametrize(\"dst_layout\", layouts)\n def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device):\n+    if is_hip():\n+        pytest.skip(\"test_convert2d is not supported in HIP\")\n+\n     if str(src_layout) == str(dst_layout):\n         pytest.skip()\n     if 'mma' in str(src_layout) and 'mma' in str(dst_layout):"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 1, "deletions": 11, "changes": 12, "file_content_changes": "@@ -85,6 +85,7 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                                      (\"float8e4nv\", \"float8e4nv\"),\n                                      (\"float8e5\", \"float8e4nv\"),\n                                      (\"float8e5\", \"float8e5\"),\n+                                     (\"float8e4b15\", \"float8e4b15\"),\n                                      (\"float8e4nv\", \"float16\"),\n                                      (\"float16\", \"float8e5\"),\n                                      (\"float16\", \"float32\"),\n@@ -105,17 +106,6 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                                      (\"bfloat16\", \"float32\"),\n                                      (\"float32\", \"bfloat16\")] for AT in [False, True] for BT in [False, True]\n         ],\n-        *[\n-            # float8e4b15 only supports row-col layout\n-            [\n-                (128, 128, 32, 1, 4, 2, None, None, None, False, True, ADTYPE, BDTYPE, True),\n-            ] for ADTYPE, BDTYPE in [(\"float8e4b15\", \"float8e5\"),\n-                                     (\"float8e4b15\", \"float16\"),\n-                                     (\"float16\", \"float8e4b15\"),\n-                                     (\"float8e5\", \"float8e5\"),\n-                                     (\"float8e4nv\", \"float8e4nv\"),\n-                                     (\"int8\", \"int8\")]\n-        ]\n     ),\n )\n def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, ADTYPE, BDTYPE, ALLOW_TF32):"}, {"filename": "python/triton/common/backend.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -5,6 +5,7 @@\n import os\n import re\n import subprocess\n+import traceback\n from typing import Dict\n \n from ..runtime.driver import DriverBase\n@@ -94,7 +95,7 @@ def get_backend(device_type: str):\n             try:\n                 importlib.import_module(device_backend_package_name, package=__spec__.name)\n             except Exception:\n-                return None\n+                traceback.print_exc()\n         else:\n             return None\n     return _backends[device_type] if device_type in _backends else None"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 45, "deletions": 117, "changes": 162, "file_content_changes": "@@ -5,18 +5,18 @@\n import json\n import os\n import re\n-import subprocess\n import tempfile\n from collections import namedtuple\n from pathlib import Path\n-from typing import Any, Tuple\n+from typing import Any\n \n from .._C.libtriton.triton import (ClusterInfo, TMAInfos, add_external_libs,\n                                    compile_ptx_to_cubin, get_env_vars, get_num_warps,\n                                    get_shared_memory_size, ir, runtime,\n-                                   translate_llvmir_to_hsaco, translate_llvmir_to_ptx,\n+                                   translate_llvmir_to_ptx,\n                                    translate_triton_gpu_to_llvmir)\n from ..common.backend import get_backend, path_to_ptxas\n+from ..common.build import is_hip\n # from ..runtime import driver, jit, JITFunction\n # TODO: runtime.errors\n from ..runtime.autotuner import OutOfResources\n@@ -188,72 +188,6 @@ def ptx_to_cubin(ptx: str, arch: int):\n     return compile_ptx_to_cubin(ptx, ptxas, arch)\n \n \n-# AMDGCN translation\n-\n-def get_amdgcn_bitcode_paths(arch):\n-    gpu_arch_agnostic_bitcode_libraries = [\"opencl.bc\",\n-                                           \"ocml.bc\",\n-                                           \"ockl.bc\",\n-                                           \"oclc_finite_only_off.bc\",\n-                                           \"oclc_daz_opt_off.bc\",\n-                                           \"oclc_correctly_rounded_sqrt_on.bc\",\n-                                           \"oclc_unsafe_math_off.bc\",\n-                                           \"oclc_wavefrontsize64_on.bc\"]\n-\n-    gfx_arch = arch[1]\n-    gfx_arch_id = re.search('gfx(\\\\w+)', gfx_arch).group(1).strip()\n-\n-    gpu_arch_specific_bitcode_library = 'oclc_isa_version_' + gfx_arch_id + \".bc\"\n-    bitcode_path_dir = os.path.join(Path(__file__).parent.resolve(), \"third_party/rocm/lib/bitcode/\")\n-\n-    amdgcn_bitcode_paths = {}\n-    i = 1\n-    for bc_lib in gpu_arch_agnostic_bitcode_libraries:\n-        bc_path = bitcode_path_dir + bc_lib\n-        if os.path.exists(bc_path):\n-            amdgcn_bitcode_paths['library_' + str(i)] = bc_path\n-            i += 1\n-    bc_gfx_path = bitcode_path_dir + gpu_arch_specific_bitcode_library\n-    if os.path.exists(bc_gfx_path):\n-        amdgcn_bitcode_paths['library_' + str(i)] = bc_gfx_path\n-\n-    return amdgcn_bitcode_paths\n-\n-\n-def get_amdgpu_arch_fulldetails():\n-    \"\"\"\n-    get the amdgpu fulll ISA details for compiling:\n-    i.e., arch_triple: amdgcn-amd-amdhsa; arch_name: gfx906; arch_features: sramecc+:xnack-\n-    \"\"\"\n-    try:\n-        # TODO: package rocm.cc with Triton\n-        rocm_path_dir = os.getenv(\"ROCM_PATH\", default=\"/opt/rocm\")\n-        rocminfo = subprocess.check_output(rocm_path_dir + '/bin/rocminfo').decode()\n-        gfx_arch_details = re.search('amd.*', rocminfo).group(0).strip().split('--')\n-        arch_triple = gfx_arch_details[0]\n-        arch_name_features = gfx_arch_details[1].split(':')\n-        arch_name = arch_name_features[0]\n-        arch_features = \"\"\n-\n-        if (len(arch_name_features) == 3):\n-            arch_features = \"+\" + re.search('\\\\w+', arch_name_features[1]).group(0) + \",\"\\\n-                            \"-\" + re.search('\\\\w+', arch_name_features[2]).group(0)\n-        return [arch_triple, arch_name, arch_features]\n-    except BaseException:\n-        return None\n-\n-\n-def llir_to_amdgcn_and_hsaco(mod: Any, gfx_arch: str, gfx_triple: str, gfx_features: str) -> Tuple[str, str]:\n-    '''\n-    Translate TritonGPU module to HSACO code based on full details of gpu architecture.\n-    :param mod: a TritonGPU dialect module\n-    :return:\n-        - AMDGCN code\n-        - Path to HSACO object\n-    '''\n-    return translate_llvmir_to_hsaco(mod, gfx_arch, gfx_triple, gfx_features)\n-\n-\n # ------------------------------------------------------------------------------\n # compiler\n # ------------------------------------------------------------------------------\n@@ -320,8 +254,10 @@ def make_hash(fn, arch, env_vars, **kwargs):\n     \"ttgir\": mlir_arg_type_pattern,\n     \"ptx\": ptx_arg_type_pattern,\n }\n-\n-ttgir_num_warps_pattern = r'\"triton_gpu.num-warps\"\\s?=\\s?(\\d+)\\s?:'\n+if is_hip():\n+    ttgir_num_warps_pattern = r'\"triton_gpu_rocm.num-warps\"\\s?=\\s?(\\d+)\\s?:'\n+else:\n+    ttgir_num_warps_pattern = r'\"triton_gpu.num-warps\"\\s?=\\s?(\\d+)\\s?:'\n \n \n def _get_jsonable_constants(constants):\n@@ -354,17 +290,10 @@ def _is_cuda(arch):\n \n \n def get_architecture_descriptor(capability):\n-    try:\n-        import torch\n-    except ImportError:\n-        raise ImportError(\"Triton requires PyTorch to be installed\")\n     if capability is None:\n-        if torch.version.hip is None:\n-            device = get_current_device()\n-            capability = get_device_capability(device)\n-            capability = capability[0] * 10 + capability[1]\n-        else:\n-            capability = get_amdgpu_arch_fulldetails()\n+        device = get_current_device()\n+        capability = get_device_capability(device)\n+        capability = capability[0] * 10 + capability[1]\n     return capability\n \n \n@@ -394,23 +323,6 @@ def get_arch_default_num_stages(device_type, capability=None):\n     return num_stages\n \n \n-def add_rocm_stages(arch, extern_libs, stages):\n-    extern_libs.update(get_amdgcn_bitcode_paths(arch))\n-\n-    for key in list(extern_libs):\n-        if extern_libs[key] == '' or extern_libs[key] is None:\n-            extern_libs.pop(key)\n-\n-    gfx_arch_full_details = arch\n-    gfx_arch = os.environ.get('MI_GPU_ARCH', gfx_arch_full_details[1])\n-    if gfx_arch is None:\n-        raise RuntimeError('gfx_arch is None (not specified)')\n-    stages[\"amdgcn\"] = (lambda path: Path(path).read_text(),\n-                        lambda src: llir_to_amdgcn_and_hsaco(src, gfx_arch,\n-                                                             gfx_arch_full_details[0],\n-                                                             gfx_arch_full_details[2]))\n-\n-\n def add_cuda_stages(arch, extern_libs, stages):\n \n     stages[\"ptx\"] = (lambda path: Path(path).read_text(),\n@@ -422,18 +334,22 @@ def add_cuda_stages(arch, extern_libs, stages):\n def compile(fn, **kwargs):\n     # Get device type to decide which backend should be used\n     device_type = kwargs.get(\"device_type\", \"cuda\")\n-    _device_backend = get_backend(device_type)\n     capability = kwargs.get(\"cc\", None)\n \n-    if device_type in [\"cuda\", \"hip\"]:\n+    if is_hip():\n+        device_type = \"hip\"\n+\n+    if device_type == \"cuda\":\n+        _device_backend = get_backend(device_type)\n         arch = get_architecture_descriptor(capability)\n     else:\n         _device_backend = get_backend(device_type)\n         assert _device_backend\n         arch = _device_backend.get_architecture_descriptor(**kwargs)\n \n     is_cuda = device_type == \"cuda\" and _is_cuda(arch)\n-    is_hip = device_type in [\"cuda\", \"hip\"] and not is_cuda\n+    if is_hip():\n+        is_cuda = False\n     context = ir.context()\n     constants = kwargs.get(\"constants\", dict())\n     num_warps = kwargs.get(\"num_warps\", get_arch_default_num_warps(device_type))\n@@ -464,14 +380,20 @@ def compile(fn, **kwargs):\n     stages[\"ast\"] = (lambda path: fn, None)\n     stages[\"ttir\"] = (lambda path: parse_mlir_module(path, context),\n                       lambda src: optimize_ttir(ast_to_ttir(src, signature, configs[0], constants, debug=debug, arch=arch), arch))\n-    stages[\"ttgir\"] = (lambda path: parse_mlir_module(path, context),\n-                       lambda src: optimize_ttgir(ttir_to_ttgir(src, num_warps, num_ctas, arch), num_stages, num_warps, num_ctas, arch, cluster_info, enable_warp_specialization, enable_persistent, optimize_epilogue))\n-    stages[\"llir\"] = (lambda path: Path(path).read_text(),\n-                      lambda src: ttgir_to_llir(src, extern_libs, arch, tma_infos))\n     if is_cuda:\n+        stages[\"ttgir\"] = (lambda path: parse_mlir_module(path, context),\n+                           lambda src: optimize_ttgir(ttir_to_ttgir(src, num_warps, num_ctas, arch), num_stages, num_warps, num_ctas, arch, cluster_info, enable_warp_specialization, enable_persistent, optimize_epilogue))\n+        stages[\"llir\"] = (lambda path: Path(path).read_text(),\n+                          lambda src: ttgir_to_llir(src, extern_libs, arch, tma_infos))\n         add_cuda_stages(arch, extern_libs, stages)\n-    elif is_hip:\n-        add_rocm_stages(arch, extern_libs, stages)\n+    elif device_type == \"hip\":\n+        _device_backend.add_stages(arch, extern_libs, stages, num_warps=num_warps, num_stages=num_stages)\n+    elif device_type == \"xpu\":\n+        stages[\"ttgir\"] = (lambda path: parse_mlir_module(path, context),\n+                           lambda src: optimize_ttgir(ttir_to_ttgir(src, num_warps, num_ctas, arch), num_stages, num_warps, num_ctas, arch, cluster_info, enable_warp_specialization, enable_persistent, optimize_epilogue))\n+        stages[\"llir\"] = (lambda path: Path(path).read_text(),\n+                          lambda src: ttgir_to_llir(src, extern_libs, arch, tma_infos))\n+        _device_backend.add_stages(arch, extern_libs, stages)\n     else:\n         # pass the user's configuration to the backend device.\n         arch[\"num_warps\"] = num_warps\n@@ -564,7 +486,7 @@ def compile(fn, **kwargs):\n             path = metadata_group.get(ir_filename)\n             if path is None:\n                 next_module = compile_kernel(module)\n-                if ir == \"amdgcn\":\n+                if ir_name == \"amdgcn\":\n                     extra_file_name = f\"{name}.hsaco_path\"\n                     metadata_group[ir_filename] = fn_cache_manager.put(next_module[0], ir_filename)\n                     metadata_group[extra_file_name] = fn_cache_manager.put(next_module[1], extra_file_name)\n@@ -587,17 +509,23 @@ def compile(fn, **kwargs):\n         else:\n             asm[ir_name] = str(next_module)\n         if ir_name == \"llir\" and \"shared\" not in metadata:\n-            metadata[\"shared\"] = get_shared_memory_size(module)\n+            if is_hip():\n+                metadata[\"shared\"] = _device_backend.get_shared_memory_size(module)\n+            else:\n+                metadata[\"shared\"] = get_shared_memory_size(module)\n         if ir_name == \"ttgir\":\n-            metadata[\"enable_warp_specialization\"] = ir.is_ws_supported(next_module)\n-            if metadata[\"enable_warp_specialization\"]:\n-                metadata[\"num_warps\"] = get_num_warps(next_module)\n+            if is_hip():\n+                metadata[\"num_warps\"] = _device_backend.get_num_warps(next_module)\n+            else:\n+                metadata[\"enable_warp_specialization\"] = ir.is_ws_supported(next_module)\n+                if metadata[\"enable_warp_specialization\"]:\n+                    metadata[\"num_warps\"] = get_num_warps(next_module)\n         if ir_name == \"ptx\":\n             metadata[\"name\"] = get_kernel_name(next_module, pattern='// .globl')\n         if ir_name == \"amdgcn\":\n             metadata[\"name\"] = get_kernel_name(next_module[0], pattern='.globl')\n             asm[\"hsaco_path\"] = next_module[1]\n-        if not is_cuda and not is_hip:\n+        if not is_cuda and not is_hip():\n             _device_backend.add_meta_info(ir_name, module, next_module, metadata, asm)\n         module = next_module\n \n@@ -622,7 +550,7 @@ def compile(fn, **kwargs):\n     ids_of_const_exprs = tuple(fn.constexprs) if isinstance(fn, JITFunction) else ()\n     ids = {\"ids_of_tensormaps\": ids_of_tensormaps, \"ids_of_folded_args\": ids_of_folded_args, \"ids_of_const_exprs\": ids_of_const_exprs}\n     # cache manager\n-    if is_cuda or is_hip:\n+    if is_cuda:\n         so_path = make_stub(name, signature, constants, ids, enable_warp_specialization=enable_warp_specialization)\n     else:\n         so_path = _device_backend.make_launcher_stub(name, signature, constants, ids)\n@@ -660,7 +588,7 @@ def __init__(self, fn, so_path, metadata, asm):\n             self.tensormaps_info = metadata[\"tensormaps_info\"]\n         self.constants = metadata[\"constants\"]\n         self.device_type = metadata[\"device_type\"]\n-        self.device_backend = get_backend(self.device_type) if self.device_type not in [\"cuda\", \"hip\"] else None\n+        self.device_backend = get_backend(self.device_type) if self.device_type not in [\"cuda\"] else None\n         # initialize asm dict\n         self.asm = asm\n         # binaries are lazily initialized\n@@ -674,7 +602,7 @@ def _init_handles(self):\n         if self.cu_module is not None:\n             return\n \n-        if self.device_type in [\"cuda\", \"hip\"]:\n+        if self.device_type in [\"cuda\"]:\n             device = get_current_device()\n             bin_path = {\n                 driver.HIP: \"hsaco_path\",\n@@ -720,7 +648,7 @@ def __getitem__(self, grid):\n         def runner(*args, stream=None):\n             args_expand = self.assemble_tensormap_to_arg(args)\n             if stream is None:\n-                if self.device_type in [\"cuda\", \"hip\"]:\n+                if self.device_type in [\"cuda\"]:\n                     stream = get_cuda_stream()\n                 else:\n                     stream = get_backend(self.device_type).get_stream(None)"}, {"filename": "python/triton/compiler/make_launcher.py", "status": "modified", "additions": 4, "deletions": 151, "changes": 155, "file_content_changes": "@@ -3,16 +3,11 @@\n import tempfile\n \n from ..common import _build\n+from ..common.build import is_hip\n from ..runtime.cache import get_cache_manager\n from ..runtime.jit import version_key\n from .utils import generate_cu_signature\n \n-\n-def is_hip():\n-    import torch\n-    return torch.version.hip is not None\n-\n-\n # ----- stub --------\n \n \n@@ -103,151 +98,9 @@ def format_of(ty):\n     format = \"iiiiiiiiiKKOOO\" + ''.join([format_of(_extracted_type(ty)) for ty in signature.values()])\n \n     # generate glue code\n-    if is_hip():\n-        src = f\"\"\"\n-    #define __HIP_PLATFORM_AMD__\n-    #include <hip/hip_runtime.h>\n-    #include <Python.h>\n-    #include <stdio.h>\n-\n-    static inline void gpuAssert(hipError_t code, const char *file, int line)\n-    {{\n-      if (code != HIP_SUCCESS)\n-      {{\n-         const char* prefix = \"Triton Error [HIP]: \";\n-         const char* str = hipGetErrorString(code);\n-         char err[1024] = {{0}};\n-         snprintf(err, 1024, \"%s Code: %d, Messsage: %s\", prefix, code, str );\n-         PyErr_SetString(PyExc_RuntimeError, err);\n-      }}\n-    }}\n-\n-    #define HIP_CHECK(ans) {{ gpuAssert((ans), __FILE__, __LINE__); }}\n-\n-    static void _launch(int gridX, int gridY, int gridZ, int num_warps, int shared_memory, hipStream_t stream, hipFunction_t function, {arg_decls}) {{\n-      void *params[] = {{ {', '.join(f\"&arg{i}\" for i in signature.keys() if i not in constants)} }};\n-      if (gridX*gridY*gridZ > 0) {{\n-          HIP_CHECK(hipModuleLaunchKernel(function, gridX, gridY, gridZ, 64*num_warps, 1, 1, shared_memory, stream, params, 0));\n-      }}\n-    }}\n-\n-    typedef struct _DevicePtrInfo {{\n-      hipDeviceptr_t dev_ptr;\n-      bool valid;\n-    }} DevicePtrInfo;\n-\n-    static inline DevicePtrInfo getPointer(PyObject *obj, int idx) {{\n-      DevicePtrInfo ptr_info;\n-      ptr_info.dev_ptr = 0;\n-      ptr_info.valid = true;\n-\n-      if (PyLong_Check(obj)) {{\n-        ptr_info.dev_ptr = (hipDeviceptr_t)PyLong_AsUnsignedLongLong(obj);\n-        return ptr_info;\n-      }}\n-\n-      if (obj == Py_None) {{\n-        // valid nullptr\n-        return ptr_info;\n-      }}\n-\n-      PyObject *ptr = PyObject_GetAttrString(obj, \"data_ptr\");\n-\n-      if (ptr) {{\n-        PyObject *empty_tuple = PyTuple_New(0);\n-        PyObject *ret = PyObject_Call(ptr, empty_tuple, NULL);\n-        Py_DECREF(empty_tuple);\n-        Py_DECREF(ptr);\n-\n-        if (!PyLong_Check(ret)) {{\n-          PyErr_SetString(PyExc_TypeError, \"data_ptr method of Pointer object must return 64-bit int\");\n-          ptr_info.valid = false;\n-          return ptr_info;\n-        }}\n-\n-        ptr_info.dev_ptr = (hipDeviceptr_t)PyLong_AsUnsignedLongLong(ret);\n-\n-        if (!ptr_info.dev_ptr)\n-          return ptr_info;\n-\n-        uint64_t dev_ptr;\n-        hipError_t status = hipPointerGetAttribute(&dev_ptr, HIP_POINTER_ATTRIBUTE_DEVICE_POINTER, ptr_info.dev_ptr);\n-        if (status == hipErrorInvalidValue) {{\n-            PyErr_Format(PyExc_ValueError,\n-                         \"Pointer argument (at %d) cannot be accessed from Triton (cpu tensor?)\", idx);\n-            ptr_info.valid = false;\n-        }}\n-\n-        ptr_info.dev_ptr = (hipDeviceptr_t)dev_ptr;\n-        return ptr_info;\n-      }}\n-\n-      PyErr_SetString(PyExc_TypeError, \"Pointer argument must be either uint64 or have data_ptr method\");\n-      ptr_info.valid = false;\n-      return ptr_info;\n-    }}\n-\n-    static PyObject* launch(PyObject* self, PyObject* args) {{\n-\n-      int gridX, gridY, gridZ;\n-      uint64_t _stream;\n-      uint64_t _function;\n-      int num_warps;\n-      int shared_memory;\n-      PyObject *launch_enter_hook = NULL;\n-      PyObject *launch_exit_hook = NULL;\n-      PyObject *compiled_kernel = NULL;\n-\n-      {' '.join([f\"{_extracted_type(ty)} _arg{i}; \" for i, ty in signature.items()])}\n-      if (!PyArg_ParseTuple(args, \\\"{format}\\\", &gridX, &gridY, &gridZ, &num_warps, &shared_memory, &_stream, &_function, &launch_enter_hook, &launch_exit_hook, &compiled_kernel{', ' + ', '.join(f\"&_arg{i}\" for i, ty in signature.items()) if len(signature) > 0 else ''})) {{\n-        return NULL;\n-      }}\n-\n-      if (launch_enter_hook != Py_None && !PyObject_CallObject(launch_enter_hook, args)) {{\n-        return NULL;\n-      }}\n-\n-      // raise exception asap\n-      {\"; \".join([f\"DevicePtrInfo ptr_info{i} = getPointer(_arg{i}, {i}); if (!ptr_info{i}.valid) return NULL;\" if ty[0] == \"*\" else \"\" for i, ty in signature.items()])};\n-      Py_BEGIN_ALLOW_THREADS;\n-      _launch(gridX, gridY, gridZ, num_warps, shared_memory, (hipStream_t)_stream, (hipFunction_t)_function{', ' + ', '.join(f\"ptr_info{i}.dev_ptr\" if ty[0]==\"*\" else f\"_arg{i}\" for i, ty in signature.items()) if len(signature) > 0 else ''});\n-      Py_END_ALLOW_THREADS;\n-\n-      if (launch_exit_hook != Py_None && !PyObject_CallObject(launch_exit_hook, args)) {{\n-        return NULL;\n-      }}\n-\n-      // return None\n-      Py_INCREF(Py_None);\n-      return Py_None;\n-    }}\n-\n-    static PyMethodDef ModuleMethods[] = {{\n-      {{\"launch\", launch, METH_VARARGS, \"Entry point for all kernels with this signature\"}},\n-      {{NULL, NULL, 0, NULL}} // sentinel\n-    }};\n-\n-    static struct PyModuleDef ModuleDef = {{\n-      PyModuleDef_HEAD_INIT,\n-      \\\"__triton_launcher\\\",\n-      NULL, //documentation\n-      -1, //size\n-      ModuleMethods\n-    }};\n-\n-    PyMODINIT_FUNC PyInit___triton_launcher(void) {{\n-      PyObject *m = PyModule_Create(&ModuleDef);\n-      if(m == NULL) {{\n-        return NULL;\n-      }}\n-      PyModule_AddFunctions(m, ModuleMethods);\n-      return m;\n-    }}\n-    \"\"\"\n-    else:\n-        folded_without_constexprs = [c for c in ids['ids_of_folded_args'] if c not in ids['ids_of_const_exprs']]\n-        params = [i for i in signature.keys() if i >= start_desc or (i not in constants and i not in folded_without_constexprs)]\n-        src = f\"\"\"\n+    folded_without_constexprs = [c for c in ids['ids_of_folded_args'] if c not in ids['ids_of_const_exprs']]\n+    params = [i for i in signature.keys() if i >= start_desc or (i not in constants and i not in folded_without_constexprs)]\n+    src = f\"\"\"\n #include \\\"cuda.h\\\"\n #include <stdbool.h>\n #include <Python.h>"}, {"filename": "python/triton/language/math.py", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -1,17 +1,18 @@\n import functools\n import os\n \n+from ..common.build import is_hip\n from . import core\n \n \n @functools.lru_cache()\n def libdevice_path():\n-    import torch\n     third_party_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"third_party\")\n-    if torch.version.hip is None:\n-        default = os.path.join(third_party_dir, \"cuda\", \"lib\", \"libdevice.10.bc\")\n+    if is_hip():\n+        default = os.path.join(third_party_dir, \"hip\", \"lib\", \"bitcode\", \"cuda2gcn.bc\")\n     else:\n-        default = ''\n+        default = os.path.join(third_party_dir, \"cuda\", \"lib\", \"libdevice.10.bc\")\n+\n     return os.getenv(\"TRITON_LIBDEVICE_PATH\", default)\n \n "}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 40, "deletions": 0, "changes": 40, "file_content_changes": "@@ -4,6 +4,7 @@\n from typing import List, Optional, Sequence, Tuple, TypeVar\n \n from .._C.libtriton.triton import ir\n+from ..common.build import is_hip\n from . import core as tl\n \n T = TypeVar('T')\n@@ -1239,6 +1240,19 @@ def atomic_xchg(ptr: tl.tensor,\n # ===----------------------------------------------------------------------===//\n \n \n+def gpu_has_mfma() -> bool:\n+    if not is_hip():\n+        return False\n+    return True  # mfma supported in ['gfx908', 'gfx90a']\n+\n+\n+def mfma_supported(M, N, K, allow_tf32, ret_scalar_ty) -> bool:\n+    if not gpu_has_mfma():\n+        return False\n+    # TODO: Add check for configurations and types.\n+    return True\n+\n+\n def dot(lhs: tl.tensor,\n         rhs: tl.tensor,\n         allow_tf32: bool,\n@@ -1292,6 +1306,32 @@ def assert_dtypes_valid(lhs_dtype, rhs_dtype, arch):\n \n     M = lhs.type.shape[0]\n     N = rhs.type.shape[1]\n+\n+    # Cast operands of types f16 and i8 for configurations where FMA only supported.\n+    if is_hip() and not mfma_supported(M, N, lhs.type.shape[1], allow_tf32, ret_scalar_ty):\n+        ret_cast_scalar_ty = tl.float32 if lhs.type.scalar.is_int() else ret_scalar_ty\n+        lhs = cast(lhs, ret_cast_scalar_ty, builder)\n+        rhs = cast(rhs, ret_cast_scalar_ty, builder)\n+        if ret_cast_scalar_ty == tl.float16:\n+            _0 = builder.create_splat(builder.get_fp16(0), [M, N])\n+        else:\n+            _0 = builder.create_splat(builder.get_fp32(0), [M, N])\n+        ret_ty = tl.block_type(ret_cast_scalar_ty, [M, N])\n+        ret = tl.tensor(builder.create_dot(lhs.handle, rhs.handle, _0, allow_tf32),\n+                        ret_ty)\n+        return cast(ret, ret_scalar_ty, builder)\n+    if is_hip() and mfma_supported(M, N, lhs.type.shape[1], allow_tf32, ret_scalar_ty) and ret_scalar_ty.primitive_bitwidth < 32:\n+        if lhs.type.scalar.is_int():\n+            ret_dot_scalar_ty = tl.int32\n+            _0 = builder.create_splat(builder.get_int32(0), [M, N])\n+        else:\n+            ret_dot_scalar_ty = tl.float32\n+            _0 = builder.create_splat(builder.get_fp32(0), [M, N])\n+        ret_ty = tl.block_type(ret_dot_scalar_ty, [M, N])\n+        ret = tl.tensor(builder.create_dot(lhs.handle, rhs.handle, _0, allow_tf32),\n+                        ret_ty)\n+        return cast(ret, ret_scalar_ty, builder)\n+\n     _0 = builder.create_splat(_0, [M, N])\n     ret_ty = tl.block_type(ret_scalar_ty, [M, N])\n     return tl.tensor(builder.create_dot(lhs.handle, rhs.handle, _0, allow_tf32),"}, {"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -21,7 +21,7 @@ def _fwd_kernel(\n     Out,\n     stride_qz, stride_qh, stride_qm, stride_qk,\n     stride_kz, stride_kh, stride_kn, stride_kk,\n-    stride_vz, stride_vh, stride_vk, stride_vn,\n+    stride_vz, stride_vh, stride_vn, stride_vk,\n     stride_oz, stride_oh, stride_om, stride_on,\n     Z, H, N_CTX,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n@@ -50,7 +50,7 @@ def _fwd_kernel(\n     V_block_ptr = tl.make_block_ptr(\n         base=V + qvk_offset,\n         shape=(N_CTX, BLOCK_DMODEL),\n-        strides=(stride_vk, stride_vn),\n+        strides=(stride_vn, stride_vk),\n         offsets=(0, 0),\n         block_shape=(BLOCK_N, BLOCK_DMODEL),\n         order=(1, 0)\n@@ -137,7 +137,7 @@ def _bwd_kernel_one_col_block(\n     D,\n     stride_dqa, stride_qz, stride_qh, stride_qm, stride_qk,\n     stride_kz, stride_kh, stride_kn, stride_kk,\n-    stride_vz, stride_vh, stride_vk, stride_vn,\n+    stride_vz, stride_vh, stride_vn, stride_vk,\n     Z, H, N_CTX,\n     off_hz, start_n, num_block,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n@@ -159,7 +159,7 @@ def _bwd_kernel_one_col_block(\n     # initialize pointers to value-like data\n     q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n     k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n-    v_ptrs = V + (offs_n[:, None] * stride_vk + offs_k[None, :] * stride_vn)\n+    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_k[None, :] * stride_vk)\n     do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n     dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n     # pointer to row-wise quantities in value-like data\n@@ -212,7 +212,7 @@ def _bwd_kernel_one_col_block(\n         q_ptrs += BLOCK_M * stride_qm\n         do_ptrs += BLOCK_M * stride_qm\n     # write-back\n-    dv_ptrs = DV + (offs_n[:, None] * stride_vk + offs_k[None, :] * stride_vn)\n+    dv_ptrs = DV + (offs_n[:, None] * stride_vn + offs_k[None, :] * stride_vk)\n     dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n     tl.store(dv_ptrs, dv)\n     tl.store(dk_ptrs, dk)\n@@ -228,7 +228,7 @@ def _bwd_kernel(\n     D,\n     stride_dqa, stride_qz, stride_qh, stride_qm, stride_qk,\n     stride_kz, stride_kh, stride_kn, stride_kk,\n-    stride_vz, stride_vh, stride_vk, stride_vn,\n+    stride_vz, stride_vh, stride_vn, stride_vk,\n     Z, H, N_CTX,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n@@ -259,7 +259,7 @@ def _bwd_kernel(\n                 D,\n                 stride_dqa, stride_qz, stride_qh, stride_qm, stride_qk,\n                 stride_kz, stride_kh, stride_kn, stride_kk,\n-                stride_vz, stride_vh, stride_vk, stride_vn,\n+                stride_vz, stride_vh, stride_vn, stride_vk,\n                 Z, H, N_CTX,\n                 off_hz, start_n, num_block_n,\n                 BLOCK_M=BLOCK_M, BLOCK_DMODEL=BLOCK_DMODEL,\n@@ -276,7 +276,7 @@ def _bwd_kernel(\n             D,\n             stride_dqa, stride_qz, stride_qh, stride_qm, stride_qk,\n             stride_kz, stride_kh, stride_kn, stride_kk,\n-            stride_vz, stride_vh, stride_vk, stride_vn,\n+            stride_vz, stride_vh, stride_vn, stride_vk,\n             Z, H, N_CTX,\n             off_hz, start_n, num_block_n,\n             BLOCK_M=BLOCK_M, BLOCK_DMODEL=BLOCK_DMODEL,"}, {"filename": "python/triton/runtime/cache.py", "status": "modified", "additions": 8, "deletions": 6, "changes": 14, "file_content_changes": "@@ -40,18 +40,20 @@ def __init__(self, key):\n         self.key = key\n         self.lock_path = None\n         # create cache directory if it doesn't exist\n-        self.cache_dir = os.environ.get('TRITON_CACHE_DIR', default_cache_dir())\n+        self.cache_dir = os.getenv('TRITON_CACHE_DIR', \"\").strip() or default_cache_dir()\n         if self.cache_dir:\n             self.cache_dir = os.path.join(self.cache_dir, self.key)\n             self.lock_path = os.path.join(self.cache_dir, \"lock\")\n             os.makedirs(self.cache_dir, exist_ok=True)\n+        else:\n+            raise RuntimeError(\"Could not create or locate cache dir\")\n \n     def _make_path(self, filename) -> str:\n         return os.path.join(self.cache_dir, filename)\n \n-    def has_file(self, filename):\n+    def has_file(self, filename) -> bool:\n         if not self.cache_dir:\n-            return False\n+            raise RuntimeError(\"Could not create or locate cache dir\")\n         return os.path.exists(self._make_path(filename))\n \n     def get_file(self, filename) -> Optional[str]:\n@@ -80,16 +82,16 @@ def get_group(self, filename: str) -> Optional[Dict[str, str]]:\n         return result\n \n     # Note a group of pushed files as being part of a group\n-    def put_group(self, filename: str, group: Dict[str, str]):\n+    def put_group(self, filename: str, group: Dict[str, str]) -> str:\n         if not self.cache_dir:\n-            return\n+            raise RuntimeError(\"Could not create or locate cache dir\")\n         grp_contents = json.dumps({\"child_paths\": sorted(list(group.keys()))})\n         grp_filename = f\"__grp__{filename}\"\n         return self.put(grp_contents, grp_filename, binary=False)\n \n     def put(self, data, filename, binary=True) -> str:\n         if not self.cache_dir:\n-            return\n+            raise RuntimeError(\"Could not create or locate cache dir\")\n         binary = isinstance(data, bytes)\n         if not binary:\n             data = str(data)"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -383,20 +383,20 @@ def {self.fn.__name__}({args_signature}grid=None, num_warps=None, num_ctas=1, nu\n         device_type = self._conclude_device_type(device_types, {pinned_memory_flags})\n \n     device_backend = None\n-    if device_type not in ['cuda', 'hip']:\n+    if device_type not in ['cuda']:\n         device_backend = get_backend(device_type)\n         if device_backend is None:\n             raise ValueError('Cannot find backend for ' + device_type)\n \n     if device is None:\n-        if device_type in ['cuda', 'hip']:\n+        if device_type in ['cuda']:\n             device = get_current_device()\n             set_current_device(device)\n         else:\n             device = device_backend.get_current_device()\n             device_backend.set_current_device(device)\n     if stream is None and not warmup:\n-        if device_type in ['cuda', 'hip']:\n+        if device_type in ['cuda']:\n             stream = get_cuda_stream(device)\n         else:\n             stream = device_backend.get_stream()"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 337, "deletions": 3, "changes": 340, "file_content_changes": "@@ -77,6 +77,36 @@ tt.func @remat_fast_load(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   tt.return\n }\n \n+// Hoist the convert on top of ext to make it cheaper.\n+// CHECK-LABEL: hoist_above_ext\n+tt.func @hoist_above_ext(%arg0: tensor<1024xf16, #layout0>, %arg1: f32) -> tensor<1024xf32, #layout1> {\n+// CHECK: %[[CVT:.+]] = triton_gpu.convert_layout\n+// CHECK: arith.extf %[[CVT]]\n+// CHECK-NOT: triton_gpu.convert_layout\n+// CHECK: tt.return\n+  %0 = arith.extf %arg0 : tensor<1024xf16, #layout0> to tensor<1024xf32, #layout0>\n+  %1 = tt.splat %arg1 : (f32) -> tensor<1024xf32, #layout0>\n+  %2 = arith.addf %0, %1 : tensor<1024xf32, #layout0>\n+  %3 = triton_gpu.convert_layout %2 : (tensor<1024xf32, #layout0>) -> tensor<1024xf32, #layout1>\n+  tt.return %3 : tensor<1024xf32, #layout1>\n+}\n+\n+// CHECK-LABEL: hoist_above_ext2\n+tt.func @hoist_above_ext2(%arg0: tensor<1024xf16, #layout0>, %arg1: f16) -> tensor<1024xf32, #layout1> {\n+// CHECK: %[[CVT:.+]] = triton_gpu.convert_layout\n+// CHECK: arith.extf %[[CVT]]\n+// CHECK-NOT: triton_gpu.convert_layout\n+// CHECK: tt.return\n+  %0 = arith.extf %arg0 : tensor<1024xf16, #layout0> to tensor<1024xf32, #layout0>\n+  %1 = tt.splat %arg1 : (f16) -> tensor<1024xf16, #layout0>\n+  %2 = arith.extf %1 : tensor<1024xf16, #layout0> to tensor<1024xf32, #layout0>\n+  %3 = arith.addf %0, %2 : tensor<1024xf32, #layout0>\n+  %4 = triton_gpu.convert_layout %3 : (tensor<1024xf32, #layout0>) -> tensor<1024xf32, #layout1>\n+  tt.return %4 : tensor<1024xf32, #layout1>\n+}\n+\n+\n+\n // CHECK-LABEL: if\n tt.func @if(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n@@ -229,8 +259,10 @@ tt.func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32,\n   // CHECK-NEXT: {{.*}} = tt.addptr {{.*}} : tensor<64x64x!tt.ptr<f32, 1>, [[$row_layout]]>, tensor<64x64xi32, [[$row_layout]]>\n   // CHECK-NEXT: scf.yield {{.*}} : tensor<64x64xf32, [[$row_layout]]>, tensor<64x64x!tt.ptr<f32, 1>, [[$row_layout]]>\n   // CHECK-NEXT: }\n-  // CHECK-NEXT: {{.*}} = triton_gpu.convert_layout [[loop_ret]]#0 : (tensor<64x64xf32, [[$row_layout]]>) -> tensor<64x64xf32, [[$col_layout_novec]]>\n   // CHECK-NOT: triton_gpu.convert_layout\n+  //     CHECK: {{.*}} = triton_gpu.convert_layout [[loop_ret]]#0 : (tensor<64x64xf32, [[$row_layout]]>) -> tensor<64x64xf32, [[$col_layout_novec]]>\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  //    CHECK:  tt.return\n   %cst = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n   %cst_0 = arith.constant dense<64> : tensor<64x64xi32, #blocked1>\n   %c1 = arith.constant 1 : index\n@@ -276,6 +308,19 @@ tt.func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32,\n }\n \n // CHECK-LABEL: loop_if\n+// CHECK-NOT: triton_gpu.convert_layout\n+//     CHECK: scf.for\n+// CHECK-NOT: triton_gpu.convert_layout\n+//     CHECK:   scf.if\n+// CHECK-NOT: triton_gpu.convert_layout\n+//     CHECK:     scf.yield\n+//     CHECK:   else\n+//     CHECK:     scf.yield\n+// CHECK-NOT: triton_gpu.convert_layout\n+//     CHECK:   scf.yield\n+//     CHECK: triton_gpu.convert_layout\n+// CHECK-NOT: triton_gpu.convert_layout\n+//     CHECK: tt.store\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func @loop_if(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32) {\n   %cst = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n@@ -1125,14 +1170,14 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n \n // -----\n \n-// Check if the SimplifyReduceCvt handles convert_layout lifted from the for loop.\n // CHECK-LABEL: reduce_cvt2\n // Match the reduction\n // CHECK: tt.reduce\n // CHECK-SAME: axis = 1\n // CHECK: (tensor<1x256xf32, #blocked>) -> tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n-// CHECK-NEXT: triton_gpu.convert_layout\n+// CHECK: triton_gpu.convert_layout\n // CHECK-NOT: triton_gpu.convert_layout\n+// CHECK: tt.return\n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [1, 0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n #blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [1, 0]}>\n@@ -1347,6 +1392,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-war\n // Check if MoveConvertOutOfLoop hangs because of adding additional conversions\n // CHECK-LABEL: loop_print\n // CHECK-NOT: triton_gpu.convert_layout\n+//     CHECK: tt.return\n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [0, 1]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n #blocked2 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n@@ -1502,3 +1548,291 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-war\n     tt.return\n   }\n }\n+\n+\n+// -----\n+\n+// Check that we don't have extra convert for flash attention IR.\n+#blocked = #triton_gpu.blocked<{sizePerThread = [4, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [1, 0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+#blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n+#blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n+#blocked4 = #triton_gpu.blocked<{sizePerThread = [1, 1, 8], threadsPerWarp = [4, 1, 8], warpsPerCTA = [4, 1, 1], order = [1, 2, 0], CTAsPerCGA = [1, 1, 1], CTASplitNum = [1, 1, 1], CTAOrder = [1, 0, 2]}>\n+#blocked5 = #triton_gpu.blocked<{sizePerThread = [1, 1, 8], threadsPerWarp = [1, 4, 8], warpsPerCTA = [1, 4, 1], order = [0, 2, 1], CTAsPerCGA = [1, 1, 1], CTASplitNum = [1, 1, 1], CTAOrder = [0, 1, 2]}>\n+#blocked6 = #triton_gpu.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n+#blocked7 = #triton_gpu.blocked<{sizePerThread = [8, 1, 1], threadsPerWarp = [8, 1, 4], warpsPerCTA = [1, 1, 4], order = [1, 0, 2], CTAsPerCGA = [1, 1, 1], CTASplitNum = [1, 1, 1], CTAOrder = [1, 0, 2]}>\n+#blocked8 = #triton_gpu.blocked<{sizePerThread = [1, 8, 1], threadsPerWarp = [1, 8, 4], warpsPerCTA = [1, 1, 4], order = [0, 1, 2], CTAsPerCGA = [1, 1, 1], CTASplitNum = [1, 1, 1], CTAOrder = [0, 1, 2]}>\n+#blocked9 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [1, 0]}>\n+module attributes {\"triton_gpu.compute-capability\" = 90 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  tt.func public @attention_fw(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg3: f32, %arg4: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32}, %arg5: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg7: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg8: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg9: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg10: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg11: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg12: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg13: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg14: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg15: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg16: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg17: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg18: i32, %arg19: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg20: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg21: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}) attributes {noinline = false} {\n+    %c0_i64 = arith.constant 0 : i64\n+    %c64_i64 = arith.constant 64 : i64\n+    %cst = arith.constant dense<0.000000e+00> : tensor<128x64xf16, #blocked>\n+    %cst_0 = arith.constant dense<0xFF800000> : tensor<128xf32, #blocked1>\n+    %cst_1 = arith.constant dense<0.000000e+00> : tensor<128xf32, #blocked1>\n+    %c64_i32 = arith.constant 64 : i32\n+    %c0_i32 = arith.constant 0 : i32\n+    %cst_2 = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #blocked2>\n+    %cst_3 = arith.constant 1.44269502 : f32\n+    %c128_i32 = arith.constant 128 : i32\n+    %0 = tt.get_program_id x : i32\n+    %1 = tt.get_program_id y : i32\n+    %2 = arith.muli %1, %arg7 : i32\n+    %3 = arith.muli %1, %arg10 : i32\n+    %4 = tt.addptr %arg0, %2 : !tt.ptr<f16, 1>, i32\n+    %5 = arith.muli %0, %c128_i32 : i32\n+    %6 = arith.extsi %arg8 : i32 to i64\n+    %7 = arith.extsi %5 : i32 to i64\n+    %8 = tt.addptr %arg1, %3 : !tt.ptr<f16, 1>, i32\n+    %9 = arith.addi %arg20, %arg21 : i32\n+    %10 = arith.extsi %arg11 : i32 to i64\n+    %11 = tt.addptr %arg2, %3 : !tt.ptr<f16, 1>, i32\n+    %12 = arith.extsi %arg14 : i32 to i64\n+    %13 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked1>\n+    %14 = tt.splat %5 : (i32) -> tensor<128xi32, #blocked1>\n+    %15 = arith.addi %14, %13 : tensor<128xi32, #blocked1>\n+    %16 = arith.mulf %arg3, %cst_3 : f32\n+    %17 = tt.splat %4 : (!tt.ptr<f16, 1>) -> tensor<128x64x!tt.ptr<f16, 1>, #blocked3>\n+    %18 = tt.splat %7 : (i64) -> tensor<128xi64, #blocked3>\n+    %19 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked3>\n+    %20 = arith.extsi %19 : tensor<128xi32, #blocked3> to tensor<128xi64, #blocked3>\n+    %21 = arith.addi %18, %20 : tensor<128xi64, #blocked3>\n+    %22 = triton_gpu.convert_layout %21 : (tensor<128xi64, #blocked3>) -> tensor<128xi64, #triton_gpu.slice<{dim = 1, parent = #blocked4}>>\n+    %23 = tt.expand_dims %22 {axis = 1 : i32} : (tensor<128xi64, #triton_gpu.slice<{dim = 1, parent = #blocked4}>>) -> tensor<128x1xi64, #blocked4>\n+    %24 = tt.splat %6 : (i64) -> tensor<128x1xi64, #blocked4>\n+    %25 = arith.muli %23, %24 : tensor<128x1xi64, #blocked4>\n+    %26 = tt.broadcast %25 : (tensor<128x1xi64, #blocked4>) -> tensor<128x64xi64, #blocked4>\n+    %27 = triton_gpu.convert_layout %26 : (tensor<128x64xi64, #blocked4>) -> tensor<128x64xi64, #blocked3>\n+    %28 = tt.addptr %17, %27 : tensor<128x64x!tt.ptr<f16, 1>, #blocked3>, tensor<128x64xi64, #blocked3>\n+    %29 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked3>\n+    %30 = arith.extsi %29 : tensor<64xi32, #blocked3> to tensor<64xi64, #blocked3>\n+    %31 = triton_gpu.convert_layout %30 : (tensor<64xi64, #blocked3>) -> tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked5}>>\n+    %32 = tt.expand_dims %31 {axis = 0 : i32} : (tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked5}>>) -> tensor<1x64xi64, #blocked5>\n+    %33 = tt.broadcast %32 : (tensor<1x64xi64, #blocked5>) -> tensor<128x64xi64, #blocked5>\n+    %34 = triton_gpu.convert_layout %33 : (tensor<128x64xi64, #blocked5>) -> tensor<128x64xi64, #blocked3>\n+    %35 = tt.addptr %28, %34 : tensor<128x64x!tt.ptr<f16, 1>, #blocked3>, tensor<128x64xi64, #blocked3>\n+    %36 = tt.load %35 {boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x64xf16, #blocked3>\n+    %37 = triton_gpu.convert_layout %36 : (tensor<128x64xf16, #blocked3>) -> tensor<128x64xf16, #blocked2>\n+    %38 = tt.splat %16 : (f32) -> tensor<128x64xf32, #blocked2>\n+    %39 = arith.extf %37 : tensor<128x64xf16, #blocked2> to tensor<128x64xf32, #blocked2>\n+    %40 = arith.mulf %39, %38 : tensor<128x64xf32, #blocked2>\n+    %41 = arith.truncf %40 : tensor<128x64xf32, #blocked2> to tensor<128x64xf16, #blocked2>\n+// CHECK-NOT: triton_gpu.convert_layout\n+//     CHECK: scf.for\n+// CHECK-NOT:   triton_gpu.convert_layout\n+//     CHECK:   triton_gpu.convert_layout %{{.*}} #triton_gpu.dot_op\n+//     CHECK:   triton_gpu.convert_layout %{{.*}} #triton_gpu.dot_op\n+// CHECK-NOT:   triton_gpu.convert_layout\n+//     CHECK:   tt.dot\n+// CHECK-NOT:   triton_gpu.convert_layout\n+//     CHECK:   triton_gpu.convert_layout %{{.*}} #triton_gpu.dot_op\n+//     CHECK:   triton_gpu.convert_layout %{{.*}} #triton_gpu.dot_op\n+// CHECK-NOT:   triton_gpu.convert_layout\n+//     CHECK:   tt.dot\n+//     CHECK:   scf.yield\n+    %42:5 = scf.for %arg22 = %c0_i32 to %9 step %c64_i32 iter_args(%arg23 = %cst_2, %arg24 = %cst_1, %arg25 = %cst_0, %arg26 = %c0_i64, %arg27 = %c0_i64) -> (tensor<128x64xf32, #blocked2>, tensor<128xf32, #blocked1>, tensor<128xf32, #blocked1>, i64, i64)  : i32 {\n+      %78 = tt.splat %8 : (!tt.ptr<f16, 1>) -> tensor<64x64x!tt.ptr<f16, 1>, #blocked6>\n+      %79 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked6>\n+      %80 = arith.extsi %79 : tensor<64xi32, #blocked6> to tensor<64xi64, #blocked6>\n+      %81 = triton_gpu.convert_layout %80 : (tensor<64xi64, #blocked6>) -> tensor<64xi64, #triton_gpu.slice<{dim = 1, parent = #blocked7}>>\n+      %82 = tt.expand_dims %81 {axis = 1 : i32} : (tensor<64xi64, #triton_gpu.slice<{dim = 1, parent = #blocked7}>>) -> tensor<64x1xi64, #blocked7>\n+      %83 = tt.broadcast %82 : (tensor<64x1xi64, #blocked7>) -> tensor<64x64xi64, #blocked7>\n+      %84 = triton_gpu.convert_layout %83 : (tensor<64x64xi64, #blocked7>) -> tensor<64x64xi64, #blocked6>\n+      %85 = tt.addptr %78, %84 : tensor<64x64x!tt.ptr<f16, 1>, #blocked6>, tensor<64x64xi64, #blocked6>\n+      %86 = tt.splat %arg26 : (i64) -> tensor<64xi64, #blocked6>\n+      %87 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked6>\n+      %88 = arith.extsi %87 : tensor<64xi32, #blocked6> to tensor<64xi64, #blocked6>\n+      %89 = arith.addi %86, %88 : tensor<64xi64, #blocked6>\n+      %90 = triton_gpu.convert_layout %89 : (tensor<64xi64, #blocked6>) -> tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked8}>>\n+      %91 = tt.expand_dims %90 {axis = 0 : i32} : (tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked8}>>) -> tensor<1x64xi64, #blocked8>\n+      %92 = tt.splat %10 : (i64) -> tensor<1x64xi64, #blocked8>\n+      %93 = arith.muli %91, %92 : tensor<1x64xi64, #blocked8>\n+      %94 = tt.broadcast %93 : (tensor<1x64xi64, #blocked8>) -> tensor<64x64xi64, #blocked8>\n+      %95 = triton_gpu.convert_layout %94 : (tensor<64x64xi64, #blocked8>) -> tensor<64x64xi64, #blocked6>\n+      %96 = tt.addptr %85, %95 : tensor<64x64x!tt.ptr<f16, 1>, #blocked6>, tensor<64x64xi64, #blocked6>\n+      %97 = tt.load %96 {boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf16, #blocked6>\n+      %98 = tt.splat %11 : (!tt.ptr<f16, 1>) -> tensor<64x64x!tt.ptr<f16, 1>, #blocked3>\n+      %99 = tt.splat %arg27 : (i64) -> tensor<64xi64, #blocked3>\n+      %100 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked3>\n+      %101 = arith.extsi %100 : tensor<64xi32, #blocked3> to tensor<64xi64, #blocked3>\n+      %102 = arith.addi %99, %101 : tensor<64xi64, #blocked3>\n+      %103 = triton_gpu.convert_layout %102 : (tensor<64xi64, #blocked3>) -> tensor<64xi64, #triton_gpu.slice<{dim = 1, parent = #blocked4}>>\n+      %104 = tt.expand_dims %103 {axis = 1 : i32} : (tensor<64xi64, #triton_gpu.slice<{dim = 1, parent = #blocked4}>>) -> tensor<64x1xi64, #blocked4>\n+      %105 = tt.splat %12 : (i64) -> tensor<64x1xi64, #blocked4>\n+      %106 = arith.muli %104, %105 : tensor<64x1xi64, #blocked4>\n+      %107 = tt.broadcast %106 : (tensor<64x1xi64, #blocked4>) -> tensor<64x64xi64, #blocked4>\n+      %108 = triton_gpu.convert_layout %107 : (tensor<64x64xi64, #blocked4>) -> tensor<64x64xi64, #blocked3>\n+      %109 = tt.addptr %98, %108 : tensor<64x64x!tt.ptr<f16, 1>, #blocked3>, tensor<64x64xi64, #blocked3>\n+      %110 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked3>\n+      %111 = arith.extsi %110 : tensor<64xi32, #blocked3> to tensor<64xi64, #blocked3>\n+      %112 = triton_gpu.convert_layout %111 : (tensor<64xi64, #blocked3>) -> tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked5}>>\n+      %113 = tt.expand_dims %112 {axis = 0 : i32} : (tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked5}>>) -> tensor<1x64xi64, #blocked5>\n+      %114 = tt.broadcast %113 : (tensor<1x64xi64, #blocked5>) -> tensor<64x64xi64, #blocked5>\n+      %115 = triton_gpu.convert_layout %114 : (tensor<64x64xi64, #blocked5>) -> tensor<64x64xi64, #blocked3>\n+      %116 = tt.addptr %109, %115 : tensor<64x64x!tt.ptr<f16, 1>, #blocked3>, tensor<64x64xi64, #blocked3>\n+      %117 = tt.load %116 {boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf16, #blocked3>\n+      %118 = triton_gpu.convert_layout %41 : (tensor<128x64xf16, #blocked2>) -> tensor<128x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked}>>\n+      %119 = triton_gpu.convert_layout %97 : (tensor<64x64xf16, #blocked6>) -> tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked}>>\n+      %120 = tt.dot %118, %119, %cst {allowTF32 = true} : tensor<128x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<128x64xf16, #blocked>\n+      %121 = triton_gpu.convert_layout %120 : (tensor<128x64xf16, #blocked>) -> tensor<128x64xf16, #blocked2>\n+      %122 = arith.extf %121 : tensor<128x64xf16, #blocked2> to tensor<128x64xf32, #blocked2>\n+      %123 = \"tt.reduce\"(%122) <{axis = 1 : i32}> ({\n+      ^bb0(%arg28: f32, %arg29: f32):\n+        %153 = arith.maxf %arg28, %arg29 : f32\n+        tt.reduce.return %153 : f32\n+      }) : (tensor<128x64xf32, #blocked2>) -> tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+      %124 = triton_gpu.convert_layout %123 : (tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<128xf32, #blocked1>\n+      %125 = arith.maxf %arg25, %124 : tensor<128xf32, #blocked1>\n+      %126 = arith.subf %arg25, %125 : tensor<128xf32, #blocked1>\n+      %127 = tt.extern_elementwise %126 {pure = true, libname = \"libdevice\", libpath = \"/root/.pyenv/versions/3.9.9/lib/python3.9/site-packages/triton/language/../third_party/cuda/lib/libdevice.10.bc\", symbol = \"__nv_exp2f\"} : (tensor<128xf32, #blocked1>) -> tensor<128xf32, #blocked1>\n+      %128 = triton_gpu.convert_layout %125 : (tensor<128xf32, #blocked1>) -> tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked9}>>\n+      %129 = tt.expand_dims %128 {axis = 1 : i32} : (tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked9}>>) -> tensor<128x1xf32, #blocked9>\n+      %130 = triton_gpu.convert_layout %129 : (tensor<128x1xf32, #blocked9>) -> tensor<128x1xf32, #blocked2>\n+      %131 = tt.broadcast %130 : (tensor<128x1xf32, #blocked2>) -> tensor<128x64xf32, #blocked2>\n+      %132 = arith.subf %122, %131 : tensor<128x64xf32, #blocked2>\n+      %133 = tt.extern_elementwise %132 {pure = true, libname = \"libdevice\", libpath = \"/root/.pyenv/versions/3.9.9/lib/python3.9/site-packages/triton/language/../third_party/cuda/lib/libdevice.10.bc\", symbol = \"__nv_exp2f\"} : (tensor<128x64xf32, #blocked2>) -> tensor<128x64xf32, #blocked2>\n+      %134 = arith.mulf %arg24, %cst_1 : tensor<128xf32, #blocked1>\n+      %135 = arith.addf %134, %127 : tensor<128xf32, #blocked1>\n+      %136 = triton_gpu.convert_layout %135 : (tensor<128xf32, #blocked1>) -> tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked9}>>\n+      %137 = tt.expand_dims %136 {axis = 1 : i32} : (tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked9}>>) -> tensor<128x1xf32, #blocked9>\n+      %138 = triton_gpu.convert_layout %137 : (tensor<128x1xf32, #blocked9>) -> tensor<128x1xf32, #blocked2>\n+      %139 = tt.broadcast %138 : (tensor<128x1xf32, #blocked2>) -> tensor<128x64xf32, #blocked2>\n+      %140 = arith.mulf %arg23, %139 : tensor<128x64xf32, #blocked2>\n+      %141 = arith.truncf %133 : tensor<128x64xf32, #blocked2> to tensor<128x64xf16, #blocked2>\n+      %142 = triton_gpu.convert_layout %141 : (tensor<128x64xf16, #blocked2>) -> tensor<128x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked}>>\n+      %143 = triton_gpu.convert_layout %117 : (tensor<64x64xf16, #blocked3>) -> tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked}>>\n+      %144 = triton_gpu.convert_layout %140 : (tensor<128x64xf32, #blocked2>) -> tensor<128x64xf32, #blocked>\n+      %145 = tt.dot %142, %143, %144 {allowTF32 = true} : tensor<128x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<128x64xf32, #blocked>\n+      %146 = triton_gpu.convert_layout %145 : (tensor<128x64xf32, #blocked>) -> tensor<128x64xf32, #blocked2>\n+      %147 = arith.mulf %arg24, %127 : tensor<128xf32, #blocked1>\n+      %148 = \"tt.reduce\"(%133) <{axis = 1 : i32}> ({\n+      ^bb0(%arg28: f32, %arg29: f32):\n+        %153 = arith.addf %arg28, %arg29 : f32\n+        tt.reduce.return %153 : f32\n+      }) : (tensor<128x64xf32, #blocked2>) -> tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+      %149 = triton_gpu.convert_layout %148 : (tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<128xf32, #blocked1>\n+      %150 = arith.addf %147, %149 : tensor<128xf32, #blocked1>\n+      %151 = arith.addi %arg26, %c64_i64 : i64\n+      %152 = arith.addi %arg27, %c64_i64 : i64\n+      scf.yield %146, %150, %125, %151, %152 : tensor<128x64xf32, #blocked2>, tensor<128xf32, #blocked1>, tensor<128xf32, #blocked1>, i64, i64\n+    }\n+    %43 = triton_gpu.convert_layout %42#1 : (tensor<128xf32, #blocked1>) -> tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked9}>>\n+    %44 = tt.expand_dims %43 {axis = 1 : i32} : (tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked9}>>) -> tensor<128x1xf32, #blocked9>\n+    %45 = triton_gpu.convert_layout %44 : (tensor<128x1xf32, #blocked9>) -> tensor<128x1xf32, #blocked2>\n+    %46 = tt.broadcast %45 : (tensor<128x1xf32, #blocked2>) -> tensor<128x64xf32, #blocked2>\n+    %47 = arith.divf %42#0, %46 : tensor<128x64xf32, #blocked2>\n+    %48 = arith.muli %1, %arg20 : i32\n+    %49 = tt.addptr %arg4, %48 : !tt.ptr<f32, 1>, i32\n+    %50 = tt.splat %49 : (!tt.ptr<f32, 1>) -> tensor<128x!tt.ptr<f32, 1>, #blocked1>\n+    %51 = tt.addptr %50, %15 : tensor<128x!tt.ptr<f32, 1>, #blocked1>, tensor<128xi32, #blocked1>\n+    %52 = tt.extern_elementwise %42#1 {pure = true, libname = \"libdevice\", libpath = \"/root/.pyenv/versions/3.9.9/lib/python3.9/site-packages/triton/language/../third_party/cuda/lib/libdevice.10.bc\", symbol = \"__nv_log2f\"} : (tensor<128xf32, #blocked1>) -> tensor<128xf32, #blocked1>\n+    %53 = arith.addf %42#2, %52 : tensor<128xf32, #blocked1>\n+    tt.store %51, %53 {cache = 1 : i32, evict = 1 : i32} : tensor<128xf32, #blocked1>\n+    %54 = tt.addptr %arg5, %2 : !tt.ptr<f16, 1>, i32\n+    %55 = arith.extsi %arg17 : i32 to i64\n+    %56 = arith.extsi %5 : i32 to i64\n+    %57 = arith.truncf %47 : tensor<128x64xf32, #blocked2> to tensor<128x64xf16, #blocked2>\n+    %58 = triton_gpu.convert_layout %57 : (tensor<128x64xf16, #blocked2>) -> tensor<128x64xf16, #blocked3>\n+    %59 = tt.splat %54 : (!tt.ptr<f16, 1>) -> tensor<128x64x!tt.ptr<f16, 1>, #blocked3>\n+    %60 = tt.splat %56 : (i64) -> tensor<128xi64, #blocked3>\n+    %61 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked3>\n+    %62 = arith.extsi %61 : tensor<128xi32, #blocked3> to tensor<128xi64, #blocked3>\n+    %63 = arith.addi %60, %62 : tensor<128xi64, #blocked3>\n+    %64 = triton_gpu.convert_layout %63 : (tensor<128xi64, #blocked3>) -> tensor<128xi64, #triton_gpu.slice<{dim = 1, parent = #blocked4}>>\n+    %65 = tt.expand_dims %64 {axis = 1 : i32} : (tensor<128xi64, #triton_gpu.slice<{dim = 1, parent = #blocked4}>>) -> tensor<128x1xi64, #blocked4>\n+    %66 = tt.splat %55 : (i64) -> tensor<128x1xi64, #blocked4>\n+    %67 = arith.muli %65, %66 : tensor<128x1xi64, #blocked4>\n+    %68 = tt.broadcast %67 : (tensor<128x1xi64, #blocked4>) -> tensor<128x64xi64, #blocked4>\n+    %69 = triton_gpu.convert_layout %68 : (tensor<128x64xi64, #blocked4>) -> tensor<128x64xi64, #blocked3>\n+    %70 = tt.addptr %59, %69 : tensor<128x64x!tt.ptr<f16, 1>, #blocked3>, tensor<128x64xi64, #blocked3>\n+    %71 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked3>\n+    %72 = arith.extsi %71 : tensor<64xi32, #blocked3> to tensor<64xi64, #blocked3>\n+    %73 = triton_gpu.convert_layout %72 : (tensor<64xi64, #blocked3>) -> tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked5}>>\n+    %74 = tt.expand_dims %73 {axis = 0 : i32} : (tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked5}>>) -> tensor<1x64xi64, #blocked5>\n+    %75 = tt.broadcast %74 : (tensor<1x64xi64, #blocked5>) -> tensor<128x64xi64, #blocked5>\n+    %76 = triton_gpu.convert_layout %75 : (tensor<128x64xi64, #blocked5>) -> tensor<128x64xi64, #blocked3>\n+    %77 = tt.addptr %70, %76 : tensor<128x64x!tt.ptr<f16, 1>, #blocked3>, tensor<128x64xi64, #blocked3>\n+    tt.store %77, %58 {cache = 1 : i32, evict = 1 : i32} : tensor<128x64xf16, #blocked3>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+// CHECK-LABEL: axis_mismatch\n+tt.func @axis_mismatch(%arg0: f32) -> tensor<1xf32, #triton_gpu.slice<{dim = 0, parent = #blocked}>> {\n+// CHECK: %[[R:.+]] = \"tt.reduce\"(%0) <{axis = 1 : i32}>\n+// CHECK: %[[C:.+]] = triton_gpu.convert_layout %[[R]]\n+// CHECK: tt.return %[[C]]\n+  %0 = tt.splat %arg0 : (f32) -> tensor<1x16xf32, #blocked>\n+  %1 = \"tt.reduce\"(%0) <{axis = 1 : i32}> ({\n+    ^bb0(%arg9: f32, %arg10: f32):\n+    %60 = arith.addf %arg9, %arg10 : f32\n+    tt.reduce.return %60 : f32\n+  }) : (tensor<1x16xf32, #blocked>) -> tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+  %2 = triton_gpu.convert_layout %1 : (tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<1xf32, #blocked1>\n+  %3 = triton_gpu.convert_layout %2 : (tensor<1xf32, #blocked1>) -> tensor<1xf32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+  tt.return %3: tensor<1xf32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+}\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+// CHECK-LABEL: reduce_to_scalar\n+//   CHECK-NOT:   triton_gpu.convert_layout\n+//       CHECK:   tt.return\n+tt.func @reduce_to_scalar(%ptr: tensor<1024x!tt.ptr<f32>, #blocked>) -> (f32, i32) {\n+  %0 = tt.load %ptr {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked>\n+  %1 = triton_gpu.convert_layout %0 : (tensor<1024xf32, #blocked>) -> tensor<1024xf32, #blocked1>\n+  %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #blocked1>\n+  %3:2 = \"tt.reduce\"(%1, %2) <{axis = 0 : i32}> ({\n+    ^bb0(%arg7: f32, %arg8: i32, %arg9: f32, %arg10: i32):\n+    %51 = \"triton_gpu.cmpf\"(%arg7, %arg9) <{predicate = 1 : i64}> : (f32, f32) -> i1\n+    %52 = \"triton_gpu.cmpi\"(%arg8, %arg10) <{predicate = 2 : i64}> : (i32, i32) -> i1\n+    %53 = arith.andi %51, %52 : i1\n+    %54 = \"triton_gpu.cmpf\"(%arg7, %arg9) <{predicate = 2 : i64}> : (f32, f32) -> i1\n+    %55 = arith.ori %54, %53 : i1\n+    %56 = arith.select %55, %arg7, %arg9 : f32\n+    %57 = arith.select %55, %arg8, %arg10 : i32\n+    tt.reduce.return %56, %57 : f32, i32\n+  }) : (tensor<1024xf32, #blocked1>, tensor<1024xi32, #blocked1>) -> (f32, i32)\n+  tt.return %3#0, %3#1: f32, i32\n+}\n+}\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+// CHECK-LABEL: whileop\n+//       CHECK: %[[L:.+]] = tt.load %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked>\n+//       CHECK: %[[W:.+]] = scf.while (%[[I:.+]] = %[[L]], %{{.*}} = %{{.*}}) : (tensor<1024xf32, #blocked>, i1) -> tensor<1024xf32, #blocked> {\n+//       CHECK:   scf.condition(%{{.*}}) %[[I]] : tensor<1024xf32, #blocked>\n+//       CHECK: } do {\n+//       CHECK: ^bb0(%[[ARG1:.+]]: tensor<1024xf32, #blocked>):\n+//       CHECK:    %[[ADD:.+]] = arith.addf %[[ARG1]], %[[ARG1]] : tensor<1024xf32, #blocked>\n+//       CHECK:    scf.yield %[[ADD]], %{{.*}} : tensor<1024xf32, #blocked>, i1\n+//       CHECK:  }\n+//       CHECK:  tt.store %{{.*}}, %[[W]] {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf32, #blocked>\n+tt.func @whileop(%ptr: tensor<1024x!tt.ptr<f32>, #blocked>, %cond: i1) {\n+  %0 = tt.load %ptr {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked>\n+  %1 = triton_gpu.convert_layout %0 : (tensor<1024xf32, #blocked>) -> tensor<1024xf32, #blocked1>\n+  %2 = scf.while (%arg0 = %1, %arg1 = %cond) : (tensor<1024xf32, #blocked1>, i1) -> (tensor<1024xf32, #blocked1>) {\n+      scf.condition(%arg1) %arg0 : tensor<1024xf32, #blocked1>\n+    } do {\n+    ^bb0(%arg0: tensor<1024xf32, #blocked1>):\n+      %4 = triton_gpu.convert_layout %arg0 : (tensor<1024xf32, #blocked1>) -> tensor<1024xf32, #blocked>\n+      %5 = arith.addf %4, %4 : tensor<1024xf32, #blocked>\n+      %6 = triton_gpu.convert_layout %5 : (tensor<1024xf32, #blocked>) -> tensor<1024xf32, #blocked1>\n+      scf.yield %6, %cond : tensor<1024xf32, #blocked1>, i1\n+    }\n+  %3 = triton_gpu.convert_layout %2 : (tensor<1024xf32, #blocked1>) -> tensor<1024xf32, #blocked>\n+  tt.store %ptr, %3 {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf32, #blocked>\n+  tt.return\n+}\n+}"}, {"filename": "test/TritonGPU/dot-operands.mlir", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -split-input-file -tritongpu-optimize-dot-operands -tritongpu-remove-layout-conversions -canonicalize | FileCheck %s\n+// RUN: triton-opt %s -split-input-file -tritongpu-optimize-dot-operands -canonicalize | FileCheck %s\n \n #Cv2 = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #Av2k1 = #triton_gpu.dot_op<{opIdx = 0, parent = #Cv2, kWidth=1}>\n@@ -15,7 +15,7 @@\n #BLR = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #BLC = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [0, 1]}>\n \n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.compute-capability\" = 80} {\n \n // CHECK: tt.func @push_elementwise\n // CHECK: %[[ALOAD:.*]] = tt.load %arg0\n@@ -69,7 +69,7 @@ tt.func @succeeds_if_arg_is_not_convert_layout(\n #blockedA = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n #blockedB = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n #mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}>\n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.compute-capability\" = 80} {\n \n // CHECK: #[[BA:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0], CTAsPerCGA = [], CTASplitNum = [], CTAOrder = []}>\n // CHECK: #[[BB:.*]] = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1], CTAsPerCGA = [], CTASplitNum = [], CTAOrder = []}>\n@@ -104,7 +104,7 @@ tt.func @push_convert_both_operands(\n #blockedA = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n #blockedB = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n #mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}>\n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.compute-capability\" = 80} {\n \n // CHECK: #[[BA:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0], CTAsPerCGA = [], CTASplitNum = [], CTAOrder = []}>\n // CHECK: #[[BB:.*]] = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1], CTAsPerCGA = [], CTASplitNum = [], CTAOrder = []}>"}, {"filename": "third_party/amd_hip_backend", "status": "added", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -0,0 +1 @@\n+Subproject commit d0ad70d55df3ebe11cc80bbb364a91551e6b6248"}]