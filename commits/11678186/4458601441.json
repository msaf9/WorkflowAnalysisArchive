[{"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 23, "deletions": 28, "changes": 51, "file_content_changes": "@@ -7,7 +7,6 @@\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n-#include \"llvm/ADT/MapVector.h\"\n \n //===----------------------------------------------------------------------===//\n //\n@@ -78,15 +77,18 @@ class LoopPipeliner {\n   /// Block arguments that loads depend on\n   SetVector<BlockArgument> depArgs;\n \n-  /// If we have a load that depends on a block argument in the current\n-  /// iteration, it is an immediate dependency. Otherwise, it is a non-immediate\n-  /// dependency, which means the load depends on a block argument in the\n-  /// previous iterations.\n+  /// If we have a load that immedidately depends on a block argument in the\n+  /// current iteration, it is an immediate dependency. Otherwise, it is a\n+  /// non-immediate dependency, which means the load depends on a block argument\n+  /// in the previous iterations.\n   /// For example:\n   /// scf.for (%arg0, %arg1, %arg2) {\n   ///   %0 = load %arg0  <--- immediate dep, this address is initialized at\n-  ///   numStages-2 %1 = load %arg1 %2 = add %1, %arg2 %3 = load %2  <---\n-  ///   non-immediate dep, %arg1 must be an update-to-date value\n+  ///   numStages-2\n+  ///   %1 = load %arg1\n+  ///   %2 = add %1, %arg2\n+  ///   %3 = load %2  <--- non-immediate dep, %arg1 must be an update-to-date\n+  ///   value\n   /// }\n   SetVector<BlockArgument> immedidateDepArgs;\n \n@@ -96,7 +98,7 @@ class LoopPipeliner {\n   SetVector<Operation *> depOps;\n \n   /// collect values that v depends on and are defined inside the loop\n-  void collectDeps(Value v, int stages, llvm::MapVector<Value, int> &deps);\n+  void collectDeps(Value v, int stages, SetVector<Value> &deps);\n \n   void setValueMapping(Value origin, Value newValue, int stage);\n \n@@ -143,8 +145,7 @@ Value LoopPipeliner::lookupOrDefault(Value origin, int stage) {\n   return valueMapping[origin][stage];\n }\n \n-void LoopPipeliner::collectDeps(Value v, int stages,\n-                                llvm::MapVector<Value, int> &deps) {\n+void LoopPipeliner::collectDeps(Value v, int stages, SetVector<Value> &deps) {\n   // Loop-invariant value, skip\n   if (v.getParentRegion() != &forOp.getLoopBody())\n     return;\n@@ -154,25 +155,18 @@ void LoopPipeliner::collectDeps(Value v, int stages,\n   if (stages < 0)\n     return;\n \n-  // Record the minimum dependency stage to check if it is immediate or not\n-  auto updateDeps = [&](Value v, int stages) {\n-    if (!deps.insert({v, stages}).second) {\n-      deps[v] = std::min(deps[v], stages);\n-    }\n-  };\n-\n   if (auto arg = v.dyn_cast<BlockArgument>()) {\n     if (arg.getArgNumber() > 0) {\n       // Skip the first arg (loop induction variable)\n       // Otherwise the op idx is arg.getArgNumber()-1\n-      updateDeps(v, stages);\n+      deps.insert(v);\n       collectDeps(yieldOp->getOperand(arg.getArgNumber() - 1), stages - 1,\n                   deps);\n     }\n   } else { // value\n     // v might be in deps, but we still need to visit v.\n     // This is because v might depend on value in previous iterations\n-    updateDeps(v, stages);\n+    deps.insert(v);\n     for (Value op : v.getDefiningOp()->getOperands())\n       collectDeps(op, stages, deps);\n   }\n@@ -228,9 +222,9 @@ LogicalResult LoopPipeliner::initialize() {\n     return failure();\n \n   // load => values that it depends on\n-  DenseMap<Value, llvm::MapVector<Value, int>> loadDeps;\n+  DenseMap<Value, SetVector<Value>> loadDeps;\n   for (triton::LoadOp loadOp : validLoads) {\n-    llvm::MapVector<Value, int> deps;\n+    SetVector<Value> deps;\n     for (Value op : loadOp->getOperands())\n       collectDeps(op, numStages - 1, deps);\n     loadDeps[loadOp] = deps;\n@@ -243,7 +237,7 @@ LogicalResult LoopPipeliner::initialize() {\n   for (triton::LoadOp loadOp : validLoads) {\n     bool isCandidate = true;\n     for (triton::LoadOp other : validLoads) {\n-      if (loadDeps[loadOp].count(other) > 0) {\n+      if (loadDeps[loadOp].contains(other)) {\n         isCandidate = false;\n         break;\n       }\n@@ -300,13 +294,14 @@ LogicalResult LoopPipeliner::initialize() {\n     // Update depArgs & depOps\n     for (Value loadOp : loads) {\n       auto &deps = loadDeps[loadOp];\n-      for (auto &[dep, stage] : deps) {\n+      for (auto &dep : deps) {\n         if (auto arg = dep.dyn_cast<BlockArgument>()) {\n           depArgs.insert(arg);\n-          if (stage == numStages - 1)\n+          if (deps.front().isa<BlockArgument>()) {\n             immedidateDepArgs.insert(arg);\n-          else\n+          } else {\n             nonImmedidateDepArgs.insert(arg);\n+          }\n         } else\n           depOps.insert(dep.getDefiningOp());\n       }\n@@ -525,10 +520,10 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   size_t depArgsBeginIdx = newLoopArgs.size();\n   for (BlockArgument depArg : depArgs) {\n     depArgsIdx[depArg] = newLoopArgs.size();\n-    if (nonImmedidateDepArgs.contains(depArg)) {\n-      newLoopArgs.push_back(valueMapping[depArg][numStages - 1]);\n-    } else\n+    if (immedidateDepArgs.contains(depArg)) {\n       newLoopArgs.push_back(valueMapping[depArg][numStages - 2]);\n+    } else\n+      newLoopArgs.push_back(valueMapping[depArg][numStages - 1]);\n   }\n \n   size_t nextIVIdx = newLoopArgs.size();"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 129, "deletions": 0, "changes": 129, "file_content_changes": "@@ -223,3 +223,132 @@ func.func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index,\n   }\n   return %loop#1 : tensor<128x128xf32, #C>\n }\n+\n+\n+// CHECK: func.func @lut_bmm\n+// CHECK: triton_gpu.insert_slice_async\n+// CHECK: triton_gpu.insert_slice_async\n+// CHECK: triton_gpu.insert_slice_async\n+// CHECK: triton_gpu.insert_slice_async\n+// CHECK: triton_gpu.async_commit_group\n+// CHECK: %[[LUT_PTR:.*]] = tt.addptr\n+// CHECK: %arg27 = %[[LUT_PTR]]\n+// CHECK: %[[LUT_BUFFER_0:.*]] = tt.load %arg27, {{.*}}\n+// CHECK: %[[LUT_BUFFER_1:.*]] = arith.muli {{.*}}, %[[LUT_BUFFER_0]]\n+// CHECK: %[[LUT_BUFFER_2:.*]] = tt.splat %[[LUT_BUFFER_1]]\n+// CHECK: %[[NEXT_BUFFER_0:.*]] = tt.addptr {{.*}}, %[[LUT_BUFFER_2]]\n+// CHECK: %[[NEXT_BUFFER_1:.*]] = tt.addptr %arg26, {{.*}}\n+// CHECK: triton_gpu.insert_slice_async %[[NEXT_BUFFER_1]]\n+// CHECK: triton_gpu.insert_slice_async %[[NEXT_BUFFER_0]]\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}> \n+func.func @lut_bmm(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32 {tt.divisibility = 16 : i32}, %arg16: i32 {tt.divisibility = 16 : i32}, %arg17: i32 {tt.divisibility = 16 : i32}) { \n+  %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #mma> \n+  %c4_i32 = arith.constant 4 : i32\n+  %c1 = arith.constant 1 : index\n+  %c0 = arith.constant 0 : index \n+  %c0_i64 = arith.constant 0 : i64\n+  %c1_i32 = arith.constant 1 : i32\n+  %0 = tt.get_program_id {axis = 2 : i32} : i32\n+  %1 = tt.get_program_id {axis = 0 : i32} : i32\n+  %2 = tt.get_program_id {axis = 1 : i32} : i32 \n+  %3 = tt.get_num_programs {axis = 0 : i32} : i32 \n+  %4 = tt.get_num_programs {axis = 1 : i32} : i32 \n+  %5 = arith.muli %1, %4 : i32\n+  %6 = arith.addi %5, %2 : i32\n+  %7 = arith.muli %4, %c4_i32 : i32\n+  %8 = arith.divsi %6, %7 : i32\n+  %9 = arith.muli %8, %c4_i32 : i32 \n+  %10 = arith.subi %3, %9 : i32 \n+  %11 = arith.cmpi slt, %10, %c4_i32 : i32 \n+  %12 = arith.select %11, %10, %c4_i32 : i32 \n+  %13 = arith.remsi %6, %12 : i32 \n+  %14 = arith.addi %9, %13 : i32 \n+  %15 = arith.remsi %6, %7 : i32 \n+  %16 = arith.divsi %15, %12 : i32 \n+  %17 = arith.muli %arg5, %0 : i32 \n+  %18 = tt.addptr %arg4, %17 : !tt.ptr<i64>, i32\n+  %19 = tt.addptr %18, %14 : !tt.ptr<i64>, i32\n+  %20 = tt.load %19 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : i64 \n+  %21 = tt.addptr %19, %c1_i32 : !tt.ptr<i64>, i32\n+  %22 = tt.load %21 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : i64 \n+  %23 = arith.subi %22, %20 : i64 \n+  %24 = arith.cmpi eq, %23, %c0_i64 : i64 \n+  cf.cond_br %24, ^bb1, ^bb2\n+^bb1:  // pred: ^bb0\n+  return\n+^bb2:  // pred: ^bb0\n+  %25 = arith.muli %arg1, %0 : i32 \n+  %26 = tt.addptr %arg0, %25 : !tt.ptr<f16>, i32\n+  %27 = arith.extsi %arg2 : i32 to i64\n+  %28 = arith.muli %27, %20 : i64 \n+  %29 = tt.addptr %26, %28 : !tt.ptr<f16>, i64\n+  %30 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+  %31 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+  %32 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+  %33 = tt.expand_dims %30 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<16x1xi32, #blocked>\n+  %34 = tt.expand_dims %31 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<16x1xi32, #blocked1>\n+  %35 = tt.expand_dims %32 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<16x1xi32, #blocked>\n+  %36 = tt.splat %arg3 : (i32) -> tensor<16x1xi32, #blocked>\n+  %37 = arith.muli %36, %33 : tensor<16x1xi32, #blocked>\n+  %38 = tt.splat %29 : (!tt.ptr<f16>) -> tensor<16x1x!tt.ptr<f16>, #blocked>\n+  %39 = tt.addptr %38, %37 : tensor<16x1x!tt.ptr<f16>, #blocked>, tensor<16x1xi32, #blocked>\n+  %40 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+  %41 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>\n+  %42 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+  %43 = tt.expand_dims %40 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x16xi32, #blocked>\n+  %44 = tt.expand_dims %41 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>) -> tensor<1x16xi32, #blocked1>\n+  %45 = tt.expand_dims %42 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x16xi32, #blocked>\n+  %46 = tt.broadcast %39 : (tensor<16x1x!tt.ptr<f16>, #blocked>) -> tensor<16x16x!tt.ptr<f16>, #blocked>\n+  %47 = tt.broadcast %43 : (tensor<1x16xi32, #blocked>) -> tensor<16x16xi32, #blocked>\n+  %48 = tt.broadcast %45 : (tensor<1x16xi32, #blocked>) -> tensor<16x16xi32, #blocked>\n+  %49 = tt.addptr %46, %47 : tensor<16x16x!tt.ptr<f16>, #blocked>, tensor<16x16xi32, #blocked>\n+  %50 = arith.muli %arg9, %0 : i32 \n+  %51 = tt.addptr %arg8, %50 : !tt.ptr<f16>, i32\n+  %52 = arith.muli %arg11, %16 : i32 \n+  %53 = tt.addptr %51, %52 : !tt.ptr<f16>, i32\n+  %54 = tt.splat %53 : (!tt.ptr<f16>) -> tensor<16x1x!tt.ptr<f16>, #blocked1>\n+  %55 = tt.addptr %54, %34 : tensor<16x1x!tt.ptr<f16>, #blocked1>, tensor<16x1xi32, #blocked1>\n+  %56 = tt.splat %arg12 : (i32) -> tensor<1x16xi32, #blocked1>\n+  %57 = arith.muli %56, %44 : tensor<1x16xi32, #blocked1>\n+  %58 = tt.broadcast %55 : (tensor<16x1x!tt.ptr<f16>, #blocked1>) -> tensor<16x16x!tt.ptr<f16>, #blocked1>\n+  %59 = tt.broadcast %57 : (tensor<1x16xi32, #blocked1>) -> tensor<16x16xi32, #blocked1>\n+  %60 = tt.addptr %58, %59 : tensor<16x16x!tt.ptr<f16>, #blocked1>, tensor<16x16xi32, #blocked1>\n+  %61 = arith.muli %arg14, %0 : i32\n+  %62 = tt.addptr %arg13, %61 : !tt.ptr<f16>, i32\n+  %63 = arith.muli %arg15, %14 : i32\n+  %64 = tt.addptr %62, %63 : !tt.ptr<f16>, i32\n+  %65 = arith.muli %arg16, %16 : i32\n+  %66 = tt.addptr %64, %65 : !tt.ptr<f16>, i32\n+  %67 = tt.splat %arg17 : (i32) -> tensor<16x1xi32, #blocked>\n+  %68 = arith.muli %67, %35 : tensor<16x1xi32, #blocked>\n+  %69 = tt.splat %66 : (!tt.ptr<f16>) -> tensor<16x1x!tt.ptr<f16>, #blocked>\n+  %70 = tt.addptr %69, %68 : tensor<16x1x!tt.ptr<f16>, #blocked>, tensor<16x1xi32, #blocked>\n+  %71 = tt.broadcast %70 : (tensor<16x1x!tt.ptr<f16>, #blocked>) -> tensor<16x16x!tt.ptr<f16>, #blocked>\n+  %72 = tt.addptr %71, %48 : tensor<16x16x!tt.ptr<f16>, #blocked>, tensor<16x16xi32, #blocked>\n+  %73 = arith.muli %arg7, %0 : i32\n+  %74 = tt.addptr %arg6, %73 : !tt.ptr<i64>, i32\n+  %75 = tt.addptr %74, %20 : !tt.ptr<i64>, i64\n+  %76 = arith.index_cast %23 : i64 to index\n+  %77 = arith.extsi %arg10 : i32 to i64\n+  %78 = tt.splat %arg2 : (i32) -> tensor<16x16xi32, #blocked>\n+  %79:3 = scf.for %arg18 = %c0 to %76 step %c1 iter_args(%arg19 = %cst, %arg20 = %49, %arg21 = %75) -> (tensor<16x16xf32, #mma>, tensor<16x16x!tt.ptr<f16>, #blocked>, !tt.ptr<i64>) {\n+    %82 = tt.load %arg20 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blocked>\n+    %83 = tt.load %arg21 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : i64\n+    %84 = arith.muli %77, %83 : i64\n+    %85 = tt.splat %84 : (i64) -> tensor<16x16xi64, #blocked1>\n+    %86 = tt.addptr %60, %85 : tensor<16x16x!tt.ptr<f16>, #blocked1>, tensor<16x16xi64, #blocked1>\n+    %87 = tt.load %86 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blocked1>\n+    %88 = triton_gpu.convert_layout %82 : (tensor<16x16xf16, #blocked>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma}>>\n+    %89 = triton_gpu.convert_layout %87 : (tensor<16x16xf16, #blocked1>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma}>>\n+    %90 = tt.dot %88, %89, %arg19 {allowTF32 = true} : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma}>> * tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma}>> -> tensor<16x16xf32, #mma>\n+    %91 = tt.addptr %arg20, %78 : tensor<16x16x!tt.ptr<f16>, #blocked>, tensor<16x16xi32, #blocked>\n+    %92 = tt.addptr %arg21, %c1_i32 : !tt.ptr<i64>, i32\n+    scf.yield %90, %91, %92 : tensor<16x16xf32, #mma>, tensor<16x16x!tt.ptr<f16>, #blocked>, !tt.ptr<i64>\n+  }\n+  %80 = arith.truncf %79#0 : tensor<16x16xf32, #mma> to tensor<16x16xf16, #mma>\n+  %81 = triton_gpu.convert_layout %80 : (tensor<16x16xf16, #mma>) -> tensor<16x16xf16, #blocked>\n+  tt.store %72, %81 {cache = 1 : i32, evict = 1 : i32} : tensor<16x16xf16, #blocked>\n+  return\n+}"}]