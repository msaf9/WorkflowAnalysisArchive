[{"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 47, "deletions": 18, "changes": 65, "file_content_changes": "@@ -27,17 +27,8 @@ namespace mlir {\n //===----------------------------------------------------------------------===//\n namespace triton {\n \n-SmallVector<unsigned>\n-getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n-                             unsigned &outVec) {\n-  auto srcTy = op.src().getType().cast<RankedTensorType>();\n-  auto dstTy = op.result().getType().cast<RankedTensorType>();\n-  Attribute srcLayout = srcTy.getEncoding();\n-  Attribute dstLayout = dstTy.getEncoding();\n-  assert(srcLayout && dstLayout &&\n-         \"Unexpect layout in getScratchConfigForCvtLayout()\");\n-  unsigned rank = dstTy.getRank();\n-  SmallVector<unsigned> paddedRepShape(rank);\n+static std::pair<SmallVector<unsigned>, SmallVector<unsigned>>\n+getCvtOrder(const Attribute &srcLayout, const Attribute &dstLayout) {\n   auto srcBlockedLayout = srcLayout.dyn_cast<BlockedEncodingAttr>();\n   auto srcMmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>();\n   auto srcDotLayout = srcLayout.dyn_cast<DotOperandEncodingAttr>();\n@@ -46,15 +37,55 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   auto dstDotLayout = dstLayout.dyn_cast<DotOperandEncodingAttr>();\n   assert(!(srcMmaLayout && dstMmaLayout) &&\n          \"Unexpected mma -> mma layout conversion\");\n-  // MMA or Dot layout does not have the \u201corder\" attribute.\n-  // so the layout in smem during the conversion of mma(dot)->blocked (or\n-  // blocked->mma(dot)) is up to the blocked layout.\n   auto inOrd = (srcMmaLayout || srcDotLayout) ? getOrder(dstLayout)\n                                               : getOrder(srcLayout);\n   auto outOrd = (dstMmaLayout || dstDotLayout) ? getOrder(srcLayout)\n                                                : getOrder(dstLayout);\n-  unsigned srcContigPerThread = getSizePerThread(srcLayout)[inOrd[0]];\n-  unsigned dstContigPerThread = getSizePerThread(dstLayout)[outOrd[0]];\n+\n+  return {inOrd, outOrd};\n+}\n+\n+static std::pair<unsigned, unsigned> getCvtContigPerThread(\n+    const Attribute &srcLayout, const ArrayRef<unsigned> inOrd,\n+    const Attribute &dstLayout, const ArrayRef<unsigned> outOrd) {\n+  auto getContigFn = [](const Attribute &layout, const Attribute &otherLayout,\n+                        const ArrayRef<unsigned> ord) {\n+    auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>();\n+    if (dotLayout) {\n+      auto parentLayout = dotLayout.getParent();\n+      auto blockedLayout = otherLayout.dyn_cast<BlockedEncodingAttr>();\n+      assert(parentLayout && \"DotOperandEncodingAttr must have a parent\");\n+      assert(blockedLayout && \"DotOperandEncodingAttr must be converted to a \"\n+                              \"BlockedEncodingAttr\");\n+      auto idx = dotLayout.getOpIdx();\n+      if (idx == 0)\n+        return static_cast<unsigned>(\n+            std::ceil(16.0 / blockedLayout.getThreadsPerWarp()[1]));\n+      else\n+        return getSizePerThread(blockedLayout)[1];\n+    } else {\n+      return getSizePerThread(layout)[ord[0]];\n+    }\n+  };\n+  unsigned srcContigPerThread = getContigFn(srcLayout, dstLayout, inOrd);\n+  unsigned dstContigPerThread = getContigFn(dstLayout, srcLayout, outOrd);\n+  return {srcContigPerThread, dstContigPerThread};\n+}\n+\n+SmallVector<unsigned>\n+getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n+                             unsigned &outVec) {\n+  auto srcTy = op.src().getType().cast<RankedTensorType>();\n+  auto dstTy = op.result().getType().cast<RankedTensorType>();\n+  Attribute srcLayout = srcTy.getEncoding();\n+  Attribute dstLayout = dstTy.getEncoding();\n+  assert(srcLayout && dstLayout &&\n+         \"Unexpect layout in getScratchConfigForCvtLayout()\");\n+  unsigned rank = dstTy.getRank();\n+  SmallVector<unsigned> paddedRepShape(rank);\n+  auto [inOrd, outOrd] = getCvtOrder(srcLayout, dstLayout);\n+  auto [srcContigPerThread, dstContigPerThread] =\n+      getCvtContigPerThread(srcLayout, inOrd, dstLayout, outOrd);\n   // TODO: Fix the legacy issue that ourOrd[0] == 0 always means\n   //       that we cannot do vectorization.\n   inVec = outOrd[0] == 0 ? 1 : inOrd[0] == 0 ? 1 : srcContigPerThread;\n@@ -151,8 +182,6 @@ class AllocationAnalysis {\n \n   /// Initializes temporary shared memory for a given operation.\n   void getScratchValueSize(Operation *op) {\n-    // TODO(Keren): Add atomic ops\n-    // TODO(Keren): Add convert ops\n     if (auto reduceOp = dyn_cast<triton::ReduceOp>(op)) {\n       // TODO(Keren): Reduce with index is not supported yet.\n       auto value = op->getOperand(0);"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -57,6 +57,8 @@ unsigned getElemsPerThread(Type type) {\n     return mmaLayout.getElemsPerThread(shape);\n   } else if (auto sharedLayout = layout.dyn_cast<SharedEncodingAttr>()) {\n     return sharedLayout.getElemsPerThread(shape);\n+  } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n+    return dotLayout.getElemsPerThread(shape);\n   } else {\n     assert(0 && \"getElemsPerThread not implemented\");\n     return 0;\n@@ -74,6 +76,7 @@ SmallVector<unsigned> getSizePerThread(Attribute layout) {\n            \"mmaLayout version = 1 is not implemented yet\");\n     return SmallVector<unsigned>{2, 2};\n   } else {\n+    llvm::errs() << \"layout: \" << layout << \"\\n\";\n     assert(0 && \"getSizePerThread not implemented\");\n     return {};\n   }\n@@ -318,6 +321,12 @@ unsigned SharedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n   return 0;\n }\n \n+unsigned\n+DotOperandEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n+  assert(0 && \"DotOPerandEncodingAttr::getElemsPerThread not implemented\");\n+  return 0;\n+}\n+\n //===----------------------------------------------------------------------===//\n // Blocked Encoding\n //===----------------------------------------------------------------------===//"}]