[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 30, "deletions": 1, "changes": 31, "file_content_changes": "@@ -1546,7 +1546,7 @@ def _kernel(dst):\n                          [('int32', 'libdevice.ffs', ''),\n                           ('float32', 'libdevice.pow', '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'),\n                           ('float64', 'libdevice.norm4d', '')])\n-def test_libdevice(dtype_str, expr, lib_path):\n+def test_libdevice_tensor(dtype_str, expr, lib_path):\n \n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n@@ -1582,3 +1582,32 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n         np.testing.assert_equal(y_ref, to_numpy(y_tri))\n     else:\n         np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n+\n+\n+@pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n+                         [('float32', 'libdevice.pow', '')])\n+def test_libdevice_scalar(dtype_str, expr, lib_path):\n+\n+    @triton.jit\n+    def kernel(X, Y, BLOCK: tl.constexpr):\n+        x = X\n+        y = GENERATE_TEST_HERE\n+        tl.store(Y + tl.arange(0, BLOCK), y)\n+\n+    shape = (128, )\n+    rs = RandomState(17)\n+    # limit the range of integers so that the sum does not overflow\n+    x = numpy_random((1,), dtype_str=dtype_str, rs=rs)\n+    y_ref = np.zeros(shape, dtype=x.dtype)\n+\n+    # numpy does not allow negative factors in power, so we use abs()\n+    x = np.abs(x)\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.pow(x, x)'})\n+    y_ref[:] = np.power(x, x)\n+\n+    # triton result\n+    x_tri = to_triton(x)[0].item()\n+    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n+    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n+    # compare\n+    np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)"}]