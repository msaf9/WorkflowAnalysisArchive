[{"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -371,6 +371,8 @@ SmallVector<unsigned, 3>\n mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n                        ArrayRef<int64_t> shape);\n \n+Value getParentValueWithSameEncoding(Attribute layout, Value value);\n+\n } // namespace mlir\n \n #endif // TRITON_ANALYSIS_UTILITY_H"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -36,6 +36,9 @@ SmallVector<unsigned> getThreadsPerWarp(Attribute layout);\n // elements. If you want non-replicated warps, use getWarpsPerCTAWithUniqueData.\n SmallVector<unsigned> getWarpsPerCTA(Attribute layout);\n \n+SmallVector<unsigned> getSizePerThread(BlockedEncodingAttr layout);\n+SmallVector<unsigned> getSizePerThread(Value value);\n+\n SmallVector<unsigned> getSizePerThread(Attribute layout,\n                                        ArrayRef<int64_t> shapePerCTA);\n \n@@ -90,6 +93,11 @@ SmallVector<unsigned> getCTAOrder(Attribute layout);\n  * (3) In the implementation of emitIndices, ShapePerCTATile will\n  *     be replicated or wraped to fit ShapePerCTA.\n  */\n+SmallVector<unsigned> getShapePerCTATile(BlockedEncodingAttr layout);\n+SmallVector<unsigned> getShapePerCTATile(MmaEncodingAttr layout,\n+                                         RankedTensorType inputType);\n+SmallVector<unsigned> getShapePerCTATile(Value value);\n+\n SmallVector<unsigned> getShapePerCTATile(Attribute layout,\n                                          ArrayRef<int64_t> shapePerCTA);\n "}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -87,8 +87,8 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n \n   auto srcShapePerCTA = getShapePerCTA(srcTy);\n   auto dstShapePerCTA = getShapePerCTA(dstTy);\n-  auto srcShapePerCTATile = getShapePerCTATile(srcLayout, srcShapePerCTA);\n-  auto dstShapePerCTATile = getShapePerCTATile(dstLayout, dstShapePerCTA);\n+  auto srcShapePerCTATile = getShapePerCTATile(op.getSrc());\n+  auto dstShapePerCTATile = getShapePerCTATile(op.getResult());\n \n   unsigned rank = dstTy.getRank();\n   SmallVector<unsigned> paddedRepShape(rank);"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -750,5 +750,8 @@ mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n                        ArrayRef<int64_t> shape) {\n   return {0};\n }\n+Value getParentValueWithSameEncoding(Attribute layout, Value value) {\n+  return {};\n+}\n \n } // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 15, "deletions": 18, "changes": 33, "file_content_changes": "@@ -238,7 +238,7 @@ struct ConvertLayoutOpConversion\n \n   // shared memory rd/st for blocked or mma layout with data padding\n   void processReplica(Location loc, ConversionPatternRewriter &rewriter,\n-                      bool stNotRd, RankedTensorType type,\n+                      bool stNotRd, Value value, RankedTensorType type,\n                       ArrayRef<unsigned> numCTAsEachRep,\n                       ArrayRef<unsigned> multiDimRepId, unsigned vec,\n                       ArrayRef<unsigned> paddedRepShape,\n@@ -251,7 +251,7 @@ struct ConvertLayoutOpConversion\n     auto sizePerThread = getSizePerThread(layout, shapePerCTA);\n     auto accumSizePerThread = product<unsigned>(sizePerThread);\n     SmallVector<unsigned> numCTATiles(rank);\n-    auto shapePerCTATile = getShapePerCTATile(layout, shapePerCTA);\n+    auto shapePerCTATile = getShapePerCTATile(value);\n     auto order = getOrder(layout);\n     for (unsigned d = 0; d < rank; ++d) {\n       numCTATiles[d] = ceil<unsigned>(shapePerCTA[d], shapePerCTATile[d]);\n@@ -323,7 +323,7 @@ struct ConvertLayoutOpConversion\n   // structure, add a new simple but clear implementation for it to avoid\n   // modifying the logic of the existing one.\n   void processReplicaForMMAV1(Location loc, ConversionPatternRewriter &rewriter,\n-                              bool stNotRd, RankedTensorType type,\n+                              bool stNotRd, Value value, RankedTensorType type,\n                               ArrayRef<unsigned> multiDimRepId, unsigned vec,\n                               ArrayRef<unsigned> paddedRepShape,\n                               ArrayRef<unsigned> outOrd,\n@@ -344,8 +344,7 @@ struct ConvertLayoutOpConversion\n     SmallVector<unsigned> numCTAs(rank, 1);\n     SmallVector<unsigned> numCTAsEachRep(rank, 1);\n     SmallVector<int64_t> shapePerCTA = getShapePerCTA(layout, shape);\n-    SmallVector<unsigned> shapePerCTATile =\n-        getShapePerCTATile(layout, shapePerCTA);\n+    SmallVector<unsigned> shapePerCTATile = getShapePerCTATile(value);\n     auto elemTy = type.getElementType();\n \n     int ctaId = 0;\n@@ -450,8 +449,7 @@ struct ConvertLayoutOpConversion\n     {\n       auto inVals = getTypeConverter()->unpackLLElements(loc, adaptor.getSrc(),\n                                                          rewriter, srcTy);\n-      auto inIndices =\n-          emitIndices(loc, rewriter, srcLayout, srcTy, /*withCTAOffset*/ false);\n+      auto inIndices = emitIndices(loc, rewriter, src, /*withCTAOffset*/ false);\n \n       assert(inIndices.size() == inVals.size() &&\n              \"Unexpected number of indices emitted\");\n@@ -474,8 +472,7 @@ struct ConvertLayoutOpConversion\n         srcShapePerCTACache.push_back(i32_val(srcShapePerCTA[i]));\n \n       SmallVector<Value> outVals;\n-      auto outIndices =\n-          emitIndices(loc, rewriter, dstLayout, dstTy, /*withCTAOffset*/ true);\n+      auto outIndices = emitIndices(loc, rewriter, dst, /*withCTAOffset*/ true);\n \n       for (unsigned i = 0; i < outIndices.size(); ++i) {\n         auto coord = outIndices[i];\n@@ -536,8 +533,8 @@ struct ConvertLayoutOpConversion\n     SmallVector<unsigned> inNumCTAs(rank);\n     SmallVector<unsigned> outNumCTAs(rank);\n     // [benzh] here logic need more check: src and dst using same shape???\n-    auto srcShapePerCTATile = getShapePerCTATile(srcLayout, srcTy.getShape());\n-    auto dstShapePerCTATile = getShapePerCTATile(dstLayout, shape);\n+    auto srcShapePerCTATile = getShapePerCTATile(src);\n+    auto dstShapePerCTATile = getShapePerCTATile(dst);\n     auto shapePerCTA = getShapePerCTA(srcLayout, shape);\n \n     // For Volta, all the coords for a CTA are calculated.\n@@ -613,11 +610,11 @@ struct ConvertLayoutOpConversion\n           srcLayout.isa<SliceEncodingAttr>() ||\n           srcLayout.isa<MmaEncodingAttr>()) {\n         if (isSrcMmaV1)\n-          processReplicaForMMAV1(loc, rewriter, /*stNotRd*/ true, srcTy,\n+          processReplicaForMMAV1(loc, rewriter, /*stNotRd*/ true, src, srcTy,\n                                  multiDimRepId, inVec, paddedRepShape, outOrd,\n                                  vals, smemBase, shape);\n         else\n-          processReplica(loc, rewriter, /*stNotRd*/ true, srcTy,\n+          processReplica(loc, rewriter, /*stNotRd*/ true, src, srcTy,\n                          inNumCTAsEachRep, multiDimRepId, inVec, paddedRepShape,\n                          outOrd, vals, smemBase);\n       } else {\n@@ -646,11 +643,11 @@ struct ConvertLayoutOpConversion\n           dstLayout.isa<SliceEncodingAttr>() ||\n           dstLayout.isa<MmaEncodingAttr>()) {\n         if (isDstMmaV1)\n-          processReplicaForMMAV1(loc, rewriter, /*stNotRd*/ false, dstTy,\n+          processReplicaForMMAV1(loc, rewriter, /*stNotRd*/ false, dst, dstTy,\n                                  multiDimRepId, outVec, paddedRepShape, outOrd,\n                                  outVals, smemBase, shape, /*isDestMma=*/true);\n         else\n-          processReplica(loc, rewriter, /*stNotRd*/ false, dstTy,\n+          processReplica(loc, rewriter, /*stNotRd*/ false, dst, dstTy,\n                          outNumCTAsEachRep, multiDimRepId, outVec,\n                          paddedRepShape, outOrd, outVals, smemBase);\n       } else {\n@@ -688,7 +685,7 @@ struct ConvertLayoutOpConversion\n \n     auto srcStrides =\n         getStridesFromShapeAndOrder(srcShape, inOrd, loc, rewriter);\n-    auto dstIndices = emitIndices(loc, rewriter, dstLayout, dstTy);\n+    auto dstIndices = emitIndices(loc, rewriter, dst);\n \n     SmallVector<Value> outVals = loadSharedToDistributed(\n         dst, dstIndices, src, smemObj, elemTy, loc, rewriter);\n@@ -760,7 +757,7 @@ struct ConvertLayoutOpConversion\n       auto ptrI8SharedTy = LLVM::LLVMPointerType::get(\n           typeConverter->convertType(rewriter.getI8Type()), 3);\n \n-      uint32_t rowsPerRep = getShapePerCTATile(mmaLayout, srcShapePerCTA)[0];\n+      uint32_t rowsPerRep = getShapePerCTATile(src)[0];\n \n       Value threadId = getThreadId(rewriter, loc);\n       Value warpId = udiv(threadId, i32_val(32));\n@@ -814,7 +811,7 @@ struct ConvertLayoutOpConversion\n     } else {\n       auto dstStrides =\n           getStridesFromShapeAndOrder(dstShapePerCTA, outOrd, loc, rewriter);\n-      auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTy, false);\n+      auto srcIndices = emitIndices(loc, rewriter, src, false);\n       storeDistributedToShared(src, adaptor.getSrc(), dstStrides, srcIndices,\n                                dst, smemBase, elemTy, loc, rewriter);\n     }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -35,7 +35,7 @@ getThreadIds(Value threadId, ArrayRef<unsigned int> shapePerCTATile,\n // Get shapePerCTATile for M or N axis.\n int getShapePerCTATileForMN(BlockedEncodingAttr layout, bool isM) {\n   auto order = layout.getOrder();\n-  auto shapePerCTATile = getShapePerCTATile(layout, {});\n+  auto shapePerCTATile = getShapePerCTATile(layout);\n \n   int mShapePerCTATile =\n       order[0] == 1 ? shapePerCTATile[order[1]] : shapePerCTATile[order[0]];\n@@ -110,7 +110,7 @@ Value loadAFMA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n   int K = aShapePerCTA[1];\n   int M = aShapePerCTA[0];\n \n-  auto shapePerCTATile = getShapePerCTATile(dLayout, {});\n+  auto shapePerCTATile = getShapePerCTATile(dLayout);\n   auto sizePerThread = getSizePerThread(dLayout, {});\n \n   Value _0 = i32_val(0);\n@@ -174,7 +174,7 @@ Value loadBFMA(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n   int K = bShapePerCTA[0];\n   int N = bShapePerCTA[1];\n \n-  auto shapePerCTATile = getShapePerCTATile(dLayout, {});\n+  auto shapePerCTATile = getShapePerCTATile(dLayout);\n   auto sizePerThread = getSizePerThread(dLayout, {});\n \n   Value _0 = i32_val(0);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/FMA.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -54,7 +54,7 @@ LogicalResult convertFMADot(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n   Value llB = adaptor.getB();\n \n   auto sizePerThread = getSizePerThread(dLayout, {});\n-  auto shapePerCTATile = getShapePerCTATile(dLayout, {});\n+  auto shapePerCTATile = mlir::triton::gpu::getShapePerCTATile(D);\n \n   int K = aShapePerCTA[1];\n   int M = aShapePerCTA[0];"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/WGMMA.cpp", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -286,7 +286,7 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n   int N = instrShape[1];\n   int K = instrShape[2];\n \n-  auto shapePerCTATile = getShapePerCTATile(mmaEncoding, dShapePerCTA);\n+  auto shapePerCTATile = getShapePerCTATile(d);\n   int numRepM = ceil<unsigned>(dShapePerCTA[0], shapePerCTATile[0]);\n   int numRepN = ceil<unsigned>(dShapePerCTA[1], shapePerCTATile[1]);\n   int numRepK = ceil<unsigned>(aTensorTy.getShape()[1], instrShape[2]);\n@@ -357,14 +357,15 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n }\n \n // Loading $c to registers, returns a Value.\n-Value loadC(Value tensor, Value llTensor) {\n+Value loadC(Value dTensor, Value tensor, Value llTensor) {\n   auto tensorTy = tensor.getType().cast<RankedTensorType>();\n   auto mmaEncoding = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>();\n   assert(mmaEncoding && \"Currently, we only support $c with a mma layout.\");\n   auto shapePerCTA = getShapePerCTA(tensorTy);\n   auto instrShape = mmaVersionToInstrShape(mmaEncoding, shapePerCTA);\n   auto wpt = mmaEncoding.getWarpsPerCTA();\n-  auto shapePerCTATile = getShapePerCTATile(mmaEncoding, shapePerCTA);\n+  // benzh@need from ACC\n+  auto shapePerCTATile = getShapePerCTATile(dTensor);\n \n   int numRepM = ceil<unsigned>(shapePerCTA[0], shapePerCTATile[0]);\n   int numRepN = ceil<unsigned>(shapePerCTA[1], shapePerCTATile[1]);\n@@ -395,7 +396,7 @@ LogicalResult convertWGMMA(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n   Value llA, llB, llC;\n   llA = adaptor.getA();\n   llB = adaptor.getB();\n-  llC = loadC(C, adaptor.getC());\n+  llC = loadC(op.getD(), C, adaptor.getC());\n \n   auto smemObjA = getSharedMemoryObjectFromStruct(loc, llA, rewriter);\n   auto smemObjB = getSharedMemoryObjectFromStruct(loc, llB, rewriter);\n@@ -423,7 +424,7 @@ LogicalResult convertAsyncWGMMA(triton::nvidia_gpu::DotAsyncOp op,\n   Value llA, llB, llC;\n   llA = adaptor.getA();\n   llB = adaptor.getB();\n-  llC = loadC(C, adaptor.getC());\n+  llC = loadC(op.getD(), C, adaptor.getC());\n \n   auto smemObjA = getSharedMemoryObjectFromStruct(loc, llA, rewriter);\n   auto smemObjB = getSharedMemoryObjectFromStruct(loc, llB, rewriter);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -306,7 +306,7 @@ struct StoreOpConversion\n       vec = std::min(vec, maskAlign);\n     }\n \n-    Value mask = getMask(valueTy, rewriter, loc);\n+    Value mask = getMask(value, rewriter, loc);\n     const size_t dtsize =\n         std::max<int>(1, valueElemTy.getIntOrFloatBitWidth() / 8);\n     const size_t valueElemNBits = dtsize * 8;\n@@ -669,7 +669,7 @@ struct AtomicCASOpConversion\n         TensorTy ? getTypeConverter()->convertType(TensorTy.getElementType())\n                  : valueTy;\n     auto valueElemNBits = valueElemTy.getIntOrFloatBitWidth();\n-    Value mask = getMask(valueTy, rewriter, loc);\n+    Value mask = getMask(op.getResult(), rewriter, loc);\n \n     Value atomPtr = getSharedMemoryBase(loc, rewriter, op.getOperation());\n     atomPtr = bitcast(atomPtr, ptr_ty(valueElemTy, 3));\n@@ -762,7 +762,7 @@ struct AtomicRMWOpConversion\n       // mask\n       numElems = tensorTy.getNumElements();\n     }\n-    Value mask = getMask(valueTy, rewriter, loc);\n+    Value mask = getMask(op.getResult(), rewriter, loc);\n \n     auto vecTy = vec_ty(valueElemTy, vec);\n     SmallVector<Value> resultVals(elemsPerThread);\n@@ -930,7 +930,7 @@ struct InsertSliceOpConversion\n     auto smemBase = gep(elemPtrTy, smemObj.base, offset);\n \n     auto llSrc = adaptor.getSource();\n-    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTy);\n+    auto srcIndices = emitIndices(loc, rewriter, src);\n     storeDistributedToShared(src, llSrc, srcStrides, srcIndices, dst, smemBase,\n                              elemTy, loc, rewriter);\n     // Barrier is not necessary.\n@@ -1044,9 +1044,9 @@ struct InsertSliceAsyncOpConversion\n     auto sizePerThread = srcBlockedLayout.getSizePerThread();\n     auto threadsPerCTA = getThreadsPerCTA(srcBlockedLayout);\n     auto inOrder = srcBlockedLayout.getOrder();\n-    DenseMap<unsigned, Value> sharedPtrs =\n-        getSwizzledSharedPtrs(loc, inVec, srcTy, resSharedLayout, resElemTy,\n-                              smemObj, rewriter, offsetVals, srcStrides);\n+    DenseMap<unsigned, Value> sharedPtrs = getSwizzledSharedPtrs(\n+        loc, inVec, src, srcTy, resSharedLayout, resElemTy, smemObj, rewriter,\n+        offsetVals, srcStrides);\n \n     // If perPhase * maxPhase > threadsPerCTA, we will have elements\n     // that share the same tile indices. The index calculation will\n@@ -1058,7 +1058,7 @@ struct InsertSliceAsyncOpConversion\n     // single vector read into multiple ones\n     auto numVecCols = std::max<unsigned>(inVec / outVec, 1);\n \n-    auto srcIndices = emitIndices(loc, rewriter, srcBlockedLayout, srcTy);\n+    auto srcIndices = emitIndices(loc, rewriter, src);\n \n     for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n       // 16 * 8 = 128bits"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 5, "deletions": 10, "changes": 15, "file_content_changes": "@@ -305,11 +305,8 @@ struct ReduceOpConversion\n       if (auto resultTy =\n               op.getResult()[i].getType().dyn_cast<RankedTensorType>()) {\n         // nd-tensor where n >= 1\n-\n-        auto resultLayout = resultTy.getEncoding();\n-\n         unsigned resultElems = getTotalElemsPerThread(resultTy);\n-        auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultTy);\n+        auto resultIndices = emitIndices(loc, rewriter, op.getResult()[i]);\n         assert(resultIndices.size() == resultElems);\n \n         SmallVector<Value> resultVals(resultElems);\n@@ -391,11 +388,10 @@ struct ReduceOpConversion\n     RankedTensorType operandType = op.getInputTypes()[0];\n     // Assumes offsets don't actually depend on type\n     SmallVector<SmallVector<unsigned>> offset =\n-        emitOffsetForLayout(helper.getSrcLayout(), operandType);\n+        emitOffsetForLayout(helper.getSrcValue());\n     unsigned srcElems = getTotalElemsPerThread(operandType);\n     auto *combineOp = &op.getCombineOp();\n-    auto srcIndices =\n-        emitIndices(op.getLoc(), rewriter, helper.getSrcLayout(), operandType);\n+    auto srcIndices = emitIndices(op.getLoc(), rewriter, helper.getSrcValue());\n     // reduce within threads\n     for (unsigned i = 0; i < srcElems; ++i) {\n       SmallVector<unsigned> key = offset[i];\n@@ -473,7 +469,7 @@ struct ReduceOpConversion\n         auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n         unsigned resultElems = getTotalElemsPerThread(resultTy);\n         SmallVector<SmallVector<unsigned>> resultOffset =\n-            emitOffsetForLayout(resultLayout, resultTy);\n+            emitOffsetForLayout(op.getResult()[i]);\n         SmallVector<Value> resultVals;\n         for (int j = 0; j < resultElems; j++) {\n           auto key = resultOffset[j];\n@@ -615,9 +611,8 @@ struct ReduceOpConversion\n       if (auto resultTy =\n               op.getResult()[i].getType().dyn_cast<RankedTensorType>()) {\n         // nd-tensor where n >= 1\n-        auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n         unsigned resultElems = getTotalElemsPerThread(resultTy);\n-        auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultTy);\n+        auto resultIndices = emitIndices(loc, rewriter, op.getResult()[i]);\n         assert(resultIndices.size() == resultElems);\n \n         SmallVector<Value> resultVals(resultElems);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -65,8 +65,8 @@ struct BroadcastOpConversion\n \n     assert(rank == resultTy.getRank());\n     auto order = triton::gpu::getOrder(srcLayout);\n-    auto srcOffsets = emitOffsetForLayout(srcLayout, srcTy);\n-    auto resultOffsets = emitOffsetForLayout(resultLayout, resultTy);\n+    auto srcOffsets = emitOffsetForLayout(op.getSrc());\n+    auto resultOffsets = emitOffsetForLayout(result);\n     SmallVector<Value> srcVals =\n         getTypeConverter()->unpackLLElements(loc, src, rewriter, srcTy);\n \n@@ -365,7 +365,7 @@ struct MakeRangeOpConversion\n     auto elemTy = rankedTy.getElementType();\n     assert(elemTy.isInteger(32));\n     Value start = createIndexAttrConstant(rewriter, loc, elemTy, op.getStart());\n-    auto idxs = emitIndices(loc, rewriter, layout, rankedTy);\n+    auto idxs = emitIndices(loc, rewriter, op.getResult());\n     unsigned elems = idxs.size();\n     SmallVector<Value> retVals(elems);\n     // TODO: slice layout has more elements than expected."}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 25, "deletions": 19, "changes": 44, "file_content_changes": "@@ -290,13 +290,12 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return base;\n   }\n \n-  DenseMap<unsigned, Value>\n-  getSwizzledSharedPtrs(Location loc, unsigned inVec, RankedTensorType srcTy,\n-                        triton::gpu::SharedEncodingAttr resSharedLayout,\n-                        Type resElemTy, SharedMemoryObject smemObj,\n-                        ConversionPatternRewriter &rewriter,\n-                        SmallVectorImpl<Value> &offsetVals,\n-                        SmallVectorImpl<Value> &srcStrides) const {\n+  DenseMap<unsigned, Value> getSwizzledSharedPtrs(\n+      Location loc, unsigned inVec, Value src, RankedTensorType srcTy,\n+      triton::gpu::SharedEncodingAttr resSharedLayout, Type resElemTy,\n+      SharedMemoryObject smemObj, ConversionPatternRewriter &rewriter,\n+      SmallVectorImpl<Value> &offsetVals,\n+      SmallVectorImpl<Value> &srcStrides) const {\n     // This utililty computes the pointers for accessing the provided swizzled\n     // shared memory layout `resSharedLayout`. More specifically, it computes,\n     // for all indices (row, col) of `srcEncoding` such that idx % inVec = 0,\n@@ -339,7 +338,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto inOrder = triton::gpu::getOrder(srcEncoding);\n     auto outOrder = triton::gpu::getOrder(resSharedLayout);\n     // Tensor indices held by the current thread, as LLVM values\n-    auto srcIndices = emitIndices(loc, rewriter, srcEncoding, srcTy, false);\n+    auto srcIndices = emitIndices(loc, rewriter, src, false);\n     // Swizzling with leading offsets (e.g. Hopper GMMA)\n     unsigned swizzlingByteWidth = 0;\n     if (resSharedLayout.getHasLeadingOffset()) {\n@@ -461,7 +460,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     assert(outElems == dstIndices.size());\n \n     DenseMap<unsigned, Value> sharedPtrs = getSwizzledSharedPtrs(\n-        loc, outVec, dstTy, srcSharedLayout, srcElemTy, smemObj, rewriter,\n+        loc, outVec, dst, dstTy, srcSharedLayout, srcElemTy, smemObj, rewriter,\n         smemObj.offsets, smemObj.strides);\n     assert(outElems % minVec == 0 && \"Unexpected number of elements\");\n     unsigned numVecs = outElems / minVec;\n@@ -517,9 +516,9 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     SmallVector<Value> offsetVals = {i32_val(0), i32_val(0)};\n     SharedMemoryObject smemObj(smemBase, srcStrides, offsetVals);\n \n-    DenseMap<unsigned, Value> sharedPtrs =\n-        getSwizzledSharedPtrs(loc, inVec, srcTy, dstSharedLayout, dstElemTy,\n-                              smemObj, rewriter, offsetVals, srcStrides);\n+    DenseMap<unsigned, Value> sharedPtrs = getSwizzledSharedPtrs(\n+        loc, inVec, src, srcTy, dstSharedLayout, dstElemTy, smemObj, rewriter,\n+        offsetVals, srcStrides);\n \n     for (unsigned i = 0; i < numElems; ++i) {\n       if (i % minVec == 0)\n@@ -536,8 +535,9 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   // -----------------------------------------------------------------------\n   // Utilities\n   // -----------------------------------------------------------------------\n-  Value getMask(Type valueTy, ConversionPatternRewriter &rewriter,\n+  Value getMask(Value value, ConversionPatternRewriter &rewriter,\n                 Location loc) const {\n+    Type valueTy = value.getType();\n     auto tensorTy = valueTy.dyn_cast<RankedTensorType>();\n     Value mask = int_val(1, 1);\n     auto tid = tid_val();\n@@ -550,8 +550,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       auto threadsPerWarp = triton::gpu::getThreadsPerWarp(layout);\n       auto warpsPerCTA = triton::gpu::getWarpsPerCTA(layout);\n       auto order = triton::gpu::getOrder(layout);\n-      auto shapePerCTATile =\n-          triton::gpu::getShapePerCTATile(layout, shapePerCTA);\n+      auto shapePerCTATile = triton::gpu::getShapePerCTATile(value);\n       Value warpSize = i32_val(32);\n       Value laneId = urem(tid, warpSize);\n       Value warpId = udiv(tid, warpSize);\n@@ -695,6 +694,10 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     }\n   }\n \n+  SmallVector<SmallVector<unsigned>> emitOffsetForLayout(Value value) const {\n+    return {};\n+  }\n+\n   SmallVector<SmallVector<unsigned>>\n   emitOffsetForLayout(Attribute layout, RankedTensorType type) const {\n     if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n@@ -715,9 +718,12 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   // -----------------------------------------------------------------------\n   // Emit indices\n   // -----------------------------------------------------------------------\n-  SmallVector<SmallVector<Value>>\n-  emitIndices(Location loc, ConversionPatternRewriter &b, Attribute layout,\n-              RankedTensorType type, bool withCTAOffset = true) const {\n+  SmallVector<SmallVector<Value>> emitIndices(Location loc,\n+                                              ConversionPatternRewriter &b,\n+                                              Value value,\n+                                              bool withCTAOffset = true) const {\n+    auto type = value.getType().cast<RankedTensorType>();\n+    auto layout = type.getEncoding();\n     IndexCacheKeyT key{layout, type, withCTAOffset};\n     auto cache = indexCacheInfo.indexCache;\n     auto insertPt = indexCacheInfo.indexInsertPoint;\n@@ -818,7 +824,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto warpsPerCTA = blockedLayout.getWarpsPerCTA();\n     auto order = blockedLayout.getOrder();\n     auto shapePerCTA = triton::gpu::getShapePerCTA(blockedLayout, shape);\n-    auto shapePerCTATile = getShapePerCTATile(blockedLayout, shapePerCTA);\n+    auto shapePerCTATile = getShapePerCTATile(blockedLayout);\n \n     unsigned rank = shape.size();\n     SmallVector<unsigned> tilesPerDim(rank);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -180,8 +180,8 @@ struct ExpandDimsOpConversion\n     auto srcLayout = srcTy.getEncoding().dyn_cast<SliceEncodingAttr>();\n     auto resultLayout = resultTy.getEncoding();\n \n-    auto srcOffsets = emitOffsetForLayout(srcLayout, srcTy);\n-    auto resultOffsets = emitOffsetForLayout(resultLayout, resultTy);\n+    auto srcOffsets = emitOffsetForLayout(op.getSrc());\n+    auto resultOffsets = emitOffsetForLayout(op.getResult());\n     DenseMap<SmallVector<unsigned>, Value, SmallVectorKeyInfo> srcValues;\n     for (size_t i = 0; i < srcOffsets.size(); i++) {\n       srcValues[srcOffsets[i]] = srcVals[i];"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 77, "deletions": 0, "changes": 77, "file_content_changes": "@@ -286,6 +286,83 @@ SmallVector<unsigned> getThreadsPerCTA(Attribute layout) {\n   return threads;\n }\n \n+SmallVector<unsigned> getShapePerCTATile(BlockedEncodingAttr blockedLayout) {\n+  SmallVector<unsigned> shape;\n+  for (unsigned d = 0, n = blockedLayout.getOrder().size(); d < n; ++d)\n+    shape.push_back(blockedLayout.getSizePerThread()[d] *\n+                    blockedLayout.getThreadsPerWarp()[d] *\n+                    blockedLayout.getWarpsPerCTA()[d]);\n+\n+  return shape;\n+}\n+\n+static SmallVector<unsigned> getMMAShapePerCTATile_(MmaEncodingAttr mmaLayout,\n+                                                    Value value,\n+                                                    bool fromInput = false) {\n+  SmallVector<unsigned> shape;\n+  auto tensorShape = getShapePerCTA(value.getType());\n+  Type inputType;\n+\n+  if (mmaLayout.isAmpere())\n+    return {16 * mmaLayout.getWarpsPerCTA()[0],\n+            8 * mmaLayout.getWarpsPerCTA()[1]};\n+  if (mmaLayout.isVolta()) {\n+    assert(!tensorShape.empty() && \"Volta needs the tensorShape\");\n+    if (tensorShape.size() == 1) // must be SliceEncoding\n+      return {static_cast<unsigned>(tensorShape[0]),\n+              static_cast<unsigned>(tensorShape[0])};\n+    return {static_cast<unsigned>(tensorShape[0]),\n+            static_cast<unsigned>(tensorShape[1])};\n+  }\n+  if (mmaLayout.isHopper()) {\n+    auto instrShape = mmaVersionToInstrShape(mmaLayout.getVersionMajor(),\n+                                             tensorShape, inputType);\n+    return {16 * mmaLayout.getWarpsPerCTA()[0],\n+            instrShape[1] * mmaLayout.getWarpsPerCTA()[1]};\n+  }\n+  assert(0 && \"Unexpected MMA layout version found\");\n+  return shape;\n+}\n+\n+SmallVector<unsigned> getShapePerCTATile(Value value) {\n+  SmallVector<unsigned> shape;\n+  auto layout = value.getType().cast<RankedTensorType>().getEncoding();\n+\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+    return getShapePerCTATile(blockedLayout);\n+  } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+    Value newValue =\n+        mlir::getParentValueWithSameEncoding(sliceLayout.getParent(), value);\n+    shape = getShapePerCTATile(newValue);\n+    shape.erase(shape.begin() + sliceLayout.getDim());\n+  } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    getMMAShapePerCTATile_(mmaLayout, value);\n+  } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n+    auto parentLayout = dotLayout.getParent();\n+    assert(parentLayout && \"DotOperandEncodingAttr must have a parent\");\n+    if (auto parentMmaLayout = parentLayout.dyn_cast<MmaEncodingAttr>()) {\n+      assert((parentMmaLayout.isAmpere() || parentMmaLayout.isHopper()) &&\n+             \"mmaLayout version = 1 is not implemented yet\");\n+      auto parentShapePerCTATile =\n+          getMMAShapePerCTATile_(parentMmaLayout, value, true);\n+      auto opIdx = dotLayout.getOpIdx();\n+      if (opIdx == 0) {\n+        return {parentShapePerCTATile[0], 16};\n+      } else if (opIdx == 1) {\n+        return {16, parentShapePerCTATile[1]};\n+      } else {\n+        assert(0 && \"DotOperandEncodingAttr opIdx must be 0 or 1\");\n+      }\n+    } else {\n+      assert(0 && \"DotOperandEncodingAttr non-MmaEncodingAttr parent not \"\n+                  \"supported yet\");\n+    }\n+  } else {\n+    assert(0 && \"Unimplemented usage of getShapePerCTATile\");\n+  }\n+  return shape;\n+}\n+\n SmallVector<unsigned> getShapePerCTATile(Attribute layout,\n                                          ArrayRef<int64_t> tensorShape) {\n   SmallVector<unsigned> shape;"}]