[{"filename": "include/triton/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -146,6 +146,13 @@ Value linearize(OpBuilder &b, Location loc, ArrayRef<Value> multiDim,\n Value linearize(OpBuilder &b, Location loc, ArrayRef<Value> multiDim,\n                 ArrayRef<unsigned> shape);\n \n+// Returns null if the op is not inside a agent region (warp specialization\n+// mode). Note that there should be at most one agent id attached to the\n+// operation.\n+std::optional<int> getWSAgentId(Operation *op);\n+std::optional<int> getWSRoleId(Operation *op);\n+void setRoleId(Operation *op, int roleId);\n+\n } // namespace mlir\n \n #endif // TRITON_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_"}, {"filename": "include/triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -73,6 +73,8 @@ createTritonNvidiaGPUFenceInsertionPass(int computeCapability = 90);\n std::unique_ptr<Pass>\n createTritonGPURewriteTensorPointerPass(int computeCapability = 80);\n \n+std::unique_ptr<Pass> createTritonNvidiaGPUWSFixupMissingAttrs();\n+\n /// Generate the code for registering passes.\n #define GEN_PASS_REGISTRATION\n #include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h.inc\""}, {"filename": "include/triton/Dialect/TritonNvidiaGPU/Transforms/Passes.td", "status": "modified", "additions": 18, "deletions": 0, "changes": 18, "file_content_changes": "@@ -225,4 +225,22 @@ def TritonGPURewriteTensorPointer : Pass</*cli-arg*/\"tritongpu-rewrite-tensor-po\n   ];\n }\n \n+def TritonGPUWSFixupMissingAttrs : Pass<\"triton-nvidia-gpu-ws-fixup-missing-attrs\", \"mlir::ModuleOp\"> {\n+  let summary = \"Fixup missing WS related attributes\";\n+\n+  let description = [{\n+    WS related attributes are attached to some key operations and are used when lowering to llvm.\n+    However these attributes maybe be dropped in the following IR transform. This pass tries to\n+    fixup the missing attributes.\n+  }];\n+\n+  let constructor = \"mlir::createTritonNvidiaGPUWSFixupMissingAttrs()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::nvidia_gpu::TritonNvidiaGPUDialect\",\n+                           \"mlir::scf::SCFDialect\",\n+                           \"mlir::arith::ArithDialect\"];\n+}\n+\n+\n #endif"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 22, "deletions": 8, "changes": 30, "file_content_changes": "@@ -5,6 +5,7 @@\n #include \"../lib/Conversion/TritonGPUToLLVM/Utility.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n \n #include \"mlir/Dialect/GPU/IR/GPUDialect.h\"\n@@ -117,15 +118,28 @@ void MembarAnalysis::update(Operation *op, BlockInfo *blockInfo,\n     return;\n   }\n \n-  if (isa<triton::gpu::AsyncWaitOp>(op) &&\n-      !isa<gpu::BarrierOp>(op->getNextNode())) {\n+  if (isa<triton::gpu::AsyncWaitOp, triton::gpu::AsyncBulkWaitOp>(op) &&\n+      !isa<gpu::BarrierOp>(op->getNextNode()) &&\n+      !(isa<LLVM::InlineAsmOp>(op->getNextNode()) &&\n+        (dyn_cast<LLVM::InlineAsmOp>(op->getNextNode())\n+             .getAsmString()\n+             .find(\"bar.sync\") != std::string::npos))) {\n     // If the current op is an async wait and the next op is not a barrier we\n     // insert a barrier op and sync\n     blockInfo->sync();\n     OpBuilder::InsertionGuard g(*builder);\n     builder->setInsertionPointAfter(op);\n-    builder->create<gpu::BarrierOp>(op->getLoc());\n-    blockInfo->sync();\n+    if (auto optionalAgentId = getWSAgentId(op)) {\n+      int agentId = *optionalAgentId, roleId = 0;\n+      if (auto optionalRoleId = getWSRoleId(op))\n+        roleId = *optionalRoleId;\n+      int barId = agentId + roleId + nameBarrierIdBegin;\n+      assert(barId < nameBarrierIdEnd);\n+      barSync(*builder, op, barId, 128);\n+    } else {\n+      builder->create<gpu::BarrierOp>(op->getLoc());\n+      blockInfo->sync();\n+    }\n     return;\n   }\n \n@@ -180,10 +194,10 @@ void MembarAnalysis::update(Operation *op, BlockInfo *blockInfo,\n     // TODO(Keren): Don't expose LLVM Dialect ops here\n     // TODO[shuhaoj]: Change hard code style of numThreads. Hide async_agent\n     // attr. Better way to determine barId (number of agents are limited).\n-    if (op->hasAttr(\"async_agent\")) {\n-      int agentId = getAgentIds(op).front(), roleId = 0;\n-      if (op->hasAttr(\"agent.mutex_role\"))\n-        roleId = op->getAttrOfType<IntegerAttr>(\"agent.mutex_role\").getInt();\n+    if (auto optionalAgentId = getWSAgentId(op)) {\n+      int agentId = *optionalAgentId, roleId = 0;\n+      if (auto optionalRoleId = getWSRoleId(op))\n+        roleId = *optionalRoleId;\n       int barId = agentId + roleId + nameBarrierIdBegin;\n       assert(barId < nameBarrierIdEnd);\n       barSync(*builder, op, barId, 128);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 14, "deletions": 13, "changes": 27, "file_content_changes": "@@ -1,5 +1,7 @@\n #include \"ConvertLayoutOpToLLVM.h\"\n #include \"Utility.h\"\n+\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n \n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n@@ -589,11 +591,10 @@ struct ConvertLayoutOpConversion\n       if (repId != 0) {\n         // TODO[shuhaoj]: change hard code style of numThreads. Hide async\n         // attr.  Better way to determine barId (number of agents are limited).\n-        if (op->hasAttr(\"async_agent\")) {\n-          int agentId = getAgentIds(op).front(), roleId = 0;\n-          if (op->hasAttr(\"agent.mutex_role\"))\n-            roleId =\n-                op->getAttrOfType<IntegerAttr>(\"agent.mutex_role\").getInt();\n+        if (auto optionalAgentId = getWSAgentId(op)) {\n+          int agentId = *optionalAgentId, roleId = 0;\n+          if (auto optionalRoleId = getWSRoleId(op))\n+            roleId = *optionalRoleId;\n           int barId = agentId + roleId + nameBarrierIdBegin;\n           assert(barId < nameBarrierIdEnd);\n           auto bar = rewriter.create<LLVM::ConstantOp>(\n@@ -624,10 +625,10 @@ struct ConvertLayoutOpConversion\n \n       // TODO[shuhaoj]: change hard code style of numThreads. Hide async_agent\n       // attr.  Better way to determine barId (number of agents are limited).\n-      if (op->hasAttr(\"async_agent\")) {\n-        int agentId = getAgentIds(op).front(), roleId = 0;\n-        if (op->hasAttr(\"agent.mutex_role\"))\n-          roleId = op->getAttrOfType<IntegerAttr>(\"agent.mutex_role\").getInt();\n+      if (auto optionalAgentId = getWSAgentId(op)) {\n+        int agentId = *optionalAgentId, roleId = 0;\n+        if (auto optionalRoleId = getWSRoleId(op))\n+          roleId = *optionalRoleId;\n         int barId = agentId + roleId + nameBarrierIdBegin;\n         assert(barId < nameBarrierIdEnd);\n         auto bar = rewriter.create<LLVM::ConstantOp>(\n@@ -793,10 +794,10 @@ struct ConvertLayoutOpConversion\n       }\n       // TODO[shuhaoj]: change hard code style of numThreads. Hide async_agent\n       // attr.  Better way to determine barId (number of agents are limited).\n-      if (op->hasAttr(\"async_agent\")) {\n-        int agentId = getAgentIds(op).front(), roleId = 0;\n-        if (op->hasAttr(\"agent.mutex_role\"))\n-          roleId = op->getAttrOfType<IntegerAttr>(\"agent.mutex_role\").getInt();\n+      if (auto optionalAgentId = getWSAgentId(op)) {\n+        int agentId = *optionalAgentId, roleId = 0;\n+        if (auto optionalRoleId = getWSRoleId(op))\n+          roleId = *optionalRoleId;\n         int barId = agentId + roleId + nameBarrierIdBegin;\n         assert(barId < nameBarrierIdEnd);\n         auto bar = rewriter.create<LLVM::ConstantOp>("}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -1,6 +1,7 @@\n #include \"ReduceOpToLLVM.h\"\n #include \"Utility.h\"\n #include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n \n using namespace mlir;\n@@ -289,7 +290,7 @@ struct ReduceOpConversion\n             triton::ReduceOp op) const {\n     // TODO[shuhaoj]: change hard code style of numThreads. Hide async_agent\n     // attr.\n-    if (op->hasAttr(\"async_agent\")) {\n+    if (getWSAgentId(op)) {\n       barSync(rewriter, op, getAgentIds(op).front(), 128);\n     } else {\n       barrier();"}, {"filename": "lib/Dialect/TritonGPU/Transforms/ReorderInstructions.cpp", "status": "modified", "additions": 10, "deletions": 3, "changes": 13, "file_content_changes": "@@ -17,6 +17,7 @@\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n \n@@ -47,6 +48,12 @@ class TritonGPUReorderInstructionsPass\n     // Sink conversions into loops when they will increase\n     // register pressure\n     DenseMap<Operation *, Operation *> opToMove;\n+    auto moveAfter = [](Operation *lhs, Operation *rhs) {\n+      auto lhsId = getWSRoleId(lhs);\n+      auto rhsId = getWSRoleId(rhs);\n+      if (lhsId == rhsId)\n+        lhs->moveAfter(rhs);\n+    };\n     m.walk([&](triton::gpu::ConvertLayoutOp op) {\n       if (!willIncreaseRegisterPressure(op))\n         return;\n@@ -70,15 +77,15 @@ class TritonGPUReorderInstructionsPass\n       Operation *argOp = op.getOperand().getDefiningOp();\n       if (!argOp)\n         return;\n-      op->moveAfter(argOp);\n+      moveAfter(op, argOp);\n     });\n     // Move transpositions just after their definition\n     opToMove.clear();\n     m.walk([&](triton::TransOp op) {\n       Operation *argOp = op.getOperand().getDefiningOp();\n       if (!argOp)\n         return;\n-      op->moveAfter(argOp);\n+      moveAfter(op, argOp);\n     });\n     // Move `dot` operand so that conversions to opIdx=1 happens after\n     // conversions to opIdx=0\n@@ -104,7 +111,7 @@ class TritonGPUReorderInstructionsPass\n       // after the conversion to OpIdx=0.\n       if (!dom.dominates(op.getOperation(), AOp.getOperation()))\n         return;\n-      op->moveAfter(AOp);\n+      moveAfter(op, AOp);\n     });\n     return;\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 24, "deletions": 0, "changes": 24, "file_content_changes": "@@ -731,4 +731,28 @@ Value linearize(OpBuilder &b, Location loc, ArrayRef<Value> multiDim,\n   return linear;\n }\n \n+std::optional<int> getWSAgentId(Operation *op) {\n+  int prevAgentId = -1;\n+  if (auto attr = op->getAttrOfType<DenseIntElementsAttr>(\"async_agent\")) {\n+    for (auto agentId : attr.getValues<int>()) {\n+      assert(prevAgentId == -1 && \"support at most one agetn id\");\n+      prevAgentId = agentId;\n+    }\n+  }\n+  if (prevAgentId == -1)\n+    return std::nullopt;\n+  return prevAgentId;\n+}\n+\n+std::optional<int> getWSRoleId(Operation *op) {\n+  if (!op->hasAttr(\"agent.mutex_role\"))\n+    return std::nullopt;\n+  return op->getAttrOfType<IntegerAttr>(\"agent.mutex_role\").getInt();\n+}\n+\n+void setRoleId(Operation *op, int roleId) {\n+  auto attr = IntegerAttr::get(IntegerType::get(op->getContext(), 32), roleId);\n+  op->setAttr(\"agent.mutex_role\", attr);\n+}\n+\n } // namespace mlir"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -6,6 +6,7 @@ add_mlir_dialect_library(TritonNvidiaGPUTransforms\n   WSPipeline.cpp\n   WSMutex.cpp\n   WSMaterialization.cpp\n+  WSFixupMissingAttrs.cpp\n   FenceInsertion.cpp\n   RewriteTensorPointer.cpp\n   Utility.cpp"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/WSFixupMissingAttrs.cpp", "status": "added", "additions": 69, "deletions": 0, "changes": 69, "file_content_changes": "@@ -0,0 +1,69 @@\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h.inc\"\n+\n+namespace mlir {\n+\n+namespace ttng = triton::nvidia_gpu;\n+\n+namespace {\n+\n+class TritonGPUWSFixupMissingAttrsPass\n+    : public TritonGPUWSFixupMissingAttrsBase<\n+          TritonGPUWSFixupMissingAttrsPass> {\n+public:\n+  TritonGPUWSFixupMissingAttrsPass() = default;\n+\n+  void runOnOperation() override {\n+    ModuleOp mod = getOperation();\n+    if (!ttng::TritonNvidiaGPUDialect::getWSSupportedAttr(mod))\n+      return;\n+    OpBuilder builder(mod);\n+    mod->walk([&](mlir::triton::FuncOp funcOp) {\n+      for (Operation &op : funcOp.getBody().front().getOperations()) {\n+        if (!isa<scf::IfOp>(&op))\n+          continue;\n+        auto agentIds = getAgentIds(&op);\n+        if (agentIds.size() != 1)\n+          continue;\n+        Block *roleIdBlock = nullptr;\n+        op.walk<WalkOrder::PreOrder>([&](Operation *subOp) {\n+          setAgentIds(subOp, agentIds);\n+          // Find the outter most common block that has roleId.\n+          // The below implementation assumes that:\n+          // - all lock/unlock ops are in the same block (denoted as B).\n+          // - there is always one scf.if op in the front of `B` which has\n+          //   role id attached.\n+          // The above assumptions are maintained by WSMutex pass currently.\n+          if (!roleIdBlock && isa<scf::IfOp>(subOp) && getWSRoleId(subOp))\n+            roleIdBlock = subOp->getBlock();\n+        });\n+        if (!roleIdBlock)\n+          continue;\n+        int roleId = 0;\n+        for (Operation &roleOp : roleIdBlock->getOperations()) {\n+          auto optionalRoleId = getWSRoleId(&roleOp);\n+          if (!optionalRoleId) {\n+            setRoleId(&roleOp, roleId);\n+          } else {\n+            roleId = *optionalRoleId;\n+          }\n+          roleOp.walk([&](Operation *subOp) { setRoleId(subOp, roleId); });\n+        }\n+      }\n+    });\n+  }\n+};\n+\n+} // namespace\n+\n+std::unique_ptr<Pass> createTritonNvidiaGPUWSFixupMissingAttrs() {\n+  return std::make_unique<TritonGPUWSFixupMissingAttrsPass>();\n+}\n+\n+} // namespace mlir"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/WSMaterialization.cpp", "status": "modified", "additions": 0, "deletions": 26, "changes": 26, "file_content_changes": "@@ -708,32 +708,6 @@ struct WSMaterializationPass\n     materializeMutexOperations(mod);\n     tryRegisterRealloc(mod);\n \n-    mod->walk([](Operation *op) {\n-      bool hasTensor = 0;\n-      auto results = op->getResults();\n-      auto operands = op->getOperands();\n-      for (auto i : results) {\n-        if (isa<RankedTensorType>(i.getType())) {\n-          hasTensor = 1;\n-          break;\n-        }\n-      }\n-      if (!hasTensor) {\n-        for (auto i : operands) {\n-          if (isa<RankedTensorType>(i.getType())) {\n-            hasTensor = 1;\n-            break;\n-          }\n-        }\n-      }\n-\n-      if (!hasTensor && !isa<ttng::MBarrierWaitOp>(op) &&\n-          !isa<ttng::ExtractMBarrierOp>(op) &&\n-          !isa<ttng::MBarrierArriveOp>(op)) {\n-        op->removeAttr(\"async_agent\");\n-      }\n-    });\n-\n     // TODO: More flexible way to set num-warps\n     // One dma, one math warp group, set num-warps = 8\n     auto i32_ty = IntegerType::get(mod->getContext(), 32);"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/WSMutex.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -264,8 +264,9 @@ void mutexSync(ModuleOp &mod, scf::IfOp &ifOp, scf::ForOp &persistentForOp,\n       });\n     for (int i = 0; i < numRoles; ++i) {\n       if (lockLocs[i] == op) {\n+        if (roleId != -1)\n+          op->setAttr(\"agent.mutex_role\", builder.getI32IntegerAttr(roleId));\n         roleId = i;\n-        op->setAttr(\"agent.mutex_role\", builder.getI32IntegerAttr(i));\n         break;\n       }\n     }"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -1628,6 +1628,10 @@ void init_triton_ir(py::module &&m) {\n              self.addPass(mlir::createTritonNvidiaGPUWSMaterializationPass(\n                  computeCapability));\n            })\n+      .def(\"add_tritongpu_ws_fixup_missing_attrs_pass\",\n+           [](mlir::PassManager &self) {\n+             self.addPass(mlir::createTritonNvidiaGPUWSFixupMissingAttrs());\n+           })\n       .def(\n           \"add_convert_triton_to_tritongpu_pass\",\n           [](mlir::PassManager &self, int numWarps, int threadsPerWarp,"}, {"filename": "python/test/unit/hopper/test_gemm.py", "status": "modified", "additions": 0, "deletions": 11, "changes": 11, "file_content_changes": "@@ -93,11 +93,6 @@ def matmul_no_scf_kernel(\n                              ]))\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n def test_gemm_no_scf(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_TYPE, USE_TMA_EPILOGUE, ENABLE_WS):\n-    if '-'.join(map(str, [USE_TMA_EPILOGUE, ENABLE_WS])) in [\n-        'True-True'\n-    ]:\n-        pytest.skip(\"error, skip\")\n-\n     if (TRANS_A):\n         a = torch.randn((K, M), device='cuda', dtype=torch.float16).T\n     else:\n@@ -338,12 +333,6 @@ def test_gemm(BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A,\n     if NUM_CTAS > 1 and NUM_WARPS == 8:\n         pytest.skip('Tensor-likes are not close!')\n \n-    # with ENABLE_TMA=1 and ENABLE_MMA_V3=1\n-    if ENABLE_WS:\n-        # example:\n-        # [128-128-64-4-1-None-None-None-False-False-False-chain-dot-float16-False-3-True]\n-        pytest.skip('hang!')\n-\n     M = BLOCK_M if M is None else M\n     N = BLOCK_N if N is None else N\n     K = BLOCK_K if K is None else K"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -120,11 +120,13 @@ def optimize_ttgir(mod, num_stages, num_warps, num_ctas, arch,\n     pm.add_tritongpu_optimize_dot_operands_pass()\n     pm.add_tritongpu_remove_layout_conversions_pass()\n     pm.add_tritongpu_decompose_conversions_pass()\n+    pm.add_tritongpu_ws_fixup_missing_attrs_pass()\n     pm.add_tritongpu_reorder_instructions_pass()\n     pm.add_cse_pass()\n     pm.add_symbol_dce_pass()\n     if arch // 10 >= 9:\n         pm.add_tritongpu_fence_insertion_pass()\n+    pm.add_tritongpu_ws_fixup_missing_attrs_pass()\n     pm.run(mod)\n     return mod\n "}]