[{"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 37, "deletions": 0, "changes": 37, "file_content_changes": "@@ -63,6 +63,43 @@ class ReduceOpHelper {\n   int axis;\n };\n \n+class ScanLoweringHelper {\n+public:\n+  explicit ScanLoweringHelper(triton::ScanOp op) : scanOp(op) {\n+    auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n+    srcEncoding = type.getEncoding();\n+  }\n+  // Return true if the lowering of the scan op is supported.\n+  bool isSupported();\n+  // Return the number of elements per thread along axis dim.\n+  unsigned getAxisNumElementsPerThreads();\n+  // Return the number of elements per thread along non-axis dims.\n+  unsigned getNonAxisNumElementsPerThread();\n+  // Return the number of threads per warp along non-axis dims.\n+  unsigned getNonAxisNumThreadsPerWarp();\n+  // Return the flat numbers of threads computing independent scan results.\n+  unsigned getNonAxisNumThreadsPerCTA();\n+  // Return the number of warps per CTA along axis dim.\n+  unsigned getAxisNumWarps();\n+  // Return the number of threads per warp along axis dim.\n+  unsigned getAxisNumThreadsPerWarp();\n+  // Return the number of blocks along axis dim.\n+  unsigned getAxisNumBlocks();\n+  // Return the number of blocks along non axis dim.\n+  unsigned getNonAxisNumBlocks();\n+  // Return the size of the scratch space needed for scan lowering.\n+  unsigned getScratchSizeInBytes();\n+\n+  Location getLoc() { return scanOp.getLoc(); }\n+  unsigned getAxis() { return scanOp.getAxis(); }\n+  triton::gpu::BlockedEncodingAttr getEncoding();\n+  Region &getCombineOp();\n+\n+private:\n+  triton::ScanOp scanOp;\n+  Attribute srcEncoding;\n+};\n+\n bool maybeSharedAllocationOp(Operation *op);\n \n bool maybeAliasOp(Operation *op);"}, {"filename": "include/triton/Dialect/Triton/IR/TritonAttrDefs.td", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -12,6 +12,7 @@ def TT_CacheModifierAttr : I32EnumAttr<\n         I32EnumAttrCase<\"CG\", 3, \"cg\">,\n         I32EnumAttrCase<\"WB\", 4, \"wb\">,\n         I32EnumAttrCase<\"CS\", 5, \"cs\">,\n+        I32EnumAttrCase<\"WT\", 6, \"wt\">,\n     ]> {\n     let cppNamespace = \"::mlir::triton\";\n }"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 26, "deletions": 0, "changes": 26, "file_content_changes": "@@ -426,6 +426,32 @@ def TT_ReduceReturnOp: TT_Op<\"reduce.return\",\n     let assemblyFormat = \"$result attr-dict `:` type($result)\";\n }\n \n+//\n+// Scan Op\n+//\n+def TT_ScanOp: TT_Op<\"scan\",\n+                       [Pure,\n+                        SameOperandsAndResultEncoding,\n+                        SameOperandsAndResultElementType,\n+                        SingleBlock,\n+                        DeclareOpInterfaceMethods<InferTypeOpInterface>]> {\n+    let summary = \"Reduction using generic combination algorithm\";\n+    let arguments = (ins Variadic<TT_Tensor>:$operands, I32Attr:$axis);\n+    let results = (outs Variadic<TT_Tensor>:$result);\n+    let regions = (region SizedRegion<1>:$combineOp);\n+    let builders = [\n+        OpBuilder<(ins \"ValueRange\":$operands, \"int\":$axis)>,\n+    ];\n+    let hasVerifier = 1;\n+}\n+\n+def TT_ScanReturnOp: TT_Op<\"scan.return\",\n+                             [HasParent<\"ScanOp\">, Pure, Terminator, ReturnLike]> {\n+    let summary = \"terminator for scan operator\";\n+    let arguments = (ins Variadic<AnyType>:$result);\n+    let assemblyFormat = \"$result attr-dict `:` type($result)\";\n+}\n+\n \n //\n // External Elementwise op"}, {"filename": "include/triton/Dialect/Triton/IR/TritonTypes.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -14,7 +14,7 @@ class TritonTypeDef<string name, string _mnemonic>\n }\n \n // Floating-point Type\n-def TT_Float : AnyTypeOf<[F8E4M3FNUZ, F8E4M3B11FNUZ, F8E5M2, F16, BF16, F32, F64], \"floating-point\">;\n+def TT_Float : AnyTypeOf<[F8E4M3FN, F8E5M2, F16, BF16, F32, F64], \"floating-point\">;\n def TT_FloatTensor : TensorOf<[TT_Float]>;\n def TT_FloatLike : AnyTypeOf<[TT_Float, TT_FloatTensor]>;\n "}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -168,6 +168,10 @@ class AllocationAnalysis {\n       ReduceOpHelper helper(reduceOp);\n       unsigned bytes = helper.getScratchSizeInBytes();\n       allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+    } else if (auto scanOp = dyn_cast<triton::ScanOp>(op)) {\n+      ScanLoweringHelper helper(scanOp);\n+      unsigned bytes = helper.getScratchSizeInBytes();\n+      allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     } else if (auto cvtLayout = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n       auto srcTy = cvtLayout.getSrc().getType().cast<RankedTensorType>();\n       auto dstTy = cvtLayout.getResult().getType().cast<RankedTensorType>();"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -86,7 +86,8 @@ void MembarAnalysis::visitTerminator(Operation *op,\n     return;\n   }\n   // Otherwise, it could be a return op\n-  if (isa<triton::ReduceReturnOp>(op) || isa<triton::ReturnOp>(op)) {\n+  if (isa<triton::ReduceReturnOp>(op) || isa<triton::ScanReturnOp>(op) ||\n+      isa<triton::ReturnOp>(op)) {\n     return;\n   }\n   llvm_unreachable(\"Unknown terminator encountered in membar analysis\");"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 87, "deletions": 0, "changes": 87, "file_content_changes": "@@ -117,6 +117,93 @@ bool ReduceOpHelper::isSupportedLayout() {\n   return false;\n }\n \n+unsigned ScanLoweringHelper::getAxisNumElementsPerThreads() {\n+  return getEncoding().getSizePerThread()[getAxis()];\n+}\n+\n+unsigned ScanLoweringHelper::getNonAxisNumElementsPerThread() {\n+  SmallVector<unsigned> sizePerThreads(getEncoding().getSizePerThread().begin(),\n+                                       getEncoding().getSizePerThread().end());\n+  sizePerThreads[getAxis()] = 1;\n+  return product<unsigned>(sizePerThreads);\n+}\n+\n+Region &ScanLoweringHelper::getCombineOp() { return scanOp.getCombineOp(); }\n+\n+unsigned ScanLoweringHelper::getAxisNumThreadsPerWarp() {\n+  return triton::gpu::getThreadsPerWarp(getEncoding())[getAxis()];\n+}\n+\n+unsigned ScanLoweringHelper::getNonAxisNumThreadsPerWarp() {\n+  auto threadsPerWarp = triton::gpu::getThreadsPerWarp(getEncoding());\n+  threadsPerWarp[getAxis()] = 1;\n+  return product<unsigned>(threadsPerWarp);\n+}\n+\n+// Return the flat numbers of threads computing independent scan results.\n+unsigned ScanLoweringHelper::getNonAxisNumThreadsPerCTA() {\n+  unsigned numParallelThreadsPerWarp = getNonAxisNumThreadsPerWarp();\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(getEncoding());\n+  warpsPerCTA[getAxis()] = 1;\n+  unsigned numParallelWarpsPerCTA = product<unsigned>(warpsPerCTA);\n+  return numParallelThreadsPerWarp * numParallelWarpsPerCTA;\n+}\n+unsigned ScanLoweringHelper::getAxisNumWarps() {\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n+  return warpsPerCTA[getAxis()];\n+}\n+\n+unsigned ScanLoweringHelper::getAxisNumBlocks() {\n+  auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n+  auto sizePerThreads = triton::gpu::getSizePerThread(srcEncoding);\n+  auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcEncoding);\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n+  unsigned axis = getAxis();\n+  return type.getShape()[axis] /\n+         (sizePerThreads[axis] * threadsPerWarp[axis] * warpsPerCTA[axis]);\n+}\n+\n+unsigned ScanLoweringHelper::getNonAxisNumBlocks() {\n+  auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n+  auto sizePerThreads = triton::gpu::getSizePerThread(srcEncoding);\n+  auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcEncoding);\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n+  unsigned axis = getAxis();\n+  unsigned numBlocks = 1;\n+  for (unsigned i = 0; i < sizePerThreads.size(); i++) {\n+    if (i == axis)\n+      continue;\n+    numBlocks *= type.getShape()[i] /\n+                 (sizePerThreads[i] * threadsPerWarp[i] * warpsPerCTA[i]);\n+  }\n+  return numBlocks;\n+}\n+\n+bool ScanLoweringHelper::isSupported() {\n+  // TODO: Support the following cases:\n+  // 1. Scan on the non-fast changing dimension\n+  // 2. Scan on non-blocking encodings\n+  // 3. Scan with multiple operands\n+  if (getAxis() != triton::gpu::getOrder(srcEncoding)[0] ||\n+      !isa<triton::gpu::BlockedEncodingAttr>(srcEncoding))\n+    return false;\n+  if (scanOp.getNumOperands() != 1)\n+    return false;\n+  return true;\n+}\n+\n+unsigned ScanLoweringHelper::getScratchSizeInBytes() {\n+  auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n+  unsigned numElement =\n+      type.getNumElements() * type.getElementTypeBitWidth() / 8;\n+  return numElement /\n+         (getAxisNumElementsPerThreads() * getAxisNumThreadsPerWarp());\n+}\n+\n+triton::gpu::BlockedEncodingAttr ScanLoweringHelper::getEncoding() {\n+  return srcEncoding.cast<triton::gpu::BlockedEncodingAttr>();\n+}\n+\n bool maybeSharedAllocationOp(Operation *op) {\n   // TODO(Keren): This function can be replaced by adding\n   // MemoryEffectOpInterface. We can then use the MemoryEffectOpInterface to"}, {"filename": "lib/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -17,6 +17,7 @@ add_mlir_conversion_library(TritonGPUToLLVM\n     TritonGPUToLLVMPass.cpp\n     PTXAsmFormat.cpp\n     ReduceOpToLLVM.cpp\n+    ScanOpToLLVM.cpp\n     Utility.cpp\n     TypeConverter.cpp\n     ViewOpToLLVM.cpp"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 451, "deletions": 301, "changes": 752, "file_content_changes": "@@ -4,227 +4,6 @@ using namespace mlir;\n using namespace mlir::triton;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n \n-/* ----- FP8E5M2 ------ */\n-// This data-type is the standard FP8E5M2 format\n-\n-const std::string Fp16_to_Fp8E5M2 =\n-    \"{                            \\n\"\n-    \".reg .b32 a<2>;              \\n\"\n-    \"and.b32 a0, $1, 0x7fff7fff;  \\n\"           // a0 &= 0x7fff7fff\n-    \"and.b32 a1, $2, 0x7fff7fff;  \\n\"           // (strip sign)\n-    \"add.u32 a0, a0, 0x00800080;  \\n\"           // a0 += 0x00800080\n-    \"add.u32 a1, a1, 0x00800080;  \\n\"           // (round to nearest)\n-    \"lop3.b32 a0, $1, 0x80008000, a0, 0xea; \\n\" // a0 = a0|(0x80008000&in0)\n-    \"lop3.b32 a1, $2, 0x80008000, a1, 0xea; \\n\" // (restore sign)\n-    \"prmt.b32 $0, a0, a1, 0x7531; \\n\\t\"         // output = a1a0\n-    \"}\";\n-\n-const std::string Fp8E5M2_to_Fp16 = \"{                           \\n\"\n-                                    \"prmt.b32 $0, 0, $2, 0x5140; \\n\\t\"\n-                                    \"prmt.b32 $1, 0, $2, 0x7362; \\n\\t\"\n-                                    \"}\";\n-\n-const std::string Fp8E5M2_to_Bf16 =\n-    \"{                                      \\n\"\n-    \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n-    \"prmt.b32 a0, 0, $2, 0x5140;            \\n\" // a0 = 0xf300f400\n-    \"prmt.b32 a1, 0, $2, 0x7362;            \\n\" // a1 = 0xf100f200\n-    \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // b0 = a0 & 0x7fff7fff\n-    \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n-    \"shr.b32  b0, b0, 3;                    \\n\" // b0 >>= 3\n-    \"shr.b32  b1, b1, 3;                    \\n\" // shift into bf16 position\n-    \"add.u32  b0, b0, 0x38003800;           \\n\" // b0.exp += 2**7-2**4\n-                                                // exponent compensate = 112\n-    \"add.u32  b1, b1, 0x38003800;           \\n\" // b1 += 112<<7 | 112<<7<<16\n-    \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n-    \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n-    \"}\";\n-\n-const std::string Bf16_to_Fp8E5M2 =\n-    \"{                                           \\n\" // bf16=fp8>>3 + 112<<7\n-    \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\" // fp8_min = 0b00000000\n-    \".reg .u32 fp8_min, fp8_max, rn_;            \\n\" // fp8_max = 0b11111111\n-    \"mov.u32 fp8_min, 0x38003800;                \\n\" // so bf16_min = 0x3800\n-    \"mov.u32 fp8_max, 0x57e057e0;                \\n\" // so bf16_max = 0x57e0\n-    \"mov.u32 rn_, 0x00100010;                    \\n\" // round to nearest\n-    \"and.b32 sign0, $1, 0x80008000;              \\n\" // sign0=in0&0x80008000\n-    \"and.b32 sign1, $2, 0x80008000;              \\n\" // (store sign)\n-    \"prmt.b32 sign, sign0, sign1, 0x7531;        \\n\"\n-    \"and.b32 nosign0, $1, 0x7fff7fff;            \\n\" // nosign0=in0&0x7fff7fff\n-    \"and.b32 nosign1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n-\n-    // nosign = clamp(nosign, min, max)\n-    \".reg .u32 nosign_0_<2>, nosign_1_<2>;       \\n\"\n-    \"and.b32 nosign_0_0, nosign0, 0xffff0000;    \\n\"\n-    \"max.u32 nosign_0_0, nosign_0_0, 0x38000000; \\n\"\n-    \"min.u32 nosign_0_0, nosign_0_0, 0x57e00000; \\n\"\n-    \"and.b32 nosign_0_1, nosign0, 0x0000ffff;    \\n\"\n-    \"max.u32 nosign_0_1, nosign_0_1, 0x3800;     \\n\"\n-    \"min.u32 nosign_0_1, nosign_0_1, 0x57e0;     \\n\"\n-    \"or.b32 nosign0, nosign_0_0, nosign_0_1;     \\n\"\n-    \"and.b32 nosign_1_0, nosign1, 0xffff0000;    \\n\"\n-    \"max.u32 nosign_1_0, nosign_1_0, 0x38000000; \\n\"\n-    \"min.u32 nosign_1_0, nosign_1_0, 0x57e00000; \\n\"\n-    \"and.b32 nosign_1_1, nosign1, 0x0000ffff;    \\n\"\n-    \"max.u32 nosign_1_1, nosign_1_1, 0x3800;     \\n\"\n-    \"min.u32 nosign_1_1, nosign_1_1, 0x57e0;     \\n\"\n-    \"or.b32 nosign1, nosign_1_0, nosign_1_1;     \\n\"\n-\n-    \"add.u32 nosign0, nosign0, rn_;              \\n\" // nosign0 += rn_\n-    \"add.u32 nosign1, nosign1, rn_;              \\n\" // (round to nearest)\n-    \"sub.u32 nosign0, nosign0, 0x38003800;       \\n\" // nosign0-=0x38003800\n-    \"sub.u32 nosign1, nosign1, 0x38003800;       \\n\" // (compensate offset)\n-    \"shl.b32 nosign0, nosign0, 3;                \\n\" // nosign0 <<= 3\n-    \"shl.b32 nosign1, nosign1, 3;                \\n\" // shift into to fp8e4\n-    \"prmt.b32 nosign, nosign0, nosign1, 0x7531;  \\n\" // nosign0 = 0xf100f200\n-                                                     // nosign1 = 0xf300f400\n-                                                     // nosign = 0xf3f4f1f2\n-    \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n-    \"}\";\n-\n-/* ----- FP8E4M3B15 ------ */\n-// This data-type is a variant of the standard FP8E4M3 format.\n-// It was designed for fast software conversion to FP16 on\n-// nvidia GPUs that do not support it natively.\n-// Specifically, this data-type:\n-//    - has infinities\n-//    - has multiple nans (when all exponent bits are 1)\n-//    - has an exponent bias of 15 (vs. 7 for fp8e4m3)\n-\n-// Fp8E4M3B15 -> Fp16 (packed)\n-// fast conversion code provided by Scott Gray @ OpenAI\n-// $0 = (($2 << 1) & 0x80008000u) | (($2 << 7) & 0x3f803f80u);\n-// $1 = (($2 << 0) & 0x80008000u) | (($2 << 0) & 0x3f803f80u);\n-// WARN: subnormal (0bs0000xxx) are not handled\n-const std::string Fp8E4M3B15_to_Fp16 =\n-    \"{                                      \\n\"\n-    \".reg .b32 a<2>;                        \\n\"\n-    \"shl.b32 a0, $2, 1;                     \\n\"\n-    \"shl.b32 a1, $2, 7;                     \\n\"\n-    \"and.b32  $0, a0, 0x80008000;           \\n\"\n-    \"lop3.b32 $0, $0, a1, 0x3f803f80, 0xf8; \\n\"\n-    \"and.b32  $1, $2, 0x80008000;           \\n\"\n-    \"lop3.b32 $1, $1, $2, 0x3f803f80, 0xf8; \\n\"\n-    \"}\";\n-\n-// Fp16 -> Fp8E4M3B15 (packed)\n-// fast conversion code provided by Scott Gray @ OpenAI\n-// ret = ((e4.x >> 1) & (0x80008000u >> 1)) |\n-//       ((e4.x >> 7) & (0x3f803f80u >> 7)) |\n-//       ((e4.y >> 0) & (0x80008000u >> 0)) |\n-//       ((e4.y >> 0) & (0x3f803f80u >> 0)) ;\n-// WARN: subnormal (0bs0000xxx) are not handled\n-const std::string Fp16_to_Fp8E4M3B15 =\n-    \"{                                       \\n\"\n-    \".reg .b32 a<2>;                         \\n\"\n-    \"shr.b32  a0, $1, 1;                     \\n\"\n-    \"shr.b32  a1, $1, 7;                     \\n\"\n-    \"and.b32  $0,     a0, 0x40004000;        \\n\"\n-    \"lop3.b32 $0, $0, a1, 0x007f007f, 0xf8;  \\n\"\n-    \"lop3.b32 $0, $0, $2, 0x80008000, 0xf8;  \\n\"\n-    \"lop3.b32 $0, $0, $2, 0x3f803f80, 0xf8;  \\n\"\n-    \"}\";\n-\n-/* ----- FP8E4M3 ------ */\n-// Note: when handled by software, this format\n-// does not handle denormals and has\n-// more than a single NaN values.\n-\n-// Fp8E4M3 -> Fp16 (packed)\n-const std::string Fp8E4M3_to_Fp16 =\n-    \"{                                      \\n\"\n-    \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n-    \"prmt.b32 a0, 0, $2, 0x5040;            \\n\" // a0 = 0xf300f400\n-    \"prmt.b32 a1, 0, $2, 0x7060;            \\n\" // a1 = 0xf100f200\n-    \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // b0 = a0 & 0x7fff7fff\n-    \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n-    \"shr.b32  b0, b0, 1;                    \\n\" // b0 >>= 1\n-    \"shr.b32  b1, b1, 1;                    \\n\" // shift into fp16 position\n-    \"add.u32  b0, b0, 0x20002000;           \\n\" // b0.exp += 2**4-2**3\n-                                                // exponent compensate = 8\n-    \"add.u32  b1, b1, 0x20002000;           \\n\" // b1 += 8<<10 | 8<<10<<16\n-    \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n-    \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n-    \"}\";\n-\n-// Fp16 -> Fp8E4M3 (packed)\n-const std::string Fp16_to_Fp8E4M3 =\n-    \"{                                      \\n\"\n-    \".reg .b32 a<2>, b<2>;                  \\n\" // see Fp8E4M3x4ToFp16x4\n-    \"sub.u32 a0, $1, 0x20002000;            \\n\" // a0 = input0 - 0x20002000\n-                                                // (compensate offset)\n-    \"sub.u32 a1, $2, 0x20002000;            \\n\" // a1 = input1 - 0x20002000\n-                                                // (8 << 10 | 8 << 10 << 16)\n-    \"shl.b32 a0, a0, 1;                     \\n\" // a0 <<= 1\n-    \"shl.b32 a1, a1, 1;                     \\n\" // shift into fp8e4 position\n-    \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // a0 &= 0x7fff7fff\n-    \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n-    \"add.u32 a0, a0, 0x00800080;            \\n\" // a0 += 0x00800080\n-    \"add.u32 a1, a1, 0x00800080;            \\n\" // (round to nearest)\n-    \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\" // b0 = a0|(0x80008000&in0)\n-    \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\" // (restore sign)\n-    \"prmt.b32 $0, b0, b1, 0x7531;           \\n\" // output = b1b0\n-    \"}\";\n-\n-// WARN: subnormal (0bs0000xxx) are not handled\n-const std::string Fp8E4M3_to_Bf16 =\n-    \"{                                      \\n\"\n-    \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n-    \"prmt.b32 a0, 0, $2, 0x5040;            \\n\" // a0 = 0xf300f400\n-    \"prmt.b32 a1, 0, $2, 0x7060;            \\n\" // a1 = 0xf100f200\n-    \"and.b32 b0, a0, 0x7fff7fff;            \\n\" // b0 = a0 & 0x7fff7fff\n-    \"and.b32 b1, a1, 0x7fff7fff;            \\n\" // (strip sign)\n-    \"shr.b32 b0, b0, 4;                     \\n\" // b0 >>= 4\n-    \"shr.b32 b1, b1, 4;                     \\n\" // shift into fp16 position\n-    \"add.u32 b0, b0, 0x3c003c00;            \\n\" // b0.exp += 2**7-2**3\n-                                                // exponent compensate = 120\n-    \"add.u32 b1, b1, 0x3c003c00;            \\n\" // b1 += 120<<7 | 120<<7<<16\n-    \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n-    \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n-    \"}\";\n-\n-const std::string Bf16_to_Fp8E4M3 =\n-    \"{                                           \\n\" // bf16=fp8>>4 + 120<<7\n-    \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\" // fp8_min = 0b00000000\n-    \".reg .u32 fp8_min, fp8_max, rn_;            \\n\" // fp8_max = 0b11111111\n-    \"mov.u32 fp8_min, 0x3c003c00;                \\n\" // so bf16_min = 0x3c00\n-    \"mov.u32 fp8_max, 0x43f043f0;                \\n\" // so bf16_max = 0x43f0\n-    \"mov.u32 rn_, 0x80008;                       \\n\" // round to nearest\n-    \"and.b32 sign0, $1, 0x80008000;              \\n\" // sign0=in0&0x80008000\n-    \"and.b32 sign1, $2, 0x80008000;              \\n\" // (store sign)\n-    \"prmt.b32 sign, sign0, sign1, 0x7531;        \\n\"\n-    \"and.b32 nosign0, $1, 0x7fff7fff;            \\n\" // nosign0=in0&0x7fff7fff\n-    \"and.b32 nosign1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n-\n-    // nosign = clamp(nosign, min, max)\n-    \".reg .u32 nosign_0_<2>, nosign_1_<2>;       \\n\"\n-    \"and.b32 nosign_0_0, nosign0, 0xffff0000;    \\n\"\n-    \"max.u32 nosign_0_0, nosign_0_0, 0x3c000000; \\n\"\n-    \"min.u32 nosign_0_0, nosign_0_0, 0x43f00000; \\n\"\n-    \"and.b32 nosign_0_1, nosign0, 0x0000ffff;    \\n\"\n-    \"max.u32 nosign_0_1, nosign_0_1, 0x3c00;     \\n\"\n-    \"min.u32 nosign_0_1, nosign_0_1, 0x43f0;     \\n\"\n-    \"or.b32 nosign0, nosign_0_0, nosign_0_1;     \\n\"\n-    \"and.b32 nosign_1_0, nosign1, 0xffff0000;    \\n\"\n-    \"max.u32 nosign_1_0, nosign_1_0, 0x3c000000; \\n\"\n-    \"min.u32 nosign_1_0, nosign_1_0, 0x43f00000; \\n\"\n-    \"and.b32 nosign_1_1, nosign1, 0x0000ffff;    \\n\"\n-    \"max.u32 nosign_1_1, nosign_1_1, 0x3c00;     \\n\"\n-    \"min.u32 nosign_1_1, nosign_1_1, 0x43f0;     \\n\"\n-    \"or.b32 nosign1, nosign_1_0, nosign_1_1;     \\n\"\n-\n-    \"add.u32 nosign0, nosign0, rn_;              \\n\" // nosign0 += rn_\n-    \"add.u32 nosign1, nosign1, rn_;              \\n\" // (round to nearest)\n-    \"sub.u32 nosign0, nosign0, 0x3c003c00;       \\n\" // nosign0-=0x3c003c00\n-    \"sub.u32 nosign1, nosign1, 0x3c003c00;       \\n\" // (compensate offset)\n-    \"shr.u32 nosign0, nosign0, 4;                \\n\" // nosign0 >>= 4\n-    \"shr.u32 nosign1, nosign1, 4;                \\n\" // shift into to fp8e4\n-    \"prmt.b32 nosign, nosign0, nosign1, 0x6420;  \\n\" // nosign0 = 0x00f100f2\n-                                                     // nosign1 = 0x00f300f4\n-                                                     // nosign = 0xf3f4f1f2\n-    \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n-    \"}\";\n-\n static SmallVector<Value> reorderValues(const SmallVector<Value> &values,\n                                         Type inType, Type ouType) {\n   auto inTensorTy = inType.dyn_cast<RankedTensorType>();\n@@ -363,57 +142,435 @@ struct FpToFpOpConversion\n   // FP8 -> FP16\n   /* ------------------ */\n \n-  static ConvertorT makeConverterFromPtx(const std::string &ptxAsm, Type inType,\n-                                         Type outType) {\n-\n-    ConvertorT converter =\n-        [ptxAsm, inType,\n-         outType](Location loc, ConversionPatternRewriter &rewriter,\n-                  const Value &v0, const Value &v1, const Value &v2,\n-                  const Value &v3) -> SmallVector<Value> {\n-      SmallVector<Value> v = {v0, v1, v2, v3};\n-      auto ctx = rewriter.getContext();\n-      int inBitwidth = inType.getIntOrFloatBitWidth();\n-      int outBitwidth = outType.getIntOrFloatBitWidth();\n-      // first, we pack `v` into 32-bit ints\n-      int inVecWidth = 32 / inBitwidth;\n-      auto inVecTy = vec_ty(inType, inVecWidth);\n-      SmallVector<Value> inPacked(4 / inVecWidth, undef(inVecTy));\n-      for (size_t i = 0; i < 4; i++)\n-        inPacked[i / inVecWidth] = insert_element(\n-            inVecTy, inPacked[i / inVecWidth], v[i], i32_val(i % inVecWidth));\n-      for (size_t i = 0; i < inPacked.size(); i++)\n-        inPacked[i] = bitcast(inPacked[i], i32_ty);\n-\n-      // then, we run the provided inline PTX\n-      int outVecWidth = 32 / outBitwidth;\n-      int outNums = 4 / outVecWidth;\n-      PTXBuilder builder;\n-      SmallVector<PTXBuilder::Operand *> operands;\n-      for (int i = 0; i < outNums; i++)\n-        operands.push_back(builder.newOperand(\"=r\"));\n-      for (Value inVal : inPacked)\n-        operands.push_back(builder.newOperand(inVal, \"r\"));\n-      auto &ptxOp = *builder.create(ptxAsm);\n-      ptxOp(operands, /*onlyAttachMLIRArgs=*/true);\n-      auto outVecTy = vec_ty(outType, outVecWidth);\n-      SmallVector<Value> outPacked;\n-      if (outNums == 1)\n-        outPacked.push_back(builder.launch(rewriter, loc, outVecTy, false));\n-      else {\n-        auto outStructTy = struct_ty(SmallVector<Type>(outNums, outVecTy));\n-        auto outStruct = builder.launch(rewriter, loc, outStructTy, false);\n-        for (int i = 0; i < outNums; i++)\n-          outPacked.push_back(extract_val(outVecTy, outStruct, i));\n-      }\n-      // unpack the output\n-      SmallVector<Value> ret;\n-      for (size_t i = 0; i < 4; i++)\n-        ret.push_back(extract_element(outType, outPacked[i / outVecWidth],\n-                                      i32_val(i % outVecWidth)));\n-      return ret;\n-    };\n-    return converter;\n+  static SmallVector<Value>\n+  convertFp8x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const char *ptxAsm, const Value &v0, const Value &v1,\n+                       const Value &v2, const Value &v3) {\n+    auto ctx = rewriter.getContext();\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    Value fp8x4Vec = undef(fp8x4VecTy);\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v0, i32_val(0));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v1, i32_val(1));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v2, i32_val(2));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v3, i32_val(3));\n+    fp8x4Vec = bitcast(fp8x4Vec, i32_ty);\n+\n+    PTXBuilder builder;\n+    auto &ptxOp = *builder.create(ptxAsm);\n+\n+    auto *o0 = builder.newOperand(\"=r\");\n+    auto *o1 = builder.newOperand(\"=r\");\n+    auto *i = builder.newOperand(fp8x4Vec, \"r\");\n+    ptxOp({o0, o1, i}, /*onlyAttachMLIRArgs=*/true);\n+\n+    auto fp16x2VecTy = vec_ty(f16_ty, 2);\n+    auto fp16x2x2StructTy =\n+        struct_ty(SmallVector<Type>{fp16x2VecTy, fp16x2VecTy});\n+    auto fp16x2x2Struct =\n+        builder.launch(rewriter, loc, fp16x2x2StructTy, false);\n+    auto fp16x2Vec0 = extract_val(fp16x2VecTy, fp16x2x2Struct, 0);\n+    auto fp16x2Vec1 = extract_val(fp16x2VecTy, fp16x2x2Struct, 1);\n+    return {extract_element(f16_ty, fp16x2Vec0, i32_val(0)),\n+            extract_element(f16_ty, fp16x2Vec0, i32_val(1)),\n+            extract_element(f16_ty, fp16x2Vec1, i32_val(0)),\n+            extract_element(f16_ty, fp16x2Vec1, i32_val(1))};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp8E4M3x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = // WARN: subnormal (0bs0000xxx) are not handled\n+        \"{                                      \\n\"\n+        \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n+        \"prmt.b32 a0, 0, $2, 0x5040;            \\n\" // a0 = 0xf300f400\n+        \"prmt.b32 a1, 0, $2, 0x7060;            \\n\" // a1 = 0xf100f200\n+        \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // b0 = a0 & 0x7fff7fff\n+        \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n+        \"shr.b32  b0, b0, 1;                    \\n\" // b0 >>= 1\n+        \"shr.b32  b1, b1, 1;                    \\n\" // shift into fp16 position\n+        \"add.u32  b0, b0, 0x20002000;           \\n\" // b0.exp += 2**4-2**3\n+                                                    // exponent compensate = 8\n+        \"add.u32  b1, b1, 0x20002000;           \\n\" // b1 += 8<<10 | 8<<10<<16\n+        \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n+        \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n+        \"}\";\n+    return convertFp8x4ToFp16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp8E5M2x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    // exponent bias of Fp8E5M2 and Fp16 are the same\n+    auto *ptxAsm = \"{                           \\n\"\n+                   \"prmt.b32 $0, 0, $2, 0x5140; \\n\\t\"\n+                   \"prmt.b32 $1, 0, $2, 0x7362; \\n\\t\"\n+                   \"}\";\n+    return convertFp8x4ToFp16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  }\n+\n+  /* ------------------ */\n+  // FP8 -> BF16\n+  /* ------------------ */\n+  static SmallVector<Value>\n+  convertFp8x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const char *ptxAsm, const Value &v0, const Value &v1,\n+                       const Value &v2, const Value &v3) {\n+    auto ctx = rewriter.getContext();\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    Value fp8x4Vec = undef(fp8x4VecTy);\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v0, i32_val(0));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v1, i32_val(1));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v2, i32_val(2));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v3, i32_val(3));\n+    fp8x4Vec = bitcast(fp8x4Vec, i32_ty);\n+\n+    PTXBuilder builder;\n+    auto &ptxOp = *builder.create(ptxAsm);\n+\n+    auto *o0 = builder.newOperand(\"=r\");\n+    auto *o1 = builder.newOperand(\"=r\");\n+    auto *i = builder.newOperand(fp8x4Vec, \"r\");\n+    ptxOp({o0, o1, i}, /* onlyAttachMLIRArgs */ true);\n+\n+    auto bf16x2VecTy = vec_ty(i16_ty, 2);\n+    auto bf16x2x2StructTy =\n+        struct_ty(SmallVector<Type>{bf16x2VecTy, bf16x2VecTy});\n+    auto bf16x2x2Struct =\n+        builder.launch(rewriter, loc, bf16x2x2StructTy, false);\n+    auto bf16x2Vec0 = extract_val(bf16x2VecTy, bf16x2x2Struct, 0);\n+    auto bf16x2Vec1 = extract_val(bf16x2VecTy, bf16x2x2Struct, 1);\n+    return {extract_element(i16_ty, bf16x2Vec0, i32_val(0)),\n+            extract_element(i16_ty, bf16x2Vec0, i32_val(1)),\n+            extract_element(i16_ty, bf16x2Vec1, i32_val(0)),\n+            extract_element(i16_ty, bf16x2Vec1, i32_val(1))};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp8E4M3x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = // WARN: subnormal (0bs0000xxx) are not handled\n+        \"{                                      \\n\"\n+        \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n+        \"prmt.b32 a0, 0, $2, 0x5040;            \\n\" // a0 = 0xf300f400\n+        \"prmt.b32 a1, 0, $2, 0x7060;            \\n\" // a1 = 0xf100f200\n+        \"and.b32 b0, a0, 0x7fff7fff;            \\n\" // b0 = a0 & 0x7fff7fff\n+        \"and.b32 b1, a1, 0x7fff7fff;            \\n\" // (strip sign)\n+        \"shr.b32 b0, b0, 4;                     \\n\" // b0 >>= 4\n+        \"shr.b32 b1, b1, 4;                     \\n\" // shift into fp16 position\n+        \"add.u32 b0, b0, 0x3c003c00;            \\n\" // b0.exp += 2**7-2**3\n+                                                    // exponent compensate = 120\n+        \"add.u32 b1, b1, 0x3c003c00;            \\n\" // b1 += 120<<7 | 120<<7<<16\n+        \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n+        \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n+        \"}\";\n+    return convertFp8x4ToBf16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  };\n+\n+  static SmallVector<Value>\n+  convertFp8E5M2x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = // WARN: subnormal (0bs00000xx) are not handled\n+        \"{                                      \\n\"\n+        \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n+        \"prmt.b32 a0, 0, $2, 0x5140;            \\n\" // a0 = 0xf300f400\n+        \"prmt.b32 a1, 0, $2, 0x7362;            \\n\" // a1 = 0xf100f200\n+        \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // b0 = a0 & 0x7fff7fff\n+        \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n+        \"shr.b32  b0, b0, 3;                    \\n\" // b0 >>= 3\n+        \"shr.b32  b1, b1, 3;                    \\n\" // shift into bf16 position\n+        \"add.u32  b0, b0, 0x38003800;           \\n\" // b0.exp += 2**7-2**4\n+                                                    // exponent compensate = 112\n+        \"add.u32  b1, b1, 0x38003800;           \\n\" // b1 += 112<<7 | 112<<7<<16\n+        \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n+        \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n+        \"}\";\n+    return convertFp8x4ToBf16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  };\n+\n+  /* ------------------ */\n+  // FP16 -> FP8\n+  /* ------------------ */\n+\n+  static SmallVector<Value>\n+  convertFp16x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const char *ptxAsm, const Value &v0, const Value &v1,\n+                       const Value &v2, const Value &v3) {\n+    auto fp16x2VecTy = vec_ty(f16_ty, 2);\n+    Value fp16x2Vec0 = undef(fp16x2VecTy);\n+    Value fp16x2Vec1 = undef(fp16x2VecTy);\n+    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v0, i32_val(0));\n+    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v1, i32_val(1));\n+    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v2, i32_val(0));\n+    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v3, i32_val(1));\n+    fp16x2Vec0 = bitcast(fp16x2Vec0, i32_ty);\n+    fp16x2Vec1 = bitcast(fp16x2Vec1, i32_ty);\n+\n+    PTXBuilder builder;\n+    auto &ptxOp = *builder.create(ptxAsm);\n+\n+    auto *o = builder.newOperand(\"=r\");\n+    auto *i0 = builder.newOperand(fp16x2Vec0, \"r\");\n+    auto *i1 = builder.newOperand(fp16x2Vec1, \"r\");\n+    ptxOp({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n+\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n+    return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp16x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = // WARN: subnormal Fp8s are not handled\n+        \"{                                      \\n\"\n+        \".reg .b32 a<2>, b<2>;                  \\n\" // see Fp8E4M3x4ToFp16x4\n+        \"sub.u32 a0, $1, 0x20002000;            \\n\" // a0 = input0 - 0x20002000\n+                                                    // (compensate offset)\n+        \"sub.u32 a1, $2, 0x20002000;            \\n\" // a1 = input1 - 0x20002000\n+                                                    // (8 << 10 | 8 << 10 << 16)\n+        \"shl.b32 a0, a0, 1;                     \\n\" // a0 <<= 1\n+        \"shl.b32 a1, a1, 1;                     \\n\" // shift into fp8e4 position\n+        \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // a0 &= 0x7fff7fff\n+        \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n+        \"add.u32 a0, a0, 0x00800080;            \\n\" // a0 += 0x00800080\n+        \"add.u32 a1, a1, 0x00800080;            \\n\" // (round to nearest)\n+        \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\" // b0 = a0|(0x80008000&in0)\n+        \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\" // (restore sign)\n+        \"prmt.b32 $0, b0, b1, 0x7531;           \\n\" // output = b1b0\n+        \"}\";\n+    return convertFp16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp16x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm =\n+        \"{                            \\n\"\n+        \".reg .b32 a<2>;              \\n\"\n+        \"and.b32 a0, $1, 0x7fff7fff;  \\n\"           // a0 &= 0x7fff7fff\n+        \"and.b32 a1, $2, 0x7fff7fff;  \\n\"           // (strip sign)\n+        \"add.u32 a0, a0, 0x00800080;  \\n\"           // a0 += 0x00800080\n+        \"add.u32 a1, a1, 0x00800080;  \\n\"           // (round to nearest)\n+        \"lop3.b32 a0, $1, 0x80008000, a0, 0xea; \\n\" // a0 = a0|(0x80008000&in0)\n+        \"lop3.b32 a1, $2, 0x80008000, a1, 0xea; \\n\" // (restore sign)\n+        \"prmt.b32 $0, a0, a1, 0x7531; \\n\\t\"         // output = a1a0\n+        \"}\";\n+    return convertFp16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  }\n+\n+  /* ------------------ */\n+  // FP32 -> FP8\n+  /* ------------------ */\n+\n+  static SmallVector<Value>\n+  convertFp32x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n+    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n+    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n+    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n+    return convertFp16x4ToFp8E4M3x4(loc, rewriter, c0, c1, c2, c3);\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp32x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n+    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n+    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n+    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n+    return convertFp16x4ToFp8E5M2x4(loc, rewriter, c0, c1, c2, c3);\n+  }\n+\n+  /* ------------------ */\n+  // BF16 -> FP8\n+  /* ------------------ */\n+\n+  static SmallVector<Value>\n+  convertBf16x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const char *ptxAsm, const Value &v0, const Value &v1,\n+                       const Value &v2, const Value &v3) {\n+    auto bf16x2VecTy = vec_ty(i16_ty, 2);\n+    Value bf16x2Vec0 = undef(bf16x2VecTy);\n+    Value bf16x2Vec1 = undef(bf16x2VecTy);\n+    bf16x2Vec0 = insert_element(bf16x2VecTy, bf16x2Vec0, v0, i32_val(0));\n+    bf16x2Vec0 = insert_element(bf16x2VecTy, bf16x2Vec0, v1, i32_val(1));\n+    bf16x2Vec1 = insert_element(bf16x2VecTy, bf16x2Vec1, v2, i32_val(0));\n+    bf16x2Vec1 = insert_element(bf16x2VecTy, bf16x2Vec1, v3, i32_val(1));\n+    bf16x2Vec0 = bitcast(bf16x2Vec0, i32_ty);\n+    bf16x2Vec1 = bitcast(bf16x2Vec1, i32_ty);\n+\n+    PTXBuilder builder;\n+    auto &ptxOp = *builder.create(ptxAsm);\n+\n+    auto *o = builder.newOperand(\"=r\");\n+    auto *i0 = builder.newOperand(bf16x2Vec0, \"r\");\n+    auto *i1 = builder.newOperand(bf16x2Vec1, \"r\");\n+    ptxOp({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n+\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n+    return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n+  }\n+\n+  static SmallVector<Value>\n+  convertBf16x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = // bf16 is clamped firstly to fp8 min/max\n+        \"{                                           \\n\" // bf16=fp8>>4 + 120<<7\n+        \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\" // fp8_min = 0b00000000\n+        \".reg .u32 fp8_min, fp8_max, rn_;            \\n\" // fp8_max = 0b11111111\n+        \"mov.u32 fp8_min, 0x3c003c00;                \\n\" // so bf16_min = 0x3c00\n+        \"mov.u32 fp8_max, 0x43f043f0;                \\n\" // so bf16_max = 0x43f0\n+        \"mov.u32 rn_, 0x80008;                       \\n\" // round to nearest\n+        \"and.b32 sign0, $1, 0x80008000;              \\n\" // sign0=in0&0x80008000\n+        \"and.b32 sign1, $2, 0x80008000;              \\n\" // (store sign)\n+        \"prmt.b32 sign, sign0, sign1, 0x7531;        \\n\"\n+        \"and.b32 nosign0, $1, 0x7fff7fff;            \\n\" // nosign0=in0&0x7fff7fff\n+        \"and.b32 nosign1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n+\n+        // nosign = clamp(nosign, min, max)\n+        \".reg .u32 nosign_0_<2>, nosign_1_<2>;       \\n\"\n+        \"and.b32 nosign_0_0, nosign0, 0xffff0000;    \\n\"\n+        \"max.u32 nosign_0_0, nosign_0_0, 0x3c000000; \\n\"\n+        \"min.u32 nosign_0_0, nosign_0_0, 0x43f00000; \\n\"\n+        \"and.b32 nosign_0_1, nosign0, 0x0000ffff;    \\n\"\n+        \"max.u32 nosign_0_1, nosign_0_1, 0x3c00;     \\n\"\n+        \"min.u32 nosign_0_1, nosign_0_1, 0x43f0;     \\n\"\n+        \"or.b32 nosign0, nosign_0_0, nosign_0_1;     \\n\"\n+        \"and.b32 nosign_1_0, nosign1, 0xffff0000;    \\n\"\n+        \"max.u32 nosign_1_0, nosign_1_0, 0x3c000000; \\n\"\n+        \"min.u32 nosign_1_0, nosign_1_0, 0x43f00000; \\n\"\n+        \"and.b32 nosign_1_1, nosign1, 0x0000ffff;    \\n\"\n+        \"max.u32 nosign_1_1, nosign_1_1, 0x3c00;     \\n\"\n+        \"min.u32 nosign_1_1, nosign_1_1, 0x43f0;     \\n\"\n+        \"or.b32 nosign1, nosign_1_0, nosign_1_1;     \\n\"\n+\n+        \"add.u32 nosign0, nosign0, rn_;              \\n\" // nosign0 += rn_\n+        \"add.u32 nosign1, nosign1, rn_;              \\n\" // (round to nearest)\n+        \"sub.u32 nosign0, nosign0, 0x3c003c00;       \\n\" // nosign0-=0x3c003c00\n+        \"sub.u32 nosign1, nosign1, 0x3c003c00;       \\n\" // (compensate offset)\n+        \"shr.u32 nosign0, nosign0, 4;                \\n\" // nosign0 >>= 4\n+        \"shr.u32 nosign1, nosign1, 4;                \\n\" // shift into to fp8e4\n+        \"prmt.b32 nosign, nosign0, nosign1, 0x6420;  \\n\" // nosign0 = 0x00f100f2\n+                                                         // nosign1 = 0x00f300f4\n+                                                         // nosign = 0xf3f4f1f2\n+        \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n+        \"}\";\n+    return convertBf16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  };\n+\n+  static SmallVector<Value>\n+  convertBf16x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto *ptxAsm = // bf16 is clamped firstly to fp8 min/max\n+        \"{                                           \\n\" // bf16=fp8>>3 + 112<<7\n+        \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\" // fp8_min = 0b00000000\n+        \".reg .u32 fp8_min, fp8_max, rn_;            \\n\" // fp8_max = 0b11111111\n+        \"mov.u32 fp8_min, 0x38003800;                \\n\" // so bf16_min = 0x3800\n+        \"mov.u32 fp8_max, 0x57e057e0;                \\n\" // so bf16_max = 0x57e0\n+        \"mov.u32 rn_, 0x00100010;                    \\n\" // round to nearest\n+        \"and.b32 sign0, $1, 0x80008000;              \\n\" // sign0=in0&0x80008000\n+        \"and.b32 sign1, $2, 0x80008000;              \\n\" // (store sign)\n+        \"prmt.b32 sign, sign0, sign1, 0x7531;        \\n\"\n+        \"and.b32 nosign0, $1, 0x7fff7fff;            \\n\" // nosign0=in0&0x7fff7fff\n+        \"and.b32 nosign1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n+\n+        // nosign = clamp(nosign, min, max)\n+        \".reg .u32 nosign_0_<2>, nosign_1_<2>;       \\n\"\n+        \"and.b32 nosign_0_0, nosign0, 0xffff0000;    \\n\"\n+        \"max.u32 nosign_0_0, nosign_0_0, 0x38000000; \\n\"\n+        \"min.u32 nosign_0_0, nosign_0_0, 0x57e00000; \\n\"\n+        \"and.b32 nosign_0_1, nosign0, 0x0000ffff;    \\n\"\n+        \"max.u32 nosign_0_1, nosign_0_1, 0x3800;     \\n\"\n+        \"min.u32 nosign_0_1, nosign_0_1, 0x57e0;     \\n\"\n+        \"or.b32 nosign0, nosign_0_0, nosign_0_1;     \\n\"\n+        \"and.b32 nosign_1_0, nosign1, 0xffff0000;    \\n\"\n+        \"max.u32 nosign_1_0, nosign_1_0, 0x38000000; \\n\"\n+        \"min.u32 nosign_1_0, nosign_1_0, 0x57e00000; \\n\"\n+        \"and.b32 nosign_1_1, nosign1, 0x0000ffff;    \\n\"\n+        \"max.u32 nosign_1_1, nosign_1_1, 0x3800;     \\n\"\n+        \"min.u32 nosign_1_1, nosign_1_1, 0x57e0;     \\n\"\n+        \"or.b32 nosign1, nosign_1_0, nosign_1_1;     \\n\"\n+\n+        \"add.u32 nosign0, nosign0, rn_;              \\n\" // nosign0 += rn_\n+        \"add.u32 nosign1, nosign1, rn_;              \\n\" // (round to nearest)\n+        \"sub.u32 nosign0, nosign0, 0x38003800;       \\n\" // nosign0-=0x38003800\n+        \"sub.u32 nosign1, nosign1, 0x38003800;       \\n\" // (compensate offset)\n+        \"shl.b32 nosign0, nosign0, 3;                \\n\" // nosign0 <<= 3\n+        \"shl.b32 nosign1, nosign1, 3;                \\n\" // shift into to fp8e4\n+        \"prmt.b32 nosign, nosign0, nosign1, 0x7531;  \\n\" // nosign0 = 0xf100f200\n+                                                         // nosign1 = 0xf300f400\n+                                                         // nosign = 0xf3f4f1f2\n+        \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n+        \"}\";\n+    return convertBf16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+  }\n+\n+  /* ------------------ */\n+  // FP8 -> FP32\n+  /* ------------------ */\n+\n+  static SmallVector<Value>\n+  convertFp8E4M3x4ToFp32x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto fp16Values = convertFp8E4M3x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n+    return {convertFp16ToFp32(loc, rewriter, fp16Values[0]),\n+            convertFp16ToFp32(loc, rewriter, fp16Values[1]),\n+            convertFp16ToFp32(loc, rewriter, fp16Values[2]),\n+            convertFp16ToFp32(loc, rewriter, fp16Values[3])};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp8E5M2x4ToFp32x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto fp16Values = convertFp8E5M2x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n+    return {rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[0]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[1]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[2]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[3])};\n+  }\n+\n+  //\n+\n+  static SmallVector<Value>\n+  convertFp8E4M3x4ToFp64x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto fp16Values = convertFp8E4M3x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n+    return {rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[0]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[1]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[2]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[3])};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp64x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n+                           const Value &v0, const Value &v1, const Value &v2,\n+                           const Value &v3) {\n+    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n+    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n+    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n+    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n+    return convertFp16x4ToFp8E4M3x4(loc, rewriter, c0, c1, c2, c3);\n   }\n \n   static Value convertBf16ToFp32(Location loc,\n@@ -463,51 +620,54 @@ struct FpToFpOpConversion\n   }\n \n   ConvertorT getConversionFunc(Type srcTy, Type dstTy) const {\n-    auto F8E4M3B15TyID = TypeID::get<mlir::Float8E4M3B11FNUZType>();\n-    auto F8E4M3TyID = TypeID::get<mlir::Float8E4M3FNUZType>();\n+    auto F8E4M3TyID = TypeID::get<mlir::Float8E4M3FNType>();\n     auto F8E5M2TyID = TypeID::get<mlir::Float8E5M2Type>();\n     auto F16TyID = TypeID::get<mlir::Float16Type>();\n     auto BF16TyID = TypeID::get<mlir::BFloat16Type>();\n     auto F32TyID = TypeID::get<mlir::Float32Type>();\n     auto F64TyID = TypeID::get<mlir::Float64Type>();\n-    static DenseMap<std::pair<TypeID, TypeID>, std::string> srcMap = {\n+    static DenseMap<std::pair<TypeID, TypeID>, ConvertorT> convertorMap = {\n         // F8 -> F16\n-        {{F8E4M3B15TyID, F16TyID}, Fp8E4M3B15_to_Fp16},\n-        {{F8E4M3TyID, F16TyID}, Fp8E4M3_to_Fp16},\n-        {{F8E5M2TyID, F16TyID}, Fp8E5M2_to_Fp16},\n+        {{F8E4M3TyID, F16TyID}, convertFp8E4M3x4ToFp16x4},\n+        {{F8E5M2TyID, F16TyID}, convertFp8E5M2x4ToFp16x4},\n         // F16 -> F8\n-        {{F16TyID, F8E4M3B15TyID}, Fp16_to_Fp8E4M3B15},\n-        {{F16TyID, F8E4M3TyID}, Fp16_to_Fp8E4M3},\n-        {{F16TyID, F8E5M2TyID}, Fp16_to_Fp8E5M2},\n+        {{F16TyID, F8E4M3TyID}, convertFp16x4ToFp8E4M3x4},\n+        {{F16TyID, F8E5M2TyID}, convertFp16x4ToFp8E5M2x4},\n         // F8 -> BF16\n-        {{F8E4M3TyID, BF16TyID}, Fp8E4M3_to_Bf16},\n-        {{F8E5M2TyID, BF16TyID}, Fp8E5M2_to_Bf16},\n+        {{F8E4M3TyID, BF16TyID}, convertFp8E4M3x4ToBf16x4},\n+        {{F8E5M2TyID, BF16TyID}, convertFp8E5M2x4ToBf16x4},\n         // BF16 -> F8\n-        {{BF16TyID, F8E4M3TyID}, Bf16_to_Fp8E4M3},\n-        {{BF16TyID, F8E5M2TyID}, Bf16_to_Fp8E5M2},\n+        {{BF16TyID, F8E4M3TyID}, convertBf16x4ToFp8E4M3x4},\n+        {{BF16TyID, F8E5M2TyID}, convertBf16x4ToFp8E5M2x4},\n+        // F8 -> F32\n+        {{F8E4M3TyID, F32TyID}, convertFp8E4M3x4ToFp32x4},\n+        {{F8E5M2TyID, F32TyID}, convertFp8E5M2x4ToFp32x4},\n+        // F32 -> F8\n+        {{F32TyID, F8E4M3TyID}, convertFp32x4ToFp8E4M3x4},\n+        {{F32TyID, F8E5M2TyID}, convertFp32x4ToFp8E5M2x4},\n     };\n \n     std::pair<TypeID, TypeID> key = {srcTy.getTypeID(), dstTy.getTypeID()};\n-    if (srcMap.count(key) == 0) {\n+    if (convertorMap.count(key) == 0) {\n       llvm::errs() << \"Unsupported conversion from \" << srcTy << \" to \" << dstTy\n                    << \"\\n\";\n       llvm_unreachable(\"\");\n     }\n-    return makeConverterFromPtx(srcMap.lookup(key),\n-                                getTypeConverter()->convertType(srcTy),\n-                                getTypeConverter()->convertType(dstTy));\n+    return convertorMap.lookup(key);\n   }\n \n   LogicalResult\n   matchAndRewrite(triton::FpToFpOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n+    // llvm::outs() << 0 << \"\\n\";\n     auto srcTensorType = op.getFrom().getType().cast<mlir::RankedTensorType>();\n     auto dstTensorType =\n         op.getResult().getType().cast<mlir::RankedTensorType>();\n-    auto srcElementType = srcTensorType.getElementType();\n-    auto dstElementType = dstTensorType.getElementType();\n     auto loc = op->getLoc();\n     // check that the number of elements is divisible by 4\n+    // Get convertor\n+    auto cvtFunc = getConversionFunc(srcTensorType.getElementType(),\n+                                     dstTensorType.getElementType());\n     // Unpack value\n     auto inVals = getTypeConverter()->unpackLLElements(loc, adaptor.getFrom(),\n                                                        rewriter, srcTensorType);\n@@ -518,19 +678,9 @@ struct FpToFpOpConversion\n     auto elems = inVals.size();\n     assert(elems % 4 == 0 &&\n            \"FP8 casting only support tensors with 4-aligned sizes\");\n-    bool isFP32src = srcElementType.isF32();\n-    bool isFP32dst = dstElementType.isF32();\n-    auto cvtFunc = getConversionFunc(isFP32src ? f16_ty : srcElementType,\n-                                     isFP32dst ? f16_ty : dstElementType);\n-    if (isFP32src)\n-      for (Value &v : inVals)\n-        v = convertFp32ToFp16(loc, rewriter, v);\n     for (size_t i = 0; i < elems; i += 4)\n       outVals.append(cvtFunc(loc, rewriter, inVals[i], inVals[i + 1],\n                              inVals[i + 2], inVals[i + 3]));\n-    if (isFP32dst)\n-      for (Value &v : outVals)\n-        v = convertFp16ToFp32(loc, rewriter, v);\n     // Pack values\n     assert(outVals.size() == elems);\n     outVals = reorderValues(outVals, srcTensorType, dstTensorType);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM/Fp8E4M3B15.cpp", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM/Fp8E5M2.cpp", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -357,6 +357,7 @@ struct StoreOpConversion\n               .o(\"wb\", op.getCache() == triton::CacheModifier::WB)\n               .o(\"cg\", op.getCache() == triton::CacheModifier::CG)\n               .o(\"cs\", op.getCache() == triton::CacheModifier::CS)\n+              .o(\"wt\", op.getCache() == triton::CacheModifier::WT)\n               .o(\"L1::evict_first\",\n                  op.getEvict() == triton::EvictionPolicy::EVICT_FIRST)\n               .o(\"L1::evict_last\","}, {"filename": "lib/Conversion/TritonGPUToLLVM/ScanOpToLLVM.cpp", "status": "added", "additions": 307, "deletions": 0, "changes": 307, "file_content_changes": "@@ -0,0 +1,307 @@\n+#include \"ScanOpToLLVM.h\"\n+#include \"TritonGPUToLLVMBase.h\"\n+#include \"triton/Analysis/Utility.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::LLVM::delinearize;\n+using ::mlir::LLVM::linearize;\n+using ::mlir::LLVM::shflUpSync;\n+using ::mlir::LLVM::storeShared;\n+\n+// Apply the region of the scan op to the acc and cur values and update acc\n+// inplace with the result.\n+static void accumulate(ConversionPatternRewriter &rewriter, Region &combineOp,\n+                       Value &acc, Value cur) {\n+  if (!acc) {\n+    acc = cur;\n+    return;\n+  }\n+  // Create a new copy of the reduce block, and inline it\n+  Block *currentBlock = rewriter.getBlock();\n+  Region &parent = *currentBlock->getParent();\n+  rewriter.cloneRegionBefore(combineOp, &parent.front());\n+  auto &newScan = parent.front();\n+  auto returnOp = dyn_cast<triton::ScanReturnOp>(newScan.getTerminator());\n+  llvm::SmallVector<Value> combineArgs = {acc, cur};\n+  rewriter.inlineBlockBefore(&newScan, &*rewriter.getInsertionPoint(),\n+                             combineArgs);\n+  auto results = returnOp.getResult();\n+  acc = results[0];\n+  // Delete the terminator, which is no longer used\n+  rewriter.eraseOp(returnOp);\n+}\n+\n+// Scan a contiguous elements within a thread and update `srcValues` in place.\n+static void scanThreadContiguousElements(SmallVector<Value> &srcValues,\n+                                         ConversionPatternRewriter &rewriter,\n+                                         ScanLoweringHelper &helper) {\n+  // TODO: this assumes that axis is the fastest moving dimension. We should\n+  // relax that.\n+  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThreads();\n+  // Loop through the blocks of contiguous elements.\n+  for (unsigned j = 0; j < srcValues.size(); j += scanElementsPerThreads) {\n+    // Reset the accumulator at the beginning of each block of contiguous\n+    // elements.\n+    Value acc;\n+    // Loop through the contiguous elements.\n+    for (unsigned i = 0; i < scanElementsPerThreads; ++i) {\n+      accumulate(rewriter, helper.getCombineOp(), acc, srcValues[i + j]);\n+      srcValues[i + j] = acc;\n+    }\n+  }\n+}\n+\n+// Apply a scan across threads of the warp for the last element of each\n+// contiguous group of elements.\n+static void warpScan(SmallVector<Value> &srcValues,\n+                     ConversionPatternRewriter &rewriter,\n+                     ScanLoweringHelper &helper, Value laneId) {\n+  Location loc = helper.getLoc();\n+  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThreads();\n+  for (unsigned j = scanElementsPerThreads - 1; j < srcValues.size();\n+       j += scanElementsPerThreads) {\n+    Value acc = srcValues[j];\n+    unsigned scanDim = helper.getAxisNumThreadsPerWarp();\n+    // Reduce within warps.\n+    for (unsigned i = 1; i <= scanDim / 2; i = i << 1) {\n+      Value shfl = shflUpSync(loc, rewriter, acc, i);\n+      Value tempAcc = acc;\n+      accumulate(rewriter, helper.getCombineOp(), tempAcc, shfl);\n+      Value mask = icmp_slt(laneId, i32_val(i));\n+      acc = select(mask, acc, tempAcc);\n+    }\n+    srcValues[j] = acc;\n+  }\n+}\n+\n+// For each set of contiguous elements within a thread we store the partial\n+// reduction into shared memory. Each parallel scan and each warp will store its\n+// own partial reductions. The shared memory is organized as follow:\n+//          -----------------------------------------------------------------\n+// chunk 0: | acc[0] warp 0 | acc[1] warp 0 | acc[0] warp 1 | acc[1] warp 1 |\n+// chunk 1: | acc[0] warp 0 | acc[1] warp 0 | acc[0] warp 1 | acc[1] warp 1 |\n+static void storeWarpAccumulator(SmallVector<Value> &srcValues,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 ScanLoweringHelper &helper, Value laneId,\n+                                 Value warpId, Value baseSharedMemPtr,\n+                                 Value parallelLaneId) {\n+  Location loc = helper.getLoc();\n+  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThreads();\n+  unsigned scanDim = helper.getAxisNumThreadsPerWarp();\n+  unsigned numParallelLane = helper.getNonAxisNumThreadsPerCTA();\n+  unsigned numWarps = helper.getAxisNumWarps();\n+  unsigned chunkId = 0;\n+  for (unsigned j = scanElementsPerThreads - 1; j < srcValues.size();\n+       j += scanElementsPerThreads, ++chunkId) {\n+    Value lastElement = srcValues[j];\n+    Value mask = icmp_eq(laneId, i32_val(scanDim - 1));\n+    Value index = add(parallelLaneId, mul(warpId, i32_val(numParallelLane)));\n+    index = add(index, i32_val(chunkId * numParallelLane * numWarps));\n+    Value writePtr = gep(baseSharedMemPtr.getType(), baseSharedMemPtr, index);\n+    storeShared(rewriter, loc, writePtr, lastElement, mask);\n+  }\n+}\n+\n+// Read the partial reductions from shared memory from each chunk of contiguous\n+// elements for each warp and parallel scan. Then combine the partial reduction\n+// with the right elements. Within a given contiguous element chunk we update\n+// all the elements by accumulating the value from the last element of the\n+// reduced value from the previous lane.\n+static void AddPartialReduce(SmallVector<Value> &srcValues,\n+                             ConversionPatternRewriter &rewriter,\n+                             ScanLoweringHelper &helper, Value sharedMemoryPtr,\n+                             Value warpId, Value laneId, Value parallelLaneId) {\n+  Location loc = helper.getLoc();\n+  unsigned numParallelLane = helper.getNonAxisNumThreadsPerCTA();\n+  unsigned numWarps = helper.getAxisNumWarps();\n+  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThreads();\n+  unsigned parallelElementsPerThread = helper.getNonAxisNumElementsPerThread();\n+  Value maskFirstWarp = icmp_eq(warpId, i32_val(0));\n+  Value maskFirstLane = icmp_eq(laneId, i32_val(0));\n+  Value maskFirstThread = and_(maskFirstWarp, maskFirstLane);\n+  struct Accumulator {\n+    Value acc;\n+    Value maskedAcc;\n+  };\n+  unsigned numScanBlocks = helper.getAxisNumBlocks();\n+  unsigned numParallelBlocks = helper.getNonAxisNumBlocks();\n+  assert(numScanBlocks * numParallelBlocks * parallelElementsPerThread *\n+             scanElementsPerThreads ==\n+         srcValues.size());\n+  SmallVector<Accumulator> accumulators(numParallelBlocks *\n+                                        parallelElementsPerThread);\n+  unsigned chunkId = 0;\n+  for (unsigned parallelBlockId = 0; parallelBlockId < numParallelBlocks;\n+       ++parallelBlockId) {\n+    for (unsigned scanBlockId = 0; scanBlockId < numScanBlocks; ++scanBlockId) {\n+      for (unsigned parallelElementId = 0;\n+           parallelElementId < parallelElementsPerThread; ++parallelElementId) {\n+        unsigned accumulatorIndex =\n+            parallelElementId + parallelBlockId * parallelElementsPerThread;\n+        Accumulator &accumulator = accumulators[accumulatorIndex];\n+        for (unsigned i = 0; i < numWarps; ++i) {\n+          Value index = add(parallelLaneId, i32_val(numParallelLane *\n+                                                    (i + chunkId * numWarps)));\n+          Value ptr = gep(sharedMemoryPtr.getType(), sharedMemoryPtr, index);\n+          Value partialReduce = load(ptr);\n+          if (!accumulator.acc) {\n+            accumulator.acc = partialReduce;\n+            accumulator.maskedAcc = partialReduce;\n+            continue;\n+          }\n+          accumulate(rewriter, helper.getCombineOp(), accumulator.acc,\n+                     partialReduce);\n+          Value mask = icmp_slt(warpId, i32_val(i + 1));\n+          accumulator.maskedAcc =\n+              select(mask, accumulator.maskedAcc, accumulator.acc);\n+        }\n+        unsigned lastElementIndex =\n+            chunkId * scanElementsPerThreads + scanElementsPerThreads - 1;\n+        Value temp = srcValues[lastElementIndex];\n+        accumulate(rewriter, helper.getCombineOp(), temp,\n+                   accumulator.maskedAcc);\n+        if (scanBlockId == 0) {\n+          // For the first warp and first chunk we don't have anything to\n+          // accumulate.\n+          temp = select(maskFirstWarp, srcValues[lastElementIndex], temp);\n+        }\n+        srcValues[lastElementIndex] = temp;\n+\n+        // Update the rest of the contiguous elements.\n+        Value lastElement =\n+            shflUpSync(loc, rewriter, srcValues[lastElementIndex], 1);\n+        lastElement = select(maskFirstLane, accumulator.maskedAcc, lastElement);\n+        for (unsigned i = 1; i < scanElementsPerThreads; ++i) {\n+          Value laneValue = srcValues[lastElementIndex - i];\n+          accumulate(rewriter, helper.getCombineOp(), laneValue, lastElement);\n+          if (scanBlockId == 0) {\n+            // For the first warp and first chunk we don't have anything to\n+            // accumulate.\n+            laneValue = select(maskFirstThread, srcValues[lastElementIndex - i],\n+                               laneValue);\n+          }\n+          srcValues[lastElementIndex - i] = laneValue;\n+        }\n+        // For the next chunk start back from the value containing the\n+        // accumulated value of all the warps.\n+        accumulator.maskedAcc = accumulator.acc;\n+        chunkId++;\n+      }\n+    }\n+  }\n+}\n+\n+namespace {\n+struct ScanOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::ScanOp> {\n+public:\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::ScanOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::ScanOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    if (succeeded(emitFastScan(op, adaptor, rewriter)))\n+      return success();\n+    return failure();\n+  }\n+\n+private:\n+  std::tuple<Value, Value, Value>\n+  getDelinearizedIds(ConversionPatternRewriter &rewriter,\n+                     ScanLoweringHelper &helper) const;\n+  LogicalResult emitFastScan(triton::ScanOp op, triton::ScanOpAdaptor adaptor,\n+                             ConversionPatternRewriter &rewriter) const;\n+};\n+\n+// Break up the threadId into lane and warp id along the scan dimension and\n+// compute a flat id for the parallel dimensions.\n+std::tuple<Value, Value, Value>\n+ScanOpConversion::getDelinearizedIds(ConversionPatternRewriter &rewriter,\n+                                     ScanLoweringHelper &helper) const {\n+  auto loc = helper.getLoc();\n+  unsigned axis = helper.getAxis();\n+  auto srcEncoding = helper.getEncoding();\n+\n+  Value threadId = getThreadId(rewriter, loc);\n+  Value warpSize = i32_val(32);\n+  Value warpId = udiv(threadId, warpSize);\n+  Value laneId = urem(threadId, warpSize);\n+\n+  auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcEncoding);\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n+  auto order = triton::gpu::getOrder(srcEncoding);\n+  SmallVector<Value> multiDimLaneId =\n+      delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n+  SmallVector<Value> multiDimWarpId =\n+      delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n+\n+  Value laneIdAxis = multiDimLaneId[axis];\n+  Value warpIdAxis = multiDimWarpId[axis];\n+\n+  multiDimLaneId[axis] = i32_val(0);\n+  threadsPerWarp[axis] = 1;\n+  Value laneIdParallel =\n+      linearize(rewriter, loc, multiDimLaneId, threadsPerWarp, order);\n+  multiDimWarpId[axis] = i32_val(0);\n+  warpsPerCTA[axis] = 1;\n+  Value warpIdParallel =\n+      linearize(rewriter, loc, multiDimWarpId, warpsPerCTA, order);\n+  Value flatIdParallel =\n+      add(laneIdParallel,\n+          mul(warpIdParallel, i32_val(helper.getNonAxisNumThreadsPerWarp())));\n+  return std::make_tuple(laneIdAxis, warpIdAxis, flatIdParallel);\n+}\n+\n+// Lowering using warp shuffle operations to do warp level scan.\n+LogicalResult\n+ScanOpConversion::emitFastScan(triton::ScanOp op, triton::ScanOpAdaptor adaptor,\n+                               ConversionPatternRewriter &rewriter) const {\n+  ScanLoweringHelper helper(op);\n+  auto loc = helper.getLoc();\n+  if (!helper.isSupported())\n+    return failure();\n+\n+  auto [laneIdAxis, warpIdAxis, flatIdParallel] =\n+      getDelinearizedIds(rewriter, helper);\n+  auto input = adaptor.getOperands()[0];\n+  auto type = op.getOperand(0).getType().cast<RankedTensorType>();\n+  SmallVector<Value> srcValues =\n+      getTypeConverter()->unpackLLElements(loc, input, rewriter, type);\n+\n+  // Scan contigous elements in a thread and update `srcValues`.\n+  scanThreadContiguousElements(srcValues, rewriter, helper);\n+  // Apply warp level scan to the last element of each chunk of contiguous\n+  // elements.\n+  warpScan(srcValues, rewriter, helper, laneIdAxis);\n+\n+  // Store the partial reducing for each warp into shared memory.\n+  Type elemPtrTys = LLVM::LLVMPointerType::get(srcValues[0].getType(), 3);\n+  Value baseSharedMemPtr = bitcast(\n+      getSharedMemoryBase(loc, rewriter, op.getOperation()), elemPtrTys);\n+  storeWarpAccumulator(srcValues, rewriter, helper, laneIdAxis, warpIdAxis,\n+                       baseSharedMemPtr, flatIdParallel);\n+  barrier();\n+  // Read back the partial reduction of each warp and accumulate them based on\n+  // warpId. Then update each chunk of contiguous elements by adding the\n+  // accumulated value from the previous lane.\n+  AddPartialReduce(srcValues, rewriter, helper, baseSharedMemPtr, warpIdAxis,\n+                   laneIdAxis, flatIdParallel);\n+\n+  Value results = getTypeConverter()->packLLElements(loc, srcValues, rewriter,\n+                                                     input.getType());\n+  rewriter.replaceOp(op, results);\n+  return success();\n+}\n+} // namespace\n+\n+void populateScanOpToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    ModuleAllocation &allocation,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit) {\n+  patterns.add<ScanOpConversion>(typeConverter, allocation, indexCacheInfo,\n+                                 benefit);\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ScanOpToLLVM.h", "status": "added", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "@@ -0,0 +1,15 @@\n+#ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_SCAN_OP_H\n+#define TRITON_CONVERSION_TRITONGPU_TO_LLVM_SCAN_OP_H\n+\n+#include \"TritonGPUToLLVMBase.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+void populateScanOpToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    ModuleAllocation &allocation,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit);\n+\n+#endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -26,6 +26,7 @@\n #include \"ElementwiseOpToLLVM.h\"\n #include \"LoadStoreOpToLLVM.h\"\n #include \"ReduceOpToLLVM.h\"\n+#include \"ScanOpToLLVM.h\"\n #include \"TritonGPUToLLVM.h\"\n #include \"TypeConverter.h\"\n #include \"ViewOpToLLVM.h\"\n@@ -372,6 +373,8 @@ class ConvertTritonGPUToLLVM\n                                       /*benefit=*/1);\n     populateReduceOpToLLVMPatterns(typeConverter, patterns, allocation,\n                                    indexCacheInfo, /*benefit=*/1);\n+    populateScanOpToLLVMPatterns(typeConverter, patterns, allocation,\n+                                 indexCacheInfo, /*benefit=*/1);\n     populateViewOpToLLVMPatterns(typeConverter, patterns, /*benefit=*/1);\n \n     // Native lowering patterns"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.cpp", "status": "modified", "additions": 1, "deletions": 4, "changes": 5, "file_content_changes": "@@ -24,10 +24,7 @@ TritonGPUToLLVMTypeConverter::TritonGPUToLLVMTypeConverter(\n     return convertTritonTensorType(type);\n   });\n   // Internally store float8 as int8\n-  addConversion([&](mlir::Float8E4M3B11FNUZType type) -> std::optional<Type> {\n-    return IntegerType::get(type.getContext(), 8);\n-  });\n-  addConversion([&](mlir::Float8E4M3FNUZType type) -> std::optional<Type> {\n+  addConversion([&](mlir::Float8E4M3FNType type) -> std::optional<Type> {\n     return IntegerType::get(type.getContext(), 8);\n   });\n   addConversion([&](mlir::Float8E5M2Type type) -> std::optional<Type> {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.cpp", "status": "modified", "additions": 17, "deletions": 6, "changes": 23, "file_content_changes": "@@ -160,34 +160,45 @@ Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n   return builder.launch(rewriter, loc, void_ty(ctx));\n }\n \n-Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n-               int i) {\n+static Value commonShflSync(Location loc, ConversionPatternRewriter &rewriter,\n+                            Value val, int i, const std::string &shuffleType,\n+                            const std::string &clamp) {\n   unsigned bits = val.getType().getIntOrFloatBitWidth();\n \n   if (bits == 64) {\n     Type vecTy = vec_ty(f32_ty, 2);\n     Value vec = bitcast(val, vecTy);\n     Value val0 = extract_element(f32_ty, vec, i32_val(0));\n     Value val1 = extract_element(f32_ty, vec, i32_val(1));\n-    val0 = shflSync(loc, rewriter, val0, i);\n-    val1 = shflSync(loc, rewriter, val1, i);\n+    val0 = commonShflSync(loc, rewriter, val0, i, shuffleType, clamp);\n+    val1 = commonShflSync(loc, rewriter, val1, i, shuffleType, clamp);\n     vec = undef(vecTy);\n     vec = insert_element(vecTy, vec, val0, i32_val(0));\n     vec = insert_element(vecTy, vec, val1, i32_val(1));\n     return bitcast(vec, val.getType());\n   }\n \n   PTXBuilder builder;\n-  auto &shfl = builder.create(\"shfl.sync\")->o(\"bfly\").o(\"b32\");\n+  auto &shfl = builder.create(\"shfl.sync\")->o(shuffleType).o(\"b32\");\n   auto *dOpr = builder.newOperand(\"=r\");\n   auto *aOpr = builder.newOperand(val, \"r\");\n   auto *bOpr = builder.newConstantOperand(i);\n-  auto *cOpr = builder.newConstantOperand(\"0x1f\");\n+  auto *cOpr = builder.newConstantOperand(clamp);\n   auto *maskOpr = builder.newConstantOperand(\"0xffffffff\");\n   shfl(dOpr, aOpr, bOpr, cOpr, maskOpr);\n   return builder.launch(rewriter, loc, val.getType(), false);\n }\n \n+Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n+               int i) {\n+  return commonShflSync(loc, rewriter, val, i, \"bfly\", \"0x1f\");\n+}\n+\n+Value shflUpSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n+                 int i) {\n+  return commonShflSync(loc, rewriter, val, i, \"up\", \"0x0\");\n+}\n+\n Value addStringToModule(Location loc, ConversionPatternRewriter &rewriter,\n                         StringRef key, StringRef content) {\n   auto moduleOp = rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -285,6 +285,8 @@ Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n \n Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n                int i);\n+Value shflUpSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n+                 int i);\n \n Value addStringToModule(Location loc, ConversionPatternRewriter &rewriter,\n                         StringRef key, StringRef content);"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 36, "deletions": 3, "changes": 39, "file_content_changes": "@@ -537,6 +537,38 @@ struct TritonReduceReturnPattern\n   }\n };\n \n+struct TritonScanPattern : public OpConversionPattern<triton::ScanOp> {\n+  using OpConversionPattern<triton::ScanOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::ScanOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto newScan = rewriter.create<triton::ScanOp>(\n+        op.getLoc(), adaptor.getOperands(), adaptor.getAxis());\n+    addNamedAttrs(newScan, adaptor.getAttributes());\n+\n+    auto &newCombineOp = newScan.getCombineOp();\n+    rewriter.cloneRegionBefore(op.getCombineOp(), newCombineOp,\n+                               newCombineOp.end());\n+    rewriter.replaceOp(op, newScan.getResult());\n+    return success();\n+  }\n+};\n+\n+struct TritonScanReturnPattern\n+    : public OpConversionPattern<triton::ScanReturnOp> {\n+  using OpConversionPattern<triton::ScanReturnOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::ScanReturnOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::ScanReturnOp>(\n+                      op, adaptor.getResult()),\n+                  adaptor.getAttributes());\n+    return success();\n+  }\n+};\n+\n struct TritonPrintPattern : public OpConversionPattern<triton::PrintOp> {\n   using OpConversionPattern<triton::PrintOp>::OpConversionPattern;\n \n@@ -623,9 +655,10 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n           TritonGenericPattern<triton::PtrToIntOp>,\n           TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n           TritonGenericPattern<triton::AddPtrOp>, TritonCatPattern,\n-          TritonReducePattern, TritonReduceReturnPattern, TritonTransPattern,\n-          TritonExpandDimsPattern, TritonMakeRangePattern, TritonDotPattern,\n-          TritonLoadPattern, TritonStorePattern,\n+          TritonReducePattern, TritonReduceReturnPattern, TritonScanPattern,\n+          TritonScanReturnPattern, TritonTransPattern, TritonExpandDimsPattern,\n+          TritonMakeRangePattern, TritonDotPattern, TritonLoadPattern,\n+          TritonStorePattern,\n           TritonExternElementwisePattern<triton::PureExternElementwiseOp>,\n           TritonExternElementwisePattern<triton::ImpureExternElementwiseOp>,\n           TritonPrintPattern, TritonAssertPattern, TritonAtomicRMWPattern,"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 30, "deletions": 0, "changes": 30, "file_content_changes": "@@ -559,6 +559,36 @@ llvm::SmallVector<Type> ReduceOp::getElementTypes() {\n \n unsigned ReduceOp::getNumOperands() { return this->getOperands().size(); }\n \n+//-- ScanOp --\n+void ScanOp::build(mlir::OpBuilder &builder, mlir::OperationState &state,\n+                   mlir::ValueRange operands, int axis) {\n+  SmallVector<Type> inferredReturnTypes;\n+  for (auto arg : operands)\n+    inferredReturnTypes.push_back(arg.getType());\n+  ReduceOp::build(builder, state, inferredReturnTypes, operands, axis);\n+}\n+\n+mlir::LogicalResult mlir::triton::ScanOp::inferReturnTypes(\n+    MLIRContext *context, std::optional<Location> location, ValueRange operands,\n+    DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,\n+    SmallVectorImpl<Type> &inferredReturnTypes) {\n+  for (auto arg : operands)\n+    inferredReturnTypes.push_back(arg.getType());\n+  return success();\n+}\n+\n+mlir::LogicalResult mlir::triton::ScanOp::verify() {\n+  if (this->getOperands().size() < 1) {\n+    return this->emitOpError() << \"must have at least 1 operand\";\n+  }\n+  for (const auto &operand : this->getOperands()) {\n+    if (!dyn_cast<RankedTensorType>(operand.getType())) {\n+      return this->emitOpError() << \"operands must be RankedTensorType\";\n+    }\n+  }\n+  return success();\n+}\n+\n //-- SplatOp --\n OpFoldResult SplatOp::fold(FoldAdaptor adaptor) {\n   auto value = adaptor.getSrc();"}, {"filename": "lib/Target/LLVMIR/CMakeLists.txt", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -4,7 +4,9 @@ add_mlir_translation_library(TritonLLVMIR\n         LINK_COMPONENTS\n         Core\n \n-        LINK_LIBS PUBLIC\n+        LINK_LIBS\n+        ${CMAKE_DL_LIBS}\n+        PUBLIC\n         MLIRArithToLLVM\n         MLIRBuiltinToLLVMIRTranslation\n         MLIRExecutionEngineUtils"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 17, "deletions": 7, "changes": 24, "file_content_changes": "@@ -91,6 +91,7 @@ void init_triton_ir(py::module &&m) {\n       .value(\"CG\", mlir::triton::CacheModifier::CG)\n       .value(\"WB\", mlir::triton::CacheModifier::WB)\n       .value(\"CS\", mlir::triton::CacheModifier::CS)\n+      .value(\"WT\", mlir::triton::CacheModifier::WT)\n       .export_values();\n \n   py::enum_<mlir::triton::MemSemantic>(m, \"MEM_SEMANTIC\")\n@@ -643,13 +644,7 @@ void init_triton_ir(py::module &&m) {\n           [](mlir::OpBuilder &self) -> mlir::Type { return self.getI64Type(); })\n       .def(\"get_fp8e4_ty\",\n            [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getType<mlir::Float8E4M3FNUZType>();\n-           })\n-      .def(\"get_fp8e4b15_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n-             // TODO: upstream FP8E4B15 into MLIR, or find a way to externally\n-             // have a float-like type compatible with float only native ops\n-             return self.getType<mlir::Float8E4M3B11FNUZType>();\n+             return self.getType<mlir::Float8E4M3FNType>();\n            })\n       .def(\"get_fp8e5_ty\",\n            [](mlir::OpBuilder &self) -> mlir::Type {\n@@ -1411,6 +1406,21 @@ void init_triton_ir(py::module &&m) {\n              return self.create<mlir::triton::ReduceReturnOp>(loc,\n                                                               return_values);\n            })\n+      .def(\"create_scan\",\n+           [](mlir::OpBuilder &self, std::vector<mlir::Value> operands,\n+              int axis) -> mlir::OpState {\n+             auto loc = self.getUnknownLoc();\n+             return self.create<mlir::triton::ScanOp>(loc, operands, axis);\n+           })\n+      .def(\"create_scan_ret\",\n+           [](mlir::OpBuilder &self, py::args args) -> mlir::OpState {\n+             auto loc = self.getUnknownLoc();\n+             llvm::SmallVector<mlir::Value> return_values;\n+             for (const auto &arg : args) {\n+               return_values.push_back(py::cast<mlir::Value>(arg));\n+             }\n+             return self.create<mlir::triton::ScanReturnOp>(loc, return_values);\n+           })\n       .def(\"create_ptr_to_int\",\n            [](mlir::OpBuilder &self, mlir::Value &val,\n               mlir::Type &type) -> mlir::Value {"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 210, "deletions": 63, "changes": 273, "file_content_changes": "@@ -706,11 +706,11 @@ def test_abs(dtype_x, device):\n     _test_unary(dtype_x, 'tl.abs(x)', 'np.abs(x) ', device=device)\n \n \n-@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4, tl.float8e5])\n-def test_abs_fp8(in_dtype, device):\n+@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n+def test_abs_f8(in_dtype, device):\n \n     @triton.jit\n-    def abs_kernel(Z, X, SIZE: tl.constexpr):\n+    def abs_kernel(X, Z, SIZE: tl.constexpr):\n         off = tl.arange(0, SIZE)\n         x = tl.load(X + off)\n         z = tl.abs(x)\n@@ -723,12 +723,13 @@ def abs_kernel(Z, X, SIZE: tl.constexpr):\n     f8 = triton.reinterpret(f8_tensor, in_dtype)\n     n_elements = f8_tensor.numel()\n     out_f8 = torch.empty_like(f8_tensor)\n+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n     abs_kernel[(1,)](f8, triton.reinterpret(out_f8, in_dtype), n_elements)\n \n     f32_tensor = convert_float_to_float32(f8_tensor, in_dtype)\n     expect = f32_tensor.abs()\n     actual_f8 = convert_float_to_float32(out_f8, in_dtype)\n-    torch.testing.assert_allclose(expect, actual_f8)\n+    torch.testing.assert_allclose(actual_f8, expect)\n \n \n # ----------------\n@@ -1214,7 +1215,7 @@ def convert_float_to_float32(fp: torch.tensor, dtype=None):\n \n     fp = fp.view(getattr(torch, f\"int{dtype.primitive_bitwidth}\"))\n     exp_width = dtype.primitive_bitwidth - dtype.fp_mantissa_width - 1\n-    exp_bias = dtype.exponent_bias\n+    exp_bias = 2 ** (exp_width - 1) - 1\n     sign = ((fp >> (dtype.primitive_bitwidth - 1)) & 0x01).int()\n     exp = ((fp >> dtype.fp_mantissa_width) & ((1 << exp_width) - 1)).int()\n     frac = (fp & ((1 << dtype.fp_mantissa_width) - 1)).int()\n@@ -1227,7 +1228,7 @@ def convert_float_to_float32(fp: torch.tensor, dtype=None):\n \n     extended_exp = ((1 << (tl.float32.primitive_bitwidth - tl.float32.fp_mantissa_width - 1)) - 1) << tl.float32.fp_mantissa_width\n     # special cases, exp is 0b11..1\n-    if dtype in [tl.float8e4, tl.float8e4b15]:\n+    if dtype == tl.float8e4:\n         # float8e4m3 does not have infinities\n         output[fp == 0b01111111] = torch.nan\n         output[fp == 0b11111111] = torch.nan\n@@ -1254,42 +1255,11 @@ def test_convert_float16_to_float32(in_dtype, device):\n     assert torch.all(f16_input[other] == f32_output[other])\n \n \n-def serialize_fp8(np_data, in_dtype):\n-    if in_dtype == tl.float8e4b15:\n-        # triton's f8e4b15 format is optimized for software emulation\n-        # as a result, each pack of 4xfp8 values:\n-        # s0b0s1b1s2b2s3b3 (for s, b sign and bits respectively)\n-        # is actually internally stored as\n-        # s0s2b0b2s1s3b1b3\n-        # we apply the conversion here\n-        f8x4 = np_data.view(np.uint32)\n-        s = [(f8x4 & (0x80000000 >> i)) << i for i in range(0, 32, 8)]\n-        b = [(f8x4 & (0x7f000000 >> i)) << i for i in range(0, 32, 8)]\n-        signs = (s[0] >> 0) | (s[1] >> 16) | (s[2] >> 1) | (s[3] >> 17)\n-        bits = (b[0] >> 1) | (b[1] >> 17) | (b[2] >> 8) | (b[3] >> 24)\n-        # tensor of triton fp8 data\n-        return (signs | bits).view(np.int8)\n-    else:\n-        return np_data\n-\n-\n-@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4, tl.float8e5])\n+@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n @pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16, torch.float32])\n-def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n-    \"\"\"\n-    For all possible float8 values (ref_fp8 = range(0, 256)), test that:\n-        - conversion tri_fp16 = convert(input=ref_fp8, out=out_dtype) matches the reference\n-        - conversion tri_fp8 = convert(input=tri_fp16, out=out_dtype) matches the original\n-    this is only possible if both conversions are correct\n-    \"\"\"\n+def test_f8_xf16_roundtrip(in_dtype, out_dtype, device):\n+    \"\"\"Tests that converting an f8 to f16 and back to f8 doesn't change its value\"\"\"\n     check_type_supported(out_dtype, device)\n-    from contextlib import nullcontext as does_not_raise\n-    expectation = does_not_raise()\n-    err_msg = None\n-    if (in_dtype == tl.float8e4b15 and out_dtype != torch.float16) or\\\n-       (in_dtype != torch.float16 and out_dtype == tl.float8e4b15):\n-        expectation = pytest.raises(triton.CompilationError)\n-        err_msg = \"fp8e4b15 can only be converted to/from fp16\"\n \n     @triton.jit\n     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n@@ -1299,28 +1269,90 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         output = input\n         tl.store(output_ptr + offsets, output, mask=mask)\n \n-    # initialize array containing all possible f8 values except NaN\n-    ref_fp8 = np.array(range(-128, 128), dtype=np.int8)\n-    is_nan = (ref_fp8 & 0b01111100) == 128 - 2**in_dtype.fp_mantissa_width\n-    exp_mask = 0b01111111 ^ ((1 << in_dtype.fp_mantissa_width) - 1)\n-    is_subnormal = np.logical_or((ref_fp8 & exp_mask) == 0, (ref_fp8 & exp_mask) == exp_mask)\n-    ref_fp8[is_nan] = 0\n-    ref_fp8[is_subnormal] = 0\n-    tri_fp8 = torch.from_numpy(serialize_fp8(ref_fp8, in_dtype)).cuda()\n-    tri_fp16 = torch.empty(256, dtype=out_dtype, device=\"cuda\")\n-    with expectation as e:\n-        copy_kernel[(1,)](triton.reinterpret(tri_fp8, in_dtype), tri_fp16, tri_fp16.shape[0], BLOCK_SIZE=1024)\n-\n-        ref_fp8 = torch.from_numpy(ref_fp8).cuda()\n-        ref_fp16 = convert_float_to_float32(ref_fp8, in_dtype)\n-        assert torch.all(tri_fp16[~is_subnormal] == ref_fp16[~is_subnormal])\n-\n-        ref_fp8 = torch.empty_like(tri_fp16, dtype=torch.int8)\n-        copy_kernel[(1,)](tri_fp16, triton.reinterpret(ref_fp8, in_dtype), tri_fp16.shape[0], BLOCK_SIZE=1024)\n-        assert torch.all(tri_fp8 == ref_fp8)\n-\n-    if err_msg is not None:\n-        assert err_msg in str(e)\n+    f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device=device)\n+    # f32_to_f8 doesn't handle nan, so we make sure f8_tensor doesn't contain any nan\n+    all_exp_ones = (f8_tensor & 0b01111100) == 128 - 2**in_dtype.fp_mantissa_width\n+    f8_tensor[all_exp_ones] = 0\n+    f8 = triton.reinterpret(f8_tensor, in_dtype)\n+    n_elements = f8_tensor.numel()\n+    xf16 = torch.empty_like(f8_tensor, dtype=out_dtype)\n+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+    copy_kernel[grid](f8, xf16, n_elements, BLOCK_SIZE=1024)\n+\n+    # exponent_mask = 0b01111100 for float8e5\n+    # exponent_mask = 0b01111000 for float8e4\n+    exponent_mask = 0b01111111 ^ ((1 << in_dtype.fp_mantissa_width) - 1)\n+    normal = torch.logical_and((f8_tensor & exponent_mask) != 0, (f8_tensor & exponent_mask) != exponent_mask)\n+    ref16 = convert_float_to_float32(f8_tensor, in_dtype)\n+    # WARN: currently only normal float8s are handled\n+    assert torch.all(xf16[normal] == ref16[normal])\n+\n+    f8_output_tensor = torch.empty_like(xf16, dtype=torch.int8)\n+    f8_output = triton.reinterpret(f8_output_tensor, in_dtype)\n+    copy_kernel[grid](xf16, f8_output, n_elements, BLOCK_SIZE=1024)\n+\n+    assert torch.all(f8_tensor == f8_output_tensor)\n+\n+\n+@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n+@pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16])\n+def test_f16_to_f8_rounding(in_dtype, out_dtype, device):\n+    \"\"\"Takes all float16s, converts them to float8 and back to float16. Checks that the absolute\n+    error is the minimum over all float8.\n+    Or the same explanation a bit mathier:\n+    for all f16 |f16 - fromf8(tof8(f16))| == min over all f8 |f16 - fromf8(f8)|\"\"\"\n+    @triton.jit\n+    def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n+        offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+        mask = offsets < n_elements\n+        input = tl.load(input_ptr + offsets, mask=mask)\n+        output = input\n+        tl.store(output_ptr + offsets, output, mask=mask)\n+\n+    i16_input = torch.tensor(range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=torch.int16, device=device)\n+    f16_input = i16_input.view(out_dtype)\n+    n_elements = f16_input.numel()\n+    f8_output_tensor = torch.empty_like(f16_input, dtype=torch.int8)\n+    f8_output = triton.reinterpret(f8_output_tensor, in_dtype)\n+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+    copy_kernel[grid](f16_input, f8_output, n_elements, BLOCK_SIZE=1024)\n+\n+    f16_output = torch.empty_like(f16_input, dtype=out_dtype)\n+    copy_kernel[grid](f8_output, f16_output, n_elements, BLOCK_SIZE=1024)\n+\n+    abs_error = torch.abs(f16_input - f16_output)\n+\n+    all_f8_vals_tensor = torch.tensor(range(2 ** 8), dtype=torch.uint8, device=device)\n+    all_f8_vals = triton.reinterpret(all_f8_vals_tensor, in_dtype)\n+    all_f8_vals_in_f16 = torch.empty_like(all_f8_vals_tensor, dtype=out_dtype)\n+    copy_kernel[grid](all_f8_vals, all_f8_vals_in_f16, n_elements=256, BLOCK_SIZE=1024)\n+\n+    all_finite_f8_vals_in_f16 = all_f8_vals_in_f16[\n+        torch.isfinite(all_f8_vals_in_f16)\n+    ]\n+\n+    min_error = torch.min(\n+        torch.abs(\n+            f16_input.reshape((-1, 1))\n+            - all_finite_f8_vals_in_f16.reshape((1, -1))\n+        ),\n+        dim=1,\n+    )[0]\n+\n+    # WARN: only normalized numbers are handled\n+    f8_normal_min = 1 << in_dtype.fp_mantissa_width  # 0b00001000 for float8e4\n+    f8_normal_max = 0b01111110 if in_dtype == tl.float8e4 else 0b01111011\n+    f16_min, f16_max, f16_max_minus_1 = convert_float_to_float32(torch.tensor([f8_normal_min, f8_normal_max, f8_normal_max - 1], dtype=torch.int8), in_dtype)\n+    assert torch.all(torch.isfinite(f16_min))\n+    assert torch.all(torch.isfinite(f16_max))\n+    thres_error = f16_max - f16_max_minus_1\n+    mismatch = torch.logical_and(\n+        torch.logical_or(abs_error != min_error, abs_error > thres_error), torch.logical_and(torch.isfinite(f16_input), torch.logical_and(torch.abs(f16_input) <= f16_max, torch.abs(f16_input) >= f16_min))\n+    )\n+    assert torch.all(\n+        torch.logical_not(mismatch)\n+    ), f\"f16_input[mismatch]={f16_input[mismatch]} f16_output[mismatch]={f16_output[mismatch]} abs_error[mismatch]={abs_error[mismatch]} min_error[mismatch]={min_error[mismatch]}\"\n+\n \n # ---------------\n # test reduce\n@@ -1494,6 +1526,112 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n             np.testing.assert_equal(z_ref, z_tri)\n \n \n+scan2d_shapes = [(16, 32), (32, 16), (2, 1024), (1024, 2), (32, 32), (1, 1024)]\n+\n+scan_configs = [\n+    (op, type, shape, 1)\n+    for type in ['int32', 'float32']\n+    for shape in scan2d_shapes\n+    for op in ['cumsum']\n+]\n+\n+\n+@pytest.mark.parametrize(\"op, dtype_str, shape, axis\", scan_configs)\n+def test_scan2d(op, dtype_str, shape, axis, device):\n+    check_type_supported(dtype_str, device)\n+\n+    # triton kernel\n+    @triton.jit\n+    def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexpr):\n+        range_m = tl.arange(0, BLOCK_M)\n+        range_n = tl.arange(0, BLOCK_N)\n+        x = tl.load(X + range_m[:, None] * BLOCK_N + range_n[None, :])\n+        z = GENERATE_TEST_HERE\n+        tl.store(Z + range_m[:, None] * BLOCK_N + range_n[None, :], z)\n+\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis=1)'})\n+    # input\n+    rs = RandomState(17)\n+    x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n+    z = np.empty_like(x)\n+    x_tri = to_triton(x, device=device)\n+    numpy_op = {'cumsum': np.cumsum}[op]\n+    z_dtype_str = dtype_str\n+    z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n+    # triton result\n+    z_tri = to_triton(z, device=device)\n+    kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis)\n+    z_tri = to_numpy(z_tri)\n+    # compare\n+    if dtype_str == 'float32':\n+        np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n+    else:\n+        np.testing.assert_equal(z_ref, z_tri)\n+\n+\n+scan_layouts = [\n+    BlockedLayout([1, 4], [4, 8], [4, 1], [1, 0]),\n+    BlockedLayout([1, 4], [8, 4], [4, 1], [1, 0]),\n+    BlockedLayout([4, 1], [4, 8], [1, 4], [1, 0]),\n+    BlockedLayout([2, 2], [4, 8], [2, 2], [1, 0]),\n+    BlockedLayout([2, 2], [8, 4], [2, 2], [1, 0]),\n+]\n+\n+\n+@pytest.mark.parametrize(\"src_layout\", scan_layouts)\n+def test_scan_layouts(src_layout, device):\n+    ir = f\"\"\"\n+    #blocked = {src_layout}\n+    module attributes {{\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32}} {{\n+    tt.func public @kernel_0d1d(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n+      %cst = arith.constant dense<32> : tensor<32x1xi32, #blocked>\n+      %0 = tt.make_range {{end = 32 : i32, start = 0 : i32}} : tensor<32xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>\n+      %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<32xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>) -> tensor<32x1xi32, #blocked>\n+      %2 = arith.muli %1, %cst : tensor<32x1xi32, #blocked>\n+      %3 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<32x1x!tt.ptr<i32>, #blocked>\n+      %4 = tt.addptr %3, %2 : tensor<32x1x!tt.ptr<i32>, #blocked>, tensor<32x1xi32, #blocked>\n+      %5 = tt.make_range {{end = 32 : i32, start = 0 : i32}} : tensor<32xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>\n+      %6 = tt.expand_dims %5 {{axis = 0 : i32}} : (tensor<32xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>) -> tensor<1x32xi32, #blocked>\n+      %7 = tt.broadcast %4 : (tensor<32x1x!tt.ptr<i32>, #blocked>) -> tensor<32x32x!tt.ptr<i32>, #blocked>\n+      %8 = tt.broadcast %6 : (tensor<1x32xi32, #blocked>) -> tensor<32x32xi32, #blocked>\n+      %9 = tt.addptr %7, %8 : tensor<32x32x!tt.ptr<i32>, #blocked>, tensor<32x32xi32, #blocked>\n+      %10 = tt.load %9 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<32x32xi32, #blocked>\n+      %11 = \"tt.scan\"(%10) <{{axis = 1 : i32}}> ({{\n+      ^bb0(%arg2: i32, %arg3: i32):\n+        %16 = arith.addi %arg2, %arg3 : i32\n+        tt.scan.return %16 : i32\n+      }}) : (tensor<32x32xi32, #blocked>) -> tensor<32x32xi32, #blocked>\n+      %12 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<32x1x!tt.ptr<i32>, #blocked>\n+      %13 = tt.addptr %12, %2 : tensor<32x1x!tt.ptr<i32>, #blocked>, tensor<32x1xi32, #blocked>\n+      %14 = tt.broadcast %13 : (tensor<32x1x!tt.ptr<i32>, #blocked>) -> tensor<32x32x!tt.ptr<i32>, #blocked>\n+      %15 = tt.addptr %14, %8 : tensor<32x32x!tt.ptr<i32>, #blocked>, tensor<32x32xi32, #blocked>\n+      tt.store %15, %11 {{cache = 1 : i32, evict = 1 : i32}} : tensor<32x32xi32, #blocked>\n+      tt.return\n+    }}\n+    }}\n+    \"\"\"\n+\n+    import tempfile\n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.ttgir') as f:\n+        f.write(ir)\n+        f.flush()\n+        kernel = triton.compile(f.name)\n+    M = 32\n+    N = 32\n+    rs = RandomState(17)\n+    x = rs.randint(-100, 100, (M, N)).astype('int32')\n+\n+    z = np.zeros((M, N)).astype('int32')\n+    x_tri = torch.tensor(x, device=device)\n+    z_tri = torch.tensor(z, device=device)\n+\n+    kernel[(1, 1, 1)](x_tri, z_tri)\n+\n+    z_ref = np.cumsum(x, axis=1)\n+\n+    np.testing.assert_equal(z_ref, z_tri.cpu().numpy())\n+\n+\n layouts = [\n     BlockedLayout([1, 4], [8, 4], [4, 1], [1, 0]),\n     BlockedLayout([1, 4], [8, 4], [4, 1], [0, 1]),\n@@ -2254,7 +2392,7 @@ def _kernel(dst, src, off, N, BLOCK_SIZE: tl.constexpr, HINT: tl.constexpr):\n # ---------------\n \n \n-@pytest.mark.parametrize(\"cache\", [\"\", \".wb\", \".cg\", \".cs\"])\n+@pytest.mark.parametrize(\"cache\", [\"\", \".wb\", \".cg\", \".cs\", \".wt\"])\n def test_store_cache_modifier(cache):\n     src = torch.empty(128, device='cuda')\n     dst = torch.empty(128, device='cuda')\n@@ -2271,18 +2409,27 @@ def _kernel(dst, src, CACHE: tl.constexpr):\n         assert 'st.global.wb' not in ptx\n         assert 'st.global.cg' not in ptx\n         assert 'st.global.cs' not in ptx\n+        assert 'st.global.wt' not in ptx\n     if cache == '.wb':\n         assert 'st.global.wb' in ptx\n         assert 'st.global.cg' not in ptx\n         assert 'st.global.cs' not in ptx\n+        assert 'st.global.wt' not in ptx\n     if cache == '.cg':\n         assert 'st.global.wb' not in ptx\n         assert 'st.global.cg' in ptx\n         assert 'st.global.cs' not in ptx\n+        assert 'st.global.wt' not in ptx\n     if cache == '.cs':\n         assert 'st.global.wb' not in ptx\n         assert 'st.global.cg' not in ptx\n         assert 'st.global.cs' in ptx\n+        assert 'st.global.wt' not in ptx\n+    if cache == '.wt':\n+        assert 'st.global.wb' not in ptx\n+        assert 'st.global.cg' not in ptx\n+        assert 'st.global.cs' not in ptx\n+        assert 'st.global.wt' in ptx\n \n # ---------------\n # test if"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 5, "deletions": 10, "changes": 15, "file_content_changes": "@@ -189,11 +189,10 @@ def visit_Call(self, node: ast.Call) -> bool:\n \n \n class CodeGenerator(ast.NodeVisitor):\n-    def __init__(self, context, prototype, gscope, attributes, constants, function_name, arch,\n+    def __init__(self, context, prototype, gscope, attributes, constants, function_name,\n                  module=None, is_kernel=False, function_types: Optional[Dict] = None,\n                  debug=False, noinline=False):\n         self.builder = ir.builder(context)\n-        self.builder.arch = arch\n         self.module = self.builder.create_module() if module is None else module\n         self.function_ret_types = {} if function_types is None else function_types\n         self.prototype = prototype\n@@ -869,9 +868,7 @@ def call_JitFunction(self, fn: JITFunction, args, kwargs):\n             gscope = sys.modules[fn.fn.__module__].__dict__\n             # If the callee is not set, we use the same debug setting as the caller\n             debug = self.debug if fn.debug is None else fn.debug\n-            generator = CodeGenerator(self.builder.context, prototype, gscope, attributes, constants, module=self.module,\n-                                      function_name=fn_name, function_types=self.function_ret_types, debug=debug, noinline=fn.noinline,\n-                                      arch=self.builder.arch)\n+            generator = CodeGenerator(self.builder.context, prototype, gscope, attributes, constants, module=self.module, function_name=fn_name, function_types=self.function_ret_types, debug=debug, noinline=fn.noinline)\n             generator.visit(fn.parse())\n             callee_ret_type = generator.last_ret_type\n             self.function_ret_types[fn_name] = callee_ret_type\n@@ -1019,9 +1016,8 @@ def str_to_ty(name):\n         ty = str_to_ty(name[1:])\n         return language.pointer_type(ty)\n     tys = {\n-        \"fp8e4\": language.float8e4,\n         \"fp8e5\": language.float8e5,\n-        \"fp8e4b15\": language.float8e4b15,\n+        \"fp8e4\": language.float8e4,\n         \"fp16\": language.float16,\n         \"bf16\": language.bfloat16,\n         \"fp32\": language.float32,\n@@ -1053,7 +1049,7 @@ def kernel_suffix(signature, specialization):\n     return suffix\n \n \n-def ast_to_ttir(fn, signature, specialization, constants, debug, arch):\n+def ast_to_ttir(fn, signature, specialization, constants, debug):\n     # canonicalize signature\n     if isinstance(signature, str):\n         signature = {k: v.strip() for k, v in enumerate(signature.split(\",\"))}\n@@ -1075,8 +1071,7 @@ def ast_to_ttir(fn, signature, specialization, constants, debug, arch):\n     prototype = language.function_type([], arg_types)\n     generator = CodeGenerator(context, prototype, gscope=gscope, constants=all_constants,\n                               function_name=function_name, attributes=new_attrs,\n-                              is_kernel=True, debug=debug,\n-                              arch=arch)\n+                              is_kernel=True, debug=debug)\n     try:\n         generator.visit(fn.parse())\n     except CompilationError as e:"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -396,7 +396,7 @@ def compile(fn, **kwargs):\n     stages = dict()\n     stages[\"ast\"] = (lambda path: fn, None)\n     stages[\"ttir\"] = (lambda path: parse_mlir_module(path, context),\n-                      lambda src: optimize_ttir(ast_to_ttir(src, signature, configs[0], constants, debug=debug, arch=arch), arch))\n+                      lambda src: optimize_ttir(ast_to_ttir(src, signature, configs[0], constants, debug=debug), arch))\n     stages[\"ttgir\"] = (lambda path: parse_mlir_module(path, context),\n                        lambda src: optimize_ttgir(ttir_to_ttgir(src, num_warps), num_stages, arch))\n     stages[\"llir\"] = (lambda path: Path(path).read_text(),"}, {"filename": "python/triton/interpreter/tl_lang.py", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -623,3 +623,9 @@ def sum(self, input, axis=None):\n     @_tensor_operation\n     def xor_sum(self, input, axis):\n         raise NotImplementedError()\n+\n+    @_tensor_operation\n+    def cumsum(self, input, axis=None):\n+        if axis is None:\n+            return torch.cumsum(input)\n+        return torch.cumsum(input, dim=axis)"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -18,6 +18,7 @@\n     arange,\n     argmin,\n     argmax,\n+    associative_scan,\n     atomic_add,\n     atomic_and,\n     atomic_cas,\n@@ -33,6 +34,7 @@\n     cat,\n     constexpr,\n     cos,\n+    cumsum,\n     debug_barrier,\n     device_assert,\n     device_print,\n@@ -45,7 +47,6 @@\n     float16,\n     float32,\n     float64,\n-    float8e4b15,\n     float8e4,\n     float8e5,\n     function_type,\n@@ -109,6 +110,7 @@\n     \"arange\",\n     \"argmin\",\n     \"argmax\",\n+    \"associative_scan\",\n     \"atomic_add\",\n     \"atomic_and\",\n     \"atomic_cas\",\n@@ -126,6 +128,7 @@\n     \"cdiv\",\n     \"constexpr\",\n     \"cos\",\n+    \"cumsum\",\n     \"debug_barrier\",\n     \"device_assert\",\n     \"device_print\",\n@@ -138,7 +141,6 @@\n     \"float16\",\n     \"float32\",\n     \"float64\",\n-    \"float8e4b15\",\n     \"float8e4\",\n     \"float8e5\",\n     \"full\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 63, "deletions": 21, "changes": 84, "file_content_changes": "@@ -76,7 +76,7 @@ def _to_tensor(x, builder):\n class dtype:\n     SINT_TYPES = ['int8', 'int16', 'int32', 'int64']\n     UINT_TYPES = ['int1', 'uint8', 'uint16', 'uint32', 'uint64']\n-    FP_TYPES = ['fp8e4b15', 'fp8e4', 'fp8e5', 'fp16', 'bf16', 'fp32', 'fp64']\n+    FP_TYPES = ['fp8e4', 'fp8e5', 'fp16', 'bf16', 'fp32', 'fp64']\n     STANDARD_FP_TYPES = ['fp16', 'bf16', 'fp32', 'fp64']\n     OTHER_TYPES = ['void']\n \n@@ -96,34 +96,24 @@ def __init__(self, name):\n             self.int_bitwidth = int(name.split('int')[-1])\n             self.primitive_bitwidth = self.int_bitwidth\n         elif name in dtype.FP_TYPES:\n-            if name == 'fp8e4b15':\n+            if name == 'fp8e4':\n                 self.fp_mantissa_width = 3\n                 self.primitive_bitwidth = 8\n-                self.exponent_bias = 15\n-            elif name == 'fp8e4':\n-                self.fp_mantissa_width = 3\n-                self.primitive_bitwidth = 8\n-                self.exponent_bias = 7\n             elif name == 'fp8e5':\n                 self.fp_mantissa_width = 2\n                 self.primitive_bitwidth = 8\n-                self.exponent_bias = 15\n             elif name == 'fp16':\n                 self.fp_mantissa_width = 10\n                 self.primitive_bitwidth = 16\n-                self.exponent_bias = 15\n             elif name == 'bf16':\n                 self.fp_mantissa_width = 7\n                 self.primitive_bitwidth = 16\n-                self.exponent_bias = 127\n             elif name == 'fp32':\n                 self.fp_mantissa_width = 23\n                 self.primitive_bitwidth = 32\n-                self.exponent_bias = 127\n             elif name == 'fp64':\n                 self.fp_mantissa_width = 53\n                 self.primitive_bitwidth = 64\n-                self.exponent_bias = 1023\n             else:\n                 raise RuntimeError(f'Unsupported floating-point type {name}')\n         elif name == 'void':\n@@ -132,12 +122,6 @@ def __init__(self, name):\n     def is_fp8(self):\n         return 'fp8' in self.name\n \n-    def is_fp8e4(self):\n-        return self.name == 'fp8e4'\n-\n-    def is_fp8e4b15(self):\n-        return self.name == 'fp8e4b15'\n-\n     def is_fp16(self):\n         return self.name == 'fp16'\n \n@@ -239,8 +223,6 @@ def to_ir(self, builder: ir.builder) -> ir.type:\n             return builder.get_fp8e5_ty()\n         elif self.name == 'fp8e4':\n             return builder.get_fp8e4_ty()\n-        elif self.name == 'fp8e4b15':\n-            return builder.get_fp8e4b15_ty()\n         elif self.name == 'fp16':\n             return builder.get_half_ty()\n         elif self.name == 'bf16':\n@@ -374,7 +356,6 @@ def to_ir(self, builder: ir.builder):\n uint64 = dtype('uint64')\n float8e5 = dtype('fp8e5')\n float8e4 = dtype('fp8e4')\n-float8e4b15 = dtype('fp8e4b15')\n float16 = dtype('fp16')\n bfloat16 = dtype('bf16')\n float32 = dtype('fp32')\n@@ -1521,6 +1502,67 @@ def xor_sum(input, axis=None, _builder=None, _generator=None):\n                   _builder=_builder, _generator=_generator)\n \n \n+# -----------------------\n+# Scans\n+# -----------------------\n+\n+def _add_scan_docstr(name: str, return_indices_arg: str = None, tie_break_arg: str = None) -> Callable[[T], T]:\n+\n+    def _decorator(func: T) -> T:\n+        docstr = \"\"\"\n+    Returns the {name} of all elements in the :code:`input` tensor along the provided :code:`axis`\n+\n+    :param input: the input values\n+    :param axis: the dimension along which the scan should be done\"\"\"\n+        func.__doc__ = docstr.format(name=name)\n+        return func\n+\n+    return _decorator\n+\n+\n+@builtin\n+def associative_scan(input, axis, combine_fn, _builder=None, _generator=None):\n+    \"\"\"Applies the combine_fn to each elements with a carry in :code:`input` tensors along the provided :code:`axis` and update the carry\n+\n+    :param input: the input tensor, or tuple of tensors\n+    :param axis: the dimension along which the reduction should be done\n+    :param combine_fn: a function to combine two groups of scalar tensors (must be marked with @triton.jit)\n+\n+    \"\"\"\n+    if isinstance(input, tensor):\n+        return associative_scan((input,), axis, combine_fn,\n+                                _builder=_builder, _generator=_generator)[0]\n+\n+    def make_combine_region(scan_op):\n+        in_scalar_tys = [t.type.scalar for t in input]\n+        prototype = function_type(in_scalar_tys, in_scalar_tys * 2)\n+\n+        region = scan_op.get_region(0)\n+        with _insertion_guard(_builder):\n+            param_types = [ty.to_ir(_builder) for ty in prototype.param_types]\n+            block = _builder.create_block_with_parent(region, param_types)\n+            args = [tensor(block.arg(i), ty)\n+                    for i, ty in enumerate(prototype.param_types)]\n+            results = _generator.call_JitFunction(combine_fn, args, kwargs={})\n+            if isinstance(results, tensor):\n+                handles = [results.handle]\n+            else:\n+                handles = [r.handle for r in results]\n+            _builder.create_scan_ret(*handles)\n+    axis = _constexpr_to_value(axis)\n+    return semantic.associative_scan(input, axis, make_combine_region, _builder)\n+\n+# cumsum\n+\n+\n+@jit\n+@_add_scan_docstr(\"cumsum\")\n+def cumsum(input, axis=0):\n+    # todo rename this to a generic function name\n+    input = _promote_reduction_input(input)\n+    return associative_scan(input, axis, _sum_combine)\n+\n+\n # -----------------------\n # Compiler Hint Ops\n # -----------------------"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 28, "deletions": 11, "changes": 39, "file_content_changes": "@@ -1,6 +1,5 @@\n from __future__ import annotations  # remove after python 3.11\n \n-import warnings\n from functools import wraps\n from typing import List, Optional, Sequence, Tuple, TypeVar\n \n@@ -677,16 +676,6 @@ def cast(input: tl.tensor,\n     src_sca_ty = src_ty.scalar\n     dst_sca_ty = dst_ty.scalar\n \n-    if builder.arch < 89 and \\\n-       (src_sca_ty.is_fp8e4() or dst_sca_ty.is_fp8e4()):\n-        warnings.warn(\"Standard tl.float8e4 format will be deprecated on SM < 89. \"\n-                      \"Please use tl.float8e4b15.\", DeprecationWarning)\n-\n-    # Unsupported conversion:\n-    if (src_sca_ty.is_fp8e4b15() and not dst_sca_ty.is_fp16()) or \\\n-       (dst_sca_ty.is_fp8e4b15() and not src_sca_ty.is_fp16()):\n-        raise ValueError('fp8e4b15 can only be converted to/from fp16')\n-\n     # Casting with customized floating types involved: fp8 <=> bf16, fp16, fp32, fp64\n     if (src_sca_ty.is_fp8() and dst_sca_ty.is_floating()) or \\\n        (src_sca_ty.is_floating() and dst_sca_ty.is_fp8()):\n@@ -807,6 +796,8 @@ def _str_to_store_cache_modifier(cache_modifier):\n             cache = ir.CACHE_MODIFIER.CG\n         elif cache_modifier == \".cs\":\n             cache = ir.CACHE_MODIFIER.CS\n+        elif cache_modifier == \".wt\":\n+            cache = ir.CACHE_MODIFIER.WT\n         else:\n             raise ValueError(f\"Cache modifier {cache_modifier} not supported\")\n     return cache\n@@ -1330,6 +1321,32 @@ def wrap_tensor(x, scalar_ty):\n     )\n \n \n+# ===----------------------------------------------------------------------===\n+#                               Associative Scan\n+# ===----------------------------------------------------------------------===\n+\n+\n+def associative_scan(\n+    inputs: Sequence[tl.tensor], axis: int, region_builder_fn, builder: ir.builder\n+) -> Tuple[tl.tensor, ...]:\n+    if len(inputs) != 1:\n+        raise ValueError(\"Current implementation only support single tensor input\")\n+    shape = inputs[0].type.shape\n+\n+    def wrap_tensor(x, scalar_ty):\n+        res_ty = tl.block_type(scalar_ty, shape)\n+        return tl.tensor(x, res_ty)\n+\n+    scan_op = builder.create_scan([t.handle for t in inputs], axis)\n+    region_builder_fn(scan_op)\n+    scan_op.verify()\n+\n+    return tuple(\n+        wrap_tensor(scan_op.get_result(i), inputs[i].type.scalar)\n+        for i in range(len(inputs))\n+    )\n+\n+\n # ===----------------------------------------------------------------------===\n #                               Math\n # ===----------------------------------------------------------------------==="}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 10, "deletions": 4, "changes": 14, "file_content_changes": "@@ -25,7 +25,7 @@ def __reduce__(self):\n \n \n class Autotuner(KernelInterface):\n-    def __init__(self, fn, arg_names, configs, key, reset_to_zero, prune_configs_by: Dict = None):\n+    def __init__(self, fn, arg_names, configs, key, reset_to_zero, prune_configs_by: Dict = None, warmup=25, rep=100):\n         '''\n         :param prune_configs_by: a dict of functions that are used to prune configs, fields:\n             'perf_model': performance model used to predicate running time with different configs, returns running time\n@@ -58,6 +58,8 @@ def _hook(args):\n         self.perf_model, self.configs_top_k = perf_model, top_k\n         self.early_config_prune = early_config_prune\n         self.fn = fn\n+        self.warmup = warmup\n+        self.rep = rep\n \n     def _bench(self, *args, config, **meta):\n         # check for conflicts, i.e. meta-parameters both provided\n@@ -78,7 +80,7 @@ def kernel_call():\n             self.hook(args)\n             self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **current)\n         try:\n-            return do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n+            return do_bench(kernel_call, warmup=self.warmup, rep=self.rep, quantiles=(0.5, 0.2, 0.8))\n         except OutOfResources:\n             return [float('inf'), float('inf'), float('inf')]\n \n@@ -173,7 +175,7 @@ def __str__(self):\n         return ', '.join(res)\n \n \n-def autotune(configs, key, prune_configs_by=None, reset_to_zero=None):\n+def autotune(configs, key, prune_configs_by=None, reset_to_zero=None, warmup=25, rep=100):\n     \"\"\"\n     Decorator for auto-tuning a :code:`triton.jit`'d function.\n \n@@ -204,9 +206,13 @@ def kernel(x_ptr, x_size, **META):\n         'early_config_prune'(optional): a function used to do early prune (eg, num_stages). It takes configs:List[Config] as its input, and returns pruned configs.\n     :param reset_to_zero: a list of argument names whose value will be reset to zero before evaluating any configs.\n     :type reset_to_zero: list[str]\n+    :param warmup: Warmup time (in ms) to pass to benchmarking, defaults to 25.\n+    :type warmup: int\n+    :param rep: Repetition time (in ms) to pass to benchmarking, defaults to 100.\n+    :type rep: int\n     \"\"\"\n     def decorator(fn):\n-        return Autotuner(fn, fn.arg_names, configs, key, reset_to_zero, prune_configs_by)\n+        return Autotuner(fn, fn.arg_names, configs, key, reset_to_zero, prune_configs_by, warmup, rep)\n \n     return decorator\n "}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -208,9 +208,8 @@ def _type_of(key):\n         dtype_str = str(key).split(\".\")[-1]\n         tys = {\n             \"bool\": \"i1\",\n-            \"float8e4\": \"fp8e4\",\n             \"float8e5\": \"fp8e5\",\n-            \"float8e4b15\": \"fp8e4b15\",\n+            \"float8e4\": \"fp8e4\",\n             \"float16\": \"fp16\",\n             \"bfloat16\": \"bf16\",\n             \"float32\": \"fp32\","}, {"filename": "test/Conversion/triton_ops.mlir", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -187,3 +187,19 @@ tt.func @print_no_arg(%arg0: !tt.ptr<f32>) {\n   tt.store %arg0, %0 {cache = 1 : i32, evict = 1 : i32} : f32\n   tt.return\n }\n+\n+// CHECK-LABEL: scan_op\n+tt.func @scan_op(%ptr: tensor<1x2x4x!tt.ptr<f32>>, %v : tensor<1x2x4xf32>) {\n+  // CHECK: tt.scan\n+  // CHECK-SAME: axis = 1\n+  // CHECK: tt.scan.return\n+  // CHECK-NEXT: (tensor<1x2x4xf32>) -> tensor<1x2x4xf32>\n+  %a = \"tt.scan\"(%v) <{axis = 1 : i32}>({\n+  ^bb0(%arg0: f32, %arg1: f32):\n+    %add = arith.addf %arg0, %arg1 : f32\n+    tt.scan.return %add : f32\n+  }) : (tensor<1x2x4xf32>) -> tensor<1x2x4xf32>\n+  tt.store %ptr, %a : tensor<1x2x4xf32>\n+  tt.return\n+\n+}"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 69, "deletions": 1, "changes": 70, "file_content_changes": "@@ -1122,7 +1122,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n \n // -----\n \n-// Check if the SimplifyReduceCvt handles convert_layout lifted from the for loop.\n+// Check if SimplifyReduceCvt handles convert_layout lifted from the for loop.\n // CHECK-LABEL: reduce_cvt2\n // Match the reduction\n // CHECK: tt.reduce\n@@ -1289,6 +1289,74 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-war\n     }\n     %26 = arith.truncf %25#0 : f32 to f16\n     tt.store %arg2, %26 {cache = 1 : i32, evict = 1 : i32} : f16\n+\n+// -----\n+\n+// Check if SimplifyReduceCvt handles the cvt,reduce->reduce,cvt conversion but not the general push forward conversion\n+// CHECK-LABEL: reduce_cvt3\n+// CHECK: tt.dot\n+// CHECK-NEXT: tt.reduce\n+// CHECK: triton_gpu.convert_layout\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n+#blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n+#blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#blocked4 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#blocked5 = #triton_gpu.blocked<{sizePerThread = [2, 2], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [0, 1]}>\n+#shared1 = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  tt.func public @reduce_cvt3(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<i32> {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #blocked>\n+    %cst_0 = arith.constant dense<32> : tensor<32x1xi32, #blocked>\n+    %0 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked1>\n+    %1 = triton_gpu.convert_layout %0 : (tensor<32xi32, #blocked1>) -> tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+    %2 = tt.expand_dims %1 {axis = 1 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<32x1xi32, #blocked2>\n+    %3 = triton_gpu.convert_layout %2 : (tensor<32x1xi32, #blocked2>) -> tensor<32x1xi32, #blocked>\n+    %4 = arith.muli %3, %cst_0 : tensor<32x1xi32, #blocked>\n+    %5 = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<32x1x!tt.ptr<f16>, #blocked>\n+    %6 = tt.addptr %5, %4 : tensor<32x1x!tt.ptr<f16>, #blocked>, tensor<32x1xi32, #blocked>\n+    %7 = triton_gpu.convert_layout %0 : (tensor<32xi32, #blocked1>) -> tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>\n+    %8 = tt.expand_dims %7 {axis = 0 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>) -> tensor<1x32xi32, #blocked3>\n+    %9 = tt.broadcast %6 : (tensor<32x1x!tt.ptr<f16>, #blocked>) -> tensor<32x32x!tt.ptr<f16>, #blocked>\n+    %10 = tt.broadcast %8 : (tensor<1x32xi32, #blocked3>) -> tensor<32x32xi32, #blocked3>\n+    %11 = triton_gpu.convert_layout %10 : (tensor<32x32xi32, #blocked3>) -> tensor<32x32xi32, #blocked>\n+    %12 = tt.addptr %9, %11 : tensor<32x32x!tt.ptr<f16>, #blocked>, tensor<32x32xi32, #blocked>\n+    %13 = tt.splat %arg1 : (!tt.ptr<f16>) -> tensor<32x1x!tt.ptr<f16>, #blocked>\n+    %14 = tt.addptr %13, %4 : tensor<32x1x!tt.ptr<f16>, #blocked>, tensor<32x1xi32, #blocked>\n+    %15 = tt.broadcast %14 : (tensor<32x1x!tt.ptr<f16>, #blocked>) -> tensor<32x32x!tt.ptr<f16>, #blocked>\n+    %16 = tt.addptr %15, %11 : tensor<32x32x!tt.ptr<f16>, #blocked>, tensor<32x32xi32, #blocked>\n+    %17 = triton_gpu.convert_layout %12 : (tensor<32x32x!tt.ptr<f16>, #blocked>) -> tensor<32x32x!tt.ptr<f16>, #blocked4>\n+    %18 = tt.load %17 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf16, #blocked4>\n+    %19 = triton_gpu.convert_layout %18 : (tensor<32x32xf16, #blocked4>) -> tensor<32x32xf16, #blocked>\n+    %20 = triton_gpu.convert_layout %16 : (tensor<32x32x!tt.ptr<f16>, #blocked>) -> tensor<32x32x!tt.ptr<f16>, #blocked4>\n+    %21 = tt.load %20 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf16, #blocked4>\n+    %22 = triton_gpu.convert_layout %21 : (tensor<32x32xf16, #blocked4>) -> tensor<32x32xf16, #blocked>\n+    %23 = triton_gpu.convert_layout %22 : (tensor<32x32xf16, #blocked>) -> tensor<32x32xf16, #shared>\n+    %24 = tt.trans %23 : (tensor<32x32xf16, #shared>) -> tensor<32x32xf16, #shared1>\n+    %25 = triton_gpu.convert_layout %24 : (tensor<32x32xf16, #shared1>) -> tensor<32x32xf16, #blocked>\n+    %26 = triton_gpu.convert_layout %19 : (tensor<32x32xf16, #blocked>) -> tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked5}>>\n+    %27 = triton_gpu.convert_layout %25 : (tensor<32x32xf16, #blocked>) -> tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked5}>>\n+    %28 = triton_gpu.convert_layout %cst : (tensor<32x32xf32, #blocked>) -> tensor<32x32xf32, #blocked5>\n+    %29 = tt.dot %26, %27, %28 {allowTF32 = true} : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked5}>> * tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked5}>> -> tensor<32x32xf32, #blocked5>\n+    %30 = triton_gpu.convert_layout %29 : (tensor<32x32xf32, #blocked5>) -> tensor<32x32xf32, #blocked>\n+    %31:2 = \"tt.reduce\"(%30, %11) <{axis = 1 : i32}> ({\n+    ^bb0(%arg3: f32, %arg4: i32, %arg5: f32, %arg6: i32):\n+      %37 = \"triton_gpu.cmpf\"(%arg3, %arg5) <{predicate = 1 : i64}> : (f32, f32) -> i1\n+      %38 = \"triton_gpu.cmpi\"(%arg4, %arg6) <{predicate = 2 : i64}> : (i32, i32) -> i1\n+      %39 = arith.andi %37, %38 : i1\n+      %40 = \"triton_gpu.cmpf\"(%arg3, %arg5) <{predicate = 2 : i64}> : (f32, f32) -> i1\n+      %41 = arith.ori %40, %39 : i1\n+      %42 = arith.select %41, %arg3, %arg5 : f32\n+      %43 = arith.select %41, %arg4, %arg6 : i32\n+      tt.reduce.return %42, %43 : f32, i32\n+    }) : (tensor<32x32xf32, #blocked>, tensor<32x32xi32, #blocked>) -> (tensor<32xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>, tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>)\n+    %32 = triton_gpu.convert_layout %31#1 : (tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<32xi32, #blocked1>\n+    %33 = tt.splat %arg2 : (!tt.ptr<i32>) -> tensor<32x!tt.ptr<i32>, #blocked1>\n+    %34 = tt.addptr %33, %0 : tensor<32x!tt.ptr<i32>, #blocked1>, tensor<32xi32, #blocked1>\n+    %35 = triton_gpu.convert_layout %34 : (tensor<32x!tt.ptr<i32>, #blocked1>) -> tensor<32x!tt.ptr<i32>, #blocked1>\n+    %36 = triton_gpu.convert_layout %32 : (tensor<32xi32, #blocked1>) -> tensor<32xi32, #blocked1>\n+    tt.store %35, %36 {cache = 1 : i32, evict = 1 : i32} : tensor<32xi32, #blocked1>\n     tt.return\n   }\n }"}]