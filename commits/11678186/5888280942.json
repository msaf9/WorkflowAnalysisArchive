[{"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -371,6 +371,8 @@ SmallVector<unsigned, 3>\n mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n                        ArrayRef<int64_t> shape);\n \n+Value getParentValueWithSameEncoding(Attribute layout, Value value);\n+\n } // namespace mlir\n \n #endif // TRITON_ANALYSIS_UTILITY_H"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -36,6 +36,9 @@ SmallVector<unsigned> getThreadsPerWarp(Attribute layout);\n // elements. If you want non-replicated warps, use getWarpsPerCTAWithUniqueData.\n SmallVector<unsigned> getWarpsPerCTA(Attribute layout);\n \n+SmallVector<unsigned> getSizePerThread(BlockedEncodingAttr layout);\n+SmallVector<unsigned> getSizePerThread(Value value);\n+\n SmallVector<unsigned> getSizePerThread(Attribute layout,\n                                        ArrayRef<int64_t> shapePerCTA);\n \n@@ -90,6 +93,11 @@ SmallVector<unsigned> getCTAOrder(Attribute layout);\n  * (3) In the implementation of emitIndices, ShapePerCTATile will\n  *     be replicated or wraped to fit ShapePerCTA.\n  */\n+SmallVector<unsigned> getShapePerCTATile(BlockedEncodingAttr layout);\n+SmallVector<unsigned> getShapePerCTATile(MmaEncodingAttr layout,\n+                                         RankedTensorType inputType);\n+SmallVector<unsigned> getShapePerCTATile(Value value);\n+\n SmallVector<unsigned> getShapePerCTATile(Attribute layout,\n                                          ArrayRef<int64_t> shapePerCTA);\n "}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -87,8 +87,8 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n \n   auto srcShapePerCTA = getShapePerCTA(srcTy);\n   auto dstShapePerCTA = getShapePerCTA(dstTy);\n-  auto srcShapePerCTATile = getShapePerCTATile(srcLayout, srcShapePerCTA);\n-  auto dstShapePerCTATile = getShapePerCTATile(dstLayout, dstShapePerCTA);\n+  auto srcShapePerCTATile = getShapePerCTATile(op.getSrc());\n+  auto dstShapePerCTATile = getShapePerCTATile(op.getResult());\n \n   unsigned rank = dstTy.getRank();\n   SmallVector<unsigned> paddedRepShape(rank);"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -750,5 +750,8 @@ mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n                        ArrayRef<int64_t> shape) {\n   return {0};\n }\n+Value getParentValueWithSameEncoding(Attribute layout, Value value) {\n+  return {};\n+}\n \n } // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 35, "deletions": 39, "changes": 74, "file_content_changes": "@@ -103,16 +103,17 @@ struct ConvertLayoutOpConversion\n \n private:\n   SmallVector<Value>\n-  getMultiDimOffset(Attribute layout, Location loc,\n-                    ConversionPatternRewriter &rewriter, unsigned elemId,\n-                    RankedTensorType type,\n+  getMultiDimOffset(Location loc, ConversionPatternRewriter &rewriter,\n+                    Value value, unsigned elemId,\n                     ArrayRef<unsigned> multiDimCTAInRepId,\n                     ArrayRef<unsigned> shapePerCTATile) const {\n+    auto type = value.getType().cast<RankedTensorType>();\n     auto shape = type.getShape();\n+    auto layout = type.getEncoding();\n     unsigned rank = shape.size();\n     if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n       auto multiDimOffsetFirstElem =\n-          emitBaseIndexForLayout(loc, rewriter, blockedLayout, type, false);\n+          emitBaseIndexForLayout(loc, rewriter, value, false);\n       SmallVector<Value> multiDimOffset(rank);\n       SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n           elemId, getSizePerThread(layout, {}), getOrder(layout));\n@@ -127,21 +128,19 @@ struct ConvertLayoutOpConversion\n     if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n       unsigned dim = sliceLayout.getDim();\n       auto parentEncoding = sliceLayout.getParent();\n-      auto parentShape = sliceLayout.paddedShape(shape);\n-      auto parentTy = RankedTensorType::get(parentShape, type.getElementType(),\n-                                            parentEncoding);\n-      auto offsets = emitOffsetForLayout(layout, type);\n-      auto parentOffset = emitOffsetForLayout(parentEncoding, parentTy);\n+      Value parentValue = getParentValueWithSameEncoding(parentEncoding, value);\n+      auto offsets = emitOffsetForLayout(value);\n+      auto parentOffset = emitOffsetForLayout(parentValue);\n       SmallVector<int> idxs;\n       for (SmallVector<unsigned> off : offsets) {\n         off.insert(off.begin() + dim, 0);\n         auto it = std::find(parentOffset.begin(), parentOffset.end(), off);\n         idxs.push_back(std::distance(parentOffset.begin(), it));\n       }\n-      auto multiDimOffsetParent = getMultiDimOffset(\n-          parentEncoding, loc, rewriter, idxs[elemId], parentTy,\n-          sliceLayout.paddedShape(multiDimCTAInRepId),\n-          sliceLayout.paddedShape(shapePerCTATile));\n+      auto multiDimOffsetParent =\n+          getMultiDimOffset(loc, rewriter, parentValue, idxs[elemId],\n+                            sliceLayout.paddedShape(multiDimCTAInRepId),\n+                            sliceLayout.paddedShape(shapePerCTATile));\n       SmallVector<Value> multiDimOffset(rank);\n       for (unsigned d = 0; d < rank + 1; ++d) {\n         if (d == dim)\n@@ -238,20 +237,21 @@ struct ConvertLayoutOpConversion\n \n   // shared memory rd/st for blocked or mma layout with data padding\n   void processReplica(Location loc, ConversionPatternRewriter &rewriter,\n-                      bool stNotRd, RankedTensorType type,\n+                      bool stNotRd, Value value,\n                       ArrayRef<unsigned> numCTAsEachRep,\n                       ArrayRef<unsigned> multiDimRepId, unsigned vec,\n                       ArrayRef<unsigned> paddedRepShape,\n                       ArrayRef<unsigned> outOrd, SmallVector<Value> &vals,\n                       Value smemBase) const {\n+    auto type = value.getType().cast<RankedTensorType>();\n     auto accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n     auto layout = type.getEncoding();\n     auto rank = type.getRank();\n     auto shapePerCTA = getShapePerCTA(layout, type.getShape());\n     auto sizePerThread = getSizePerThread(layout, shapePerCTA);\n     auto accumSizePerThread = product<unsigned>(sizePerThread);\n     SmallVector<unsigned> numCTATiles(rank);\n-    auto shapePerCTATile = getShapePerCTATile(layout, shapePerCTA);\n+    auto shapePerCTATile = getShapePerCTATile(value);\n     auto order = getOrder(layout);\n     for (unsigned d = 0; d < rank; ++d) {\n       numCTATiles[d] = ceil<unsigned>(shapePerCTA[d], shapePerCTATile[d]);\n@@ -282,9 +282,8 @@ struct ConvertLayoutOpConversion\n       //       consider of caching the index calculation result in case\n       //       of performance issue observed.\n       for (unsigned elemId = 0; elemId < accumSizePerThread; elemId += vec) {\n-        SmallVector<Value> multiDimOffset =\n-            getMultiDimOffset(layout, loc, rewriter, elemId, type,\n-                              multiDimCTAInRepId, shapePerCTATile);\n+        SmallVector<Value> multiDimOffset = getMultiDimOffset(\n+            loc, rewriter, value, elemId, multiDimCTAInRepId, shapePerCTATile);\n         Value offset =\n             linearize(rewriter, loc, multiDimOffset, paddedRepShape, outOrd);\n         auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n@@ -323,14 +322,15 @@ struct ConvertLayoutOpConversion\n   // structure, add a new simple but clear implementation for it to avoid\n   // modifying the logic of the existing one.\n   void processReplicaForMMAV1(Location loc, ConversionPatternRewriter &rewriter,\n-                              bool stNotRd, RankedTensorType type,\n+                              bool stNotRd, Value value,\n                               ArrayRef<unsigned> multiDimRepId, unsigned vec,\n                               ArrayRef<unsigned> paddedRepShape,\n                               ArrayRef<unsigned> outOrd,\n                               SmallVector<Value> &vals, Value smemBase,\n                               ArrayRef<int64_t> shape,\n                               bool isDestMma = false) const {\n     unsigned accumNumCTAsEachRep = 1;\n+    RankedTensorType type = value.getType().cast<RankedTensorType>();\n     auto layout = type.getEncoding();\n     MmaEncodingAttr mma = layout.dyn_cast<MmaEncodingAttr>();\n     auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>();\n@@ -344,8 +344,7 @@ struct ConvertLayoutOpConversion\n     SmallVector<unsigned> numCTAs(rank, 1);\n     SmallVector<unsigned> numCTAsEachRep(rank, 1);\n     SmallVector<int64_t> shapePerCTA = getShapePerCTA(layout, shape);\n-    SmallVector<unsigned> shapePerCTATile =\n-        getShapePerCTATile(layout, shapePerCTA);\n+    SmallVector<unsigned> shapePerCTATile = getShapePerCTATile(value);\n     auto elemTy = type.getElementType();\n \n     int ctaId = 0;\n@@ -373,9 +372,8 @@ struct ConvertLayoutOpConversion\n       for (unsigned elemId = 0; elemId < accumSizePerThread; ++elemId) {\n         // TODO[Superjomn]: Move the coordinate computation out of loop, it is\n         // duplicate in Volta.\n-        SmallVector<Value> multiDimOffset =\n-            getMultiDimOffset(layout, loc, rewriter, elemId, type,\n-                              multiDimCTAInRepId, shapePerCTATile);\n+        SmallVector<Value> multiDimOffset = getMultiDimOffset(\n+            loc, rewriter, value, elemId, multiDimCTAInRepId, shapePerCTATile);\n         coord2val[elemId] = std::make_pair(multiDimOffset, vals[elemId]);\n       }\n \n@@ -450,8 +448,7 @@ struct ConvertLayoutOpConversion\n     {\n       auto inVals = getTypeConverter()->unpackLLElements(loc, adaptor.getSrc(),\n                                                          rewriter, srcTy);\n-      auto inIndices =\n-          emitIndices(loc, rewriter, srcLayout, srcTy, /*withCTAOffset*/ false);\n+      auto inIndices = emitIndices(loc, rewriter, src, /*withCTAOffset*/ false);\n \n       assert(inIndices.size() == inVals.size() &&\n              \"Unexpected number of indices emitted\");\n@@ -474,8 +471,7 @@ struct ConvertLayoutOpConversion\n         srcShapePerCTACache.push_back(i32_val(srcShapePerCTA[i]));\n \n       SmallVector<Value> outVals;\n-      auto outIndices =\n-          emitIndices(loc, rewriter, dstLayout, dstTy, /*withCTAOffset*/ true);\n+      auto outIndices = emitIndices(loc, rewriter, dst, /*withCTAOffset*/ true);\n \n       for (unsigned i = 0; i < outIndices.size(); ++i) {\n         auto coord = outIndices[i];\n@@ -536,8 +532,8 @@ struct ConvertLayoutOpConversion\n     SmallVector<unsigned> inNumCTAs(rank);\n     SmallVector<unsigned> outNumCTAs(rank);\n     // [benzh] here logic need more check: src and dst using same shape???\n-    auto srcShapePerCTATile = getShapePerCTATile(srcLayout, srcTy.getShape());\n-    auto dstShapePerCTATile = getShapePerCTATile(dstLayout, shape);\n+    auto srcShapePerCTATile = getShapePerCTATile(src);\n+    auto dstShapePerCTATile = getShapePerCTATile(dst);\n     auto shapePerCTA = getShapePerCTA(srcLayout, shape);\n \n     // For Volta, all the coords for a CTA are calculated.\n@@ -613,13 +609,13 @@ struct ConvertLayoutOpConversion\n           srcLayout.isa<SliceEncodingAttr>() ||\n           srcLayout.isa<MmaEncodingAttr>()) {\n         if (isSrcMmaV1)\n-          processReplicaForMMAV1(loc, rewriter, /*stNotRd*/ true, srcTy,\n+          processReplicaForMMAV1(loc, rewriter, /*stNotRd*/ true, src,\n                                  multiDimRepId, inVec, paddedRepShape, outOrd,\n                                  vals, smemBase, shape);\n         else\n-          processReplica(loc, rewriter, /*stNotRd*/ true, srcTy,\n-                         inNumCTAsEachRep, multiDimRepId, inVec, paddedRepShape,\n-                         outOrd, vals, smemBase);\n+          processReplica(loc, rewriter, /*stNotRd*/ true, src, inNumCTAsEachRep,\n+                         multiDimRepId, inVec, paddedRepShape, outOrd, vals,\n+                         smemBase);\n       } else {\n         assert(0 && \"ConvertLayout with input layout not implemented\");\n         return failure();\n@@ -646,11 +642,11 @@ struct ConvertLayoutOpConversion\n           dstLayout.isa<SliceEncodingAttr>() ||\n           dstLayout.isa<MmaEncodingAttr>()) {\n         if (isDstMmaV1)\n-          processReplicaForMMAV1(loc, rewriter, /*stNotRd*/ false, dstTy,\n+          processReplicaForMMAV1(loc, rewriter, /*stNotRd*/ false, dst,\n                                  multiDimRepId, outVec, paddedRepShape, outOrd,\n                                  outVals, smemBase, shape, /*isDestMma=*/true);\n         else\n-          processReplica(loc, rewriter, /*stNotRd*/ false, dstTy,\n+          processReplica(loc, rewriter, /*stNotRd*/ false, dst,\n                          outNumCTAsEachRep, multiDimRepId, outVec,\n                          paddedRepShape, outOrd, outVals, smemBase);\n       } else {\n@@ -688,7 +684,7 @@ struct ConvertLayoutOpConversion\n \n     auto srcStrides =\n         getStridesFromShapeAndOrder(srcShape, inOrd, loc, rewriter);\n-    auto dstIndices = emitIndices(loc, rewriter, dstLayout, dstTy);\n+    auto dstIndices = emitIndices(loc, rewriter, dst);\n \n     SmallVector<Value> outVals = loadSharedToDistributed(\n         dst, dstIndices, src, smemObj, elemTy, loc, rewriter);\n@@ -760,7 +756,7 @@ struct ConvertLayoutOpConversion\n       auto ptrI8SharedTy = LLVM::LLVMPointerType::get(\n           typeConverter->convertType(rewriter.getI8Type()), 3);\n \n-      uint32_t rowsPerRep = getShapePerCTATile(mmaLayout, srcShapePerCTA)[0];\n+      uint32_t rowsPerRep = getShapePerCTATile(src)[0];\n \n       Value threadId = getThreadId(rewriter, loc);\n       Value warpId = udiv(threadId, i32_val(32));\n@@ -814,7 +810,7 @@ struct ConvertLayoutOpConversion\n     } else {\n       auto dstStrides =\n           getStridesFromShapeAndOrder(dstShapePerCTA, outOrd, loc, rewriter);\n-      auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTy, false);\n+      auto srcIndices = emitIndices(loc, rewriter, src, false);\n       storeDistributedToShared(src, adaptor.getSrc(), dstStrides, srcIndices,\n                                dst, smemBase, elemTy, loc, rewriter);\n     }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -35,7 +35,7 @@ getThreadIds(Value threadId, ArrayRef<unsigned int> shapePerCTATile,\n // Get shapePerCTATile for M or N axis.\n int getShapePerCTATileForMN(BlockedEncodingAttr layout, bool isM) {\n   auto order = layout.getOrder();\n-  auto shapePerCTATile = getShapePerCTATile(layout, {});\n+  auto shapePerCTATile = getShapePerCTATile(layout);\n \n   int mShapePerCTATile =\n       order[0] == 1 ? shapePerCTATile[order[1]] : shapePerCTATile[order[0]];\n@@ -110,7 +110,7 @@ Value loadAFMA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n   int K = aShapePerCTA[1];\n   int M = aShapePerCTA[0];\n \n-  auto shapePerCTATile = getShapePerCTATile(dLayout, {});\n+  auto shapePerCTATile = getShapePerCTATile(dLayout);\n   auto sizePerThread = getSizePerThread(dLayout, {});\n \n   Value _0 = i32_val(0);\n@@ -174,7 +174,7 @@ Value loadBFMA(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n   int K = bShapePerCTA[0];\n   int N = bShapePerCTA[1];\n \n-  auto shapePerCTATile = getShapePerCTATile(dLayout, {});\n+  auto shapePerCTATile = getShapePerCTATile(dLayout);\n   auto sizePerThread = getSizePerThread(dLayout, {});\n \n   Value _0 = i32_val(0);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/FMA.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -54,7 +54,7 @@ LogicalResult convertFMADot(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n   Value llB = adaptor.getB();\n \n   auto sizePerThread = getSizePerThread(dLayout, {});\n-  auto shapePerCTATile = getShapePerCTATile(dLayout, {});\n+  auto shapePerCTATile = mlir::triton::gpu::getShapePerCTATile(D);\n \n   int K = aShapePerCTA[1];\n   int M = aShapePerCTA[0];"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/WGMMA.cpp", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -286,7 +286,7 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n   int N = instrShape[1];\n   int K = instrShape[2];\n \n-  auto shapePerCTATile = getShapePerCTATile(mmaEncoding, dShapePerCTA);\n+  auto shapePerCTATile = getShapePerCTATile(d);\n   int numRepM = ceil<unsigned>(dShapePerCTA[0], shapePerCTATile[0]);\n   int numRepN = ceil<unsigned>(dShapePerCTA[1], shapePerCTATile[1]);\n   int numRepK = ceil<unsigned>(aTensorTy.getShape()[1], instrShape[2]);\n@@ -357,14 +357,15 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n }\n \n // Loading $c to registers, returns a Value.\n-Value loadC(Value tensor, Value llTensor) {\n+Value loadC(Value dTensor, Value tensor, Value llTensor) {\n   auto tensorTy = tensor.getType().cast<RankedTensorType>();\n   auto mmaEncoding = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>();\n   assert(mmaEncoding && \"Currently, we only support $c with a mma layout.\");\n   auto shapePerCTA = getShapePerCTA(tensorTy);\n   auto instrShape = mmaVersionToInstrShape(mmaEncoding, shapePerCTA);\n   auto wpt = mmaEncoding.getWarpsPerCTA();\n-  auto shapePerCTATile = getShapePerCTATile(mmaEncoding, shapePerCTA);\n+  // benzh@need from ACC\n+  auto shapePerCTATile = getShapePerCTATile(dTensor);\n \n   int numRepM = ceil<unsigned>(shapePerCTA[0], shapePerCTATile[0]);\n   int numRepN = ceil<unsigned>(shapePerCTA[1], shapePerCTATile[1]);\n@@ -395,7 +396,7 @@ LogicalResult convertWGMMA(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n   Value llA, llB, llC;\n   llA = adaptor.getA();\n   llB = adaptor.getB();\n-  llC = loadC(C, adaptor.getC());\n+  llC = loadC(op.getD(), C, adaptor.getC());\n \n   auto smemObjA = getSharedMemoryObjectFromStruct(loc, llA, rewriter);\n   auto smemObjB = getSharedMemoryObjectFromStruct(loc, llB, rewriter);\n@@ -423,7 +424,7 @@ LogicalResult convertAsyncWGMMA(triton::nvidia_gpu::DotAsyncOp op,\n   Value llA, llB, llC;\n   llA = adaptor.getA();\n   llB = adaptor.getB();\n-  llC = loadC(C, adaptor.getC());\n+  llC = loadC(op.getD(), C, adaptor.getC());\n \n   auto smemObjA = getSharedMemoryObjectFromStruct(loc, llA, rewriter);\n   auto smemObjB = getSharedMemoryObjectFromStruct(loc, llB, rewriter);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -306,7 +306,7 @@ struct StoreOpConversion\n       vec = std::min(vec, maskAlign);\n     }\n \n-    Value mask = getMask(valueTy, rewriter, loc);\n+    Value mask = getMask(value, rewriter, loc);\n     const size_t dtsize =\n         std::max<int>(1, valueElemTy.getIntOrFloatBitWidth() / 8);\n     const size_t valueElemNBits = dtsize * 8;\n@@ -669,7 +669,7 @@ struct AtomicCASOpConversion\n         TensorTy ? getTypeConverter()->convertType(TensorTy.getElementType())\n                  : valueTy;\n     auto valueElemNBits = valueElemTy.getIntOrFloatBitWidth();\n-    Value mask = getMask(valueTy, rewriter, loc);\n+    Value mask = getMask(op.getResult(), rewriter, loc);\n \n     Value atomPtr = getSharedMemoryBase(loc, rewriter, op.getOperation());\n     atomPtr = bitcast(atomPtr, ptr_ty(valueElemTy, 3));\n@@ -762,7 +762,7 @@ struct AtomicRMWOpConversion\n       // mask\n       numElems = tensorTy.getNumElements();\n     }\n-    Value mask = getMask(valueTy, rewriter, loc);\n+    Value mask = getMask(op.getResult(), rewriter, loc);\n \n     auto vecTy = vec_ty(valueElemTy, vec);\n     SmallVector<Value> resultVals(elemsPerThread);\n@@ -930,7 +930,7 @@ struct InsertSliceOpConversion\n     auto smemBase = gep(elemPtrTy, smemObj.base, offset);\n \n     auto llSrc = adaptor.getSource();\n-    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTy);\n+    auto srcIndices = emitIndices(loc, rewriter, src);\n     storeDistributedToShared(src, llSrc, srcStrides, srcIndices, dst, smemBase,\n                              elemTy, loc, rewriter);\n     // Barrier is not necessary.\n@@ -1044,9 +1044,9 @@ struct InsertSliceAsyncOpConversion\n     auto sizePerThread = srcBlockedLayout.getSizePerThread();\n     auto threadsPerCTA = getThreadsPerCTA(srcBlockedLayout);\n     auto inOrder = srcBlockedLayout.getOrder();\n-    DenseMap<unsigned, Value> sharedPtrs =\n-        getSwizzledSharedPtrs(loc, inVec, srcTy, resSharedLayout, resElemTy,\n-                              smemObj, rewriter, offsetVals, srcStrides);\n+    DenseMap<unsigned, Value> sharedPtrs = getSwizzledSharedPtrs(\n+        loc, inVec, src, srcTy, resSharedLayout, resElemTy, smemObj, rewriter,\n+        offsetVals, srcStrides);\n \n     // If perPhase * maxPhase > threadsPerCTA, we will have elements\n     // that share the same tile indices. The index calculation will\n@@ -1058,7 +1058,7 @@ struct InsertSliceAsyncOpConversion\n     // single vector read into multiple ones\n     auto numVecCols = std::max<unsigned>(inVec / outVec, 1);\n \n-    auto srcIndices = emitIndices(loc, rewriter, srcBlockedLayout, srcTy);\n+    auto srcIndices = emitIndices(loc, rewriter, src);\n \n     for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n       // 16 * 8 = 128bits"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 5, "deletions": 10, "changes": 15, "file_content_changes": "@@ -305,11 +305,8 @@ struct ReduceOpConversion\n       if (auto resultTy =\n               op.getResult()[i].getType().dyn_cast<RankedTensorType>()) {\n         // nd-tensor where n >= 1\n-\n-        auto resultLayout = resultTy.getEncoding();\n-\n         unsigned resultElems = getTotalElemsPerThread(resultTy);\n-        auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultTy);\n+        auto resultIndices = emitIndices(loc, rewriter, op.getResult()[i]);\n         assert(resultIndices.size() == resultElems);\n \n         SmallVector<Value> resultVals(resultElems);\n@@ -391,11 +388,10 @@ struct ReduceOpConversion\n     RankedTensorType operandType = op.getInputTypes()[0];\n     // Assumes offsets don't actually depend on type\n     SmallVector<SmallVector<unsigned>> offset =\n-        emitOffsetForLayout(helper.getSrcLayout(), operandType);\n+        emitOffsetForLayout(helper.getSrcValue());\n     unsigned srcElems = getTotalElemsPerThread(operandType);\n     auto *combineOp = &op.getCombineOp();\n-    auto srcIndices =\n-        emitIndices(op.getLoc(), rewriter, helper.getSrcLayout(), operandType);\n+    auto srcIndices = emitIndices(op.getLoc(), rewriter, helper.getSrcValue());\n     // reduce within threads\n     for (unsigned i = 0; i < srcElems; ++i) {\n       SmallVector<unsigned> key = offset[i];\n@@ -473,7 +469,7 @@ struct ReduceOpConversion\n         auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n         unsigned resultElems = getTotalElemsPerThread(resultTy);\n         SmallVector<SmallVector<unsigned>> resultOffset =\n-            emitOffsetForLayout(resultLayout, resultTy);\n+            emitOffsetForLayout(op.getResult()[i]);\n         SmallVector<Value> resultVals;\n         for (int j = 0; j < resultElems; j++) {\n           auto key = resultOffset[j];\n@@ -615,9 +611,8 @@ struct ReduceOpConversion\n       if (auto resultTy =\n               op.getResult()[i].getType().dyn_cast<RankedTensorType>()) {\n         // nd-tensor where n >= 1\n-        auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n         unsigned resultElems = getTotalElemsPerThread(resultTy);\n-        auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultTy);\n+        auto resultIndices = emitIndices(loc, rewriter, op.getResult()[i]);\n         assert(resultIndices.size() == resultElems);\n \n         SmallVector<Value> resultVals(resultElems);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -65,8 +65,8 @@ struct BroadcastOpConversion\n \n     assert(rank == resultTy.getRank());\n     auto order = triton::gpu::getOrder(srcLayout);\n-    auto srcOffsets = emitOffsetForLayout(srcLayout, srcTy);\n-    auto resultOffsets = emitOffsetForLayout(resultLayout, resultTy);\n+    auto srcOffsets = emitOffsetForLayout(op.getSrc());\n+    auto resultOffsets = emitOffsetForLayout(result);\n     SmallVector<Value> srcVals =\n         getTypeConverter()->unpackLLElements(loc, src, rewriter, srcTy);\n \n@@ -365,7 +365,7 @@ struct MakeRangeOpConversion\n     auto elemTy = rankedTy.getElementType();\n     assert(elemTy.isInteger(32));\n     Value start = createIndexAttrConstant(rewriter, loc, elemTy, op.getStart());\n-    auto idxs = emitIndices(loc, rewriter, layout, rankedTy);\n+    auto idxs = emitIndices(loc, rewriter, op.getResult());\n     unsigned elems = idxs.size();\n     SmallVector<Value> retVals(elems);\n     // TODO: slice layout has more elements than expected."}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 64, "deletions": 58, "changes": 122, "file_content_changes": "@@ -1,6 +1,6 @@\n #ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_BASE_H\n #define TRITON_CONVERSION_TRITONGPU_TO_LLVM_BASE_H\n-\n+#include \"llvm/ADT/TypeSwitch.h\"\n // TODO: refactor so that it doesn't fail if Allocation.h\n // is included after utility.h (due to conflict in `store` macro\n // and <atomic>\n@@ -290,13 +290,12 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return base;\n   }\n \n-  DenseMap<unsigned, Value>\n-  getSwizzledSharedPtrs(Location loc, unsigned inVec, RankedTensorType srcTy,\n-                        triton::gpu::SharedEncodingAttr resSharedLayout,\n-                        Type resElemTy, SharedMemoryObject smemObj,\n-                        ConversionPatternRewriter &rewriter,\n-                        SmallVectorImpl<Value> &offsetVals,\n-                        SmallVectorImpl<Value> &srcStrides) const {\n+  DenseMap<unsigned, Value> getSwizzledSharedPtrs(\n+      Location loc, unsigned inVec, Value src, RankedTensorType srcTy,\n+      triton::gpu::SharedEncodingAttr resSharedLayout, Type resElemTy,\n+      SharedMemoryObject smemObj, ConversionPatternRewriter &rewriter,\n+      SmallVectorImpl<Value> &offsetVals,\n+      SmallVectorImpl<Value> &srcStrides) const {\n     // This utililty computes the pointers for accessing the provided swizzled\n     // shared memory layout `resSharedLayout`. More specifically, it computes,\n     // for all indices (row, col) of `srcEncoding` such that idx % inVec = 0,\n@@ -339,7 +338,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto inOrder = triton::gpu::getOrder(srcEncoding);\n     auto outOrder = triton::gpu::getOrder(resSharedLayout);\n     // Tensor indices held by the current thread, as LLVM values\n-    auto srcIndices = emitIndices(loc, rewriter, srcEncoding, srcTy, false);\n+    auto srcIndices = emitIndices(loc, rewriter, src, false);\n     // Swizzling with leading offsets (e.g. Hopper GMMA)\n     unsigned swizzlingByteWidth = 0;\n     if (resSharedLayout.getHasLeadingOffset()) {\n@@ -461,7 +460,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     assert(outElems == dstIndices.size());\n \n     DenseMap<unsigned, Value> sharedPtrs = getSwizzledSharedPtrs(\n-        loc, outVec, dstTy, srcSharedLayout, srcElemTy, smemObj, rewriter,\n+        loc, outVec, dst, dstTy, srcSharedLayout, srcElemTy, smemObj, rewriter,\n         smemObj.offsets, smemObj.strides);\n     assert(outElems % minVec == 0 && \"Unexpected number of elements\");\n     unsigned numVecs = outElems / minVec;\n@@ -517,9 +516,9 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     SmallVector<Value> offsetVals = {i32_val(0), i32_val(0)};\n     SharedMemoryObject smemObj(smemBase, srcStrides, offsetVals);\n \n-    DenseMap<unsigned, Value> sharedPtrs =\n-        getSwizzledSharedPtrs(loc, inVec, srcTy, dstSharedLayout, dstElemTy,\n-                              smemObj, rewriter, offsetVals, srcStrides);\n+    DenseMap<unsigned, Value> sharedPtrs = getSwizzledSharedPtrs(\n+        loc, inVec, src, srcTy, dstSharedLayout, dstElemTy, smemObj, rewriter,\n+        offsetVals, srcStrides);\n \n     for (unsigned i = 0; i < numElems; ++i) {\n       if (i % minVec == 0)\n@@ -536,8 +535,9 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   // -----------------------------------------------------------------------\n   // Utilities\n   // -----------------------------------------------------------------------\n-  Value getMask(Type valueTy, ConversionPatternRewriter &rewriter,\n+  Value getMask(Value value, ConversionPatternRewriter &rewriter,\n                 Location loc) const {\n+    Type valueTy = value.getType();\n     auto tensorTy = valueTy.dyn_cast<RankedTensorType>();\n     Value mask = int_val(1, 1);\n     auto tid = tid_val();\n@@ -550,8 +550,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       auto threadsPerWarp = triton::gpu::getThreadsPerWarp(layout);\n       auto warpsPerCTA = triton::gpu::getWarpsPerCTA(layout);\n       auto order = triton::gpu::getOrder(layout);\n-      auto shapePerCTATile =\n-          triton::gpu::getShapePerCTATile(layout, shapePerCTA);\n+      auto shapePerCTATile = triton::gpu::getShapePerCTATile(value);\n       Value warpSize = i32_val(32);\n       Value laneId = urem(tid, warpSize);\n       Value warpId = udiv(tid, warpSize);\n@@ -642,9 +641,10 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n \n   SmallVector<Value> emitBaseIndexForLayout(Location loc,\n                                             ConversionPatternRewriter &rewriter,\n-                                            Attribute layout,\n-                                            RankedTensorType type,\n+                                            Value value,\n                                             bool withCTAOffset) const {\n+    auto type = value.getType().cast<RankedTensorType>();\n+    auto layout = type.getEncoding();\n     auto shape = type.getShape();\n     IndexCacheKeyT key{layout, type, withCTAOffset};\n     auto cache = indexCacheInfo.baseIndexCache;\n@@ -670,11 +670,9 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n                                                           mmaLayout, type);\n       } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n         auto parentLayout = sliceLayout.getParent();\n-        auto parentShape = sliceLayout.paddedShape(type.getShape());\n-        RankedTensorType parentTy = RankedTensorType::get(\n-            parentShape, type.getElementType(), parentLayout);\n-        result = emitBaseIndexForLayout(loc, rewriter, parentLayout, parentTy,\n-                                        withCTAOffset);\n+        Value parentValue = getParentValueWithSameEncoding(parentLayout, value);\n+        result =\n+            emitBaseIndexForLayout(loc, rewriter, parentValue, withCTAOffset);\n         result.erase(result.begin() + sliceLayout.getDim());\n         // CTAOffset has been added in emitBaseIndexForLayout of parentLayout\n         return result;\n@@ -695,8 +693,9 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     }\n   }\n \n-  SmallVector<SmallVector<unsigned>>\n-  emitOffsetForLayout(Attribute layout, RankedTensorType type) const {\n+  SmallVector<SmallVector<unsigned>> emitOffsetForLayout(Value value) const {\n+    RankedTensorType type = value.getType().cast<RankedTensorType>();\n+    Attribute layout = type.getEncoding();\n     if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n       return emitOffsetForBlockedLayout(blockedLayout, type);\n     if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n@@ -705,19 +704,22 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       if (mmaLayout.isAmpere())\n         return emitOffsetForMmaLayoutV2(mmaLayout, type);\n       if (mmaLayout.isHopper())\n-        return emitOffsetForMmaLayoutV3(mmaLayout, type);\n+        return emitOffsetForMmaLayoutV3(value, mmaLayout, type);\n     }\n     if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>())\n-      return emitOffsetForSliceLayout(sliceLayout, type);\n+      return emitOffsetForSliceLayout(value, sliceLayout, type);\n     llvm_unreachable(\"unsupported emitOffsetForLayout\");\n   }\n \n   // -----------------------------------------------------------------------\n   // Emit indices\n   // -----------------------------------------------------------------------\n-  SmallVector<SmallVector<Value>>\n-  emitIndices(Location loc, ConversionPatternRewriter &b, Attribute layout,\n-              RankedTensorType type, bool withCTAOffset = true) const {\n+  SmallVector<SmallVector<Value>> emitIndices(Location loc,\n+                                              ConversionPatternRewriter &b,\n+                                              Value value,\n+                                              bool withCTAOffset = true) const {\n+    auto type = value.getType().cast<RankedTensorType>();\n+    auto layout = type.getEncoding();\n     IndexCacheKeyT key{layout, type, withCTAOffset};\n     auto cache = indexCacheInfo.indexCache;\n     auto insertPt = indexCacheInfo.indexInsertPoint;\n@@ -728,20 +730,18 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       if (cache)\n         restoreInsertionPointIfSet(insertPt, b);\n       SmallVector<SmallVector<Value>> result;\n-      if (auto blocked = layout.dyn_cast<BlockedEncodingAttr>()) {\n-        result = emitIndicesForDistributedLayout(loc, b, blocked, type,\n-                                                 withCTAOffset);\n-      } else if (auto mma = layout.dyn_cast<MmaEncodingAttr>()) {\n-        result =\n-            emitIndicesForDistributedLayout(loc, b, mma, type, withCTAOffset);\n-      } else if (auto slice = layout.dyn_cast<SliceEncodingAttr>()) {\n-        result =\n-            emitIndicesForDistributedLayout(loc, b, slice, type, withCTAOffset);\n-      } else {\n-        llvm_unreachable(\n-            \"emitIndices for layouts other than blocked & slice not \"\n-            \"implemented yet\");\n-      }\n+      llvm::TypeSwitch<Attribute>(layout)\n+          .Case<BlockedEncodingAttr, MmaEncodingAttr, SliceEncodingAttr>(\n+              [&](auto attr) {\n+                result = emitIndicesForDistributedLayout(loc, b, value,\n+                                                         withCTAOffset);\n+              })\n+          .Default([](auto attr) {\n+            llvm_unreachable(\n+                \"emitIndices for layouts other than blocked & slice not \"\n+                \"implemented yet\");\n+          });\n+\n       if (cache) {\n         cache->insert(std::make_pair(key, result));\n         *insertPt = b.saveInsertionPoint();\n@@ -818,7 +818,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto warpsPerCTA = blockedLayout.getWarpsPerCTA();\n     auto order = blockedLayout.getOrder();\n     auto shapePerCTA = triton::gpu::getShapePerCTA(blockedLayout, shape);\n-    auto shapePerCTATile = getShapePerCTATile(blockedLayout, shapePerCTA);\n+    auto shapePerCTATile = getShapePerCTATile(blockedLayout);\n \n     unsigned rank = shape.size();\n     SmallVector<unsigned> tilesPerDim(rank);\n@@ -1066,13 +1066,18 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   }\n \n   SmallVector<SmallVector<unsigned>>\n-  emitOffsetForMmaLayoutV3(const MmaEncodingAttr &mmaLayout,\n+  emitOffsetForMmaLayoutV3(Value value, const MmaEncodingAttr &mmaLayout,\n                            RankedTensorType type) const {\n     auto shape = type.getShape();\n     auto shapePerCTA = getShapePerCTA(mmaLayout, shape);\n     SmallVector<SmallVector<unsigned>> ret;\n-    ArrayRef<unsigned int> instrShape =\n-        mmaVersionToInstrShape(mmaLayout, shapePerCTA);\n+    auto inputType = value.getDefiningOp()\n+                         ->getOperands()[0]\n+                         .getType()\n+                         .cast<RankedTensorType>()\n+                         .getElementType();\n+    ArrayRef<unsigned int> instrShape = mmaVersionToInstrShape(\n+        mmaLayout.getVersionMajor(), shapePerCTA, inputType);\n \n     for (unsigned i = 0; i < shapePerCTA[0];\n          i += getShapePerCTATile(mmaLayout, shapePerCTA)[0]) {\n@@ -1091,14 +1096,17 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n \n   // Emit indices calculation within each ConversionPattern, and returns a\n   // [elemsPerThread X rank] index matrix.\n-  SmallVector<SmallVector<Value>> emitIndicesForDistributedLayout(\n-      Location loc, ConversionPatternRewriter &rewriter, Attribute layout,\n-      RankedTensorType type, bool withCTAOffset) const {\n+  SmallVector<SmallVector<Value>>\n+  emitIndicesForDistributedLayout(Location loc,\n+                                  ConversionPatternRewriter &rewriter,\n+                                  Value value, bool withCTAOffset) const {\n+\n+    RankedTensorType type = value.getType().cast<RankedTensorType>();\n     // step 1, delinearize threadId to get the base index\n     auto multiDimBase =\n-        emitBaseIndexForLayout(loc, rewriter, layout, type, withCTAOffset);\n+        emitBaseIndexForLayout(loc, rewriter, value, withCTAOffset);\n     // step 2, get offset of each element\n-    auto offset = emitOffsetForLayout(layout, type);\n+    auto offset = emitOffsetForLayout(value);\n     // step 3, add offset to base, and reorder the sequence\n     // of indices to guarantee that elems in the same\n     // sizePerThread are adjacent in order\n@@ -1114,14 +1122,12 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   }\n \n   SmallVector<SmallVector<unsigned>>\n-  emitOffsetForSliceLayout(const SliceEncodingAttr &sliceLayout,\n+  emitOffsetForSliceLayout(Value value, const SliceEncodingAttr &sliceLayout,\n                            RankedTensorType type) const {\n     auto parentEncoding = sliceLayout.getParent();\n     unsigned dim = sliceLayout.getDim();\n-    auto parentShape = sliceLayout.paddedShape(type.getShape());\n-    RankedTensorType parentTy = RankedTensorType::get(\n-        parentShape, type.getElementType(), parentEncoding);\n-    auto parentOffsets = emitOffsetForLayout(parentEncoding, parentTy);\n+    Value parentValue = getParentValueWithSameEncoding(parentEncoding, value);\n+    auto parentOffsets = emitOffsetForLayout(parentValue);\n \n     unsigned numOffsets = parentOffsets.size();\n     SmallVector<SmallVector<unsigned>> resultOffsets;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -180,8 +180,8 @@ struct ExpandDimsOpConversion\n     auto srcLayout = srcTy.getEncoding().dyn_cast<SliceEncodingAttr>();\n     auto resultLayout = resultTy.getEncoding();\n \n-    auto srcOffsets = emitOffsetForLayout(srcLayout, srcTy);\n-    auto resultOffsets = emitOffsetForLayout(resultLayout, resultTy);\n+    auto srcOffsets = emitOffsetForLayout(op.getSrc());\n+    auto resultOffsets = emitOffsetForLayout(op.getResult());\n     DenseMap<SmallVector<unsigned>, Value, SmallVectorKeyInfo> srcValues;\n     for (size_t i = 0; i < srcOffsets.size(); i++) {\n       srcValues[srcOffsets[i]] = srcVals[i];"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 77, "deletions": 0, "changes": 77, "file_content_changes": "@@ -286,6 +286,83 @@ SmallVector<unsigned> getThreadsPerCTA(Attribute layout) {\n   return threads;\n }\n \n+SmallVector<unsigned> getShapePerCTATile(BlockedEncodingAttr blockedLayout) {\n+  SmallVector<unsigned> shape;\n+  for (unsigned d = 0, n = blockedLayout.getOrder().size(); d < n; ++d)\n+    shape.push_back(blockedLayout.getSizePerThread()[d] *\n+                    blockedLayout.getThreadsPerWarp()[d] *\n+                    blockedLayout.getWarpsPerCTA()[d]);\n+\n+  return shape;\n+}\n+\n+static SmallVector<unsigned> getMMAShapePerCTATile_(MmaEncodingAttr mmaLayout,\n+                                                    Value value,\n+                                                    bool fromInput = false) {\n+  SmallVector<unsigned> shape;\n+  auto tensorShape = getShapePerCTA(value.getType());\n+  Type inputType;\n+\n+  if (mmaLayout.isAmpere())\n+    return {16 * mmaLayout.getWarpsPerCTA()[0],\n+            8 * mmaLayout.getWarpsPerCTA()[1]};\n+  if (mmaLayout.isVolta()) {\n+    assert(!tensorShape.empty() && \"Volta needs the tensorShape\");\n+    if (tensorShape.size() == 1) // must be SliceEncoding\n+      return {static_cast<unsigned>(tensorShape[0]),\n+              static_cast<unsigned>(tensorShape[0])};\n+    return {static_cast<unsigned>(tensorShape[0]),\n+            static_cast<unsigned>(tensorShape[1])};\n+  }\n+  if (mmaLayout.isHopper()) {\n+    auto instrShape = mmaVersionToInstrShape(mmaLayout.getVersionMajor(),\n+                                             tensorShape, inputType);\n+    return {16 * mmaLayout.getWarpsPerCTA()[0],\n+            instrShape[1] * mmaLayout.getWarpsPerCTA()[1]};\n+  }\n+  assert(0 && \"Unexpected MMA layout version found\");\n+  return shape;\n+}\n+\n+SmallVector<unsigned> getShapePerCTATile(Value value) {\n+  SmallVector<unsigned> shape;\n+  auto layout = value.getType().cast<RankedTensorType>().getEncoding();\n+\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+    return getShapePerCTATile(blockedLayout);\n+  } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+    Value newValue =\n+        mlir::getParentValueWithSameEncoding(sliceLayout.getParent(), value);\n+    shape = getShapePerCTATile(newValue);\n+    shape.erase(shape.begin() + sliceLayout.getDim());\n+  } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    getMMAShapePerCTATile_(mmaLayout, value);\n+  } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n+    auto parentLayout = dotLayout.getParent();\n+    assert(parentLayout && \"DotOperandEncodingAttr must have a parent\");\n+    if (auto parentMmaLayout = parentLayout.dyn_cast<MmaEncodingAttr>()) {\n+      assert((parentMmaLayout.isAmpere() || parentMmaLayout.isHopper()) &&\n+             \"mmaLayout version = 1 is not implemented yet\");\n+      auto parentShapePerCTATile =\n+          getMMAShapePerCTATile_(parentMmaLayout, value, true);\n+      auto opIdx = dotLayout.getOpIdx();\n+      if (opIdx == 0) {\n+        return {parentShapePerCTATile[0], 16};\n+      } else if (opIdx == 1) {\n+        return {16, parentShapePerCTATile[1]};\n+      } else {\n+        assert(0 && \"DotOperandEncodingAttr opIdx must be 0 or 1\");\n+      }\n+    } else {\n+      assert(0 && \"DotOperandEncodingAttr non-MmaEncodingAttr parent not \"\n+                  \"supported yet\");\n+    }\n+  } else {\n+    assert(0 && \"Unimplemented usage of getShapePerCTATile\");\n+  }\n+  return shape;\n+}\n+\n SmallVector<unsigned> getShapePerCTATile(Attribute layout,\n                                          ArrayRef<int64_t> tensorShape) {\n   SmallVector<unsigned> shape;"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/DumpLayout.cpp", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "file_content_changes": "@@ -61,7 +61,9 @@ class IndexEmitter {\n   emitIndices(Attribute layout, llvm::ArrayRef<int64_t> shape,\n               bool withCTAOffset) {\n     auto type = RankedTensorType::get(shape, rewriter.getF16Type(), layout);\n-    return base.emitIndices(loc, rewriter, layout, type, withCTAOffset);\n+    Value dummy = rewriter.create<arith::ConstantOp>(\n+        loc, type, rewriter.getZeroAttr(type));\n+    return base.emitIndices(loc, rewriter, dummy, withCTAOffset);\n   }\n \n   llvm::DenseMap<unsigned, Value>\n@@ -71,8 +73,10 @@ class IndexEmitter {\n     auto srcTy = RankedTensorType::get(shape, elemTy, srcLayout);\n     SharedMemoryObject smemObj(getMockSmemBase(), shape,\n                                sharedLayout.getOrder(), loc, rewriter);\n-    return base.getSwizzledSharedPtrs(loc, /*inVec=*/1, srcTy, sharedLayout,\n-                                      elemTy, smemObj, rewriter,\n+    Value dummy = rewriter.create<arith::ConstantOp>(\n+        loc, srcTy, rewriter.getZeroAttr(srcTy));\n+    return base.getSwizzledSharedPtrs(loc, /*inVec=*/1, dummy, srcTy,\n+                                      sharedLayout, elemTy, smemObj, rewriter,\n                                       smemObj.offsets, smemObj.strides);\n   }\n "}]