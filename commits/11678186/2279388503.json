[{"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 143, "deletions": 112, "changes": 255, "file_content_changes": "@@ -3,11 +3,9 @@\n ====================\n \"\"\"\n \n-import torch\n-\n import triton\n import triton.language as tl\n-\n+import torch\n try:\n     # This is https://github.com/NVIDIA/apex, NOT the apex on PyPi, so it\n     # should not be added to extras_require in setup.py.\n@@ -16,15 +14,18 @@\n except ModuleNotFoundError:\n     HAS_APEX = False\n \n+# fmt: off\n \n-# Forward Pass\n @triton.jit\n def _layer_norm_fwd_fused(\n-        Out,  A, \n-        Weight,  Bias, \n-        Mean, Rstd,\n-        stride, N, eps,\n-        BLOCK_SIZE: tl.constexpr):\n+    Out, \n+    A, \n+    Weight, \n+    Bias, \n+    Mean, Rstd,\n+    stride, N, eps,\n+    BLOCK_SIZE: tl.constexpr,\n+):\n     # position of elements processed by this program\n     row = tl.program_id(0)\n     Out += row * stride\n@@ -34,14 +35,14 @@ def _layer_norm_fwd_fused(\n     _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n     for off in range(0, N, BLOCK_SIZE):\n         cols = off + tl.arange(0, BLOCK_SIZE)\n-        a = tl.load(A + cols, mask=cols<N, other=0., eviction_policy=\"evict_last\")\n+        a = tl.load(A + cols, mask=cols<N, other=0., eviction_policy=\"evict_last\").to(tl.float32)\n         _mean += a\n     mean = tl.sum(_mean, axis = 0) / N\n     # compute variance\n     _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n     for off in range(0, N, BLOCK_SIZE):\n         cols = off + tl.arange(0, BLOCK_SIZE)\n-        a = tl.load(A + cols, mask=cols<N, other=0., eviction_policy=\"evict_last\")\n+        a = tl.load(A + cols, mask=cols<N, other=0., eviction_policy=\"evict_last\").to(tl.float32)\n         a = tl.where(cols<N, a - mean, 0.)\n         _var += a * a\n     var = tl.sum(_var, axis = 0) / N\n@@ -53,76 +54,73 @@ def _layer_norm_fwd_fused(\n     for off in range(0, N, BLOCK_SIZE):\n         cols = off + tl.arange(0, BLOCK_SIZE)\n         mask = cols < N\n-        a = tl.load(A + cols, mask=mask, other=0., eviction_policy=\"evict_first\")\n         weight = tl.load(Weight + cols, mask=mask)\n         bias = tl.load(Bias + cols, mask=mask)\n+        a = tl.load(A + cols, mask=mask, other=0., eviction_policy=\"evict_first\").to(tl.float32)\n         a_hat = (a - mean) * rstd\n         out = a_hat * weight + bias\n         # # write-back\n         tl.store(Out + cols, out, mask=mask)\n \n-\n-# Backward pass (DX + partial DW + partial DB)\n+# Backward pass (DA + partial DW + partial DB)\n @triton.jit\n-def _layer_norm_bwd_dx_fused(DX, DY, DW, DB, X, W, B, M, V, Lock, stride, N, eps,\n-                             GROUP_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):\n+def _layer_norm_bwd_dx_fused(\n+    _DA, \n+    _DOut, \n+    _A,\n+    Weight,\n+    Mean, Rstd,\n+    stride, NumRows, NumCols, eps, \n+    BLOCK_SIZE_N: tl.constexpr,\n+):\n     # position of elements processed by this program\n-    row = tl.program_id(0)\n-    cols = tl.arange(0, BLOCK_SIZE_N)\n-    mask = cols < N\n-    # offset data pointers to start at the row of interest\n-    X += row * stride\n-    DY += row * stride\n-    DX += row * stride\n-    # offset locks and weight/bias gradient pointer\n-    # each kernel instance accumulates partial sums for\n-    # DW and DB into one of GROUP_SIZE_M independent buffers\n-    # these buffers stay in the L2, which allow this kernel\n-    # to be fast\n-    lock_id = row % GROUP_SIZE_M\n-    Lock += lock_id\n-    Count = Lock + GROUP_SIZE_M\n-    DW = DW + lock_id * N + cols\n-    DB = DB + lock_id * N + cols\n+    pid = tl.program_id(0)\n+    row = pid\n+    A = _A + row*stride\n+    DOut = _DOut + row*stride\n+    DA = _DA + row*stride\n+    mean = tl.load(Mean + row)\n+    rstd = tl.load(Rstd + row)\n     # load data to SRAM\n-    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n-    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n-    w = tl.load(W + cols, mask=mask).to(tl.float32)\n-    mean = tl.load(M + row)\n-    rstd = tl.load(V + row)\n-    # compute dx\n-    xhat = (x - mean) * rstd\n-    wdy = w * dy\n-    xhat = tl.where(mask, xhat, 0.)\n-    wdy = tl.where(mask, wdy, 0.)\n-    mean1 = tl.sum(xhat * wdy, axis=0) / N\n-    mean2 = tl.sum(wdy, axis=0) / N\n-    dx = (wdy - (xhat * mean1 + mean2)) * rstd\n-    # write-back dx\n-    tl.store(DX + cols, dx, mask=mask)\n-    # accumulate partial sums for dw/db\n-    partial_dw = (dy * xhat).to(w.dtype)\n-    partial_db = (dy).to(w.dtype)\n-    while tl.atomic_cas(Lock, 0, 1) == 1:\n-        pass\n-    count = tl.load(Count)\n-    # first store doesn't accumulate\n-    if count == 0:\n-        tl.atomic_xchg(Count, 1)\n-    else:\n-        partial_dw += tl.load(DW, mask=mask)\n-        partial_db += tl.load(DB, mask=mask)\n-    tl.store(DW, partial_dw, mask=mask)\n-    tl.store(DB, partial_db, mask=mask)\n-    # release lock\n-    tl.atomic_xchg(Lock, 0)\n-\n-# Backward pass (total DW + total DB)\n+    _mean1 = tl.zeros([BLOCK_SIZE_N], dtype=tl.float32)\n+    _mean2 = tl.zeros([BLOCK_SIZE_N], dtype=tl.float32)\n+    for off in range(0, NumCols, BLOCK_SIZE_N):\n+        cols = off + tl.arange(0, BLOCK_SIZE_N)\n+        mask = cols < NumCols\n+        a = tl.load(A + cols, mask=mask, other=0).to(tl.float32)\n+        dout = tl.load(DOut + cols, mask=mask, other=0).to(tl.float32)\n+        weight = tl.load(Weight + cols, mask=mask, other=0).to(tl.float32)\n+        a_hat = (a - mean) * rstd\n+        wdout = weight * dout\n+        _mean1 += a_hat * wdout\n+        _mean2 += wdout\n+    mean1 = tl.sum(_mean1, axis=0) / NumCols\n+    mean2 = 0.\n+    mean2 = tl.sum(_mean2, axis=0) / NumCols\n+    for off in range(0, NumCols, BLOCK_SIZE_N):\n+        cols = off + tl.arange(0, BLOCK_SIZE_N)\n+        mask = cols < NumCols\n+        a = tl.load(A + cols, mask=mask, other=0).to(tl.float32)\n+        dout = tl.load(DOut + cols, mask=mask, other=0).to(tl.float32)\n+        weight = tl.load(Weight + cols, mask=mask, other=0).to(tl.float32)\n+        a_hat = (a - mean) * rstd\n+        wdout = weight * dout\n+        da = (wdout - (a_hat * mean1 + mean2)) * rstd\n+        # write-back dx\n+        tl.store(DA + cols, da, mask=mask)\n \n \n+# Backward pass (total DW + total DB)\n @triton.jit\n-def _layer_norm_bwd_dwdb(DW, DB, FINAL_DW, FINAL_DB, M, N,\n-                         BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):\n+def _layer_norm_bwd_dwdb(\n+    A, DOut,\n+    Mean, Var,\n+    DW, \n+    DB, \n+    M, N,\n+    BLOCK_SIZE_M: tl.constexpr,\n+    BLOCK_SIZE_N: tl.constexpr,\n+):\n     pid = tl.program_id(0)\n     cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n     dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n@@ -131,76 +129,109 @@ def _layer_norm_bwd_dwdb(DW, DB, FINAL_DW, FINAL_DB, M, N,\n         rows = i + tl.arange(0, BLOCK_SIZE_M)\n         mask = (rows[:, None] < M) & (cols[None, :] < N)\n         offs = rows[:, None] * N + cols[None, :]\n-        dw += tl.load(DW + offs, mask=mask, other=0.)\n-        db += tl.load(DB + offs, mask=mask, other=0.)\n+        a    = tl.load(A + offs, mask=mask, other=0.).to(tl.float32)\n+        dout = tl.load(DOut + offs, mask=mask, other=0.).to(tl.float32)\n+        mean = tl.load(Mean + rows, mask=rows<M, other=0.)\n+        rstd = tl.load(Var + rows, mask=rows<M, other=0.)\n+        a_hat = (a - mean[:, None]) * rstd[:, None]\n+        dw += dout * a_hat\n+        db += dout\n     sum_dw = tl.sum(dw, axis=0)\n     sum_db = tl.sum(db, axis=0)\n-    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n-    tl.store(FINAL_DB + cols, sum_db, mask=cols < N)\n+    tl.store(DW + cols, sum_dw, mask=cols < N)\n+    tl.store(DB + cols, sum_db, mask=cols < N)\n \n \n class LayerNorm(torch.autograd.Function):\n-\n     @staticmethod\n-    def forward(ctx, x, normalized_shape, weight, bias, eps):\n+    def forward(ctx, a, normalized_shape, weight, bias, eps):\n         # allocate output\n-        y = torch.ones_like(x)\n+        out = torch.empty_like(a)\n         # reshape input data into 2D tensor\n-        x_arg = x.reshape(-1, x.shape[-1])\n-        M, N = x_arg.shape\n-        mean = torch.zeros((M, ), dtype=torch.float32, device='cuda')\n-        rstd = torch.zeros((M, ), dtype=torch.float32, device='cuda')\n+        a_arg = a.reshape(-1, a.shape[-1])\n+        M, N = a_arg.shape\n+        mean = torch.empty((M,), dtype=torch.float32, device=\"cuda\")\n+        rstd = torch.empty((M,), dtype=torch.float32, device=\"cuda\")\n         # Less than 64KB per feature: enqueue fused kernel\n-        BLOCK_SIZE = triton.next_power_of_2(N)\n+        MAX_FUSED_SIZE = 65536 // a.element_size()\n+        BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n         BLOCK_SIZE = max(BLOCK_SIZE, 128)\n         BLOCK_SIZE = min(BLOCK_SIZE, 4096)\n         # heuristics for number of warps\n         num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\n-        # enqueue kernel\n-        _layer_norm_fwd_fused[(M,)](y, x_arg, weight, bias, mean, rstd,\n-                                    x_arg.stride(0), N, eps,\n-                                    BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)\n-        ctx.save_for_backward(x, weight, bias, mean, rstd)\n+        _layer_norm_fwd_fused[(M,)](\n+            out, \n+            a_arg, \n+            weight, \n+            bias, \n+            mean, rstd,\n+            a_arg.stride(0), N, eps,\n+            BLOCK_SIZE=BLOCK_SIZE,\n+            num_warps=num_warps,\n+        )\n+        ctx.save_for_backward(\n+            a, weight, bias, mean, rstd,\n+        )\n         ctx.BLOCK_SIZE = BLOCK_SIZE\n         ctx.num_warps = num_warps\n         ctx.eps = eps\n-        return y\n+        if hasattr(bias, \"config\"):\n+            assert bias.config.grad_scale_name == weight.config.grad_scale_name\n+            grad_scale_name = bias.config.grad_scale_name\n+        else:\n+            grad_scale_name = None\n+        ctx.grad_scale_gain_bias_name = grad_scale_name\n+        return out\n \n     @staticmethod\n-    def backward(ctx, dy):\n-        x, w, b, m, v = ctx.saved_tensors\n+    def backward(ctx, dout):\n+        assert dout.is_contiguous()\n+        a, weight, bias, mean, var = ctx.saved_tensors\n         # heuristics for amount of parallel reduction stream for DG/DB\n-        N = w.shape[0]\n-        GROUP_SIZE_M = 64\n-        if N <= 8192: GROUP_SIZE_M = 96\n-        if N <= 4096: GROUP_SIZE_M = 128\n-        if N <= 1024: GROUP_SIZE_M = 256\n+        N = weight.shape[0]\n         # allocate output\n-        locks = torch.zeros(2 * GROUP_SIZE_M, dtype=torch.int32, device='cuda')\n-        _dw = torch.empty((GROUP_SIZE_M, w.shape[0]), dtype=x.dtype, device=w.device)\n-        _db = torch.empty((GROUP_SIZE_M, w.shape[0]), dtype=x.dtype, device=w.device)\n-        dw = torch.empty((w.shape[0],), dtype=w.dtype, device=w.device)\n-        db = torch.empty((w.shape[0],), dtype=w.dtype, device=w.device)\n-        dx = torch.empty_like(dy)\n+        da       = torch.empty_like(dout)\n         # enqueue kernel using forward pass heuristics\n         # also compute partial sums for DW and DB\n-        x_arg = x.reshape(-1, x.shape[-1])\n+        x_arg = a.reshape(-1, a.shape[-1])\n         M, N = x_arg.shape\n-        _layer_norm_bwd_dx_fused[(M,)](dx, dy, _dw, _db, x, w, b, m, v, locks,\n-                                       x_arg.stride(0), N, ctx.eps,\n-                                       BLOCK_SIZE_N=ctx.BLOCK_SIZE,\n-                                       GROUP_SIZE_M=GROUP_SIZE_M,\n-                                       num_warps=ctx.num_warps)\n-        grid = lambda meta: [triton.cdiv(N, meta['BLOCK_SIZE_N'])]\n+        dweight  = torch.empty((weight.shape[0],), dtype=weight.dtype, device=weight.device)\n+        dbias    = torch.empty((weight.shape[0],), dtype=weight.dtype, device=weight.device)\n+        _layer_norm_bwd_dx_fused[(M,)](\n+            da, \n+            dout, \n+            a, \n+            weight, \n+            mean, var,\n+            x_arg.stride(0), M, N,\n+            ctx.eps,\n+            BLOCK_SIZE_N=ctx.BLOCK_SIZE,\n+            num_warps=ctx.num_warps,\n+        )\n         # accumulate partial sums in separate kernel\n-        _layer_norm_bwd_dwdb[grid](_dw, _db, dw, db, GROUP_SIZE_M, N,\n-                                   BLOCK_SIZE_M=32,\n-                                   BLOCK_SIZE_N=128)\n-        return dx, None, dw, db, None\n-\n+        grid = lambda meta: [triton.cdiv(N, meta[\"BLOCK_SIZE_N\"])]\n+        _layer_norm_bwd_dwdb[grid](\n+            a, dout,\n+            mean, var,\n+            dweight,\n+            dbias, \n+            M,\n+            N,\n+            BLOCK_SIZE_M=32,\n+            BLOCK_SIZE_N=128,\n+        )\n+        return (da, None, dweight, dbias, None, None,\n+                None, None, None, None,\n+                None,\n+                None, None, None,\n+                None,\n+                None, None, None,\n+                None, None, None,\n+                None, None, None)\n \n-layer_norm = LayerNorm.apply\n \n+def layer_norm(a, normalized_shape, weight, bias, eps):\n+    return LayerNorm.apply(a, normalized_shape, weight, bias, eps)\n \n def test_layer_norm(M, N, dtype, eps=1e-5, device='cuda'):\n     torch.manual_seed(0)"}]