[{"filename": "include/triton/Analysis/Membar.h", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "file_content_changes": "@@ -56,8 +56,12 @@ class MembarAnalysis {\n     bool isIntersected(const RegionInfo &other, Allocation *allocation) const {\n       return /*RAW*/ isIntersected(syncWriteBuffers, other.syncReadBuffers,\n                                    allocation) ||\n-             /*WAR*/ isIntersected(syncReadBuffers, other.syncWriteBuffers,\n-                                   allocation);\n+             /*WAR*/\n+             isIntersected(syncReadBuffers, other.syncWriteBuffers,\n+                           allocation) ||\n+             /*WAW*/\n+             isIntersected(syncWriteBuffers, other.syncWriteBuffers,\n+                           allocation);\n     }\n \n     /// Clears the buffers because a barrier is inserted."}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -187,6 +187,7 @@ def TT_StoreOp : TT_Op<\"store\",\n //\n def TT_AtomicRMWOp : TT_Op<\"atomic_rmw\", [SameOperandsAndResultShape,\n                                           SameOperandsAndResultEncoding,\n+                                          MemoryEffects<[MemRead]>,\n                                           MemoryEffects<[MemWrite]>,\n                                           TypesMatchWith<\"infer ptr type from value type\",\n                                                          \"val\", \"ptr\",\n@@ -208,7 +209,9 @@ def TT_AtomicRMWOp : TT_Op<\"atomic_rmw\", [SameOperandsAndResultShape,\n     let results = (outs TT_Type:$result);\n }\n \n-def TT_AtomicCASOp : TT_Op<\"atomic_cas\", [SameOperandsAndResultShape,\n+def TT_AtomicCASOp : TT_Op<\"atomic_cas\", [MemoryEffects<[MemRead]>,\n+                                          MemoryEffects<[MemWrite]>,\n+                                          SameOperandsAndResultShape,\n                                           SameOperandsAndResultEncoding]> {\n     let summary = \"atomic cas\";\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -79,8 +79,7 @@ def TTG_SelectOp : TTG_Op<\"select\", [NoSideEffect]> {\n def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n                                     [AttrSizedOperandSegments,\n                                      ResultsAreSharedEncoding,\n-                                     // MemoryEffects<[MemRead]>, doesn't work with CSE but seems like it should?\n-                                     NoSideEffect,\n+                                     MemoryEffects<[MemRead]>,\n                                      TypesMatchWith<\"infer mask type from src type\",\n                                                     \"src\", \"mask\", \"getI1SameShape($_self)\",\n                                                     \"($_op.getOperands().size() <= 3) || std::equal_to<>()\">,\n@@ -158,7 +157,8 @@ def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n   let printer = [{ return printInsertSliceAsyncOp(p, *this); }];\n }\n \n-def TTG_AllocTensorOp : TTG_Op<\"alloc_tensor\", [NoSideEffect, ResultsAreSharedEncoding]> {\n+def TTG_AllocTensorOp : TTG_Op<\"alloc_tensor\", [MemoryEffects<[MemAlloc]>,  // Allocate shared memory\n+                                                ResultsAreSharedEncoding]> {\n   let summary = \"allocate tensor\";\n \n   let description = [{"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 31, "deletions": 0, "changes": 31, "file_content_changes": "@@ -112,6 +112,20 @@ SmallVector<unsigned> getScratchConfigForReduce(triton::ReduceOp op) {\n   return smemShape;\n }\n \n+// TODO: extend beyond scalars\n+SmallVector<unsigned> getScratchConfigForAtomicRMW(triton::AtomicRMWOp op) {\n+  SmallVector<unsigned> smemShape;\n+  auto ptrTy = op.ptr().getType();\n+  if (auto tensorType = ptrTy.dyn_cast<RankedTensorType>()) {\n+    // do nothing or just assert because shared memory is not used in tensor\n+  } else {\n+    // need only bytes for scalar\n+    // always vec = 1 and elemsPerThread = 1 for scalar?\n+    smemShape.push_back(1);\n+  }\n+  return smemShape;\n+}\n+\n class AllocationAnalysis {\n public:\n   AllocationAnalysis(Operation *operation, Allocation *allocation)\n@@ -200,6 +214,23 @@ class AllocationAnalysis {\n                    elems * kPtrBitWidth / 8 :\n                    elems * srcTy.getElementTypeBitWidth() / 8;\n       allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+    } else if (auto atomicRMWOp = dyn_cast<triton::AtomicRMWOp>(op)) {\n+      auto value = op->getOperand(0);\n+      // only scalar requires scratch memory\n+      // make it explicit for readability\n+      if (value.getType().dyn_cast<RankedTensorType>()) {\n+        // nothing to do\n+      } else {\n+        auto smemShape = getScratchConfigForAtomicRMW(atomicRMWOp);\n+        unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n+                                         std::multiplies{});\n+        auto elemTy =\n+            value.getType().cast<triton::PointerType>().getPointeeType();\n+        auto bytes = elemTy.isa<triton::PointerType>()\n+                         ? elems * kPtrBitWidth / 8\n+                         : elems * elemTy.getIntOrFloatBitWidth() / 8;\n+        allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+      }\n     }\n   }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 111, "deletions": 68, "changes": 179, "file_content_changes": "@@ -2239,16 +2239,16 @@ struct AllocTensorOpConversion\n     smemBase = bitcast(smemBase, elemPtrTy);\n     auto order = resultTy.getEncoding().cast<SharedEncodingAttr>().getOrder();\n     // workaround for 3D tensors\n-    // TODO: We need to modify the pipeline pass to give a proper shared encoding to 3D tensors\n+    // TODO: We need to modify the pipeline pass to give a proper shared\n+    // encoding to 3D tensors\n     SmallVector<unsigned> newOrder;\n-    if (resultTy.getShape().size() == 3) \n+    if (resultTy.getShape().size() == 3)\n       newOrder = {1 + order[0], 1 + order[1], 0};\n     else\n       newOrder = SmallVector<unsigned>(order.begin(), order.end());\n \n-    \n-    auto smemObj =\n-        SharedMemoryObject(smemBase, resultTy.getShape(), newOrder, loc, rewriter);\n+    auto smemObj = SharedMemoryObject(smemBase, resultTy.getShape(), newOrder,\n+                                      loc, rewriter);\n     auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n     rewriter.replaceOp(op, retVal);\n     return success();\n@@ -2882,6 +2882,8 @@ struct ConvertLayoutOpConversion\n       SmallVector<Value> multiDimWarpId(2);\n       multiDimWarpId[0] = urem(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n       multiDimWarpId[1] = udiv(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n+      multiDimWarpId[0] = urem(multiDimWarpId[0], idx_val(shape[0] / 16));\n+      multiDimWarpId[1] = urem(multiDimWarpId[1], idx_val(shape[1] / 8));\n       Value four = idx_val(4);\n       Value mmaGrpId = udiv(laneId, four);\n       Value mmaGrpIdP8 = add(mmaGrpId, idx_val(8));\n@@ -3661,7 +3663,7 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n \n     // Here we assume the DotOp's operands always comes from shared memory.\n     auto AShape = A.getType().cast<RankedTensorType>().getShape();\n-    size_t reduceAxis = 1;\n+    size_t reduceAxis = op.transA() ? 0 : 1;\n     unsigned K = AShape[reduceAxis];\n     bool isOuter = K == 1;\n \n@@ -4124,8 +4126,9 @@ struct DotOpMmaV2ConversionHelper {\n struct MMA16816ConversionHelper {\n   MmaEncodingAttr mmaLayout;\n   ArrayRef<unsigned int> wpt;\n+  SmallVector<unsigned int> properWpt;\n \n-  Value thread, lane, warp, warpMN, warpN, warpM;\n+  Value thread, lane, warp;\n \n   DotOpMmaV2ConversionHelper helper;\n   ConversionPatternRewriter &rewriter;\n@@ -4135,23 +4138,34 @@ struct MMA16816ConversionHelper {\n \n   using ValueTable = std::map<std::pair<unsigned, unsigned>, Value>;\n \n-  MMA16816ConversionHelper(MmaEncodingAttr mmaLayout, Value thread,\n-                           ConversionPatternRewriter &rewriter,\n+  // dotOperand: type of either one operand of dotOp.\n+  MMA16816ConversionHelper(Type dotOperand, MmaEncodingAttr mmaLayout,\n+                           Value thread, ConversionPatternRewriter &rewriter,\n                            TypeConverter *typeConverter, Location loc)\n       : mmaLayout(mmaLayout), thread(thread), helper(mmaLayout),\n         rewriter(rewriter), typeConverter(typeConverter), loc(loc),\n-        ctx(mmaLayout.getContext()) {\n-    wpt = mmaLayout.getWarpsPerCTA();\n+        ctx(mmaLayout.getContext()), wpt(mmaLayout.getWarpsPerCTA()) {\n+    helper.deduceMmaType(dotOperand);\n \n     Value _32 = i32_val(32);\n     lane = urem(thread, _32);\n     warp = udiv(thread, _32);\n-    warpMN = udiv(warp, i32_val(wpt[0]));\n-    warpM = urem(warp, i32_val(wpt[0]));\n-    warpN = urem(warpMN, i32_val(wpt[1]));\n   }\n \n-  // Get the mmaInstrShape from either $a or $b.\n+  // Get a warpId for M axis.\n+  Value getWarpM(int M) const {\n+    auto matShape = helper.getMmaMatShape();\n+    return urem(urem(warp, i32_val(wpt[0])), i32_val(M / matShape[0]));\n+  }\n+\n+  // Get a warpId for N axis.\n+  Value getWarpN(int N) const {\n+    auto matShape = helper.getMmaMatShape();\n+    Value warpMN = udiv(warp, i32_val(wpt[0]));\n+    return urem(urem(warpMN, i32_val(wpt[1])), i32_val(N / matShape[1]));\n+  }\n+\n+  // Get the mmaInstrShape deducing either from $a or $b.\n   std::tuple<int, int, int> getMmaInstrShape(Type operand) const {\n     helper.deduceMmaType(operand);\n     auto mmaInstrShape = helper.getMmaInstrShape();\n@@ -4161,6 +4175,7 @@ struct MMA16816ConversionHelper {\n     return std::make_tuple(mmaInstrM, mmaInstrN, mmaInstrK);\n   }\n \n+  // Get the mmaMatShape deducing either from $a or $b.\n   std::tuple<int, int, int> getMmaMatShape(Type operand) const {\n     helper.deduceMmaType(operand);\n     auto matShape = helper.getMmaMatShape();\n@@ -4210,28 +4225,28 @@ struct MMA16816ConversionHelper {\n   }\n \n   // Get number of elements per thread for $a operand.\n-  static size_t getANumElemsPerThread(RankedTensorType operand,\n-                                      ArrayRef<unsigned> wpt) {\n+  static size_t getANumElemsPerThread(RankedTensorType operand, int wpt) {\n     auto shape = operand.getShape();\n-    int repM = getNumRepM(operand, shape[0], wpt[0]);\n+    int repM = getNumRepM(operand, shape[0], wpt);\n     int repK = getNumRepK_(operand, shape[1]);\n     return 4 * repM * repK;\n   }\n \n   // Get number of elements per thread for $b operand.\n-  static size_t getBNumElemsPerThread(RankedTensorType operand,\n-                                      ArrayRef<unsigned> wpt) {\n+  static size_t getBNumElemsPerThread(RankedTensorType operand, int wpt) {\n     auto shape = operand.getShape();\n     int repK = getNumRepK_(operand, shape[0]);\n-    int repN = getNumRepN(operand, shape[1], wpt[1]);\n+    int repN = getNumRepN(operand, shape[1], wpt);\n     return 4 * std::max(repN / 2, 1) * repK;\n   }\n \n   // Loading $a from smem to registers, returns a LLVM::Struct.\n   Value loadA(Value tensor, const SharedMemoryObject &smemObj) const {\n     auto aTensorTy = tensor.getType().cast<RankedTensorType>();\n-    auto shape = aTensorTy.getShape();\n+    auto layout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n \n+    SmallVector<int64_t> shape(aTensorTy.getShape().begin(),\n+                               aTensorTy.getShape().end());\n     ValueTable ha;\n     std::function<void(int, int)> loadFn;\n     auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(aTensorTy);\n@@ -4241,6 +4256,7 @@ struct MMA16816ConversionHelper {\n     int numRepK = getNumRepK(aTensorTy, shape[1]);\n \n     if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n+      Value warpM = getWarpM(shape[0]);\n       // load from smem\n       loadFn = getLoadMatrixFn(\n           tensor, smemObj, mmaLayout, mmaLayout.getWarpsPerCTA()[0] /*wpt*/,\n@@ -4268,12 +4284,17 @@ struct MMA16816ConversionHelper {\n   Value loadB(Value tensor, const SharedMemoryObject &smemObj) {\n     ValueTable hb;\n     auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-    auto shape = tensorTy.getShape();\n+    auto layout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+\n+    SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n+                               tensorTy.getShape().end());\n+\n     auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(tensorTy);\n     auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(tensorTy);\n     int numRepK = getNumRepK(tensorTy, shape[0]);\n     int numRepN = getNumRepN(tensorTy, shape[1]);\n \n+    Value warpN = getWarpN(shape[1]);\n     auto loadFn = getLoadMatrixFn(\n         tensor, smemObj, mmaLayout, mmaLayout.getWarpsPerCTA()[1] /*wpt*/,\n         0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShape*/,\n@@ -4319,7 +4340,11 @@ struct MMA16816ConversionHelper {\n     auto aTensorTy = a.getType().cast<RankedTensorType>();\n     auto dTensorTy = d.getType().cast<RankedTensorType>();\n \n-    auto aShape = aTensorTy.getShape();\n+    SmallVector<int64_t> aShape(aTensorTy.getShape().begin(),\n+                                aTensorTy.getShape().end());\n+    if (op.transA())\n+      std::swap(aShape[0], aShape[1]);\n+\n     auto dShape = dTensorTy.getShape();\n \n     // shape / shape_per_cta\n@@ -4602,9 +4627,9 @@ Value ConvertLayoutOpConversion::lowerSharedToDotOperandMMA(\n   Value res;\n \n   if (!isOuter && mmaLayout.getVersion() == 2 && isHMMA) { // tensor core v2\n-    MMA16816ConversionHelper mmaHelper(mmaLayout, getThreadId(rewriter, loc),\n-                                       rewriter, getTypeConverter(),\n-                                       op.getLoc());\n+    MMA16816ConversionHelper mmaHelper(src.getType(), mmaLayout,\n+                                       getThreadId(rewriter, loc), rewriter,\n+                                       getTypeConverter(), op.getLoc());\n \n     if (dotOperandLayout.getOpIdx() == 0) {\n       // operand $a\n@@ -4695,12 +4720,15 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adaptor,\n                        .cast<RankedTensorType>()\n                        .getEncoding()\n                        .cast<MmaEncodingAttr>();\n-  MMA16816ConversionHelper mmaHelper(mmaLayout, getThreadId(rewriter, loc),\n-                                     rewriter, getTypeConverter(), loc);\n \n   Value A = op.a();\n   Value B = op.b();\n   Value C = op.c();\n+\n+  MMA16816ConversionHelper mmaHelper(A.getType(), mmaLayout,\n+                                     getThreadId(rewriter, loc), rewriter,\n+                                     getTypeConverter(), loc);\n+\n   auto ATensorTy = A.getType().cast<RankedTensorType>();\n   auto BTensorTy = B.getType().cast<RankedTensorType>();\n \n@@ -5532,13 +5560,13 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n         if (mmaLayout.getVersion() == 2) {\n           if (dotOpLayout.getOpIdx() == 0) { // $a\n             int elems =\n-                MMA16816ConversionHelper::getANumElemsPerThread(type, wpt);\n+                MMA16816ConversionHelper::getANumElemsPerThread(type, wpt[0]);\n             return LLVM::LLVMStructType::getLiteral(\n                 ctx, SmallVector<Type>(elems, vecTy));\n           }\n           if (dotOpLayout.getOpIdx() == 1) { // $b\n             int elems =\n-                MMA16816ConversionHelper::getBNumElemsPerThread(type, wpt);\n+                MMA16816ConversionHelper::getBNumElemsPerThread(type, wpt[1]);\n             return struct_ty(SmallVector<Type>(elems, vecTy));\n           }\n         }\n@@ -5919,10 +5947,11 @@ struct AtomicRMWOpConversion\n       triton::AtomicRMWOp>::ConvertTritonGPUOpToLLVMPattern;\n \n   AtomicRMWOpConversion(LLVMTypeConverter &converter,\n+                        const Allocation *allocation, Value smem,\n                         AxisInfoAnalysis &axisAnalysisPass,\n                         PatternBenefit benefit)\n-      : ConvertTritonGPUOpToLLVMPattern<triton::AtomicRMWOp>(converter,\n-                                                             benefit),\n+      : ConvertTritonGPUOpToLLVMPattern<triton::AtomicRMWOp>(\n+            converter, allocation, smem, benefit),\n         LoadStoreConversionBase(axisAnalysisPass) {}\n \n   LogicalResult\n@@ -5943,30 +5972,29 @@ struct AtomicRMWOpConversion\n     auto valElements = getElementsFromStruct(loc, llVal, rewriter);\n     auto ptrElements = getElementsFromStruct(loc, llPtr, rewriter);\n     auto maskElements = getElementsFromStruct(loc, llMask, rewriter);\n-\n-    // TODO[dongdongl]: Support scalar\n-\n+   \n     auto valueTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n-    if (!valueTy)\n-      return failure();\n     Type valueElemTy =\n-        getTypeConverter()->convertType(valueTy.getElementType());\n-\n-    auto valTy = val.getType().cast<RankedTensorType>();\n+        valueTy ? getTypeConverter()->convertType(valueTy.getElementType())\n+                : op.getResult().getType();\n     const size_t valueElemNbits = valueElemTy.getIntOrFloatBitWidth();\n-    auto vec = getVectorSize(ptr);\n-    vec = std::min<unsigned>(vec, valTy.getElementType().isF16() ? 2 : 1);\n-\n-    auto vecTy = vec_ty(valueElemTy, vec);\n     auto elemsPerThread = getElemsPerThread(val.getType());\n-    // mask\n+    // vec = 1 for scalar\n+    auto vec = getVectorSize(ptr);\n     Value mask = int_val(1, 1);\n-    auto shape = valueTy.getShape();\n-    auto numElements = product(shape);\n     auto tid = tid_val();\n-    mask = and_(mask, icmp_slt(mul(tid, i32_val(elemsPerThread)),\n-                               i32_val(numElements)));\n+    // tensor\n+    if (valueTy) {\n+      auto valTy = val.getType().cast<RankedTensorType>();\n+      vec = std::min<unsigned>(vec, valTy.getElementType().isF16() ? 2 : 1);\n+      // mask\n+      auto shape = valueTy.getShape();\n+      auto numElements = product(shape);\n+      mask = and_(mask, icmp_slt(mul(tid, i32_val(elemsPerThread)),\n+                                 i32_val(numElements)));\n+    }\n \n+    auto vecTy = vec_ty(valueElemTy, vec);\n     SmallVector<Value> resultVals(elemsPerThread);\n     for (size_t i = 0; i < elemsPerThread; i += vec) {\n       Value rmwVal = undef(vecTy);\n@@ -5980,10 +6008,12 @@ struct AtomicRMWOpConversion\n       rmwMask = and_(rmwMask, mask);\n       std::string sTy;\n       PTXBuilder ptxBuilder;\n-\n-      auto *dstOpr = ptxBuilder.newOperand(\"=r\");\n+      std::string tyId = valueElemNbits * vec == 64\n+                             ? \"l\"\n+                             : (valueElemNbits * vec == 32 ? \"r\" : \"h\");\n+      auto *dstOpr = ptxBuilder.newOperand(\"=\" + tyId);\n       auto *ptrOpr = ptxBuilder.newAddrOperand(rmwPtr, \"l\");\n-      auto *valOpr = ptxBuilder.newOperand(rmwVal, \"r\");\n+      auto *valOpr = ptxBuilder.newOperand(rmwVal, tyId);\n \n       auto &atom = ptxBuilder.create<>(\"atom\")->global().o(\"gpu\");\n       auto rmwOp = stringifyRMWOp(atomicRmwAttr).str();\n@@ -6025,18 +6055,32 @@ struct AtomicRMWOpConversion\n         return failure();\n       }\n       atom.o(rmwOp).o(sTy);\n-      atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n-\n-      auto ret = ptxBuilder.launch(rewriter, loc, valueElemTy);\n-      for (int ii = 0; ii < vec; ++ii) {\n-        resultVals[i * vec + ii] =\n-            vec == 1 ? ret : extract_element(valueElemTy, ret, idx_val(ii));\n+      if (valueTy) {\n+        atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n+        auto ret = ptxBuilder.launch(rewriter, loc, valueElemTy);\n+        for (int ii = 0; ii < vec; ++ii) {\n+          resultVals[i * vec + ii] =\n+              vec == 1 ? ret : extract_element(valueElemTy, ret, idx_val(ii));\n+        }\n+      } else {\n+        rmwMask = and_(rmwMask, icmp_eq(tid, i32_val(0)));\n+        atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n+        auto old = ptxBuilder.launch(rewriter, loc, valueElemTy);\n+        Value atomPtr = getSharedMemoryBase(loc, rewriter, op.getOperation());\n+        atomPtr = bitcast(atomPtr, ptr_ty(valueElemTy, 3));\n+        store(old, atomPtr);\n+        barrier();\n+        Value ret = load(atomPtr);\n+        barrier();\n+        rewriter.replaceOp(op, {ret});\n       }\n     }\n-    Type structTy = getTypeConverter()->convertType(valueTy);\n-    Value resultStruct =\n-        getStructFromElements(loc, resultVals, rewriter, structTy);\n-    rewriter.replaceOp(op, {resultStruct});\n+    if (valueTy) {\n+      Type structTy = getTypeConverter()->convertType(valueTy);\n+      Value resultStruct =\n+          getStructFromElements(loc, resultVals, rewriter, structTy);\n+      rewriter.replaceOp(op, {resultStruct});\n+    }\n     return success();\n   }\n };\n@@ -6122,7 +6166,7 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<ReduceOpConversion>(typeConverter, allocation, smem, benefit);\n   patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n                                           benefit);\n-  patterns.add<AtomicRMWOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n+  patterns.add<AtomicRMWOpConversion>(typeConverter, allocation, smem, axisInfoAnalysis, benefit);\n   patterns.add<ExtractSliceOpConversion>(typeConverter, allocation, smem,\n                                          benefit);\n   patterns.add<GetProgramIdOpConversion>(typeConverter, benefit);\n@@ -6159,10 +6203,9 @@ class ConvertTritonGPUToLLVM\n       if (srcBlocked && dstDotOp) {\n         auto tmpType = RankedTensorType::get(\n             dstType.getShape(), dstType.getElementType(),\n-            triton::gpu::SharedEncodingAttr::get(mod.getContext(), dstDotOp,\n-                                                 srcType.getShape(),\n-                                                 getOrder(srcBlocked),\n-                                                 srcType.getElementType()));\n+            triton::gpu::SharedEncodingAttr::get(\n+                mod.getContext(), dstDotOp, srcType.getShape(),\n+                getOrder(srcBlocked), srcType.getElementType()));\n         auto tmp = builder.create<triton::gpu::ConvertLayoutOp>(\n             cvtOp.getLoc(), tmpType, cvtOp.getOperand());\n         auto newConvert = builder.create<triton::gpu::ConvertLayoutOp>("}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 12, "deletions": 1, "changes": 13, "file_content_changes": "@@ -1106,7 +1106,18 @@ void init_triton_ir(py::module &&m) {\n               mlir::Value &ptr, mlir::Value &val,\n               mlir::Value &mask) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n-             mlir::Type dstType = val.getType();\n+             mlir::Type dstType;\n+             if (auto srcTensorType = ptr.getType().dyn_cast<mlir::RankedTensorType>()) {\n+               mlir::Type dstElemType = srcTensorType.getElementType()\n+                                            .cast<mlir::triton::PointerType>()\n+                                            .getPointeeType();\n+               dstType = mlir::RankedTensorType::get(srcTensorType.getShape(),\n+                                                     dstElemType);\n+             } else {\n+               auto ptrType = mlir::getElementTypeOrSelf(ptr)\n+                                  .cast<mlir::triton::PointerType>();\n+               dstType = ptrType.getPointeeType();\n+             }\n              return self.create<mlir::triton::AtomicRMWOp>(loc, dstType, rmwOp,\n                                                            ptr, val, mask);\n            })"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 61, "deletions": 81, "changes": 142, "file_content_changes": "@@ -595,100 +595,80 @@ def without_fn(X, Y, A, B, C):\n         assert c_tri == c_ref\n \n \n-# # ---------------\n-# # test atomics\n-# # ---------------\n-# @pytest.mark.parametrize(\"op, dtype_x_str, mode\", itertools.chain.from_iterable([\n-#     [\n-#         ('add', 'float16', mode),\n-#         ('add', 'uint32', mode), ('add', 'int32', mode), ('add', 'float32', mode),\n-#         ('max', 'uint32', mode), ('max', 'int32', mode), ('max', 'float32', mode),\n-#         ('min', 'uint32', mode), ('min', 'int32', mode), ('min', 'float32', mode),\n-#     ]\n-#     for mode in ['all_neg', 'all_pos', 'min_neg', 'max_pos']]))\n-# def test_atomic_rmw(op, dtype_x_str, mode, device='cuda'):\n-#     n_programs = 5\n-\n-#     # triton kernel\n-#     @triton.jit\n-#     def kernel(X, Z):\n-#         pid = tl.program_id(0)\n-#         x = tl.load(X + pid)\n-#         old = GENERATE_TEST_HERE\n-\n-#     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.atomic_{op}(Z, x)'})\n-#     numpy_op = {'add': np.sum, 'max': np.max, 'min': np.min}[op]\n-#     max_neutral = float('-inf') if dtype_x_str in float_dtypes else np.iinfo(getattr(np, dtype_x_str)).min\n-#     min_neutral = float('inf') if dtype_x_str in float_dtypes else np.iinfo(getattr(np, dtype_x_str)).max\n-#     neutral = {'add': 0, 'max': max_neutral, 'min': min_neutral}[op]\n+# ---------------\n+# test atomics\n+# ---------------\n+@pytest.mark.parametrize(\"op, dtype_x_str, mode\", itertools.chain.from_iterable([\n+    [\n+        ('add', 'float16', mode),\n+        ('add', 'uint32', mode), ('add', 'int32', mode), ('add', 'float32', mode),\n+        ('max', 'uint32', mode), ('max', 'int32', mode), ('max', 'float32', mode),\n+        ('min', 'uint32', mode), ('min', 'int32', mode), ('min', 'float32', mode),\n+    ]\n+    for mode in ['all_neg', 'all_pos', 'min_neg', 'max_pos']]))\n+def test_atomic_rmw(op, dtype_x_str, mode, device='cuda'):\n+    n_programs = 5\n \n-#     # triton result\n-#     rs = RandomState(17)\n-#     x = numpy_random((n_programs, ), dtype_str=dtype_x_str, rs=rs)\n-#     if mode == 'all_neg':\n-#         x = -np.abs(x)\n-#     if mode == 'all_pos':\n-#         x = np.abs(x)\n-#     if mode == 'min_neg':\n-#         idx = rs.randint(n_programs, size=(1, )).item()\n-#         x[idx] = -np.max(np.abs(x)) - 1\n-#     if mode == 'max_pos':\n-#         idx = rs.randint(n_programs, size=(1, )).item()\n-#         x[idx] = np.max(np.abs(x)) + 1\n-#     x_tri = to_triton(x, device=device)\n+    # triton kernel\n+    @triton.jit\n+    def kernel(X, Z):\n+        pid = tl.program_id(0)\n+        x = tl.load(X + pid)\n+        old = GENERATE_TEST_HERE\n \n-#     z_tri = to_triton(np.array([neutral], dtype=getattr(np, dtype_x_str)), device=device)\n-#     kernel[(n_programs, )](x_tri, z_tri)\n-#     # torch result\n-#     z_ref = numpy_op(x).astype(getattr(np, dtype_x_str))\n-#     # compare\n-#     exact = op not in ['add']\n-#     if exact:\n-#         assert z_ref.item() == to_numpy(z_tri).item()\n-#     else:\n-#         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.atomic_{op}(Z, x)'})\n+    numpy_op = {'add': np.sum, 'max': np.max, 'min': np.min}[op]\n+    max_neutral = float('-inf') if dtype_x_str in float_dtypes else np.iinfo(getattr(np, dtype_x_str)).min\n+    min_neutral = float('inf') if dtype_x_str in float_dtypes else np.iinfo(getattr(np, dtype_x_str)).max\n+    neutral = {'add': 0, 'max': max_neutral, 'min': min_neutral}[op]\n \n+    # triton result\n+    rs = RandomState(17)\n+    x = numpy_random((n_programs, ), dtype_str=dtype_x_str, rs=rs)\n+    if mode == 'all_neg':\n+        x = -np.abs(x)\n+    if mode == 'all_pos':\n+        x = np.abs(x)\n+    if mode == 'min_neg':\n+        idx = rs.randint(n_programs, size=(1, )).item()\n+        x[idx] = -np.max(np.abs(x)) - 1\n+    if mode == 'max_pos':\n+        idx = rs.randint(n_programs, size=(1, )).item()\n+        x[idx] = np.max(np.abs(x)) + 1\n+    x_tri = to_triton(x, device=device)\n \n-# @pytest.mark.parametrize(\"axis\", [0, 1])\n-# def test_tensor_atomic_rmw(axis, device=\"cuda\"):\n-#     shape0, shape1 = 8, 8\n-#     # triton kernel\n+    z_tri = to_triton(np.array([neutral], dtype=getattr(np, dtype_x_str)), device=device)\n+    kernel[(n_programs, )](x_tri, z_tri)\n+    # torch result\n+    z_ref = numpy_op(x).astype(getattr(np, dtype_x_str))\n+    # compare\n+    exact = op not in ['add']\n+    if exact:\n+        assert z_ref.item() == to_numpy(z_tri).item()\n+    else:\n+        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n \n-#     @triton.jit\n-#     def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n-#         off0 = tl.arange(0, SHAPE0)\n-#         off1 = tl.arange(0, SHAPE1)\n-#         x = tl.load(X + off0[:, None] * SHAPE1 + off1[None, :])\n-#         z = tl.sum(x, axis=AXIS)\n-#         tl.atomic_add(Z + off0, z)\n-#     rs = RandomState(17)\n-#     x = numpy_random((shape0, shape1), dtype_str=\"float32\", rs=rs)\n-#     # reference result\n-#     z_ref = np.sum(x, axis=axis)\n-#     # triton result\n-#     x_tri = to_triton(x, device=device)\n-#     z_tri = to_triton(np.zeros((shape0,), dtype=\"float32\"), device=device)\n-#     kernel[(1,)](z_tri, x_tri, axis, shape0, shape1)\n-#     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n+#TODO[dongdongl]:add more cases with size of tensor less than warp size\n+@pytest.mark.parametrize(\"axis\", [0, 1])\n+def test_tensor_atomic_rmw(axis, device=\"cuda\"):\n+    shape0, shape1 = 8, 8\n+    # triton kernel\n \n-def test_tensor_atomic_rmw_add_elementwise(device=\"cuda\"):\n-    shape0, shape1 = 2, 8\n     @triton.jit\n-    def kernel(Z, X, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n+    def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n         off0 = tl.arange(0, SHAPE0)\n         off1 = tl.arange(0, SHAPE1)\n         x = tl.load(X + off0[:, None] * SHAPE1 + off1[None, :])\n-        tl.atomic_add(Z + off0[:, None] * SHAPE1 + off1[None, :], x)\n-\n+        z = tl.sum(x, axis=AXIS)\n+        tl.atomic_add(Z + off0, z)\n     rs = RandomState(17)\n     x = numpy_random((shape0, shape1), dtype_str=\"float32\", rs=rs)\n-    z = numpy_random((shape0, shape1), dtype_str=\"float32\", rs=rs)\n-    # reference\n-    z_ref = z + x\n+    # reference result\n+    z_ref = np.sum(x, axis=axis)\n     # triton result\n-    x_tri = torch.from_numpy(x).to(device=device)\n-    z_tri = torch.from_numpy(z).to(device=device)\n-    kernel[(1,)](z_tri, x_tri, shape0, shape1)\n+    x_tri = to_triton(x, device=device)\n+    z_tri = to_triton(np.zeros((shape0,), dtype=\"float32\"), device=device)\n+    kernel[(1,)](z_tri, x_tri, axis, shape0, shape1)\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n \n # def test_atomic_cas():"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -27,8 +27,6 @@ def matmul_no_scf_kernel(\n     c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n     tl.store(c_ptrs, c)\n \n-# TODO: num_warps could only be 4 for now\n-\n \n @pytest.mark.parametrize('SHAPE,NUM_WARPS,TRANS_A,TRANS_B', [\n     (shape, num_warps, trans_a, trans_b)\n@@ -172,7 +170,10 @@ def get_proper_err(a, b, golden):\n     # Non-forloop\n     [64, 32, 64, 4, 64, 32, 64, False, False],\n     [128, 64, 128, 4, 128, 64, 128, False, False],\n+    [16, 16, 16, 16, 16, 16, 16, False, False],  # wpt overflow issue\n     # K-Forloop\n+    [32, 32, 64, 4, 32, 32, 32, False, False], # Single shared encoding\n+    [16, 16, 128, 4, 16, 16, 16, False, False], # Single shared encoding and small k\n     [64, 32, 128, 4, 64, 32, 64, False, False],\n     [128, 16, 128, 4, 128, 16, 32, False, False],\n     [32, 16, 128, 4, 32, 16, 32, False, False],\n@@ -186,6 +187,7 @@ def get_proper_err(a, b, golden):\n     [128, 256, 128, 4, 128, 256, 32, False, False],\n     [256, 128, 64, 4, 256, 128, 16, False, False],\n     [128, 64, 128, 4, 128, 64, 32, False, False],\n+    # [16, 16, 64, 4, 16, 16, 16, False, False],  # TODO failed due to pipeline pass\n     # trans\n     [128, 64, 128, 4, 128, 64, 32, True, False],\n     [128, 64, 128, 4, 128, 64, 32, False, True],"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -875,8 +875,8 @@ def atomic_max(ptr: tl.tensor,\n     # return atomic_umin(i_ptr, i_val) if val < 0\n     i_val = bitcast(val, tl.int32, builder)\n     i_ptr = bitcast(ptr, tl.pointer_type(tl.int32, 1), builder)\n-    pos = greater_equal(val, tl.tensor(ir.constant_float.get(sca_ty.to_ir(builder), 0), sca_ty), builder)\n-    neg = less_than(val, tl.tensor(ir.constant_float.get(sca_ty.to_ir(builder), 0), sca_ty), builder)\n+    pos = greater_equal(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n+    neg = less_than(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n     pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MAX, i_ptr.handle, i_val.handle, and_(mask, pos, builder).handle), i_val.type)\n     neg_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMIN, i_ptr.handle, i_val.handle, and_(mask, neg, builder).handle), i_val.type)\n     return where(pos, pos_ret, neg_ret, builder)\n@@ -907,8 +907,8 @@ def atomic_min(ptr: tl.tensor,\n     # return atomic_umax(i_ptr, i_val) if val < 0\n     i_val = bitcast(val, tl.int32, builder)\n     i_ptr = bitcast(ptr, tl.pointer_type(tl.int32, 1), builder)\n-    pos = greater_equal(val, tl.tensor(ir.constant_float.get(sca_ty.to_ir(builder), 0), sca_ty), builder)\n-    neg = less_than(val, tl.tensor(ir.constant_float.get(sca_ty.to_ir(builder), 0), sca_ty), builder)\n+    pos = greater_equal(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n+    neg = less_than(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n     pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MIN,\n                                                   i_ptr.handle,\n                                                   i_val.handle,"}]