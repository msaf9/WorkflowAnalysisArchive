[{"filename": "python/triton/compiler.py", "status": "modified", "additions": 17, "deletions": 34, "changes": 51, "file_content_changes": "@@ -856,8 +856,7 @@ def build_triton_ir(fn, signature, specialization, constants):\n     ret.context = context\n     return ret, generator\n \n-def make_triton_ir(fn, signature, specialization, constants):\n-    mod, _ = build_triton_ir(fn, signature, specialization, constants)\n+def optimize_triton_ir(mod):\n     pm = _triton.ir.pass_manager(mod.context)\n     pm.enable_debug()\n     pm.add_inliner_pass()\n@@ -868,6 +867,9 @@ def make_triton_ir(fn, signature, specialization, constants):\n     pm.run(mod)\n     return mod\n \n+def make_triton_ir(fn, signature, specialization, constants):\n+    mod, _ = build_triton_ir(fn, signature, specialization, constants)\n+    return optimize_triton_ir(mod)\n \n def make_tritongpu_ir(mod, num_warps, num_stages):\n     pm = _triton.ir.pass_manager(mod.context)\n@@ -901,19 +903,26 @@ def make_llvm_ir(mod, extern_libs):\n     return _triton.translate_triton_gpu_to_llvmir(mod)\n \n \n-def make_ptx(mod: Any, device: int) -> Tuple[str, int]:\n+def make_ptx(mod: Any, **kwargs) -> Tuple[str, int]:\n     '''\n     Translate TritonGPU module to PTX code.\n     :param mod: a TritonGPU dialect module\n     :return:\n         - PTX code\n         - shared memory alloaction size\n     '''\n-    assert device >= 0, \"device should be provided.\"\n-    _, cuda_version = path_to_ptxas()\n-    compute_capability = torch.cuda.get_device_capability(device)\n-    compute_capability = compute_capability[0] * 10 + compute_capability[1]\n-    ptx_version = ptx_get_version(cuda_version)\n+    if \"device\" in kwargs:\n+        assert \"compute_capability\" not in kwargs\n+        assert \"ptx_version\" not in kwargs\n+        device = int(kwargs[\"device\"])\n+        assert device >= 0, \"device should be provided.\"\n+        _, cuda_version = path_to_ptxas()\n+        compute_capability = torch.cuda.get_device_capability(device)\n+        compute_capability = compute_capability[0] * 10 + compute_capability[1]\n+        ptx_version = ptx_get_version(cuda_version)\n+    else:\n+        compute_capability = kwargs[\"compute_capability\"]\n+        ptx_version = kwargs[\"ptx_version\"]\n     return _triton.translate_llvmir_to_ptx(mod, compute_capability, ptx_version)\n \n \n@@ -1275,32 +1284,6 @@ def make_fn_cache_key(fn_hash, signature, configs, constants, num_warps, num_sta\n     return key\n \n \n-def _compile(fn, signature: str, device: int = -1, constants=dict(), specialization=instance_descriptor(), num_warps: int = 4, num_stages: int = 3, extern_libs=None, output: str = \"ttgir\") -> Tuple[str, int, str]:\n-    valid_outputs = (\"ttir\", \"ttgir\", \"ptx\", \"cubin\")\n-    assert output in valid_outputs, \"output should be one of [%s], but get \\\"%s\\\"\" % (','.join(valid_outputs), output)\n-    # triton-ir\n-    module = make_triton_ir(fn, signature, specialization, constants)\n-    ttir = module.str()\n-    if output == \"ttir\":\n-        return ttir\n-    # tritongpu-ir\n-    module = make_tritongpu_ir(module, num_warps, num_stages)\n-    ttgir = module.str()\n-    if output == \"ttgir\":\n-        return ttgir\n-    # llvm\n-    llvm_ir = make_llvm_ir(module, extern_libs)\n-    shem_size = _triton.get_shared_memory_size(module)\n-    # ptx\n-    ptx, kernel_name = make_ptx(llvm_ir, device)\n-    if output == \"ptx\":\n-        return ptx, shem_size, kernel_name\n-    # cubin\n-    cubin = make_cubin(ptx, device)\n-    if output == \"cubin\":\n-        return cubin, ptx, ttgir, ttir, shem_size, kernel_name\n-    assert False\n-\n def read_or_execute(cache_manager, force_compile, file_name, metadata,\n                     run_if_found: Callable[[str], bytes] = None,\n                     run_if_not_found: Callable = None):"}, {"filename": "python/triton/tools/aot.py", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -38,14 +38,13 @@\n         exit(0)\n \n     # triton-ir -> triton-gpu-ir\n-    module = triton.compiler.make_tritongpu_ir(module, num_warps=4)\n-    module = triton.compiler.optimize_tritongpu_ir(module, num_stages=3)\n+    module = triton.compiler.make_tritongpu_ir(module, num_warps=4, num_stages=3)\n     if args.target == 'triton-gpu-ir':\n         print(module.str())\n         exit(0)\n \n     # triton-gpu-ir -> llvm-ir\n-    module = triton.compiler.make_llvm_ir(module)\n+    module = triton.compiler.make_llvm_ir(module, extern_libs=None)\n     if args.target == 'llvm-ir':\n         print(module)\n         exit(0)"}]