[{"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 89, "deletions": 0, "changes": 89, "file_content_changes": "@@ -230,6 +230,54 @@ void LoadOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n   state.addTypes({resultType});\n }\n \n+// load(ptr, splat(1), ...)        -> load(ptr, ...)\n+// load(ptr, splat(0), other, ...) -> other\n+struct CanonicalizeMaskedLoadPattern\n+    : public mlir::OpRewritePattern<triton::LoadOp> {\n+  CanonicalizeMaskedLoadPattern(mlir::MLIRContext *context)\n+      : OpRewritePattern<triton::LoadOp>(context, 1) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(triton::LoadOp loadOp,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto mask = loadOp.getMask();\n+    if (!mask)\n+      return mlir::failure();\n+\n+    auto constantMask =\n+        llvm::dyn_cast_or_null<arith::ConstantOp>(mask.getDefiningOp());\n+    if (!constantMask)\n+      return mlir::failure();\n+\n+    auto splatMask = constantMask.getValue().dyn_cast<SplatElementsAttr>();\n+    if (!splatMask)\n+      return mlir::failure();\n+\n+    if (splatMask.getSplatValue<IntegerAttr>().getValue() == true) {\n+      // mask = splat(1)\n+      rewriter.replaceOpWithNewOp<triton::LoadOp>(\n+          loadOp, loadOp.getType(), loadOp.getPtr(), Value(), Value(),\n+          loadOp.getBoundaryCheckAttr(), loadOp.getPaddingAttr(),\n+          loadOp.getCache(), loadOp.getEvict(), loadOp.getIsVolatile());\n+    } else {\n+      // mask = splat(0)\n+\n+      // If there's no \"other\", the value is \"undef\".  Perhaps we want to\n+      // optimize it in the future.x\n+      auto otherVal = loadOp.getOther();\n+      if (!otherVal)\n+        return mlir::failure();\n+      rewriter.replaceOp(loadOp, otherVal);\n+    }\n+    return mlir::success();\n+  }\n+};\n+\n+void triton::LoadOp::getCanonicalizationPatterns(RewritePatternSet &results,\n+                                                 MLIRContext *context) {\n+  results.add<CanonicalizeMaskedLoadPattern>(context);\n+}\n+\n //-- StoreOp --\n void StoreOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n                     ::mlir::Value ptr, ::mlir::Value value,\n@@ -257,6 +305,47 @@ void StoreOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n                         evict);\n }\n \n+// store(ptr, value, splat(1), ...) -> store(ptr, value, ...)\n+// store(ptr, value, splat(0), ...) -> [none]\n+struct CanonicalizeMaskedStorePattern\n+    : public mlir::OpRewritePattern<triton::StoreOp> {\n+  CanonicalizeMaskedStorePattern(mlir::MLIRContext *context)\n+      : OpRewritePattern<triton::StoreOp>(context, 1) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(triton::StoreOp storeOp,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto mask = storeOp.getMask();\n+    if (!mask)\n+      return mlir::failure();\n+\n+    auto constantMask =\n+        llvm::dyn_cast_or_null<arith::ConstantOp>(mask.getDefiningOp());\n+    if (!constantMask)\n+      return mlir::failure();\n+\n+    auto splatMask = constantMask.getValue().dyn_cast<SplatElementsAttr>();\n+    if (!splatMask)\n+      return mlir::failure();\n+\n+    if (splatMask.getSplatValue<IntegerAttr>().getValue() == true) {\n+      // mask = splat(1)\n+      rewriter.replaceOpWithNewOp<triton::StoreOp>(\n+          storeOp, storeOp.getPtr(), storeOp.getValue(), storeOp.getCache(),\n+          storeOp.getEvict());\n+    } else {\n+      // mask = splat(0)\n+      rewriter.eraseOp(storeOp);\n+    }\n+    return mlir::success();\n+  }\n+};\n+\n+void triton::StoreOp::getCanonicalizationPatterns(RewritePatternSet &results,\n+                                                  MLIRContext *context) {\n+  results.add<CanonicalizeMaskedStorePattern>(context);\n+}\n+\n //-- TransOp --\n mlir::LogicalResult mlir::triton::TransOp::inferReturnTypes(\n     MLIRContext *context, std::optional<Location> location, ValueRange operands,"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.cpp", "status": "modified", "additions": 0, "deletions": 89, "changes": 89, "file_content_changes": "@@ -101,95 +101,6 @@ class CombineSelectMaskedLoadPattern : public mlir::RewritePattern {\n   }\n };\n \n-// load(ptr, splat(1), ...)        -> load(ptr, ...)\n-// load(ptr, splat(0), other, ...) -> other\n-struct CanonicalizeMaskedLoadPattern\n-    : public mlir::OpRewritePattern<triton::LoadOp> {\n-  CanonicalizeMaskedLoadPattern(mlir::MLIRContext *context)\n-      : OpRewritePattern<triton::LoadOp>(context, 1) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(triton::LoadOp loadOp,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto mask = loadOp.getMask();\n-    if (!mask)\n-      return mlir::failure();\n-\n-    auto constantMask =\n-        llvm::dyn_cast_or_null<arith::ConstantOp>(mask.getDefiningOp());\n-    if (!constantMask)\n-      return mlir::failure();\n-\n-    auto splatMask = constantMask.getValue().dyn_cast<SplatElementsAttr>();\n-    if (!splatMask)\n-      return mlir::failure();\n-\n-    if (splatMask.getSplatValue<IntegerAttr>().getValue() == true) {\n-      // mask = splat(1)\n-      rewriter.replaceOpWithNewOp<triton::LoadOp>(\n-          loadOp, loadOp.getType(), loadOp.getPtr(), Value(), Value(),\n-          loadOp.getBoundaryCheckAttr(), loadOp.getPaddingAttr(),\n-          loadOp.getCache(), loadOp.getEvict(), loadOp.getIsVolatile());\n-    } else {\n-      // mask = splat(0)\n-\n-      // If there's no \"other\", the value is \"undef\".  Perhaps we want to\n-      // optimize it in the future.x\n-      auto otherVal = loadOp.getOther();\n-      if (!otherVal)\n-        return mlir::failure();\n-      rewriter.replaceOp(loadOp, otherVal);\n-    }\n-    return mlir::success();\n-  }\n-};\n-\n-void triton::LoadOp::getCanonicalizationPatterns(RewritePatternSet &results,\n-                                                 MLIRContext *context) {\n-  results.add<CanonicalizeMaskedLoadPattern>(context);\n-}\n-\n-// store(ptr, value, splat(1), ...) -> store(ptr, value, ...)\n-// store(ptr, value, splat(0), ...) -> [none]\n-struct CanonicalizeMaskedStorePattern\n-    : public mlir::OpRewritePattern<triton::StoreOp> {\n-  CanonicalizeMaskedStorePattern(mlir::MLIRContext *context)\n-      : OpRewritePattern<triton::StoreOp>(context, 1) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(triton::StoreOp storeOp,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto mask = storeOp.getMask();\n-    if (!mask)\n-      return mlir::failure();\n-\n-    auto constantMask =\n-        llvm::dyn_cast_or_null<arith::ConstantOp>(mask.getDefiningOp());\n-    if (!constantMask)\n-      return mlir::failure();\n-\n-    auto splatMask = constantMask.getValue().dyn_cast<SplatElementsAttr>();\n-    if (!splatMask)\n-      return mlir::failure();\n-\n-    if (splatMask.getSplatValue<IntegerAttr>().getValue() == true) {\n-      // mask = splat(1)\n-      rewriter.replaceOpWithNewOp<triton::StoreOp>(\n-          storeOp, storeOp.getPtr(), storeOp.getValue(), storeOp.getCache(),\n-          storeOp.getEvict());\n-    } else {\n-      // mask = splat(0)\n-      rewriter.eraseOp(storeOp);\n-    }\n-    return mlir::success();\n-  }\n-};\n-\n-void triton::StoreOp::getCanonicalizationPatterns(RewritePatternSet &results,\n-                                                  MLIRContext *context) {\n-  results.add<CanonicalizeMaskedStorePattern>(context);\n-}\n-\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/Triton/Transforms/Passes.h.inc\"\n "}, {"filename": "python/test/regression/test_functional_regressions.py", "status": "added", "additions": 68, "deletions": 0, "changes": 68, "file_content_changes": "@@ -0,0 +1,68 @@\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+def test_chained_matmul():\n+    # Regression test for issue #1601\n+    def chained_matmul_reference(a, b, c):\n+        intermediate = torch.einsum('MK,NK->MN', a, b)\n+        return torch.einsum('MN,NK->MK', intermediate, c)\n+\n+    @triton.jit\n+    def chained_matmul_kernel(\n+            A,  # shape: (m, k)\n+            B,  # shape: (n, k)\n+            C,  # shape: (n, k)\n+            out,  # shape: (m, k)\n+            m, n, k: tl.constexpr,\n+            block_m: tl.constexpr,\n+            block_n: tl.constexpr,\n+            block_k: tl.constexpr):\n+\n+        tl.static_assert(block_k == k,\n+                         f\"expected block_k == k but got {block_k} != {k}\")\n+\n+        block_ix = tl.program_id(0)\n+        a_tile = (block_ix * block_m + tl.arange(0, block_m))[:, None] * block_k \\\n+            + tl.arange(0, block_k)[None, :]\n+\n+        a = tl.load(A + a_tile, mask=a_tile < m * k, other=0.0)\n+\n+        acc = tl.zeros([block_m, block_k], dtype=tl.float32)\n+\n+        for loop_block_start in range(0, n, block_n):\n+            bc_tile = (loop_block_start + tl.arange(0, block_n))[:, None] * block_k \\\n+                + tl.arange(0, block_k)[None, :]\n+            b = tl.load(B + bc_tile, mask=bc_tile < n * k, other=0.0)\n+\n+            intermediate = tl.dot(a, tl.trans(b))\n+            intermediate_mask = ((loop_block_start + tl.arange(0, block_n)) < n)[None, :] \\\n+                * (tl.arange(0, block_m) < m)[:, None]\n+\n+            intermediate = tl.where(intermediate_mask, intermediate, 0.0)\n+\n+            c = tl.load(C + bc_tile, mask=bc_tile < n * k)\n+\n+            acc += tl.dot(intermediate.to(A.dtype.element_ty), c)\n+\n+        tl.store(out + a_tile, acc.to(A.dtype.element_ty), mask=a_tile < m * k)\n+\n+    m, n, k = 32, 64, 128\n+    block_m, block_n, block_k = 16, 32, k\n+\n+    grid = (triton.cdiv(m, block_m),)\n+    a = torch.randint(low=0, high=2, size=(m, k), dtype=torch.float16,\n+                      device='cuda')\n+    b = torch.randint(low=0, high=2, size=(n, k), dtype=torch.float16,\n+                      device='cuda')\n+    c = torch.randint_like(b, low=0, high=2)\n+    triton_result = torch.zeros_like(a)\n+\n+    torch_result = chained_matmul_reference(a, b, c)\n+    chained_matmul_kernel[grid](a, b, c, triton_result, m, n, k,\n+                                block_m=block_m, block_n=block_n,\n+                                block_k=block_k)\n+\n+    assert (torch_result == triton_result).all()"}, {"filename": "python/test/unit/runtime/test_cache.py", "status": "modified", "additions": 0, "deletions": 35, "changes": 35, "file_content_changes": "@@ -1,7 +1,5 @@\n-import multiprocessing\n import os\n import shutil\n-from collections import namedtuple\n \n import pytest\n import torch\n@@ -198,39 +196,6 @@ def kernel_add_device(a, b, o, N: tl.constexpr):\n     assert inline_ttir != noinline_ttir\n \n \n-instance_descriptor = namedtuple(\"instance_descriptor\", [\"divisible_by_16\", \"equal_to_1\"])\n-\n-\n-def compile_fn(config, cc):\n-    @triton.jit\n-    def kernel_sub(a, b, o, N: tl.constexpr):\n-        idx = tl.arange(0, N)\n-        tl.store(o + idx, tl.load(a + idx) - tl.load(b + idx) * 777)\n-    triton.compile(\n-        fn=kernel_sub,\n-        signature={0: \"*fp32\", 1: \"*fp32\", 2: \"*fp32\"},\n-        device=0,\n-        constants={3: 32},\n-        configs=[config],\n-        warm_cache_only=True,\n-        cc=cc,\n-    )\n-\n-\n-def test_compile_in_subproc() -> None:\n-    major, minor = torch.cuda.get_device_capability(0)\n-    cc = major * 10 + minor\n-    config = instance_descriptor(tuple(range(4)), ())\n-\n-    multiprocessing.set_start_method('spawn')\n-    proc = multiprocessing.Process(\n-        target=compile_fn,\n-        args=(config, cc))\n-    proc.start()\n-    proc.join()\n-    assert proc.exitcode == 0\n-\n-\n def test_memory_leak() -> None:\n     @triton.jit\n     def kernel(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):"}, {"filename": "python/test/unit/runtime/test_subproc.py", "status": "added", "additions": 83, "deletions": 0, "changes": 83, "file_content_changes": "@@ -0,0 +1,83 @@\n+import multiprocessing\n+import os\n+import shutil\n+from collections import namedtuple\n+\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+tmpdir = \".tmp\"\n+\n+\n+def reset_tmp_dir():\n+    os.environ[\"TRITON_CACHE_DIR\"] = tmpdir\n+    if os.path.exists(tmpdir):\n+        shutil.rmtree(tmpdir)\n+\n+\n+instance_descriptor = namedtuple(\"instance_descriptor\", [\"divisible_by_16\", \"equal_to_1\"])\n+\n+\n+def compile_fn(config, cc):\n+    @triton.jit\n+    def kernel_sub(a, b, o, N: tl.constexpr):\n+        idx = tl.arange(0, N)\n+        tl.store(o + idx, tl.load(a + idx) - tl.load(b + idx) * 777)\n+    triton.compile(\n+        fn=kernel_sub,\n+        signature={0: \"*fp32\", 1: \"*fp32\", 2: \"*fp32\"},\n+        device=0,\n+        constants={3: 32},\n+        configs=[config],\n+        warm_cache_only=True,\n+        cc=cc,\n+    )\n+\n+\n+def test_compile_in_subproc() -> None:\n+    major, minor = torch.cuda.get_device_capability(0)\n+    cc = major * 10 + minor\n+    config = instance_descriptor(tuple(range(4)), ())\n+\n+    multiprocessing.set_start_method('fork')\n+    proc = multiprocessing.Process(\n+        target=compile_fn,\n+        args=(config, cc))\n+    proc.start()\n+    proc.join()\n+    assert proc.exitcode == 0\n+\n+\n+def compile_fn_dot(config, cc):\n+    @triton.jit\n+    def kernel_dot(Z):\n+        offs = tl.arange(0, 16)[:, None] * 16 + tl.arange(0, 16)[None, :]\n+        z = tl.load(Z + offs)\n+        z = tl.dot(z, z)\n+        tl.store(Z + offs, z)\n+\n+    triton.compile(\n+        fn=kernel_dot,\n+        signature={0: \"*fp32\"},\n+        device=0,\n+        configs=[config],\n+        warm_cache_only=True,\n+        cc=cc,\n+    )\n+\n+\n+def test_compile_in_forked_subproc() -> None:\n+    reset_tmp_dir()\n+    major, minor = torch.cuda.get_device_capability(0)\n+    cc = major * 10 + minor\n+    config = instance_descriptor(tuple(range(1)), ())\n+\n+    assert multiprocessing.get_start_method() == 'fork'\n+    proc = multiprocessing.Process(\n+        target=compile_fn_dot,\n+        args=(config, cc))\n+    proc.start()\n+    proc.join()\n+    assert proc.exitcode == 0"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 0, "deletions": 13, "changes": 13, "file_content_changes": "@@ -3,7 +3,6 @@\n from functools import wraps\n from typing import List, Optional, Sequence, Tuple, TypeVar\n \n-import triton\n from . import core as tl\n from triton._C.libtriton.triton import ir\n \n@@ -1181,18 +1180,6 @@ def dot(lhs: tl.tensor,\n         allow_tf32: bool,\n         out_dtype: tl.dtype,\n         builder: ir.builder) -> tl.tensor:\n-    try:\n-        import torch\n-    except ImportError:\n-        raise ImportError(\"Triton requires PyTorch to be installed\")\n-    if torch.version.hip is None:\n-        device = triton.runtime.jit.get_current_device()\n-        capability = triton.runtime.jit.get_device_capability(device)\n-        capability = capability[0] * 10 + capability[1]\n-        if capability < 70:\n-            assert (\n-                not rhs.dtype.is_fp16() and not rhs.dtype.is_fp8()\n-            ), \"Float8 and Float16 types are not supported for compute capability < 70 (use Float32 or above)\"\n     assert lhs.type.is_block() and rhs.type.is_block()\n     assert lhs.dtype == rhs.dtype, \"lhs and rhs must have the same dtype!\"\n     assert len(lhs.shape) == 2 and len(rhs.shape) == 2"}]