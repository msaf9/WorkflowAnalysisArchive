[{"filename": ".hypothesis/unicode_data/13.0.0/charmap.json.gz", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 10, "deletions": 8, "changes": 18, "file_content_changes": "@@ -12,19 +12,19 @@ namespace mlir {\n \n class ReduceOpHelper {\n public:\n-  explicit ReduceOpHelper(triton::ReduceOp rop)\n-      : op(rop.getOperation()), axis(rop.getAxis()) {\n-    auto firstTy = rop.getOperands()[0].getType().cast<RankedTensorType>();\n+  explicit ReduceOpHelper(triton::ReduceOp op)\n+      : op(op.getOperation()), axis(op.getAxis()) {\n+    auto firstTy = op.getOperands()[0].getType().cast<RankedTensorType>();\n     srcShape = firstTy.getShape();\n     srcEncoding = firstTy.getEncoding();\n-    srcElementTypes = rop.getElementTypes();\n+    srcElementTypes = op.getElementTypes();\n \n-    for (const auto &t : rop.getInputTypes()) {\n+    for (const auto &t : op.getInputTypes()) {\n       if (t.getShape() != srcShape) {\n-        rop.emitError() << \"shape mismatch\";\n+        op.emitError() << \"shape mismatch\";\n       }\n       if (t.getEncoding() != srcEncoding) {\n-        rop.emitError() << \"encoding mismatch\";\n+        op.emitError() << \"encoding mismatch\";\n       }\n     }\n   }\n@@ -33,6 +33,8 @@ class ReduceOpHelper {\n \n   Attribute getSrcLayout() { return srcEncoding; }\n \n+  triton::ReduceOp getOperation() { return op; }\n+\n   bool isFastReduction();\n \n   unsigned getInterWarpSize();\n@@ -54,7 +56,7 @@ class ReduceOpHelper {\n   bool isSupportedLayout();\n \n private:\n-  Operation *op;\n+  triton::ReduceOp op;\n   ArrayRef<int64_t> srcShape;\n   Attribute srcEncoding;\n   SmallVector<Type> srcElementTypes;"}, {"filename": "include/triton/Dialect/Triton/IR/TritonTypes.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -14,7 +14,7 @@ class TritonTypeDef<string name, string _mnemonic>\n }\n \n // Floating-point Type\n-def TT_Float : AnyTypeOf<[F8E4M3FN, F8E5M2, F16, BF16, F32, F64], \"floating-point\">;\n+def TT_Float : AnyTypeOf<[F8E4M3FNUZ, F8E4M3B11FNUZ, F8E5M2, F16, BF16, F32, F64], \"floating-point\">;\n def TT_FloatTensor : TensorOf<[TT_Float]>;\n def TT_FloatLike : AnyTypeOf<[TT_Float, TT_FloatTensor]>;\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -28,8 +28,13 @@ unsigned getTotalElemsPerThread(Attribute layout, ArrayRef<int64_t> shape,\n \n SmallVector<unsigned> getElemsPerThread(Type type);\n \n+// Returns the number of threads per warp that may have access to replicated\n+// elements. If you want non-replicated threads, use\n+// getThreadsPerWarpWithUniqueData.\n SmallVector<unsigned> getThreadsPerWarp(Attribute layout);\n \n+// Returns the number of warps per CTA that may have access to replicated\n+// elements. If you want non-replicated warps, use getWarpsPerCTAWithUniqueData.\n SmallVector<unsigned> getWarpsPerCTA(Attribute layout);\n \n SmallVector<unsigned> getSizePerThread(Attribute layout);"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -5,11 +5,15 @@\n #include \"mlir/IR/Matchers.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Tools/Sys/GetEnv.hpp\"\n #include <deque>\n \n namespace mlir {\n \n bool ReduceOpHelper::isFastReduction() {\n+  // Disable fast reduction only for debugging purpose\n+  if (::triton::tools::getBoolEnv(\"DISABLE_FAST_REDUCTION\"))\n+    return false;\n   return axis == triton::gpu::getOrder(getSrcLayout())[0];\n }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 6, "deletions": 3, "changes": 9, "file_content_changes": "@@ -1,8 +1,10 @@\n #include \"ConvertLayoutOpToLLVM.h\"\n #include \"Utility.h\"\n \n+using ::mlir::LLVM::delinearize;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n using ::mlir::LLVM::getStridesFromShapeAndOrder;\n+using ::mlir::LLVM::linearize;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::getContigPerThread;\n using ::mlir::triton::gpu::getOrder;\n@@ -143,9 +145,10 @@ struct ConvertLayoutOpConversion\n       Value laneId = urem(threadId, warpSize);\n       Value warpId = udiv(threadId, warpSize);\n       // TODO: fix the bug in MMAEncodingAttr document\n-      SmallVector<Value> multiDimWarpId(2);\n-      multiDimWarpId[0] = urem(warpId, i32_val(mmaLayout.getWarpsPerCTA()[0]));\n-      multiDimWarpId[1] = udiv(warpId, i32_val(mmaLayout.getWarpsPerCTA()[0]));\n+      auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n+      auto order = triton::gpu::getOrder(mmaLayout);\n+      SmallVector<Value> multiDimWarpId =\n+          delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n       Value _1 = i32_val(1);\n       Value _2 = i32_val(2);\n       Value _4 = i32_val(4);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 7, "deletions": 6, "changes": 13, "file_content_changes": "@@ -4,6 +4,7 @@\n using namespace mlir;\n \n using ValueTable = std::map<std::pair<unsigned, unsigned>, Value>;\n+using ::mlir::LLVM::delinearize;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n using ::mlir::LLVM::getStridesFromShapeAndOrder;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n@@ -552,14 +553,14 @@ Value loadArg(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n   int kWidth = encoding.getMMAv2kWidth();\n \n   auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n+  auto order = triton::gpu::getOrder(mmaLayout);\n   Value warp = udiv(thread, i32_val(32));\n   Value lane = urem(thread, i32_val(32));\n-  // Note: warps are currently column major in MMA layout\n-  Value warpRowIndex = urem(warp, i32_val(warpsPerCTA[0]));\n-  Value warpColIndex =\n-      urem(udiv(warp, i32_val(warpsPerCTA[0])), i32_val(warpsPerCTA[1]));\n-  Value warpM = urem(warpRowIndex, i32_val(shape[0] / 16));\n-  Value warpN = urem(warpColIndex, i32_val(shape[1] / 8));\n+\n+  SmallVector<Value> multiDimWarpId =\n+      delinearize(rewriter, loc, warp, warpsPerCTA, order);\n+  Value warpM = urem(multiDimWarpId[0], i32_val(shape[0] / 16));\n+  Value warpN = urem(multiDimWarpId[1], i32_val(shape[1] / 8));\n \n   int warpsPerTile;\n   if (isA)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 301, "deletions": 451, "changes": 752, "file_content_changes": "@@ -4,6 +4,227 @@ using namespace mlir;\n using namespace mlir::triton;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n \n+/* ----- FP8E5M2 ------ */\n+// This data-type is the standard FP8E5M2 format\n+\n+const std::string Fp16_to_Fp8E5M2 =\n+    \"{                            \\n\"\n+    \".reg .b32 a<2>;              \\n\"\n+    \"and.b32 a0, $1, 0x7fff7fff;  \\n\"           // a0 &= 0x7fff7fff\n+    \"and.b32 a1, $2, 0x7fff7fff;  \\n\"           // (strip sign)\n+    \"add.u32 a0, a0, 0x00800080;  \\n\"           // a0 += 0x00800080\n+    \"add.u32 a1, a1, 0x00800080;  \\n\"           // (round to nearest)\n+    \"lop3.b32 a0, $1, 0x80008000, a0, 0xea; \\n\" // a0 = a0|(0x80008000&in0)\n+    \"lop3.b32 a1, $2, 0x80008000, a1, 0xea; \\n\" // (restore sign)\n+    \"prmt.b32 $0, a0, a1, 0x7531; \\n\\t\"         // output = a1a0\n+    \"}\";\n+\n+const std::string Fp8E5M2_to_Fp16 = \"{                           \\n\"\n+                                    \"prmt.b32 $0, 0, $2, 0x5140; \\n\\t\"\n+                                    \"prmt.b32 $1, 0, $2, 0x7362; \\n\\t\"\n+                                    \"}\";\n+\n+const std::string Fp8E5M2_to_Bf16 =\n+    \"{                                      \\n\"\n+    \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n+    \"prmt.b32 a0, 0, $2, 0x5140;            \\n\" // a0 = 0xf300f400\n+    \"prmt.b32 a1, 0, $2, 0x7362;            \\n\" // a1 = 0xf100f200\n+    \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // b0 = a0 & 0x7fff7fff\n+    \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n+    \"shr.b32  b0, b0, 3;                    \\n\" // b0 >>= 3\n+    \"shr.b32  b1, b1, 3;                    \\n\" // shift into bf16 position\n+    \"add.u32  b0, b0, 0x38003800;           \\n\" // b0.exp += 2**7-2**4\n+                                                // exponent compensate = 112\n+    \"add.u32  b1, b1, 0x38003800;           \\n\" // b1 += 112<<7 | 112<<7<<16\n+    \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n+    \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n+    \"}\";\n+\n+const std::string Bf16_to_Fp8E5M2 =\n+    \"{                                           \\n\" // bf16=fp8>>3 + 112<<7\n+    \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\" // fp8_min = 0b00000000\n+    \".reg .u32 fp8_min, fp8_max, rn_;            \\n\" // fp8_max = 0b11111111\n+    \"mov.u32 fp8_min, 0x38003800;                \\n\" // so bf16_min = 0x3800\n+    \"mov.u32 fp8_max, 0x57e057e0;                \\n\" // so bf16_max = 0x57e0\n+    \"mov.u32 rn_, 0x00100010;                    \\n\" // round to nearest\n+    \"and.b32 sign0, $1, 0x80008000;              \\n\" // sign0=in0&0x80008000\n+    \"and.b32 sign1, $2, 0x80008000;              \\n\" // (store sign)\n+    \"prmt.b32 sign, sign0, sign1, 0x7531;        \\n\"\n+    \"and.b32 nosign0, $1, 0x7fff7fff;            \\n\" // nosign0=in0&0x7fff7fff\n+    \"and.b32 nosign1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n+\n+    // nosign = clamp(nosign, min, max)\n+    \".reg .u32 nosign_0_<2>, nosign_1_<2>;       \\n\"\n+    \"and.b32 nosign_0_0, nosign0, 0xffff0000;    \\n\"\n+    \"max.u32 nosign_0_0, nosign_0_0, 0x38000000; \\n\"\n+    \"min.u32 nosign_0_0, nosign_0_0, 0x57e00000; \\n\"\n+    \"and.b32 nosign_0_1, nosign0, 0x0000ffff;    \\n\"\n+    \"max.u32 nosign_0_1, nosign_0_1, 0x3800;     \\n\"\n+    \"min.u32 nosign_0_1, nosign_0_1, 0x57e0;     \\n\"\n+    \"or.b32 nosign0, nosign_0_0, nosign_0_1;     \\n\"\n+    \"and.b32 nosign_1_0, nosign1, 0xffff0000;    \\n\"\n+    \"max.u32 nosign_1_0, nosign_1_0, 0x38000000; \\n\"\n+    \"min.u32 nosign_1_0, nosign_1_0, 0x57e00000; \\n\"\n+    \"and.b32 nosign_1_1, nosign1, 0x0000ffff;    \\n\"\n+    \"max.u32 nosign_1_1, nosign_1_1, 0x3800;     \\n\"\n+    \"min.u32 nosign_1_1, nosign_1_1, 0x57e0;     \\n\"\n+    \"or.b32 nosign1, nosign_1_0, nosign_1_1;     \\n\"\n+\n+    \"add.u32 nosign0, nosign0, rn_;              \\n\" // nosign0 += rn_\n+    \"add.u32 nosign1, nosign1, rn_;              \\n\" // (round to nearest)\n+    \"sub.u32 nosign0, nosign0, 0x38003800;       \\n\" // nosign0-=0x38003800\n+    \"sub.u32 nosign1, nosign1, 0x38003800;       \\n\" // (compensate offset)\n+    \"shl.b32 nosign0, nosign0, 3;                \\n\" // nosign0 <<= 3\n+    \"shl.b32 nosign1, nosign1, 3;                \\n\" // shift into to fp8e4\n+    \"prmt.b32 nosign, nosign0, nosign1, 0x7531;  \\n\" // nosign0 = 0xf100f200\n+                                                     // nosign1 = 0xf300f400\n+                                                     // nosign = 0xf3f4f1f2\n+    \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n+    \"}\";\n+\n+/* ----- FP8E4M3B15 ------ */\n+// This data-type is a variant of the standard FP8E4M3 format.\n+// It was designed for fast software conversion to FP16 on\n+// nvidia GPUs that do not support it natively.\n+// Specifically, this data-type:\n+//    - has infinities\n+//    - has multiple nans (when all exponent bits are 1)\n+//    - has an exponent bias of 15 (vs. 7 for fp8e4m3)\n+\n+// Fp8E4M3B15 -> Fp16 (packed)\n+// fast conversion code provided by Scott Gray @ OpenAI\n+// $0 = (($2 << 1) & 0x80008000u) | (($2 << 7) & 0x3f803f80u);\n+// $1 = (($2 << 0) & 0x80008000u) | (($2 << 0) & 0x3f803f80u);\n+// WARN: subnormal (0bs0000xxx) are not handled\n+const std::string Fp8E4M3B15_to_Fp16 =\n+    \"{                                      \\n\"\n+    \".reg .b32 a<2>;                        \\n\"\n+    \"shl.b32 a0, $2, 1;                     \\n\"\n+    \"shl.b32 a1, $2, 7;                     \\n\"\n+    \"and.b32  $0, a0, 0x80008000;           \\n\"\n+    \"lop3.b32 $0, $0, a1, 0x3f803f80, 0xf8; \\n\"\n+    \"and.b32  $1, $2, 0x80008000;           \\n\"\n+    \"lop3.b32 $1, $1, $2, 0x3f803f80, 0xf8; \\n\"\n+    \"}\";\n+\n+// Fp16 -> Fp8E4M3B15 (packed)\n+// fast conversion code provided by Scott Gray @ OpenAI\n+// ret = ((e4.x >> 1) & (0x80008000u >> 1)) |\n+//       ((e4.x >> 7) & (0x3f803f80u >> 7)) |\n+//       ((e4.y >> 0) & (0x80008000u >> 0)) |\n+//       ((e4.y >> 0) & (0x3f803f80u >> 0)) ;\n+// WARN: subnormal (0bs0000xxx) are not handled\n+const std::string Fp16_to_Fp8E4M3B15 =\n+    \"{                                       \\n\"\n+    \".reg .b32 a<2>;                         \\n\"\n+    \"shr.b32  a0, $1, 1;                     \\n\"\n+    \"shr.b32  a1, $1, 7;                     \\n\"\n+    \"and.b32  $0,     a0, 0x40004000;        \\n\"\n+    \"lop3.b32 $0, $0, a1, 0x007f007f, 0xf8;  \\n\"\n+    \"lop3.b32 $0, $0, $2, 0x80008000, 0xf8;  \\n\"\n+    \"lop3.b32 $0, $0, $2, 0x3f803f80, 0xf8;  \\n\"\n+    \"}\";\n+\n+/* ----- FP8E4M3 ------ */\n+// Note: when handled by software, this format\n+// does not handle denormals and has\n+// more than a single NaN values.\n+\n+// Fp8E4M3 -> Fp16 (packed)\n+const std::string Fp8E4M3_to_Fp16 =\n+    \"{                                      \\n\"\n+    \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n+    \"prmt.b32 a0, 0, $2, 0x5040;            \\n\" // a0 = 0xf300f400\n+    \"prmt.b32 a1, 0, $2, 0x7060;            \\n\" // a1 = 0xf100f200\n+    \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // b0 = a0 & 0x7fff7fff\n+    \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n+    \"shr.b32  b0, b0, 1;                    \\n\" // b0 >>= 1\n+    \"shr.b32  b1, b1, 1;                    \\n\" // shift into fp16 position\n+    \"add.u32  b0, b0, 0x20002000;           \\n\" // b0.exp += 2**4-2**3\n+                                                // exponent compensate = 8\n+    \"add.u32  b1, b1, 0x20002000;           \\n\" // b1 += 8<<10 | 8<<10<<16\n+    \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n+    \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n+    \"}\";\n+\n+// Fp16 -> Fp8E4M3 (packed)\n+const std::string Fp16_to_Fp8E4M3 =\n+    \"{                                      \\n\"\n+    \".reg .b32 a<2>, b<2>;                  \\n\" // see Fp8E4M3x4ToFp16x4\n+    \"sub.u32 a0, $1, 0x20002000;            \\n\" // a0 = input0 - 0x20002000\n+                                                // (compensate offset)\n+    \"sub.u32 a1, $2, 0x20002000;            \\n\" // a1 = input1 - 0x20002000\n+                                                // (8 << 10 | 8 << 10 << 16)\n+    \"shl.b32 a0, a0, 1;                     \\n\" // a0 <<= 1\n+    \"shl.b32 a1, a1, 1;                     \\n\" // shift into fp8e4 position\n+    \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // a0 &= 0x7fff7fff\n+    \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n+    \"add.u32 a0, a0, 0x00800080;            \\n\" // a0 += 0x00800080\n+    \"add.u32 a1, a1, 0x00800080;            \\n\" // (round to nearest)\n+    \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\" // b0 = a0|(0x80008000&in0)\n+    \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\" // (restore sign)\n+    \"prmt.b32 $0, b0, b1, 0x7531;           \\n\" // output = b1b0\n+    \"}\";\n+\n+// WARN: subnormal (0bs0000xxx) are not handled\n+const std::string Fp8E4M3_to_Bf16 =\n+    \"{                                      \\n\"\n+    \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n+    \"prmt.b32 a0, 0, $2, 0x5040;            \\n\" // a0 = 0xf300f400\n+    \"prmt.b32 a1, 0, $2, 0x7060;            \\n\" // a1 = 0xf100f200\n+    \"and.b32 b0, a0, 0x7fff7fff;            \\n\" // b0 = a0 & 0x7fff7fff\n+    \"and.b32 b1, a1, 0x7fff7fff;            \\n\" // (strip sign)\n+    \"shr.b32 b0, b0, 4;                     \\n\" // b0 >>= 4\n+    \"shr.b32 b1, b1, 4;                     \\n\" // shift into fp16 position\n+    \"add.u32 b0, b0, 0x3c003c00;            \\n\" // b0.exp += 2**7-2**3\n+                                                // exponent compensate = 120\n+    \"add.u32 b1, b1, 0x3c003c00;            \\n\" // b1 += 120<<7 | 120<<7<<16\n+    \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n+    \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n+    \"}\";\n+\n+const std::string Bf16_to_Fp8E4M3 =\n+    \"{                                           \\n\" // bf16=fp8>>4 + 120<<7\n+    \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\" // fp8_min = 0b00000000\n+    \".reg .u32 fp8_min, fp8_max, rn_;            \\n\" // fp8_max = 0b11111111\n+    \"mov.u32 fp8_min, 0x3c003c00;                \\n\" // so bf16_min = 0x3c00\n+    \"mov.u32 fp8_max, 0x43f043f0;                \\n\" // so bf16_max = 0x43f0\n+    \"mov.u32 rn_, 0x80008;                       \\n\" // round to nearest\n+    \"and.b32 sign0, $1, 0x80008000;              \\n\" // sign0=in0&0x80008000\n+    \"and.b32 sign1, $2, 0x80008000;              \\n\" // (store sign)\n+    \"prmt.b32 sign, sign0, sign1, 0x7531;        \\n\"\n+    \"and.b32 nosign0, $1, 0x7fff7fff;            \\n\" // nosign0=in0&0x7fff7fff\n+    \"and.b32 nosign1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n+\n+    // nosign = clamp(nosign, min, max)\n+    \".reg .u32 nosign_0_<2>, nosign_1_<2>;       \\n\"\n+    \"and.b32 nosign_0_0, nosign0, 0xffff0000;    \\n\"\n+    \"max.u32 nosign_0_0, nosign_0_0, 0x3c000000; \\n\"\n+    \"min.u32 nosign_0_0, nosign_0_0, 0x43f00000; \\n\"\n+    \"and.b32 nosign_0_1, nosign0, 0x0000ffff;    \\n\"\n+    \"max.u32 nosign_0_1, nosign_0_1, 0x3c00;     \\n\"\n+    \"min.u32 nosign_0_1, nosign_0_1, 0x43f0;     \\n\"\n+    \"or.b32 nosign0, nosign_0_0, nosign_0_1;     \\n\"\n+    \"and.b32 nosign_1_0, nosign1, 0xffff0000;    \\n\"\n+    \"max.u32 nosign_1_0, nosign_1_0, 0x3c000000; \\n\"\n+    \"min.u32 nosign_1_0, nosign_1_0, 0x43f00000; \\n\"\n+    \"and.b32 nosign_1_1, nosign1, 0x0000ffff;    \\n\"\n+    \"max.u32 nosign_1_1, nosign_1_1, 0x3c00;     \\n\"\n+    \"min.u32 nosign_1_1, nosign_1_1, 0x43f0;     \\n\"\n+    \"or.b32 nosign1, nosign_1_0, nosign_1_1;     \\n\"\n+\n+    \"add.u32 nosign0, nosign0, rn_;              \\n\" // nosign0 += rn_\n+    \"add.u32 nosign1, nosign1, rn_;              \\n\" // (round to nearest)\n+    \"sub.u32 nosign0, nosign0, 0x3c003c00;       \\n\" // nosign0-=0x3c003c00\n+    \"sub.u32 nosign1, nosign1, 0x3c003c00;       \\n\" // (compensate offset)\n+    \"shr.u32 nosign0, nosign0, 4;                \\n\" // nosign0 >>= 4\n+    \"shr.u32 nosign1, nosign1, 4;                \\n\" // shift into to fp8e4\n+    \"prmt.b32 nosign, nosign0, nosign1, 0x6420;  \\n\" // nosign0 = 0x00f100f2\n+                                                     // nosign1 = 0x00f300f4\n+                                                     // nosign = 0xf3f4f1f2\n+    \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n+    \"}\";\n+\n static SmallVector<Value> reorderValues(const SmallVector<Value> &values,\n                                         Type inType, Type ouType) {\n   auto inTensorTy = inType.dyn_cast<RankedTensorType>();\n@@ -142,435 +363,57 @@ struct FpToFpOpConversion\n   // FP8 -> FP16\n   /* ------------------ */\n \n-  static SmallVector<Value>\n-  convertFp8x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const char *ptxAsm, const Value &v0, const Value &v1,\n-                       const Value &v2, const Value &v3) {\n-    auto ctx = rewriter.getContext();\n-    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n-    Value fp8x4Vec = undef(fp8x4VecTy);\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v0, i32_val(0));\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v1, i32_val(1));\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v2, i32_val(2));\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v3, i32_val(3));\n-    fp8x4Vec = bitcast(fp8x4Vec, i32_ty);\n-\n-    PTXBuilder builder;\n-    auto &ptxOp = *builder.create(ptxAsm);\n-\n-    auto *o0 = builder.newOperand(\"=r\");\n-    auto *o1 = builder.newOperand(\"=r\");\n-    auto *i = builder.newOperand(fp8x4Vec, \"r\");\n-    ptxOp({o0, o1, i}, /*onlyAttachMLIRArgs=*/true);\n-\n-    auto fp16x2VecTy = vec_ty(f16_ty, 2);\n-    auto fp16x2x2StructTy =\n-        struct_ty(SmallVector<Type>{fp16x2VecTy, fp16x2VecTy});\n-    auto fp16x2x2Struct =\n-        builder.launch(rewriter, loc, fp16x2x2StructTy, false);\n-    auto fp16x2Vec0 = extract_val(fp16x2VecTy, fp16x2x2Struct, 0);\n-    auto fp16x2Vec1 = extract_val(fp16x2VecTy, fp16x2x2Struct, 1);\n-    return {extract_element(f16_ty, fp16x2Vec0, i32_val(0)),\n-            extract_element(f16_ty, fp16x2Vec0, i32_val(1)),\n-            extract_element(f16_ty, fp16x2Vec1, i32_val(0)),\n-            extract_element(f16_ty, fp16x2Vec1, i32_val(1))};\n-  }\n-\n-  static SmallVector<Value>\n-  convertFp8E4M3x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto *ptxAsm = // WARN: subnormal (0bs0000xxx) are not handled\n-        \"{                                      \\n\"\n-        \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n-        \"prmt.b32 a0, 0, $2, 0x5040;            \\n\" // a0 = 0xf300f400\n-        \"prmt.b32 a1, 0, $2, 0x7060;            \\n\" // a1 = 0xf100f200\n-        \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // b0 = a0 & 0x7fff7fff\n-        \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n-        \"shr.b32  b0, b0, 1;                    \\n\" // b0 >>= 1\n-        \"shr.b32  b1, b1, 1;                    \\n\" // shift into fp16 position\n-        \"add.u32  b0, b0, 0x20002000;           \\n\" // b0.exp += 2**4-2**3\n-                                                    // exponent compensate = 8\n-        \"add.u32  b1, b1, 0x20002000;           \\n\" // b1 += 8<<10 | 8<<10<<16\n-        \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n-        \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n-        \"}\";\n-    return convertFp8x4ToFp16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n-  }\n-\n-  static SmallVector<Value>\n-  convertFp8E5M2x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    // exponent bias of Fp8E5M2 and Fp16 are the same\n-    auto *ptxAsm = \"{                           \\n\"\n-                   \"prmt.b32 $0, 0, $2, 0x5140; \\n\\t\"\n-                   \"prmt.b32 $1, 0, $2, 0x7362; \\n\\t\"\n-                   \"}\";\n-    return convertFp8x4ToFp16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n-  }\n-\n-  /* ------------------ */\n-  // FP8 -> BF16\n-  /* ------------------ */\n-  static SmallVector<Value>\n-  convertFp8x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const char *ptxAsm, const Value &v0, const Value &v1,\n-                       const Value &v2, const Value &v3) {\n-    auto ctx = rewriter.getContext();\n-    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n-    Value fp8x4Vec = undef(fp8x4VecTy);\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v0, i32_val(0));\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v1, i32_val(1));\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v2, i32_val(2));\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v3, i32_val(3));\n-    fp8x4Vec = bitcast(fp8x4Vec, i32_ty);\n-\n-    PTXBuilder builder;\n-    auto &ptxOp = *builder.create(ptxAsm);\n-\n-    auto *o0 = builder.newOperand(\"=r\");\n-    auto *o1 = builder.newOperand(\"=r\");\n-    auto *i = builder.newOperand(fp8x4Vec, \"r\");\n-    ptxOp({o0, o1, i}, /* onlyAttachMLIRArgs */ true);\n-\n-    auto bf16x2VecTy = vec_ty(i16_ty, 2);\n-    auto bf16x2x2StructTy =\n-        struct_ty(SmallVector<Type>{bf16x2VecTy, bf16x2VecTy});\n-    auto bf16x2x2Struct =\n-        builder.launch(rewriter, loc, bf16x2x2StructTy, false);\n-    auto bf16x2Vec0 = extract_val(bf16x2VecTy, bf16x2x2Struct, 0);\n-    auto bf16x2Vec1 = extract_val(bf16x2VecTy, bf16x2x2Struct, 1);\n-    return {extract_element(i16_ty, bf16x2Vec0, i32_val(0)),\n-            extract_element(i16_ty, bf16x2Vec0, i32_val(1)),\n-            extract_element(i16_ty, bf16x2Vec1, i32_val(0)),\n-            extract_element(i16_ty, bf16x2Vec1, i32_val(1))};\n-  }\n-\n-  static SmallVector<Value>\n-  convertFp8E4M3x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto *ptxAsm = // WARN: subnormal (0bs0000xxx) are not handled\n-        \"{                                      \\n\"\n-        \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n-        \"prmt.b32 a0, 0, $2, 0x5040;            \\n\" // a0 = 0xf300f400\n-        \"prmt.b32 a1, 0, $2, 0x7060;            \\n\" // a1 = 0xf100f200\n-        \"and.b32 b0, a0, 0x7fff7fff;            \\n\" // b0 = a0 & 0x7fff7fff\n-        \"and.b32 b1, a1, 0x7fff7fff;            \\n\" // (strip sign)\n-        \"shr.b32 b0, b0, 4;                     \\n\" // b0 >>= 4\n-        \"shr.b32 b1, b1, 4;                     \\n\" // shift into fp16 position\n-        \"add.u32 b0, b0, 0x3c003c00;            \\n\" // b0.exp += 2**7-2**3\n-                                                    // exponent compensate = 120\n-        \"add.u32 b1, b1, 0x3c003c00;            \\n\" // b1 += 120<<7 | 120<<7<<16\n-        \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n-        \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n-        \"}\";\n-    return convertFp8x4ToBf16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n-  };\n-\n-  static SmallVector<Value>\n-  convertFp8E5M2x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto *ptxAsm = // WARN: subnormal (0bs00000xx) are not handled\n-        \"{                                      \\n\"\n-        \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n-        \"prmt.b32 a0, 0, $2, 0x5140;            \\n\" // a0 = 0xf300f400\n-        \"prmt.b32 a1, 0, $2, 0x7362;            \\n\" // a1 = 0xf100f200\n-        \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // b0 = a0 & 0x7fff7fff\n-        \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n-        \"shr.b32  b0, b0, 3;                    \\n\" // b0 >>= 3\n-        \"shr.b32  b1, b1, 3;                    \\n\" // shift into bf16 position\n-        \"add.u32  b0, b0, 0x38003800;           \\n\" // b0.exp += 2**7-2**4\n-                                                    // exponent compensate = 112\n-        \"add.u32  b1, b1, 0x38003800;           \\n\" // b1 += 112<<7 | 112<<7<<16\n-        \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n-        \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n-        \"}\";\n-    return convertFp8x4ToBf16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n-  };\n-\n-  /* ------------------ */\n-  // FP16 -> FP8\n-  /* ------------------ */\n-\n-  static SmallVector<Value>\n-  convertFp16x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const char *ptxAsm, const Value &v0, const Value &v1,\n-                       const Value &v2, const Value &v3) {\n-    auto fp16x2VecTy = vec_ty(f16_ty, 2);\n-    Value fp16x2Vec0 = undef(fp16x2VecTy);\n-    Value fp16x2Vec1 = undef(fp16x2VecTy);\n-    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v0, i32_val(0));\n-    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v1, i32_val(1));\n-    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v2, i32_val(0));\n-    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v3, i32_val(1));\n-    fp16x2Vec0 = bitcast(fp16x2Vec0, i32_ty);\n-    fp16x2Vec1 = bitcast(fp16x2Vec1, i32_ty);\n-\n-    PTXBuilder builder;\n-    auto &ptxOp = *builder.create(ptxAsm);\n-\n-    auto *o = builder.newOperand(\"=r\");\n-    auto *i0 = builder.newOperand(fp16x2Vec0, \"r\");\n-    auto *i1 = builder.newOperand(fp16x2Vec1, \"r\");\n-    ptxOp({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n-\n-    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n-    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n-    return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n-  }\n-\n-  static SmallVector<Value>\n-  convertFp16x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto *ptxAsm = // WARN: subnormal Fp8s are not handled\n-        \"{                                      \\n\"\n-        \".reg .b32 a<2>, b<2>;                  \\n\" // see Fp8E4M3x4ToFp16x4\n-        \"sub.u32 a0, $1, 0x20002000;            \\n\" // a0 = input0 - 0x20002000\n-                                                    // (compensate offset)\n-        \"sub.u32 a1, $2, 0x20002000;            \\n\" // a1 = input1 - 0x20002000\n-                                                    // (8 << 10 | 8 << 10 << 16)\n-        \"shl.b32 a0, a0, 1;                     \\n\" // a0 <<= 1\n-        \"shl.b32 a1, a1, 1;                     \\n\" // shift into fp8e4 position\n-        \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // a0 &= 0x7fff7fff\n-        \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n-        \"add.u32 a0, a0, 0x00800080;            \\n\" // a0 += 0x00800080\n-        \"add.u32 a1, a1, 0x00800080;            \\n\" // (round to nearest)\n-        \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\" // b0 = a0|(0x80008000&in0)\n-        \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\" // (restore sign)\n-        \"prmt.b32 $0, b0, b1, 0x7531;           \\n\" // output = b1b0\n-        \"}\";\n-    return convertFp16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n-  }\n-\n-  static SmallVector<Value>\n-  convertFp16x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto *ptxAsm =\n-        \"{                            \\n\"\n-        \".reg .b32 a<2>;              \\n\"\n-        \"and.b32 a0, $1, 0x7fff7fff;  \\n\"           // a0 &= 0x7fff7fff\n-        \"and.b32 a1, $2, 0x7fff7fff;  \\n\"           // (strip sign)\n-        \"add.u32 a0, a0, 0x00800080;  \\n\"           // a0 += 0x00800080\n-        \"add.u32 a1, a1, 0x00800080;  \\n\"           // (round to nearest)\n-        \"lop3.b32 a0, $1, 0x80008000, a0, 0xea; \\n\" // a0 = a0|(0x80008000&in0)\n-        \"lop3.b32 a1, $2, 0x80008000, a1, 0xea; \\n\" // (restore sign)\n-        \"prmt.b32 $0, a0, a1, 0x7531; \\n\\t\"         // output = a1a0\n-        \"}\";\n-    return convertFp16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n-  }\n-\n-  /* ------------------ */\n-  // FP32 -> FP8\n-  /* ------------------ */\n-\n-  static SmallVector<Value>\n-  convertFp32x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n-    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n-    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n-    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n-    return convertFp16x4ToFp8E4M3x4(loc, rewriter, c0, c1, c2, c3);\n-  }\n-\n-  static SmallVector<Value>\n-  convertFp32x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n-    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n-    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n-    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n-    return convertFp16x4ToFp8E5M2x4(loc, rewriter, c0, c1, c2, c3);\n-  }\n-\n-  /* ------------------ */\n-  // BF16 -> FP8\n-  /* ------------------ */\n-\n-  static SmallVector<Value>\n-  convertBf16x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const char *ptxAsm, const Value &v0, const Value &v1,\n-                       const Value &v2, const Value &v3) {\n-    auto bf16x2VecTy = vec_ty(i16_ty, 2);\n-    Value bf16x2Vec0 = undef(bf16x2VecTy);\n-    Value bf16x2Vec1 = undef(bf16x2VecTy);\n-    bf16x2Vec0 = insert_element(bf16x2VecTy, bf16x2Vec0, v0, i32_val(0));\n-    bf16x2Vec0 = insert_element(bf16x2VecTy, bf16x2Vec0, v1, i32_val(1));\n-    bf16x2Vec1 = insert_element(bf16x2VecTy, bf16x2Vec1, v2, i32_val(0));\n-    bf16x2Vec1 = insert_element(bf16x2VecTy, bf16x2Vec1, v3, i32_val(1));\n-    bf16x2Vec0 = bitcast(bf16x2Vec0, i32_ty);\n-    bf16x2Vec1 = bitcast(bf16x2Vec1, i32_ty);\n-\n-    PTXBuilder builder;\n-    auto &ptxOp = *builder.create(ptxAsm);\n-\n-    auto *o = builder.newOperand(\"=r\");\n-    auto *i0 = builder.newOperand(bf16x2Vec0, \"r\");\n-    auto *i1 = builder.newOperand(bf16x2Vec1, \"r\");\n-    ptxOp({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n-\n-    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n-    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n-    return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n-  }\n-\n-  static SmallVector<Value>\n-  convertBf16x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto *ptxAsm = // bf16 is clamped firstly to fp8 min/max\n-        \"{                                           \\n\" // bf16=fp8>>4 + 120<<7\n-        \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\" // fp8_min = 0b00000000\n-        \".reg .u32 fp8_min, fp8_max, rn_;            \\n\" // fp8_max = 0b11111111\n-        \"mov.u32 fp8_min, 0x3c003c00;                \\n\" // so bf16_min = 0x3c00\n-        \"mov.u32 fp8_max, 0x43f043f0;                \\n\" // so bf16_max = 0x43f0\n-        \"mov.u32 rn_, 0x80008;                       \\n\" // round to nearest\n-        \"and.b32 sign0, $1, 0x80008000;              \\n\" // sign0=in0&0x80008000\n-        \"and.b32 sign1, $2, 0x80008000;              \\n\" // (store sign)\n-        \"prmt.b32 sign, sign0, sign1, 0x7531;        \\n\"\n-        \"and.b32 nosign0, $1, 0x7fff7fff;            \\n\" // nosign0=in0&0x7fff7fff\n-        \"and.b32 nosign1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n-\n-        // nosign = clamp(nosign, min, max)\n-        \".reg .u32 nosign_0_<2>, nosign_1_<2>;       \\n\"\n-        \"and.b32 nosign_0_0, nosign0, 0xffff0000;    \\n\"\n-        \"max.u32 nosign_0_0, nosign_0_0, 0x3c000000; \\n\"\n-        \"min.u32 nosign_0_0, nosign_0_0, 0x43f00000; \\n\"\n-        \"and.b32 nosign_0_1, nosign0, 0x0000ffff;    \\n\"\n-        \"max.u32 nosign_0_1, nosign_0_1, 0x3c00;     \\n\"\n-        \"min.u32 nosign_0_1, nosign_0_1, 0x43f0;     \\n\"\n-        \"or.b32 nosign0, nosign_0_0, nosign_0_1;     \\n\"\n-        \"and.b32 nosign_1_0, nosign1, 0xffff0000;    \\n\"\n-        \"max.u32 nosign_1_0, nosign_1_0, 0x3c000000; \\n\"\n-        \"min.u32 nosign_1_0, nosign_1_0, 0x43f00000; \\n\"\n-        \"and.b32 nosign_1_1, nosign1, 0x0000ffff;    \\n\"\n-        \"max.u32 nosign_1_1, nosign_1_1, 0x3c00;     \\n\"\n-        \"min.u32 nosign_1_1, nosign_1_1, 0x43f0;     \\n\"\n-        \"or.b32 nosign1, nosign_1_0, nosign_1_1;     \\n\"\n-\n-        \"add.u32 nosign0, nosign0, rn_;              \\n\" // nosign0 += rn_\n-        \"add.u32 nosign1, nosign1, rn_;              \\n\" // (round to nearest)\n-        \"sub.u32 nosign0, nosign0, 0x3c003c00;       \\n\" // nosign0-=0x3c003c00\n-        \"sub.u32 nosign1, nosign1, 0x3c003c00;       \\n\" // (compensate offset)\n-        \"shr.u32 nosign0, nosign0, 4;                \\n\" // nosign0 >>= 4\n-        \"shr.u32 nosign1, nosign1, 4;                \\n\" // shift into to fp8e4\n-        \"prmt.b32 nosign, nosign0, nosign1, 0x6420;  \\n\" // nosign0 = 0x00f100f2\n-                                                         // nosign1 = 0x00f300f4\n-                                                         // nosign = 0xf3f4f1f2\n-        \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n-        \"}\";\n-    return convertBf16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n-  };\n-\n-  static SmallVector<Value>\n-  convertBf16x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto *ptxAsm = // bf16 is clamped firstly to fp8 min/max\n-        \"{                                           \\n\" // bf16=fp8>>3 + 112<<7\n-        \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\" // fp8_min = 0b00000000\n-        \".reg .u32 fp8_min, fp8_max, rn_;            \\n\" // fp8_max = 0b11111111\n-        \"mov.u32 fp8_min, 0x38003800;                \\n\" // so bf16_min = 0x3800\n-        \"mov.u32 fp8_max, 0x57e057e0;                \\n\" // so bf16_max = 0x57e0\n-        \"mov.u32 rn_, 0x00100010;                    \\n\" // round to nearest\n-        \"and.b32 sign0, $1, 0x80008000;              \\n\" // sign0=in0&0x80008000\n-        \"and.b32 sign1, $2, 0x80008000;              \\n\" // (store sign)\n-        \"prmt.b32 sign, sign0, sign1, 0x7531;        \\n\"\n-        \"and.b32 nosign0, $1, 0x7fff7fff;            \\n\" // nosign0=in0&0x7fff7fff\n-        \"and.b32 nosign1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n-\n-        // nosign = clamp(nosign, min, max)\n-        \".reg .u32 nosign_0_<2>, nosign_1_<2>;       \\n\"\n-        \"and.b32 nosign_0_0, nosign0, 0xffff0000;    \\n\"\n-        \"max.u32 nosign_0_0, nosign_0_0, 0x38000000; \\n\"\n-        \"min.u32 nosign_0_0, nosign_0_0, 0x57e00000; \\n\"\n-        \"and.b32 nosign_0_1, nosign0, 0x0000ffff;    \\n\"\n-        \"max.u32 nosign_0_1, nosign_0_1, 0x3800;     \\n\"\n-        \"min.u32 nosign_0_1, nosign_0_1, 0x57e0;     \\n\"\n-        \"or.b32 nosign0, nosign_0_0, nosign_0_1;     \\n\"\n-        \"and.b32 nosign_1_0, nosign1, 0xffff0000;    \\n\"\n-        \"max.u32 nosign_1_0, nosign_1_0, 0x38000000; \\n\"\n-        \"min.u32 nosign_1_0, nosign_1_0, 0x57e00000; \\n\"\n-        \"and.b32 nosign_1_1, nosign1, 0x0000ffff;    \\n\"\n-        \"max.u32 nosign_1_1, nosign_1_1, 0x3800;     \\n\"\n-        \"min.u32 nosign_1_1, nosign_1_1, 0x57e0;     \\n\"\n-        \"or.b32 nosign1, nosign_1_0, nosign_1_1;     \\n\"\n-\n-        \"add.u32 nosign0, nosign0, rn_;              \\n\" // nosign0 += rn_\n-        \"add.u32 nosign1, nosign1, rn_;              \\n\" // (round to nearest)\n-        \"sub.u32 nosign0, nosign0, 0x38003800;       \\n\" // nosign0-=0x38003800\n-        \"sub.u32 nosign1, nosign1, 0x38003800;       \\n\" // (compensate offset)\n-        \"shl.b32 nosign0, nosign0, 3;                \\n\" // nosign0 <<= 3\n-        \"shl.b32 nosign1, nosign1, 3;                \\n\" // shift into to fp8e4\n-        \"prmt.b32 nosign, nosign0, nosign1, 0x7531;  \\n\" // nosign0 = 0xf100f200\n-                                                         // nosign1 = 0xf300f400\n-                                                         // nosign = 0xf3f4f1f2\n-        \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n-        \"}\";\n-    return convertBf16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n-  }\n-\n-  /* ------------------ */\n-  // FP8 -> FP32\n-  /* ------------------ */\n-\n-  static SmallVector<Value>\n-  convertFp8E4M3x4ToFp32x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto fp16Values = convertFp8E4M3x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n-    return {convertFp16ToFp32(loc, rewriter, fp16Values[0]),\n-            convertFp16ToFp32(loc, rewriter, fp16Values[1]),\n-            convertFp16ToFp32(loc, rewriter, fp16Values[2]),\n-            convertFp16ToFp32(loc, rewriter, fp16Values[3])};\n-  }\n-\n-  static SmallVector<Value>\n-  convertFp8E5M2x4ToFp32x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto fp16Values = convertFp8E5M2x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n-    return {rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[0]),\n-            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[1]),\n-            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[2]),\n-            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[3])};\n-  }\n-\n-  //\n-\n-  static SmallVector<Value>\n-  convertFp8E4M3x4ToFp64x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto fp16Values = convertFp8E4M3x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n-    return {rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[0]),\n-            rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[1]),\n-            rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[2]),\n-            rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[3])};\n-  }\n-\n-  static SmallVector<Value>\n-  convertFp64x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n-    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n-    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n-    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n-    return convertFp16x4ToFp8E4M3x4(loc, rewriter, c0, c1, c2, c3);\n+  static ConvertorT makeConverterFromPtx(const std::string &ptxAsm, Type inType,\n+                                         Type outType) {\n+\n+    ConvertorT converter =\n+        [ptxAsm, inType,\n+         outType](Location loc, ConversionPatternRewriter &rewriter,\n+                  const Value &v0, const Value &v1, const Value &v2,\n+                  const Value &v3) -> SmallVector<Value> {\n+      SmallVector<Value> v = {v0, v1, v2, v3};\n+      auto ctx = rewriter.getContext();\n+      int inBitwidth = inType.getIntOrFloatBitWidth();\n+      int outBitwidth = outType.getIntOrFloatBitWidth();\n+      // first, we pack `v` into 32-bit ints\n+      int inVecWidth = 32 / inBitwidth;\n+      auto inVecTy = vec_ty(inType, inVecWidth);\n+      SmallVector<Value> inPacked(4 / inVecWidth, undef(inVecTy));\n+      for (size_t i = 0; i < 4; i++)\n+        inPacked[i / inVecWidth] = insert_element(\n+            inVecTy, inPacked[i / inVecWidth], v[i], i32_val(i % inVecWidth));\n+      for (size_t i = 0; i < inPacked.size(); i++)\n+        inPacked[i] = bitcast(inPacked[i], i32_ty);\n+\n+      // then, we run the provided inline PTX\n+      int outVecWidth = 32 / outBitwidth;\n+      int outNums = 4 / outVecWidth;\n+      PTXBuilder builder;\n+      SmallVector<PTXBuilder::Operand *> operands;\n+      for (int i = 0; i < outNums; i++)\n+        operands.push_back(builder.newOperand(\"=r\"));\n+      for (Value inVal : inPacked)\n+        operands.push_back(builder.newOperand(inVal, \"r\"));\n+      auto &ptxOp = *builder.create(ptxAsm);\n+      ptxOp(operands, /*onlyAttachMLIRArgs=*/true);\n+      auto outVecTy = vec_ty(outType, outVecWidth);\n+      SmallVector<Value> outPacked;\n+      if (outNums == 1)\n+        outPacked.push_back(builder.launch(rewriter, loc, outVecTy, false));\n+      else {\n+        auto outStructTy = struct_ty(SmallVector<Type>(outNums, outVecTy));\n+        auto outStruct = builder.launch(rewriter, loc, outStructTy, false);\n+        for (int i = 0; i < outNums; i++)\n+          outPacked.push_back(extract_val(outVecTy, outStruct, i));\n+      }\n+      // unpack the output\n+      SmallVector<Value> ret;\n+      for (size_t i = 0; i < 4; i++)\n+        ret.push_back(extract_element(outType, outPacked[i / outVecWidth],\n+                                      i32_val(i % outVecWidth)));\n+      return ret;\n+    };\n+    return converter;\n   }\n \n   static Value convertBf16ToFp32(Location loc,\n@@ -620,54 +463,51 @@ struct FpToFpOpConversion\n   }\n \n   ConvertorT getConversionFunc(Type srcTy, Type dstTy) const {\n-    auto F8E4M3TyID = TypeID::get<mlir::Float8E4M3FNType>();\n+    auto F8E4M3B15TyID = TypeID::get<mlir::Float8E4M3B11FNUZType>();\n+    auto F8E4M3TyID = TypeID::get<mlir::Float8E4M3FNUZType>();\n     auto F8E5M2TyID = TypeID::get<mlir::Float8E5M2Type>();\n     auto F16TyID = TypeID::get<mlir::Float16Type>();\n     auto BF16TyID = TypeID::get<mlir::BFloat16Type>();\n     auto F32TyID = TypeID::get<mlir::Float32Type>();\n     auto F64TyID = TypeID::get<mlir::Float64Type>();\n-    static DenseMap<std::pair<TypeID, TypeID>, ConvertorT> convertorMap = {\n+    static DenseMap<std::pair<TypeID, TypeID>, std::string> srcMap = {\n         // F8 -> F16\n-        {{F8E4M3TyID, F16TyID}, convertFp8E4M3x4ToFp16x4},\n-        {{F8E5M2TyID, F16TyID}, convertFp8E5M2x4ToFp16x4},\n+        {{F8E4M3B15TyID, F16TyID}, Fp8E4M3B15_to_Fp16},\n+        {{F8E4M3TyID, F16TyID}, Fp8E4M3_to_Fp16},\n+        {{F8E5M2TyID, F16TyID}, Fp8E5M2_to_Fp16},\n         // F16 -> F8\n-        {{F16TyID, F8E4M3TyID}, convertFp16x4ToFp8E4M3x4},\n-        {{F16TyID, F8E5M2TyID}, convertFp16x4ToFp8E5M2x4},\n+        {{F16TyID, F8E4M3B15TyID}, Fp16_to_Fp8E4M3B15},\n+        {{F16TyID, F8E4M3TyID}, Fp16_to_Fp8E4M3},\n+        {{F16TyID, F8E5M2TyID}, Fp16_to_Fp8E5M2},\n         // F8 -> BF16\n-        {{F8E4M3TyID, BF16TyID}, convertFp8E4M3x4ToBf16x4},\n-        {{F8E5M2TyID, BF16TyID}, convertFp8E5M2x4ToBf16x4},\n+        {{F8E4M3TyID, BF16TyID}, Fp8E4M3_to_Bf16},\n+        {{F8E5M2TyID, BF16TyID}, Fp8E5M2_to_Bf16},\n         // BF16 -> F8\n-        {{BF16TyID, F8E4M3TyID}, convertBf16x4ToFp8E4M3x4},\n-        {{BF16TyID, F8E5M2TyID}, convertBf16x4ToFp8E5M2x4},\n-        // F8 -> F32\n-        {{F8E4M3TyID, F32TyID}, convertFp8E4M3x4ToFp32x4},\n-        {{F8E5M2TyID, F32TyID}, convertFp8E5M2x4ToFp32x4},\n-        // F32 -> F8\n-        {{F32TyID, F8E4M3TyID}, convertFp32x4ToFp8E4M3x4},\n-        {{F32TyID, F8E5M2TyID}, convertFp32x4ToFp8E5M2x4},\n+        {{BF16TyID, F8E4M3TyID}, Bf16_to_Fp8E4M3},\n+        {{BF16TyID, F8E5M2TyID}, Bf16_to_Fp8E5M2},\n     };\n \n     std::pair<TypeID, TypeID> key = {srcTy.getTypeID(), dstTy.getTypeID()};\n-    if (convertorMap.count(key) == 0) {\n+    if (srcMap.count(key) == 0) {\n       llvm::errs() << \"Unsupported conversion from \" << srcTy << \" to \" << dstTy\n                    << \"\\n\";\n       llvm_unreachable(\"\");\n     }\n-    return convertorMap.lookup(key);\n+    return makeConverterFromPtx(srcMap.lookup(key),\n+                                getTypeConverter()->convertType(srcTy),\n+                                getTypeConverter()->convertType(dstTy));\n   }\n \n   LogicalResult\n   matchAndRewrite(triton::FpToFpOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    // llvm::outs() << 0 << \"\\n\";\n     auto srcTensorType = op.getFrom().getType().cast<mlir::RankedTensorType>();\n     auto dstTensorType =\n         op.getResult().getType().cast<mlir::RankedTensorType>();\n+    auto srcElementType = srcTensorType.getElementType();\n+    auto dstElementType = dstTensorType.getElementType();\n     auto loc = op->getLoc();\n     // check that the number of elements is divisible by 4\n-    // Get convertor\n-    auto cvtFunc = getConversionFunc(srcTensorType.getElementType(),\n-                                     dstTensorType.getElementType());\n     // Unpack value\n     auto inVals = getTypeConverter()->unpackLLElements(loc, adaptor.getFrom(),\n                                                        rewriter, srcTensorType);\n@@ -678,9 +518,19 @@ struct FpToFpOpConversion\n     auto elems = inVals.size();\n     assert(elems % 4 == 0 &&\n            \"FP8 casting only support tensors with 4-aligned sizes\");\n+    bool isFP32src = srcElementType.isF32();\n+    bool isFP32dst = dstElementType.isF32();\n+    auto cvtFunc = getConversionFunc(isFP32src ? f16_ty : srcElementType,\n+                                     isFP32dst ? f16_ty : dstElementType);\n+    if (isFP32src)\n+      for (Value &v : inVals)\n+        v = convertFp32ToFp16(loc, rewriter, v);\n     for (size_t i = 0; i < elems; i += 4)\n       outVals.append(cvtFunc(loc, rewriter, inVals[i], inVals[i + 1],\n                              inVals[i + 2], inVals[i + 3]));\n+    if (isFP32dst)\n+      for (Value &v : outVals)\n+        v = convertFp16ToFp32(loc, rewriter, v);\n     // Pack values\n     assert(outVals.size() == elems);\n     outVals = reorderValues(outVals, srcTensorType, dstTensorType);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM/Fp8E4M3B15.cpp", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM/Fp8E5M2.cpp", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 11, "deletions": 3, "changes": 14, "file_content_changes": "@@ -1,8 +1,11 @@\n #include \"ReduceOpToLLVM.h\"\n+#include \"Utility.h\"\n \n using namespace mlir;\n using namespace mlir::triton;\n \n+using ::mlir::LLVM::delinearize;\n+using ::mlir::LLVM::linearize;\n using ::mlir::LLVM::shflSync;\n using ::mlir::LLVM::storeShared;\n using ::mlir::triton::gpu::getOrder;\n@@ -367,8 +370,10 @@ struct ReduceOpConversion\n     Value warpId = udiv(threadId, warpSize);\n     Value laneId = urem(threadId, warpSize);\n \n-    auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcLayout);\n-    auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcLayout);\n+    auto threadsPerWarp =\n+        triton::gpu::getThreadsPerWarpWithUniqueData(srcLayout, srcShape);\n+    auto warpsPerCTA =\n+        triton::gpu::getWarpsPerCTAWithUniqueData(srcLayout, srcShape);\n     auto order = getOrder(srcLayout);\n     SmallVector<Value> multiDimLaneId =\n         delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n@@ -412,8 +417,11 @@ struct ReduceOpConversion\n     //\n     // Each thread needs to process:\n     //   elemsPerThread = sizeInterWarps * s1 * s2 .. Sn / numThreads\n+\n+    auto mod = op.getOperation()->getParentOfType<ModuleOp>();\n     unsigned numThreads =\n-        product<unsigned>(triton::gpu::getWarpsPerCTA(srcLayout)) * 32;\n+        product<unsigned>(triton::gpu::getWarpsPerCTA(srcLayout)) *\n+        triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n     unsigned elemsPerThread = std::max<unsigned>(elems / numThreads, 1);\n     Value readOffset = threadId;\n     for (unsigned round = 0; round < elemsPerThread; ++round) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 14, "deletions": 68, "changes": 82, "file_content_changes": "@@ -15,6 +15,7 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n+using ::mlir::LLVM::delinearize;\n using ::mlir::LLVM::SharedMemoryObject;\n using ::mlir::triton::gpu::BlockedEncodingAttr;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n@@ -504,65 +505,6 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return mask;\n   }\n \n-  // Convert an \\param index to a multi-dim coordinate given \\param shape and\n-  // \\param order.\n-  SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n-                                 Location loc, Value linear,\n-                                 ArrayRef<unsigned> shape,\n-                                 ArrayRef<unsigned> order) const {\n-    unsigned rank = shape.size();\n-    assert(rank == order.size());\n-    auto reordered = reorder(shape, order);\n-    auto reorderedMultiDim = delinearize(rewriter, loc, linear, reordered);\n-    SmallVector<Value> multiDim(rank);\n-    for (unsigned i = 0; i < rank; ++i) {\n-      multiDim[order[i]] = reorderedMultiDim[i];\n-    }\n-    return multiDim;\n-  }\n-\n-  SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n-                                 Location loc, Value linear,\n-                                 ArrayRef<unsigned> shape) const {\n-    unsigned rank = shape.size();\n-    assert(rank > 0);\n-    SmallVector<Value> multiDim(rank);\n-    if (rank == 1) {\n-      multiDim[0] = linear;\n-    } else {\n-      Value remained = linear;\n-      for (auto &&en : llvm::enumerate(shape.drop_back())) {\n-        Value dimSize = i32_val(en.value());\n-        multiDim[en.index()] = urem(remained, dimSize);\n-        remained = udiv(remained, dimSize);\n-      }\n-      multiDim[rank - 1] = remained;\n-    }\n-    return multiDim;\n-  }\n-\n-  Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n-                  ArrayRef<Value> multiDim, ArrayRef<unsigned> shape,\n-                  ArrayRef<unsigned> order) const {\n-    return linearize(rewriter, loc, reorder<Value>(multiDim, order),\n-                     reorder<unsigned>(shape, order));\n-  }\n-\n-  Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n-                  ArrayRef<Value> multiDim, ArrayRef<unsigned> shape) const {\n-    auto rank = multiDim.size();\n-    Value linear = i32_val(0);\n-    if (rank > 0) {\n-      linear = multiDim.back();\n-      for (auto [dim, dimShape] :\n-           llvm::reverse(llvm::zip(multiDim.drop_back(), shape.drop_back()))) {\n-        Value dimSize = i32_val(dimShape);\n-        linear = add(mul(linear, dimSize), dim);\n-      }\n-    }\n-    return linear;\n-  }\n-\n   Value dot(ConversionPatternRewriter &rewriter, Location loc,\n             ArrayRef<Value> offsets, ArrayRef<Value> strides) const {\n     assert(offsets.size() == strides.size());\n@@ -927,19 +869,23 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n                               const MmaEncodingAttr &mmaLayout,\n                               RankedTensorType type) const {\n     auto shape = type.getShape();\n-    auto _warpsPerCTA = mmaLayout.getWarpsPerCTA();\n-    assert(_warpsPerCTA.size() == 2);\n-    SmallVector<Value> warpsPerCTA = {i32_val(_warpsPerCTA[0]),\n-                                      i32_val(_warpsPerCTA[1])};\n+    auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n+    assert(warpsPerCTA.size() == 2);\n+    auto order = triton::gpu::getOrder(mmaLayout);\n     Value threadId = getThreadId(rewriter, loc);\n     Value warpSize = i32_val(32);\n     Value laneId = urem(threadId, warpSize);\n     Value warpId = udiv(threadId, warpSize);\n-    Value warpId0 = urem(urem(warpId, warpsPerCTA[0]), i32_val(shape[0] / 16));\n-    Value warpId1 = urem(urem(udiv(warpId, warpsPerCTA[0]), warpsPerCTA[1]),\n-                         i32_val(shape[1] / 8));\n-    Value offWarp0 = mul(warpId0, i32_val(16));\n-    Value offWarp1 = mul(warpId1, i32_val(8));\n+\n+    SmallVector<Value> multiDimWarpId =\n+        delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n+    unsigned lastAxis = order[order.size() - 1];\n+    multiDimWarpId[lastAxis] =\n+        urem(multiDimWarpId[lastAxis], i32_val(warpsPerCTA[lastAxis]));\n+    multiDimWarpId[0] = urem(multiDimWarpId[0], i32_val(shape[0] / 16));\n+    multiDimWarpId[1] = urem(multiDimWarpId[1], i32_val(shape[1] / 8));\n+    Value offWarp0 = mul(multiDimWarpId[0], i32_val(16));\n+    Value offWarp1 = mul(multiDimWarpId[1], i32_val(8));\n \n     SmallVector<Value> multiDimBase(2);\n     multiDimBase[0] = add(udiv(laneId, i32_val(4)), offWarp0);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.cpp", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -24,7 +24,10 @@ TritonGPUToLLVMTypeConverter::TritonGPUToLLVMTypeConverter(\n     return convertTritonTensorType(type);\n   });\n   // Internally store float8 as int8\n-  addConversion([&](mlir::Float8E4M3FNType type) -> std::optional<Type> {\n+  addConversion([&](mlir::Float8E4M3B11FNUZType type) -> std::optional<Type> {\n+    return IntegerType::get(type.getContext(), 8);\n+  });\n+  addConversion([&](mlir::Float8E4M3FNUZType type) -> std::optional<Type> {\n     return IntegerType::get(type.getContext(), 8);\n   });\n   addConversion([&](mlir::Float8E5M2Type type) -> std::optional<Type> {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.cpp", "status": "modified", "additions": 76, "deletions": 0, "changes": 76, "file_content_changes": "@@ -70,6 +70,82 @@ getStridesFromShapeAndOrder(ArrayRef<int64_t> shape, ArrayRef<unsigned> order,\n   return strides;\n }\n \n+// Convert an \\param index to a multi-dim coordinate given \\param shape and\n+// \\param order.\n+SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n+                               Location loc, Value linear,\n+                               ArrayRef<unsigned> shape,\n+                               ArrayRef<unsigned> order) {\n+  unsigned rank = shape.size();\n+  assert(rank == order.size());\n+  auto reordered = reorder(shape, order);\n+  SmallVector<Value> reorderedMultiDim(rank);\n+  if (auto constantOp = linear.getDefiningOp<arith::ConstantOp>()) {\n+    unsigned intVal =\n+        constantOp.getValue().cast<IntegerAttr>().getValue().getSExtValue();\n+    reorderedMultiDim = delinearize(rewriter, loc, intVal, reordered);\n+  } else {\n+    reorderedMultiDim = delinearize(rewriter, loc, linear, reordered);\n+  }\n+  SmallVector<Value> multiDim(rank);\n+  for (unsigned i = 0; i < rank; ++i) {\n+    multiDim[order[i]] = reorderedMultiDim[i];\n+  }\n+  return multiDim;\n+}\n+\n+SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n+                               Location loc, unsigned linear,\n+                               ArrayRef<unsigned> shape) {\n+  unsigned rank = shape.size();\n+  assert(rank > 0);\n+  SmallVector<Value> multiDim(rank);\n+  unsigned remained = linear;\n+  for (auto &&en : llvm::enumerate(shape)) {\n+    unsigned dimSize = en.value();\n+    multiDim[en.index()] = i32_val(remained % dimSize);\n+    remained = remained / dimSize;\n+  }\n+  return multiDim;\n+}\n+\n+SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n+                               Location loc, Value linear,\n+                               ArrayRef<unsigned> shape) {\n+  unsigned rank = shape.size();\n+  assert(rank > 0);\n+  SmallVector<Value> multiDim(rank);\n+  Value remained = linear;\n+  for (auto &&en : llvm::enumerate(shape)) {\n+    Value dimSize = i32_val(en.value());\n+    multiDim[en.index()] = urem(remained, dimSize);\n+    remained = udiv(remained, dimSize);\n+  }\n+  return multiDim;\n+}\n+\n+Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n+                ArrayRef<Value> multiDim, ArrayRef<unsigned> shape,\n+                ArrayRef<unsigned> order) {\n+  return linearize(rewriter, loc, reorder<Value>(multiDim, order),\n+                   reorder<unsigned>(shape, order));\n+}\n+\n+Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n+                ArrayRef<Value> multiDim, ArrayRef<unsigned> shape) {\n+  auto rank = multiDim.size();\n+  Value linear = i32_val(0);\n+  if (rank > 0) {\n+    linear = multiDim.back();\n+    for (auto [dim, dimShape] :\n+         llvm::reverse(llvm::zip(multiDim.drop_back(), shape.drop_back()))) {\n+      Value dimSize = i32_val(dimShape);\n+      linear = add(mul(linear, dimSize), dim);\n+    }\n+  }\n+  return linear;\n+}\n+\n Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n                   Value val, Value pred) {\n   MLIRContext *ctx = rewriter.getContext();"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 22, "deletions": 0, "changes": 22, "file_content_changes": "@@ -258,6 +258,28 @@ SharedMemoryObject\n getSharedMemoryObjectFromStruct(Location loc, Value llvmStruct,\n                                 ConversionPatternRewriter &rewriter);\n \n+// Convert an \\param index to a multi-dim coordinate given \\param shape and\n+// \\param order.\n+SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n+                               Location loc, Value linear,\n+                               ArrayRef<unsigned> shape,\n+                               ArrayRef<unsigned> order);\n+\n+SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n+                               Location loc, unsigned linear,\n+                               ArrayRef<unsigned> shape);\n+\n+SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n+                               Location loc, Value linear,\n+                               ArrayRef<unsigned> shape);\n+\n+Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n+                ArrayRef<Value> multiDim, ArrayRef<unsigned> shape,\n+                ArrayRef<unsigned> order);\n+\n+Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n+                ArrayRef<Value> multiDim, ArrayRef<unsigned> shape);\n+\n Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n                   Value val, Value pred);\n "}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -85,6 +85,8 @@ SmallVector<unsigned> getThreadsPerWarp(Attribute layout) {\n   if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     auto parent = sliceLayout.getParent();\n     auto parentThreadsPerWarp = getThreadsPerWarp(parent);\n+    assert(parentThreadsPerWarp.size() == 2 &&\n+           \"getThreadsPerWarp only implemented for 2D slice layout\");\n     SmallVector<unsigned> threadsPerWarp = parentThreadsPerWarp;\n     threadsPerWarp.erase(threadsPerWarp.begin() + sliceLayout.getDim());\n     for (unsigned i = 0; i < threadsPerWarp.size(); i++)\n@@ -129,6 +131,8 @@ SmallVector<unsigned> getWarpsPerCTA(Attribute layout) {\n   if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     auto parent = sliceLayout.getParent();\n     auto parentWarpsPerCTA = getWarpsPerCTA(parent);\n+    assert(parentWarpsPerCTA.size() == 2 &&\n+           \"getWarpsPerCTA only implemented for 2D slice layout\");\n     SmallVector<unsigned> warpsPerCTA = parentWarpsPerCTA;\n     warpsPerCTA.erase(warpsPerCTA.begin() + sliceLayout.getDim());\n     for (unsigned i = 0; i < warpsPerCTA.size(); i++)"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 28, "deletions": 7, "changes": 35, "file_content_changes": "@@ -816,28 +816,49 @@ void LoopPipeliner::prefetchNextIteration(scf::ForOp newForOp,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n \n   // Prefetch load deps\n+  // If a load-dependent instruction that uses a block argument, we\n+  // shouldn't update the new mapping of the block argument in the current\n+  // iteration.\n+  // For example.\n+  // %a = add %arg0, %c\n+  // %b = add %arg0, %d\n+  //\n+  // Update %arg0 will cause the value of %b to be incorrect.\n+  // We do need to use the next iteration value of %arg0 because it could be a\n+  // immediate arg of a load op.\n+  // load %arg0\n+  // %a = add %arg0, %c\n+  // yield %a\n+  //\n+  // We reroder instructions so %a and yield are actually before load. load\n+  // %arg0 should use the updated %arg0.\n+  IRMapping curMapping = nextMapping;\n   for (Operation *op : orderedDeps)\n     if (!validLoads.contains(op->getResult(0))) {\n       if (immediateOpStages[op].contains(numStages - 2))\n         // A post load op that provides values for numStage - 2\n-        nextMapping.map(forOp.getInductionVar(), curIV);\n+        curMapping.map(forOp.getInductionVar(), curIV);\n       else\n-        nextMapping.map(forOp.getInductionVar(), nextIV);\n+        curMapping.map(forOp.getInductionVar(), nextIV);\n       Operation *nextOp;\n       if (auto loadOp = dyn_cast<triton::LoadOp>(op)) {\n         auto newMask =\n-            getLoadMask(loadOp, nextMapping.lookupOrDefault(loadOp.getMask()),\n+            getLoadMask(loadOp, curMapping.lookupOrDefault(loadOp.getMask()),\n                         nextLoopCond, builder);\n         nextOp = builder.create<triton::LoadOp>(\n             loadOp.getLoc(), loadOp.getResult().getType(),\n-            nextMapping.lookupOrDefault(loadOp.getPtr()), newMask,\n-            nextMapping.lookupOrDefault(loadOp.getOther()),\n+            curMapping.lookupOrDefault(loadOp.getPtr()), newMask,\n+            curMapping.lookupOrDefault(loadOp.getOther()),\n             loadOp.getBoundaryCheckAttr(), loadOp.getPaddingAttr(),\n             loadOp.getCache(), loadOp.getEvict(), loadOp.getIsVolatile());\n         addNamedAttrs(nextOp, op->getDiscardableAttrDictionary());\n+        curMapping.map(loadOp.getResult(), nextOp->getResult(0));\n         nextMapping.map(loadOp.getResult(), nextOp->getResult(0));\n-      } else\n-        nextOp = builder.clone(*op, nextMapping);\n+      } else {\n+        nextOp = builder.clone(*op, curMapping);\n+        for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults()))\n+          nextMapping.map(op->getResult(dstIdx), nextOp->getResult(dstIdx));\n+      }\n \n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults()))\n         setValueMappingYield(newForOp, op->getResult(dstIdx),"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 7, "deletions": 1, "changes": 8, "file_content_changes": "@@ -643,7 +643,13 @@ void init_triton_ir(py::module &&m) {\n           [](mlir::OpBuilder &self) -> mlir::Type { return self.getI64Type(); })\n       .def(\"get_fp8e4_ty\",\n            [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getType<mlir::Float8E4M3FNType>();\n+             return self.getType<mlir::Float8E4M3FNUZType>();\n+           })\n+      .def(\"get_fp8e4b15_ty\",\n+           [](mlir::OpBuilder &self) -> mlir::Type {\n+             // TODO: upstream FP8E4B15 into MLIR, or find a way to externally\n+             // have a float-like type compatible with float only native ops\n+             return self.getType<mlir::Float8E4M3B11FNUZType>();\n            })\n       .def(\"get_fp8e5_ty\",\n            [](mlir::OpBuilder &self) -> mlir::Type {"}, {"filename": "python/test/unit/language/conftest.py", "status": "added", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -0,0 +1,14 @@\n+# content of conftest.py\n+\n+import pytest\n+\n+\n+def pytest_addoption(parser):\n+    parser.addoption(\n+        \"--device\", action=\"store\", default='cuda'\n+    )\n+\n+\n+@pytest.fixture\n+def device(request):\n+    return request.config.getoption(\"--device\")"}, {"filename": "python/test/unit/language/test_annotations.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -7,13 +7,13 @@\n import triton.language as tl\n \n \n-def test_annotations():\n+def test_annotations(device):\n \n     @triton.jit\n     def _kernel(X: torch.Tensor, N: int, BLOCK_SIZE: tl.constexpr):\n         pass\n \n-    x = torch.empty(1, device='cuda')\n+    x = torch.empty(1, device=device)\n     _kernel[(1,)](x, x.shape[0], 32)\n     try:\n         _kernel[(1,)](x.shape[0], x.shape[0], 32)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 276, "deletions": 286, "changes": 562, "file_content_changes": "@@ -101,13 +101,19 @@ def patch_kernel(template, to_replace):\n     return kernel\n \n \n-def check_type_supported(dtype):\n+def check_cuda_only(device):\n+    if device not in ['cuda']:\n+        pytest.skip(\"Only for cuda\")\n+\n+\n+def check_type_supported(dtype, device):\n     '''\n     skip test if dtype is not supported on the current device\n     '''\n-    cc = torch.cuda.get_device_capability()\n-    if cc[0] < 8 and (dtype is tl.bfloat16 or dtype == \"bfloat16\" or dtype is torch.bfloat16):\n-        pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n+    if device in ['cuda']:\n+        cc = torch.cuda.get_device_capability()\n+        if cc[0] < 8 and (dtype is tl.bfloat16 or dtype == \"bfloat16\" or dtype is torch.bfloat16):\n+            pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n \n \n class MmaLayout:\n@@ -142,20 +148,20 @@ def __str__(self):\n \n \n @pytest.mark.parametrize(\"dtype_x\", list(dtypes) + [\"bfloat16\"])\n-def test_empty_kernel(dtype_x, device='cuda'):\n+def test_empty_kernel(dtype_x, device):\n     SIZE = 128\n \n     @triton.jit\n     def kernel(X, SIZE: tl.constexpr):\n         pass\n-    check_type_supported(dtype_x)\n+    check_type_supported(dtype_x, device)\n     x = to_triton(numpy_random(SIZE, dtype_str=dtype_x), device=device, dst_type=dtype_x)\n     kernel[(1, )](x, SIZE=SIZE, num_warps=4)\n \n \n # generic test functions\n def _test_unary(dtype_x, expr, numpy_expr=None, device='cuda'):\n-    check_type_supported(dtype_x)  # early return if dtype_x is not supported\n+    check_type_supported(dtype_x, device)  # early return if dtype_x is not supported\n     SIZE = 128\n     # define the kernel / launch-grid\n \n@@ -213,8 +219,8 @@ def _binary_op_dtype_override(a: str, b: str) -> Optional[np.dtype]:\n \n \n def _test_binary(dtype_x, dtype_y, expr, numpy_expr=None, mode_x='real', mode_y='real', device='cuda', y_low=None, y_high=None):\n-    check_type_supported(dtype_x)  # early return if dtype_x is not supported\n-    check_type_supported(dtype_y)\n+    check_type_supported(dtype_x, device)  # early return if dtype_x is not supported\n+    check_type_supported(dtype_y, device)\n     SIZE = 128\n     # define the kernel / launch-grid\n \n@@ -284,7 +290,7 @@ def _mod_operation_ill_conditioned(dtype_x, dtype_y) -> bool:\n     for dtype_x in dtypes_with_bfloat16\n     for dtype_y in dtypes_with_bfloat16\n ])\n-def test_bin_op(dtype_x, dtype_y, op, device='cuda'):\n+def test_bin_op(dtype_x, dtype_y, op, device):\n     expr = f' x {op} y'\n     if op == '%' and dtype_x in int_dtypes + uint_dtypes and dtype_y in int_dtypes + uint_dtypes:\n         # LLVM has 'numpy.fmod', not 'numpy.remainder', semantics on integer remainders.\n@@ -317,7 +323,7 @@ def test_bin_op(dtype_x, dtype_y, op, device='cuda'):\n                          [(dtype_x, dtype_y) for dtype_x in int_dtypes for dtype_y in int_dtypes] +\n                          [(dtype_x, dtype_y) for dtype_x in uint_dtypes for dtype_y in uint_dtypes]\n                          )\n-def test_floordiv(dtype_x, dtype_y, device='cuda'):\n+def test_floordiv(dtype_x, dtype_y, device):\n     # Triton has IEEE, not numpy/torch, semantics for %, and those carry\n     # through to //, so we have to use a nonstandard expression to get a\n     # reference result for //.\n@@ -326,7 +332,7 @@ def test_floordiv(dtype_x, dtype_y, device='cuda'):\n     _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n \n \n-def test_unsigned_name_mangling(device='cuda'):\n+def test_unsigned_name_mangling(device):\n     # Test that uint32 and int32 are mangled differently by the compiler\n     SIZE = 128\n     # define the kernel / launch-grid\n@@ -372,7 +378,7 @@ def kernel(O1, O2, X, Y, SIZE: tl.constexpr):\n     for dtype_x in dtypes + dtypes_with_bfloat16\n     for dtype_y in dtypes + dtypes_with_bfloat16\n ])\n-def test_bitwise_op(dtype_x, dtype_y, op, device='cuda'):\n+def test_bitwise_op(dtype_x, dtype_y, op, device):\n     expr = f'x {op} y'\n     if (dtype_x in uint_dtypes and dtype_y in int_dtypes and _bitwidth(dtype_x) >= _bitwidth(dtype_y)):\n         numpy_expr = f'x.astype(np.{dtype_x}) {op} y.astype(np.{dtype_x})'\n@@ -395,7 +401,7 @@ def test_bitwise_op(dtype_x, dtype_y, op, device='cuda'):\n     for dtype_x in int_dtypes + uint_dtypes\n     for dtype_y in int_dtypes + uint_dtypes\n ])\n-def test_shift_op(dtype_x, dtype_y, op, device='cuda'):\n+def test_shift_op(dtype_x, dtype_y, op, device):\n     expr = f'x {op} y'\n     bw = max(_bitwidth(dtype_x), _bitwidth(dtype_y))\n     if dtype_x.startswith('int'):\n@@ -428,7 +434,7 @@ def test_shift_op(dtype_x, dtype_y, op, device='cuda'):\n                                                     ('nan', 'nan')]\n \n                           ])\n-def test_compare_op(dtype_x, dtype_y, op, mode_x, mode_y, device='cuda'):\n+def test_compare_op(dtype_x, dtype_y, op, mode_x, mode_y, device):\n     expr = f'x {op} y'\n     if (dtype_x in uint_dtypes and dtype_y in int_dtypes and _bitwidth(dtype_x) >= _bitwidth(dtype_y)):\n         numpy_expr = f'x.astype(np.{dtype_x}) {op} y.astype(np.{dtype_x})'\n@@ -443,7 +449,7 @@ def test_compare_op(dtype_x, dtype_y, op, mode_x, mode_y, device='cuda'):\n # test broadcast\n # ---------------\n @pytest.mark.parametrize(\"dtype\", dtypes_with_bfloat16)\n-def test_broadcast(dtype):\n+def test_broadcast(dtype, device):\n     @triton.jit\n     def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.constexpr):\n         offset1 = tl.arange(0, M)\n@@ -460,9 +466,9 @@ def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.con\n     y = numpy_random(N, dtype_str=dtype, rs=rs)\n     _, y_broadcasted_np = np.broadcast_arrays(x, y)\n \n-    x_tri = to_triton(x, device='cuda', dst_type=dtype)\n-    y_tri = to_triton(y, device='cuda', dst_type=dtype)\n-    y_broadcasted_tri = to_triton(np.empty((M, N), dtype=y_broadcasted_np.dtype), device='cuda', dst_type=dtype)\n+    x_tri = to_triton(x, device=device, dst_type=dtype)\n+    y_tri = to_triton(y, device=device, dst_type=dtype)\n+    y_broadcasted_tri = to_triton(np.empty((M, N), dtype=y_broadcasted_np.dtype), device=device, dst_type=dtype)\n \n     broadcast_kernel[(1,)](x_tri, y_tri, y_broadcasted_tri, M=M, N=N)\n     assert (y_broadcasted_np == to_numpy(y_broadcasted_tri)).all()\n@@ -472,8 +478,8 @@ def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.con\n # ------------------\n \n \n-def test_invalid_slice():\n-    dst = torch.empty(128, device='cuda')\n+def test_invalid_slice(device):\n+    dst = torch.empty(128, device=device)\n \n     @triton.jit\n     def _kernel(dst):\n@@ -486,7 +492,7 @@ def _kernel(dst):\n # ----------------\n # test expand_dims\n # ----------------\n-def test_expand_dims():\n+def test_expand_dims(device):\n     @triton.jit\n     def expand_dims_kernel(dummy, N: tl.constexpr):\n         offset1 = tl.arange(0, N)\n@@ -516,11 +522,11 @@ def expand_dims_kernel(dummy, N: tl.constexpr):\n         tl.static_assert(t.shape == [N, 1, 1, 1])\n \n     N = 32\n-    dummy_tensor = torch.empty((), device=\"cuda\")\n+    dummy_tensor = torch.empty((), device=device)\n     expand_dims_kernel[(1,)](dummy_tensor, N)\n \n \n-def test_expand_dims_error_cases():\n+def test_expand_dims_error_cases(device):\n     @triton.jit\n     def dim_out_of_range1(dummy, N: tl.constexpr):\n         offset1 = tl.arange(0, N)\n@@ -548,7 +554,7 @@ def duplicate_dim2(dummy, N: tl.constexpr):\n         t = tl.expand_dims(offset1, (0, -3))\n \n     N = 32\n-    dummy_tensor = torch.empty((), device=\"cuda\")\n+    dummy_tensor = torch.empty((), device=device)\n \n     with pytest.raises(triton.CompilationError, match=\"invalid axis -3\"):\n         dim_out_of_range1[(1,)](dummy_tensor, N)\n@@ -566,8 +572,8 @@ def duplicate_dim2(dummy, N: tl.constexpr):\n # ----------------------------\n # test invalid program id axis\n # ----------------------------\n-def test_invalid_pid_axis():\n-    dst = torch.empty(128, device='cuda')\n+def test_invalid_pid_axis(device):\n+    dst = torch.empty(128, device=device)\n \n     @triton.jit\n     def _kernel(dst):\n@@ -581,12 +587,12 @@ def _kernel(dst):\n # test where\n # ---------------\n @pytest.mark.parametrize(\"dtype\", dtypes_with_bfloat16 + [\"*int32\"])\n-def test_where(dtype):\n+def test_where(dtype, device):\n     select_ptrs = False\n     if dtype == \"*int32\":\n         dtype = \"int64\"\n         select_ptrs = True\n-    check_type_supported(dtype)\n+    check_type_supported(dtype, device)\n \n     @triton.jit\n     def where_kernel(cond_ptr, a_ptr, b_ptr, output_ptr, n_elements,\n@@ -616,10 +622,10 @@ def where_kernel(cond_ptr, a_ptr, b_ptr, output_ptr, n_elements,\n     y = numpy_random(SIZE, dtype_str=dtype, rs=rs)\n     z = np.where(cond, x, y)\n \n-    cond_tri = to_triton(cond, device='cuda')\n-    x_tri = to_triton(x, device='cuda', dst_type=dtype)\n-    y_tri = to_triton(y, device='cuda', dst_type=dtype)\n-    z_tri = to_triton(np.empty(SIZE, dtype=z.dtype), device='cuda', dst_type=dtype)\n+    cond_tri = to_triton(cond, device=device)\n+    x_tri = to_triton(x, device=device, dst_type=dtype)\n+    y_tri = to_triton(y, device=device, dst_type=dtype)\n+    z_tri = to_triton(np.empty(SIZE, dtype=z.dtype), device=device, dst_type=dtype)\n \n     grid = lambda meta: (triton.cdiv(SIZE, meta['BLOCK_SIZE']),)\n     where_kernel[grid](cond_tri, x_tri, y_tri, z_tri, SIZE, BLOCK_SIZE=1024, TEST_POINTERS=select_ptrs, TEST_SCALAR_POINTERS=False)\n@@ -630,7 +636,7 @@ def where_kernel(cond_ptr, a_ptr, b_ptr, output_ptr, n_elements,\n         assert (z == to_numpy(z_tri)).all()\n \n \n-def test_where_broadcast():\n+def test_where_broadcast(device):\n     @triton.jit\n     def where_kernel(cond_ptr, a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n         xoffsets = tl.arange(0, BLOCK_SIZE)[:, None]\n@@ -656,9 +662,9 @@ def where_scalar_condition(a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n     x = numpy_random((SIZE, SIZE), dtype_str=dtype, rs=rs)\n     mask = numpy_random(SIZE, 'bool', rs=rs)\n     z = np.where(mask, x, 0)\n-    cond_tri = to_triton(mask, device=\"cuda\")\n-    x_tri = to_triton(x, device='cuda', dst_type=dtype)\n-    z_tri = to_triton(np.empty((SIZE, SIZE), dtype=z.dtype), device='cuda', dst_type=dtype)\n+    cond_tri = to_triton(mask, device=device)\n+    x_tri = to_triton(x, device=device, dst_type=dtype)\n+    z_tri = to_triton(np.empty((SIZE, SIZE), dtype=z.dtype), device=device, dst_type=dtype)\n     where_kernel[(1,)](cond_tri, x_tri, z_tri, SIZE)\n     assert (z == to_numpy(z_tri)).all()\n     where_scalar_condition[(1,)](x_tri, z_tri, SIZE)\n@@ -675,7 +681,7 @@ def where_scalar_condition(a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n ] + [\n     (dtype_x, ' ~x') for dtype_x in int_dtypes\n ])\n-def test_unary_op(dtype_x, expr, device='cuda'):\n+def test_unary_op(dtype_x, expr, device):\n     _test_unary(dtype_x, expr, device=device)\n \n # ----------------\n@@ -684,7 +690,7 @@ def test_unary_op(dtype_x, expr, device='cuda'):\n \n \n @pytest.mark.parametrize(\"dtype_x, expr\", [(dtype_x, expr) for dtype_x in [\"float32\", \"float64\"] for expr in ['exp', 'log', 'cos', 'sin']])\n-def test_math_op(dtype_x, expr, device='cuda'):\n+def test_math_op(dtype_x, expr, device):\n     _test_unary(dtype_x, f'tl.{expr}(x)', f'np.{expr}(x) ', device=device)\n \n # ----------------\n@@ -696,12 +702,12 @@ def test_math_op(dtype_x, expr, device='cuda'):\n     (dtype_x)\n     for dtype_x in dtypes_with_bfloat16\n ])\n-def test_abs(dtype_x, device='cuda'):\n+def test_abs(dtype_x, device):\n     _test_unary(dtype_x, 'tl.abs(x)', 'np.abs(x) ', device=device)\n \n \n-@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n-def test_abs_f8(in_dtype):\n+@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4, tl.float8e5])\n+def test_abs_fp8(in_dtype, device):\n \n     @triton.jit\n     def abs_kernel(Z, X, SIZE: tl.constexpr):\n@@ -710,14 +716,13 @@ def abs_kernel(Z, X, SIZE: tl.constexpr):\n         z = tl.abs(x)\n         tl.store(Z + off, z)\n \n-    f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device='cuda')\n+    f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device=device)\n     # f32_to_f8 doesn't handle nan, so we make sure f8_tensor doesn't contain any nan\n     all_exp_ones = (f8_tensor & 0b01111100) == 128 - 2**in_dtype.fp_mantissa_width\n     f8_tensor[all_exp_ones] = 0\n     f8 = triton.reinterpret(f8_tensor, in_dtype)\n     n_elements = f8_tensor.numel()\n     out_f8 = torch.empty_like(f8_tensor)\n-    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n     abs_kernel[(1,)](f8, triton.reinterpret(out_f8, in_dtype), n_elements)\n \n     f32_tensor = convert_float_to_float32(f8_tensor, in_dtype)\n@@ -750,7 +755,7 @@ def make_ptr_str(name, shape):\n               ':, :, None']\n     for d in ['int32', 'uint32', 'uint16']\n ])\n-def test_index1d(expr, dtype_str, device='cuda'):\n+def test_index1d(expr, dtype_str, device):\n     rank_x = expr.count(':')\n     rank_y = expr.count(',') + 1\n     shape_x = [32 for _ in range(rank_x)]\n@@ -785,7 +790,7 @@ def generate_kernel(shape_x, shape_z):\n     z_ref = eval(expr) + y\n     # triton result\n     z_tri = to_triton(np.empty_like(z_ref), device=device)\n-    x_tri = to_triton(x)\n+    x_tri = to_triton(x, device=device)\n     kernel_match[(1, )](z_tri, x_tri, num_warps=1, SIZE=shape_x[0])\n     # compare\n     assert (z_ref == to_numpy(z_tri)).all()\n@@ -814,9 +819,7 @@ def tuples_fn(a, b):\n         a * b\n \n \n-def test_tuples():\n-    device = 'cuda'\n-\n+def test_tuples(device):\n     @triton.jit\n     def with_fn(X, Y, A, B, C):\n         x = tl.load(X)\n@@ -907,9 +910,7 @@ def noinline_multi_values_fn(x, y, Z):\n \n \n @pytest.mark.parametrize(\"mode\", [\"simple\", \"call_graph\", \"shared\", \"dynamic\", \"multi_values\"])\n-def test_noinline(mode):\n-    device = 'cuda'\n-\n+def test_noinline(mode, device):\n     @triton.jit\n     def kernel(X, Y, Z):\n         x = tl.load(X)\n@@ -946,7 +947,9 @@ def kernel(X, Y, Z):\n     ]\n     for mode in ['all_neg', 'all_pos', 'min_neg', 'max_pos']\n     for sem in [None, 'acquire', 'release', 'acq_rel', 'relaxed']]))\n-def test_atomic_rmw(op, dtype_x_str, mode, sem, device='cuda'):\n+def test_atomic_rmw(op, dtype_x_str, mode, sem, device):\n+    check_cuda_only(device)\n+\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         if dtype_x_str == 'float16':\n@@ -996,7 +999,7 @@ def kernel(X, Z):\n     assert f\"atom.global.gpu.{sem_str}\" in h.asm[\"ptx\"]\n \n \n-def test_atomic_rmw_predicate(device=\"cuda\"):\n+def test_atomic_rmw_predicate(device):\n     @triton.jit\n     def kernel(X):\n         val = tl.program_id(0)\n@@ -1009,7 +1012,7 @@ def kernel(X):\n \n @pytest.mark.parametrize(\"shape, axis\",\n                          [(shape, axis) for shape in [(2, 2), (2, 8), (8, 2), (8, 8), (32, 32)] for axis in [0, 1]])\n-def test_tensor_atomic_rmw(shape, axis, device=\"cuda\"):\n+def test_tensor_atomic_rmw(shape, axis, device):\n     shape0, shape1 = shape\n     # triton kernel\n \n@@ -1035,7 +1038,7 @@ def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr)\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n \n \n-def test_tensor_atomic_rmw_block(device=\"cuda\"):\n+def test_tensor_atomic_rmw_block(device):\n     shape = (8, 8)\n \n     @triton.jit\n@@ -1052,13 +1055,13 @@ def kernel(X, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"sem\", [None, 'acquire', 'release', 'acq_rel', 'relaxed'])\n-def test_atomic_cas(sem):\n+def test_atomic_cas(sem, device):\n     # 1. make sure that atomic_cas changes the original value (Lock)\n     @triton.jit\n     def change_value(Lock):\n         tl.atomic_cas(Lock, 0, 1)\n \n-    Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n+    Lock = torch.zeros((1,), device=device, dtype=torch.int32)\n     change_value[(1,)](Lock)\n \n     assert (Lock[0] == 1)\n@@ -1075,8 +1078,8 @@ def serialized_add(data, Lock, SEM: tl.constexpr):\n         # release lock\n         tl.atomic_xchg(Lock, 0)\n \n-    Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n-    data = torch.zeros((128,), device='cuda', dtype=torch.float32)\n+    Lock = torch.zeros((1,), device=device, dtype=torch.int32)\n+    data = torch.zeros((128,), device=device, dtype=torch.float32)\n     ref = torch.full((128,), 64.0)\n     h = serialized_add[(64,)](data, Lock, SEM=sem)\n     sem_str = \"acq_rel\" if sem is None else sem\n@@ -1103,10 +1106,10 @@ def serialized_add(data, Lock, SEM: tl.constexpr):\n ] + [\n     (f'int{x}', f'uint{x}', True) for x in [8, 16, 32, 64]\n ])\n-def test_cast(dtype_x, dtype_z, bitcast, device='cuda'):\n+def test_cast(dtype_x, dtype_z, bitcast, device):\n     # bfloat16 on cc < 80 will not be tested\n-    check_type_supported(dtype_x)\n-    check_type_supported(dtype_z)\n+    check_type_supported(dtype_x, device)\n+    check_type_supported(dtype_z, device)\n \n     # This is tricky because numpy doesn't have bfloat, and torch doesn't have uints.\n     x0 = 43 if dtype_x in int_dtypes else 43.5\n@@ -1116,7 +1119,7 @@ def test_cast(dtype_x, dtype_z, bitcast, device='cuda'):\n         x_tri = torch.tensor([x0], dtype=getattr(torch, dtype_x), device=device)\n     else:\n         x = np.array([x0], dtype=getattr(np, dtype_x))\n-        x_tri = to_triton(x)\n+        x_tri = to_triton(x, device=device)\n \n     # triton kernel\n     @triton.jit\n@@ -1148,8 +1151,8 @@ def kernel(X, Z, BITCAST: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"dtype_str, num_warps\", [(dtype_str, num_warps) for dtype_str in int_dtypes + float_dtypes for num_warps in [4, 8]])\n-def test_cat(dtype_str, num_warps):\n-    check_type_supported(dtype_str)\n+def test_cat(dtype_str, num_warps, device):\n+    check_type_supported(dtype_str, device)\n \n     @triton.jit\n     def kernel(X, Y, Z, N: tl.constexpr):\n@@ -1159,19 +1162,19 @@ def kernel(X, Y, Z, N: tl.constexpr):\n         z = tl.cat(x, y, can_reorder=True)\n         tl.store(Z + tl.arange(0, 2 * N), z)\n \n-    x = torch.arange(0, 128, device='cuda').to(getattr(torch, dtype_str))\n-    y = torch.arange(-128, 0, device='cuda').to(getattr(torch, dtype_str))\n+    x = torch.arange(0, 128, device=device).to(getattr(torch, dtype_str))\n+    y = torch.arange(-128, 0, device=device).to(getattr(torch, dtype_str))\n     z_ref = torch.cat([x, y], dim=0).sum()\n-    z = torch.zeros((256,), dtype=getattr(torch, dtype_str), device='cuda')\n+    z = torch.zeros((256,), dtype=getattr(torch, dtype_str), device=device)\n     kernel[(1, )](x, y, z, N=128, num_warps=num_warps)\n     assert z.sum() == z_ref\n     # check if there's no duplicate value in z\n     assert z.unique().size(0) == z.size(0)\n \n \n @pytest.mark.parametrize(\"dtype_str\", list(torch_dtypes))\n-def test_store_constant(dtype_str):\n-    check_type_supported(dtype_str)\n+def test_store_constant(dtype_str, device):\n+    check_type_supported(dtype_str, device)\n \n     \"\"\"Tests that boolean True is stored as 1\"\"\"\n     @triton.jit\n@@ -1184,14 +1187,14 @@ def kernel(output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     triton_dtype_str = 'uint8' if dtype_str == 'bool' else dtype_str\n     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.zeros([BLOCK_SIZE], dtype=tl.{triton_dtype_str}) + 1'})\n     block_size = 128\n-    ref = torch.ones([block_size], dtype=getattr(torch, dtype_str), device='cuda')\n-    output = torch.zeros([block_size], dtype=getattr(torch, dtype_str), device='cuda')\n+    ref = torch.ones([block_size], dtype=getattr(torch, dtype_str), device=device)\n+    output = torch.zeros([block_size], dtype=getattr(torch, dtype_str), device=device)\n     kernel[(1,)](output, block_size, BLOCK_SIZE=block_size)\n \n     assert torch.all(output == ref)\n \n \n-def test_load_store_same_ptr():\n+def test_load_store_same_ptr(device):\n     @triton.jit()\n     def kernel(in_out_ptr):\n         pid = tl.program_id(axis=0)\n@@ -1200,7 +1203,7 @@ def kernel(in_out_ptr):\n         tl.store(in_out_ptr + pid, out)\n \n     for _ in range(1000):\n-        x = torch.ones((65536,), device=\"cuda\", dtype=torch.float32)\n+        x = torch.ones((65536,), device=device, dtype=torch.float32)\n         kernel[(65536,)](x, num_warps=32)\n         assert torch.all(x == 2)\n \n@@ -1211,7 +1214,7 @@ def convert_float_to_float32(fp: torch.tensor, dtype=None):\n \n     fp = fp.view(getattr(torch, f\"int{dtype.primitive_bitwidth}\"))\n     exp_width = dtype.primitive_bitwidth - dtype.fp_mantissa_width - 1\n-    exp_bias = 2 ** (exp_width - 1) - 1\n+    exp_bias = dtype.exponent_bias\n     sign = ((fp >> (dtype.primitive_bitwidth - 1)) & 0x01).int()\n     exp = ((fp >> dtype.fp_mantissa_width) & ((1 << exp_width) - 1)).int()\n     frac = (fp & ((1 << dtype.fp_mantissa_width) - 1)).int()\n@@ -1224,7 +1227,7 @@ def convert_float_to_float32(fp: torch.tensor, dtype=None):\n \n     extended_exp = ((1 << (tl.float32.primitive_bitwidth - tl.float32.fp_mantissa_width - 1)) - 1) << tl.float32.fp_mantissa_width\n     # special cases, exp is 0b11..1\n-    if dtype == tl.float8e4:\n+    if dtype in [tl.float8e4, tl.float8e4b15]:\n         # float8e4m3 does not have infinities\n         output[fp == 0b01111111] = torch.nan\n         output[fp == 0b11111111] = torch.nan\n@@ -1236,9 +1239,9 @@ def convert_float_to_float32(fp: torch.tensor, dtype=None):\n \n \n @pytest.mark.parametrize(\"in_dtype\", [torch.float16, torch.bfloat16])\n-def test_convert_float16_to_float32(in_dtype):\n+def test_convert_float16_to_float32(in_dtype, device):\n     \"\"\"Tests that check convert_float_to_float32 function\"\"\"\n-    check_type_supported(in_dtype)\n+    check_type_supported(in_dtype, device)\n \n     f16_input = torch.tensor(range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=torch.int16).view(in_dtype)\n     f32_output = convert_float_to_float32(f16_input)\n@@ -1251,52 +1254,43 @@ def test_convert_float16_to_float32(in_dtype):\n     assert torch.all(f16_input[other] == f32_output[other])\n \n \n-@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n-@pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16, torch.float32])\n-def test_f8_xf16_roundtrip(in_dtype, out_dtype):\n-    \"\"\"Tests that converting an f8 to f16 and back to f8 doesn't change its value\"\"\"\n-    check_type_supported(out_dtype)\n-\n-    @triton.jit\n-    def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n-        offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n-        mask = offsets < n_elements\n-        input = tl.load(input_ptr + offsets, mask=mask)\n-        output = input\n-        tl.store(output_ptr + offsets, output, mask=mask)\n-\n-    f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device='cuda')\n-    # f32_to_f8 doesn't handle nan, so we make sure f8_tensor doesn't contain any nan\n-    all_exp_ones = (f8_tensor & 0b01111100) == 128 - 2**in_dtype.fp_mantissa_width\n-    f8_tensor[all_exp_ones] = 0\n-    f8 = triton.reinterpret(f8_tensor, in_dtype)\n-    n_elements = f8_tensor.numel()\n-    xf16 = torch.empty_like(f8_tensor, dtype=out_dtype)\n-    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n-    copy_kernel[grid](f8, xf16, n_elements, BLOCK_SIZE=1024)\n-\n-    # exponent_mask = 0b01111100 for float8e5\n-    # exponent_mask = 0b01111000 for float8e4\n-    exponent_mask = 0b01111111 ^ ((1 << in_dtype.fp_mantissa_width) - 1)\n-    normal = torch.logical_and((f8_tensor & exponent_mask) != 0, (f8_tensor & exponent_mask) != exponent_mask)\n-    ref16 = convert_float_to_float32(f8_tensor, in_dtype)\n-    # WARN: currently only normal float8s are handled\n-    assert torch.all(xf16[normal] == ref16[normal])\n-\n-    f8_output_tensor = torch.empty_like(xf16, dtype=torch.int8)\n-    f8_output = triton.reinterpret(f8_output_tensor, in_dtype)\n-    copy_kernel[grid](xf16, f8_output, n_elements, BLOCK_SIZE=1024)\n+def serialize_fp8(np_data, in_dtype):\n+    if in_dtype == tl.float8e4b15:\n+        # triton's f8e4b15 format is optimized for software emulation\n+        # as a result, each pack of 4xfp8 values:\n+        # s0b0s1b1s2b2s3b3 (for s, b sign and bits respectively)\n+        # is actually internally stored as\n+        # s0s2b0b2s1s3b1b3\n+        # we apply the conversion here\n+        f8x4 = np_data.view(np.uint32)\n+        s = [(f8x4 & (0x80000000 >> i)) << i for i in range(0, 32, 8)]\n+        b = [(f8x4 & (0x7f000000 >> i)) << i for i in range(0, 32, 8)]\n+        signs = (s[0] >> 0) | (s[1] >> 16) | (s[2] >> 1) | (s[3] >> 17)\n+        bits = (b[0] >> 1) | (b[1] >> 17) | (b[2] >> 8) | (b[3] >> 24)\n+        # tensor of triton fp8 data\n+        return (signs | bits).view(np.int8)\n+    else:\n+        return np_data\n \n-    assert torch.all(f8_tensor == f8_output_tensor)\n \n+@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4, tl.float8e5])\n+@pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16, torch.float32])\n+def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n+    \"\"\"\n+    For all possible float8 values (ref_fp8 = range(0, 256)), test that:\n+        - conversion tri_fp16 = convert(input=ref_fp8, out=out_dtype) matches the reference\n+        - conversion tri_fp8 = convert(input=tri_fp16, out=out_dtype) matches the original\n+    this is only possible if both conversions are correct\n+    \"\"\"\n+    check_type_supported(out_dtype, device)\n+    from contextlib import nullcontext as does_not_raise\n+    expectation = does_not_raise()\n+    err_msg = None\n+    if (in_dtype == tl.float8e4b15 and out_dtype != torch.float16) or\\\n+       (in_dtype != torch.float16 and out_dtype == tl.float8e4b15):\n+        expectation = pytest.raises(triton.CompilationError)\n+        err_msg = \"fp8e4b15 can only be converted to/from fp16\"\n \n-@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n-@pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16])\n-def test_f16_to_f8_rounding(in_dtype, out_dtype):\n-    \"\"\"Takes all float16s, converts them to float8 and back to float16. Checks that the absolute\n-    error is the minimum over all float8.\n-    Or the same explanation a bit mathier:\n-    for all f16 |f16 - fromf8(tof8(f16))| == min over all f8 |f16 - fromf8(f8)|\"\"\"\n     @triton.jit\n     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n@@ -1305,50 +1299,28 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         output = input\n         tl.store(output_ptr + offsets, output, mask=mask)\n \n-    i16_input = torch.tensor(range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=torch.int16, device='cuda')\n-    f16_input = i16_input.view(out_dtype)\n-    n_elements = f16_input.numel()\n-    f8_output_tensor = torch.empty_like(f16_input, dtype=torch.int8)\n-    f8_output = triton.reinterpret(f8_output_tensor, in_dtype)\n-    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n-    copy_kernel[grid](f16_input, f8_output, n_elements, BLOCK_SIZE=1024)\n-\n-    f16_output = torch.empty_like(f16_input, dtype=out_dtype)\n-    copy_kernel[grid](f8_output, f16_output, n_elements, BLOCK_SIZE=1024)\n-\n-    abs_error = torch.abs(f16_input - f16_output)\n-\n-    all_f8_vals_tensor = torch.tensor(range(2 ** 8), dtype=torch.uint8, device='cuda')\n-    all_f8_vals = triton.reinterpret(all_f8_vals_tensor, in_dtype)\n-    all_f8_vals_in_f16 = torch.empty_like(all_f8_vals_tensor, dtype=out_dtype)\n-    copy_kernel[grid](all_f8_vals, all_f8_vals_in_f16, n_elements=256, BLOCK_SIZE=1024)\n-\n-    all_finite_f8_vals_in_f16 = all_f8_vals_in_f16[\n-        torch.isfinite(all_f8_vals_in_f16)\n-    ]\n-\n-    min_error = torch.min(\n-        torch.abs(\n-            f16_input.reshape((-1, 1))\n-            - all_finite_f8_vals_in_f16.reshape((1, -1))\n-        ),\n-        dim=1,\n-    )[0]\n-\n-    # WARN: only normalized numbers are handled\n-    f8_normal_min = 1 << in_dtype.fp_mantissa_width  # 0b00001000 for float8e4\n-    f8_normal_max = 0b01111110 if in_dtype == tl.float8e4 else 0b01111011\n-    f16_min, f16_max, f16_max_minus_1 = convert_float_to_float32(torch.tensor([f8_normal_min, f8_normal_max, f8_normal_max - 1], dtype=torch.int8), in_dtype)\n-    assert torch.all(torch.isfinite(f16_min))\n-    assert torch.all(torch.isfinite(f16_max))\n-    thres_error = f16_max - f16_max_minus_1\n-    mismatch = torch.logical_and(\n-        torch.logical_or(abs_error != min_error, abs_error > thres_error), torch.logical_and(torch.isfinite(f16_input), torch.logical_and(torch.abs(f16_input) <= f16_max, torch.abs(f16_input) >= f16_min))\n-    )\n-    assert torch.all(\n-        torch.logical_not(mismatch)\n-    ), f\"f16_input[mismatch]={f16_input[mismatch]} f16_output[mismatch]={f16_output[mismatch]} abs_error[mismatch]={abs_error[mismatch]} min_error[mismatch]={min_error[mismatch]}\"\n-\n+    # initialize array containing all possible f8 values except NaN\n+    ref_fp8 = np.array(range(-128, 128), dtype=np.int8)\n+    is_nan = (ref_fp8 & 0b01111100) == 128 - 2**in_dtype.fp_mantissa_width\n+    exp_mask = 0b01111111 ^ ((1 << in_dtype.fp_mantissa_width) - 1)\n+    is_subnormal = np.logical_or((ref_fp8 & exp_mask) == 0, (ref_fp8 & exp_mask) == exp_mask)\n+    ref_fp8[is_nan] = 0\n+    ref_fp8[is_subnormal] = 0\n+    tri_fp8 = torch.from_numpy(serialize_fp8(ref_fp8, in_dtype)).cuda()\n+    tri_fp16 = torch.empty(256, dtype=out_dtype, device=\"cuda\")\n+    with expectation as e:\n+        copy_kernel[(1,)](triton.reinterpret(tri_fp8, in_dtype), tri_fp16, tri_fp16.shape[0], BLOCK_SIZE=1024)\n+\n+        ref_fp8 = torch.from_numpy(ref_fp8).cuda()\n+        ref_fp16 = convert_float_to_float32(ref_fp8, in_dtype)\n+        assert torch.all(tri_fp16[~is_subnormal] == ref_fp16[~is_subnormal])\n+\n+        ref_fp8 = torch.empty_like(tri_fp16, dtype=torch.int8)\n+        copy_kernel[(1,)](tri_fp16, triton.reinterpret(ref_fp8, in_dtype), tri_fp16.shape[0], BLOCK_SIZE=1024)\n+        assert torch.all(tri_fp8 == ref_fp8)\n+\n+    if err_msg is not None:\n+        assert err_msg in str(e)\n \n # ---------------\n # test reduce\n@@ -1375,8 +1347,8 @@ def get_reduced_dtype(dtype_str, op):\n                                      'sum']\n                           for dtype in dtypes_with_bfloat16\n                           for shape in [32, 64, 128, 512]])\n-def test_reduce1d(op, dtype_str, shape, device='cuda'):\n-    check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n+def test_reduce1d(op, dtype_str, shape, device):\n+    check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n \n     # triton kernel\n     @triton.jit\n@@ -1448,7 +1420,7 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n reduce2d_shapes = [(2, 32), (4, 32), (4, 128)]\n # TODO: fix and uncomment\n # , (32, 64), (64, 128)]\n-if 'V100' in torch.cuda.get_device_name(0):\n+if torch.cuda.is_available() and 'V100' in torch.cuda.get_device_name(0):\n     reduce2d_shapes += [(128, 256) and (32, 1024)]\n \n \n@@ -1464,8 +1436,8 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"op, dtype_str, shape, axis\", reduce_configs1 + reduce_configs2)\n-def test_reduce2d(op, dtype_str, shape, axis, device='cuda'):\n-    check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n+def test_reduce2d(op, dtype_str, shape, axis, device):\n+    check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n \n     # triton kernel\n     @triton.jit\n@@ -1486,7 +1458,7 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n     rs = RandomState(17)\n     # limit the range of integers so that the sum does not overflow\n     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n-    x_tri = to_triton(x)\n+    x_tri = to_triton(x, device=device)\n     numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n                 'argmin': np.argmin, 'argmax': np.argmax}[op]\n     z_dtype_str = get_reduced_dtype(dtype_str, op)\n@@ -1525,14 +1497,15 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n layouts = [\n     BlockedLayout([1, 4], [8, 4], [4, 1], [1, 0]),\n     BlockedLayout([1, 4], [8, 4], [4, 1], [0, 1]),\n-    MmaLayout(version=(2, 0), warps_per_cta=[4, 1])\n+    MmaLayout(version=(2, 0), warps_per_cta=[4, 1]),\n+    MmaLayout(version=(2, 0), warps_per_cta=[2, 2])\n ]\n \n \n @pytest.mark.parametrize(\"M, N\", [[128, 16], [128, 128], [32, 128]])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n @pytest.mark.parametrize(\"axis\", [0, 1])\n-def test_reduce_layouts(M, N, src_layout, axis, device='cuda'):\n+def test_reduce_layouts(M, N, src_layout, axis, device):\n     rdims_2d = f\"1x{N}\" if axis == 0 else f\"{M}x1\"\n     rdims_1d = f\"{N}\" if axis == 0 else f\"{M}\"\n     store_range = \"%7\" if axis == 0 else \"%1\"\n@@ -1604,7 +1577,7 @@ def test_reduce_layouts(M, N, src_layout, axis, device='cuda'):\n \n @pytest.mark.parametrize(\"M\", [32, 64, 128, 256])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n-def test_store_op(M, src_layout, device='cuda'):\n+def test_store_op(M, src_layout, device):\n     ir = f\"\"\"\n     #src = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n@@ -1653,7 +1626,7 @@ def test_store_op(M, src_layout, device='cuda'):\n @pytest.mark.parametrize(\"dst_layout\", layouts)\n @pytest.mark.parametrize(\"src_dim\", [0, 1])\n @pytest.mark.parametrize(\"dst_dim\", [0, 1])\n-def test_convert1d(M, src_layout, dst_layout, src_dim, dst_dim, device='cuda'):\n+def test_convert1d(M, src_layout, dst_layout, src_dim, dst_dim, device):\n     ir = f\"\"\"\n     #dst = {dst_layout}\n     #src = {src_layout}\n@@ -1710,7 +1683,18 @@ def _welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n \n @pytest.mark.parametrize(\"M, N\", [[128, 128], [256, 128], [256, 256], [128, 256]])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n-def test_chain_reduce(M, N, src_layout, device='cuda'):\n+@pytest.mark.parametrize(\"op\", [\"sum\", \"max\"])\n+def test_chain_reduce(M, N, src_layout, op, device):\n+    op_str = \"\"\n+    if op == \"sum\":\n+        op_str = f\"\"\"\n+        %13 = arith.addi %arg2, %arg3 : i32\n+        tt.reduce.return %13 : i32\"\"\"\n+    elif op == \"max\":\n+        op_str = f\"\"\"\n+        %13 = \"triton_gpu.cmpi\"(%arg2, %arg3) <{{predicate = 4 : i64}}> : (i32, i32) -> i1\n+        %14 = arith.select %13, %arg2, %arg3 : i32\n+        tt.reduce.return %14 : i32\"\"\"\n     ir = f\"\"\"\n     #src = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n@@ -1729,13 +1713,11 @@ def test_chain_reduce(M, N, src_layout, device='cuda'):\n         %10 = tt.load %9 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xi32, #src>\n         %11 = \"tt.reduce\"(%10) ({{\n         ^bb0(%arg2: i32, %arg3: i32):\n-        %13 = arith.addi %arg2, %arg3 : i32\n-        tt.reduce.return %13 : i32\n+        {op_str}\n         }}) {{axis = 1 : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n         %12 = \"tt.reduce\"(%11) ({{\n         ^bb0(%arg2: i32, %arg3: i32):\n-        %13 = arith.addi %arg2, %arg3 : i32\n-        tt.reduce.return %13 : i32\n+        {op_str}\n         }}) {{axis = 0 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> i32\n         tt.store %arg1, %12 {{cache = 1 : i32, evict = 1 : i32}} : i32\n         tt.return\n@@ -1757,12 +1739,15 @@ def test_chain_reduce(M, N, src_layout, device='cuda'):\n     z_tri = torch.tensor(z, device=device)\n \n     pgm = kernel[(1, 1, 1)](x_tri, z_tri)\n-    z_ref = np.sum(x)\n+    if op == \"sum\":\n+        z_ref = np.sum(x)\n+    elif op == \"max\":\n+        z_ref = np.max(x)\n \n     np.testing.assert_allclose(z_ref, z_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n \n \n-def test_generic_reduction(device='cuda'):\n+def test_generic_reduction(device):\n \n     @triton.jit\n     def var_mean_kernel(X, out_mean, out_var, BLOCK: tl.constexpr):\n@@ -1798,8 +1783,8 @@ def var_mean_kernel(X, out_mean, out_var, BLOCK: tl.constexpr):\n                           for dtype in ['float16', 'float32']\n                              for shape in [(64, 64), (128, 128)]\n                              for perm in [(1, 0)]])\n-def test_permute(dtype_str, shape, perm, device='cuda'):\n-    check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n+def test_permute(dtype_str, shape, perm, device):\n+    check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n \n     # triton kernel\n     @triton.jit\n@@ -1869,7 +1854,9 @@ def kernel(X, stride_xm, stride_xn,\n                                                       ('float16', 'float16'),\n                                                       ('float16', 'float32'),\n                                                       ('float32', 'float32')]])\n-def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype, device='cuda'):\n+def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype, device):\n+    check_cuda_only(device)\n+\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n@@ -2028,9 +2015,9 @@ def kernel(X, stride_xm, stride_xk,\n \n \n @pytest.mark.parametrize(\"dtype_str\", int_dtypes + float_dtypes + ['bfloat16'])\n-def test_full(dtype_str):\n+def test_full(dtype_str, device):\n     dtype = getattr(torch, dtype_str)\n-    check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n+    check_type_supported(dtype, device)  # bfloat16 on cc < 80 will not be tested\n \n     @triton.jit\n     def kernel_static(out):\n@@ -2045,9 +2032,9 @@ def kernel_dynamic(out, val, dtype: tl.constexpr):\n         tl.store(out_ptr, a)\n \n     kernel_static_patched = patch_kernel(kernel_static, {'GENERATE_TEST_HERE': f\"tl.full((128,), 2, tl.{dtype_str})\"})\n-    out_static = torch.zeros((128), dtype=dtype, device=\"cuda\")\n+    out_static = torch.zeros((128), dtype=dtype, device=device)\n     kernel_static_patched[(1,)](out_static)\n-    out_dynamic = torch.zeros((128), dtype=dtype, device=\"cuda\")\n+    out_dynamic = torch.zeros((128), dtype=dtype, device=device)\n     kernel_dynamic[(1,)](out_dynamic, 2, getattr(triton.language, dtype_str))\n     assert torch.all(out_static == 2)\n     assert torch.all(out_dynamic == 2)\n@@ -2059,20 +2046,20 @@ def kernel_dynamic(out, val, dtype: tl.constexpr):\n                           ('float(\"nan\")', \"f32\"), ('float(\"-nan\")', \"f32\"),\n                           (0., \"f32\"),\n                           (5, \"i32\"), (2**40, \"i64\"),])\n-def test_constexpr(literal, dtype_str):\n+def test_constexpr(literal, dtype_str, device):\n     @triton.jit\n     def kernel(out_ptr):\n         val = GENERATE_TEST_HERE\n         tl.store(out_ptr.to(tl.pointer_type(val.dtype)), val)\n \n     kernel_patched = patch_kernel(kernel, {'GENERATE_TEST_HERE': f\"{literal}\"})\n-    out = torch.zeros((1,), dtype=torch.float32, device=\"cuda\")\n+    out = torch.zeros((1,), dtype=torch.float32, device=device)\n     h = kernel_patched[(1,)](out)\n     assert re.search(r\"arith.constant .* : \" + dtype_str, h.asm[\"ttir\"]) is not None\n \n # TODO: uncomment once DotOperandEncoding::getElemsPerThread is implemented\n # @pytest.mark.parametrize(\"dtype_str\", ['float32', 'float16'])\n-# def test_dot_without_load(dtype_str):\n+# def test_dot_without_load(dtype_str, device=device):\n #     @triton.jit\n #     def _kernel(out):\n #         a = GENERATE_TEST_HERE\n@@ -2082,10 +2069,10 @@ def kernel(out_ptr):\n #         tl.store(out_ptr, c)\n \n #     kernel = patch_kernel(_kernel, {'GENERATE_TEST_HERE': f\"tl.full((32, 32), 1.0, tl.{dtype_str})\"})\n-#     a = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n-#     b = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n+#     a = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=device)\n+#     b = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=device)\n #     out_ref = torch.matmul(a, b)\n-#     out = torch.zeros((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n+#     out = torch.zeros((32, 32), dtype=getattr(torch, dtype_str), device=device)\n #     kernel[(1,)](out)\n #     assert torch.all(out == out_ref)\n \n@@ -2095,7 +2082,7 @@ def kernel(out_ptr):\n \n \n @pytest.mark.parametrize(\"start\", [0, 1, 7, 16])\n-def test_arange(start, device='cuda'):\n+def test_arange(start, device):\n     BLOCK = 128\n     z_tri = torch.empty(BLOCK, dtype=torch.int32, device=device)\n \n@@ -2115,9 +2102,9 @@ def _kernel(z, BLOCK: tl.constexpr,\n \n \n @pytest.mark.parametrize(\"dtype_str, size, size_diff\", [(dtype_str, size, size_diff) for dtype_str in torch_dtypes for size in [128, 512] for size_diff in [0, 1, 2, 3, 4]])\n-def test_masked_load(dtype_str, size, size_diff, device='cuda'):\n+def test_masked_load(dtype_str, size, size_diff, device):\n     dtype = getattr(torch, dtype_str)\n-    check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n+    check_type_supported(dtype, device)  # bfloat16 on cc < 80 will not be tested\n \n     input_size = size - size_diff\n     output_size = size\n@@ -2150,8 +2137,8 @@ def _kernel(in_ptr, out_ptr, in_size: tl.constexpr, out_size: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"dtype\", [torch.bfloat16, torch.float16, torch.float32])\n-def test_masked_load_shared_memory(dtype, device='cuda'):\n-    check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n+def test_masked_load_shared_memory(dtype, device):\n+    check_type_supported(dtype, device)  # bfloat16 on cc < 80 will not be tested\n \n     M = 32\n     N = 32\n@@ -2199,9 +2186,9 @@ def _kernel(in1_ptr, in2_ptr, output_ptr,\n \n \n @pytest.mark.parametrize(\"cache\", [\"\", \".ca\", \".cg\"])\n-def test_load_cache_modifier(cache):\n-    src = torch.empty(128, device='cuda')\n-    dst = torch.empty(128, device='cuda')\n+def test_load_cache_modifier(cache, device):\n+    src = torch.empty(128, device=device)\n+    dst = torch.empty(128, device=device)\n \n     @triton.jit\n     def _kernel(dst, src, CACHE: tl.constexpr):\n@@ -2223,9 +2210,9 @@ def _kernel(dst, src, CACHE: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"N\", [16, 10, 11, 1024])\n-def test_vectorization(N):\n-    src = torch.empty(1024, device='cuda')\n-    dst = torch.empty(1024, device='cuda')\n+def test_vectorization(N, device):\n+    src = torch.empty(1024, device=device)\n+    dst = torch.empty(1024, device=device)\n \n     @triton.jit\n     def _kernel(dst, src, N, BLOCK_SIZE: tl.constexpr):\n@@ -2242,10 +2229,10 @@ def _kernel(dst, src, N, BLOCK_SIZE: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"has_hints\", [False, True])\n-def test_vectorization_hints(has_hints):\n-    src = torch.empty(1024, device='cuda')\n-    dst = torch.empty(1024, device='cuda')\n-    off = torch.zeros(1, device='cuda', dtype=torch.int32)\n+def test_vectorization_hints(has_hints, device):\n+    src = torch.empty(1024, device=device)\n+    dst = torch.empty(1024, device=device)\n+    off = torch.zeros(1, device=device, dtype=torch.int32)\n \n     @triton.jit\n     def _kernel(dst, src, off, N, BLOCK_SIZE: tl.constexpr, HINT: tl.constexpr):\n@@ -2320,10 +2307,10 @@ def _impl(value=10):\n     return value\n \n \n-def test_default():\n+def test_default(device):\n     value = 5\n-    ret0 = torch.zeros(1, dtype=torch.int32, device='cuda')\n-    ret1 = torch.zeros(1, dtype=torch.int32, device='cuda')\n+    ret0 = torch.zeros(1, dtype=torch.int32, device=device)\n+    ret1 = torch.zeros(1, dtype=torch.int32, device=device)\n \n     @triton.jit\n     def _kernel(ret0, ret1, value):\n@@ -2339,7 +2326,7 @@ def _kernel(ret0, ret1, value):\n # ----------------\n \n \n-def test_noop(device='cuda'):\n+def test_noop(device):\n     @triton.jit\n     def kernel(x):\n         pass\n@@ -2366,7 +2353,7 @@ def kernel(x):\n     (2**31, 'i64'), (2**32 - 1, 'i64'), (2**32, 'i64'), (2**63 - 1, 'i64'),\n     (-2**63, 'i64'), (2**63, 'u64'), (2**64 - 1, 'u64')\n ])\n-def test_value_specialization(value: int, value_type: str, device='cuda') -> None:\n+def test_value_specialization(value: int, value_type: str, device) -> None:\n     spec_type = None\n \n     def cache_hook(*args, **kwargs):\n@@ -2378,7 +2365,7 @@ def cache_hook(*args, **kwargs):\n     def kernel(VALUE, X):\n         pass\n \n-    x = torch.tensor([3.14159], device='cuda')\n+    x = torch.tensor([3.14159], device=device)\n     pgm = kernel[(1, )](value, x)\n \n     JITFunction.cache_hook = None\n@@ -2393,13 +2380,13 @@ def kernel(VALUE, X):\n     \"value, overflow\",\n     [(2**64 - 1, False), (2**64, True), (-2**63, False), (-2**63 - 1, True)]\n )\n-def test_value_specialization_overflow(value: int, overflow: bool, device='cuda') -> None:\n+def test_value_specialization_overflow(value: int, overflow: bool, device) -> None:\n \n     @triton.jit\n     def kernel(VALUE, X):\n         pass\n \n-    x = torch.tensor([3.14159], device='cuda')\n+    x = torch.tensor([3.14159], device=device)\n \n     if overflow:\n         with pytest.raises(OverflowError):\n@@ -2415,7 +2402,7 @@ def kernel(VALUE, X):\n @pytest.mark.parametrize(\"op\", ['+', '-', '*', '/', '%', '<', '>', '<<', '>>', '&', '^', '|'])\n @pytest.mark.parametrize(\"is_lhs_constexpr\", [False, True])\n @pytest.mark.parametrize(\"is_rhs_constexpr\", [True, False])\n-def test_bin_op_constexpr(op, is_lhs_constexpr, is_rhs_constexpr):\n+def test_bin_op_constexpr(op, is_lhs_constexpr, is_rhs_constexpr, device):\n \n     @triton.jit\n     def kernel(Z, X, Y):\n@@ -2436,34 +2423,34 @@ def kernel(Z, X, Y):\n         y = numpy_random((1,), dtype_str=\"float32\")\n     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f\"{x_str} {op} {y_str}\"})\n     z = np.array(eval(f\"{x_str} {op} {y_str}\"))\n-    x_tri = to_triton(x)\n-    y_tri = to_triton(y)\n-    z_tri = to_triton(np.empty((1,), dtype=z.dtype))\n+    x_tri = to_triton(x, device=device)\n+    y_tri = to_triton(y, device=device)\n+    z_tri = to_triton(np.empty((1,), dtype=z.dtype), device=device)\n     kernel[(1,)](z_tri, x_tri, y_tri)\n     np.testing.assert_allclose(z, to_numpy(z_tri))\n \n \n-def test_constexpr_shape():\n+def test_constexpr_shape(device):\n \n     @triton.jit\n     def kernel(X):\n         off = tl.arange(0, 128 + 128)\n         tl.store(X + off, off)\n \n-    x_tri = to_triton(np.empty((256, ), dtype=np.int32))\n+    x_tri = to_triton(np.empty((256, ), dtype=np.int32), device=device)\n     kernel[(1,)](x_tri)\n     np.testing.assert_equal(to_numpy(x_tri), np.arange(0, 256))\n \n \n-def test_constexpr_scalar_shape():\n+def test_constexpr_scalar_shape(device):\n \n     @triton.jit\n     def kernel(X, s):\n         off = tl.arange(0, 256)\n         val = off % (256 // s)\n         tl.store(X + off, val)\n \n-    x_tri = to_triton(np.empty((256, ), dtype=np.int32))\n+    x_tri = to_triton(np.empty((256, ), dtype=np.int32), device=device)\n     kernel[(1,)](x_tri, 32)\n     np.testing.assert_equal(to_numpy(x_tri), np.arange(0, 256) % 8)\n \n@@ -2497,7 +2484,7 @@ def vecmul_kernel(ptr, n_elements, rep, type: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"type\", [\"inline\", \"noinline\"])\n-def test_call(type):\n+def test_call(type, device):\n \n     @triton.jit\n     def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n@@ -2506,7 +2493,7 @@ def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n \n     size = 1024\n     rand_val = numpy_random((size,), dtype_str=\"float32\")\n-    rand_val_tri = to_triton(rand_val, device='cuda')\n+    rand_val_tri = to_triton(rand_val, device=device)\n     err_msg = \"\"\n     try:\n         kernel[(size // 128,)](rand_val_tri, size, 3, 5, type)\n@@ -2524,8 +2511,8 @@ def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n # -------------\n \n \n-@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and_dynamic\", \"if_and_static\"])\n-def test_if(if_type):\n+@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and\"])\n+def test_if(if_type, device):\n \n     @triton.jit\n     def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr, StaticVaue: tl.constexpr):\n@@ -2549,16 +2536,17 @@ def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr\n             else:\n                 tl.store(Ret, tl.load(XFalse))\n \n-    cond = torch.ones(1, dtype=torch.int32, device='cuda')\n-    x_true = torch.tensor([3.14], dtype=torch.float32, device='cuda')\n-    x_false = torch.tensor([1.51], dtype=torch.float32, device='cuda')\n-    ret = torch.empty(1, dtype=torch.float32, device='cuda')\n+    cond = torch.ones(1, dtype=torch.int32, device=device)\n+    x_true = torch.tensor([3.14], dtype=torch.float32, device=device)\n+    x_false = torch.tensor([1.51], dtype=torch.float32, device=device)\n+    ret = torch.empty(1, dtype=torch.float32, device=device)\n+\n     kernel[(1,)](cond, x_true, x_false, ret, if_type, True, 1)\n     assert torch.equal(ret, x_true)\n \n \n-def test_num_warps_pow2():\n-    dst = torch.empty(128, device='cuda')\n+def test_num_warps_pow2(device):\n+    dst = torch.empty(128, device=device)\n \n     @triton.jit\n     def _kernel(dst):\n@@ -2582,7 +2570,7 @@ def _kernel(dst):\n                           ('float32', 'math.pow', tl.math.libdevice_path()),\n                           ('float64', 'math.pow_dtype', tl.math.libdevice_path()),\n                           ('float64', 'math.norm4d', '')])\n-def test_math_tensor(dtype_str, expr, lib_path):\n+def test_math_tensor(dtype_str, expr, lib_path, device):\n \n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n@@ -2623,9 +2611,9 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{expr}(x, x, x, x)'})\n         y_ref = np.sqrt(4 * np.power(x, 2))\n \n-    x_tri = to_triton(x)\n+    x_tri = to_triton(x, device=device)\n     # triton result\n-    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n+    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device=device)\n     kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n     # compare\n     if expr == 'math.ffs':\n@@ -2638,7 +2626,7 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n                          [('float32', 'math.pow', ''),\n                           ('float64', 'math.pow_dtype', ''),\n                           ('float64', 'math.pow', tl.math.libdevice_path())])\n-def test_math_scalar(dtype_str, expr, lib_path):\n+def test_math_scalar(dtype_str, expr, lib_path, device):\n \n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n@@ -2663,8 +2651,8 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n         y_ref[:] = np.power(x, 0.5)\n \n     # triton result\n-    x_tri = to_triton(x)[0].item()\n-    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n+    x_tri = to_triton(x, device=device)[0].item()\n+    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device=device)\n     kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n     # compare\n     np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n@@ -2677,7 +2665,7 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n @pytest.mark.parametrize(\"lo, hi, iv\", [(2**35, 2**35 + 20, 1), (2**35, 2**35 + 20, 2), (2**35, 2**35 + 20, 3),\n                                         (15, -16, -1), (15, -16, -2), (15, -16, -3),\n                                         (-18, -22, -1), (22, 18, -1)])\n-def test_for_iv(lo, hi, iv):\n+def test_for_iv(lo, hi, iv, device):\n \n     @triton.jit\n     def kernel(Out, lo, hi, iv: tl.constexpr):\n@@ -2689,12 +2677,12 @@ def kernel(Out, lo, hi, iv: tl.constexpr):\n \n     lo = 2**35\n     hi = 2**35 + 20\n-    out = to_triton(np.zeros((1,), dtype=np.int64), device='cuda')\n+    out = to_triton(np.zeros((1,), dtype=np.int64), device=device)\n     kernel[(1,)](out, lo, hi, iv)\n     assert out[0] == sum(range(lo, hi, iv))\n \n \n-def test_if_else():\n+def test_if_else(device):\n \n     @triton.jit\n     def kernel(Cond, TrueVal, FalseVal, Out):\n@@ -2704,10 +2692,10 @@ def kernel(Cond, TrueVal, FalseVal, Out):\n             val = tl.load(FalseVal)\n         tl.store(Out, val)\n \n-    out = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n-    true_val = to_triton(np.full((1,), 1, dtype=np.int32), device='cuda')\n-    false_val = to_triton(np.full((1,), 2, dtype=np.int32), device='cuda')\n-    cond = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    out = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n+    true_val = to_triton(np.full((1,), 1, dtype=np.int32), device=device)\n+    false_val = to_triton(np.full((1,), 2, dtype=np.int32), device=device)\n+    cond = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n     # True\n     cond[0] = True\n     kernel[(1,)](cond, true_val, false_val, out)\n@@ -2719,7 +2707,7 @@ def kernel(Cond, TrueVal, FalseVal, Out):\n \n \n @pytest.mark.parametrize(\"mode\", [\"dynamic\", \"static\"])\n-def test_if_return(mode):\n+def test_if_return(mode, device):\n \n     @triton.jit\n     def kernel(ExitEarly, Out, cond: tl.constexpr, mode: tl.constexpr):\n@@ -2733,8 +2721,8 @@ def kernel(ExitEarly, Out, cond: tl.constexpr, mode: tl.constexpr):\n                 return\n         tl.store(Out, 1)\n \n-    out = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n-    exit_early = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    out = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n+    exit_early = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n     # exit early path taken\n     exit_early[0] = 1\n     kernel[(1,)](exit_early, out, True, mode)\n@@ -2779,7 +2767,7 @@ def add_fn_static_cond(x, cond: tl.constexpr):\n @pytest.mark.parametrize(\"call_type\", [\"attribute\", \"attribute_jit\",\n                                        \"jit\", \"jit_if\", \"jit_ifexp\", \"jit_expr\",\n                                        \"jit_static_cond\", \"jit_noinline\", \"jit_extern\"])\n-def test_if_call(call_type):\n+def test_if_call(call_type, device):\n     @triton.jit\n     def kernel(Out, call_type: tl.constexpr):\n         pid = tl.program_id(0)\n@@ -2838,15 +2826,15 @@ def kernel(Out, call_type: tl.constexpr):\n \n         tl.store(Out, o)\n \n-    out = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    out = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n     kernel[(1,)](out, call_type)\n     assert to_numpy(out)[0] == 1\n \n \n @pytest.mark.parametrize(\"_cond1\", [True, False])\n @pytest.mark.parametrize(\"_cond2\", [True, False])\n @pytest.mark.parametrize(\"_cond3\", [True, False])\n-def test_nested_if_else_return(_cond1, _cond2, _cond3):\n+def test_nested_if_else_return(_cond1, _cond2, _cond3, device):\n \n     @triton.jit\n     def kernel(Cond1, Cond2, Cond3, Val1, Val2, Val3, Out):\n@@ -2863,13 +2851,13 @@ def kernel(Cond1, Cond2, Cond3, Val1, Val2, Val3, Out):\n                 val = tl.load(Val3)\n         tl.store(Out, val)\n \n-    out = to_triton(np.full((1,), -1, dtype=np.int32), device='cuda')\n-    cond1 = to_triton(np.full((1,), _cond1, dtype=np.int32), device='cuda')\n-    cond2 = to_triton(np.full((1,), _cond2, dtype=np.int32), device='cuda')\n-    cond3 = to_triton(np.full((1,), _cond3, dtype=np.int32), device='cuda')\n-    val1 = to_triton(np.full((1,), 1, dtype=np.int32), device='cuda')\n-    val2 = to_triton(np.full((1,), 2, dtype=np.int32), device='cuda')\n-    val3 = to_triton(np.full((1,), 3, dtype=np.int32), device='cuda')\n+    out = to_triton(np.full((1,), -1, dtype=np.int32), device=device)\n+    cond1 = to_triton(np.full((1,), _cond1, dtype=np.int32), device=device)\n+    cond2 = to_triton(np.full((1,), _cond2, dtype=np.int32), device=device)\n+    cond3 = to_triton(np.full((1,), _cond3, dtype=np.int32), device=device)\n+    val1 = to_triton(np.full((1,), 1, dtype=np.int32), device=device)\n+    val2 = to_triton(np.full((1,), 2, dtype=np.int32), device=device)\n+    val3 = to_triton(np.full((1,), 3, dtype=np.int32), device=device)\n     kernel[(1,)](cond1, cond2, cond3, val1, val2, val3, out)\n     targets = {\n         (True, True, True): val1[0],\n@@ -2884,7 +2872,7 @@ def kernel(Cond1, Cond2, Cond3, Val1, Val2, Val3, Out):\n     assert out[0] == targets[(_cond1, _cond2, _cond3)]\n \n \n-def test_while():\n+def test_while(device):\n \n     @triton.jit\n     def kernel(InitI, Bound, CutOff, OutI, OutJ):\n@@ -2897,16 +2885,16 @@ def kernel(InitI, Bound, CutOff, OutI, OutJ):\n         tl.store(OutI, curr_i)\n         tl.store(OutJ, j)\n \n-    out_i = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n-    out_j = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n-    init_i = to_triton(np.full((1,), 1, dtype=np.int32), device='cuda')\n-    bound = to_triton(np.full((1,), 10, dtype=np.int32), device='cuda')\n-    cut_off = to_triton(np.full((1,), 5, dtype=np.int32), device='cuda')\n+    out_i = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n+    out_j = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n+    init_i = to_triton(np.full((1,), 1, dtype=np.int32), device=device)\n+    bound = to_triton(np.full((1,), 10, dtype=np.int32), device=device)\n+    cut_off = to_triton(np.full((1,), 5, dtype=np.int32), device=device)\n     kernel[(1,)](init_i, bound, cut_off, out_i, out_j)\n     assert out_i[0] == init_i[0] + 1\n     assert out_j[0] == cut_off[0] + 1\n \n-# def test_for_if():\n+# def test_for_if(device):\n \n #     @triton.jit\n #     def kernel(bound, cutoff, M, N):\n@@ -2920,8 +2908,8 @@ def kernel(InitI, Bound, CutOff, OutI, OutJ):\n #         tl.store(M, m)\n #         tl.store(N, n)\n \n-#     m = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n-#     n = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+#     m = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n+#     n = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n #     kernel[(1,)](10, 7, m, n)\n #     print(m[0])\n #     print(n[0])\n@@ -2931,7 +2919,8 @@ def kernel(InitI, Bound, CutOff, OutI, OutJ):\n # -----------------------\n \n \n-def test_globaltimer():\n+def test_globaltimer(device):\n+    check_cuda_only(device)\n \n     @triton.jit\n     def kernel(Out1, Out2):\n@@ -2942,21 +2931,22 @@ def kernel(Out1, Out2):\n         end = tl.extra.cuda.globaltimer()\n         tl.store(Out2, end - start)\n \n-    out1 = to_triton(np.zeros((128,), dtype=np.int64), device='cuda')\n-    out2 = to_triton(np.zeros((1,), dtype=np.int64), device='cuda')\n+    out1 = to_triton(np.zeros((128,), dtype=np.int64), device=device)\n+    out2 = to_triton(np.zeros((1,), dtype=np.int64), device=device)\n     h = kernel[(1,)](out1, out2)\n     assert out2[0] > 0\n     # 2 inlined globaltimers + one extra in the wrapper extern function\n     assert h.asm[\"ptx\"].count(\"%globaltimer\") == 3\n \n \n-def test_smid():\n+def test_smid(device):\n+    check_cuda_only(device)\n \n     @triton.jit\n     def kernel(Out):\n         tl.store(Out + tl.program_id(0), tl.extra.cuda.smid())\n \n-    out = to_triton(np.zeros((1024,), dtype=np.int32), device='cuda')\n+    out = to_triton(np.zeros((1024,), dtype=np.int32), device=device)\n     h = kernel[(out.shape[0],)](out)\n     assert out.sort()[0].unique().shape[0] > 0\n     assert h.asm[\"ptx\"].count(\"%smid\") == 2\n@@ -2994,7 +2984,7 @@ def kernel(Out):\n @pytest.mark.parametrize(\"src_layout\", layouts)\n @pytest.mark.parametrize(\"interm_layout\", intermediate_layouts)\n @pytest.mark.parametrize(\"dst_layout\", layouts)\n-def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device='cuda'):\n+def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device):\n     if str(src_layout) == str(dst_layout):\n         pytest.skip()\n     if 'mma' in str(src_layout) and 'mma' in str(dst_layout):\n@@ -3046,7 +3036,7 @@ def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device='\n }\n \"\"\"\n \n-    x = to_triton(numpy_random(shape, dtype_str=dtype))\n+    x = to_triton(numpy_random(shape, dtype_str=dtype), device=device)\n     z = torch.empty_like(x)\n \n     # write the IR to a temporary file using mkstemp\n@@ -3060,23 +3050,23 @@ def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device='\n     assert torch.equal(z, x)\n \n \n-def test_load_scalar_with_mask():\n+def test_load_scalar_with_mask(device):\n     @triton.jit\n     def kernel(Input, Index, Out, N: int):\n         index = tl.load(Index)\n         scalar = tl.load(Input + index, mask=index < N, other=0)\n         tl.store(Out, scalar, mask=index < N)\n-    Index = torch.tensor([0], dtype=torch.int32, device='cuda')\n-    Input = torch.tensor([0], dtype=torch.int32, device='cuda')\n-    Out = torch.empty_like(Index, device='cuda')\n+    Index = torch.tensor([0], dtype=torch.int32, device=device)\n+    Input = torch.tensor([0], dtype=torch.int32, device=device)\n+    Out = torch.empty_like(Index, device=device)\n     kernel[(1,)](Input, Index, Out, Index.numel())\n     assert Out.data[0] == 0\n \n \n # This test is used to test our own PTX codegen for float16 and int16 conversions\n # maybe delete it later after ptxas has been fixed\n @pytest.mark.parametrize(\"dtype_str\", ['float16', 'int16'])\n-def test_ptx_cast(dtype_str):\n+def test_ptx_cast(dtype_str, device):\n     @triton.jit\n     def kernel(in_ptr0, out_ptr2, xnumel, rnumel, dtype: tl.constexpr, XBLOCK: tl.constexpr, RBLOCK: tl.constexpr):\n         xoffset = tl.program_id(0) * XBLOCK\n@@ -3106,7 +3096,7 @@ def kernel(in_ptr0, out_ptr2, xnumel, rnumel, dtype: tl.constexpr, XBLOCK: tl.co\n         triton_dtype = tl.float32\n \n     s0 = 4\n-    buf11 = -torch.ones((6 * s0, 197, 197), device='cuda', dtype=torch_dtype)\n-    buf14 = -torch.ones((s0, 6, 197, 197), device='cuda', dtype=torch_dtype)\n+    buf11 = -torch.ones((6 * s0, 197, 197), device=device, dtype=torch_dtype)\n+    buf14 = -torch.ones((s0, 6, 197, 197), device=device, dtype=torch_dtype)\n     kernel[(4728,)](buf11, buf14, 1182 * s0, 197, triton_dtype, 1, 256, num_warps=2)\n     assert buf14.to(torch.float32).mean() == -2.0"}, {"filename": "python/test/unit/language/test_random.py", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -115,7 +115,7 @@ def random_raw(self):\n                          [(size, seed) for size in ['10', '4,53', '10000']\n                           for seed in [0, 42, 124, 54, 0xffffffff, 0xdeadbeefcafeb0ba]]\n                          )\n-def test_randint(size, seed, device='cuda'):\n+def test_randint(size, seed, device):\n     size = list(map(int, size.split(',')))\n \n     @triton.jit\n@@ -141,7 +141,7 @@ def kernel(X, N, seed):\n                          [(size, seed) for size in [1000000]\n                           for seed in [0, 42, 124, 54]]\n                          )\n-def test_rand(size, seed, device='cuda'):\n+def test_rand(size, seed, device):\n     @triton.jit\n     def kernel(X, N, seed):\n         offset = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)\n@@ -162,7 +162,7 @@ def kernel(X, N, seed):\n                          [(size, seed) for size in [1000000]\n                           for seed in [0, 42, 124, 54]]\n                          )\n-def test_randn(size, seed, device='cuda'):\n+def test_randn(size, seed, device):\n     @triton.jit\n     def kernel(X, N, seed):\n         offset = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)\n@@ -179,7 +179,7 @@ def kernel(X, N, seed):\n \n # tl.rand() should never produce >=1.0\n \n-def test_rand_limits():\n+def test_rand_limits(device):\n     @triton.jit\n     def kernel(input, output, n: tl.constexpr):\n         idx = tl.arange(0, n)\n@@ -190,8 +190,8 @@ def kernel(input, output, n: tl.constexpr):\n     min_max_int32 = torch.tensor([\n         torch.iinfo(torch.int32).min,\n         torch.iinfo(torch.int32).max,\n-    ], dtype=torch.int32, device='cuda')\n-    output = torch.empty(2, dtype=torch.float32, device='cuda')\n+    ], dtype=torch.int32, device=device)\n+    output = torch.empty(2, dtype=torch.float32, device=device)\n     kernel[(1,)](min_max_int32, output, 2)\n \n     assert output[0] == output[1]"}, {"filename": "python/triton/common/backend.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -85,10 +85,10 @@ def register_backend(device_type: str, backend_cls: type):\n \n def get_backend(device_type: str):\n     if device_type not in _backends:\n-        device_backend_package_name = f\"triton.third_party.{device_type}\"\n-        if importlib.util.find_spec(device_backend_package_name):\n+        device_backend_package_name = f\"...third_party.{device_type}\"\n+        if importlib.util.find_spec(device_backend_package_name, package=__spec__.name):\n             try:\n-                importlib.import_module(device_backend_package_name)\n+                importlib.import_module(device_backend_package_name, package=__spec__.name)\n             except Exception:\n                 return None\n         else:"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 10, "deletions": 5, "changes": 15, "file_content_changes": "@@ -189,10 +189,11 @@ def visit_Call(self, node: ast.Call) -> bool:\n \n \n class CodeGenerator(ast.NodeVisitor):\n-    def __init__(self, context, prototype, gscope, attributes, constants, function_name,\n+    def __init__(self, context, prototype, gscope, attributes, constants, function_name, arch,\n                  module=None, is_kernel=False, function_types: Optional[Dict] = None,\n                  debug=False, noinline=False):\n         self.builder = ir.builder(context)\n+        self.builder.arch = arch\n         self.module = self.builder.create_module() if module is None else module\n         self.function_ret_types = {} if function_types is None else function_types\n         self.prototype = prototype\n@@ -868,7 +869,9 @@ def call_JitFunction(self, fn: JITFunction, args, kwargs):\n             gscope = sys.modules[fn.fn.__module__].__dict__\n             # If the callee is not set, we use the same debug setting as the caller\n             debug = self.debug if fn.debug is None else fn.debug\n-            generator = CodeGenerator(self.builder.context, prototype, gscope, attributes, constants, module=self.module, function_name=fn_name, function_types=self.function_ret_types, debug=debug, noinline=fn.noinline)\n+            generator = CodeGenerator(self.builder.context, prototype, gscope, attributes, constants, module=self.module,\n+                                      function_name=fn_name, function_types=self.function_ret_types, debug=debug, noinline=fn.noinline,\n+                                      arch=self.builder.arch)\n             generator.visit(fn.parse())\n             callee_ret_type = generator.last_ret_type\n             self.function_ret_types[fn_name] = callee_ret_type\n@@ -1016,8 +1019,9 @@ def str_to_ty(name):\n         ty = str_to_ty(name[1:])\n         return language.pointer_type(ty)\n     tys = {\n-        \"fp8e5\": language.float8e5,\n         \"fp8e4\": language.float8e4,\n+        \"fp8e5\": language.float8e5,\n+        \"fp8e4b15\": language.float8e4b15,\n         \"fp16\": language.float16,\n         \"bf16\": language.bfloat16,\n         \"fp32\": language.float32,\n@@ -1049,7 +1053,7 @@ def kernel_suffix(signature, specialization):\n     return suffix\n \n \n-def ast_to_ttir(fn, signature, specialization, constants, debug):\n+def ast_to_ttir(fn, signature, specialization, constants, debug, arch):\n     # canonicalize signature\n     if isinstance(signature, str):\n         signature = {k: v.strip() for k, v in enumerate(signature.split(\",\"))}\n@@ -1071,7 +1075,8 @@ def ast_to_ttir(fn, signature, specialization, constants, debug):\n     prototype = language.function_type([], arg_types)\n     generator = CodeGenerator(context, prototype, gscope=gscope, constants=all_constants,\n                               function_name=function_name, attributes=new_attrs,\n-                              is_kernel=True, debug=debug)\n+                              is_kernel=True, debug=debug,\n+                              arch=arch)\n     try:\n         generator.visit(fn.parse())\n     except CompilationError as e:"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -11,7 +11,6 @@\n from pathlib import Path\n from typing import Any, Tuple\n \n-# import triton\n from .._C.libtriton.triton import (add_external_libs, compile_ptx_to_cubin,\n                                    get_shared_memory_size, ir,\n                                    translate_llvmir_to_hsaco, translate_llvmir_to_ptx,\n@@ -397,7 +396,7 @@ def compile(fn, **kwargs):\n     stages = dict()\n     stages[\"ast\"] = (lambda path: fn, None)\n     stages[\"ttir\"] = (lambda path: parse_mlir_module(path, context),\n-                      lambda src: optimize_ttir(ast_to_ttir(src, signature, configs[0], constants, debug=debug), arch))\n+                      lambda src: optimize_ttir(ast_to_ttir(src, signature, configs[0], constants, debug=debug, arch=arch), arch))\n     stages[\"ttgir\"] = (lambda path: parse_mlir_module(path, context),\n                        lambda src: optimize_ttgir(ttir_to_ttgir(src, num_warps), num_stages, arch))\n     stages[\"llir\"] = (lambda path: Path(path).read_text(),"}, {"filename": "python/triton/interpreter/memory_map.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -2,7 +2,7 @@\n \n import dataclasses\n \n-from triton.interpreter import torch_wrapper\n+from . import torch_wrapper\n \n torch = torch_wrapper.torch\n "}, {"filename": "python/triton/interpreter/tl_lang.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,6 +1,5 @@\n from __future__ import annotations\n \n-# import triton\n from ..language import core as lcore\n from . import torch_wrapper\n from .core import ExecutionContext"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -45,6 +45,7 @@\n     float16,\n     float32,\n     float64,\n+    float8e4b15,\n     float8e4,\n     float8e5,\n     function_type,\n@@ -137,6 +138,7 @@\n     \"float16\",\n     \"float32\",\n     \"float64\",\n+    \"float8e4b15\",\n     \"float8e4\",\n     \"float8e5\",\n     \"full\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 21, "deletions": 3, "changes": 24, "file_content_changes": "@@ -6,7 +6,6 @@\n from typing import Callable, List, Sequence, TypeVar\n \n from .._C.libtriton.triton import ir\n-# import triton\n from ..runtime.jit import jit\n from . import semantic\n \n@@ -77,7 +76,7 @@ def _to_tensor(x, builder):\n class dtype:\n     SINT_TYPES = ['int8', 'int16', 'int32', 'int64']\n     UINT_TYPES = ['int1', 'uint8', 'uint16', 'uint32', 'uint64']\n-    FP_TYPES = ['fp8e4', 'fp8e5', 'fp16', 'bf16', 'fp32', 'fp64']\n+    FP_TYPES = ['fp8e4b15', 'fp8e4', 'fp8e5', 'fp16', 'bf16', 'fp32', 'fp64']\n     STANDARD_FP_TYPES = ['fp16', 'bf16', 'fp32', 'fp64']\n     OTHER_TYPES = ['void']\n \n@@ -97,24 +96,34 @@ def __init__(self, name):\n             self.int_bitwidth = int(name.split('int')[-1])\n             self.primitive_bitwidth = self.int_bitwidth\n         elif name in dtype.FP_TYPES:\n-            if name == 'fp8e4':\n+            if name == 'fp8e4b15':\n                 self.fp_mantissa_width = 3\n                 self.primitive_bitwidth = 8\n+                self.exponent_bias = 15\n+            elif name == 'fp8e4':\n+                self.fp_mantissa_width = 3\n+                self.primitive_bitwidth = 8\n+                self.exponent_bias = 7\n             elif name == 'fp8e5':\n                 self.fp_mantissa_width = 2\n                 self.primitive_bitwidth = 8\n+                self.exponent_bias = 15\n             elif name == 'fp16':\n                 self.fp_mantissa_width = 10\n                 self.primitive_bitwidth = 16\n+                self.exponent_bias = 15\n             elif name == 'bf16':\n                 self.fp_mantissa_width = 7\n                 self.primitive_bitwidth = 16\n+                self.exponent_bias = 127\n             elif name == 'fp32':\n                 self.fp_mantissa_width = 23\n                 self.primitive_bitwidth = 32\n+                self.exponent_bias = 127\n             elif name == 'fp64':\n                 self.fp_mantissa_width = 53\n                 self.primitive_bitwidth = 64\n+                self.exponent_bias = 1023\n             else:\n                 raise RuntimeError(f'Unsupported floating-point type {name}')\n         elif name == 'void':\n@@ -123,6 +132,12 @@ def __init__(self, name):\n     def is_fp8(self):\n         return 'fp8' in self.name\n \n+    def is_fp8e4(self):\n+        return self.name == 'fp8e4'\n+\n+    def is_fp8e4b15(self):\n+        return self.name == 'fp8e4b15'\n+\n     def is_fp16(self):\n         return self.name == 'fp16'\n \n@@ -224,6 +239,8 @@ def to_ir(self, builder: ir.builder) -> ir.type:\n             return builder.get_fp8e5_ty()\n         elif self.name == 'fp8e4':\n             return builder.get_fp8e4_ty()\n+        elif self.name == 'fp8e4b15':\n+            return builder.get_fp8e4b15_ty()\n         elif self.name == 'fp16':\n             return builder.get_half_ty()\n         elif self.name == 'bf16':\n@@ -357,6 +374,7 @@ def to_ir(self, builder: ir.builder):\n uint64 = dtype('uint64')\n float8e5 = dtype('fp8e5')\n float8e4 = dtype('fp8e4')\n+float8e4b15 = dtype('fp8e4b15')\n float16 = dtype('fp16')\n bfloat16 = dtype('bf16')\n float32 = dtype('fp32')"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 12, "deletions": 1, "changes": 13, "file_content_changes": "@@ -1,10 +1,11 @@\n from __future__ import annotations  # remove after python 3.11\n \n+import warnings\n from functools import wraps\n from typing import List, Optional, Sequence, Tuple, TypeVar\n \n+from .._C.libtriton.triton import ir\n from . import core as tl\n-from triton._C.libtriton.triton import ir\n \n T = TypeVar('T')\n \n@@ -676,6 +677,16 @@ def cast(input: tl.tensor,\n     src_sca_ty = src_ty.scalar\n     dst_sca_ty = dst_ty.scalar\n \n+    if builder.arch < 89 and \\\n+       (src_sca_ty.is_fp8e4() or dst_sca_ty.is_fp8e4()):\n+        warnings.warn(\"Standard tl.float8e4 format will be deprecated on SM < 89. \"\n+                      \"Please use tl.float8e4b15.\", DeprecationWarning)\n+\n+    # Unsupported conversion:\n+    if (src_sca_ty.is_fp8e4b15() and not dst_sca_ty.is_fp16()) or \\\n+       (dst_sca_ty.is_fp8e4b15() and not src_sca_ty.is_fp16()):\n+        raise ValueError('fp8e4b15 can only be converted to/from fp16')\n+\n     # Casting with customized floating types involved: fp8 <=> bf16, fp16, fp32, fp64\n     if (src_sca_ty.is_fp8() and dst_sca_ty.is_floating()) or \\\n        (src_sca_ty.is_floating() and dst_sca_ty.is_fp8()):"}, {"filename": "python/triton/ops/blocksparse/matmul.py", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -3,9 +3,6 @@\n from ... import cdiv, heuristics, jit\n from ... import language as tl\n \n-# import triton\n-# import language as tl\n-\n # ********************************************************\n # --------------------------------------------------------\n # Sparse = Dense x Dense (SDD)"}, {"filename": "python/triton/ops/blocksparse/softmax.py", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -1,7 +1,5 @@\n import torch\n \n-# import triton\n-# import language as tl\n from ... import jit\n from ... import language as tl\n from ... import next_power_of_2"}, {"filename": "python/triton/ops/cross_entropy.py", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -1,7 +1,5 @@\n import torch\n \n-# import triton\n-# import language as tl\n from .. import heuristics, jit\n from .. import language as tl\n from .. import next_power_of_2"}, {"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -10,9 +10,6 @@\n from .. import cdiv, jit\n from .. import language as tl\n \n-# import triton\n-# import language as tl\n-\n \n @jit\n def _fwd_kernel("}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -4,9 +4,6 @@\n from .. import language as tl\n from .matmul_perf_model import early_config_prune, estimate_matmul_time\n \n-# import triton\n-# import language as tl\n-\n \n def init_to_zero(name):\n     return lambda nargs: nargs[name].zero_()"}, {"filename": "python/triton/ops/matmul_perf_model.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -2,7 +2,6 @@\n \n import torch\n \n-# import triton\n from .. import cdiv\n from .._C.libtriton.triton import runtime\n from ..runtime import driver"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 4, "deletions": 5, "changes": 9, "file_content_changes": "@@ -11,8 +11,6 @@\n from typing import (Callable, Generic, Iterable, List, Optional, TypeVar, Union, cast,\n                     overload)\n \n-# import triton\n-# from .. import compile, CompiledKernel\n from ..common.backend import get_backend\n \n TRITON_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n@@ -71,7 +69,7 @@ def visit_Attribute(self, node):\n         lhs = self.visit(node.value)\n         while isinstance(lhs, ast.Attribute):\n             lhs = self.visit(lhs.value)\n-        if lhs is None or getattr(lhs, \"__name__\", \"\") == \"triton\":\n+        if lhs is None or (getattr(lhs, \"__name__\", \"\") == \"triton\" or getattr(lhs, \"__name__\", \"\").endswith(\".triton\")):\n             return None\n         return getattr(lhs, node.attr)\n \n@@ -81,7 +79,7 @@ def visit_Call(self, node):\n             return\n         if inspect.isbuiltin(func):\n             return\n-        if func.__module__ and func.__module__.startswith('triton.'):\n+        if func.__module__ and (func.__module__.startswith('triton.') or '.triton.' in func.__module__):\n             return\n         assert isinstance(func, JITFunction), f\"Function \\\"{func.__name__}\\\" is being called from a Triton function but is not a Triton function itself. Decorate it with @triton.jit to fix this\"\n         if func.hash is None:\n@@ -210,8 +208,9 @@ def _type_of(key):\n         dtype_str = str(key).split(\".\")[-1]\n         tys = {\n             \"bool\": \"i1\",\n-            \"float8e5\": \"fp8e5\",\n             \"float8e4\": \"fp8e4\",\n+            \"float8e5\": \"fp8e5\",\n+            \"float8e4b15\": \"fp8e4b15\",\n             \"float16\": \"fp16\",\n             \"bfloat16\": \"bf16\",\n             \"float32\": \"fp32\","}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 65, "deletions": 0, "changes": 65, "file_content_changes": "@@ -432,3 +432,68 @@ tt.func @cross_iter_dep(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n   }\n   tt.return %119#0 : tensor<32x32xf32, #C>\n }\n+\n+// CHECK: tt.func @dep_arg_two_uses\n+// CHECK: tt.expand_dims\n+// CHECK: tt.expand_dims\n+// CHECK: tt.expand_dims %arg5\n+// CHECK-NEXT: tt.expand_dims %arg5\n+// CHECK: %[[PTR0:.*]] = tt.splat %arg6\n+// CHECK: %[[PTR1:.*]] = tt.addptr %[[PTR0]]\n+// CHECK-NEXT: tt.load %[[PTR1]]\n+tt.func @dep_arg_two_uses(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32},\n+                          %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32},\n+                          %arg2: !tt.ptr<f16> {tt.divisibility = 16 : i32}) -> tensor<128x128xf32, #C> {\n+  %23 = arith.constant 100 : index\n+  %c64 = arith.constant 64 : i64\n+  %56 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>\n+  %57 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>\n+  %58 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #BL}>>\n+  %83 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>\n+  %85 = tt.splat %c64 : (i64) -> tensor<1x32xi64, #AL>\n+  %86 = tt.splat %c64 : (i64) -> tensor<1x32xi64, #AL>\n+  %68 = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n+  %c32_index = arith.constant 32 : index\n+  %c32_i32 = arith.index_cast %c32_index : index to i32\n+  %80 = tt.splat %arg2 : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n+  %cst_6 = arith.constant dense<0.000000e+00> : tensor<32x128xf32, #BL>\n+  %88 = arith.truncf %cst_6 : tensor<32x128xf32, #BL> to tensor<32x128xf16, #BL>\n+  %cst = arith.constant dense<0.000000e+00> : tensor<128x128xf32, #C>\n+  %90 = tt.splat %c64 : (i64) -> tensor<32x128xi64, #BL>\n+  %92 = tt.addptr %arg1, %c32_i32 : !tt.ptr<i32>, i32\n+  %c0_index = arith.constant 0 : index\n+  %91:5 = scf.for %arg19 = %c0_index to %23 step %c32_index iter_args(%arg20 = %68, %arg21 = %83, %arg22 = %92, %arg23 = %cst, %arg24 = %80) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>, !tt.ptr<i32>, tensor<128x128xf32, #C>, tensor<32x128x!tt.ptr<f16>, #BL>)   {\n+    %1750 = arith.subi %23, %arg19 : index\n+    %175 = arith.index_cast %1750 : index to i32\n+    %176 = tt.splat %175 : (i32) -> tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>\n+    %177 = tt.splat %175 : (i32) -> tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #BL}>>\n+    %178 = \"triton_gpu.cmpi\"(%57, %176) <{predicate = 2 : i64}> : (tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>, tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>) -> tensor<32xi1, #triton_gpu.slice<{dim = 0, parent = #AL}>>\n+    %179 = \"triton_gpu.cmpi\"(%58, %177) <{predicate = 2 : i64}> : (tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #BL}>>, tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #BL}>>) -> tensor<32xi1, #triton_gpu.slice<{dim = 1, parent = #BL}>>\n+    %180 = tt.expand_dims %178 {axis = 0 : i32} : (tensor<32xi1, #triton_gpu.slice<{dim = 0, parent = #AL}>>) -> tensor<1x32xi1, #AL>\n+    %181 = tt.expand_dims %179 {axis = 1 : i32} : (tensor<32xi1, #triton_gpu.slice<{dim = 1, parent = #BL}>>) -> tensor<32x1xi1, #BL>\n+    %182 = tt.expand_dims %arg21 {axis = 0 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>) -> tensor<1x32xi32, #AL>\n+    %183 = tt.expand_dims %arg21 {axis = 0 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>) -> tensor<1x32xi32, #AL>\n+    %184 = arith.extsi %182 : tensor<1x32xi32, #AL> to tensor<1x32xi64, #AL>\n+    %185 = arith.extsi %183 : tensor<1x32xi32, #AL> to tensor<1x32xi64, #AL>\n+    %186 = arith.muli %184, %85 : tensor<1x32xi64, #AL>\n+    %187 = arith.muli %185, %86 : tensor<1x32xi64, #AL>\n+    %188 = tt.broadcast %186 : (tensor<1x32xi64, #AL>) -> tensor<128x32xi64, #AL>\n+    %189 = tt.broadcast %187 : (tensor<1x32xi64, #AL>) -> tensor<128x32xi64, #AL>\n+    %190 = tt.addptr %arg20, %188 : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi64, #AL>\n+    %191 = tt.addptr %arg20, %189 : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi64, #AL>\n+    %192 = tt.broadcast %180 : (tensor<1x32xi1, #AL>) -> tensor<128x32xi1, #AL>\n+    %193 = tt.load %191, %192 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n+    %194 = tt.splat %arg22 : (!tt.ptr<i32>) -> tensor<32x!tt.ptr<i32>, #triton_gpu.slice<{dim = 0, parent = #AL}>>\n+    %195 = tt.addptr %194, %56 : tensor<32x!tt.ptr<i32>, #triton_gpu.slice<{dim = 0, parent = #AL}>>, tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>\n+    %196 = tt.load %195 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>\n+    %197 = tt.addptr %arg22, %c32_i32 : !tt.ptr<i32>, i32\n+    %198 = tt.broadcast %181 : (tensor<32x1xi1, #BL>) -> tensor<32x128xi1, #BL>\n+    %199 = tt.load %arg24, %198, %88 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n+    %200 = triton_gpu.convert_layout %193 : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth = 2}>>\n+    %201 = triton_gpu.convert_layout %199 : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth = 2}>>\n+    %202 = tt.dot %200, %201, %arg23 {allowTF32 = true} : tensor<128x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth = 2}>> * tensor<32x128xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth = 2}>> -> tensor<128x128xf32, #C>\n+    %203 = tt.addptr %arg24, %90 : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi64, #BL>\n+    scf.yield %190, %196, %197, %202, %203 : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>, !tt.ptr<i32>, tensor<128x128xf32, #C>, tensor<32x128x!tt.ptr<f16>, #BL>\n+  }\n+  tt.return %91#3 : tensor<128x128xf32, #C>\n+}"}]