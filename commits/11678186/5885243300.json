[{"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 11, "deletions": 8, "changes": 19, "file_content_changes": "@@ -10,11 +10,11 @@ using ::mlir::triton::gpu::getTotalElemsPerThread;\n const std::string Fp16_to_Fp8E5M2 =\n     \"{                            \\n\"\n     \".reg .b32 a<2>;              \\n\"\n-    \"and.b32 a0, $1, 0xfffefffe;  \\n\"           // a0 &= 0xfffefffe\n-    \"and.b32 a1, $2, 0xfffefffe;  \\n\"           // (strip lowest bit)\n-    \"add.u32 a0, a0, 0x00800080;  \\n\"           // a0 += 0x00800080\n-    \"add.u32 a1, a1, 0x00800080;  \\n\"           // (round to nearest)\n-    \"prmt.b32 $0, a0, a1, 0x7531; \\n\\t\"         // output = a1a0\n+    \"and.b32 a0, $1, 0xfffefffe;  \\n\"   // a0 &= 0xfffefffe\n+    \"and.b32 a1, $2, 0xfffefffe;  \\n\"   // (strip lowest bit)\n+    \"add.u32 a0, a0, 0x00800080;  \\n\"   // a0 += 0x00800080\n+    \"add.u32 a1, a1, 0x00800080;  \\n\"   // (round to nearest)\n+    \"prmt.b32 $0, a0, a1, 0x7531; \\n\\t\" // output = a1a0\n     \"}\";\n \n const std::string Fp8E5M2_to_Fp16 = \"{                           \\n\"\n@@ -170,7 +170,8 @@ const std::string Fp8E4M3_to_Fp16 =\n     \"add.u32  b0, b0, a0;                   \\n\" // b0 = b0 + a0\n     \"add.u32  b1, b1, a1;                   \\n\" // (move sign to the left)\n     \"mad.lo.u32 $0, b0, 128, 0x20002000;    \\n\" // out0 = (b0 << 7) + 0x20002000\n-    \"mad.lo.u32 $1, b1, 128, 0x20002000;    \\n\" // (shift into position and bias exponent)\n+    \"mad.lo.u32 $1, b1, 128, 0x20002000;    \\n\" // (shift into position and bias\n+                                                // exponent)\n     \"}\";\n \n // Fp16 -> Fp8E4M3 (packed)\n@@ -180,7 +181,8 @@ const std::string Fp16_to_Fp8E4M3 =\n     \"and.b32 a0, $1, 0x7fff7fff;            \\n\" // a0 = input0 & 0x7fff7fff\n     \"and.b32 a1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n     \"mad.lo.u32 a0, a0, 2, 0x40804080;      \\n\" // shift exponent (<< 1),\n-    \"mad.lo.u32 a1, a1, 2, 0x40804080;      \\n\" // correct bias (0x40004000), and round to nearest\n+    \"mad.lo.u32 a1, a1, 2, 0x40804080;      \\n\" // correct bias (0x40004000),\n+                                                // and round to nearest\n     \"lop3.b32 b0, $1, 0x80008000, a0, 0xe2; \\n\" // b0 = 0x80008000 ? in0 : a0\n     \"lop3.b32 b1, $2, 0x80008000, a1, 0xe2; \\n\" // (restore sign)\n     \"prmt.b32 $0, b0, b1, 0x7531;           \\n\" // output = b1b0\n@@ -197,7 +199,8 @@ const std::string Fp8E4M3_to_Bf16 =\n     \"mad.lo.u32 b0, b0, 15, a0;             \\n\" // b0 = b0 * 15 + a0\n     \"mad.lo.u32 b1, b1, 15, a1;             \\n\" // (move sign to the left)\n     \"mad.lo.u32 $0, b0, 16, 0x3c003c00;     \\n\" // out0 = (b0 << 4) + 0x3c003c00\n-    \"mad.lo.u32 $1, b1, 16, 0x3c003c00;     \\n\" // (shift into position and bias exponent)\n+    \"mad.lo.u32 $1, b1, 16, 0x3c003c00;     \\n\" // (shift into position and bias\n+                                                // exponent)\n     \"}\";\n \n const std::string Bf16_to_Fp8E4M3 ="}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -374,6 +374,7 @@ def kernel(x, y, ORDER: tl.constexpr, SIZE: tl.constexpr):\n     kernel[1,](x_tri, y_tri, order, SIZE)\n     np.testing.assert_allclose(y, to_numpy(y_tri))\n \n+\n @pytest.mark.parametrize(\"dtype_x, dtype_y\",\n                          [(dtype_x, dtype_y) for dtype_x in int_dtypes for dtype_y in int_dtypes] +\n                          [(dtype_x, dtype_y) for dtype_x in uint_dtypes for dtype_y in uint_dtypes]"}]