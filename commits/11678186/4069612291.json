[{"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 1, "deletions": 15, "changes": 16, "file_content_changes": "@@ -281,20 +281,6 @@ LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n   return success();\n }\n \n-// TODO: Interface\n-LogicalResult getForwardEncoding(Attribute sourceEncoding, Operation *op,\n-                                 Attribute &ret) {\n-  if (op->hasTrait<mlir::OpTrait::Elementwise>()) {\n-    ret = sourceEncoding;\n-    return success();\n-  }\n-  if (isa<triton::ReduceOp>(op)) {\n-    ret = Attribute();\n-    return success();\n-  }\n-  return failure();\n-}\n-\n inline bool expensiveLoadOrStore(Operation *op,\n                                  const Attribute &targetEncoding) {\n   // Case 1: A size 1 tensor is not expensive since all threads will load the\n@@ -347,7 +333,7 @@ LogicalResult simulateBackwardRematerialization(\n     // If the current operation is expensive to rematerialize,\n     // we stop everything\n     if (expensiveToRemat(currOp, currLayout))\n-      break;\n+      return mlir::failure();\n     // A conversion will be removed here (i.e. transferred to operands)\n     numCvts -= 1;\n     // Done processing"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 40, "deletions": 0, "changes": 40, "file_content_changes": "@@ -54,6 +54,46 @@ func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   // CHECK: return %6 : tensor<1024xi32, [[target_layout]]>\n }\n \n+// CHECK-LABEL: remat_load_store\n+func @remat_load_store(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %0 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #layout0>\n+  %1 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<64x!tt.ptr<i32>, #layout0>\n+  %2 = tt.addptr %1, %0 : tensor<64x!tt.ptr<i32>, #layout0>, tensor<64xi32, #layout0>\n+  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xi32, #layout0>\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %4 = triton_gpu.convert_layout %3 : (tensor<64xi32, #layout0>) -> tensor<64xi32, #layout1>\n+  %5 = triton_gpu.convert_layout %2 : (tensor<64x!tt.ptr<i32>, #layout0>) -> tensor<64x!tt.ptr<i32>, #layout1>\n+  tt.store %5, %4 : tensor<64xi32, #layout1>\n+  return\n+}\n+\n+// Don't rematerialize vectorized loads\n+// CHECK-LABEL: remat_expensive\n+func @remat_expensive(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %0 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #layout1>\n+  %1 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<64x!tt.ptr<i32>, #layout1>\n+  %2 = tt.addptr %1, %0 : tensor<64x!tt.ptr<i32>, #layout1>, tensor<64xi32, #layout1>\n+  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xi32, #layout1>\n+  // CHECK: triton_gpu.convert_layout\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %4 = triton_gpu.convert_layout %3 : (tensor<64xi32, #layout1>) -> tensor<64xi32, #layout0>\n+  %5 = triton_gpu.convert_layout %2 : (tensor<64x!tt.ptr<i32>, #layout1>) -> tensor<64x!tt.ptr<i32>, #layout0>\n+  tt.store %5, %4 : tensor<64xi32, #layout0>\n+  return\n+}\n+\n+// Always rematerialize single value loads\n+// CHECK-LABEL: remat_single_value\n+func @remat_single_value(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %0 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<1x!tt.ptr<i32>, #layout1>\n+  %1 = tt.load %0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1xi32, #layout1>\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %2 = triton_gpu.convert_layout %1 : (tensor<1xi32, #layout1>) -> tensor<1xi32, #layout0>\n+  %3 = triton_gpu.convert_layout %0 : (tensor<1x!tt.ptr<i32>, #layout1>) -> tensor<1x!tt.ptr<i32>, #layout0>\n+  tt.store %3, %2 : tensor<1xi32, #layout0>\n+  return\n+}\n+\n // CHECK-LABEL: if\n func @if(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout"}]