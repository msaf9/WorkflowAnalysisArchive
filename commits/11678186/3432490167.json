[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 23, "deletions": 19, "changes": 42, "file_content_changes": "@@ -3010,6 +3010,7 @@ class MMA16816SmemLoader {\n       Value i8Elems[4][4];\n       Type elemTy = type::i8Ty(ctx);\n       Type elemPtrTy = ptr_ty(elemTy);\n+      Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n       if (kOrder == 1) {\n         for (int i = 0; i < 2; ++i)\n           for (int j = 0; j < 4; ++j)\n@@ -3024,7 +3025,7 @@ class MMA16816SmemLoader {\n           for (int e = 0; e < 4; ++e)\n             i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n                                           i8Elems[m][e], i32_val(e));\n-          i32Elems[m] = bitcast(i8v4Elems[m], i32_ty);\n+          i32Elems[m] = bitcast(i8v4Elems[m], i8x4Ty);\n         }\n       } else { // k first\n         for (int j = 0; j < 4; ++j)\n@@ -3040,7 +3041,7 @@ class MMA16816SmemLoader {\n           for (int e = 0; e < 4; ++e)\n             i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n                                           i8Elems[m][e], i32_val(e));\n-          i32Elems[m] = bitcast(i8v4Elems[m], i32_ty);\n+          i32Elems[m] = bitcast(i8v4Elems[m], i8x4Ty);\n         }\n       }\n \n@@ -3724,8 +3725,7 @@ struct MMA16816ConversionHelper {\n         loadFn(2 * m, 2 * k);\n \n     // step2. Format the values to LLVM::Struct to passing to mma codegen.\n-    Value result = composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK);\n-    return result;\n+    return composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK);\n   }\n \n   // Loading $b from smem to registers, returns a LLVM::Struct.\n@@ -3969,9 +3969,10 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n   Value src = op.src();\n   Value dst = op.result();\n   auto dstTensorTy = dst.getType().cast<RankedTensorType>();\n-\n+  auto srcTensorTy = src.getType().cast<RankedTensorType>();\n   auto dotOperandLayout =\n       dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n+  auto sharedLayout = srcTensorTy.getEncoding().cast<SharedEncodingAttr>();\n \n   MmaEncodingAttr mmaLayout =\n       dotOperandLayout.getParent().dyn_cast_or_null<MmaEncodingAttr>();\n@@ -3981,9 +3982,9 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n   {\n     int K{};\n     if (dotOperandLayout.getOpIdx() == 0) // $a\n-      K = dstTensorTy.getShape()[1];\n+      K = dstTensorTy.getShape()[sharedLayout.getOrder()[0]];\n     else // $b\n-      K = dstTensorTy.getShape()[0];\n+      K = dstTensorTy.getShape()[sharedLayout.getOrder()[1]];\n     isOuter = K == 1;\n   }\n \n@@ -4054,9 +4055,8 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adaptor,\n   loadedB = adaptor.b();\n   loadedC = mmaHelper.loadC(op.c(), adaptor.c());\n \n-  auto ret = mmaHelper.convertDot(A, B, C, op.d(), loadedA, loadedB, loadedC,\n-                                  op, adaptor);\n-  return ret;\n+  return mmaHelper.convertDot(A, B, C, op.d(), loadedA, loadedB, loadedC, op,\n+                              adaptor);\n }\n \n // Simply port the old code here to avoid large difference and make debugging\n@@ -4743,20 +4743,26 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n       auto mmaLayout = dot_op_layout.getParent().cast<MmaEncodingAttr>();\n       auto wpt = mmaLayout.getWarpsPerCTA();\n       Type elemTy = type.getElementType();\n+      auto vecSize = 1;\n+      if (elemTy.getIntOrFloatBitWidth() == 16) {\n+        vecSize = 2;\n+      } else if (elemTy.getIntOrFloatBitWidth() == 8) {\n+        vecSize = 4;\n+      } else {\n+        assert(false && \"Unsupported element type\");\n+      }\n+      Type vecTy = vec_ty(elemTy, vecSize);\n       if (mmaLayout.getVersion() == 2) {\n-\n         if (dot_op_layout.getOpIdx() == 0) { // $a\n           int elems =\n               MMA16816ConversionHelper::getANumElemsPerThread(type, wpt);\n-          Type x2Ty = vec_ty(elemTy, 2);\n           return LLVM::LLVMStructType::getLiteral(\n-              ctx, SmallVector<Type>(elems, x2Ty));\n+              ctx, SmallVector<Type>(elems, vecTy));\n         }\n         if (dot_op_layout.getOpIdx() == 1) { // $b\n           int elems =\n               MMA16816ConversionHelper::getBNumElemsPerThread(type, wpt);\n-          Type x2Ty = vec_ty(elemTy, 2);\n-          return struct_ty(SmallVector<Type>(elems, x2Ty));\n+          return struct_ty(SmallVector<Type>(elems, vecTy));\n         }\n       }\n \n@@ -4765,13 +4771,11 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n \n         if (dot_op_layout.getOpIdx() == 0) { // $a\n           int elems = helper.numElemsPerThreadA(type);\n-          Type x2Ty = vec_ty(elemTy, 2);\n-          return struct_ty(SmallVector<Type>(elems, x2Ty));\n+          return struct_ty(SmallVector<Type>(elems, vecTy));\n         }\n         if (dot_op_layout.getOpIdx() == 1) { // $b\n           int elems = helper.numElemsPerThreadB(type);\n-          Type x2Ty = vec_ty(elemTy, 2);\n-          return struct_ty(SmallVector<Type>(elems, x2Ty));\n+          return struct_ty(SmallVector<Type>(elems, vecTy));\n         }\n       }\n "}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 26, "deletions": 26, "changes": 52, "file_content_changes": "@@ -55,31 +55,31 @@ def test_gemm_no_scf(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n     assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)\n \n \n-#@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS', [\n-#    [64, 128, 128, 1],\n-#    [128, 128, 128, 4],\n-#    [16, 8, 32, 1],\n-#    [32, 16, 64, 2],\n-#    [32, 16, 64, 4],\n-#])\n-#def test_gemm_no_scf_int8(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n-#    a = torch.randint(-5, 5, (SIZE_M, SIZE_K), device='cuda', dtype=torch.int8)\n-#    b = torch.randint(-5, 5, (SIZE_K, SIZE_N), device='cuda', dtype=torch.int8)\n-#    c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.int32)\n-#\n-#    grid = lambda META: (1, )\n-#    matmul_no_scf_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n-#                               stride_am=a.stride(0), stride_ak=a.stride(1),\n-#                               stride_bk=b.stride(0), stride_bn=b.stride(1),\n-#                               stride_cm=c.stride(0), stride_cn=c.stride(1),\n-#                               M=SIZE_M, N=SIZE_N, K=SIZE_K,\n-#                               num_warps=NUM_WARPS)\n-#\n-#    aa = a.cpu()\n-#    bb = b.cpu()\n-#    golden = torch.matmul(aa.float(), bb.float()).int()\n-#    torch.set_printoptions(profile=\"full\")\n-#    torch.testing.assert_close(c.cpu(), golden, check_dtype=False)\n+@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS', [\n+    [64, 128, 128, 1],\n+    [128, 128, 128, 4],\n+    [16, 8, 32, 1],\n+    [32, 16, 64, 2],\n+    [32, 16, 64, 4],\n+])\n+def test_gemm_no_scf_int8(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n+    a = torch.randint(-5, 5, (SIZE_M, SIZE_K), device='cuda', dtype=torch.int8)\n+    b = torch.randint(-5, 5, (SIZE_K, SIZE_N), device='cuda', dtype=torch.int8)\n+    c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.int32)\n+\n+    grid = lambda META: (1, )\n+    matmul_no_scf_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n+                               stride_am=a.stride(0), stride_ak=a.stride(1),\n+                               stride_bk=b.stride(0), stride_bn=b.stride(1),\n+                               stride_cm=c.stride(0), stride_cn=c.stride(1),\n+                               M=SIZE_M, N=SIZE_N, K=SIZE_K,\n+                               num_warps=NUM_WARPS)\n+\n+    aa = a.cpu()\n+    bb = b.cpu()\n+    golden = torch.matmul(aa.float(), bb.float()).int()\n+    torch.set_printoptions(profile=\"full\")\n+    torch.testing.assert_close(c.cpu(), golden, check_dtype=False)\n \n \n @triton.jit\n@@ -231,4 +231,4 @@ def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLO\n #\n #    golden = torch.matmul(a, b)\n #    torch.testing.assert_close(c, golden)\n-#\n\\ No newline at end of file\n+#"}]