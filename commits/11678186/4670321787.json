[{"filename": "python/test/unit/runtime/test_launch.py", "status": "modified", "additions": 68, "deletions": 67, "changes": 135, "file_content_changes": "@@ -1,19 +1,18 @@\n import gc\n-import importlib\n-import os\n-import sys\n-import tempfile\n-import textwrap\n-import time\n+# import importlib\n+# import os\n+# import sys\n+# import tempfile\n+# import textwrap\n+# import time\n import tracemalloc\n-from typing import Tuple\n \n import torch\n \n import triton\n import triton.language as tl\n \n-LATENCY_THRESHOLD_US = 46\n+# from typing import Tuple\n \n \n def test_memory_leak() -> None:\n@@ -44,62 +43,64 @@ def kernel(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):\n         tracemalloc.stop()\n \n \n-def test_kernel_launch_latency() -> None:\n-    def define_kernel(kernel_name: str, num_tensor_args: int) -> str:\n-        arg_str = \",\".join([f\"arg{i}: torch.Tensor\" for i in range(num_tensor_args)])\n-        arg_str += \", n_elements: int, BLOCK_SIZE: tl.constexpr\"\n-        func_str = f\"\"\"\n-        import torch\n-\n-        import triton\n-        import triton.language as tl\n-\n-        @triton.jit\n-        def {kernel_name}({arg_str}):\n-            pass\n-        \"\"\"\n-        with tempfile.NamedTemporaryFile(mode=\"w+t\", suffix=\".py\", delete=False) as temp_file:\n-            temp_file.write(textwrap.dedent(func_str))\n-            temp_file_path = temp_file.name\n-\n-        return temp_file_path\n-\n-    def import_kernel(file_path, kernel_name):\n-        directory, filename = os.path.split(file_path)\n-        module_name, _ = os.path.splitext(filename)\n-        sys.path.insert(0, directory)\n-\n-        module = importlib.import_module(module_name)\n-        kernel = getattr(module, kernel_name)\n-        return kernel\n-\n-    def empty(*kernel_args: Tuple[torch.Tensor]):\n-        first_arg = kernel_args[0]\n-        n_elements = first_arg.numel()\n-        grid = (triton.cdiv(n_elements, 1024),)\n-        device = torch.cuda.current_device()\n-        # Warmup\n-        empty_kernel[grid](*kernel_args, n_elements, BLOCK_SIZE=1024, device=device)\n-        torch.cuda.synchronize()\n-        # Measure launch overhead at steady state\n-        num_runs = 1000\n-        start_time = time.time()\n-        for i in range(num_runs):\n-            empty_kernel[grid](*kernel_args, n_elements, BLOCK_SIZE=1024, device=device)\n-        end_time = time.time()\n-        latency_us = (end_time - start_time) / num_runs * 1e6\n-\n-        assert latency_us < LATENCY_THRESHOLD_US, \"Kernel launch time has increased!\"\n-\n-    num_tensor_args = 40\n-    kernel_name = 'empty_kernel'\n-    file_path = define_kernel(kernel_name, num_tensor_args)\n-    empty_kernel = import_kernel(file_path, kernel_name)\n-\n-    # Initialize random tensors for the empty_kernel\n-    torch.manual_seed(0)\n-    size = 1024\n-    kernel_args = (torch.rand(size, device='cuda') for i in range(num_tensor_args))\n-\n-    # Run empty, which would run empty_kernel internally\n-    empty(*kernel_args)\n+# LATENCY_THRESHOLD_US = 46\n+\n+# def test_kernel_launch_latency() -> None:\n+#     def define_kernel(kernel_name: str, num_tensor_args: int) -> str:\n+#         arg_str = \",\".join([f\"arg{i}: torch.Tensor\" for i in range(num_tensor_args)])\n+#         arg_str += \", n_elements: int, BLOCK_SIZE: tl.constexpr\"\n+#         func_str = f\"\"\"\n+#         import torch\n+\n+#         import triton\n+#         import triton.language as tl\n+\n+#         @triton.jit\n+#         def {kernel_name}({arg_str}):\n+#             pass\n+#         \"\"\"\n+#         with tempfile.NamedTemporaryFile(mode=\"w+t\", suffix=\".py\", delete=False) as temp_file:\n+#             temp_file.write(textwrap.dedent(func_str))\n+#             temp_file_path = temp_file.name\n+\n+#         return temp_file_path\n+\n+#     def import_kernel(file_path, kernel_name):\n+#         directory, filename = os.path.split(file_path)\n+#         module_name, _ = os.path.splitext(filename)\n+#         sys.path.insert(0, directory)\n+\n+#         module = importlib.import_module(module_name)\n+#         kernel = getattr(module, kernel_name)\n+#         return kernel\n+\n+#     def empty(*kernel_args: Tuple[torch.Tensor]):\n+#         first_arg = kernel_args[0]\n+#         n_elements = first_arg.numel()\n+#         grid = (triton.cdiv(n_elements, 1024),)\n+#         device = torch.cuda.current_device()\n+#         # Warmup\n+#         empty_kernel[grid](*kernel_args, n_elements, BLOCK_SIZE=1024, device=device)\n+#         torch.cuda.synchronize()\n+#         # Measure launch overhead at steady state\n+#         num_runs = 1000\n+#         start_time = time.time()\n+#         for i in range(num_runs):\n+#             empty_kernel[grid](*kernel_args, n_elements, BLOCK_SIZE=1024, device=device)\n+#         end_time = time.time()\n+#         latency_us = (end_time - start_time) / num_runs * 1e6\n+\n+#         assert latency_us < LATENCY_THRESHOLD_US, \"Kernel launch time has increased!\"\n+\n+#     num_tensor_args = 40\n+#     kernel_name = 'empty_kernel'\n+#     file_path = define_kernel(kernel_name, num_tensor_args)\n+#     empty_kernel = import_kernel(file_path, kernel_name)\n+\n+#     # Initialize random tensors for the empty_kernel\n+#     torch.manual_seed(0)\n+#     size = 1024\n+#     kernel_args = (torch.rand(size, device='cuda') for i in range(num_tensor_args))\n+\n+#     # Run empty, which would run empty_kernel internally\n+#     empty(*kernel_args)"}]