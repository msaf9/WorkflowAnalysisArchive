[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 25, "deletions": 14, "changes": 39, "file_content_changes": "@@ -467,17 +467,17 @@ struct SharedMemoryObject {\n   // XXX(Keren): a special allocator for 3d tensors. It's a workaround for\n   // now since we don't have a correct way to encoding 3d tensors in the\n   // pipeline pass.\n-  SharedMemoryObject(Value base, ArrayRef<int64_t> shape, Location loc,\n-                     ConversionPatternRewriter &rewriter)\n-      : base(base) {\n-    auto stride = 1;\n-    for (auto dim : llvm::reverse(shape)) {\n-      strides.emplace_back(i32_val(stride));\n-      offsets.emplace_back(i32_val(0));\n-      stride *= dim;\n-    }\n-    strides = llvm::to_vector<4>(llvm::reverse(strides));\n-  }\n+  // SharedMemoryObject(Value base, ArrayRef<int64_t> shape, Location loc,\n+  //                    ConversionPatternRewriter &rewriter)\n+  //     : base(base) {\n+  //   auto stride = 1;\n+  //   for (auto dim : llvm::reverse(shape)) {\n+  //     strides.emplace_back(i32_val(stride));\n+  //     offsets.emplace_back(i32_val(0));\n+  //     stride *= dim;\n+  //   }\n+  //   strides = llvm::to_vector<4>(llvm::reverse(strides));\n+  // }\n \n   SmallVector<Value> getElems() const {\n     SmallVector<Value> elems;\n@@ -2251,8 +2251,15 @@ struct AllocTensorOpConversion\n         getTypeConverter()->convertType(resultTy.getElementType());\n     auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n     smemBase = bitcast(smemBase, elemPtrTy);\n+    auto order = resultTy.getEncoding().cast<SharedEncodingAttr>().getOrder();\n+    // workaround for 3D tensors\n+    // We need to modify the pipeline pass to give a proper shared encoding to 3D tensors\n+    SmallVector<unsigned> newOrder;\n+    if (resultTy.getShape().size() == 3) \n+      newOrder = {1 + order[0], 1 + order[1], 0};\n+\n     auto smemObj =\n-        SharedMemoryObject(smemBase, resultTy.getShape(), loc, rewriter);\n+        SharedMemoryObject(smemBase, resultTy.getShape(), newOrder, loc, rewriter);\n     auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n     rewriter.replaceOp(op, retVal);\n     return success();\n@@ -2302,6 +2309,10 @@ struct ExtractSliceOpConversion\n         strideVals.emplace_back(smemObj.strides[i]);\n       }\n     }\n+\n+    // llvm::outs() << \"extract slice\\n\";\n+    // llvm::outs() << strideVals[0] << \" \" << smemObj.strides[1] << \"\\n\";\n+    // llvm::outs() << strideVals[1] << \" \" << smemObj.strides[2] << \"\\n\";\n     auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n     auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n     auto resTy = op.getType().dyn_cast<RankedTensorType>();\n@@ -3262,8 +3273,8 @@ class MMA16816SmemLoader {\n     cMatShape = matShape[order[0]];\n     sMatShape = matShape[order[1]];\n \n-    cStride = smemStrides[1];\n-    sStride = smemStrides[0];\n+    cStride = smemStrides[order[0]];\n+    sStride = smemStrides[order[1]];\n \n     // rule: k must be the fast-changing axis.\n     needTrans = kOrder != order[0];"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -187,8 +187,8 @@ def get_proper_err(a, b, golden):\n     [256, 128, 64, 4, 256, 128, 16, False, False],\n     [128, 64, 128, 4, 128, 64, 32, False, False],\n     # TODO[goostavz]: fix these cases\n-    #[128, 64, 128, 4, 128, 64, 32, True, False],\n-    #[128, 64, 128, 4, 128, 64, 32, False, True],\n+    [128, 64, 128, 4, 128, 64, 32, True, False],\n+    [128, 64, 128, 4, 128, 64, 32, False, True],\n ])\n def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B):\n     if (TRANS_A):"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -887,7 +887,7 @@ def ttir_to_ttgir(mod, num_warps, num_stages):\n     pm.add_tritongpu_pipeline_pass(num_stages)\n     # Prefetch must be done after pipeline pass because pipeline pass\n     # extracts slices from the original tensor.\n-    pm.add_tritongpu_prefetch_pass()\n+    # pm.add_tritongpu_prefetch_pass()\n     pm.add_canonicalizer_pass()\n     pm.add_cse_pass()\n     pm.add_triton_gpu_combine_pass()"}]