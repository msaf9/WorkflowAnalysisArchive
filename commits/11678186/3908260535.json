[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 61, "deletions": 44, "changes": 105, "file_content_changes": "@@ -227,88 +227,105 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n                                          SmallVectorImpl<Value>& offsetVals,\n                                          SmallVectorImpl<Value>& srcStrides\n                                          ) const {\n-\n+    // This utililty computes the pointers for accessing the provided swizzled\n+    // shared memory layout `resSharedLayout`. More specifically, it computes, for all\n+    // indices (row, col) of `srcEncoding` such that idx % inVec = 0, the pointer:\n+    // ptr[(row, col)] = base + (rowOff * strides[ord[1]] + colOff)\n+    // where : \n+    //   compute phase = (row // perPhase) % maxPhase\n+    //   rowOff = row\n+    //   colOff = colOffSwizzled + colOffOrdered\n+    //     colOffSwizzled = ((col // outVec) ^ phase) * outVec\n+    //     colOffOrdered = (col % outVec) // minVec * minVec\n+    //\n+    // Note 1: \n+    // -------\n+    // Because swizzling happens at a granularity of outVec, we need to\n+    // decompose the offset into a swizzled factor and a non-swizzled (ordered) \n+    // factor\n+    //\n+    // Note 2:\n+    // -------\n+    // If we have x, y, z of the form:\n+    // x = 0b00000xxxx\n+    // y = 0byyyyy0000\n+    // z = 0b00000zzzz\n+    // then (x + y) XOR z = 0byyyyxxxx XOR 0b00000zzzz = (x XOR z) + y\n+    // This means that we can use some immediate offsets for shared memory operations.\n+    // More specifically\n     auto dstPtrTy = ptr_ty(resElemTy, 3);\n     auto dstOffset = dot(rewriter, loc, offsetVals, smemObj.strides);\n     Value dstPtrBase = gep(dstPtrTy, smemObj.base, dstOffset);\n \n     auto srcEncoding = srcTy.getEncoding();\n     auto srcShape = srcTy.getShape();\n-    unsigned outVec = resSharedLayout.getVec();\n-    unsigned minVec = std::min(outVec, inVec);\n     unsigned numElems = triton::gpu::getElemsPerThread(srcTy);\n+    // swizzling params as described in TritonGPUAttrDefs.td\n+    unsigned outVec = resSharedLayout.getVec();\n     unsigned perPhase = resSharedLayout.getPerPhase();\n     unsigned maxPhase = resSharedLayout.getMaxPhase();\n+    // order\n     auto inOrder = triton::gpu::getOrder(srcEncoding);\n     auto outOrder = triton::gpu::getOrder(resSharedLayout);\n-\n+    // tensor indices held by the current thread, as LLVM values\n     auto srcIndices = emitIndices(loc, rewriter, srcEncoding, srcShape);\n-\n-\n+    // return values\n     DenseMap<unsigned, Value> ret;\n+    // cache for non-immediate offsets\n     DenseMap<unsigned, Value> cacheCol, cacheRow;\n-\n+    unsigned minVec = std::min(outVec, inVec);\n     for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n       // extract multi dimensional index for current element\n       auto idx = srcIndices[elemIdx];\n       Value idxCol = idx[inOrder[0]]; // contiguous dimension\n       Value idxRow = idx[inOrder[1]]; // discontiguous dimension\n       Value strideCol = srcStrides[inOrder[0]]; \n       Value strideRow = srcStrides[inOrder[1]]; \n-      // extract dynamic/static offset\n-      unsigned baseOffsetCol = 0;\n+      // extract dynamic/static offset for immediate offseting\n+      unsigned immedateOffCol = 0;\n       if (auto add = dyn_cast_or_null<LLVM::AddOp>(idxCol.getDefiningOp()))\n       if (auto _cst = dyn_cast_or_null<LLVM::ConstantOp>(add.getRhs().getDefiningOp())) {\n         unsigned cst = _cst.getValue().cast<IntegerAttr>()\n                            .getValue().getSExtValue();\n         unsigned key = cst % (outVec * maxPhase);\n         cacheCol.insert({key, idxCol});\n         idxCol = cacheCol[key];\n-        baseOffsetCol = cst / (outVec * maxPhase) * (outVec * maxPhase);\n+        immedateOffCol = cst / (outVec * maxPhase) * (outVec * maxPhase);\n       }\n-      unsigned baseOffsetRow = 0;\n+      // extract dynamic/static offset for immediate offseting\n+      unsigned immedateOffRow = 0;\n       if (auto add = dyn_cast_or_null<LLVM::AddOp>(idxRow.getDefiningOp()))\n       if (auto _cst = dyn_cast_or_null<LLVM::ConstantOp>(add.getRhs().getDefiningOp())) {\n         unsigned cst = _cst.getValue().cast<IntegerAttr>()\n                            .getValue().getSExtValue();\n         unsigned key = cst % (perPhase * maxPhase);\n         cacheRow.insert({key, idxRow});\n         idxRow = cacheRow[key];\n-        baseOffsetRow = cst / (perPhase * maxPhase) * (perPhase * maxPhase);\n+        immedateOffRow = cst / (perPhase * maxPhase) * (perPhase * maxPhase);\n       }\n-      // Swizzling\n-      // Since the swizzling index is related to outVec, and we know minVec\n-      // already, inVec doesn't matter\n-      //\n-      // (Numbers represent row indices)\n-      // Example1:\n-      // outVec = 2, inVec = 2, minVec = 2\n-      // outVec = 2, inVec = 4, minVec = 2\n-      //     | [1 2] [3 4] [5 6] ... |\n-      //     | [3 4] [1 2] [7 8] ... |\n-      //     | [5 6] [7 8] [1 2] ... |\n-      // Example2:\n-      // outVec = 4, inVec = 2, minVec = 2\n-      //     | [1 2 3 4] [5 6 7 8] [9 10 11 12] ... |\n-      //     | [5 6 7 8] [1 2 3 4] [13 14 15 16] ... |\n-      //     | [9 10 11 12] [13 14 15 16] [1 2 3 4] ... |\n+      // compute phase = (row // perPhase) % maxPhase\n       Value phase = urem(udiv(idxRow, i32_val(perPhase)), i32_val(maxPhase));\n-      Value rowOffset = mul(idxRow, strideRow);\n-      Value colOffset = xor_(udiv(idxCol, i32_val(outVec)), phase);\n-      colOffset = mul(colOffset, i32_val(outVec));\n-      Value remained = urem(idxCol, i32_val(outVec));\n-      remained = udiv(remained, i32_val(minVec));\n-      colOffset = add(colOffset, mul(remained, i32_val(minVec)));\n-\n-      Value tileOffset = add(rowOffset, mul(colOffset, strideCol));\n-      tileOffset = gep(dstPtrTy, dstPtrBase, tileOffset);\n-\n-      Value baseOffset =\n-          add(mul(i32_val(baseOffsetRow), srcStrides[outOrder[1]]),\n-              i32_val(baseOffsetCol));\n-      Value basePtr = gep(dstPtrTy, tileOffset, baseOffset);\n-\n-      ret[elemIdx] = basePtr;\n+      // row offset is simply row index\n+      Value rowOff = mul(idxRow, strideRow);\n+      // because swizzling happens at a granularity of outVec, we need to\n+      // decompose the offset into a swizzled factor and a non-swizzled (ordered) \n+      // factor:\n+      // colOffSwizzled = ((col // outVec) ^ phase) * outVec\n+      // colOffOrdered = (col % outVec) // minVec * minVec\n+      Value colOffSwizzled = xor_(udiv(idxCol, i32_val(outVec)), phase);\n+      colOffSwizzled = mul(colOffSwizzled, i32_val(outVec));\n+      Value colOffOrdered =  urem(idxCol, i32_val(outVec));\n+      colOffOrdered = udiv(colOffOrdered, i32_val(minVec));\n+      colOffOrdered = mul(colOffOrdered, i32_val(minVec));\n+      Value colOff = add(colOffSwizzled, colOffOrdered);\n+      // compute non-immediate offset\n+      Value offset = add(rowOff, mul(colOff, strideCol));\n+      Value currPtr = gep(dstPtrTy, dstPtrBase, offset);\n+      // compute immediate offset\n+      Value immedateOff =\n+          add(mul(i32_val(immedateOffRow), srcStrides[inOrder[1]]),\n+              i32_val(immedateOffCol));\n+      ret[elemIdx] = gep(dstPtrTy, currPtr, immedateOff);\n     }\n     return ret;\n   }"}]