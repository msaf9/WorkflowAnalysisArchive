[{"filename": "python/test/unit/runtime/test_launch.py", "status": "modified", "additions": 8, "deletions": 7, "changes": 15, "file_content_changes": "@@ -45,7 +45,7 @@ def kernel(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):\n \n \n def test_kernel_launch_latency() -> None:\n-    def define_kernel(num_tensor_args):\n+    def define_kernel(kernel_name: str, num_tensor_args: int) -> str:\n         arg_str = \",\".join([f\"arg{i}: torch.Tensor\" for i in range(num_tensor_args)])\n         arg_str += \", n_elements: int, BLOCK_SIZE: tl.constexpr\"\n         func_str = f\"\"\"\n@@ -55,7 +55,7 @@ def define_kernel(num_tensor_args):\n         import triton.language as tl\n \n         @triton.jit\n-        def empty_kernel({arg_str}):\n+        def {kernel_name}({arg_str}):\n             pass\n         \"\"\"\n         with tempfile.NamedTemporaryFile(mode=\"w+t\", suffix=\".py\", delete=False) as temp_file:\n@@ -64,14 +64,14 @@ def empty_kernel({arg_str}):\n \n         return temp_file_path\n \n-    def import_kernel(file_path):\n+    def import_kernel(file_path, kernel_name):\n         directory, filename = os.path.split(file_path)\n         module_name, _ = os.path.splitext(filename)\n         sys.path.insert(0, directory)\n \n         module = importlib.import_module(module_name)\n-        empty_kernel = module.empty_kernel\n-        return empty_kernel\n+        kernel = getattr(module, kernel_name)\n+        return kernel\n \n     def empty(*kernel_args: Tuple[torch.Tensor]):\n         first_arg = kernel_args[0]\n@@ -92,8 +92,9 @@ def empty(*kernel_args: Tuple[torch.Tensor]):\n         assert latency_us < LATENCY_THRESHOLD_US, \"Kernel launch time has increased!\"\n \n     num_tensor_args = 40\n-    file_path = define_kernel(num_tensor_args)\n-    empty_kernel = import_kernel(file_path)\n+    kernel_name = 'empty_kernel'\n+    file_path = define_kernel(kernel_name, num_tensor_args)\n+    empty_kernel = import_kernel(file_path, kernel_name)\n \n     # Initialize random tensors for the empty_kernel\n     torch.manual_seed(0)"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -248,7 +248,7 @@ def _make_launcher(self):\n             if i in self.do_not_specialize:\n                 continue\n             arg_annotation = self.__annotations__.get(arg, None)\n-            if True:\n+            if not arg_annotation:\n                 specializations += [f'({arg}.data_ptr() % {JITFunction.divisibility} == 0) if hasattr({arg}, \"data_ptr\") '\n                                     f'else ({arg} % {JITFunction.divisibility} == 0, {arg} == 1) if isinstance({arg}, int) '\n                                     f'else (False,)']\n@@ -263,7 +263,7 @@ def _make_launcher(self):\n         grid_args = ','.join([f'\"{arg}\": {arg}' for arg in self.arg_names])\n \n         src = f\"\"\"\n-def {self.fn.__name__}({', '.join(self.arg_names)}, grid, device=None, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False):\n+def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False, device=None):\n     sig_key =  {sig_keys},\n     constexpr_key = {f'{constexpr_keys},' if len(constexpr_keys) > 0 else ()}\n     spec_key = {f'{spec_keys},' if len(spec_keys) > 0 else ()}"}]