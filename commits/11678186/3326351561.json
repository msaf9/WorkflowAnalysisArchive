[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 53, "deletions": 27, "changes": 80, "file_content_changes": "@@ -645,8 +645,8 @@ Value convertSplatLikeOp(Type elemType, Type resType, Value constVal,\n     auto srcType = typeConverter->convertType(elemType);\n     auto llSrc = bitcast(srcType, constVal);\n     size_t elemsPerThread = getElemsPerThread(layout, tensorTy.getShape());\n-    llvm::SmallVector<Value, 4> elems(elemsPerThread, llSrc);\n-    llvm::SmallVector<Type, 4> elemTypes(elems.size(), srcType);\n+    llvm::SmallVector<Value> elems(elemsPerThread, llSrc);\n+    llvm::SmallVector<Type> elemTypes(elems.size(), srcType);\n     auto structTy =\n         LLVM::LLVMStructType::getLiteral(rewriter.getContext(), elemTypes);\n \n@@ -2551,7 +2551,7 @@ struct DotOpConversionHelper {\n     mmaType = getTensorCoreTypeFromOperand(operandTy);\n   }\n \n-  // Deduce the M and N from either $c or $d type.\n+  // Get the M and N of mat instruction shape.\n   static std::tuple<int, int> getMatShapeMN() {\n     // According to DotOpConversionHelper::mmaMatShape, all the matrix shape's\n     // M,N are {8,8}\n@@ -2685,8 +2685,7 @@ struct DotOpConversionHelper {\n     return mmaMatShape.at(mmaType);\n   }\n \n-  // Deduce the TensorCoreType from either $a or $b's type. This method is not\n-  // safe, but we cannot get the DotOp in some getjmaMatShape usage case.\n+  // Deduce the TensorCoreType from either $a or $b's type.\n   static TensorCoreType getTensorCoreTypeFromOperand(Type operandTy) {\n     auto tensorTy = operandTy.cast<RankedTensorType>();\n     auto elemTy = tensorTy.getElementType();\n@@ -2860,22 +2859,58 @@ struct MMA16816ConversionHelper {\n \n   // \\param operand is either $a or $b's type.\n   inline int getNumRepM(Type operand, int M) const {\n-    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(operand);\n-    return std::max<int>(M / (wpt[0] * mmaInstrM), 1);\n+    return getNumRepM(operand, M, wpt[0]);\n   }\n \n   // \\param operand is either $a or $b's type.\n   inline int getNumRepN(Type operand, int N) const {\n-    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(operand);\n-    return std::max<int>(N / (wpt[1] * mmaInstrN), 1);\n+    return getNumRepN(operand, N, wpt[1]);\n   }\n \n   // \\param operand is either $a or $b's type.\n   inline int getNumRepK(Type operand, int K) const {\n-    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(operand);\n+    return getNumRepK_(operand, K);\n+  }\n+\n+  static int getNumRepM(Type operand, int M, int wpt) {\n+    auto tensorCoreType =\n+        DotOpConversionHelper::getTensorCoreTypeFromOperand(operand);\n+    int mmaInstrM = DotOpConversionHelper::getMmaInstrShape(tensorCoreType)[0];\n+    return std::max<int>(M / (wpt * mmaInstrM), 1);\n+  }\n+\n+  static int getNumRepN(Type operand, int N, int wpt) {\n+    auto tensorCoreType =\n+        DotOpConversionHelper::getTensorCoreTypeFromOperand(operand);\n+    int mmaInstrN = DotOpConversionHelper::getMmaInstrShape(tensorCoreType)[1];\n+    return std::max<int>(N / (wpt * mmaInstrN), 1);\n+  }\n+\n+  static int getNumRepK_(Type operand, int K) {\n+    auto tensorCoreType =\n+        DotOpConversionHelper::getTensorCoreTypeFromOperand(operand);\n+    int mmaInstrK = DotOpConversionHelper::getMmaInstrShape(tensorCoreType)[2];\n     return std::max<int>(K / mmaInstrK, 1);\n   }\n \n+  // Get number of elements per thread for $a operand.\n+  static size_t getANumElemsPerThread(RankedTensorType operand,\n+                                      ArrayRef<unsigned> wpt) {\n+    auto shape = operand.getShape();\n+    int repM = getNumRepM(operand, shape[0], wpt[0]);\n+    int repK = getNumRepK_(operand, shape[1]);\n+    return 4 * repM * repK;\n+  }\n+\n+  // Get number of elements per thread for $b operand.\n+  static size_t getBNumElemsPerThread(RankedTensorType operand,\n+                                      ArrayRef<unsigned> wpt) {\n+    auto shape = operand.getShape();\n+    int repK = getNumRepK_(operand, shape[0]);\n+    int repN = getNumRepN(operand, shape[1], wpt[1]);\n+    return 4 * std::max(repN / 2, 1) * repK;\n+  }\n+\n   // Loading $a from smem to registers, returns a LLVM::Struct.\n   Value loadA(Value tensor, Value llTensor) const {\n     auto aTensorTy = tensor.getType().cast<RankedTensorType>();\n@@ -3287,44 +3322,35 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n             ctx, SmallVector<Type>(fcSize, type.getElementType()));\n       }\n \n-      llvm::errs() << \"Unexpected mma layout detected in TypeConverter\";\n+      llvm::errs()\n+          << \"Unexpected mma layout detected in TritonToLLVMTypeConverter\";\n       return llvm::None;\n \n     } else if (auto dot_op_layout =\n                    layout.dyn_cast_or_null<DotOperandEncodingAttr>()) {\n       auto mmaLayout = dot_op_layout.getParent().cast<MmaEncodingAttr>();\n       if (mmaLayout.getVersion() == 2) {\n         auto wpt = mmaLayout.getWarpsPerCTA();\n-        auto tensorCoreType =\n-            DotOpConversionHelper::getTensorCoreTypeFromOperand(type);\n-        // {M, N, K}\n-        auto mmaInstrShape =\n-            DotOpConversionHelper::getMmaInstrShape(tensorCoreType);\n         Type elemTy = type.getElementType();\n \n         if (dot_op_layout.getOpIdx() == 0) { // $a\n-          int M = type.getShape()[0];\n-          int K = type.getShape()[1];\n-          int repM = std::max<int>(M / (wpt[0] * mmaInstrShape[0]), 1);\n-          int repK = std::max<int>(K / mmaInstrShape[2], 1);\n-          int elems = 4 * repM * repK;\n+          int elems =\n+              MMA16816ConversionHelper::getANumElemsPerThread(type, wpt);\n           Type x2Ty = vec_ty(elemTy, 2);\n           return LLVM::LLVMStructType::getLiteral(\n               ctx, SmallVector<Type>(elems, x2Ty));\n         }\n         if (dot_op_layout.getOpIdx() == 1) { // $b\n-          int K = type.getShape()[0];\n-          int N = type.getShape()[1];\n-          int repN = std::max<int>(N / (wpt[1] * mmaInstrShape[1]), 1);\n-          int repK = std::max<int>(K / mmaInstrShape[2], 1);\n-          int elems = 4 * std::max(repN / 2, 1) * repK;\n+          int elems =\n+              MMA16816ConversionHelper::getBNumElemsPerThread(type, wpt);\n           Type x2Ty = vec_ty(elemTy, 2);\n           return LLVM::LLVMStructType::getLiteral(\n               ctx, SmallVector<Type>(elems, x2Ty));\n         }\n       }\n \n-      llvm::errs() << \"Unexpected dot operand layout detected in TypeConverter\";\n+      llvm::errs() << \"Unexpected dot operand layout detected in \"\n+                      \"TritonToLLVMTypeConverter\";\n       return llvm::None;\n     }\n "}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "file_content_changes": "@@ -84,18 +84,17 @@ def matmul_kernel(\n \n \n @pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K', [\n-    # No forloop\n+    # Non-forloop\n     [64, 32, 64, 4, 64, 32, 64],\n     [128, 64, 128, 4, 128, 64, 128],\n-    # Simple forloop\n+    # K-Forloop\n     [64, 32, 128, 4, 64, 32, 64],\n     [128, 16, 128, 4, 128, 16, 32],\n     [32, 16, 128, 4, 32, 16, 32],\n     [32, 64, 128, 4, 32, 64, 32],\n     [32, 128, 256, 4, 32, 128, 64],\n-    [64, 128, 64, 4, 64, 128, 32],  # n can't be 128\n+    [64, 128, 64, 4, 64, 128, 32],\n     [128, 128, 64, 4, 128, 128, 32],\n-\n     [64, 64, 128, 4, 64, 64, 32],\n     [128, 128, 128, 4, 128, 128, 32],\n     [128, 128, 256, 4, 128, 128, 64],"}]