[{"filename": "docs/python-api/triton.language.rst", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -34,6 +34,7 @@ Shape Manipulation Ops\n     :nosignatures:\n \n     broadcast_to\n+    expand_dims\n     reshape\n     ravel\n "}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 80, "deletions": 0, "changes": 80, "file_content_changes": "@@ -457,6 +457,86 @@ def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.con\n     assert (y_broadcasted_np == to_numpy(y_broadcasted_tri)).all()\n \n \n+# ----------------\n+# test expand_dims\n+# ----------------\n+def test_expand_dims():\n+    @triton.jit\n+    def expand_dims_kernel(dummy, N: tl.constexpr):\n+        offset1 = tl.arange(0, N)\n+\n+        t = tl.expand_dims(offset1, 0)\n+        tl.static_assert(t.shape == [1, N])\n+\n+        t = tl.expand_dims(offset1, 1)\n+        tl.static_assert(t.shape == [N, 1])\n+\n+        t = tl.expand_dims(offset1, -1)\n+        tl.static_assert(t.shape == [N, 1])\n+\n+        t = tl.expand_dims(offset1, -2)\n+        tl.static_assert(t.shape == [1, N])\n+\n+        t = tl.expand_dims(offset1, (0, -1))\n+        tl.static_assert(t.shape == [1, N, 1])\n+\n+        t = tl.expand_dims(offset1, (0, 1, 3))\n+        tl.static_assert(t.shape == [1, 1, N, 1])\n+\n+        t = tl.expand_dims(offset1, (-4, 2, -1))\n+        tl.static_assert(t.shape == [1, N, 1, 1])\n+\n+        t = tl.expand_dims(offset1, (3, 1, 2))\n+        tl.static_assert(t.shape == [N, 1, 1, 1])\n+\n+    N = 32\n+    dummy_tensor = torch.empty((), device=\"cuda\")\n+    expand_dims_kernel[(1,)](dummy_tensor, N)\n+\n+\n+def test_expand_dims_error_cases():\n+    @triton.jit\n+    def dim_out_of_range1(dummy, N: tl.constexpr):\n+        offset1 = tl.arange(0, N)\n+\n+        t = tl.expand_dims(offset1, -2)\n+        t = tl.expand_dims(offset1, -3)\n+\n+    @triton.jit\n+    def dim_out_of_range2(dummy, N: tl.constexpr):\n+        offset1 = tl.arange(0, N)\n+\n+        t = tl.expand_dims(offset1, 1)\n+        t = tl.expand_dims(offset1, 2)\n+\n+    @triton.jit\n+    def duplicate_dim1(dummy, N: tl.constexpr):\n+        offset1 = tl.arange(0, N)\n+\n+        t = tl.expand_dims(offset1, (0, 0))\n+\n+    @triton.jit\n+    def duplicate_dim2(dummy, N: tl.constexpr):\n+        offset1 = tl.arange(0, N)\n+\n+        t = tl.expand_dims(offset1, (0, -3))\n+\n+    N = 32\n+    dummy_tensor = torch.empty((), device=\"cuda\")\n+\n+    with pytest.raises(triton.CompilationError, match=\"invalid axis -3\"):\n+        dim_out_of_range1[(1,)](dummy_tensor, N)\n+\n+    with pytest.raises(triton.CompilationError, match=\"invalid axis 2\"):\n+        dim_out_of_range2[(1,)](dummy_tensor, N)\n+\n+    with pytest.raises(triton.CompilationError, match=r\"duplicate axes, normalized axes = \\[0, 0\\]\"):\n+        duplicate_dim1[(1,)](dummy_tensor, N)\n+\n+    with pytest.raises(triton.CompilationError, match=r\"duplicate axes, normalized axes = \\[0, 0\\]\"):\n+        duplicate_dim2[(1,)](dummy_tensor, N)\n+\n+\n # ---------------\n # test where\n # ---------------"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -39,6 +39,7 @@\n     dot,\n     dtype,\n     exp,\n+    expand_dims,\n     full,\n     fdiv,\n     float16,\n@@ -130,6 +131,7 @@\n     \"dot\",\n     \"dtype\",\n     \"exp\",\n+    \"expand_dims\",\n     \"extra\",\n     \"fdiv\",\n     \"float16\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 39, "deletions": 4, "changes": 43, "file_content_changes": "@@ -3,7 +3,7 @@\n from contextlib import contextmanager\n from enum import Enum\n from functools import wraps\n-from typing import Callable, List, TypeVar\n+from typing import Callable, List, Sequence, TypeVar\n \n import triton\n from . import semantic\n@@ -883,6 +883,41 @@ def reshape(input, shape, _builder=None):\n     shape = _shape_check_impl(shape)\n     return semantic.reshape(input, shape, _builder)\n \n+\n+def _wrap_axis(axis, ndim):\n+    if not (-ndim <= axis < ndim):\n+        raise ValueError(f\"invalid axis {axis}. Expected {-ndim} <= axis < {ndim}\")\n+\n+    return axis if axis >= 0 else axis + ndim\n+\n+\n+@builtin\n+def expand_dims(input, axis, _builder=None):\n+    \"\"\"\n+    Expand the shape of a tensor, by inserting new length-1 dimensions.\n+\n+    Axis indices are with respect to the resulting tensor, so\n+    ``result.shape[axis]`` will be 1 for each axis.\n+\n+    :param input: The input tensor.\n+    :type input: tl.tensor\n+    :param axis: The indices to add new axes\n+    :type axis: int | Sequence[int]\n+\n+    \"\"\"\n+    axis = _constexpr_to_value(axis)\n+    axes = list(axis) if isinstance(axis, Sequence) else [axis]\n+    new_ndim = len(input.shape) + len(axes)\n+    axes = [_wrap_axis(_constexpr_to_value(d), new_ndim) for d in axes]\n+\n+    if len(set(axes)) != len(axes):\n+        raise ValueError(f\"expand_dims recieved duplicate axes, normalized axes = {axes}\")\n+\n+    ret = input\n+    for a in sorted(axes):\n+        ret = semantic.expand_dims(ret, a, _builder)\n+    return ret\n+\n # -----------------------\n # Linear Algebra\n # -----------------------\n@@ -1281,9 +1316,9 @@ def _argreduce(input, axis, combine_fn, _builder=None, _generator=None):\n \n     if len(input.shape) > 1:\n         # Broadcast index across the non-reduced axes\n-        expand_dims_index = [constexpr(None)] * len(input.shape)\n-        expand_dims_index[axis] = slice(None)\n-        index = index.__getitem__(expand_dims_index, _builder=_builder)\n+        axes_to_expand = [constexpr(d) for d in range(len(input.shape))]\n+        del axes_to_expand[axis]\n+        index = expand_dims(index, axes_to_expand, _builder=_builder)\n         index = broadcast_to(index, input.shape, _builder=_builder)\n \n     rvalue, rindices = reduce((input, index), axis, combine_fn,"}]