[{"filename": "python/test/unit/runtime/test_cache.py", "status": "modified", "additions": 33, "deletions": 0, "changes": 33, "file_content_changes": "@@ -231,6 +231,39 @@ def test_compile_in_subproc() -> None:\n     assert proc.exitcode == 0\n \n \n+def compile_fn_dot(config, cc):\n+    @triton.jit\n+    def kernel_dot(Z):\n+        offs = tl.arange(0, 16)[:, None] * 16 + tl.arange(0, 16)[None, :]\n+        z = tl.load(Z + offs)\n+        z = tl.dot(z, z)\n+        tl.store(Z + offs, z)\n+\n+    triton.compile(\n+        fn=kernel_dot,\n+        signature={0: \"*fp32\"},\n+        device=0,\n+        configs=[config],\n+        warm_cache_only=True,\n+        cc=cc,\n+    )\n+\n+\n+def test_compile_in_forked_subproc() -> None:\n+    reset_tmp_dir()\n+    major, minor = torch.cuda.get_device_capability(0)\n+    cc = major * 10 + minor\n+    config = instance_descriptor(tuple(range(1)), ())\n+\n+    multiprocessing.set_start_method('fork')\n+    proc = multiprocessing.Process(\n+        target=compile_fn_dot,\n+        args=(config, cc))\n+    proc.start()\n+    proc.join()\n+    assert proc.exitcode == 0\n+\n+\n def test_memory_leak() -> None:\n     @triton.jit\n     def kernel(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 0, "deletions": 8, "changes": 8, "file_content_changes": "@@ -1185,14 +1185,6 @@ def dot(lhs: tl.tensor,\n         import torch\n     except ImportError:\n         raise ImportError(\"Triton requires PyTorch to be installed\")\n-    if torch.version.hip is None:\n-        device = triton.runtime.jit.get_current_device()\n-        capability = triton.runtime.jit.get_device_capability(device)\n-        capability = capability[0] * 10 + capability[1]\n-        if capability < 70:\n-            assert (\n-                not rhs.dtype.is_fp16() and not rhs.dtype.is_fp8()\n-            ), \"Float8 and Float16 types are not supported for compute capability < 70 (use Float32 or above)\"\n     assert lhs.type.is_block() and rhs.type.is_block()\n     assert lhs.dtype == rhs.dtype, \"lhs and rhs must have the same dtype!\"\n     assert len(lhs.shape) == 2 and len(rhs.shape) == 2"}]