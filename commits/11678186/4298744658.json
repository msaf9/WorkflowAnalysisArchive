[{"filename": ".github/workflows/torchinductor/scripts/run_torchinductor_perf.sh", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -27,6 +27,11 @@ cd \"$ROOT\" || exit\n for model in \"${MODELS[@]}\"; do\n   echo \"Checking performance test for $model\"\n   python3 \"$INDUCTOR\"/scripts/check_perf.py --new \"$TEST_REPORTS_DIR\"/\"$model\".csv --baseline \"$INDUCTOR\"/data/\"$model\".csv\n+  EXIT_STATUS=$?\n+  if [ \"$EXIT_STATUS\" -ne 0 ]; then\n+    echo \"Performance test for $model failed\"\n+    exit \"$EXIT_STATUS\"\n+  fi\n done\n \n # unlock GPU clocks"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 23, "deletions": 23, "changes": 46, "file_content_changes": "@@ -170,30 +170,30 @@ LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n }\n \n inline bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n-  //// Case 1: A size 1 tensor is not expensive since all threads will load the\n-  //// same\n-  //if (isSingleValue(op->getOperand(0)))\n-  //  return false;\n-  //auto ptr = op->getOperand(0);\n+  // Case 1: A size 1 tensor is not expensive since all threads will load the\n+  // same\n+  if (isSingleValue(op->getOperand(0)))\n+    return false;\n+  // auto ptr = op->getOperand(0);\n   //// Case 2: We assume that `evict_last` loads/stores have high hit rate\n-  //if (auto load = dyn_cast<triton::LoadOp>(op))\n-  //  if (load.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n-  //    return false;\n-  //if (auto store = dyn_cast<triton::StoreOp>(op))\n-  //  if (store.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n-  //    return false;\n-  //if (auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>()) {\n-  //  auto encoding = tensorTy.getEncoding();\n-  //  // Case 3: Different type conversion is expensive (e.g., mma <-> block)\n-  //  if (encoding.getTypeID() != targetEncoding.getTypeID())\n-  //    return true;\n-  //  auto sizePerThread = triton::gpu::getSizePerThread(encoding);\n-  //  auto targetSizePerThread = triton::gpu::getSizePerThread(targetEncoding);\n-  //  auto order = triton::gpu::getOrder(encoding);\n-  //  auto targetOrder = triton::gpu::getOrder(targetEncoding);\n-  //  // Case 4: The targeEncoding may expose more vectorization opportunities\n-  //  return sizePerThread[order[0]] >= targetSizePerThread[targetOrder[0]];\n-  //}\n+  // if (auto load = dyn_cast<triton::LoadOp>(op))\n+  //   if (load.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n+  //     return false;\n+  // if (auto store = dyn_cast<triton::StoreOp>(op))\n+  //   if (store.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n+  //     return false;\n+  // if (auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>()) {\n+  //   auto encoding = tensorTy.getEncoding();\n+  //   // Case 3: Different type conversion is expensive (e.g., mma <-> block)\n+  //   if (encoding.getTypeID() != targetEncoding.getTypeID())\n+  //     return true;\n+  //   auto sizePerThread = triton::gpu::getSizePerThread(encoding);\n+  //   auto targetSizePerThread = triton::gpu::getSizePerThread(targetEncoding);\n+  //   auto order = triton::gpu::getOrder(encoding);\n+  //   auto targetOrder = triton::gpu::getOrder(targetEncoding);\n+  //   // Case 4: The targeEncoding may expose more vectorization opportunities\n+  //   return sizePerThread[order[0]] >= targetSizePerThread[targetOrder[0]];\n+  // }\n   return true;\n }\n "}]