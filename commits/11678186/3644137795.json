[{"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -114,6 +114,7 @@ void populateArithmeticPatternsAndLegality(\n       GenericOpPattern<arith::TruncIOp>, GenericOpPattern<arith::TruncFOp>,\n       GenericOpPattern<arith::ExtUIOp>, GenericOpPattern<arith::ExtSIOp>,\n       GenericOpPattern<arith::ExtFOp>, GenericOpPattern<arith::SIToFPOp>,\n+      GenericOpPattern<arith::FPToSIOp>, GenericOpPattern<arith::FPToUIOp>,\n       GenericOpPattern<arith::UIToFPOp>>(typeConverter, context);\n }\n "}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -677,6 +677,7 @@ def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr)\n     kernel[(1,)](z_tri, x_tri, axis, shape0, shape1)\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n \n+\n def test_atomic_cas():\n     # 1. make sure that atomic_cas changes the original value (Lock)\n     @triton.jit\n@@ -742,9 +743,11 @@ def test_cast(dtype_x, dtype_z, bitcast, device='cuda'):\n     # triton kernel\n     @triton.jit\n     def kernel(X, Z, BITCAST: tl.constexpr):\n-        x = tl.load(X)\n+        x_ptr = X + tl.arange(0, 1)\n+        z_ptr = Z + tl.arange(0, 1)\n+        x = tl.load(x_ptr)\n         z = x.to(Z.dtype.element_ty, bitcast=BITCAST)\n-        tl.store(Z, z)\n+        tl.store(z_ptr, z)\n \n     dtype_z_np = dtype_z if dtype_z != 'int1' else 'bool_'\n     # triton result"}]