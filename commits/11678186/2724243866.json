[{"filename": ".github/workflows/wheels.yml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -18,7 +18,7 @@ jobs:\n       - name: Patch setup.py\n         run: |\n           #sed -i 's/name\\=\\\"triton\\\"/name=\"triton-nightly\"/g' python/setup.py\n-          export LATEST_DATE=$(git show -s --format=%ci `git rev-parse HEAD` | cut -d ' ' -f 1 | sed 's/-//g')\n+          export LATEST_DATE=$(TZ=UTC0 git show --quiet --date='format-local:%Y%m%d' --format=\"%cd\")\n           sed -i -r \"s/version\\=\\\"(.*)\\\"/version=\\\"\\1-dev\"$LATEST_DATE\"\\\"/g\" python/setup.py\n           echo \"\" >> python/setup.cfg\n           echo \"[build_ext]\" >> python/setup.cfg"}, {"filename": "lib/codegen/selection/generator.cc", "status": "modified", "additions": 63, "deletions": 23, "changes": 86, "file_content_changes": "@@ -86,7 +86,7 @@ Value* geper::operator()(Value *ptr, Value* off, const std::string& name){\n // types\n #define void_ty              builder_->getVoidTy()\n #define f16_ty               builder_->getHalfTy()\n-#define bf16_ty              builder_->getBFloatTy()\n+#define bf16_ty              builder_->getInt16Ty()\n #define f32_ty               builder_->getFloatTy()\n #define i1_ty                builder_->getInt1Ty()\n #define i8_ty                builder_->getInt8Ty()\n@@ -178,7 +178,7 @@ Type *generator::cvt(ir::type *ty) {\n     case ir::type::VoidTyID:      return Type::getVoidTy(*ctx_);\n     case ir::type::FP8TyID:       return Type::getInt8Ty(*ctx_);\n     case ir::type::FP16TyID:      return Type::getHalfTy(*ctx_);\n-    case ir::type::BF16TyID:      return Type::getBFloatTy(*ctx_);\n+    case ir::type::BF16TyID:      return Type::getInt16Ty(*ctx_); // use int16 as storage type\n     case ir::type::FP32TyID:      return Type::getFloatTy(*ctx_);\n     case ir::type::FP64TyID:      return Type::getDoubleTy(*ctx_);\n     case ir::type::LabelTyID:     return Type::getLabelTy(*ctx_);\n@@ -378,8 +378,8 @@ void generator::visit_launch_inst(ir::launch_inst *launch) {\n  */\n void generator::visit_binary_operator(ir::binary_operator*x) {\n   using ll = llvm::Instruction::BinaryOps;\n+  using tt = ir::binary_op_t;\n   auto cvt = [](ir::binary_op_t op){\n-    using tt = ir::binary_op_t;\n     switch(op) {\n       case tt::Add: return ll::Add;\n       case tt::FAdd: return ll::FAdd;\n@@ -406,20 +406,51 @@ void generator::visit_binary_operator(ir::binary_operator*x) {\n   for(indices_t idx: idxs_.at(x)){\n     Value *lhs = vals_[x->get_operand(0)][idx];\n     Value *rhs = vals_[x->get_operand(1)][idx];\n-    auto op = cvt(x->get_op());\n-    if(op == ll::Add)\n-       vals_[x][idx] = add(lhs, rhs);\n-     else if(op == ll::Mul)\n-       vals_[x][idx] = mul(lhs, rhs);\n-     else if(op == ll::FDiv && !x->get_fdiv_ieee_rounding() &&\n-             x->get_type()->get_scalar_ty()->is_fp32_ty()){\n-       InlineAsm *ptx = InlineAsm::get(FunctionType::get(f32_ty, {f32_ty, f32_ty}, false),\n-                                      \" div.full.f32 $0, $1, $2;\", \"=r,r,r\", false);\n-       vals_[x][idx] = builder_->CreateCall(ptx, {lhs, rhs});\n+    // manually select bf16 bin op\n+    if (x->get_operand(0)->get_type()->get_scalar_ty()->is_bf16_ty()) {\n+      assert(x->get_operand(1)->get_type()->get_scalar_ty()->is_bf16_ty());\n+      if (x->get_op() == tt::FAdd) {  // a + b = a * 1.0 + b\n+        InlineAsm *bf16_add_asm =\n+            InlineAsm::get(FunctionType::get(bf16_ty, {bf16_ty, bf16_ty}, false),\n+                           \"{ .reg .b16 c;         \\n\\t\"\n+                           \"   mov.b16 c, 0x3f80U; \\n\\t\" // 1.0\n+                           \"   fma.rn.bf16 $0, $1, c, $2; } \\n\\t\",\n+                           \"=h,h,h\", false);\n+        vals_[x][idx] = builder_->CreateCall(bf16_add_asm, {lhs, rhs});\n+      } else if (x->get_op() == tt::FSub) {  // a - b = b * (-1.0) + a\n+        InlineAsm *bf16_sub_asm =\n+            InlineAsm::get(FunctionType::get(bf16_ty, {bf16_ty, bf16_ty}, false),\n+                           \" { .reg .b16 c;         \\n\\t\"\n+                           \"    mov.b16 c, 0xbf80U; \\n\\t\" // -1.0\n+                           \"    fma.rn.bf16 $0, $2, c, $1;} \\n\\t\",\n+                           \"=h,h,h\", false);\n+        vals_[x][idx] = builder_->CreateCall(bf16_sub_asm, {lhs, rhs});\n+      } else if (x->get_op() == tt::FMul) {  // a * b = a*b + 0\n+        InlineAsm *bf16_mul_asm =\n+          InlineAsm::get(FunctionType::get(bf16_ty, {bf16_ty, bf16_ty}, false),\n+                           \" { .reg .b16 c;        \\n\\t\"\n+                           \"    mov.b16 c, 0x8000U; \\n\\t\" // 0.0\n+                           \"    fma.rn.bf16 $0, $1, $2, c;} \\n\\t\",\n+                           \"=h,h,h\", false);\n+        vals_[x][idx] = builder_->CreateCall(bf16_mul_asm, {lhs, rhs});\n+      } else\n+        throw std::runtime_error(\"invalid bin op for bf16\");\n+    } else {  // not bf16\n+      auto op = cvt(x->get_op());\n+      if(op == ll::Add)\n+        vals_[x][idx] = add(lhs, rhs);\n+      else if(op == ll::Mul)\n+        vals_[x][idx] = mul(lhs, rhs);\n+      else if(op == ll::FDiv && !x->get_fdiv_ieee_rounding() &&\n+              x->get_type()->get_scalar_ty()->is_fp32_ty()){\n+        InlineAsm *ptx = InlineAsm::get(FunctionType::get(f32_ty, {f32_ty, f32_ty}, false),\n+                                        \" div.full.f32 $0, $1, $2;\", \"=r,r,r\", false);\n+        vals_[x][idx] = builder_->CreateCall(ptx, {lhs, rhs});\n \n-     }\n-     else\n-       vals_[x][idx] = bin_op(op, lhs, rhs);\n+      }\n+      else\n+        vals_[x][idx] = bin_op(op, lhs, rhs);\n+    }\n   }\n }\n \n@@ -970,8 +1001,6 @@ void generator::visit_store_inst(ir::store_inst * x){\n   has_l2_evict_policy = false;\n   auto idxs    = idxs_.at(val_op);\n   Type *ty = cvt(val_op->get_type()->get_scalar_ty());\n-  if (ty->isBFloatTy()) // llvm11-nvptx cannot select bf16 store\n-    ty = f16_ty;\n   if(ty->isIntegerTy(1))\n     ty = builder_->getInt8Ty();\n   for(size_t i = 0; i < idxs.size(); i += vec){\n@@ -2830,9 +2859,6 @@ void generator::visit_layout_convert(ir::value *out, ir::value *in){\n   // pointer to temporary shared memory\n   Type *ty = cvt(out->get_type()->get_scalar_ty());\n \n-  if (ty->isBFloatTy()) // llvm11-nvptx cannot select bf16 store\n-    ty = f16_ty;\n-\n   // Orders\n   analysis::distributed_layout* in_layout = dynamic_cast<analysis::distributed_layout*>(layouts_->get(in));\n   analysis::distributed_layout* out_layout = dynamic_cast<analysis::distributed_layout*>(layouts_->get(out));\n@@ -3229,8 +3255,22 @@ void generator::visit_constant_int(ir::constant_int *x){\n \n void generator::visit_constant_fp(ir::constant_fp *x){\n   Type *ty = cvt(x->get_type()->get_scalar_ty());\n-  for(indices_t idx: idxs_.at(x))\n-    vals_[x][idx] = ConstantFP::get(ty, x->get_value());\n+  for(indices_t idx: idxs_.at(x)) {\n+    // manually select bf16 constant\n+    if (x->get_type()->get_scalar_ty()->is_bf16_ty()) {\n+      // highest 16 bits of fp32\n+      float fp32_value = x->get_value();\n+      uint16_t bf16_raw = (*reinterpret_cast<uint32_t*>(&fp32_value) \n+                            & 0xffff0000) >> 16;\n+      std::stringstream const_str;\n+      const_str << \"0x\" << std::hex << bf16_raw << \"U\"; // unsigned\n+      InlineAsm *bf16_const = InlineAsm::get(FunctionType::get(bf16_ty, {}, false),\n+                                             \" mov.b16 $0, \" + const_str.str() + \";\",\n+                                             \"=h\", false);\n+      vals_[x][idx] = builder_->CreateCall(bf16_const, {});\n+    } else\n+      vals_[x][idx] = ConstantFP::get(ty, x->get_value());\n+  }\n }\n \n void generator::visit_alloc_const(ir::alloc_const *alloc) {"}, {"filename": "lib/ir/constant.cc", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -18,6 +18,8 @@ constant *constant::get_null_value(type *ty) {\n     return constant_int::get(ty, 0);\n   case type::FP16TyID:\n     return constant_fp::get(type::get_fp16_ty(ctx), 0);\n+  case type::BF16TyID:\n+    return constant_fp::get(type::get_bf16_ty(ctx), 0);\n   case type::FP32TyID:\n     return constant_fp::get(type::get_fp32_ty(ctx), 0);\n   case type::FP64TyID:"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 8, "deletions": 1, "changes": 9, "file_content_changes": "@@ -236,8 +236,14 @@ void parse_args(py::list& args, py::list do_not_specialize, const std::string& f\n         continue;\n       }\n       // argument is `constexpr`\n-      if(py::hasattr(arg, \"value\")){\n+      if (py::hasattr(arg, \"value\")) {\n         py::object value = arg.attr(\"value\");\n+        // check if value is a callable object using PyCallable_Check\n+        if (PyCallable_Check(value.ptr())) {\n+          throw std::runtime_error(\n+              \"constant argument cannot be a callable object: \" +\n+              std::string(py::str(arg)));\n+        }\n         py::object name = arg_names[i];\n         constants[name] = value;\n         py::object repr = py::repr(value);\n@@ -872,6 +878,7 @@ void init_triton_ir(py::module &&m) {\n       .def(\"create_int_cast\", &ir::builder::create_int_cast, ret::reference)\n       .def(\"create_downcast\", &ir::builder::create_downcast, ret::reference)\n       .def(\"create_int_to_ptr\", &ir::builder::create_int_to_ptr, ret::reference)\n+      .def(\"create_ptr_to_int\", &ir::builder::create_ptr_to_int, ret::reference)\n       // phi\n       .def(\"create_phi\", &ir::builder::create_phi, ret::reference)\n       // Binary instructions"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 125, "deletions": 27, "changes": 152, "file_content_changes": "@@ -17,6 +17,7 @@\n uint_dtypes = ['uint8', 'uint16', 'uint32', 'uint64']\n float_dtypes = ['float16', 'float32', 'float64']\n dtypes = int_dtypes + uint_dtypes + float_dtypes\n+dtypes_with_bfloat16 = dtypes + ['bfloat16']\n \n \n def _bitwidth(dtype: str) -> int:\n@@ -33,27 +34,39 @@ def numpy_random(shape, dtype_str, rs: Optional[RandomState] = None, low=None, h\n         shape = (shape, )\n     if rs is None:\n         rs = RandomState(seed=17)\n-    dtype = getattr(np, dtype_str)\n     if dtype_str in int_dtypes + uint_dtypes:\n         iinfo = np.iinfo(getattr(np, dtype_str))\n         low = iinfo.min if low is None else max(low, iinfo.min)\n         high = iinfo.max if high is None else min(high, iinfo.max)\n+        dtype = getattr(np, dtype_str)\n         x = rs.randint(low, high, shape, dtype=dtype)\n         x[x == 0] = 1  # Hack. Never return zero so tests of division don't error out.\n         return x\n     elif dtype_str in float_dtypes:\n-        return rs.normal(0, 1, shape).astype(dtype)\n+        return rs.normal(0, 1, shape).astype(dtype_str)\n+    elif dtype_str == 'bfloat16':\n+        return (rs.normal(0, 1, shape).astype('float32').view('uint32')\n+                & np.uint32(0xffff0000)).view('float32')\n+    elif dtype_str in ['bool', 'int1', 'bool_']:\n+        return rs.normal(0, 1, shape) > 0.0\n     else:\n         raise RuntimeError(f'Unknown dtype {dtype_str}')\n \n \n-def to_triton(x: np.ndarray, device='cuda') -> Union[TensorWrapper, torch.Tensor]:\n+def to_triton(x: np.ndarray, device='cuda', dst_type=None) -> Union[TensorWrapper, torch.Tensor]:\n+    '''\n+    Note: We need dst_type becasue the type of x can be different from dst_type.\n+          For example: x is of type `float32`, dst_type is `bfloat16`.\n+          If dst_type is None, we infer dst_type from x.\n+    '''\n     t = x.dtype.name\n     if t in uint_dtypes:\n         signed_type_name = t.lstrip('u')  # e.g. \"uint16\" -> \"int16\"\n         x_signed = x.astype(getattr(np, signed_type_name))\n         return reinterpret(torch.tensor(x_signed, device=device), getattr(tl, t))\n     else:\n+        if t == 'float32' and dst_type == 'bfloat16':\n+            return torch.tensor(x, device=device).bfloat16()\n         return torch.tensor(x, device=device)\n \n \n@@ -72,6 +85,8 @@ def to_numpy(x):\n     if isinstance(x, TensorWrapper):\n         return x.base.cpu().numpy().astype(getattr(np, torch_dtype_name(x.dtype)))\n     elif isinstance(x, torch.Tensor):\n+        if x.dtype is torch.bfloat16:\n+            return x.cpu().float().numpy()\n         return x.cpu().numpy()\n     else:\n         raise ValueError(f\"Not a triton-compatible tensor: {x}\")\n@@ -84,19 +99,30 @@ def patch_kernel(template, to_replace):\n     return kernel\n \n \n-@pytest.mark.parametrize(\"dtype_x\", [dtype_x for dtype_x in dtypes])\n+def check_type_supported(dtype):\n+    '''\n+    skip test if dtype is not supported on the current device\n+    '''\n+    cc = _triton.runtime.cc(_triton.runtime.backend.CUDA, torch.cuda.current_device())\n+    if cc < 80 and (dtype is tl.bfloat16 or dtype == \"bfloat16\" or dtype is torch.bfloat16):\n+        pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n+\n+\n+@pytest.mark.parametrize(\"dtype_x\", [dtype_x for dtype_x in dtypes] + [\"bfloat16\"])\n def test_empty_kernel(dtype_x, device='cuda'):\n     SIZE = 128\n \n     @triton.jit\n     def kernel(X, SIZE: tl.constexpr):\n         pass\n-    x = to_triton(numpy_random(SIZE, dtype_str=dtype_x), device=device)\n+    check_type_supported(dtype_x)\n+    x = to_triton(numpy_random(SIZE, dtype_str=dtype_x), device=device, dst_type=dtype_x)\n     kernel[(1, )](x, SIZE=SIZE, num_warps=4)\n \n \n # generic test functions\n def _test_unary(dtype_x, expr, numpy_expr=None, device='cuda'):\n+    check_type_supported(dtype_x)  # early return if dtype_x is not supported\n     SIZE = 128\n     # define the kernel / launch-grid\n \n@@ -115,8 +141,8 @@ def kernel(Z, X, SIZE: tl.constexpr):\n     # reference result\n     z_ref = eval(expr if numpy_expr is None else numpy_expr)\n     # triton result\n-    x_tri = to_triton(x, device=device)\n-    z_tri = to_triton(np.empty_like(z_ref), device=device)\n+    x_tri = to_triton(x, device=device, dst_type=dtype_x)\n+    z_tri = to_triton(np.empty_like(z_ref), device=device, dst_type=dtype_x)\n     kernel[(1, )](z_tri, x_tri, SIZE=SIZE, num_warps=4)\n     # compare\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n@@ -154,6 +180,8 @@ def _binary_op_dtype_override(a: str, b: str) -> Optional[np.dtype]:\n \n \n def _test_binary(dtype_x, dtype_y, expr, numpy_expr=None, mode_x='real', mode_y='real', device='cuda', y_low=None, y_high=None):\n+    check_type_supported(dtype_x)  # early return if dtype_x is not supported\n+    check_type_supported(dtype_y)\n     SIZE = 128\n     # define the kernel / launch-grid\n \n@@ -180,8 +208,8 @@ def kernel(Z, X, Y, SIZE: tl.constexpr):\n     if dtype_z is not None:\n         z_ref = z_ref.astype(dtype_z)\n     # triton result\n-    x_tri = to_triton(x, device=device)\n-    y_tri = to_triton(y, device=device)\n+    x_tri = to_triton(x, device=device, dst_type=dtype_x)\n+    y_tri = to_triton(y, device=device, dst_type=dtype_y)\n     z_tri = to_triton(np.empty(SIZE, dtype=z_ref.dtype), device=device)\n     kernel[(1, )](z_tri, x_tri, y_tri, SIZE=SIZE, num_warps=4)\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), err_msg=expr, rtol=0.01)\n@@ -193,15 +221,20 @@ def _mod_operation_ill_conditioned(dtype_x, dtype_y) -> bool:\n     # remainders than stock LLVM. We currently don't expect to match it\n     # bit-for-bit.\n     return (dtype_x, dtype_y) in [\n+        ('int32', 'bfloat16'),\n         ('int32', 'float16'),\n         ('int32', 'float32'),\n+        ('int64', 'bfloat16'),\n         ('int64', 'float16'),\n         ('int64', 'float32'),\n         ('int64', 'float64'),\n+        ('uint16', 'bfloat16'),\n         ('uint16', 'float16'),\n         ('uint16', 'float32'),\n+        ('uint32', 'bfloat16'),\n         ('uint32', 'float16'),\n         ('uint32', 'float32'),\n+        ('uint64', 'bfloat16'),\n         ('uint64', 'float16'),\n         ('uint64', 'float32'),\n         ('uint64', 'float64'),\n@@ -215,15 +248,15 @@ def _mod_operation_ill_conditioned(dtype_x, dtype_y) -> bool:\n @pytest.mark.parametrize(\"dtype_x, dtype_y, op\", [\n     (dtype_x, dtype_y, op)\n     for op in ['+', '-', '*', '/', '%']\n-    for dtype_x in dtypes\n-    for dtype_y in dtypes\n+    for dtype_x in dtypes_with_bfloat16\n+    for dtype_y in dtypes_with_bfloat16\n ])\n def test_bin_op(dtype_x, dtype_y, op, device='cuda'):\n     expr = f' x {op} y'\n     if op == '%' and dtype_x in int_dtypes + uint_dtypes and dtype_y in int_dtypes + uint_dtypes:\n         # LLVM has 'numpy.fmod', not 'numpy.remainder', semantics on integer remainders.\n         numpy_expr = 'np.fmod(x, y)'\n-    elif op in ('/', '%') and dtype_x in ('int16', 'float16') and dtype_y in ('int16', 'float16'):\n+    elif op in ('/', '%') and dtype_x in ('int16', 'float16', 'bfloat16') and dtype_y in ('int16', 'float16', 'bfloat16'):\n         # Triton promotes 16-bit floating-point / and % to 32-bit because there\n         # are no native div or FRem operations on float16. Since we have to\n         # convert anyway, we may as well take the accuracy bump.\n@@ -266,8 +299,8 @@ def test_floordiv(dtype_x, dtype_y, device='cuda'):\n @pytest.mark.parametrize(\"dtype_x, dtype_y, op\", [\n     (dtype_x, dtype_y, op)\n     for op in ['&', '|', '^']\n-    for dtype_x in dtypes\n-    for dtype_y in dtypes\n+    for dtype_x in dtypes + dtypes_with_bfloat16\n+    for dtype_y in dtypes + dtypes_with_bfloat16\n ])\n def test_bitwise_op(dtype_x, dtype_y, op, device='cuda'):\n     expr = f'x {op} y'\n@@ -333,11 +366,55 @@ def test_compare_op(dtype_x, dtype_y, op, mode_x, mode_y, device='cuda'):\n     _test_binary(dtype_x, dtype_y, expr, numpy_expr, mode_x=mode_x, mode_y=mode_y, device=device)\n \n \n+# ---------------\n+# test where\n+# ---------------\n+@pytest.mark.parametrize(\"dtype\", dtypes_with_bfloat16 + [\"*int32\"])\n+def test_where(dtype):\n+    select_ptrs = False\n+    if dtype == \"*int32\":\n+        dtype = \"int64\"\n+        select_ptrs = True\n+    check_type_supported(dtype)\n+\n+    @triton.jit\n+    def where_kernel(cond_ptr, a_ptr, b_ptr, output_ptr, n_elements,\n+                     BLOCK_SIZE: tl.constexpr,\n+                     TEST_POINTERS: tl.constexpr):\n+        offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+        mask = offsets < n_elements\n+        decide = tl.load(cond_ptr + offsets, mask=mask)\n+        if TEST_POINTERS:\n+            a = tl.load(a_ptr + offsets, mask=mask).to(tl.pi32_t)\n+            b = tl.load(b_ptr + offsets, mask=mask).to(tl.pi32_t)\n+        else:\n+            a = tl.load(a_ptr + offsets, mask=mask)\n+            b = tl.load(b_ptr + offsets, mask=mask)\n+        output = tl.where(decide, a, b)\n+        tl.store(output_ptr + offsets, output, mask=mask)\n+\n+    SIZE = 1_000\n+    rs = RandomState(17)\n+    cond = numpy_random(SIZE, 'bool', rs)\n+    x = numpy_random(SIZE, dtype_str=dtype, rs=rs)\n+    y = numpy_random(SIZE, dtype_str=dtype, rs=rs)\n+    z = np.where(cond, x, y)\n+\n+    cond_tri = to_triton(cond, device='cuda')\n+    x_tri = to_triton(x, device='cuda', dst_type=dtype)\n+    y_tri = to_triton(y, device='cuda', dst_type=dtype)\n+    z_tri = to_triton(np.empty(SIZE, dtype=z.dtype), device='cuda', dst_type=dtype)\n+\n+    grid = lambda meta: (triton.cdiv(SIZE, meta['BLOCK_SIZE']),)\n+    where_kernel[grid](cond_tri, x_tri, y_tri, z_tri, SIZE, BLOCK_SIZE=1024, TEST_POINTERS=select_ptrs)\n+    assert (z == to_numpy(z_tri)).all()\n+\n+\n # ---------------\n # test unary ops\n # ---------------\n @pytest.mark.parametrize(\"dtype_x, expr\", [\n-    (dtype_x, ' -x') for dtype_x in dtypes\n+    (dtype_x, ' -x') for dtype_x in dtypes_with_bfloat16\n ] + [\n     (dtype_x, ' ~x') for dtype_x in int_dtypes\n ])\n@@ -732,9 +809,10 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n @pytest.mark.parametrize(\"op, dtype_str, shape\",\n                          [(op, dtype, shape)\n                           for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n-                          for dtype in dtypes\n+                          for dtype in dtypes_with_bfloat16\n                           for shape in [32, 64, 128, 512]])\n def test_reduce1d(op, dtype_str, shape, device='cuda'):\n+    check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n \n     # triton kernel\n     @triton.jit\n@@ -752,9 +830,18 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n                 'argmin': np.argmin, 'argmax': np.argmax}[op]\n     # numpy result\n     z_dtype_str = 'int32' if op == 'argmin' or op == 'argmax' else dtype_str\n-    z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n+    z_tri_dtype_str = z_dtype_str\n+    if op not in ['argmin', 'argmax'] and dtype_str == 'bfloat16':\n+        z_dtype_str = 'float32'\n+        z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n+        # trunc mantissa for a fair comparison of accuracy\n+        z_ref = (z_ref.view('uint32') & np.uint32(0xffff0000)).view('float32')\n+        z_tri_dtype_str = 'bfloat16'\n+    else:\n+        z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n     # triton result\n-    z_tri = to_triton(numpy_random((1,), dtype_str=z_dtype_str, rs=rs), device=device)\n+    z_tri = to_triton(numpy_random((1,), dtype_str=z_dtype_str, rs=rs),\n+                      device=device, dst_type=z_tri_dtype_str)\n     kernel[(1,)](x_tri, z_tri, BLOCK=shape)\n     z_tri = to_numpy(z_tri)\n     # compare\n@@ -770,7 +857,7 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n \n \n reduce_configs1 = [\n-    (op, dtype, (1, 1024), axis) for dtype in dtypes\n+    (op, dtype, (1, 1024), axis) for dtype in dtypes_with_bfloat16\n     for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n     for axis in [1]\n ]\n@@ -805,11 +892,19 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n     numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n                 'argmin': np.argmin, 'argmax': np.argmax}[op]\n     z_dtype_str = 'int32' if op == 'argmin' or op == 'argmax' else dtype_str\n+    z_tri_dtype_str = z_dtype_str\n     # numpy result\n-    z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n+    if op not in ['argmin', 'argmax'] and dtype_str == 'bfloat16':\n+        z_dtype_str = 'float32'\n+        z_tri_dtype_str = 'bfloat16'\n+        z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n+        # trunc mantissa for a fair comparison of accuracy\n+        z_ref = (z_ref.view('uint32') & np.uint32(0xffff0000)).view('float32')\n+    else:\n+        z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n     # triton result\n     z_tri = to_triton(numpy_random((shape[1 - axis],), dtype_str=z_dtype_str, rs=rs),\n-                      device=device)\n+                      device=device, dst_type=z_tri_dtype_str)\n     kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis)\n     z_tri = to_numpy(z_tri)\n     # compare\n@@ -834,10 +929,11 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n \n @pytest.mark.parametrize(\"dtype_str, shape, perm\",\n                          [(dtype, shape, perm)\n-                          for dtype in ['float16', 'float32']\n+                          for dtype in ['bfloat16', 'float16', 'float32']\n                              for shape in [(64, 64), (128, 128)]\n                              for perm in [(1, 0)]])\n def test_permute(dtype_str, shape, perm, device='cuda'):\n+    check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n \n     # triton kernel\n     @triton.jit\n@@ -852,16 +948,16 @@ def kernel(X, stride_xm, stride_xn,\n     # input\n     x = numpy_random(shape, dtype_str=dtype_str)\n     # triton result\n-    z_tri = to_triton(np.empty_like(x), device=device)\n-    z_tri_contiguous = to_triton(np.empty_like(x), device=device)\n-    x_tri = to_triton(x, device=device)\n+    z_tri = to_triton(np.empty_like(x), device=device, dst_type=dtype_str)\n+    z_tri_contiguous = to_triton(np.empty_like(x), device=device, dst_type=dtype_str)\n+    x_tri = to_triton(x, device=device, dst_type=dtype_str)\n     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n                          z_tri, z_tri.stride(1), z_tri.stride(0),\n                          BLOCK_M=shape[0], BLOCK_N=shape[1])\n     pgm_contiguous = kernel[(1, 1)](x_tri, x_tri.stride(1), x_tri.stride(0),\n                                     z_tri_contiguous, z_tri_contiguous.stride(0), z_tri_contiguous.stride(1),\n                                     BLOCK_M=shape[0], BLOCK_N=shape[1])\n-    # torch result\n+    # numpy result\n     z_ref = x.transpose(*perm)\n     # compare\n     triton.testing.assert_almost_equal(z_tri, z_ref)\n@@ -1038,8 +1134,10 @@ def _kernel(z, BLOCK: tl.constexpr,\n # Testing masked loads with an intermate copy to shared memory run.\n \n \n-@pytest.mark.parametrize(\"dtype\", [torch.float16, torch.float32])\n+@pytest.mark.parametrize(\"dtype\", [torch.bfloat16, torch.float16, torch.float32])\n def test_masked_load_shared_memory(dtype, device='cuda'):\n+    check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n+\n     M = 32\n     N = 32\n     K = 16"}, {"filename": "python/test/unit/operators/test_cross_entropy.py", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "file_content_changes": "@@ -2,18 +2,22 @@\n import torch\n \n import triton\n+import triton._C.libtriton.triton as _triton\n \n \n @pytest.mark.parametrize(\"M, N, dtype, mode\",\n                          [\n                              (M, N, dtype, mode) for M in [1024, 821]\n                              for N in [512, 857, 1871, 2089, 8573, 31000]\n-                             for dtype in ['float16', 'float32']\n+                             for dtype in ['bfloat16', 'float16', 'float32']\n                              for mode in ['forward', 'backward']\n                          ]\n                          )\n def test_op(M, N, dtype, mode):\n-    dtype = {'float16': torch.float16, 'float32': torch.float32}[dtype]\n+    cc = _triton.runtime.cc(_triton.runtime.backend.CUDA, torch.cuda.current_device())\n+    if cc < 80 and dtype == \"bfloat16\":\n+        pytest.skip(\"Only test bfloat16 on devices with sm >= 80\")\n+    dtype = {'bfloat16': torch.bfloat16, 'float16': torch.float16, 'float32': torch.float32}[dtype]\n     # create inputs\n     x = torch.randn(M, N, dtype=dtype, device='cuda', requires_grad=True)\n     idx = 4 + torch.ones(M, dtype=torch.int64, device='cuda')"}, {"filename": "python/test/unit/runtime/test_cache.py", "status": "modified", "additions": 20, "deletions": 0, "changes": 20, "file_content_changes": "@@ -130,3 +130,23 @@ def get_cache_str(*args, **kwargs):\n     cache_str_match = re.match(r'_(\\w+)\\[multipleof\\(\\d+\\)]_float32\\*\\[multipleof\\(16\\)\\]', cache_str[-1])\n     spec_type = None if cache_str_match is None else cache_str_match.group(1)\n     assert spec_type == value_type\n+\n+\n+def test_constexpr_not_callable() -> None:\n+    @triton.jit\n+    def kernel(X, c: tl.constexpr):\n+        tl.store(X, 2)\n+\n+    x = torch.empty(1, dtype=torch.int32, device='cuda')\n+    error = False\n+    try:\n+        kernel[(1, )](x, c=\"str\")\n+    except BaseException:\n+        error = True\n+    assert error is False\n+    # try and catch\n+    try:\n+        kernel[(1, )](x, c=tl.abs)\n+    except BaseException:\n+        error = True\n+    assert error is True"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 33, "deletions": 20, "changes": 53, "file_content_changes": "@@ -58,14 +58,22 @@ def computation_type_impl(a_ty: tl.dtype, b_ty: tl.dtype, div_or_mod: bool) -> t\n         return tl.float32\n     # 3 ) if one operand is half, the other is implicitly converted to half\n     #     unless we're doing / or %, which do not exist natively in PTX for fp16.\n+    #     Supported PTX op: add, sub, mul, fma, neg, abs, min, max, tanh, ex2, setp\n     if a_ty.is_fp16() or b_ty.is_fp16():\n         if div_or_mod:\n             return tl.float32\n         else:\n             return tl.float16\n+    # 4) return bf16 only if both operands are of bf16\n+    if a_ty.is_bf16() or b_ty.is_bf16():\n+        if div_or_mod:\n+            return tl.float32\n+        if a_ty.is_bf16() and b_ty.is_bf16():\n+            return tl.bfloat16\n+        return tl.float32\n     if not a_ty.is_int() or not b_ty.is_int():\n         assert False\n-    # 4 ) both operands are integer and undergo\n+    # 5 ) both operands are integer and undergo\n     #    integer promotion\n     if div_or_mod and a_ty.int_signedness != b_ty.int_signedness:\n         raise ValueError(\"Cannot use /, #, or % with \" + a_ty.__repr__() + \" and \" + b_ty.__repr__() + \" because they have different signedness;\"\n@@ -629,7 +637,7 @@ def cast(input: tl.tensor,\n     if src_sca_ty.is_ptr() and dst_sca_ty.is_int():\n         bitwidth = dst_sca_ty.int_bitwidth\n         if bitwidth == 64:\n-            return tl.tensor(builder.create_cast(ir.PtrToInt, input.handle, dst_ty.to_ir(builder)),\n+            return tl.tensor(builder.create_ptr_to_int(input.handle, dst_ty.to_ir(builder)),\n                              dst_ty)\n         if bitwidth == 1:\n             return not_equal(cast(input, tl.int64, builder),\n@@ -768,16 +776,25 @@ def atomic_cas(ptr: tl.tensor,\n                cmp: tl.tensor,\n                val: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    # TODO: type checking\n+    element_ty = ptr.type.scalar.element_ty\n+    if element_ty.primitive_bitwidth not in [16, 32, 64]:\n+        raise ValueError(\"atomic_cas only supports elements with width {16, 32, 64}\")\n     return tl.tensor(builder.create_atomic_cas(ptr.handle, cmp.handle, val.handle), val.type)\n \n \n def atom_red_typechecking_impl(ptr: tl.tensor,\n                                val: tl.tensor,\n                                mask: tl.tensor,\n+                               op: str,\n                                builder: ir.builder) -> Tuple[tl.tensor, tl.tensor, tl.tensor]:\n     if not ptr.type.scalar.is_ptr():\n         raise ValueError(\"Pointer argument of store instruction is \" + ptr.type.__repr__())\n+\n+    element_ty = ptr.type.scalar.element_ty\n+    if element_ty is tl.float16 and op != 'add':\n+        raise ValueError(\"atomic_\" + op + \" does not support fp16\")\n+    if element_ty in [tl.int1, tl.int8, tl.int16, tl.bfloat16]:\n+        raise ValueError(\"atomic_\" + op + \" does not support \" + element_ty)\n     if ptr.type.is_block():\n         if mask:\n             mask = broadcast_impl_shape(mask, ptr.type.get_block_shapes(), builder)\n@@ -798,7 +815,7 @@ def atomic_max(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'max', builder)\n     sca_ty = val.type.scalar\n     # direct call to atomic_max for integers\n     if sca_ty.is_int():\n@@ -830,7 +847,7 @@ def atomic_min(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'min', builder)\n     sca_ty = val.type.scalar\n     # direct call to atomic_min for integers\n     if sca_ty.is_int():\n@@ -870,7 +887,7 @@ def atomic_add(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'add', builder)\n     sca_ty = val.type.scalar\n     op = ir.ATOMIC_OP.FADD if sca_ty.is_floating() else ir.ATOMIC_OP.ADD\n     return tl.tensor(builder.create_atomic_rmw(op, ptr.handle, val.handle, mask.handle), val.type)\n@@ -880,31 +897,31 @@ def atomic_and(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'and', builder)\n     return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.AND, ptr.handle, val.handle, mask.handle), val.type)\n \n \n def atomic_or(ptr: tl.tensor,\n               val: tl.tensor,\n               mask: tl.tensor,\n               builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'or', builder)\n     return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.OR, ptr.handle, val.handle, mask.handle), val.type)\n \n \n def atomic_xor(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'xor', builder)\n     return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XOR, ptr.handle, val.handle, mask.handle), val.type)\n \n \n def atomic_xchg(ptr: tl.tensor,\n                 val: tl.tensor,\n                 mask: tl.tensor,\n                 builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'xchg', builder)\n     return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XCHG, ptr.handle, val.handle, mask.handle), val.type)\n \n # ===----------------------------------------------------------------------===//\n@@ -952,16 +969,8 @@ def where(condition: tl.tensor,\n         x = broadcast_impl_shape(x, condition.type.get_block_shapes(), builder)\n         y = broadcast_impl_shape(y, condition.type.get_block_shapes(), builder)\n \n-    # TODO: we need to check x's and y's shape?\n-    x_ty = x.type.scalar\n-    y_ty = y.type.scalar\n-    ty = computation_type_impl(x_ty, y_ty, div_or_mod=False)\n-    x = cast(x, ty, builder)\n-    y = cast(y, ty, builder)\n-    if x.type.is_block():\n-        ret_ty = tl.block_type(ty, x.type.shape)\n-    else:\n-        ret_ty = ty\n+    x, y = binary_op_type_checking_impl(x, y, builder, True, True)\n+    ret_ty = x.type\n     return tl.tensor(builder.create_select(condition.handle, x.handle, y.handle), ret_ty)\n \n \n@@ -978,6 +987,10 @@ def reduce_impl(input: tl.tensor, axis: int, builder: ir.builder, name: str,\n     if scalar_ty.is_int() and scalar_ty.int_bitwidth <= 32:\n         input = cast(input, tl.int32, builder)\n \n+    # hardware doesn't support FMAX, FMIN, CMP for bfloat16\n+    if scalar_ty is tl.bfloat16:\n+        input = cast(input, tl.float32, builder)\n+\n     # choose the right unsigned operation\n     if scalar_ty.is_int_unsigned():\n         int_op_to_unit = {"}, {"filename": "python/triton/ops/cross_entropy.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -65,7 +65,7 @@ def _backward(PROBS, IDX, DPROBS, N, BLOCK: tl.constexpr):\n     # write result in-place in PROBS\n     dout = tl.load(DPROBS + row)\n     din = (probs - delta) * dout\n-    tl.store(PROBS, din.to(tl.float16), mask=cols < N)\n+    tl.store(PROBS, din.to(PROBS.dtype.element_ty), mask=cols < N)\n \n \n class _cross_entropy(torch.autograd.Function):"}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -236,8 +236,8 @@ def matmul_kernel(\n         b_ptrs += BLOCK_SIZE_K * stride_bk\n     # you can fuse arbitrary activation functions here\n     # while the accumulator is still in FP32!\n-    if ACTIVATION:\n-        accumulator = ACTIVATION(accumulator)\n+    if ACTIVATION == \"leaky_relu\":\n+        accumulator = leaky_relu(accumulator)\n     c = accumulator.to(tl.float16)\n \n     # -----------------------------------------------------------\n@@ -261,7 +261,7 @@ def leaky_relu(x):\n # and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel\n \n \n-def matmul(a, b, activation=None):\n+def matmul(a, b, activation=\"\"):\n     # checks constraints\n     assert a.shape[1] == b.shape[0], \"incompatible dimensions\"\n     assert a.is_contiguous(), \"matrix A must be contiguous\"\n@@ -347,7 +347,7 @@ def benchmark(M, N, K, provider):\n         )\n     if provider == 'triton + relu':\n         ms, min_ms, max_ms = triton.testing.do_bench(\n-            lambda: matmul(a, b, activation=leaky_relu)\n+            lambda: matmul(a, b, activation=\"leaky_relu\")\n         )\n     perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n     return perf(ms), perf(max_ms), perf(min_ms)"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,7 +1,7 @@\n \"\"\"\n Fused Attention\n ===============\n-This is a Triton implementation of the Flash Attention algorithm \n+This is a Triton implementation of the Flash Attention algorithm\n (see: Dao et al., https://arxiv.org/pdf/2205.14135v2.pdf; Rabe and Staats https://arxiv.org/pdf/2112.05682v2.pdf)\n \"\"\"\n "}]