[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 21, "deletions": 5, "changes": 26, "file_content_changes": "@@ -92,7 +92,7 @@ jobs:\n         if: ${{ env.BACKEND == 'CUDA' && env.ENABLE_TMA == '1' && env.ENABLE_MMA_V3 == '1'}}\n         run: |\n           cd python/test/unit\n-          python3 -m pytest -n 8 --ignore=runtime --ignore=language/test_line_info.py\n+          python3 -m pytest -n 8 --ignore=runtime --ignore=operators --ignore=language/test_line_info.py\n           # run runtime tests serially to avoid race condition with cache handling.\n           python3 -m pytest runtime/\n           # run test_line_info.py separately with TRITON_DISABLE_LINE_INFO=0\n@@ -102,17 +102,33 @@ jobs:\n         if: ${{ env.BACKEND == 'CUDA' && env.ENABLE_TMA == '0' && env.ENABLE_MMA_V3 == '0'}}\n         run: |\n           cd python/test/unit\n-          python3 -m pytest -n 8 --ignore=runtime --ignore=hopper --ignore=language/test_line_info.py\n+          python3 -m pytest -n 8 --ignore=runtime --ignore=hopper --ignore=operators --ignore=language/test_line_info.py\n           # run runtime tests serially to avoid race condition with cache handling.\n           python3 -m pytest runtime/\n           # run test_line_info.py separately with TRITON_DISABLE_LINE_INFO=0\n           TRITON_DISABLE_LINE_INFO=0 python3 -m pytest language/test_line_info.py\n \n+      - name: Clear cache\n+        run: |\n+          rm -rf ~/.triton\n+\n+      - name: Run partial tests on CUDA with ENABLE_TMA=1 and ENABLE_MMA_V3=1\n+        if: ${{ env.BACKEND == 'CUDA' && env.ENABLE_TMA == '1' && env.ENABLE_MMA_V3 == '1'}}\n+        run: |\n+          cd python/test/unit\n+          python3 -m pytest -n 8 operators\n+\n+      - name: Run partial tests on CUDA with ENABLE_TMA=0 and ENABLE_MMA_V3=0\n+        if: ${{ env.BACKEND == 'CUDA' && env.ENABLE_TMA == '0' && env.ENABLE_MMA_V3 == '0'}}\n+        run: |\n+          cd python/test/unit\n+          python3 -m pytest -n 8 operators\n+\n       - name: Create artifacts archive\n         if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n         run: |\n           cd ~/.triton\n-          tar -czvf artifacts.tar.gz cache\n+          tar -czf artifacts.tar.gz cache\n \n       - name: Upload artifacts archive\n         if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n@@ -140,6 +156,7 @@ jobs:\n \n   Integration-Tests-Third-Party:\n     needs: Runner-Preparation\n+    if: false\n \n     runs-on: ${{ matrix.runner }}\n \n@@ -320,7 +337,7 @@ jobs:\n       - name: Compare artifacts\n         run: |\n           set +e\n-          python3 python/test/tools/compare_files.py --path1 reference --path2 current --kernels python/test/kernel_comparison/kernels.yml\n+          python3 python/test/tools/compare_files.py --path1 reference --path2 current\n           exit_code=$?\n           set -e\n           echo $exit_code\n@@ -334,7 +351,6 @@ jobs:\n             echo \"Error while comparing artifacts\"\n             echo \"COMPARISON_RESULT=error\" >> $GITHUB_ENV\n           fi\n-          echo \"COMPARISON_RESULT=env.COMPARISON_RESULT\"\n       - name: Check exit code and handle failure\n         if: ${{ env.COMPARISON_RESULT == 'error' }}\n         run: |"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -143,11 +143,11 @@ compared to 1*64 when the hasLeadingOffset is false.\n \n         // ---- begin Ampere ----\n         if (mmaEnc.isAmpere()) {\n-          int perPhase = 128 / (shapePerCTA[order[0]] * 4 / dotOpEnc.getMMAv2kWidth());\n+          int perPhase = 128 / (shapePerCTA[order[0]] * 4 / dotOpEnc.getKWidth());\n           perPhase = std::max<int>(perPhase, 1);\n-          std::vector<size_t> matShape = {8, 8, 4 * dotOpEnc.getMMAv2kWidth()};\n+          std::vector<size_t> matShape = {8, 8, 4 * dotOpEnc.getKWidth()};\n           // for now, disable swizzle when using transposed int8 tensor cores\n-          if ((32 / typeWidthInBit != dotOpEnc.getMMAv2kWidth()) && order[0] == inner)\n+          if ((32 / typeWidthInBit != dotOpEnc.getKWidth()) && order[0] == inner)\n             return get(context, 1, 1, 1, order, CTALayout);\n \n           // --- handle A operand ---\n@@ -655,7 +655,7 @@ section 9.7.13.4.1 for more details.\n     ins\n     \"unsigned\":$opIdx,\n     \"Attribute\":$parent,\n-    \"unsigned\":$MMAv2kWidth\n+    \"unsigned\":$kWidth\n   );\n \n   let builders = ["}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -582,7 +582,7 @@ Value loadArg(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n   int matShapeM = 8, matShapeN = 8, matShapeK = 2 * 64 / bitwidth;\n \n   auto numRep = encoding.getMMAv2Rep(shapePerCTA, bitwidth);\n-  int kWidth = encoding.getMMAv2kWidth();\n+  int kWidth = encoding.getKWidth();\n \n   auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n   auto order = triton::gpu::getOrder(mmaLayout);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -154,7 +154,6 @@ const std::string Fp16_to_Fp8E4M3B15x4 =\n     \"lop3.b32 $0, $0, $2, 0xbf80bf80, 0xf8;  \\n\"\n     \"}\";\n \n-/* ----- FP8E4M3 ------ */\n // Fp8E4M3 (x2) -> Fp16 (x2) (packed)\n const std::string Fp8E4M3_to_Fp16 = \"{ \\n\"\n                                     \"cvt.rn.f16x2.e4m3x2 $0, $1; \\n\""}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1258,7 +1258,7 @@ void DotOperandEncodingAttr::print(mlir::AsmPrinter &printer) const {\n   printer << \"<{\"\n           << \"opIdx = \" << getOpIdx() << \", parent = \" << getParent();\n   if (mmaParent && mmaParent.isAmpere())\n-    printer << \", kWidth = \" << getMMAv2kWidth();\n+    printer << \", kWidth = \" << getKWidth();\n   printer << \"}>\";\n }\n \n@@ -1496,7 +1496,7 @@ struct TritonGPUInferLayoutInterface\n     // Verify that the encodings are valid.\n     if (!aEncoding || !bEncoding)\n       return op->emitError(\"mismatching encoding between A and B operands\");\n-    if (aEncoding.getMMAv2kWidth() != bEncoding.getMMAv2kWidth())\n+    if (aEncoding.getKWidth() != bEncoding.getKWidth())\n       return op->emitError(\"mismatching kWidth between A and B operands\");\n     return success();\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -203,8 +203,8 @@ LogicalResult Prefetcher::initialize() {\n     auto bType = dot.getB().getType().cast<RankedTensorType>();\n     auto aEnc = aType.getEncoding().cast<triton::gpu::DotOperandEncodingAttr>();\n     auto bEnc = bType.getEncoding().cast<triton::gpu::DotOperandEncodingAttr>();\n-    int aKWidth = aEnc.getMMAv2kWidth();\n-    int bKWidth = bEnc.getMMAv2kWidth();\n+    int aKWidth = aEnc.getKWidth();\n+    int bKWidth = bEnc.getKWidth();\n     assert(aKWidth == bKWidth);\n \n     auto kSize = aType.getShape()[1];"}, {"filename": "python/test/tools/compare_files.py", "status": "modified", "additions": 11, "deletions": 24, "changes": 35, "file_content_changes": "@@ -9,9 +9,8 @@\n \n \n class ComparisonResult:\n-    def __init__(self, name: str, extension: str, numComparisons: int, diffs: List[str] = None, errors: List[str] = None):\n+    def __init__(self, name: str, numComparisons: int, diffs: List[str] = None, errors: List[str] = None):\n         self.name = name\n-        self.extension = extension\n         self.numComparisons = numComparisons\n         self.diffs = [] if diffs is None else diffs\n         self.errors = [] if errors is None else errors\n@@ -20,7 +19,7 @@ def isSuccess(self) -> bool:\n         return len(self.diffs) == 0 and len(self.errors) == 0\n \n     def __str__(self) -> str:\n-        return f\"name={self.name}, extension={self.extension}, numComparisons={self.numComparisons}, success={self.isSuccess()}\"\n+        return f\"name={self.name}, numComparisons={self.numComparisons}, success={self.isSuccess()}\"\n \n \n def listFilesWithExtension(path: str, extension: str) -> List[str]:\n@@ -143,9 +142,9 @@ def doFilesMatch(path1: str, path2: str) -> bool:\n     return True\n \n \n-def compareMatchingFiles(name: str, extension: str, nameToHashes1: Dict[str, List[str]], nameToHashes2: Dict[str, List[str]], args) -> ComparisonResult:\n+def compareMatchingFiles(name: str, nameToHashes1: Dict[str, List[str]], nameToHashes2: Dict[str, List[str]], args) -> ComparisonResult:\n     \"\"\"\n-        Compare files with the given name/extension in all hashes in both paths\n+        Compare files with the given name in all hashes in both paths\n         Return the first mismatching files as a tuple (file1, file2), otherwise, return an empty tuple\n     \"\"\"\n     hashes1 = nameToHashes1.get(name, [])\n@@ -164,14 +163,14 @@ def compareMatchingFiles(name: str, extension: str, nameToHashes1: Dict[str, Lis\n             if not doFilesMatch(path1, path2):\n                 continue\n             numComparisons += 1\n-            extFile1 = listFilesWithExtension(path1, extension)[0]\n-            extFile2 = listFilesWithExtension(path2, extension)[0]\n+            extFile1 = listFilesWithExtension(path1, \"ptx\")[0]\n+            extFile2 = listFilesWithExtension(path2, \"ptx\")[0]\n             diff = diffFiles(extFile1, extFile2)\n             if len(diff) > 0:\n                 diffs.append(diffFiles(extFile2, extFile1))\n     if numComparisons == 0:\n         errors.append(f\"Did not find any matching files for {name}\")\n-    return ComparisonResult(name=name, extension=extension, numComparisons=numComparisons, diffs=diffs, errors=errors)\n+    return ComparisonResult(name=name, numComparisons=numComparisons, diffs=diffs, errors=errors)\n \n \n def dumpResults(results: List[ComparisonResult], fileName: str):\n@@ -203,20 +202,15 @@ def main(args) -> bool:\n     nameToHashes1 = getNameToHashesDict(args.path1)\n     nameToHashes2 = getNameToHashesDict(args.path2)\n \n-    yamlFilePath = args.kernels\n-    if not os.path.exists(yamlFilePath):\n-        print(f\"Path {yamlFilePath} does not exist!\")\n-        sys.exit(2)\n-    nameAndExtension = loadYamlFile(yamlFilePath)[\"name_and_extension\"]\n+    # Get all kernels that need to be checked\n+    kernelNames = set(nameToHashes1.keys()).union(set(nameToHashes2.keys()))\n \n     results = []\n     # iterate over the kernels that need to be checked\n-    for d in nameAndExtension:\n-        name = d[\"name\"]  # kernel name\n-        extension = d[\"extension\"]  # extension of the file to be compared (e.g. ptx)\n+    for name in kernelNames:\n         # Compare all hashes on path 1 with all hashes on path 2\n         # result is either the mismatching (file1, file2) with \"extension\" or empty tuple if no mismatch\n-        result = compareMatchingFiles(name, extension, nameToHashes1, nameToHashes2, args)\n+        result = compareMatchingFiles(name, nameToHashes1, nameToHashes2, args)\n         print(result)\n         # Otherwise, add it to the mismatches\n         results.append(result)\n@@ -250,12 +244,5 @@ def main(args) -> bool:\n         required=True,\n         help=(\"Path to second cache directory\"),\n     )\n-    parser.add_argument(\n-        \"--kernels\",\n-        type=str,\n-        default=None,\n-        required=True,\n-        help=(\"Path to kernels yaml file\"),\n-    )\n     args = parser.parse_args()\n     main(args)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 23, "deletions": 0, "changes": 23, "file_content_changes": "@@ -354,6 +354,29 @@ def test_bin_op(dtype_x, dtype_y, op, num_ctas, device):\n             num_ctas=num_ctas)\n \n \n+@pytest.mark.parametrize(\"dtype, order\", [(dtype, order) for dtype in dtypes_with_bfloat16 for order in [0, 1]])\n+def test_addptr(dtype, order, device):\n+    check_type_supported(dtype, device)\n+\n+    @triton.jit\n+    def kernel(x, y, ORDER: tl.constexpr, SIZE: tl.constexpr):\n+        offs = tl.arange(0, SIZE)\n+        if ORDER == 0:\n+            tl.store(y + offs, tl.load(x + offs))\n+        else:\n+            tl.store(offs + y, tl.load(offs + x))\n+\n+    SIZE = 1024\n+    rs = RandomState(17)\n+    x = numpy_random(SIZE, dtype_str=dtype, rs=rs)\n+    y = numpy_random(SIZE, dtype_str=dtype, rs=rs)\n+    x_tri = to_triton(x, dst_type=dtype, device=device)\n+    y_tri = to_triton(y, dst_type=dtype, device=device)\n+    y = x\n+    kernel[1,](x_tri, y_tri, order, SIZE)\n+    np.testing.assert_allclose(y, to_numpy(y_tri))\n+\n+\n @pytest.mark.parametrize(\"dtype_x, dtype_y\",\n                          [(dtype_x, dtype_y) for dtype_x in int_dtypes for dtype_y in int_dtypes] +\n                          [(dtype_x, dtype_y) for dtype_x in uint_dtypes for dtype_y in uint_dtypes]"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -132,6 +132,8 @@ def add(input: tl.tensor,\n     # ptr + offset\n     if other_scalar_ty.is_ptr() and not input_scalar_ty.is_ptr():\n         input, other = other, input\n+        input_scalar_ty = input.type.scalar\n+        other_scalar_ty = other.type.scalar\n     if input_scalar_ty.is_ptr():\n         return tl.tensor(builder.create_addptr(input.handle, other.handle), input.type)\n     # float + float"}]