[{"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -339,7 +339,7 @@ def TT_DotOp : TT_Op<\"dot\", [NoSideEffect,\n         $d = matrix_multiply($a, $b) + $c\n     }];\n \n-    let arguments = (ins TT_FpIntTensor:$a, TT_FpIntTensor:$b, TT_FpIntTensor:$c, BoolAttr:$allowTF32, BoolAttr:$transA, BoolAttr:$transB);\n+    let arguments = (ins TT_FpIntTensor:$a, TT_FpIntTensor:$b, TT_FpIntTensor:$c, BoolAttr:$allowTF32);\n \n     let results = (outs TT_FpIntTensor:$d);\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotHelpers.h", "status": "modified", "additions": 1, "deletions": 8, "changes": 9, "file_content_changes": "@@ -942,11 +942,6 @@ struct MMA16816ConversionHelper {\n \n     SmallVector<int64_t> shape(aTensorTy.getShape().begin(),\n                                aTensorTy.getShape().end());\n-    // TODO[Superjomn]: transA cannot be accessed in ConvertLayoutOp.\n-    bool transA = false;\n-    if (transA) {\n-      std::swap(shape[0], shape[1]);\n-    }\n \n     ValueTable ha;\n     std::function<void(int, int)> loadFn;\n@@ -1052,8 +1047,6 @@ struct MMA16816ConversionHelper {\n \n     SmallVector<int64_t> aShape(aTensorTy.getShape().begin(),\n                                 aTensorTy.getShape().end());\n-    if (op.transA())\n-      std::swap(aShape[0], aShape[1]);\n \n     auto dShape = dTensorTy.getShape();\n \n@@ -1462,8 +1455,8 @@ Value DotOpMmaV1ConversionHelper::loadB(\n   SmallVector<unsigned> order(sharedLayout.getOrder().begin(),\n                               sharedLayout.getOrder().end());\n \n-  Value smem = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n \n+  Value smem = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n   bool isBRow = order[0] != 0;\n   bool isBVec4 = isBRow && shape[order[0]] <= 16;\n   // TODO[Superjomn]: Support the case when isBVec4=false later"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 13, "changes": 14, "file_content_changes": "@@ -3253,7 +3253,7 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n \n     // Here we assume the DotOp's operands always comes from shared memory.\n     auto AShape = A.getType().cast<RankedTensorType>().getShape();\n-    size_t reduceAxis = op.transA() ? 0 : 1;\n+    size_t reduceAxis = 1;\n     unsigned K = AShape[reduceAxis];\n     bool isOuter = K == 1;\n \n@@ -3492,22 +3492,10 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n   auto DShape = DTensorTy.getShape();\n   auto wpt = mmaLayout.getWarpsPerCTA();\n \n-  bool transA = op.transA();\n-  bool transB = op.transB();\n-\n   // TODO[Superjomn]: order cannot accessed in DotOp.\n   SmallVector<unsigned> AOrder({1, 0});\n   SmallVector<unsigned> BOrder({1, 0});\n \n-  if (transA) {\n-    std::swap(AShape[0], AShape[1]);\n-    std::swap(AOrder[0], AOrder[1]);\n-  }\n-  if (transB) {\n-    std::swap(BShape[0], BShape[1]);\n-    std::swap(BOrder[0], BOrder[0]);\n-  }\n-\n   bool isARow = AOrder[0] != 0;\n   bool isBRow = BOrder[0] != 0;\n   bool isAVec4 = !isARow && AShape[isARow] <= 16; // fp16*4 = 16bytes"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -245,9 +245,8 @@ struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n                                            bType.getElementType(), encoding);\n       b = rewriter.create<triton::gpu::ConvertLayoutOp>(b.getLoc(), dstType, b);\n     }\n-    rewriter.replaceOpWithNewOp<triton::DotOp>(\n-        op, retType, a, b, adaptor.c(), adaptor.allowTF32(), adaptor.transA(),\n-        adaptor.transB());\n+    rewriter.replaceOpWithNewOp<triton::DotOp>(op, retType, a, b, adaptor.c(),\n+                                               adaptor.allowTF32());\n     return success();\n   }\n };"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.td", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -12,21 +12,21 @@ include \"triton/Dialect/Triton/IR/TritonOps.td\"\n // AddIOp(d, DotOp(a, b, c)) and c==0 => DotOp(a, b, d)\n // AddFOp(d, DotOp(a, b, c)) and c==0 => DotOp(a, b, d)\n def CombineDotAddIPattern : Pat<\n-        (Arith_AddIOp $d, (TT_DotOp:$res $a, $b, $c, $allowTF32, $transA, $transB)),\n-        (TT_DotOp $a, $b, $d, $allowTF32, $transA, $transB),\n+        (Arith_AddIOp $d, (TT_DotOp:$res $a, $b, $c, $allowTF32)),\n+        (TT_DotOp $a, $b, $d, $allowTF32),\n         [(Constraint<CPred<\"isZero($0)\">> $c)]>;\n def CombineDotAddFPattern : Pat<\n-        (Arith_AddFOp $d, (TT_DotOp:$res $a, $b, $c, $allowTF32, $transA, $transB)),\n-        (TT_DotOp $a, $b, $d, $allowTF32, $transA, $transB),\n+        (Arith_AddFOp $d, (TT_DotOp:$res $a, $b, $c, $allowTF32)),\n+        (TT_DotOp $a, $b, $d, $allowTF32),\n         [(Constraint<CPred<\"isZero($0)\">> $c)]>;\n \n def CombineDotAddIRevPattern : Pat<\n-        (Arith_AddIOp (TT_DotOp:$res $a, $b, $c, $allowTF32, $transA, $transB), $d),\n-        (TT_DotOp $a, $b, $d, $allowTF32, $transA, $transB),\n+        (Arith_AddIOp (TT_DotOp:$res $a, $b, $c, $allowTF32), $d),\n+        (TT_DotOp $a, $b, $d, $allowTF32),\n         [(Constraint<CPred<\"isZero($0)\">> $c)]>;\n def CombineDotAddFRevPattern : Pat<\n-        (Arith_AddFOp (TT_DotOp:$res $a, $b, $c, $allowTF32, $transA, $transB), $d),\n-        (TT_DotOp $a, $b, $d, $allowTF32, $transA, $transB),\n+        (Arith_AddFOp (TT_DotOp:$res $a, $b, $c, $allowTF32), $d),\n+        (TT_DotOp $a, $b, $d, $allowTF32),\n         [(Constraint<CPred<\"isZero($0)\">> $c)]>;\n \n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -781,8 +781,7 @@ class BlockedToMMA : public mlir::RewritePattern {\n     a = rewriter.create<triton::gpu::ConvertLayoutOp>(a.getLoc(), newAType, a);\n     b = rewriter.create<triton::gpu::ConvertLayoutOp>(b.getLoc(), newBType, b);\n     auto newDot = rewriter.create<triton::DotOp>(\n-        dotOp.getLoc(), newRetType, a, b, newAcc, dotOp.allowTF32(),\n-        dotOp.transA(), dotOp.transB());\n+        dotOp.getLoc(), newRetType, a, b, newAcc, dotOp.allowTF32());\n \n     rewriter.replaceOpWithNewOp<triton::gpu::ConvertLayoutOp>(\n         op, oldRetType, newDot.getResult());"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 19, "deletions": 11, "changes": 30, "file_content_changes": "@@ -123,9 +123,13 @@ void LoopPipeliner::collectDeps(Value v, int stages, DenseSet<Value> &deps) {\n     return;\n \n   if (auto arg = v.dyn_cast<BlockArgument>()) {\n-    deps.insert(v);\n-    // Note: we have iv as the first arg, so the op idx is arg.getArgNumber()-1\n-    collectDeps(yieldOp->getOperand(arg.getArgNumber() - 1), stages - 1, deps);\n+    if (arg.getArgNumber() > 0) {\n+      // Skip the first arg (loop induction variable)\n+      // Otherwise the op idx is arg.getArgNumber()-1\n+      deps.insert(v);\n+      collectDeps(yieldOp->getOperand(arg.getArgNumber() - 1), stages - 1,\n+                  deps);\n+    }\n   } else { // value\n     // v might be in deps, but we still need to visit v.\n     // This is because v might depend on value in previous iterations\n@@ -376,11 +380,11 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   OpBuilder builder(forOp);\n \n   // Order of new args:\n-  //   (original args),\n-  //   (insertSliceAsync buffer at stage numStages - 1)  for each load\n-  //   (extracted tensor)  for each load\n-  //   (depArgs at stage numStages-1)\n-  //   (iv at stage numStages-1)\n+  //   (original args)\n+  //   (insertSliceAsync buffer at stage numStages - 1) for each load\n+  //   (extracted tensor) for each load\n+  //   (depArgs at stage numStages - 1)\n+  //   (iv at stage numStages - 2)\n   //   (pipeline iteration index)\n   //   (loop iteration index)\n   SmallVector<Value> newLoopArgs;\n@@ -421,6 +425,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   BlockAndValueMapping mapping;\n   for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n     mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n+  mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n \n   // 2.1 clone the loop body, replace original args with args of the new ForOp\n   // Insert async wait if necessary.\n@@ -469,6 +474,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   Value nextLoopCond =\n       builder.create<arith::CmpIOp>(nextIV.getLoc(), arith::CmpIPredicate::slt,\n                                     nextIV, newForOp.getUpperBound());\n+  nextMapping.map(forOp.getInductionVar(), nextIV);\n \n   // Slice index\n   SmallVector<Value> nextBuffers;\n@@ -598,9 +604,11 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   for (Value nextSlice : extractSlices)\n     yieldValues.push_back(nextSlice);\n \n-  for (size_t i = depArgsBeginIdx; i < nextIVIdx; ++i)\n-    yieldValues.push_back(\n-        depArgsMapping.lookup(newForOp.getRegionIterArgs()[i]));\n+  for (size_t i = depArgsBeginIdx; i < nextIVIdx; ++i) {\n+    auto arg = newForOp.getRegionIterArgs()[i];\n+    assert(depArgsMapping.count(arg) && \"Missing loop-carried value\");\n+    yieldValues.push_back(depArgsMapping[arg]);\n+  }\n   yieldValues.push_back(nextIV);\n   yieldValues.push_back(pipelineIterIdx);\n   yieldValues.push_back(loopIterIdx);"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -1184,11 +1184,10 @@ void init_triton_ir(py::module &&m) {\n            })\n       .def(\"create_dot\",\n            [](mlir::OpBuilder &self, mlir::Value &a, mlir::Value &b,\n-              mlir::Value &c, bool allowTF32, bool transA,\n-              bool transB) -> mlir::Value {\n+              mlir::Value &c, bool allowTF32) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n              return self.create<mlir::triton::DotOp>(loc, c.getType(), a, b, c,\n-                                                     allowTF32, transA, transB);\n+                                                     allowTF32);\n            })\n       .def(\"create_exp\",\n            [](mlir::OpBuilder &self, mlir::Value &val) -> mlir::Value {"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1126,7 +1126,7 @@ def kernel(X, stride_xm, stride_xk,\n         if CHAIN_DOT:\n             # tl.store(Zs, z)\n             # tl.debug_barrier()\n-            z = tl.dot(tl.trans(z.to(tl.float16)), tl.load(Ws))\n+            z = tl.dot(z.to(tl.float16), tl.load(Ws))\n         tl.store(Zs, z)\n     # input\n     rs = RandomState(17)\n@@ -1173,7 +1173,7 @@ def kernel(X, stride_xm, stride_xk,\n         denom = np.sum(num, axis=-1, keepdims=True)\n         z_ref = num / denom\n     if epilogue == 'chain-dot':\n-        z_ref = np.matmul(z_ref.T, w)\n+        z_ref = np.matmul(z_ref, w)\n     # compare\n     # print(z_ref[:,0], z_tri[:,0])\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -762,6 +762,9 @@ def visit_Str(self, node):\n \n     def visit_Attribute(self, node):\n         lhs = self.visit(node.value)\n+        if isinstance(lhs, triton.language.tensor):\n+          if node.attr == \"T\":\n+            return triton.language.semantic.trans(lhs, builder=self.builder)\n         return getattr(lhs, node.attr)\n \n     def visit_Expr(self, node):"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 5, "deletions": 6, "changes": 11, "file_content_changes": "@@ -603,10 +603,9 @@ def __getitem__(self, slices, _builder=None):\n                 assert False, \"unsupported\"\n         return ret\n \n-\n-    # x[:, None, :, None]\n-    # x = expand_dims(x, axis=1)\n-    # x = expand_dims(x, axis=2)\n+    @property\n+    def T(self):\n+        assert False, \"Transposition must be created by the AST Visitor\"\n \n     @builtin\n     def to(self, dtype, bitcast=False, _builder=None):\n@@ -770,7 +769,7 @@ def reshape(input, shape, _builder=None):\n \n \n @builtin\n-def dot(input, other, allow_tf32=True, trans_a=False, trans_b=False, _builder=None):\n+def dot(input, other, allow_tf32=True, _builder=None):\n     \"\"\"\n     Returns the matrix product of two blocks.\n \n@@ -782,7 +781,7 @@ def dot(input, other, allow_tf32=True, trans_a=False, trans_b=False, _builder=No\n     :type other: 2D tensor of scalar-type in {:code:`float16`, :code:`bfloat16`, :code:`float32`}\n     \"\"\"\n     allow_tf32 = _constexpr_to_value(allow_tf32)\n-    return semantic.dot(input, other, allow_tf32, trans_a, trans_b, _builder)\n+    return semantic.dot(input, other, allow_tf32, _builder)\n \n \n # -----------------------"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "file_content_changes": "@@ -976,8 +976,6 @@ def atomic_xchg(ptr: tl.tensor,\n def dot(lhs: tl.tensor,\n         rhs: tl.tensor,\n         allow_tf32: bool,\n-        trans_a: bool,\n-        trans_b: bool,\n         builder: ir.builder) -> tl.tensor:\n     assert lhs.type.is_block() and rhs.type.is_block()\n     if lhs.type.scalar.is_int():\n@@ -986,11 +984,11 @@ def dot(lhs: tl.tensor,\n     else:\n         _0 = builder.get_float32(0)\n         ret_scalar_ty = tl.float32\n-    M = lhs.type.shape[1 if trans_a else 0]\n-    N = rhs.type.shape[0 if trans_b else 1]\n+    M = lhs.type.shape[0]\n+    N = rhs.type.shape[1]\n     _0 = builder.create_splat(_0, [M, N])\n     ret_ty = tl.block_type(ret_scalar_ty, [M, N])\n-    return tl.tensor(builder.create_dot(lhs.handle, rhs.handle, _0, allow_tf32, trans_a, trans_b),\n+    return tl.tensor(builder.create_dot(lhs.handle, rhs.handle, _0, allow_tf32),\n                      ret_ty)\n \n "}, {"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -257,5 +257,5 @@ def bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='c\n                                                      grad_to_none=[x], rep=500)\n     return gbps(ms), gbps(max_ms), gbps(min_ms)\n \n-\n+# test_layer_norm(1151, 8192, torch.float16)\n bench_layer_norm.run(save_path='.', print_data=True)"}, {"filename": "test/Triton/combine.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -13,10 +13,10 @@ func @test_combine_dot_add_pattern() -> (tensor<128x128xf32>, tensor<128x128xf32\n \n     %dot_out = tt.dot %a, %b, %zero {allowTF32 = true, transA = false, transB = false} : tensor<128x128xf32> * tensor<128x128xf32> -> tensor<128x128xf32>\n \n-    // CHECK-NEXT: %[[res0:.*]] = tt.dot %[[a]], %[[b]], %[[d]] {allowTF32 = true, transA = false, transB = false} : tensor<128x128xf32> * tensor<128x128xf32> -> tensor<128x128xf32>\n+    // CHECK-NEXT: %[[res0:.*]] = tt.dot %[[a]], %[[b]], %[[d]] {allowTF32 = true} : tensor<128x128xf32> * tensor<128x128xf32> -> tensor<128x128xf32>\n     %res0 = arith.addf %dot_out, %d : tensor<128x128xf32>\n \n-    // CHECK-NEXT: %[[res1:.*]] = tt.dot %[[a]], %[[b]], %[[d]] {allowTF32 = true, transA = false, transB = false} : tensor<128x128xf32> * tensor<128x128xf32> -> tensor<128x128xf32>\n+    // CHECK-NEXT: %[[res1:.*]] = tt.dot %[[a]], %[[b]], %[[d]] {allowTF32 = true} : tensor<128x128xf32> * tensor<128x128xf32> -> tensor<128x128xf32>\n     %res1 = arith.addf %d, %dot_out : tensor<128x128xf32>\n \n     return %res0, %res1 : tensor<128x128xf32>, tensor<128x128xf32>"}]