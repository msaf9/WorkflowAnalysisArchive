[{"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 9, "deletions": 49, "changes": 58, "file_content_changes": "@@ -349,27 +349,23 @@ class RematerializeForward : public mlir::RewritePattern {\n     SetVector<Operation *> cvtSlices;\n     auto filter = [&](Operation *op) {\n       return op->getBlock() == cvt->getBlock() &&\n+             !isa<triton::gpu::ConvertLayoutOp, scf::YieldOp>(op) &&\n              !(isa<triton::ReduceOp>(op) &&\n-               !op->getResult(0).getType().isa<RankedTensorType>()) &&\n-             !isa<triton::gpu::ConvertLayoutOp>(op) && !isa<scf::YieldOp>(op);\n+               !op->getResult(0).getType().isa<RankedTensorType>());\n     };\n     mlir::getForwardSlice(cvt.getResult(), &cvtSlices, filter);\n-    if (cvtSlices.empty()) {\n+    if (cvtSlices.empty())\n       return failure();\n-    }\n \n-    llvm::MapVector<Value, Attribute> toConvert;\n     for (Operation *op : cvtSlices) {\n       // don't rematerialize anything expensive\n-      if (expensiveToRemat(op, dstEncoding)) {\n+      if (expensiveToRemat(op, dstEncoding))\n         return failure();\n-      }\n       // don't rematerialize non-element-wise\n       if (!op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n           !op->hasTrait<mlir::OpTrait::Elementwise>() &&\n-          !isa<triton::StoreOp>(op) && !isa<triton::ReduceOp>(op)) {\n+          !isa<triton::StoreOp, triton::ReduceOp>(op))\n         return failure();\n-      }\n       // don't rematerialize if it adds an extra conversion that can't\n       // be removed\n       for (Value arg : op->getOperands()) {\n@@ -380,9 +376,8 @@ class RematerializeForward : public mlir::RewritePattern {\n         int numAddedConvs = simulateBackwardRematerialization(\n             argOp, processed, layout, toConvert, srcEncoding);\n         if (argOp && !isa<triton::gpu::ConvertLayoutOp>(argOp) &&\n-            cvtSlices.count(argOp) == 0 && numAddedConvs > 0) {\n+            cvtSlices.count(argOp) == 0 && numAddedConvs > 0)\n           return failure();\n-        }\n       }\n     }\n \n@@ -425,7 +420,6 @@ class RematerializeBackward : public mlir::RewritePattern {\n     SetVector<Operation *> processed;\n     SetVector<Attribute> layout;\n     llvm::MapVector<Value, Attribute> toConvert;\n-    std::vector<std::pair<Operation *, Attribute>> queue;\n     if (simulateBackwardRematerialization(cvt, processed, layout, toConvert,\n                                           targetType.getEncoding()) > 0)\n       return mlir::failure();\n@@ -507,49 +501,15 @@ class MoveConvertOutOfLoop : public mlir::RewritePattern {\n     auto forOp = cast<scf::ForOp>(op);\n     auto iterArgs = forOp.getRegionIterArgs();\n     for (const auto &iterArg : llvm::enumerate(iterArgs)) {\n-      // if (iterArg.index() != 1)\n-      //   continue;\n       // skip non-tensor types\n       if (!iterArg.value().getType().isa<RankedTensorType>())\n         continue;\n-      // we only move `iterArg` out of the loop if\n-      //   - there is only a single conversion use\n-      //   - moving this conversion out of the loop will not generate\n-      //     any extra non-removable conversion\n-      auto users = iterArg.value().getUsers();\n-      // check first condition\n-      SetVector<Type> cvtTargetTypes;\n-      for (auto user : users) {\n-        if (isa<triton::gpu::ConvertLayoutOp>(user)) {\n-          auto newType =\n-              user->getResults()[0].getType().cast<RankedTensorType>();\n-          auto oldType = user->getOperand(0).getType().cast<RankedTensorType>();\n-          if (oldType.getEncoding().isa<triton::gpu::SharedEncodingAttr>() &&\n-              newType.getEncoding()\n-                  .isa<triton::gpu::DotOperandEncodingAttr>()) {\n-            continue;\n-          }\n-          if (newType.getEncoding().isa<triton::gpu::SharedEncodingAttr>()) {\n-            if (newType.getEncoding()\n-                    .cast<triton::gpu::SharedEncodingAttr>()\n-                    .getVec() == 1)\n-              continue;\n-          }\n-          cvtTargetTypes.insert(newType);\n-        }\n-      }\n-      if (cvtTargetTypes.size() != 1)\n+      SmallVector<Operation *> cvts;\n+      if (canMoveOutOfLoop(iterArg.value(), cvts).failed())\n         continue;\n-      // TODO: check second condition\n-      for (auto user : users) {\n-        if (isa<triton::gpu::ConvertLayoutOp>(user))\n-          continue;\n-      }\n       // check\n-      for (auto op : iterArg.value().getUsers()) {\n+      for (auto *op : cvts) {\n         auto cvt = dyn_cast<triton::gpu::ConvertLayoutOp>(op);\n-        if (!cvt)\n-          continue;\n         auto targetType = op->getResultTypes()[0].cast<RankedTensorType>();\n         auto newFor = rematerializeForLoop(rewriter, forOp, iterArg.index(),\n                                            targetType, cvt);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 60, "deletions": 0, "changes": 60, "file_content_changes": "@@ -269,4 +269,64 @@ void rematerializeConversionChain(\n   }\n }\n \n+LogicalResult canMoveOutOfLoop(BlockArgument arg,\n+                               SmallVector<Operation *> &cvts) {\n+  auto parentOp = arg.getOwner()->getParentOp();\n+  // Don't move if arg is defined in a while loop\n+  if (isa<scf::WhileOp>(parentOp))\n+    return failure();\n+  // Skip if arg is not defined in scf.for\n+  if (!isa<scf::ForOp>(parentOp))\n+    return success();\n+  auto forOp = cast<scf::ForOp>(parentOp);\n+  // We only move `iterArg` out of the loop if\n+  // 1. There is no conversion\n+  // 2. There is only a single conversion\n+  // 3. Moving this conversion out of the loop will not generate any extra\n+  // non-removable conversion\n+  DenseSet<Type> cvtTypes;\n+  SetVector<Operation *> others;\n+  auto oldType = arg.getType().cast<RankedTensorType>();\n+  for (auto user : arg.getUsers()) {\n+    if (isa<triton::gpu::ConvertLayoutOp>(user)) {\n+      // Don't move if the conversion target is a dot operand or shared memory\n+      auto newType = user->getResults()[0].getType().cast<RankedTensorType>();\n+      if (oldType.getEncoding().isa<triton::gpu::SharedEncodingAttr>() &&\n+          newType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>()) {\n+        continue;\n+      }\n+      if (newType.getEncoding().isa<triton::gpu::SharedEncodingAttr>()) {\n+        if (newType.getEncoding()\n+                .cast<triton::gpu::SharedEncodingAttr>()\n+                .getVec() == 1)\n+          continue;\n+      }\n+      cvts.emplace_back(user);\n+      cvtTypes.insert(newType);\n+    } else\n+      others.insert(user);\n+  }\n+  // First condition\n+  if (cvts.empty())\n+    return success();\n+  if (cvtTypes.size() == 1) {\n+    // Second condition\n+    if (others.empty())\n+      return success();\n+    // Third condition: not complete\n+    // If the other or the cvt is in the different block, we cannot push the\n+    // conversion forward or backward\n+    for (auto *cvt : cvts) {\n+      if (cvt->getBlock() != forOp.getBody())\n+        return failure();\n+    }\n+    for (auto *other : others) {\n+      if (other->getBlock() != forOp.getBody())\n+        return failure();\n+    }\n+    return success();\n+  }\n+  return failure();\n+}\n+\n } // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -16,6 +16,8 @@ bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding);\n \n bool expensiveToRemat(Operation *op, Attribute &targetEncoding);\n \n+// skipInit is True when we only consider the operands of the initOp but\n+// not the initOp itself.\n int simulateBackwardRematerialization(\n     Operation *initOp, SetVector<Operation *> &processed,\n     SetVector<Attribute> &layout, llvm::MapVector<Value, Attribute> &toConvert,\n@@ -28,6 +30,10 @@ void rematerializeConversionChain(\n     const llvm::MapVector<Value, Attribute> &toConvert,\n     mlir::PatternRewriter &rewriter, SetVector<Operation *> &processed,\n     IRMapping &mapping);\n+\n+LogicalResult canMoveOutOfLoop(BlockArgument arg,\n+                               SmallVector<Operation *> &cvts);\n+\n } // namespace mlir\n \n #endif // TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 52, "deletions": 0, "changes": 52, "file_content_changes": "@@ -267,6 +267,58 @@ tt.func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32,\n   tt.return\n }\n \n+// CHECK-LABEL: loop_if\n+tt.func @loop_if(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32) {\n+  %cst = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n+  %cst_0 = arith.constant dense<64> : tensor<64x64xi32, #blocked1>\n+  %c1 = arith.constant 1 : index\n+  %c32 = arith.constant 32 : index\n+  %c0 = arith.constant 0 : index\n+  %i0 = arith.constant 0 : i32\n+  %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked1>\n+  %00 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #slice1dim1>\n+  %01 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #slice2dim0>\n+  %1 = tt.expand_dims %00 {axis = 1 : i32} : (tensor<64xi32, #slice1dim1>) -> tensor<64x1xi32, #blocked1>\n+  %2 = tt.splat %arg1 : (i32) -> tensor<64x1xi32, #blocked1>\n+  %3 = arith.muli %1, %2 : tensor<64x1xi32, #blocked1>\n+  %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n+  %6 = tt.expand_dims %01 {axis = 0 : i32} : (tensor<64xi32, #slice2dim0>) -> tensor<1x64xi32, #blocked2>\n+  %7 = tt.broadcast %5 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %8 = tt.broadcast %6 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+  %9 = triton_gpu.convert_layout %8 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n+  %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n+  %11:2 = scf.for %arg5 = %c0 to %c32 step %c1 iter_args(%arg6 = %cst_1, %arg7 = %10) -> (tensor<64x64xf32, #blocked1>, tensor<64x64x!tt.ptr<f32>, #blocked1>) {\n+    %33 = \"triton_gpu.cmpi\"(%i0, %i0) {predicate = 4 : i64} : (i32, i32) -> i1\n+    %34 = scf.if %33 -> (tensor<64x64xf32, #blocked1>) {\n+      %23 = triton_gpu.convert_layout %arg7 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked3>\n+      %24 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked3>\n+      %25 = triton_gpu.convert_layout %cst_1 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked3>\n+      %26 = tt.load %23, %24, %25 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, #blocked3>\n+      %27 = triton_gpu.convert_layout %26 : (tensor<64x64xf32, #blocked3>) -> tensor<64x64xf32, #blocked1>\n+      scf.yield %27 : tensor<64x64xf32, #blocked1>\n+    } else {\n+      scf.yield %arg6 : tensor<64x64xf32, #blocked1>\n+    }\n+    %28 = arith.addf %arg6, %34 : tensor<64x64xf32, #blocked1>\n+    %29 = tt.addptr %arg7, %cst_0 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n+    scf.yield %28, %29 : tensor<64x64xf32, #blocked1>, tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  }\n+  %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %13 = tt.addptr %12, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n+  %14 = tt.splat %arg3 : (i32) -> tensor<1x64xi32, #blocked2>\n+  %15 = arith.muli %6, %14 : tensor<1x64xi32, #blocked2>\n+  %16 = tt.broadcast %13 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %17 = tt.broadcast %15 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+  %18 = triton_gpu.convert_layout %17 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n+  %19 = tt.addptr %16, %18 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n+  %20 = triton_gpu.convert_layout %19 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %21 = triton_gpu.convert_layout %11#0 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked1>\n+  %22 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked1>\n+  tt.store %20, %21, %22 : tensor<64x64xf32, #blocked1>\n+  tt.return\n+}\n+\n // CHECK-LABEL: vecadd\n tt.func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n   // CHECK-NOT: triton_gpu.convert_layout"}]