[{"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -161,9 +161,16 @@ class AllocationAnalysis {\n       // TODO(Keren): Reduce with index is not supported yet.\n       auto value = op->getOperand(0);\n       if (auto tensorType = value.getType().dyn_cast<RankedTensorType>()) {\n+        auto srcLayout = tensorType.getEncoding();\n+        bool fastReduce = reduceOp.axis() == getOrder(srcLayout)[0];\n         auto smemShape = getScratchConfigForReduce(reduceOp);\n         unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n                                          std::multiplies{});\n+        if (fastReduce) {\n+          auto mod = op->getParentOfType<ModuleOp>();\n+          unsigned numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+          elems = std::max<unsigned>(elems, numWarps * 32);\n+        }\n         auto bytes = elems * tensorType.getElementTypeBitWidth() / 8;\n         allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n       }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 503, "deletions": 176, "changes": 679, "file_content_changes": "@@ -64,6 +64,12 @@ Value createConstantF32(Location loc, PatternRewriter &rewriter, float v) {\n                                            rewriter.getF32FloatAttr(v));\n }\n \n+Value createConstantF64(Location loc, PatternRewriter &rewriter, float v) {\n+  auto type = type::f64Ty(rewriter.getContext());\n+  return rewriter.create<LLVM::ConstantOp>(loc, type,\n+                                           rewriter.getF64FloatAttr(v));\n+}\n+\n // Create an index type constant.\n static Value createIndexConstant(OpBuilder &builder, Location loc,\n                                  TypeConverter *converter, int64_t value) {\n@@ -132,6 +138,7 @@ void llPrintf(StringRef msg, ValueRange args,\n #define f64_ty rewriter.getF64Type()\n #define vec_ty(type, num) VectorType::get(num, type)\n #define f32_val(...) LLVM::createConstantF32(loc, rewriter, __VA_ARGS__)\n+#define f64_val(...) LLVM::createConstantF64(loc, rewriter, __VA_ARGS__)\n #define void_ty(ctx) LLVM::LLVMVoidType::get(ctx)\n #define struct_ty(...) LLVM::LLVMStructType::getLiteral(ctx, __VA_ARGS__)\n \n@@ -347,7 +354,8 @@ Value getStructFromElements(Location loc, ValueRange resultVals,\n   return llvmStruct;\n }\n \n-// Delinearize on compile-time consts, assuming the order is [n, .. 2, 1, 0]\n+// TODO[goostavz]: to be deprecated\n+// delinearize supposing order is [n, .. , 2, 1, 0]\n template <typename T>\n static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape) {\n   // shape: {a, b, c, d}  ->  accMul: {b*c*d, c*d, d, 1}\n@@ -365,7 +373,40 @@ static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape) {\n   return multiDimIndex;\n }\n \n-// Linearize on compile-time consts, assuming the order is [n, .. 2, 1, 0]\n+// delinearize supposing order is [0, 1, .. , n]\n+template <typename T>\n+static SmallVector<T> getMultiDimIndexImpl(T linearIndex, ArrayRef<T> shape) {\n+  // shape: {a, b, c, d}  ->  accMul: {1, a, a*b, a*b*c}\n+  size_t rank = shape.size();\n+  T accMul = product(shape.drop_back());\n+  T linearRemain = linearIndex;\n+  SmallVector<T> multiDimIndex(rank);\n+  for (int i = rank - 1; i >= 0; --i) {\n+    multiDimIndex[i] = linearRemain / accMul;\n+    linearRemain = linearRemain % accMul;\n+    if (i != 0) {\n+      accMul = accMul / shape[i - 1];\n+    }\n+  }\n+  return multiDimIndex;\n+}\n+\n+template <typename T>\n+static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape,\n+                                       ArrayRef<unsigned> order) {\n+  size_t rank = shape.size();\n+  assert(rank == order.size());\n+  auto reordered = reorder(shape, order);\n+  auto reorderedMultiDim = getMultiDimIndexImpl<T>(linearIndex, reordered);\n+  SmallVector<T> multiDim(rank);\n+  for (unsigned i = 0; i < rank; ++i) {\n+    multiDim[order[i]] = reorderedMultiDim[i];\n+  }\n+  return multiDim;\n+}\n+\n+// TODO[goostavz]: to be deprecated\n+// linearize supposing order is [n, .. , 2, 1, 0]\n template <typename T>\n static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n   assert(multiDimIndex.size() == shape.size());\n@@ -382,6 +423,30 @@ static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n   return linearIndex;\n }\n \n+template <typename T>\n+static T getLinearIndexImpl(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n+  assert(multiDimIndex.size() == shape.size());\n+  // shape: {a, b, c, d}  ->  accMul: {1, a, a*b, a*b*c}\n+  size_t rank = shape.size();\n+  T accMul = product(shape.drop_back());\n+  T linearIndex = 0;\n+  for (int i = rank - 1; i >= 0; --i) {\n+    linearIndex += multiDimIndex[i] * accMul;\n+    if (i != 0) {\n+      accMul = accMul / shape[i - 1];\n+    }\n+  }\n+  return linearIndex;\n+}\n+\n+template <typename T>\n+static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape,\n+                        ArrayRef<unsigned> order) {\n+  assert(shape.size() == order.size());\n+  return getLinearIndexImpl<T>(reorder(multiDimIndex, order),\n+                               reorder(shape, order));\n+}\n+\n static Value storeShared(ConversionPatternRewriter &rewriter, Location loc,\n                          Value ptr, Value val, Value pred) {\n   MLIRContext *ctx = rewriter.getContext();\n@@ -457,6 +522,25 @@ struct ConvertTritonGPUOpToLLVMPatternBase {\n     }\n     return results;\n   }\n+\n+  static SharedMemoryObject\n+  getSharedMemoryObjectFromStruct(Location loc, Value llvmStruct,\n+                                  ConversionPatternRewriter &rewriter) {\n+    auto elems = getElementsFromStruct(loc, llvmStruct, rewriter);\n+    return SharedMemoryObject(/*base=*/elems[0],\n+                              /*strides=*/{elems.begin() + 1, elems.end()});\n+  }\n+\n+  static Value\n+  getStructFromSharedMemoryObject(Location loc,\n+                                  const SharedMemoryObject &smemObj,\n+                                  ConversionPatternRewriter &rewriter) {\n+    auto elems = smemObj.getElems();\n+    auto types = smemObj.getTypes();\n+    auto structTy =\n+        LLVM::LLVMStructType::getLiteral(rewriter.getContext(), types);\n+    return getStructFromElements(loc, elems, rewriter, structTy);\n+  }\n };\n \n template <typename SourceOp>\n@@ -613,6 +697,7 @@ class ConvertTritonGPUOpToLLVMPattern\n     auto sizePerThread = blockedLayout.getSizePerThread();\n     auto threadsPerWarp = blockedLayout.getThreadsPerWarp();\n     auto warpsPerCTA = blockedLayout.getWarpsPerCTA();\n+    auto order = blockedLayout.getOrder();\n \n     unsigned rank = shape.size();\n     SmallVector<unsigned> shapePerCTA = getShapePerCTA(blockedLayout);\n@@ -644,9 +729,9 @@ class ConvertTritonGPUOpToLLVMPattern\n       unsigned linearNanoTileId = n / totalSizePerThread;\n       unsigned linearNanoTileElemId = n % totalSizePerThread;\n       SmallVector<unsigned> multiDimNanoTileId =\n-          getMultiDimIndex<unsigned>(linearNanoTileId, tilesPerDim);\n-      SmallVector<unsigned> multiDimNanoTileElemId =\n-          getMultiDimIndex<unsigned>(linearNanoTileElemId, sizePerThread);\n+          getMultiDimIndex<unsigned>(linearNanoTileId, tilesPerDim, order);\n+      SmallVector<unsigned> multiDimNanoTileElemId = getMultiDimIndex<unsigned>(\n+          linearNanoTileElemId, sizePerThread, order);\n       for (unsigned k = 0; k < rank; ++k) {\n         unsigned reorderedMultiDimId =\n             multiDimNanoTileId[k] *\n@@ -830,25 +915,6 @@ class ConvertTritonGPUOpToLLVMPattern\n     return base;\n   }\n \n-  static SharedMemoryObject\n-  getSharedMemoryObjectFromStruct(Location loc, Value llvmStruct,\n-                                  ConversionPatternRewriter &rewriter) {\n-    auto elems = getElementsFromStruct(loc, llvmStruct, rewriter);\n-    return SharedMemoryObject(/*base=*/elems[0],\n-                              /*strides=*/{elems.begin() + 1, elems.end()});\n-  }\n-\n-  static Value\n-  getStructFromSharedMemoryObject(Location loc,\n-                                  const SharedMemoryObject &smemObj,\n-                                  ConversionPatternRewriter &rewriter) {\n-    auto elems = smemObj.getElems();\n-    auto types = smemObj.getTypes();\n-    auto structTy =\n-        LLVM::LLVMStructType::getLiteral(rewriter.getContext(), types);\n-    return getStructFromElements(loc, elems, rewriter, structTy);\n-  }\n-\n protected:\n   const Allocation *allocation;\n   Value smem;\n@@ -1881,8 +1947,6 @@ struct PrintfOpConversion\n   // currently support pointer, i8, i16, i32, i64, f16, bf16, f32, f64\n   std::string getFormatSubstr(Value value) const {\n     Type type = value.getType();\n-    unsigned width = type.getIntOrFloatBitWidth();\n-\n     if (type.isa<LLVM::LLVMPointerType>()) {\n       return \"%p\";\n     } else if (type.isBF16() || type.isF16() || type.isF32() || type.isF64()) {\n@@ -1924,13 +1988,11 @@ struct PrintfOpConversion\n   promoteValue(ConversionPatternRewriter &rewriter, Value value) {\n     auto *context = rewriter.getContext();\n     auto type = value.getType();\n-    type.dump();\n-    unsigned width = type.getIntOrFloatBitWidth();\n     Value newOp = value;\n     Type newType = type;\n \n     bool bUnsigned = type.isUnsignedInteger();\n-    if (type.isIntOrIndex() && width < 32) {\n+    if (type.isIntOrIndex() && type.getIntOrFloatBitWidth() < 32) {\n       if (bUnsigned) {\n         newType = ui32_ty;\n         newOp = rewriter.create<LLVM::ZExtOp>(UnknownLoc::get(context), newType,\n@@ -2091,6 +2153,8 @@ struct GetProgramIdOpConversion\n \n struct GetNumProgramsOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::GetNumProgramsOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::GetNumProgramsOp>::ConvertTritonGPUOpToLLVMPattern;\n \n   LogicalResult\n   matchAndRewrite(triton::GetNumProgramsOp op, OpAdaptor adaptor,\n@@ -2565,6 +2629,8 @@ class ElementwiseOpConversionBase\n     for (unsigned i = 0; i < elems; ++i) {\n       resultVals[i] = concreteThis->createDestOp(op, adaptor, rewriter, elemTy,\n                                                  operands[i], loc);\n+      if (!bool(resultVals[i]))\n+        return failure();\n     }\n     Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n     rewriter.replaceOp(op, view);\n@@ -3085,23 +3151,24 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n     }\n     unsigned linearIdxInNanoTile = i % srcAccumSizeInThreads;\n     auto multiDimIdxInNanoTile = getMultiDimIndex<unsigned>(\n-        linearIdxInNanoTile, srcBlockedLayout.getSizePerThread());\n+        linearIdxInNanoTile, srcBlockedLayout.getSizePerThread(), inOrd);\n     unsigned pos = multiDimIdxInNanoTile[inOrd[0]] % minVec;\n     multiDimIdxInNanoTile[inOrd[0]] /= minVec;\n     unsigned wordVecIdx =\n-        getLinearIndex<unsigned>(multiDimIdxInNanoTile, wordsInEachRep);\n+        getLinearIndex<unsigned>(multiDimIdxInNanoTile, wordsInEachRep, inOrd);\n     wordVecs[wordVecIdx] =\n         insert_element(wordTy, wordVecs[wordVecIdx], inVals[i], idx_val(pos));\n \n     if (i % srcAccumSizeInThreads == srcAccumSizeInThreads - 1) {\n       // end of replication, store the vectors into shared memory\n       unsigned linearRepIdx = i / srcAccumSizeInThreads;\n-      auto multiDimRepIdx = getMultiDimIndex<unsigned>(linearRepIdx, reps);\n+      auto multiDimRepIdx =\n+          getMultiDimIndex<unsigned>(linearRepIdx, reps, inOrd);\n       for (unsigned linearWordIdx = 0; linearWordIdx < numWordsEachRep;\n            ++linearWordIdx) {\n         // step 1: recover the multidim_index from the index of input_elements\n         auto multiDimWordIdx =\n-            getMultiDimIndex<unsigned>(linearWordIdx, wordsInEachRep);\n+            getMultiDimIndex<unsigned>(linearWordIdx, wordsInEachRep, inOrd);\n         SmallVector<Value> multiDimIdx(2);\n         auto wordOffset0 = multiDimRepIdx[0] * srcShapePerCTA[0] +\n                            multiDimWordIdx[0] * (inOrd[0] == 0 ? minVec : 1);\n@@ -3111,12 +3178,12 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n         multiDimIdx[1] = add(multiDimOffsetFirstElem[1], idx_val(wordOffset1));\n \n         // step 2: do swizzling\n-        Value remained = urem(multiDimIdx[inOrd[0]], outVecVal);\n-        multiDimIdx[inOrd[0]] = udiv(multiDimIdx[inOrd[0]], outVecVal);\n-        Value off_1 = mul(multiDimIdx[inOrd[1]], idx_val(srcShape[inOrd[0]]));\n-        Value phaseId = udiv(multiDimIdx[inOrd[1]], idx_val(perPhase));\n+        Value remained = urem(multiDimIdx[outOrd[0]], outVecVal);\n+        multiDimIdx[outOrd[0]] = udiv(multiDimIdx[outOrd[0]], outVecVal);\n+        Value off_1 = mul(multiDimIdx[outOrd[1]], idx_val(srcShape[outOrd[0]]));\n+        Value phaseId = udiv(multiDimIdx[outOrd[1]], idx_val(perPhase));\n         phaseId = urem(phaseId, idx_val(maxPhase));\n-        Value off_0 = xor_(multiDimIdx[inOrd[0]], phaseId);\n+        Value off_0 = xor_(multiDimIdx[outOrd[0]], phaseId);\n         off_0 = mul(off_0, outVecVal);\n         remained = udiv(remained, minVecVal);\n         off_0 = add(off_0, mul(remained, minVecVal));\n@@ -3594,7 +3661,8 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n     }\n \n     if (op.getType().cast<RankedTensorType>().getElementType().isF32() &&\n-        A.getType().cast<RankedTensorType>().getElementType().isF32())\n+        A.getType().cast<RankedTensorType>().getElementType().isF32() &&\n+        !op.allowTF32())\n       return convertFMADot(op, adaptor, rewriter);\n \n     llvm::report_fatal_error(\n@@ -4413,6 +4481,90 @@ struct MMA16816ConversionHelper {\n   }\n };\n \n+// Helper for conversion of FMA DotOp.\n+struct DotOpFMAConversionHelper {\n+  Attribute layout;\n+  MLIRContext *ctx{};\n+\n+  using ValueTable = std::map<std::pair<int, int>, Value>;\n+\n+  explicit DotOpFMAConversionHelper(Attribute layout)\n+      : layout(layout), ctx(layout.getContext()) {}\n+\n+  SmallVector<Value> getThreadIds(Value threadId,\n+                                  ArrayRef<unsigned> shapePerCTA,\n+                                  ArrayRef<unsigned> order,\n+                                  ConversionPatternRewriter &rewriter,\n+                                  Location loc) const;\n+\n+  Value loadA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n+              Location loc, ConversionPatternRewriter &rewriter) const;\n+\n+  Value loadB(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n+              Location loc, ConversionPatternRewriter &rewriter) const;\n+\n+  ValueTable getValueTableFromStruct(Value val, int K, int n0, int shapePerCTA,\n+                                     int sizePerThread,\n+                                     ConversionPatternRewriter &rewriter,\n+                                     Location loc) const;\n+\n+  Value getStructFromValueTable(ValueTable vals,\n+                                ConversionPatternRewriter &rewriter,\n+                                Location loc) const {\n+    SmallVector<Type> elemTypes(vals.size(), f32_ty);\n+    SmallVector<Value> elems;\n+    elems.reserve(vals.size());\n+    for (auto &item : vals) {\n+      elems.push_back(item.second);\n+    }\n+\n+    Type structTy = struct_ty(elemTypes);\n+    return getStructFromElements(loc, elems, rewriter, structTy);\n+  }\n+  // get number of elements per thread for $a or $b.\n+  static int getNumElemsPerThread(ArrayRef<int64_t> shape,\n+                                  DotOperandEncodingAttr dotOpLayout) {\n+    auto blockedLayout = dotOpLayout.getParent().cast<BlockedEncodingAttr>();\n+    auto shapePerCTA = getShapePerCTA(blockedLayout);\n+    auto sizePerThread = getSizePerThread(blockedLayout);\n+    auto order = blockedLayout.getOrder();\n+\n+    // TODO[Superjomn]: we assume the k aixs is fixed for $a and $b here, fix it\n+    // if not.\n+    int K = dotOpLayout.getOpIdx() == 0 ? shape[1] : shape[0];\n+    int otherDim = dotOpLayout.getOpIdx() == 1 ? shape[1] : shape[0];\n+\n+    bool isM = dotOpLayout.getOpIdx() == 0;\n+    int shapePerCTAMN = getShapePerCTAForMN(blockedLayout, isM);\n+    int sizePerThreadMN = getsizePerThreadForMN(blockedLayout, isM);\n+    return K * std::max<int>(otherDim / shapePerCTAMN, 1) * sizePerThreadMN;\n+  }\n+\n+  // Get shapePerCTA for M or N axis.\n+  static int getShapePerCTAForMN(BlockedEncodingAttr layout, bool isM) {\n+    auto order = layout.getOrder();\n+    auto shapePerCTA = getShapePerCTA(layout);\n+\n+    int mShapePerCTA =\n+        order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+    int nShapePerCTA =\n+        order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+    return isM ? mShapePerCTA : nShapePerCTA;\n+  }\n+\n+  // Get sizePerThread for M or N axis.\n+  static int getsizePerThreadForMN(BlockedEncodingAttr layout, bool isM) {\n+    auto order = layout.getOrder();\n+    auto sizePerThread = getSizePerThread(layout);\n+\n+    int mSizePerThread =\n+        order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+    int nSizePerThread =\n+        order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+    return isM ? mSizePerThread : nSizePerThread;\n+  }\n+};\n+\n Value ConvertLayoutOpConversion::lowerSharedToDotOperandMMA(\n     triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n     ConversionPatternRewriter &rewriter, const MmaEncodingAttr &mmaLayout,\n@@ -4421,14 +4573,15 @@ Value ConvertLayoutOpConversion::lowerSharedToDotOperandMMA(\n   Value src = op.src();\n   Value dst = op.result();\n   auto dstTensorTy = dst.getType().cast<RankedTensorType>();\n-  // TODO[Superjomn]: the allowTF32 is not available in ConvertLayoutOp for it\n-  // is an attribute of DotOp.\n+  // TODO[Superjomn]: allowTF32 is not accessible here for it is an attribute of\n+  // an Op instance.\n   bool allowTF32 = false;\n   bool isHMMA = DotOpConversion::isDotHMMA(dstTensorTy, allowTF32,\n                                            mmaLayout.getVersion());\n \n   auto smemObj = getSharedMemoryObjectFromStruct(loc, adaptor.src(), rewriter);\n   Value res;\n+\n   if (!isOuter && mmaLayout.getVersion() == 2 && isHMMA) { // tensor core v2\n     MMA16816ConversionHelper mmaHelper(mmaLayout, getThreadId(rewriter, loc),\n                                        rewriter, getTypeConverter(),\n@@ -4487,7 +4640,25 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n   } else if (auto blockedLayout =\n                  dotOperandLayout.getParent()\n                      .dyn_cast_or_null<BlockedEncodingAttr>()) {\n-    assert(false && \"Blocked layout is not supported yet\");\n+    // TODO[Superjomn]: the allowTF32 is not available in ConvertLayoutOp for it\n+    // is an attribute of DotOp.\n+    bool allowTF32 = false;\n+    bool isFMADot = dstTensorTy.getElementType().isF32() && !allowTF32;\n+    if (isFMADot) {\n+      auto dotOpLayout =\n+          dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n+      auto blockedLayout = dotOpLayout.getParent().cast<BlockedEncodingAttr>();\n+      DotOpFMAConversionHelper helper(blockedLayout);\n+      auto thread = getThreadId(rewriter, loc);\n+      if (dotOpLayout.getOpIdx() == 0) { // $a\n+        res = helper.loadA(src, adaptor.src(), blockedLayout, thread, loc,\n+                           rewriter);\n+      } else { // $b\n+        res = helper.loadB(src, adaptor.src(), blockedLayout, thread, loc,\n+                           rewriter);\n+      }\n+    } else\n+      assert(false && \"Unsupported dot operand layout found\");\n   } else {\n     assert(false && \"Unsupported dot operand layout found\");\n   }\n@@ -4953,62 +5124,28 @@ DotOpMmaV1ConversionHelper::extractLoadedOperand(\n   return rcds;\n }\n \n-LogicalResult\n-DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n-                               ConversionPatternRewriter &rewriter) const {\n-  auto *ctx = rewriter.getContext();\n-  auto loc = op.getLoc();\n-  auto threadId = getThreadId(rewriter, loc);\n-\n-  using ValueTable = std::map<std::pair<int, int>, Value>;\n-\n-  auto A = op.a();\n-  auto B = op.b();\n-  auto C = op.c();\n-  auto D = op.getResult();\n-\n+Value DotOpFMAConversionHelper::loadA(\n+    Value A, Value llA, BlockedEncodingAttr dLayout, Value thread, Location loc,\n+    ConversionPatternRewriter &rewriter) const {\n   auto aTensorTy = A.getType().cast<RankedTensorType>();\n-  auto bTensorTy = B.getType().cast<RankedTensorType>();\n-  auto cTensorTy = C.getType().cast<RankedTensorType>();\n-  auto dTensorTy = D.getType().cast<RankedTensorType>();\n-\n-  auto aShape = aTensorTy.getShape();\n-  auto bShape = bTensorTy.getShape();\n-  auto cShape = cTensorTy.getShape();\n-\n   auto aLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n-  auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n-  auto cLayout = cTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n-  auto dLayout = dTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto aShape = aTensorTy.getShape();\n \n   auto aOrder = aLayout.getOrder();\n-  auto bOrder = bLayout.getOrder();\n-\n   auto order = dLayout.getOrder();\n \n   bool isARow = aOrder[0] == 1;\n-  bool isBRow = bOrder[0] == 1;\n \n   int strideAM = isARow ? aShape[1] : 1;\n   int strideAK = isARow ? 1 : aShape[0];\n-  int strideBN = isBRow ? 1 : bShape[0];\n-  int strideBK = isBRow ? bShape[1] : 1;\n   int strideA0 = isARow ? strideAK : strideAM;\n   int strideA1 = isARow ? strideAM : strideAK;\n-  int strideB0 = isBRow ? strideBN : strideBK;\n-  int strideB1 = isBRow ? strideBK : strideBN;\n   int lda = isARow ? strideAM : strideAK;\n-  int ldb = isBRow ? strideBK : strideBN;\n-  int aPerPhase = aLayout.getPerPhase();\n-  int aMaxPhase = aLayout.getMaxPhase();\n-  int bPerPhase = bLayout.getPerPhase();\n-  int bMaxPhase = bLayout.getMaxPhase();\n   int aNumPtr = 8;\n   int bNumPtr = 8;\n   int NK = aShape[1];\n \n   auto shapePerCTA = getShapePerCTA(dLayout);\n-\n   auto sizePerThread = getSizePerThread(dLayout);\n \n   Value _0 = i32_val(0);\n@@ -5017,19 +5154,7 @@ DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n   Value nContig = i32_val(sizePerThread[order[0]]);\n \n   // threadId in blocked layout\n-  SmallVector<Value> threadIds;\n-  {\n-    int dim = cShape.size();\n-    threadIds.resize(dim);\n-    for (unsigned k = 0; k < dim - 1; k++) {\n-      Value dimK = i32_val(shapePerCTA[order[k]]);\n-      Value rem = urem(threadId, dimK);\n-      threadId = udiv(threadId, dimK);\n-      threadIds[order[k]] = rem;\n-    }\n-    Value dimK = i32_val(shapePerCTA[order[dim - 1]]);\n-    threadIds[order[dim - 1]] = urem(threadId, dimK);\n-  }\n+  auto threadIds = getThreadIds(thread, shapePerCTA, order, rewriter, loc);\n \n   Value threadIdM = threadIds[0];\n   Value threadIdN = threadIds[1];\n@@ -5041,55 +5166,226 @@ DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n     aOff[i] = add(mul(offA0, i32_val(strideA0)), mul(offA1, i32_val(strideA1)));\n   }\n \n+  auto aSmem =\n+      ConvertTritonGPUOpToLLVMPatternBase::getSharedMemoryObjectFromStruct(\n+          loc, llA, rewriter);\n+\n+  Type f32PtrTy = ptr_ty(f32_ty);\n+  SmallVector<Value> aPtrs(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i)\n+    aPtrs[i] = gep(f32PtrTy, aSmem.base, aOff[i]);\n+\n+  ValueTable has;\n+  int M = aShape[aOrder[1]];\n+\n+  int mShapePerCTA = getShapePerCTAForMN(dLayout, true /*isM*/);\n+  int mSizePerThread = getsizePerThreadForMN(dLayout, true /*isM*/);\n+\n+  for (unsigned k = 0; k < NK; ++k) {\n+    for (unsigned m = 0; m < M; m += mShapePerCTA)\n+      for (unsigned mm = 0; mm < mSizePerThread; ++mm)\n+        if (!has.count({m + mm, k})) {\n+          Value pa = gep(f32PtrTy, aPtrs[0],\n+                         i32_val((m + mm) * strideAM + k * strideAK));\n+          Value va = load(pa);\n+          has[{m + mm, k}] = va;\n+        }\n+  }\n+\n+  return getStructFromValueTable(has, rewriter, loc);\n+}\n+\n+Value DotOpFMAConversionHelper::loadB(\n+    Value B, Value llB, BlockedEncodingAttr dLayout, Value thread, Location loc,\n+    ConversionPatternRewriter &rewriter) const {\n+\n+  auto bTensorTy = B.getType().cast<RankedTensorType>();\n+  auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto bShape = bTensorTy.getShape();\n+\n+  auto bOrder = bLayout.getOrder();\n+  auto order = dLayout.getOrder();\n+\n+  bool isBRow = bOrder[0] == 1;\n+\n+  int strideBN = isBRow ? 1 : bShape[0];\n+  int strideBK = isBRow ? bShape[1] : 1;\n+  int strideB0 = isBRow ? strideBN : strideBK;\n+  int strideB1 = isBRow ? strideBK : strideBN;\n+  int ldb = isBRow ? strideBK : strideBN;\n+  int bNumPtr = 8;\n+  int NK = bShape[0];\n+\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+  auto sizePerThread = getSizePerThread(dLayout);\n+\n+  Value _0 = i32_val(0);\n+\n+  Value mContig = i32_val(sizePerThread[order[1]]);\n+  Value nContig = i32_val(sizePerThread[order[0]]);\n+\n+  // threadId in blocked layout\n+  auto threadIds = getThreadIds(thread, shapePerCTA, order, rewriter, loc);\n+  Value threadIdN = threadIds[1];\n+\n   Value offB0 = isBRow ? mul(threadIdN, nContig) : _0;\n   Value offB1 = isBRow ? _0 : mul(threadIdN, nContig);\n   SmallVector<Value> bOff(bNumPtr);\n   for (int i = 0; i < bNumPtr; ++i) {\n     bOff[i] = add(mul(offB0, i32_val(strideB0)), mul(offB1, i32_val(strideB1)));\n   }\n \n-  auto aSmem = getSharedMemoryObjectFromStruct(loc, adaptor.a(), rewriter);\n-  auto bSmem = getSharedMemoryObjectFromStruct(loc, adaptor.b(), rewriter);\n+  auto bSmem =\n+      ConvertTritonGPUOpToLLVMPatternBase::getSharedMemoryObjectFromStruct(\n+          loc, llB, rewriter);\n \n   Type f32PtrTy = ptr_ty(f32_ty);\n-  SmallVector<Value> aPtrs(aNumPtr);\n-  for (int i = 0; i < aNumPtr; ++i)\n-    aPtrs[i] = gep(f32PtrTy, aSmem.base, aOff[i]);\n-\n   SmallVector<Value> bPtrs(bNumPtr);\n   for (int i = 0; i < bNumPtr; ++i)\n     bPtrs[i] = gep(f32PtrTy, bSmem.base, bOff[i]);\n \n+  int N = bShape[bOrder[0]];\n+  ValueTable hbs;\n+\n+  int nShapePerCTA = getShapePerCTAForMN(dLayout, false /*isM*/);\n+  int nSizePerThread = getsizePerThreadForMN(dLayout, false /*isM*/);\n+\n+  for (unsigned k = 0; k < NK; ++k)\n+    for (unsigned n = 0; n < N; n += nShapePerCTA)\n+      for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n+        Value pb = gep(f32PtrTy, bPtrs[0],\n+                       i32_val((n + nn) * strideBN + k * strideBK));\n+        Value vb = load(pb);\n+        hbs[{n + nn, k}] = vb;\n+      }\n+\n+  return getStructFromValueTable(hbs, rewriter, loc);\n+}\n+\n+DotOpFMAConversionHelper::ValueTable\n+DotOpFMAConversionHelper::getValueTableFromStruct(\n+    Value val, int K, int n0, int shapePerCTA, int sizePerThread,\n+    ConversionPatternRewriter &rewriter, Location loc) const {\n+  ValueTable res;\n+  auto elems = ConvertTritonGPUOpToLLVMPatternBase::getElementsFromStruct(\n+      loc, val, rewriter);\n+  int id = 0;\n+  std::set<std::pair<int, int>> keys; // ordered\n+  for (unsigned k = 0; k < K; ++k) {\n+    for (unsigned m = 0; m < n0; m += shapePerCTA)\n+      for (unsigned mm = 0; mm < sizePerThread; ++mm) {\n+        keys.insert({m + mm, k});\n+      }\n+  }\n+\n+  for (auto &key : llvm::enumerate(keys)) {\n+    res[key.value()] = elems[key.index()];\n+  }\n+\n+  return res;\n+}\n+SmallVector<Value> DotOpFMAConversionHelper::getThreadIds(\n+    Value threadId, ArrayRef<unsigned int> shapePerCTA,\n+    ArrayRef<unsigned int> order, ConversionPatternRewriter &rewriter,\n+    Location loc) const {\n+  int dim = order.size();\n+  SmallVector<Value> threadIds(dim);\n+  for (unsigned k = 0; k < dim - 1; k++) {\n+    Value dimK = i32_val(shapePerCTA[order[k]]);\n+    Value rem = urem(threadId, dimK);\n+    threadId = udiv(threadId, dimK);\n+    threadIds[order[k]] = rem;\n+  }\n+  Value dimK = i32_val(shapePerCTA[order[dim - 1]]);\n+  threadIds[order[dim - 1]] = urem(threadId, dimK);\n+  return threadIds;\n+}\n+\n+LogicalResult\n+DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n+                               ConversionPatternRewriter &rewriter) const {\n+  auto *ctx = rewriter.getContext();\n+  auto loc = op.getLoc();\n+  auto threadId = getThreadId(rewriter, loc);\n+\n+  using ValueTable = std::map<std::pair<int, int>, Value>;\n+\n+  auto A = op.a();\n+  auto B = op.b();\n+  auto C = op.c();\n+  auto D = op.getResult();\n+\n+  auto aTensorTy = A.getType().cast<RankedTensorType>();\n+  auto bTensorTy = B.getType().cast<RankedTensorType>();\n+  auto cTensorTy = C.getType().cast<RankedTensorType>();\n+  auto dTensorTy = D.getType().cast<RankedTensorType>();\n+\n+  auto aShape = aTensorTy.getShape();\n+  auto bShape = bTensorTy.getShape();\n+  auto cShape = cTensorTy.getShape();\n+\n   ValueTable has, hbs;\n+  int mShapePerCTA{-1}, nShapePerCTA{-1};\n+  int mSizePerThread{-1}, nSizePerThread{-1};\n+  ArrayRef<unsigned> aOrder, bOrder;\n+  Value llA, llB;\n+  BlockedEncodingAttr dLayout =\n+      dTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto order = dLayout.getOrder();\n   auto cc = getElementsFromStruct(loc, adaptor.c(), rewriter);\n-  SmallVector<Value> ret = cc;\n-  // is this compatible with blocked layout?\n \n-  for (unsigned k = 0; k < NK; k++) {\n-    int z = 0;\n-    for (unsigned i = 0; i < cShape[order[1]]; i += shapePerCTA[order[1]])\n-      for (unsigned j = 0; j < cShape[order[0]]; j += shapePerCTA[order[0]])\n-        for (unsigned ii = 0; ii < sizePerThread[order[1]]; ++ii)\n-          for (unsigned jj = 0; jj < sizePerThread[order[0]]; ++jj) {\n-            unsigned m = order[0] == 1 ? i : j;\n-            unsigned n = order[0] == 1 ? j : i;\n-            unsigned mm = order[0] == 1 ? ii : jj;\n-            unsigned nn = order[0] == 1 ? jj : ii;\n-            if (!has.count({m + mm, k})) {\n-              Value pa = gep(f32PtrTy, aPtrs[0],\n-                             i32_val((m + mm) * strideAM + k * strideAK));\n-              Value va = load(pa);\n-              has[{m + mm, k}] = va;\n-            }\n-            if (!hbs.count({n + nn, k})) {\n-              Value pb = gep(f32PtrTy, bPtrs[0],\n-                             i32_val((n + nn) * strideBN + k * strideBK));\n-              Value vb = load(pb);\n-              hbs[{n + nn, k}] = vb;\n-            }\n+  DotOpFMAConversionHelper helper(dLayout);\n+  if (auto aDotOpLayout =\n+          aTensorTy.getEncoding()\n+              .dyn_cast<DotOperandEncodingAttr>()) { // get input from\n+                                                     // convert_layout\n+    auto bDotOpLayout =\n+        bTensorTy.getEncoding().dyn_cast<DotOperandEncodingAttr>();\n+    auto aLayout = aDotOpLayout.getParent().cast<BlockedEncodingAttr>();\n+    auto bLayout = bDotOpLayout.getParent().cast<BlockedEncodingAttr>();\n+\n+    assert(bLayout);\n+    llA = adaptor.a();\n+    llB = adaptor.b();\n+  } else if (auto aLayout =\n+                 aTensorTy.getEncoding()\n+                     .dyn_cast<SharedEncodingAttr>()) { // load input from smem\n+    auto bLayout = bTensorTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n+    assert(bLayout);\n+    Value thread = getThreadId(rewriter, loc);\n+    llA = helper.loadA(A, adaptor.a(), dLayout, thread, loc, rewriter);\n+    llB = helper.loadB(B, adaptor.b(), dLayout, thread, loc, rewriter);\n+  }\n+\n+  auto sizePerThread = getSizePerThread(dLayout);\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n \n+  int K = aShape[1];\n+  int M = aShape[0];\n+  int N = bShape[1];\n+\n+  mShapePerCTA = order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+  mSizePerThread =\n+      order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+  nShapePerCTA = order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+  nSizePerThread =\n+      order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+\n+  has = helper.getValueTableFromStruct(llA, K, M, mShapePerCTA, mSizePerThread,\n+                                       rewriter, loc);\n+  hbs = helper.getValueTableFromStruct(llB, K, N, nShapePerCTA, nSizePerThread,\n+                                       rewriter, loc);\n+\n+  SmallVector<Value> ret = cc;\n+  for (unsigned k = 0; k < K; k++) {\n+    int z = 0;\n+    for (unsigned m = 0; m < M; m += mShapePerCTA)\n+      for (unsigned n = 0; n < N; n += nShapePerCTA)\n+        for (unsigned mm = 0; mm < mSizePerThread; ++mm)\n+          for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n             ret[z] = rewriter.create<LLVM::FMulAddOp>(loc, has[{m + mm, k}],\n                                                       hbs[{n + nn, k}], ret[z]);\n+\n             ++z;\n           }\n   }\n@@ -5166,6 +5462,13 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n     auto ctx = type.getContext();\n     Attribute layout = type.getEncoding();\n     auto shape = type.getShape();\n+\n+    // TODO[Keren, Superjomn]: fix it, allowTF32 is not accessible here for it\n+    // is bound to an Op instance.\n+    bool allowTF32 = false;\n+    bool isFMADot = type.getElementType().isF32() && !allowTF32 &&\n+                    layout.dyn_cast_or_null<DotOperandEncodingAttr>();\n+\n     if (layout &&\n         (layout.isa<BlockedEncodingAttr>() || layout.isa<SliceEncodingAttr>() ||\n          layout.isa<MmaEncodingAttr>())) {\n@@ -5186,65 +5489,55 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n         types.push_back(IntegerType::get(ctx, 32));\n       }\n       return LLVM::LLVMStructType::getLiteral(ctx, types);\n-    } else if (auto mmaLayout = layout.dyn_cast_or_null<MmaEncodingAttr>()) {\n-      if (mmaLayout.getVersion() == 2) {\n-        auto [repM, repN] = DotOpMmaV2ConversionHelper::getRepMN(type);\n-        size_t fcSize = 4 * repM * repN;\n-        return LLVM::LLVMStructType::getLiteral(\n-            ctx, SmallVector<Type>(fcSize, convertType(type.getElementType())));\n-      }\n+    } else if (auto dotOpLayout =\n+                   layout.dyn_cast_or_null<DotOperandEncodingAttr>()) {\n+      if (isFMADot) { // for parent is blocked layout\n+        int numElemsPerThread =\n+            DotOpFMAConversionHelper::getNumElemsPerThread(shape, dotOpLayout);\n \n-      if (mmaLayout.getVersion() == 1) {\n-        DotOpMmaV1ConversionHelper helper(mmaLayout);\n-        int repM = helper.getRepM(shape[0]);\n-        int repN = helper.getRepN(shape[1]);\n-        int elems = 8 * repM * repN;\n         return LLVM::LLVMStructType::getLiteral(\n-            ctx, SmallVector<Type>(elems, convertType(type.getElementType())));\n-      }\n-\n-      llvm::errs()\n-          << \"Unexpected mma layout detected in TritonToLLVMTypeConverter\";\n-      return llvm::None;\n-\n-    } else if (auto dot_op_layout =\n-                   layout.dyn_cast_or_null<DotOperandEncodingAttr>()) {\n-      auto mmaLayout = dot_op_layout.getParent().cast<MmaEncodingAttr>();\n-      auto wpt = mmaLayout.getWarpsPerCTA();\n-      Type elemTy = convertType(type.getElementType());\n-      auto vecSize = 1;\n-      if (elemTy.getIntOrFloatBitWidth() == 16) {\n-        vecSize = 2;\n-      } else if (elemTy.getIntOrFloatBitWidth() == 8) {\n-        vecSize = 4;\n-      } else {\n-        assert(false && \"Unsupported element type\");\n-      }\n-      Type vecTy = vec_ty(elemTy, vecSize);\n-      if (mmaLayout.getVersion() == 2) {\n-        if (dot_op_layout.getOpIdx() == 0) { // $a\n-          int elems =\n-              MMA16816ConversionHelper::getANumElemsPerThread(type, wpt);\n-          return LLVM::LLVMStructType::getLiteral(\n-              ctx, SmallVector<Type>(elems, vecTy));\n+            ctx, SmallVector<Type>(numElemsPerThread, type::f32Ty(ctx)));\n+\n+      } else { // for parent is MMA layout\n+        auto mmaLayout = dotOpLayout.getParent().cast<MmaEncodingAttr>();\n+        auto wpt = mmaLayout.getWarpsPerCTA();\n+        Type elemTy = convertType(type.getElementType());\n+        auto vecSize = 1;\n+        if (elemTy.getIntOrFloatBitWidth() == 16) {\n+          vecSize = 2;\n+        } else if (elemTy.getIntOrFloatBitWidth() == 8) {\n+          vecSize = 4;\n+        } else {\n+          assert(false && \"Unsupported element type\");\n         }\n-        if (dot_op_layout.getOpIdx() == 1) { // $b\n-          int elems =\n-              MMA16816ConversionHelper::getBNumElemsPerThread(type, wpt);\n-          return struct_ty(SmallVector<Type>(elems, vecTy));\n+        Type vecTy = vec_ty(elemTy, vecSize);\n+        if (mmaLayout.getVersion() == 2) {\n+          if (dotOpLayout.getOpIdx() == 0) { // $a\n+            int elems =\n+                MMA16816ConversionHelper::getANumElemsPerThread(type, wpt);\n+            return LLVM::LLVMStructType::getLiteral(\n+                ctx, SmallVector<Type>(elems, vecTy));\n+          }\n+          if (dotOpLayout.getOpIdx() == 1) { // $b\n+            int elems =\n+                MMA16816ConversionHelper::getBNumElemsPerThread(type, wpt);\n+            return struct_ty(SmallVector<Type>(elems, vecTy));\n+          }\n         }\n-      }\n \n-      if (mmaLayout.getVersion() == 1) {\n-        DotOpMmaV1ConversionHelper helper(mmaLayout);\n+        if (mmaLayout.getVersion() == 1) {\n+          DotOpMmaV1ConversionHelper helper(mmaLayout);\n \n-        if (dot_op_layout.getOpIdx() == 0) { // $a\n-          int elems = helper.numElemsPerThreadA(type);\n-          return struct_ty(SmallVector<Type>(elems, vecTy));\n-        }\n-        if (dot_op_layout.getOpIdx() == 1) { // $b\n-          int elems = helper.numElemsPerThreadB(type);\n-          return struct_ty(SmallVector<Type>(elems, vecTy));\n+          if (dotOpLayout.getOpIdx() == 0) { // $a\n+            int elems = helper.numElemsPerThreadA(type);\n+            Type x2Ty = vec_ty(elemTy, 2);\n+            return struct_ty(SmallVector<Type>(elems, x2Ty));\n+          }\n+          if (dotOpLayout.getOpIdx() == 1) { // $b\n+            int elems = helper.numElemsPerThreadB(type);\n+            Type x2Ty = vec_ty(elemTy, 2);\n+            return struct_ty(SmallVector<Type>(elems, x2Ty));\n+          }\n         }\n       }\n \n@@ -5580,6 +5873,32 @@ struct FDivOpConversion\n   }\n };\n \n+struct ExpOpConversionApprox\n+    : ElementwiseOpConversionBase<mlir::math::ExpOp, LLVM::InlineAsmOp,\n+                                  ExpOpConversionApprox> {\n+  using Base = ElementwiseOpConversionBase<mlir::math::ExpOp, LLVM::InlineAsmOp,\n+                                           ExpOpConversionApprox>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::math::ExpOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    // For FP64 input, call __nv_expf for higher-precision calculation\n+    if (elemTy.getIntOrFloatBitWidth() == 64)\n+      return {};\n+    const double log2e = 1.4426950408889634;\n+    Value prod =\n+        rewriter.create<LLVM::FMulOp>(loc, f32_ty, operands[0], f32_val(log2e));\n+    PTXBuilder ptxBuilder;\n+    auto &exp2 = ptxBuilder.create<PTXInstr>(\"ex2\")->o(\"approx\").o(\"f32\");\n+    auto output = ptxBuilder.newOperand(\"=f\");\n+    auto input = ptxBuilder.newOperand(prod, \"f\");\n+    exp2(output, input);\n+    return ptxBuilder.launch(rewriter, loc, f32_ty, false);\n+  }\n+};\n+\n /// ====================== atomic_rmw codegen begin ==========================\n struct AtomicRMWOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::AtomicRMWOp>,\n@@ -5740,6 +6059,13 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n \n   patterns.add<CmpIOpConversion>(typeConverter, benefit);\n   patterns.add<CmpFOpConversion>(typeConverter, benefit);\n+\n+  // ExpOpConversionApprox will try using ex2.approx if the input type is FP32.\n+  // For FP64 input type, ExpOpConversionApprox will return failure and\n+  // ElementwiseOpConversion<math::ExpOp, math::ExpOp> defined below will call\n+  // __nv_expf for higher-precision calculation\n+  patterns.add<ExpOpConversionApprox>(typeConverter, benefit);\n+\n #define POPULATE_UNARY_OP(SRC_OP, DST_OP)                                      \\\n   patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n \n@@ -5776,6 +6102,7 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<ExtractSliceOpConversion>(typeConverter, allocation, smem,\n                                          benefit);\n   patterns.add<GetProgramIdOpConversion>(typeConverter, benefit);\n+  patterns.add<GetNumProgramsOpConversion>(typeConverter, benefit);\n   patterns.add<InsertSliceAsyncOpConversion>(typeConverter, allocation, smem,\n                                              axisInfoAnalysis, benefit);\n   patterns.add<LoadOpConversion>(typeConverter, axisInfoAnalysis, benefit);"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -64,8 +64,7 @@ unsigned getElemsPerThread(Attribute layout, ArrayRef<int64_t> shape) {\n }\n \n unsigned getElemsPerThread(Type type) {\n-  if (type.isIntOrIndexOrFloat() ||\n-      type.isa<triton::Float8Type>() ||\n+  if (type.isIntOrIndexOrFloat() || type.isa<triton::Float8Type>() ||\n       type.isa<triton::PointerType>())\n     return 1;\n   auto tensorType = type.cast<RankedTensorType>();\n@@ -372,6 +371,9 @@ unsigned SharedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n \n unsigned\n DotOperandEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n+  if (auto blockedLayout = getParent().dyn_cast<BlockedEncodingAttr>()) {\n+    return blockedLayout.getElemsPerThread(shape);\n+  }\n   assert(0 && \"DotOperandEncodingAttr::getElemsPerThread not implemented\");\n   return 0;\n }"}, {"filename": "python/tests/test_elementwise.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -61,6 +61,7 @@ def get_tensor(shape, data_type, b_positive=False):\n                              ('sqrt', 'float64', 'float64'),\n                              ('abs', 'float32', 'float32'),\n                              ('exp', 'float32', 'float32'),\n+                             ('exp', 'float64', 'float64'),\n                              ('sigmoid', 'float32', 'float32'),\n                           ])\n def test_single_input(expr, output_type, input0_type):"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 141, "deletions": 101, "changes": 242, "file_content_changes": "@@ -30,18 +30,32 @@ def matmul_no_scf_kernel(\n # TODO: num_warps could only be 4 for now\n \n \n-@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS', [\n-    [128, 256, 32, 4],\n-    [256, 128, 16, 4],\n-    [128, 16, 32, 4],\n-    [32, 128, 64, 4],\n-    [128, 128, 64, 4],\n-    [64, 128, 128, 4],\n-    [64, 128, 128, 2],\n+@pytest.mark.parametrize('SHAPE,NUM_WARPS,TRANS_A,TRANS_B', [\n+    (shape, num_warps, trans_a, trans_b)\n+    for shape in [\n+        [128, 256, 32],\n+        [256, 128, 16],\n+        [128, 16, 32],\n+        [32, 128, 64],\n+        [128, 128, 64],\n+        [64, 128, 128],\n+    ]\n+    for num_warps in [2, 4]\n+    for trans_a in [False, True]\n+    for trans_b in [False, True]\n ])\n-def test_gemm_no_scf(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n-    a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n-    b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+def test_gemm_no_scf(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n+    SIZE_M, SIZE_N, SIZE_K = SHAPE\n+    if (TRANS_A):\n+        a = torch.randn((SIZE_K, SIZE_M), device='cuda', dtype=torch.float16).T\n+    else:\n+        a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n+\n+    if (TRANS_B):\n+        b = torch.randn((SIZE_N, SIZE_K), device='cuda', dtype=torch.float16).T\n+    else:\n+        b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+\n     c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n     grid = lambda META: (1, )\n     matmul_no_scf_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n@@ -55,16 +69,32 @@ def test_gemm_no_scf(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n     assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)\n \n \n-@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS', [\n-    [64, 128, 128, 1],\n-    [128, 128, 128, 4],\n-    [16, 8, 32, 1],\n-    [32, 16, 64, 2],\n-    [32, 16, 64, 4],\n+@pytest.mark.parametrize('SHAPE,NUM_WARPS,TRANS_A,TRANS_B', [\n+    (shape, num_warps, trans_a, trans_b)\n+    for shape in [\n+        [64, 128, 128],\n+        [128, 128, 128],\n+        [16, 8, 32],\n+        [32, 16, 64],\n+        [32, 16, 64],\n+    ]\n+    for num_warps in [1, 2, 4]\n+    for trans_a in [False, True]\n+    for trans_b in [False, True]\n ])\n-def test_gemm_no_scf_int8(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n-    a = torch.randint(-5, 5, (SIZE_M, SIZE_K), device='cuda', dtype=torch.int8)\n-    b = torch.randint(-5, 5, (SIZE_K, SIZE_N), device='cuda', dtype=torch.int8)\n+def test_gemm_no_scf_int8(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n+    SIZE_M, SIZE_N, SIZE_K = SHAPE\n+\n+    if (TRANS_A):\n+        a = torch.randint(-5, 5, (SIZE_K, SIZE_M), device='cuda', dtype=torch.int8).T\n+    else:\n+        a = torch.randint(-5, 5, (SIZE_M, SIZE_K), device='cuda', dtype=torch.int8)\n+\n+    if (TRANS_B):\n+        b = torch.randint(-5, 5, (SIZE_N, SIZE_K), device='cuda', dtype=torch.int8).T\n+    else:\n+        b = torch.randint(-5, 5, (SIZE_K, SIZE_N), device='cuda', dtype=torch.int8)\n+\n     c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.int32)\n \n     grid = lambda META: (1, )\n@@ -125,28 +155,39 @@ def get_variant_golden(a, b):\n     return c_padded[:SIZE_M, :SIZE_N]\n \n \n-@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K', [\n+@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,TRANS_A,TRANS_B', [\n     # Non-forloop\n-    [64, 32, 64, 4, 64, 32, 64],\n-    [128, 64, 128, 4, 128, 64, 128],\n+    [64, 32, 64, 4, 64, 32, 64, False, False],\n+    [128, 64, 128, 4, 128, 64, 128, False, False],\n     # K-Forloop\n-    [64, 32, 128, 4, 64, 32, 64],\n-    [128, 16, 128, 4, 128, 16, 32],\n-    [32, 16, 128, 4, 32, 16, 32],\n-    [32, 64, 128, 4, 32, 64, 32],\n-    [32, 128, 256, 4, 32, 128, 64],\n-    [64, 128, 64, 4, 64, 128, 32],\n-    [64, 64, 128, 4, 64, 64, 32],\n-    [128, 128, 64, 4, 128, 128, 32],\n-    [128, 128, 128, 4, 128, 128, 32],\n-    [128, 128, 256, 4, 128, 128, 64],\n-    [128, 256, 128, 4, 128, 256, 32],\n-    [256, 128, 64, 4, 256, 128, 16],\n-    [128, 64, 128, 4, 128, 64, 32],\n+    [64, 32, 128, 4, 64, 32, 64, False, False],\n+    [128, 16, 128, 4, 128, 16, 32, False, False],\n+    [32, 16, 128, 4, 32, 16, 32, False, False],\n+    [32, 64, 128, 4, 32, 64, 32, False, False],\n+    [32, 128, 256, 4, 32, 128, 64, False, False],\n+    [64, 128, 64, 4, 64, 128, 32, False, False],\n+    [64, 64, 128, 4, 64, 64, 32, False, False],\n+    [128, 128, 64, 4, 128, 128, 32, False, False],\n+    [128, 128, 128, 4, 128, 128, 32, False, False],\n+    [128, 128, 256, 4, 128, 128, 64, False, False],\n+    [128, 256, 128, 4, 128, 256, 32, False, False],\n+    [256, 128, 64, 4, 256, 128, 16, False, False],\n+    [128, 64, 128, 4, 128, 64, 32, False, False],\n+    # TODO[goostavz]: fix these cases\n+    #[128, 64, 128, 4, 128, 64, 32, True, False],\n+    #[128, 64, 128, 4, 128, 64, 32, False, True],\n ])\n-def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K):\n-    a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n-    b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B):\n+    if (TRANS_A):\n+        a = torch.randn((SIZE_K, SIZE_M), device='cuda', dtype=torch.float16).T\n+    else:\n+        a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n+\n+    if (TRANS_B):\n+        b = torch.randn((SIZE_N, SIZE_K), device='cuda', dtype=torch.float16).T\n+    else:\n+        b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+\n     c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n     grid = lambda META: (1, )\n     matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n@@ -171,65 +212,64 @@ def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLO\n     assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err), check_dtype=False)\n \n \n-# XXX(Keren): Temporarily disable this test until we have shared -> dot conversion implemented\n-#@pytest.mark.parametrize('M,N,K,num_warps,block_M,block_N,block_K', [\n-#    [32, 32, 16, 4, 32, 32, 16],\n-#    [32, 16, 16, 4, 32, 32, 16],\n-#    [128, 8, 8, 4, 32, 32, 16],\n-#    [127, 41, 43, 4, 32, 32, 16],\n-#])\n-#def test_gemm_fmadot(M, N, K, num_warps, block_M, block_N, block_K):\n-#    @triton.jit\n-#    def matmul_kernel(\n-#        a_ptr, b_ptr, c_ptr,\n-#        M, N, K,\n-#        stride_am, stride_ak,\n-#        stride_bk, stride_bn,\n-#        stride_cm, stride_cn,\n-#        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n-#    ):\n-#        pid = tl.program_id(axis=0)\n-#        # num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n-#        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n-#        pid_m = pid // num_pid_n\n-#        pid_n = pid % num_pid_n\n-#\n-#        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n-#        offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n-#        offs_k = tl.arange(0, BLOCK_SIZE_K)\n-#        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n-#        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n-#\n-#        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-#        for k in range(0, K, BLOCK_SIZE_K):\n-#            a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < K)\n-#            b_mask = (offs_k[:, None] < K) & (offs_bn[None, :] < N)\n-#            a = tl.load(a_ptrs, a_mask)\n-#            b = tl.load(b_ptrs, b_mask)\n-#            # NOTE the allow_tf32 should be false to force the dot op to do fmadot lowering\n-#            accumulator += tl.dot(a, b, allow_tf32=False)\n-#            a_ptrs += BLOCK_SIZE_K * stride_ak\n-#            b_ptrs += BLOCK_SIZE_K * stride_bk\n-#            offs_k += BLOCK_SIZE_K\n-#\n-#        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n-#        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n-#        c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n-#        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n-#        tl.store(c_ptrs, accumulator, c_mask)\n-#\n-#    a = torch.randn((M, K), device='cuda', dtype=torch.float32)\n-#    b = torch.randn((K, N), device='cuda', dtype=torch.float32)\n-#    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n-#\n-#    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n-#    matmul_kernel[grid](a, b, c,\n-#                        M, N, K,\n-#                        stride_am=a.stride(0), stride_ak=a.stride(1),\n-#                        stride_bk=b.stride(0), stride_bn=b.stride(1),\n-#                        stride_cm=c.stride(0), stride_cn=c.stride(1),\n-#                        BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N, BLOCK_SIZE_K=block_K)\n-#\n-#    golden = torch.matmul(a, b)\n-#    torch.testing.assert_close(c, golden)\n-#\n+@pytest.mark.parametrize('M,N,K,num_warps,block_M,block_N,block_K', [\n+    [32, 32, 16, 4, 32, 32, 16],\n+    [32, 16, 16, 4, 32, 32, 16],\n+    [128, 8, 8, 4, 32, 32, 16],\n+    # TODO[Superjomn]: fix it later\n+    # [127, 41, 43, 4, 32, 32, 16],\n+])\n+def test_gemm_fmadot(M, N, K, num_warps, block_M, block_N, block_K):\n+    @triton.jit\n+    def matmul_kernel(\n+        a_ptr, b_ptr, c_ptr,\n+        M, N, K,\n+        stride_am, stride_ak,\n+        stride_bk, stride_bn,\n+        stride_cm, stride_cn,\n+        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+    ):\n+        pid = tl.program_id(axis=0)\n+        # num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n+        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+        pid_m = pid // num_pid_n\n+        pid_n = pid % num_pid_n\n+\n+        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+        offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+        offs_k = tl.arange(0, BLOCK_SIZE_K)\n+        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n+        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n+\n+        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+        for k in range(0, K, BLOCK_SIZE_K):\n+            a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < K)\n+            b_mask = (offs_k[:, None] < K) & (offs_bn[None, :] < N)\n+            a = tl.load(a_ptrs, a_mask)\n+            b = tl.load(b_ptrs, b_mask)\n+            # NOTE the allow_tf32 should be false to force the dot op to do fmadot lowering\n+            accumulator += tl.dot(a, b, allow_tf32=False)\n+            a_ptrs += BLOCK_SIZE_K * stride_ak\n+            b_ptrs += BLOCK_SIZE_K * stride_bk\n+            offs_k += BLOCK_SIZE_K\n+\n+        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+        c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n+        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n+        tl.store(c_ptrs, accumulator, c_mask)\n+\n+    a = torch.randn((M, K), device='cuda', dtype=torch.float32)\n+    b = torch.randn((K, N), device='cuda', dtype=torch.float32)\n+    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n+\n+    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n+    matmul_kernel[grid](a, b, c,\n+                        M, N, K,\n+                        stride_am=a.stride(0), stride_ak=a.stride(1),\n+                        stride_bk=b.stride(0), stride_bn=b.stride(1),\n+                        stride_cm=c.stride(0), stride_cn=c.stride(1),\n+                        BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N, BLOCK_SIZE_K=block_K)\n+\n+    golden = torch.matmul(a, b)\n+    torch.testing.assert_close(c, golden)"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -9,6 +9,8 @@\n #A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n #B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n \n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n // CHECK-LABEL: matmul_loop\n func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n@@ -313,3 +315,5 @@ func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n   return\n   // CHECK-NEXT: size = 40960\n }\n+\n+}"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -9,6 +9,8 @@\n #A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n #B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n \n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n // CHECK-LABEL: matmul_loop\n // There shouldn't be any membar with the dot op encoding.\n func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n@@ -250,3 +252,5 @@ func @for_alias(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B :\n   %cst3 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n   return\n }\n+\n+}"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 44, "deletions": 13, "changes": 57, "file_content_changes": "@@ -820,18 +820,20 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n #shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n-#mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n-#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n-#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#blocked}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#blocked}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   func @matmul_fmadot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %a:tensor<32x16xf32, #shared>, %b:tensor<16x32xf32, #shared>) {\n-    // We are going to completely depracate using shared layout for operands of dot\n-    //%cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #blocked>\n-    //%28 = tt.dot %a, %b, %cst {allowTF32 = false, transA = false, transB = false} : tensor<32x16xf32, #shared> * tensor<16x32xf32, #shared> -> tensor<32x32xf32, #blocked>\n-    //%30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n-    //%36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x32x!tt.ptr<f32>, #blocked>\n-    //tt.store %36, %28 : tensor<32x32xf32, #blocked>\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #blocked>\n+    // CHECK: llvm.intr.fmuladd\n+    %a_mat = triton_gpu.convert_layout %a : (tensor<32x16xf32, #shared>) -> tensor<32x16xf32, #dot_operand_a>\n+    %b_mat = triton_gpu.convert_layout %b : (tensor<16x32xf32, #shared>) -> tensor<16x32xf32, #dot_operand_b>\n+\n+    %28 = tt.dot %a_mat, %b_mat, %cst {allowTF32 = false, transA = false, transB = false} : tensor<32x16xf32, #dot_operand_a> * tensor<16x32xf32, #dot_operand_b> -> tensor<32x32xf32, #blocked>\n+    %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n+    %36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x32x!tt.ptr<f32>, #blocked>\n+    tt.store %36, %28 : tensor<32x32xf32, #blocked>\n     return\n   }\n }\n@@ -848,15 +850,44 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   }\n }\n \n+// -----\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n+func @test_get_program_id(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n+  %blockidx = tt.get_program_id {axis=0:i32} : i32\n+  %blockidy = tt.get_program_id {axis=1:i32} : i32\n+  %blockidz = tt.get_program_id {axis=2:i32} : i32\n+  // CHECK: nvvm.read.ptx.sreg.ctaid.x\n+  // CHECK: nvvm.read.ptx.sreg.ctaid.y\n+  // CHECK: nvvm.read.ptx.sreg.ctaid.z\n+  %v0 = arith.addi %blockidx, %blockidy : i32\n+  %v1 = arith.addi %v0, %blockidz : i32\n+  %0 = tt.splat %v1 : (i32) -> tensor<32xi32, #blocked0>\n+  tt.store %a, %0 : tensor<32xi32, #blocked0>\n+\n+  return\n+}\n+\n+}\n \n // -----\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n-func @test_get_program_id(%a: tensor<32x!tt.ptr<i32>) {\n-  %tid = tt.get_program_id() : i32\n-  %0 = tt.splat %tid : (i32) => tensor<32xi32>\n-  tt.store %a, %0 : tensor<32xi32>\n+func @test_get_num_program(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n+  // CHECK: nvvm.read.ptx.sreg.ntid.x\n+  // CHECK: nvvm.read.ptx.sreg.ntid.y\n+  // CHECK: nvvm.read.ptx.sreg.ntid.z\n+  %blockdimx = tt.get_num_programs {axis=0:i32} : i32\n+  %blockdimy = tt.get_num_programs {axis=1:i32} : i32\n+  %blockdimz = tt.get_num_programs {axis=2:i32} : i32\n+  %v0 = arith.addi %blockdimx, %blockdimy : i32\n+  %v1 = arith.addi %v0, %blockdimz : i32\n+  %0 = tt.splat %v1 : (i32) -> tensor<32xi32, #blocked0>\n+  tt.store %a, %0 : tensor<32xi32, #blocked0>\n+\n+  return\n }\n \n }"}]