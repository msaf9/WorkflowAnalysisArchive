[{"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "file_content_changes": "@@ -78,10 +78,9 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT,\n     pre_hook = None if SPLIT_K == 1 else lambda nargs: nargs['C'].zero_()\n     configs = [triton.Config(kwargs=kwargs, num_warps=NWARP, num_stages=NSTAGE, pre_hook=pre_hook)]\n     kernel = triton.ops._matmul.kernel\n-    decorators = kernel.kernel_decorators\n-    kernel.kernel_decorators = []\n-    triton.autotune(configs, [])(kernel)\n-    kernel.kernel_decorators += decorators[1:]\n+    kernel.run.configs = configs\n+    # kernel.run = kernel.run.run.run\n+    \n     # get matrix shape\n     M = BLOCK_M if M is None else M\n     N = BLOCK_N if N is None else N"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -906,7 +906,7 @@ def ty_to_cpp(ty):\n         \"i64\": \"int64_t\",\n         \"u32\": \"uint32_t\",\n         \"u64\": \"uint64_t\",\n-        \"f32\": \"float\",\n+        \"fp32\": \"float\",\n     }[ty]\n \n "}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -26,9 +26,6 @@ def get_configs_io_bound():\n     return configs\n \n \n-@triton.heuristics({\n-    'EVEN_K': lambda args: args['K'] % (args['BLOCK_K'] * args['SPLIT_K']) == 0,\n-})\n @triton.autotune(\n     configs=[\n         # basic configs for compute-bound matmuls\n@@ -59,6 +56,9 @@ def get_configs_io_bound():\n         'top_k': 10\n     },\n )\n+@triton.heuristics({\n+    'EVEN_K': lambda args: args['K'] % (args['BLOCK_K'] * args['SPLIT_K']) == 0,\n+})\n @triton.jit\n def _kernel(A, B, C, M, N, K,\n             stride_am, stride_ak,"}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 16, "deletions": 11, "changes": 27, "file_content_changes": "@@ -8,7 +8,7 @@\n \n \n class Autotuner:\n-    def __init__(self, kernel, arg_names, configs, key, reset_to_zero, prune_configs_by: Dict = None):\n+    def __init__(self, run, arg_names, configs, key, reset_to_zero, prune_configs_by: Dict = None):\n         '''\n         :param prune_configs_by: a dict of functions that are used to prune configs, fields:\n             'perf_model': performance model used to predicate running time with different configs, returns running time\n@@ -21,7 +21,7 @@ def __init__(self, kernel, arg_names, configs, key, reset_to_zero, prune_configs\n             self.configs = configs\n         self.key_idx = [arg_names.index(k) for k in key]\n         self.cache = dict()\n-        self.kernel = kernel\n+        self.run = run\n         # hook to reset all required tensor to zeros before relaunching a kernel\n         self.hook = lambda args: 0\n         if reset_to_zero is not None:\n@@ -58,7 +58,7 @@ def kernel_call():\n             if config.pre_hook:\n                 config.pre_hook(self.nargs)\n             self.hook(args)\n-            self.kernel(*args, num_warps=config.num_warps, num_stages=config.num_stages, **current)\n+            self.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **current)\n         return do_bench(kernel_call)\n \n     def __call__(self, *args, **kwargs):\n@@ -91,7 +91,7 @@ def __call__(self, *args, **kwargs):\n         self.best_config = config\n         if config.pre_hook is not None:\n             config.pre_hook(self.nargs)\n-        return self.kernel(*args, num_warps=config.num_warps, num_stages=config.num_stages, **kwargs, **config.kwargs)\n+        return self.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **kwargs, **config.kwargs)\n \n \n class Config:\n@@ -165,6 +165,17 @@ def decorator(fn):\n         return fn\n     return decorator\n \n+class Heuristics:\n+\n+    def __init__(self, kernel, arg_names, values) -> None:\n+        self.run = kernel\n+        self.values = values\n+        self.arg_names = arg_names\n+\n+    def __call__(self, *args, **kwargs):\n+        for v, heur in self.values.items():\n+            kwargs[v] = heur({**dict(zip(self.arg_names, args)), **kwargs})\n+        return self.run(*args, **kwargs)\n \n def heuristics(values):\n     \"\"\"\n@@ -185,13 +196,7 @@ def kernel(x_ptr, x_size, **META):\n     .type values: dict[str, Callable[[list[Any]], Any]]\n     \"\"\"\n     def decorator(fn):\n-        old_run = fn.run\n-\n-        def new_run(*args, **meta):\n-            for v, heur in values.items():\n-                meta[v] = heur({**dict(zip(fn.arg_names, args)), **meta})\n-            return old_run(*args, **meta)\n-        fn.run = new_run\n+        fn.run = Heuristics(fn.run, fn.arg_names, values)\n         return fn\n \n     return decorator"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -169,7 +169,7 @@ def _key_of(arg):\n             else:\n                 return \"i64\"\n         elif isinstance(arg, float):\n-            return 'f32'\n+            return 'fp32'\n         elif arg is None:\n             return None\n         else:"}]