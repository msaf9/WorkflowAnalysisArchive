[{"filename": "bin/triton-translate.cpp", "status": "modified", "additions": 0, "deletions": 10, "changes": 10, "file_content_changes": "@@ -64,16 +64,6 @@ OwningOpRef<ModuleOp> loadMLIRModule(llvm::StringRef inputFilename,\n     return nullptr;\n   }\n \n-  mlir::PassManager pm(module->getContext());\n-  applyPassManagerCLOptions(pm);\n-\n-  pm.addPass(createConvertTritonGPUToLLVMPass());\n-\n-  if (failed(pm.run(module->getOperation()))) {\n-    llvm::errs() << \"Pass execution failed\";\n-    return nullptr;\n-  }\n-\n   return module;\n }\n "}, {"filename": "include/triton/Analysis/Allocation.h", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "file_content_changes": "@@ -14,7 +14,12 @@ namespace mlir {\n \n namespace triton {\n class AllocationAnalysis;\n-}\n+\n+SmallVector<unsigned>\n+getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n+                             unsigned &outVec);\n+\n+} // namespace triton\n \n /// Modified from llvm-15.0: llvm/ADT/AddressRanges.h\n /// A class that represents an interval, specified using a start and an end"}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -2,7 +2,10 @@\n #define TRITON_ANALYSIS_UTILITY_H\n \n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include <algorithm>\n+#include <numeric>\n #include <string>\n+\n namespace mlir {\n \n bool isSharedEncoding(Value value);\n@@ -11,6 +14,12 @@ bool maybeSharedAllocationOp(Operation *op);\n \n std::string getValueOperandName(Value value, AsmState &state);\n \n+template <typename Int> Int product(llvm::ArrayRef<Int> arr) {\n+  return std::accumulate(arr.begin(), arr.end(), 1, std::multiplies{});\n+}\n+\n+template <typename Int> Int ceil(Int m, Int n) { return (m + n - 1) / n; }\n+\n } // namespace mlir\n \n #endif // TRITON_ANALYSIS_UTILITY_H"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -18,6 +18,14 @@ class TritonLLVMConversionTarget : public ConversionTarget {\n                                       mlir::LLVMTypeConverter &typeConverter);\n };\n \n+class TritonLLVMFunctionConversionTarget : public ConversionTarget {\n+  mlir::LLVMTypeConverter &typeConverter;\n+\n+public:\n+  explicit TritonLLVMFunctionConversionTarget(\n+      MLIRContext &ctx, mlir::LLVMTypeConverter &typeConverter);\n+};\n+\n namespace triton {\n \n // Names for identifying different NVVM annotations. It is used as attribute"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -16,4 +16,16 @@\n #define GET_OP_CLASSES\n #include \"triton/Dialect/TritonGPU/IR/Ops.h.inc\"\n \n+namespace mlir {\n+namespace triton {\n+namespace gpu {\n+\n+unsigned getElemsPerThread(Attribute layout, ArrayRef<int64_t> shape);\n+\n+unsigned getShapePerCTA(const Attribute &layout, unsigned d);\n+\n+} // namespace gpu\n+} // namespace triton\n+} // namespace mlir\n+\n #endif // TRITON_DIALECT_TRITONGPU_IR_DIALECT_H_"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 13, "deletions": 2, "changes": 15, "file_content_changes": "@@ -31,6 +31,10 @@ Then, attaching $\\mathcal{L} to a tensor $T$ would mean that:\n \n Right now, Triton implements two classes of layouts: shared, and distributed.\n   }];\n+\n+  code extraBaseClassDeclaration = [{\n+    unsigned getElemsPerThread(ArrayRef<int64_t> shape) const;\n+  }];\n }\n \n //===----------------------------------------------------------------------===//\n@@ -64,6 +68,8 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n     \"unsigned\":$vec, \"unsigned\":$perPhase, \"unsigned\":$maxPhase,\n     ArrayRefParameter<\"unsigned\", \"order of axes by the rate of changing\">:$order\n   );\n+\n+  let extraClassDeclaration = extraBaseClassDeclaration;\n }\n \n //===----------------------------------------------------------------------===//\n@@ -93,6 +99,8 @@ Then the data of A would be distributed as follow between the 16 CUDA threads:\n L(A) = [ {0,8} , {1,9} , {2,10}, {3,11}, {0,8} , {1, 9} , {2, 10}, {3, 11},\n          {4,12}, {5,13}, {6,14}, {7,15}, {4,12}, {5, 13}, {6, 14}, {7, 15} ]\n   }];\n+\n+  let extraClassDeclaration = extraBaseClassDeclaration;\n }\n \n //===----------------------------------------------------------------------===//\n@@ -171,11 +179,10 @@ for\n     }]>\n   ];\n \n-  let extraClassDeclaration = [{\n+  let extraClassDeclaration = extraBaseClassDeclaration # [{\n     SliceEncodingAttr squeeze(int axis);\n   }];\n \n-\n   let parameters = (\n     ins\n     ArrayRefParameter<\"unsigned\">:$sizePerThread,\n@@ -282,6 +289,8 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n     \"unsigned\":$version,\n     ArrayRefParameter<\"unsigned\">:$warpsPerCTA\n   );\n+\n+  let extraClassDeclaration = extraBaseClassDeclaration;\n }\n \n def SliceEncodingAttr : DistributedEncoding<\"SliceEncoding\"> {\n@@ -311,6 +320,8 @@ def SliceEncodingAttr : DistributedEncoding<\"SliceEncoding\"> {\n     // TODO: constraint here to only take distributed encodings\n     \"Attribute\":$parent\n   );\n+\n+  let extraClassDeclaration = extraBaseClassDeclaration;\n }\n \n "}, {"filename": "include/triton/tools/sys/getenv.hpp", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -22,6 +22,7 @@\n #ifndef TDL_TOOLS_SYS_GETENV_HPP\n #define TDL_TOOLS_SYS_GETENV_HPP\n \n+#include <algorithm>\n #include <cstdlib>\n #include <string>\n \n@@ -37,6 +38,14 @@ inline std::string getenv(const char *name) {\n   return result;\n }\n \n+inline bool getBoolEnv(const std::string &env) {\n+  const char *s = std::getenv(env.c_str());\n+  std::string str(s ? s : \"\");\n+  std::transform(str.begin(), str.end(), str.begin(),\n+                 [](unsigned char c) { return std::tolower(c); });\n+  return (str == \"on\" || str == \"true\" || str == \"1\");\n+}\n+\n } // namespace tools\n \n } // namespace triton"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 74, "deletions": 0, "changes": 74, "file_content_changes": "@@ -8,13 +8,66 @@\n \n #include <algorithm>\n #include <limits>\n+#include <numeric>\n+\n+using ::mlir::triton::gpu::BlockedEncodingAttr;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n \n namespace mlir {\n \n //===----------------------------------------------------------------------===//\n // Shared Memory Allocation Analysis\n //===----------------------------------------------------------------------===//\n namespace triton {\n+\n+SmallVector<unsigned>\n+getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n+                             unsigned &outVec) {\n+  auto srcTy = op.src().getType().cast<RankedTensorType>();\n+  auto dstTy = op.result().getType().cast<RankedTensorType>();\n+  Attribute srcLayout = srcTy.getEncoding();\n+  Attribute dstLayout = dstTy.getEncoding();\n+  assert(srcLayout && dstLayout &&\n+         \"Unexpect layout in getScratchConfigForCvtLayout()\");\n+  unsigned rank = dstTy.getRank();\n+  SmallVector<unsigned> paddedRepShape(rank);\n+  // TODO: move to TritonGPUAttrDefs.h.inc\n+  auto getShapePerCTA = [&](const Attribute &layout, unsigned d) -> unsigned {\n+    if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+      return blockedLayout.getSizePerThread()[d] *\n+             blockedLayout.getThreadsPerWarp()[d] *\n+             blockedLayout.getWarpsPerCTA()[d];\n+    } else {\n+      assert(0 && \"Unimplemented usage of getShapePerCTA\");\n+      return 0;\n+    }\n+  };\n+  if (srcLayout.isa<BlockedEncodingAttr>() &&\n+      dstLayout.isa<BlockedEncodingAttr>()) {\n+    auto srcBlockedLayout = srcLayout.cast<BlockedEncodingAttr>();\n+    auto dstBlockedLayout = dstLayout.cast<BlockedEncodingAttr>();\n+    auto inOrd = srcBlockedLayout.getOrder();\n+    auto outOrd = dstBlockedLayout.getOrder();\n+    // TODO: Fix the legacy issue that ourOrd[0] == 0 always means\n+    //       that we cannot do vectorization.\n+    inVec = outOrd[0] == 0  ? 1\n+            : inOrd[0] == 0 ? 1\n+                            : srcBlockedLayout.getSizePerThread()[inOrd[0]];\n+    outVec =\n+        outOrd[0] == 0 ? 1 : dstBlockedLayout.getSizePerThread()[outOrd[0]];\n+    unsigned pad = std::max(inVec, outVec);\n+    for (unsigned d = 0; d < rank; ++d) {\n+      paddedRepShape[d] = std::max(\n+          std::min<unsigned>(srcTy.getShape()[d], getShapePerCTA(srcLayout, d)),\n+          std::min<unsigned>(dstTy.getShape()[d],\n+                             getShapePerCTA(dstLayout, d)));\n+    }\n+    paddedRepShape[outOrd[0]] += pad;\n+  }\n+  return paddedRepShape;\n+}\n+\n class AllocationAnalysis {\n public:\n   AllocationAnalysis(Operation *operation, Allocation *allocation)\n@@ -73,6 +126,27 @@ class AllocationAnalysis {\n                      tensorType.getElementTypeBitWidth() / 8;\n         allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n       }\n+    } else if (auto cvtLayout = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n+      auto srcTy = cvtLayout.src().getType().cast<RankedTensorType>();\n+      auto dstTy = cvtLayout.result().getType().cast<RankedTensorType>();\n+      auto srcEncoding = srcTy.getEncoding();\n+      auto dstEncoding = dstTy.getEncoding();\n+      if (srcEncoding.isa<SharedEncodingAttr>() ||\n+          dstEncoding.isa<SharedEncodingAttr>()) {\n+        // Only blocked -> blocked conversion requires for scratch allocation\n+        return;\n+      }\n+      // ConvertLayoutOp with both input/output non-shared_layout\n+      // TODO: Besides of implementing ConvertLayoutOp via shared memory, it's\n+      //       also possible to realize it with other approaches in restricted\n+      //       conditions, such as warp-shuffle\n+      unsigned inVec = 0;\n+      unsigned outVec = 0;\n+      auto smemShape = getScratchConfigForCvtLayout(cvtLayout, inVec, outVec);\n+      unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n+                                       std::multiplies{});\n+      auto bytes = elems * srcTy.getElementTypeBitWidth() / 8;\n+      allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     }\n   }\n "}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "file_content_changes": "@@ -1,4 +1,5 @@\n #include \"mlir/Analysis/DataFlowAnalysis.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"llvm/Support/raw_ostream.h\"\n #include <iostream>\n \n@@ -46,6 +47,11 @@ AxisInfo AxisInfo::getPessimisticValueState(Value value) {\n           fun.getArgAttr(blockArg.getArgNumber(), \"tt.divisibility\");\n       if (attr)\n         divHint = attr.cast<IntegerAttr>().getValue().getZExtValue();\n+    } else if (auto fun = dyn_cast<LLVM::LLVMFuncOp>(op)) {\n+      Attribute attr =\n+          fun.getArgAttr(blockArg.getArgNumber(), \"tt.divisibility\");\n+      if (attr)\n+        divHint = attr.cast<IntegerAttr>().getValue().getZExtValue();\n     }\n   }\n   DimVectorT contiguity(rank, 1);\n@@ -203,6 +209,13 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n     }\n     curr = AxisInfo(contiguity, divisibility, constancy);\n   }\n+  // UnrealizedConversionCast\n+  // This is needed by TritonGPUToLLVM, to get AxisInfo when the graph is\n+  // in the process of a PartialConversion, where UnrealizedConversionCast\n+  // may exist\n+  if (llvm::isa<mlir::UnrealizedConversionCastOp>(op)) {\n+    curr = operands[0]->getValue();\n+  }\n   if (curr.getRank() == 0) {\n     return markAllPessimisticFixpoint(op->getResults());\n   }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 494, "deletions": 114, "changes": 608, "file_content_changes": "@@ -11,7 +11,9 @@\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/IR/Matchers.h\"\n #include \"mlir/Transforms/DialectConversion.h\"\n+#include \"triton/Analysis/Allocation.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n+#include \"triton/Analysis/Utility.h\"\n #include \"triton/Conversion/MLIRTypes.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h\"\n #include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPU.h\"\n@@ -26,8 +28,11 @@\n using namespace mlir;\n using namespace mlir::triton;\n using ::mlir::triton::gpu::BlockedEncodingAttr;\n+using ::mlir::triton::gpu::getElemsPerThread;\n+using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n+using ::mlir::triton::gpu::SliceEncodingAttr;\n \n namespace mlir {\n namespace LLVM {\n@@ -43,11 +48,6 @@ namespace type = mlir::triton::type;\n \n class TritonGPUToLLVMTypeConverter;\n \n-// TODO(Superjomn) Move to somewhere general utilities locates.\n-template <typename Int> size_t product(llvm::ArrayRef<Int> arr) {\n-  return std::accumulate(arr.begin(), arr.end(), 1, std::multiplies{});\n-}\n-\n // FuncOpConversion/FuncOpConversionBase is borrowed from\n // https://github.com/llvm/llvm-project/blob/fae656b2dd80246c3c6f01e9c77c49560368752c/mlir/lib/Conversion/FuncToLLVM/FuncToLLVM.cpp#L276\n // since it is not exposed on header files in mlir v14\n@@ -214,36 +214,6 @@ struct ReturnOpConversion : public ConvertOpToLLVMPattern<::mlir::ReturnOp> {\n   }\n };\n \n-static int64_t getLinearIndex(std::vector<int64_t> multidim_index,\n-                              ArrayRef<int64_t> shape) {\n-  assert(multidim_index.size() == shape.size());\n-  // sizes {a, b, c, d}  ->  acc_mul {b*c*d, c*d, d, 1}\n-  int64_t rank = shape.size();\n-  int64_t acc_mul = 1;\n-  for (int64_t i = 1; i < rank; ++i) {\n-    acc_mul *= shape[i];\n-  }\n-  int64_t linear_index = 0;\n-  for (int64_t i = 0; i < rank; ++i) {\n-    linear_index += multidim_index[i] * acc_mul;\n-    if (i != (rank - 1)) {\n-      acc_mul = acc_mul / shape[i + 1];\n-    }\n-  }\n-  return linear_index;\n-}\n-\n-static unsigned getElemsPerThread(BlockedEncodingAttr layout,\n-                                  ArrayRef<int64_t> shape) {\n-  size_t rank = shape.size();\n-  SmallVector<unsigned> elemsPerThreadPerDim(rank);\n-  for (size_t i = 0; i < rank; ++i) {\n-    unsigned t = layout.getThreadsPerWarp()[i] * layout.getWarpsPerCTA()[i];\n-    elemsPerThreadPerDim[i] = (shape[i] + t - 1) / t;\n-  }\n-  return product<unsigned>(elemsPerThreadPerDim);\n-}\n-\n static Value createIndexAttrConstant(OpBuilder &builder, Location loc,\n                                      Type resultType, int64_t value) {\n   return builder.create<LLVM::ConstantOp>(\n@@ -309,9 +279,9 @@ static T getLinearIndex(ArrayRef<T> multidim_index, ArrayRef<T> shape) {\n }\n \n struct ConvertTritonGPUOpToLLVMPatternBase {\n-  SmallVector<Value>\n+  static SmallVector<Value>\n   getElementsFromStruct(Location loc, Value llvmStruct, unsigned elems,\n-                        ConversionPatternRewriter &rewriter) const {\n+                        ConversionPatternRewriter &rewriter) {\n     SmallVector<Value> results(elems);\n     for (unsigned i = 0; i < elems; ++i) {\n       Type type =\n@@ -344,7 +314,12 @@ class ConvertTritonGPUOpToLLVMPattern\n     for (unsigned i = 0; i < rank; ++i) {\n       reordered[i] = shape[order[i]];\n     }\n-    return delinearize(rewriter, loc, linear, reordered);\n+    auto reorderedMultiDim = delinearize(rewriter, loc, linear, reordered);\n+    SmallVector<Value> multiDim(rank);\n+    for (unsigned i = 0; i < rank; ++i) {\n+      multiDim[order[i]] = reorderedMultiDim[i];\n+    }\n+    return multiDim;\n   }\n \n   SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n@@ -370,13 +345,29 @@ class ConvertTritonGPUOpToLLVMPattern\n     return multiDim;\n   }\n \n-  // Emit indices calculation within each ConversionPattern\n-  // TODO: [goostavz] Double confirm the redundant indices calculations will\n-  //       be eliminated in the consequent MLIR/LLVM optimization\n-  SmallVector<SmallVector<Value>>\n-  emitIndicesForBlockedLayout(Location loc, ConversionPatternRewriter &b,\n-                              const BlockedEncodingAttr &blocked_layout,\n-                              ArrayRef<int64_t> shape) const {\n+  Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n+                  ArrayRef<Value> multiDim, ArrayRef<unsigned> shape) const {\n+    int rank = multiDim.size();\n+    Value linear = createIndexAttrConstant(\n+        rewriter, loc, this->getTypeConverter()->getIndexType(), 0);\n+    if (rank > 0) {\n+      linear = multiDim.front();\n+      for (auto &&z : llvm::zip(multiDim.drop_front(), shape.drop_front())) {\n+        Value dimSize = createIndexAttrConstant(\n+            rewriter, loc, this->getTypeConverter()->getIndexType(),\n+            std::get<1>(z));\n+        linear = rewriter.create<LLVM::AddOp>(\n+            loc, rewriter.create<LLVM::MulOp>(loc, linear, dimSize),\n+            std::get<0>(z));\n+      }\n+    }\n+    return linear;\n+  }\n+\n+  SmallVector<Value>\n+  emitBaseIndexForBlockedLayout(Location loc, ConversionPatternRewriter &b,\n+                                const BlockedEncodingAttr &blocked_layout,\n+                                ArrayRef<int64_t> shape) const {\n     auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n     auto cast = b.create<UnrealizedConversionCastOp>(\n         loc, TypeRange{llvmIndexTy},\n@@ -391,7 +382,6 @@ class ConvertTritonGPUOpToLLVMPattern\n     auto warpsPerCTA = blocked_layout.getWarpsPerCTA();\n     auto order = blocked_layout.getOrder();\n     unsigned rank = shape.size();\n-    SmallVector<Value, 4> threadIds(rank);\n \n     // step 1, delinearize threadId to get the base index\n     SmallVector<Value> multiDimWarpId =\n@@ -400,8 +390,19 @@ class ConvertTritonGPUOpToLLVMPattern\n         delinearize(b, loc, laneId, threadsPerWarp, order);\n     SmallVector<Value> multiDimBase(rank);\n     for (unsigned k = 0; k < rank; ++k) {\n-      // multiDimBase[k] = (multiDimThreadId[k] + multiDimWarpId[k] *\n-      // threadsPerWarp[k]) *\n+      // Wrap around multiDimWarpId/multiDimThreadId incase\n+      // shape[k] > shapePerCTA[k]\n+      unsigned maxWarps =\n+          ceil<unsigned>(shape[k], sizePerThread[k] * threadsPerWarp[k]);\n+      unsigned maxThreads = ceil<unsigned>(shape[k], sizePerThread[k]);\n+      multiDimWarpId[k] = b.create<LLVM::URemOp>(\n+          loc, multiDimWarpId[k],\n+          createIndexAttrConstant(b, loc, llvmIndexTy, maxWarps));\n+      multiDimThreadId[k] = b.create<LLVM::URemOp>(\n+          loc, multiDimThreadId[k],\n+          createIndexAttrConstant(b, loc, llvmIndexTy, maxThreads));\n+      // multiDimBase[k] = (multiDimThreadId[k] +\n+      //                    multiDimWarpId[k] * threadsPerWarp[k]) *\n       //                   sizePerThread[k];\n       Value threadsPerWarpK =\n           createIndexAttrConstant(b, loc, llvmIndexTy, threadsPerWarp[k]);\n@@ -413,17 +414,100 @@ class ConvertTritonGPUOpToLLVMPattern\n               loc, multiDimThreadId[k],\n               b.create<LLVM::MulOp>(loc, multiDimWarpId[k], threadsPerWarpK)));\n     }\n+    return multiDimBase;\n+  }\n+\n+  SmallVector<SmallVector<Value>> emitIndices(Location loc,\n+                                              ConversionPatternRewriter &b,\n+                                              const Attribute &layout,\n+                                              ArrayRef<int64_t> shape) const {\n+    if (auto blocked = layout.dyn_cast<BlockedEncodingAttr>()) {\n+      return emitIndicesForBlockedLayout(loc, b, blocked, shape);\n+    } else if (auto slice = layout.dyn_cast<SliceEncodingAttr>()) {\n+      return emitIndicesForSliceLayout(loc, b, slice, shape);\n+    } else {\n+      assert(0 && \"emitIndices for layouts other than blocked & slice not \"\n+                  \"implemented yet\");\n+      return {};\n+    }\n+  }\n+\n+  SmallVector<SmallVector<Value>>\n+  emitIndicesForSliceLayout(Location loc, ConversionPatternRewriter &b,\n+                            const SliceEncodingAttr &sliceLayout,\n+                            ArrayRef<int64_t> shape) const {\n+    auto parent = sliceLayout.getParent();\n+    unsigned dim = sliceLayout.getDim();\n+    size_t rank = shape.size();\n+    if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n+      SmallVector<int64_t> paddedShape(rank + 1);\n+      for (unsigned d = 0; d < rank + 1; ++d) {\n+        if (d < dim) {\n+          paddedShape[d] = shape[d];\n+        } else if (d == dim) {\n+          paddedShape[d] = 1;\n+        } else {\n+          paddedShape[d] = shape[d - 1];\n+        }\n+      }\n+      auto paddedIndices =\n+          emitIndicesForBlockedLayout(loc, b, blockedParent, paddedShape);\n+      unsigned numIndices = paddedIndices.size();\n+      SmallVector<SmallVector<Value>> resultIndices(numIndices);\n+      for (unsigned i = 0; i < numIndices; ++i) {\n+        for (unsigned d = 0; d < rank + 1; ++d) {\n+          if (d != dim) {\n+            resultIndices[i].push_back(paddedIndices[i][d]);\n+          }\n+        }\n+      }\n+      return resultIndices;\n+\n+    } else if (auto sliceParent = parent.dyn_cast<SliceEncodingAttr>()) {\n+      assert(0 && \"emitIndicesForSliceLayout with parent of sliceLayout\"\n+                  \"is not implemented yet\");\n+      return {};\n+\n+    } else {\n+      assert(0 && \"emitIndicesForSliceLayout with parent other than blocked & \"\n+                  \"slice not implemented yet\");\n+      return {};\n+    }\n+  }\n+\n+  // Emit indices calculation within each ConversionPattern\n+  // TODO: [goostavz] Double confirm the redundant indices calculations will\n+  //       be eliminated in the consequent MLIR/LLVM optimization. We might\n+  //       implement a indiceCache if necessary.\n+  SmallVector<SmallVector<Value>>\n+  emitIndicesForBlockedLayout(Location loc, ConversionPatternRewriter &b,\n+                              const BlockedEncodingAttr &blockedLayout,\n+                              ArrayRef<int64_t> shape) const {\n+    auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n+    auto sizePerThread = blockedLayout.getSizePerThread();\n+    auto threadsPerWarp = blockedLayout.getThreadsPerWarp();\n+    auto warpsPerCTA = blockedLayout.getWarpsPerCTA();\n+    unsigned rank = shape.size();\n+    SmallVector<unsigned> shapePerCTA(rank);\n+    for (unsigned k = 0; k < rank; ++k) {\n+      shapePerCTA[k] = sizePerThread[k] * threadsPerWarp[k] * warpsPerCTA[k];\n+    }\n+\n+    // step 1, delinearize threadId to get the base index\n+    auto multiDimBase =\n+        emitBaseIndexForBlockedLayout(loc, b, blockedLayout, shape);\n \n     // step 2, get offset of each element\n     unsigned elemsPerThread = 1;\n     SmallVector<SmallVector<unsigned>> offset(rank);\n     SmallVector<unsigned> multiDimElemsPerThread(rank);\n     for (unsigned k = 0; k < rank; ++k) {\n-      multiDimElemsPerThread[k] = shape[k] / threadsPerWarp[k] / warpsPerCTA[k];\n+      multiDimElemsPerThread[k] =\n+          ceil<unsigned>(shape[k], shapePerCTA[k]) * sizePerThread[k];\n       elemsPerThread *= multiDimElemsPerThread[k];\n+      // 1 block in minimum if shape[k] is less than shapePerCTA[k]\n       for (unsigned blockOffset = 0;\n-           blockOffset <\n-           shape[k] / (sizePerThread[k] * threadsPerWarp[k] * warpsPerCTA[k]);\n+           blockOffset < ceil<unsigned>(shape[k], shapePerCTA[k]);\n            ++blockOffset)\n         for (unsigned warpOffset = 0; warpOffset < warpsPerCTA[k]; ++warpOffset)\n           for (unsigned threadOffset = 0; threadOffset < threadsPerWarp[k];\n@@ -445,7 +529,7 @@ class ConvertTritonGPUOpToLLVMPattern\n                         std::multiplies<unsigned>());\n     SmallVector<unsigned> threadsPerDim(rank);\n     for (unsigned k = 0; k < rank; ++k) {\n-      threadsPerDim[k] = shape[k] / sizePerThread[k];\n+      threadsPerDim[k] = ceil<unsigned>(shape[k], sizePerThread[k]);\n     }\n     for (unsigned n = 0; n < elemsPerThread; ++n) {\n       unsigned linearNanoTileId = n / accumSizePerThread;\n@@ -469,6 +553,20 @@ class ConvertTritonGPUOpToLLVMPattern\n \n     return multiDimIdx;\n   }\n+\n+  Value getSharedMemoryBase(Location loc, ConversionPatternRewriter &rewriter,\n+                            Value smem, const Allocation *allocation,\n+                            Operation *op) const {\n+    auto ptrTy = LLVM::LLVMPointerType::get(\n+        this->getTypeConverter()->convertType(rewriter.getIntegerType(8)), 3);\n+    auto bufferId = allocation->getBufferId(op);\n+    assert(bufferId != Allocation::InvalidBufferId && \"BufferId not found\");\n+    size_t offset = allocation->getOffset(bufferId);\n+    auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n+    Value offVal = createIndexAttrConstant(rewriter, loc, llvmIndexTy, offset);\n+    Value base = rewriter.create<LLVM::GEPOp>(loc, ptrTy, smem, offVal);\n+    return base;\n+  }\n };\n \n // Convert SplatOp or arith::ConstantOp with SplatElementsAttr to a\n@@ -482,19 +580,10 @@ Value convertSplatLikeOp(Type elemType, Type resType, Value constVal,\n                          ConversionPatternRewriter &rewriter, Location loc) {\n \n   auto tensorTy = resType.cast<RankedTensorType>();\n-  auto layout = tensorTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto layout = tensorTy.getEncoding();\n   auto srcType = typeConverter->convertType(elemType);\n   auto llSrc = rewriter.create<LLVM::BitcastOp>(loc, srcType, constVal);\n-\n-  auto numElems = layout.getSizePerThread();\n-  size_t totalElems =\n-      std::accumulate(tensorTy.getShape().begin(), tensorTy.getShape().end(), 1,\n-                      std::multiplies<>{});\n-  size_t numThreads =\n-      product(layout.getWarpsPerCTA()) * product(layout.getThreadsPerWarp());\n-  // TODO(Superjomn) add numElemsPerThread to the layout encodings.\n-  size_t numElemsPerThread = totalElems / numThreads;\n-\n+  size_t numElemsPerThread = getElemsPerThread(layout, tensorTy.getShape());\n   llvm::SmallVector<Value, 4> elems(numElemsPerThread, llSrc);\n   llvm::SmallVector<Type, 4> elemTypes(elems.size(), srcType);\n   auto structTy =\n@@ -580,7 +669,7 @@ struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n     auto shape = ty.getShape();\n     // Here, we assume that all inputs should have a blockedLayout\n \n-    unsigned valueElems = getElemsPerThread(layout, shape);\n+    unsigned valueElems = layout.getElemsPerThread(shape);\n \n     auto llvmElemTy = typeConverter->convertType(ty.getElementType());\n     auto llvmElemPtrPtrTy =\n@@ -595,16 +684,15 @@ struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n     auto ty = val.getType().cast<RankedTensorType>();\n     // Here, we assume that all inputs should have a blockedLayout\n     auto layout = ty.getEncoding().dyn_cast<BlockedEncodingAttr>();\n+    assert(layout && \"unexpected layout in getLayout\");\n     auto shape = ty.getShape();\n-    unsigned valueElems = getElemsPerThread(layout, shape);\n+    unsigned valueElems = layout.getElemsPerThread(shape);\n     return std::make_tuple(layout, valueElems);\n   }\n \n   unsigned getAlignment(Value val, const BlockedEncodingAttr &layout) const {\n     auto axisInfo = getAxisInfo(val);\n-\n     auto order = layout.getOrder();\n-\n     unsigned maxMultiple = axisInfo->getDivisibility(order[0]);\n     unsigned maxContig = axisInfo->getContiguity(order[0]);\n     unsigned alignment = std::min(maxMultiple, maxContig);\n@@ -614,22 +702,18 @@ struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n   unsigned getVectorizeSize(Value ptr,\n                             const BlockedEncodingAttr &layout) const {\n     auto axisInfo = getAxisInfo(ptr);\n-    auto contig = axisInfo->getContiguity();\n     // Here order should be ordered by contiguous first, so the first element\n     // should have the largest contiguous.\n     auto order = layout.getOrder();\n     unsigned align = getAlignment(ptr, layout);\n \n-    auto getTensorShape = [](Value val) -> ArrayRef<int64_t> {\n-      auto ty = val.getType().cast<RankedTensorType>();\n-      auto shape = ty.getShape();\n-      return shape;\n-    };\n-\n-    // unsigned contigPerThread = layout.getSizePerThread()[order[0]];\n-    unsigned contigPerThread = getElemsPerThread(layout, getTensorShape(ptr));\n+    auto ty = ptr.getType().dyn_cast<RankedTensorType>();\n+    assert(ty);\n+    auto shape = ty.getShape();\n \n+    unsigned contigPerThread = layout.getSizePerThread()[order[0]];\n     unsigned vec = std::min(align, contigPerThread);\n+    vec = std::min<unsigned>(shape[order[0]], vec);\n \n     return vec;\n   }\n@@ -819,44 +903,60 @@ struct BroadcastOpConversion\n     auto srcShape = srcTy.getShape();\n     auto resultShape = resultTy.getShape();\n     unsigned rank = srcTy.getRank();\n-    // TODO: [goostavz] double confirm the op semantics with Phil\n     assert(rank == resultTy.getRank());\n \n     SmallVector<int64_t, 4> srcLogicalShape(2 * rank);\n     SmallVector<int64_t, 4> resultLogicalShape(2 * rank);\n     SmallVector<unsigned, 2> broadcastDims;\n-    SmallVector<int64_t, 2> broadcastSizes;\n-    int64_t duplicates = 1;\n     for (unsigned d = 0; d < rank; ++d) {\n-      int64_t numCtas = resultShape[d] / (resultLayout.getSizePerThread()[d] *\n-                                          resultLayout.getThreadsPerWarp()[d] *\n-                                          resultLayout.getWarpsPerCTA()[d]);\n+      unsigned resultShapePerCTA = resultLayout.getSizePerThread()[d] *\n+                                   resultLayout.getThreadsPerWarp()[d] *\n+                                   resultLayout.getWarpsPerCTA()[d];\n+      int64_t numCtas = ceil<unsigned>(resultShape[d], resultShapePerCTA);\n       if (srcShape[d] != resultShape[d]) {\n         assert(srcShape[d] == 1);\n         broadcastDims.push_back(d);\n-        broadcastSizes.push_back(resultShape[d]);\n         srcLogicalShape[d] = 1;\n-        srcLogicalShape[d + rank] = 1;\n-        duplicates *= resultShape[d];\n+        srcLogicalShape[d + rank] =\n+            std::max(unsigned(1), srcLayout.getSizePerThread()[d]);\n       } else {\n         srcLogicalShape[d] = numCtas;\n         srcLogicalShape[d + rank] = resultLayout.getSizePerThread()[d];\n       }\n       resultLogicalShape[d] = numCtas;\n       resultLogicalShape[d + rank] = resultLayout.getSizePerThread()[d];\n     }\n-    unsigned srcElems = getElemsPerThread(srcLayout, srcShape);\n+    int64_t duplicates = 1;\n+    SmallVector<int64_t, 2> broadcastSizes(broadcastDims.size() * 2);\n+    for (auto it : llvm::enumerate(broadcastDims)) {\n+      // Incase there are multiple indices in the src that is actually\n+      // calculating the same element, srcLogicalShape may not need to be 1.\n+      // Such as the case when src of shape [256, 1], and with a blocked layout:\n+      // sizePerThread: [1, 4];  threadsPerWarp: [1, 32]; warpsPerCTA: [1, 2]\n+      int64_t d = resultLogicalShape[it.value()] / srcLogicalShape[it.value()];\n+      broadcastSizes[it.index()] = d;\n+      duplicates *= d;\n+      d = resultLogicalShape[it.value() + rank] /\n+          srcLogicalShape[it.value() + rank];\n+      broadcastSizes[it.index() + broadcastDims.size()] = d;\n+      duplicates *= d;\n+    }\n+\n+    unsigned srcElems = srcLayout.getElemsPerThread(srcShape);\n     auto elemTy = resultTy.getElementType();\n     auto srcVals = getElementsFromStruct(loc, src, srcElems, rewriter);\n-    unsigned resultElems = getElemsPerThread(resultLayout, resultShape);\n+    unsigned resultElems = resultLayout.getElemsPerThread(resultShape);\n     SmallVector<Value> resultVals(resultElems);\n     for (unsigned i = 0; i < srcElems; ++i) {\n       auto srcMultiDim = getMultiDimIndex<int64_t>(i, srcLogicalShape);\n-      auto resultMultiDim = srcMultiDim;\n       for (int64_t j = 0; j < duplicates; ++j) {\n+        auto resultMultiDim = srcMultiDim;\n         auto bcastMultiDim = getMultiDimIndex<int64_t>(j, broadcastSizes);\n         for (auto bcastDim : llvm::enumerate(broadcastDims)) {\n-          resultMultiDim[bcastDim.value()] = bcastMultiDim[bcastDim.index()];\n+          resultMultiDim[bcastDim.value()] += bcastMultiDim[bcastDim.index()];\n+          resultMultiDim[bcastDim.value() + rank] +=\n+              bcastMultiDim[bcastDim.index() + broadcastDims.size()] *\n+              srcLogicalShape[bcastDim.index() + broadcastDims.size()];\n         }\n         auto resultLinearIndex =\n             getLinearIndex<int64_t>(resultMultiDim, resultLogicalShape);\n@@ -871,27 +971,29 @@ struct BroadcastOpConversion\n   }\n };\n \n-struct ViewOpConversion\n-    : public ConvertTritonGPUOpToLLVMPattern<triton::ViewOp> {\n-  using ConvertTritonGPUOpToLLVMPattern<\n-      triton::ViewOp>::ConvertTritonGPUOpToLLVMPattern;\n+template <typename SourceOp>\n+struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n+  using OpAdaptor = typename SourceOp::Adaptor;\n+  explicit ViewLikeOpConversion(LLVMTypeConverter &typeConverter,\n+                                PatternBenefit benefit = 1)\n+      : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n \n   LogicalResult\n-  matchAndRewrite(triton::ViewOp op, OpAdaptor adaptor,\n+  matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     // We cannot directly\n     //   rewriter.replaceOp(op, adaptor.src());\n     // due to MLIR's restrictions\n     Location loc = op->getLoc();\n-    auto resultTy = op.getType().cast<RankedTensorType>();\n-    auto resultLayout = resultTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n+    auto resultTy = op.getType().template cast<RankedTensorType>();\n     auto resultShape = resultTy.getShape();\n-    unsigned elems = getElemsPerThread(resultLayout, resultShape);\n+    unsigned elems = getElemsPerThread(resultTy.getEncoding(), resultShape);\n     Type elemTy =\n         this->getTypeConverter()->convertType(resultTy.getElementType());\n     SmallVector<Type> types(elems, elemTy);\n-    Type structTy = LLVM::LLVMStructType::getLiteral(getContext(), types);\n-    auto vals = getElementsFromStruct(loc, adaptor.src(), elems, rewriter);\n+    Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n+    auto vals =\n+        this->getElementsFromStruct(loc, adaptor.src(), elems, rewriter);\n     Value view = getStructFromElements(loc, vals, rewriter, structTy);\n     rewriter.replaceOp(op, view);\n     return success();\n@@ -911,12 +1013,12 @@ struct MakeRangeOpConversion\n     Location loc = op->getLoc();\n     auto rankedTy = op.result().getType().dyn_cast<RankedTensorType>();\n     auto shape = rankedTy.getShape();\n-    auto layout = rankedTy.getEncoding().cast<BlockedEncodingAttr>();\n+    auto layout = rankedTy.getEncoding();\n \n     auto elemTy = rankedTy.getElementType();\n     assert(elemTy.isInteger(32));\n     Value start = createIndexAttrConstant(rewriter, loc, elemTy, op.start());\n-    auto idxs = emitIndicesForBlockedLayout(loc, rewriter, layout, shape);\n+    auto idxs = emitIndices(loc, rewriter, layout, shape);\n     unsigned elems = idxs.size();\n     SmallVector<Value> retVals(elems);\n     for (auto multiDim : llvm::enumerate(idxs)) {\n@@ -1184,8 +1286,9 @@ struct AddPtrOpConversion\n     Location loc = op->getLoc();\n     auto resultTy = op.getType().dyn_cast<RankedTensorType>();\n     auto resultLayout = resultTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n+    assert(resultLayout && \"Unexpected resultLayout in AddPtrOpConversion\");\n     auto resultShape = resultTy.getShape();\n-    unsigned elems = getElemsPerThread(resultLayout, resultShape);\n+    unsigned elems = resultLayout.getElemsPerThread(resultShape);\n     Type elemTy =\n         this->getTypeConverter()->convertType(resultTy.getElementType());\n     SmallVector<Type> types(elems, elemTy);\n@@ -1225,7 +1328,8 @@ class BinaryOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n     auto resultLayout =\n         resultTy.getEncoding().template dyn_cast<BlockedEncodingAttr>();\n     auto resultShape = resultTy.getShape();\n-    unsigned elems = getElemsPerThread(resultLayout, resultShape);\n+    assert(resultLayout && \"Unexpected resultLayout in BinaryOpConversion\");\n+    unsigned elems = resultLayout.getElemsPerThread(resultShape);\n     Type elemTy =\n         this->getTypeConverter()->convertType(resultTy.getElementType());\n     SmallVector<Type> types(elems, elemTy);\n@@ -1244,6 +1348,228 @@ class BinaryOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n   }\n };\n \n+struct ConvertLayoutOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::ConvertLayoutOp> {\n+public:\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::gpu::ConvertLayoutOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  ConvertLayoutOpConversion(LLVMTypeConverter &converter,\n+                            const Allocation *allocation, Value smem,\n+                            PatternBenefit benefit)\n+      : ConvertTritonGPUOpToLLVMPattern<triton::gpu::ConvertLayoutOp>(converter,\n+                                                                      benefit),\n+        allocation_(allocation), smem_(smem) {}\n+\n+  LogicalResult\n+  matchAndRewrite(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto loc = op.getLoc();\n+    Value src = op.src();\n+    Value dst = op.result();\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    Attribute srcLayout = srcTy.getEncoding();\n+    Attribute dstLayout = dstTy.getEncoding();\n+    if ((!srcLayout.isa<BlockedEncodingAttr>()) ||\n+        (!dstLayout.isa<BlockedEncodingAttr>())) {\n+      // TODO: not implemented\n+      assert(0 &&\n+             \"convert_layout except for blocked -> blocked is not implemented\");\n+      return failure();\n+    }\n+    auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n+    Value smemBase = getSharedMemoryBase(loc, rewriter, smem_, allocation_,\n+                                         op.getOperation());\n+    auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n+    smemBase = rewriter.create<LLVM::BitcastOp>(loc, elemPtrTy, smemBase);\n+\n+    auto shape = dstTy.getShape();\n+    unsigned rank = dstTy.getRank();\n+    auto getContigPerThread = [&](const Attribute &layout,\n+                                  unsigned d) -> unsigned {\n+      if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+        return blockedLayout.getSizePerThread()[d];\n+      } else {\n+        assert(0 && \"Unimplemented usage of getContigPerThread\");\n+        return 0;\n+      }\n+    };\n+    auto getAccumElemsPerThread = [&](const Attribute &layout) -> unsigned {\n+      if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+        return product<unsigned>(blockedLayout.getSizePerThread());\n+      } else {\n+        assert(0 && \"Unimplemented usage of getAccumElemsPerThread\");\n+        return 0;\n+      }\n+    };\n+    auto getOrder = [&](const Attribute &layout) -> ArrayRef<unsigned> {\n+      if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+        return blockedLayout.getOrder();\n+      } else {\n+        assert(0 && \"Unimplemented usage of getAccumElemsPerThread\");\n+        return {};\n+      }\n+    };\n+    SmallVector<unsigned> numReplicates(rank);\n+    SmallVector<unsigned> inNumCTAsEachRep(rank);\n+    SmallVector<unsigned> outNumCTAsEachRep(rank);\n+    SmallVector<unsigned> inNumCTAs(rank);\n+    SmallVector<unsigned> outNumCTAs(rank);\n+    for (unsigned d = 0; d < rank; ++d) {\n+      unsigned inPerCTA =\n+          std::min(unsigned(shape[d]), getShapePerCTA(srcLayout, d));\n+      unsigned outPerCTA =\n+          std::min(unsigned(shape[d]), getShapePerCTA(dstLayout, d));\n+      unsigned maxPerCTA = std::max(inPerCTA, outPerCTA);\n+      numReplicates[d] = ceil<unsigned>(shape[d], maxPerCTA);\n+      inNumCTAsEachRep[d] = maxPerCTA / inPerCTA;\n+      outNumCTAsEachRep[d] = maxPerCTA / outPerCTA;\n+      // TODO: confirm this\n+      assert(maxPerCTA % inPerCTA == 0 && maxPerCTA % outPerCTA == 0);\n+      inNumCTAs[d] = ceil<unsigned>(shape[d], inPerCTA);\n+      outNumCTAs[d] = ceil<unsigned>(shape[d], outPerCTA);\n+    }\n+    // Potentially we need to store for multiple CTAs in this replication\n+    unsigned accumNumReplicates = product<unsigned>(numReplicates);\n+    unsigned accumInSizePerThread = getAccumElemsPerThread(srcLayout);\n+    unsigned elems = getElemsPerThread(srcLayout, srcTy.getShape());\n+    auto vals = getElementsFromStruct(loc, adaptor.src(), elems, rewriter);\n+    unsigned inVec = 0;\n+    unsigned outVec = 0;\n+    auto paddedRepShape = getScratchConfigForCvtLayout(op, inVec, outVec);\n+\n+    unsigned outElems = getElemsPerThread(dstLayout, shape);\n+    auto outOrd = getOrder(dstLayout);\n+    SmallVector<Value> outVals(outElems);\n+    for (unsigned repId = 0; repId < accumNumReplicates; ++repId) {\n+      auto multiDimRepId = getMultiDimIndex<unsigned>(repId, numReplicates);\n+      rewriter.create<mlir::gpu::BarrierOp>(loc);\n+      if (auto srcBlockedLayout = srcLayout.dyn_cast<BlockedEncodingAttr>()) {\n+        processReplicaBlocked(loc, rewriter, /*stNotRd*/ true, srcTy,\n+                              inNumCTAsEachRep, multiDimRepId, inVec,\n+                              paddedRepShape, outOrd, vals, smemBase);\n+      } else {\n+        assert(0 && \"ConvertLayout with input layout not implemented\");\n+        return failure();\n+      }\n+      rewriter.create<mlir::gpu::BarrierOp>(loc);\n+      if (auto dstBlockedLayout = dstLayout.dyn_cast<BlockedEncodingAttr>()) {\n+        processReplicaBlocked(loc, rewriter, /*stNotRd*/ false, dstTy,\n+                              outNumCTAsEachRep, multiDimRepId, outVec,\n+                              paddedRepShape, outOrd, outVals, smemBase);\n+      } else {\n+        assert(0 && \"ConvertLayout with output layout not implemented\");\n+        return failure();\n+      }\n+    }\n+\n+    SmallVector<Type> types(outElems, llvmElemTy);\n+    Type structTy = LLVM::LLVMStructType::getLiteral(getContext(), types);\n+    Value result = getStructFromElements(loc, outVals, rewriter, structTy);\n+    rewriter.replaceOp(op, result);\n+    return success();\n+  }\n+\n+private:\n+  template <typename T>\n+  SmallVector<T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) const {\n+    size_t rank = order.size();\n+    assert(input.size() == rank);\n+    SmallVector<T> result(rank);\n+    for (auto it : llvm::enumerate(order)) {\n+      result[rank - 1 - it.value()] = input[it.index()];\n+    }\n+    return result;\n+  };\n+\n+  void processReplicaBlocked(Location loc, ConversionPatternRewriter &rewriter,\n+                             bool stNotRd, RankedTensorType type,\n+                             ArrayRef<unsigned> numCTAsEachRep,\n+                             ArrayRef<unsigned> multiDimRepId, unsigned vec,\n+                             ArrayRef<unsigned> paddedRepShape,\n+                             ArrayRef<unsigned> outOrd,\n+                             SmallVector<Value> &vals, Value smemBase) const {\n+    unsigned accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n+    auto layout = type.getEncoding().cast<BlockedEncodingAttr>();\n+    auto rank = type.getRank();\n+    auto sizePerThread = layout.getSizePerThread();\n+    auto accumSizePerThread = product<unsigned>(sizePerThread);\n+    auto llvmIndexTy = getTypeConverter()->getIndexType();\n+    SmallVector<unsigned> numCTAs(rank);\n+    SmallVector<unsigned> shapePerCTA(rank);\n+    for (unsigned d = 0; d < rank; ++d) {\n+      shapePerCTA[d] = layout.getSizePerThread()[d] *\n+                       layout.getThreadsPerWarp()[d] *\n+                       layout.getWarpsPerCTA()[d];\n+      numCTAs[d] = ceil<unsigned>(type.getShape()[d], shapePerCTA[d]);\n+    }\n+    auto llvmElemTy = getTypeConverter()->convertType(type.getElementType());\n+    auto multiDimOffsetFirstElem =\n+        emitBaseIndexForBlockedLayout(loc, rewriter, layout, type.getShape());\n+    for (unsigned ctaId = 0; ctaId < accumNumCTAsEachRep; ++ctaId) {\n+      auto multiDimCTAInRepId =\n+          getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep);\n+      SmallVector<unsigned> multiDimCTAId(rank);\n+      for (auto it : llvm::enumerate(multiDimCTAInRepId)) {\n+        auto d = it.index();\n+        multiDimCTAId[d] = multiDimRepId[d] * numCTAsEachRep[d] + it.value();\n+      }\n+\n+      unsigned linearCTAId = getLinearIndex<unsigned>(multiDimCTAId, numCTAs);\n+      // TODO: This is actually redundant index calculation, we should\n+      //       consider of caching the index calculation result in case\n+      //       of performance issue observed.\n+      // for (unsigned elemId = linearCTAId * accumSizePerThread;\n+      //      elemId < (linearCTAId + 1) * accumSizePerThread; elemId += vec) {\n+      for (unsigned elemId = 0; elemId < accumSizePerThread; elemId += vec) {\n+        auto multiDimElemId =\n+            getMultiDimIndex<unsigned>(elemId, layout.getSizePerThread());\n+        SmallVector<Value> multiDimOffset(rank);\n+        for (unsigned d = 0; d < rank; ++d) {\n+          multiDimOffset[d] = rewriter.create<LLVM::AddOp>(\n+              loc, multiDimOffsetFirstElem[d],\n+              createIndexAttrConstant(rewriter, loc, llvmIndexTy,\n+                                      multiDimCTAInRepId[d] * shapePerCTA[d] +\n+                                          multiDimElemId[d]));\n+        }\n+        Value offset =\n+            linearize(rewriter, loc, reorder<Value>(multiDimOffset, outOrd),\n+                      reorder<unsigned>(paddedRepShape, outOrd));\n+        auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n+        Value ptr =\n+            rewriter.create<LLVM::GEPOp>(loc, elemPtrTy, smemBase, offset);\n+        auto vecTy = VectorType::get(vec, llvmElemTy);\n+        ptr = rewriter.create<LLVM::BitcastOp>(\n+            loc, LLVM::LLVMPointerType::get(vecTy, 3), ptr);\n+        if (stNotRd) {\n+          Value valVec = rewriter.create<LLVM::UndefOp>(loc, vecTy);\n+          for (unsigned v = 0; v < vec; ++v) {\n+            Value vVal = createIndexAttrConstant(\n+                rewriter, loc, getTypeConverter()->getIndexType(), v);\n+            valVec = rewriter.create<LLVM::InsertElementOp>(\n+                loc, vecTy, valVec,\n+                vals[elemId + linearCTAId * accumSizePerThread + v], vVal);\n+          }\n+          rewriter.create<LLVM::StoreOp>(loc, valVec, ptr);\n+        } else {\n+          Value valVec = rewriter.create<LLVM::LoadOp>(loc, ptr);\n+          for (unsigned v = 0; v < vec; ++v) {\n+            Value vVal = createIndexAttrConstant(\n+                rewriter, loc, getTypeConverter()->getIndexType(), v);\n+            vals[elemId + linearCTAId * accumSizePerThread + v] =\n+                rewriter.create<LLVM::ExtractElementOp>(loc, llvmElemTy, valVec,\n+                                                        vVal);\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  const Allocation *allocation_;\n+  Value smem_;\n+};\n+\n class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n public:\n   using TypeConverter::convertType;\n@@ -1266,9 +1592,10 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n \n   llvm::Optional<Type> convertTritonTensorType(RankedTensorType type) {\n     Attribute layout = type.getEncoding();\n-    if (auto blocked_layout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+    if (layout && (layout.isa<BlockedEncodingAttr>() ||\n+                   layout.isa<SliceEncodingAttr>())) {\n       unsigned numElementsPerThread =\n-          getElemsPerThread(blocked_layout, type.getShape());\n+          getElemsPerThread(layout, type.getShape());\n       SmallVector<Type, 4> types(numElementsPerThread,\n                                  convertType(type.getElementType()));\n       return LLVM::LLVMStructType::getLiteral(&getContext(), types);\n@@ -1285,7 +1612,8 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n \n void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                   RewritePatternSet &patterns, int numWarps,\n-                                  AxisInfoAnalysis &analysis,\n+                                  AxisInfoAnalysis &axisInfoAnalysis,\n+                                  const Allocation *allocation, Value smem,\n                                   PatternBenefit benefit = 1) {\n   patterns.add<ArithConstantSplatOpConversion>(typeConverter, benefit);\n   patterns.add<BinaryOpConversion<arith::AddIOp, LLVM::AddOp>>(typeConverter,\n@@ -1296,17 +1624,19 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                                                benefit);\n   patterns.add<BinaryOpConversion<arith::MulFOp, LLVM::FMulOp>>(typeConverter,\n                                                                 benefit);\n-\n   patterns.add<BroadcastOpConversion>(typeConverter, benefit);\n-  patterns.add<FuncOpConversion>(typeConverter, numWarps, benefit);\n   patterns.add<AddPtrOpConversion>(typeConverter, benefit);\n+  patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n+                                          benefit);\n   patterns.add<GetProgramIdOpConversion>(typeConverter, benefit);\n-  patterns.add<LoadOpConversion>(typeConverter, analysis, benefit);\n+  patterns.add<LoadOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n   patterns.add<MakeRangeOpConversion>(typeConverter, benefit);\n   patterns.add<ReturnOpConversion>(typeConverter, benefit);\n   patterns.add<SplatOpConversion>(typeConverter, benefit);\n-  patterns.add<StoreOpConversion>(typeConverter, analysis, benefit);\n-  patterns.add<ViewOpConversion>(typeConverter, benefit);\n+  patterns.add<StoreOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n+  patterns.add<ViewLikeOpConversion<triton::ViewOp>>(typeConverter, benefit);\n+  patterns.add<ViewLikeOpConversion<triton::ExpandDimsOp>>(typeConverter,\n+                                                           benefit);\n }\n \n class ConvertTritonGPUToLLVM\n@@ -1322,19 +1652,34 @@ class ConvertTritonGPUToLLVM\n     // TODO: need confirm\n     option.overrideIndexBitwidth(32);\n     TritonGPUToLLVMTypeConverter typeConverter(context, option);\n+    TritonLLVMFunctionConversionTarget funcTarget(*context, typeConverter);\n     TritonLLVMConversionTarget target(*context, typeConverter);\n \n-    RewritePatternSet patterns(context);\n-\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n \n+    // step 1: Convert FuncOp to LLVMFuncOp via partial conversion\n+    // step 2: Allocate for shared memories\n+    // step 3: Convert the rest of ops via partial conversion\n+    // The reason for a seperation between 1/3 is that, step 2 is out of\n+    // the scope of Dialect Conversion, thus we need to make sure the smem_\n+    // is not revised during the conversion of step 3.\n+    RewritePatternSet func_patterns(context);\n+    func_patterns.add<FuncOpConversion>(typeConverter, numWarps, 1 /*benefit*/);\n+    if (failed(\n+            applyPartialConversion(mod, funcTarget, std::move(func_patterns))))\n+      return signalPassFailure();\n+\n+    Allocation allocation(mod);\n     auto axisAnalysis = runAxisAnalysis(mod);\n+    initSharedMemory(allocation.getSharedMemorySize(), typeConverter);\n \n     // We set a higher benefit here to ensure triton's patterns runs before\n     // arith patterns for some encoding not supported by the community\n     // patterns.\n+    RewritePatternSet patterns(context);\n     populateTritonToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                 *axisAnalysis, 10 /*benefit*/);\n+                                 *axisAnalysis, &allocation, smem_,\n+                                 10 /*benefit*/);\n \n     // Add arith/math's patterns to help convert scalar expression to LLVM.\n     mlir::arith::populateArithmeticToLLVMConversionPatterns(typeConverter,\n@@ -1352,10 +1697,35 @@ class ConvertTritonGPUToLLVM\n     auto axisAnalysisPass =\n         std::make_unique<AxisInfoAnalysis>(module->getContext());\n     axisAnalysisPass->run(module);\n+\n     return axisAnalysisPass;\n   }\n+\n+  void initSharedMemory(size_t size,\n+                        TritonGPUToLLVMTypeConverter &typeConverter);\n+\n+  Value smem_;\n };\n \n+void ConvertTritonGPUToLLVM::initSharedMemory(\n+    size_t size, TritonGPUToLLVMTypeConverter &typeConverter) {\n+  ModuleOp mod = getOperation();\n+  OpBuilder b(mod.getBodyRegion());\n+  auto loc = mod.getLoc();\n+  auto elemTy = typeConverter.convertType(b.getIntegerType(8));\n+  auto arrayTy = LLVM::LLVMArrayType::get(elemTy, size);\n+  auto global = b.create<LLVM::GlobalOp>(\n+      loc, arrayTy, /*isConstant=*/false, LLVM::Linkage::Internal,\n+      \"global_smem\", /*value=*/Attribute(),\n+      /*alignment=*/0, mlir::gpu::GPUDialect::getWorkgroupAddressSpace());\n+  SmallVector<LLVM::LLVMFuncOp> funcs;\n+  mod.walk([&](LLVM::LLVMFuncOp func) { funcs.push_back(func); });\n+  assert(funcs.size() == 1 &&\n+         \"Inliner pass is expected before TritonGPUToLLVM\");\n+  b.setInsertionPointToStart(&funcs[0].getBody().front());\n+  smem_ = b.create<LLVM::AddressOfOp>(loc, global);\n+}\n+\n } // namespace\n \n namespace mlir {\n@@ -1366,10 +1736,20 @@ TritonLLVMConversionTarget::TritonLLVMConversionTarget(\n   addLegalDialect<LLVM::LLVMDialect>();\n   addLegalDialect<NVVM::NVVMDialect>();\n   // addIllegalDialect<triton::TritonDialect>();\n+  // addIllegalDialect<triton::gpu::TritonGPUDialect>();\n   addIllegalDialect<mlir::gpu::GPUDialect>();\n   addLegalOp<mlir::UnrealizedConversionCastOp>();\n }\n \n+TritonLLVMFunctionConversionTarget::TritonLLVMFunctionConversionTarget(\n+    MLIRContext &ctx, mlir::LLVMTypeConverter &typeConverter)\n+    : ConversionTarget(ctx), typeConverter(typeConverter) {\n+  addLegalDialect<LLVM::LLVMDialect>();\n+  // addLegalDialect<NVVM::NVVMDialect>();\n+  addIllegalOp<mlir::FuncOp>();\n+  addLegalOp<mlir::UnrealizedConversionCastOp>();\n+}\n+\n namespace triton {\n \n std::unique_ptr<OperationPass<ModuleOp>> createConvertTritonGPUToLLVMPass() {"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 80, "deletions": 0, "changes": 80, "file_content_changes": "@@ -39,6 +39,37 @@ static Type getPointeeType(Type type) {\n   return Type();\n }\n \n+namespace gpu {\n+\n+// TODO: Inheritation of layout attributes\n+unsigned getElemsPerThread(Attribute layout, ArrayRef<int64_t> shape) {\n+  size_t rank = shape.size();\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+    return blockedLayout.getElemsPerThread(shape);\n+  } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+    return sliceLayout.getElemsPerThread(shape);\n+  } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    return mmaLayout.getElemsPerThread(shape);\n+  } else if (auto sharedLayout = layout.dyn_cast<SharedEncodingAttr>()) {\n+    return sharedLayout.getElemsPerThread(shape);\n+  } else {\n+    assert(0 && \"getElemsPerThread not implemented\");\n+    return 0;\n+  }\n+}\n+\n+unsigned getShapePerCTA(const Attribute &layout, unsigned d) {\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+    return blockedLayout.getSizePerThread()[d] *\n+           blockedLayout.getThreadsPerWarp()[d] *\n+           blockedLayout.getWarpsPerCTA()[d];\n+  } else {\n+    assert(0 && \"Unimplemented usage of getShapePerCTA\");\n+    return 0;\n+  }\n+};\n+\n+} // namespace gpu\n } // namespace triton\n } // namespace mlir\n \n@@ -108,6 +139,55 @@ SliceEncodingAttr BlockedEncodingAttr::squeeze(int axis) {\n   return SliceEncodingAttr::get(getContext(), axis, *this);\n }\n \n+unsigned BlockedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n+  size_t rank = shape.size();\n+  assert(rank == getSizePerThread().size() &&\n+         \"unexpected rank in BlockedEncodingAttr::getElemsPerThread\");\n+  SmallVector<unsigned> elemsPerThreadPerDim(rank);\n+  for (size_t i = 0; i < rank; ++i) {\n+    unsigned t =\n+        getSizePerThread()[i] * getThreadsPerWarp()[i] * getWarpsPerCTA()[i];\n+    elemsPerThreadPerDim[i] =\n+        ceil<unsigned>(shape[i], t) * getSizePerThread()[i];\n+  }\n+  return product<unsigned>(elemsPerThreadPerDim);\n+}\n+\n+unsigned SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n+  size_t rank = shape.size();\n+  auto parent = getParent();\n+  unsigned dim = getDim();\n+  if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n+    assert(rank == blockedParent.getSizePerThread().size() - 1 &&\n+           \"unexpected rank in SliceEncodingAttr::getElemsPerThread\");\n+    SmallVector<int64_t> paddedShape(rank + 1);\n+    for (unsigned d = 0; d < rank + 1; ++d) {\n+      if (d < dim)\n+        paddedShape[d] = shape[d];\n+      else if (d == dim)\n+        paddedShape[d] = 1;\n+      else\n+        paddedShape[d] = shape[d - 1];\n+    }\n+    return blockedParent.getElemsPerThread(paddedShape);\n+  } else {\n+    assert(0 && \"getElemsPerThread not implemented\");\n+    return 0;\n+  }\n+}\n+\n+unsigned MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n+  // TODO:\n+  assert(0 && \"MmaEncodingAttr::getElemsPerThread not implemented\");\n+  return 0;\n+}\n+\n+unsigned SharedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n+  // TODO:\n+  assert(0 && \"SharedEncodingAttr::getElemsPerThread not implemented\");\n+  return 0;\n+}\n+\n //===----------------------------------------------------------------------===//\n // Blocked Encoding\n //===----------------------------------------------------------------------===//"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -14,6 +14,7 @@\n #include \"mlir/Transforms/Passes.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h\"\n #include \"triton/driver/llvm.h\"\n+#include \"triton/tools/sys/getenv.hpp\"\n #include \"llvm/IR/Constants.h\"\n \n namespace mlir {\n@@ -124,6 +125,17 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n                            mlir::ModuleOp module) {\n   mlir::PassManager pm(module->getContext());\n   applyPassManagerCLOptions(pm);\n+  auto printingFlags = mlir::OpPrintingFlags();\n+  printingFlags.elideLargeElementsAttrs(16);\n+  pm.enableIRPrinting(\n+      /*shouldPrintBeforePass=*/nullptr,\n+      /*shouldPrintAfterPass=*/\n+      [](mlir::Pass *pass, mlir::Operation *) {\n+        return ::triton::tools::getBoolEnv(\"MLIR_ENABLE_DUMP\");\n+      },\n+      /*printModuleScope=*/false,\n+      /*printAfterOnlyOnChange=*/true,\n+      /*printAfterOnlyOnFailure*/ false, llvm::dbgs(), printingFlags);\n \n   pm.addPass(createConvertTritonGPUToLLVMPass());\n   // Conanicalize to eliminate the remaining UnrealizedConversionCastOp"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 2, "deletions": 9, "changes": 11, "file_content_changes": "@@ -19,6 +19,7 @@\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n #include \"triton/Target/PTX/PTXTranslation.h\"\n+#include \"triton/tools/sys/getenv.hpp\"\n \n #include \"llvm/IR/LegacyPassManager.h\"\n #include \"llvm/IR/Module.h\"\n@@ -100,14 +101,6 @@ long pow2_divisor(long N) {\n   return 1;\n }\n \n-bool getBoolEnv(const std::string &env) {\n-  const char *s = std::getenv(env.c_str());\n-  std::string str(s ? s : \"\");\n-  std::transform(str.begin(), str.end(), str.begin(),\n-                 [](unsigned char c) { return std::tolower(c); });\n-  return (str == \"on\" || str == \"true\" || str == \"1\");\n-}\n-\n // Returns something like \"int16\", whether dtype is a torch.dtype or\n // triton.language.dtype.\n std::string dtype_cache_key_part(const py::object &dtype) {\n@@ -1635,7 +1628,7 @@ void init_triton_ir(py::module &&m) {\n                  /*shouldPrintBeforePass=*/nullptr,\n                  /*shouldPrintAfterPass=*/\n                  [](mlir::Pass *pass, mlir::Operation *) {\n-                   return getBoolEnv(\"MLIR_ENABLE_DUMP\");\n+                   return ::triton::tools::getBoolEnv(\"MLIR_ENABLE_DUMP\");\n                  },\n                  /*printModuleScope=*/false,\n                  /*printAfterOnlyOnChange=*/true,"}, {"filename": "python/tests/test_transpose.py", "status": "added", "additions": 68, "deletions": 0, "changes": 68, "file_content_changes": "@@ -0,0 +1,68 @@\n+import pytest\n+import torch\n+from torch.testing import assert_allclose\n+\n+import triton\n+import triton.language as tl\n+import triton.runtime as runtime\n+\n+\n+@triton.jit\n+def kernel(x_ptr, stride_xm,\n+           z_ptr, stride_zn,\n+           SIZE_M: tl.constexpr, SIZE_N: tl.constexpr):\n+    off_m = tl.arange(0, SIZE_M)\n+    off_n = tl.arange(0, SIZE_N)\n+    Xs = x_ptr + off_m[:, None] * stride_xm + off_n[None, :] * 1\n+    Zs = z_ptr + off_m[:, None] * 1 + off_n[None, :] * stride_zn\n+    tl.store(Zs, tl.load(Xs))\n+\n+# These sizes cover the case of:\n+# - blocked layout and sliced layout with block parent\n+#  -- blocked layout in which sizePerThread/threadsPerWarp/warpsPerCTA\n+#     need/need not to be wrapped\n+#  -- sliced layout incase sizePerThread need to be wrapped\n+#  -- different orders\n+# - LayoutConversion from blocked -> blocked\n+# - tt.Broadcast which requires for broadcast in either/both of\n+#   CTA/perThread level\n+\n+# What is not covered and requires for TODO:\n+# - vectorization load/store of shared memory\n+# - multiple replication of layout conversion\n+\n+\n+@pytest.mark.parametrize('NUM_WARPS,SIZE_M,SIZE_N', [\n+    [1, 16, 16],\n+    [1, 32, 32],\n+    [1, 32, 64],\n+    [2, 64, 128],\n+    [2, 128, 64]\n+])\n+def test_convert_layout_impl(NUM_WARPS, SIZE_M, SIZE_N):\n+    # TODO: this is to initialize the cuda context since it is not properly\n+    #       dealed with in the existing runtime, remove this when the runtime\n+    #       is updated\n+    torch.zeros([10], device=torch.device('cuda'))\n+    device = torch.cuda.current_device()\n+    binary = runtime.build_kernel(kernel,\n+                                  \"*fp32,i32,*fp32,i32\",\n+                                  constants={\"SIZE_M\": SIZE_M,\n+                                             \"SIZE_N\": SIZE_N},\n+                                  num_warps=NUM_WARPS,\n+                                  num_stages=3)\n+    grid = lambda META: (1, )\n+\n+    x = torch.randn((SIZE_M, SIZE_N), device='cuda', dtype=torch.float32)\n+    z = torch.empty((SIZE_N, SIZE_M), device=x.device, dtype=x.dtype)\n+    runtime.launch_kernel(kernel=binary,\n+                          device=device,\n+                          grid=grid,\n+                          x_ptr=x,\n+                          stride_xm=x.stride(0),\n+                          z_ptr=z,\n+                          stride_zn=z.stride(0),\n+                          SIZE_M=tl.constexpr(SIZE_M),\n+                          SIZE_N=tl.constexpr(SIZE_N))\n+    golden_z = torch.t(x)\n+    assert_allclose(z, golden_z, rtol=1e-7, atol=1e-7)"}, {"filename": "python/tests/test_vecadd_no_scf.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -48,6 +48,7 @@ def kernel(x_ptr,\n \n \n def test_vecadd_no_scf():\n+    vecadd_no_scf_tester(num_warps=4, block_size=256)\n     vecadd_no_scf_tester(num_warps=2, block_size=256)\n     vecadd_no_scf_tester(num_warps=1, block_size=256)\n "}, {"filename": "test/Conversion/triton_to_llvm.mlir", "status": "removed", "additions": 0, "deletions": 37, "changes": 37, "file_content_changes": "@@ -1,37 +0,0 @@\n-// RUN: triton-opt %s -split-input-file -convert-triton-to-tritongpu=num-warps=2 -convert-triton-gpu-to-llvm | FileCheck %s\n-\n-func @test_splat(%ptr: !tt.ptr<f32>) {\n-  // Here, 128 elements, 64(2*32) threads, so each need to process 2 elements\n-  //\n-  // CHECK: %0 = llvm.bitcast %arg0 : !llvm.ptr<f32, 1> to !llvm.ptr<f32, 1>\n-  // CHECK: %1 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 1>, ptr<f32, 1>)>\n-  // CHECK: %2 = llvm.insertvalue %0, %1[0] : !llvm.struct<(ptr<f32, 1>, ptr<f32, 1>)>\n-  // CHECK: %3 = llvm.insertvalue %0, %2[1] : !llvm.struct<(ptr<f32, 1>, ptr<f32, 1>)>\n-  %ptrs = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n-  %a = arith.constant 1.0 : f32\n-  %true = arith.constant 1 : i1\n-  %b = tt.splat %a : (f32) -> tensor<128xf32>\n-\n-  // Here, each thread process only 1 element\n-  // CHECK: %{{.*}} = llvm.mlir.undef : !llvm.struct<(i1)>\n-  %mask = tt.splat %true : (i1) -> tensor<64xi1>\n-\n-  return\n-}\n-\n-// -----\n-\n-func @test_store_splat(%ptr: !tt.ptr<f32>) {\n-  %ptrs = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n-  %a = arith.constant 1.0 : f32\n-  %true = arith.constant 1 : i1\n-\n-  %vs = tt.splat %a : (f32) -> tensor<128xf32>\n-  %mask = tt.splat %true : (i1) -> tensor<128xi1>\n-\n-  // CHECK: %{{.*}} = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"@${{.*}} st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\",\n-  // CHECK-SAME: \"r,l,b\" %{{.*}}, %{{.*}}, %{{.*}} : (i32, !llvm.ptr<f32, 1>, i1) -> !llvm.void\n-  tt.store %ptrs, %vs, %mask : tensor<128xf32>\n-\n-  return\n-}"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 224, "deletions": 18, "changes": 242, "file_content_changes": "@@ -1,16 +1,13 @@\n // RUN: triton-opt %s -split-input-file --convert-triton-gpu-to-llvm | FileCheck %s\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-\n-// CHECK: llvm.func @test_empty_kernel(%arg0: i32, %arg1: !llvm.ptr<f16, 1>)\n-// Here the 128 comes from the 4 in module attribute multiples 32\n-// CHECK:  attributes {nvvm.kernel = 1 : ui1, nvvm.maxntid = 128 : si32} {{.*}}\n-func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n-\n-  // CHECK:  llvm.return\n-  return\n-}\n-\n+  // CHECK: llvm.func @test_empty_kernel(%arg0: i32, %arg1: !llvm.ptr<f16, 1>)\n+  // Here the 128 comes from the 4 in module attribute multiples 32\n+  // CHECK:  attributes {nvvm.kernel = 1 : ui1, nvvm.maxntid = 128 : si32} {{.*}}\n+  func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n+    // CHECK:  llvm.return\n+    return\n+  }\n } // end module\n \n // -----\n@@ -58,7 +55,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n \n // -----\n \n-// TODO: Pending on the support of isSplat constant\n+// TODO: masked load with vectorization is pending on TODO\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: masked_load_const_other\n@@ -71,10 +68,23 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // -----\n \n+// TODO: masked load with vectorization is pending on TODO\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: masked_load_const_other_vec\n+  func @masked_load_const_other_vec(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>) {\n+    %cst_0 = arith.constant dense<0.000000e+00> : tensor<256xf32, #blocked0>\n+    %1 = tt.load %a_ptr_init, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n+    return\n+  }\n+}\n+\n+// -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n-  // CHECK-LABEL: kernel__Pfp32_Pfp32_Pfp32_i32__3c256\n-  func @kernel__Pfp32_Pfp32_Pfp32_i32__3c256(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n+  // CHECK-LABEL: global_load_store_no_vec\n+  func @global_load_store_no_vec(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n     %c256_i32 = arith.constant 256 : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n@@ -86,22 +96,107 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n     %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n \n-    // CHECK: ld.global.v4.b32\n+    // Load 4 elements from vector0\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n+\n+    // Load 4 elements from vector1\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n     %9 = tt.load %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n-    // CHECK: ld.global.v4.b32\n     %10 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n     %11 = arith.addf %9, %10 : tensor<256xf32, #blocked0>\n     %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n     %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n \n     // Store 4 elements to global\n-    // CHECK: st.global.b32.v4\n+    // CHECK: @${{.*}} st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n+    // CHECK: @${{.*}} st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n+    // CHECK: @${{.*}} st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n+    // CHECK: @${{.*}} st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n     tt.store %13, %11 : tensor<256xf32, #blocked0>\n     return\n   }\n }\n \n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n+  // CHECK-LABEL: global_load_store_vec4\n+  func @global_load_store_vec4(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n+    %c256_i32 = arith.constant 256 : i32\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.muli %0, %c256_i32 : i32\n+    %2 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n+    %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n+    %4 = arith.addi %3, %2 : tensor<256xi32, #blocked0>\n+    %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %6 = tt.addptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+\n+    // Load 4 elements from A with single one vectorized load instruction\n+    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+\n+    // Load 4 elements from B with single one vectorized load instruction\n+    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+\n+    %9 = tt.load %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n+    %10 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n+    %11 = arith.addf %9, %10 : tensor<256xf32, #blocked0>\n+    %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+\n+    // Store 4 elements to global with single one vectorized store instruction\n+    // CHECK: @$5 st.global.b32.v4 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n+    tt.store %13, %11 : tensor<256xf32, #blocked0>\n+    return\n+  }\n+}\n+\n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: global_load_store_vec8\n+    func @global_load_store_vec8(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n+    %c256_i32 = arith.constant 256 : i32\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.muli %0, %c256_i32 : i32\n+    %2 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n+    %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n+    %4 = arith.addi %3, %2 : tensor<256xi32, #blocked0>\n+    %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %6 = tt.addptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+\n+    // Load 8 elements from A with two vectorized load instruction\n+    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n \n+    // Load 8 elements from B with two vectorized load instruction\n+    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+\n+    %9 = tt.load %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n+    %10 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n+    %11 = arith.addf %9, %10 : tensor<256xf32, #blocked0>\n+    %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+\n+    // Store 8 elements to global with two vectorized store instruction\n+    // CHECK: @$5 st.global.b32.v4 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n+    // CHECK: @$5 st.global.b32.v4 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n+    tt.store %13, %11 : tensor<256xf32, #blocked0>\n+    return\n+  }\n+}\n \n // TODO: Add a testcase to verify the optimization when ptr of the LoadOp\n //       is from an addptr with const idx\n@@ -217,10 +312,121 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_store\n   func @basic_store(%ptrs: tensor<256x!tt.ptr<f32>, #blocked0>, %vals: tensor<256xf32, #blocked0>, %mask: tensor<256xi1, #blocked0>) {\n     // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n-    // CHECK-SAME: @${{.*}} st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\", \"r,l,b\" %{{.*}}, %{{.*}}, %{{.*}} : (i32, !llvm.ptr<f32, 1>, i1) -> !llvm.void\n+    // CHECK-SAME: st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n     // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n-    // CHECK-SAME: @${{.*}} st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\", \"r,l,b\" %{{.*}}, %{{.*}}, %{{.*}} : (i32, !llvm.ptr<f32, 1>, i1) -> !llvm.void\n+    // CHECK-SAME: st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n     tt.store %ptrs, %vals, %mask : tensor<256xf32, #blocked0>\n     return\n   }\n }\n+\n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [0, 1]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<1088 x i8>\n+  // CHECK-LABEL: convert_layout_blocked_blocked\n+  func @convert_layout_blocked_blocked(%arg0: tensor<16x16xf32, #blocked0>) {\n+    // CHECK: llvm.mlir.addressof @global_smem\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<16x16xf32, #blocked0>) -> tensor<16x16xf32, #blocked1>\n+    return\n+  }\n+}\n+\n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [16, 2], warpsPerCTA = [1, 1], order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<1280 x i8>\n+  // CHECK-LABEL: convert_layout_blocked_blocked_vec\n+  func @convert_layout_blocked_blocked_vec(%arg0: tensor<16x16xf32, #blocked0>) {\n+    // CHECK: llvm.mlir.addressof @global_smem\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<16x16xf32, #blocked0>) -> tensor<16x16xf32, #blocked1>\n+    return\n+  }\n+}\n+\n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<640 x i8>\n+  // CHECK-LABEL: convert_layout_blocked_blocked_multi_rep\n+  func @convert_layout_blocked_blocked_multi_rep(%arg0: tensor<16x16xf32, #blocked0>) {\n+    // CHECK: llvm.mlir.addressof @global_smem\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<16x16xf32, #blocked0>) -> tensor<16x16xf32, #blocked1>\n+    return\n+  }\n+}\n+\n+// TODO: problems in MLIR's parser on slice layout\n+// #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n+// module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+//   func @make_range_sliced_layout() {\n+//     %0 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked0}>>\n+//     return\n+//   }\n+// }\n\\ No newline at end of file"}]