[{"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/Utility.cpp", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -489,8 +489,6 @@ bool isWSCandidateLoad(Operation *op) {\n       cvtOp.getResult().getType().cast<RankedTensorType>().getEncoding();\n   if (!encoding || !encoding.dyn_cast<ttg::SharedEncodingAttr>())\n     return false;\n-  if (ttg::getNumCTAs(encoding) > 1)\n-    return false;\n \n   DenseSet<Value> depSet;\n   if (failed(getDependentValues(op->getResult(0), depSet)))"}, {"filename": "python/test/unit/hopper/test_persistent_warp_specialized_gemm.py", "status": "modified", "additions": 10, "deletions": 13, "changes": 23, "file_content_changes": "@@ -782,20 +782,17 @@ def full_static_persistent_matmul_kernel(\n                              (*shape_w_c, trans_a, trans_b, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                              for shape_w_c in [\n                                  [64, 64, 32, 4, 1, 128, 256, 64],\n-                                 # TODO: enable when num_ctas != 1 is supported.\n-                                 # [128, 128, 16, 4, 4, 512, 256, 64],\n-                                 # [128, 256, 32, 4, 8, 256, 256, 192],\n-                                 # [512, 256, 32, 4, 8, 1024, 256, 192],\n+                                 [128, 128, 16, 4, 4, 512, 256, 64],\n+                                 [128, 256, 32, 4, 8, 256, 256, 192],\n+                                 [512, 256, 32, 4, 8, 1024, 256, 192],\n                                  # BLOCK_K >= 128\n                                  [64, 128, 128, 4, 1, 512, 256, 256],\n                                  [128, 128, 128, 4, 1, 256, 256, 192],\n-                                 # TODO: enable when num_ctas != 1 is supported.\n-                                 # [128, 128, 128, 4, 2, 256, 256, 192],\n+                                 [128, 128, 128, 4, 2, 256, 256, 192],\n                                  # small BLOCK_M and BLOCK_K\n                                  [16, 32, 32, 4, 1, 128, 256, 64],\n                                  [32, 32, 16, 4, 1, 256, 256, 192],\n-                                 # TODO: enable when num_ctas != 1 is supported.\n-                                 # [16, 32, 64, 4, 4, 512, 256, 64],\n+                                 [16, 32, 64, 4, 4, 512, 256, 64],\n                              ]\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n@@ -806,13 +803,13 @@ def full_static_persistent_matmul_kernel(\n                          ] + [(*shape_w_c, trans_a, trans_b, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n                               for shape_w_c in [\n                              [64, 64, 16, 4, 1, 128, 128, 64],\n-                             *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4] for num_ctas in [1]],\n+                             *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4] for num_ctas in [1, 2, 4]],\n                              # for chain-dot\n                              [128, 128, 64, 4, 1, None, None, None],\n                              [64, 64, 16, 4, 1, None, None, None],\n                              # small BLOCK_M and BLOCK_K\n                              [16, 16, 64, 4, 1, 128, 128, 64],\n-                             *[[16, 32, 64, num_warps, num_ctas, 256, 256, 256] for num_warps in [4] for num_ctas in [1]],\n+                             *[[16, 32, 64, num_warps, num_ctas, 256, 256, 256] for num_warps in [4] for num_ctas in [1, 2]],\n                              #  # TODO: enable when num_warps != 4 is supported.\n                              #  # repeat\n                              #  # [64, 64, 32, 8, 1, 128, 256, 64],\n@@ -841,9 +838,9 @@ def full_static_persistent_matmul_kernel(\n                              (*shape_w_c, *shape, False, True, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                              # irregular shapes\n                              for shape_w_c in [\n-                                 [128, 128, 64, 4, 1]\n-                                 # [256, 128, 64, 4, 2],\n-                                 # [128, 128, 128, 4, 2],\n+                                 [128, 128, 64, 4, 1],\n+                                 [256, 128, 64, 4, 2],\n+                                 [128, 128, 128, 4, 2]\n                              ]\n                              for shape in list(itertools.product([*range(512, 4096, 360)], [*range(512, 4096, 360)], [512, 1024]))\n                              for out_dtype in ['float16', 'float32']"}]