[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 10, "deletions": 18, "changes": 28, "file_content_changes": "@@ -455,13 +455,6 @@ class ConvertTritonGPUOpToLLVMPattern\n     return multiDimBase;\n   }\n \n-  SmallVector<Value>\n-  emitBaseIndexForBlockedLayout(Location loc, ConversionPatternRewriter &b,\n-                                const MmaEncodingAttr &mmaLayout,\n-                                ArrayRef<int64_t> shape) const {\n-    // ongoing\n-  }\n-\n   SmallVector<SmallVector<Value>> emitIndices(Location loc,\n                                               ConversionPatternRewriter &b,\n                                               const Attribute &layout,\n@@ -2446,8 +2439,8 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n \n   // Load A or B matrix.\n   auto getLoadMatrixFn =\n-      [&](Value tensor, int wpt, int kOrder, ArrayRef<int> instrShape,\n-          ArrayRef<int> matShape, Value warpId,\n+      [&](Value tensor, Value llTensor, int wpt, int kOrder,\n+          ArrayRef<int> instrShape, ArrayRef<int> matShape, Value warpId,\n           decltype(ha) &vals) -> std::function<void(int, int)> {\n     auto tensorTy = tensor.getType().cast<RankedTensorType>();\n     // We assumes that the input operand of Dot should be from shared layout.\n@@ -2468,10 +2461,9 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n     SmallVector<Value> ptrs(numPtrs);\n \n     Type smemPtrTy = helper.getShemPtrTy();\n-    auto smemBase = getSharedMemoryBase(loc, rewriter, tensor);\n     for (int i = 0; i < numPtrs; ++i) {\n-      ptrs[i] = bit_cast(\n-          smemPtrTy, gep(smemBase.getType(), smemBase, ValueRange({offs[i]})));\n+      ptrs[i] =\n+          bit_cast(smemPtrTy, gep(smemPtrTy, llTensor, ValueRange({offs[i]})));\n     }\n \n     bool needTrans = kOrder != order[0];\n@@ -2499,16 +2491,16 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n \n   std::function<void(int, int)> loadA;\n   std::function<void(int, int)> loadB = getLoadMatrixFn(\n-      B, mmaLayout.getWarpsPerCTA()[1] /*wpt*/, 0 /*kOrder*/,\n-      {mmaInstrK, mmaInstrN} /*instrShpae*/,\n+      B, adapter.b() /*llTensor*/, mmaLayout.getWarpsPerCTA()[1] /*wpt*/,\n+      0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShpae*/,\n       {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/);\n \n   if (aTensorTy.getEncoding()\n           .dyn_cast<SharedEncodingAttr>()) { // load from smem\n-    loadA = getLoadMatrixFn(A, mmaLayout.getWarpsPerCTA()[0] /*wpt*/,\n-                            1 /*kOrder*/, {mmaInstrM, mmaInstrK} /*instrShpae*/,\n-                            {matShapeM, matShapeK} /*matShape*/,\n-                            warpM /*warpId*/, ha /*vals*/);\n+    loadA = getLoadMatrixFn(\n+        A, adapter.a() /*llTensor*/, mmaLayout.getWarpsPerCTA()[0] /*wpt*/,\n+        1 /*kOrder*/, {mmaInstrM, mmaInstrK} /*instrShpae*/,\n+        {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, ha /*vals*/);\n   } else if (auto blockedLayout =\n                  aTensorTy.getEncoding()\n                      .dyn_cast<BlockedEncodingAttr>()) { // load from registers,"}]