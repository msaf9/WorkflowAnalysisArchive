[{"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -144,7 +144,7 @@ class MoveOpAfterLayoutConversion : public mlir::RewritePattern {\n     //\n     if (!argOp)\n       return failure();\n-    //\n+    // we only handle loads since the goal of this pass is to\n     SetVector<Operation *> processed;\n     SetVector<Attribute> layout;\n     llvm::MapVector<Value, Attribute> toConvert;\n@@ -182,7 +182,7 @@ class TritonGPUOptimizeDotOperandsPass\n     mlir::RewritePatternSet patterns(context);\n     patterns.add<OptimizeConvertToDotOperand>(context);\n     patterns.add<ConvertTransConvert>(context);\n-    patterns.add<MoveOpAfterLayoutConversion>(context);\n+    // patterns.add<MoveOpAfterLayoutConversion>(context);\n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n       signalPassFailure();\n     if (fixupLoops(m).failed())"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -split-input-file -tritongpu-optimize-dot-operands -tritongpu-pipeline=num-stages=3 -canonicalize | FileCheck %s\n+// RUN: triton-opt %s -split-input-file -tritongpu-pipeline=num-stages=3 -canonicalize | FileCheck %s\n \n // 4 warps\n // matmul: 128x32 @ 32x128 -> 128x128\n@@ -72,14 +72,14 @@ func.func @matmul_loop(%lb : index, %ub : index, %step : index,\n   %a_off = arith.constant dense<4> : tensor<128x32xi32, #AL>\n   %b_off = arith.constant dense<4> : tensor<32x128xi32, #BL>\n   \n-  %b_scale = arith.constant dense<4.> : tensor<32x128xf16, #BL>\n+  %b_scale = arith.constant dense<4.> : tensor<32x128xf16, #B>\n \n   %loop:3 = scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n     %a_ = tt.load %a_ptr {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n     %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n     %b__ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n-    %b_ = arith.mulf %b__, %b_scale: tensor<32x128xf16, #BL>\n-    %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n+    %b_ = triton_gpu.convert_layout %b__ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n+    %b = arith.mulf %b_, %b_scale: tensor<32x128xf16, #B>\n \n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n "}]