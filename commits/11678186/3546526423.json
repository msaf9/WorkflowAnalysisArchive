[{"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 23, "deletions": 0, "changes": 23, "file_content_changes": "@@ -8,6 +8,29 @@\n \n namespace mlir {\n \n+class ReduceOpHelper {\n+public:\n+  explicit ReduceOpHelper(triton::ReduceOp op) : op(op) {\n+    srcTy = op.operand().getType().cast<RankedTensorType>();\n+  }\n+\n+  ArrayRef<int64_t> getSrcShape() { return srcTy.getShape(); }\n+\n+  Attribute getSrcLayout() { return srcTy.getEncoding(); }\n+\n+  bool isFastReduction();\n+\n+  unsigned getInterWarpSize();\n+\n+  unsigned getIntraWarpSize();\n+\n+  unsigned getThreadsReductionAxis();\n+\n+private:\n+  triton::ReduceOp op;\n+  RankedTensorType srcTy{};\n+};\n+\n bool isSharedEncoding(Value value);\n \n bool maybeSharedAllocationOp(Operation *op);"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 8, "deletions": 14, "changes": 22, "file_content_changes": "@@ -89,24 +89,19 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n }\n \n SmallVector<unsigned> getScratchConfigForReduce(triton::ReduceOp op) {\n-  auto srcTy = op.operand().getType().cast<RankedTensorType>();\n-  auto srcLayout = srcTy.getEncoding();\n-  auto srcShape = srcTy.getShape();\n-  auto axis = op.axis();\n-\n-  bool fastReduce = axis == getOrder(srcLayout)[0];\n+  ReduceOpHelper helper(op);\n \n   SmallVector<unsigned> smemShape;\n+  auto srcShape = helper.getSrcShape();\n   for (auto d : srcShape)\n     smemShape.push_back(d);\n \n-  if (fastReduce) {\n-    unsigned sizeInterWarps = gpu::getWarpsPerCTA(srcLayout)[axis];\n-    smemShape[axis] = sizeInterWarps;\n+  auto axis = op.axis();\n+  if (helper.isFastReduction()) {\n+    smemShape[axis] = helper.getInterWarpSize();\n   } else {\n-    unsigned threadsPerCTAAxis = gpu::getThreadsPerWarp(srcLayout)[axis] *\n-                                 gpu::getWarpsPerCTA(srcLayout)[axis];\n-    smemShape[axis] = std::min(smemShape[axis], threadsPerCTAAxis);\n+    smemShape[axis] =\n+        std::min(smemShape[axis], helper.getThreadsReductionAxis());\n   }\n \n   return smemShape;\n@@ -177,8 +172,7 @@ class AllocationAnalysis {\n       // TODO(Keren): Reduce with index is not supported yet.\n       auto value = op->getOperand(0);\n       if (auto tensorType = value.getType().dyn_cast<RankedTensorType>()) {\n-        auto srcLayout = tensorType.getEncoding();\n-        bool fastReduce = reduceOp.axis() == getOrder(srcLayout)[0];\n+        bool fastReduce = ReduceOpHelper(reduceOp).isFastReduction();\n         auto smemShape = getScratchConfigForReduce(reduceOp);\n         unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n                                          std::multiplies{});"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 32, "deletions": 0, "changes": 32, "file_content_changes": "@@ -5,6 +5,38 @@\n \n namespace mlir {\n \n+bool ReduceOpHelper::isFastReduction() {\n+  auto srcLayout = srcTy.getEncoding();\n+  auto axis = op.axis();\n+  return axis == triton::gpu::getOrder(srcLayout)[0];\n+}\n+\n+unsigned ReduceOpHelper::getInterWarpSize() {\n+  auto srcLayout = srcTy.getEncoding();\n+  auto srcShape = srcTy.getShape();\n+  auto axis = op.axis();\n+  auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n+  unsigned sizeIntraWarps = getIntraWarpSize();\n+  return std::min(srcReduceDimSize / sizeIntraWarps,\n+                  triton::gpu::getWarpsPerCTA(srcLayout)[axis]);\n+}\n+\n+unsigned ReduceOpHelper::getIntraWarpSize() {\n+  auto srcLayout = srcTy.getEncoding();\n+  auto srcShape = srcTy.getShape();\n+  auto axis = op.axis();\n+  auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n+  return std::min(srcReduceDimSize,\n+                  triton::gpu::getThreadsPerWarp(srcLayout)[axis]);\n+}\n+\n+unsigned ReduceOpHelper::getThreadsReductionAxis() {\n+  auto srcLayout = srcTy.getEncoding();\n+  auto axis = op.axis();\n+  return triton::gpu::getThreadsPerWarp(srcLayout)[axis] *\n+         triton::gpu::getWarpsPerCTA(srcLayout)[axis];\n+}\n+\n bool isSharedEncoding(Value value) {\n   auto type = value.getType();\n   if (auto tensorType = type.dyn_cast<RankedTensorType>()) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 5, "deletions": 7, "changes": 12, "file_content_changes": "@@ -1563,9 +1563,7 @@ struct ReduceOpConversion\n LogicalResult\n ReduceOpConversion::matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n                                     ConversionPatternRewriter &rewriter) const {\n-  auto srcTy = op.operand().getType().cast<RankedTensorType>();\n-  auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n-  if (op.axis() == srcLayout.getOrder()[0])\n+  if (ReduceOpHelper(op).isFastReduction())\n     return matchAndRewriteFast(op, adaptor, rewriter);\n   return matchAndRewriteBasic(op, adaptor, rewriter);\n }\n@@ -1763,11 +1761,11 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n   smemBase = bitcast(smemBase, elemPtrTy);\n \n-  auto order = getOrder(srcLayout);\n-  unsigned sizeIntraWarps =\n-      std::min(static_cast<unsigned>(srcShape[axis]), threadsPerWarp[axis]);\n-  unsigned sizeInterWarps = warpsPerCTA[axis];\n+  ReduceOpHelper helper(op);\n+  unsigned sizeIntraWarps = helper.getIntraWarpSize();\n+  unsigned sizeInterWarps = helper.getInterWarpSize();\n \n+  auto order = getOrder(srcLayout);\n   unsigned srcElems = getElemsPerThread(srcTy);\n   auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n   auto srcValues = getElementsFromStruct(loc, adaptor.operand(), rewriter);"}]