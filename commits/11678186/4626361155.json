[{"filename": "python/test/unit/operators/test_blocksparse.py", "status": "modified", "additions": 8, "deletions": 11, "changes": 19, "file_content_changes": "@@ -81,17 +81,14 @@ def test_matmul(MODE, TRANS_A, TRANS_B, BLOCK, DTYPE, Z=3, H=2, M=512, N=384, K=\n     a_tri.retain_grad()\n     b_tri.retain_grad()\n     op = triton.ops.blocksparse.matmul(layout, BLOCK, MODE, trans_a=TRANS_A, trans_b=TRANS_B, device=\"cuda\")\n-    try:\n-        c_tri = op(a_tri, b_tri)\n-        c_tri.backward(dc_tri)\n-        da_tri = a_tri.grad\n-        db_tri = b_tri.grad\n-        # compare\n-        torch.testing.assert_allclose(c_ref, c_tri)\n-        torch.testing.assert_allclose(da_ref, da_tri)\n-        torch.testing.assert_allclose(db_ref, db_tri)\n-    except triton.OutOfResourcesError as e:\n-        pytest.skip(str(e))\n+    c_tri = op(a_tri, b_tri)\n+    c_tri.backward(dc_tri)\n+    da_tri = a_tri.grad\n+    db_tri = b_tri.grad\n+    # compare\n+    torch.testing.assert_allclose(c_ref, c_tri)\n+    torch.testing.assert_allclose(da_ref, da_tri)\n+    torch.testing.assert_allclose(db_ref, db_tri)\n \n \n configs = ["}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 10, "deletions": 8, "changes": 18, "file_content_changes": "@@ -93,18 +93,18 @@ def assert_close(x, y, atol=None, rtol=None, err_msg=''):\n     import numpy as np\n     import torch\n \n-    # absolute tolerance hook\n-    def default_atol(dtype):\n-        return 1e-2\n+    # canonicalize arguments to be tensors\n+    if not isinstance(x, torch.Tensor):\n+        x = torch.tensor(x)\n+    if not isinstance(y, torch.Tensor):\n+        y = torch.tensor(y)\n+    # absolute tolerance\n     if atol is None:\n-        atol = default_atol\n+        atol = 1e-2\n     atol = atol(x.dtype) if callable(atol) else atol\n     # relative tolerance hook\n-\n-    def default_rtol(dtype):\n-        return 0.\n     if rtol is None:\n-        rtol = default_rtol\n+        rtol = 0.\n     rtol = rtol(x.dtype) if callable(rtol) else rtol\n     # we use numpy instead of pytorch\n     # as it seems more memory efficient\n@@ -117,6 +117,8 @@ def default_rtol(dtype):\n         if y.dtype == torch.bfloat16:\n             y = y.float()\n         y = y.cpu().detach().numpy()\n+    # we handle size==1 case separately as we can\n+    # provide better error message there\n     if x.size > 1 or y.size > 1:\n         np.testing.assert_allclose(x, y, atol=atol, rtol=rtol, equal_nan=True)\n         return"}]