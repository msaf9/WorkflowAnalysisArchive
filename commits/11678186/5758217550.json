[{"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 36, "deletions": 34, "changes": 70, "file_content_changes": "@@ -32,33 +32,34 @@ def _fwd_kernel(\n     stride_kz, stride_kh, stride_kn, stride_kk,\n     stride_vz, stride_vh, stride_vk, stride_vn,\n     stride_oz, stride_oh, stride_om, stride_on,\n-    Z, H, N_CTX,\n+    Z, H, N_CTX, P_SEQ,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n     IS_CAUSAL: tl.constexpr,\n ):\n     start_m = tl.program_id(0)\n     off_hz = tl.program_id(1)\n-    qvk_offset = off_hz * stride_qh\n+    q_offset = off_hz * stride_qh\n+    kv_offset = off_hz * stride_kh\n     Q_block_ptr = tl.make_block_ptr(\n-        base=Q + qvk_offset,\n+        base=Q + q_offset,\n         shape=(N_CTX, BLOCK_DMODEL),\n         strides=(stride_qm, stride_qk),\n         offsets=(start_m * BLOCK_M, 0),\n         block_shape=(BLOCK_M, BLOCK_DMODEL),\n         order=(1, 0)\n     )\n     K_block_ptr = tl.make_block_ptr(\n-        base=K + qvk_offset,\n-        shape=(BLOCK_DMODEL, N_CTX),\n+        base=K + kv_offset,\n+        shape=(BLOCK_DMODEL, N_CTX + P_SEQ),\n         strides=(stride_kk, stride_kn),\n         offsets=(0, 0),\n         block_shape=(BLOCK_DMODEL, BLOCK_N),\n         order=(0, 1)\n     )\n     V_block_ptr = tl.make_block_ptr(\n-        base=V + qvk_offset,\n-        shape=(N_CTX, BLOCK_DMODEL),\n+        base=V + kv_offset,\n+        shape=(N_CTX + P_SEQ, BLOCK_DMODEL),\n         strides=(stride_vk, stride_vn),\n         offsets=(0, 0),\n         block_shape=(BLOCK_N, BLOCK_DMODEL),\n@@ -80,15 +81,15 @@ def _fwd_kernel(\n     q = (q * qk_scale).to(tl.float16)\n     # loop over k, v and update accumulator\n     lo = 0\n-    hi = (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX\n+    hi = P_SEQ + (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX + P_SEQ\n     for start_n in range(lo, hi, BLOCK_N):\n         # -- load k, v --\n         k = tl.load(K_block_ptr)\n         v = tl.load(V_block_ptr)\n         # -- compute qk ---\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n         if IS_CAUSAL:\n-            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n+            qk = tl.where(P_SEQ + offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n         qk += tl.dot(q, k)\n         # -- compute scaling constant ---\n         m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n@@ -110,7 +111,7 @@ def _fwd_kernel(\n     tl.store(l_ptrs, m_i + tl.math.log2(l_i))\n     # write back O\n     O_block_ptr = tl.make_block_ptr(\n-        base=Out + qvk_offset,\n+        base=Out + q_offset,\n         shape=(N_CTX, BLOCK_DMODEL),\n         strides=(stride_om, stride_on),\n         offsets=(start_m * BLOCK_M, 0),\n@@ -146,8 +147,8 @@ def _bwd_kernel(\n     stride_qz, stride_qh, stride_qm, stride_qk,\n     stride_kz, stride_kh, stride_kn, stride_kk,\n     stride_vz, stride_vh, stride_vk, stride_vn,\n-    Z, H, N_CTX,\n-    num_block,\n+    Z, H, N_CTX, P_SEQ,\n+    num_block_q, num_block_kv,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n     CAUSAL: tl.constexpr,\n@@ -158,20 +159,20 @@ def _bwd_kernel(\n     qk_scale = sm_scale * 1.44269504\n     # offset pointers for batch/head\n     Q += off_z * stride_qz + off_h * stride_qh\n-    K += off_z * stride_qz + off_h * stride_qh\n-    V += off_z * stride_qz + off_h * stride_qh\n+    K += off_z * stride_kz + off_h * stride_kh\n+    V += off_z * stride_vz + off_h * stride_vh\n     DO += off_z * stride_qz + off_h * stride_qh\n     DQ += off_z * stride_qz + off_h * stride_qh\n-    DK += off_z * stride_qz + off_h * stride_qh\n-    DV += off_z * stride_qz + off_h * stride_qh\n-    for start_n in range(0, num_block):\n+    DK += off_z * stride_kz + off_h * stride_kh\n+    DV += off_z * stride_vz + off_h * stride_vh\n+    for start_n in range(0, num_block_kv):\n         if CAUSAL:\n-            lo = start_n * BLOCK_M\n+            lo = tl.math.max(start_n * BLOCK_M - P_SEQ, 0)\n         else:\n             lo = 0\n         # initialize row/col offsets\n         offs_qm = lo + tl.arange(0, BLOCK_M)\n-        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n+        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M) \n         offs_m = tl.arange(0, BLOCK_N)\n         offs_k = tl.arange(0, BLOCK_DMODEL)\n         # initialize pointers to value-like data\n@@ -183,20 +184,20 @@ def _bwd_kernel(\n         # pointer to row-wise quantities in value-like data\n         D_ptrs = D + off_hz * N_CTX\n         l_ptrs = L + off_hz * N_CTX\n-        # initialize dv amd dk\n-        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+        # initialize dk amd dv\n         dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n         # k and v stay in SRAM throughout\n         k = tl.load(k_ptrs)\n         v = tl.load(v_ptrs)\n         # loop over rows\n-        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n+        for start_m in range(lo, num_block_q * BLOCK_M, BLOCK_M):\n             offs_m_curr = start_m + offs_m\n             # load q, k, v, do on-chip\n             q = tl.load(q_ptrs)\n             # recompute p = softmax(qk, dim=-1).T\n             if CAUSAL:\n-                qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\"-inf\"))\n+                qk = tl.where(P_SEQ + offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\"-inf\"))\n             else:\n                 qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n             qk += tl.dot(q, tl.trans(k))\n@@ -223,10 +224,10 @@ def _bwd_kernel(\n             q_ptrs += BLOCK_M * stride_qm\n             do_ptrs += BLOCK_M * stride_qm\n         # write-back\n-        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n         dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n-        tl.store(dv_ptrs, dv)\n+        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n         tl.store(dk_ptrs, dk)\n+        tl.store(dv_ptrs, dv)\n \n \n empty = torch.empty(128, device=\"cuda\")\n@@ -245,7 +246,7 @@ def forward(ctx, q, k, v, causal, sm_scale):\n         BLOCK_N = 64\n         grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n-\n+        P_SEQ = 0 if q.shape[-2] == k.shape[-2] else k.shape[-2] - q.shape[-2]\n         num_warps = 4 if Lk <= 64 else 8\n         _fwd_kernel[grid](\n             q, k, v, sm_scale,\n@@ -255,7 +256,7 @@ def forward(ctx, q, k, v, causal, sm_scale):\n             k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n             v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n             o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n-            q.shape[0], q.shape[1], q.shape[2],\n+            q.shape[0], q.shape[1], q.shape[2], P_SEQ,\n             BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=Lk,\n             IS_CAUSAL=causal,\n             num_warps=num_warps,\n@@ -266,6 +267,7 @@ def forward(ctx, q, k, v, causal, sm_scale):\n         ctx.sm_scale = sm_scale\n         ctx.BLOCK_DMODEL = Lk\n         ctx.causal = causal\n+        ctx.P_SEQ = P_SEQ\n         return o\n \n     @staticmethod\n@@ -290,8 +292,8 @@ def backward(ctx, do):\n             q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n             k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n             v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n-            q.shape[0], q.shape[1], q.shape[2],\n-            ctx.grid[0],\n+            q.shape[0], q.shape[1], q.shape[2], ctx.P_SEQ,\n+            ctx.grid[0], triton.cdiv(k.shape[2], BLOCK),\n             BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n             BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n             CAUSAL=ctx.causal,\n@@ -303,17 +305,17 @@ def backward(ctx, do):\n attention = _attention.apply\n \n \n-@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(6, 9, 1024, 64)])\n+@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD, P_SEQ', [(6, 9, 1024, 64, 128)])\n @pytest.mark.parametrize('causal', [False, True])\n-def test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\n+def test_op(Z, H, N_CTX, D_HEAD, P_SEQ, causal, dtype=torch.float16):\n     torch.manual_seed(20)\n     q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n-    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n-    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX + P_SEQ, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX + P_SEQ, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n     sm_scale = 0.5\n     dout = torch.randn_like(q)\n     # reference implementation\n-    M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n+    M = torch.tril(torch.ones((N_CTX, N_CTX + P_SEQ), device=\"cuda\"), diagonal=P_SEQ)\n     p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n     if causal:\n         p[:, :, M == 0] = float(\"-inf\")"}]