[{"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 18, "deletions": 15, "changes": 33, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -split-input-file -tritongpu-pipeline=num-stages=3 -canonicalize | FileCheck %s\n+// RUN: triton-opt %s -split-input-file -tritongpu-optimize-dot-operands -tritongpu-pipeline=num-stages=3 -canonicalize | FileCheck %s\n \n // 4 warps\n // matmul: 128x32 @ 32x128 -> 128x128\n@@ -32,9 +32,10 @@\n // CHECK: %[[A0:.*]] = triton_gpu.extract_slice %[[A1BUFFER]][0, 0, 0]\n // CHECK: %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]][0, 0, 0]\n // CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n+// CHECK:   %[[arg_b0_dot_op_0:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n // CHECK:   %[[arg_a0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_a0]]\n-// CHECK:   %[[arg_b0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n-// CHECK:   tt.dot %[[arg_a0_dot_op]], %[[arg_b0_dot_op]], {{.*}}\n+// CHECK:   %[[arg_b0_dot_op_1:.*]] = arith.mulf %[[arg_b0_dot_op_0]]\n+// CHECK:   tt.dot %[[arg_a0_dot_op]], %[[arg_b0_dot_op_1]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK:   %[[NEXT_A_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n@@ -89,7 +90,6 @@ func.func @matmul_loop(%lb : index, %ub : index, %step : index,\n   return %loop#2: tensor<128x128xf32, #C>\n }\n \n-\n // CHECK: func.func @matmul_loop_nested\n // CHECK-DAG: %[[CONSTANT_0:.*]] = arith.constant 0 : i32\n // CHECK-DAG: %[[CONSTANT_1:.*]] = arith.constant 1 : i32\n@@ -106,8 +106,8 @@ func.func @matmul_loop(%lb : index, %ub : index, %step : index,\n // CHECK:   %[[A0:.*]] = triton_gpu.extract_slice %[[A1BUFFER]][0, 0, 0]\n // CHECK:   %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]][0, 0, 0]\n // CHECK:   scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n-// CHECK:     %[[arg_a0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_a0]]\n // CHECK:     %[[arg_b0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n+// CHECK:     %[[arg_a0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_a0]]\n // CHECK:     tt.dot %[[arg_a0_dot_op]], %[[arg_b0_dot_op]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n@@ -121,8 +121,10 @@ func.func @matmul_loop(%lb : index, %ub : index, %step : index,\n // CHECK:     scf.yield {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_BUFFER]], %[[NEXT_B_BUFFER]], %[[NEXT_A]], %[[NEXT_B]], {{.*}}, {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n func.func @matmul_loop_nested(%lb : index, %ub : index, %step : index,\n                          %A : !tt.ptr<f16> {tt.divisibility = 16 : i32},\n-                         %B : !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n-  scf.for %iv0 = %lb to %ub step %step {\n+                         %B : !tt.ptr<f16> {tt.divisibility = 16 : i32}) -> tensor<128x128xf32, #C>{\n+\n+  %c_start = arith.constant dense<0.00e+00> : tensor<128x128xf32, #C>\n+  %loop1:1 = scf.for %iv0 = %lb to %ub step %step iter_args(%c_init = %c_start) -> (tensor<128x128xf32, #C>) {\n     // A ptrs\n     %a_ptr_splat = tt.splat %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n     %a_tmp0 = tt.make_range {end = 32: i32, start = 0: i32} : tensor<32xi32, #ALs0>\n@@ -140,12 +142,11 @@ func.func @matmul_loop_nested(%lb : index, %ub : index, %step : index,\n     %a_other = arith.constant dense<0.00e+00> : tensor<128x32xf16, #AL>\n     %b_mask = arith.constant dense<true> : tensor<32x128xi1, #BL>\n     %b_other = arith.constant dense<0.00e+00> : tensor<32x128xf16, #BL>\n-    %c_init = arith.constant dense<0.00e+00> : tensor<128x128xf32, #C>\n \n     %a_off = arith.constant dense<4> : tensor<128x32xi32, #AL>\n     %b_off = arith.constant dense<4> : tensor<32x128xi32, #BL>\n \n-    scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n+    %loop2:3 = scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n       %a_ = tt.load %a_ptr, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n       %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n       %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n@@ -157,9 +158,11 @@ func.func @matmul_loop_nested(%lb : index, %ub : index, %step : index,\n       %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n       scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n     }\n+\n+    scf.yield %loop2#2 : tensor<128x128xf32, #C>\n   }\n-  return\n-}\n+  return %loop1#0 : tensor<128x128xf32, #C>\n+} \n \n \n // CHECK: func.func @matmul_loop_single_pipeline\n@@ -185,7 +188,7 @@ func.func @matmul_loop_nested(%lb : index, %ub : index, %step : index,\n // CHECK:   scf.yield {{.*}}, {{.*}}, %[[NEXT_B_BUFFER]], %[[NEXT_B]], {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n func.func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index,\n                                   %A : !tt.ptr<f16> {tt.divisibility = 16 : i32},\n-                                  %B : !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n+                                  %B : !tt.ptr<f16> {tt.divisibility = 16 : i32}) -> tensor<128x128xf32, #C> {\n   // A ptrs\n   %a_ptr_splat = tt.splat %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %a_tmp0 = tt.make_range {end = 32: i32, start = 0: i32} : tensor<32xi32, #ALs0>\n@@ -211,12 +214,12 @@ func.func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index,\n \n   %b_off = arith.constant dense<4> : tensor<32x128xi32, #BL>\n \n-  scf.for %iv = %lb to %ub step %step iter_args(%b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n+  %loop:2 = scf.for %iv = %lb to %ub step %step iter_args(%b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n     %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n     %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n     %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_b_ptr, %c : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n-  return\n-}\n+  return %loop#1 : tensor<128x128xf32, #C>\n+}\n\\ No newline at end of file"}]