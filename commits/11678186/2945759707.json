[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 27, "deletions": 26, "changes": 53, "file_content_changes": "@@ -746,16 +746,11 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     assert torch.all(f8_tensor == f8_output_tensor)\n \n \n-@pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16])\n-def test_xf16_to_f8_rounding(dtype):\n-    \"\"\"Takes all xfloat16s, converts them to float8 and back to xfloat16.\n-    Checks that the absolute error is the minimum over all float8.\n-\n+def test_f16_to_f8_rounding():\n+    \"\"\"Takes all float16s, converts them to float8 and back to float16. Checks that the absolute\n+    error is the minimum over all float8.\n     Or the same explanation a bit mathier:\n-    for all xf16 |xf16 - fromf8(tof8(xf16))| == min\n-    over all f8 |xf16 - fromf8(f8)|\"\"\"\n-    check_type_supported(dtype)\n-\n+    for all f16 |f16 - fromf8(tof8(f16))| == min over all f8 |f16 - fromf8(f8)|\"\"\"\n     @triton.jit\n     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n@@ -764,42 +759,48 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         output = input\n         tl.store(output_ptr + offsets, output, mask=mask)\n \n-    xf16_input = torch.arange(-int(2 ** (16 - 1)), int(2 ** (16 - 1)), device='cuda', dtype=torch.int16).view(dtype)\n-    n_elements = xf16_input.numel()\n-    f8_output_tensor = torch.empty_like(xf16_input, dtype=torch.int8)\n+    # torch.view with a dtype isn't supported in triton's torch yet so use numpy's view\n+    f16_input_np = (\n+        np.array(\n+            range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=np.int16,\n+        )\n+        .view(np.float16)\n+    )\n+    f16_input = torch.tensor(f16_input_np, dtype=torch.float16, device='cuda')\n+    n_elements = f16_input.numel()\n+    f8_output_tensor = torch.empty_like(f16_input, dtype=torch.int8)\n     f8_output = triton.reinterpret(f8_output_tensor, tl.float8)\n     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n-    copy_kernel[grid](xf16_input, f8_output, n_elements, BLOCK_SIZE=1024)\n+    copy_kernel[grid](f16_input, f8_output, n_elements, BLOCK_SIZE=1024)\n \n-    xf16_output = torch.empty_like(xf16_input, dtype=dtype)\n-    copy_kernel[grid](f8_output, xf16_output, n_elements, BLOCK_SIZE=1024)\n+    f16_output = torch.empty_like(f16_input, dtype=torch.float16)\n+    copy_kernel[grid](f8_output, f16_output, n_elements, BLOCK_SIZE=1024)\n \n-    abs_error = torch.abs(xf16_input - xf16_output)\n+    abs_error = torch.abs(f16_input - f16_output)\n \n-    all_f8_vals_tensor = torch.arange(0, 256, device='cuda', dtype=torch.uint8)\n+    all_f8_vals_tensor = torch.tensor(range(2 ** 8), dtype=torch.uint8, device='cuda')\n     all_f8_vals = triton.reinterpret(all_f8_vals_tensor, tl.float8)\n-    all_f8_vals_in_xf16 = torch.empty_like(all_f8_vals_tensor, dtype=dtype)\n-    copy_kernel[grid](all_f8_vals, all_f8_vals_in_xf16, n_elements=256, BLOCK_SIZE=1024)\n+    all_f8_vals_in_f16 = torch.empty_like(all_f8_vals_tensor, dtype=torch.float16)\n+    copy_kernel[grid](all_f8_vals, all_f8_vals_in_f16, n_elements=256, BLOCK_SIZE=1024)\n \n-    all_finite_f8_vals_in_xf16 = all_f8_vals_in_xf16[\n-        torch.isfinite(all_f8_vals_in_xf16)\n+    all_finite_f8_vals_in_f16 = all_f8_vals_in_f16[\n+        torch.isfinite(all_f8_vals_in_f16)\n     ]\n \n     min_error = torch.min(\n         torch.abs(\n-            xf16_input.reshape((-1, 1))\n-            - all_finite_f8_vals_in_xf16.reshape((1, -1))\n+            f16_input.reshape((-1, 1))\n+            - all_finite_f8_vals_in_f16.reshape((1, -1))\n         ),\n         dim=1,\n     )[0]\n     # 1.9375 is float8 max\n     mismatch = torch.logical_and(\n-        abs_error != min_error, torch.logical_and(torch.isfinite(xf16_input),\n-                                                  torch.abs(xf16_input) < 1.9375)\n+        abs_error != min_error, torch.logical_and(torch.isfinite(f16_input), torch.abs(f16_input) < 1.9375)\n     )\n     assert torch.all(\n         torch.logical_not(mismatch)\n-    ), f\"xf16_input[mismatch]={xf16_input[mismatch]} xf16_output[mismatch]={xf16_output[mismatch]} abs_error[mismatch]={abs_error[mismatch]} min_error[mismatch]={min_error[mismatch]}\"\n+    ), f\"f16_input[mismatch]={f16_input[mismatch]} f16_output[mismatch]={f16_output[mismatch]} abs_error[mismatch]={abs_error[mismatch]} min_error[mismatch]={min_error[mismatch]}\"\n \n \n # ---------------"}]