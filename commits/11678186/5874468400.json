[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 7, "deletions": 2, "changes": 9, "file_content_changes": "@@ -52,6 +52,7 @@ jobs:\n           echo \"BACKEND=CUDA\" >> \"${GITHUB_ENV}\"\n           echo \"ENABLE_TMA=0\" >> \"${GITHUB_ENV}\"\n           echo \"ENABLE_MMA_V3=0\" >> \"${GITHUB_ENV}\"\n+          echo \"TRITON_DISABLE_LINE_INFO=1\" >> \"${GITHUB_ENV}\"\n \n       - name: Clear cache\n         run: |\n@@ -91,17 +92,21 @@ jobs:\n         if: ${{ env.BACKEND == 'CUDA' && env.ENABLE_TMA == '1' && env.ENABLE_MMA_V3 == '1'}}\n         run: |\n           cd python/test/unit\n-          python3 -m pytest -n 8 --ignore=runtime --ignore=operators\n+          python3 -m pytest -n 8 --ignore=runtime --ignore=operators --ignore=language/test_line_info.py\n           # run runtime tests serially to avoid race condition with cache handling.\n           python3 -m pytest runtime/\n+          # run test_line_info.py separately with TRITON_DISABLE_LINE_INFO=0\n+          TRITON_DISABLE_LINE_INFO=0 python3 -m pytest language/test_line_info.py\n \n       - name: Run python tests on CUDA with ENABLE_TMA=0 and ENABLE_MMA_V3=0\n         if: ${{ env.BACKEND == 'CUDA' && env.ENABLE_TMA == '0' && env.ENABLE_MMA_V3 == '0'}}\n         run: |\n           cd python/test/unit\n-          python3 -m pytest -n 8 --ignore=runtime --ignore=hopper --ignore=operators\n+          python3 -m pytest -n 8 --ignore=runtime --ignore=hopper --ignore=operators --ignore=language/test_line_info.py\n           # run runtime tests serially to avoid race condition with cache handling.\n           python3 -m pytest runtime/\n+          # run test_line_info.py separately with TRITON_DISABLE_LINE_INFO=0\n+          TRITON_DISABLE_LINE_INFO=0 python3 -m pytest language/test_line_info.py\n \n       - name: Clear cache\n         run: |"}, {"filename": "docs/meetups/08-22-2023.md", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -5,5 +5,8 @@\n \n ##### Items:\n 1. H100 updates\n-2. Linalg updates\n-3. Open discussion\n+2. Triton release plan update\n+3. Linalg updates\n+4. Intel GPU Backend status update.\n+2. Intel working on the CPU backend for Triton.\n+4. Open discussion"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/Passes.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -27,7 +27,7 @@ def ConvertTritonGPUToLLVM : Pass<\"convert-triton-gpu-to-llvm\", \"mlir::ModuleOp\"\n         Option<\"computeCapability\", \"compute-capability\",\n                \"int32_t\", /*default*/\"80\",\n                \"device compute capability\">,\n-        Option<\"TmaMetadata\", \"tma-metadata\",\n+        Option<\"tmaMetadata\", \"tma-metadata\",\n                \"mlir::triton::gpu::TMAMetadataTy*\", /*default*/\"nullptr\",\n                \"tma metadata to the runtime\">,\n         Option<\"isROCM\", \"is-rocm\","}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -14,10 +14,12 @@ template <typename T> class OperationPass;\n \n namespace triton {\n \n-std::unique_ptr<OperationPass<ModuleOp>> createConvertTritonGPUToLLVMPass(\n-    int computeCapability = 80,\n-    mlir::triton::gpu::TMAMetadataTy *tmaMetadata = nullptr,\n-    bool isROCM = false);\n+#define GEN_PASS_DECL\n+#include \"triton/Conversion/TritonGPUToLLVM/Passes.h.inc\"\n+\n+std::unique_ptr<OperationPass<ModuleOp>> createConvertTritonGPUToLLVMPass();\n+std::unique_ptr<OperationPass<ModuleOp>>\n+createConvertTritonGPUToLLVMPass(const ConvertTritonGPUToLLVMOptions &options);\n \n } // namespace triton\n "}, {"filename": "include/triton/Tools/Sys/GetEnv.hpp", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -24,7 +24,9 @@\n \n #include <algorithm>\n #include <cstdlib>\n+#include <set>\n #include <string>\n+\n namespace triton {\n \n const std::set<std::string> ENV_VARS = {"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 1, "deletions": 5, "changes": 6, "file_content_changes": "@@ -667,14 +667,10 @@ class SelectOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n   AxisInfo\n   getAxisInfo(OpTy op,\n               ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n-    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n-    if (!resTy)\n-      return AxisInfo();\n-    auto shape = resTy.getShape();\n-    auto rank = shape.size();\n     auto condConstancy = operands[0]->getValue().getConstancy();\n     auto lhsInfo = operands[1]->getValue();\n     auto rhsInfo = operands[2]->getValue();\n+    auto rank = lhsInfo.getRank();\n \n     AxisInfo::DimVectorT contiguity, divisibility, constancy;\n     std::optional<int64_t> constantValue;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 36, "deletions": 24, "changes": 60, "file_content_changes": "@@ -19,7 +19,7 @@ using ::mlir::triton::gpu::SharedEncodingAttr;\n // Data loader for mma.16816 instruction.\n class MMA16816SmemLoader {\n public:\n-  MMA16816SmemLoader(int warpsPerTile, ArrayRef<uint32_t> order,\n+  MMA16816SmemLoader(int nPerWarp, int warpsPerTile, ArrayRef<uint32_t> order,\n                      ArrayRef<uint32_t> warpsPerCTA, uint32_t kOrder,\n                      int kWidth, ArrayRef<Value> smemStrides,\n                      ArrayRef<int64_t> tileShape, ArrayRef<int> instrShape,\n@@ -93,6 +93,8 @@ class MMA16816SmemLoader {\n   int inWarpMatOffset;\n   // Offset in number of matrices to increment on non-k dim across warps\n   int warpMatOffset;\n+\n+  int nPerWarp;\n };\n \n SmallVector<Value>\n@@ -131,10 +133,18 @@ MMA16816SmemLoader::computeLdmatrixMatOffs(Value warpId, Value lane,\n   // address (s0,s1) annotates.\n \n   Value matOff[2];\n-  matOff[kOrder ^ 1] = add(\n-      mul(warpId, i32_val(warpMatOffset)), // warp offset (kOrder=1)\n-      mul(nkMatArr,\n-          i32_val(inWarpMatOffset))); // matrix offset inside a warp (kOrder=1)\n+  // When B's shape(k, n) is (16, 8) and ldmatrix.x4 is used, the shared memory\n+  // access will be out of bound. In the future we should change this case to\n+  // ldmatrix.x2\n+  if (kOrder == 0 && nPerWarp == 8) {\n+    matOff[kOrder ^ 1] = mul(warpId, i32_val(warpMatOffset));\n+  } else {\n+    matOff[kOrder ^ 1] = add(\n+        mul(warpId, i32_val(warpMatOffset)), // warp offset (kOrder=1)\n+        mul(nkMatArr,\n+            i32_val(\n+                inWarpMatOffset))); // matrix offset inside a warp (kOrder=1)\n+  }\n   matOff[kOrder] = kMatArr;\n \n   // Physical offset (before swizzling)\n@@ -390,13 +400,13 @@ MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> ptrs, Type matTy,\n }\n \n MMA16816SmemLoader::MMA16816SmemLoader(\n-    int warpsPerTile, ArrayRef<uint32_t> order, ArrayRef<uint32_t> warpsPerCTA,\n-    uint32_t kOrder, int kWidth, ArrayRef<Value> smemStrides,\n-    ArrayRef<int64_t> tileShape, ArrayRef<int> instrShape,\n-    ArrayRef<int> matShape, int perPhase, int maxPhase, int elemBytes,\n-    ConversionPatternRewriter &rewriter,\n+    int nPerWarp, int warpsPerTile, ArrayRef<uint32_t> order,\n+    ArrayRef<uint32_t> warpsPerCTA, uint32_t kOrder, int kWidth,\n+    ArrayRef<Value> smemStrides, ArrayRef<int64_t> tileShape,\n+    ArrayRef<int> instrShape, ArrayRef<int> matShape, int perPhase,\n+    int maxPhase, int elemBytes, ConversionPatternRewriter &rewriter,\n     TritonGPUToLLVMTypeConverter *typeConverter, const Location &loc)\n-    : order(order.begin(), order.end()),\n+    : nPerWarp(nPerWarp), order(order.begin(), order.end()),\n       warpsPerCTA(warpsPerCTA.begin(), warpsPerCTA.end()), kOrder(kOrder),\n       kWidth(kWidth), tileShape(tileShape.begin(), tileShape.end()),\n       instrShape(instrShape.begin(), instrShape.end()),\n@@ -490,6 +500,7 @@ std::function<void(int, int)> getLoadMatrixFn(\n     bool isA, TritonGPUToLLVMTypeConverter *typeConverter,\n     ConversionPatternRewriter &rewriter, Location loc) {\n   auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto shapePerCTA = getShapePerCTA(tensorTy);\n   Type eltTy = tensorTy.getElementType();\n   // We assumes that the input operand of Dot should be from shared layout.\n   // TODO(Superjomn) Consider other layouts if needed later.\n@@ -511,13 +522,16 @@ std::function<void(int, int)> getLoadMatrixFn(\n   if (kWidth != (4 / elemBytes))\n     assert(vecPhase == 1 || vecPhase == 4 * kWidth);\n \n+  int nPerWarp =\n+      std::max<int>(shapePerCTA[1] / mmaLayout.getWarpsPerCTA()[1], 8);\n+\n   // (a, b) is the coordinate.\n   auto load = [=, &rewriter, &vals](int a, int b) {\n-    MMA16816SmemLoader loader(\n-        warpsPerTile, sharedLayout.getOrder(), mmaLayout.getWarpsPerCTA(),\n-        kOrder, kWidth, smemObj.strides, tensorTy.getShape() /*tileShape*/,\n-        instrShape, matShape, perPhase, maxPhase, elemBytes, rewriter,\n-        typeConverter, loc);\n+    MMA16816SmemLoader loader(nPerWarp, warpsPerTile, sharedLayout.getOrder(),\n+                              mmaLayout.getWarpsPerCTA(), kOrder, kWidth,\n+                              smemObj.strides, shapePerCTA /*tileShape*/,\n+                              instrShape, matShape, perPhase, maxPhase,\n+                              elemBytes, rewriter, typeConverter, loc);\n     // Offset of a slice within the original tensor in shared memory\n     Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n     SmallVector<Value> offs =\n@@ -559,17 +573,15 @@ Value loadArg(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n               TritonGPUToLLVMTypeConverter *typeConverter, Value thread,\n               bool isA) {\n   auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto shapePerCTA = getShapePerCTA(tensorTy);\n   int bitwidth = tensorTy.getElementTypeBitWidth();\n   auto mmaLayout = encoding.getParent().cast<MmaEncodingAttr>();\n \n-  SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n-                             tensorTy.getShape().end());\n-\n   ValueTable vals;\n   int mmaInstrM = 16, mmaInstrN = 8, mmaInstrK = 4 * 64 / bitwidth;\n   int matShapeM = 8, matShapeN = 8, matShapeK = 2 * 64 / bitwidth;\n \n-  auto numRep = encoding.getMMAv2Rep(tensorTy.getShape(), bitwidth);\n+  auto numRep = encoding.getMMAv2Rep(shapePerCTA, bitwidth);\n   int kWidth = encoding.getMMAv2kWidth();\n \n   auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n@@ -579,14 +591,14 @@ Value loadArg(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n \n   SmallVector<Value> multiDimWarpId =\n       delinearize(rewriter, loc, warp, warpsPerCTA, order);\n-  Value warpM = urem(multiDimWarpId[0], i32_val(shape[0] / 16));\n-  Value warpN = urem(multiDimWarpId[1], i32_val(shape[1] / 8));\n+  Value warpM = urem(multiDimWarpId[0], i32_val(shapePerCTA[0] / 16));\n+  Value warpN = urem(multiDimWarpId[1], i32_val(shapePerCTA[1] / 8));\n \n   int warpsPerTile;\n   if (isA)\n-    warpsPerTile = std::min<int>(warpsPerCTA[0], shape[0] / 16);\n+    warpsPerTile = std::min<int>(warpsPerCTA[0], shapePerCTA[0] / 16);\n   else\n-    warpsPerTile = std::min<int>(warpsPerCTA[1], shape[1] / 16);\n+    warpsPerTile = std::min<int>(warpsPerCTA[1], shapePerCTA[1] / 16);\n \n   std::function<void(int, int)> loadFn;\n   if (isA)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 22, "deletions": 0, "changes": 22, "file_content_changes": "@@ -569,6 +569,28 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n         mask = and_(mask, icmp_slt(mul(threadDim, i32_val(sizePerThread[dim])),\n                                    i32_val(shape[dim])));\n       }\n+      // Do not write duplicated data when multicast is enabled\n+      if (triton::gpu::getNumCTAs(layout) > 1) {\n+        auto _0 = i32_val(0);\n+        auto CTAsPerCGA = triton::gpu::getCTAsPerCGA(layout);\n+        auto CTASplitNum = triton::gpu::getCTASplitNum(layout);\n+        auto CTAOrder = triton::gpu::getCTAOrder(layout);\n+\n+        auto clusterCTAId = getClusterCTAId(rewriter, loc);\n+        auto multiDimClusterCTAId =\n+            delinearize(rewriter, loc, clusterCTAId, CTAsPerCGA, CTAOrder);\n+\n+        for (unsigned dim = 0; dim < rank; ++dim) {\n+          // Skip when multicast is not enabled in this dimension\n+          if (CTAsPerCGA[dim] == CTASplitNum[dim])\n+            continue;\n+          // This wrapping rule must be consistent with emitCTAOffsetForLayout\n+          unsigned splitNum = std::min<unsigned>(shape[dim], CTASplitNum[dim]);\n+          multiDimClusterCTAId[dim] =\n+              urem(multiDimClusterCTAId[dim], i32_val(splitNum));\n+          mask = and_(mask, icmp_eq(multiDimClusterCTAId[dim], _0));\n+        }\n+      }\n     } else {\n       // If the tensor is not ranked, then it is a scalar and only thread 0 can\n       // write"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 16, "deletions": 21, "changes": 37, "file_content_changes": "@@ -40,13 +40,17 @@\n \n #include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n \n+namespace mlir {\n+namespace triton {\n+#define GEN_PASS_DEF_CONVERTTRITONGPUTOLLVM\n+#include \"triton/Conversion/TritonGPUToLLVM/Passes.h.inc\"\n+} // namespace triton\n+} // namespace mlir\n+\n using namespace mlir;\n using namespace mlir::triton;\n namespace ttng = mlir::triton::nvidia_gpu;\n \n-#define GEN_PASS_CLASSES\n-#include \"triton/Conversion/TritonGPUToLLVM/Passes.h.inc\"\n-\n namespace {\n \n // pass ws related named attrs.\n@@ -372,15 +376,10 @@ class TritonLLVMConversionTarget : public ConversionTarget {\n   }\n };\n \n-class ConvertTritonGPUToLLVM\n-    : public ConvertTritonGPUToLLVMBase<ConvertTritonGPUToLLVM> {\n-\n-public:\n-  explicit ConvertTritonGPUToLLVM(int computeCapability,\n-                                  mlir::triton::gpu::TMAMetadataTy *tmaMetadata,\n-                                  bool isROCM)\n-      : computeCapability(computeCapability), tmaMetadata(tmaMetadata),\n-        isROCM(isROCM) {}\n+struct ConvertTritonGPUToLLVM\n+    : public triton::impl::ConvertTritonGPUToLLVMBase<ConvertTritonGPUToLLVM> {\n+  using ConvertTritonGPUToLLVMBase<\n+      ConvertTritonGPUToLLVM>::ConvertTritonGPUToLLVMBase;\n \n   void runOnOperation() override {\n     MLIRContext *context = &getContext();\n@@ -569,10 +568,6 @@ class ConvertTritonGPUToLLVM\n            CacheKeyDenseMapInfo>\n       indexCache;\n \n-  int computeCapability{};\n-  bool isROCM{};\n-  mlir::triton::gpu::TMAMetadataTy *tmaMetadata;\n-\n   void initSharedMemory(ModuleAllocation &allocation,\n                         TritonGPUToLLVMTypeConverter &typeConverter) {\n     ModuleOp mod = getOperation();\n@@ -862,12 +857,12 @@ class ConvertTritonGPUToLLVM\n namespace mlir {\n namespace triton {\n \n+std::unique_ptr<OperationPass<ModuleOp>> createConvertTritonGPUToLLVMPass() {\n+  return std::make_unique<ConvertTritonGPUToLLVM>();\n+}\n std::unique_ptr<OperationPass<ModuleOp>>\n-createConvertTritonGPUToLLVMPass(int computeCapability,\n-                                 mlir::triton::gpu::TMAMetadataTy *tmaMetadata,\n-                                 bool isROCM) {\n-  return std::make_unique<::ConvertTritonGPUToLLVM>(computeCapability,\n-                                                    tmaMetadata, isROCM);\n+createConvertTritonGPUToLLVMPass(const ConvertTritonGPUToLLVMOptions &options) {\n+  return std::make_unique<ConvertTritonGPUToLLVM>(options);\n }\n \n } // namespace triton"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 9, "deletions": 6, "changes": 15, "file_content_changes": "@@ -849,11 +849,13 @@ DotOperandEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape,\n \n unsigned DotOperandEncodingAttr::getTotalElemsPerThread(ArrayRef<int64_t> shape,\n                                                         Type eltTy) const {\n+  auto shapePerCTA = getShapePerCTA(*this, shape);\n   if (auto mmaParent = getParent().dyn_cast<MmaEncodingAttr>()) {\n     int warpsPerCTAM = mmaParent.getWarpsPerCTA()[0];\n     int warpsPerCTAN = mmaParent.getWarpsPerCTA()[1];\n+    // A100\n     if (mmaParent.isAmpere()) {\n-      auto rep = getMMAv2Rep(shape, eltTy.getIntOrFloatBitWidth());\n+      auto rep = getMMAv2Rep(shapePerCTA, eltTy.getIntOrFloatBitWidth());\n       if (getOpIdx() == 0)\n         return 4 * rep[0] * rep[1];\n       if (getOpIdx() == 1)\n@@ -925,8 +927,8 @@ unsigned DotOperandEncodingAttr::getTotalElemsPerThread(ArrayRef<int64_t> shape,\n     auto order = blockedLayout.getOrder();\n     auto sizePerThread = getSizePerThread(blockedLayout);\n \n-    int K = getOpIdx() == 0 ? shape[1] : shape[0];\n-    int otherDim = getOpIdx() == 1 ? shape[1] : shape[0];\n+    int K = getOpIdx() == 0 ? shapePerCTA[1] : shapePerCTA[0];\n+    int otherDim = getOpIdx() == 1 ? shapePerCTA[1] : shapePerCTA[0];\n \n     bool isM = getOpIdx() == 0;\n \n@@ -1460,9 +1462,10 @@ struct TritonGPUInferLayoutInterface\n     return success();\n   }\n \n-  LogicalResult inferDotOpEncoding(Attribute operandEncoding, unsigned opIdx,\n-                                   Attribute retEncoding,\n-                                   Optional<Location> location) const override {\n+  LogicalResult\n+  inferDotOpEncoding(Attribute operandEncoding, unsigned opIdx,\n+                     Attribute retEncoding,\n+                     std::optional<Location> location) const override {\n     auto mmaRetEncoding = retEncoding.dyn_cast<MmaEncodingAttr>();\n     if (mmaRetEncoding && mmaRetEncoding.isHopper()) {\n       // TODO: support gmma when A/B does not reside in shared memory"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/PlanCTA.cpp", "status": "modified", "additions": 11, "deletions": 1, "changes": 12, "file_content_changes": "@@ -325,8 +325,18 @@ bool CTAPlanner::processReduce(triton::FuncOp &funcOp) {\n       }\n     }\n \n+    llvm::SmallVector<unsigned> CTASplitNum = CTAsPerCGA;\n+\n+    // If numCTAs > 1 and the only dimension is the reduced dimension, after the\n+    // above two for-loops, CTAsPerCGA = [0] and remainingCTAs = numCTAs. We set\n+    // CTAsPerCGA[0] = numCTAs and keep CTASplitNum[0] = 1 to ensure that no\n+    // cross-CTA reduction is required, although this will introduce duplicated\n+    // calculation\n+    if (remainingCTAs > 0)\n+      CTAsPerCGA[order[rank - 1]] *= remainingCTAs;\n+\n     auto CTALayout =\n-        ttg::CTALayoutAttr::get(context, CTAsPerCGA, CTAsPerCGA, CTAOrder);\n+        ttg::CTALayoutAttr::get(context, CTAsPerCGA, CTASplitNum, CTAOrder);\n     if (!tiled)\n       setTiling(CTALayout.getCTAsPerCGA());\n     auto newSrcLayout = replaceCTALayout(srcLayout, srcShape, CTALayout);"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/WSMaterialization.cpp", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -30,6 +30,8 @@\n #include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n \n+#include <set>\n+\n using namespace mlir;\n namespace ttg = triton::gpu;\n namespace ttng = triton::nvidia_gpu;"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -351,7 +351,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n   pm.addPass(mlir::createConvertSCFToCFPass());\n   pm.addPass(mlir::createConvertIndexToLLVMPass());\n   pm.addPass(\n-      createConvertTritonGPUToLLVMPass(computeCapability, &tmaInfos, isROCM));\n+      createConvertTritonGPUToLLVMPass({computeCapability, &tmaInfos, isROCM}));\n   pm.addPass(createConvertNVGPUToLLVMPass());\n   pm.addPass(mlir::createArithToLLVMConversionPass());\n   pm.addPass(mlir::createCanonicalizerPass());"}, {"filename": "python/setup.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -335,6 +335,7 @@ def build_extension(self, ext):\n             \"matplotlib\",\n             \"pandas\",\n             \"tabulate\",\n+            \"torch\",\n         ],\n     },\n )"}, {"filename": "python/test/kernel_comparison/kernels.yml", "status": "modified", "additions": 14, "deletions": 12, "changes": 26, "file_content_changes": "@@ -1,31 +1,33 @@\n name_and_extension:\n-  - name: _kernel_0d1d2d34567c89c1011c\n+  - name: _kernel_0d1d2d3de4de5de6c7de8de9c10de11c\n     extension: ptx\n-  - name: _kernel_0d1d2d3d4d5d6d7c8d9c10d11c\n+  - name: _kernel_0d1d2d3de4de5de6de7c8de9c10de11c\n     extension: ptx\n-  - name: _kernel_0d1d2d3d4d5d6d7c8c9d10d11c\n+  - name: _kernel_0d1d2d345de6c789c1011c\n     extension: ptx\n   - name: _kernel_0d1d2d3456c789c1011c\n     extension: ptx\n-  - name: _kernel_0d1d2d345d6d7c8c9d1011c\n+  - name: _kernel_0d1d2d3de4de5de6c7de8c9de10de11c\n     extension: ptx\n   - name: _kernel_0d1d2d34567c8c91011c\n     extension: ptx\n   - name: _kernel_0d1d2d3456c78c91011c\n     extension: ptx\n-  - name: _kernel_0d1d2d345d6c78c9d1011c\n+  - name: _kernel_0d1d2d3de4de5de6de7c8c9de10de11c\n+    extension: ptx\n+  - name: _kernel_0d1d2d34567c89c1011c\n     extension: ptx\n-  - name: _kernel_0d1d2d345d6c789c1011c\n+  - name: _kernel_0d1d2d345de6de7c89c1011c\n     extension: ptx\n-  - name: _kernel_0d1d2d3d4d5d6c7d8d9c10d11c\n+  - name: _kernel_0d1d2d345de6de7c8c9de1011c\n     extension: ptx\n-  - name: _kernel_0d1d2d3d4d5d6c7d8c9d10d11c\n+  - name: kernel_0d1d2de\n     extension: ptx\n-  - name: _kernel_0d1d2d345d6d7c89c1011c\n+  - name: _kernel_0d1d2d345de6c78c9de1011c\n     extension: ptx\n-  - name: _bwd_kernel_0d1d2d34d5d6d7d8d9d10d11d12d13d14d15d16c17d18d19d20c21d22d23d24c2526d27d\n+  - name: _bwd_kernel_0d1d2d34d5d6d7d8d9d10d11de12de13de14de15c16de17de18de19c20de21de22de23c2425de26de\n     extension: ptx\n-  - name: _fwd_kernel_0d1d2d34d5d6d7d8d9d10c11d12d13d14c15d16d17d18c19d20d21d22c2324d25d\n+  - name: _fwd_kernel_0d1d2d34d5d6de7de8de9c10de11de12de13c14de15de16de17c18de19de20de21c2223de24de\n     extension: ptx\n-  - name: _bwd_preprocess_0d1d2d3d4d\n+  - name: _bwd_preprocess_0d1d2d\n     extension: ptx"}]