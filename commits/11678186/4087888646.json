[{"filename": "CMakeLists.txt", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -72,6 +72,8 @@ if (NOT MLIR_DIR)\n       find_package(LLVM 11 REQUIRED COMPONENTS \"nvptx;amdgpu\")\n     endif()\n     message(STATUS \"Found LLVM ${LLVM_PACKAGE_VERSION}\")\n+    # FindLLVM outputs LLVM_LIBRARY_DIRS but we expect LLVM_LIBRARY_DIR here\n+    set(LLVM_LIBRARY_DIR ${LLVM_LIBRARY_DIRS})\n     if(APPLE)\n       set(CMAKE_OSX_DEPLOYMENT_TARGET \"10.14\")\n     endif()"}, {"filename": "cmake/FindLLVM.cmake", "status": "modified", "additions": 5, "deletions": 7, "changes": 12, "file_content_changes": "@@ -33,14 +33,12 @@\n # We also want an user-specified LLVM_ROOT_DIR to take precedence over the\n # system default locations such as /usr/local/bin. Executing find_program()\n # multiples times is the approach recommended in the docs.\n-set(llvm_config_names llvm-config-12.0 llvm-config120 llvm-config-12 llvm-config-12-64\n-                      llvm-config-11.0 llvm-config110 llvm-config-11 llvm-config-11-64\n-                      llvm-config-10.0 llvm-config100 llvm-config-10 llvm-config-10-64\n-                      llvm-config-9.0 llvm-config90 llvm-config-9 llvm-config-9-64\n-                      llvm-config-8.0 llvm-config80 llvm-config-8 llvm-config-8-64\n-                      llvm-config-7.0 llvm-config70 llvm-config-7 llvm-config-7-64\n-                      llvm-config-6.0 llvm-config60\n+set(llvm_config_names llvm-config-6.0 llvm-config60\n                       llvm-config)\n+foreach(v RANGE 7 17)\n+    # names like llvm-config-7.0 llvm-config70 llvm-config-7 llvm-config-7-64\n+    list(PREPEND llvm_config_names llvm-config-${v}.0 llvm-config${v}0 llvm-config-${v} llvm-config-${v}-64)\n+endforeach()\n find_program(LLVM_CONFIG\n     NAMES ${llvm_config_names}\n     PATHS ${LLVM_ROOT_DIR}/bin NO_DEFAULT_PATH"}, {"filename": "include/triton/Analysis/AxisInfo.h", "status": "modified", "additions": 152, "deletions": 32, "changes": 184, "file_content_changes": "@@ -3,52 +3,62 @@\n \n #include \"mlir/Analysis/DataFlowAnalysis.h\"\n #include \"llvm/Support/raw_ostream.h\"\n-#include <iostream>\n \n+#include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n \n+#include <optional>\n+#include <type_traits>\n+\n namespace mlir {\n \n //===----------------------------------------------------------------------===//\n // AxisInfo\n //===----------------------------------------------------------------------===//\n \n /// This lattice value represents known information on the axes of a lattice.\n-/// Axis information is represented by a std::map<int, int>\n class AxisInfo {\n public:\n-  typedef SmallVector<int, 4> DimVectorT;\n+  typedef SmallVector<int64_t, 4> DimVectorT;\n \n public:\n-  // Default constructor\n+  /// Default constructor\n   AxisInfo() : AxisInfo({}, {}, {}) {}\n-  // Construct contiguity info with known contiguity\n+  /// Construct contiguity info with known contiguity\n   AxisInfo(DimVectorT knownContiguity, DimVectorT knownDivisibility,\n            DimVectorT knownConstancy)\n+      : AxisInfo(knownContiguity, knownDivisibility, knownConstancy, {}) {}\n+  AxisInfo(DimVectorT knownContiguity, DimVectorT knownDivisibility,\n+           DimVectorT knownConstancy, std::optional<int64_t> knownConstantValue)\n       : contiguity(knownContiguity), divisibility(knownDivisibility),\n-        constancy(knownConstancy), rank(contiguity.size()) {\n-    assert(knownDivisibility.size() == (size_t)rank);\n-    assert(knownConstancy.size() == (size_t)rank);\n+        constancy(knownConstancy), constantValue(knownConstantValue),\n+        rank(contiguity.size()) {\n+    assert(knownContiguity.size() == static_cast<size_t>(rank));\n+    assert(knownDivisibility.size() == static_cast<size_t>(rank));\n+    assert(knownConstancy.size() == static_cast<size_t>(rank));\n   }\n \n-  // Accessors\n-  int getContiguity(size_t d) const { return contiguity[d]; }\n+  /// Accessors\n+  int64_t getContiguity(size_t dim) const { return contiguity[dim]; }\n   const DimVectorT &getContiguity() const { return contiguity; }\n \n-  int getDivisibility(size_t d) const { return divisibility[d]; }\n+  int64_t getDivisibility(size_t dim) const { return divisibility[dim]; }\n   const DimVectorT &getDivisibility() const { return divisibility; }\n \n-  int getConstancy(size_t d) const { return constancy[d]; }\n+  int64_t getConstancy(size_t dim) const { return constancy[dim]; }\n   const DimVectorT &getConstancy() const { return constancy; }\n \n   int getRank() const { return rank; }\n \n-  // Comparison\n+  std::optional<int64_t> getConstantValue() const { return constantValue; }\n+\n+  /// Comparison\n   bool operator==(const AxisInfo &other) const {\n     return (contiguity == other.contiguity) &&\n            (divisibility == other.divisibility) &&\n-           (constancy == other.constancy);\n+           (constancy == other.constancy) &&\n+           (constantValue == other.constantValue) && (rank == other.rank);\n   }\n \n   /// The pessimistic value state of the contiguity is unknown.\n@@ -57,13 +67,18 @@ class AxisInfo {\n   }\n   static AxisInfo getPessimisticValueState(Value value);\n \n-  // The gcd of both arguments for each dimension\n+  /// The gcd of both arguments for each dimension\n   static AxisInfo join(const AxisInfo &lhs, const AxisInfo &rhs);\n \n private:\n   /// The _contiguity_ information maps the `d`-th\n   /// dimension to the length of the shortest\n-  /// sequence of contiguous integers along it\n+  /// sequence of contiguous integers along it.\n+  /// Suppose we have an array of N elements,\n+  /// with a contiguity value C,\n+  /// the array can be divided into a list of\n+  /// N/C sequences of C contiguous elements.\n+  /// Since we have N = 2^k, C must be a power of two.\n   /// For example:\n   /// [10, 11, 12, 13, 18, 19, 20, 21]\n   /// [20, 21, 22, 23, 28, 29, 30, 31]\n@@ -97,42 +112,147 @@ class AxisInfo {\n   /// dimension to the length of the shortest\n   /// sequence of constant integer along it. This is\n   /// particularly useful to infer the contiguity\n-  /// of operations (e.g., add) involving a constant\n+  /// of operations (e.g., add) involving a constant.\n+  /// Suppose we have an array of N elements,\n+  /// with a constancy value C,\n+  /// the array can be divided into a list of\n+  /// N/C sequences of C elements with the same value.\n+  /// Since we have N = 2^k, C must be a power of two.\n   /// For example\n   /// [8, 8, 8, 8, 12, 12, 12, 12]\n   /// [16, 16, 16, 16, 20, 20, 20, 20]\n   /// would have constancy [1, 4]\n   DimVectorT constancy;\n \n+  /// The constant value of the lattice if we can infer it.\n+  std::optional<int64_t> constantValue;\n+\n   // number of dimensions of the lattice\n-  int rank;\n+  int rank{};\n };\n \n-class AxisInfoAnalysis : public ForwardDataFlowAnalysis<AxisInfo> {\n+class AxisInfoVisitor {\n+public:\n+  AxisInfoVisitor() = default;\n+  virtual ~AxisInfoVisitor() = default;\n \n-private:\n-  static const int maxPow2Divisor = 65536;\n+  static bool isContiguousDim(const AxisInfo &info, ArrayRef<int64_t> shape,\n+                              int dim) {\n+    return info.getContiguity(dim) == shape[dim];\n+  }\n+\n+  static bool isConstantDim(const AxisInfo &info, ArrayRef<int64_t> shape,\n+                            int dim) {\n+    return info.getConstancy(dim) == shape[dim];\n+  }\n+\n+  virtual AxisInfo\n+  getAxisInfo(Operation *op, ArrayRef<LatticeElement<AxisInfo> *> operands) = 0;\n \n-  int highestPowOf2Divisor(int n) {\n-    if (n == 0)\n-      return maxPow2Divisor;\n-    return (n & (~(n - 1)));\n+  virtual bool match(Operation *op) = 0;\n+};\n+\n+/// Base class for all operations\n+template <typename OpTy> class AxisInfoVisitorImpl : public AxisInfoVisitor {\n+public:\n+  using AxisInfoVisitor::AxisInfoVisitor;\n+\n+  AxisInfo getAxisInfo(Operation *op,\n+                       ArrayRef<LatticeElement<AxisInfo> *> operands) final {\n+    return getAxisInfo(cast<OpTy>(op), operands);\n   }\n \n-  AxisInfo visitBinaryOp(\n-      Operation *op, AxisInfo lhsInfo, AxisInfo rhsInfo,\n-      const std::function<int(AxisInfo, AxisInfo, int)> &getContiguity,\n-      const std::function<int(AxisInfo, AxisInfo, int)> &getDivisibility,\n-      const std::function<int(AxisInfo, AxisInfo, int)> &getConstancy);\n+  bool match(Operation *op) final { return isa<OpTy>(op); }\n+\n+  virtual AxisInfo getAxisInfo(OpTy op,\n+                               ArrayRef<LatticeElement<AxisInfo> *> operands) {\n+    llvm_unreachable(\"Unimplemented getAxisInfo\");\n+  }\n+};\n+\n+/// Binary operations\n+template <typename OpTy>\n+class BinaryOpVisitorImpl : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n+\n+  AxisInfo getAxisInfo(OpTy op,\n+                       ArrayRef<LatticeElement<AxisInfo> *> operands) override {\n+    auto lhsInfo = operands[0]->getValue();\n+    auto rhsInfo = operands[1]->getValue();\n+    auto rank = lhsInfo.getRank();\n+    assert(operands.size() == 2 && \"Expected two operands\");\n+    AxisInfo::DimVectorT contiguity;\n+    AxisInfo::DimVectorT divisibility;\n+    AxisInfo::DimVectorT constancy;\n+    auto constantValue = getConstantValue(op, lhsInfo, rhsInfo);\n+    for (auto d = 0; d < rank; ++d) {\n+      if (constantValue.has_value()) {\n+        contiguity.push_back(1);\n+        constancy.push_back(\n+            std::max(lhsInfo.getConstancy(d), rhsInfo.getConstancy(d)));\n+        divisibility.push_back(highestPowOf2Divisor(constantValue.value()));\n+      } else {\n+        contiguity.push_back(getContiguity(op, lhsInfo, rhsInfo, d));\n+        constancy.push_back(getConstancy(op, lhsInfo, rhsInfo, d));\n+        divisibility.push_back(getDivisibility(op, lhsInfo, rhsInfo, d));\n+      }\n+    }\n+    return AxisInfo(contiguity, divisibility, constancy, constantValue);\n+  }\n+\n+protected:\n+  virtual int64_t getContiguity(OpTy op, const AxisInfo &lhs,\n+                                const AxisInfo &rhs, int dim) {\n+    return 1;\n+  }\n+\n+  virtual int64_t getDivisibility(OpTy op, const AxisInfo &lhs,\n+                                  const AxisInfo &rhs, int dim) {\n+    return 1;\n+  }\n+\n+  virtual int64_t getConstancy(OpTy op, const AxisInfo &lhs,\n+                               const AxisInfo &rhs, int dim) {\n+    return 1;\n+  }\n+\n+  virtual std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                                  const AxisInfo &rhs) {\n+    return {};\n+  }\n+};\n+\n+class AxisInfoVisitorList {\n+public:\n+  template <typename... Ts, typename = std::enable_if_t<sizeof...(Ts) != 0>>\n+  void append() {\n+    (visitors.emplace_back(std::make_unique<Ts>()), ...);\n+  }\n+\n+  AxisInfo apply(Operation *op, ArrayRef<LatticeElement<AxisInfo> *> operands) {\n+    for (auto &visitor : visitors)\n+      if (visitor->match(op))\n+        return visitor->getAxisInfo(op, operands);\n+    return AxisInfo();\n+  }\n+\n+private:\n+  std::vector<std::unique_ptr<AxisInfoVisitor>> visitors;\n+};\n+\n+class AxisInfoAnalysis : public ForwardDataFlowAnalysis<AxisInfo> {\n+private:\n+  AxisInfoVisitorList visitors;\n \n public:\n-  using ForwardDataFlowAnalysis<AxisInfo>::ForwardDataFlowAnalysis;\n+  AxisInfoAnalysis(MLIRContext *context);\n \n   ChangeResult\n   visitOperation(Operation *op,\n                  ArrayRef<LatticeElement<AxisInfo> *> operands) override;\n \n-  unsigned getPtrVectorSize(Value ptr);\n+  unsigned getPtrContiguity(Value ptr);\n \n   unsigned getPtrAlignment(Value ptr);\n "}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 10, "deletions": 1, "changes": 11, "file_content_changes": "@@ -65,7 +65,7 @@ template <typename Int> Int product(llvm::ArrayRef<Int> arr) {\n \n template <typename Int> Int ceil(Int m, Int n) { return (m + n - 1) / n; }\n \n-/// output[i] = input[order[i]]\n+// output[i] = input[order[i]]\n template <typename T, typename RES_T = T>\n SmallVector<RES_T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) {\n   size_t rank = order.size();\n@@ -77,6 +77,15 @@ SmallVector<RES_T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) {\n   return result;\n }\n \n+template <typename T> T highestPowOf2Divisor(T n) {\n+  if (n == 0) {\n+    return (static_cast<T>(1) << (sizeof(T) * 8 - 2));\n+  }\n+  return (n & (~(n - 1)));\n+}\n+\n+bool isSingleValue(Value value);\n+\n bool isMmaToDotShortcut(triton::gpu::MmaEncodingAttr &mmaLayout,\n                         triton::gpu::DotOperandEncodingAttr &dotOperandLayout);\n "}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 10, "deletions": 5, "changes": 15, "file_content_changes": "@@ -228,7 +228,8 @@ def TT_AtomicCASOp : TT_Op<\"atomic_cas\", [MemoryEffects<[MemRead]>,\n // Shape Manipulation Ops\n //\n def TT_SplatOp : TT_Op<\"splat\", [NoSideEffect,\n-                                 SameOperandsAndResultElementType]> {\n+                                 SameOperandsAndResultElementType,\n+                                 SameOperandsAndResultEncoding]> {\n     let summary = \"splat\";\n \n     let arguments = (ins TT_Type:$src);\n@@ -253,7 +254,8 @@ def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [NoSideEffect,\n }\n \n def TT_ViewOp : TT_Op<\"view\", [NoSideEffect,\n-                               SameOperandsAndResultElementType]> {\n+                               SameOperandsAndResultElementType,\n+                               SameOperandsAndResultEncoding]> {\n     let summary = \"view\";\n \n     let arguments = (ins TT_Tensor:$src);\n@@ -265,7 +267,8 @@ def TT_ViewOp : TT_Op<\"view\", [NoSideEffect,\n }\n \n def TT_BroadcastOp : TT_Op<\"broadcast\", [NoSideEffect,\n-                                         SameOperandsAndResultElementType]> {\n+                                         SameOperandsAndResultElementType,\n+                                         SameOperandsAndResultEncoding]> {\n     let summary = \"broadcast. No left-padding as of now.\";\n \n     let arguments = (ins TT_Type:$src);\n@@ -278,7 +281,8 @@ def TT_BroadcastOp : TT_Op<\"broadcast\", [NoSideEffect,\n }\n \n def TT_CatOp : TT_Op<\"cat\", [NoSideEffect,\n-                             SameOperandsAndResultElementType]> {\n+                             SameOperandsAndResultElementType,\n+                             SameOperandsAndResultEncoding]> {\n     let summary = \"concatenate 2 tensors\";\n \n     let arguments = (ins TT_Tensor:$lhs, TT_Tensor:$rhs);\n@@ -289,7 +293,8 @@ def TT_CatOp : TT_Op<\"cat\", [NoSideEffect,\n }\n \n def TT_TransOp : TT_Op<\"trans\", [NoSideEffect,\n-                                 DeclareOpInterfaceMethods<InferTypeOpInterface>]> {\n+                                 DeclareOpInterfaceMethods<InferTypeOpInterface>,\n+                                 SameOperandsAndResultElementType]> {\n \n     let summary = \"transpose a tensor\";\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -31,7 +31,7 @@ SmallVector<unsigned> getWarpsPerCTA(const Attribute &layout);\n \n SmallVector<unsigned> getSizePerThread(const Attribute &layout);\n \n-SmallVector<unsigned> getContigPerThread(Attribute layout);\n+SmallVector<unsigned> getContigPerThread(const Attribute &layout);\n \n SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout);\n "}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 710, "deletions": 162, "changes": 872, "file_content_changes": "@@ -1,45 +1,54 @@\n #include \"mlir/Analysis/DataFlowAnalysis.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"llvm/Support/raw_ostream.h\"\n-#include <iostream>\n \n #include \"triton/Analysis/AxisInfo.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n \n namespace mlir {\n \n-//===----------------------------------------------------------------------===//\n-// AxisInfo\n-//===----------------------------------------------------------------------===//\n-\n // Function for extended Euclidean Algorithm\n-static int gcd_impl(int a, int b, int *x, int *y) {\n+static int64_t gcdImpl(int64_t a, int64_t b, int64_t *x, int64_t *y) {\n   // Base Case\n   if (a == 0) {\n     *x = 0;\n     *y = 1;\n     return b;\n   }\n-  int x1, y1; // To store results of recursive call\n-  int gcd = gcd_impl(b % a, a, &x1, &y1);\n+  int64_t x1, y1; // To store results of recursive call\n+  int64_t gcd = gcdImpl(b % a, a, &x1, &y1);\n   // Update x and y using results of\n   // recursive call\n   *x = y1 - (b / a) * x1;\n   *y = x1;\n   return gcd;\n }\n \n-static int gcd(int a, int b) {\n-  int x, y;\n-  return gcd_impl(a, b, &x, &y);\n+static int64_t gcd(int64_t a, int64_t b) {\n+  if (a == 0)\n+    return b;\n+  if (b == 0)\n+    return a;\n+  int64_t x, y;\n+  return gcdImpl(a, b, &x, &y);\n }\n \n+static constexpr int log2Int(int64_t num) {\n+  return (num > 1) ? 1 + log2Int(num / 2) : 0;\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// AxisInfo\n+//===----------------------------------------------------------------------===//\n+\n AxisInfo AxisInfo::getPessimisticValueState(Value value) {\n-  size_t rank = 1;\n+  auto rank = 1;\n   if (TensorType ty = value.getType().dyn_cast<TensorType>())\n     rank = ty.getRank();\n-  int divHint = 1;\n+  auto contiHint = 1;\n+  auto divHint = 1;\n+  auto constHint = 1;\n   BlockArgument blockArg = value.dyn_cast<BlockArgument>();\n   if (blockArg && blockArg.getOwner()->isEntryBlock()) {\n     Operation *op = blockArg.getOwner()->getParentOp();\n@@ -53,139 +62,342 @@ AxisInfo AxisInfo::getPessimisticValueState(Value value) {\n           fun.getArgAttr(blockArg.getArgNumber(), \"tt.divisibility\");\n       if (attr)\n         divHint = attr.cast<IntegerAttr>().getValue().getZExtValue();\n+    } else {\n+      // Derive the divisibility of the induction variable only when\n+      // the step and the lower bound are both constants\n+      if (auto forOp = dyn_cast<scf::ForOp>(op)) {\n+        if (blockArg == forOp.getInductionVar()) {\n+          if (auto lowerBound =\n+                  forOp.getLowerBound().getDefiningOp<arith::ConstantOp>()) {\n+            if (auto step =\n+                    forOp.getStep().getDefiningOp<arith::ConstantOp>()) {\n+              auto lowerBoundVal = lowerBound.getValue()\n+                                       .cast<IntegerAttr>()\n+                                       .getValue()\n+                                       .getZExtValue();\n+              auto stepVal =\n+                  step.getValue().cast<IntegerAttr>().getValue().getZExtValue();\n+              auto k = gcd(lowerBoundVal, stepVal);\n+              if (k != 0)\n+                divHint = k;\n+            }\n+          }\n+        }\n+      }\n     }\n   }\n-  DimVectorT contiguity(rank, 1);\n-  DimVectorT divisibility(rank, divHint);\n-  DimVectorT constancy(rank, 1);\n-  return AxisInfo(contiguity, divisibility, constancy);\n+\n+  return AxisInfo(/*knownContiguity=*/DimVectorT(rank, contiHint),\n+                  /*knownDivisibility=*/DimVectorT(rank, divHint),\n+                  /*knownConstancy=*/DimVectorT(rank, constHint));\n }\n \n // The gcd of both arguments for each dimension\n AxisInfo AxisInfo::join(const AxisInfo &lhs, const AxisInfo &rhs) {\n-  DimVectorT retContiguity;\n-  DimVectorT retDivisibility;\n-  DimVectorT retConstancy;\n-  for (int d = 0; d < lhs.getRank(); ++d) {\n-    retContiguity.push_back(gcd(lhs.getContiguity(d), rhs.getContiguity(d)));\n-    retDivisibility.push_back(\n-        gcd(lhs.getDivisibility(d), rhs.getDivisibility(d)));\n-    retConstancy.push_back(gcd(lhs.getConstancy(d), rhs.getConstancy(d)));\n-  }\n-  return AxisInfo(retContiguity, retDivisibility, retConstancy);\n+  DimVectorT contiguity;\n+  DimVectorT divisibility;\n+  DimVectorT constancy;\n+  for (auto d = 0; d < lhs.getRank(); ++d) {\n+    contiguity.push_back(gcd(lhs.getContiguity(d), rhs.getContiguity(d)));\n+    divisibility.push_back(gcd(lhs.getDivisibility(d), rhs.getDivisibility(d)));\n+    constancy.push_back(gcd(lhs.getConstancy(d), rhs.getConstancy(d)));\n+  }\n+  std::optional<int64_t> constantValue;\n+  if (lhs.getConstantValue().has_value() &&\n+      rhs.getConstantValue().has_value() &&\n+      lhs.getConstantValue() == rhs.getConstantValue())\n+    constantValue = lhs.getConstantValue();\n+  return AxisInfo(contiguity, divisibility, constancy, constantValue);\n }\n \n //===----------------------------------------------------------------------===//\n-// AxisInfoAnalysis\n+// AxisInfoVisitor\n //===----------------------------------------------------------------------===//\n \n-AxisInfo AxisInfoAnalysis::visitBinaryOp(\n-    Operation *op, AxisInfo lhsInfo, AxisInfo rhsInfo,\n-    const std::function<int(AxisInfo, AxisInfo, int)> &getContiguity,\n-    const std::function<int(AxisInfo, AxisInfo, int)> &getDivisibility,\n-    const std::function<int(AxisInfo, AxisInfo, int)> &getConstancy) {\n-  int rank = lhsInfo.getRank();\n-  AxisInfo::DimVectorT newContiguity;\n-  AxisInfo::DimVectorT newDivisibility;\n-  AxisInfo::DimVectorT newConstancy;\n-  for (int d = 0; d < rank; ++d) {\n-    newContiguity.push_back(getContiguity(lhsInfo, rhsInfo, d));\n-    newDivisibility.push_back(getDivisibility(lhsInfo, rhsInfo, d));\n-    newConstancy.push_back(getConstancy(lhsInfo, rhsInfo, d));\n-  }\n-  return AxisInfo(newContiguity, newDivisibility, newConstancy);\n-}\n+template <typename OpTy>\n+class CastOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n \n-ChangeResult AxisInfoAnalysis::visitOperation(\n-    Operation *op, ArrayRef<LatticeElement<AxisInfo> *> operands) {\n-  AxisInfo curr;\n-  // This preserves the input axes (e.g., cast):\n-  if (llvm::isa<arith::ExtSIOp, arith::ExtUIOp, arith::TruncIOp,\n-                triton::PtrToIntOp, triton::IntToPtrOp,\n-                triton::gpu::ConvertLayoutOp>(op))\n-    curr = operands[0]->getValue();\n-  // Constant ranges\n-  if (triton::MakeRangeOp make_range =\n-          llvm::dyn_cast<triton::MakeRangeOp>(op)) {\n-    int start = make_range.start();\n-    int end = make_range.end();\n-    AxisInfo::DimVectorT contiguity = {end - start};\n-    AxisInfo::DimVectorT divisibility = {highestPowOf2Divisor(start)};\n-    AxisInfo::DimVectorT constancy = {1};\n-    curr = AxisInfo(contiguity, divisibility, constancy);\n-  }\n-  // Constant\n-  if (arith::ConstantOp constant = llvm::dyn_cast<arith::ConstantOp>(op)) {\n-    auto intAttr = constant.getValue().dyn_cast<IntegerAttr>();\n-    if (intAttr) {\n-      size_t val = intAttr.getValue().getZExtValue();\n-      curr = AxisInfo({1}, {highestPowOf2Divisor(val)}, {1});\n+  AxisInfo getAxisInfo(OpTy op,\n+                       ArrayRef<LatticeElement<AxisInfo> *> operands) override {\n+    return operands[0]->getValue();\n+  }\n+};\n+\n+class MakeRangeOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<triton::MakeRangeOp> {\n+public:\n+  using AxisInfoVisitorImpl<triton::MakeRangeOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo getAxisInfo(triton::MakeRangeOp op,\n+                       ArrayRef<LatticeElement<AxisInfo> *> operands) override {\n+    auto start = op.start();\n+    auto end = op.end();\n+    return AxisInfo(/*contiguity=*/{end - start},\n+                    /*divisibility=*/{highestPowOf2Divisor(start)},\n+                    /*constancy=*/{1});\n+  }\n+};\n+\n+class ConstantOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<arith::ConstantOp> {\n+public:\n+  using AxisInfoVisitorImpl<arith::ConstantOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo getAxisInfo(arith::ConstantOp op,\n+                       ArrayRef<LatticeElement<AxisInfo> *> operands) override {\n+    auto intAttr = op.getValue().dyn_cast<IntegerAttr>();\n+    auto boolAttr = op.getValue().dyn_cast<BoolAttr>();\n+    if (intAttr || boolAttr) {\n+      int64_t value{};\n+      if (intAttr)\n+        value = intAttr.getValue().getZExtValue();\n+      else\n+        value = boolAttr.getValue() ? 1 : 0;\n+      return AxisInfo(/*contiguity=*/{1},\n+                      /*divisibility=*/{highestPowOf2Divisor(value)},\n+                      /*constancy=*/{1},\n+                      /*knownConstantValue=*/{value});\n     }\n     // TODO: generalize to dense attr\n-    auto splatAttr = constant.getValue().dyn_cast<SplatElementsAttr>();\n-    if (splatAttr && splatAttr.getElementType().isInteger(32)) {\n-      auto value = splatAttr.getSplatValue<int>();\n+    auto splatAttr = op.getValue().dyn_cast<SplatElementsAttr>();\n+    if (splatAttr && splatAttr.getElementType().isIntOrIndex()) {\n+      int64_t value = splatAttr.getSplatValue<APInt>().getZExtValue();\n       TensorType ty = splatAttr.getType().cast<TensorType>();\n-      curr = AxisInfo(\n-          AxisInfo::DimVectorT(ty.getRank(), 1),\n+      return AxisInfo(\n+          /*contiguity=*/AxisInfo::DimVectorT(ty.getRank(), 1),\n+          /*divisibility=*/\n           AxisInfo::DimVectorT(ty.getRank(), highestPowOf2Divisor(value)),\n-          AxisInfo::DimVectorT(ty.getShape().begin(), ty.getShape().end()));\n+          /*constancy=*/\n+          AxisInfo::DimVectorT(ty.getShape().begin(), ty.getShape().end()),\n+          /*knownConstantValue=*/{value});\n+    }\n+    return AxisInfo();\n+  }\n+};\n+\n+template <typename OpTy>\n+class AddSubOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                        int dim) override {\n+    return std::max(gcd(lhs.getConstancy(dim), rhs.getContiguity(dim)),\n+                    gcd(lhs.getContiguity(dim), rhs.getConstancy(dim)));\n+  }\n+\n+  int64_t getDivisibility(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                          int dim) override {\n+    // lhs = k * d_lhs = k * k' * gcd(d_lhs, d_rhs)\n+    // rhs = p * d_rhs = p * p' * gcd(d_lhs, d_rhs)\n+    // lhs + rhs = k * d_lhs + p * d_rhs = (k * d_lhs + p * d_rhs) *\n+    // gcd(d_lhs, d_rhs)\n+    return gcd(lhs.getDivisibility(dim), rhs.getDivisibility(dim));\n+  }\n+\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value()) {\n+      if constexpr (std::is_same_v<OpTy, arith::AddIOp> ||\n+                    std::is_same_v<OpTy, triton::AddPtrOp>) {\n+        return {lhs.getConstantValue().value() +\n+                rhs.getConstantValue().value()};\n+      } else if constexpr (std::is_same_v<OpTy, arith::SubIOp>) {\n+        return {lhs.getConstantValue().value() -\n+                rhs.getConstantValue().value()};\n+      }\n     }\n+    return {};\n   }\n-  // TODO: refactor & complete binary ops\n-  // Addition\n-  if (llvm::isa<arith::AddIOp, triton::AddPtrOp>(op)) {\n-    auto newContiguity = [&](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return std::max(gcd(lhs.getContiguity(d), rhs.getConstancy(d)),\n-                      gcd(lhs.getConstancy(d), rhs.getContiguity(d)));\n-    };\n-    auto newConstancy = [&](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getConstancy(d), rhs.getConstancy(d));\n-    };\n-    auto newDivisibility = [&](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getDivisibility(d), rhs.getDivisibility(d));\n-    };\n-    curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n-                         newContiguity, newDivisibility, newConstancy);\n-  }\n-  // Multiplication\n-  if (llvm::isa<arith::MulIOp>(op)) {\n-    auto newContiguity = [](AxisInfo lhs, AxisInfo rhs, int d) { return 1; };\n-    auto newConstancy = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getConstancy(d), rhs.getConstancy(d));\n-    };\n-    auto newDivisibility = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return lhs.getDivisibility(d) * rhs.getDivisibility(d);\n-    };\n-    curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n-                         newContiguity, newDivisibility, newConstancy);\n-  }\n-  // Remainder\n-  if (llvm::isa<arith::RemSIOp, arith::RemUIOp>(op)) {\n-    auto newContiguity = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getContiguity(d), rhs.getDivisibility(d));\n-    };\n-    auto newDivisibility = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getDivisibility(d), rhs.getDivisibility(d));\n-    };\n-    auto newConstancy = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getConstancy(d), rhs.getConstancy(d));\n-    };\n-    curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n-                         newContiguity, newDivisibility, newConstancy);\n-  }\n-  // TODO: All other binary ops\n-  if (llvm::isa<arith::AndIOp, arith::OrIOp>(op)) {\n-    auto newContiguity = [](AxisInfo lhs, AxisInfo rhs, int d) { return 1; };\n-    auto newDivisibility = [](AxisInfo lhs, AxisInfo rhs, int d) { return 1; };\n-    auto newConstancy = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getConstancy(d), rhs.getConstancy(d));\n-    };\n-    curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n-                         newContiguity, newDivisibility, newConstancy);\n-  }\n-  // Splat\n-  if (llvm::isa<triton::SplatOp>(op)) {\n+};\n+\n+class MulIOpAxisInfoVisitor final : public BinaryOpVisitorImpl<arith::MulIOp> {\n+public:\n+  using BinaryOpVisitorImpl<arith::MulIOp>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(arith::MulIOp op, const AxisInfo &lhs,\n+                        const AxisInfo &rhs, int dim) override {\n+    // lhs * 1 = lhs\n+    auto lhsContiguity =\n+        rhs.getConstantValue().has_value() && rhs.getConstantValue() == 1\n+            ? lhs.getContiguity(dim)\n+            : 1;\n+    // 1 * rhs = rhs\n+    auto rhsContiguity =\n+        lhs.getConstantValue().has_value() && lhs.getConstantValue() == 1\n+            ? rhs.getContiguity(dim)\n+            : 1;\n+    return std::max(lhsContiguity, rhsContiguity);\n+  }\n+\n+  int64_t getConstancy(arith::MulIOp op, const AxisInfo &lhs,\n+                       const AxisInfo &rhs, int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  int64_t getDivisibility(arith::MulIOp op, const AxisInfo &lhs,\n+                          const AxisInfo &rhs, int dim) override {\n+    // lhs = k * d_lhs\n+    // rhs = p * d_rhs\n+    // lhs * rhs = k * d_lhs * p * d_rhs = k * p * d_lhs * d_rhs\n+    return lhs.getDivisibility(dim) * rhs.getDivisibility(dim);\n+  }\n+\n+  std::optional<int64_t> getConstantValue(arith::MulIOp op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() * rhs.getConstantValue().value()};\n+    return {};\n+  }\n+};\n+\n+template <typename OpTy>\n+class DivOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                        int dim) override {\n+    // lhs / 1 = lhs\n+    return rhs.getConstantValue().has_value() &&\n+                   rhs.getConstantValue().value() == 1\n+               ? lhs.getContiguity(dim)\n+               : 1;\n+  }\n+\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return BinaryOpVisitorImpl<OpTy>::getConstancy(op, lhs, rhs, dim);\n+    auto shape = resTy.getShape();\n+    // Case 1: both lhs and rhs are constants.\n+    auto constancy = gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+    // Case 2: lhs contiguous, rhs constant.\n+    // lhs: d_lhs * k, d_lhs * k + 1, ..., d_lhs * k + n\n+    // rhs: d_rhs * p, d_rhs * p, ..., d_rhs * p\n+    // lhs / rhs = d_lhs * k / (d_rhs * p), (d_lhs * k + 1) / (d_rhs * p),\n+    // ..., (d_lhs * k + n) / (d_rhs * p)\n+    // Because d_lhs % d_rhs = 0 || d_rhs % d_lhs = 0,\n+    // the minimal constancy is gcd(d_lhs, d_rhs).\n+    // Since gcd(d_lhs, d_rhs) maybe > len(lhs),\n+    // we need to use another gcd to get the actual constancy.\n+    if (AxisInfoVisitor::isContiguousDim(lhs, shape, dim) &&\n+        AxisInfoVisitor::isConstantDim(rhs, shape, dim)) {\n+      constancy = std::max(constancy, gcd(lhs.getContiguity(dim),\n+                                          gcd(lhs.getDivisibility(dim),\n+                                              rhs.getDivisibility(dim))));\n+    }\n+    return constancy;\n+  }\n+\n+  int64_t getDivisibility(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                          int dim) override {\n+    // lhs = k * d_lhs = k * k' * gcd(d_lhs, d_rhs)\n+    // rhs = p * d_rhs = p * p' * gcd(d_lhs, d_rhs)\n+    // lhs / rhs = k * k' * gcd(d_lhs, d_rhs) / (p * p' * gcd(d_lhs, d_rhs))\n+    //           = k / p * k' / p'\n+    // gcd(k', p') = divisibility(d_lhs / gcd(d_lhs, d_rhs), d_rhs / gcd(d_lhs,\n+    // d_rhs))\n+    auto lhsDivisibility = lhs.getDivisibility(dim);\n+    auto rhsDivisibility = rhs.getDivisibility(dim);\n+    auto initGcd = gcd(lhsDivisibility, rhsDivisibility);\n+    return std::max(lhsDivisibility / initGcd, rhsDivisibility / initGcd);\n+  };\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() / rhs.getConstantValue().value()};\n+    return {};\n+  }\n+};\n+\n+template <typename OpTy>\n+class RemOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                        int dim) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return BinaryOpVisitorImpl<OpTy>::getContiguity(op, lhs, rhs, dim);\n+    auto shape = resTy.getShape();\n+    int64_t contiguity = 1;\n+    // lhs contiguous, rhs constant\n+    // lhs: d_lhs * k, d_lhs * k + 1, ..., d_lhs * k + n\n+    // rhs: d_rhs * p, d_rhs * p, ..., d_rhs * p\n+    // lhs % rhs = d_lhs * k % (d_rhs * p), (d_lhs * k + 1) % (d_rhs * p),\n+    // ..., (d_lhs * k + n) % (d_rhs * p)\n+    // Because d_lhs % d_rhs = 0 || d_rhs % d_lhs = 0,\n+    // The minimal contiguity is gcd(d_lhs, d_rhs).\n+    // Since gcd(d_lhs, d_rhs) maybe > len(lhs),\n+    // we need to use another gcd to get the actual contiguity.\n+    if (AxisInfoVisitor::isContiguousDim(lhs, shape, dim) &&\n+        AxisInfoVisitor::isConstantDim(rhs, shape, dim)) {\n+      contiguity = std::max(contiguity, gcd(lhs.getContiguity(dim),\n+                                            gcd(lhs.getDivisibility(dim),\n+                                                rhs.getDivisibility(dim))));\n+    }\n+    return contiguity;\n+  }\n+\n+  int64_t getDivisibility(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                          int dim) override {\n+    // lhs: d_lhs * k = gcd(d_lhs, d_rhs) * k' * k = gcd(d_lhs, d_rhs) * k''\n+    // rhs: d_rhs * p = gcd(d_lhs, d_rhs) * p' * p = gcd(d_lhs, d_rhs) * p''\n+    // lhs = gcd(d_lhs, d_rhs) * k'' = gcd(d_lhs, d_rhs) * d + r\n+    // r must be divisible by gcd(d_lhs, d_rhs)\n+    return gcd(lhs.getDivisibility(dim), rhs.getDivisibility(dim));\n+  };\n+\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return BinaryOpVisitorImpl<OpTy>::getConstancy(op, lhs, rhs, dim);\n+    auto shape = resTy.getShape();\n+    // lhs % 1 = 0\n+    return rhs.getConstantValue().has_value() &&\n+                   rhs.getConstantValue().value() == 1\n+               ? shape[dim]\n+               : gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() % rhs.getConstantValue().value()};\n+    else if (rhs.getConstantValue().has_value() &&\n+             rhs.getConstantValue().value() == 1)\n+      return {0};\n+    return {};\n+  }\n+};\n+\n+class SplatOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<triton::SplatOp> {\n+public:\n+  using AxisInfoVisitorImpl<triton::SplatOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo getAxisInfo(triton::SplatOp op,\n+                       ArrayRef<LatticeElement<AxisInfo> *> operands) override {\n     Type _retTy = *op->result_type_begin();\n     TensorType retTy = _retTy.cast<TensorType>();\n     AxisInfo opInfo = operands[0]->getValue();\n@@ -197,21 +409,37 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n       divisibility.push_back(opInfo.getDivisibility(0));\n       constancy.push_back(retTy.getShape()[d]);\n     }\n-    curr = AxisInfo(contiguity, divisibility, constancy);\n+    return AxisInfo(contiguity, divisibility, constancy,\n+                    operands[0]->getValue().getConstantValue());\n   }\n-  // expandDims\n-  if (auto expandDims = llvm::dyn_cast<triton::ExpandDimsOp>(op)) {\n+};\n+\n+class ExpandDimsOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<triton::ExpandDimsOp> {\n+public:\n+  using AxisInfoVisitorImpl<triton::ExpandDimsOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo getAxisInfo(triton::ExpandDimsOp op,\n+                       ArrayRef<LatticeElement<AxisInfo> *> operands) override {\n     AxisInfo opInfo = operands[0]->getValue();\n     AxisInfo::DimVectorT contiguity = opInfo.getContiguity();\n     AxisInfo::DimVectorT divisibility = opInfo.getDivisibility();\n     AxisInfo::DimVectorT constancy = opInfo.getConstancy();\n-    contiguity.insert(contiguity.begin() + expandDims.axis(), 1);\n-    divisibility.insert(divisibility.begin() + expandDims.axis(), 1);\n-    constancy.insert(constancy.begin() + expandDims.axis(), 1);\n-    curr = AxisInfo(contiguity, divisibility, constancy);\n+    contiguity.insert(contiguity.begin() + op.axis(), 1);\n+    divisibility.insert(divisibility.begin() + op.axis(), 1);\n+    constancy.insert(constancy.begin() + op.axis(), 1);\n+    return AxisInfo(contiguity, divisibility, constancy,\n+                    operands[0]->getValue().getConstantValue());\n   }\n-  // Broadcast\n-  if (llvm::isa<triton::BroadcastOp>(op)) {\n+};\n+\n+class BroadcastOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<triton::BroadcastOp> {\n+public:\n+  using AxisInfoVisitorImpl<triton::BroadcastOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo getAxisInfo(triton::BroadcastOp op,\n+                       ArrayRef<LatticeElement<AxisInfo> *> operands) override {\n     Type _retTy = *op->result_type_begin();\n     Type _opTy = *op->operand_type_begin();\n     TensorType retTy = _retTy.cast<TensorType>();\n@@ -228,42 +456,362 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n       constancy.push_back(opShape[d] == 1 ? retShape[d]\n                                           : opInfo.getConstancy(d));\n     }\n-    curr = AxisInfo(contiguity, divisibility, constancy);\n+    return AxisInfo(contiguity, divisibility, constancy,\n+                    operands[0]->getValue().getConstantValue());\n   }\n+};\n \n-  // CmpI\n-  if ((llvm::dyn_cast<arith::CmpIOp>(op) ||\n-       llvm::dyn_cast<triton::gpu::CmpIOp>(op)) &&\n-      op->getResult(0).getType().dyn_cast<TensorType>()) {\n-    auto resTy = op->getResult(0).getType().cast<TensorType>();\n+template <typename OpTy>\n+class CmpOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n+\n+  AxisInfo getAxisInfo(OpTy op,\n+                       ArrayRef<LatticeElement<AxisInfo> *> operands) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return AxisInfo();\n+    auto shape = resTy.getShape();\n     short rank = resTy.getRank();\n     auto lhsInfo = operands[0]->getValue();\n     auto rhsInfo = operands[1]->getValue();\n-    auto shape = resTy.getShape();\n \n     AxisInfo::DimVectorT contiguity, divisibility, constancy;\n+    std::optional<int64_t> constantValue;\n     for (short d = 0; d < rank; ++d) {\n-      if (rhsInfo.getConstancy(d) % lhsInfo.getContiguity(d) == 0 ||\n-          rhsInfo.getConstancy(d) % lhsInfo.getConstancy(d))\n-        constancy.push_back(\n-            gcd(lhsInfo.getDivisibility(d), rhsInfo.getDivisibility(d)));\n-      else\n-        constancy.push_back(1);\n+      int64_t constHint = 1;\n+      if (lhsInfo.getConstantValue().has_value() &&\n+          rhsInfo.getConstantValue().has_value()) {\n+        constHint = lhsInfo.getConstancy(d);\n+        constantValue =\n+            compare(getPredicate(op), lhsInfo.getConstantValue().value(),\n+                    rhsInfo.getConstantValue().value())\n+                ? 1\n+                : 0;\n+      } else {\n+        // Case 1: lhs and rhs are both partial constants\n+        constHint = gcd(lhsInfo.getConstancy(d), rhsInfo.getConstancy(d));\n+        // Case 2: lhs all constant, rhs all contiguous\n+        // NOTE:\n+        // lhs: 4 4 4 4\n+        // rhs: 4 5 6 7\n+        // lhs ge rhs: 1, 0, 0, 0\n+        // Case 3: lhs all contiguous, rhs all constant\n+        // NOTE\n+        // lhs: 4 5 6 7\n+        // rhs: 4 4 4 4\n+        // lhs sle rhs: 1, 0, 0, 0\n+        if (/*Case 2=*/(\n+                notGePredicate(getPredicate(op)) &&\n+                (AxisInfoVisitor::isConstantDim(lhsInfo, shape, d) &&\n+                 AxisInfoVisitor::isContiguousDim(rhsInfo, shape, d))) ||\n+            /*Case 3=*/(notLePredicate(getPredicate(op)) &&\n+                        (AxisInfoVisitor::isContiguousDim(lhsInfo, shape, d) &&\n+                         AxisInfoVisitor::isConstantDim(rhsInfo, shape, d)))) {\n+          constHint = std::max(constHint, gcd(lhsInfo.getContiguity(d),\n+                                              gcd(lhsInfo.getDivisibility(d),\n+                                                  rhsInfo.getDivisibility(d))));\n+        }\n+      }\n \n-      divisibility.push_back(shape[d]);\n+      constancy.push_back(constHint);\n+      divisibility.push_back(1);\n       contiguity.push_back(1);\n     }\n \n-    curr = AxisInfo(contiguity, divisibility, constancy);\n+    return AxisInfo(contiguity, divisibility, constancy, constantValue);\n+  }\n+\n+private:\n+  static arith::CmpIPredicate getPredicate(triton::gpu::CmpIOp op) {\n+    return op.predicate();\n+  }\n+\n+  static arith::CmpIPredicate getPredicate(arith::CmpIOp op) {\n+    return op.getPredicate();\n+  }\n+\n+  static bool notGePredicate(arith::CmpIPredicate predicate) {\n+    return predicate != arith::CmpIPredicate::sge &&\n+           predicate != arith::CmpIPredicate::uge;\n+  }\n+\n+  static bool notLePredicate(arith::CmpIPredicate predicate) {\n+    return predicate != arith::CmpIPredicate::sle &&\n+           predicate != arith::CmpIPredicate::ule;\n+  }\n+\n+  static bool compare(arith::CmpIPredicate predicate, int64_t lhs,\n+                      int64_t rhs) {\n+    switch (predicate) {\n+    case arith::CmpIPredicate::eq:\n+      return lhs == rhs;\n+    case arith::CmpIPredicate::ne:\n+      return lhs != rhs;\n+    case arith::CmpIPredicate::slt:\n+      return lhs < rhs;\n+    case arith::CmpIPredicate::sle:\n+      return lhs <= rhs;\n+    case arith::CmpIPredicate::sgt:\n+      return lhs > rhs;\n+    case arith::CmpIPredicate::sge:\n+      return lhs >= rhs;\n+    case arith::CmpIPredicate::ult:\n+      return (uint64_t)lhs < (uint64_t)rhs;\n+    case arith::CmpIPredicate::ule:\n+      return (uint64_t)lhs <= (uint64_t)rhs;\n+    case arith::CmpIPredicate::ugt:\n+      return (uint64_t)lhs > (uint64_t)rhs;\n+    case arith::CmpIPredicate::uge:\n+      return (uint64_t)lhs >= (uint64_t)rhs;\n+    default:\n+      break;\n+    }\n+    llvm_unreachable(\"unknown comparison predicate\");\n+  }\n+};\n+\n+template <typename OpTy>\n+class SelectOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n+\n+  AxisInfo getAxisInfo(OpTy op,\n+                       ArrayRef<LatticeElement<AxisInfo> *> operands) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return AxisInfo();\n+    auto shape = resTy.getShape();\n+    auto rank = shape.size();\n+    auto condConstancy = operands[0]->getValue().getConstancy();\n+    auto lhsInfo = operands[1]->getValue();\n+    auto rhsInfo = operands[2]->getValue();\n+\n+    AxisInfo::DimVectorT contiguity, divisibility, constancy;\n+    std::optional<int64_t> constantValue;\n+    if (operands[0]->getValue().getConstantValue().has_value()) {\n+      if (operands[0]->getValue().getConstantValue() == 0) {\n+        contiguity = rhsInfo.getContiguity();\n+        divisibility = rhsInfo.getDivisibility();\n+        constancy = rhsInfo.getConstancy();\n+        constantValue = rhsInfo.getConstantValue();\n+      } else {\n+        contiguity = lhsInfo.getContiguity();\n+        divisibility = lhsInfo.getDivisibility();\n+        constancy = lhsInfo.getConstancy();\n+        constantValue = lhsInfo.getConstantValue();\n+      }\n+    } else {\n+      for (auto d = 0; d < rank; ++d) {\n+        constancy.push_back(\n+            std::min(gcd(lhsInfo.getConstancy(d), condConstancy[d]),\n+                     gcd(rhsInfo.getConstancy(d), condConstancy[d])));\n+        divisibility.push_back(\n+            std::min(lhsInfo.getDivisibility(d), rhsInfo.getDivisibility(d)));\n+        contiguity.push_back(\n+            std::min(gcd(lhsInfo.getContiguity(d), condConstancy[d]),\n+                     gcd(rhsInfo.getContiguity(d), condConstancy[d])));\n+      }\n+      if (lhsInfo.getConstantValue().has_value() &&\n+          rhsInfo.getConstantValue().has_value() &&\n+          lhsInfo.getConstantValue() == rhsInfo.getConstantValue())\n+        constantValue = lhsInfo.getConstantValue();\n+    }\n+\n+    return AxisInfo(contiguity, divisibility, constancy, constantValue);\n+  }\n+};\n+\n+template <typename OpTy>\n+class LogicalOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value()) {\n+      if constexpr (std::is_same<OpTy, arith::AndIOp>::value) {\n+        return {lhs.getConstantValue().value() &\n+                rhs.getConstantValue().value()};\n+      } else if constexpr (std::is_same<OpTy, arith::OrIOp>::value) {\n+        return {lhs.getConstantValue().value() |\n+                rhs.getConstantValue().value()};\n+      } else if constexpr (std::is_same<OpTy, arith::XOrIOp>::value) {\n+        return {lhs.getConstantValue().value() ^\n+                rhs.getConstantValue().value()};\n+      }\n+    }\n+    return {};\n   }\n+};\n \n-  // UnrealizedConversionCast\n+class ShLIOpAxisInfoVisitor final : public BinaryOpVisitorImpl<arith::ShLIOp> {\n+public:\n+  using BinaryOpVisitorImpl<arith::ShLIOp>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(arith::ShLIOp op, const AxisInfo &lhs,\n+                        const AxisInfo &rhs, int dim) override {\n+    if (rhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().value() == 0)\n+      return lhs.getContiguity(dim);\n+    else\n+      return 1;\n+  }\n+\n+  int64_t getDivisibility(arith::ShLIOp op, const AxisInfo &lhs,\n+                          const AxisInfo &rhs, int dim) override {\n+    auto shift = rhs.getConstantValue().has_value()\n+                     ? rhs.getConstantValue().value()\n+                     : rhs.getDivisibility(dim);\n+    auto numBits = log2Int(lhs.getDivisibility(dim));\n+    auto maxBits = log2Int(highestPowOf2Divisor<int64_t>(0));\n+    // Make sure the return value doesn't exceed highestPowOf2Divisor<int64>(0)\n+    if (shift + numBits > maxBits)\n+      return highestPowOf2Divisor<int64_t>(0);\n+    return lhs.getDivisibility(dim) << shift;\n+  }\n+\n+  int64_t getConstancy(arith::ShLIOp op, const AxisInfo &lhs,\n+                       const AxisInfo &rhs, int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(arith::ShLIOp op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() << rhs.getConstantValue().value()};\n+    return {};\n+  }\n+};\n+\n+template <typename OpTy>\n+class ShROpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                        int dim) override {\n+    if (rhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().value() == 0)\n+      return lhs.getContiguity(dim);\n+    else\n+      return 1;\n+  }\n+\n+  int64_t getDivisibility(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                          int dim) override {\n+    if (rhs.getConstantValue().has_value())\n+      return std::max<int64_t>(1, lhs.getDivisibility(dim) /\n+                                      (1 << rhs.getConstantValue().value()));\n+    else\n+      return std::max<int64_t>(1, lhs.getDivisibility(dim) /\n+                                      (1 << rhs.getDivisibility(dim)));\n+  }\n+\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() >> rhs.getConstantValue().value()};\n+    return {};\n+  }\n+};\n+\n+template <typename OpTy>\n+class MaxMinOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n+\n+  AxisInfo getAxisInfo(OpTy op,\n+                       ArrayRef<LatticeElement<AxisInfo> *> operands) override {\n+    auto lhsInfo = operands[0]->getValue();\n+    auto rhsInfo = operands[1]->getValue();\n+    std::optional<int64_t> constantValue;\n+    if (lhsInfo.getConstantValue().has_value() &&\n+        rhsInfo.getConstantValue().has_value()) {\n+      if constexpr (std::is_same_v<OpTy, arith::MaxSIOp> ||\n+                    std::is_same_v<OpTy, arith::MaxUIOp>) {\n+        constantValue = {std::max(lhsInfo.getConstantValue().value(),\n+                                  rhsInfo.getConstantValue().value())};\n+      } else if constexpr (std::is_same_v<OpTy, arith::MinSIOp> ||\n+                           std::is_same_v<OpTy, arith::MinUIOp>) {\n+        constantValue = {std::min(lhsInfo.getConstantValue().value(),\n+                                  rhsInfo.getConstantValue().value())};\n+      }\n+    }\n+    auto rank = lhsInfo.getRank();\n+    return AxisInfo(/*knownContiguity=*/AxisInfo::DimVectorT(rank, 1),\n+                    /*knownDivisibility=*/AxisInfo::DimVectorT(rank, 1),\n+                    /*knownConstancy=*/AxisInfo::DimVectorT(rank, 1),\n+                    /*constantValue=*/constantValue);\n+  }\n+};\n+\n+//===----------------------------------------------------------------------===//\n+// AxisInfoAnalysis\n+//===----------------------------------------------------------------------===//\n+\n+AxisInfoAnalysis::AxisInfoAnalysis(MLIRContext *context)\n+    : ForwardDataFlowAnalysis<AxisInfo>(context) {\n+  // UnrealizedConversionCast:\n   // This is needed by TritonGPUToLLVM, to get AxisInfo when the graph is\n   // in the process of a PartialConversion, where UnrealizedConversionCast\n   // may exist\n-  if (llvm::isa<mlir::UnrealizedConversionCastOp>(op)) {\n-    curr = operands[0]->getValue();\n-  }\n+  visitors.append<CastOpAxisInfoVisitor<arith::ExtSIOp>,\n+                  CastOpAxisInfoVisitor<arith::ExtUIOp>,\n+                  CastOpAxisInfoVisitor<arith::TruncIOp>,\n+                  CastOpAxisInfoVisitor<arith::IndexCastOp>,\n+                  CastOpAxisInfoVisitor<triton::PtrToIntOp>,\n+                  CastOpAxisInfoVisitor<triton::IntToPtrOp>,\n+                  CastOpAxisInfoVisitor<triton::gpu::ConvertLayoutOp>,\n+                  CastOpAxisInfoVisitor<mlir::UnrealizedConversionCastOp>,\n+                  CastOpAxisInfoVisitor<triton::BitcastOp>>();\n+  visitors.append<MakeRangeOpAxisInfoVisitor>();\n+  visitors.append<ConstantOpAxisInfoVisitor>();\n+  visitors.append<AddSubOpAxisInfoVisitor<triton::AddPtrOp>,\n+                  AddSubOpAxisInfoVisitor<arith::AddIOp>,\n+                  AddSubOpAxisInfoVisitor<arith::SubIOp>>();\n+  visitors.append<MulIOpAxisInfoVisitor>();\n+  visitors.append<DivOpAxisInfoVisitor<arith::DivSIOp>,\n+                  DivOpAxisInfoVisitor<arith::DivUIOp>>();\n+  visitors.append<RemOpAxisInfoVisitor<arith::RemSIOp>,\n+                  RemOpAxisInfoVisitor<arith::RemUIOp>>();\n+  visitors.append<BroadcastOpAxisInfoVisitor>();\n+  visitors.append<SplatOpAxisInfoVisitor>();\n+  visitors.append<ExpandDimsOpAxisInfoVisitor>();\n+  visitors.append<CmpOpAxisInfoVisitor<arith::CmpIOp>,\n+                  CmpOpAxisInfoVisitor<triton::gpu::CmpIOp>>();\n+  visitors.append<LogicalOpAxisInfoVisitor<arith::AndIOp>,\n+                  LogicalOpAxisInfoVisitor<arith::OrIOp>,\n+                  LogicalOpAxisInfoVisitor<arith::XOrIOp>>();\n+  visitors.append<SelectOpAxisInfoVisitor<mlir::SelectOp>,\n+                  SelectOpAxisInfoVisitor<triton::gpu::SelectOp>>();\n+  visitors.append<ShLIOpAxisInfoVisitor, ShROpAxisInfoVisitor<arith::ShRUIOp>,\n+                  ShROpAxisInfoVisitor<arith::ShRSIOp>>();\n+  visitors.append<MaxMinOpAxisInfoVisitor<arith::MaxSIOp>,\n+                  MaxMinOpAxisInfoVisitor<arith::MaxUIOp>,\n+                  MaxMinOpAxisInfoVisitor<arith::MinSIOp>,\n+                  MaxMinOpAxisInfoVisitor<arith::MinUIOp>>();\n+}\n+\n+ChangeResult AxisInfoAnalysis::visitOperation(\n+    Operation *op, ArrayRef<LatticeElement<AxisInfo> *> operands) {\n+  AxisInfo curr = visitors.apply(op, operands);\n   if (curr.getRank() == 0) {\n     return markAllPessimisticFixpoint(op->getResults());\n   }\n@@ -276,7 +824,7 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n   return result;\n }\n \n-unsigned AxisInfoAnalysis::getPtrVectorSize(Value ptr) {\n+unsigned AxisInfoAnalysis::getPtrContiguity(Value ptr) {\n   auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n   if (!tensorTy)\n     return 1;\n@@ -289,10 +837,10 @@ unsigned AxisInfoAnalysis::getPtrVectorSize(Value ptr) {\n   unsigned align = getPtrAlignment(ptr);\n \n   unsigned contigPerThread = triton::gpu::getSizePerThread(layout)[order[0]];\n-  unsigned vec = std::min(align, contigPerThread);\n-  vec = std::min<unsigned>(shape[order[0]], vec);\n+  contigPerThread = std::min(align, contigPerThread);\n+  contigPerThread = std::min<unsigned>(shape[order[0]], contigPerThread);\n \n-  return vec;\n+  return contigPerThread;\n }\n \n unsigned AxisInfoAnalysis::getPtrAlignment(Value ptr) {\n@@ -302,8 +850,8 @@ unsigned AxisInfoAnalysis::getPtrAlignment(Value ptr) {\n   auto axisInfo = lookupLatticeElement(ptr)->getValue();\n   auto layout = tensorTy.getEncoding();\n   auto order = triton::gpu::getOrder(layout);\n-  unsigned maxMultiple = axisInfo.getDivisibility(order[0]);\n-  unsigned maxContig = axisInfo.getContiguity(order[0]);\n+  auto maxMultiple = axisInfo.getDivisibility(order[0]);\n+  auto maxContig = axisInfo.getContiguity(order[0]);\n   unsigned alignment = std::min(maxMultiple, maxContig);\n   return alignment;\n }"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -29,8 +29,8 @@ void MembarAnalysis::dfsOperation(Operation *operation,\n         // Copy the parent info as the current info.\n         RegionInfo regionInfo = *parentRegionInfo;\n         for (auto &block : region.getBlocks()) {\n-          assert(region.getBlocks().size() == 1 &&\n-                 \"Multiple blocks in a region is not supported\");\n+          // assert(region.getBlocks().size() == 1 &&\n+          //        \"Multiple blocks in a region is not supported\");\n           for (auto &op : block.getOperations()) {\n             // Traverse the nested operation.\n             dfsOperation(&op, &regionInfo, builder);"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 89, "deletions": 14, "changes": 103, "file_content_changes": "@@ -2,6 +2,7 @@\n #include \"mlir/IR/Dialect.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include <deque>\n \n namespace mlir {\n \n@@ -164,7 +165,55 @@ bool isMmaToDotShortcut(triton::gpu::MmaEncodingAttr &mmaLayout,\n          dotOperandLayout.getParent() == mmaLayout;\n }\n \n+bool isSingleValue(Value value) {\n+  // Don't consider load as expensive if it is loading a scalar.\n+  if (auto tensorTy = value.getType().dyn_cast<RankedTensorType>())\n+    return tensorTy.getNumElements() == 1;\n+  // TODO: Handle other cases.\n+  // For example, when ptr is a tensor of single value.\n+  // It means that ptr is a resultant of broadcast or generated through\n+  // a chain of broadcast and other operations.\n+  // Rematerialize it without considering contiguous memory access pattern is\n+  // fine.\n+  return true;\n+}\n+\n namespace {\n+\n+/// A data structure similar to SetVector but maintains\n+/// a deque instead of a vector to allow for efficient\n+/// push_back and pop_front operations.\n+/// Using SetVector doesn't suffice our needs because\n+/// it only pushes and pops from the back.\n+/// For example, if we have a queue like this:\n+/// 0->4 1->2->3\n+///    ^--------\n+/// where 3 depends on 4, once we pop 3, we found\n+/// 4 is not ready, so we check 2 and push 3 back\n+/// to the queue.\n+struct DFSSubgraphState {\n+  DFSSubgraphState() : set(), deque() {}\n+  DenseSet<Operation *> set;\n+  std::deque<Operation *> deque;\n+\n+  bool push_back(Operation *op) {\n+    if (set.insert(op).second) {\n+      deque.push_back(op);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  Operation *pop_front() {\n+    Operation *op = deque.front();\n+    deque.pop_front();\n+    set.erase(op);\n+    return op;\n+  }\n+\n+  bool empty() { return deque.empty(); }\n+};\n+\n /// DFS post-order implementation that maintains a global count to work across\n /// multiple invocations, to help implement topological sort on multi-root DAGs.\n /// We traverse all operations but only record the ones that appear in\n@@ -174,23 +223,49 @@ struct DFSState {\n   const SetVector<Operation *> &toSort;\n   SmallVector<Operation *, 16> topologicalCounts;\n   DenseSet<Operation *> seen;\n+\n+  /// We mark each op as ready if all its operands are seen. If an op is ready,\n+  /// we add it to the queue. Otherwise, we keep adding its operands to the\n+  /// ancestors set.\n+  void addToReadyQueue(Operation *op, DFSSubgraphState &subGraph,\n+                       SmallVector<Operation *, 4> &readyQueue) {\n+    bool ready = true;\n+    for (Value operand : op->getOperands()) {\n+      auto def = operand.getDefiningOp();\n+      if (def && !seen.count(def)) {\n+        subGraph.push_back(def);\n+        ready = false;\n+      }\n+    }\n+    if (ready)\n+      readyQueue.push_back(op);\n+  }\n };\n \n void dfsPostorder(Operation *root, DFSState *state) {\n-  SmallVector<Operation *> queue(1, root);\n-  std::vector<Operation *> ops;\n-  while (!queue.empty()) {\n-    Operation *current = queue.pop_back_val();\n-    if (!state->seen.insert(current).second)\n-      continue;\n-    ops.push_back(current);\n-    for (Value result : current->getResults()) {\n-      for (Operation *op : result.getUsers())\n-        queue.push_back(op);\n-    }\n-    for (Region &region : current->getRegions()) {\n-      for (Operation &op : region.getOps())\n-        queue.push_back(&op);\n+  DFSSubgraphState subGraph;\n+  subGraph.push_back(root);\n+  SmallVector<Operation *> ops;\n+  while (!subGraph.empty()) {\n+    // Nodes in the ready queue are ready to be processed.\n+    // Meaning that either their operands are all seen or they have null\n+    // operands.\n+    SmallVector<Operation *, 4> readyQueue;\n+    auto *current = subGraph.pop_front();\n+    state->addToReadyQueue(current, subGraph, readyQueue);\n+    while (!readyQueue.empty()) {\n+      Operation *current = readyQueue.pop_back_val();\n+      if (!state->seen.insert(current).second)\n+        continue;\n+      ops.push_back(current);\n+      for (Value result : current->getResults()) {\n+        for (Operation *op : result.getUsers())\n+          state->addToReadyQueue(op, subGraph, readyQueue);\n+      }\n+      for (Region &region : current->getRegions()) {\n+        for (Operation &op : region.getOps())\n+          state->addToReadyQueue(&op, subGraph, readyQueue);\n+      }\n     }\n   }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -462,7 +462,7 @@ struct CmpIOpConversion\n \n #undef __PRED_ENUM\n     }\n-    return LLVM::ICmpPredicate::eq;\n+    llvm_unreachable(\"Unknown arith::CmpIPredicate\");\n   }\n };\n \n@@ -510,7 +510,7 @@ struct CmpFOpConversion\n \n #undef __PRED_ENUM\n     }\n-    return LLVM::FCmpPredicate::_true;\n+    llvm_unreachable(\"Unknown arith::CmpFPredicate\");\n   }\n };\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 23, "deletions": 2, "changes": 25, "file_content_changes": "@@ -31,8 +31,26 @@ struct LoadStoreConversionBase {\n     return valueVals;\n   }\n \n+  unsigned getContiguity(Value ptr) const {\n+    auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorTy)\n+      return 1;\n+    return axisAnalysisPass.getPtrContiguity(ptr);\n+  }\n+\n   unsigned getVectorSize(Value ptr) const {\n-    return axisAnalysisPass.getPtrVectorSize(ptr);\n+    auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorTy)\n+      return 1;\n+    auto contiguity = getContiguity(ptr);\n+    unsigned numElemBits = 0;\n+    auto ptrTy = tensorTy.getElementType().cast<triton::PointerType>();\n+    auto pointeeType = ptrTy.getPointeeType();\n+    numElemBits = pointeeType.isa<triton::Float8Type>()\n+                      ? 8\n+                      : pointeeType.getIntOrFloatBitWidth();\n+    // The maximum vector size is 128 bits on NVIDIA GPUs.\n+    return std::min<unsigned>(128 / numElemBits, contiguity);\n   }\n \n   unsigned getMaskAlignment(Value mask) const {\n@@ -734,7 +752,10 @@ struct InsertSliceAsyncOpConversion\n       assert(srcElems.size() == otherElems.size());\n     }\n \n-    unsigned inVec = getVectorSize(src);\n+    // We don't use getVec() here because we are copying from memory to memory.\n+    // If contiguity > vector size, we can have one pointer maintaining the\n+    // start of the vector and the other pointer moving to the next vector.\n+    unsigned inVec = getContiguity(src);\n     unsigned outVec = resSharedLayout.getVec();\n     unsigned minVec = std::min(outVec, inVec);\n     unsigned numElems = getElemsPerThread(srcTy);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -70,6 +70,7 @@ struct BroadcastOpConversion\n     auto resultOffsets = emitOffsetForLayout(resultLayout, resultShape);\n     SmallVector<Value> srcVals = getElementsFromStruct(loc, src, rewriter);\n     if (auto srcMma = srcLayout.dyn_cast<MmaEncodingAttr>()) {\n+\n       // NOTE: This is just an naive fix, but for MMA layout, and 2-d fix should\n       // be all right.\n       // TODO[Superjomn]: Replace this with a generic implementation."}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -342,7 +342,7 @@ class ConvertTritonGPUToLLVM\n       auto resSharedLayout =\n           dstTy.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n       auto resElemTy = dstTy.getElementType();\n-      unsigned inVec = axisInfoAnalysis.getPtrVectorSize(src);\n+      unsigned inVec = axisInfoAnalysis.getPtrContiguity(src);\n       unsigned outVec = resSharedLayout.getVec();\n       unsigned minVec = std::min(outVec, inVec);\n       auto maxBitWidth ="}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 89, "deletions": 2, "changes": 91, "file_content_changes": "@@ -573,13 +573,99 @@ class SCFIfPattern : public OpConversionPattern<scf::IfOp> {\n   }\n };\n \n+// This is borrowed from ConvertFIfOpTypes in\n+//    SCF/Transforms/StructuralTypeConversions.cpp\n+class SCFWhilePattern : public OpConversionPattern<scf::WhileOp> {\n+public:\n+  using OpConversionPattern<scf::WhileOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(scf::WhileOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto *converter = getTypeConverter();\n+    assert(converter);\n+    SmallVector<Type> newResultTypes;\n+    if (failed(converter->convertTypes(op.getResultTypes(), newResultTypes)))\n+      return failure();\n+\n+    auto newOp = rewriter.create<scf::WhileOp>(op.getLoc(), newResultTypes,\n+                                               adaptor.getOperands());\n+    for (auto i : {0u, 1u}) {\n+      auto &dstRegion = newOp.getRegion(i);\n+      rewriter.inlineRegionBefore(op.getRegion(i), dstRegion, dstRegion.end());\n+      if (failed(rewriter.convertRegionTypes(&dstRegion, *converter)))\n+        return rewriter.notifyMatchFailure(op, \"could not convert body types\");\n+    }\n+    rewriter.replaceOp(op, newOp.getResults());\n+    return success();\n+  }\n+};\n+\n+class SCFConditionPattern : public OpConversionPattern<scf::ConditionOp> {\n+public:\n+  using OpConversionPattern<scf::ConditionOp>::OpConversionPattern;\n+  LogicalResult\n+  matchAndRewrite(scf::ConditionOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    rewriter.updateRootInPlace(\n+        op, [&]() { op->setOperands(adaptor.getOperands()); });\n+    return success();\n+  }\n+};\n+\n void populateSCFPatterns(TritonGPUTypeConverter &typeConverter,\n                          RewritePatternSet &patterns) {\n   MLIRContext *context = patterns.getContext();\n-  patterns.add<SCFYieldPattern, SCFForPattern, SCFIfPattern>(typeConverter,\n-                                                             context);\n+  patterns.add<SCFYieldPattern, SCFForPattern, SCFIfPattern, SCFWhilePattern,\n+               SCFConditionPattern>(typeConverter, context);\n }\n \n+// CF\n+\n+class CFBranchPattern : public OpConversionPattern<BranchOp> {\n+public:\n+  using OpConversionPattern<BranchOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(BranchOp op, BranchOp::Adaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto converter = getTypeConverter();\n+    auto newOp = rewriter.replaceOpWithNewOp<BranchOp>(op, op.getSuccessor(),\n+                                                       adaptor.getOperands());\n+    return success();\n+  }\n+};\n+\n+class CFCondBranchPattern : public OpConversionPattern<CondBranchOp> {\n+public:\n+  using OpConversionPattern<CondBranchOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(CondBranchOp op, CondBranchOp::Adaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto converter = getTypeConverter();\n+    auto newOp = rewriter.replaceOpWithNewOp<CondBranchOp>(\n+        op, adaptor.getCondition(), op.getTrueDest(),\n+        adaptor.getTrueDestOperands(), op.getFalseDest(),\n+        adaptor.getFalseDestOperands());\n+\n+    if (failed(rewriter.convertRegionTypes(newOp.getTrueDest()->getParent(),\n+                                           *converter)))\n+      return failure();\n+    if (failed(rewriter.convertRegionTypes(newOp.getFalseDest()->getParent(),\n+                                           *converter)))\n+      return failure();\n+    return success();\n+  }\n+};\n+\n+void populateCFPatterns(TritonGPUTypeConverter &typeConverter,\n+                        RewritePatternSet &patterns) {\n+  MLIRContext *context = patterns.getContext();\n+  patterns.add<CFBranchPattern, CFCondBranchPattern>(typeConverter, context);\n+}\n+//\n+\n class ConvertTritonToTritonGPU\n     : public ConvertTritonToTritonGPUBase<ConvertTritonToTritonGPU> {\n public:\n@@ -603,6 +689,7 @@ class ConvertTritonToTritonGPU\n     // TODO: can we use\n     //    mlir::scf::populateSCFStructurealTypeConversionsAndLegality(...) here?\n     populateSCFPatterns(typeConverter, patterns);\n+    populateCFPatterns(typeConverter, patterns);\n \n     if (failed(applyPartialConversion(mod, target, std::move(patterns))))\n       return signalPassFailure();"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -143,7 +143,7 @@ SmallVector<unsigned> getSizePerThread(const Attribute &layout) {\n   }\n }\n \n-SmallVector<unsigned> getContigPerThread(Attribute layout) {\n+SmallVector<unsigned> getContigPerThread(const Attribute &layout) {\n   if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     assert(mmaLayout.isVolta() || mmaLayout.isAmpere());\n     return {1, 2};"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 121, "deletions": 117, "changes": 238, "file_content_changes": "@@ -95,7 +95,7 @@ class SimplifyReduceCvt : public mlir::RewritePattern {\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n     auto reduce = cast<triton::ReduceOp>(*op);\n-    auto reduceArg = dyn_cast<triton::gpu::ConvertLayoutOp>(\n+    auto reduceArg = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n         reduce.getOperand().getDefiningOp());\n     if (!reduceArg)\n       return mlir::failure();\n@@ -281,36 +281,46 @@ LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n   return success();\n }\n \n-// TODO: Interface\n-LogicalResult getForwardEncoding(Attribute sourceEncoding, Operation *op,\n-                                 Attribute &ret) {\n-  if (op->hasTrait<mlir::OpTrait::Elementwise>()) {\n-    ret = sourceEncoding;\n-    return success();\n-  }\n-  if (isa<triton::ReduceOp>(op)) {\n-    ret = Attribute();\n-    return success();\n+inline bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n+  // Case 1: A size 1 tensor is not expensive since all threads will load the\n+  // same\n+  if (isSingleValue(op->getOperand(0)))\n+    return false;\n+  auto ptr = op->getOperand(0);\n+  if (auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>()) {\n+    auto encoding = tensorTy.getEncoding();\n+    // Case 2: Different type conversion is expensive (e.g., mma <-> block)\n+    if (encoding.getTypeID() != targetEncoding.getTypeID())\n+      return true;\n+    auto sizePerThread = triton::gpu::getSizePerThread(encoding);\n+    auto targetSizePerThread = triton::gpu::getSizePerThread(targetEncoding);\n+    auto order = triton::gpu::getOrder(encoding);\n+    auto targetOrder = triton::gpu::getOrder(targetEncoding);\n+    // Case 3: The targeEncoding may expose more vectorization opportunities\n+    return sizePerThread[order[0]] >= targetSizePerThread[targetOrder[0]];\n   }\n-  return failure();\n+  return false;\n }\n \n-inline bool expensive_to_remat(Operation *op) {\n+inline bool expensiveToRemat(Operation *op, Attribute &targetEncoding) {\n   if (!op)\n     return true;\n+  if (isa<triton::LoadOp, triton::StoreOp>(op))\n+    return expensiveLoadOrStore(op, targetEncoding);\n   if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n-          triton::gpu::InsertSliceAsyncOp, triton::LoadOp, triton::StoreOp,\n-          triton::AtomicRMWOp, triton::AtomicCASOp, triton::DotOp>(op))\n+          triton::gpu::InsertSliceAsyncOp, triton::AtomicRMWOp,\n+          triton::AtomicCASOp, triton::DotOp>(op))\n     return true;\n-  if (isa<scf::YieldOp, scf::ForOp>(op))\n+  if (isa<scf::YieldOp, scf::ForOp, scf::IfOp, scf::WhileOp, scf::ConditionOp>(\n+          op))\n     return true;\n   return false;\n }\n \n LogicalResult simulateBackwardRematerialization(\n     Operation *initOp, SetVector<Operation *> &processed,\n     SetVector<Attribute> &layout, llvm::MapVector<Value, Attribute> &toConvert,\n-    Attribute targetEncoding) {\n+    const Attribute &targetEncoding) {\n   // DFS\n   std::vector<std::pair<Operation *, Attribute>> queue;\n   queue.emplace_back(initOp, targetEncoding);\n@@ -324,34 +334,38 @@ LogicalResult simulateBackwardRematerialization(\n     queue.pop_back();\n     // If the current operation is expensive to rematerialize,\n     // we stop everything\n-    if (expensive_to_remat(currOp))\n-      return mlir::failure();\n-    // we would propagate the conversion here\n+    if (expensiveToRemat(currOp, currLayout))\n+      break;\n+    // A conversion will be removed here (i.e. transferred to operands)\n     numCvts -= 1;\n-    // check if the conversion could be folded at this operation\n-    if (isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n-            triton::MakeRangeOp, triton::SplatOp>(*currOp))\n-      continue;\n-    // done processing\n+    // Done processing\n     processed.insert(currOp);\n     layout.insert(currLayout);\n-    // add all operands to the queue\n+    // Add all operands to the queue\n     for (Value argI : currOp->getOperands()) {\n       Attribute newEncoding;\n-      // cannot invert the current encoding for this operand\n+      // Cannot invert the current encoding for this operand\n       // we stop everything\n-      if (failed(invertEncoding(currLayout, currOp, newEncoding))) {\n+      if (failed(invertEncoding(currLayout, currOp, newEncoding)))\n         return mlir::failure();\n-      }\n       if (toConvert.count(argI) && toConvert[argI] != newEncoding)\n         return mlir::failure();\n-      //\n       Operation *opArgI = argI.getDefiningOp();\n       toConvert.insert({argI, newEncoding});\n-      if (!opArgI || processed.contains(opArgI) ||\n-          (opArgI->getBlock() != initOp->getBlock()))\n+      // 1. Only convert RankedTensorType\n+      // 2. Skip if there's no defining op\n+      // 3. Skip if the defining op has already been processed\n+      // 4. Skip or the defining op is in a different block\n+      if (!argI.getType().isa<RankedTensorType>() || !opArgI ||\n+          processed.contains(opArgI) ||\n+          opArgI->getBlock() != currOp->getBlock())\n         continue;\n-      // we add one expensive conversion for the current operand\n+      // If the conversion can be folded into opArgI then\n+      // we don't count this conversion as expensive\n+      if (isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n+              triton::MakeRangeOp, triton::SplatOp>(*opArgI))\n+        continue;\n+      // We add one expensive conversion for the current operand\n       numCvts += 1;\n       queue.emplace_back(opArgI, newEncoding);\n     }\n@@ -375,12 +389,12 @@ Operation *cloneWithInferType(mlir::PatternRewriter &rewriter, Operation *op,\n   newOp->getResult(0).setType(newType);\n   auto typeInfer = dyn_cast<InferTypeOpInterface>(newOp);\n   if (typeInfer) {\n-    SmallVector<Type, 1> newType;\n+    SmallVector<Type, 1> newTypes;\n     auto success = typeInfer.inferReturnTypes(\n         newOp->getContext(), newOp->getLoc(), newOp->getOperands(),\n-        newOp->getAttrDictionary(), newOp->getRegions(), newType);\n+        newOp->getAttrDictionary(), newOp->getRegions(), newTypes);\n     if (succeeded(success))\n-      newOp->getResult(0).setType(newType.front());\n+      newOp->getResult(0).setType(newTypes.front());\n   }\n   return newOp;\n }\n@@ -395,56 +409,83 @@ class MoveConvertOutOfIf : public mlir::RewritePattern {\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n     auto ifOp = cast<scf::IfOp>(*op);\n+    // If \u201cscf.if\u201d defines no values, \u201cscf.yield\u201d will be inserted implicitly.\n+    // However, \"scf.else\" is not required to be present, so we need to check\n+    // if it exists.\n     auto thenYield = ifOp.thenYield();\n-    auto elseYield = ifOp.elseYield();\n     int numOps = thenYield.getNumOperands();\n     SmallVector<Value> newThenYieldOps = thenYield.getOperands();\n-    SmallVector<Value> newElseYieldOps = elseYield.getOperands();\n     SetVector<Operation *> thenCvts;\n-    SetVector<Operation *> elseCvts;\n     SmallVector<Type> newRetTypes;\n \n+    bool hasElse = !ifOp.getElseRegion().empty();\n+\n+    scf::YieldOp elseYield;\n+    SmallVector<Value> newElseYieldOps;\n+    SetVector<Operation *> elseCvts;\n+    if (hasElse) {\n+      elseYield = ifOp.elseYield();\n+      newElseYieldOps = elseYield.getOperands();\n+    }\n+\n     BlockAndValueMapping mapping;\n     for (size_t i = 0; i < numOps; i++) {\n       auto thenCvt = dyn_cast<triton::gpu::ConvertLayoutOp>(\n           thenYield.getOperand(i).getDefiningOp());\n-      auto elseCvt = dyn_cast<triton::gpu::ConvertLayoutOp>(\n-          elseYield.getOperand(i).getDefiningOp());\n-      if (thenCvt && elseCvt &&\n-          std::distance(thenCvt->user_begin(), thenCvt->user_end()) == 1 &&\n-          std::distance(elseCvt->user_begin(), elseCvt->user_end()) == 1 &&\n-          thenCvt.getOperand().getType() == elseCvt.getOperand().getType()) {\n-        mapping.map(thenCvt.getResult(), thenCvt.getOperand());\n-        mapping.map(elseCvt.getResult(), elseCvt.getOperand());\n-        newRetTypes.push_back(thenCvt.getOperand().getType());\n-        thenCvts.insert((Operation *)thenCvt);\n-        elseCvts.insert((Operation *)elseCvt);\n-      } else\n-        newRetTypes.push_back(thenYield.getOperand(i).getType());\n+      if (hasElse) {\n+        auto elseYield = ifOp.elseYield();\n+        auto elseCvt = dyn_cast<triton::gpu::ConvertLayoutOp>(\n+            elseYield.getOperand(i).getDefiningOp());\n+        if (thenCvt && elseCvt &&\n+            std::distance(elseCvt->user_begin(), elseCvt->user_end()) == 1 &&\n+            std::distance(thenCvt->user_begin(), thenCvt->user_end()) == 1 &&\n+            thenCvt.getOperand().getType() == elseCvt.getOperand().getType()) {\n+          // If thenCvt and elseCvt's type are the same, it means a single\n+          // conversion is enough to replace both of them. We can move the\n+          // conversion out of scf.if and replace both thenCvt and elseCvt with\n+          // the new conversion.\n+          mapping.map(thenCvt.getResult(), thenCvt.getOperand());\n+          thenCvts.insert((Operation *)thenCvt);\n+          newRetTypes.push_back(thenCvt.getOperand().getType());\n+          mapping.map(elseCvt.getResult(), elseCvt.getOperand());\n+          elseCvts.insert((Operation *)elseCvt);\n+        } else\n+          // Cannot move out of scf.if because thenCvt != elseCvt\n+          // Moving it out of scf.if will introduce a new conversion\n+          newRetTypes.push_back(thenYield.getOperand(i).getType());\n+      } else {\n+        if (thenCvt &&\n+            std::distance(thenCvt->user_begin(), thenCvt->user_end()) == 1) {\n+          // If there's only a single use of the conversion then we can move it\n+          mapping.map(thenCvt.getResult(), thenCvt.getOperand());\n+          thenCvts.insert((Operation *)thenCvt);\n+          newRetTypes.push_back(thenCvt.getOperand().getType());\n+        } else\n+          // Cannot move out of scf.if because either there's another use of\n+          // the conversion or there's no conversion at all\n+          newRetTypes.push_back(thenYield.getOperand(i).getType());\n+      }\n     }\n     if (mapping.getValueMap().empty())\n       return mlir::failure();\n \n-    rewriter.setInsertionPoint(op);\n     auto newIfOp = rewriter.create<scf::IfOp>(ifOp.getLoc(), newRetTypes,\n-                                              ifOp.getCondition(), true);\n-    // rematerialize `then` block\n-    rewriter.setInsertionPointToEnd(newIfOp.thenBlock());\n-    for (Operation &op : ifOp.thenBlock()->getOperations()) {\n-      if (thenCvts.contains(&op)) {\n-        mapping.map(op.getResult(0), mapping.lookup(op.getOperand(0)));\n-        continue;\n-      }\n-      rewriter.clone(op, mapping);\n-    }\n-    // rematerialize `else` block\n-    rewriter.setInsertionPointToEnd(newIfOp.elseBlock());\n-    for (Operation &op : ifOp.elseBlock()->getOperations()) {\n-      if (elseCvts.contains(&op)) {\n-        mapping.map(op.getResult(0), mapping.lookup(op.getOperand(0)));\n-        continue;\n+                                              ifOp.getCondition(), hasElse);\n+    auto rematerialize = [&](Block *block, SetVector<Operation *> &cvts) {\n+      for (Operation &op : block->getOperations()) {\n+        if (cvts.contains(&op)) {\n+          if (mapping.contains(op.getOperand(0)))\n+            mapping.map(op.getResult(0), mapping.lookup(op.getOperand(0)));\n+          continue;\n+        }\n+        rewriter.clone(op, mapping);\n       }\n-      rewriter.clone(op, mapping);\n+    };\n+    rewriter.setInsertionPointToEnd(newIfOp.thenBlock());\n+    rematerialize(ifOp.thenBlock(), thenCvts);\n+    if (hasElse) {\n+      rewriter.setInsertionPointToEnd(newIfOp.elseBlock());\n+      rematerialize(ifOp.elseBlock(), elseCvts);\n     }\n \n     rewriter.setInsertionPointAfter(newIfOp);\n@@ -477,6 +518,7 @@ class FoldConvertAndReduce : public mlir::RewritePattern {\n         cvt.getOperand().getType().cast<RankedTensorType>().getEncoding();\n     auto dstEncoding =\n         cvt.getResult().getType().cast<RankedTensorType>().getEncoding();\n+    // XXX: why is this needed?\n     if (srcEncoding.isa<triton::gpu::SliceEncodingAttr>())\n       return failure();\n     SetVector<Operation *> cvtSlices;\n@@ -493,7 +535,7 @@ class FoldConvertAndReduce : public mlir::RewritePattern {\n     llvm::MapVector<Value, Attribute> toConvert;\n     for (Operation *op : cvtSlices) {\n       // don't rematerialize anything expensive\n-      if (expensive_to_remat(op))\n+      if (expensiveToRemat(op, srcEncoding))\n         return failure();\n       // don't rematerialize non-element-wise\n       if (!op->hasTrait<mlir::OpTrait::Elementwise>())\n@@ -580,50 +622,8 @@ class RematerializeBackward : public mlir::RewritePattern {\n     SetVector<Attribute> layout;\n     llvm::MapVector<Value, Attribute> toConvert;\n     std::vector<std::pair<Operation *, Attribute>> queue;\n-    queue.emplace_back(cvt, targetType.getEncoding());\n-    int numCvts = 1;\n-    while (!queue.empty()) {\n-      Operation *currOp;\n-      Attribute currLayout;\n-      std::tie(currOp, currLayout) = queue.back();\n-      queue.pop_back();\n-      // If the current operation is expensive to rematerialize,\n-      // we stop everything\n-      if (expensive_to_remat(currOp))\n-        break;\n-      // a conversion will be removed here (i.e. transferred to operands)\n-      numCvts -= 1;\n-      // done processing\n-      processed.insert(currOp);\n-      layout.insert(currLayout);\n-      // add all operands to the queue\n-      for (Value argI : currOp->getOperands()) {\n-        Attribute newEncoding;\n-        // cannot invert the current encoding for this operand\n-        // we stop everything\n-        if (failed(invertEncoding(currLayout, currOp, newEncoding)))\n-          return mlir::failure();\n-        if (toConvert.count(argI) && toConvert[argI] != newEncoding)\n-          return mlir::failure();\n-        //\n-        Operation *opArgI = argI.getDefiningOp();\n-        toConvert.insert({argI, newEncoding});\n-        if (!opArgI || processed.contains(opArgI) ||\n-            (opArgI->getBlock() != cvt->getBlock()))\n-          continue;\n-        // if the conversion can be folded into opArgI then\n-        // we don't count this conversion as expensive\n-        if (isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n-                triton::MakeRangeOp, triton::SplatOp>(*opArgI))\n-          continue;\n-        // we add one expensive conversion for the current operand\n-        numCvts += 1;\n-        queue.emplace_back(opArgI, newEncoding);\n-      }\n-    }\n-    // if rematerialization would add more conversions than it removes\n-    // then we don't do it\n-    if (numCvts > 0)\n+    if (failed(simulateBackwardRematerialization(\n+            cvt, processed, layout, toConvert, targetType.getEncoding())))\n       return mlir::failure();\n \n     SmallVector<Value, 4> sortedValues;\n@@ -825,7 +825,8 @@ class RematerializeForward : public mlir::RewritePattern {\n       for (Value arg : op->getOperands()) {\n         Operation *argOp = arg.getDefiningOp();\n         if (argOp && (argOp != cvt) &&\n-            !isa<arith::ConstantOp, triton::SplatOp>(argOp)) {\n+            !isa<arith::ConstantOp, triton::SplatOp, triton::MakeRangeOp>(\n+                argOp)) {\n           return failure();\n         }\n       }\n@@ -867,8 +868,11 @@ int computeCapabilityToMMAVersion(int computeCapability) {\n     return 1;\n   } else if (computeCapability < 90) {\n     return 2;\n+  } else if (computeCapability < 100) {\n+    // FIXME: temporarily add this to pass unis tests\n+    return 2;\n   } else {\n-    assert(false && \"computeCapability > 90 not supported\");\n+    assert(false && \"computeCapability > 100 not supported\");\n     return 3;\n   }\n }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 21, "deletions": 8, "changes": 29, "file_content_changes": "@@ -1,5 +1,7 @@\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/BlockAndValueMapping.h\"\n+#include \"mlir/IR/TypeUtilities.h\"\n+#include \"triton/Analysis/AxisInfo.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n \n@@ -62,13 +64,13 @@ class LoopPipeliner {\n   DenseMap<Value, SmallVector<Value>> valueMapping;\n \n   /// Block arguments that loads depend on\n-  DenseSet<BlockArgument> depArgs;\n+  SetVector<BlockArgument> depArgs;\n \n   /// Operations (inside the loop body) that loads depend on\n-  DenseSet<Operation *> depOps;\n+  SetVector<Operation *> depOps;\n \n   /// collect values that v depends on and are defined inside the loop\n-  void collectDeps(Value v, int stages, DenseSet<Value> &deps);\n+  void collectDeps(Value v, int stages, SetVector<Value> &deps);\n \n   void setValueMapping(Value origin, Value newValue, int stage);\n \n@@ -112,7 +114,7 @@ Value LoopPipeliner::lookupOrDefault(Value origin, int stage) {\n   return valueMapping[origin][stage];\n }\n \n-void LoopPipeliner::collectDeps(Value v, int stages, DenseSet<Value> &deps) {\n+void LoopPipeliner::collectDeps(Value v, int stages, SetVector<Value> &deps) {\n   // Loop-invariant value, skip\n   if (v.getParentRegion() != &forOp.getLoopBody())\n     return;\n@@ -158,20 +160,31 @@ ttg::AllocTensorOp LoopPipeliner::allocateEmptyBuffer(Operation *op,\n LogicalResult LoopPipeliner::initialize() {\n   Block *loop = forOp.getBody();\n \n+  AxisInfoAnalysis axisInfoAnalysis(forOp.getContext());\n+  axisInfoAnalysis.run(forOp->getParentOfType<ModuleOp>());\n+\n   // can we use forOp.walk(...) here?\n   SmallVector<triton::LoadOp, 2> allLoads;\n   for (Operation &op : *loop)\n-    if (auto loadOp = dyn_cast<triton::LoadOp>(&op))\n-      allLoads.push_back(loadOp);\n+    if (auto loadOp = dyn_cast<triton::LoadOp>(&op)) {\n+      auto ptr = loadOp.ptr();\n+      unsigned vec = axisInfoAnalysis.getPtrContiguity(ptr);\n+      auto ty = getElementTypeOrSelf(ptr.getType())\n+                    .cast<triton::PointerType>()\n+                    .getPointeeType();\n+      unsigned width = vec * ty.getIntOrFloatBitWidth();\n+      if (width >= 32)\n+        allLoads.push_back(loadOp);\n+    }\n \n   // Early stop: no need to continue if there is no load in the loop.\n   if (allLoads.empty())\n     return failure();\n \n   // load => values that it depends on\n-  DenseMap<Value, DenseSet<Value>> loadDeps;\n+  DenseMap<Value, SetVector<Value>> loadDeps;\n   for (triton::LoadOp loadOp : allLoads) {\n-    DenseSet<Value> deps;\n+    SetVector<Value> deps;\n     for (Value op : loadOp->getOperands())\n       collectDeps(op, numStages - 1, deps);\n     loadDeps[loadOp] = deps;"}, {"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -34,15 +34,15 @@ std::string translateLLVMIRToPTX(llvm::Module &module, int cc, int version) {\n   // LLVM version in use may not officially support target hardware.\n   // Supported versions for LLVM 14 are here:\n   // https://github.com/llvm/llvm-project/blob/f28c006a5895fc0e329fe15fead81e37457cb1d1/clang/include/clang/Basic/BuiltinsNVPTX.def\n-  int maxPTX = std::min(75, version);\n-  int maxCC = std::min(86, cc);\n+  int maxPTX = std::min(80, version);\n+  int maxCC = std::min(90, cc);\n   // options\n   auto options = llvm::cl::getRegisteredOptions();\n   auto *shortPtr =\n       static_cast<llvm::cl::opt<bool> *>(options[\"nvptx-short-ptr\"]);\n   assert(shortPtr);\n   shortPtr->setValue(true);\n-  std::string sm = \"sm_\" + std::to_string(maxCC);\n+  std::string sm = cc == 90 ? \"sm_90a\" : \"sm_\" + std::to_string(cc);\n   // max PTX version\n   int ptxMajor = maxPTX / 10;\n   int ptxMinor = maxPTX % 10;"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 60, "deletions": 16, "changes": 76, "file_content_changes": "@@ -197,7 +197,8 @@ void init_triton_ir(py::module &&m) {\n       .def(\"replace_all_uses_with\",\n            [](mlir::Value &self, mlir::Value &newValue) {\n              self.replaceAllUsesWith(newValue);\n-           });\n+           })\n+      .def(\"get_type\", &mlir::Value::getType);\n \n   py::class_<mlir::BlockArgument, mlir::Value>(m, \"block_argument\");\n \n@@ -211,6 +212,11 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::Block &self, int index) -> mlir::BlockArgument {\n              return self.getArgument(index);\n            })\n+      .def(\"add_argument\",\n+           [](mlir::Block &self, mlir::Type ty) {\n+             auto loc = mlir::UnknownLoc::get(ty.getContext());\n+             self.addArgument(ty, loc);\n+           })\n       .def(\"get_num_arguments\", &mlir::Block::getNumArguments)\n       .def(\"dump\", &mlir::Block::dump)\n       .def(\"move_before\", &mlir::Block::moveBefore)\n@@ -226,20 +232,34 @@ void init_triton_ir(py::module &&m) {\n              self.dropAllUses();\n              self.erase();\n            })\n-      .def(\"replace_use_in_block_with\", [](mlir::Block &self, mlir::Value &v,\n-                                           mlir::Value &newVal) {\n-        v.replaceUsesWithIf(newVal, [&](mlir::OpOperand &operand) {\n-          mlir::Operation *user = operand.getOwner();\n-          mlir::Block *currentBlock = user->getBlock();\n-          while (currentBlock) {\n-            if (currentBlock == &self)\n-              return true;\n-            // Move up one level\n-            currentBlock = currentBlock->getParent()->getParentOp()->getBlock();\n-          }\n-          return false;\n-        });\n-      });\n+      .def(\"replace_use_in_block_with\",\n+           [](mlir::Block &self, mlir::Value &v, mlir::Value &newVal) {\n+             v.replaceUsesWithIf(newVal, [&](mlir::OpOperand &operand) {\n+               mlir::Operation *user = operand.getOwner();\n+               mlir::Block *currentBlock = user->getBlock();\n+               while (currentBlock) {\n+                 if (currentBlock == &self)\n+                   return true;\n+                 // Move up one level\n+                 currentBlock =\n+                     currentBlock->getParent()->getParentOp()->getBlock();\n+               }\n+               return false;\n+             });\n+           })\n+      .def(\"__str__\",\n+           [](mlir::Block &self) {\n+             std::string str;\n+             llvm::raw_string_ostream os(str);\n+             self.print(os);\n+             return str;\n+           })\n+      .def(\"has_terminator\",\n+           [](mlir::Block &self) {\n+             return !self.empty() &&\n+                    self.back().hasTrait<mlir::OpTrait::IsTerminator>();\n+           })\n+      .def(\"erase\", [](mlir::Block &self) { self.erase(); });\n \n   // using eattr = ir::attribute_kind_t;\n   // py::enum_<eattr>(m, \"attribute_kind\")\n@@ -435,6 +455,10 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::OpBuilder &self, mlir::Block &block) {\n              self.setInsertionPointToEnd(&block);\n            })\n+      .def(\"set_insertion_point_after\",\n+           [](mlir::OpBuilder &self, mlir::Operation &op) {\n+             self.setInsertionPointAfter(&op);\n+           })\n       .def(\n           \"get_insertion_block\",\n           [](mlir::OpBuilder &self) -> mlir::Block * {\n@@ -622,6 +646,22 @@ void init_triton_ir(py::module &&m) {\n             return new mlir::Block();\n           },\n           ret::reference)\n+      // Unstructured control flow\n+      .def(\"create_cond_branch\",\n+           [](mlir::OpBuilder &self, mlir::Value condition,\n+              mlir::Block *trueDest, mlir::Block *falseDest) {\n+             auto loc = self.getUnknownLoc();\n+             self.create<mlir::CondBranchOp>(loc, condition, trueDest,\n+                                             falseDest);\n+             return;\n+           })\n+      .def(\"create_branch\",\n+           [](mlir::OpBuilder &self, mlir::Block *dest,\n+              std::vector<mlir::Value> &args) {\n+             auto loc = self.getUnknownLoc();\n+             self.create<mlir::BranchOp>(loc, dest, args);\n+             return;\n+           })\n       // Structured control flow\n       .def(\"create_for_op\",\n            [](mlir::OpBuilder &self, mlir::Value &lb, mlir::Value &ub,\n@@ -1393,6 +1433,7 @@ void init_triton_translation(py::module &m) {\n   m.def(\n       \"translate_triton_gpu_to_llvmir\",\n       [](mlir::ModuleOp op, int computeCapability) {\n+        py::gil_scoped_release allow_threads;\n         llvm::LLVMContext llvmContext;\n         auto llvmModule = ::mlir::triton::translateTritonGPUToLLVMIR(\n             &llvmContext, op, computeCapability);\n@@ -1410,6 +1451,7 @@ void init_triton_translation(py::module &m) {\n   m.def(\n       \"translate_llvmir_to_ptx\",\n       [](const std::string llvmIR, int capability, int version) -> std::string {\n+        py::gil_scoped_release allow_threads;\n         // create LLVM module from C++\n         llvm::LLVMContext context;\n         std::unique_ptr<llvm::MemoryBuffer> buffer =\n@@ -1453,7 +1495,9 @@ void init_triton_translation(py::module &m) {\n           std::string cmd;\n           int err;\n           cmd = ptxasPath + \" -v --gpu-name=sm_\" + std::to_string(capability) +\n-                \" \" + _fsrc + \" -o \" + _fsrc + \".o 2> \" + _flog;\n+                (capability == 90 ? \"a \" : \" \") + _fsrc + \" -o \" + _fsrc +\n+                \".o 2> \" + _flog;\n+\n           err = system(cmd.c_str());\n           if (err != 0) {\n             std::ifstream _log(_flog);"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 148, "deletions": 15, "changes": 163, "file_content_changes": "@@ -1648,24 +1648,10 @@ def _kernel(dst):\n # -------------\n \n \n-def system_libdevice_path() -> str:\n-    _SYSTEM_LIBDEVICE_SEARCH_PATHS = [\n-        '/usr/lib/cuda/nvvm/libdevice/libdevice.10.bc',\n-        '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc',\n-    ]\n-    SYSTEM_LIBDEVICE_PATH: Optional[str] = None\n-    for _p in _SYSTEM_LIBDEVICE_SEARCH_PATHS:\n-        if os.path.exists(_p):\n-            SYSTEM_LIBDEVICE_PATH = _p\n-    assert SYSTEM_LIBDEVICE_PATH is not None, \\\n-        \"Could not find libdevice.10.bc path\"\n-    return SYSTEM_LIBDEVICE_PATH\n-\n-\n @pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n                          [('int32', 'libdevice.ffs', ''),\n                           ('float32', 'libdevice.log2', ''),\n-                          ('float32', 'libdevice.pow', system_libdevice_path()),\n+                          ('float32', 'libdevice.pow', tl.libdevice.LIBDEVICE_PATH),\n                           ('float64', 'libdevice.norm4d', '')])\n def test_libdevice_tensor(dtype_str, expr, lib_path):\n \n@@ -1736,6 +1722,140 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n     # compare\n     np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n \n+# -----------------------\n+# test control flow\n+# -----------------------\n+\n+\n+def test_if_else():\n+\n+    @triton.jit\n+    def kernel(Cond, TrueVal, FalseVal, Out):\n+        if tl.load(Cond):\n+            val = tl.load(TrueVal)\n+        else:\n+            val = tl.load(FalseVal)\n+        tl.store(Out, val)\n+\n+    out = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    true_val = to_triton(np.full((1,), 1, dtype=np.int32), device='cuda')\n+    false_val = to_triton(np.full((1,), 2, dtype=np.int32), device='cuda')\n+    cond = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    # True\n+    cond[0] = True\n+    kernel[(1,)](cond, true_val, false_val, out)\n+    assert to_numpy(out)[0] == true_val[0]\n+    # False\n+    cond[0] = False\n+    kernel[(1,)](cond, true_val, false_val, out)\n+    assert to_numpy(out)[0] == false_val[0]\n+\n+\n+def test_if_return():\n+\n+    @triton.jit\n+    def kernel(ExitEarly, Out):\n+        if tl.load(ExitEarly):\n+            tl.store(Out, 0)\n+            return\n+        tl.store(Out, 1)\n+\n+    out = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    exit_early = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    # exit early path taken\n+    exit_early[0] = 1\n+    kernel[(1,)](exit_early, out)\n+    assert to_numpy(out)[0] == 0\n+    # exit early path not taken\n+    exit_early[0] = 0\n+    kernel[(1,)](exit_early, out)\n+    assert to_numpy(out)[0] == 1\n+\n+\n+@pytest.mark.parametrize(\"_cond1\", [True, False])\n+@pytest.mark.parametrize(\"_cond2\", [True, False])\n+@pytest.mark.parametrize(\"_cond3\", [True, False])\n+def test_nested_if_else_return(_cond1, _cond2, _cond3):\n+\n+    @triton.jit\n+    def kernel(Cond1, Cond2, Cond3, Val1, Val2, Val3, Out):\n+        val = 0\n+        if tl.load(Cond1):\n+            if tl.load(Cond2):\n+                val = tl.load(Val1)\n+            else:\n+                return\n+        else:\n+            if tl.load(Cond3):\n+                val = tl.load(Val2)\n+            else:\n+                val = tl.load(Val3)\n+        tl.store(Out, val)\n+\n+    out = to_triton(np.full((1,), -1, dtype=np.int32), device='cuda')\n+    cond1 = to_triton(np.full((1,), _cond1, dtype=np.int32), device='cuda')\n+    cond2 = to_triton(np.full((1,), _cond2, dtype=np.int32), device='cuda')\n+    cond3 = to_triton(np.full((1,), _cond3, dtype=np.int32), device='cuda')\n+    val1 = to_triton(np.full((1,), 1, dtype=np.int32), device='cuda')\n+    val2 = to_triton(np.full((1,), 2, dtype=np.int32), device='cuda')\n+    val3 = to_triton(np.full((1,), 3, dtype=np.int32), device='cuda')\n+    kernel[(1,)](cond1, cond2, cond3, val1, val2, val3, out)\n+    targets = {\n+        (True, True, True): val1[0],\n+        (True, True, False): val1[0],\n+        (True, False, True): out[0],\n+        (True, False, False): out[0],\n+        (False, True, True): val2[0],\n+        (False, True, False): val3[0],\n+        (False, False, True): val2[0],\n+        (False, False, False): val3[0],\n+    }\n+    assert out[0] == targets[(_cond1, _cond2, _cond3)]\n+\n+\n+def test_while():\n+\n+    @triton.jit\n+    def kernel(InitI, Bound, CutOff, OutI, OutJ):\n+        init_i = tl.load(InitI)\n+        curr_i = init_i\n+        j = 0\n+        while curr_i == init_i and j < tl.load(Bound):\n+            curr_i = curr_i + (j == tl.load(CutOff))\n+            j += 1\n+        tl.store(OutI, curr_i)\n+        tl.store(OutJ, j)\n+\n+    out_i = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    out_j = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    init_i = to_triton(np.full((1,), 1, dtype=np.int32), device='cuda')\n+    bound = to_triton(np.full((1,), 10, dtype=np.int32), device='cuda')\n+    cut_off = to_triton(np.full((1,), 5, dtype=np.int32), device='cuda')\n+    kernel[(1,)](init_i, bound, cut_off, out_i, out_j)\n+    assert out_i[0] == init_i[0] + 1\n+    assert out_j[0] == cut_off[0] + 1\n+\n+# def test_for_if():\n+\n+#     @triton.jit\n+#     def kernel(bound, cutoff, M, N):\n+#         m = 0\n+#         n = 0\n+#         for i in range(bound):\n+#             if i > cutoff:\n+#                 m = m + 1\n+#             else:\n+#                 n = n + 1\n+#         tl.store(M, m)\n+#         tl.store(N, n)\n+\n+#     m = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+#     n = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+#     kernel[(1,)](10, 7, m, n)\n+#     print(m[0])\n+#     print(n[0])\n+\n+\n # -----------------------\n # test layout conversions\n # -----------------------\n@@ -1827,3 +1947,16 @@ def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n     kernel[(1, 1, 1)](x.data_ptr(), z.data_ptr())\n \n     assert torch.equal(z, x)\n+\n+\n+def test_load_scalar_with_mask():\n+    @triton.jit\n+    def kernel(Input, Index, Out, N: int):\n+        index = tl.load(Index)\n+        scalar = tl.load(Input + index, mask=index < N, other=0)\n+        tl.store(Out, scalar, mask=index < N)\n+    Index = torch.tensor([0], dtype=torch.int32, device='cuda')\n+    Input = torch.tensor([0], dtype=torch.int32, device='cuda')\n+    Out = torch.empty_like(Index, device='cuda')\n+    kernel[(1,)](Input, Index, Out, Index.numel())\n+    assert Out.data[0] == 0"}, {"filename": "python/test/unit/operators/test_flash_attention.py", "status": "added", "additions": 43, "deletions": 0, "changes": 43, "file_content_changes": "@@ -0,0 +1,43 @@\n+import pytest\n+import torch\n+\n+import triton\n+\n+\n+@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n+def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n+    capability = torch.cuda.get_device_capability()\n+    if capability[0] < 8:\n+        pytest.skip(\"Flash attention only supported for compute capability < 80\")\n+    torch.manual_seed(20)\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n+    sm_scale = 0.2\n+    dout = torch.randn_like(q)\n+    # reference implementation\n+    M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n+    p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n+    for z in range(Z):\n+        for h in range(H):\n+            p[:, :, M == 0] = float(\"-inf\")\n+    p = torch.softmax(p.float(), dim=-1).half()\n+    # p = torch.exp(p)\n+    ref_out = torch.matmul(p, v)\n+    ref_out.backward(dout)\n+    ref_dv, v.grad = v.grad.clone(), None\n+    ref_dk, k.grad = k.grad.clone(), None\n+    ref_dq, q.grad = q.grad.clone(), None\n+    # # triton implementation\n+    tri_out = triton.ops.attention(q, k, v, sm_scale)\n+    # print(ref_out)\n+    # print(tri_out)\n+    tri_out.backward(dout)\n+    tri_dv, v.grad = v.grad.clone(), None\n+    tri_dk, k.grad = k.grad.clone(), None\n+    tri_dq, q.grad = q.grad.clone(), None\n+    # compare\n+    triton.testing.assert_almost_equal(ref_out, tri_out)\n+    triton.testing.assert_almost_equal(ref_dv, tri_dv)\n+    triton.testing.assert_almost_equal(ref_dk, tri_dk)\n+    triton.testing.assert_almost_equal(ref_dq, tri_dq)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 218, "deletions": 137, "changes": 355, "file_content_changes": "@@ -97,10 +97,11 @@ def __enter__(self):\n         self.prev_defs = self.generator.local_defs.copy()\n         self.generator.local_defs = {}\n         self.insert_block = self.generator.builder.get_insertion_block()\n+        self.insert_point = self.generator.builder.get_insertion_point()\n         return self.liveins, self.insert_block\n \n     def __exit__(self, *args, **kwargs):\n-        self.generator.builder.set_insertion_point_to_end(self.insert_block)\n+        self.generator.builder.restore_insertion_point(self.insert_point)\n         self.generator.lscope = self.liveins\n         self.generator.local_defs = self.prev_defs\n \n@@ -127,6 +128,7 @@ def __init__(self, context, prototype, gscope, attributes, constants, function_n\n             'isinstance': isinstance,\n             'getattr': getattr,\n         }\n+        self.scf_stack = []\n         # SSA-construction\n         # name => triton.language.tensor\n         self.local_defs: Dict[str, triton.language.tensor] = {}\n@@ -189,18 +191,25 @@ def visit_List(self, node):\n     # By design, only non-kernel functions can return\n     def visit_Return(self, node):\n         ret_value = self.visit(node.value)\n+        # ret_block = self.builder.create_block()\n+        # post_ret_block = self.builder.create_block()\n+        # self.builder.create_branch(ret_block)\n+        # self.builder.set_insertion_point_to_end(ret_block)\n         if ret_value is None:\n             self.builder.ret([])\n-            return None\n-        if isinstance(ret_value, tuple):\n+            ret_ty = None\n+        elif isinstance(ret_value, tuple):\n             ret_values = [triton.language.core._to_tensor(v, self.builder) for v in ret_value]\n             ret_types = [v.type for v in ret_values]\n             self.builder.ret([v.handle for v in ret_values])\n-            return tuple(ret_types)\n+            ret_ty = tuple(ret_types)\n         else:\n             ret = triton.language.core._to_tensor(ret_value, self.builder)\n             self.builder.ret([ret.handle])\n-            return ret.type\n+            ret_ty = ret.type\n+        # self.builder.create_branch(post_ret_block)\n+        # self.builder.set_insertion_point_to_end(post_ret_block)\n+        return ret_ty\n \n     def visit_FunctionDef(self, node):\n         arg_names, kwarg_names = self.visit(node.args)\n@@ -350,81 +359,126 @@ def visit_BinOp(self, node):\n         else:\n             return getattr(lhs, fn)(rhs)\n \n+    def visit_then_else_blocks(self, node, liveins, then_block, else_block):\n+        # then block\n+        self.builder.set_insertion_point_to_start(then_block)\n+        self.visit_compound_statement(node.body)\n+        then_block = self.builder.get_insertion_block()\n+        then_defs = self.local_defs.copy()\n+        # else block\n+        else_defs = {}\n+        if node.orelse:\n+            self.builder.set_insertion_point_to_start(else_block)\n+            self.lscope = liveins.copy()\n+            self.local_defs = {}\n+            self.visit_compound_statement(node.orelse)\n+            else_defs = self.local_defs.copy()\n+            else_block = self.builder.get_insertion_block()\n+\n+        # update block arguments\n+        names = []\n+        ret_types = []\n+        ir_ret_types = []\n+        # variables in livein whose value is updated in `if`\n+        for name in liveins:\n+            # check type\n+            for defs, block_name in [(then_defs, 'then'), (else_defs, 'else')]:\n+                if name in defs:\n+                    assert defs[name].type == liveins[name].type,\\\n+                        f'initial value for `{name}` is of type {liveins[name].type}, '\\\n+                        f'but the {block_name} block redefines it as {defs[name].type}'\n+            if name in then_defs or name in else_defs:\n+                names.append(name)\n+                ret_types.append(then_defs[name].type if name in then_defs else else_defs[name].type)\n+                ir_ret_types.append(then_defs[name].handle.get_type() if name in then_defs else else_defs[name].handle.get_type())\n+            # variable defined in then but not in else\n+            if name in then_defs and name not in else_defs:\n+                else_defs[name] = liveins[name]\n+            # variable defined in else but not in then\n+            if name in else_defs and name not in then_defs:\n+                then_defs[name] = liveins[name]\n+        # variables that are both in then and else but not in liveins\n+        # TODO: could probably be cleaned up\n+        for name in then_defs.keys() & else_defs.keys():\n+            if name in names:\n+                continue\n+            then_ty = then_defs[name].type\n+            else_ty = else_defs[name].type\n+            assert then_ty == else_ty,\\\n+                f'mismatched type for {name} between then block ({then_ty}) '\\\n+                f'and else block ({else_ty})'\n+            names.append(name)\n+            ret_types.append(then_ty)\n+            ir_ret_types.append(then_defs[name].handle.get_type())\n+\n+        return then_defs, else_defs, then_block, else_block, names, ret_types, ir_ret_types\n+\n+    def visit_if_top_level(self, cond, node):\n+        with enter_sub_region(self) as sr:\n+            liveins, ip_block = sr\n+            then_block = self.builder.create_block()\n+            else_block = self.builder.create_block()\n+            # create basic-block after conditional\n+            endif_block = self.builder.create_block()\n+            # create branch\n+            self.builder.set_insertion_point_to_end(ip_block)\n+            self.builder.create_cond_branch(cond.handle, then_block, else_block)\n+            # visit then and else blocks\n+            then_defs, else_defs, then_block, else_block, names, ret_types, ir_ret_types = \\\n+                self.visit_then_else_blocks(node, liveins, then_block, else_block)\n+            # then terminator\n+            self.builder.set_insertion_point_to_end(then_block)\n+            if not then_block.has_terminator():\n+                self.builder.create_branch(endif_block, [then_defs[n].handle for n in names])\n+            # else terminator\n+            self.builder.set_insertion_point_to_end(else_block)\n+            if not else_block.has_terminator():\n+                self.builder.create_branch(endif_block, [else_defs[n].handle for n in names])\n+            for ty in ir_ret_types:\n+                endif_block.add_argument(ty)\n+        # change block\n+        self.builder.set_insertion_point_to_start(endif_block)\n+        # update value\n+        for i, name in enumerate(names):\n+            new_tensor = triton.language.core.tensor(endif_block.arg(i), ret_types[i])\n+            self.set_value(name, new_tensor)\n+\n+    # TODO: refactor\n+    def visit_if_scf(self, cond, node):\n+        with enter_sub_region(self) as sr:\n+            liveins, _ = sr\n+            ip = self.builder.get_insertion_point()\n+            then_block = self.builder.create_block()\n+            else_block = self.builder.create_block() if node.orelse else None\n+            then_defs, else_defs, then_block, else_block, names, ret_types, _ = \\\n+                self.visit_then_else_blocks(node, liveins, then_block, else_block)\n+            # create if op\n+            self.builder.restore_insertion_point(ip)\n+            if_op = self.builder.create_if_op([ty.to_ir(self.builder) for ty in ret_types], cond.handle, True)\n+            then_block.merge_block_before(if_op.get_then_block())\n+            self.builder.set_insertion_point_to_end(if_op.get_then_block())\n+            if len(names) > 0:\n+                self.builder.create_yield_op([then_defs[n].handle for n in names])\n+            if not node.orelse:\n+                else_block = if_op.get_else_block()\n+            else:\n+                else_block.merge_block_before(if_op.get_else_block())\n+            self.builder.set_insertion_point_to_end(if_op.get_else_block())\n+            if len(names) > 0:\n+                self.builder.create_yield_op([else_defs[n].handle for n in names])\n+        # update values\n+        for i, name in enumerate(names):\n+            new_tensor = triton.language.core.tensor(if_op.get_result(i), ret_types[i])\n+            self.set_value(name, new_tensor)\n+\n     def visit_If(self, node):\n         cond = self.visit(node.test)\n         if isinstance(cond, triton.language.tensor):\n             cond = cond.to(triton.language.int1, _builder=self.builder)\n-            with enter_sub_region(self) as sr:\n-                liveins, ip_block = sr\n-                liveins_copy = liveins.copy()\n-                then_block = self.builder.create_block()\n-                self.builder.set_insertion_point_to_start(then_block)\n-                self.visit_compound_statement(node.body)\n-                then_defs = self.local_defs.copy()\n-\n-                # when need an else block when:\n-                # 1. we have an orelse node\n-                #   or\n-                # 2. the then block defines new variable\n-                else_defs = {}\n-                if then_defs or node.orelse:\n-                    if node.orelse:\n-                        self.lscope = liveins\n-                        self.local_defs = {}\n-                        else_block = self.builder.create_block()\n-                        self.builder.set_insertion_point_to_end(else_block)\n-                        self.visit_compound_statement(node.orelse)\n-                        else_defs = self.local_defs.copy()\n-                    else:\n-                        # collect else_defs\n-                        for name in then_defs:\n-                            if name in liveins:\n-                                assert self.is_triton_tensor(then_defs[name])\n-                                assert self.is_triton_tensor(liveins[name])\n-                                else_defs[name] = liveins[name]\n-                # collect yields\n-                names = []\n-                ret_types = []\n-                for then_name in then_defs:\n-                    for else_name in else_defs:\n-                        if then_name == else_name:\n-                            if then_defs[then_name].type == else_defs[else_name].type:\n-                                names.append(then_name)\n-                                ret_types.append(then_defs[then_name].type)\n-\n-                # defined in else block but not in then block\n-                # to find in parent scope and yield them\n-                for else_name in else_defs:\n-                    if else_name in liveins and else_name not in then_defs:\n-                        if else_defs[else_name].type == liveins[else_name].type:\n-                            names.append(else_name)\n-                            ret_types.append(else_defs[else_name].type)\n-                            then_defs[else_name] = liveins_copy[else_name]\n-                self.builder.set_insertion_point_to_end(ip_block)\n-\n-                if then_defs or node.orelse:  # with else block\n-                    if_op = self.builder.create_if_op([ty.to_ir(self.builder) for ty in ret_types], cond.handle, True)\n-                    then_block.merge_block_before(if_op.get_then_block())\n-                    self.builder.set_insertion_point_to_end(if_op.get_then_block())\n-                    if len(names) > 0:\n-                        self.builder.create_yield_op([then_defs[n].handle for n in names])\n-                    if not node.orelse:\n-                        else_block = if_op.get_else_block()\n-                    else:\n-                        else_block.merge_block_before(if_op.get_else_block())\n-                    self.builder.set_insertion_point_to_end(if_op.get_else_block())\n-                    if len(names) > 0:\n-                        self.builder.create_yield_op([else_defs[n].handle for n in names])\n-                else:  # no else block\n-                    if_op = self.builder.create_if_op([ty.to_ir(self.builder) for ty in ret_types], cond.handle, False)\n-                    then_block.merge_block_before(if_op.get_then_block())\n-\n-            # update values yielded by IfOp\n-            for i, name in enumerate(names):\n-                new_tensor = triton.language.core.tensor(if_op.get_result(i), ret_types[i])\n-                self.lscope[name] = new_tensor\n-                self.local_defs[name] = new_tensor\n-\n+            if self.scf_stack:\n+                self.visit_if_scf(cond, node)\n+            else:\n+                self.visit_if_top_level(cond, node)\n         else:\n             if isinstance(cond, triton.language.constexpr):\n                 cond = cond.value\n@@ -474,12 +528,10 @@ def visit_Compare(self, node):\n \n     def visit_UnaryOp(self, node):\n         op = self.visit(node.operand)\n-        if type(node.op) == ast.Not:\n-            assert isinstance(op, triton.language.constexpr), \"`not` only supported for constexpr at the moment\"\n-            return triton.language.constexpr(not op)\n         fn = {\n             ast.USub: '__neg__',\n             ast.UAdd: '__pos__',\n+            ast.Not: '__not__',\n             ast.Invert: '__invert__',\n         }[type(node.op)]\n         if self.is_triton_tensor(op):\n@@ -490,54 +542,65 @@ def visit_While(self, node):\n         with enter_sub_region(self) as sr:\n             liveins, insert_block = sr\n \n-            # condition (the before region)\n-            cond_block = self.builder.create_block()\n-            self.builder.set_insertion_point_to_start(cond_block)\n-            cond = self.visit(node.test)\n-\n             # loop body (the after region)\n-            loop_block = self.builder.create_block()\n-            self.builder.set_insertion_point_to_start(loop_block)\n+            # loop_block = self.builder.create_block()\n+            dummy = self.builder.create_block()\n+            self.builder.set_insertion_point_to_start(dummy)\n+            self.scf_stack.append(node)\n             self.visit_compound_statement(node.body)\n+            self.scf_stack.pop()\n             loop_defs = self.local_defs\n \n             # collect loop-carried values\n             names = []\n             ret_types = []\n             init_args = []\n-            yields = []\n             for name in loop_defs:\n                 if name in liveins:\n                     # We should not def new constexpr\n                     assert self.is_triton_tensor(loop_defs[name])\n                     assert self.is_triton_tensor(liveins[name])\n-                    if loop_defs[name].type == liveins[name].type:\n-                        # these are loop-carried values\n-                        names.append(name)\n-                        ret_types.append(loop_defs[name].type)\n-                        init_args.append(liveins[name])\n-                        yields.append(loop_defs[name])\n+                    assert loop_defs[name].type == liveins[name].type\n+                    # these are loop-carried values\n+                    names.append(name)\n+                    ret_types.append(loop_defs[name].type)\n+                    init_args.append(liveins[name])\n \n             self.builder.set_insertion_point_to_end(insert_block)\n             while_op = self.builder.create_while_op([ty.to_ir(self.builder) for ty in ret_types],\n                                                     [arg.handle for arg in init_args])\n             # merge the condition region\n             before_block = self.builder.create_block_with_parent(while_op.get_before(),\n                                                                  [ty.to_ir(self.builder) for ty in ret_types])\n-            cond_block.merge_block_before(before_block)\n+            self.builder.set_insertion_point_to_start(before_block)\n+            for i, name in enumerate(names):\n+                self.lscope[name] = triton.language.core.tensor(before_block.arg(i), ret_types[i])\n+                self.local_defs[name] = self.lscope[name]\n+            cond = self.visit(node.test)\n             self.builder.set_insertion_point_to_end(before_block)\n             # create ConditionOp: e.g., scf.condition(%cond) %arg0, %arg1, ...\n             self.builder.create_condition_op(cond.handle, [before_block.arg(i) for i in range(len(init_args))])\n             # merge the loop body\n             after_block = self.builder.create_block_with_parent(while_op.get_after(),\n                                                                 [ty.to_ir(self.builder) for ty in ret_types])\n-            loop_block.merge_block_before(after_block)\n-            self.builder.set_insertion_point_to_end(after_block)\n+\n+            # generate loop body\n+            self.builder.set_insertion_point_to_start(after_block)\n+            for i, name in enumerate(names):\n+                self.lscope[name] = triton.language.core.tensor(after_block.arg(i), ret_types[i])\n+                self.local_defs[name] = self.lscope[name]\n+            self.scf_stack.append(node)\n+            self.visit_compound_statement(node.body)\n+            self.scf_stack.pop()\n+            loop_defs = self.local_defs\n+            yields = []\n+            for name in loop_defs:\n+                if name in liveins:\n+                    yields.append(loop_defs[name])\n             self.builder.create_yield_op([y.handle for y in yields])\n \n         # update global uses in while_op\n         for i, name in enumerate(names):\n-            before_block.replace_use_in_block_with(init_args[i].handle, before_block.arg(i))\n             after_block.replace_use_in_block_with(init_args[i].handle, after_block.arg(i))\n \n         # WhileOp defines new values, update the symbol table (lscope, local_defs)\n@@ -562,29 +625,29 @@ def visit_ExtSlice(self, node):\n         return [self.visit(dim) for dim in node.dims]\n \n     def visit_For(self, node):\n-        iterator = self.visit(node.iter.func)\n-        if iterator != self.builtins['range']:\n-            raise RuntimeError('Only `range` iterator currently supported')\n+        IteratorClass = self.visit(node.iter.func)\n+        iter_args = [self.visit(arg) for arg in node.iter.args]\n+        if IteratorClass == triton.language.static_range:\n+            iterator = IteratorClass(*iter_args)\n+            static_range = range(iterator.start.value,\n+                                 iterator.end.value,\n+                                 iterator.step.value)\n+            for i in static_range:\n+                self.lscope[node.target.id] = triton.language.constexpr(i)\n+                self.visit_compound_statement(node.body)\n+                for stmt in node.orelse:\n+                    ast.NodeVisitor.generic_visit(self, stmt)\n+            return\n+\n+        if IteratorClass != self.builtins['range']:\n+            raise RuntimeError('Only `range` and `static_range` iterators are currently supported')\n+\n         # visit iterator arguments\n         # note: only `range` iterator is supported now\n-        iter_args = [self.visit(arg) for arg in node.iter.args]\n         # collect lower bound (lb), upper bound (ub), and step\n         lb = iter_args[0] if len(iter_args) > 1 else self.visit(ast.Num(0))\n         ub = iter_args[1] if len(iter_args) > 1 else self.visit(node.iter.args[0])\n         step = iter_args[2] if len(iter_args) > 2 else self.visit(ast.Num(1))\n-        # static for loops: all iterator arguments are constexpr\n-        if isinstance(lb, triton.language.constexpr) and \\\n-           isinstance(ub, triton.language.constexpr) and \\\n-           isinstance(step, triton.language.constexpr):\n-            sta_range = iterator(lb.value, ub.value, step.value)\n-            static_unrolling = os.environ.get('TRITON_STATIC_LOOP_UNROLLING', False)\n-            if static_unrolling and len(sta_range) <= 10:\n-                for i in sta_range:\n-                    self.lscope[node.target.id] = triton.language.constexpr(i)\n-                    self.visit_compound_statement(node.body)\n-                    for stmt in node.orelse:\n-                        ast.NodeVisitor.generic_visit(self, stmt)\n-                return\n         # handle negative constant step (not supported by scf.for in MLIR)\n         negative_step = False\n         if isinstance(step, triton.language.constexpr) and step.value < 0:\n@@ -605,13 +668,16 @@ def visit_For(self, node):\n \n         with enter_sub_region(self) as sr:\n             liveins, insert_block = sr\n+            ip = self.builder.get_insertion_point()\n \n             # create loop body block\n             block = self.builder.create_block()\n             self.builder.set_insertion_point_to_start(block)\n-\n-            # visit loop body\n+            # dry visit loop body\n+            self.scf_stack.append(node)\n             self.visit_compound_statement(node.body)\n+            self.scf_stack.pop()\n+            block.erase()\n \n             # If a variable (name) is defined in both its parent & itself, then it's\n             # a loop-carried variable. (They must be of the same type)\n@@ -622,17 +688,35 @@ def visit_For(self, node):\n                 if name in liveins:\n                     assert self.is_triton_tensor(self.local_defs[name]), f'{name} is not tensor'\n                     assert self.is_triton_tensor(liveins[name])\n-                    if self.local_defs[name].type != liveins[name].type:\n-                        local_value = self.local_defs[name]\n-                        self.local_defs[name] = local_value.to(liveins[name].dtype, _builder=self.builder)\n+                    assert self.local_defs[name].type == liveins[name].type,\\\n+                        f'Loop-carried variable {name} has initial type {liveins[name].type} '\\\n+                        f'but is re-assigned to {self.local_defs[name].type} in loop! '\\\n+                        f'Please make sure that the type stays consistent.'\n+\n                     names.append(name)\n                     init_args.append(triton.language.core._to_tensor(liveins[name], self.builder))\n                     yields.append(triton.language.core._to_tensor(self.local_defs[name], self.builder))\n \n             # create ForOp\n-            self.builder.set_insertion_point_to_end(insert_block)\n+            self.builder.restore_insertion_point(ip)\n             for_op = self.builder.create_for_op(lb, ub, step, [arg.handle for arg in init_args])\n-            block.merge_block_before(for_op.get_body(0))\n+\n+            self.scf_stack.append(node)\n+            self.builder.set_insertion_point_to_start(for_op.get_body(0))\n+            for i, name in enumerate(names):\n+                self.set_value(name, triton.language.core.tensor(for_op.get_body(0).arg(i + 1), yields[i].type))\n+            self.visit_compound_statement(node.body)\n+            self.scf_stack.pop()\n+            yields = []\n+            for name in self.local_defs:\n+                if name in liveins:\n+                    yields.append(triton.language.core._to_tensor(self.local_defs[name], self.builder))\n+\n+            # create YieldOp\n+            if len(yields) > 0:\n+                self.builder.create_yield_op([y.handle for y in yields])\n+            for_op_region = for_op.get_body(0).get_parent()\n+            assert for_op_region.size() == 1, \"We use SCF, so the loop body should only have one block\"\n \n             # update induction variable with actual value, and replace all uses\n             self.builder.set_insertion_point_to_start(for_op.get_body(0))\n@@ -643,17 +727,6 @@ def visit_For(self, node):\n             self.lscope[node.target.id].handle.replace_all_uses_with(iv)\n             self.set_value(node.target.id, triton.language.core.tensor(iv, triton.language.core.int32))\n \n-            # create YieldOp\n-            self.builder.set_insertion_point_to_end(for_op.get_body(0))\n-            if len(yields) > 0:\n-                self.builder.create_yield_op([y.handle for y in yields])\n-            for_op_region = for_op.get_body(0).get_parent()\n-            assert for_op_region.size() == 1, \"We use SCF, so the loop body should only have one block\"\n-            # replace global uses with block arguments\n-            for i, name in enumerate(names):\n-                # arg0 is the induction variable\n-                for_op.get_body(0).replace_use_in_block_with(init_args[i].handle, for_op.get_body(0).arg(i + 1))\n-\n         # update lscope & local_defs (ForOp defines new values)\n         for i, name in enumerate(names):\n             self.set_value(name, triton.language.core.tensor(for_op.get_result(i), yields[i].type))\n@@ -832,6 +905,13 @@ def kernel_suffix(signature, specialization):\n # ------------------------------------------------------------------------------\n \n \n+def parse_mlir_module(path, context):\n+    module = _triton.ir.parse_mlir_module(path, context)\n+    # module takes ownership of the context\n+    module.context = context\n+    return module\n+\n+\n def build_triton_ir(fn, signature, specialization, constants):\n     # canonicalize signature\n     if isinstance(signature, str):\n@@ -1136,8 +1216,9 @@ def format_of(ty):\n     unsigned attr;\n     CUresult status =\n         cuPointerGetAttribute(&attr, CU_POINTER_ATTRIBUTE_MEMORY_TYPE, ptr_info.dev_ptr);\n-    if (!(attr == CU_MEMORYTYPE_DEVICE || attr == CU_MEMORYTYPE_UNIFIED) ||\n-        !(status == CUDA_SUCCESS)) {{\n+    if (ptr_info.dev_ptr &&\n+        (!(attr == CU_MEMORYTYPE_DEVICE || attr == CU_MEMORYTYPE_UNIFIED) ||\n+         !(status == CUDA_SUCCESS))) {{\n         PyErr_Format(PyExc_ValueError,\n                      \"Pointer argument (at %d) cannot be accessed from Triton (cpu tensor?)\", idx);\n         ptr_info.valid = false;\n@@ -1467,11 +1548,11 @@ def compile(fn, **kwargs):\n     # build compilation stages\n     stages = {\n         \"ast\": (lambda path: fn, None),\n-        \"ttir\": (lambda path: _triton.ir.parse_mlir_module(path, context),\n+        \"ttir\": (lambda path: parse_mlir_module(path, context),\n                  lambda src: ast_to_ttir(src, signature, configs[0], constants)),\n-        \"ttgir\": (lambda path: _triton.ir.parse_mlir_module(path, context),\n+        \"ttgir\": (lambda path: parse_mlir_module(path, context),\n                   lambda src: ttir_to_ttgir(src, num_warps, num_stages, capability)),\n-        \"llir\": (lambda path: Path(path).read_bytes(),\n+        \"llir\": (lambda path: Path(path).read_text(),\n                  lambda src: ttgir_to_llir(src, extern_libs, capability)),\n         \"ptx\": (lambda path: Path(path).read_text(),\n                 lambda src: llir_to_ptx(src, capability)),"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -65,6 +65,7 @@\n     store,\n     sum,\n     swizzle2d,\n+    static_range,\n     tensor,\n     trans,\n     triton,\n@@ -162,6 +163,7 @@\n     \"sin\",\n     \"softmax\",\n     \"sqrt\",\n+    \"static_range\",\n     \"store\",\n     \"sum\",\n     \"swizzle2d\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 49, "deletions": 1, "changes": 50, "file_content_changes": "@@ -31,7 +31,7 @@ def _to_tensor(x, builder):\n         return _to_tensor(x.value, builder)\n     elif isinstance(x, tensor):\n         return x\n-    assert False, f'cannot convert {x} to tensor'\n+    assert False, f\"cannot convert {x} of type {type(x)} to tensor\"\n \n \n class dtype:\n@@ -424,6 +424,18 @@ def __pos__(self):\n     def __invert__(self):\n         return constexpr(~self.value)\n \n+    def __pow__(self, other):\n+        return constexpr(self.value ** other.value)\n+\n+    def __rshift__(self, other):\n+        return constexpr(self.value >> other.value)\n+\n+    def __lshift__(self, other):\n+        return constexpr(self.value << other.value)\n+\n+    def __not__(self):\n+        return constexpr(not self.value)\n+\n     def __call__(self, *args, **kwds):\n         return self.value(*args, **kwds)\n \n@@ -606,6 +618,12 @@ def logical_or(self, other, _builder=None):\n         other = _to_tensor(other, _builder)\n         return semantic.logical_or(self, other, _builder)\n \n+    # note: __not__ isn't actually a magic method in python\n+    # but it's ok because our ASTVisitor handles it\n+    @builtin\n+    def __not__(self, _builder=None):\n+        return semantic.not_(self, _builder)\n+\n     @builtin\n     def __getitem__(self, slices, _builder=None):\n         if isinstance(slices, slice):\n@@ -1289,3 +1307,33 @@ def printf(prefix, *args, _builder=None):\n     for arg in args:\n         new_args.append(_to_tensor(arg, _builder))\n     return semantic.printf(new_prefix, new_args, _builder)\n+\n+# -----------------------\n+# Iterators\n+# -----------------------\n+\n+\n+class static_range:\n+\n+    \"\"\"Iterator that counts upward forever.\"\"\"\n+\n+    def __init__(self, arg1, arg2=None, step=None):\n+        assert isinstance(arg1, constexpr)\n+        if step is None:\n+            self.step = constexpr(1)\n+        else:\n+            assert isinstance(step, constexpr)\n+            self.step = step\n+        if arg2 is None:\n+            self.start = constexpr(0)\n+            self.end = arg1\n+        else:\n+            assert isinstance(arg2, constexpr)\n+            self.start = arg1\n+            self.end = arg2\n+\n+    def __iter__(self):\n+        raise RuntimeError(\"static_range can only be used in @triton.jit'd functions\")\n+\n+    def __next__(self):\n+        raise RuntimeError(\"static_range can only be used in @triton.jit'd functions\")"}, {"filename": "python/triton/language/random.py", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -17,7 +17,7 @@ def philox_impl(c0, c1, c2, c3, k0, k1, n_rounds: tl.constexpr = N_ROUNDS_DEFAUL\n     \"\"\"\n     Run `n_rounds` rounds of Philox for state (c0, c1, c2, c3) and key (k0, k1).\n     \"\"\"\n-    for _ in range(n_rounds):\n+    for _ in tl.static_range(n_rounds):\n         # update random state\n         A = PHILOX_ROUND_A\n         B = PHILOX_ROUND_B\n@@ -37,6 +37,10 @@ def philox(seed, c0, c1, c2, c3, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     seed = seed.to(tl.uint64)\n     seed_hi = ((seed >> 32) & 0xffffffff).to(tl.uint32)\n     seed_lo = (seed & 0xffffffff).to(tl.uint32)\n+    c0 = c0.to(tl.uint32, bitcast=True)\n+    c1 = c1.to(tl.uint32, bitcast=True)\n+    c2 = c2.to(tl.uint32, bitcast=True)\n+    c3 = c3.to(tl.uint32, bitcast=True)\n     return philox_impl(c0, c1, c2, c3, seed_lo, seed_hi, n_rounds)\n \n "}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "file_content_changes": "@@ -312,6 +312,12 @@ def logical_or(input: tl.tensor, other: tl.tensor, builder: ir.builder) -> tl.te\n     return or_(input, other, builder)\n \n \n+def not_(input: tl.tensor, builder: ir.builder):\n+    if not input.type.is_int1():\n+        input = bitcast(input, tl.dtype(\"int1\"), builder)\n+    return invert(input, builder)\n+\n+\n def lshr(input: tl.tensor,\n          other: tl.tensor,\n          builder: ir.builder) -> tl.tensor:\n@@ -808,7 +814,7 @@ def store(ptr: tl.tensor,\n         raise ValueError(\"Pointer argument of store instruction is \" + ptr.type.__repr__())\n     if ptr.type.is_block():\n         val = broadcast_impl_shape(val, ptr.type.get_block_shapes(), builder)\n-    if mask:\n+    if mask and ptr.type.is_block():\n         mask = broadcast_impl_shape(mask, ptr.type.get_block_shapes(), builder)\n     ptr_ty = ptr.type.scalar\n     elt_ty = ptr_ty.element_ty\n@@ -853,7 +859,7 @@ def atom_red_typechecking_impl(ptr: tl.tensor,\n     if element_ty is tl.float16 and op != 'add':\n         raise ValueError(\"atomic_\" + op + \" does not support fp16\")\n     if element_ty in [tl.int1, tl.int8, tl.int16, tl.bfloat16]:\n-        raise ValueError(\"atomic_\" + op + \" does not support \" + element_ty)\n+        raise ValueError(\"atomic_\" + op + \" does not support \" + str(element_ty))\n     if ptr.type.is_block():\n         if mask:\n             mask = broadcast_impl_shape(mask, ptr.type.get_block_shapes(), builder)"}, {"filename": "python/triton/ops/__init__.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -1,6 +1,7 @@\n # from .conv import _conv, conv\n from . import blocksparse\n from .cross_entropy import _cross_entropy, cross_entropy\n+from .flash_attention import attention\n from .matmul import _matmul, matmul\n \n __all__ = [\n@@ -9,4 +10,5 @@\n     \"cross_entropy\",\n     \"_matmul\",\n     \"matmul\",\n+    \"attention\",\n ]"}, {"filename": "python/triton/ops/flash_attention.py", "status": "added", "additions": 267, "deletions": 0, "changes": 267, "file_content_changes": "@@ -0,0 +1,267 @@\n+\"\"\"\n+Fused Attention\n+===============\n+This is a Triton implementation of the Flash Attention algorithm\n+(see: Dao et al., https://arxiv.org/pdf/2205.14135v2.pdf; Rabe and Staats https://arxiv.org/pdf/2112.05682v2.pdf)\n+\"\"\"\n+\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def _fwd_kernel(\n+    Q, K, V, sm_scale,\n+    L, M,\n+    Out,\n+    stride_qz, stride_qh, stride_qm, stride_qk,\n+    stride_kz, stride_kh, stride_kn, stride_kk,\n+    stride_vz, stride_vh, stride_vk, stride_vn,\n+    stride_oz, stride_oh, stride_om, stride_on,\n+    Z, H, N_CTX,\n+    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n+    BLOCK_N: tl.constexpr,\n+):\n+    start_m = tl.program_id(0)\n+    off_hz = tl.program_id(1)\n+    # initialize offsets\n+    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+    offs_n = tl.arange(0, BLOCK_N)\n+    offs_d = tl.arange(0, BLOCK_DMODEL)\n+    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n+    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n+    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n+    # Initialize pointers to Q, K, V\n+    q_ptrs = Q + off_q\n+    k_ptrs = K + off_k\n+    v_ptrs = V + off_v\n+    # initialize pointer to m and l\n+    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n+    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+    # load q: it will stay in SRAM throughout\n+    q = tl.load(q_ptrs)\n+    # loop over k, v and update accumulator\n+    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n+        # -- compute qk ----\n+        k = tl.load(k_ptrs)\n+        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n+        qk += tl.dot(q, k)\n+        qk *= sm_scale\n+        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n+        # compute new m\n+        m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n+        # correct old l\n+        l_prev *= tl.exp(m_prev - m_curr)\n+        # attention weights\n+        p = tl.exp(qk - m_curr[:, None])\n+        l_curr = tl.sum(p, 1) + l_prev\n+        # rescale operands of matmuls\n+        l_rcp = 1. / l_curr\n+        p *= l_rcp\n+        acc *= (l_prev * l_rcp)[:, None]\n+        # update acc\n+        p = p.to(tl.float16)\n+        v = tl.load(v_ptrs)\n+        acc += tl.dot(p, v)\n+        # update m_i and l_i\n+        l_prev = l_curr\n+        m_prev = m_curr\n+        # update pointers\n+        k_ptrs += BLOCK_N * stride_kn\n+        v_ptrs += BLOCK_N * stride_vk\n+    # rematerialize offsets to save registers\n+    start_m = tl.program_id(0)\n+    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+    # write back l and m\n+    l_ptrs = L + off_hz * N_CTX + offs_m\n+    m_ptrs = M + off_hz * N_CTX + offs_m\n+    tl.store(l_ptrs, l_prev)\n+    tl.store(m_ptrs, m_prev)\n+    # initialize pointers to output\n+    offs_n = tl.arange(0, BLOCK_DMODEL)\n+    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n+    out_ptrs = Out + off_o\n+    tl.store(out_ptrs, acc)\n+\n+\n+@triton.jit\n+def _bwd_preprocess(\n+    Out, DO, L,\n+    NewDO, Delta,\n+    BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\n+):\n+    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n+    off_n = tl.arange(0, D_HEAD)\n+    # load\n+    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n+    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n+    denom = tl.load(L + off_m).to(tl.float32)\n+    # compute\n+    do = do / denom[:, None]\n+    delta = tl.sum(o * do, axis=1)\n+    # write-back\n+    tl.store(NewDO + off_m[:, None] * D_HEAD + off_n[None, :], do)\n+    tl.store(Delta + off_m, delta)\n+\n+\n+@triton.jit\n+def _bwd_kernel(\n+    Q, K, V, sm_scale, Out, DO,\n+    DQ, DK, DV,\n+    L, M,\n+    D,\n+    stride_qz, stride_qh, stride_qm, stride_qk,\n+    stride_kz, stride_kh, stride_kn, stride_kk,\n+    stride_vz, stride_vh, stride_vk, stride_vn,\n+    Z, H, N_CTX,\n+    num_block,\n+    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n+    BLOCK_N: tl.constexpr,\n+):\n+    off_hz = tl.program_id(0)\n+    off_z = off_hz // H\n+    off_h = off_hz % H\n+    # offset pointers for batch/head\n+    Q += off_z * stride_qz + off_h * stride_qh\n+    K += off_z * stride_qz + off_h * stride_qh\n+    V += off_z * stride_qz + off_h * stride_qh\n+    DO += off_z * stride_qz + off_h * stride_qh\n+    DQ += off_z * stride_qz + off_h * stride_qh\n+    DK += off_z * stride_qz + off_h * stride_qh\n+    DV += off_z * stride_qz + off_h * stride_qh\n+    for start_n in range(0, num_block):\n+        lo = start_n * BLOCK_M\n+        # initialize row/col offsets\n+        offs_qm = lo + tl.arange(0, BLOCK_M)\n+        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n+        offs_m = tl.arange(0, BLOCK_N)\n+        offs_k = tl.arange(0, BLOCK_DMODEL)\n+        # initialize pointers to value-like data\n+        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n+        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        # pointer to row-wise quantities in value-like data\n+        D_ptrs = D + off_hz * N_CTX\n+        m_ptrs = M + off_hz * N_CTX\n+        # initialize dv amd dk\n+        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+        # k and v stay in SRAM throughout\n+        k = tl.load(k_ptrs)\n+        v = tl.load(v_ptrs)\n+        # loop over rows\n+        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n+            offs_m_curr = start_m + offs_m\n+            # load q, k, v, do on-chip\n+            q = tl.load(q_ptrs)\n+            # recompute p = softmax(qk, dim=-1).T\n+            # NOTE: `do` is pre-divided by `l`; no normalization here\n+            qk = tl.dot(q, tl.trans(k))\n+            qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\"-inf\"))\n+            m = tl.load(m_ptrs + offs_m_curr)\n+            p = tl.exp(qk * sm_scale - m[:, None])\n+            # compute dv\n+            do = tl.load(do_ptrs)\n+            dv += tl.dot(tl.trans(p.to(tl.float16)), do)\n+            # compute dp = dot(v, do)\n+            Di = tl.load(D_ptrs + offs_m_curr)\n+            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n+            dp += tl.dot(do, tl.trans(v))\n+            # compute ds = p * (dp - delta[:, None])\n+            ds = p * dp * sm_scale\n+            # compute dk = dot(ds.T, q)\n+            dk += tl.dot(tl.trans(ds.to(tl.float16)), q)\n+            # compute dq\n+            dq = tl.load(dq_ptrs)\n+            dq += tl.dot(ds.to(tl.float16), k)\n+            tl.store(dq_ptrs, dq)\n+            # increment pointers\n+            dq_ptrs += BLOCK_M * stride_qm\n+            q_ptrs += BLOCK_M * stride_qm\n+            do_ptrs += BLOCK_M * stride_qm\n+        # write-back\n+        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n+        tl.store(dv_ptrs, dv)\n+        tl.store(dk_ptrs, dk)\n+\n+\n+class _attention(torch.autograd.Function):\n+\n+    @staticmethod\n+    def forward(ctx, q, k, v, sm_scale):\n+        # only support for Ampere now\n+        capability = torch.cuda.get_device_capability()\n+        if capability[0] < 8:\n+            raise RuntimeError(\"Flash attention currently only supported for compute capability < 80\")\n+        BLOCK = 128\n+        # shape constraints\n+        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n+        assert Lq == Lk and Lk == Lv\n+        # assert Lk in {16, 32, 64, 128}\n+        assert Lk in {64}  # TODO: fix other cases\n+        o = torch.empty_like(q)\n+        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n+        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        num_warps = 4 if Lk <= 64 else 8\n+\n+        _fwd_kernel[grid](\n+            q, k, v, sm_scale,\n+            L, m,\n+            o,\n+            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n+            q.shape[0], q.shape[1], q.shape[2],\n+            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n+            BLOCK_DMODEL=Lk, num_warps=num_warps,\n+            num_stages=2,\n+        )\n+\n+        ctx.save_for_backward(q, k, v, o, L, m)\n+        ctx.grid = grid\n+        ctx.sm_scale = sm_scale\n+        ctx.BLOCK_DMODEL = Lk\n+        return o\n+\n+    @staticmethod\n+    def backward(ctx, do):\n+        BLOCK = 128\n+        q, k, v, o, l, m = ctx.saved_tensors\n+        do = do.contiguous()\n+        dq = torch.zeros_like(q, dtype=torch.float32)\n+        dk = torch.empty_like(k)\n+        dv = torch.empty_like(v)\n+        do_scaled = torch.empty_like(do)\n+        delta = torch.empty_like(l)\n+        _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n+            o, do, l,\n+            do_scaled, delta,\n+            BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n+        )\n+        _bwd_kernel[(ctx.grid[1],)](\n+            q, k, v, ctx.sm_scale,\n+            o, do_scaled,\n+            dq, dk, dv,\n+            l, m,\n+            delta,\n+            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+            q.shape[0], q.shape[1], q.shape[2],\n+            ctx.grid[0],\n+            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n+            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n+            num_stages=1,\n+        )\n+        return dq, dk, dv, None\n+\n+\n+attention = _attention.apply"}, {"filename": "test/Analysis/test-alignment.mlir", "status": "modified", "additions": 325, "deletions": 37, "changes": 362, "file_content_changes": "@@ -1,51 +1,336 @@\n // RUN: triton-opt %s -test-print-alignment -split-input-file 2>&1 | FileCheck %s\n \n+// CHECK-LABEL: cast\n+func @cast() {\n+  // CHECK: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [1]\n+  %cst = arith.constant 1 : i32\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [1]\n+  %0 = arith.extsi %cst : i32 to i64\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [1]\n+  %cst_tensor = arith.constant dense<1> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [1]\n+  %1 = tt.bitcast %cst_tensor : tensor<128xi32> -> tensor<128xi64>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: add\n+func @add() {\n+  // CHECK: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [1]\n+  %1 = arith.constant dense<1> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n+  %2 = arith.addi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [127]\n+  %3 = arith.constant dense<127> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [128] ; ConstantValue: [128]\n+  %4 = arith.addi %1, %3 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: sub\n+func @sub() {\n+  // CHECK: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [1]\n+  %1 = arith.constant dense<1> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n+  %2 = arith.subi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [129]\n+  %3 = arith.constant dense<129> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [128] ; ConstantValue: [128]\n+  %4 = arith.subi %3, %1 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: mul\n+func @mul() {\n+  // CHECK: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [1]\n+  %1 = arith.constant dense<1> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %2 = arith.muli %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [128] ; ConstantValue: [128]\n+  %3 = arith.constant dense<128> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [128] ; ConstantValue: [128]\n+  %4 = arith.muli %3, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [2] ; Constancy: [128] ; ConstantValue: [2]\n+  %5 = arith.constant dense<2> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [256] ; Constancy: [128] ; ConstantValue: [256]\n+  %6 = arith.muli %4, %5 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: div\n+func @div() {\n+  // CHECK: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [1]\n+  %1 = arith.constant dense<1> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %2 = arith.divsi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %3 = arith.divui %1, %0 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [64] ; Constancy: [128] ; ConstantValue: [64]\n+  %4 = arith.constant dense<64> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16777216] ; Constancy: [64] ; ConstantValue: [None]\n+  %5 = arith.divsi %0, %4 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16777216] ; Constancy: [1] ; ConstantValue: [None]\n+  %6 = arith.divsi %4, %0 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [2] ; Constancy: [128] ; ConstantValue: [66]\n+  %7 = arith.constant dense<66> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [536870912] ; Constancy: [2] ; ConstantValue: [None]\n+  %8 = arith.divui %0, %7 : tensor<128xi32>\n+  return \n+}\n+\n+// -----\n+\n+// CHECK-LABEL: rem\n+func @rem() {\n+  // CHECK: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [1]\n+  %1 = arith.constant dense<1> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [4611686018427387904] ; Constancy: [128] ; ConstantValue: [0]\n+  %2 = arith.remsi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n+  %3 = arith.remui %1, %0 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [64] ; Constancy: [128] ; ConstantValue: [64]\n+  %4 = arith.constant dense<64> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [64] ; Divisibility: [64] ; Constancy: [1] ; ConstantValue: [None]\n+  %5 = arith.remsi %0, %4 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [64] ; Constancy: [1] ; ConstantValue: [None]\n+  %6 = arith.remsi %4, %0 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [2] ; Constancy: [128] ; ConstantValue: [66]\n+  %7 = arith.constant dense<66> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [2] ; Divisibility: [2] ; Constancy: [1] ; ConstantValue: [None]\n+  %8 = arith.remui %0, %7 : tensor<128xi32>\n+  return \n+}\n+\n+// -----\n+\n+// CHECK-LABEL: broadcast\n+func @broadcast() {\n+  // CHECK: Contiguity: [1] ; Divisibility: [64] ; Constancy: [128] ; ConstantValue: [64]\n+  %0 = arith.constant dense<64> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [64, 1] ; Constancy: [128, 1] ; ConstantValue: [64]\n+  %1 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<128xi32>) -> tensor<128x1xi32>\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [64, 1] ; Constancy: [128, 128] ; ConstantValue: [64]\n+  %2 = tt.broadcast %1 : (tensor<128x1xi32>) -> tensor<128x128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: splat\n+func @splat(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n+  // CHECK: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 128] ; ConstantValue: [None]\n+  %0 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: cmp\n+func @cmp() {\n+  // CHECK: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [4611686018427387904] ; Constancy: [128] ; ConstantValue: [0]\n+  %1 = arith.constant dense<0> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [None]\n+  %2 = arith.cmpi eq, %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [None]\n+  %3 = arith.cmpi slt, %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n+  %4 = arith.cmpi sle, %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [None]\n+  %5 = arith.cmpi sge, %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [8] ; Constancy: [128] ; ConstantValue: [8]\n+  %6 = arith.constant dense<8> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [8] ; ConstantValue: [None]\n+  %7 = arith.cmpi sgt, %0, %6 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [0]\n+  %8 = arith.cmpi sgt, %1, %6 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: logic\n+func @logic() {\n+  // CHECK: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [64] ; Constancy: [128] ; ConstantValue: [64]\n+  %1 = arith.constant dense<64> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16777216] ; Constancy: [64] ; ConstantValue: [None]\n+  %2 = arith.divsi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [8] ; Constancy: [128] ; ConstantValue: [8]\n+  %3 = arith.constant dense<8> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [134217728] ; Constancy: [8] ; ConstantValue: [None]\n+  %4 = arith.divsi %0, %3 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n+  %5 = arith.andi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n+  %6 = arith.ori %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n+  %7 = arith.xori %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [8] ; ConstantValue: [None]\n+  %8 = arith.andi %2, %4 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [8] ; ConstantValue: [None]\n+  %9 = arith.ori %2, %4 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [8] ; ConstantValue: [None]\n+  %10 = arith.xori %2, %4 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: select\n+func @select() {\n+  // CHECK: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [4611686018427387904] ; Constancy: [128] ; ConstantValue: [0]\n+  %1 = arith.constant dense<0> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [None]\n+  %2 = arith.cmpi eq, %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [None]\n+  %3 = arith.cmpi slt, %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [4611686018427387904] ; Constancy: [1] ; ConstantValue: [0]\n+  %4 = arith.constant 0 : i1\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [4611686018427387904] ; Constancy: [128] ; ConstantValue: [0]\n+  %7 = tt.splat %4 : (i1) -> tensor<128xi1>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [4611686018427387904] ; Constancy: [128] ; ConstantValue: [0]\n+  %5 = select %4, %3, %7 : tensor<128xi1>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [None]\n+  %8 = \"triton_gpu.select\"(%7, %3, %2) : (tensor<128xi1>, tensor<128xi1>, tensor<128xi1>) -> tensor<128xi1>\n+  return\n+}\n+\n+// -----\n+\n+func @shift() {\n+  // CHECK: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [8] ; Constancy: [128] ; ConstantValue: [8]\n+  %1 = arith.constant dense<8> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [4] ; Constancy: [128] ; ConstantValue: [4]\n+  %2 = arith.constant dense<4> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [274877906944] ; Constancy: [1] ; ConstantValue: [None]\n+  %3 = arith.shli %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [67108864] ; Constancy: [1] ; ConstantValue: [None]\n+  %4 = arith.shrsi %0, %2 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [128] ; ConstantValue: [128]\n+  %5 = arith.shli %1, %2 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+func @max_min() {\n+  // CHECK: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [64] ; Constancy: [1] ; ConstantValue: [None]\n+  %1 = tt.make_range {end = 192 : i32, start = 64 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n+  %2 = arith.maxsi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n+  %3 = arith.minsi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [8] ; Constancy: [128] ; ConstantValue: [8]\n+  %4 = arith.constant dense<8> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [4] ; Constancy: [128] ; ConstantValue: [4]\n+  %5 = arith.constant dense<4> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [8]\n+  %6 = arith.maxsi %4, %5 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: for\n+func @for() {\n+  // CHECK: Contiguity: [1, 1] ; Divisibility: [4611686018427387904, 4611686018427387904] ; Constancy: [128, 32] ; ConstantValue: [0]\n+  %a_init = arith.constant dense<0> : tensor<128x32xi32>\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [128, 32] ; ConstantValue: [1]\n+  %b_init = arith.constant dense<1> : tensor<128x32xi32>\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [4, 4] ; Constancy: [128, 32] ; ConstantValue: [4]\n+  %c_init = arith.constant dense<4> : tensor<128x32xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [1] ; ConstantValue: [128]\n+  %ub = arith.constant 128 : index\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [4611686018427387904] ; Constancy: [1] ; ConstantValue: [0]\n+  %lb = arith.constant 0 : index\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16] ; Constancy: [1] ; ConstantValue: [16]\n+  %step = arith.constant 16 : index\n+  %a, %b, %c = scf.for %iv = %lb to %ub step %step iter_args(%a = %a_init, %b = %b_init, %c = %c_init) -> (tensor<128x32xi32>, tensor<128x32xi32>, tensor<128x32xi32>) {\n+    // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16] ; Constancy: [1] ; ConstantValue: [None]\n+    %t = arith.index_cast %iv : index to i32\n+    // CHECK: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [128, 32] ; ConstantValue: [None]\n+    // CHECK: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [128, 32] ; ConstantValue: [None]\n+    // CHECK: Contiguity: [1, 1] ; Divisibility: [4, 4] ; Constancy: [128, 32] ; ConstantValue: [4]\n+    scf.yield %b, %a, %c : tensor<128x32xi32>, tensor<128x32xi32>, tensor<128x32xi32>\n+  }\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: permute_2d\n func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n-  // CHECK: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [1, 1]\n+  // CHECK: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [128, 128] ; ConstantValue: [1]\n   %cst = arith.constant dense<true> : tensor<128x128xi1>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %cst_0 = arith.constant dense<0.000000e+00> : tensor<128x128xf32>\n-  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [65536] ; Constancy: [1]\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n-  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [65536] ; Constancy: [1]\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n   %1 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n-  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [65536, 1] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [1073741824, 1] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %2 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<128xi32>) -> tensor<128x1xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1]\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1] ; ConstantValue: [None]\n   %3 = tt.splat %arg1 : (i32) -> tensor<128x1xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [1048576, 16] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [17179869184, 16] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %4 = arith.muli %2, %3 : tensor<128x1xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1]\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1] ; ConstantValue: [None]\n   %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %6 = tt.addptr %5, %4 : tensor<128x1x!tt.ptr<f32>>, tensor<128x1xi32>\n-  // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 65536] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 1073741824] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %7 = tt.expand_dims %1 {axis = 0 : i32}: (tensor<128xi32>) -> tensor<1x128xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 128]\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 128] ; ConstantValue: [None]\n   %8 = tt.broadcast %6 : (tensor<128x1x!tt.ptr<f32>>) -> tensor<128x128x!tt.ptr<f32>>\n-  // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 65536] ; Constancy: [128, 1]\n+  // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 1073741824] ; Constancy: [128, 1] ; ConstantValue: [None]\n   %9 = tt.broadcast %7 : (tensor<1x128xi32>) -> tensor<128x128xi32>\n-  // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 16] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 16] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %10 = tt.addptr %8, %9 : tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>\n-  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [65536, 1] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [1073741824, 1] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %11 = tt.expand_dims %0 {axis = 1 : i32}: (tensor<128xi32>) -> tensor<128x1xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1]\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1] ; ConstantValue: [None]\n   %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>>\n-  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %13 = tt.addptr %12, %11 : tensor<128x1x!tt.ptr<f32>>, tensor<128x1xi32>\n-  // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 65536] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 1073741824] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %14 = tt.expand_dims %1 {axis = 0 : i32} : (tensor<128xi32>) -> tensor<1x128xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 128]\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 128] ; ConstantValue: [None]\n   %15 = tt.splat %arg3 : (i32) -> tensor<1x128xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 1048576] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 17179869184] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %16 = arith.muli %14, %15 : tensor<1x128xi32>\n-  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 128]\n+  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 128] ; ConstantValue: [None]\n   %17 = tt.broadcast %13 : (tensor<128x1x!tt.ptr<f32>>) -> tensor<128x128x!tt.ptr<f32>>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 1048576] ; Constancy: [128, 1]\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 17179869184] ; Constancy: [128, 1] ; ConstantValue: [None]\n   %18 = tt.broadcast %16 : (tensor<1x128xi32>) -> tensor<128x128xi32>\n-  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %19 = tt.addptr %17, %18 : tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %20 = tt.load %10, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf32>\n   tt.store %19, %20, %cst : tensor<128x128xf32>\n   return\n@@ -56,28 +341,29 @@ func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {t\n module {\n \n // This is a tiny test for verifying StoreOp-related alignment, It simply store a constant to a buffer.\n+// CHECK-LABEL: store_constant_align\n func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n: i32 {tt.divisibility = 16 : i32}) {\n-  // CHECK: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1]\n+  // CHECK: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n   %pid = tt.get_program_id {axis = 0 : i32} : i32\n-  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [1]\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [1] ; ConstantValue: [128]\n   %c128_i32 = arith.constant 128 : i32\n-  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [1]\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [1] ; ConstantValue: [None]\n   %1 = arith.muli %pid, %c128_i32 : i32\n-  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [65536] ; Constancy: [1]\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n   %2 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n- // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [128]\n+ // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [128] ; ConstantValue: [None]\n   %3 = tt.splat %1 : (i32) -> tensor<128xi32>\n- // CHECK-NEXT: Contiguity: [128] ; Divisibility: [128] ; Constancy: [1]\n+ // CHECK-NEXT: Contiguity: [128] ; Divisibility: [128] ; Constancy: [1] ; ConstantValue: [None]\n   %4 = arith.addi %3, %2 : tensor<128xi32>\n-  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16] ; Constancy: [128]\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16] ; Constancy: [128] ; ConstantValue: [None]\n   %5 = tt.splat %addr : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n-  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [16] ; Constancy: [1]\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [16] ; Constancy: [1] ; ConstantValue: [None]\n   %6 = tt.addptr %5, %4 : tensor<128x!tt.ptr<f32>>, tensor<128xi32>\n-  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16] ; Constancy: [128]\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16] ; Constancy: [128] ; ConstantValue: [None]\n   %9 = tt.splat %n : (i32) -> tensor<128xi32>\n-  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [16]\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [16] ; ConstantValue: [None]\n   %mask = arith.cmpi slt, %4, %9 : tensor<128xi32>\n-  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1]\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n   %cst = arith.constant dense<0.0> : tensor<128xf32>\n   tt.store %5, %cst, %mask : tensor<128xf32>\n   return\n@@ -89,6 +375,7 @@ func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n:\n \n // This IR is dumped from vecadd test.\n // Note, the hint {tt.divisibility = 16 : i32} for %n_elements affects the alignment of mask.\n+// CHECK-LABEL: vecadd_mask_align_16\n func @vecadd_mask_align_16(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32 {tt.divisibility = 16 : i32}) {\n   %c64_i32 = arith.constant 64 : i32\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n@@ -101,13 +388,13 @@ func @vecadd_mask_align_16(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %ar\n   %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n   %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   %9 = tt.splat %n_elements : (i32) -> tensor<64xi32>\n-  // CHECK: Contiguity: [1] ; Divisibility: [64] ; Constancy: [16] ( %{{.*}} = arith.cmpi slt, %{{.*}}, %{{.*}} : tensor<64xi32> )\n+  // CHECK: Contiguity: [1] ; Divisibility: [1] ; Constancy: [16] ; ConstantValue: [None] ( %{{.*}} = arith.cmpi slt, %{{.*}}, %{{.*}} : tensor<64xi32> )\n   %mask = arith.cmpi slt, %4, %9 : tensor<64xi32>\n   %11 = tt.load %6, %mask {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n   %12 = tt.load %8, %mask {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n   %13 = arith.addf %11, %12 : tensor<64xf32>\n   %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n-  // CHECK: Contiguity: [64] ; Divisibility: [16] ; Constancy: [1] ( %{{.*}} = tt.addptr %{{.*}}, %{{.*}} : tensor<64x!tt.ptr<f32>>, tensor<64xi32> )\n+  // CHECK: Contiguity: [64] ; Divisibility: [16] ; Constancy: [1] ; ConstantValue: [None] ( %{{.*}} = tt.addptr %{{.*}}, %{{.*}} : tensor<64x!tt.ptr<f32>>, tensor<64xi32> )\n   %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   tt.store %15, %13, %mask : tensor<64xf32>\n   return\n@@ -117,6 +404,7 @@ func @vecadd_mask_align_16(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %ar\n \n // This IR is dumped from vecadd test.\n // Note, there is no divisibility hint for %n_elements, Triton should assume its divisibility to be 1 by default.\n+// CHECK-LABEL: vecadd_mask_align_1\n func @vecadd_mask_align_1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32) {\n   %c64_i32 = arith.constant 64 : i32\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n@@ -129,7 +417,7 @@ func @vecadd_mask_align_1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg\n   %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n   %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   %9 = tt.splat %n_elements : (i32) -> tensor<64xi32>\n-  // CHECK: Contiguity: [1] ; Divisibility: [64] ; Constancy: [1] ( %{{.*}} = arith.cmpi slt, %{{.*}}, %{{.*}} : tensor<64xi32> )\n+  // CHECK: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None] ( %{{.*}} = arith.cmpi slt, %{{.*}}, %{{.*}} : tensor<64xi32> )\n   %10 = arith.cmpi slt, %4, %9 : tensor<64xi32>\n   %11 = tt.load %6, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n   %12 = tt.load %8, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 617, "deletions": 1, "changes": 618, "file_content_changes": "@@ -2,11 +2,13 @@\n \n #layout0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n #layout1 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n+#layout2 = #triton_gpu.mma<{version = 2, warpsPerCTA = [4, 1]}>\n \n // CHECK: [[target_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n // CHECK: [[row_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n // CHECK: [[col_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [0, 1]}>\n // CHECK: [[col_layout_novec:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n+// CHECK-LABEL: cst\n func @cst() -> tensor<1024xi32, #layout1> {\n   %cst = arith.constant dense<0> : tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %cst : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n@@ -15,6 +17,7 @@ func @cst() -> tensor<1024xi32, #layout1> {\n   return %1: tensor<1024xi32, #layout1>\n }\n \n+// CHECK-LABEL: range\n func @range() -> tensor<1024xi32, #layout1> {\n   %0 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %0 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n@@ -23,6 +26,7 @@ func @range() -> tensor<1024xi32, #layout1> {\n   return %1: tensor<1024xi32, #layout1>\n }\n \n+// CHECK-LABEL: splat\n func @splat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   %0 = tt.splat %arg0 : (i32) -> tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %0 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n@@ -31,6 +35,7 @@ func @splat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   return %1: tensor<1024xi32, #layout1>\n }\n \n+// CHECK-LABEL: remat\n func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   %0 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #layout0>\n   %1 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #layout0>\n@@ -50,6 +55,141 @@ func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   // CHECK: return %6 : tensor<1024xi32, [[target_layout]]>\n }\n \n+// CHECK-LABEL: remat_load_store\n+func @remat_load_store(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %0 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #layout0>\n+  %1 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<64x!tt.ptr<i32>, #layout0>\n+  %2 = tt.addptr %1, %0 : tensor<64x!tt.ptr<i32>, #layout0>, tensor<64xi32, #layout0>\n+  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xi32, #layout0>\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %4 = triton_gpu.convert_layout %3 : (tensor<64xi32, #layout0>) -> tensor<64xi32, #layout1>\n+  %5 = triton_gpu.convert_layout %2 : (tensor<64x!tt.ptr<i32>, #layout0>) -> tensor<64x!tt.ptr<i32>, #layout1>\n+  tt.store %5, %4 : tensor<64xi32, #layout1>\n+  return\n+}\n+\n+// Don't rematerialize vectorized loads\n+// CHECK-LABEL: remat_expensive\n+func @remat_expensive(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %0 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #layout1>\n+  %1 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<64x!tt.ptr<i32>, #layout1>\n+  %2 = tt.addptr %1, %0 : tensor<64x!tt.ptr<i32>, #layout1>, tensor<64xi32, #layout1>\n+  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xi32, #layout1>\n+  // CHECK: triton_gpu.convert_layout\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %4 = triton_gpu.convert_layout %3 : (tensor<64xi32, #layout1>) -> tensor<64xi32, #layout0>\n+  %5 = triton_gpu.convert_layout %2 : (tensor<64x!tt.ptr<i32>, #layout1>) -> tensor<64x!tt.ptr<i32>, #layout0>\n+  tt.store %5, %4 : tensor<64xi32, #layout0>\n+  return\n+}\n+\n+// Don't rematerialize loads when original and target layouts are different\n+// CHECK-LABEL: remat_multi_layout\n+func @remat_multi_layout(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %0 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #layout0>\n+  %1 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<64x!tt.ptr<i32>, #layout0>\n+  %2 = tt.addptr %1, %0 : tensor<64x!tt.ptr<i32>, #layout0>, tensor<64xi32, #layout0>\n+  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xi32, #layout0>\n+  // CHECK: triton_gpu.convert_layout\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %4 = triton_gpu.convert_layout %3 : (tensor<64xi32, #layout0>) -> tensor<64xi32, #layout2>\n+  %5 = triton_gpu.convert_layout %2 : (tensor<64x!tt.ptr<i32>, #layout0>) -> tensor<64x!tt.ptr<i32>, #layout2>\n+  tt.store %5, %4 : tensor<64xi32, #layout2>\n+  return\n+}\n+\n+// Always rematerialize single value loads\n+// CHECK-LABEL: remat_single_value\n+func @remat_single_value(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %0 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<1x!tt.ptr<i32>, #layout1>\n+  %1 = tt.load %0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1xi32, #layout1>\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %2 = triton_gpu.convert_layout %1 : (tensor<1xi32, #layout1>) -> tensor<1xi32, #layout0>\n+  %3 = triton_gpu.convert_layout %0 : (tensor<1x!tt.ptr<i32>, #layout1>) -> tensor<1x!tt.ptr<i32>, #layout0>\n+  tt.store %3, %2 : tensor<1xi32, #layout0>\n+  return\n+}\n+\n+// CHECK-LABEL: if\n+func @if(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout1>\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout1>\n+  %2 = arith.muli %1, %c32_i32 : tensor<1024xi32, #layout1>\n+  %3 = arith.addi %2, %c32_i32 : tensor<1024xi32, #layout1>\n+  %4 = arith.cmpi sgt, %0, %arg0 : i32\n+  %5 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>, #layout0>\n+  scf.if %4 {\n+    %6 = triton_gpu.convert_layout %2 : (tensor<1024xi32, #layout1>) -> tensor<1024xi32, #layout0>\n+    tt.store %5, %6 : tensor<1024xi32, #layout0>\n+  }\n+  return\n+}\n+\n+// CHECK-LABEL: if_convert_else_not\n+func @if_convert_else_not(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout0>\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout0>\n+  %9 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout1>\n+  %2 = arith.muli %1, %c32_i32 : tensor<1024xi32, #layout0>\n+  %3 = arith.addi %2, %c32_i32 : tensor<1024xi32, #layout0>\n+  %4 = arith.cmpi sgt, %0, %arg0 : i32\n+  %5 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>, #layout1>\n+  %8 = scf.if %4 -> tensor<1024xi32, #layout1> {\n+    %6 = triton_gpu.convert_layout %2 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n+    scf.yield %6 : tensor<1024xi32, #layout1>\n+  } else {\n+    scf.yield %9 : tensor<1024xi32, #layout1>\n+  }\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  tt.store %5, %8 : tensor<1024xi32, #layout1>\n+  return\n+}\n+\n+// CHECK-LABEL: if_not_else_convert\n+func @if_not_else_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout0>\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout0>\n+  %9 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout1>\n+  %2 = arith.muli %1, %c32_i32 : tensor<1024xi32, #layout0>\n+  %3 = arith.addi %2, %c32_i32 : tensor<1024xi32, #layout0>\n+  %4 = arith.cmpi sgt, %0, %arg0 : i32\n+  %5 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>, #layout1>\n+  %8 = scf.if %4 -> tensor<1024xi32, #layout1> {\n+    scf.yield %9 : tensor<1024xi32, #layout1>\n+  } else {\n+    %7 = triton_gpu.convert_layout %3 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n+    scf.yield %7 : tensor<1024xi32, #layout1>\n+  }\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  tt.store %5, %8 : tensor<1024xi32, #layout1>\n+  return\n+}\n+\n+// CHECK-LABEL: if_else_both_convert\n+func @if_else_both_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %c32_i32 = arith.constant dense<32> : tensor<1024xi32, #layout0>\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = tt.splat %0 : (i32) -> tensor<1024xi32, #layout0>\n+  %2 = arith.muli %1, %c32_i32 : tensor<1024xi32, #layout0>\n+  %3 = arith.addi %2, %c32_i32 : tensor<1024xi32, #layout0>\n+  %4 = arith.cmpi sgt, %0, %arg0 : i32\n+  %5 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>, #layout1>\n+  %8 = scf.if %4 -> tensor<1024xi32, #layout1> {\n+    %6 = triton_gpu.convert_layout %2 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n+    scf.yield %6 : tensor<1024xi32, #layout1>\n+  } else {\n+    %7 = triton_gpu.convert_layout %3 : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n+    scf.yield %7 : tensor<1024xi32, #layout1>\n+  }\n+  // CHECK: triton_gpu.convert_layout\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  tt.store %5, %8 : tensor<1024xi32, #layout1>\n+  return\n+}\n \n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n@@ -234,4 +374,480 @@ func @select(%arg0: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f6\n     scf.yield %30 : tensor<1x512xf64, #blocked2>\n   }\n   return\n-}\n\\ No newline at end of file\n+}\n+\n+// Make sure the following IR doesn't hang the compiler.\n+// CHECK-LABEL: long_func\n+func public @long_func(%arg0: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg5: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg6: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg7: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg8: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg9: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg10: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg11: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg12: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg13: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg14: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg15: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg16: i32 {tt.divisibility = 16 : i32}) {\n+  %cst = arith.constant dense<1.000000e+00> : tensor<1024xf32, #blocked0>\n+  %cst_0 = arith.constant dense<5.000000e-04> : tensor<1024xf32, #blocked0>\n+  %cst_1 = arith.constant dense<0.999499976> : tensor<1024xf32, #blocked0>\n+  %cst_2 = arith.constant dense<1.000000e+04> : tensor<1024xf32, #blocked0>\n+  %cst_3 = arith.constant dense<5000> : tensor<1024xi32, #blocked0>\n+  %cst_4 = arith.constant dense<150> : tensor<1024xi32, #blocked0>\n+  %cst_5 = arith.constant dense<false> : tensor<1024xi1, #blocked0>\n+  %cst_6 = arith.constant dense<2> : tensor<1024xi32, #blocked0>\n+  %cst_7 = arith.constant dense<4999> : tensor<1024xi32, #blocked0>\n+  %cst_8 = arith.constant dense<2499> : tensor<1024xi32, #blocked0>\n+  %cst_9 = arith.constant dense<2500> : tensor<1024xi32, #blocked0>\n+  %cst_10 = arith.constant dense<0.91629076> : tensor<1024xf32, #blocked0>\n+  %c2499_i32 = arith.constant 2499 : i32\n+  %cst_11 = arith.constant dense<1024> : tensor<1024xi32, #blocked0>\n+  %c1024_i32 = arith.constant 1024 : i32\n+  %cst_12 = arith.constant dense<1> : tensor<1024xi32, #blocked0>\n+  %cst_13 = arith.constant dense<0.000000e+00> : tensor<1024xf32, #blocked0>\n+  %cst_14 = arith.constant dense<0> : tensor<1024xi32, #blocked0>\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = arith.muli %0, %c1024_i32 : i32\n+  %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #blocked0>\n+  %3 = tt.splat %1 : (i32) -> tensor<1024xi32, #blocked0>\n+  %4 = arith.addi %3, %2 : tensor<1024xi32, #blocked0>\n+  %5 = \"triton_gpu.cmpi\"(%4, %cst_11) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %6 = tt.splat %arg5 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %7 = tt.addptr %6, %4 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %8 = triton_gpu.convert_layout %7 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked1>\n+  %9 = triton_gpu.convert_layout %5 : (tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked1>\n+  %10 = tt.load %8, %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked1>\n+  %11 = triton_gpu.convert_layout %10 : (tensor<1024xf32, #blocked1>) -> tensor<1024xf32, #blocked0>\n+  %12 = tt.splat %arg7 : (!tt.ptr<i64>) -> tensor<1024x!tt.ptr<i64>, #blocked0>\n+  %13 = tt.addptr %12, %4 : tensor<1024x!tt.ptr<i64>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %14 = triton_gpu.convert_layout %13 : (tensor<1024x!tt.ptr<i64>, #blocked0>) -> tensor<1024x!tt.ptr<i64>, #blocked2>\n+  %15 = triton_gpu.convert_layout %5 : (tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked2>\n+  %16 = tt.load %14, %15 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xi64, #blocked2>\n+  %17 = triton_gpu.convert_layout %16 : (tensor<1024xi64, #blocked2>) -> tensor<1024xi64, #blocked0>\n+  %18 = tt.splat %arg8 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %19 = tt.addptr %18, %4 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %20 = triton_gpu.convert_layout %19 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked1>\n+  %21 = triton_gpu.convert_layout %5 : (tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked1>\n+  %22 = tt.load %20, %21 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked1>\n+  %23 = triton_gpu.convert_layout %22 : (tensor<1024xf32, #blocked1>) -> tensor<1024xf32, #blocked0>\n+  %24 = arith.subf %cst_13, %11 : tensor<1024xf32, #blocked0>\n+  %25 = math.exp %24 : tensor<1024xf32, #blocked0>\n+  %26 = arith.sitofp %cst_12 : tensor<1024xi32, #blocked0> to tensor<1024xf32, #blocked0>\n+  %27 = arith.addf %25, %26 : tensor<1024xf32, #blocked0>\n+  %28 = arith.divf %26, %27 : tensor<1024xf32, #blocked0>\n+  %29 = tt.addptr %arg6, %c2499_i32 : !tt.ptr<f32>, i32\n+  %30 = tt.load %29 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : f32\n+  %31 = arith.subf %11, %cst_10 : tensor<1024xf32, #blocked0>\n+  %32 = arith.subf %cst_13, %31 : tensor<1024xf32, #blocked0>\n+  %33 = math.exp %32 : tensor<1024xf32, #blocked0>\n+  %34 = arith.addf %33, %26 : tensor<1024xf32, #blocked0>\n+  %35 = arith.divf %26, %34 : tensor<1024xf32, #blocked0>\n+  %36 = tt.splat %30 : (f32) -> tensor<1024xf32, #blocked0>\n+  %37 = \"triton_gpu.cmpf\"(%36, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %38 = \"triton_gpu.select\"(%37, %cst_14, %cst_9) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %39 = \"triton_gpu.select\"(%37, %cst_8, %cst_7) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %40 = arith.subi %39, %38 : tensor<1024xi32, #blocked0>\n+  %41 = \"triton_gpu.cmpi\"(%40, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %42 = \"triton_gpu.cmpi\"(%41, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %43 = arith.remsi %40, %cst_6 : tensor<1024xi32, #blocked0>\n+  %44 = \"triton_gpu.cmpi\"(%43, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %45 = arith.divsi %40, %cst_6 : tensor<1024xi32, #blocked0>\n+  %46 = arith.subi %45, %cst_12 : tensor<1024xi32, #blocked0>\n+  %47 = \"triton_gpu.select\"(%44, %46, %45) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %48 = \"triton_gpu.select\"(%42, %47, %45) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %49 = arith.addi %38, %48 : tensor<1024xi32, #blocked0>\n+  %50 = \"triton_gpu.cmpi\"(%38, %39) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %51 = \"triton_gpu.select\"(%50, %49, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %52 = tt.splat %arg6 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %53 = tt.addptr %52, %51 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %54 = triton_gpu.convert_layout %53 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %55 = tt.load %54 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %56 = \"triton_gpu.cmpf\"(%55, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %57 = \"triton_gpu.cmpi\"(%56, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %58 = arith.andi %57, %50 : tensor<1024xi1, #blocked0>\n+  %59 = arith.addi %51, %cst_12 : tensor<1024xi32, #blocked0>\n+  %60 = \"triton_gpu.select\"(%58, %59, %38) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %61 = arith.andi %56, %50 : tensor<1024xi1, #blocked0>\n+  %62 = \"triton_gpu.select\"(%61, %51, %39) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %63 = \"triton_gpu.cmpi\"(%60, %62) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %64 = arith.subi %62, %60 : tensor<1024xi32, #blocked0>\n+  %65 = \"triton_gpu.cmpi\"(%64, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %66 = \"triton_gpu.cmpi\"(%65, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %67 = arith.remsi %64, %cst_6 : tensor<1024xi32, #blocked0>\n+  %68 = \"triton_gpu.cmpi\"(%67, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %69 = arith.divsi %64, %cst_6 : tensor<1024xi32, #blocked0>\n+  %70 = arith.subi %69, %cst_12 : tensor<1024xi32, #blocked0>\n+  %71 = \"triton_gpu.select\"(%68, %70, %69) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %72 = \"triton_gpu.select\"(%66, %71, %69) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %73 = arith.addi %60, %72 : tensor<1024xi32, #blocked0>\n+  %74 = \"triton_gpu.select\"(%63, %73, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %75 = tt.addptr %52, %74 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %76 = triton_gpu.convert_layout %75 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %77 = tt.load %76 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %78 = \"triton_gpu.cmpf\"(%77, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %79 = \"triton_gpu.cmpi\"(%78, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %80 = arith.andi %79, %63 : tensor<1024xi1, #blocked0>\n+  %81 = arith.addi %74, %cst_12 : tensor<1024xi32, #blocked0>\n+  %82 = \"triton_gpu.select\"(%80, %81, %60) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %83 = arith.andi %78, %63 : tensor<1024xi1, #blocked0>\n+  %84 = \"triton_gpu.select\"(%83, %74, %62) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %85 = \"triton_gpu.cmpi\"(%82, %84) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %86 = arith.subi %84, %82 : tensor<1024xi32, #blocked0>\n+  %87 = \"triton_gpu.cmpi\"(%86, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %88 = \"triton_gpu.cmpi\"(%87, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %89 = arith.remsi %86, %cst_6 : tensor<1024xi32, #blocked0>\n+  %90 = \"triton_gpu.cmpi\"(%89, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %91 = arith.divsi %86, %cst_6 : tensor<1024xi32, #blocked0>\n+  %92 = arith.subi %91, %cst_12 : tensor<1024xi32, #blocked0>\n+  %93 = \"triton_gpu.select\"(%90, %92, %91) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %94 = \"triton_gpu.select\"(%88, %93, %91) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %95 = arith.addi %82, %94 : tensor<1024xi32, #blocked0>\n+  %96 = \"triton_gpu.select\"(%85, %95, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %97 = tt.addptr %52, %96 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %98 = triton_gpu.convert_layout %97 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %99 = tt.load %98 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %100 = \"triton_gpu.cmpf\"(%99, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %101 = \"triton_gpu.cmpi\"(%100, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %102 = arith.andi %101, %85 : tensor<1024xi1, #blocked0>\n+  %103 = arith.addi %96, %cst_12 : tensor<1024xi32, #blocked0>\n+  %104 = \"triton_gpu.select\"(%102, %103, %82) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %105 = arith.andi %100, %85 : tensor<1024xi1, #blocked0>\n+  %106 = \"triton_gpu.select\"(%105, %96, %84) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %107 = \"triton_gpu.cmpi\"(%104, %106) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %108 = arith.subi %106, %104 : tensor<1024xi32, #blocked0>\n+  %109 = \"triton_gpu.cmpi\"(%108, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %110 = \"triton_gpu.cmpi\"(%109, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %111 = arith.remsi %108, %cst_6 : tensor<1024xi32, #blocked0>\n+  %112 = \"triton_gpu.cmpi\"(%111, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %113 = arith.divsi %108, %cst_6 : tensor<1024xi32, #blocked0>\n+  %114 = arith.subi %113, %cst_12 : tensor<1024xi32, #blocked0>\n+  %115 = \"triton_gpu.select\"(%112, %114, %113) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %116 = \"triton_gpu.select\"(%110, %115, %113) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %117 = arith.addi %104, %116 : tensor<1024xi32, #blocked0>\n+  %118 = \"triton_gpu.select\"(%107, %117, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %119 = tt.addptr %52, %118 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %120 = triton_gpu.convert_layout %119 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %121 = tt.load %120 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %122 = \"triton_gpu.cmpf\"(%121, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %123 = \"triton_gpu.cmpi\"(%122, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %124 = arith.andi %123, %107 : tensor<1024xi1, #blocked0>\n+  %125 = arith.addi %118, %cst_12 : tensor<1024xi32, #blocked0>\n+  %126 = \"triton_gpu.select\"(%124, %125, %104) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %127 = arith.andi %122, %107 : tensor<1024xi1, #blocked0>\n+  %128 = \"triton_gpu.select\"(%127, %118, %106) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %129 = \"triton_gpu.cmpi\"(%126, %128) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %130 = arith.subi %128, %126 : tensor<1024xi32, #blocked0>\n+  %131 = \"triton_gpu.cmpi\"(%130, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %132 = \"triton_gpu.cmpi\"(%131, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %133 = arith.remsi %130, %cst_6 : tensor<1024xi32, #blocked0>\n+  %134 = \"triton_gpu.cmpi\"(%133, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %135 = arith.divsi %130, %cst_6 : tensor<1024xi32, #blocked0>\n+  %136 = arith.subi %135, %cst_12 : tensor<1024xi32, #blocked0>\n+  %137 = \"triton_gpu.select\"(%134, %136, %135) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %138 = \"triton_gpu.select\"(%132, %137, %135) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %139 = arith.addi %126, %138 : tensor<1024xi32, #blocked0>\n+  %140 = \"triton_gpu.select\"(%129, %139, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %141 = tt.addptr %52, %140 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %142 = triton_gpu.convert_layout %141 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %143 = tt.load %142 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %144 = \"triton_gpu.cmpf\"(%143, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %145 = \"triton_gpu.cmpi\"(%144, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %146 = arith.andi %145, %129 : tensor<1024xi1, #blocked0>\n+  %147 = arith.addi %140, %cst_12 : tensor<1024xi32, #blocked0>\n+  %148 = \"triton_gpu.select\"(%146, %147, %126) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %149 = arith.andi %144, %129 : tensor<1024xi1, #blocked0>\n+  %150 = \"triton_gpu.select\"(%149, %140, %128) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %151 = \"triton_gpu.cmpi\"(%148, %150) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %152 = arith.subi %150, %148 : tensor<1024xi32, #blocked0>\n+  %153 = \"triton_gpu.cmpi\"(%152, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %154 = \"triton_gpu.cmpi\"(%153, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %155 = arith.remsi %152, %cst_6 : tensor<1024xi32, #blocked0>\n+  %156 = \"triton_gpu.cmpi\"(%155, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %157 = arith.divsi %152, %cst_6 : tensor<1024xi32, #blocked0>\n+  %158 = arith.subi %157, %cst_12 : tensor<1024xi32, #blocked0>\n+  %159 = \"triton_gpu.select\"(%156, %158, %157) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %160 = \"triton_gpu.select\"(%154, %159, %157) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %161 = arith.addi %148, %160 : tensor<1024xi32, #blocked0>\n+  %162 = \"triton_gpu.select\"(%151, %161, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %163 = tt.addptr %52, %162 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %164 = triton_gpu.convert_layout %163 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %165 = tt.load %164 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %166 = \"triton_gpu.cmpf\"(%165, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %167 = \"triton_gpu.cmpi\"(%166, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %168 = arith.andi %167, %151 : tensor<1024xi1, #blocked0>\n+  %169 = arith.addi %162, %cst_12 : tensor<1024xi32, #blocked0>\n+  %170 = \"triton_gpu.select\"(%168, %169, %148) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %171 = arith.andi %166, %151 : tensor<1024xi1, #blocked0>\n+  %172 = \"triton_gpu.select\"(%171, %162, %150) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %173 = \"triton_gpu.cmpi\"(%170, %172) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %174 = arith.subi %172, %170 : tensor<1024xi32, #blocked0>\n+  %175 = \"triton_gpu.cmpi\"(%174, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %176 = \"triton_gpu.cmpi\"(%175, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %177 = arith.remsi %174, %cst_6 : tensor<1024xi32, #blocked0>\n+  %178 = \"triton_gpu.cmpi\"(%177, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %179 = arith.divsi %174, %cst_6 : tensor<1024xi32, #blocked0>\n+  %180 = arith.subi %179, %cst_12 : tensor<1024xi32, #blocked0>\n+  %181 = \"triton_gpu.select\"(%178, %180, %179) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %182 = \"triton_gpu.select\"(%176, %181, %179) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %183 = arith.addi %170, %182 : tensor<1024xi32, #blocked0>\n+  %184 = \"triton_gpu.select\"(%173, %183, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %185 = tt.addptr %52, %184 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %186 = triton_gpu.convert_layout %185 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %187 = tt.load %186 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %188 = \"triton_gpu.cmpf\"(%187, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %189 = \"triton_gpu.cmpi\"(%188, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %190 = arith.andi %189, %173 : tensor<1024xi1, #blocked0>\n+  %191 = arith.addi %184, %cst_12 : tensor<1024xi32, #blocked0>\n+  %192 = \"triton_gpu.select\"(%190, %191, %170) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %193 = arith.andi %188, %173 : tensor<1024xi1, #blocked0>\n+  %194 = \"triton_gpu.select\"(%193, %184, %172) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %195 = \"triton_gpu.cmpi\"(%192, %194) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %196 = arith.subi %194, %192 : tensor<1024xi32, #blocked0>\n+  %197 = \"triton_gpu.cmpi\"(%196, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %198 = \"triton_gpu.cmpi\"(%197, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %199 = arith.remsi %196, %cst_6 : tensor<1024xi32, #blocked0>\n+  %200 = \"triton_gpu.cmpi\"(%199, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %201 = arith.divsi %196, %cst_6 : tensor<1024xi32, #blocked0>\n+  %202 = arith.subi %201, %cst_12 : tensor<1024xi32, #blocked0>\n+  %203 = \"triton_gpu.select\"(%200, %202, %201) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %204 = \"triton_gpu.select\"(%198, %203, %201) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %205 = arith.addi %192, %204 : tensor<1024xi32, #blocked0>\n+  %206 = \"triton_gpu.select\"(%195, %205, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %207 = tt.addptr %52, %206 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %208 = triton_gpu.convert_layout %207 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %209 = tt.load %208 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %210 = \"triton_gpu.cmpf\"(%209, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %211 = \"triton_gpu.cmpi\"(%210, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %212 = arith.andi %211, %195 : tensor<1024xi1, #blocked0>\n+  %213 = arith.addi %206, %cst_12 : tensor<1024xi32, #blocked0>\n+  %214 = \"triton_gpu.select\"(%212, %213, %192) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %215 = arith.andi %210, %195 : tensor<1024xi1, #blocked0>\n+  %216 = \"triton_gpu.select\"(%215, %206, %194) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %217 = \"triton_gpu.cmpi\"(%214, %216) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %218 = arith.subi %216, %214 : tensor<1024xi32, #blocked0>\n+  %219 = \"triton_gpu.cmpi\"(%218, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %220 = \"triton_gpu.cmpi\"(%219, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %221 = arith.remsi %218, %cst_6 : tensor<1024xi32, #blocked0>\n+  %222 = \"triton_gpu.cmpi\"(%221, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %223 = arith.divsi %218, %cst_6 : tensor<1024xi32, #blocked0>\n+  %224 = arith.subi %223, %cst_12 : tensor<1024xi32, #blocked0>\n+  %225 = \"triton_gpu.select\"(%222, %224, %223) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %226 = \"triton_gpu.select\"(%220, %225, %223) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %227 = arith.addi %214, %226 : tensor<1024xi32, #blocked0>\n+  %228 = \"triton_gpu.select\"(%217, %227, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %229 = tt.addptr %52, %228 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %230 = triton_gpu.convert_layout %229 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %231 = tt.load %230 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %232 = \"triton_gpu.cmpf\"(%231, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %233 = \"triton_gpu.cmpi\"(%232, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %234 = arith.andi %233, %217 : tensor<1024xi1, #blocked0>\n+  %235 = arith.addi %228, %cst_12 : tensor<1024xi32, #blocked0>\n+  %236 = \"triton_gpu.select\"(%234, %235, %214) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %237 = arith.andi %232, %217 : tensor<1024xi1, #blocked0>\n+  %238 = \"triton_gpu.select\"(%237, %228, %216) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %239 = \"triton_gpu.cmpi\"(%236, %238) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %240 = arith.subi %238, %236 : tensor<1024xi32, #blocked0>\n+  %241 = \"triton_gpu.cmpi\"(%240, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %242 = \"triton_gpu.cmpi\"(%241, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %243 = arith.remsi %240, %cst_6 : tensor<1024xi32, #blocked0>\n+  %244 = \"triton_gpu.cmpi\"(%243, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %245 = arith.divsi %240, %cst_6 : tensor<1024xi32, #blocked0>\n+  %246 = arith.subi %245, %cst_12 : tensor<1024xi32, #blocked0>\n+  %247 = \"triton_gpu.select\"(%244, %246, %245) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %248 = \"triton_gpu.select\"(%242, %247, %245) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %249 = arith.addi %236, %248 : tensor<1024xi32, #blocked0>\n+  %250 = \"triton_gpu.select\"(%239, %249, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %251 = tt.addptr %52, %250 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %252 = triton_gpu.convert_layout %251 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %253 = tt.load %252 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %254 = \"triton_gpu.cmpf\"(%253, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %255 = \"triton_gpu.cmpi\"(%254, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %256 = arith.andi %255, %239 : tensor<1024xi1, #blocked0>\n+  %257 = arith.addi %250, %cst_12 : tensor<1024xi32, #blocked0>\n+  %258 = \"triton_gpu.select\"(%256, %257, %236) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %259 = arith.andi %254, %239 : tensor<1024xi1, #blocked0>\n+  %260 = \"triton_gpu.select\"(%259, %250, %238) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %261 = \"triton_gpu.cmpi\"(%258, %260) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %262 = arith.subi %260, %258 : tensor<1024xi32, #blocked0>\n+  %263 = \"triton_gpu.cmpi\"(%262, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %264 = \"triton_gpu.cmpi\"(%263, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %265 = arith.remsi %262, %cst_6 : tensor<1024xi32, #blocked0>\n+  %266 = \"triton_gpu.cmpi\"(%265, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %267 = arith.divsi %262, %cst_6 : tensor<1024xi32, #blocked0>\n+  %268 = arith.subi %267, %cst_12 : tensor<1024xi32, #blocked0>\n+  %269 = \"triton_gpu.select\"(%266, %268, %267) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %270 = \"triton_gpu.select\"(%264, %269, %267) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %271 = arith.addi %258, %270 : tensor<1024xi32, #blocked0>\n+  %272 = \"triton_gpu.select\"(%261, %271, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %273 = tt.addptr %52, %272 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %274 = triton_gpu.convert_layout %273 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %275 = tt.load %274 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %276 = \"triton_gpu.cmpf\"(%275, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %277 = \"triton_gpu.cmpi\"(%276, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %278 = arith.andi %277, %261 : tensor<1024xi1, #blocked0>\n+  %279 = arith.addi %272, %cst_12 : tensor<1024xi32, #blocked0>\n+  %280 = \"triton_gpu.select\"(%278, %279, %258) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %281 = arith.andi %276, %261 : tensor<1024xi1, #blocked0>\n+  %282 = \"triton_gpu.select\"(%281, %272, %260) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %283 = \"triton_gpu.cmpi\"(%280, %282) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %284 = arith.subi %282, %280 : tensor<1024xi32, #blocked0>\n+  %285 = \"triton_gpu.cmpi\"(%284, %cst_14) {predicate = 2 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %286 = \"triton_gpu.cmpi\"(%285, %cst_5) {predicate = 1 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %287 = arith.remsi %284, %cst_6 : tensor<1024xi32, #blocked0>\n+  %288 = \"triton_gpu.cmpi\"(%287, %cst_14) {predicate = 1 : i64} : (tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %289 = arith.divsi %284, %cst_6 : tensor<1024xi32, #blocked0>\n+  %290 = arith.subi %289, %cst_12 : tensor<1024xi32, #blocked0>\n+  %291 = \"triton_gpu.select\"(%288, %290, %289) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %292 = \"triton_gpu.select\"(%286, %291, %289) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %293 = arith.addi %280, %292 : tensor<1024xi32, #blocked0>\n+  %294 = \"triton_gpu.select\"(%283, %293, %cst_14) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %295 = tt.addptr %52, %294 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %296 = triton_gpu.convert_layout %295 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %297 = tt.load %296 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #blocked0>\n+  %298 = \"triton_gpu.cmpf\"(%297, %35) {predicate = 3 : i64} : (tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %299 = \"triton_gpu.cmpi\"(%298, %cst_5) {predicate = 0 : i64} : (tensor<1024xi1, #blocked0>, tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %300 = arith.andi %299, %283 : tensor<1024xi1, #blocked0>\n+  %301 = arith.addi %294, %cst_12 : tensor<1024xi32, #blocked0>\n+  %302 = \"triton_gpu.select\"(%300, %301, %280) : (tensor<1024xi1, #blocked0>, tensor<1024xi32, #blocked0>, tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked0>\n+  %303 = arith.extsi %cst_12 : tensor<1024xi32, #blocked0> to tensor<1024xi64, #blocked0>\n+  %304 = \"triton_gpu.cmpi\"(%17, %303) {predicate = 0 : i64} : (tensor<1024xi64, #blocked0>, tensor<1024xi64, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %305 = arith.fptosi %23 : tensor<1024xf32, #blocked0> to tensor<1024xi64, #blocked0>\n+  %306 = arith.extsi %cst_14 : tensor<1024xi32, #blocked0> to tensor<1024xi64, #blocked0>\n+  %307 = \"triton_gpu.cmpi\"(%306, %305) {predicate = 4 : i64} : (tensor<1024xi64, #blocked0>, tensor<1024xi64, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %308 = arith.extsi %cst_4 : tensor<1024xi32, #blocked0> to tensor<1024xi64, #blocked0>\n+  %309 = \"triton_gpu.cmpi\"(%305, %308) {predicate = 4 : i64} : (tensor<1024xi64, #blocked0>, tensor<1024xi64, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %310 = \"triton_gpu.select\"(%309, %306, %305) : (tensor<1024xi1, #blocked0>, tensor<1024xi64, #blocked0>, tensor<1024xi64, #blocked0>) -> tensor<1024xi64, #blocked0>\n+  %311 = \"triton_gpu.select\"(%307, %306, %310) : (tensor<1024xi1, #blocked0>, tensor<1024xi64, #blocked0>, tensor<1024xi64, #blocked0>) -> tensor<1024xi64, #blocked0>\n+  %312 = \"triton_gpu.select\"(%304, %311, %306) : (tensor<1024xi1, #blocked0>, tensor<1024xi64, #blocked0>, tensor<1024xi64, #blocked0>) -> tensor<1024xi64, #blocked0>\n+  %313 = arith.extsi %cst_3 : tensor<1024xi32, #blocked0> to tensor<1024xi64, #blocked0>\n+  %314 = arith.muli %312, %313 : tensor<1024xi64, #blocked0>\n+  %315 = arith.extsi %302 : tensor<1024xi32, #blocked0> to tensor<1024xi64, #blocked0>\n+  %316 = arith.addi %315, %314 : tensor<1024xi64, #blocked0>\n+  %317 = arith.trunci %316 : tensor<1024xi64, #blocked0> to tensor<1024xi32, #blocked0>\n+  %318 = arith.extsi %317 : tensor<1024xi32, #blocked0> to tensor<1024xi64, #blocked0>\n+  %319 = tt.splat %arg9 : (!tt.ptr<f64>) -> tensor<1024x!tt.ptr<f64>, #blocked0>\n+  %320 = tt.addptr %319, %318 : tensor<1024x!tt.ptr<f64>, #blocked0>, tensor<1024xi64, #blocked0>\n+  %321 = triton_gpu.convert_layout %320 : (tensor<1024x!tt.ptr<f64>, #blocked0>) -> tensor<1024x!tt.ptr<f64>, #blocked0>\n+  %322 = tt.load %321 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf64, #blocked0>\n+  %323 = arith.extf %cst_2 : tensor<1024xf32, #blocked0> to tensor<1024xf64, #blocked0>\n+  %324 = \"triton_gpu.cmpf\"(%322, %323) {predicate = 2 : i64} : (tensor<1024xf64, #blocked0>, tensor<1024xf64, #blocked0>) -> tensor<1024xi1, #blocked0>\n+  %325 = tt.splat %arg10 : (!tt.ptr<f64>) -> tensor<1024x!tt.ptr<f64>, #blocked0>\n+  %326 = tt.addptr %325, %318 : tensor<1024x!tt.ptr<f64>, #blocked0>, tensor<1024xi64, #blocked0>\n+  %327 = triton_gpu.convert_layout %326 : (tensor<1024x!tt.ptr<f64>, #blocked0>) -> tensor<1024x!tt.ptr<f64>, #blocked0>\n+  %328 = tt.load %327 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf64, #blocked0>\n+  %329 = arith.divf %328, %322 : tensor<1024xf64, #blocked0>\n+  %330 = arith.truncf %329 : tensor<1024xf64, #blocked0> to tensor<1024xf32, #blocked0>\n+  %331 = arith.mulf %330, %cst_1 : tensor<1024xf32, #blocked0>\n+  %332 = arith.mulf %35, %cst_0 : tensor<1024xf32, #blocked0>\n+  %333 = arith.addf %331, %332 : tensor<1024xf32, #blocked0>\n+  %334 = \"triton_gpu.select\"(%324, %333, %35) : (tensor<1024xi1, #blocked0>, tensor<1024xf32, #blocked0>, tensor<1024xf32, #blocked0>) -> tensor<1024xf32, #blocked0>\n+  %335 = tt.addptr %319, %317 : tensor<1024x!tt.ptr<f64>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %336 = triton_gpu.convert_layout %335 : (tensor<1024x!tt.ptr<f64>, #blocked0>) -> tensor<1024x!tt.ptr<f64>, #blocked0>\n+  %337 = tt.load %336 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf64, #blocked0>\n+  %338 = arith.extf %cst : tensor<1024xf32, #blocked0> to tensor<1024xf64, #blocked0>\n+  %339 = arith.mulf %337, %338 : tensor<1024xf64, #blocked0>\n+  %340 = tt.addptr %325, %317 : tensor<1024x!tt.ptr<f64>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %341 = triton_gpu.convert_layout %340 : (tensor<1024x!tt.ptr<f64>, #blocked0>) -> tensor<1024x!tt.ptr<f64>, #blocked0>\n+  %342 = tt.load %341 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf64, #blocked0>\n+  %343 = arith.mulf %342, %338 : tensor<1024xf64, #blocked0>\n+  %344 = tt.splat %arg11 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %345 = tt.addptr %344, %4 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %346 = triton_gpu.convert_layout %345 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked1>\n+  %347 = triton_gpu.convert_layout %28 : (tensor<1024xf32, #blocked0>) -> tensor<1024xf32, #blocked1>\n+  %348 = triton_gpu.convert_layout %5 : (tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked1>\n+  tt.store %346, %347, %348 : tensor<1024xf32, #blocked1>\n+  %349 = tt.splat %arg12 : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>, #blocked0>\n+  %350 = tt.addptr %349, %4 : tensor<1024x!tt.ptr<i32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %351 = triton_gpu.convert_layout %350 : (tensor<1024x!tt.ptr<i32>, #blocked0>) -> tensor<1024x!tt.ptr<i32>, #blocked1>\n+  %352 = triton_gpu.convert_layout %317 : (tensor<1024xi32, #blocked0>) -> tensor<1024xi32, #blocked1>\n+  %353 = triton_gpu.convert_layout %5 : (tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked1>\n+  tt.store %351, %352, %353 : tensor<1024xi32, #blocked1>\n+  %354 = tt.splat %arg13 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>, #blocked0>\n+  %355 = tt.addptr %354, %4 : tensor<1024x!tt.ptr<f32>, #blocked0>, tensor<1024xi32, #blocked0>\n+  %356 = triton_gpu.convert_layout %355 : (tensor<1024x!tt.ptr<f32>, #blocked0>) -> tensor<1024x!tt.ptr<f32>, #blocked1>\n+  %357 = triton_gpu.convert_layout %334 : (tensor<1024xf32, #blocked0>) -> tensor<1024xf32, #blocked1>\n+  %358 = triton_gpu.convert_layout %5 : (tensor<1024xi1, #blocked0>) -> tensor<1024xi1, #blocked1>\n+  tt.store %356, %357, %358 : tensor<1024xf32, #blocked1>\n+  %359 = tt.splat %arg14 : (!tt.ptr<f64>) -> tensor<1024x!tt.ptr<f64>, #blocked0>\n+  %360 = tt.addptr %359, %318 : tensor<1024x!tt.ptr<f64>, #blocked0>, tensor<1024xi64, #blocked0>\n+  %361 = triton_gpu.convert_layout %360 : (tensor<1024x!tt.ptr<f64>, #blocked0>) -> tensor<1024x!tt.ptr<f64>, #blocked0>\n+  %362 = triton_gpu.convert_layout %339 : (tensor<1024xf64, #blocked0>) -> tensor<1024xf64, #blocked0>\n+  tt.store %361, %362 : tensor<1024xf64, #blocked0>\n+  %363 = tt.splat %arg15 : (!tt.ptr<f64>) -> tensor<1024x!tt.ptr<f64>, #blocked0>\n+  %364 = tt.addptr %363, %318 : tensor<1024x!tt.ptr<f64>, #blocked0>, tensor<1024xi64, #blocked0>\n+  %365 = triton_gpu.convert_layout %364 : (tensor<1024x!tt.ptr<f64>, #blocked0>) -> tensor<1024x!tt.ptr<f64>, #blocked0>\n+  %366 = triton_gpu.convert_layout %343 : (tensor<1024xf64, #blocked0>) -> tensor<1024xf64, #blocked0>\n+  tt.store %365, %366 : tensor<1024xf64, #blocked0>\n+  return\n+}\n+\n+// A mnist model from torch inductor.\n+// Check if topological sort is working correct and there's no unnecessary convert\n+// CHECK-LABEL: mnist\n+func public @mnist(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: i32) {\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %cst = arith.constant dense<10> : tensor<16x1xi32, #blocked2>\n+  %cst_0 = arith.constant dense<10> : tensor<1x16xi32, #blocked3>\n+  %c16_i32 = arith.constant 16 : i32\n+  %cst_1 = arith.constant dense<64> : tensor<16x1xi32, #blocked2>\n+  %cst_2 = arith.constant dense<0xFF800000> : tensor<16x16xf32, #blocked2>\n+  %cst_3 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #blocked2>\n+  %cst_4 = arith.constant dense<0> : tensor<16x16xi32, #blocked2>\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = arith.muli %0, %c16_i32 : i32\n+  %2 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked0>\n+  %3 = triton_gpu.convert_layout %2 : (tensor<16xi32, #blocked0>) -> tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+  %4 = tt.expand_dims %3 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<16x1xi32, #blocked1>\n+  %5 = triton_gpu.convert_layout %4 : (tensor<16x1xi32, #blocked1>) -> tensor<16x1xi32, #blocked2>\n+  %6 = tt.splat %1 : (i32) -> tensor<16x1xi32, #blocked2>\n+  %7 = arith.addi %6, %5 : tensor<16x1xi32, #blocked2>\n+  %8 = \"triton_gpu.cmpi\"(%7, %cst_1) {predicate = 2 : i64} : (tensor<16x1xi32, #blocked2>, tensor<16x1xi32, #blocked2>) -> tensor<16x1xi1, #blocked2>\n+  %9 = triton_gpu.convert_layout %2 : (tensor<16xi32, #blocked0>) -> tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>\n+  %10 = tt.expand_dims %9 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>) -> tensor<1x16xi32, #blocked3>\n+  %11 = \"triton_gpu.cmpi\"(%10, %cst_0) {predicate = 2 : i64} : (tensor<1x16xi32, #blocked3>, tensor<1x16xi32, #blocked3>) -> tensor<1x16xi1, #blocked3>\n+  %12 = arith.muli %7, %cst : tensor<16x1xi32, #blocked2>\n+  %13 = tt.broadcast %10 : (tensor<1x16xi32, #blocked3>) -> tensor<16x16xi32, #blocked3>\n+  %14 = triton_gpu.convert_layout %13 : (tensor<16x16xi32, #blocked3>) -> tensor<16x16xi32, #blocked2>\n+  %15 = tt.broadcast %12 : (tensor<16x1xi32, #blocked2>) -> tensor<16x16xi32, #blocked2>\n+  %16 = arith.addi %14, %15 : tensor<16x16xi32, #blocked2>\n+  %17 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<16x16x!tt.ptr<f32>, #blocked2>\n+  %18 = tt.addptr %17, %16 : tensor<16x16x!tt.ptr<f32>, #blocked2>, tensor<16x16xi32, #blocked2>\n+  %19 = tt.broadcast %11 : (tensor<1x16xi1, #blocked3>) -> tensor<16x16xi1, #blocked3>\n+  %20 = triton_gpu.convert_layout %19 : (tensor<16x16xi1, #blocked3>) -> tensor<16x16xi1, #blocked2>\n+  %21 = tt.broadcast %8 : (tensor<16x1xi1, #blocked2>) -> tensor<16x16xi1, #blocked2>\n+  %22 = arith.andi %20, %21 : tensor<16x16xi1, #blocked2>\n+  %23 = triton_gpu.convert_layout %18 : (tensor<16x16x!tt.ptr<f32>, #blocked2>) -> tensor<16x16x!tt.ptr<f32>, #blocked4>\n+  %24 = triton_gpu.convert_layout %22 : (tensor<16x16xi1, #blocked2>) -> tensor<16x16xi1, #blocked4>\n+  %25 = tt.load %23, %24 {cache = 1 : i32, evict = 3 : i32, isVolatile = false} : tensor<16x16xf32, #blocked4>\n+  %26 = triton_gpu.convert_layout %25 : (tensor<16x16xf32, #blocked4>) -> tensor<16x16xf32, #blocked2>\n+  %27 = \"triton_gpu.cmpf\"(%cst_2, %26) {predicate = 4 : i64} : (tensor<16x16xf32, #blocked2>, tensor<16x16xf32, #blocked2>) -> tensor<16x16xi1, #blocked2>\n+  %28 = arith.andi %22, %27 : tensor<16x16xi1, #blocked2>\n+  %29 = \"triton_gpu.select\"(%28, %26, %cst_2) : (tensor<16x16xi1, #blocked2>, tensor<16x16xf32, #blocked2>, tensor<16x16xf32, #blocked2>) -> tensor<16x16xf32, #blocked2>\n+  %30 = tt.reduce %29 {axis = 1 : i32, redOp = 12 : i32} : tensor<16x16xf32, #blocked2> -> tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+  %31 = triton_gpu.convert_layout %30 : (tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<16xf32, #blocked0>\n+  %32 = triton_gpu.convert_layout %31 : (tensor<16xf32, #blocked0>) -> tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+  %33 = tt.expand_dims %32 {axis = 1 : i32} : (tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<16x1xf32, #blocked1>\n+  %34 = triton_gpu.convert_layout %33 : (tensor<16x1xf32, #blocked1>) -> tensor<16x1xf32, #blocked2>\n+  %35 = arith.sitofp %cst_4 : tensor<16x16xi32, #blocked2> to tensor<16x16xf32, #blocked2>\n+  %36 = arith.addf %35, %cst_3 : tensor<16x16xf32, #blocked2>\n+  %37 = triton_gpu.convert_layout %18 : (tensor<16x16x!tt.ptr<f32>, #blocked2>) -> tensor<16x16x!tt.ptr<f32>, #blocked4>\n+  %38 = triton_gpu.convert_layout %22 : (tensor<16x16xi1, #blocked2>) -> tensor<16x16xi1, #blocked4>\n+  %39 = tt.load %37, %38 {cache = 1 : i32, evict = 3 : i32, isVolatile = false} : tensor<16x16xf32, #blocked4>\n+  %40 = triton_gpu.convert_layout %39 : (tensor<16x16xf32, #blocked4>) -> tensor<16x16xf32, #blocked2>\n+  %41 = tt.broadcast %34 : (tensor<16x1xf32, #blocked2>) -> tensor<16x16xf32, #blocked2>\n+  %42 = arith.subf %40, %41 : tensor<16x16xf32, #blocked2>\n+  %43 = math.exp %42 : tensor<16x16xf32, #blocked2>\n+  %44 = arith.addf %36, %43 : tensor<16x16xf32, #blocked2>\n+  %45 = \"triton_gpu.select\"(%22, %44, %36) : (tensor<16x16xi1, #blocked2>, tensor<16x16xf32, #blocked2>, tensor<16x16xf32, #blocked2>) -> tensor<16x16xf32, #blocked2>\n+  %46 = tt.reduce %45 {axis = 1 : i32, redOp = 2 : i32} : tensor<16x16xf32, #blocked2> -> tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+  %47 = triton_gpu.convert_layout %46 : (tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<16xf32, #blocked0>\n+  %48 = triton_gpu.convert_layout %47 : (tensor<16xf32, #blocked0>) -> tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+  %49 = tt.expand_dims %48 {axis = 1 : i32} : (tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<16x1xf32, #blocked1>\n+  %50 = triton_gpu.convert_layout %49 : (tensor<16x1xf32, #blocked1>) -> tensor<16x1xf32, #blocked2>\n+  %51 = triton_gpu.convert_layout %18 : (tensor<16x16x!tt.ptr<f32>, #blocked2>) -> tensor<16x16x!tt.ptr<f32>, #blocked4>\n+  %52 = triton_gpu.convert_layout %22 : (tensor<16x16xi1, #blocked2>) -> tensor<16x16xi1, #blocked4>\n+  %53 = tt.load %51, %52 {cache = 1 : i32, evict = 3 : i32, isVolatile = false} : tensor<16x16xf32, #blocked4>\n+  %54 = triton_gpu.convert_layout %53 : (tensor<16x16xf32, #blocked4>) -> tensor<16x16xf32, #blocked2>\n+  %55 = arith.subf %54, %41 : tensor<16x16xf32, #blocked2>\n+  %56 = math.log %50 : tensor<16x1xf32, #blocked2>\n+  %57 = tt.broadcast %56 : (tensor<16x1xf32, #blocked2>) -> tensor<16x16xf32, #blocked2>\n+  %58 = arith.subf %55, %57 : tensor<16x16xf32, #blocked2>\n+  %59 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<16x16x!tt.ptr<f32>, #blocked2>\n+  %60 = tt.addptr %59, %16 : tensor<16x16x!tt.ptr<f32>, #blocked2>, tensor<16x16xi32, #blocked2>\n+  %61 = triton_gpu.convert_layout %60 : (tensor<16x16x!tt.ptr<f32>, #blocked2>) -> tensor<16x16x!tt.ptr<f32>, #blocked4>\n+  %62 = triton_gpu.convert_layout %58 : (tensor<16x16xf32, #blocked2>) -> tensor<16x16xf32, #blocked4>\n+  %63 = triton_gpu.convert_layout %22 : (tensor<16x16xi1, #blocked2>) -> tensor<16x16xi1, #blocked4>\n+  tt.store %61, %62, %63 : tensor<16x16xf32, #blocked4>\n+  return\n+}"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 49, "deletions": 10, "changes": 59, "file_content_changes": "@@ -4,6 +4,8 @@\n // matmul: 128x32 @ 32x128 -> 128x128\n #AL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#ALs0 = #triton_gpu.slice<{parent=#AL, dim=0}>\n+#BLs0 = #triton_gpu.slice<{parent=#BL, dim=0}>\n #C = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #A = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n #B = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n@@ -44,9 +46,22 @@\n // CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n // CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n // CHECK:   scf.yield {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_BUFFER]], %[[NEXT_B_BUFFER]], %[[NEXT_A]], %[[NEXT_B]], {{.*}}, {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n-func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n-  %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n-  %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n+func @matmul_loop(%lb : index, %ub : index, %step : index, \n+                  %A : !tt.ptr<f16> {tt.divisibility = 16 : i32}, \n+                  %B : !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n+  // A ptrs\n+  %a_ptr_splat = tt.splat %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n+  %a_tmp0 = tt.make_range {end = 32: i32, start = 0: i32} : tensor<32xi32, #ALs0>\n+  %a_tmp1 = tt.expand_dims %a_tmp0 {axis = 0 : i32} : (tensor<32xi32, #ALs0>) -> tensor<1x32xi32, #AL>\n+  %a_offs = tt.broadcast %a_tmp1 : (tensor<1x32xi32, #AL>) -> tensor<128x32xi32, #AL>\n+  %a_ptr_init = tt.addptr %a_ptr_splat, %a_offs : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi32, #AL>\n+  // B ptrs\n+  %b_ptr_splat = tt.splat %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n+  %b_tmp0 = tt.make_range {end = 128: i32, start = 0: i32} : tensor<128xi32, #BLs0>\n+  %b_tmp1 = tt.expand_dims %b_tmp0 {axis = 0 : i32} : (tensor<128xi32, #BLs0>) -> tensor<1x128xi32, #BL>\n+  %b_offs = tt.broadcast %b_tmp1 : (tensor<1x128xi32, #BL>) -> tensor<32x128xi32, #BL>\n+  %b_ptr_init = tt.addptr %b_ptr_splat, %b_offs : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n+  \n \n   %a_mask = arith.constant dense<true> : tensor<128x32xi1, #AL>\n   %a_other = arith.constant dense<0.00e+00> : tensor<128x32xf16, #AL>\n@@ -103,11 +118,23 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n // CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n // CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n // CHECK:     scf.yield {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_BUFFER]], %[[NEXT_B_BUFFER]], %[[NEXT_A]], %[[NEXT_B]], {{.*}}, {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n-func @matmul_loop_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+func @matmul_loop_nested(%lb : index, %ub : index, %step : index, \n+                         %A : !tt.ptr<f16> {tt.divisibility = 16 : i32}, \n+                         %B : !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n   scf.for %iv0 = %lb to %ub step %step {\n-    %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n-    %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n-\n+    // A ptrs\n+    %a_ptr_splat = tt.splat %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n+    %a_tmp0 = tt.make_range {end = 32: i32, start = 0: i32} : tensor<32xi32, #ALs0>\n+    %a_tmp1 = tt.expand_dims %a_tmp0 {axis = 0 : i32} : (tensor<32xi32, #ALs0>) -> tensor<1x32xi32, #AL>\n+    %a_offs = tt.broadcast %a_tmp1 : (tensor<1x32xi32, #AL>) -> tensor<128x32xi32, #AL>\n+    %a_ptr_init = tt.addptr %a_ptr_splat, %a_offs : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi32, #AL>\n+    // B ptrs\n+    %b_ptr_splat = tt.splat %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n+    %b_tmp0 = tt.make_range {end = 128: i32, start = 0: i32} : tensor<128xi32, #BLs0>\n+    %b_tmp1 = tt.expand_dims %b_tmp0 {axis = 0 : i32} : (tensor<128xi32, #BLs0>) -> tensor<1x128xi32, #BL>\n+    %b_offs = tt.broadcast %b_tmp1 : (tensor<1x128xi32, #BL>) -> tensor<32x128xi32, #BL>\n+    %b_ptr_init = tt.addptr %b_ptr_splat, %b_offs : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n+  \n     %a_mask = arith.constant dense<true> : tensor<128x32xi1, #AL>\n     %a_other = arith.constant dense<0.00e+00> : tensor<128x32xf16, #AL>\n     %b_mask = arith.constant dense<true> : tensor<32x128xi1, #BL>\n@@ -156,9 +183,21 @@ func @matmul_loop_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f\n // CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n // CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n // CHECK:   scf.yield {{.*}}, {{.*}}, %[[NEXT_B_BUFFER]], %[[NEXT_B]], {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n-func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n-  %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n-  %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n+func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index, \n+                                  %A : !tt.ptr<f16> {tt.divisibility = 16 : i32}, \n+                                  %B : !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n+  // A ptrs\n+  %a_ptr_splat = tt.splat %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n+  %a_tmp0 = tt.make_range {end = 32: i32, start = 0: i32} : tensor<32xi32, #ALs0>\n+  %a_tmp1 = tt.expand_dims %a_tmp0 {axis = 0 : i32} : (tensor<32xi32, #ALs0>) -> tensor<1x32xi32, #AL>\n+  %a_offs = tt.broadcast %a_tmp1 : (tensor<1x32xi32, #AL>) -> tensor<128x32xi32, #AL>\n+  %a_ptr_init = tt.addptr %a_ptr_splat, %a_offs : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi32, #AL>\n+  // B ptrs\n+  %b_ptr_splat = tt.splat %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n+  %b_tmp0 = tt.make_range {end = 128: i32, start = 0: i32} : tensor<128xi32, #BLs0>\n+  %b_tmp1 = tt.expand_dims %b_tmp0 {axis = 0 : i32} : (tensor<128xi32, #BLs0>) -> tensor<1x128xi32, #BL>\n+  %b_offs = tt.broadcast %b_tmp1 : (tensor<1x128xi32, #BL>) -> tensor<32x128xi32, #BL>\n+  %b_ptr_init = tt.addptr %b_ptr_splat, %b_offs : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n \n   %a_mask = arith.constant dense<true> : tensor<128x32xi1, #AL>\n   %a_other = arith.constant dense<0.00e+00> : tensor<128x32xf16, #AL>"}, {"filename": "test/lib/Analysis/TestAlias.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -36,8 +36,8 @@ struct TestAliasPass\n   void runOnOperation() override {\n     Operation *operation = getOperation();\n     auto &os = llvm::errs();\n-    auto op_name = SymbolTable::getSymbolName(operation).getValue().str();\n-    os << op_name << \"\\n\";\n+    auto opName = SymbolTable::getSymbolName(operation).getValue().str();\n+    os << opName << \"\\n\";\n \n     SharedMemoryAliasAnalysis analysis(&getContext());\n     analysis.run(operation);"}, {"filename": "test/lib/Analysis/TestAllocation.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -19,9 +19,9 @@ struct TestAllocationPass\n   void runOnOperation() override {\n     Operation *operation = getOperation();\n     auto &os = llvm::errs();\n-    // Convert to std::string can remove quotes from op_name\n-    auto op_name = SymbolTable::getSymbolName(operation).getValue().str();\n-    os << op_name << \"\\n\";\n+    // Convert to std::string can remove quotes from opName\n+    auto opName = SymbolTable::getSymbolName(operation).getValue().str();\n+    os << opName << \"\\n\";\n     Allocation allocation(operation);\n     operation->walk([&](Operation *op) {\n       auto scratchBufferId = allocation.getBufferId(op);"}, {"filename": "test/lib/Analysis/TestAxisInfo.cpp", "status": "modified", "additions": 11, "deletions": 3, "changes": 14, "file_content_changes": "@@ -11,7 +11,7 @@ struct TestAxisInfoPass\n   // LLVM15+\n   // MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestAlignmentPass);\n \n-  void print(const std::string &name, raw_ostream &os, ArrayRef<int> vals) {\n+  void print(const std::string &name, raw_ostream &os, ArrayRef<int64_t> vals) {\n     os << name << \": [\";\n     for (size_t d = 0; d < vals.size(); d++) {\n       if (d != 0)\n@@ -29,7 +29,8 @@ struct TestAxisInfoPass\n   void runOnOperation() override {\n     Operation *operation = getOperation();\n     auto &os = llvm::errs();\n-    os << \"Testing: \" << operation->getName() << \"\\n\";\n+    auto opName = SymbolTable::getSymbolName(operation).getValue().str();\n+    os << opName << \"\\n\";\n     AxisInfoAnalysis analysis(&getContext());\n     analysis.run(operation);\n     operation->walk([&](Operation *op) {\n@@ -51,7 +52,14 @@ struct TestAxisInfoPass\n         print(\"Divisibility\", os, info.getDivisibility());\n         os << \" ; \";\n         print(\"Constancy\", os, info.getConstancy());\n-        os << \" ( \";\n+        os << \" ; \";\n+        auto constantValue = info.getConstantValue();\n+        os << \"ConstantValue: [\";\n+        if (constantValue.has_value())\n+          os << constantValue.value();\n+        else\n+          os << \"None\";\n+        os << \"] ( \";\n         result.print(os);\n         os << \" ) \";\n         os << \"\\n\";"}, {"filename": "test/lib/Analysis/TestMembar.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -23,8 +23,8 @@ struct TestMembarPass\n     Operation *operation = getOperation();\n     auto &os = llvm::errs();\n     // Convert to std::string can remove quotes from op_name\n-    auto op_name = SymbolTable::getSymbolName(operation).getValue().str();\n-    os << op_name << \"\\n\";\n+    auto opName = SymbolTable::getSymbolName(operation).getValue().str();\n+    os << opName << \"\\n\";\n     Allocation allocation(operation);\n     MembarAnalysis membarPass(&allocation);\n     membarPass.run();"}]