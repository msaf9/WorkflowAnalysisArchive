[{"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 10, "deletions": 8, "changes": 18, "file_content_changes": "@@ -1036,17 +1036,19 @@ struct AbsFOpConversion\n   using Adaptor = typename Base::OpAdaptor;\n \n   Value createDestOp(mlir::math::AbsFOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type llElemTy,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n                      ValueRange operands, Location loc) const {\n-    auto ttElemTy = getElementTypeOrSelf(op.getType());\n-    auto bf16 = rewriter.getBF16Type();\n-    if (ttElemTy == bf16) {\n-      Value tmp = bitcast(operands[0], bf16);\n-      tmp = rewriter.create<LLVM::FAbsOp>(loc, bf16, tmp);\n-      return bitcast(tmp, llElemTy);\n+    if (llvm::isa<IntegerType>(elemTy)) {\n+      // Mask out the sign bit\n+      auto num_bits = getElementTypeOrSelf(op.getType()).getIntOrFloatBitWidth();\n+      assert(num_bits <= 16);\n+      auto mask = (1u << (num_bits - 1u)) - 1u;\n+      auto maskAttr = rewriter.getIntegerAttr(elemTy, mask);\n+      auto maskConst = rewriter.create<LLVM::ConstantOp>(loc, maskAttr);\n+      return and_(operands[0], maskConst);\n     }\n \n-    return rewriter.create<LLVM::FAbsOp>(loc, llElemTy, operands[0]);\n+    return rewriter.create<LLVM::FAbsOp>(loc, elemTy, operands[0]);\n   }\n };\n "}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 27, "deletions": 0, "changes": 27, "file_content_changes": "@@ -532,6 +532,33 @@ def test_math_op(expr, device='cuda'):\n def test_abs(dtype_x, device='cuda'):\n     _test_unary(dtype_x, 'tl.abs(x)', 'np.abs(x) ', device=device)\n \n+\n+@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n+def test_abs_f8(in_dtype):\n+\n+    @triton.jit\n+    def abs_kernel(Z, X, SIZE: tl.constexpr):\n+        off = tl.arange(0, SIZE)\n+        x = tl.load(X + off)\n+        z = tl.abs(x)\n+        tl.store(Z + off, z)\n+\n+    f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device='cuda')\n+    # f32_to_f8 doesn't handle nan, so we make sure f8_tensor doesn't contain any nan\n+    all_exp_ones = (f8_tensor & 0b01111100) == 128 - 2**in_dtype.fp_mantissa_width\n+    f8_tensor[all_exp_ones] = 0\n+    f8 = triton.reinterpret(f8_tensor, in_dtype)\n+    n_elements = f8_tensor.numel()\n+    out_f8 = torch.empty_like(f8_tensor)\n+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+    abs_kernel[(1,)](f8, triton.reinterpret(out_f8, in_dtype), n_elements)\n+\n+    f32_tensor = convert_float_to_float32(f8_tensor, in_dtype)\n+    expect = f32_tensor.abs()\n+    actual_f8 = convert_float_to_float32(out_f8, in_dtype)\n+    torch.testing.assert_allclose(expect, actual_f8)\n+\n+\n # ----------------\n # test indexing\n # ----------------"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1234,6 +1234,7 @@ def abs(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n     else:\n         assert False, f\"Unexpected dtype {dtype}\"\n \n+\n ##\n \n "}]