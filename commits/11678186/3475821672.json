[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 12, "deletions": 9, "changes": 21, "file_content_changes": "@@ -5958,19 +5958,22 @@ struct AtomicRMWOpConversion\n     auto elemsPerThread = getElemsPerThread(val.getType());\n     SmallVector<Value> resultVals(elemsPerThread);\n     for (size_t i = 0; i < elemsPerThread; i += vec) {\n-      Value rmvVal = undef(vecTy);\n+      Value rmwVal = undef(vecTy);\n       for (int ii = 0; ii < vec; ++ii) {\n         Value iiVal = createIndexAttrConstant(\n             rewriter, loc, getTypeConverter()->getIndexType(), ii);\n-        rmvVal = insert_element(vecTy, rmvVal, valElements[i], iiVal);\n+        rmwVal = insert_element(vecTy, rmwVal, valElements[i + ii], iiVal);\n       }\n-      Value rmwPtr = bitcast(ptrElements[i], ptr_ty(valTy.getElementType()));\n+      if(vec == 1){\n+        rmwVal = extract_element(valueElemTy, rmwVal, idx_val(0));\n+      }\n+      Value rmwPtr = bitcast(ptrElements[i], ptr_ty(valTy.getElementType(), 1));\n       std::string sTy;\n       PTXBuilder ptxBuilder;\n \n       auto *dstOpr = ptxBuilder.newOperand(\"=r\");\n-      auto *ptrOpr = ptxBuilder.newAddrOperand(rmwPtr, \"r\");\n-      auto *valOpr = ptxBuilder.newOperand(rmvVal, \"r\");\n+      auto *ptrOpr = ptxBuilder.newAddrOperand(rmwPtr, \"l\");\n+      auto *valOpr = ptxBuilder.newOperand(rmwVal, \"r\");\n \n       auto &atom = ptxBuilder.create<>(\"atom\")->global().o(\"gpu\");\n       auto rmwOp = stringifyRMWOp(atomicRmwAttr).str();\n@@ -6012,12 +6015,12 @@ struct AtomicRMWOpConversion\n         return failure();\n       }\n       atom.o(rmwOp).o(sTy);\n-\n-      atom(dstOpr, ptrOpr, valOpr);\n-      auto ret = ptxBuilder.launch(rewriter, loc, valueElemTy, false);\n+      Value pred = int_val(1, 1);//TODO:[dongdongl] actual mask \n+      atom(dstOpr, ptrOpr, valOpr).predicate(pred);\n+      auto ret = ptxBuilder.launch(rewriter, loc, valueElemTy);\n       for (int ii = 0; ii < vec; ++ii) {\n         resultVals[i * vec + ii] =\n-            vec == 1 ? ret : extract_element(vecTy, ret, idx_val(ii));\n+            vec == 1 ? ret : extract_element(valueElemTy, ret, idx_val(ii));\n       }\n     }\n     Type structTy = getTypeConverter()->convertType(valueTy);"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -378,7 +378,7 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n       TritonGenericPattern<triton::AddPtrOp>, TritonReducePattern,\n       TritonExpandDimsPattern, TritonMakeRangePattern, TritonDotPattern,\n       TritonLoadPattern, TritonStorePattern, TritonExtElemwisePattern,\n-      TritonPrintfPattern>(typeConverter, context);\n+      TritonPrintfPattern, TritonAtomicRMWPattern>(typeConverter, context);\n }\n \n //"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -1086,9 +1086,7 @@ void init_triton_ir(py::module &&m) {\n               mlir::Value &ptr, mlir::Value &val,\n               mlir::Value &mask) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n-             auto ptrType = mlir::getElementTypeOrSelf(ptr)\n-                                .cast<mlir::triton::PointerType>();\n-             mlir::Type dstType = ptrType.getPointeeType();\n+             mlir::Type dstType = val.getType();\n              return self.create<mlir::triton::AtomicRMWOp>(loc, dstType, rmwOp,\n                                                            ptr, val, mask);\n            })"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 19, "deletions": 0, "changes": 19, "file_content_changes": "@@ -671,6 +671,25 @@ def without_fn(X, Y, A, B, C):\n #     kernel[(1,)](z_tri, x_tri, axis, shape0, shape1)\n #     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n \n+def test_tensor_atomic_rmw_add_elementwise(device=\"cuda\"):\n+    shape0, shape1 = 16, 16\n+    @triton.jit\n+    def kernel(Z, X, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n+        off0 = tl.arange(0, SHAPE0)\n+        off1 = tl.arange(0, SHAPE1)\n+        x = tl.load(X + off0[:, None] * SHAPE1 + off1[None, :])\n+        tl.atomic_add(Z + off0[:, None] * SHAPE1 + off1[None, :], x)\n+\n+    rs = RandomState(17)\n+    x = numpy_random((shape0, shape1), dtype_str=\"float32\", rs=rs)\n+    z = numpy_random((shape0, shape1), dtype_str=\"float32\", rs=rs)\n+    # reference\n+    z_ref = z + x\n+    # triton result\n+    x_tri = torch.from_numpy(x).to(device=device)\n+    z_tri = torch.from_numpy(z).to(device=device)\n+    kernel[(1,)](z_tri, x_tri, shape0, shape1)\n+    np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n \n # def test_atomic_cas():\n #     # 1. make sure that atomic_cas changes the original value (Lock)"}]