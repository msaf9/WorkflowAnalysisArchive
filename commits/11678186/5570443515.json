[{"filename": "include/triton/Dialect/Triton/IR/Dialect.h", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -45,6 +45,12 @@ class DialectInferLayoutInterface\n   inferDotOpEncoding(Attribute operandEncoding, unsigned opIdx,\n                      Attribute retEncoding,\n                      std::optional<Location> location) const = 0;\n+\n+  // Verify that the encoding are compatible to be used together in a dot\n+  // operation\n+  virtual LogicalResult\n+  verifyDotOpEncodingCompatibility(Operation *op, Attribute operandEncodingA,\n+                                   Attribute operandEncodingB) const = 0;\n };\n \n } // namespace triton"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 6, "deletions": 8, "changes": 14, "file_content_changes": "@@ -6,7 +6,6 @@\n #include \"mlir/IR/OperationSupport.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/Triton/IR/Types.h\"\n-#include \"triton/Dialect/TritonGPU/IR/Attributes.h\"\n \n namespace mlir {\n namespace triton {\n@@ -404,18 +403,17 @@ LogicalResult mlir::triton::DotOp::verify() {\n   auto bTy = getOperand(1).getType().cast<RankedTensorType>();\n   if (aTy.getElementType() != bTy.getElementType())\n     return emitError(\"element types of operands A and B must match\");\n-  auto aEncoding =\n-      aTy.getEncoding().dyn_cast_or_null<triton::gpu::DotOperandEncodingAttr>();\n-  auto bEncoding =\n-      bTy.getEncoding().dyn_cast_or_null<triton::gpu::DotOperandEncodingAttr>();\n+  auto aEncoding = aTy.getEncoding();\n+  auto bEncoding = bTy.getEncoding();\n   if (!aEncoding && !bEncoding)\n     return mlir::success();\n   // Verify that the encodings are valid.\n   if (!aEncoding || !bEncoding)\n     return emitError(\"mismatching encoding between A and B operands\");\n-  if (aEncoding.getMMAv2kWidth() != bEncoding.getMMAv2kWidth())\n-    return emitError(\"mismatching kWidth between A and B operands\");\n-  return mlir::success();\n+  Dialect &dialect = aEncoding.getDialect();\n+  auto interface = cast<DialectInferLayoutInterface>(&dialect);\n+  return interface->verifyDotOpEncodingCompatibility(getOperation(), aEncoding,\n+                                                     bEncoding);\n }\n \n //-- ReduceOp --"}, {"filename": "lib/Dialect/Triton/Transforms/ReorderBroadcast.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -143,7 +143,8 @@ struct MoveBroadcastAfterElementwisePattern\n     auto operands = op->getOperands();\n     triton::BroadcastOp broadcastOp;\n     for (auto operand : operands) {\n-      if (broadcastOp = operand.getDefiningOp<triton::BroadcastOp>()) {\n+      broadcastOp = operand.getDefiningOp<triton::BroadcastOp>();\n+      if (broadcastOp) {\n         break;\n       }\n     }"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 17, "deletions": 0, "changes": 17, "file_content_changes": "@@ -1106,6 +1106,23 @@ struct TritonGPUInferLayoutInterface\n           location, \"Dot's a/b's encoding should be of DotOperandEncodingAttr\");\n     return success();\n   }\n+\n+  LogicalResult\n+  verifyDotOpEncodingCompatibility(Operation *op, Attribute operandEncodingA,\n+                                   Attribute operandEncodingB) const override {\n+    auto aEncoding =\n+        operandEncodingA.dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n+    auto bEncoding =\n+        operandEncodingB.dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n+    if (!aEncoding && !bEncoding)\n+      return mlir::success();\n+    // Verify that the encodings are valid.\n+    if (!aEncoding || !bEncoding)\n+      return op->emitError(\"mismatching encoding between A and B operands\");\n+    if (aEncoding.getMMAv2kWidth() != bEncoding.getMMAv2kWidth())\n+      return op->emitError(\"mismatching kWidth between A and B operands\");\n+    return success();\n+  }\n };\n \n //===----------------------------------------------------------------------===//"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 23, "deletions": 21, "changes": 44, "file_content_changes": "@@ -761,27 +761,29 @@ scf::ForOp LoopPipeliner::cloneForOp(ArrayRef<Value> newLoopArgs,\n     mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n   mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n \n-  // Clone the loop body, replace original args with args of the new ForOp\n-  // Insert async wait if necessary.\n+  // Clone the loop body, replace original args with args of the new ForOp.\n+  // We want to find cvt ops that match the following pattern:\n+  // %0 = load %ptr\n+  // %1 (dotOperand) = cvt %0\n   for (Operation &op : forOp.getBody()->without_terminator()) {\n-    // is modified\n-    auto it = std::find(validLoads.begin(), validLoads.end(), op.getOperand(0));\n-    if (it == validLoads.end()) {\n-      Operation *newOp = cloneWithInferType(builder, &op, mapping);\n-      continue;\n-    }\n-\n-    // we replace the use new load use with a convert layout\n-    size_t i = std::distance(validLoads.begin(), it);\n-    auto cvtDstTy = op.getResult(0).getType().cast<RankedTensorType>();\n-    if (!cvtDstTy.getEncoding().isa<ttg::DotOperandEncodingAttr>()) {\n-      builder.clone(op, mapping);\n-      continue;\n+    if (auto cvtOp = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n+      auto result = op.getResult(0);\n+      auto cvtDstTy = result.getType().cast<RankedTensorType>();\n+      if (cvtDstTy.getEncoding().isa<ttg::DotOperandEncodingAttr>()) {\n+        auto it =\n+            std::find(validLoads.begin(), validLoads.end(), op.getOperand(0));\n+        if (it != validLoads.end()) {\n+          // We replace the use new load use with a convert layout\n+          auto loadArgIdx = std::distance(validLoads.begin(), it);\n+          auto cvt = builder.create<ttg::ConvertLayoutOp>(\n+              result.getLoc(), cvtDstTy,\n+              newForOp.getRegionIterArgs()[loadIdx + loadArgIdx]);\n+          mapping.map(result, cvt.getResult());\n+          continue;\n+        }\n+      }\n     }\n-    auto cvt = builder.create<ttg::ConvertLayoutOp>(\n-        op.getResult(0).getLoc(), cvtDstTy,\n-        newForOp.getRegionIterArgs()[loadIdx + i]);\n-    mapping.map(op.getResult(0), cvt.getResult());\n+    cloneWithInferType(builder, &op, mapping);\n   }\n \n   return newForOp;\n@@ -807,11 +809,11 @@ void LoopPipeliner::prefetchNextIteration(scf::ForOp newForOp,\n                                     nextIV, newForOp.getUpperBound());\n \n   pipelineIterIdx = newForOp.getRegionIterArgs()[ivIndex + 1];\n-  Value insertSliceIndex = builder.create<arith::RemSIOp>(\n+  Value insertSliceIndex = builder.create<arith::RemUIOp>(\n       nextIV.getLoc(), pipelineIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n   loopIterIdx = newForOp.getRegionIterArgs()[ivIndex + 2];\n-  Value extractSliceIndex = builder.create<arith::RemSIOp>(\n+  Value extractSliceIndex = builder.create<arith::RemUIOp>(\n       nextIV.getLoc(), loopIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n "}, {"filename": "python/setup.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -125,8 +125,8 @@ def download_and_copy_ptxas():\n \n     base_dir = os.path.dirname(__file__)\n     src_path = \"bin/ptxas\"\n-    version = \"12.1.105\"\n-    url = f\"https://conda.anaconda.org/nvidia/label/cuda-12.1.1/linux-64/cuda-nvcc-{version}-0.tar.bz2\"\n+    version = \"12.2.91\"\n+    url = f\"https://conda.anaconda.org/nvidia/label/cuda-12.2.0/linux-64/cuda-nvcc-{version}-0.tar.bz2\"\n     dst_prefix = os.path.join(base_dir, \"triton\")\n     dst_suffix = os.path.join(\"third_party\", \"cuda\", src_path)\n     dst_path = os.path.join(dst_prefix, dst_suffix)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 21, "deletions": 1, "changes": 22, "file_content_changes": "@@ -2493,14 +2493,18 @@ def test_default(device):\n     ret1 = torch.zeros(1, dtype=torch.int32, device=device)\n \n     @triton.jit\n-    def _kernel(ret0, ret1, value):\n+    def _kernel(ret0, ret1, value=3):\n         tl.store(ret0, _impl())\n         tl.store(ret1, _impl(value))\n \n     _kernel[(1,)](ret0, ret1, value)\n     assert ret0.item() == 10\n     assert ret1.item() == value\n \n+    _kernel[(1,)](ret0, ret1)\n+    assert ret0.item() == 10\n+    assert ret1.item() == 3\n+\n # ---------------\n # test noop\n # ----------------\n@@ -3078,6 +3082,22 @@ def kernel(InitI, Bound, CutOff, OutI, OutInitI, OutJ):\n     assert out_i[0] == init_i[0] + 1\n     assert out_j[0] == bound[0]\n \n+\n+def test_while(device):\n+    @triton.jit\n+    def nested_while(data, countPtr):\n+        for i in range(10):\n+            count = tl.load(countPtr)\n+            while count > 0:\n+                tl.store(data, tl.load(data) + 1.0)\n+                count = count - 2\n+\n+    counter = torch.tensor([8], dtype=torch.int32, device=device)\n+    data = torch.zeros((1,), device=device, dtype=torch.float32)\n+    nested_while[(1,)](data, counter)\n+    assert data[0] == 40\n+\n+\n # def test_for_if(device):\n \n #     @triton.jit"}, {"filename": "python/test/unit/tools/test_aot.py", "status": "modified", "additions": 4, "deletions": 5, "changes": 9, "file_content_changes": "@@ -92,7 +92,7 @@ def kernel(C, A, B,\n   cuMemAlloc(&B, K * N * 2);\n   cuMemAlloc(&C, M * N * 4);\n   cuStreamCreate(&stream, 0);\n-  load_kernel();\n+  load_matmul_fp16xfp16_16x16x16();\n \n   // initialize input data\n   int16_t hA[M*K];\n@@ -105,10 +105,9 @@ def kernel(C, A, B,\n   cuMemcpyHtoD(B, hB, K*N*2);\n \n   // launch kernel\n-  int numWarps = 1;\n   int gX = 1, gY = 1, gZ = 1;\n   cuStreamSynchronize(stream);\n-  kernel(stream, M/BM, N/BN, 1, numWarps, C, A, B, N, K, N);\n+  matmul_fp16xfp16_16x16x16(stream, M/BM, N/BN, 1, C, A, B, N, K, N);\n   cuStreamSynchronize(stream);\n \n   // read data\n@@ -119,7 +118,7 @@ def kernel(C, A, B,\n \n \n   // free cuda handles\n-  unload_kernel();\n+  unload_matmul_fp16xfp16_16x16x16();\n   cuMemFree(A);\n   cuMemFree(B);\n   cuMemFree(C);\n@@ -153,7 +152,7 @@ def test_compile_link_matmul():\n             for hb in hints:\n                 sig = f'*fp32:16, *{dtype}:16, *{dtype}:16, i32{ha}, 1, i32{hb}, 1, i32:16, 1, {BM}, {BN}, {BK}'\n                 name = f\"matmul_{dtype}x{dtype}_{BM}x{BN}x{BK}\"\n-                subprocess.run([sys.executable, compiler_path, \"-n\", \"kernel\", \"--signature\", sig, \"--out-name\", name, \"-o\", name, kernel_path], check=True, cwd=tmp_dir)\n+                subprocess.run([sys.executable, compiler_path, \"-n\", \"kernel\", \"--signature\", sig, \"--out-name\", name, \"-o\", name, \"-w\", \"1\", kernel_path], check=True, cwd=tmp_dir)\n \n         # link all desired configs\n         h_files = glob.glob(os.path.join(tmp_dir, \"*.h\"))"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -659,6 +659,7 @@ def visit_UnaryOp(self, node):\n     def visit_While(self, node):\n         with enter_sub_region(self) as sr:\n             liveins, insert_block = sr\n+            ip, last_loc = self._get_insertion_point_and_loc()\n \n             # loop body (the after region)\n             # loop_block = self.builder.create_block()\n@@ -668,6 +669,7 @@ def visit_While(self, node):\n             self.visit_compound_statement(node.body)\n             self.scf_stack.pop()\n             loop_defs = self.local_defs\n+            dummy.erase()\n \n             # collect loop-carried values\n             names = []\n@@ -684,7 +686,7 @@ def visit_While(self, node):\n                     ret_types.append(loop_defs[name].type)\n                     init_args.append(liveins[name])\n \n-            self.builder.set_insertion_point_to_end(insert_block)\n+            self._set_insertion_point_and_loc(ip, last_loc)\n             while_op = self.builder.create_while_op([ty.to_ir(self.builder) for ty in ret_types],\n                                                     [arg.handle for arg in init_args])\n             # merge the condition region"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 25, "deletions": 3, "changes": 28, "file_content_changes": "@@ -7,7 +7,7 @@\n \n from .._C.libtriton.triton import ir\n from ..runtime.jit import jit\n-from . import semantic\n+from . import math, semantic\n \n T = TypeVar('T')\n \n@@ -1422,6 +1422,11 @@ def _argmax_combine_tie_break_fast(value1, index1, value2, index2):\n     return _argmax_combine(value1, index1, value2, index2, False)\n \n \n+@jit\n+def _fast_max(x, y):\n+    return math.max(x, y)\n+\n+\n @jit\n @_add_reduction_docstr(\"maximum\",\n                        return_indices_arg=\"return_indices\",\n@@ -1434,7 +1439,13 @@ def max(input, axis=None, return_indices=False, return_indices_tie_break_left=Tr\n         else:\n             return _reduce_with_indices(input, axis, _argmax_combine_tie_break_fast)\n     else:\n-        return reduce(input, axis, maximum)\n+        if constexpr(input.dtype.primitive_bitwidth) < 32:\n+            if constexpr(input.dtype.is_floating()):\n+                input = input.to(float32)\n+            else:\n+                assert input.dtype.is_integer_type()\n+                input = input.to(int32)\n+        return reduce(input, axis, _fast_max)\n \n \n @jit\n@@ -1468,6 +1479,11 @@ def _argmin_combine_tie_break_fast(value1, index1, value2, index2):\n     return _argmin_combine(value1, index1, value2, index2, False)\n \n \n+@jit\n+def _fast_min(x, y):\n+    return math.min(x, y)\n+\n+\n @jit\n @_add_reduction_docstr(\"minimum\",\n                        return_indices_arg=\"return_indices\",\n@@ -1480,7 +1496,13 @@ def min(input, axis=None, return_indices=False, return_indices_tie_break_left=Tr\n         else:\n             return _reduce_with_indices(input, axis, _argmin_combine_tie_break_fast)\n     else:\n-        return reduce(input, axis, minimum)\n+        if constexpr(input.dtype.primitive_bitwidth) < 32:\n+            if constexpr(input.dtype.is_floating()):\n+                input = input.to(float32)\n+            else:\n+                assert input.dtype.is_integer_type()\n+                input = input.to(int32)\n+        return reduce(input, axis, _fast_min)\n \n \n @jit"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -316,9 +316,10 @@ def _make_launcher(self):\n \n         spec_keys = ', '.join(specializations)\n         grid_args = ','.join([f'\"{arg}\": {arg}' for arg in self.arg_names])\n+        args_signature = ', '.join(name if dflt == inspect._empty else f'{name} = {dflt}' for name, dflt in zip(self.arg_names, self.arg_defaults))\n \n         src = f\"\"\"\n-def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False, device=None, device_type=None):\n+def {self.fn.__name__}({args_signature}, grid=None, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False, device=None, device_type=None):\n     from ..compiler import compile, CompiledKernel\n     sig_key =  {sig_keys},\n     constexpr_key = {f'{constexpr_keys},' if len(constexpr_keys) > 0 else ()}\n@@ -327,6 +328,7 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n     if not extern_libs is None:\n       key = (key, tuple(extern_libs.items()))\n     assert num_warps > 0 and (num_warps & (num_warps - 1)) == 0, \"num_warps must be a power of 2\"\n+    assert grid is not None\n     if callable(grid):\n         grid = grid({{{grid_args}}})\n     grid_size = len(grid)\n@@ -407,7 +409,8 @@ def __init__(self, fn, version=None, do_not_specialize=None, debug=None, noinlin\n         # function signature information\n         signature = inspect.signature(fn)\n         self.arg_names = [v.name for v in signature.parameters.values()]\n-        self.has_defaults = any(v.default != inspect._empty for v in signature.parameters.values())\n+        self.arg_defaults = [v.default for v in signature.parameters.values()]\n+        self.has_defaults = any(v != inspect._empty for v in self.arg_defaults)\n         # specialization hints\n         self.do_not_specialize = [] if do_not_specialize is None else do_not_specialize\n         self.do_not_specialize = {self.arg_names.index(arg) if isinstance(arg, str) else arg for arg in self.do_not_specialize}"}, {"filename": "python/triton/third_party/cuda/bin/ptxas", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/triton/tools/compile.c", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -41,7 +41,7 @@ void load_{kernel_name}() {{\n     void *bin = (void *)&CUBIN_NAME;\n     int shared = {shared};\n     CUDA_CHECK(cuModuleLoadData(&{kernel_name}_mod, bin));\n-    CUDA_CHECK(cuModuleGetFunction(&{kernel_name}_func, {kernel_name}_mod, \"{kernel_name}\"));\n+    CUDA_CHECK(cuModuleGetFunction(&{kernel_name}_func, {kernel_name}_mod, \"{triton_kernel_name}\"));\n     // set dynamic shared memory if necessary\n     int shared_optin;\n     CUDA_CHECK(cuDeviceGetAttribute(&shared_optin, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN, dev));\n@@ -54,11 +54,11 @@ void load_{kernel_name}() {{\n /*\n {kernel_docstring}\n */\n-CUresult {kernel_name}(CUstream stream, unsigned int gX,unsigned int gY,unsigned int gZ,unsigned int numWarps, {signature}) {{\n+CUresult {kernel_name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {signature}) {{\n     if ({kernel_name}_func == NULL)\n        load_{kernel_name}();\n     void *args[{num_args}] = {{ {arg_pointers} }};\n     // TODO: shared memory\n     if(gX * gY * gZ > 0)\n-      return cuLaunchKernel({kernel_name}_func, gX, gY, gZ, numWarps * 32, 1, 1, {shared}, stream, args, NULL);\n+      return cuLaunchKernel({kernel_name}_func, gX, gY, gZ, {num_warps} * 32, 1, 1, {shared}, stream, args, NULL);\n }}"}, {"filename": "python/triton/tools/compile.h", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -11,5 +11,6 @@\n void unload_{kernel_name}(void);\n void load_{kernel_name}(void);\n // tt-linker: {kernel_name}:{signature}\n-CUresult{kernel_name}(CUstream stream, unsigned int gX, unsigned int gY,\n-                      unsigned int gZ, unsigned int numWarps, {signature});\n+CUresult{_placeholder} {kernel_name}(CUstream stream, unsigned int gX,\n+                                     unsigned int gY, unsigned int gZ,\n+                                     {signature});"}, {"filename": "python/triton/tools/compile.py", "status": "modified", "additions": 25, "deletions": 7, "changes": 32, "file_content_changes": "@@ -1,8 +1,10 @@\n import binascii\n+import hashlib\n import importlib.util\n import sys\n from argparse import ArgumentParser\n from pathlib import Path\n+from typing import List\n \n import triton\n from triton.compiler.code_generator import kernel_suffix\n@@ -26,7 +28,7 @@\n \n The resulting entry point will have signature\n \n-CUresult kernel_{specialization_suffix}(CUstream stream, unsigned gX, unsigned gY, unsigned gZ, unsigned numWarps, float* arg0, int32_t arg1, int32_t arg2)\n+CUresult kernel_{specialization_suffix}(CUstream stream, unsigned gX, unsigned gY, unsigned gZ, float* arg0, int32_t arg1, int32_t arg2)\n \n Different such specialized entry points can be combined using the `linker.py` script.\n \n@@ -39,12 +41,16 @@\n     # command-line arguments\n     parser = ArgumentParser(description=desc)\n     parser.add_argument(\"path\", help=\"Path to Python source containing desired kernel in its scope. File will be executed.\")\n-    parser.add_argument(\"--kernel-name\", \"-n\", type=str, default=\"\")\n-    parser.add_argument(\"--out-path\", \"-o\", type=Path, help=\"Out filename\")\n+    parser.add_argument(\"--kernel-name\", \"-n\", type=str, default=\"\", help=\"Name of the kernel to compile\", required=True)\n+    parser.add_argument(\"--num-warps\", \"-w\", type=int, default=1, help=\"Number of warps to launch the kernel\")\n     parser.add_argument(\"--out-name\", \"-on\", type=str, default=None, help=\"Out name for the compiled kernel\")\n-    parser.add_argument(\"--signature\", \"-s\", type=str, help=\"Signature of the kernel\")\n+    parser.add_argument(\"--out-path\", \"-o\", type=Path, default=None, help=\"Out filename\")\n+    parser.add_argument(\"--signature\", \"-s\", type=str, help=\"Signature of the kernel\", required=True)\n     args = parser.parse_args()\n \n+    out_name = args.out_name if args.out_name else args.kernel_name\n+    out_path = args.out_path if args.out_path else out_name\n+\n     # execute python sources and extract functions wrapped in JITFunction\n     arg_path = Path(args.path)\n     sys.path.insert(0, str(arg_path.parent))\n@@ -56,6 +62,13 @@\n     # validate and parse signature\n     signature = list(map(lambda s: s.strip(\" \"), args.signature.split(\",\")))\n \n+    def hash_signature(signature: List[str]):\n+        m = hashlib.sha256()\n+        m.update(\" \".join(signature).encode())\n+        return m.hexdigest()[:8]\n+\n+    sig_hash = hash_signature(signature)\n+\n     def constexpr(s):\n         try:\n             ret = int(s)\n@@ -68,6 +81,7 @@ def constexpr(s):\n         except ValueError:\n             pass\n         return None\n+\n     hints = {i: constexpr(s.split(\":\")[1]) for i, s in enumerate(signature) if \":\" in s}\n     hints = {k: v for k, v in hints.items() if v is not None}\n     constexprs = {i: constexpr(s) for i, s in enumerate(signature)}\n@@ -80,24 +94,28 @@ def constexpr(s):\n     divisible_by_16 = [i for i, h in hints.items() if h == 16]\n     equal_to_1 = [i for i, h in hints.items() if h == 1]\n     config = triton.compiler.instance_descriptor(divisible_by_16=divisible_by_16, equal_to_1=equal_to_1)\n-    ccinfo = triton.compile(kernel, signature=signature, constants=constexprs, configs=[config], num_warps=1)\n+    ccinfo = triton.compile(kernel, signature=signature, constants=constexprs, configs=[config], num_warps=args.num_warps)\n     arg_names = [kernel.arg_names[i] for i in signature.keys()]\n \n     # dump C stub code\n     suffix = kernel_suffix(signature.values(), config)\n-    func_name = '_'.join([kernel.__name__, suffix])\n+    func_name = '_'.join([out_name, sig_hash, suffix])\n+    triton_kernel_name = '_'.join([args.kernel_name, suffix])\n     hex_ = str(binascii.hexlify(ccinfo.asm[\"cubin\"]))[2:-1]\n     params = {\n         \"kernel_name\": func_name,\n+        \"triton_kernel_name\": triton_kernel_name,\n         \"bin_size\": len(hex_),\n         \"bin_data\": \", \".join([f\"0x{x}{y}\" for x, y in zip(hex_[::2], hex_[1::2])]),\n         \"signature\": \", \".join([f\"{ty_to_cpp(ty)} {name}\" for name, ty in zip(arg_names, signature.values())]),\n         \"arg_pointers\": \", \".join([f\"&{arg}\" for arg in arg_names]),\n         \"num_args\": len(arg_names),\n         \"kernel_docstring\": \"\",\n         \"shared\": ccinfo.shared,\n+        \"num_warps\": args.num_warps,\n+        \"_placeholder\": \"\",\n     }\n     for ext in ['h', 'c']:\n         template_path = Path(__file__).parent / f\"compile.{ext}\"\n-        with args.out_path.with_suffix(f\".{suffix}.{ext}\").open(\"w\") as fp:\n+        with out_path.with_suffix(f\".{sig_hash}_{suffix}.{ext}\").open(\"w\") as fp:\n             fp.write(Path(template_path).read_text().format(**params))"}, {"filename": "python/triton/tools/link.py", "status": "modified", "additions": 12, "deletions": 10, "changes": 22, "file_content_changes": "@@ -18,6 +18,7 @@ class KernelLinkerMeta:\n     arg_names: Sequence[str]\n     arg_ctypes: Sequence[str]\n     sizes: Sequence[Union[int, None]]\n+    sig_hash: str\n     suffix: str\n     num_specs: int\n     \"\"\" number of specialized arguments \"\"\"\n@@ -30,7 +31,7 @@ def __init__(self) -> None:\n         # [kernel_name, c signature]\n         self.linker_directives = re.compile(\"//[\\\\s]*tt-linker:[\\\\s]*([\\\\w]+):(.+)\")\n         # [name, suffix]\n-        self.kernel_name = re.compile(\"([\\\\w]+)_([\\\\w]+)\")\n+        self.kernel_name = re.compile(\"^([\\\\w]+)_([\\\\w]+)_([\\\\w]+)$\")\n         # [(argnum, d|c)]\n         self.kernel_suffix = re.compile(\"([0-9]+)([c,d])\")\n         # [(type, name)]\n@@ -45,7 +46,7 @@ def extract_linker_meta(self, header: str):\n                 m = self.linker_directives.match(ln)\n                 if _exists(m):\n                     ker_name, c_sig = m.group(1), m.group(2)\n-                    name, suffix = self._match_name(ker_name)\n+                    name, sig_hash, suffix = self._match_name(ker_name)\n                     c_types, arg_names = self._match_c_sig(c_sig)\n                     num_specs, sizes = self._match_suffix(suffix)\n                     self._add_kernel(\n@@ -54,6 +55,7 @@ def extract_linker_meta(self, header: str):\n                             arg_names=arg_names,\n                             arg_ctypes=c_types,\n                             sizes=sizes,\n+                            sig_hash=sig_hash,\n                             suffix=suffix,\n                             num_specs=num_specs,\n                         ),\n@@ -62,8 +64,8 @@ def extract_linker_meta(self, header: str):\n     def _match_name(self, ker_name: str):\n         m = self.kernel_name.match(ker_name)\n         if _exists(m):\n-            name, suffix = m.group(1), m.group(2)\n-            return name, suffix\n+            name, sig_hash, suffix = m.group(1), m.group(2), m.group(3)\n+            return name, sig_hash, suffix\n         raise LinkerError(f\"{ker_name} is not a valid kernel name\")\n \n     def _match_c_sig(self, c_sig: str):\n@@ -110,7 +112,7 @@ def gen_signature(m):\n \n def make_decls(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n     return f\"\"\"\n-CUresult {name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, unsigned int numWarps, {gen_signature(metas[-1])});\n+CUresult {name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {gen_signature(metas[-1])});\n void load_{name}();\n void unload_{name}();\n     \"\"\"\n@@ -119,26 +121,26 @@ def make_decls(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n def make_kernel_dispatcher(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n     src = f\"// launcher for: {name}\\n\"\n     for meta in sorted(metas, key=lambda m: -m.num_specs):\n-        src += f\"CUresult {name}_{meta.suffix}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, unsigned int numWarps, {gen_signature(meta)});\\n\"\n+        src += f\"CUresult {name}_{meta.sig_hash}_{meta.suffix}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {gen_signature(meta)});\\n\"\n     src += \"\\n\"\n \n-    src += f\"CUresult {name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, unsigned int numWarps, {gen_signature(metas[-1])}){{\"\n+    src += f\"CUresult {name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {gen_signature(metas[-1])}){{\"\n     src += \"\\n\"\n     for meta in sorted(metas, key=lambda m: -m.num_specs):\n         cond_fn = lambda val, hint: f\"({val} % {hint} == 0)\" if hint == 16 else f\"({val} == {hint})\" if hint == 1 else None\n         conds = \" && \".join([cond_fn(val, hint) for val, hint in zip(meta.arg_names, meta.sizes) if hint is not None])\n         src += f\"  if ({conds})\\n\"\n-        src += f\"    return {name}_{meta.suffix}(stream, gX, gY, gZ, numWarps, {', '.join(meta.arg_names)});\\n\"\n+        src += f\"    return {name}_{meta.sig_hash}_{meta.suffix}(stream, gX, gY, gZ, {', '.join(meta.arg_names)});\\n\"\n     src += \"}\\n\"\n \n     for mode in [\"load\", \"unload\"]:\n         src += f\"\\n// {mode} for: {name}\\n\"\n         for meta in sorted(metas, key=lambda m: -m.num_specs):\n-            src += f\"void {mode}_{name}_{meta.suffix}();\\n\"\n+            src += f\"void {mode}_{name}_{meta.sig_hash}_{meta.suffix}();\\n\"\n         src += f\"void {mode}_{name}() {{\"\n         src += \"\\n\"\n         for meta in sorted(metas, key=lambda m: -m.num_specs):\n-            src += f\"  {mode}_{name}_{meta.suffix}();\\n\"\n+            src += f\"  {mode}_{name}_{meta.sig_hash}_{meta.suffix}();\\n\"\n         src += \"}\\n\"\n     return src\n "}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -13,6 +13,11 @@\n import triton.language as tl\n \n \n+@triton.jit\n+def max_fn(x, y):\n+    return tl.math.max(x, y)\n+\n+\n @triton.jit\n def _fwd_kernel(\n     Q, K, V, sm_scale,"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -37,8 +37,8 @@\n // CHECK:   %[[arg_b0_dot_op_0:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n // CHECK:   %[[arg_b0_dot_op_1:.*]] = arith.mulf %[[arg_b0_dot_op_0]]\n // CHECK:   tt.dot %[[arg_a0_dot_op]], %[[arg_b0_dot_op_1]], {{.*}}\n-// CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remui %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remui %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK:   %[[NEXT_A_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:   %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n@@ -110,8 +110,8 @@ tt.func @matmul_loop(%lb : index, %ub : index, %step : index,\n // CHECK:     %[[arg_a0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_a0]]\n // CHECK:     %[[arg_b0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n // CHECK:     tt.dot %[[arg_a0_dot_op]], %[[arg_b0_dot_op]], {{.*}}\n-// CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remui %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remui %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK:     %[[NEXT_A_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:     %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:     triton_gpu.async_wait {num = 2 : i32}\n@@ -179,8 +179,8 @@ tt.func @matmul_loop_nested(%lb : index, %ub : index, %step : index,\n // CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n // CHECK:   %[[arg_b0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n // CHECK:   tt.dot {{.*}}, %[[arg_b0_dot_op]], {{.*}}\n-// CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remui %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remui %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK:   %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:   triton_gpu.async_wait {num = 1 : i32}\n // CHECK:   %[[NEXT_B:.*]] = triton_gpu.extract_slice %[[NEXT_B_BUFFER]][%[[EXTRACT_IDX]], 0, 0]"}]