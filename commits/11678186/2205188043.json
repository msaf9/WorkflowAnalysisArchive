[{"filename": ".github/workflows/documentation.yml", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -18,6 +18,11 @@ jobs:\n         with:\n           ref: 'gh-pages'\n \n+      - name: Clear docs\n+        run: |\n+          rm -r /tmp/triton-docs\n+        continue-on-error: true\n+\n       - name: Checkout branch\n         uses: actions/checkout@v1\n \n@@ -31,7 +36,6 @@ jobs:\n         run: |\n           git branch\n           # update docs\n-          rm -r /tmp/triton-docs; \n           mkdir /tmp/triton-docs;\n           mv docs/_build/html/* /tmp/triton-docs/\n           git checkout gh-pages"}, {"filename": "lib/ir/builder.cc", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,4 +1,3 @@\n-#include <bits/types/clock_t.h>\n #include <string>\n #include <algorithm>\n #include <iostream>"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 37, "deletions": 21, "changes": 58, "file_content_changes": "@@ -1032,28 +1032,44 @@ def kernel(VALUE, X):\n             kernel[(1, )](value, x)\n     else:\n         kernel[(1, )](value, x)\n-# -------------------------\n-# test dynamic parallelism\n-# -------------------------\n \n \n-@triton.jit\n-def mult(x, alpha):\n-    tl.store(x + tl.program_id(0), alpha)\n+# ----------------\n+# test constexpr\n+# ----------------\n \n+@pytest.mark.parametrize(\"op\", ['+', '-', '*', '/', '%', '<', '>'])\n+@pytest.mark.parametrize(\"is_lhs_constexpr\", [False, True])\n+@pytest.mark.parametrize(\"is_rhs_constexpr\", [True, False])\n+def test_bin_op_constexpr(op, is_lhs_constexpr, is_rhs_constexpr):\n \n-@triton.jit\n-def stub(X, alpha, grid_0, grid_1, grid_2):\n-    tl.launch(mult, [X, alpha], [grid_0, grid_1, grid_2])\n-\n-\n-# def test_dyn_par(cond=True, device='cuda'):\n-#     n_pids = 10\n-#     # pids = torch.arange(n_pids, device=device)\n-#     # alpha = 2.0\n-#     # x_ref = pids * alpha\n-#     x_tri = torch.full((10,), fill_value=-1., device=device)\n-#     # cond = torch.tensor([cond], device=device)\n-#     stub[(1,)](x_tri, 3.14, n_pids, 1, 1)\n-#     print(x_tri)\n-#     # triton.testing.assert_almost_equal(x_ref, x_tri)\n+    @triton.jit\n+    def kernel(Z, X, Y):\n+        x = tl.load(X)\n+        y = tl.load(Y)\n+        z = GENERATE_TEST_HERE\n+        tl.store(Z, z)\n+\n+    x_str = \"3.14\" if is_lhs_constexpr else \"x\"\n+    y_str = \"4.13\" if is_rhs_constexpr else \"y\"\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f\"{x_str} {op} {y_str}\"})\n+    x = numpy_random((1,), dtype_str=\"float32\")\n+    y = numpy_random((1,), dtype_str=\"float32\")\n+    z = np.array(eval(f\"{x_str} {op} {y_str}\"))\n+    x_tri = to_triton(x)\n+    y_tri = to_triton(y)\n+    z_tri = to_triton(np.empty((1,), dtype=z.dtype))\n+    kernel[(1,)](z_tri, x_tri, y_tri)\n+    np.testing.assert_allclose(z, to_numpy(z_tri))\n+\n+\n+def test_constexpr_shape():\n+\n+    @triton.jit\n+    def kernel(X):\n+        off = tl.arange(0, 128 + 128)\n+        tl.store(X + off, off)\n+\n+    x_tri = to_triton(np.empty((256, ), dtype=np.int32))\n+    kernel[(1,)](x_tri)\n+    np.testing.assert_equal(to_numpy(x_tri), np.arange(0, 256))"}, {"filename": "python/triton/code_gen.py", "status": "modified", "additions": 47, "deletions": 27, "changes": 74, "file_content_changes": "@@ -23,6 +23,13 @@\n from .tools.disasm import extract\n \n \n+def current_cuda_stream(device_idx=0):\n+    # Torch's torch.cuda.current_stream() is slow. We provide this\n+    # function to give the user an opportunity to monkey-patch their\n+    # own faster current stream lookup.\n+    return torch.cuda.current_stream().cuda_stream\n+\n+\n def mangle_ty(ty):\n     if ty.is_ptr():\n         return 'P' + mangle_ty(ty.element_ty)\n@@ -54,6 +61,7 @@ def mangle_fn(name, arg_tys, constants):\n     mangled_constants = '_'.join([f'{i}c{key(constants[i])}' for i in sorted(constants)])\n     mangled_constants = mangled_constants.replace('.', '_d_')\n     mangled_constants = mangled_constants.replace(\"'\", '_sq_')\n+    mangled_constants = mangled_constants.replace(\"e-\", '_em_')\n     ret = f'{name}__{mangled_arg_names}__{mangled_constants}'\n     return ret\n \n@@ -388,12 +396,14 @@ def visit_Tuple(self, node):\n         return tuple(args)\n \n     def visit_BinOp(self, node):\n+        # visit operand\n         lhs = self.visit(node.left)\n         rhs = self.visit(node.right)\n-        if isinstance(lhs, triton.language.constexpr):\n-            lhs = lhs.value\n-        if isinstance(rhs, triton.language.constexpr):\n-            rhs = rhs.value\n+        is_lhs_constexpr = isinstance(lhs, triton.language.constexpr)\n+        is_rhs_constexpr = isinstance(rhs, triton.language.constexpr)\n+        lhs = lhs.value if is_lhs_constexpr else lhs\n+        rhs = rhs.value if is_rhs_constexpr else rhs\n+        # get function name\n         fn = {\n             ast.Add: '__add__',\n             ast.Sub: '__sub__',\n@@ -408,6 +418,10 @@ def visit_BinOp(self, node):\n             ast.BitOr: '__or__',\n             ast.BitXor: '__xor__',\n         }[type(node.op)]\n+        # return a new constexpr if both arg are constexprs\n+        if is_lhs_constexpr and is_rhs_constexpr:\n+            return triton.language.constexpr(getattr(lhs, fn)(rhs))\n+        # call operator\n         if is_triton_tensor(lhs):\n             return getattr(lhs, fn)(rhs, _builder=self.builder)\n         elif is_triton_tensor(rhs):\n@@ -466,14 +480,16 @@ def visit_Compare(self, node):\n         assert len(node.ops) == 1\n         lhs = self.visit(node.left)\n         rhs = self.visit(node.comparators[0])\n-        if isinstance(lhs, triton.language.constexpr):\n-            lhs = lhs.value\n-        if isinstance(rhs, triton.language.constexpr):\n-            rhs = rhs.value\n+        is_lhs_constexpr = isinstance(lhs, triton.language.constexpr)\n+        is_rhs_constexpr = isinstance(rhs, triton.language.constexpr)\n+        lhs = lhs.value if is_lhs_constexpr else lhs\n+        rhs = rhs.value if is_rhs_constexpr else rhs\n+        # handle `is`` and `is not``\n         if type(node.ops[0]) == ast.Is:\n             return triton.language.constexpr(lhs is rhs)\n         if type(node.ops[0]) == ast.IsNot:\n             return triton.language.constexpr(lhs is not rhs)\n+        # function name\n         fn = {\n             ast.Eq: '__eq__',\n             ast.NotEq: '__ne__',\n@@ -482,29 +498,32 @@ def visit_Compare(self, node):\n             ast.Gt: '__gt__',\n             ast.GtE: '__ge__',\n         }[type(node.ops[0])]\n+        # return a new constexpr if both arg are constexprs\n+        if is_lhs_constexpr and is_rhs_constexpr:\n+            return triton.language.constexpr(getattr(lhs, fn)(rhs))\n+        # call operator\n         if is_triton_tensor(lhs):\n             return getattr(lhs, fn)(rhs, _builder=self.builder)\n         elif is_triton_tensor(rhs):\n             fn = fn[:2] + 'r' + fn[2:]\n             return getattr(rhs, fn)(lhs, _builder=self.builder)\n         else:\n-            return getattr(lhs, fn)(rhs)\n+            assert False\n \n     def visit_UnaryOp(self, node):\n         op = self.visit(node.operand)\n         if type(node.op) == ast.Not:\n             assert isinstance(op, triton.language.constexpr), \"`not` only supported for constexpr at the moment\"\n             return triton.language.constexpr(not op)\n-        if isinstance(op, triton.language.constexpr):\n-            op = op.value\n         fn = {\n             ast.USub: '__neg__',\n             ast.UAdd: '__pos__',\n             ast.Invert: '__invert__',\n         }[type(node.op)]\n-        if is_triton_tensor(op):\n-            return getattr(op, fn)(_builder=self.builder)\n-        return getattr(op, fn)()\n+        if isinstance(op, triton.language.constexpr):\n+            return triton.language.constexpr(getattr(op.value, fn)())\n+        assert is_triton_tensor(op)\n+        return getattr(op, fn)(_builder=self.builder)\n \n     def visit_While(self, node):\n         current_bb = self.builder.get_insert_block()\n@@ -654,6 +673,10 @@ def visit_Call(self, node):\n             args = [arg.value if isinstance(arg, triton.language.constexpr) else arg\n                     for arg in args]\n             ret = fn(*args, **kws)\n+            if isinstance(ret, (bool, int, float)):\n+                ret = triton.language.core.constexpr(ret)\n+            else:\n+                ret = triton.language.core._to_tensor(ret, self.builder)\n         # special case: dynamic parallelism\n         # in this case the core primitive returns a proxy\n         # if isinstance(ret, triton.language.core.LaunchProxy):\n@@ -787,6 +810,7 @@ def __reduce__(self):\n \n \n class Kernel:\n+\n     @staticmethod\n     def _type_name(obj):\n         type_names = {\n@@ -915,28 +939,24 @@ def __call__(self, *wargs, grid, num_warps=4, num_stages=2, **kwargs):\n             raise TypeError(f\"Function takes {len(self.fn.arg_names)} positional arguments but {len(wargs)} were given\")\n         # handle annotations\n         for pos, _type in self.fn.annotations.items():\n+            assert _type == triton.language.constexpr, \"only constexpr annotations are supported for now\"\n             wargs[pos] = _type(wargs[pos])\n         # check that tensors are on GPU.\n         for arg in wargs:\n             if hasattr(arg, 'data_ptr'):\n                 assert arg.is_cuda, \"All tensors must be on GPU!\"\n-        # query device index and cuda stream\n+        # set device (i.e., make sure torch has the context initialized)\n         device = torch.cuda.current_device()\n         torch.cuda.set_device(device)\n+        # query compute capability\n         cc = torch.cuda.get_device_capability(device)\n         cc = str(cc[0]) + '-' + str(cc[1])\n-        # # query stream\n-        # # this is hacky but much faster than `torch.cuda.current_stream(device).cuda_stream`\n-        # # https://github.com/pytorch/pytorch/blob/master/c10/core/Stream.h#L154\n-        # # building a C wrapper to re-use the unpack function would add a build-time torch dependency\n-        # # and require different wheels for different torch versions -- undesirable!\n-        # bits = torch._C._cuda_getCurrentStream(device)\n-        # mask = 1 << 47\n-        # stream = ((bits & 0xFFFFFFFFFFFF) ^ mask) - mask\n-        stream = torch.cuda.current_stream(device).cuda_stream\n-        # make key for cache\n-        return _triton.runtime.launch(wargs, self.fn.do_not_specialize, self.fn.cache_key + cc, self.fn.arg_names, device, stream,\n-                                      self.fn.bin_cache, num_warps, num_stages, self.add_to_cache, grid)\n+        cache_key = self.fn.cache_key + cc\n+        # query current stream\n+        stream = current_cuda_stream(device)\n+        return _triton.runtime.launch(wargs, self.fn.do_not_specialize, cache_key, self.fn.arg_names,\n+                                      device, stream, self.fn.bin_cache, num_warps, num_stages, self.add_to_cache,\n+                                      grid)\n \n \n class Launcher:"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 9, "deletions": 62, "changes": 71, "file_content_changes": "@@ -337,68 +337,6 @@ def __init__(self, value):\n     def __repr__(self) -> str:\n         return f\"constexpr[{self.value}]\"\n \n-    def __add__(self, other):\n-        return self.value + other.value\n-\n-    def __radd__(self, other):\n-        return other.value + self.value\n-\n-    def __sub__(self, other):\n-        return self.value - other.value\n-\n-    def __rsub__(self, other):\n-        return other.value - self.value\n-\n-    def __mul__(self, other):\n-        return self.value * other.value\n-\n-    def __rmul__(self, other):\n-        return other.value * self.value\n-\n-    def __truediv__(self, other):\n-        return self.value / other.value\n-\n-    def __rtruediv__(self, other):\n-        return other.value / self.value\n-\n-    def __floordiv__(self, other):\n-        return self.value // other.value\n-\n-    def __rfloordiv__(self, other):\n-        return other.value // self.value\n-\n-    #\n-\n-    def __gt__(self, other):\n-        return self.value > other.value\n-\n-    def __rgt__(self, other):\n-        return other.value > self.value\n-\n-    def __ge__(self, other):\n-        return self.value >= other.value\n-\n-    def __rge__(self, other):\n-        return other.value >= self.value\n-\n-    def __lt__(self, other):\n-        return self.value < other.value\n-\n-    def __rlt__(self, other):\n-        return other.value < self.value\n-\n-    def __le__(self, other):\n-        return self.value <= other.value\n-\n-    def __rle__(self, other):\n-        return other.value <= self.value\n-\n-    def __eq__(self, other):\n-        return self.value == other.value\n-\n-    def __ne__(self, other):\n-        return self.value != other.value\n-\n     def __bool__(self):\n         return bool(self.value)\n \n@@ -442,6 +380,9 @@ def __init__(self, handle, type: dtype):\n         self.numel = 1\n         for s in self.shape:\n             self.numel *= s\n+        is_pow2 = (self.numel and (not(self.numel & (self.numel - 1))))\n+        if not is_pow2:\n+            raise ValueError(\"Triton tensors must have a power-of-two number of elements\")\n         self.numel = constexpr(self.numel)\n         self.type = type  # Tensor type (can be block_type)\n         # Following the practice in pytorch, dtype is scalar type\n@@ -496,6 +437,11 @@ def __mod__(self, other, _builder=None):\n         other = _to_tensor(other, _builder)\n         return semantic.mod(self, other, _builder)\n \n+    @builtin\n+    def __rmod__(self, other, _builder=None):\n+        other = _to_tensor(other, _builder)\n+        return semantic.mod(other, self, _builder)\n+\n     # unary operators\n     @builtin\n     def __neg__(self, _builder=None):\n@@ -564,6 +510,7 @@ def __lt__(self, other, _builder=None):\n \n     @builtin\n     def __rlt__(self, other, _builder=None):\n+        other = _to_tensor(other, _builder)\n         return semantic.less_than(other, self, _builder)\n \n     # <="}, {"filename": "python/tutorials/01-vector-add.py", "status": "modified", "additions": 1, "deletions": 6, "changes": 7, "file_content_changes": "@@ -24,11 +24,9 @@ def add_kernel(\n     y_ptr,  # *Pointer* to second input vector\n     output_ptr,  # *Pointer* to output vector\n     n_elements,  # Size of the vector\n-    time_start_ptr, time_end_ptr,\n     BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process\n                  # NOTE: `constexpr` so it can be used as a shape value\n ):\n-    tl.atomic_min(time_start_ptr, tl.clock())\n     # There are multiple 'program's processing different data. We identify which program\n     # we are here\n     pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0\n@@ -47,7 +45,6 @@ def add_kernel(\n     output = x + y\n     # Write x + y back to DRAM\n     tl.store(output_ptr + offsets, output, mask=mask)\n-    tl.atomic_max(time_end_ptr, tl.clock())\n \n \n # %%\n@@ -56,8 +53,6 @@ def add_kernel(\n \n \n def add(x: torch.Tensor, y: torch.Tensor):\n-    time_start = torch.zeros(1, dtype=torch.int64, device='cuda')\n-    time_end = torch.zeros(1, dtype=torch.int64, device='cuda')\n     # We need to preallocate the output\n     output = torch.empty_like(x)\n     assert x.is_cuda and y.is_cuda and output.is_cuda\n@@ -70,7 +65,7 @@ def add(x: torch.Tensor, y: torch.Tensor):\n     #  - each torch.tensor object is implicitly converted into a pointer to its first element.\n     #  - `triton.jit`'ed functions can be index with a launch grid to obtain a callable GPU kernel\n     #  - don't forget to pass meta-parameters as keywords arguments\n-    add_kernel[grid](x, y, output, n_elements, time_start, time_end, BLOCK_SIZE=1024)\n+    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n     # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\n     # running asynchronously at this point.\n     return output"}]