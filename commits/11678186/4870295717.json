[{"filename": "python/test/regression/test_functional_regressions.py", "status": "added", "additions": 68, "deletions": 0, "changes": 68, "file_content_changes": "@@ -0,0 +1,68 @@\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+def test_chained_matmul():\n+    # Regression test for issue #1601\n+    def chained_matmul_reference(a, b, c):\n+        intermediate = torch.einsum('MK,NK->MN', a, b)\n+        return torch.einsum('MN,NK->MK', intermediate, c)\n+\n+    @triton.jit\n+    def chained_matmul_kernel(\n+            A,  # shape: (m, k)\n+            B,  # shape: (n, k)\n+            C,  # shape: (n, k)\n+            out,  # shape: (m, k)\n+            m, n, k: tl.constexpr,\n+            block_m: tl.constexpr,\n+            block_n: tl.constexpr,\n+            block_k: tl.constexpr):\n+\n+        tl.static_assert(block_k == k,\n+                         f\"expected block_k == k but got {block_k} != {k}\")\n+\n+        block_ix = tl.program_id(0)\n+        a_tile = (block_ix * block_m + tl.arange(0, block_m))[:, None] * block_k \\\n+            + tl.arange(0, block_k)[None, :]\n+\n+        a = tl.load(A + a_tile, mask=a_tile < m * k, other=0.0)\n+\n+        acc = tl.zeros([block_m, block_k], dtype=tl.float32)\n+\n+        for loop_block_start in range(0, n, block_n):\n+            bc_tile = (loop_block_start + tl.arange(0, block_n))[:, None] * block_k \\\n+                + tl.arange(0, block_k)[None, :]\n+            b = tl.load(B + bc_tile, mask=bc_tile < n * k, other=0.0)\n+\n+            intermediate = tl.dot(a, tl.trans(b))\n+            intermediate_mask = ((loop_block_start + tl.arange(0, block_n)) < n)[None, :] \\\n+                * (tl.arange(0, block_m) < m)[:, None]\n+\n+            intermediate = tl.where(intermediate_mask, intermediate, 0.0)\n+\n+            c = tl.load(C + bc_tile, mask=bc_tile < n * k)\n+\n+            acc += tl.dot(intermediate.to(A.dtype.element_ty), c)\n+\n+        tl.store(out + a_tile, acc.to(A.dtype.element_ty), mask=a_tile < m * k)\n+\n+    m, n, k = 32, 64, 128\n+    block_m, block_n, block_k = 16, 32, k\n+\n+    grid = (triton.cdiv(m, block_m),)\n+    a = torch.randint(low=0, high=2, size=(m, k), dtype=torch.float16,\n+                      device='cuda')\n+    b = torch.randint(low=0, high=2, size=(n, k), dtype=torch.float16,\n+                      device='cuda')\n+    c = torch.randint_like(b, low=0, high=2)\n+    triton_result = torch.zeros_like(a)\n+\n+    torch_result = chained_matmul_reference(a, b, c)\n+    chained_matmul_kernel[grid](a, b, c, triton_result, m, n, k,\n+                                block_m=block_m, block_n=block_n,\n+                                block_k=block_k)\n+\n+    assert (torch_result == triton_result).all()"}]