[{"filename": "python/triton/compiler.py", "status": "modified", "additions": 15, "deletions": 14, "changes": 29, "file_content_changes": "@@ -5,6 +5,7 @@\n import functools\n import hashlib\n import io\n+import json\n import os\n import shutil\n import subprocess\n@@ -15,7 +16,6 @@\n from sysconfig import get_paths\n from typing import Any, Dict, Set, Tuple, Union\n \n-import json\n import setuptools\n import torch\n from filelock import FileLock\n@@ -1094,7 +1094,6 @@ def put(self, data, filename, binary=True):\n             os.rename(filepath + \".tmp\", filepath)\n \n \n-\n # utilties for generating and compiling C wrappers\n \n \n@@ -1162,13 +1161,15 @@ def _build(name, src, srcdir):\n         setuptools.setup(**args)\n     return so\n \n+\n def make_so_cache_key(signature, constants):\n     # Get unique key for the compiled code\n     signature = {k: 'ptr' if v[0] == '*' else v for k, v in signature.items()}\n     key = f\"{''.join(signature.values())}{constants}\"\n     key = hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n     return key\n \n+\n def make_fn_cache_key(fn_hash, signature, configs, constants, num_warps, num_stages):\n     # Get unique key for the compiled code\n     get_conf_key = lambda conf: (sorted(conf.divisible_by_16), sorted(conf.equal_to_1))\n@@ -1177,6 +1178,7 @@ def make_fn_cache_key(fn_hash, signature, configs, constants, num_warps, num_sta\n     key = hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n     return key\n \n+\n def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: int = 4, num_stages: int = 3, extern_libs=None, configs=None):\n     # we get the kernel, i.e. the first function generated in the module\n     assert len(configs) == 1\n@@ -1189,13 +1191,13 @@ def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: i\n     # retrieve stub from cache if it exists\n     if not so_cache_manager.has_file(so_name):\n         with tempfile.TemporaryDirectory() as tmpdir:\n-          src = generate_launcher(name, constants, signature)\n-          src_path = os.path.join(tmpdir, \"main.c\")\n-          with open(src_path, \"w\") as f:\n-              f.write(src)\n-          so = _build(fn.__name__, src_path, tmpdir)\n-          with open(so, \"rb\") as f:\n-              so_cache_manager.put(f.read(), so_name, binary=True)\n+            src = generate_launcher(name, constants, signature)\n+            src_path = os.path.join(tmpdir, \"main.c\")\n+            with open(src_path, \"w\") as f:\n+                f.write(src)\n+            so = _build(fn.__name__, src_path, tmpdir)\n+            with open(so, \"rb\") as f:\n+                so_cache_manager.put(f.read(), so_name, binary=True)\n \n     # retrieve cached shared object if it exists\n     fn_cache_key = make_fn_cache_key(fn.cache_key, signature, configs, constants, num_warps, num_stages)\n@@ -1207,7 +1209,7 @@ def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: i\n         metadata = {\"name\": kernel_name, \"shared\": shared, \"num_warps\": num_warps, \"num_stages\": num_stages}\n         fn_cache_manager.put(asm[\"cubin\"], cubin_name)\n         fn_cache_manager.put(json.dumps(metadata), data_name, binary=False)\n-    \n+\n     return CompiledKernel(name, so_cache_manager._make_path(so_name), fn_cache_manager.cache_dir)\n \n \n@@ -1229,10 +1231,9 @@ def __init__(self, fn_name, so_path, cache_dir):\n         cu_path = os.path.join(cache_dir, f\"{fn_name}.cubin\")\n         device = torch.cuda.current_device()\n         with open(cu_path, \"rb\") as cubin:\n-          mod, func, n_regs, n_spills = _triton.code_gen.load_binary(metadata[\"name\"], cubin.read(), self.shared, device)\n-          self.cu_module = mod\n-          self.cu_function = func\n-\n+            mod, func, n_regs, n_spills = _triton.code_gen.load_binary(metadata[\"name\"], cubin.read(), self.shared, device)\n+            self.cu_module = mod\n+            self.cu_function = func\n \n     def __getitem__(self, grid):\n         def runner(*args, stream=None):"}]