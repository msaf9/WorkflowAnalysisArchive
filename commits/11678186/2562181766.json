[{"filename": "lib/codegen/analysis/layout.cc", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -238,9 +238,11 @@ mma_layout::mma_layout(size_t num_warps,\n     for(ir::value* v: values)\n     for(ir::user* u: v->get_users()){\n       auto* dot = dynamic_cast<ir::dot_inst*>(u);\n-      if((dot && dot->get_operand(2)!=v) || !layout_a->to_shared())\n+      auto* cts = dynamic_cast<ir::copy_to_shared_inst*>(u);\n+      if((dot && dot->get_operand(2)!=v) || !layout_a->to_shared() || cts)\n         one_warp_per_row = shape[0] / spw_[0] >= num_warps;\n     }\n+    // std::cout << one_warp_per_row << std::endl;\n \n     if(one_warp_per_row){\n       wpt_[1] = 1;"}, {"filename": "lib/codegen/analysis/liveness.cc", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -45,6 +45,10 @@ void liveness::run(ir::module &mod) {\n         gen.insert(layouts_->get(layouts_->tmp(i))->to_shared());\n         kill.insert(layouts_->get(layouts_->tmp(i))->to_shared());\n       }\n+      if(layouts_->has_tmp_index(i)){\n+        gen.insert(layouts_->get(layouts_->tmp_index(i))->to_shared());\n+        kill.insert(layouts_->get(layouts_->tmp_index(i))->to_shared());\n+      }\n       // live-out\n       std::set<shared_layout*> live_out;\n       std::vector<ir::instruction*> succs = {last_inst};"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -844,9 +844,9 @@ def test_dot(epilogue, allow_tf32, dtype, device='cuda'):\n         elif dtype == 'float32' and allow_tf32:\n             pytest.skip(\"Only test tf32 on devices with sm >= 80\")\n \n-    M, N, K = 128, 128, 128\n+    M, N, K = 128, 128, 64\n     num_warps = 8\n-    trans_a, trans_b = True, False\n+    trans_a, trans_b = False, False\n \n     # triton kernel\n     @triton.jit\n@@ -889,9 +889,9 @@ def kernel(X, stride_xm, stride_xk,\n         tl.store(Zs, z)\n     # input\n     rs = RandomState(17)\n-    x = numpy_random((K, M) if trans_a else (M, K), dtype_str=dtype, rs=rs)\n-    y = numpy_random((N, K) if trans_b else (K, N), dtype_str=dtype, rs=rs)\n-    w = numpy_random((N, N), dtype_str=dtype, rs=rs)\n+    x = numpy_random((K, M) if trans_a else (M, K), dtype_str=dtype, rs=rs)*.1\n+    y = numpy_random((N, K) if trans_b else (K, N), dtype_str=dtype, rs=rs)*.1\n+    w = numpy_random((N, N), dtype_str=dtype, rs=rs)*.1\n     if allow_tf32:\n         x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n         y = (y.view('uint32') & np.uint32(0xffffe000)).view('float32')\n@@ -900,7 +900,7 @@ def kernel(X, stride_xm, stride_xk,\n     y_tri = to_triton(y, device=device)\n     w_tri = to_triton(w, device=device)\n     # triton result\n-    z = numpy_random((M, N), dtype_str=dtype, rs=rs)\n+    z = 1 + numpy_random((M, N), dtype_str=dtype, rs=rs)*.1\n     z_tri = to_triton(z, device=device)\n     if epilogue == 'trans':\n         z_tri = torch.as_strided(z_tri, (M, N), z_tri.stride()[::-1])"}]