[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1284,7 +1284,7 @@ def kernel(X, stride_xm, stride_xn,\n                                            [128, 128, 64, 4],\n                                            [64, 128, 128, 4],\n                                            [32, 128, 64, 2],\n-                                           [32, 32, 16, 4],\n+                                           [64, 64, 32, 4],\n                                            [128, 128, 64, 2],\n                                            [64, 128, 128, 2]]\n                           for allow_tf32 in [True]\n@@ -1438,7 +1438,7 @@ def kernel(X, stride_xm, stride_xk,\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n     # make sure ld/st are vectorized\n     ptx = pgm.asm['ptx']\n-    if K > 16 and (N > 16 or M > 16):\n+    if K > 16 or N > 16 or M > 16:\n         # XXX: skip small sizes because they are not vectorized\n         assert 'ld.global.v4' in ptx\n         assert 'st.global.v4' in ptx"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -1183,6 +1183,8 @@ def dot(lhs: tl.tensor,\n         and rhs.shape[1].value >= 16,\\\n         \"small blocks not supported!\"\n     if lhs.type.scalar.is_int():\n+        assert lhs.type.scalar == tl.int8, \"only int8 supported!\"\n+        assert lhs.shape[1].value >= 32, \"small blocks not supported!\"\n         _0 = builder.get_int32(0)\n         ret_scalar_ty = tl.int32\n     elif lhs.type.scalar.is_fp32() or lhs.type.scalar.is_bf16():"}]