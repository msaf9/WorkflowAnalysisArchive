[{"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -121,6 +121,9 @@ class MoveOpAfterLayoutConversion : public mlir::RewritePattern {\n         cvtArgOp->getDialect()->getTypeID() !=\n             mlir::TypeID::get<arith::ArithDialect>())\n       return mlir::failure();\n+    // not handled in elementwise lowering.\n+    if (isa<arith::TruncIOp, arith::TruncFOp>(cvtArgOp))\n+      return mlir::failure();\n     // only considers conversions to dot operand\n     if (!cvtTy.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n       return mlir::failure();"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 48, "deletions": 19, "changes": 67, "file_content_changes": "@@ -175,15 +175,16 @@ class LayoutPropagation {\n   Operation *rewriteOp(Operation *op);\n   // Rewrite a for op based on the layout picked by the analysis.\n   Operation *rewriteForOp(scf::ForOp forOp);\n+  Operation *rewriteIfOp(scf::IfOp ifOp);\n   Operation *rewriteYieldOp(scf::YieldOp yieldOp);\n-  // Dump the current stage of layout information.\n   Operation *cloneElementwise(OpBuilder &rewriter, Operation *op,\n                               Attribute encoding);\n   // Map the original value to the rewritten one.\n   void map(Value old, Value newV);\n   // Return the mapped value in the given encoding. This will insert a convert\n   // if the encoding is different than the encoding decided at resolve time.\n   Value getValueAs(Value value, Attribute encoding);\n+  // Dump the current stage of layout information.\n   void dump();\n \n private:\n@@ -264,13 +265,11 @@ void LayoutPropagation::initAnchorLayout() {\n void LayoutPropagation::setEncoding(ValueRange values, LayoutInfo &info,\n                                     SmallVector<Value> &changed,\n                                     Operation *op) {\n-  SmallVector<Attribute> encodings(info.encodings.begin(),\n-                                   info.encodings.end());\n   for (Value value : values) {\n     if (!value.getType().isa<RankedTensorType>())\n       continue;\n     bool hasChanged = false;\n-    for (auto encoding : encodings) {\n+    for (auto encoding : info.encodings) {\n       auto dstEncoding = inferDstEncoding(op, encoding);\n       if (dstEncoding)\n         hasChanged |= layouts[value].encodings.insert(*dstEncoding);\n@@ -295,12 +294,12 @@ SmallVector<Value> LayoutPropagation::propagateToUsers(Value value,\n       auto parent = yieldOp->getParentOp();\n       SmallVector<Value> valuesToPropagate = {\n           parent->getResult(use.getOperandNumber())};\n-      if (auto forOp = dyn_cast<scf::ForOp>(parent)) {\n+      if (auto forOp = dyn_cast<scf::ForOp>(parent))\n         valuesToPropagate.push_back(\n             forOp.getRegionIterArg(use.getOperandNumber()));\n+      if (isa<scf::ForOp, scf::IfOp>(parent))\n         setEncoding({valuesToPropagate}, info, changed, user);\n-      }\n-      // TODO: handle scf.if and while.\n+      // TODO: handle while.\n       continue;\n     }\n     // Workaround: don't propagate through truncI\n@@ -324,7 +323,7 @@ void LayoutPropagation::propagateLayout() {\n   }\n   while (!queue.empty()) {\n     Value currentValue = queue.back();\n-    LayoutInfo &info = layouts[currentValue];\n+    LayoutInfo info = layouts[currentValue];\n     queue.pop_back();\n     SmallVector<Value> changed = propagateToUsers(currentValue, info);\n     queue.insert(queue.end(), changed.begin(), changed.end());\n@@ -520,17 +519,46 @@ Operation *LayoutPropagation::rewriteForOp(scf::ForOp forOp) {\n     }\n     map(oldArg, newArg);\n   }\n-  opToDelete.push_back(forOp.getOperation());\n   return newForOp.getOperation();\n }\n \n+Operation *LayoutPropagation::rewriteIfOp(scf::IfOp ifOp) {\n+  SmallVector<Value> operands;\n+  OpBuilder rewriter(ifOp);\n+  SmallVector<Type> newResultTypes(ifOp->getResultTypes());\n+  for (unsigned i = 0, e = ifOp->getNumResults(); i < e; ++i) {\n+    auto it = layouts.find(ifOp->getResult(i));\n+    if (it == layouts.end())\n+      continue;\n+    auto origType = ifOp->getResult(i).getType().cast<RankedTensorType>();\n+    Attribute encoding = *(it->second.encodings.begin());\n+    newResultTypes[i] = RankedTensorType::get(\n+        origType.getShape(), origType.getElementType(), encoding);\n+  }\n+  auto newIfOp = rewriter.create<scf::IfOp>(ifOp.getLoc(), newResultTypes,\n+                                            ifOp.getCondition(), true, true);\n+  newIfOp.getThenRegion().takeBody(ifOp.getThenRegion());\n+  newIfOp.getElseRegion().takeBody(ifOp.getElseRegion());\n+  for (auto [oldResult, newResult] :\n+       llvm::zip(ifOp.getResults(), newIfOp.getResults())) {\n+    if (oldResult.getType() == newResult.getType()) {\n+      oldResult.replaceAllUsesWith(newResult);\n+      continue;\n+    }\n+    map(oldResult, newResult);\n+  }\n+  return newIfOp.getOperation();\n+}\n+\n Operation *LayoutPropagation::rewriteYieldOp(scf::YieldOp yieldOp) {\n   OpBuilder rewriter(yieldOp);\n   Operation *newYield = rewriter.clone(*yieldOp.getOperation());\n   Operation *parentOp = yieldOp->getParentOp();\n   for (OpOperand &operand : yieldOp->getOpOperands()) {\n-    Value result = parentOp->getResult(operand.getOperandNumber());\n-    auto tensorType = result.getType().dyn_cast<RankedTensorType>();\n+    Type yieldType = operand.get().getType();\n+    if (isa<scf::ForOp, scf::IfOp>(parentOp))\n+      yieldType = parentOp->getResult(operand.getOperandNumber()).getType();\n+    auto tensorType = yieldType.dyn_cast<RankedTensorType>();\n     if (!tensorType)\n       continue;\n     Value newOperand = getValueAs(operand.get(), tensorType.getEncoding());\n@@ -541,21 +569,26 @@ Operation *LayoutPropagation::rewriteYieldOp(scf::YieldOp yieldOp) {\n }\n \n Operation *LayoutPropagation::rewriteOp(Operation *op) {\n-  if (auto forOp = dyn_cast<scf::ForOp>(op)) {\n+  opToDelete.push_back(op);\n+  if (auto forOp = dyn_cast<scf::ForOp>(op))\n     return rewriteForOp(forOp);\n-  }\n+  if (auto ifOp = dyn_cast<scf::IfOp>(op))\n+    return rewriteIfOp(ifOp);\n   OpBuilder rewriter(op);\n   Attribute encoding = *layouts[op->getResult(0)].encodings.begin();\n   if (auto convertOp = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n-    Attribute srcEncoding = *layouts[convertOp.getOperand()].encodings.begin();\n+    Attribute srcEncoding =\n+        convertOp.getOperand().getType().cast<RankedTensorType>().getEncoding();\n+    auto it = layouts.find(convertOp.getOperand());\n+    if (it != layouts.end())\n+      srcEncoding = *(it->second.encodings.begin());\n     Value src = getValueAs(convertOp.getOperand(), srcEncoding);\n     auto tensorType = op->getResult(0).getType().cast<RankedTensorType>();\n     auto newType = RankedTensorType::get(tensorType.getShape(),\n                                          tensorType.getElementType(), encoding);\n     auto cvt = rewriter.create<triton::gpu::ConvertLayoutOp>(op->getLoc(),\n                                                              newType, src);\n     map(op->getResult(0), cvt.getResult());\n-    opToDelete.push_back(op);\n     return cvt.getOperation();\n   }\n   if (canFoldIntoConversion(op, encoding)) {\n@@ -566,7 +599,6 @@ Operation *LayoutPropagation::rewriteOp(Operation *op) {\n     auto cvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n         op->getLoc(), newType, newOp->getResult(0));\n     map(op->getResult(0), cvt.getResult());\n-    opToDelete.push_back(op);\n     return cvt.getOperation();\n   }\n   if (op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() ||\n@@ -577,7 +609,6 @@ Operation *LayoutPropagation::rewriteOp(Operation *op) {\n     for (auto [oldResult, newResult] :\n          llvm::zip(op->getResults(), newOp->getResults()))\n       map(oldResult, newResult);\n-    opToDelete.push_back(op);\n     return newOp;\n   }\n   assert(0 && \"unexpected op in rewrite\");\n@@ -587,8 +618,6 @@ Operation *LayoutPropagation::rewriteOp(Operation *op) {\n static bool canBeRemat(Operation *op) {\n   if (isa<triton::LoadOp, triton::StoreOp>(op))\n     return !isExpensiveLoadOrStore(op);\n-  if (isa<triton::CatOp, triton::ViewOp>(op))\n-    return false;\n   if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n           triton::gpu::InsertSliceAsyncOp, triton::AtomicRMWOp,\n           triton::AtomicCASOp, triton::DotOp>(op))"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -400,6 +400,8 @@ LogicalResult getConvertBackwardSlice(Value root, SetVector<Value> &slice,\n     if (auto *definingOp = currentValue.getDefiningOp()) {\n       if (canFoldIntoConversion(definingOp, encoding))\n         continue;\n+      if (isa<triton::CatOp>(definingOp))\n+        return failure();\n       for (Value operand : definingOp->getOperands()) {\n         auto srcEncoding = inferSrcEncoding(definingOp, encoding);\n         if (!srcEncoding)"}]