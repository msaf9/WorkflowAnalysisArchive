[{"filename": "lib/Conversion/TritonGPUToLLVM/DotHelpers.h", "status": "modified", "additions": 30, "deletions": 35, "changes": 65, "file_content_changes": "@@ -65,20 +65,13 @@ struct DotOpMmaV1ConversionHelper {\n     return struct_ty(SmallVector<Type>{8, fp32Ty});\n   }\n \n-  // number of fp16x2 elements for $a.\n-  int numElemsPerThreadA(RankedTensorType tensorTy) const {\n-    SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n-                               tensorTy.getShape().end());\n-    SmallVector<unsigned> order(getOrder().begin(), getOrder().end());\n-    // TODO[Superjomn]: transA is not available here.\n-    bool transA = false;\n-    if (transA) {\n-      std::swap(shape[0], shape[1]);\n-      std::swap(order[0], order[1]);\n-    }\n-\n-    bool isARow = order[0] != 0;\n-    bool isAVec4 = !isARow && shape[order[0]] <= 16; // fp16*4 = 16bytes\n+  // \\param shapeTransed: the shape or reordered shape if transpose needed.\n+  // \\param orderTransed: the order or reordered order if transpose needed.\n+  unsigned getNumM(ArrayRef<int64_t> shapeTransed,\n+                   ArrayRef<unsigned> orderTransed) const {\n+    bool isARow = orderTransed[0] != 0;\n+    bool isAVec4 =\n+        !isARow && shapeTransed[orderTransed[0]] <= 16; // fp16*4 = 16bytes\n     // TODO[Superjomn]: Support the case when isAVec4=false later\n     // Currently, we only support ld.v2, for the mma layout varies with\n     // different ld vector width.\n@@ -93,8 +86,14 @@ struct DotOpMmaV1ConversionHelper {\n     SmallVector<int> rep({repM, 0, repK}); // pad N with 0\n     SmallVector<int> spw({spwM, 0, 1});    // pad N with 0\n \n-    int NK = shape[1];\n-    unsigned numM = rep[0] * shape[0] / (spw[0] * wpt[0]);\n+    unsigned numM = rep[0] * shapeTransed[0] / (spw[0] * wpt[0]);\n+    return numM;\n+  }\n+\n+  int numElemsPerThreadA(ArrayRef<int64_t> shapeTransed,\n+                         ArrayRef<unsigned> orderTransed) const {\n+    int numM = getNumM(shapeTransed, orderTransed);\n+    int NK = shapeTransed[1];\n \n     // NOTE: We couldn't get the vec from the shared layout.\n     // int vecA = sharedLayout.getVec();\n@@ -105,19 +104,10 @@ struct DotOpMmaV1ConversionHelper {\n   }\n \n   // number of fp16x2 elements for $b.\n-  int numElemsPerThreadB(RankedTensorType tensorTy) const {\n-    SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n-                               tensorTy.getShape().end());\n-    SmallVector<unsigned> order(getOrder().begin(), getOrder().end());\n-    // TODO[Superjomn]: transB is not available here.\n-    bool transB = false;\n-    if (transB) {\n-      std::swap(shape[0], shape[1]);\n-      std::swap(order[0], order[1]);\n-    }\n-\n-    bool isBRow = order[0] != 0;\n-    bool isBVec4 = isBRow && shape[order[0]] <= 16;\n+  unsigned getNumN(ArrayRef<int64_t> shapeTransed,\n+                   ArrayRef<unsigned> orderTransed) const {\n+    bool isBRow = orderTransed[0] != 0;\n+    bool isBVec4 = isBRow && shapeTransed[orderTransed[0]] <= 16;\n     // TODO[Superjomn]: Support the case when isBVec4=false later\n     // Currently, we only support ld.v2, for the mma layout varies with\n     // different ld vector width.\n@@ -127,14 +117,20 @@ struct DotOpMmaV1ConversionHelper {\n     SmallVector<int> fpw({2, 2, 1});\n     SmallVector<int> rep({0, 2 * packSize1, 1});       // pad M with 0\n     SmallVector<int> spw({0, fpw[1] * 4 * rep[1], 1}); // pad M with 0\n+\n+    unsigned numN = rep[1] * shapeTransed[1] / (spw[1] * wpt[1]);\n+    return numN;\n+  }\n+\n+  int numElemsPerThreadB(ArrayRef<int64_t> shapeTransed,\n+                         ArrayRef<unsigned> orderTransed) const {\n+    unsigned numN = getNumN(shapeTransed, orderTransed);\n+    int NK = shapeTransed[0];\n     // NOTE: We couldn't get the vec from the shared layout.\n     // int vecB = sharedLayout.getVec();\n     // TODO[Superjomn]: Consider the case when vecA > 4\n     bool vecGt4 = false;\n     int elemsPerLd = vecGt4 ? 4 : 2;\n-    int NK = shape[0];\n-\n-    unsigned numN = rep[1] * shape[1] / (spw[1] * wpt[1]);\n     return (numN / 2) * (NK / 4) * elemsPerLd;\n   }\n \n@@ -1416,8 +1412,6 @@ Value DotOpMmaV1ConversionHelper::loadA(\n   for (int i = 0; i < numPtrA; i++)\n     ptrA[i] = gep(ptr_ty(f16_ty), smemBase, offA[i]);\n \n-  unsigned numM = std::max<int>(rep[0] * shape[0] / (spw[0] * wpt[0]), 1);\n-\n   Type f16PtrTy = ptr_ty(f16_ty);\n \n   auto ld = [&](decltype(has) &vals, int m, int k, Value val0, Value val1) {\n@@ -1449,6 +1443,7 @@ Value DotOpMmaV1ConversionHelper::loadA(\n     }\n   };\n \n+  unsigned numM = getNumM(shape, order);\n   for (unsigned k = 0; k < NK; k += 4)\n     for (unsigned m = 0; m < numM / 2; ++m)\n       loadA(m, k);\n@@ -1571,7 +1566,7 @@ Value DotOpMmaV1ConversionHelper::loadB(\n     }\n   };\n \n-  unsigned numN = rep[1] * shape[1] / (spw[1] * wpt[1]);\n+  unsigned numN = getNumN(shape, order);\n   for (unsigned k = 0; k < NK; k += 4)\n     for (unsigned n = 0; n < numN / 2; ++n) {\n       if (!hbs.count({n, k}))"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 13, "deletions": 3, "changes": 16, "file_content_changes": "@@ -3840,7 +3840,8 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n   llvm::Optional<Type> convertTritonTensorType(RankedTensorType type) {\n     auto ctx = type.getContext();\n     Attribute layout = type.getEncoding();\n-    auto shape = type.getShape();\n+    SmallVector<int64_t> shape(type.getShape().begin(), type.getShape().end());\n+\n     if (layout &&\n         (layout.isa<BlockedEncodingAttr>() || layout.isa<SliceEncodingAttr>() ||\n          layout.isa<MmaEncodingAttr>())) {\n@@ -3903,13 +3904,22 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n         if (mmaLayout.getVersion() == 1) {\n           DotOpMmaV1ConversionHelper helper(mmaLayout);\n \n+          // TODO[Superjomn]: Both transA and transB are not available here.\n+          bool trans = false;\n+          // TODO[Superjomn]: The order of A and B are not available here.\n+          SmallVector<unsigned> order({1, 0});\n+          if (trans) {\n+            std::swap(shape[0], shape[1]);\n+            std::swap(order[0], order[1]);\n+          }\n+\n           if (dotOpLayout.getOpIdx() == 0) { // $a\n-            int elems = helper.numElemsPerThreadA(type);\n+            int elems = helper.numElemsPerThreadA(shape, order);\n             Type x2Ty = vec_ty(elemTy, 2);\n             return struct_ty(SmallVector<Type>(elems, x2Ty));\n           }\n           if (dotOpLayout.getOpIdx() == 1) { // $b\n-            int elems = helper.numElemsPerThreadB(type);\n+            int elems = helper.numElemsPerThreadB(shape, order);\n             Type x2Ty = vec_ty(elemTy, 2);\n             return struct_ty(SmallVector<Type>(elems, x2Ty));\n           }"}]