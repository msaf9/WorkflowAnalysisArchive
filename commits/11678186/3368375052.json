[{"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 29, "deletions": 13, "changes": 42, "file_content_changes": "@@ -30,8 +30,8 @@ class TT_Op<string mnemonic, list<Trait> traits = []> :\n //   fptoui, fptosi, uitofp, sitofp,\n //   extf, tructf,\n //   extui, extsi, tructi\n-def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [SameOperandsAndResultShape, \n-                                         SameOperandsAndResultEncoding, \n+def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [SameOperandsAndResultShape,\n+                                         SameOperandsAndResultEncoding,\n                                          NoSideEffect,\n                                          /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n     let summary = \"Cast int64 to pointer\";\n@@ -43,7 +43,7 @@ def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [SameOperandsAndResultShape,\n     let assemblyFormat = \"$from attr-dict `:` type($from) `->` type($result)\";\n }\n \n-def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape, \n+def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape,\n                                          SameOperandsAndResultEncoding,\n                                          NoSideEffect,\n                                          /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n@@ -57,7 +57,7 @@ def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape,\n }\n \n // arith.bitcast doesn't support pointers\n-def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape, \n+def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape,\n                                      SameOperandsAndResultEncoding,\n                                      NoSideEffect,\n                                      /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n@@ -72,7 +72,7 @@ def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape,\n     // TODO: Add verifier\n }\n \n-def TT_FpToFp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape, \n+def TT_FpToFp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n                                    SameOperandsAndResultEncoding,\n                                    NoSideEffect,\n                                    /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n@@ -99,7 +99,7 @@ def TT_FpToFp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n //\n \n def TT_AddPtrOp : TT_Op<\"addptr\",\n-                     [NoSideEffect, \n+                     [NoSideEffect,\n                      SameOperandsAndResultShape,\n                      SameOperandsAndResultEncoding,\n                       TypesMatchWith<\"result type matches ptr type\",\n@@ -224,7 +224,7 @@ def TT_AtomicCASOp : TT_Op<\"atomic_cas\", [SameOperandsAndResultShape,\n //\n // Shape Manipulation Ops\n //\n-def TT_SplatOp : TT_Op<\"splat\", [NoSideEffect, \n+def TT_SplatOp : TT_Op<\"splat\", [NoSideEffect,\n                                  SameOperandsAndResultElementType]> {\n     let summary = \"splat\";\n \n@@ -237,8 +237,8 @@ def TT_SplatOp : TT_Op<\"splat\", [NoSideEffect,\n     let hasFolder = 1;\n }\n \n-def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [NoSideEffect, \n-                                            DeclareOpInterfaceMethods<InferTypeOpInterface>, \n+def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [NoSideEffect,\n+                                            DeclareOpInterfaceMethods<InferTypeOpInterface>,\n                                             SameOperandsAndResultElementType]> {\n     let summary = \"expand_dims\";\n \n@@ -249,7 +249,7 @@ def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [NoSideEffect,\n     let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n }\n \n-def TT_ViewOp : TT_Op<\"view\", [NoSideEffect, \n+def TT_ViewOp : TT_Op<\"view\", [NoSideEffect,\n                                SameOperandsAndResultElementType]> {\n     let summary = \"view\";\n \n@@ -261,7 +261,7 @@ def TT_ViewOp : TT_Op<\"view\", [NoSideEffect,\n \n }\n \n-def TT_BroadcastOp : TT_Op<\"broadcast\", [NoSideEffect, \n+def TT_BroadcastOp : TT_Op<\"broadcast\", [NoSideEffect,\n                                          SameOperandsAndResultElementType]> {\n     let summary = \"broadcast. No left-padding as of now.\";\n \n@@ -274,7 +274,7 @@ def TT_BroadcastOp : TT_Op<\"broadcast\", [NoSideEffect,\n     let hasFolder = 1;\n }\n \n-def TT_CatOp : TT_Op<\"cat\", [NoSideEffect, \n+def TT_CatOp : TT_Op<\"cat\", [NoSideEffect,\n                              SameOperandsAndResultElementType]> {\n     let summary = \"concatenate 2 tensors\";\n \n@@ -307,7 +307,7 @@ def TT_GetNumProgramsOp : TT_Op<\"get_num_programs\", [NoSideEffect]> {\n //\n // Dot Op\n //\n-def TT_DotOp : TT_Op<\"dot\", [NoSideEffect, \n+def TT_DotOp : TT_Op<\"dot\", [NoSideEffect,\n                              DeclareOpInterfaceMethods<InferTypeOpInterface>,\n                              TypesMatchWith<\"result's type matches accumulator's type\",\n                                             \"d\", \"c\", \"$_self\">]> {\n@@ -385,4 +385,20 @@ def TT_MakeRangeOp : TT_Op<\"make_range\", [NoSideEffect]> {\n     let assemblyFormat = \"attr-dict `:` type($result)\";\n }\n \n+//\n+// Make PrintfOp\n+//\n+def TT_PrintfOp : TT_Op<\"printf\", [MemoryEffects<[MemWrite]>]>,\n+  Arguments<(ins StrAttr:$prefix,\n+                Variadic<AnyTypeOf<[TT_Type]>>:$args)> {\n+  let summary = \"Device-side printf, as in CUDA for debugging\";\n+  let description = [{\n+    `tt.printf` takes a literal string prefix and an arbitrary number of scalar or tensor arguments that should be printed.\n+    format are generated automatically from the arguments.\n+  }];\n+  let assemblyFormat = [{\n+    $prefix attr-dict ($args^ `:` type($args))?\n+  }];\n+}\n+\n #endif // Triton_OPS"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 97, "deletions": 10, "changes": 107, "file_content_changes": "@@ -148,8 +148,43 @@ LLVM::LLVMFuncOp getVprintfDeclaration(OpBuilder &builder) {\n                                     funcType);\n }\n \n-void llPrintf(StringRef msg, ValueRange args, OpBuilder &builder) {\n+std::pair<Type, Value> promoteValue(OpBuilder &builder, Value value) {\n+  auto *context = builder.getContext();\n+  Type int32Ty = builder.getIntegerType(32);\n+  Type uint32Ty = builder.getIntegerType(32, false);\n+  Type f32Ty = builder.getF32Type();\n+  Type f64Ty = builder.getF64Type();\n+\n+  auto type = value.getType();\n+  unsigned width = type.getIntOrFloatBitWidth();\n+  Value newOp;\n+  Type newType;\n+  if (32 <= width && type.isIntOrIndex()) {\n+    newType = type;\n+    newOp = value;\n+  } else {\n+    bool bUnsigned = type.isUnsignedInteger();\n+    if (type.isIntOrIndex()) {\n+      if (bUnsigned) {\n+        newType = uint32Ty;\n+        newOp = builder.create<LLVM::ZExtOp>(UnknownLoc::get(context), newType,\n+                                             value);\n+      } else {\n+        newType = int32Ty;\n+        newOp = builder.create<LLVM::SExtOp>(UnknownLoc::get(context), newType,\n+                                             value);\n+      }\n+    } else {\n+      newType = f64Ty;\n+      newOp = builder.create<LLVM::FPExtOp>(UnknownLoc::get(context), newType,\n+                                            value);\n+    }\n+  }\n \n+  return {newType, newOp};\n+}\n+\n+void llPrintf(StringRef msg, ValueRange args, OpBuilder &builder) {\n   static const char formatStringPrefix[] = \"printfFormat_\";\n   assert(!msg.empty() && \"printf with empty string not support\");\n \n@@ -160,6 +195,10 @@ void llPrintf(StringRef msg, ValueRange args, OpBuilder &builder) {\n   Type int8Ty = builder.getIntegerType(8);\n   Type int8Ptr = LLVM::LLVMPointerType::get(int8Ty);\n   Type int32Ty = builder.getIntegerType(32);\n+  Type uint32Ty = builder.getIntegerType(32, false);\n+  Type f32Ty = builder.getF32Type();\n+  Type f64Ty = builder.getF64Type();\n+\n   Value one = builder.create<LLVM::ConstantOp>(\n       UnknownLoc::get(context), int32Ty, builder.getI32IntegerAttr(1));\n   Value zero = builder.create<LLVM::ConstantOp>(\n@@ -173,6 +212,7 @@ void llPrintf(StringRef msg, ValueRange args, OpBuilder &builder) {\n   } while (moduleOp.lookupSymbol(stringConstName));\n \n   llvm::SmallString<20> formatString(msg);\n+  formatString.push_back('\\n');\n   formatString.push_back('\\0');\n   size_t formatStringSize = formatString.size_in_bytes();\n   auto globalType = LLVM::LLVMArrayType::get(int8Ty, formatStringSize);\n@@ -195,17 +235,24 @@ void llPrintf(StringRef msg, ValueRange args, OpBuilder &builder) {\n \n   Value bufferPtr =\n       builder.create<LLVM::NullOp>(UnknownLoc::get(context), int8Ptr);\n+\n+  SmallVector<Value, 16> newArgs;\n   if (args.size() >= 1) {\n     SmallVector<Type> argTypes;\n     for (auto arg : args) {\n-      argTypes.push_back(arg.getType());\n+      Type newType;\n+      Value newArg;\n+      std::tie(newType, newArg) = promoteValue(builder, arg);\n+      argTypes.push_back(newType);\n+      newArgs.push_back(newArg);\n     }\n+\n     Type structTy = LLVM::LLVMStructType::getLiteral(context, argTypes);\n     auto allocated = builder.create<LLVM::AllocaOp>(\n         UnknownLoc::get(context), LLVM::LLVMPointerType::get(structTy), one,\n         /*alignment=*/0);\n \n-    for (const auto &entry : llvm::enumerate(args)) {\n+    for (const auto &entry : llvm::enumerate(newArgs)) {\n       auto index = builder.create<LLVM::ConstantOp>(\n           UnknownLoc::get(context), int32Ty,\n           builder.getI32IntegerAttr(entry.index()));\n@@ -1897,13 +1944,7 @@ class ElementwiseOpConversionBase\n     }\n     Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n     rewriter.replaceOp(op, view);\n-    // example for add printf\n-#if 0\n-    {\n-      auto builder = rewriter.atBlockTerminator(rewriter.getInsertionBlock());\n-      LLVM::llPrintf(\"hello world %d, %d\\n\", {i32_val(2008), i32_val(2023)}, builder);\n-    }\n-#endif\n+\n     return success();\n   }\n \n@@ -4085,6 +4126,51 @@ struct FDivOpConversion\n   }\n };\n \n+struct PrintfOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::PrintfOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::PrintfOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::PrintfOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto loc = op->getLoc();\n+    SmallVector<Value, 16> operands;\n+    for (auto operand : adaptor.getOperands()) {\n+      auto sub_operands = this->getElementsFromStruct(loc, operand, rewriter);\n+      for (auto elem : sub_operands) {\n+        operands.push_back(elem);\n+      }\n+    }\n+    std::string formatStr;\n+    llvm::raw_string_ostream os(formatStr);\n+    os << op.prefix();\n+    if (operands.size() > 0) {\n+      os << getFormatSubstr(operands[0]);\n+    }\n+\n+    for (size_t i = 1; i < operands.size(); ++i) {\n+      os << \", \" << getFormatSubstr(operands[i]);\n+    }\n+    mlir::LLVM::llPrintf(formatStr, operands, rewriter);\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+  std::string getFormatSubstr(Value value) const {\n+    Type type = value.getType();\n+    unsigned width = type.getIntOrFloatBitWidth();\n+\n+    if (type.isBF16() || type.isF16() || type.isF32() || type.isF64()) {\n+      return \"%f\";\n+    } else if (type.isSignedInteger()) {\n+      return \"%i\";\n+    } else if (type.isUnsignedInteger() || type.isSignlessInteger()) {\n+      return \"%u\";\n+    }\n+    assert(false && \"not supported type\");\n+  }\n+};\n+\n void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                   RewritePatternSet &patterns, int numWarps,\n                                   AxisInfoAnalysis &axisInfoAnalysis,\n@@ -4171,6 +4257,7 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<ViewLikeOpConversion<triton::ExpandDimsOp>>(typeConverter,\n                                                            benefit);\n   patterns.add<DotOpConversion>(typeConverter, allocation, smem, benefit);\n+  patterns.add<PrintfOpConversion>(typeConverter, benefit);\n }\n \n class ConvertTritonGPUToLLVM"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 14, "deletions": 2, "changes": 16, "file_content_changes": "@@ -339,6 +339,18 @@ struct TritonReducePattern : public OpConversionPattern<triton::ReduceOp> {\n   }\n };\n \n+struct TritonPrintfPattern : public OpConversionPattern<triton::PrintfOp> {\n+  using OpConversionPattern<PrintfOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(PrintfOp op, typename PrintfOp::Adaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    rewriter.replaceOpWithNewOp<triton::PrintfOp>(op, op.prefixAttr(),\n+                                                  adaptor.getOperands());\n+    return success();\n+  }\n+};\n+\n void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n                             RewritePatternSet &patterns) {\n   MLIRContext *context = patterns.getContext();\n@@ -350,8 +362,8 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n       TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n       TritonGenericPattern<triton::AddPtrOp>, TritonReducePattern,\n       TritonExpandDimsPattern, TritonMakeRangePattern, TritonDotPattern,\n-      TritonLoadPattern, TritonStorePattern, TritonExtElemwisePattern>(\n-      typeConverter, context);\n+      TritonLoadPattern, TritonStorePattern, TritonExtElemwisePattern,\n+      TritonPrintfPattern>(typeConverter, context);\n }\n \n //"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "file_content_changes": "@@ -1177,6 +1177,17 @@ void init_triton_ir(py::module &&m) {\n              auto loc = self.getUnknownLoc();\n              return self.create<mlir::SelectOp>(loc, condition, trueValue,\n                                                 falseValue);\n+           })\n+      .def(\"create_printf\",\n+           [](mlir::OpBuilder &self, const std::string &prefix,\n+              const std::vector<mlir::Value> &values) -> void {\n+             auto loc = self.getUnknownLoc();\n+             // llvm::StringRef prefixRef(prefix);\n+             self.create<mlir::triton::PrintfOp>(\n+                 loc,\n+                 mlir::StringAttr::get(self.getContext(),\n+                                       llvm::StringRef(prefix)),\n+                 values);\n            });\n \n   py::class_<mlir::PassManager>(m, \"pass_manager\")"}, {"filename": "python/tests/_printf.py", "status": "added", "additions": 55, "deletions": 0, "changes": 55, "file_content_changes": "@@ -0,0 +1,55 @@\n+import pytest\n+import torch\n+from torch.testing import assert_close\n+\n+import triton\n+import triton.language as tl\n+\n+torch_type = {\n+    \"bool\": torch.bool,\n+    'int8': torch.int8,\n+    'uint8': torch.uint8,\n+    'int16': torch.int16,\n+    \"int32\": torch.int32,\n+    'int64': torch.long,\n+    'float16': torch.float16,\n+    'bfloat16': torch.bfloat16,\n+    \"float32\": torch.float32,\n+    \"float64\": torch.float64\n+}\n+\n+\n+def get_tensor(shape, data_type, b_positive=False):\n+    x = None\n+    if data_type.startswith('int'):\n+        x = torch.arange(0, shape[0], dtype=torch_type[data_type], device='cuda')\n+    else:\n+        x = torch.arange(0, shape[0], dtype=torch_type[data_type], device='cuda')\n+\n+    return x\n+\n+\n+@pytest.mark.parametrize('data_type',\n+                         [(\"int8\"),\n+                          ('int16'),\n+                          ('int32'),\n+                          (\"int64\"),\n+                          ('float16'),\n+                          (\"float32\"),\n+                          (\"float64\")])\n+def test_printf(data_type):\n+    @triton.jit\n+    def kernel(X, Y, BLOCK: tl.constexpr):\n+        x = tl.load(X + tl.arange(0, BLOCK))\n+        tl.printf(\"\", x)\n+        tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+    shape = (128, )\n+    # limit the range of integers so that the sum does not overflow\n+    x = get_tensor(shape, data_type)\n+    y = torch.zeros(shape, dtype=x.dtype, device=\"cuda\")\n+    kernel[(1,)](x, y, BLOCK=shape[0])\n+    assert_close(y, x)\n+\n+\n+test_printf(\"float16\")"}, {"filename": "python/tests/_printf.sh", "status": "added", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -0,0 +1 @@\n+python _printf.py >/tmp/___printf.log 2>&1"}, {"filename": "python/tests/test_printf.py", "status": "added", "additions": 19, "deletions": 0, "changes": 19, "file_content_changes": "@@ -0,0 +1,19 @@\n+import os\n+\n+\n+def test_printf():\n+    os.system('./_printf.sh')\n+    lines = None\n+    with open(\"/tmp/___printf.log\", \"r\") as f:\n+        lines = f.readlines()\n+\n+    new_lines = set()\n+    for line in lines:\n+        try:\n+            value = int(float(line))\n+            new_lines.add(value)\n+        except Exception as e:\n+            print(e)\n+\n+    for i in range(128):\n+        assert i in new_lines"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 19, "deletions": 0, "changes": 19, "file_content_changes": "@@ -1197,3 +1197,22 @@ def swizzle2d(i, j, size_i, size_j, size_g):\n @triton.jit\n def zeros_like(input):\n     return zeros(input.shape, input.dtype)\n+\n+\n+@builtin\n+def printf(prefix, *args, _builder=None):\n+    import string\n+    new_prefix = prefix\n+    if isinstance(prefix, constexpr):\n+        new_prefix = prefix.value\n+    assert isinstance(new_prefix, str), f\"{new_prefix} is not string\"\n+    b_ascii = True\n+    for ch in new_prefix:\n+        if ch not in string.printable:\n+            b_ascii = False\n+            break\n+    assert b_ascii, f\"{new_prefix} is not an ascii string\"\n+    new_args = []\n+    for arg in args:\n+        new_args.append(_to_tensor(arg, _builder))\n+    return semantic.printf(new_prefix, new_args, _builder)"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -1123,3 +1123,10 @@ def max_contiguous(x: tl.tensor, values: List[int]) -> tl.tensor:\n \n def debug_barrier(builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_barrier(''), tl.void)\n+\n+\n+def printf(prefix: str, args: List[tl.tensor], builder: ir.builder) -> tl.tensor:\n+    new_args = []\n+    for arg in args:\n+        new_args.append(arg.handle)\n+    return tl.tensor(builder.create_printf(prefix, new_args), tl.void)"}]