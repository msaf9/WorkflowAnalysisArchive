[{"filename": "lib/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -9,6 +9,7 @@ add_mlir_conversion_library(TritonGPUToLLVM\n     PTXAsmFormat.cpp\n     ReduceOpToLLVM.cpp\n     Utility.cpp\n+    TypeConverter.cpp\n     ViewOpToLLVM.cpp\n     DotOpHelpers.cpp\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 13, "deletions": 16, "changes": 29, "file_content_changes": "@@ -4,10 +4,8 @@\n \n using ::mlir::LLVM::DotOpFMAConversionHelper;\n using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n-using ::mlir::LLVM::getElementsFromStruct;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n using ::mlir::LLVM::getStridesFromShapeAndOrder;\n-using ::mlir::LLVM::getStructFromElements;\n using ::mlir::LLVM::MMA16816ConversionHelper;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::getContigPerThread;\n@@ -393,7 +391,8 @@ struct ConvertLayoutOpConversion\n     // Potentially we need to store for multiple CTAs in this replication\n     auto accumNumReplicates = product<unsigned>(numReplicates);\n     // unsigned elems = getElemsPerThread(srcTy);\n-    auto vals = getElementsFromStruct(loc, adaptor.getSrc(), rewriter);\n+    auto vals = getTypeConverter()->unpackLLElements(loc, adaptor.getSrc(),\n+                                                     rewriter, srcTy);\n     unsigned inVec = 0;\n     unsigned outVec = 0;\n     auto paddedRepShape = getScratchConfigForCvtLayout(op, inVec, outVec);\n@@ -444,7 +443,8 @@ struct ConvertLayoutOpConversion\n     SmallVector<Type> types(outElems, llvmElemTy);\n     auto *ctx = llvmElemTy.getContext();\n     Type structTy = struct_ty(types);\n-    Value result = getStructFromElements(loc, outVals, rewriter, structTy);\n+    Value result =\n+        getTypeConverter()->packLLElements(loc, outVals, rewriter, dstTy);\n     rewriter.replaceOp(op, result);\n \n     return success();\n@@ -521,10 +521,10 @@ struct ConvertLayoutOpConversion\n       auto thread = getThreadId(rewriter, loc);\n       if (dotOpLayout.getOpIdx() == 0) { // $a\n         res = helper.loadA(src, adaptor.getSrc(), blockedLayout, thread, loc,\n-                           rewriter);\n+                           getTypeConverter(), rewriter);\n       } else { // $b\n         res = helper.loadB(src, adaptor.getSrc(), blockedLayout, thread, loc,\n-                           rewriter);\n+                           getTypeConverter(), rewriter);\n       }\n     } else {\n       assert(false && \"Unsupported dot operand layout found\");\n@@ -547,7 +547,8 @@ struct ConvertLayoutOpConversion\n     auto dstDotLayout = dstLayout.cast<DotOperandEncodingAttr>();\n     if (isMmaToDotShortcut(srcMmaLayout, dstDotLayout)) {\n       // get source values\n-      auto vals = getElementsFromStruct(loc, adaptor.getSrc(), rewriter);\n+      auto vals = getTypeConverter()->unpackLLElements(loc, adaptor.getSrc(),\n+                                                       rewriter, srcTy);\n       unsigned elems = getElemsPerThread(srcTy);\n       Type elemTy =\n           this->getTypeConverter()->convertType(srcTy.getElementType());\n@@ -578,12 +579,8 @@ struct ConvertLayoutOpConversion\n         reorderedVals.push_back(vecVals[i + 3]);\n       }\n \n-      // return composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK);\n-\n-      Type structTy =\n-          LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n-      Value view =\n-          getStructFromElements(loc, reorderedVals, rewriter, structTy);\n+      Value view = getTypeConverter()->packLLElements(loc, reorderedVals,\n+                                                      rewriter, dstTy);\n       rewriter.replaceOp(op, view);\n       return success();\n     }\n@@ -636,12 +633,12 @@ struct ConvertLayoutOpConversion\n         // TODO[Superjomn]: transA is not available here.\n         bool transA = false;\n         res = helper.loadA(src, smemObj, getThreadId(rewriter, loc), loc,\n-                           rewriter);\n+                           getTypeConverter(), rewriter, dst.getType());\n       } else if (dotOperandLayout.getOpIdx() == 1) { // operand $b\n         // TODO[Superjomn]: transB is not available here.\n         bool transB = false;\n         res = helper.loadB(src, smemObj, getThreadId(rewriter, loc), loc,\n-                           rewriter);\n+                           getTypeConverter(), rewriter, dst.getType());\n       }\n     } else {\n       assert(false && \"Unsupported mma layout found\");\n@@ -651,7 +648,7 @@ struct ConvertLayoutOpConversion\n };\n \n void populateConvertLayoutOpToLLVMPatterns(\n-    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n     const Allocation *allocation, Value smem,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -9,7 +9,7 @@ using namespace mlir::triton;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n \n void populateConvertLayoutOpToLLVMPatterns(\n-    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n     const Allocation *allocation, Value smem,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.cpp", "status": "modified", "additions": 29, "deletions": 23, "changes": 52, "file_content_changes": "@@ -1,4 +1,5 @@\n #include \"DotOpHelpers.h\"\n+#include \"TypeConverter.h\"\n \n namespace mlir {\n namespace LLVM {\n@@ -59,7 +60,8 @@ int DotOpMmaV1ConversionHelper::numElemsPerThreadB(ArrayRef<int64_t> shape,\n \n Value DotOpMmaV1ConversionHelper::loadA(\n     Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n-    ConversionPatternRewriter &rewriter) const {\n+    TritonGPUToLLVMTypeConverter *typeConverter,\n+    ConversionPatternRewriter &rewriter, Type resultTy) const {\n   auto *ctx = rewriter.getContext();\n   auto tensorTy = tensor.getType().cast<RankedTensorType>();\n   auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n@@ -165,14 +167,14 @@ Value DotOpMmaV1ConversionHelper::loadA(\n     elems.push_back(item.second.second);\n   }\n \n-  Type resTy = struct_ty(SmallVector<Type>(elems.size(), elemX2Ty));\n-  Value res = getStructFromElements(loc, elems, rewriter, resTy);\n+  Value res = typeConverter->packLLElements(loc, elems, rewriter, resultTy);\n   return res;\n }\n \n Value DotOpMmaV1ConversionHelper::loadB(\n     Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n-    ConversionPatternRewriter &rewriter) const {\n+    TritonGPUToLLVMTypeConverter *typeConverter,\n+    ConversionPatternRewriter &rewriter, Type resultTy) const {\n   // smem\n   auto strides = smemObj.strides;\n \n@@ -279,8 +281,7 @@ Value DotOpMmaV1ConversionHelper::loadB(\n     elems.push_back(item.second.second);\n   }\n \n-  Type resTy = struct_ty(SmallVector<Type>(elems.size(), elemX2Ty));\n-  Value res = getStructFromElements(loc, elems, rewriter, resTy);\n+  Value res = typeConverter->packLLElements(loc, elems, rewriter, resultTy);\n   return res;\n }\n \n@@ -347,10 +348,11 @@ DotOpMmaV1ConversionHelper::computeOffsets(Value threadId, bool isARow,\n \n DotOpMmaV1ConversionHelper::ValueTable\n DotOpMmaV1ConversionHelper::extractLoadedOperand(\n-    Value llStruct, int NK, ConversionPatternRewriter &rewriter) const {\n+    Value llStruct, int NK, ConversionPatternRewriter &rewriter,\n+    TritonGPUToLLVMTypeConverter *typeConverter, Type type) const {\n   ValueTable rcds;\n-  SmallVector<Value> elems =\n-      getElementsFromStruct(llStruct.getLoc(), llStruct, rewriter);\n+  SmallVector<Value> elems = typeConverter->unpackLLElements(\n+      llStruct.getLoc(), llStruct, rewriter, type);\n \n   int offset = 0;\n   for (int i = 0; offset < elems.size(); ++i) {\n@@ -1070,6 +1072,7 @@ LogicalResult MMA16816ConversionHelper::convertDot(Value a, Value b, Value c,\n   helper.deduceMmaType(op);\n \n   auto aTensorTy = a.getType().cast<RankedTensorType>();\n+  auto bTensorTy = b.getType().cast<RankedTensorType>();\n   auto dTensorTy = d.getType().cast<RankedTensorType>();\n \n   SmallVector<int64_t> aShape(aTensorTy.getShape().begin(),\n@@ -1083,10 +1086,10 @@ LogicalResult MMA16816ConversionHelper::convertDot(Value a, Value b, Value c,\n   int numRepK = getNumRepK(aTensorTy, aShape[1]);\n \n   ValueTable ha =\n-      getValuesFromDotOperandLayoutStruct(loadedA, numRepM, numRepK);\n+      getValuesFromDotOperandLayoutStruct(loadedA, numRepM, numRepK, aTensorTy);\n   ValueTable hb = getValuesFromDotOperandLayoutStruct(\n-      loadedB, std::max(numRepN / 2, 1), numRepK);\n-  auto fc = getElementsFromStruct(loc, loadedC, rewriter);\n+      loadedB, std::max(numRepN / 2, 1), numRepK, bTensorTy);\n+  auto fc = typeConverter->unpackLLElements(loc, loadedC, rewriter, dTensorTy);\n \n   auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n     unsigned colsPerThread = numRepN * 2;\n@@ -1132,7 +1135,7 @@ LogicalResult MMA16816ConversionHelper::convertDot(Value a, Value b, Value c,\n   // replace with new packed result\n   Type structTy = LLVM::LLVMStructType::getLiteral(\n       ctx, SmallVector<Type>(fc.size(), resElemTy));\n-  Value res = getStructFromElements(loc, fc, rewriter, structTy);\n+  Value res = typeConverter->packLLElements(loc, fc, rewriter, structTy);\n   rewriter.replaceOp(op, res);\n \n   return success();\n@@ -1211,14 +1214,14 @@ Value MMA16816ConversionHelper::composeValuesToDotOperandLayoutStruct(\n   Type elemTy = elems[0].getType();\n   Type structTy = LLVM::LLVMStructType::getLiteral(\n       ctx, SmallVector<Type>(elems.size(), elemTy));\n-  auto result = getStructFromElements(loc, elems, rewriter, structTy);\n+  auto result = typeConverter->packLLElements(loc, elems, rewriter, structTy);\n   return result;\n }\n MMA16816ConversionHelper::ValueTable\n MMA16816ConversionHelper::getValuesFromDotOperandLayoutStruct(Value value,\n-                                                              int n0,\n-                                                              int n1) const {\n-  auto elems = getElementsFromStruct(loc, value, rewriter);\n+                                                              int n0, int n1,\n+                                                              Type type) const {\n+  auto elems = typeConverter->unpackLLElements(loc, value, rewriter, type);\n \n   int offset{};\n   ValueTable vals;\n@@ -1250,6 +1253,7 @@ SmallVector<Value> DotOpFMAConversionHelper::getThreadIds(\n }\n Value DotOpFMAConversionHelper::loadA(\n     Value A, Value llA, BlockedEncodingAttr dLayout, Value thread, Location loc,\n+    TritonGPUToLLVMTypeConverter *typeConverter,\n     ConversionPatternRewriter &rewriter) const {\n   auto aTensorTy = A.getType().cast<RankedTensorType>();\n   auto aLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n@@ -1309,10 +1313,11 @@ Value DotOpFMAConversionHelper::loadA(\n         vas.emplace_back(va);\n       }\n \n-  return getStructFromValueTable(vas, rewriter, loc, elemTy);\n+  return getStructFromValueTable(vas, rewriter, loc, typeConverter, elemTy);\n }\n Value DotOpFMAConversionHelper::loadB(\n     Value B, Value llB, BlockedEncodingAttr dLayout, Value thread, Location loc,\n+    TritonGPUToLLVMTypeConverter *typeConverter,\n     ConversionPatternRewriter &rewriter) const {\n   auto bTensorTy = B.getType().cast<RankedTensorType>();\n   auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n@@ -1372,14 +1377,15 @@ Value DotOpFMAConversionHelper::loadB(\n         vbs.emplace_back(vb);\n       }\n \n-  return getStructFromValueTable(vbs, rewriter, loc, elemTy);\n+  return getStructFromValueTable(vbs, rewriter, loc, typeConverter, elemTy);\n }\n DotOpFMAConversionHelper::ValueTable\n DotOpFMAConversionHelper::getValueTableFromStruct(\n     Value val, int K, int n0, int shapePerCTA, int sizePerThread,\n-    ConversionPatternRewriter &rewriter, Location loc) const {\n+    ConversionPatternRewriter &rewriter, Location loc,\n+    TritonGPUToLLVMTypeConverter *typeConverter, Type type) const {\n   ValueTable res;\n-  auto elems = getElementsFromStruct(loc, val, rewriter);\n+  auto elems = typeConverter->unpackLLElements(loc, val, rewriter, type);\n   int index = 0;\n   for (unsigned k = 0; k < K; ++k) {\n     for (unsigned m = 0; m < n0; m += shapePerCTA)\n@@ -1391,7 +1397,7 @@ DotOpFMAConversionHelper::getValueTableFromStruct(\n }\n Value DotOpFMAConversionHelper::getStructFromValueTable(\n     ArrayRef<Value> vals, ConversionPatternRewriter &rewriter, Location loc,\n-    Type elemTy) const {\n+    TritonGPUToLLVMTypeConverter *typeConverter, Type elemTy) const {\n   SmallVector<Type> elemTypes(vals.size(), elemTy);\n   SmallVector<Value> elems;\n   elems.reserve(vals.size());\n@@ -1400,7 +1406,7 @@ Value DotOpFMAConversionHelper::getStructFromValueTable(\n   }\n \n   Type structTy = struct_ty(elemTypes);\n-  return getStructFromElements(loc, elems, rewriter, structTy);\n+  return typeConverter->packLLElements(loc, elems, rewriter, structTy);\n }\n int DotOpFMAConversionHelper::getNumElemsPerThread(\n     ArrayRef<int64_t> shape, DotOperandEncodingAttr dotOpLayout) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.h", "status": "modified", "additions": 25, "deletions": 16, "changes": 41, "file_content_changes": "@@ -25,6 +25,8 @@\n \n #include \"Utility.h\"\n \n+class TritonGPUToLLVMTypeConverter;\n+\n namespace mlir {\n namespace LLVM {\n using namespace mlir::triton;\n@@ -123,11 +125,13 @@ struct DotOpMmaV1ConversionHelper {\n \n   // Loading $a from smem to registers, returns a LLVM::Struct.\n   Value loadA(Value tensor, const SharedMemoryObject &smemObj, Value thread,\n-              Location loc, ConversionPatternRewriter &rewriter) const;\n+              Location loc, TritonGPUToLLVMTypeConverter *converter,\n+              ConversionPatternRewriter &rewriter, Type resultTy) const;\n \n   // Loading $b from smem to registers, returns a LLVM::Struct.\n   Value loadB(Value tensor, const SharedMemoryObject &smemObj, Value thread,\n-              Location loc, ConversionPatternRewriter &rewriter) const;\n+              Location loc, TritonGPUToLLVMTypeConverter *converter,\n+              ConversionPatternRewriter &rewriter, Type resultTy) const;\n \n   static ArrayRef<unsigned> getOrder() { return mmaOrder; }\n \n@@ -144,9 +148,9 @@ struct DotOpMmaV1ConversionHelper {\n                  ConversionPatternRewriter &rewriter, Location loc) const;\n \n   // Extract values belong to $a or $b from a LLVMStruct, the shape is n0xn1.\n-  DotOpMmaV1ConversionHelper::ValueTable\n-  extractLoadedOperand(Value llStruct, int NK,\n-                       ConversionPatternRewriter &rewriter) const;\n+  DotOpMmaV1ConversionHelper::ValueTable extractLoadedOperand(\n+      Value llStruct, int NK, ConversionPatternRewriter &rewriter,\n+      TritonGPUToLLVMTypeConverter *typeConverter, Type type) const;\n \n   // Get the number of elements of this thread in M axis. The N axis could be\n   // further deduced with the accSize / elemsM. \\param wpt: the wpt in M axis\n@@ -410,7 +414,7 @@ struct MMA16816ConversionHelper {\n \n   DotOpMmaV2ConversionHelper helper;\n   ConversionPatternRewriter &rewriter;\n-  TypeConverter *typeConverter;\n+  TritonGPUToLLVMTypeConverter *typeConverter;\n   Location loc;\n   MLIRContext *ctx{};\n \n@@ -419,7 +423,8 @@ struct MMA16816ConversionHelper {\n   // dotOperand: type of either one operand of dotOp.\n   MMA16816ConversionHelper(Type dotOperand, MmaEncodingAttr mmaLayout,\n                            Value thread, ConversionPatternRewriter &rewriter,\n-                           TypeConverter *typeConverter, Location loc)\n+                           TritonGPUToLLVMTypeConverter *typeConverter,\n+                           Location loc)\n       : mmaLayout(mmaLayout), wpt(mmaLayout.getWarpsPerCTA()), thread(thread),\n         helper(mmaLayout), rewriter(rewriter), typeConverter(typeConverter),\n         loc(loc), ctx(mmaLayout.getContext()) {\n@@ -560,8 +565,8 @@ struct MMA16816ConversionHelper {\n   Value composeValuesToDotOperandLayoutStruct(const ValueTable &vals, int n0,\n                                               int n1) const;\n \n-  ValueTable getValuesFromDotOperandLayoutStruct(Value value, int n0,\n-                                                 int n1) const;\n+  ValueTable getValuesFromDotOperandLayoutStruct(Value value, int n0, int n1,\n+                                                 Type type) const;\n };\n \n // Helper for conversion of FMA DotOp.\n@@ -580,19 +585,23 @@ struct DotOpFMAConversionHelper {\n                ConversionPatternRewriter &rewriter, Location loc) const;\n \n   Value loadA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n-              Location loc, ConversionPatternRewriter &rewriter) const;\n+              Location loc, TritonGPUToLLVMTypeConverter *typeConverter,\n+              ConversionPatternRewriter &rewriter) const;\n \n   Value loadB(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n-              Location loc, ConversionPatternRewriter &rewriter) const;\n+              Location loc, TritonGPUToLLVMTypeConverter *typeConverter,\n+              ConversionPatternRewriter &rewriter) const;\n \n-  ValueTable getValueTableFromStruct(Value val, int K, int n0, int shapePerCTA,\n-                                     int sizePerThread,\n-                                     ConversionPatternRewriter &rewriter,\n-                                     Location loc) const;\n+  ValueTable getValueTableFromStruct(\n+      Value val, int K, int n0, int shapePerCTA, int sizePerThread,\n+      ConversionPatternRewriter &rewriter, Location loc,\n+      TritonGPUToLLVMTypeConverter *typeConverter, Type type) const;\n \n   Value getStructFromValueTable(ArrayRef<Value> vals,\n                                 ConversionPatternRewriter &rewriter,\n-                                Location loc, Type elemTy) const;\n+                                Location loc,\n+                                TritonGPUToLLVMTypeConverter *typeConverter,\n+                                Type elemTy) const;\n \n   // get number of elements per thread for $a or $b.\n   static int getNumElemsPerThread(ArrayRef<int64_t> shape,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.cpp", "status": "modified", "additions": 17, "deletions": 16, "changes": 33, "file_content_changes": "@@ -7,8 +7,6 @@ using namespace mlir::triton;\n \n using ::mlir::LLVM::DotOpFMAConversionHelper;\n using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n-using ::mlir::LLVM::getElementsFromStruct;\n-using ::mlir::LLVM::getStructFromElements;\n using ::mlir::LLVM::MMA16816ConversionHelper;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n@@ -129,15 +127,17 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n     unsigned numN = helper.getNumN(BShape[1], isBRow, isBVec4_);\n     unsigned NK = AShape[1];\n \n-    auto has = helper.extractLoadedOperand(adaptor.getA(), NK, rewriter);\n-    auto hbs = helper.extractLoadedOperand(adaptor.getB(), NK, rewriter);\n+    auto has = helper.extractLoadedOperand(adaptor.getA(), NK, rewriter,\n+                                           getTypeConverter(), ATensorTy);\n+    auto hbs = helper.extractLoadedOperand(adaptor.getB(), NK, rewriter,\n+                                           getTypeConverter(), BTensorTy);\n \n     // Initialize accumulators with external values, the acc holds the\n     // accumulator value that is shared between the MMA instructions inside a\n     // DotOp, we can call the order of the values the accumulator-internal\n     // order.\n-    SmallVector<Value> acc =\n-        getElementsFromStruct(loc, adaptor.getC(), rewriter);\n+    SmallVector<Value> acc = getTypeConverter()->unpackLLElements(\n+        loc, adaptor.getC(), rewriter, DTensorTy);\n     size_t resSize = acc.size();\n \n     // The resVals holds the final result of the DotOp.\n@@ -209,9 +209,8 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n       resVals[i] = acc[i];\n     }\n \n-    Type structTy = LLVM::LLVMStructType::getLiteral(\n-        ctx, SmallVector<Type>(resSize, type::f32Ty(ctx)));\n-    Value res = getStructFromElements(loc, resVals, rewriter, structTy);\n+    Value res =\n+        getTypeConverter()->packLLElements(loc, resVals, rewriter, DTensorTy);\n     rewriter.replaceOp(op, res);\n     return success();\n   }\n@@ -236,7 +235,8 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n     BlockedEncodingAttr dLayout =\n         dTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n     auto order = dLayout.getOrder();\n-    auto cc = getElementsFromStruct(loc, adaptor.getC(), rewriter);\n+    auto cc = getTypeConverter()->unpackLLElements(loc, adaptor.getC(),\n+                                                   rewriter, dTensorTy);\n \n     DotOpFMAConversionHelper helper(dLayout);\n     Value llA = adaptor.getA();\n@@ -259,9 +259,11 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n         order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n \n     auto has = helper.getValueTableFromStruct(llA, K, M, mShapePerCTA,\n-                                              mSizePerThread, rewriter, loc);\n+                                              mSizePerThread, rewriter, loc,\n+                                              getTypeConverter(), aTensorTy);\n     auto hbs = helper.getValueTableFromStruct(llB, K, N, nShapePerCTA,\n-                                              nSizePerThread, rewriter, loc);\n+                                              nSizePerThread, rewriter, loc,\n+                                              getTypeConverter(), bTensorTy);\n \n     SmallVector<Value> ret = cc;\n     bool isCRow = order[0] == 1;\n@@ -281,16 +283,15 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n             }\n     }\n \n-    auto res = getStructFromElements(\n-        loc, ret, rewriter,\n-        struct_ty(SmallVector<Type>(ret.size(), ret[0].getType())));\n+    auto res =\n+        getTypeConverter()->packLLElements(loc, ret, rewriter, dTensorTy);\n     rewriter.replaceOp(op, res);\n \n     return success();\n   }\n };\n \n-void populateDotOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n+void populateDotOpToLLVMPatterns(TritonGPUToLLVMTypeConverter &typeConverter,\n                                  RewritePatternSet &patterns, int numWarps,\n                                  AxisInfoAnalysis &axisInfoAnalysis,\n                                  const Allocation *allocation, Value smem,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -6,7 +6,7 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n-void populateDotOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n+void populateDotOpToLLVMPatterns(TritonGPUToLLVMTypeConverter &typeConverter,\n                                  RewritePatternSet &patterns, int numWarps,\n                                  AxisInfoAnalysis &axisInfoAnalysis,\n                                  const Allocation *allocation, Value smem,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 21, "deletions": 25, "changes": 46, "file_content_changes": "@@ -2,9 +2,6 @@\n \n using namespace mlir;\n using namespace mlir::triton;\n-\n-using ::mlir::LLVM::getElementsFromStruct;\n-using ::mlir::LLVM::getStructFromElements;\n using ::mlir::triton::gpu::getElemsPerThread;\n \n struct FpToFpOpConversion\n@@ -346,7 +343,8 @@ struct FpToFpOpConversion\n       // Vectorized casting\n       assert(elems % 4 == 0 &&\n              \"FP8 casting only support tensors with 4-aligned sizes\");\n-      auto elements = getElementsFromStruct(loc, adaptor.getFrom(), rewriter);\n+      auto elements = getTypeConverter()->unpackLLElements(\n+          loc, adaptor.getFrom(), rewriter, srcTensorType);\n       for (size_t i = 0; i < elems; i += 4) {\n         auto converted = convertor(loc, rewriter, elements[i], elements[i + 1],\n                                    elements[i + 2], elements[i + 3]);\n@@ -369,10 +367,8 @@ struct FpToFpOpConversion\n     }\n \n     assert(resultVals.size() == elems);\n-    auto convertedDstTensorType =\n-        this->getTypeConverter()->convertType(dstTensorType);\n-    auto result = getStructFromElements(loc, resultVals, rewriter,\n-                                        convertedDstTensorType);\n+    auto result = getTypeConverter()->packLLElements(loc, resultVals, rewriter,\n+                                                     dstTensorType);\n     rewriter.replaceOp(op, result);\n     return success();\n   }\n@@ -384,8 +380,8 @@ class ElementwiseOpConversionBase\n public:\n   using OpAdaptor = typename SourceOp::Adaptor;\n \n-  explicit ElementwiseOpConversionBase(LLVMTypeConverter &typeConverter,\n-                                       PatternBenefit benefit = 1)\n+  explicit ElementwiseOpConversionBase(\n+      TritonGPUToLLVMTypeConverter &typeConverter, PatternBenefit benefit = 1)\n       : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n \n   LogicalResult\n@@ -401,15 +397,16 @@ class ElementwiseOpConversionBase\n     Type structTy = this->getTypeConverter()->convertType(resultTy);\n \n     auto *concreteThis = static_cast<const ConcreteT *>(this);\n-    auto operands = getOperands(rewriter, adaptor, elems, loc);\n+    auto operands = getOperands(rewriter, adaptor, resultTy, elems, loc);\n     SmallVector<Value> resultVals(elems);\n     for (unsigned i = 0; i < elems; ++i) {\n       resultVals[i] = concreteThis->createDestOp(op, adaptor, rewriter, elemTy,\n                                                  operands[i], loc);\n       if (!bool(resultVals[i]))\n         return failure();\n     }\n-    Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n+    Value view = this->getTypeConverter()->packLLElements(loc, resultVals,\n+                                                          rewriter, resultTy);\n     rewriter.replaceOp(op, view);\n \n     return success();\n@@ -418,10 +415,11 @@ class ElementwiseOpConversionBase\n protected:\n   SmallVector<SmallVector<Value>>\n   getOperands(ConversionPatternRewriter &rewriter, OpAdaptor adaptor,\n-              const unsigned elems, Location loc) const {\n+              Type operandTy, const unsigned elems, Location loc) const {\n     SmallVector<SmallVector<Value>> operands(elems);\n     for (auto operand : adaptor.getOperands()) {\n-      auto sub_operands = getElementsFromStruct(loc, operand, rewriter);\n+      auto sub_operands = this->getTypeConverter()->unpackLLElements(\n+          loc, operand, rewriter, operandTy);\n       for (size_t i = 0; i < elems; ++i) {\n         operands[i].push_back(sub_operands[i]);\n       }\n@@ -820,12 +818,10 @@ struct ExpOpConversionApprox\n   }\n };\n \n-void populateElementwiseOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                         RewritePatternSet &patterns,\n-                                         int numWarps,\n-                                         AxisInfoAnalysis &axisInfoAnalysis,\n-                                         const Allocation *allocation,\n-                                         Value smem, PatternBenefit benefit) {\n+void populateElementwiseOpToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem, PatternBenefit benefit) {\n #define POPULATE_TERNARY_OP(SRC_OP, DST_OP)                                    \\\n   patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n   POPULATE_TERNARY_OP(triton::gpu::SelectOp, LLVM::SelectOp)\n@@ -882,8 +878,8 @@ void populateElementwiseOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<FpToFpOpConversion>(typeConverter, benefit);\n \n   patterns.add<ExtElemwiseOpConversion>(typeConverter, benefit);\n-  // ExpOpConversionApprox will try using ex2.approx if the input type is FP32.\n-  // For FP64 input type, ExpOpConversionApprox will return failure and\n+  // ExpOpConversionApprox will try using ex2.approx if the input type is\n+  // FP32. For FP64 input type, ExpOpConversionApprox will return failure and\n   // ElementwiseOpConversion<math::ExpOp, math::ExpOp> defined below will call\n   // __nv_expf for higher-precision calculation\n   patterns.add<ExpOpConversionApprox>(typeConverter, benefit);\n@@ -1030,9 +1026,9 @@ bool isLegalElementwiseOp(Operation *op) {\n   return true;\n }\n \n-void populateElementwiseOpToPTXPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                        RewritePatternSet &patterns,\n-                                        PatternBenefit benefit) {\n+void populateElementwiseOpToPTXPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    PatternBenefit benefit) {\n   patterns.add<FPExtOpConversion>(typeConverter, benefit);\n   patterns.add<FPTruncOpConversion>(typeConverter, benefit);\n   patterns.add<TruncOpConversion>(typeConverter, benefit);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.h", "status": "modified", "additions": 7, "deletions": 9, "changes": 16, "file_content_changes": "@@ -6,17 +6,15 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n-void populateElementwiseOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                         RewritePatternSet &patterns,\n-                                         int numWarps,\n-                                         AxisInfoAnalysis &axisInfoAnalysis,\n-                                         const Allocation *allocation,\n-                                         Value smem, PatternBenefit benefit);\n+void populateElementwiseOpToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n+    const Allocation *allocation, Value smem, PatternBenefit benefit);\n \n bool isLegalElementwiseOp(Operation *op);\n \n-void populateElementwiseOpToPTXPatterns(mlir::LLVMTypeConverter &typeConverter,\n-                                        RewritePatternSet &patterns,\n-                                        PatternBenefit benefit);\n+void populateElementwiseOpToPTXPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    PatternBenefit benefit);\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 47, "deletions": 41, "changes": 88, "file_content_changes": "@@ -7,9 +7,7 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n-using ::mlir::LLVM::getElementsFromStruct;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n-using ::mlir::LLVM::getStructFromElements;\n using ::mlir::triton::gpu::getElemsPerThread;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n@@ -18,19 +16,6 @@ struct LoadStoreConversionBase {\n   explicit LoadStoreConversionBase(AxisInfoAnalysis &axisAnalysisPass)\n       : axisAnalysisPass(axisAnalysisPass) {}\n \n-  // Get corresponding LLVM element values of \\param value.\n-  static SmallVector<Value> getLLVMElems(Value value, Value llValue,\n-                                         ConversionPatternRewriter &rewriter,\n-                                         Location loc) {\n-    if (!value)\n-      return {};\n-    if (!llValue.getType().isa<LLVM::LLVMStructType>())\n-      return {llValue};\n-    // Here, we assume that all inputs should have a blockedLayout\n-    auto valueVals = getElementsFromStruct(loc, llValue, rewriter);\n-    return valueVals;\n-  }\n-\n   unsigned getContiguity(Value ptr) const {\n     auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n     if (!tensorTy)\n@@ -62,7 +47,7 @@ struct LoadOpConversion\n   using ConvertTritonGPUOpToLLVMPattern<\n       triton::LoadOp>::ConvertTritonGPUOpToLLVMPattern;\n \n-  LoadOpConversion(LLVMTypeConverter &converter,\n+  LoadOpConversion(TritonGPUToLLVMTypeConverter &converter,\n                    AxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n       : ConvertTritonGPUOpToLLVMPattern<triton::LoadOp>(converter, benefit),\n         LoadStoreConversionBase(axisAnalysisPass) {}\n@@ -92,13 +77,15 @@ struct LoadOpConversion\n       vec = std::min<size_t>(vec, getMaskAlignment(mask));\n \n     // Get the LLVM values for pointers\n-    auto ptrElems = getLLVMElems(ptr, llPtr, rewriter, loc);\n+    auto ptrElems = getTypeConverter()->unpackLLElements(loc, llPtr, rewriter,\n+                                                         ptr.getType());\n     assert(ptrElems.size() == numElems);\n \n     // Get the LLVM values for mask\n     SmallVector<Value> maskElems;\n     if (llMask) {\n-      maskElems = getLLVMElems(mask, llMask, rewriter, loc);\n+      maskElems = getTypeConverter()->unpackLLElements(loc, llMask, rewriter,\n+                                                       mask.getType());\n       assert(maskElems.size() == numElems);\n     }\n \n@@ -114,7 +101,11 @@ struct LoadOpConversion\n       otherIsSplatConstInt = true;\n       splatVal = constAttr.getSplatValue<APInt>().getSExtValue();\n     }\n-    auto otherElems = getLLVMElems(other, llOther, rewriter, loc);\n+    SmallVector<Value> otherElems;\n+    if (other) {\n+      otherElems = getTypeConverter()->unpackLLElements(loc, llOther, rewriter,\n+                                                        other.getType());\n+    }\n \n     // vectorized iteration through all the pointer/mask/other elements\n     const int valueElemNbits =\n@@ -245,8 +236,8 @@ struct LoadOpConversion\n     } // end vec\n \n     Type llvmResultStructTy = getTypeConverter()->convertType(valueTy);\n-    Value resultStruct =\n-        getStructFromElements(loc, loadedVals, rewriter, llvmResultStructTy);\n+    Value resultStruct = getTypeConverter()->packLLElements(\n+        loc, loadedVals, rewriter, llvmResultStructTy);\n     rewriter.replaceOp(op, {resultStruct});\n     return success();\n   }\n@@ -258,7 +249,7 @@ struct StoreOpConversion\n   using ConvertTritonGPUOpToLLVMPattern<\n       triton::StoreOp>::ConvertTritonGPUOpToLLVMPattern;\n \n-  StoreOpConversion(LLVMTypeConverter &converter,\n+  StoreOpConversion(TritonGPUToLLVMTypeConverter &converter,\n                     AxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n       : ConvertTritonGPUOpToLLVMPattern<triton::StoreOp>(converter, benefit),\n         LoadStoreConversionBase(axisAnalysisPass) {}\n@@ -284,14 +275,17 @@ struct StoreOpConversion\n     unsigned vec = getVectorSize(ptr);\n     unsigned numElems = getElemsPerThread(ptr.getType());\n \n-    auto ptrElems = getLLVMElems(ptr, llPtr, rewriter, loc);\n-    auto valueElems = getLLVMElems(value, llValue, rewriter, loc);\n+    auto ptrElems = getTypeConverter()->unpackLLElements(loc, llPtr, rewriter,\n+                                                         ptr.getType());\n+    auto valueElems = getTypeConverter()->unpackLLElements(\n+        loc, llValue, rewriter, value.getType());\n     assert(ptrElems.size() == valueElems.size());\n \n     // Determine the vectorization size\n     SmallVector<Value> maskElems;\n     if (llMask) {\n-      maskElems = getLLVMElems(mask, llMask, rewriter, loc);\n+      maskElems = getTypeConverter()->unpackLLElements(loc, llMask, rewriter,\n+                                                       mask.getType());\n       assert(valueElems.size() == maskElems.size());\n \n       unsigned maskAlign = getMaskAlignment(mask);\n@@ -373,7 +367,7 @@ struct AtomicCASOpConversion\n   using ConvertTritonGPUOpToLLVMPattern<\n       triton::AtomicCASOp>::ConvertTritonGPUOpToLLVMPattern;\n \n-  AtomicCASOpConversion(LLVMTypeConverter &converter,\n+  AtomicCASOpConversion(TritonGPUToLLVMTypeConverter &converter,\n                         const Allocation *allocation, Value smem,\n                         AxisInfoAnalysis &axisAnalysisPass,\n                         PatternBenefit benefit)\n@@ -391,9 +385,12 @@ struct AtomicCASOpConversion\n     Value llCmp = adaptor.getCmp();\n     Value llVal = adaptor.getVal();\n \n-    auto ptrElements = getElementsFromStruct(loc, llPtr, rewriter);\n-    auto cmpElements = getElementsFromStruct(loc, llCmp, rewriter);\n-    auto valElements = getElementsFromStruct(loc, llVal, rewriter);\n+    auto ptrElements = getTypeConverter()->unpackLLElements(\n+        loc, llPtr, rewriter, op.getPtr().getType());\n+    auto cmpElements = getTypeConverter()->unpackLLElements(\n+        loc, llCmp, rewriter, op.getCmp().getType());\n+    auto valElements = getTypeConverter()->unpackLLElements(\n+        loc, llVal, rewriter, op.getVal().getType());\n \n     auto valueTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n     Type valueElemTy =\n@@ -447,7 +444,7 @@ struct AtomicRMWOpConversion\n   using ConvertTritonGPUOpToLLVMPattern<\n       triton::AtomicRMWOp>::ConvertTritonGPUOpToLLVMPattern;\n \n-  AtomicRMWOpConversion(LLVMTypeConverter &converter,\n+  AtomicRMWOpConversion(TritonGPUToLLVMTypeConverter &converter,\n                         const Allocation *allocation, Value smem,\n                         AxisInfoAnalysis &axisAnalysisPass,\n                         PatternBenefit benefit)\n@@ -462,16 +459,21 @@ struct AtomicRMWOpConversion\n     MLIRContext *ctx = rewriter.getContext();\n \n     auto atomicRmwAttr = op.getAtomicRmwOp();\n-    Value ptr = op.getPtr();\n+\n     Value val = op.getVal();\n+    Value ptr = op.getPtr();\n+    Value _mask = op.getMask();\n \n     Value llPtr = adaptor.getPtr();\n     Value llVal = adaptor.getVal();\n     Value llMask = adaptor.getMask();\n \n-    auto valElements = getElementsFromStruct(loc, llVal, rewriter);\n-    auto ptrElements = getElementsFromStruct(loc, llPtr, rewriter);\n-    auto maskElements = getElementsFromStruct(loc, llMask, rewriter);\n+    auto valElements = getTypeConverter()->unpackLLElements(\n+        loc, llVal, rewriter, val.getType());\n+    auto ptrElements = getTypeConverter()->unpackLLElements(\n+        loc, llPtr, rewriter, ptr.getType());\n+    auto maskElements = getTypeConverter()->unpackLLElements(\n+        loc, llMask, rewriter, _mask.getType());\n \n     auto valueTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n     Type valueElemTy =\n@@ -587,8 +589,8 @@ struct AtomicRMWOpConversion\n     }\n     if (valueTy) {\n       Type structTy = getTypeConverter()->convertType(valueTy);\n-      Value resultStruct =\n-          getStructFromElements(loc, resultVals, rewriter, structTy);\n+      Value resultStruct = getTypeConverter()->packLLElements(\n+          loc, resultVals, rewriter, structTy);\n       rewriter.replaceOp(op, {resultStruct});\n     }\n     return success();\n@@ -668,7 +670,8 @@ struct InsertSliceAsyncOpConversion\n       triton::gpu::InsertSliceAsyncOp>::ConvertTritonGPUOpToLLVMPattern;\n \n   InsertSliceAsyncOpConversion(\n-      LLVMTypeConverter &converter, const Allocation *allocation, Value smem,\n+      TritonGPUToLLVMTypeConverter &converter, const Allocation *allocation,\n+      Value smem,\n       ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n       AxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n       : ConvertTritonGPUOpToLLVMPattern<triton::gpu::InsertSliceAsyncOp>(\n@@ -704,7 +707,8 @@ struct InsertSliceAsyncOpConversion\n     Value llIndex = adaptor.getIndex();\n \n     // %src\n-    auto srcElems = getLLVMElems(src, llSrc, rewriter, loc);\n+    auto srcElems = getTypeConverter()->unpackLLElements(loc, llSrc, rewriter,\n+                                                         src.getType());\n \n     // %dst\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n@@ -730,7 +734,8 @@ struct InsertSliceAsyncOpConversion\n     // %mask\n     SmallVector<Value> maskElems;\n     if (llMask) {\n-      maskElems = getLLVMElems(mask, llMask, rewriter, loc);\n+      maskElems = getTypeConverter()->unpackLLElements(loc, llMask, rewriter,\n+                                                       mask.getType());\n       assert(srcElems.size() == maskElems.size());\n     }\n \n@@ -741,7 +746,8 @@ struct InsertSliceAsyncOpConversion\n       // It's not necessary for now because the pipeline pass will skip\n       // generating insert_slice_async if the load op has any \"other\" tensor.\n       // assert(false && \"insert_slice_async: Other value not supported yet\");\n-      otherElems = getLLVMElems(other, llOther, rewriter, loc);\n+      otherElems = getTypeConverter()->unpackLLElements(loc, llOther, rewriter,\n+                                                        other.getType());\n       assert(srcElems.size() == otherElems.size());\n     }\n \n@@ -821,7 +827,7 @@ struct InsertSliceAsyncOpConversion\n };\n \n void populateLoadStoreOpToLLVMPatterns(\n-    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n     const Allocation *allocation, Value smem,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -7,7 +7,7 @@ using namespace mlir;\n using namespace mlir::triton;\n \n void populateLoadStoreOpToLLVMPatterns(\n-    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n     const Allocation *allocation, Value smem,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 9, "deletions": 17, "changes": 26, "file_content_changes": "@@ -3,8 +3,6 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n-using ::mlir::LLVM::getElementsFromStruct;\n-using ::mlir::LLVM::getStructFromElements;\n using ::mlir::LLVM::shflSync;\n using ::mlir::LLVM::storeShared;\n using ::mlir::triton::gpu::getElemsPerThread;\n@@ -161,7 +159,8 @@ struct ReduceOpConversion\n \n     unsigned srcElems = getElemsPerThread(srcTy);\n     auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n-    auto srcValues = getElementsFromStruct(loc, adaptor.getOperand(), rewriter);\n+    auto srcValues = getTypeConverter()->unpackLLElements(\n+        loc, adaptor.getOperand(), rewriter, srcTy);\n \n     SmallVector<SmallVector<unsigned>> offset =\n         emitOffsetForLayout(srcLayout, srcShape);\n@@ -260,12 +259,8 @@ struct ReduceOpConversion\n         Value indexReadPtr = gep(indexPtrTy, indexSmemBase, readOffset);\n         resultVals[i] = withIndex ? load(indexReadPtr) : load(readPtr);\n       }\n-\n-      SmallVector<Type> resultTypes(resultElems,\n-                                    withIndex ? llvmIndexTy : llvmElemTy);\n-      Type structTy =\n-          LLVM::LLVMStructType::getLiteral(this->getContext(), resultTypes);\n-      Value ret = getStructFromElements(loc, resultVals, rewriter, structTy);\n+      Value ret = getTypeConverter()->packLLElements(loc, resultVals, rewriter,\n+                                                     resultTy);\n       rewriter.replaceOp(op, ret);\n     } else {\n       // 0d-tensor -> scalar\n@@ -311,7 +306,8 @@ struct ReduceOpConversion\n \n     unsigned srcElems = getElemsPerThread(srcTy);\n     auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n-    auto srcValues = getElementsFromStruct(loc, adaptor.getOperand(), rewriter);\n+    auto srcValues = getTypeConverter()->unpackLLElements(\n+        loc, adaptor.getOperand(), rewriter, srcTy);\n \n     SmallVector<SmallVector<unsigned>> offset =\n         emitOffsetForLayout(srcLayout, srcShape);\n@@ -462,12 +458,8 @@ struct ReduceOpConversion\n         Value indexReadPtr = gep(indexPtrTy, indexSmemBase, readOffset);\n         resultVals[i] = withIndex ? load(indexReadPtr) : load(readPtr);\n       }\n-\n-      SmallVector<Type> resultTypes(resultElems,\n-                                    withIndex ? llvmIndexTy : llvmElemTy);\n-      Type structTy =\n-          LLVM::LLVMStructType::getLiteral(this->getContext(), resultTypes);\n-      Value ret = getStructFromElements(loc, resultVals, rewriter, structTy);\n+      Value ret = getTypeConverter()->packLLElements(loc, resultVals, rewriter,\n+                                                     resultTy);\n       rewriter.replaceOp(op, ret);\n     } else {\n       // 0d-tensor -> scalar\n@@ -480,7 +472,7 @@ struct ReduceOpConversion\n };\n \n void populateReduceOpToLLVMPatterns(\n-    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n     const Allocation *allocation, Value smem,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -7,7 +7,7 @@ using namespace mlir;\n using namespace mlir::triton;\n \n void populateReduceOpToLLVMPatterns(\n-    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n     const Allocation *allocation, Value smem,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 20, "deletions": 19, "changes": 39, "file_content_changes": "@@ -6,9 +6,7 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n-using ::mlir::LLVM::getElementsFromStruct;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n-using ::mlir::LLVM::getStructFromElements;\n using ::mlir::triton::gpu::getElemsPerThread;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n@@ -69,7 +67,8 @@ struct BroadcastOpConversion\n     auto order = triton::gpu::getOrder(srcLayout);\n     auto srcOffsets = emitOffsetForLayout(srcLayout, srcShape);\n     auto resultOffsets = emitOffsetForLayout(resultLayout, resultShape);\n-    SmallVector<Value> srcVals = getElementsFromStruct(loc, src, rewriter);\n+    SmallVector<Value> srcVals =\n+        getTypeConverter()->unpackLLElements(loc, src, rewriter, srcTy);\n \n     DenseMap<SmallVector<unsigned>, Value, SmallVectorKeyInfo> srcValues;\n     for (size_t i = 0; i < srcOffsets.size(); i++) {\n@@ -85,10 +84,8 @@ struct BroadcastOpConversion\n       resultVals.push_back(srcValues.lookup(offset));\n     }\n \n-    auto llvmStructTy = getTypeConverter()->convertType(resultTy);\n-\n     Value resultStruct =\n-        getStructFromElements(loc, resultVals, rewriter, llvmStructTy);\n+        getTypeConverter()->packLLElements(loc, resultVals, rewriter, resultTy);\n     rewriter.replaceOp(op, {resultStruct});\n     return success();\n   }\n@@ -104,8 +101,9 @@ struct PrintOpConversion\n                   ConversionPatternRewriter &rewriter) const override {\n     auto loc = op->getLoc();\n     SmallVector<Value, 16> operands;\n-    for (auto operand : adaptor.getOperands()) {\n-      auto sub_operands = getElementsFromStruct(loc, operand, rewriter);\n+    for (size_t i = 0; i < op.getNumOperands(); i++) {\n+      auto sub_operands = getTypeConverter()->unpackLLElements(\n+          loc, adaptor.getOperands()[i], rewriter, op.getOperand(i).getType());\n       for (auto elem : sub_operands) {\n         operands.push_back(elem);\n       }\n@@ -251,7 +249,8 @@ struct AssertOpConversion\n                   ConversionPatternRewriter &rewriter) const override {\n     auto loc = op.getLoc();\n     auto ctx = rewriter.getContext();\n-    auto elems = getElementsFromStruct(loc, adaptor.getCondition(), rewriter);\n+    auto elems = getTypeConverter()->unpackLLElements(\n+        loc, adaptor.getCondition(), rewriter, op.getCondition().getType());\n     auto elemTy = elems[0].getType();\n     Value condition = int_val(elemTy.getIntOrFloatBitWidth(), 0);\n     for (auto elem : elems) {\n@@ -343,7 +342,7 @@ struct MakeRangeOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::MakeRangeOp> {\n \n   MakeRangeOpConversion(\n-      LLVMTypeConverter &converter,\n+      TritonGPUToLLVMTypeConverter &converter,\n       ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n       PatternBenefit benefit)\n       : ConvertTritonGPUOpToLLVMPattern<triton::MakeRangeOp>(\n@@ -371,9 +370,8 @@ struct MakeRangeOpConversion\n       assert(multiDim.value().size() == 1);\n       retVals[multiDim.index()] = add(multiDim.value()[0], start);\n     }\n-    SmallVector<Type> types(elems, elemTy);\n-    Type structTy = LLVM::LLVMStructType::getLiteral(getContext(), types);\n-    Value result = getStructFromElements(loc, retVals, rewriter, structTy);\n+    Value result =\n+        getTypeConverter()->packLLElements(loc, retVals, rewriter, rankedTy);\n     rewriter.replaceOp(op, result);\n     return success();\n   }\n@@ -434,20 +432,23 @@ struct AddPtrOpConversion\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n     auto resultTy = op.getType();\n+    auto offsetTy = op.getOffset().getType();\n+    auto ptrTy = op.getPtr().getType();\n     auto resultTensorTy = resultTy.dyn_cast<RankedTensorType>();\n     if (resultTensorTy) {\n       unsigned elems = getElemsPerThread(resultTy);\n       Type elemTy =\n           getTypeConverter()->convertType(resultTensorTy.getElementType());\n-      SmallVector<Type> types(elems, elemTy);\n-      Type structTy = LLVM::LLVMStructType::getLiteral(getContext(), types);\n-      auto ptrs = getElementsFromStruct(loc, adaptor.getPtr(), rewriter);\n-      auto offsets = getElementsFromStruct(loc, adaptor.getOffset(), rewriter);\n+      auto ptrs = getTypeConverter()->unpackLLElements(loc, adaptor.getPtr(),\n+                                                       rewriter, ptrTy);\n+      auto offsets = getTypeConverter()->unpackLLElements(\n+          loc, adaptor.getOffset(), rewriter, offsetTy);\n       SmallVector<Value> resultVals(elems);\n       for (unsigned i = 0; i < elems; ++i) {\n         resultVals[i] = gep(elemTy, ptrs[i], offsets[i]);\n       }\n-      Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n+      Value view = getTypeConverter()->packLLElements(loc, resultVals, rewriter,\n+                                                      resultTy);\n       rewriter.replaceOp(op, view);\n     } else {\n       assert(resultTy.isa<triton::PointerType>());\n@@ -612,7 +613,7 @@ void vprintf_array(Value thread, ArrayRef<Value> arr, std::string info,\n } // namespace mlir\n \n void populateTritonGPUToLLVMPatterns(\n-    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n     const Allocation *allocation, Value smem,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -7,7 +7,7 @@ using namespace mlir;\n using namespace mlir::triton;\n \n void populateTritonGPUToLLVMPatterns(\n-    mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n     const Allocation *allocation, Value smem,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 32, "deletions": 25, "changes": 57, "file_content_changes": "@@ -7,6 +7,7 @@\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"triton/Analysis/Allocation.h\"\n \n+#include \"TypeConverter.h\"\n //\n #include \"DotOpHelpers.h\"\n #include \"Utility.h\"\n@@ -178,22 +179,22 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     OpBuilder::InsertPoint *indexInsertPoint;\n   };\n \n-  explicit ConvertTritonGPUOpToLLVMPatternBase(LLVMTypeConverter &typeConverter)\n+  explicit ConvertTritonGPUOpToLLVMPatternBase(\n+      TritonGPUToLLVMTypeConverter &typeConverter)\n       : converter(&typeConverter) {}\n \n-  explicit ConvertTritonGPUOpToLLVMPatternBase(LLVMTypeConverter &typeConverter,\n-                                               const Allocation *allocation,\n-                                               Value smem)\n+  explicit ConvertTritonGPUOpToLLVMPatternBase(\n+      TritonGPUToLLVMTypeConverter &typeConverter, const Allocation *allocation,\n+      Value smem)\n       : converter(&typeConverter), allocation(allocation), smem(smem) {}\n \n-  explicit ConvertTritonGPUOpToLLVMPatternBase(LLVMTypeConverter &typeConverter,\n-                                               const Allocation *allocation,\n-                                               Value smem,\n-                                               IndexCacheInfo indexCacheInfo)\n+  explicit ConvertTritonGPUOpToLLVMPatternBase(\n+      TritonGPUToLLVMTypeConverter &typeConverter, const Allocation *allocation,\n+      Value smem, IndexCacheInfo indexCacheInfo)\n       : converter(&typeConverter), allocation(allocation), smem(smem),\n         indexCacheInfo(indexCacheInfo) {}\n \n-  LLVMTypeConverter *getTypeConverter() const { return converter; }\n+  TritonGPUToLLVMTypeConverter *getTypeConverter() const { return converter; }\n \n   static Value\n   getStructFromSharedMemoryObject(Location loc,\n@@ -203,7 +204,13 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto types = smemObj.getTypes();\n     auto structTy =\n         LLVM::LLVMStructType::getLiteral(rewriter.getContext(), types);\n-    return getStructFromElements(loc, elems, rewriter, structTy);\n+    // pack into struct\n+    Value llvmStruct = rewriter.create<LLVM::UndefOp>(loc, structTy);\n+    for (const auto &v : llvm::enumerate(elems)) {\n+      assert(v.value() && \"can not insert null values\");\n+      llvmStruct = insert_val(structTy, llvmStruct, v.value(), v.index());\n+    }\n+    return llvmStruct;\n   }\n \n   Value getThreadId(ConversionPatternRewriter &rewriter, Location loc) const {\n@@ -371,7 +378,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     unsigned maxPhase = dstSharedLayout.getMaxPhase();\n     unsigned numElems = triton::gpu::getElemsPerThread(srcTy);\n     assert(numElems == srcIndices.size());\n-    auto inVals = LLVM::getElementsFromStruct(loc, llSrc, rewriter);\n+    auto inVals =\n+        getTypeConverter()->unpackLLElements(loc, llSrc, rewriter, srcTy);\n     auto wordTy = vec_ty(elemTy, minVec);\n     auto elemPtrTy = ptr_ty(elemTy);\n     Value outVecVal = i32_val(outVec);\n@@ -868,7 +876,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   }\n \n protected:\n-  LLVMTypeConverter *converter;\n+  TritonGPUToLLVMTypeConverter *converter;\n   const Allocation *allocation;\n   Value smem;\n   IndexCacheInfo indexCacheInfo;\n@@ -881,30 +889,29 @@ class ConvertTritonGPUOpToLLVMPattern\n public:\n   using OpAdaptor = typename SourceOp::Adaptor;\n \n-  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n-                                           PatternBenefit benefit = 1)\n+  explicit ConvertTritonGPUOpToLLVMPattern(\n+      TritonGPUToLLVMTypeConverter &typeConverter, PatternBenefit benefit = 1)\n       : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n         ConvertTritonGPUOpToLLVMPatternBase(typeConverter) {}\n \n-  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n-                                           const Allocation *allocation,\n-                                           Value smem,\n-                                           PatternBenefit benefit = 1)\n+  explicit ConvertTritonGPUOpToLLVMPattern(\n+      TritonGPUToLLVMTypeConverter &typeConverter, const Allocation *allocation,\n+      Value smem, PatternBenefit benefit = 1)\n       : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n         ConvertTritonGPUOpToLLVMPatternBase(typeConverter, allocation, smem) {}\n \n-  explicit ConvertTritonGPUOpToLLVMPattern(LLVMTypeConverter &typeConverter,\n-                                           const Allocation *allocation,\n-                                           Value smem,\n-                                           IndexCacheInfo indexCacheInfo,\n-                                           PatternBenefit benefit = 1)\n+  explicit ConvertTritonGPUOpToLLVMPattern(\n+      TritonGPUToLLVMTypeConverter &typeConverter, const Allocation *allocation,\n+      Value smem, IndexCacheInfo indexCacheInfo, PatternBenefit benefit = 1)\n       : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n         ConvertTritonGPUOpToLLVMPatternBase(typeConverter, allocation, smem,\n                                             indexCacheInfo) {}\n \n protected:\n-  LLVMTypeConverter *getTypeConverter() const {\n-    return ((ConvertTritonGPUOpToLLVMPatternBase *)this)->getTypeConverter();\n+  TritonGPUToLLVMTypeConverter *getTypeConverter() const {\n+    LLVMTypeConverter *ret =\n+        ((ConvertTritonGPUOpToLLVMPatternBase *)this)->getTypeConverter();\n+    return (TritonGPUToLLVMTypeConverter *)ret;\n   }\n };\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.cpp", "status": "added", "additions": 182, "deletions": 0, "changes": 182, "file_content_changes": "@@ -0,0 +1,182 @@\n+#include \"TypeConverter.h\"\n+#include \"DotOpHelpers.h\"\n+#include \"Utility.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"triton/Conversion/MLIRTypes.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::LLVM::DotOpFMAConversionHelper;\n+using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n+using ::mlir::LLVM::MMA16816ConversionHelper;\n+using ::mlir::triton::gpu::BlockedEncodingAttr;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::getElemsPerThread;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n+using ::mlir::triton::gpu::SliceEncodingAttr;\n+\n+TritonGPUToLLVMTypeConverter::TritonGPUToLLVMTypeConverter(\n+    MLIRContext *ctx, LowerToLLVMOptions &option,\n+    const DataLayoutAnalysis *analysis)\n+    : LLVMTypeConverter(ctx, option, analysis) {\n+  addConversion([&](triton::PointerType type) -> llvm::Optional<Type> {\n+    return convertTritonPointerType(type);\n+  });\n+  addConversion([&](RankedTensorType type) -> llvm::Optional<Type> {\n+    return convertTritonTensorType(type);\n+  });\n+  // Internally store float8 as int8\n+  addConversion([&](triton::Float8Type type) -> llvm::Optional<Type> {\n+    return IntegerType::get(type.getContext(), 8);\n+  });\n+  // Internally store bfloat16 as int16\n+  addConversion([&](BFloat16Type type) -> llvm::Optional<Type> {\n+    return IntegerType::get(type.getContext(), 16);\n+  });\n+}\n+\n+Type TritonGPUToLLVMTypeConverter::convertTritonPointerType(\n+    triton::PointerType type) {\n+  // Recursively translate pointee type\n+  return LLVM::LLVMPointerType::get(convertType(type.getPointeeType()),\n+                                    type.getAddressSpace());\n+}\n+\n+Value TritonGPUToLLVMTypeConverter::packLLElements(\n+    Location loc, ValueRange resultVals, ConversionPatternRewriter &rewriter,\n+    Type type) {\n+  auto structType = this->convertType(type);\n+  if (!structType.isa<LLVM::LLVMStructType>()) {\n+    return *resultVals.begin();\n+  }\n+\n+  Value llvmStruct = rewriter.create<LLVM::UndefOp>(loc, structType);\n+  // llvm::outs() << structType << \"\\n\";\n+  for (const auto &v : llvm::enumerate(resultVals)) {\n+    assert(v.value() && \"can not insert null values\");\n+    llvmStruct = insert_val(structType, llvmStruct, v.value(), v.index());\n+  }\n+  return llvmStruct;\n+}\n+\n+SmallVector<Value> TritonGPUToLLVMTypeConverter::unpackLLElements(\n+    Location loc, Value llvmStruct, ConversionPatternRewriter &rewriter,\n+    Type type) {\n+  assert(bool(llvmStruct) && \"can not unpack null values\");\n+  if (llvmStruct.getType().isIntOrIndexOrFloat() ||\n+      llvmStruct.getType().isa<triton::PointerType>() ||\n+      llvmStruct.getType().isa<LLVM::LLVMPointerType>())\n+    return {llvmStruct};\n+  ArrayRef<Type> types =\n+      llvmStruct.getType().cast<LLVM::LLVMStructType>().getBody();\n+  SmallVector<Value> results(types.size());\n+  for (unsigned i = 0; i < types.size(); ++i) {\n+    Type type = types[i];\n+    results[i] = extract_val(type, llvmStruct, i);\n+  }\n+  return results;\n+}\n+\n+llvm::Optional<Type>\n+TritonGPUToLLVMTypeConverter::convertTritonTensorType(RankedTensorType type) {\n+  auto ctx = type.getContext();\n+  Attribute layout = type.getEncoding();\n+  SmallVector<int64_t> shape(type.getShape().begin(), type.getShape().end());\n+\n+  if (layout &&\n+      (layout.isa<BlockedEncodingAttr>() || layout.isa<SliceEncodingAttr>() ||\n+       layout.isa<MmaEncodingAttr>())) {\n+    unsigned numElementsPerThread = getElemsPerThread(type);\n+    SmallVector<Type, 4> types(numElementsPerThread,\n+                               convertType(type.getElementType()));\n+    return LLVM::LLVMStructType::getLiteral(ctx, types);\n+  } else if (auto shared_layout =\n+                 layout.dyn_cast_or_null<SharedEncodingAttr>()) {\n+    SmallVector<Type, 4> types;\n+    // base ptr\n+    auto ptrType =\n+        LLVM::LLVMPointerType::get(convertType(type.getElementType()), 3);\n+    types.push_back(ptrType);\n+    // shape dims\n+    auto rank = type.getRank();\n+    // offsets + strides\n+    for (auto i = 0; i < rank * 2; i++) {\n+      types.push_back(IntegerType::get(ctx, 32));\n+    }\n+    return LLVM::LLVMStructType::getLiteral(ctx, types);\n+  } else if (auto dotOpLayout =\n+                 layout.dyn_cast_or_null<DotOperandEncodingAttr>()) {\n+    if (dotOpLayout.getParent()\n+            .isa<BlockedEncodingAttr>()) { // for parent is blocked layout\n+      int numElemsPerThread =\n+          DotOpFMAConversionHelper::getNumElemsPerThread(shape, dotOpLayout);\n+\n+      return LLVM::LLVMStructType::getLiteral(\n+          ctx, SmallVector<Type>(numElemsPerThread, type::f32Ty(ctx)));\n+    } else { // for parent is MMA layout\n+      auto mmaLayout = dotOpLayout.getParent().cast<MmaEncodingAttr>();\n+      auto wpt = mmaLayout.getWarpsPerCTA();\n+      Type elemTy = convertType(type.getElementType());\n+      if (mmaLayout.isAmpere()) {\n+        const llvm::DenseMap<int, Type> targetTyMap = {\n+            {32, vec_ty(elemTy, 1)},\n+            {16, vec_ty(elemTy, 2)},\n+            {8, vec_ty(elemTy, 4)},\n+        };\n+        Type targetTy;\n+        if (targetTyMap.count(elemTy.getIntOrFloatBitWidth())) {\n+          targetTy = targetTyMap.lookup(elemTy.getIntOrFloatBitWidth());\n+          // <2xi16>/<4xi8> => i32\n+          // We are doing this because NVPTX inserts extra integer instrs to\n+          // pack & unpack vectors of sub-word integers\n+          // Note: this needs to be synced with\n+          //       DotOpMmaV2ConversionHelper::loadX4\n+          if (elemTy.isa<IntegerType>() &&\n+              (elemTy.getIntOrFloatBitWidth() == 8 ||\n+               elemTy.getIntOrFloatBitWidth() == 16))\n+            targetTy = IntegerType::get(ctx, 32);\n+        } else {\n+          assert(false && \"Unsupported element type\");\n+        }\n+        if (dotOpLayout.getOpIdx() == 0) { // $a\n+          auto elems =\n+              MMA16816ConversionHelper::getANumElemsPerThread(type, wpt[0]);\n+          return struct_ty(SmallVector<Type>(elems, targetTy));\n+        }\n+        if (dotOpLayout.getOpIdx() == 1) { // $b\n+          auto elems =\n+              MMA16816ConversionHelper::getBNumElemsPerThread(type, wpt[1]);\n+          return struct_ty(SmallVector<Type>(elems, targetTy));\n+        }\n+      }\n+\n+      if (mmaLayout.isVolta()) {\n+        auto [isARow, isBRow, isAVec4, isBVec4, mmaId] =\n+            mmaLayout.decodeVoltaLayoutStates();\n+        DotOpMmaV1ConversionHelper helper(mmaLayout);\n+        if (dotOpLayout.getOpIdx() == 0) { // $a\n+          DotOpMmaV1ConversionHelper::AParam param(isARow, isAVec4);\n+          int elems =\n+              helper.numElemsPerThreadA(shape, isARow, isAVec4, param.vec);\n+          Type x2Ty = vec_ty(elemTy, 2);\n+          return struct_ty(SmallVector<Type>(elems, x2Ty));\n+        }\n+        if (dotOpLayout.getOpIdx() == 1) { // $b\n+          DotOpMmaV1ConversionHelper::BParam param(isBRow, isBVec4);\n+          int elems =\n+              helper.numElemsPerThreadB(shape, isBRow, isBVec4, param.vec);\n+          Type x2Ty = vec_ty(elemTy, 2);\n+          return struct_ty(SmallVector<Type>(elems, x2Ty));\n+        }\n+      }\n+    }\n+\n+    llvm::errs() << \"Unexpected dot operand layout detected in \"\n+                    \"TritonToLLVMTypeConverter\";\n+    return std::nullopt;\n+  }\n+\n+  return std::nullopt;\n+}\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.h", "status": "modified", "additions": 9, "deletions": 133, "changes": 142, "file_content_changes": "@@ -1,154 +1,30 @@\n #ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_TYPECONVERTER_H\n #define TRITON_CONVERSION_TRITONGPU_TO_LLVM_TYPECONVERTER_H\n \n+#include \"mlir/Conversion/LLVMCommon/TypeConverter.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"triton/Conversion/MLIRTypes.h\"\n \n-#include \"DotOpHelpers.h\"\n-#include \"Utility.h\"\n-\n using namespace mlir;\n using namespace mlir::triton;\n \n-using ::mlir::LLVM::DotOpFMAConversionHelper;\n-using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n-using ::mlir::LLVM::MMA16816ConversionHelper;\n-using ::mlir::triton::gpu::BlockedEncodingAttr;\n-using ::mlir::triton::gpu::DotOperandEncodingAttr;\n-using ::mlir::triton::gpu::getElemsPerThread;\n-using ::mlir::triton::gpu::MmaEncodingAttr;\n-using ::mlir::triton::gpu::SharedEncodingAttr;\n-using ::mlir::triton::gpu::SliceEncodingAttr;\n-\n class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n public:\n   using TypeConverter::convertType;\n \n   TritonGPUToLLVMTypeConverter(MLIRContext *ctx, LowerToLLVMOptions &option,\n-                               const DataLayoutAnalysis *analysis = nullptr)\n-      : LLVMTypeConverter(ctx, option, analysis) {\n-    addConversion([&](triton::PointerType type) -> llvm::Optional<Type> {\n-      return convertTritonPointerType(type);\n-    });\n-    addConversion([&](RankedTensorType type) -> llvm::Optional<Type> {\n-      return convertTritonTensorType(type);\n-    });\n-    // Internally store float8 as int8\n-    addConversion([&](triton::Float8Type type) -> llvm::Optional<Type> {\n-      return IntegerType::get(type.getContext(), 8);\n-    });\n-    // Internally store bfloat16 as int16\n-    addConversion([&](BFloat16Type type) -> llvm::Optional<Type> {\n-      return IntegerType::get(type.getContext(), 16);\n-    });\n-  }\n-\n-  Type convertTritonPointerType(triton::PointerType type) {\n-    // Recursively translate pointee type\n-    return LLVM::LLVMPointerType::get(convertType(type.getPointeeType()),\n-                                      type.getAddressSpace());\n-  }\n-\n-  llvm::Optional<Type> convertTritonTensorType(RankedTensorType type) {\n-    auto ctx = type.getContext();\n-    Attribute layout = type.getEncoding();\n-    SmallVector<int64_t> shape(type.getShape().begin(), type.getShape().end());\n-\n-    if (layout &&\n-        (layout.isa<BlockedEncodingAttr>() || layout.isa<SliceEncodingAttr>() ||\n-         layout.isa<MmaEncodingAttr>())) {\n-      unsigned numElementsPerThread = getElemsPerThread(type);\n-      SmallVector<Type, 4> types(numElementsPerThread,\n-                                 convertType(type.getElementType()));\n-      return LLVM::LLVMStructType::getLiteral(ctx, types);\n-    } else if (auto shared_layout =\n-                   layout.dyn_cast_or_null<SharedEncodingAttr>()) {\n-      SmallVector<Type, 4> types;\n-      // base ptr\n-      auto ptrType =\n-          LLVM::LLVMPointerType::get(convertType(type.getElementType()), 3);\n-      types.push_back(ptrType);\n-      // shape dims\n-      auto rank = type.getRank();\n-      // offsets + strides\n-      for (auto i = 0; i < rank * 2; i++) {\n-        types.push_back(IntegerType::get(ctx, 32));\n-      }\n-      return LLVM::LLVMStructType::getLiteral(ctx, types);\n-    } else if (auto dotOpLayout =\n-                   layout.dyn_cast_or_null<DotOperandEncodingAttr>()) {\n-      if (dotOpLayout.getParent()\n-              .isa<BlockedEncodingAttr>()) { // for parent is blocked layout\n-        int numElemsPerThread =\n-            DotOpFMAConversionHelper::getNumElemsPerThread(shape, dotOpLayout);\n+                               const DataLayoutAnalysis *analysis = nullptr);\n \n-        return LLVM::LLVMStructType::getLiteral(\n-            ctx, SmallVector<Type>(numElemsPerThread, type::f32Ty(ctx)));\n-      } else { // for parent is MMA layout\n-        auto mmaLayout = dotOpLayout.getParent().cast<MmaEncodingAttr>();\n-        auto wpt = mmaLayout.getWarpsPerCTA();\n-        Type elemTy = convertType(type.getElementType());\n-        if (mmaLayout.isAmpere()) {\n-          const llvm::DenseMap<int, Type> targetTyMap = {\n-              {32, vec_ty(elemTy, 1)},\n-              {16, vec_ty(elemTy, 2)},\n-              {8, vec_ty(elemTy, 4)},\n-          };\n-          Type targetTy;\n-          if (targetTyMap.count(elemTy.getIntOrFloatBitWidth())) {\n-            targetTy = targetTyMap.lookup(elemTy.getIntOrFloatBitWidth());\n-            // <2xi16>/<4xi8> => i32\n-            // We are doing this because NVPTX inserts extra integer instrs to\n-            // pack & unpack vectors of sub-word integers\n-            // Note: this needs to be synced with\n-            //       DotOpMmaV2ConversionHelper::loadX4\n-            if (elemTy.isa<IntegerType>() &&\n-                (elemTy.getIntOrFloatBitWidth() == 8 ||\n-                 elemTy.getIntOrFloatBitWidth() == 16))\n-              targetTy = IntegerType::get(ctx, 32);\n-          } else {\n-            assert(false && \"Unsupported element type\");\n-          }\n-          if (dotOpLayout.getOpIdx() == 0) { // $a\n-            auto elems =\n-                MMA16816ConversionHelper::getANumElemsPerThread(type, wpt[0]);\n-            return struct_ty(SmallVector<Type>(elems, targetTy));\n-          }\n-          if (dotOpLayout.getOpIdx() == 1) { // $b\n-            auto elems =\n-                MMA16816ConversionHelper::getBNumElemsPerThread(type, wpt[1]);\n-            return struct_ty(SmallVector<Type>(elems, targetTy));\n-          }\n-        }\n+  Type convertTritonPointerType(triton::PointerType type);\n \n-        if (mmaLayout.isVolta()) {\n-          auto [isARow, isBRow, isAVec4, isBVec4, mmaId] =\n-              mmaLayout.decodeVoltaLayoutStates();\n-          DotOpMmaV1ConversionHelper helper(mmaLayout);\n-          if (dotOpLayout.getOpIdx() == 0) { // $a\n-            DotOpMmaV1ConversionHelper::AParam param(isARow, isAVec4);\n-            int elems =\n-                helper.numElemsPerThreadA(shape, isARow, isAVec4, param.vec);\n-            Type x2Ty = vec_ty(elemTy, 2);\n-            return struct_ty(SmallVector<Type>(elems, x2Ty));\n-          }\n-          if (dotOpLayout.getOpIdx() == 1) { // $b\n-            DotOpMmaV1ConversionHelper::BParam param(isBRow, isBVec4);\n-            int elems =\n-                helper.numElemsPerThreadB(shape, isBRow, isBVec4, param.vec);\n-            Type x2Ty = vec_ty(elemTy, 2);\n-            return struct_ty(SmallVector<Type>(elems, x2Ty));\n-          }\n-        }\n-      }\n+  Value packLLElements(Location loc, ValueRange resultVals,\n+                       ConversionPatternRewriter &rewriter, Type type);\n \n-      llvm::errs() << \"Unexpected dot operand layout detected in \"\n-                      \"TritonToLLVMTypeConverter\";\n-      return std::nullopt;\n-    }\n+  SmallVector<Value> unpackLLElements(Location loc, Value llvmStruct,\n+                                      ConversionPatternRewriter &rewriter,\n+                                      Type type);\n \n-    return std::nullopt;\n-  }\n+  llvm::Optional<Type> convertTritonTensorType(RankedTensorType type);\n };\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.cpp", "status": "modified", "additions": 9, "deletions": 32, "changes": 41, "file_content_changes": "@@ -1,41 +1,11 @@\n #include \"Utility.h\"\n+#include \"TypeConverter.h\"\n \n namespace mlir {\n \n namespace LLVM {\n using namespace mlir::triton;\n \n-Value getStructFromElements(Location loc, ValueRange resultVals,\n-                            ConversionPatternRewriter &rewriter,\n-                            Type structType) {\n-  if (!structType.isa<LLVM::LLVMStructType>()) {\n-    return *resultVals.begin();\n-  }\n-\n-  Value llvmStruct = rewriter.create<LLVM::UndefOp>(loc, structType);\n-  for (const auto &v : llvm::enumerate(resultVals)) {\n-    assert(v.value() && \"can not insert null values\");\n-    llvmStruct = insert_val(structType, llvmStruct, v.value(), v.index());\n-  }\n-  return llvmStruct;\n-}\n-\n-SmallVector<Value> getElementsFromStruct(Location loc, Value llvmStruct,\n-                                         ConversionPatternRewriter &rewriter) {\n-  if (llvmStruct.getType().isIntOrIndexOrFloat() ||\n-      llvmStruct.getType().isa<triton::PointerType>() ||\n-      llvmStruct.getType().isa<LLVM::LLVMPointerType>())\n-    return {llvmStruct};\n-  ArrayRef<Type> types =\n-      llvmStruct.getType().cast<LLVM::LLVMStructType>().getBody();\n-  SmallVector<Value> results(types.size());\n-  for (unsigned i = 0; i < types.size(); ++i) {\n-    Type type = types[i];\n-    results[i] = extract_val(type, llvmStruct, i);\n-  }\n-  return results;\n-}\n-\n Value createConstantI32(Location loc, PatternRewriter &rewriter, int32_t v) {\n   auto i32ty = rewriter.getIntegerType(32);\n   return rewriter.create<LLVM::ConstantOp>(loc, i32ty,\n@@ -73,7 +43,14 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n SharedMemoryObject\n getSharedMemoryObjectFromStruct(Location loc, Value llvmStruct,\n                                 ConversionPatternRewriter &rewriter) {\n-  auto elems = getElementsFromStruct(loc, llvmStruct, rewriter);\n+  ArrayRef<Type> types =\n+      llvmStruct.getType().cast<LLVM::LLVMStructType>().getBody();\n+  SmallVector<Value> elems(types.size());\n+  for (unsigned i = 0; i < types.size(); ++i) {\n+    Type type = types[i];\n+    elems[i] = extract_val(type, llvmStruct, i);\n+  }\n+\n   auto rank = (elems.size() - 1) / 2;\n   return {/*base=*/elems[0],\n           /*strides=*/{elems.begin() + 1, elems.begin() + 1 + rank},"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 0, "deletions": 8, "changes": 8, "file_content_changes": "@@ -172,14 +172,6 @@ T getLinearIndex(llvm::ArrayRef<T> multiDimIndex, llvm::ArrayRef<T> shape,\n namespace LLVM {\n using namespace mlir::triton;\n \n-Value getStructFromElements(Location loc, ValueRange resultVals,\n-                            ConversionPatternRewriter &rewriter,\n-                            Type structType);\n-\n-SmallVector<Value> getElementsFromStruct(Location loc, Value llvmStruct,\n-                                         ConversionPatternRewriter &rewriter);\n-\n-/// Create a 32-bit integer constant.\n Value createConstantI32(Location loc, PatternRewriter &rewriter, int32_t v);\n \n /// Create a 32-bit float constant."}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 15, "deletions": 24, "changes": 39, "file_content_changes": "@@ -7,9 +7,7 @@ using namespace mlir::triton;\n using ::mlir::LLVM::DotOpFMAConversionHelper;\n using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n using ::mlir::LLVM::DotOpMmaV2ConversionHelper;\n-using ::mlir::LLVM::getElementsFromStruct;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n-using ::mlir::LLVM::getStructFromElements;\n using ::mlir::LLVM::MMA16816ConversionHelper;\n using ::mlir::triton::gpu::getElemsPerThread;\n \n@@ -25,19 +23,15 @@ struct SplatOpConversion\n   // @resType: the return type of the Splat-like op.\n   // @constVal: a LLVM::ConstantOp or other scalar value.\n   static Value convertSplatLikeOp(Type elemType, Type resType, Value constVal,\n-                                  TypeConverter *typeConverter,\n+                                  TritonGPUToLLVMTypeConverter *typeConverter,\n                                   ConversionPatternRewriter &rewriter,\n                                   Location loc) {\n     auto tensorTy = resType.cast<RankedTensorType>();\n     auto srcType = typeConverter->convertType(elemType);\n     auto llSrc = bitcast(constVal, srcType);\n     size_t elemsPerThread = getElemsPerThread(tensorTy);\n     llvm::SmallVector<Value> elems(elemsPerThread, llSrc);\n-    llvm::SmallVector<Type> elemTypes(elems.size(), srcType);\n-    auto structTy =\n-        LLVM::LLVMStructType::getLiteral(rewriter.getContext(), elemTypes);\n-\n-    return getStructFromElements(loc, elems, rewriter, structTy);\n+    return typeConverter->packLLElements(loc, elems, rewriter, resType);\n   }\n \n   LogicalResult matchAndRewrite(triton::SplatOp op, OpAdaptor adaptor,\n@@ -95,7 +89,7 @@ struct ArithConstantSplatOpConversion\n struct CatOpConversion : public ConvertTritonGPUOpToLLVMPattern<CatOp> {\n   using OpAdaptor = typename CatOp::Adaptor;\n \n-  explicit CatOpConversion(LLVMTypeConverter &typeConverter,\n+  explicit CatOpConversion(TritonGPUToLLVMTypeConverter &typeConverter,\n                            PatternBenefit benefit = 1)\n       : ConvertTritonGPUOpToLLVMPattern<CatOp>(typeConverter, benefit) {}\n \n@@ -109,17 +103,19 @@ struct CatOpConversion : public ConvertTritonGPUOpToLLVMPattern<CatOp> {\n         this->getTypeConverter()->convertType(resultTy.getElementType());\n     SmallVector<Type> types(elems, elemTy);\n     // unpack input values\n-    auto lhsVals = getElementsFromStruct(loc, adaptor.getLhs(), rewriter);\n-    auto rhsVals = getElementsFromStruct(loc, adaptor.getRhs(), rewriter);\n+    auto lhsVals = getTypeConverter()->unpackLLElements(\n+        loc, adaptor.getLhs(), rewriter, op.getOperand(0).getType());\n+    auto rhsVals = getTypeConverter()->unpackLLElements(\n+        loc, adaptor.getRhs(), rewriter, op.getOperand(1).getType());\n     // concatenate (and potentially reorder) values\n     SmallVector<Value> retVals;\n     for (Value v : lhsVals)\n       retVals.push_back(v);\n     for (Value v : rhsVals)\n       retVals.push_back(v);\n     // pack and replace\n-    Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n-    Value ret = getStructFromElements(loc, retVals, rewriter, structTy);\n+    Value ret =\n+        getTypeConverter()->packLLElements(loc, retVals, rewriter, resultTy);\n     rewriter.replaceOp(op, ret);\n     return success();\n   }\n@@ -128,24 +124,19 @@ struct CatOpConversion : public ConvertTritonGPUOpToLLVMPattern<CatOp> {\n template <typename SourceOp>\n struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n   using OpAdaptor = typename SourceOp::Adaptor;\n-  explicit ViewLikeOpConversion(LLVMTypeConverter &typeConverter,\n+  explicit ViewLikeOpConversion(TritonGPUToLLVMTypeConverter &typeConverter,\n                                 PatternBenefit benefit = 1)\n       : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n \n   LogicalResult\n   matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    // We cannot directly run `rewriter.replaceOp(op, adaptor.getSrc())`\n-    // due to MLIR's restrictions\n     Location loc = op->getLoc();\n     auto resultTy = op.getType().template cast<RankedTensorType>();\n-    unsigned elems = getElemsPerThread(resultTy);\n-    Type elemTy =\n-        this->getTypeConverter()->convertType(resultTy.getElementType());\n-    SmallVector<Type> types(elems, elemTy);\n-    Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n-    auto vals = getElementsFromStruct(loc, adaptor.getSrc(), rewriter);\n-    Value view = getStructFromElements(loc, vals, rewriter, structTy);\n+    auto vals = this->getTypeConverter()->unpackLLElements(\n+        loc, adaptor.getSrc(), rewriter, op.getOperand().getType());\n+    Value view =\n+        this->getTypeConverter()->packLLElements(loc, vals, rewriter, resultTy);\n     rewriter.replaceOp(op, view);\n     return success();\n   }\n@@ -174,7 +165,7 @@ struct TransOpConversion\n   }\n };\n \n-void populateViewOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n+void populateViewOpToLLVMPatterns(TritonGPUToLLVMTypeConverter &typeConverter,\n                                   RewritePatternSet &patterns, int numWarps,\n                                   AxisInfoAnalysis &axisInfoAnalysis,\n                                   const Allocation *allocation, Value smem,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -6,7 +6,7 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n-void populateViewOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n+void populateViewOpToLLVMPatterns(TritonGPUToLLVMTypeConverter &typeConverter,\n                                   RewritePatternSet &patterns, int numWarps,\n                                   AxisInfoAnalysis &axisInfoAnalysis,\n                                   const Allocation *allocation, Value smem,"}]