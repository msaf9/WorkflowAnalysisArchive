[{"filename": "python/test/unit/operators/test_flash_attention.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -5,7 +5,8 @@\n import triton.ops\n \n \n-@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n+@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64),\n+                                                 (4, 48, 1024, 128)])\n @pytest.mark.parametrize('dtype', [torch.float16, torch.bfloat16])\n def test_op(Z, H, N_CTX, D_HEAD, dtype):\n     capability = torch.cuda.get_device_capability()"}]