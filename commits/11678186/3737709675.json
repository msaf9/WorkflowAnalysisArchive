[{"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 514, "deletions": 488, "changes": 1002, "file_content_changes": "@@ -15,45 +15,20 @@ using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::getSizePerThread;\n \n-LogicalResult ConvertLayoutOpConversion::matchAndRewrite(\n-    triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n-    ConversionPatternRewriter &rewriter) const {\n-  Value src = op.src();\n-  Value dst = op.result();\n-  auto srcTy = src.getType().cast<RankedTensorType>();\n-  auto dstTy = dst.getType().cast<RankedTensorType>();\n-  Attribute srcLayout = srcTy.getEncoding();\n-  Attribute dstLayout = dstTy.getEncoding();\n-  if (srcLayout.isa<BlockedEncodingAttr>() &&\n-      dstLayout.isa<SharedEncodingAttr>()) {\n-    return lowerBlockedToShared(op, adaptor, rewriter);\n-  }\n-  if (srcLayout.isa<SharedEncodingAttr>() &&\n-      dstLayout.isa<DotOperandEncodingAttr>()) {\n-    return lowerSharedToDotOperand(op, adaptor, rewriter);\n-  }\n-  if ((srcLayout.isa<BlockedEncodingAttr>() ||\n-       srcLayout.isa<MmaEncodingAttr>() ||\n-       srcLayout.isa<SliceEncodingAttr>()) &&\n-      (dstLayout.isa<BlockedEncodingAttr>() ||\n-       dstLayout.isa<MmaEncodingAttr>() ||\n-       dstLayout.isa<SliceEncodingAttr>())) {\n-    return lowerDistributedToDistributed(op, adaptor, rewriter);\n-  }\n-  if (srcLayout.isa<MmaEncodingAttr>() &&\n-      dstLayout.isa<DotOperandEncodingAttr>()) {\n-    return lowerMmaToDotOperand(op, adaptor, rewriter);\n-  }\n-  // TODO: to be implemented\n-  llvm_unreachable(\"unsupported layout conversion\");\n-  return failure();\n+bool isMmaToDotShortcut(MmaEncodingAttr &mmaLayout,\n+                        DotOperandEncodingAttr &dotOperandLayout) {\n+  // dot_op<opIdx=0, parent=#mma> = #mma\n+  // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>\n+  return mmaLayout.getWarpsPerCTA()[1] == 1 &&\n+         dotOperandLayout.getOpIdx() == 0 &&\n+         dotOperandLayout.getParent() == mmaLayout;\n }\n \n-void ConvertLayoutOpConversion::storeBlockedToShared(\n-    Value src, Value llSrc,\n-    ArrayRef<Value> srcStrides, ArrayRef<Value> srcIndices,\n-    Value dst, Value smemBase, Type elemPtrTy,\n-    Location loc, ConversionPatternRewriter &rewriter) {\n+void storeBlockedToShared(Value src, Value llSrc,\n+                          ArrayRef<Value> srcStrides,\n+                          ArrayRef<Value> srcIndices, Value dst,\n+                          Value smemBase, Type elemPtrTy, Location loc,\n+                          ConversionPatternRewriter &rewriter) {\n   auto srcTy = src.getType().cast<RankedTensorType>();\n   auto srcShape = srcTy.getShape();\n   assert(srcShape.size() == 2 && \"Unexpected rank of insertSlice\");\n@@ -159,490 +134,541 @@ void ConvertLayoutOpConversion::storeBlockedToShared(\n   }\n }\n \n-SmallVector<Value> ConvertLayoutOpConversion::getMultiDimOffset(\n-    Attribute layout, Location loc, ConversionPatternRewriter &rewriter,\n-    unsigned elemId, ArrayRef<int64_t> shape,\n-    ArrayRef<unsigned> multiDimCTAInRepId,\n-    ArrayRef<unsigned> shapePerCTA) const {\n-  unsigned rank = shape.size();\n-  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n-    auto multiDimOffsetFirstElem =\n-        emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n-    SmallVector<Value> multiDimOffset(rank);\n-    SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n-        elemId, getSizePerThread(layout), getOrder(layout));\n-    for (unsigned d = 0; d < rank; ++d) {\n-      multiDimOffset[d] = add(multiDimOffsetFirstElem[d],\n-                              idx_val(multiDimCTAInRepId[d] * shapePerCTA[d] +\n-                                      multiDimElemId[d]));\n+struct ConvertLayoutOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::ConvertLayoutOp> {\n+public:\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::gpu::ConvertLayoutOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Value src = op.src();\n+    Value dst = op.result();\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    Attribute srcLayout = srcTy.getEncoding();\n+    Attribute dstLayout = dstTy.getEncoding();\n+    if (srcLayout.isa<BlockedEncodingAttr>() &&\n+        dstLayout.isa<SharedEncodingAttr>()) {\n+      return lowerBlockedToShared(op, adaptor, rewriter);\n     }\n-    return multiDimOffset;\n-  }\n-  if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n-    unsigned dim = sliceLayout.getDim();\n-    auto multiDimOffsetParent =\n-        getMultiDimOffset(sliceLayout.getParent(), loc, rewriter, elemId,\n-                          sliceLayout.paddedShape(shape),\n-                          sliceLayout.paddedShape(multiDimCTAInRepId),\n-                          sliceLayout.paddedShape(shapePerCTA));\n-    SmallVector<Value> multiDimOffset(rank);\n-    for (unsigned d = 0; d < rank + 1; ++d) {\n-      if (d == dim)\n-        continue;\n-      unsigned slicedD = d < dim ? d : (d - 1);\n-      multiDimOffset[slicedD] = multiDimOffsetParent[d];\n+    if (srcLayout.isa<SharedEncodingAttr>() &&\n+        dstLayout.isa<DotOperandEncodingAttr>()) {\n+      return lowerSharedToDotOperand(op, adaptor, rewriter);\n+    }\n+    if ((srcLayout.isa<BlockedEncodingAttr>() ||\n+         srcLayout.isa<MmaEncodingAttr>() ||\n+         srcLayout.isa<SliceEncodingAttr>()) &&\n+        (dstLayout.isa<BlockedEncodingAttr>() ||\n+         dstLayout.isa<MmaEncodingAttr>() ||\n+         dstLayout.isa<SliceEncodingAttr>())) {\n+      return lowerDistributedToDistributed(op, adaptor, rewriter);\n+    }\n+    if (srcLayout.isa<MmaEncodingAttr>() &&\n+        dstLayout.isa<DotOperandEncodingAttr>()) {\n+      return lowerMmaToDotOperand(op, adaptor, rewriter);\n     }\n-    return multiDimOffset;\n+    // TODO: to be implemented\n+    llvm_unreachable(\"unsupported layout conversion\");\n+    return failure();\n   }\n-  if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    SmallVector<Value> mmaColIdx(4);\n-    SmallVector<Value> mmaRowIdx(2);\n-    Value threadId = getThreadId(rewriter, loc);\n-    Value warpSize = idx_val(32);\n-    Value laneId = urem(threadId, warpSize);\n-    Value warpId = udiv(threadId, warpSize);\n-    // TODO: fix the bug in MMAEncodingAttr document\n-    SmallVector<Value> multiDimWarpId(2);\n-    multiDimWarpId[0] = urem(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n-    multiDimWarpId[1] = udiv(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n-    Value _1 = idx_val(1);\n-    Value _2 = idx_val(2);\n-    Value _4 = idx_val(4);\n-    Value _8 = idx_val(8);\n-    Value _16 = idx_val(16);\n-    if (mmaLayout.isAmpere()) {\n-      multiDimWarpId[0] = urem(multiDimWarpId[0], idx_val(shape[0] / 16));\n-      multiDimWarpId[1] = urem(multiDimWarpId[1], idx_val(shape[1] / 8));\n-      Value mmaGrpId = udiv(laneId, _4);\n-      Value mmaGrpIdP8 = add(mmaGrpId, _8);\n-      Value mmaThreadIdInGrp = urem(laneId, _4);\n-      Value mmaThreadIdInGrpM2 = mul(mmaThreadIdInGrp, _2);\n-      Value mmaThreadIdInGrpM2P1 = add(mmaThreadIdInGrpM2, _1);\n-      Value rowWarpOffset = mul(multiDimWarpId[0], _16);\n-      mmaRowIdx[0] = add(mmaGrpId, rowWarpOffset);\n-      mmaRowIdx[1] = add(mmaGrpIdP8, rowWarpOffset);\n-      Value colWarpOffset = mul(multiDimWarpId[1], _8);\n-      mmaColIdx[0] = add(mmaThreadIdInGrpM2, colWarpOffset);\n-      mmaColIdx[1] = add(mmaThreadIdInGrpM2P1, colWarpOffset);\n-    } else if (mmaLayout.isVolta()) {\n-      multiDimWarpId[0] = urem(multiDimWarpId[0], idx_val(shape[0] / 16));\n-      multiDimWarpId[1] = urem(multiDimWarpId[1], idx_val(shape[1] / 16));\n-      Value laneIdDiv16 = udiv(laneId, _16);\n-      Value laneIdRem16 = urem(laneId, _16);\n-      Value laneIdRem2 = urem(laneId, _2);\n-      Value laneIdRem16Div8 = udiv(laneIdRem16, _8);\n-      Value laneIdRem16Div4 = udiv(laneIdRem16, _4);\n-      Value laneIdRem16Div4Rem2 = urem(laneIdRem16Div4, _2);\n-      Value laneIdRem4Div2 = udiv(urem(laneId, _4), _2);\n-      Value rowWarpOffset = mul(multiDimWarpId[0], _16);\n-      Value colWarpOffset = mul(multiDimWarpId[1], _16);\n-      mmaRowIdx[0] =\n-          add(add(mul(laneIdDiv16, _8), mul(laneIdRem16Div4Rem2, _4)),\n-              laneIdRem2);\n-      mmaRowIdx[0] = add(mmaRowIdx[0], rowWarpOffset);\n-      mmaRowIdx[1] = add(mmaRowIdx[0], _2);\n-      mmaColIdx[0] = add(mul(laneIdRem16Div8, _4), mul(laneIdRem4Div2, _2));\n-      mmaColIdx[0] = add(mmaColIdx[0], colWarpOffset);\n-      mmaColIdx[1] = add(mmaColIdx[0], _1);\n-      mmaColIdx[2] = add(mmaColIdx[0], _8);\n-      mmaColIdx[3] = add(mmaColIdx[0], idx_val(9));\n-    } else {\n-      llvm_unreachable(\"Unexpected MMALayout version\");\n+\n+private:\n+  SmallVector<Value> getMultiDimOffset(Attribute layout, Location loc,\n+                                       ConversionPatternRewriter &rewriter,\n+                                       unsigned elemId, ArrayRef<int64_t> shape,\n+                                       ArrayRef<unsigned> multiDimCTAInRepId,\n+                                       ArrayRef<unsigned> shapePerCTA) const {\n+    unsigned rank = shape.size();\n+    if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+      auto multiDimOffsetFirstElem =\n+          emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n+      SmallVector<Value> multiDimOffset(rank);\n+      SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n+          elemId, getSizePerThread(layout), getOrder(layout));\n+      for (unsigned d = 0; d < rank; ++d) {\n+        multiDimOffset[d] = add(multiDimOffsetFirstElem[d],\n+                                idx_val(multiDimCTAInRepId[d] * shapePerCTA[d] +\n+                                        multiDimElemId[d]));\n+      }\n+      return multiDimOffset;\n+    }\n+    if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+      unsigned dim = sliceLayout.getDim();\n+      auto multiDimOffsetParent =\n+          getMultiDimOffset(sliceLayout.getParent(), loc, rewriter, elemId,\n+                            sliceLayout.paddedShape(shape),\n+                            sliceLayout.paddedShape(multiDimCTAInRepId),\n+                            sliceLayout.paddedShape(shapePerCTA));\n+      SmallVector<Value> multiDimOffset(rank);\n+      for (unsigned d = 0; d < rank + 1; ++d) {\n+        if (d == dim)\n+          continue;\n+        unsigned slicedD = d < dim ? d : (d - 1);\n+        multiDimOffset[slicedD] = multiDimOffsetParent[d];\n+      }\n+      return multiDimOffset;\n     }\n+    if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+      SmallVector<Value> mmaColIdx(4);\n+      SmallVector<Value> mmaRowIdx(2);\n+      Value threadId = getThreadId(rewriter, loc);\n+      Value warpSize = idx_val(32);\n+      Value laneId = urem(threadId, warpSize);\n+      Value warpId = udiv(threadId, warpSize);\n+      // TODO: fix the bug in MMAEncodingAttr document\n+      SmallVector<Value> multiDimWarpId(2);\n+      multiDimWarpId[0] = urem(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n+      multiDimWarpId[1] = udiv(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n+      Value _1 = idx_val(1);\n+      Value _2 = idx_val(2);\n+      Value _4 = idx_val(4);\n+      Value _8 = idx_val(8);\n+      Value _16 = idx_val(16);\n+      if (mmaLayout.isAmpere()) {\n+        multiDimWarpId[0] = urem(multiDimWarpId[0], idx_val(shape[0] / 16));\n+        multiDimWarpId[1] = urem(multiDimWarpId[1], idx_val(shape[1] / 8));\n+        Value mmaGrpId = udiv(laneId, _4);\n+        Value mmaGrpIdP8 = add(mmaGrpId, _8);\n+        Value mmaThreadIdInGrp = urem(laneId, _4);\n+        Value mmaThreadIdInGrpM2 = mul(mmaThreadIdInGrp, _2);\n+        Value mmaThreadIdInGrpM2P1 = add(mmaThreadIdInGrpM2, _1);\n+        Value rowWarpOffset = mul(multiDimWarpId[0], _16);\n+        mmaRowIdx[0] = add(mmaGrpId, rowWarpOffset);\n+        mmaRowIdx[1] = add(mmaGrpIdP8, rowWarpOffset);\n+        Value colWarpOffset = mul(multiDimWarpId[1], _8);\n+        mmaColIdx[0] = add(mmaThreadIdInGrpM2, colWarpOffset);\n+        mmaColIdx[1] = add(mmaThreadIdInGrpM2P1, colWarpOffset);\n+      } else if (mmaLayout.isVolta()) {\n+        multiDimWarpId[0] = urem(multiDimWarpId[0], idx_val(shape[0] / 16));\n+        multiDimWarpId[1] = urem(multiDimWarpId[1], idx_val(shape[1] / 16));\n+        Value laneIdDiv16 = udiv(laneId, _16);\n+        Value laneIdRem16 = urem(laneId, _16);\n+        Value laneIdRem2 = urem(laneId, _2);\n+        Value laneIdRem16Div8 = udiv(laneIdRem16, _8);\n+        Value laneIdRem16Div4 = udiv(laneIdRem16, _4);\n+        Value laneIdRem16Div4Rem2 = urem(laneIdRem16Div4, _2);\n+        Value laneIdRem4Div2 = udiv(urem(laneId, _4), _2);\n+        Value rowWarpOffset = mul(multiDimWarpId[0], _16);\n+        Value colWarpOffset = mul(multiDimWarpId[1], _16);\n+        mmaRowIdx[0] =\n+            add(add(mul(laneIdDiv16, _8), mul(laneIdRem16Div4Rem2, _4)),\n+                laneIdRem2);\n+        mmaRowIdx[0] = add(mmaRowIdx[0], rowWarpOffset);\n+        mmaRowIdx[1] = add(mmaRowIdx[0], _2);\n+        mmaColIdx[0] = add(mul(laneIdRem16Div8, _4), mul(laneIdRem4Div2, _2));\n+        mmaColIdx[0] = add(mmaColIdx[0], colWarpOffset);\n+        mmaColIdx[1] = add(mmaColIdx[0], _1);\n+        mmaColIdx[2] = add(mmaColIdx[0], _8);\n+        mmaColIdx[3] = add(mmaColIdx[0], idx_val(9));\n+      } else {\n+        llvm_unreachable(\"Unexpected MMALayout version\");\n+      }\n \n-    assert(rank == 2);\n-    SmallVector<Value> multiDimOffset(rank);\n-    if (mmaLayout.isAmpere()) {\n-      multiDimOffset[0] = elemId < 2 ? mmaRowIdx[0] : mmaRowIdx[1];\n-      multiDimOffset[1] = elemId % 2 == 0 ? mmaColIdx[0] : mmaColIdx[1];\n-      multiDimOffset[0] = add(\n-          multiDimOffset[0], idx_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n-      multiDimOffset[1] = add(\n-          multiDimOffset[1], idx_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n-    } else if (mmaLayout.isVolta()) {\n-      // the order of elements in a thread:\n-      //   c0, c1, ...  c4, c5\n-      //   c2, c3, ...  c6, c7\n-      if (elemId < 2) {\n-        multiDimOffset[0] = mmaRowIdx[0];\n-        multiDimOffset[1] = mmaColIdx[elemId % 2];\n-      } else if (elemId >= 2 && elemId < 4) {\n-        multiDimOffset[0] = mmaRowIdx[1];\n-        multiDimOffset[1] = mmaColIdx[elemId % 2];\n-      } else if (elemId >= 4 && elemId < 6) {\n-        multiDimOffset[0] = mmaRowIdx[0];\n-        multiDimOffset[1] = mmaColIdx[elemId % 2 + 2];\n-      } else if (elemId >= 6) {\n-        multiDimOffset[0] = mmaRowIdx[1];\n-        multiDimOffset[1] = mmaColIdx[elemId % 2 + 2];\n+      assert(rank == 2);\n+      SmallVector<Value> multiDimOffset(rank);\n+      if (mmaLayout.isAmpere()) {\n+        multiDimOffset[0] = elemId < 2 ? mmaRowIdx[0] : mmaRowIdx[1];\n+        multiDimOffset[1] = elemId % 2 == 0 ? mmaColIdx[0] : mmaColIdx[1];\n+        multiDimOffset[0] = add(\n+            multiDimOffset[0], idx_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n+        multiDimOffset[1] = add(\n+            multiDimOffset[1], idx_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n+      } else if (mmaLayout.isVolta()) {\n+        // the order of elements in a thread:\n+        //   c0, c1, ...  c4, c5\n+        //   c2, c3, ...  c6, c7\n+        if (elemId < 2) {\n+          multiDimOffset[0] = mmaRowIdx[0];\n+          multiDimOffset[1] = mmaColIdx[elemId % 2];\n+        } else if (elemId >= 2 && elemId < 4) {\n+          multiDimOffset[0] = mmaRowIdx[1];\n+          multiDimOffset[1] = mmaColIdx[elemId % 2];\n+        } else if (elemId >= 4 && elemId < 6) {\n+          multiDimOffset[0] = mmaRowIdx[0];\n+          multiDimOffset[1] = mmaColIdx[elemId % 2 + 2];\n+        } else if (elemId >= 6) {\n+          multiDimOffset[0] = mmaRowIdx[1];\n+          multiDimOffset[1] = mmaColIdx[elemId % 2 + 2];\n+        }\n+        multiDimOffset[0] = add(\n+            multiDimOffset[0], idx_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n+        multiDimOffset[1] = add(\n+            multiDimOffset[1], idx_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n+      } else {\n+        llvm_unreachable(\"Unexpected MMALayout version\");\n       }\n-      multiDimOffset[0] = add(\n-          multiDimOffset[0], idx_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n-      multiDimOffset[1] = add(\n-          multiDimOffset[1], idx_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n-    } else {\n-      llvm_unreachable(\"Unexpected MMALayout version\");\n+      return multiDimOffset;\n     }\n-    return multiDimOffset;\n+    llvm_unreachable(\"unexpected layout in getMultiDimOffset\");\n   }\n-  llvm_unreachable(\"unexpected layout in getMultiDimOffset\");\n-}\n \n-void ConvertLayoutOpConversion::processReplica(\n-    Location loc, ConversionPatternRewriter &rewriter,\n-    bool stNotRd, RankedTensorType type,\n-    ArrayRef<unsigned> numCTAsEachRep,\n-    ArrayRef<unsigned> multiDimRepId, unsigned vec,\n-    ArrayRef<unsigned> paddedRepShape,\n-    ArrayRef<unsigned> outOrd, SmallVector<Value> &vals,\n-    Value smemBase) const {\n-  auto accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n-  auto layout = type.getEncoding();\n-  auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>();\n-  auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>();\n-  auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>();\n-  auto rank = type.getRank();\n-  auto sizePerThread = getSizePerThread(layout);\n-  auto accumSizePerThread = product<unsigned>(sizePerThread);\n-  SmallVector<unsigned> numCTAs(rank);\n-  auto shapePerCTA = getShapePerCTA(layout);\n-  auto order = getOrder(layout);\n-  for (unsigned d = 0; d < rank; ++d) {\n-    numCTAs[d] = ceil<unsigned>(type.getShape()[d], shapePerCTA[d]);\n-  }\n-  auto elemTy = type.getElementType();\n-  bool isInt1 = elemTy.isInteger(1);\n-  bool isPtr = elemTy.isa<triton::PointerType>();\n-  auto llvmElemTyOrig = getTypeConverter()->convertType(elemTy);\n-  if (isInt1)\n-    elemTy = IntegerType::get(elemTy.getContext(), 8);\n-  else if (isPtr)\n-    elemTy = IntegerType::get(elemTy.getContext(), 64);\n-\n-  auto llvmElemTy = getTypeConverter()->convertType(elemTy);\n-\n-  for (unsigned ctaId = 0; ctaId < accumNumCTAsEachRep; ++ctaId) {\n-    auto multiDimCTAInRepId =\n-        getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep, order);\n-    SmallVector<unsigned> multiDimCTAId(rank);\n-    for (const auto &it : llvm::enumerate(multiDimCTAInRepId)) {\n-      auto d = it.index();\n-      multiDimCTAId[d] = multiDimRepId[d] * numCTAsEachRep[d] + it.value();\n+  // shared memory rd/st for blocked or mma layout with data padding\n+  void processReplica(Location loc, ConversionPatternRewriter &rewriter,\n+                      bool stNotRd, RankedTensorType type,\n+                      ArrayRef<unsigned> numCTAsEachRep,\n+                      ArrayRef<unsigned> multiDimRepId, unsigned vec,\n+                      ArrayRef<unsigned> paddedRepShape,\n+                      ArrayRef<unsigned> outOrd, SmallVector<Value> &vals,\n+                      Value smemBase) const {\n+    auto accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n+    auto layout = type.getEncoding();\n+    auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>();\n+    auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>();\n+    auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>();\n+    auto rank = type.getRank();\n+    auto sizePerThread = getSizePerThread(layout);\n+    auto accumSizePerThread = product<unsigned>(sizePerThread);\n+    SmallVector<unsigned> numCTAs(rank);\n+    auto shapePerCTA = getShapePerCTA(layout);\n+    auto order = getOrder(layout);\n+    for (unsigned d = 0; d < rank; ++d) {\n+      numCTAs[d] = ceil<unsigned>(type.getShape()[d], shapePerCTA[d]);\n     }\n+    auto elemTy = type.getElementType();\n+    bool isInt1 = elemTy.isInteger(1);\n+    bool isPtr = elemTy.isa<triton::PointerType>();\n+    auto llvmElemTyOrig = getTypeConverter()->convertType(elemTy);\n+    if (isInt1)\n+      elemTy = IntegerType::get(elemTy.getContext(), 8);\n+    else if (isPtr)\n+      elemTy = IntegerType::get(elemTy.getContext(), 64);\n+\n+    auto llvmElemTy = getTypeConverter()->convertType(elemTy);\n+\n+    for (unsigned ctaId = 0; ctaId < accumNumCTAsEachRep; ++ctaId) {\n+      auto multiDimCTAInRepId =\n+          getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep, order);\n+      SmallVector<unsigned> multiDimCTAId(rank);\n+      for (const auto &it : llvm::enumerate(multiDimCTAInRepId)) {\n+        auto d = it.index();\n+        multiDimCTAId[d] = multiDimRepId[d] * numCTAsEachRep[d] + it.value();\n+      }\n \n-    auto linearCTAId = getLinearIndex<unsigned>(multiDimCTAId, numCTAs, order);\n-    // TODO: This is actually redundant index calculation, we should\n-    //       consider of caching the index calculation result in case\n-    //       of performance issue observed.\n-    for (unsigned elemId = 0; elemId < accumSizePerThread; elemId += vec) {\n-      SmallVector<Value> multiDimOffset =\n-          getMultiDimOffset(layout, loc, rewriter, elemId, type.getShape(),\n-                            multiDimCTAInRepId, shapePerCTA);\n-      Value offset =\n-          linearize(rewriter, loc, multiDimOffset, paddedRepShape, outOrd);\n-\n-      auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n-      Value ptr = gep(elemPtrTy, smemBase, offset);\n-      auto vecTy = vec_ty(llvmElemTy, vec);\n-      ptr = bitcast(ptr, ptr_ty(vecTy, 3));\n-      if (stNotRd) {\n-        Value valVec = undef(vecTy);\n-        for (unsigned v = 0; v < vec; ++v) {\n-          auto currVal = vals[elemId + linearCTAId * accumSizePerThread + v];\n-          if (isInt1)\n-            currVal = zext(llvmElemTy, currVal);\n-          else if (isPtr)\n-            currVal = ptrtoint(llvmElemTy, currVal);\n-          valVec = insert_element(vecTy, valVec, currVal, idx_val(v));\n-        }\n-        store(valVec, ptr);\n-      } else {\n-        Value valVec = load(ptr);\n-        for (unsigned v = 0; v < vec; ++v) {\n-          Value currVal = extract_element(llvmElemTy, valVec, idx_val(v));\n-          if (isInt1)\n-            currVal =\n-                icmp_ne(currVal, rewriter.create<LLVM::ConstantOp>(\n-                                     loc, i8_ty, rewriter.getI8IntegerAttr(0)));\n-          else if (isPtr)\n-            currVal = inttoptr(llvmElemTyOrig, currVal);\n-          vals[elemId + linearCTAId * accumSizePerThread + v] = currVal;\n+      auto linearCTAId = getLinearIndex<unsigned>(multiDimCTAId, numCTAs, order);\n+      // TODO: This is actually redundant index calculation, we should\n+      //       consider of caching the index calculation result in case\n+      //       of performance issue observed.\n+      for (unsigned elemId = 0; elemId < accumSizePerThread; elemId += vec) {\n+        SmallVector<Value> multiDimOffset =\n+            getMultiDimOffset(layout, loc, rewriter, elemId, type.getShape(),\n+                              multiDimCTAInRepId, shapePerCTA);\n+        Value offset =\n+            linearize(rewriter, loc, multiDimOffset, paddedRepShape, outOrd);\n+\n+        auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n+        Value ptr = gep(elemPtrTy, smemBase, offset);\n+        auto vecTy = vec_ty(llvmElemTy, vec);\n+        ptr = bitcast(ptr, ptr_ty(vecTy, 3));\n+        if (stNotRd) {\n+          Value valVec = undef(vecTy);\n+          for (unsigned v = 0; v < vec; ++v) {\n+            auto currVal = vals[elemId + linearCTAId * accumSizePerThread + v];\n+            if (isInt1)\n+              currVal = zext(llvmElemTy, currVal);\n+            else if (isPtr)\n+              currVal = ptrtoint(llvmElemTy, currVal);\n+            valVec = insert_element(vecTy, valVec, currVal, idx_val(v));\n+          }\n+          store(valVec, ptr);\n+        } else {\n+          Value valVec = load(ptr);\n+          for (unsigned v = 0; v < vec; ++v) {\n+            Value currVal = extract_element(llvmElemTy, valVec, idx_val(v));\n+            if (isInt1)\n+              currVal =\n+                  icmp_ne(currVal, rewriter.create<LLVM::ConstantOp>(\n+                                       loc, i8_ty, rewriter.getI8IntegerAttr(0)));\n+            else if (isPtr)\n+              currVal = inttoptr(llvmElemTyOrig, currVal);\n+            vals[elemId + linearCTAId * accumSizePerThread + v] = currVal;\n+          }\n         }\n       }\n     }\n   }\n-}\n \n-LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n-    triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n-    ConversionPatternRewriter &rewriter) const {\n-  auto loc = op.getLoc();\n-  Value src = op.src();\n-  Value dst = op.result();\n-  auto srcTy = src.getType().cast<RankedTensorType>();\n-  auto dstTy = dst.getType().cast<RankedTensorType>();\n-  Attribute srcLayout = srcTy.getEncoding();\n-  Attribute dstLayout = dstTy.getEncoding();\n-  auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n-  Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n-  auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n-  smemBase = bitcast(smemBase, elemPtrTy);\n-  auto shape = dstTy.getShape();\n-  unsigned rank = dstTy.getRank();\n-  SmallVector<unsigned> numReplicates(rank);\n-  SmallVector<unsigned> inNumCTAsEachRep(rank);\n-  SmallVector<unsigned> outNumCTAsEachRep(rank);\n-  SmallVector<unsigned> inNumCTAs(rank);\n-  SmallVector<unsigned> outNumCTAs(rank);\n-  auto srcShapePerCTA = getShapePerCTA(srcLayout);\n-  auto dstShapePerCTA = getShapePerCTA(dstLayout);\n-  for (unsigned d = 0; d < rank; ++d) {\n-    unsigned inPerCTA = std::min<unsigned>(shape[d], srcShapePerCTA[d]);\n-    unsigned outPerCTA = std::min<unsigned>(shape[d], dstShapePerCTA[d]);\n-    unsigned maxPerCTA = std::max(inPerCTA, outPerCTA);\n-    numReplicates[d] = ceil<unsigned>(shape[d], maxPerCTA);\n-    inNumCTAsEachRep[d] = maxPerCTA / inPerCTA;\n-    outNumCTAsEachRep[d] = maxPerCTA / outPerCTA;\n-    assert(maxPerCTA % inPerCTA == 0 && maxPerCTA % outPerCTA == 0);\n-    inNumCTAs[d] = ceil<unsigned>(shape[d], inPerCTA);\n-    outNumCTAs[d] = ceil<unsigned>(shape[d], outPerCTA);\n-  }\n-  // Potentially we need to store for multiple CTAs in this replication\n-  auto accumNumReplicates = product<unsigned>(numReplicates);\n-  // unsigned elems = getElemsPerThread(srcTy);\n-  auto vals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n-  unsigned inVec = 0;\n-  unsigned outVec = 0;\n-  auto paddedRepShape = getScratchConfigForCvtLayout(op, inVec, outVec);\n-\n-  unsigned outElems = getElemsPerThread(dstTy);\n-  auto outOrd = getOrder(dstLayout);\n-  SmallVector<Value> outVals(outElems);\n-\n-  for (unsigned repId = 0; repId < accumNumReplicates; ++repId) {\n-    auto multiDimRepId =\n-        getMultiDimIndex<unsigned>(repId, numReplicates, outOrd);\n-    if (repId != 0)\n-      barrier();\n-    if (srcLayout.isa<BlockedEncodingAttr>() ||\n-        srcLayout.isa<SliceEncodingAttr>() ||\n-        srcLayout.isa<MmaEncodingAttr>()) {\n-      processReplica(loc, rewriter, /*stNotRd*/ true, srcTy, inNumCTAsEachRep,\n-                     multiDimRepId, inVec, paddedRepShape, outOrd, vals,\n-                     smemBase);\n-    } else {\n-      assert(0 && \"ConvertLayout with input layout not implemented\");\n-      return failure();\n+  // blocked/mma -> blocked/mma.\n+  // Data padding in shared memory to avoid bank conflict.\n+  LogicalResult\n+  lowerDistributedToDistributed(triton::gpu::ConvertLayoutOp op,\n+                                OpAdaptor adaptor,\n+                                ConversionPatternRewriter &rewriter) const {\n+    auto loc = op.getLoc();\n+    Value src = op.src();\n+    Value dst = op.result();\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    Attribute srcLayout = srcTy.getEncoding();\n+    Attribute dstLayout = dstTy.getEncoding();\n+    auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n+    Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n+    auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n+    smemBase = bitcast(smemBase, elemPtrTy);\n+    auto shape = dstTy.getShape();\n+    unsigned rank = dstTy.getRank();\n+    SmallVector<unsigned> numReplicates(rank);\n+    SmallVector<unsigned> inNumCTAsEachRep(rank);\n+    SmallVector<unsigned> outNumCTAsEachRep(rank);\n+    SmallVector<unsigned> inNumCTAs(rank);\n+    SmallVector<unsigned> outNumCTAs(rank);\n+    auto srcShapePerCTA = getShapePerCTA(srcLayout);\n+    auto dstShapePerCTA = getShapePerCTA(dstLayout);\n+    for (unsigned d = 0; d < rank; ++d) {\n+      unsigned inPerCTA = std::min<unsigned>(shape[d], srcShapePerCTA[d]);\n+      unsigned outPerCTA = std::min<unsigned>(shape[d], dstShapePerCTA[d]);\n+      unsigned maxPerCTA = std::max(inPerCTA, outPerCTA);\n+      numReplicates[d] = ceil<unsigned>(shape[d], maxPerCTA);\n+      inNumCTAsEachRep[d] = maxPerCTA / inPerCTA;\n+      outNumCTAsEachRep[d] = maxPerCTA / outPerCTA;\n+      assert(maxPerCTA % inPerCTA == 0 && maxPerCTA % outPerCTA == 0);\n+      inNumCTAs[d] = ceil<unsigned>(shape[d], inPerCTA);\n+      outNumCTAs[d] = ceil<unsigned>(shape[d], outPerCTA);\n     }\n-    barrier();\n-    if (dstLayout.isa<BlockedEncodingAttr>() ||\n-        dstLayout.isa<SliceEncodingAttr>() ||\n-        dstLayout.isa<MmaEncodingAttr>()) {\n-      processReplica(loc, rewriter, /*stNotRd*/ false, dstTy, outNumCTAsEachRep,\n-                     multiDimRepId, outVec, paddedRepShape, outOrd, outVals,\n-                     smemBase);\n-    } else {\n-      assert(0 && \"ConvertLayout with output layout not implemented\");\n-      return failure();\n+    // Potentially we need to store for multiple CTAs in this replication\n+    auto accumNumReplicates = product<unsigned>(numReplicates);\n+    // unsigned elems = getElemsPerThread(srcTy);\n+    auto vals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n+    unsigned inVec = 0;\n+    unsigned outVec = 0;\n+    auto paddedRepShape = getScratchConfigForCvtLayout(op, inVec, outVec);\n+\n+    unsigned outElems = getElemsPerThread(dstTy);\n+    auto outOrd = getOrder(dstLayout);\n+    SmallVector<Value> outVals(outElems);\n+\n+    for (unsigned repId = 0; repId < accumNumReplicates; ++repId) {\n+      auto multiDimRepId =\n+          getMultiDimIndex<unsigned>(repId, numReplicates, outOrd);\n+      if (repId != 0)\n+        barrier();\n+      if (srcLayout.isa<BlockedEncodingAttr>() ||\n+          srcLayout.isa<SliceEncodingAttr>() ||\n+          srcLayout.isa<MmaEncodingAttr>()) {\n+        processReplica(loc, rewriter, /*stNotRd*/ true, srcTy, inNumCTAsEachRep,\n+                       multiDimRepId, inVec, paddedRepShape, outOrd, vals,\n+                       smemBase);\n+      } else {\n+        assert(0 && \"ConvertLayout with input layout not implemented\");\n+        return failure();\n+      }\n+      barrier();\n+      if (dstLayout.isa<BlockedEncodingAttr>() ||\n+          dstLayout.isa<SliceEncodingAttr>() ||\n+          dstLayout.isa<MmaEncodingAttr>()) {\n+        processReplica(loc, rewriter, /*stNotRd*/ false, dstTy, outNumCTAsEachRep,\n+                       multiDimRepId, outVec, paddedRepShape, outOrd, outVals,\n+                       smemBase);\n+      } else {\n+        assert(0 && \"ConvertLayout with output layout not implemented\");\n+        return failure();\n+      }\n     }\n-  }\n \n-  SmallVector<Type> types(outElems, llvmElemTy);\n-  auto *ctx = llvmElemTy.getContext();\n-  Type structTy = struct_ty(types);\n-  Value result = getStructFromElements(loc, outVals, rewriter, structTy);\n-  rewriter.replaceOp(op, result);\n+    SmallVector<Type> types(outElems, llvmElemTy);\n+    auto *ctx = llvmElemTy.getContext();\n+    Type structTy = struct_ty(types);\n+    Value result = getStructFromElements(loc, outVals, rewriter, structTy);\n+    rewriter.replaceOp(op, result);\n \n-  return success();\n-}\n+    return success();\n+  }\n \n-LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n-    triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n-    ConversionPatternRewriter &rewriter) const {\n-  auto loc = op.getLoc();\n-  Value src = op.src();\n-  Value dst = op.result();\n-  auto srcTy = src.getType().cast<RankedTensorType>();\n-  auto srcShape = srcTy.getShape();\n-  auto dstTy = dst.getType().cast<RankedTensorType>();\n-  auto dstShape = dstTy.getShape();\n-  assert(srcShape.size() == 2 &&\n-         \"Unexpected rank of ConvertLayout(blocked->shared)\");\n-  auto srcBlockedLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n-  auto dstSharedLayout = dstTy.getEncoding().cast<SharedEncodingAttr>();\n-  auto inOrd = srcBlockedLayout.getOrder();\n-  auto outOrd = dstSharedLayout.getOrder();\n-  Value smemBase = getSharedMemoryBase(loc, rewriter, dst);\n-  auto elemTy = getTypeConverter()->convertType(srcTy.getElementType());\n-  auto elemPtrTy = ptr_ty(getTypeConverter()->convertType(elemTy), 3);\n-  smemBase = bitcast(smemBase, elemPtrTy);\n-\n-  auto srcStrides = getStridesFromShapeAndOrder(srcShape, inOrd, loc, rewriter);\n-  auto srcIndices =\n-      emitBaseIndexForBlockedLayout(loc, rewriter, srcBlockedLayout, srcShape);\n-  storeBlockedToShared(src, adaptor.src(), srcStrides, srcIndices, dst,\n-                       smemBase, elemPtrTy, loc, rewriter);\n-\n-  auto smemObj = SharedMemoryObject(smemBase, dstShape, outOrd, loc, rewriter);\n-  auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n-  rewriter.replaceOp(op, retVal);\n-  return success();\n-}\n+  // blocked -> shared.\n+  // Swizzling in shared memory to avoid bank conflict. Normally used for\n+  // A/B operands of dots.\n+  LogicalResult lowerBlockedToShared(triton::gpu::ConvertLayoutOp op,\n+                                     OpAdaptor adaptor,\n+                                     ConversionPatternRewriter &rewriter) const {\n+    auto loc = op.getLoc();\n+    Value src = op.src();\n+    Value dst = op.result();\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto srcShape = srcTy.getShape();\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    auto dstShape = dstTy.getShape();\n+    assert(srcShape.size() == 2 &&\n+           \"Unexpected rank of ConvertLayout(blocked->shared)\");\n+    auto srcBlockedLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+    auto dstSharedLayout = dstTy.getEncoding().cast<SharedEncodingAttr>();\n+    auto inOrd = srcBlockedLayout.getOrder();\n+    auto outOrd = dstSharedLayout.getOrder();\n+    Value smemBase = getSharedMemoryBase(loc, rewriter, dst);\n+    auto elemTy = getTypeConverter()->convertType(srcTy.getElementType());\n+    auto elemPtrTy = ptr_ty(getTypeConverter()->convertType(elemTy), 3);\n+    smemBase = bitcast(smemBase, elemPtrTy);\n+\n+    auto srcStrides = getStridesFromShapeAndOrder(srcShape, inOrd, loc, rewriter);\n+    auto srcIndices =\n+        emitBaseIndexForBlockedLayout(loc, rewriter, srcBlockedLayout, srcShape);\n+    storeBlockedToShared(src, adaptor.src(), srcStrides, srcIndices, dst,\n+                         smemBase, elemPtrTy, loc, rewriter);\n+\n+    auto smemObj = SharedMemoryObject(smemBase, dstShape, outOrd, loc, rewriter);\n+    auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n+    rewriter.replaceOp(op, retVal);\n+    return success();\n+  }\n \n-LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n-    triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n-    ConversionPatternRewriter &rewriter) const {\n-  auto loc = op.getLoc();\n-  Value src = op.src();\n-  Value dst = op.result();\n-  auto dstTensorTy = dst.getType().cast<RankedTensorType>();\n-  auto srcTensorTy = src.getType().cast<RankedTensorType>();\n-  auto dotOperandLayout =\n-      dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n-  auto sharedLayout = srcTensorTy.getEncoding().cast<SharedEncodingAttr>();\n-\n-  bool isOuter{};\n-  int K{};\n-  if (dotOperandLayout.getOpIdx() == 0) // $a\n-    K = dstTensorTy.getShape()[sharedLayout.getOrder()[0]];\n-  else // $b\n-    K = dstTensorTy.getShape()[sharedLayout.getOrder()[1]];\n-  isOuter = K == 1;\n-\n-  Value res;\n-  if (auto mmaLayout =\n-          dotOperandLayout.getParent().dyn_cast_or_null<MmaEncodingAttr>()) {\n-    res = lowerSharedToDotOperandMMA(op, adaptor, rewriter, mmaLayout,\n-                                     dotOperandLayout, isOuter);\n-  } else if (auto blockedLayout =\n-                 dotOperandLayout.getParent()\n-                     .dyn_cast_or_null<BlockedEncodingAttr>()) {\n-    auto dotOpLayout = dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n-    DotOpFMAConversionHelper helper(blockedLayout);\n-    auto thread = getThreadId(rewriter, loc);\n-    if (dotOpLayout.getOpIdx() == 0) { // $a\n-      res = helper.loadA(src, adaptor.src(), blockedLayout, thread, loc,\n-                         rewriter);\n-    } else { // $b\n-      res = helper.loadB(src, adaptor.src(), blockedLayout, thread, loc,\n-                         rewriter);\n+  // shared -> mma_operand\n+  LogicalResult\n+  lowerSharedToDotOperand(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+                          ConversionPatternRewriter &rewriter) const {\n+    auto loc = op.getLoc();\n+    Value src = op.src();\n+    Value dst = op.result();\n+    auto dstTensorTy = dst.getType().cast<RankedTensorType>();\n+    auto srcTensorTy = src.getType().cast<RankedTensorType>();\n+    auto dotOperandLayout =\n+        dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n+    auto sharedLayout = srcTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+\n+    bool isOuter{};\n+    int K{};\n+    if (dotOperandLayout.getOpIdx() == 0) // $a\n+      K = dstTensorTy.getShape()[sharedLayout.getOrder()[0]];\n+    else // $b\n+      K = dstTensorTy.getShape()[sharedLayout.getOrder()[1]];\n+    isOuter = K == 1;\n+\n+    Value res;\n+    if (auto mmaLayout =\n+            dotOperandLayout.getParent().dyn_cast_or_null<MmaEncodingAttr>()) {\n+      res = lowerSharedToDotOperandMMA(op, adaptor, rewriter, mmaLayout,\n+                                       dotOperandLayout, isOuter);\n+    } else if (auto blockedLayout =\n+                   dotOperandLayout.getParent()\n+                       .dyn_cast_or_null<BlockedEncodingAttr>()) {\n+      auto dotOpLayout = dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n+      DotOpFMAConversionHelper helper(blockedLayout);\n+      auto thread = getThreadId(rewriter, loc);\n+      if (dotOpLayout.getOpIdx() == 0) { // $a\n+        res = helper.loadA(src, adaptor.src(), blockedLayout, thread, loc,\n+                           rewriter);\n+      } else { // $b\n+        res = helper.loadB(src, adaptor.src(), blockedLayout, thread, loc,\n+                           rewriter);\n+      }\n+    } else {\n+      assert(false && \"Unsupported dot operand layout found\");\n     }\n-  } else {\n-    assert(false && \"Unsupported dot operand layout found\");\n-  }\n \n-  rewriter.replaceOp(op, res);\n-  return success();\n-}\n+    rewriter.replaceOp(op, res);\n+    return success();\n+  }\n \n-LogicalResult ConvertLayoutOpConversion::lowerMmaToDotOperand(\n-    triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n-    ConversionPatternRewriter &rewriter) const {\n-  auto loc = op.getLoc();\n-  auto srcTy = op.src().getType().cast<RankedTensorType>();\n-  auto dstTy = op.result().getType().cast<RankedTensorType>();\n-  auto srcLayout = srcTy.getEncoding();\n-  auto dstLayout = dstTy.getEncoding();\n-  auto srcMmaLayout = srcLayout.cast<MmaEncodingAttr>();\n-  auto dstDotLayout = dstLayout.cast<DotOperandEncodingAttr>();\n-  if (isMmaToDotShortcut(srcMmaLayout, dstDotLayout)) {\n-    // get source values\n-    auto vals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n-    unsigned elems = getElemsPerThread(srcTy);\n-    Type elemTy = this->getTypeConverter()->convertType(srcTy.getElementType());\n-    // for the destination type, we need to pack values together\n-    // so they can be consumed by tensor core operations\n-    unsigned vecSize =\n-        std::max<unsigned>(32 / elemTy.getIntOrFloatBitWidth(), 1);\n-    Type vecTy = vec_ty(elemTy, vecSize);\n-    SmallVector<Type> types(elems / vecSize, vecTy);\n-    SmallVector<Value> vecVals;\n-    for (unsigned i = 0; i < elems; i += vecSize) {\n-      Value packed = rewriter.create<LLVM::UndefOp>(loc, vecTy);\n-      for (unsigned j = 0; j < vecSize; j++)\n-        packed = insert_element(vecTy, packed, vals[i + j], i32_val(j));\n-      vecVals.push_back(packed);\n-    }\n+  // mma -> dot_operand\n+  LogicalResult lowerMmaToDotOperand(triton::gpu::ConvertLayoutOp op,\n+                                     OpAdaptor adaptor,\n+                                     ConversionPatternRewriter &rewriter) const {\n+    auto loc = op.getLoc();\n+    auto srcTy = op.src().getType().cast<RankedTensorType>();\n+    auto dstTy = op.result().getType().cast<RankedTensorType>();\n+    auto srcLayout = srcTy.getEncoding();\n+    auto dstLayout = dstTy.getEncoding();\n+    auto srcMmaLayout = srcLayout.cast<MmaEncodingAttr>();\n+    auto dstDotLayout = dstLayout.cast<DotOperandEncodingAttr>();\n+    if (isMmaToDotShortcut(srcMmaLayout, dstDotLayout)) {\n+      // get source values\n+      auto vals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n+      unsigned elems = getElemsPerThread(srcTy);\n+      Type elemTy = this->getTypeConverter()->convertType(srcTy.getElementType());\n+      // for the destination type, we need to pack values together\n+      // so they can be consumed by tensor core operations\n+      unsigned vecSize =\n+          std::max<unsigned>(32 / elemTy.getIntOrFloatBitWidth(), 1);\n+      Type vecTy = vec_ty(elemTy, vecSize);\n+      SmallVector<Type> types(elems / vecSize, vecTy);\n+      SmallVector<Value> vecVals;\n+      for (unsigned i = 0; i < elems; i += vecSize) {\n+        Value packed = rewriter.create<LLVM::UndefOp>(loc, vecTy);\n+        for (unsigned j = 0; j < vecSize; j++)\n+          packed = insert_element(vecTy, packed, vals[i + j], i32_val(j));\n+        vecVals.push_back(packed);\n+      }\n \n-    // This needs to be ordered the same way that\n-    // ldmatrix.x4 would order it\n-    // TODO: this needs to be refactor so we don't\n-    // implicitly depends on how emitOffsetsForMMAV2\n-    // is implemented\n-    SmallVector<Value> reorderedVals;\n-    for (unsigned i = 0; i < vecVals.size(); i += 4) {\n-      reorderedVals.push_back(vecVals[i]);\n-      reorderedVals.push_back(vecVals[i + 2]);\n-      reorderedVals.push_back(vecVals[i + 1]);\n-      reorderedVals.push_back(vecVals[i + 3]);\n-    }\n+      // This needs to be ordered the same way that\n+      // ldmatrix.x4 would order it\n+      // TODO: this needs to be refactor so we don't\n+      // implicitly depends on how emitOffsetsForMMAV2\n+      // is implemented\n+      SmallVector<Value> reorderedVals;\n+      for (unsigned i = 0; i < vecVals.size(); i += 4) {\n+        reorderedVals.push_back(vecVals[i]);\n+        reorderedVals.push_back(vecVals[i + 2]);\n+        reorderedVals.push_back(vecVals[i + 1]);\n+        reorderedVals.push_back(vecVals[i + 3]);\n+      }\n \n-    // return composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK);\n+      // return composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK);\n \n-    Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n-    Value view = getStructFromElements(loc, reorderedVals, rewriter, structTy);\n-    rewriter.replaceOp(op, view);\n-    return success();\n+      Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n+      Value view = getStructFromElements(loc, reorderedVals, rewriter, structTy);\n+      rewriter.replaceOp(op, view);\n+      return success();\n+    }\n+    return failure();\n   }\n-  return failure();\n-}\n \n-Value ConvertLayoutOpConversion::lowerSharedToDotOperandMMA(\n-    triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n-    ConversionPatternRewriter &rewriter, const MmaEncodingAttr &mmaLayout,\n-    const DotOperandEncodingAttr &dotOperandLayout, bool isOuter) const {\n-  auto loc = op.getLoc();\n-  Value src = op.src();\n-  Value dst = op.result();\n-  bool isHMMA = supportMMA(dst, mmaLayout.getVersionMajor());\n-\n-  auto smemObj = getSharedMemoryObjectFromStruct(loc, adaptor.src(), rewriter);\n-  Value res;\n-\n-  if (!isOuter && mmaLayout.isAmpere() && isHMMA) { // tensor core v2\n-    MMA16816ConversionHelper mmaHelper(src.getType(), mmaLayout,\n-                                       getThreadId(rewriter, loc), rewriter,\n-                                       getTypeConverter(), op.getLoc());\n-\n-    if (dotOperandLayout.getOpIdx() == 0) {\n-      // operand $a\n-      res = mmaHelper.loadA(src, smemObj);\n-    } else if (dotOperandLayout.getOpIdx() == 1) {\n-      // operand $b\n-      res = mmaHelper.loadB(src, smemObj);\n-    }\n-  } else if (!isOuter && mmaLayout.isVolta() && isHMMA) { // tensor core v1\n-    DotOpMmaV1ConversionHelper helper(mmaLayout);\n-    bool isMMAv1Row =\n-        dotOperandLayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n-    auto srcSharedLayout = src.getType()\n-                               .cast<RankedTensorType>()\n-                               .getEncoding()\n-                               .cast<SharedEncodingAttr>();\n-\n-    // Can only convert [1, 0] to row or [0, 1] to col for now\n-    if ((srcSharedLayout.getOrder()[0] == 1 && !isMMAv1Row) ||\n-        (srcSharedLayout.getOrder()[0] == 0 && isMMAv1Row)) {\n-      llvm::errs() << \"Unsupported Shared -> DotOperand[MMAv1] conversion\\n\";\n-      return Value();\n-    }\n+  // shared -> dot_operand if the result layout is mma\n+  Value lowerSharedToDotOperandMMA(\n+      triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+      ConversionPatternRewriter &rewriter, const MmaEncodingAttr &mmaLayout,\n+      const DotOperandEncodingAttr &dotOperandLayout, bool isOuter) const {\n+    auto loc = op.getLoc();\n+    Value src = op.src();\n+    Value dst = op.result();\n+    bool isHMMA = supportMMA(dst, mmaLayout.getVersionMajor());\n+\n+    auto smemObj = getSharedMemoryObjectFromStruct(loc, adaptor.src(), rewriter);\n+    Value res;\n+\n+    if (!isOuter && mmaLayout.isAmpere() && isHMMA) { // tensor core v2\n+      MMA16816ConversionHelper mmaHelper(src.getType(), mmaLayout,\n+                                         getThreadId(rewriter, loc), rewriter,\n+                                         getTypeConverter(), op.getLoc());\n+\n+      if (dotOperandLayout.getOpIdx() == 0) {\n+        // operand $a\n+        res = mmaHelper.loadA(src, smemObj);\n+      } else if (dotOperandLayout.getOpIdx() == 1) {\n+        // operand $b\n+        res = mmaHelper.loadB(src, smemObj);\n+      }\n+    } else if (!isOuter && mmaLayout.isVolta() && isHMMA) { // tensor core v1\n+      DotOpMmaV1ConversionHelper helper(mmaLayout);\n+      bool isMMAv1Row =\n+          dotOperandLayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+      auto srcSharedLayout = src.getType()\n+                                 .cast<RankedTensorType>()\n+                                 .getEncoding()\n+                                 .cast<SharedEncodingAttr>();\n+\n+      // Can only convert [1, 0] to row or [0, 1] to col for now\n+      if ((srcSharedLayout.getOrder()[0] == 1 && !isMMAv1Row) ||\n+          (srcSharedLayout.getOrder()[0] == 0 && isMMAv1Row)) {\n+        llvm::errs() << \"Unsupported Shared -> DotOperand[MMAv1] conversion\\n\";\n+        return Value();\n+      }\n \n-    if (dotOperandLayout.getOpIdx() == 0) { // operand $a\n-      // TODO[Superjomn]: transA is not available here.\n-      bool transA = false;\n-      res = helper.loadA(src, transA, smemObj, getThreadId(rewriter, loc), loc,\n-                         rewriter);\n-    } else if (dotOperandLayout.getOpIdx() == 1) { // operand $b\n-      // TODO[Superjomn]: transB is not available here.\n-      bool transB = false;\n-      res = helper.loadB(src, transB, smemObj, getThreadId(rewriter, loc), loc,\n-                         rewriter);\n+      if (dotOperandLayout.getOpIdx() == 0) { // operand $a\n+        // TODO[Superjomn]: transA is not available here.\n+        bool transA = false;\n+        res = helper.loadA(src, transA, smemObj, getThreadId(rewriter, loc), loc,\n+                           rewriter);\n+      } else if (dotOperandLayout.getOpIdx() == 1) { // operand $b\n+        // TODO[Superjomn]: transB is not available here.\n+        bool transB = false;\n+        res = helper.loadB(src, transB, smemObj, getThreadId(rewriter, loc), loc,\n+                           rewriter);\n+      }\n+    } else {\n+      assert(false && \"Unsupported mma layout found\");\n     }\n-  } else {\n-    assert(false && \"Unsupported mma layout found\");\n+    return res;\n   }\n-  return res;\n-}\n+};\n \n void populateConvertLayoutOpToLLVMPatterns(\n     mlir::LLVMTypeConverter &typeConverter,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.h", "status": "modified", "additions": 8, "deletions": 71, "changes": 79, "file_content_changes": "@@ -8,77 +8,14 @@ using namespace mlir::triton;\n \n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n \n-struct ConvertLayoutOpConversion\n-    : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::ConvertLayoutOp> {\n-public:\n-  using ConvertTritonGPUOpToLLVMPattern<\n-      triton::gpu::ConvertLayoutOp>::ConvertTritonGPUOpToLLVMPattern;\n-\n-  LogicalResult\n-  matchAndRewrite(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override;\n-\n-  static bool isMmaToDotShortcut(MmaEncodingAttr &mmaLayout,\n-                                 DotOperandEncodingAttr &dotOperandLayout) {\n-    // dot_op<opIdx=0, parent=#mma> = #mma\n-    // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>\n-    return mmaLayout.getWarpsPerCTA()[1] == 1 &&\n-           dotOperandLayout.getOpIdx() == 0 &&\n-           dotOperandLayout.getParent() == mmaLayout;\n-  }\n-\n-  static void storeBlockedToShared(Value src, Value llSrc,\n-                                   ArrayRef<Value> srcStrides,\n-                                   ArrayRef<Value> srcIndices, Value dst,\n-                                   Value smemBase, Type elemPtrTy, Location loc,\n-                                   ConversionPatternRewriter &rewriter);\n-\n-private:\n-  SmallVector<Value> getMultiDimOffset(Attribute layout, Location loc,\n-                                       ConversionPatternRewriter &rewriter,\n-                                       unsigned elemId, ArrayRef<int64_t> shape,\n-                                       ArrayRef<unsigned> multiDimCTAInRepId,\n-                                       ArrayRef<unsigned> shapePerCTA) const;\n-\n-  // shared memory rd/st for blocked or mma layout with data padding\n-  void processReplica(Location loc, ConversionPatternRewriter &rewriter,\n-                      bool stNotRd, RankedTensorType type,\n-                      ArrayRef<unsigned> numCTAsEachRep,\n-                      ArrayRef<unsigned> multiDimRepId, unsigned vec,\n-                      ArrayRef<unsigned> paddedRepShape,\n-                      ArrayRef<unsigned> outOrd, SmallVector<Value> &vals,\n-                      Value smemBase) const;\n-\n-  // blocked/mma -> blocked/mma.\n-  // Data padding in shared memory to avoid bank conflict.\n-  LogicalResult\n-  lowerDistributedToDistributed(triton::gpu::ConvertLayoutOp op,\n-                                OpAdaptor adaptor,\n-                                ConversionPatternRewriter &rewriter) const;\n-\n-  // blocked -> shared.\n-  // Swizzling in shared memory to avoid bank conflict. Normally used for\n-  // A/B operands of dots.\n-  LogicalResult lowerBlockedToShared(triton::gpu::ConvertLayoutOp op,\n-                                     OpAdaptor adaptor,\n-                                     ConversionPatternRewriter &rewriter) const;\n-\n-  // shared -> mma_operand\n-  LogicalResult\n-  lowerSharedToDotOperand(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n-                          ConversionPatternRewriter &rewriter) const;\n-\n-  // mma -> dot_operand\n-  LogicalResult lowerMmaToDotOperand(triton::gpu::ConvertLayoutOp op,\n-                                     OpAdaptor adaptor,\n-                                     ConversionPatternRewriter &rewriter) const;\n-\n-  // shared -> dot_operand if the result layout is mma\n-  Value lowerSharedToDotOperandMMA(\n-      triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n-      ConversionPatternRewriter &rewriter, const MmaEncodingAttr &mmaLayout,\n-      const DotOperandEncodingAttr &dotOperandLayout, bool isOuter) const;\n-};\n+bool isMmaToDotShortcut(MmaEncodingAttr &mmaLayout,\n+                        DotOperandEncodingAttr &dotOperandLayout);\n+\n+void storeBlockedToShared(Value src, Value llSrc,\n+                          ArrayRef<Value> srcStrides,\n+                          ArrayRef<Value> srcIndices, Value dst,\n+                          Value smemBase, Type elemPtrTy, Location loc,\n+                          ConversionPatternRewriter &rewriter);\n \n void populateConvertLayoutOpToLLVMPatterns(\n     mlir::LLVMTypeConverter &typeConverter,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -641,9 +641,9 @@ struct InsertSliceOpConversion\n     auto llSrc = adaptor.source();\n     auto srcIndices =\n         emitBaseIndexForBlockedLayout(loc, rewriter, srcLayout, srcShape);\n-    ConvertLayoutOpConversion::storeBlockedToShared(src, llSrc, srcStrides,\n-                                                    srcIndices, dst, smemBase,\n-                                                    elemPtrTy, loc, rewriter);\n+    storeBlockedToShared(src, llSrc, srcStrides,\n+                         srcIndices, dst, smemBase,\n+                         elemPtrTy, loc, rewriter);\n     // Barrier is not necessary.\n     // The membar pass knows that it writes to shared memory and will handle it\n     // properly."}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -251,8 +251,7 @@ class ConvertTritonGPUToLLVM\n           srcType.getEncoding().dyn_cast<triton::gpu::MmaEncodingAttr>();\n       auto dstDotOp =\n           dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n-      if (srcMma && dstDotOp &&\n-          !ConvertLayoutOpConversion::isMmaToDotShortcut(srcMma, dstDotOp)) {\n+      if (srcMma && dstDotOp && !isMmaToDotShortcut(srcMma, dstDotOp)) {\n         auto tmpType = RankedTensorType::get(\n             dstType.getShape(), dstType.getElementType(),\n             triton::gpu::BlockedEncodingAttr::get("}]