[{"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 155, "deletions": 86, "changes": 241, "file_content_changes": "@@ -25,68 +25,113 @@ def _fwd_kernel(\n     Z, H, N_CTX,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n+    MODE: tl.constexpr,\n ):\n     start_m = tl.program_id(0)\n     off_hz = tl.program_id(1)\n+    qvk_offset = off_hz * stride_qh\n+    Q_block_ptr = tl.make_block_ptr(\n+        base=Q + qvk_offset,\n+        shape=(N_CTX, BLOCK_DMODEL),\n+        strides=(stride_qm, stride_qk),\n+        offsets=(start_m * BLOCK_M, 0),\n+        block_shape=(BLOCK_M, BLOCK_DMODEL),\n+        order=(1, 0)\n+    )\n+    K_block_ptr = tl.make_block_ptr(\n+        base=K + qvk_offset,\n+        shape=(BLOCK_DMODEL, N_CTX),\n+        strides=(stride_kk, stride_kn),\n+        offsets=(0, 0),\n+        block_shape=(BLOCK_DMODEL, BLOCK_N),\n+        order=(0, 1)\n+    )\n+    V_block_ptr = tl.make_block_ptr(\n+        base=V + qvk_offset,\n+        shape=(N_CTX, BLOCK_DMODEL),\n+        strides=(stride_vk, stride_vn),\n+        offsets=(0, 0),\n+        block_shape=(BLOCK_N, BLOCK_DMODEL),\n+        order=(1, 0)\n+    )\n+    O_block_ptr = tl.make_block_ptr(\n+        base=Out + qvk_offset,\n+        shape=(N_CTX, BLOCK_DMODEL),\n+        strides=(stride_om, stride_on),\n+        offsets=(start_m * BLOCK_M, 0),\n+        block_shape=(BLOCK_M, BLOCK_DMODEL),\n+        order=(1, 0)\n+    )\n     # initialize offsets\n     offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n     offs_n = tl.arange(0, BLOCK_N)\n-    offs_d = tl.arange(0, BLOCK_DMODEL)\n-    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n-    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    # Initialize pointers to Q, K, V\n-    q_ptrs = Q + off_q\n-    k_ptrs = K + off_k\n-    v_ptrs = V + off_v\n     # initialize pointer to m and l\n-    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n-    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n+    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n     acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+    # causal check on every loop iteration can be expensive\n+    # and peeling the last iteration of the loop does not work well with ptxas\n+    # so we have a mode to do the causal check in a separate kernel entirely\n+    if MODE == 0:  # entire non-causal attention\n+        lo, hi = 0, N_CTX\n+    if MODE == 1:  # entire causal attention\n+        lo, hi = 0, (start_m + 1) * BLOCK_M\n+    if MODE == 2:  # off band-diagonal\n+        lo, hi = 0, start_m * BLOCK_M\n+    if MODE == 3:  # on band-diagonal\n+        l_ptrs = L + off_hz * N_CTX + offs_m\n+        m_ptrs = M + off_hz * N_CTX + offs_m\n+        m_i = tl.load(m_ptrs)\n+        l_i = tl.load(l_ptrs)\n+        acc += tl.load(O_block_ptr).to(tl.float32)\n+        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n+    # credits to: Adam P. Goucher (https://github.com/apgoucher):\n+    # scale sm_scale by 1/log_2(e) and use\n+    # 2^x instead of exp in the loop because CSE and LICM\n+    # don't work as expected with `exp` in the loop\n+    qk_scale = sm_scale * 1.44269504\n     # load q: it will stay in SRAM throughout\n-    q = tl.load(q_ptrs)\n+    q = tl.load(Q_block_ptr)\n+    q = (q * qk_scale).to(tl.float16)\n     # loop over k, v and update accumulator\n-    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n+    for start_n in range(lo, hi, BLOCK_N):\n+        start_n = tl.multiple_of(start_n, BLOCK_N)\n         # -- compute qk ----\n-        k = tl.load(k_ptrs)\n+        k = tl.load(tl.advance(K_block_ptr, (0, start_n)))\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n         qk += tl.dot(q, k)\n-        qk *= sm_scale\n-        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n-        # compute new m\n-        m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n-        # correct old l\n-        l_prev *= tl.exp(m_prev - m_curr)\n-        # attention weights\n-        p = tl.exp(qk - m_curr[:, None])\n-        l_curr = tl.sum(p, 1) + l_prev\n-        # rescale operands of matmuls\n-        l_rcp = 1. / l_curr\n-        p *= l_rcp[:, None]\n-        acc *= (l_prev * l_rcp)[:, None]\n+        if MODE == 1 or MODE == 3:\n+            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n+        # -- compute m_ij, p, l_ij\n+        m_ij = tl.max(qk, 1)\n+        p = tl.math.exp2(qk - m_ij[:, None])\n+        l_ij = tl.sum(p, 1)\n+        # -- update m_i and l_i\n+        m_i_new = tl.maximum(m_i, m_ij)\n+        alpha = tl.math.exp2(m_i - m_i_new)\n+        beta = tl.math.exp2(m_ij - m_i_new)\n+        l_i *= alpha\n+        l_i_new = l_i + beta * l_ij\n+        # scale p\n+        p_scale = beta / l_i_new\n+        p = p * p_scale[:, None]\n+        # scale acc\n+        acc_scale = l_i / l_i_new\n+        acc = acc * acc_scale[:, None]\n         # update acc\n-        p = p.to(Q.dtype.element_ty)\n-        v = tl.load(v_ptrs)\n+        v = tl.load(tl.advance(V_block_ptr, (start_n, 0)))\n+        p = p.to(tl.float16)\n         acc += tl.dot(p, v)\n         # update m_i and l_i\n-        l_prev = l_curr\n-        m_prev = m_curr\n-        # update pointers\n-        k_ptrs += BLOCK_N * stride_kn\n-        v_ptrs += BLOCK_N * stride_vk\n-    # rematerialize offsets to save registers\n-    start_m = tl.program_id(0)\n-    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+        l_i = l_i_new\n+        m_i = m_i_new\n     # write back l and m\n     l_ptrs = L + off_hz * N_CTX + offs_m\n     m_ptrs = M + off_hz * N_CTX + offs_m\n-    tl.store(l_ptrs, l_prev)\n-    tl.store(m_ptrs, m_prev)\n-    # initialize pointers to output\n-    offs_n = tl.arange(0, BLOCK_DMODEL)\n-    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n-    out_ptrs = Out + off_o\n-    tl.store(out_ptrs, acc)\n+    tl.store(l_ptrs, l_i)\n+    tl.store(m_ptrs, m_i)\n+    # write back O\n+    tl.store(O_block_ptr, acc.to(tl.float16))\n \n \n @triton.jit\n@@ -122,10 +167,12 @@ def _bwd_kernel(\n     num_block,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n+    MODE: tl.constexpr,\n ):\n     off_hz = tl.program_id(0)\n     off_z = off_hz // H\n     off_h = off_hz % H\n+    qk_scale = sm_scale * 1.44269504\n     # offset pointers for batch/head\n     Q += off_z * stride_qz + off_h * stride_qh\n     K += off_z * stride_qz + off_h * stride_qh\n@@ -135,7 +182,10 @@ def _bwd_kernel(\n     DK += off_z * stride_qz + off_h * stride_qh\n     DV += off_z * stride_qz + off_h * stride_qh\n     for start_n in range(0, num_block):\n-        lo = start_n * BLOCK_M\n+        if MODE == 0:\n+            lo = 0\n+        else:\n+            lo = start_n * BLOCK_M\n         # initialize row/col offsets\n         offs_qm = lo + tl.arange(0, BLOCK_M)\n         offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n@@ -163,10 +213,15 @@ def _bwd_kernel(\n             q = tl.load(q_ptrs)\n             # recompute p = softmax(qk, dim=-1).T\n             # NOTE: `do` is pre-divided by `l`; no normalization here\n-            qk = tl.dot(q, tl.trans(k))\n-            qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\"-inf\"))\n+            # if MODE == 1:\n+            if MODE == 1:\n+                qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\"-inf\"))\n+            else:\n+                qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n+            qk += tl.dot(q, tl.trans(k))\n+            qk *= qk_scale\n             m = tl.load(m_ptrs + offs_m_curr)\n-            p = tl.exp(qk * sm_scale - m[:, None])\n+            p = tl.math.exp2(qk - m[:, None])\n             # compute dv\n             do = tl.load(do_ptrs)\n             dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\n@@ -199,37 +254,42 @@ def _bwd_kernel(\n class _attention(torch.autograd.Function):\n \n     @staticmethod\n-    def forward(ctx, q, k, v, sm_scale):\n+    def forward(ctx, q, k, v, causal, sm_scale):\n         BLOCK = 128\n         # shape constraints\n         Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n         assert Lq == Lk and Lk == Lv\n         assert Lk in {16, 32, 64, 128}\n         o = torch.empty_like(q)\n-        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n+        grid = (triton.cdiv(q.shape[2], 128), q.shape[0] * q.shape[1], 1)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n-        num_warps = 4 if Lk <= 64 else 8\n \n-        _fwd_kernel[grid](\n-            q, k, v, sm_scale,\n-            L, m,\n-            o,\n-            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n-            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n-            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n-            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n-            q.shape[0], q.shape[1], q.shape[2],\n-            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n-            BLOCK_DMODEL=Lk, num_warps=num_warps,\n-            num_stages=2,\n-        )\n-        # print(h.asm[\"ttgir\"])\n+        num_warps = 4 if Lk <= 64 else 8\n+        if causal:\n+            modes = [1] if q.shape[2] <= 2048 else [2, 3]\n+        else:\n+            modes = [0]\n+        for mode in modes:\n+            _fwd_kernel[grid](\n+                q, k, v, sm_scale,\n+                L, m,\n+                o,\n+                q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+                k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+                v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+                o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n+                q.shape[0], q.shape[1], q.shape[2],\n+                BLOCK_M=128, BLOCK_N=BLOCK, BLOCK_DMODEL=Lk,\n+                MODE=mode,\n+                num_warps=num_warps,\n+                num_stages=2)\n \n         ctx.save_for_backward(q, k, v, o, L, m)\n         ctx.grid = grid\n         ctx.sm_scale = sm_scale\n         ctx.BLOCK_DMODEL = Lk\n+        ctx.causal = causal\n         return o\n \n     @staticmethod\n@@ -242,6 +302,10 @@ def backward(ctx, do):\n         dv = torch.empty_like(v)\n         do_scaled = torch.empty_like(do)\n         delta = torch.empty_like(l)\n+        if ctx.causal:\n+            mode = 1\n+        else:\n+            mode = 0\n         _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n             o, do, l,\n             do_scaled, delta,\n@@ -260,40 +324,40 @@ def backward(ctx, do):\n             ctx.grid[0],\n             BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n             BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n+            MODE=mode,\n             num_stages=1,\n         )\n-        # print(h.asm[\"ttgir\"])\n-        return dq, dk, dv, None\n+        return dq, dk, dv, None, None\n \n \n attention = _attention.apply\n \n \n-@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n-def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n+@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(6, 9, 1024, 64)])\n+@pytest.mark.parametrize('causal', [False, True])\n+def test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\n     torch.manual_seed(20)\n-    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n-    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n-    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n-    sm_scale = 0.2\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    sm_scale = 0.5\n     dout = torch.randn_like(q)\n     # reference implementation\n     M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n     p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n-    for z in range(Z):\n-        for h in range(H):\n-            p[:, :, M == 0] = float(\"-inf\")\n+    if causal:\n+        for z in range(Z):\n+            for h in range(H):\n+                p[:, :, M == 0] = float(\"-inf\")\n     p = torch.softmax(p.float(), dim=-1).half()\n     # p = torch.exp(p)\n     ref_out = torch.matmul(p, v)\n     ref_out.backward(dout)\n     ref_dv, v.grad = v.grad.clone(), None\n     ref_dk, k.grad = k.grad.clone(), None\n     ref_dq, q.grad = q.grad.clone(), None\n-    # # triton implementation\n-    tri_out = attention(q, k, v, sm_scale)\n-    # print(ref_out)\n-    # print(tri_out)\n+    # triton implementation\n+    tri_out = attention(q, k, v, causal, sm_scale).half()\n     tri_out.backward(dout)\n     tri_dv, v.grad = v.grad.clone(), None\n     tri_dk, k.grad = k.grad.clone(), None\n@@ -315,19 +379,19 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n # vary seq length for fixed head and batch=4\n configs = [triton.testing.Benchmark(\n     x_names=['N_CTX'],\n-    x_vals=[2**i for i in range(10, 14)],\n+    x_vals=[2**i for i in range(10, 15)],\n     line_arg='provider',\n     line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n     line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\n     styles=[('red', '-'), ('blue', '-')],\n     ylabel='ms',\n     plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n-    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode}\n-) for mode in ['fwd', 'bwd']]\n+    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode, 'causal': causal}\n+) for mode in ['fwd', 'bwd'] for causal in [False, True]]\n \n \n @triton.testing.perf_report(configs)\n-def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.float16, device=\"cuda\"):\n+def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, causal, mode, provider, dtype=torch.float16, device=\"cuda\"):\n     assert mode in ['fwd', 'bwd']\n     warmup = 25\n     rep = 100\n@@ -336,13 +400,12 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.f\n         k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n         v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n         sm_scale = 1.3\n-        fn = lambda: attention(q, k, v, sm_scale)\n+        fn = lambda: attention(q, k, v, causal, sm_scale)\n         if mode == 'bwd':\n             o = fn()\n             do = torch.randn_like(o)\n             fn = lambda: o.backward(do, retain_graph=True)\n         ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n-        return ms\n     if provider == \"flash\":\n         lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n         cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n@@ -354,7 +417,13 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.f\n             do = torch.randn_like(o)\n             fn = lambda: o.backward(do, retain_graph=True)\n         ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n-        return ms\n+    flops_per_matmul = 2. * BATCH * H * N_CTX * N_CTX * D_HEAD\n+    total_flops = 2 * flops_per_matmul\n+    if causal:\n+        total_flops *= 0.5\n+    if mode == 'bwd':\n+        total_flops *= 2.5  # 2.0(bwd) + 0.5(recompute)\n+    return total_flops / ms * 1e-9\n \n \n # only works on post-Ampere GPUs right now"}]