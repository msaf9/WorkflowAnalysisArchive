[{"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -519,7 +519,6 @@ def compile(fn, **kwargs):\n \n     # Add device type to meta information\n     metadata[\"device_type\"] = device_type\n-    metadata[\"cache_key\"] = fn_cache_manager.key\n \n     first_stage = list(stages.keys()).index(ext)\n     asm = dict()\n@@ -629,7 +628,7 @@ def __init__(self, fn, so_path, metadata, asm):\n         if \"tensormaps_info\" in metadata:\n             self.tensormaps_info = metadata[\"tensormaps_info\"]\n         self.constants = metadata[\"constants\"]\n-        self.cache_key = metadata[\"cache_key\"]\n+        self.cache_key = fn.cache_key\n         self.device_type = metadata[\"device_type\"]\n         self.device_backend = get_backend(self.device_type) if self.device_type not in [\"cuda\", \"hip\"] else None\n         # initialize asm dict"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -172,7 +172,7 @@ def _bwd_kernel(\n             lo = 0\n         # initialize row/col offsets\n         offs_qm = lo + tl.arange(0, BLOCK_M)\n-        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M) \n+        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n         offs_m = tl.arange(0, BLOCK_N)\n         offs_k = tl.arange(0, BLOCK_DMODEL)\n         # initialize pointers to value-like data"}]