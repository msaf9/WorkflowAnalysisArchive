[{"filename": "lib/Conversion/TritonGPUToLLVM/DotHelpers.h", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -992,8 +992,9 @@ struct MMA16816ConversionHelper {\n     if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n       Value warpM = getWarpM(shape[0]);\n       // load from smem\n+    // we use ldmatrix.x4 so each warp processes 16x16 elements.\n       int wpt =\n-          std::min<int>(mmaLayout.getWarpsPerCTA()[0], shape[0] / matShapeM);\n+          std::min<int>(mmaLayout.getWarpsPerCTA()[0], shape[0] / 16);\n       loadFn =\n           getLoadMatrixFn(tensor, smemObj, mmaLayout, wpt /*wpt*/, 1 /*kOrder*/,\n                           {mmaInstrM, mmaInstrK} /*instrShape*/,\n@@ -1036,8 +1037,9 @@ struct MMA16816ConversionHelper {\n     int numRepN = getNumRepN(tensorTy, shape[1]);\n \n     Value warpN = getWarpN(shape[1]);\n+    // we use ldmatrix.x4 so each warp processes 16x16 elements.\n     int wpt =\n-        std::min<int>(mmaLayout.getWarpsPerCTA()[1], shape[1] / matShapeN);\n+        std::min<int>(mmaLayout.getWarpsPerCTA()[1], shape[1] / 16);\n     auto loadFn =\n         getLoadMatrixFn(tensor, smemObj, mmaLayout, wpt /*wpt*/, 0 /*kOrder*/,\n                         {mmaInstrK, mmaInstrN} /*instrShape*/,\n@@ -1318,6 +1320,7 @@ struct DotOpFMAConversionHelper {\n     int K = dotOpLayout.getOpIdx() == 0 ? shape[1] : shape[0];\n     int otherDim = dotOpLayout.getOpIdx() == 1 ? shape[1] : shape[0];\n \n+\n     bool isM = dotOpLayout.getOpIdx() == 0;\n     int shapePerCTAMN = getShapePerCTAForMN(blockedLayout, isM);\n     int sizePerThreadMN = getSizePerThreadForMN(blockedLayout, isM);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 16, "deletions": 1, "changes": 17, "file_content_changes": "@@ -542,7 +542,21 @@ class ConvertTritonGPUOpToLLVMPattern\n   SmallVector<SmallVector<unsigned>>\n   emitOffsetForMmaLayoutV1(const MmaEncodingAttr &mmaLayout,\n                            ArrayRef<int64_t> shape) const {\n-    llvm_unreachable(\"emitOffsetForMmaLayoutV1 not implemented\");\n+    SmallVector<SmallVector<unsigned>> ret;\n+\n+    for (unsigned i = 0; i < shape[0]; i += getShapePerCTA(mmaLayout)[0]) {\n+      for (unsigned j = 0; j < shape[1]; j += getShapePerCTA(mmaLayout)[1]) {\n+        ret.push_back({i, j});\n+        ret.push_back({i, j + 1});\n+        ret.push_back({i + 2, j});\n+        ret.push_back({i + 2, j + 1});\n+        ret.push_back({i, j + 8});\n+        ret.push_back({i, j + 9});\n+        ret.push_back({i + 2, j + 8});\n+        ret.push_back({i + 2, j + 9});\n+      }\n+    }\n+    return ret;\n   }\n \n   SmallVector<Value>\n@@ -3671,6 +3685,7 @@ DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n   int nSizePerThread =\n       order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n \n+\n   auto has = helper.getValueTableFromStruct(llA, K, M, mShapePerCTA,\n                                             mSizePerThread, rewriter, loc);\n   auto hbs = helper.getValueTableFromStruct(llB, K, N, nShapePerCTA,"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -179,8 +179,7 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n     for (unsigned d = 0, n = getOrder(parent).size(); d < n; ++d) {\n       if (d == dim)\n         continue;\n-      shape.push_back(getSizePerThread(parent)[d] *\n-                      getThreadsPerWarp(parent)[d] * getWarpsPerCTA(parent)[d]);\n+      shape.push_back(getShapePerCTA(parent)[d]);\n     }\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     if (mmaLayout.getVersion() == 2)"}, {"filename": "python/test/unit/operators/test_blocksparse.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -10,7 +10,7 @@\n @pytest.mark.parametrize(\"TRANS_B\", [False, True])\n @pytest.mark.parametrize(\"BLOCK\", [16, 32, 64])\n # TODO: float32 fails\n-@pytest.mark.parametrize(\"DTYPE\", [torch.float32])\n+@pytest.mark.parametrize(\"DTYPE\", [torch.float16])\n def test_matmul(MODE, TRANS_A, TRANS_B, BLOCK, DTYPE, Z=3, H=2, M=512, N=384, K=256):\n     seed = 0\n     torch.manual_seed(seed)"}]