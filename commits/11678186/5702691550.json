[{"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -496,7 +496,7 @@ def visit_then_else_blocks(self, node, liveins, then_block, else_block):\n             # check type\n             for defs, block_name in [(then_defs, 'then'), (else_defs, 'else')]:\n                 if name in defs:\n-                    assert defs[name].type == liveins[name].type,\\\n+                    assert defs[name].type == liveins[name].type, \\\n                         f'initial value for `{name}` is of type {liveins[name].type}, '\\\n                         f'but the {block_name} block redefines it as {defs[name].type}'\n             if name in then_defs or name in else_defs:\n@@ -516,7 +516,7 @@ def visit_then_else_blocks(self, node, liveins, then_block, else_block):\n                 continue\n             then_ty = then_defs[name].type\n             else_ty = else_defs[name].type\n-            assert then_ty == else_ty,\\\n+            assert then_ty == else_ty, \\\n                 f'mismatched type for {name} between then block ({then_ty}) '\\\n                 f'and else block ({else_ty})'\n             names.append(name)\n@@ -814,7 +814,7 @@ def visit_For(self, node):\n                 if name in liveins:\n                     assert _is_triton_tensor(self.local_defs[name]), f'{name} is not tensor'\n                     assert _is_triton_tensor(liveins[name])\n-                    assert self.local_defs[name].type == liveins[name].type,\\\n+                    assert self.local_defs[name].type == liveins[name].type, \\\n                         f'Loop-carried variable {name} has initial type {liveins[name].type} '\\\n                         f'but is re-assigned to {self.local_defs[name].type} in loop! '\\\n                         f'Please make sure that the type stays consistent.'"}, {"filename": "python/triton/language/math.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,8 +1,8 @@\n import functools\n import os\n \n-from . import core\n from ..common.build import is_hip\n+from . import core\n \n \n @functools.lru_cache()"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1278,7 +1278,7 @@ def dot(lhs: tl.tensor,\n     assert len(rhs.shape) == 2, f\"Second input shape ({rhs.shape}) is not two dimensional!\"\n     assert lhs.shape[1].value == rhs.shape[0].value, f\"First input shape ({lhs.shape}) and second input shape {rhs.shape} are not compatible for matmul (second index of first shape ({lhs.shape[1].value}) must be equal to first index of second shape ({rhs.shape[0].value})\"\n     assert lhs.shape[0].value >= 16 and lhs.shape[1].value >= 16 \\\n-        and rhs.shape[1].value >= 16,\\\n+        and rhs.shape[1].value >= 16, \\\n         f\"All values in both first input shape ({lhs.shape}) and second input shape ({rhs.shape}) must be >= 16!\"\n     if lhs.type.scalar.is_int():\n         assert lhs.type.scalar == tl.int8, \"only int8 supported!\""}, {"filename": "python/triton/ops/blocksparse/matmul.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -395,8 +395,8 @@ def backward(ctx, dc):\n                 a, dc, not ctx.trans_a, ctx.trans_c, ctx.trans_b, ctx.spdims, ctx.block, ctx.db_lut, ctx.db_width,\n             )\n         dout = dc if ctx.has_out else None\n-        return da, db, None, None, None,\\\n-            None, None, None, None,\\\n+        return da, db, None, None, None, \\\n+            None, None, None, None, \\\n             None, None, None, None, None, dout\n \n "}]