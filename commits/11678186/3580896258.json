[{"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -146,8 +146,6 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n     return nullptr;\n   }\n \n-  // llvm::outs() << module << \"\\n\";\n-\n   std::map<std::string, std::string> externLibs;\n   SmallVector<LLVM::LLVMFuncOp> funcs;\n   module.walk([&](LLVM::LLVMFuncOp func) {"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 0, "deletions": 10, "changes": 10, "file_content_changes": "@@ -196,7 +196,6 @@ def _bwd_kernel(\n \n \n empty = torch.empty(128, device=\"cuda\")\n-# _fwd_kernel = triton.compile(\"./flash-attention.ptx\", shared=32768)\n class _attention(torch.autograd.Function):\n \n     @staticmethod\n@@ -226,15 +225,6 @@ def forward(ctx, q, k, v, sm_scale):\n             BLOCK_DMODEL=Lk, num_warps=num_warps,\n             num_stages=1,\n         )\n-        # _fwd_kernel[grid](\n-        #     q.data_ptr(), k.data_ptr(), v.data_ptr(), sm_scale,\n-        #     tmp.data_ptr(), L.data_ptr(), m.data_ptr(),\n-        #     o.data_ptr(),\n-        #     q.stride(0), q.stride(1), q.stride(2),\n-        #     k.stride(0), k.stride(1), k.stride(2),\n-        #     v.stride(0), v.stride(1), v.stride(2),\n-        #     o.stride(0), o.stride(1), o.stride(2),\n-        #     q.shape[0], q.shape[1], q.shape[2])\n \n         ctx.save_for_backward(q, k, v, o, L, m)\n         ctx.BLOCK = BLOCK"}]