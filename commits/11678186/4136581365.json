[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -7,6 +7,10 @@ on:\n       - main\n       - triton-mlir\n \n+concurrency:\n+  group: ${{ github.ref }}\n+  cancel-in-progress: ${{ github.ref != 'refs/heads/master' }}\n+\n jobs:\n   Runner-Preparation:\n     runs-on: ubuntu-latest"}, {"filename": "include/triton/Dialect/Triton/IR/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -8,7 +8,7 @@ set(LLVM_TARGET_DEFINITIONS TritonDialect.td)\n mlir_tablegen(Dialect.h.inc -gen-dialect-decls)\n mlir_tablegen(Dialect.cpp.inc -gen-dialect-defs)\n \n-set(LLVM_TARGET_DEFINITIONS TritonOps.td)\n+set(LLVM_TARGET_DEFINITIONS TritonTypes.td)\n mlir_tablegen(Types.h.inc -gen-typedef-decls)\n mlir_tablegen(Types.cpp.inc -gen-typedef-defs)\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -331,7 +331,7 @@ struct MakeRangeOpConversion\n     // TODO: slice layout has more elements than expected.\n     // Unexpected behavior for make range, but generally OK when followed by\n     // expand dims + broadcast. very weird behavior otherwise potentially.\n-    for (const auto multiDim : llvm::enumerate(idxs)) {\n+    for (const auto &multiDim : llvm::enumerate(idxs)) {\n       assert(multiDim.value().size() == 1);\n       retVals[multiDim.index()] = add(multiDim.value()[0], start);\n     }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "modified", "additions": 26, "deletions": 80, "changes": 106, "file_content_changes": "@@ -2,7 +2,6 @@\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n-#include \"mlir/Analysis/SliceAnalysis.h\"\n #include <numeric>\n \n using namespace mlir;\n@@ -11,66 +10,36 @@ using namespace mlir::triton;\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n \n-int argMax(ArrayRef<int64_t> arr) {\n-  auto it = std::max_element(arr.begin(), arr.end());\n-  return std::distance(arr.begin(), it);\n-}\n-\n-template<class T>\n-SmallVector<unsigned, 4> argSort(const T& arr){\n-  SmallVector<unsigned, 4> ret(arr.size());\n-  std::iota(ret.begin(), ret.end(), 0);\n-  std::sort(ret.begin(), ret.end(), [&](unsigned x, unsigned y) {\n-    return arr[x] > arr[y];\n-  });\n-  return ret;\n-}\n-\n-typedef DenseMap<Value, std::function<Type(Type)>> LayoutMap;\n-\n struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n   Attribute getCoalescedEncoding(AxisInfoAnalysis &axisInfo, Value ptr,\n                                  int numWarps) {\n     auto origType = ptr.getType().cast<RankedTensorType>();\n     // Get the shape of the tensor.\n     size_t rank = origType.getRank();\n     AxisInfo info = axisInfo.lookupLatticeElement(ptr)->getValue();\n-    // Get the contiguity order of `ptr`\n-    auto order = argSort(info.getContiguity());\n-    // The desired divisibility is the maximum divisibility\n-    // among all dependent pointers who have the same order as\n-    // `ptr`\n-    SetVector<Value> withSameOrder;\n-    withSameOrder.insert(ptr);\n-    if(ptr.getDefiningOp())\n-      for(Operation *op: mlir::getSlice(ptr.getDefiningOp())){\n-        for(Value val: op->getResults()){\n-          if(val.getType() != origType) \n-            continue;\n-          auto valInfo = axisInfo.lookupLatticeElement(val);\n-          auto currOrder = argSort(valInfo->getValue().getContiguity());\n-          if(order == currOrder)\n-            withSameOrder.insert(val);\n-        }\n-      }\n+    // Layout order in decreasing order of contiguity\n+    SmallVector<unsigned, 4> order(rank);\n+    std::iota(order.begin(), order.end(), 0);\n+    auto contiguity = info.getContiguity();\n+    std::sort(order.begin(), order.end(), [&](unsigned x, unsigned y) {\n+      return contiguity[x] > contiguity[y];\n+    });\n+\n     int numElems = product(origType.getShape());\n     int numThreads = numWarps * 32;\n     int numElemsPerThread = std::max(numElems / numThreads, 1);\n+\n     // Thread tile size depends on memory alignment\n     SmallVector<unsigned, 4> sizePerThread(rank, 1);\n     unsigned elemNumBits = getPointeeBitWidth(origType);\n     unsigned elemNumBytes = std::max(elemNumBits / 8, 1u);\n-    unsigned perThread = 1;\n-    for(Value val: withSameOrder){\n-      AxisInfo info = axisInfo.lookupLatticeElement(val)->getValue();\n-      unsigned maxMultipleBytes = info.getDivisibility(order[0]);\n-      unsigned maxMultiple = std::max(maxMultipleBytes / elemNumBytes, 1u);\n-      unsigned maxContig = info.getContiguity(order[0]);\n-      unsigned alignment = std::min(maxMultiple, maxContig);\n-      unsigned currPerThread = std::min(alignment, 128 / elemNumBits);\n-      perThread = std::max(perThread, currPerThread);\n-    }\n+    unsigned maxMultipleBytes = info.getDivisibility(order[0]);\n+    unsigned maxMultiple = std::max(maxMultipleBytes / elemNumBytes, 1u);\n+    unsigned maxContig = info.getContiguity(order[0]);\n+    unsigned alignment = std::min(maxMultiple, maxContig);\n+    unsigned perThread = std::min(alignment, 128 / elemNumBits);\n     sizePerThread[order[0]] = std::min<int>(perThread, numElemsPerThread);\n+\n     SmallVector<unsigned> dims(rank);\n     std::iota(dims.begin(), dims.end(), 0);\n     // create encoding\n@@ -90,12 +59,16 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n   }\n \n   template <class T>\n-  void coalesceOp(LayoutMap& layoutMap, Operation *op, Value ptr,\n+  void coalesceOp(AxisInfoAnalysis &axisInfo, Operation *op, Value ptr,\n                   OpBuilder builder) {\n     RankedTensorType ty = ptr.getType().template dyn_cast<RankedTensorType>();\n     if (!ty)\n       return;\n-    auto convertType = layoutMap.lookup(ptr);\n+    auto mod = op->getParentOfType<ModuleOp>();\n+    int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+\n+    AxisInfo info = axisInfo.lookupLatticeElement(ptr)->getValue();\n+    auto convertType = getTypeConverter(axisInfo, ptr, numWarps);\n     // convert operands\n     SmallVector<Value, 4> newArgs;\n     for (auto v : op->getOperands()) {\n@@ -133,33 +106,6 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     AxisInfoAnalysis axisInfo(&getContext());\n     axisInfo.run(op);\n \n-    // For each i/o operation, we determine what layout\n-    // the pointers should have for best memory coalescing\n-    LayoutMap layoutMap;\n-    op->walk([&](Operation *curr) {\n-        Value ptr;\n-        if (auto op = dyn_cast<triton::LoadOp>(curr)) \n-          ptr = op.ptr();\n-        if (auto op = dyn_cast<triton::AtomicRMWOp>(curr)) \n-          ptr = op.ptr();\n-        if (auto op = dyn_cast<triton::AtomicCASOp>(curr)) \n-          ptr = op.ptr();\n-        if (auto op = dyn_cast<triton::gpu::InsertSliceAsyncOp>(curr)) \n-          ptr = op.src();\n-        if (auto op = dyn_cast<triton::StoreOp>(curr)) \n-          ptr = op.ptr();\n-        if(!ptr)\n-          return;\n-        RankedTensorType ty =  ptr.getType().template dyn_cast<RankedTensorType>();\n-        if(!ty || !ty.getElementType().isa<PointerType>())\n-          return;\n-        AxisInfo info = axisInfo.lookupLatticeElement(ptr)->getValue();\n-        auto mod = curr->getParentOfType<ModuleOp>();\n-        int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n-        auto convertType = getTypeConverter(axisInfo, ptr, numWarps);\n-        layoutMap[ptr] = convertType;\n-    });\n-\n     // For each memory op that has a layout L1:\n     // 1. Create a coalesced memory layout L2 of the pointer operands\n     // 2. Convert all operands from layout L1 to layout L2\n@@ -170,24 +116,24 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     op->walk([&](Operation *curr) {\n       OpBuilder builder(curr);\n       if (auto load = dyn_cast<triton::LoadOp>(curr)) {\n-        coalesceOp<triton::LoadOp>(layoutMap, curr, load.ptr(), builder);\n+        coalesceOp<triton::LoadOp>(axisInfo, curr, load.ptr(), builder);\n         return;\n       }\n       if (auto op = dyn_cast<triton::AtomicRMWOp>(curr)) {\n-        coalesceOp<triton::AtomicRMWOp>(layoutMap, curr, op.ptr(), builder);\n+        coalesceOp<triton::AtomicRMWOp>(axisInfo, curr, op.ptr(), builder);\n         return;\n       }\n       if (auto op = dyn_cast<triton::AtomicCASOp>(curr)) {\n-        coalesceOp<triton::AtomicCASOp>(layoutMap, curr, op.ptr(), builder);\n+        coalesceOp<triton::AtomicCASOp>(axisInfo, curr, op.ptr(), builder);\n         return;\n       }\n       if (auto load = dyn_cast<triton::gpu::InsertSliceAsyncOp>(curr)) {\n-        coalesceOp<triton::gpu::InsertSliceAsyncOp>(layoutMap, curr, load.src(),\n+        coalesceOp<triton::gpu::InsertSliceAsyncOp>(axisInfo, curr, load.src(),\n                                                     builder);\n         return;\n       }\n       if (auto store = dyn_cast<triton::StoreOp>(curr)) {\n-        coalesceOp<triton::StoreOp>(layoutMap, curr, store.ptr(), builder);\n+        coalesceOp<triton::StoreOp>(axisInfo, curr, store.ptr(), builder);\n         return;\n       }\n     });"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 38, "deletions": 55, "changes": 93, "file_content_changes": "@@ -417,6 +417,41 @@ Operation *cloneWithInferType(mlir::PatternRewriter &rewriter, Operation *op,\n   return newOp;\n }\n \n+// op(cvt(arg_0), arg_1, ..., arg_n)\n+// -> cvt(op(arg_0, cvt(arg_1), ..., cvt(arg_n)))\n+void pushConversionForward(triton::gpu::ConvertLayoutOp cvt,\n+                           SetVector<Operation *> &cvtSlices,\n+                           mlir::PatternRewriter &rewriter) {\n+  auto srcEncoding =\n+      cvt.getOperand().getType().cast<RankedTensorType>().getEncoding();\n+  auto dstEncoding =\n+      cvt.getResult().getType().cast<RankedTensorType>().getEncoding();\n+  BlockAndValueMapping mapping;\n+  auto op = cvtSlices.front();\n+  for (Value arg : op->getOperands()) {\n+    if (arg.getDefiningOp() == cvt)\n+      mapping.map(arg, cvt.getOperand());\n+    else {\n+      auto oldType = arg.getType().cast<RankedTensorType>();\n+      auto newType = RankedTensorType::get(\n+          oldType.getShape(), oldType.getElementType(), srcEncoding);\n+      auto cvtI = rewriter.create<triton::gpu::ConvertLayoutOp>(arg.getLoc(),\n+                                                                newType, arg);\n+      if (Operation *argOp = arg.getDefiningOp())\n+        cvtI->moveAfter(argOp);\n+      mapping.map(arg, cvtI);\n+    }\n+  }\n+  rewriter.setInsertionPoint(op);\n+  auto *newOp = cloneWithInferType(rewriter, op, mapping);\n+  auto newType = newOp->getResult(0).getType().cast<RankedTensorType>();\n+  auto newCvtType = RankedTensorType::get(\n+      newType.getShape(), newType.getElementType(), dstEncoding);\n+  auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+      newOp->getLoc(), newCvtType, newOp->getResult(0));\n+  rewriter.replaceOp(op, newCvt->getResults());\n+}\n+\n //\n class MoveConvertOutOfIf : public mlir::RewritePattern {\n public:\n@@ -578,42 +613,7 @@ class FoldConvertAndReduce : public mlir::RewritePattern {\n       }\n     }\n \n-\n-    BlockAndValueMapping mapping;\n-    auto op = cvtSlices.front();\n-    for (Value arg : op->getOperands()) {\n-      if (arg.getDefiningOp() == cvt)\n-        mapping.map(arg, cvt.getOperand());\n-      else {\n-        auto oldType = arg.getType().cast<RankedTensorType>();\n-        auto newType = RankedTensorType::get(\n-            oldType.getShape(), oldType.getElementType(), srcEncoding);\n-        auto cvtI = rewriter.create<triton::gpu::ConvertLayoutOp>(arg.getLoc(),\n-                                                                  newType, arg);\n-        if (Operation *argOp = arg.getDefiningOp())\n-          cvtI->moveAfter(argOp);\n-        mapping.map(arg, cvtI);\n-      }\n-    }\n-    rewriter.setInsertionPoint(op);\n-    Operation *newOp = rewriter.clone(*op, mapping);\n-    if(newOp->getNumResults() > 1)\n-      return failure();\n-    if(newOp->getNumResults() == 1){\n-      auto oldType = op->getResult(0).getType().cast<RankedTensorType>();\n-      auto newType = RankedTensorType::get(oldType.getShape(),\n-                                           oldType.getElementType(), srcEncoding);\n-\n-      newOp->getResult(0).setType(newType);\n-      auto newCvtType = RankedTensorType::get(\n-          oldType.getShape(), oldType.getElementType(), dstEncoding);\n-      auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-          newOp->getLoc(), newCvtType, newOp->getResult(0));\n-      rewriter.replaceOp(op, newCvt->getResults());\n-    }\n-    else{\n-      rewriter.eraseOp(op);\n-    }\n+    pushConversionForward(cvt, cvtSlices, rewriter);\n     return success();\n   }\n };\n@@ -863,27 +863,10 @@ class RematerializeForward : public mlir::RewritePattern {\n       }\n     }\n \n-    // otherwise, we push the conversion forward\n+    // Otherwise, we push the conversion forward\n     // since we'll be able to move it out of\n     // the loop once it reaches the yield op\n-    // op(cvt(arg_0), arg_1, ..., arg_n)\n-    // -> cvt(op(arg_0, cvt(arg_1), ..., cvt(arg_n)))\n-    BlockAndValueMapping mapping;\n-    auto op = cvtSlices.front();\n-    for (Value arg : op->getOperands()) {\n-      if (arg.getDefiningOp() == cvt)\n-        mapping.map(arg, cvt.getOperand());\n-      else {\n-        auto cvtI = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-            arg.getLoc(), cvt.getOperand().getType(), arg);\n-        mapping.map(arg, cvtI);\n-      }\n-    }\n-    Operation *newOp = rewriter.clone(*op, mapping);\n-    newOp->getResult(0).setType(cvt.getOperand().getType());\n-    auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        newOp->getLoc(), cvt.getResult().getType(), newOp->getResult(0));\n-    rewriter.replaceOp(op, newCvt->getResults());\n+    pushConversionForward(cvt, cvtSlices, rewriter);\n     return success();\n   }\n };"}, {"filename": "python/setup.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -79,9 +79,9 @@ def get_thirdparty_packages(triton_cache_path):\n     for p in packages:\n         package_root_dir = os.path.join(triton_cache_path, p.package)\n         package_dir = os.path.join(package_root_dir, p.name)\n-        test_file_path = os.path.join(package_dir, p.test_file)\n         if p.syspath_var_name in os.environ:\n             package_dir = os.environ[p.syspath_var_name]\n+        test_file_path = os.path.join(package_dir, p.test_file)\n         if not os.path.exists(test_file_path):\n             try:\n                 shutil.rmtree(package_root_dir)\n@@ -193,7 +193,7 @@ def build_extension(self, ext):\n         else:\n             import multiprocessing\n             cmake_args += [\"-DCMAKE_BUILD_TYPE=\" + cfg]\n-            build_args += [\"--\", '-j' + str(2 * multiprocessing.cpu_count())]\n+            build_args += ['-j' + str(2 * multiprocessing.cpu_count())]\n \n         env = os.environ.copy()\n         subprocess.check_call([\"cmake\", self.base_dir] + cmake_args, cwd=self.build_temp, env=env)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -330,7 +330,10 @@ def test_bitwise_op(dtype_x, dtype_y, op, device='cuda'):\n def test_shift_op(dtype_x, dtype_y, op, device='cuda'):\n     expr = f'x {op} y'\n     bw = max(_bitwidth(dtype_x), _bitwidth(dtype_y))\n-    dtype_z = f'uint{bw}'\n+    if dtype_x.startswith('int'):\n+        dtype_z = f'int{bw}'\n+    else:\n+        dtype_z = f'uint{bw}'\n     numpy_expr = f'x.astype(np.{dtype_z}) {op} y.astype(np.{dtype_z})'\n     _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device, y_low=0, y_high=65)\n \n@@ -627,7 +630,7 @@ def kernel(X, Z):\n \n     # triton result\n     rs = RandomState(17)\n-    x = numpy_random((n_programs, ), dtype_str=dtype_x_str, rs=rs)\n+    x = np.array([2**i for i in range(n_programs)], dtype=getattr(np, dtype_x_str))\n     if mode == 'all_neg':\n         x = -np.abs(x)\n     if mode == 'all_pos':"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -549,7 +549,10 @@ def __lshift__(self, other, _builder=None):\n     @builtin\n     def __rshift__(self, other, _builder=None):\n         other = _to_tensor(other, _builder)\n-        return semantic.lshr(self, other, _builder)\n+        if self.dtype.is_int_signed():\n+            return semantic.ashr(self, other, _builder)\n+        else:\n+            return semantic.lshr(self, other, _builder)\n \n     # comparison operators\n "}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -325,6 +325,13 @@ def lshr(input: tl.tensor,\n     return tl.tensor(builder.create_lshr(input.handle, other.handle), input.type)\n \n \n+def ashr(input: tl.tensor,\n+         other: tl.tensor,\n+         builder: ir.builder) -> tl.tensor:\n+    input, other = bitwise_op_type_checking_impl(input, other, builder)\n+    return tl.tensor(builder.create_ashr(input.handle, other.handle), input.type)\n+\n+\n def shl(input: tl.tensor,\n         other: tl.tensor,\n         builder: ir.builder) -> tl.tensor:\n@@ -1010,6 +1017,7 @@ def dot(lhs: tl.tensor,\n         allow_tf32: bool,\n         builder: ir.builder) -> tl.tensor:\n     assert lhs.type.is_block() and rhs.type.is_block()\n+    assert lhs.dtype == rhs.dtype, \"lhs and rhs must have the same dtype!\"\n     assert len(lhs.shape) == 2 and len(rhs.shape) == 2\n     assert lhs.shape[1].value == rhs.shape[0].value\n     assert lhs.shape[0].value >= 16 and lhs.shape[1].value >= 16 \\"}, {"filename": "python/triton/ops/blocksparse/softmax.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -231,7 +231,7 @@ def __init__(self, layout, block, device, is_dense=False):\n \n     def __call__(self, a, *, scale=1.0, rel_logits=None, is_causal=False):\n         if rel_logits is not None and rel_logits.dtype != a.dtype:\n-            raise ValueError(\"relative position embedding must be %s\" % a.dtype)\n+            raise ValueError(f\"relative position embedding must be {a.dtype}\")\n         a = _softmax.apply(\n             a, scale, rel_logits, is_causal,\n             self.spdims, self.block, self.lut, self.maxlut, self.is_dense,"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 146, "deletions": 0, "changes": 146, "file_content_changes": "@@ -851,3 +851,149 @@ func public @mnist(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.\n   tt.store %61, %62, %63 : tensor<16x16xf32, #blocked4>\n   return\n }\n+\n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [8, 1], order = [0, 1]}>\n+#blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [2, 4], order = [0, 1]}>\n+#blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 8], order = [0, 1]}>\n+#blocked4 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 8], warpsPerCTA = [8, 1], order = [1, 0]}>\n+#blocked5 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [8, 1], order = [1, 0]}>\n+// cmpf and cmpi have different operands and result types\n+// CHECK-LABEL: cmp\n+func public @cmp(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) {\n+  %c64 = arith.constant 64 : index\n+  %c2048 = arith.constant 2048 : index\n+  %c0 = arith.constant 0 : index\n+  %c64_i32 = arith.constant 64 : i32\n+  %cst = arith.constant dense<-3.40282347E+38> : tensor<64x64xf32, #blocked2>\n+  %cst_0 = arith.constant dense<4194304> : tensor<64x1xi32, #blocked2>\n+  %cst_1 = arith.constant dense<12> : tensor<64x1xi32, #blocked2>\n+  %cst_2 = arith.constant dense<2048> : tensor<1x64xi32, #blocked3>\n+  %cst_3 = arith.constant dense<0> : tensor<64x64xi32, #blocked2>\n+  %cst_4 = arith.constant dense<2048> : tensor<64x1xi32, #blocked2>\n+  %cst_5 = arith.constant dense<49152> : tensor<64x1xi32, #blocked2>\n+  %cst_6 = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked2>\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = arith.muli %0, %c64_i32 : i32\n+  %2 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked0>\n+  %3 = triton_gpu.convert_layout %2 : (tensor<64xi32, #blocked0>) -> tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+  %4 = tt.expand_dims %3 {axis = 1 : i32} : (tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<64x1xi32, #blocked1>\n+  %5 = triton_gpu.convert_layout %4 : (tensor<64x1xi32, #blocked1>) -> tensor<64x1xi32, #blocked2>\n+  %6 = tt.splat %1 : (i32) -> tensor<64x1xi32, #blocked2>\n+  %7 = arith.addi %6, %5 : tensor<64x1xi32, #blocked2>\n+  %8 = \"triton_gpu.cmpi\"(%7, %cst_5) {predicate = 2 : i64} : (tensor<64x1xi32, #blocked2>, tensor<64x1xi32, #blocked2>) -> tensor<64x1xi1, #blocked2>\n+  %9 = triton_gpu.convert_layout %2 : (tensor<64xi32, #blocked0>) -> tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>\n+  %10 = tt.expand_dims %9 {axis = 0 : i32} : (tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>) -> tensor<1x64xi32, #blocked3>\n+  %11 = arith.remsi %7, %cst_4 : tensor<64x1xi32, #blocked2>\n+  %12 = arith.divsi %7, %cst_4 : tensor<64x1xi32, #blocked2>\n+  %13 = arith.sitofp %cst_3 : tensor<64x64xi32, #blocked2> to tensor<64x64xf32, #blocked2>\n+  %14 = arith.addf %13, %cst_6 : tensor<64x64xf32, #blocked2>\n+  %15 = arith.muli %7, %cst_4 : tensor<64x1xi32, #blocked2>\n+  %16 = tt.broadcast %15 : (tensor<64x1xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+  %17 = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<64x64x!tt.ptr<f16>, #blocked2>\n+  %18 = tt.broadcast %8 : (tensor<64x1xi1, #blocked2>) -> tensor<64x64xi1, #blocked2>\n+  %19 = arith.muli %11, %cst_4 : tensor<64x1xi32, #blocked2>\n+  %20 = tt.broadcast %19 : (tensor<64x1xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+  %21 = arith.divsi %12, %cst_1 : tensor<64x1xi32, #blocked2>\n+  %22 = arith.muli %21, %cst_0 : tensor<64x1xi32, #blocked2>\n+  %23 = tt.broadcast %22 : (tensor<64x1xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+  %24 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x64x!tt.ptr<f32>, #blocked2>\n+  %25 = scf.for %arg6 = %c0 to %c2048 step %c64 iter_args(%arg7 = %14) -> (tensor<64x64xf32, #blocked2>) {\n+    %44 = arith.index_cast %arg6 : index to i32\n+    %45 = tt.splat %44 : (i32) -> tensor<1x64xi32, #blocked3>\n+    %46 = arith.addi %45, %10 : tensor<1x64xi32, #blocked3>\n+    %47 = \"triton_gpu.cmpi\"(%46, %cst_2) {predicate = 2 : i64} : (tensor<1x64xi32, #blocked3>, tensor<1x64xi32, #blocked3>) -> tensor<1x64xi1, #blocked3>\n+    %48 = tt.broadcast %46 : (tensor<1x64xi32, #blocked3>) -> tensor<64x64xi32, #blocked3>\n+    %49 = triton_gpu.convert_layout %48 : (tensor<64x64xi32, #blocked3>) -> tensor<64x64xi32, #blocked2>\n+    %50 = arith.addi %49, %16 : tensor<64x64xi32, #blocked2>\n+    %51 = tt.addptr %17, %50 : tensor<64x64x!tt.ptr<f16>, #blocked2>, tensor<64x64xi32, #blocked2>\n+    %52 = tt.broadcast %47 : (tensor<1x64xi1, #blocked3>) -> tensor<64x64xi1, #blocked3>\n+    %53 = triton_gpu.convert_layout %52 : (tensor<64x64xi1, #blocked3>) -> tensor<64x64xi1, #blocked2>\n+    %54 = arith.andi %53, %18 : tensor<64x64xi1, #blocked2>\n+    %55 = triton_gpu.convert_layout %51 : (tensor<64x64x!tt.ptr<f16>, #blocked2>) -> tensor<64x64x!tt.ptr<f16>, #blocked4>\n+    %56 = triton_gpu.convert_layout %54 : (tensor<64x64xi1, #blocked2>) -> tensor<64x64xi1, #blocked4>\n+    %57 = tt.load %55, %56 {cache = 1 : i32, evict = 3 : i32, isVolatile = false} : tensor<64x64xf16, #blocked4>\n+    %58 = triton_gpu.convert_layout %57 : (tensor<64x64xf16, #blocked4>) -> tensor<64x64xf16, #blocked2>\n+    %59 = arith.extf %58 : tensor<64x64xf16, #blocked2> to tensor<64x64xf32, #blocked2>\n+    %60 = arith.addi %49, %20 : tensor<64x64xi32, #blocked2>\n+    %61 = arith.addi %60, %23 : tensor<64x64xi32, #blocked2>\n+    %62 = tt.addptr %24, %61 : tensor<64x64x!tt.ptr<f32>, #blocked2>, tensor<64x64xi32, #blocked2>\n+    %63 = triton_gpu.convert_layout %62 : (tensor<64x64x!tt.ptr<f32>, #blocked2>) -> tensor<64x64x!tt.ptr<f32>, #blocked5>\n+    %64 = triton_gpu.convert_layout %54 : (tensor<64x64xi1, #blocked2>) -> tensor<64x64xi1, #blocked5>\n+    %65 = tt.load %63, %64 {cache = 1 : i32, evict = 3 : i32, isVolatile = false} : tensor<64x64xf32, #blocked5>\n+    %66 = triton_gpu.convert_layout %65 : (tensor<64x64xf32, #blocked5>) -> tensor<64x64xf32, #blocked2>\n+    %67 = arith.addf %59, %66 : tensor<64x64xf32, #blocked2>\n+    %68 = \"triton_gpu.cmpf\"(%67, %67) {predicate = 13 : i64} : (tensor<64x64xf32, #blocked2>, tensor<64x64xf32, #blocked2>) -> tensor<64x64xi1, #blocked2>\n+    %69 = \"triton_gpu.cmpf\"(%67, %cst) {predicate = 2 : i64} : (tensor<64x64xf32, #blocked2>, tensor<64x64xf32, #blocked2>) -> tensor<64x64xi1, #blocked2>\n+    %70 = \"triton_gpu.select\"(%69, %67, %cst) : (tensor<64x64xi1, #blocked2>, tensor<64x64xf32, #blocked2>, tensor<64x64xf32, #blocked2>) -> tensor<64x64xf32, #blocked2>\n+    %71 = \"triton_gpu.select\"(%68, %67, %70) : (tensor<64x64xi1, #blocked2>, tensor<64x64xf32, #blocked2>, tensor<64x64xf32, #blocked2>) -> tensor<64x64xf32, #blocked2>\n+    %72 = math.exp %71 : tensor<64x64xf32, #blocked2>\n+    %73 = arith.addf %arg7, %72 : tensor<64x64xf32, #blocked2>\n+    %74 = \"triton_gpu.select\"(%54, %73, %arg7) : (tensor<64x64xi1, #blocked2>, tensor<64x64xf32, #blocked2>, tensor<64x64xf32, #blocked2>) -> tensor<64x64xf32, #blocked2>\n+    scf.yield %74 : tensor<64x64xf32, #blocked2>\n+  }\n+  %26 = tt.reduce %25 {axis = 1 : i32, redOp = 2 : i32} : tensor<64x64xf32, #blocked2> -> tensor<64xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+  %27 = triton_gpu.convert_layout %26 : (tensor<64xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<64xf32, #blocked0>\n+  %28 = triton_gpu.convert_layout %27 : (tensor<64xf32, #blocked0>) -> tensor<64xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+  %29 = tt.expand_dims %28 {axis = 1 : i32} : (tensor<64xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<64x1xf32, #blocked1>\n+  %30 = triton_gpu.convert_layout %29 : (tensor<64x1xf32, #blocked1>) -> tensor<64x1xf32, #blocked2>\n+  %31 = arith.muli %7, %cst_4 : tensor<64x1xi32, #blocked2>\n+  %32 = tt.broadcast %31 : (tensor<64x1xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+  %33 = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<64x64x!tt.ptr<f16>, #blocked2>\n+  %34 = tt.broadcast %8 : (tensor<64x1xi1, #blocked2>) -> tensor<64x64xi1, #blocked2>\n+  %35 = arith.muli %11, %cst_4 : tensor<64x1xi32, #blocked2>\n+  %36 = tt.broadcast %35 : (tensor<64x1xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+  %37 = arith.divsi %12, %cst_1 : tensor<64x1xi32, #blocked2>\n+  %38 = arith.muli %37, %cst_0 : tensor<64x1xi32, #blocked2>\n+  %39 = tt.broadcast %38 : (tensor<64x1xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+  %40 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x64x!tt.ptr<f32>, #blocked2>\n+  %41 = tt.broadcast %30 : (tensor<64x1xf32, #blocked2>) -> tensor<64x64xf32, #blocked2>\n+  %42 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x64x!tt.ptr<f32>, #blocked2>\n+  %43 = tt.splat %arg3 : (!tt.ptr<f16>) -> tensor<64x64x!tt.ptr<f16>, #blocked2>\n+  scf.for %arg6 = %c0 to %c2048 step %c64 {\n+    %44 = arith.index_cast %arg6 : index to i32\n+    %45 = tt.splat %44 : (i32) -> tensor<1x64xi32, #blocked3>\n+    %46 = arith.addi %45, %10 : tensor<1x64xi32, #blocked3>\n+    %47 = \"triton_gpu.cmpi\"(%46, %cst_2) {predicate = 2 : i64} : (tensor<1x64xi32, #blocked3>, tensor<1x64xi32, #blocked3>) -> tensor<1x64xi1, #blocked3>\n+    %48 = tt.broadcast %46 : (tensor<1x64xi32, #blocked3>) -> tensor<64x64xi32, #blocked3>\n+    %49 = triton_gpu.convert_layout %48 : (tensor<64x64xi32, #blocked3>) -> tensor<64x64xi32, #blocked2>\n+    %50 = arith.addi %49, %32 : tensor<64x64xi32, #blocked2>\n+    %51 = tt.addptr %33, %50 : tensor<64x64x!tt.ptr<f16>, #blocked2>, tensor<64x64xi32, #blocked2>\n+    %52 = tt.broadcast %47 : (tensor<1x64xi1, #blocked3>) -> tensor<64x64xi1, #blocked3>\n+    %53 = triton_gpu.convert_layout %52 : (tensor<64x64xi1, #blocked3>) -> tensor<64x64xi1, #blocked2>\n+    %54 = arith.andi %53, %34 : tensor<64x64xi1, #blocked2>\n+    %55 = triton_gpu.convert_layout %51 : (tensor<64x64x!tt.ptr<f16>, #blocked2>) -> tensor<64x64x!tt.ptr<f16>, #blocked4>\n+    %56 = triton_gpu.convert_layout %54 : (tensor<64x64xi1, #blocked2>) -> tensor<64x64xi1, #blocked4>\n+    %57 = tt.load %55, %56 {cache = 1 : i32, evict = 3 : i32, isVolatile = false} : tensor<64x64xf16, #blocked4>\n+    %58 = triton_gpu.convert_layout %57 : (tensor<64x64xf16, #blocked4>) -> tensor<64x64xf16, #blocked2>\n+    %59 = arith.extf %58 : tensor<64x64xf16, #blocked2> to tensor<64x64xf32, #blocked2>\n+    %60 = arith.addi %49, %36 : tensor<64x64xi32, #blocked2>\n+    %61 = arith.addi %60, %39 : tensor<64x64xi32, #blocked2>\n+    %62 = tt.addptr %40, %61 : tensor<64x64x!tt.ptr<f32>, #blocked2>, tensor<64x64xi32, #blocked2>\n+    %63 = triton_gpu.convert_layout %62 : (tensor<64x64x!tt.ptr<f32>, #blocked2>) -> tensor<64x64x!tt.ptr<f32>, #blocked5>\n+    %64 = triton_gpu.convert_layout %54 : (tensor<64x64xi1, #blocked2>) -> tensor<64x64xi1, #blocked5>\n+    %65 = tt.load %63, %64 {cache = 1 : i32, evict = 3 : i32, isVolatile = false} : tensor<64x64xf32, #blocked5>\n+    %66 = triton_gpu.convert_layout %65 : (tensor<64x64xf32, #blocked5>) -> tensor<64x64xf32, #blocked2>\n+    %67 = arith.addf %59, %66 : tensor<64x64xf32, #blocked2>\n+    %68 = \"triton_gpu.cmpf\"(%67, %67) {predicate = 13 : i64} : (tensor<64x64xf32, #blocked2>, tensor<64x64xf32, #blocked2>) -> tensor<64x64xi1, #blocked2>\n+    %69 = \"triton_gpu.cmpf\"(%67, %cst) {predicate = 2 : i64} : (tensor<64x64xf32, #blocked2>, tensor<64x64xf32, #blocked2>) -> tensor<64x64xi1, #blocked2>\n+    %70 = \"triton_gpu.select\"(%69, %67, %cst) : (tensor<64x64xi1, #blocked2>, tensor<64x64xf32, #blocked2>, tensor<64x64xf32, #blocked2>) -> tensor<64x64xf32, #blocked2>\n+    %71 = \"triton_gpu.select\"(%68, %67, %70) : (tensor<64x64xi1, #blocked2>, tensor<64x64xf32, #blocked2>, tensor<64x64xf32, #blocked2>) -> tensor<64x64xf32, #blocked2>\n+    %72 = math.exp %71 : tensor<64x64xf32, #blocked2>\n+    %73 = arith.divf %72, %41 : tensor<64x64xf32, #blocked2>\n+    %74 = tt.addptr %42, %50 : tensor<64x64x!tt.ptr<f32>, #blocked2>, tensor<64x64xi32, #blocked2>\n+    %75 = triton_gpu.convert_layout %74 : (tensor<64x64x!tt.ptr<f32>, #blocked2>) -> tensor<64x64x!tt.ptr<f32>, #blocked5>\n+    %76 = triton_gpu.convert_layout %73 : (tensor<64x64xf32, #blocked2>) -> tensor<64x64xf32, #blocked5>\n+    %77 = triton_gpu.convert_layout %54 : (tensor<64x64xi1, #blocked2>) -> tensor<64x64xi1, #blocked5>\n+    tt.store %75, %76, %77 : tensor<64x64xf32, #blocked5>\n+    %78 = tt.addptr %43, %50 : tensor<64x64x!tt.ptr<f16>, #blocked2>, tensor<64x64xi32, #blocked2>\n+    %79 = arith.truncf %73 : tensor<64x64xf32, #blocked2> to tensor<64x64xf16, #blocked2>\n+    %80 = triton_gpu.convert_layout %78 : (tensor<64x64x!tt.ptr<f16>, #blocked2>) -> tensor<64x64x!tt.ptr<f16>, #blocked4>\n+    %81 = triton_gpu.convert_layout %79 : (tensor<64x64xf16, #blocked2>) -> tensor<64x64xf16, #blocked4>\n+    %82 = triton_gpu.convert_layout %54 : (tensor<64x64xi1, #blocked2>) -> tensor<64x64xi1, #blocked4>\n+    tt.store %80, %81, %82 : tensor<64x64xf16, #blocked4>\n+  }\n+  return\n+}"}]