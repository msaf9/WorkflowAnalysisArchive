[{"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 12, "deletions": 2, "changes": 14, "file_content_changes": "@@ -5,6 +5,7 @@\n from typing import Dict\n \n from ..testing import do_bench\n+from ..compiler import OutOfResources\n from .jit import KernelInterface\n \n \n@@ -60,7 +61,10 @@ def kernel_call():\n                 config.pre_hook(self.nargs)\n             self.hook(args)\n             self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **current)\n-        return do_bench(kernel_call)\n+        try:\n+          return do_bench(kernel_call)\n+        except OutOfResources:\n+          return float('inf')\n \n     def run(self, *args, **kwargs):\n         self.nargs = dict(zip(self.arg_names, args))\n@@ -83,7 +87,13 @@ def run(self, *args, **kwargs):\n         self.best_config = config\n         if config.pre_hook is not None:\n             config.pre_hook(self.nargs)\n-        return self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **kwargs, **config.kwargs)\n+        try:\n+          return self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **kwargs, **config.kwargs)\n+        except OutOfResources as E:\n+          raise RuntimeError(\"Autotuner couldn't find an appropriate configuration. \"\\\n+                             \"All proposed configurations use too much shared memory. \"\\\n+                             \"Triton does support spilling to DRAM. \"\n+                             \"Try reducing block sizes.\")\n \n     def prune_configs(self, kwargs):\n         pruned_configs = self.configs"}]