[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 18, "deletions": 17, "changes": 35, "file_content_changes": "@@ -3480,39 +3480,40 @@ def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device):\n     \"\"\"\n \n     conversion = f\"\"\"\n-    %12 = {GPU_DIALECT}.convert_layout %9 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #dst>\n-    %13 = {GPU_DIALECT}.convert_layout %11 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #dst>\n+    %12 = triton_gpu.convert_layout %9 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #dst>\n+    %13 = triton_gpu.convert_layout %11 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #dst>\n     \"\"\" if interm_layout is None else f\"\"\"\n-    %15 = {GPU_DIALECT}.convert_layout %9 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #interm>\n-    %16 = {GPU_DIALECT}.convert_layout %15 : (tensor<128x128xi32, #interm>) -> tensor<128x128xi32, #src>\n-    %17 = {GPU_DIALECT}.convert_layout %11 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #interm>\n-    %18 = {GPU_DIALECT}.convert_layout %17 : (tensor<128x128xf16, #interm>) -> tensor<128x128xf16, #src>\n+    %15 = triton_gpu.convert_layout %9 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #interm>\n+    %16 = triton_gpu.convert_layout %15 : (tensor<128x128xi32, #interm>) -> tensor<128x128xi32, #src>\n+    %17 = triton_gpu.convert_layout %11 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #interm>\n+    %18 = triton_gpu.convert_layout %17 : (tensor<128x128xf16, #interm>) -> tensor<128x128xf16, #src>\n \n-    %12 = {GPU_DIALECT}.convert_layout %16 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #dst>\n-    %13 = {GPU_DIALECT}.convert_layout %18 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #dst>\n+    %12 = triton_gpu.convert_layout %16 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #dst>\n+    %13 = triton_gpu.convert_layout %18 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #dst>\n     \"\"\"\n \n     ir = layouts + \"\"\"\n     module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n   tt.func public @kernel_0d1d(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n     %cst = arith.constant dense<128> : tensor<128x1xi32, #src>\n-    %0 = tt.make_range {{end = 128 : i32, start = 0 : i32}} : tensor<128xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>\n-    %1 = tt.make_range {{end = 128 : i32, start = 0 : i32}} : tensor<128xi32, #{GPU_DIALECT}.slice<{{dim = 0, parent = #src}}>>\n+    %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #src}>>\n+    %1 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 0, parent = #src}>>\n     %2 = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<128x128x!tt.ptr<f16>, #src>\n-    %4 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<128xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #src}}>>) -> tensor<128x1xi32, #src>\n+    %4 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #src}>>) -> tensor<128x1xi32, #src>\n     %5 = arith.muli %4, %cst : tensor<128x1xi32, #src>\n-    %6 = tt.expand_dims %1 {{axis = 0 : i32}} : (tensor<128xi32, #{GPU_DIALECT}.slice<{{dim = 0, parent = #src}}>>) -> tensor<1x128xi32, #src>\n+    %6 = tt.expand_dims %1 {axis = 0 : i32} : (tensor<128xi32, #triton_gpu.slice<{dim = 0, parent = #src}>>) -> tensor<1x128xi32, #src>\n     %7 = tt.broadcast %6 : (tensor<1x128xi32, #src>) -> tensor<128x128xi32, #src>\n     %8 = tt.broadcast %5 : (tensor<128x1xi32, #src>) -> tensor<128x128xi32, #src>\n     %9 = arith.addi %8, %7 : tensor<128x128xi32, #src>\n     %10 = tt.addptr %2, %9 : tensor<128x128x!tt.ptr<f16>, #src>, tensor<128x128xi32, #src>\n-    %11 = tt.load %10 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<128x128xf16, #src>\n-    %3 = tt.splat %arg1 : (!tt.ptr<f16>) -> tensor<128x128x!tt.ptr<f16>, #dst>\\\n-    {conversion}%14 = tt.addptr %3, %12 : tensor<128x128x!tt.ptr<f16>, #dst>, tensor<128x128xi32, #dst>\n+    %11 = tt.load %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf16, #src>\n+    %3 = tt.splat %arg1 : (!tt.ptr<f16>) -> tensor<128x128x!tt.ptr<f16>, #dst>\n+    \"\"\" + conversion + \"\"\"\n+    %14 = tt.addptr %3, %12 : tensor<128x128x!tt.ptr<f16>, #dst>, tensor<128x128xi32, #dst>\n     tt.store %14, %13 : tensor<128x128xf16, #dst>\n     tt.return\n-  }}\n-}}\n+  }\n+}\n \"\"\"\n \n     x = to_triton(numpy_random(shape, dtype_str=dtype), device=device)"}]