[{"filename": ".gitignore", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -7,3 +7,6 @@ python/build/\n python/triton.egg-info/\n python/triton/_C/libtriton.pyd\n python/triton/_C/libtriton.so\n+\n+.vscode\n+.vs"}, {"filename": "CMakeLists.txt", "status": "modified", "additions": 83, "deletions": 69, "changes": 152, "file_content_changes": "@@ -3,6 +3,8 @@ include(ExternalProject)\n \n set(CMAKE_CXX_STANDARD 17)\n \n+set(CMAKE_INCLUDE_CURRENT_DIR ON)\n+\n if(NOT TRITON_LLVM_BUILD_DIR)\n     set(TRITON_LLVM_BUILD_DIR ${CMAKE_BINARY_DIR})\n endif()\n@@ -15,8 +17,8 @@ if(NOT WIN32)\n endif()\n \n # Options\n-option(BUILD_TUTORIALS \"Build C++ Triton tutorials\" ON)\n-option(BUILD_PYTHON_MODULE \"Build Python Triton bindings\" OFF)\n+option(TRITON_BUILD_TUTORIALS \"Build C++ Triton tutorials\" ON)\n+option(TRITON_BUILD_PYTHON_MODULE \"Build Python Triton bindings\" OFF)\n \n # Default build type\n if(NOT CMAKE_BUILD_TYPE)\n@@ -47,7 +49,8 @@ endif()\n ##########\n # LLVM\n ##########\n-if(\"${LLVM_LIBRARY_DIR}\" STREQUAL \"\")\n+if (NOT MLIR_DIR)\n+  if(NOT LLVM_LIBRARY_DIR)\n     if(WIN32)\n       find_package(LLVM 13 REQUIRED COMPONENTS nvptx amdgpu)\n \n@@ -66,80 +69,90 @@ if(\"${LLVM_LIBRARY_DIR}\" STREQUAL \"\")\n     if(APPLE)\n       set(CMAKE_OSX_DEPLOYMENT_TARGET \"10.14\")\n     endif()\n-# sometimes we don't want to use llvm-config, since it may have been downloaded for some specific linux distros\n-else()\n+  # sometimes we don't want to use llvm-config, since it may have been downloaded for some specific linux distros\n+  else()\n     set(LLVM_LDFLAGS \"-L${LLVM_LIBRARY_DIR}\")\n     set(LLVM_LIBRARIES\n-libLLVMNVPTXCodeGen.a\n-libLLVMNVPTXDesc.a\n-libLLVMNVPTXInfo.a\n-libLLVMAMDGPUDisassembler.a\n-libLLVMMCDisassembler.a\n-libLLVMAMDGPUCodeGen.a\n-libLLVMMIRParser.a\n-libLLVMGlobalISel.a\n-libLLVMSelectionDAG.a\n-libLLVMipo.a\n-libLLVMInstrumentation.a\n-libLLVMVectorize.a\n-libLLVMLinker.a\n-libLLVMIRReader.a\n-libLLVMAsmParser.a\n-libLLVMFrontendOpenMP.a\n-libLLVMAsmPrinter.a\n-libLLVMDebugInfoDWARF.a\n-libLLVMCodeGen.a\n-libLLVMTarget.a\n-libLLVMScalarOpts.a\n-libLLVMInstCombine.a\n-libLLVMAggressiveInstCombine.a\n-libLLVMTransformUtils.a\n-libLLVMBitWriter.a\n-libLLVMAnalysis.a\n-libLLVMProfileData.a\n-libLLVMObject.a\n-libLLVMTextAPI.a\n-libLLVMBitReader.a\n-libLLVMAMDGPUAsmParser.a\n-libLLVMMCParser.a\n-libLLVMAMDGPUDesc.a\n-libLLVMAMDGPUUtils.a\n-libLLVMMC.a\n-libLLVMDebugInfoCodeView.a\n-libLLVMDebugInfoMSF.a\n-libLLVMCore.a\n-libLLVMRemarks.a\n-libLLVMBitstreamReader.a\n-libLLVMBinaryFormat.a\n-libLLVMAMDGPUInfo.a\n-libLLVMSupport.a\n-libLLVMDemangle.a\n-libLLVMPasses.a\n-libLLVMAnalysis.a\n-libLLVMTransformUtils.a\n-libLLVMScalarOpts.a\n-libLLVMTransformUtils.a\n-libLLVMipo.a\n-libLLVMObjCARCOpts.a\n-libLLVMCoroutines.a\n-libLLVMAnalysis.a\n-)\n+      libLLVMNVPTXCodeGen.a\n+      libLLVMNVPTXDesc.a\n+      libLLVMNVPTXInfo.a\n+      libLLVMAMDGPUDisassembler.a\n+      libLLVMMCDisassembler.a\n+      libLLVMAMDGPUCodeGen.a\n+      libLLVMMIRParser.a\n+      libLLVMGlobalISel.a\n+      libLLVMSelectionDAG.a\n+      libLLVMipo.a\n+      libLLVMInstrumentation.a\n+      libLLVMVectorize.a\n+      libLLVMLinker.a\n+      libLLVMIRReader.a\n+      libLLVMAsmParser.a\n+      libLLVMFrontendOpenMP.a\n+      libLLVMAsmPrinter.a\n+      libLLVMDebugInfoDWARF.a\n+      libLLVMCodeGen.a\n+      libLLVMTarget.a\n+      libLLVMScalarOpts.a\n+      libLLVMInstCombine.a\n+      libLLVMAggressiveInstCombine.a\n+      libLLVMTransformUtils.a\n+      libLLVMBitWriter.a\n+      libLLVMAnalysis.a\n+      libLLVMProfileData.a\n+      libLLVMObject.a\n+      libLLVMTextAPI.a\n+      libLLVMBitReader.a\n+      libLLVMAMDGPUAsmParser.a\n+      libLLVMMCParser.a\n+      libLLVMAMDGPUDesc.a\n+      libLLVMAMDGPUUtils.a\n+      libLLVMMC.a\n+      libLLVMDebugInfoCodeView.a\n+      libLLVMDebugInfoMSF.a\n+      libLLVMCore.a\n+      libLLVMRemarks.a\n+      libLLVMBitstreamReader.a\n+      libLLVMBinaryFormat.a\n+      libLLVMAMDGPUInfo.a\n+      libLLVMSupport.a\n+      libLLVMDemangle.a\n+      libLLVMPasses.a\n+      libLLVMAnalysis.a\n+      libLLVMTransformUtils.a\n+      libLLVMScalarOpts.a\n+      libLLVMTransformUtils.a\n+      libLLVMipo.a\n+      libLLVMObjCARCOpts.a\n+      libLLVMCoroutines.a\n+      libLLVMAnalysis.a\n+    )\n+  endif()\n+  set (MLIR_DIR ${LLVM_LIBRARY_DIR}/cmake/mlir)\n endif()\n-include_directories(${LLVM_INCLUDE_DIRS})\n \n # Python module\n-if(BUILD_PYTHON_MODULE)\n+if(TRITON_BUILD_PYTHON_MODULE)\n     message(STATUS \"Adding Python module\")\n-    set(PYTHON_SRC_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/src)\n-    include_directories(\".\" ${PYTHON_SRC_PATH} ${PYTHON_INCLUDE_DIRS})\n-    link_directories(${PYTHON_LINK_DIRS})\n+    if (PYTHON_INCLUDE_DIRS)\n+      set(PYTHON_SRC_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/src)\n+      include_directories(\".\" ${PYTHON_SRC_PATH} ${PYTHON_INCLUDE_DIRS})\n+      link_directories(${PYTHON_LINK_DIRS})\n+    else()\n+      find_package(Python3 REQUIRED COMPONENTS Development)\n+      set(PYTHON_SRC_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/src)\n+      include_directories(\".\" ${PYTHON_SRC_PATH} ${Python3_INCLUDE_DIRS})\n+      link_directories(${Python3_LIBRARY_DIRS})\n+      link_libraries(${Python3_LIBRARIES})\n+      add_link_options(${Python3_LINK_OPTIONS})\n+    endif()\n     set(PYTHON_SRC ${PYTHON_SRC_PATH}/main.cc ${PYTHON_SRC_PATH}/triton.cc)\n endif()\n \n \n # # Triton\n # file(GLOB_RECURSE LIBTRITON_SRC lib/*.cc)\n-# if (WIN32 AND BUILD_PYTHON_MODULE)\n+# if (WIN32 AND TRITON_BUILD_PYTHON_MODULE)\n #     find_package(Python3 REQUIRED COMPONENTS Development)\n #     Python3_add_library(triton SHARED ${LIBTRITON_SRC} ${PYTHON_SRC})\n #     set_target_properties(triton PROPERTIES SUFFIX \".pyd\")\n@@ -150,17 +163,18 @@ endif()\n \n \n # MLIR\n-find_package(MLIR REQUIRED CONFIG PATHS ${LLVM_LIBRARY_DIR}/cmake/mlir)\n+find_package(MLIR REQUIRED CONFIG PATHS ${MLIR_DIR})\n \n-list(APPEND CMAKE_MODULE_PATH ${LLVM_LIBRARY_DIR}/cmake/llvm)\n-list(APPEND CMAKE_MODULE_PATH ${LLVM_LIBRARY_DIR}/cmake/mlir)\n+list(APPEND CMAKE_MODULE_PATH \"${MLIR_CMAKE_DIR}\")\n+list(APPEND CMAKE_MODULE_PATH \"${LLVM_CMAKE_DIR}\")\n \n include(TableGen) # required by AddMLIR\n include(AddLLVM)\n include(AddMLIR)\n # include(HandleLLVMOptions) # human-friendly error message\n \n include_directories(${MLIR_INCLUDE_DIRS})\n+include_directories(${LLVM_INCLUDE_DIRS})\n include_directories(${PROJECT_SOURCE_DIR}/include)\n include_directories(${PROJECT_BINARY_DIR}/include) # Tablegen'd files\n # link_directories(${LLVM_LIBRARY_DIR})\n@@ -209,7 +223,7 @@ else()\n endif()\n \n \n-if(BUILD_PYTHON_MODULE AND NOT WIN32)\n+if(TRITON_BUILD_PYTHON_MODULE AND NOT WIN32)\n     set(CMAKE_SHARED_LIBRARY_SUFFIX \".so\")\n     # Check if the platform is MacOS\n     if(APPLE)"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h", "status": "modified", "additions": 41, "deletions": 7, "changes": 48, "file_content_changes": "@@ -2,6 +2,7 @@\n #define TRITON_CONVERSION_TRITON_GPU_TO_LLVM_ASM_FORMAT_H_\n \n #include \"mlir/IR/Value.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/ADT/StringRef.h\"\n #include <memory>\n@@ -99,8 +100,9 @@ struct PTXBuilder {\n     std::string dump() const;\n   };\n \n-  template <typename INSTR = PTXInstr> INSTR *create(const std::string &name) {\n-    instrs.emplace_back(std::make_unique<INSTR>(this, name));\n+  template <typename INSTR = PTXInstr, typename... Args>\n+  INSTR *create(Args &&...args) {\n+    instrs.emplace_back(std::make_unique<INSTR>(this, args...));\n     return static_cast<INSTR *>(instrs.back().get());\n   }\n \n@@ -188,6 +190,7 @@ struct PTXInstrCommon {\n   using Operand = PTXBuilder::Operand;\n \n   // clang-format off\n+  PTXInstrExecution& operator()() { return call({}); }\n   PTXInstrExecution& operator()(Operand* a) { return call({a}); }\n   PTXInstrExecution& operator()(Operand* a, Operand* b) { return call({a, b}); }\n   PTXInstrExecution& operator()(Operand* a, Operand* b, Operand* c) { return call({a, b, c}); }\n@@ -238,30 +241,61 @@ struct PTXInstr : public PTXInstrBase<PTXInstr> {\n // PtxIOInstr store(\"st\");\n // store.predicate(pValue).global().v(32).b(1); // @%0 st.global.v32.b1\n // store.addAddr(addrValue, \"l\", off);\n-struct PtxIOInstr : public PTXInstrBase<PtxIOInstr> {\n-  using PTXInstrBase<PtxIOInstr>::PTXInstrBase;\n+struct PTXIOInstr : public PTXInstrBase<PTXIOInstr> {\n+  using PTXInstrBase<PTXIOInstr>::PTXInstrBase;\n \n   // Add \".global\" suffix to instruction\n-  PtxIOInstr &global(bool predicate = true) {\n+  PTXIOInstr &global(bool predicate = true) {\n     o(\"global\", predicate);\n     return *this;\n   }\n \n   // Add \".v\" suffix to instruction\n-  PtxIOInstr &v(int vecWidth, bool predicate = true) {\n+  PTXIOInstr &v(int vecWidth, bool predicate = true) {\n     if (vecWidth > 1) {\n       o(\"v\" + std::to_string(vecWidth), predicate);\n     }\n     return *this;\n   }\n \n   // Add \".b\" suffix to instruction\n-  PtxIOInstr &b(int width) {\n+  PTXIOInstr &b(int width) {\n     o(\"b\" + std::to_string(width));\n     return *this;\n   }\n };\n \n+struct PTXCpAsyncInstrBase : public PTXInstrBase<PTXCpAsyncInstrBase> {\n+  explicit PTXCpAsyncInstrBase(PTXBuilder *builder)\n+      : PTXInstrBase(builder, \"cp.async\") {}\n+};\n+\n+struct PTXCpAsyncCommitGroupInstr : public PTXCpAsyncInstrBase {\n+  explicit PTXCpAsyncCommitGroupInstr(PTXBuilder *builder)\n+      : PTXCpAsyncInstrBase(builder) {\n+    o(\"commit_group\");\n+  }\n+};\n+\n+struct PTXCpAsyncWaitGroupInstr : public PTXCpAsyncInstrBase {\n+  explicit PTXCpAsyncWaitGroupInstr(PTXBuilder *builder)\n+      : PTXCpAsyncInstrBase(builder) {\n+    o(\"wait_group\");\n+  }\n+};\n+\n+struct PTXCpAsyncLoadInstr : public PTXCpAsyncInstrBase {\n+  explicit PTXCpAsyncLoadInstr(PTXBuilder *builder,\n+                               triton::CacheModifier modifier,\n+                               triton::EvictionPolicy policy)\n+      : PTXCpAsyncInstrBase(builder) {\n+    o(triton::stringifyCacheModifier(modifier).str());\n+    o(\"shared\");\n+    o(\"global\");\n+    o(\"L2::\" + triton::stringifyEvictionPolicy(policy).str());\n+  }\n+};\n+\n // Record the operands and context for \"launching\" a PtxInstr.\n struct PTXInstrExecution {\n   using Operand = PTXBuilder::Operand;"}, {"filename": "include/triton/Dialect/Triton/IR/TritonAttrDefs.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -16,7 +16,7 @@ def TT_CacheModifierAttr : I32EnumAttr<\n def TT_EvictionPolicyAttr : I32EnumAttr<\n     \"EvictionPolicy\", \"\",\n     [\n-        I32EnumAttrCase<\"NORMAL\", 1, \"normal\">,\n+        I32EnumAttrCase<\"NORMAL\", 1, \"evict_normal\">,\n         I32EnumAttrCase<\"EVICT_FIRST\", 2, \"evict_first\">,\n         I32EnumAttrCase<\"EVICT_LAST\", 3, \"evict_last\">\n     ]> {"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -24,6 +24,8 @@ unsigned getElemsPerThread(Attribute layout, ArrayRef<int64_t> shape);\n \n SmallVector<unsigned> getSizePerThread(Attribute layout);\n \n+SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout);\n+\n SmallVector<unsigned> getShapePerCTA(const Attribute &layout);\n \n SmallVector<unsigned> getOrder(const Attribute &layout);"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -54,7 +54,7 @@ in memory. For example, a swizzled row-major layout could store its data\n as follows:\n \n A_{0, 0}  A_{0, 1}  A_{0, 2}  A_{0, 3} ...   [phase 0] \\ per_phase = 2\n-A_{1, 0}  A_{0, 1}  A_{1, 2}  A_{1, 3} ...   [phase 0] /\n+A_{1, 0}  A_{1, 1}  A_{1, 2}  A_{1, 3} ...   [phase 0] /\n groups of vec=2 elements\n are stored contiguously\n _ _ _ _ /\\_ _ _ _"}, {"filename": "lib/Conversion/TritonGPUToLLVM/PtxAsmFormat.cpp", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -141,11 +141,12 @@ PTXInstrExecution &PTXInstrCommon::operator()(ArrayRef<Operand *> oprs) {\n std::string PTXInstrExecution::dump() const {\n   std::string osStr;\n   llvm::raw_string_ostream os(osStr);\n-  if (pred)\n+  if (pred) {\n     if (!pred->repr)\n       os << \"@\" << pred->dump() << \" \";\n     else\n-      os << pred->repr(pred->idx);\n+      os << pred->repr(pred->idx) << \" \";\n+  }\n \n   std::string instrRepr = strJoin(instr->instrParts, \".\");\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 443, "deletions": 84, "changes": 527, "file_content_changes": "@@ -6,6 +6,7 @@\n #include \"mlir/Conversion/LLVMCommon/LoweringOptions.h\"\n #include \"mlir/Conversion/LLVMCommon/Pattern.h\"\n #include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n+#include \"mlir/Conversion/StandardToLLVM/ConvertStandardToLLVM.h\"\n #include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"\n #include \"mlir/Dialect/GPU/GPUDialect.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n@@ -32,6 +33,7 @@ using ::mlir::triton::gpu::getElemsPerThread;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::getSizePerThread;\n+using ::mlir::triton::gpu::getThreadsPerCTA;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n using ::mlir::triton::gpu::SliceEncodingAttr;\n@@ -75,7 +77,7 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n #define add(...) rewriter.create<LLVM::AddOp>(loc, __VA_ARGS__)\n #define mul(...) rewriter.create<LLVM::MulOp>(loc, __VA_ARGS__)\n #define xor_(...) rewriter.create<LLVM::XOrOp>(loc, __VA_ARGS__)\n-#define bit_cast(...) rewriter.create<LLVM::BitcastOp>(loc, __VA_ARGS__)\n+#define bitcast(...) rewriter.create<LLVM::BitcastOp>(loc, __VA_ARGS__)\n #define gep(...) rewriter.create<LLVM::GEPOp>(loc, __VA_ARGS__)\n #define ptr_ty(...) LLVM::LLVMPointerType::get(__VA_ARGS__)\n #define insert_val(...) rewriter.create<LLVM::InsertValueOp>(loc, __VA_ARGS__)\n@@ -86,6 +88,7 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n   rewriter.create<LLVM::ExtractElementOp>(loc, __VA_ARGS__)\n #define load(...) rewriter.create<LLVM::LoadOp>(loc, __VA_ARGS__)\n #define store(val, ptr) rewriter.create<LLVM::StoreOp>(loc, val, ptr)\n+#define select(...) rewriter.create<LLVM::SelectOp>(loc, __VA_ARGS__)\n #define address_of(...) rewriter.create<LLVM::AddressOfOp>(loc, __VA_ARGS__)\n #define barrier rewriter.create<mlir::gpu::BarrierOp>(loc)\n #define undef(...) rewriter.create<LLVM::UndefOp>(loc, __VA_ARGS__)\n@@ -111,16 +114,16 @@ namespace type = mlir::triton::type;\n \n class TritonGPUToLLVMTypeConverter;\n \n-// TODO: keep these before we have better debug log utilities\n+// TODO[goostavz]: Remove these methods after we have better debug log utilities\n template <typename T>\n-void print_array(ArrayRef<T> array, const std::string &str) {\n-  std::cout << str << \": \";\n+void printArray(ArrayRef<T> array, const std::string &info) {\n+  std::cout << info << \": \";\n   for (const T &e : array)\n     std::cout << e << \",\";\n   std::cout << std::endl;\n }\n-template <typename T> void print_scalar(const T &e, const std::string &str) {\n-  std::cout << str << \": \" << e << std::endl;\n+template <typename T> void printScalar(const T &e, const std::string &info) {\n+  std::cout << info << \": \" << e << std::endl;\n }\n \n // FuncOpConversion/FuncOpConversionBase is borrowed from\n@@ -546,6 +549,9 @@ class ConvertTritonGPUOpToLLVMPattern\n     auto warpsPerCTA = blockedLayout.getWarpsPerCTA();\n     unsigned rank = shape.size();\n     SmallVector<unsigned> shapePerCTA = getShapePerCTA(blockedLayout);\n+    SmallVector<unsigned> tilesPerDim(rank);\n+    for (unsigned k = 0; k < rank; ++k)\n+      tilesPerDim[k] = ceil<unsigned>(shape[k], shapePerCTA[k]);\n \n     // step 1, delinearize threadId to get the base index\n     auto multiDimBase =\n@@ -556,8 +562,7 @@ class ConvertTritonGPUOpToLLVMPattern\n     SmallVector<SmallVector<unsigned>> offset(rank);\n     for (unsigned k = 0; k < rank; ++k) {\n       // 1 block in minimum if shape[k] is less than shapePerCTA[k]\n-      for (unsigned blockOffset = 0;\n-           blockOffset < ceil<unsigned>(shape[k], shapePerCTA[k]);\n+      for (unsigned blockOffset = 0; blockOffset < tilesPerDim[k];\n            ++blockOffset)\n         for (unsigned warpOffset = 0; warpOffset < warpsPerCTA[k]; ++warpOffset)\n           for (unsigned threadOffset = 0; threadOffset < threadsPerWarp[k];\n@@ -575,15 +580,12 @@ class ConvertTritonGPUOpToLLVMPattern\n     SmallVector<SmallVector<Value>> multiDimIdx(elemsPerThread,\n                                                 SmallVector<Value>(rank));\n     unsigned totalSizePerThread = product<unsigned>(sizePerThread);\n-    SmallVector<unsigned> threadsPerDim(rank);\n-    for (unsigned k = 0; k < rank; ++k)\n-      threadsPerDim[k] = ceil<unsigned>(shape[k], sizePerThread[k]);\n \n     for (unsigned n = 0; n < elemsPerThread; ++n) {\n       unsigned linearNanoTileId = n / totalSizePerThread;\n       unsigned linearNanoTileElemId = n % totalSizePerThread;\n       SmallVector<unsigned> multiDimNanoTileId =\n-          getMultiDimIndex<unsigned>(linearNanoTileId, threadsPerDim);\n+          getMultiDimIndex<unsigned>(linearNanoTileId, tilesPerDim);\n       SmallVector<unsigned> multiDimNanoTileElemId =\n           getMultiDimIndex<unsigned>(linearNanoTileElemId, sizePerThread);\n       for (unsigned k = 0; k < rank; ++k) {\n@@ -630,7 +632,7 @@ Value convertSplatLikeOp(Type elemType, Type resType, Value constVal,\n   auto tensorTy = resType.cast<RankedTensorType>();\n   auto layout = tensorTy.getEncoding();\n   auto srcType = typeConverter->convertType(elemType);\n-  auto llSrc = bit_cast(srcType, constVal);\n+  auto llSrc = bitcast(srcType, constVal);\n   size_t elemsPerThread = getElemsPerThread(layout, tensorTy.getShape());\n   llvm::SmallVector<Value, 4> elems(elemsPerThread, llSrc);\n   llvm::SmallVector<Type, 4> elemTypes(elems.size(), srcType);\n@@ -706,22 +708,14 @@ struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n   // Get corresponding LLVM element values of \\param value.\n   SmallVector<Value> getLLVMElems(Value value, Value llValue,\n                                   const BlockedEncodingAttr &layout,\n-                                  TypeConverter *typeConverter,\n                                   ConversionPatternRewriter &rewriter,\n                                   Location loc) const {\n     if (!value)\n       return {};\n \n-    auto ty = value.getType().cast<RankedTensorType>();\n-    auto shape = ty.getShape();\n+    auto shape = value.getType().cast<RankedTensorType>().getShape();\n     // Here, we assume that all inputs should have a blockedLayout\n-\n     unsigned valueElems = layout.getElemsPerThread(shape);\n-\n-    auto llvmElemTy = typeConverter->convertType(ty.getElementType());\n-    auto llvmElemPtrPtrTy =\n-        LLVM::LLVMPointerType::get(LLVM::LLVMPointerType::get(llvmElemTy));\n-\n     auto valueVals = getElementsFromStruct(loc, llValue, valueElems, rewriter);\n     return valueVals;\n   }\n@@ -810,22 +804,27 @@ struct StoreOpConversion\n \n     auto [layout, numElems] = getLayout(ptr);\n \n-    auto ptrElems =\n-        getLLVMElems(ptr, llPtr, layout, getTypeConverter(), rewriter, loc);\n-    auto valueElems =\n-        getLLVMElems(value, llValue, layout, getTypeConverter(), rewriter, loc);\n+    auto ptrElems = getLLVMElems(ptr, llPtr, layout, rewriter, loc);\n+    auto valueElems = getLLVMElems(value, llValue, layout, rewriter, loc);\n     assert(ptrElems.size() == valueElems.size());\n \n+    // Determine the vectorization size\n+    size_t vec = getVectorizeSize(ptr, layout);\n     SmallVector<Value> maskElems;\n     if (llMask) {\n-      maskElems =\n-          getLLVMElems(mask, llMask, layout, getTypeConverter(), rewriter, loc);\n+      maskElems = getLLVMElems(mask, llMask, layout, rewriter, loc);\n       assert(valueElems.size() == maskElems.size());\n+      auto maskOrder = mask.getType()\n+                           .cast<RankedTensorType>()\n+                           .getEncoding()\n+                           .cast<BlockedEncodingAttr>()\n+                           .getOrder();\n+\n+      auto maskAxis = getAxisInfo(mask);\n+      size_t maskAlign = std::max<int>(maskAxis->getConstancy(maskOrder[0]), 1);\n+      vec = std::min(vec, maskAlign);\n     }\n \n-    // Determine the vectorization size\n-    size_t vec = getVectorizeSize(ptr, layout);\n-\n     const size_t dtsize =\n         std::max<int>(1, valueElemTy.getIntOrFloatBitWidth() / 8);\n     const size_t valueElemNbits = dtsize * 8;\n@@ -848,7 +847,7 @@ struct StoreOpConversion\n       const bool hasL2EvictPolicy = false;\n \n       PTXBuilder ptxBuilder;\n-      auto &ptxStoreInstr = *ptxBuilder.create<PtxIOInstr>(\"st\");\n+      auto &ptxStoreInstr = *ptxBuilder.create<PTXIOInstr>(\"st\");\n \n       llvm::SmallVector<std::string> asmArgs;\n \n@@ -866,15 +865,15 @@ struct StoreOpConversion\n           Value elem = valueElems[elemOffset];\n           if (elem.getType().isInteger(1))\n             elem = rewriter.create<LLVM::SExtOp>(loc, type::i8Ty(ctx), elem);\n-          elem = bit_cast(valueElemTy, elem);\n+          elem = bitcast(valueElemTy, elem);\n \n           Type u32Ty = typeConverter->convertType(type::u32Ty(ctx));\n           llWord =\n               insert_element(wordTy, llWord, elem,\n                              rewriter.create<LLVM::ConstantOp>(\n                                  loc, u32Ty, IntegerAttr::get(u32Ty, elemIdx)));\n         }\n-        llWord = bit_cast(valArgTy, llWord);\n+        llWord = bitcast(valArgTy, llWord);\n         std::string constraint =\n             (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n         asmArgList->listAppend(ptxBuilder.newOperand(llWord, constraint));\n@@ -1100,14 +1099,12 @@ struct LoadOpConversion\n \n     auto [layout, numElems] = getLayout(ptr);\n \n-    auto ptrElems =\n-        getLLVMElems(ptr, llPtr, layout, getTypeConverter(), rewriter, loc);\n+    auto ptrElems = getLLVMElems(ptr, llPtr, layout, rewriter, loc);\n     assert(ptrElems.size() == numElems);\n \n     SmallVector<Value> maskElems;\n     if (llMask) {\n-      maskElems =\n-          getLLVMElems(mask, llMask, layout, getTypeConverter(), rewriter, loc);\n+      maskElems = getLLVMElems(mask, llMask, layout, rewriter, loc);\n       assert(ptrElems.size() == maskElems.size());\n     }\n \n@@ -1132,8 +1129,7 @@ struct LoadOpConversion\n       splatVal = constAttr.getSplatValue<APInt>().getSExtValue();\n     }\n \n-    auto otherElems =\n-        getLLVMElems(other, llOther, layout, getTypeConverter(), rewriter, loc);\n+    auto otherElems = getLLVMElems(other, llOther, layout, rewriter, loc);\n \n     SmallVector<Value> loadedVals;\n     for (size_t vecStart = 0; vecStart < numElems; vecStart += vec) {\n@@ -1153,7 +1149,7 @@ struct LoadOpConversion\n       const bool hasL2EvictPolicy = false;\n \n       PTXBuilder ptxBuilder;\n-      auto &ld = *ptxBuilder.create<PtxIOInstr>(\"ld\");\n+      auto &ld = *ptxBuilder.create<PTXIOInstr>(\"ld\");\n \n       // TODO(Superjomn) Need to check masks before vectorize the load for all\n       // the values share one predicate? Here assume all the mask values are\n@@ -1198,7 +1194,6 @@ struct LoadOpConversion\n       else\n         ld(dstsOpr, addrOpr, evictOpr).predicate(pred, \"b\");\n \n-      SmallVector<Value> others;\n       if (other) {\n         for (size_t ii = 0; ii < nWords; ++ii) {\n           PTXInstr &mov = *ptxBuilder.create<>(\"mov\");\n@@ -1214,14 +1209,13 @@ struct LoadOpConversion\n                 rewriter, loc, this->getTypeConverter()->getIndexType(), s);\n             v = insert_element(vecTy, v, falseVal, sVal);\n           }\n-          v = bit_cast(IntegerType::get(getContext(), width), v);\n+          v = bitcast(IntegerType::get(getContext(), width), v);\n \n           PTXInstr::Operand *opr{};\n           if (otherIsSplatConstInt) {\n             opr = ptxBuilder.newConstantOperand(splatVal);\n           } else {\n             opr = ptxBuilder.newOperand(v, readConstraint);\n-            others.push_back(v);\n           }\n \n           mov(dstsOpr->listGet(ii), opr).predicateNot(pred, \"b\");\n@@ -1253,7 +1247,7 @@ struct LoadOpConversion\n         } else {\n           curr = ret;\n         }\n-        curr = bit_cast(\n+        curr = bitcast(\n             LLVM::getFixedVectorType(valueElemTy, width / valueElemNbits),\n             curr);\n         rets.push_back(curr);\n@@ -1302,24 +1296,33 @@ struct AddPtrOpConversion\n   matchAndRewrite(triton::AddPtrOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n-    auto resultTy = op.getType().dyn_cast<RankedTensorType>();\n-    auto resultLayout = resultTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n-    assert(resultLayout && \"Unexpected resultLayout in AddPtrOpConversion\");\n-    auto resultShape = resultTy.getShape();\n-    unsigned elems = resultLayout.getElemsPerThread(resultShape);\n-    Type elemTy =\n-        this->getTypeConverter()->convertType(resultTy.getElementType());\n-    SmallVector<Type> types(elems, elemTy);\n-    Type structTy = LLVM::LLVMStructType::getLiteral(getContext(), types);\n-    auto ptrs = getElementsFromStruct(loc, adaptor.ptr(), elems, rewriter);\n-    auto offsets =\n-        getElementsFromStruct(loc, adaptor.offset(), elems, rewriter);\n-    SmallVector<Value> resultVals(elems);\n-    for (unsigned i = 0; i < elems; ++i) {\n-      resultVals[i] = gep(elemTy, ptrs[i], offsets[i]);\n+    auto resultTy = op.getType();\n+    auto resultTensorTy = resultTy.dyn_cast<RankedTensorType>();\n+    if (resultTensorTy) {\n+      auto resultLayout =\n+          resultTensorTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n+      assert(resultLayout && \"Unexpected resultLayout in AddPtrOpConversion\");\n+      auto resultShape = resultTensorTy.getShape();\n+      unsigned elems = resultLayout.getElemsPerThread(resultShape);\n+      Type elemTy =\n+          getTypeConverter()->convertType(resultTensorTy.getElementType());\n+      SmallVector<Type> types(elems, elemTy);\n+      Type structTy = LLVM::LLVMStructType::getLiteral(getContext(), types);\n+      auto ptrs = getElementsFromStruct(loc, adaptor.ptr(), elems, rewriter);\n+      auto offsets =\n+          getElementsFromStruct(loc, adaptor.offset(), elems, rewriter);\n+      SmallVector<Value> resultVals(elems);\n+      for (unsigned i = 0; i < elems; ++i) {\n+        resultVals[i] = gep(elemTy, ptrs[i], offsets[i]);\n+      }\n+      Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n+      rewriter.replaceOp(op, view);\n+    } else {\n+      assert(resultTy.isa<triton::PointerType>());\n+      Type llResultTy = getTypeConverter()->convertType(resultTy);\n+      Value result = gep(llResultTy, adaptor.ptr(), adaptor.offset());\n+      rewriter.replaceOp(op, result);\n     }\n-    Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n-    rewriter.replaceOp(op, view);\n     return success();\n   }\n };\n@@ -1360,9 +1363,8 @@ struct ExtractSliceOpConversion\n \n     // axis > 0 will result in non-contiguous memory access if the result tensor\n     // is an alias of the source tensor.\n-    auto axis =\n-        op->getAttrOfType<IntegerAttr>(\"axis\").cast<IntegerAttr>().getInt();\n-    assert(axis == 0 && \"Only axis=0 is supported for now\");\n+    auto axis = op->getAttrOfType<IntegerAttr>(\"axis\").getInt();\n+    assert(axis == 0 && \"extract_slice: Only axis=0 is supported for now\");\n \n     // Example:\n     // %dst = extract_slice %src, %index {axis = 0}\n@@ -1372,24 +1374,25 @@ struct ExtractSliceOpConversion\n     auto base = product<int64_t>(dstTy.getShape());\n     auto baseVal = createIndexAttrConstant(\n         rewriter, loc, getTypeConverter()->getIndexType(), base);\n-    Value offset = rewriter.create<LLVM::MulOp>(loc, adaptor.index(), baseVal);\n+    Value offset = mul(adaptor.index(), baseVal);\n \n     auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n     auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n-    Value resultVal =\n-        rewriter.create<LLVM::GEPOp>(loc, elemPtrTy, adaptor.src(), offset);\n+    Value resultVal = gep(elemPtrTy, adaptor.src(), offset);\n     rewriter.replaceOp(op, resultVal);\n     return success();\n   }\n };\n \n-template <typename SourceOp, typename DestOp>\n-class BinaryOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n+// A CRTP style of base class.\n+template <typename SourceOp, typename DestOp, typename ConcreteT>\n+class BinaryOpConversionBase\n+    : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n public:\n   using OpAdaptor = typename SourceOp::Adaptor;\n \n-  explicit BinaryOpConversion(LLVMTypeConverter &typeConverter,\n-                              PatternBenefit benefit = 1)\n+  explicit BinaryOpConversionBase(LLVMTypeConverter &typeConverter,\n+                                  PatternBenefit benefit = 1)\n       : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n \n   LogicalResult\n@@ -1410,20 +1413,140 @@ class BinaryOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n         this->getTypeConverter()->convertType(resultTy.getElementType());\n     SmallVector<Type> types(elems, elemTy);\n     Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n-    auto lhss =\n-        this->getElementsFromStruct(loc, adaptor.getLhs(), elems, rewriter);\n-    auto rhss =\n-        this->getElementsFromStruct(loc, adaptor.getRhs(), elems, rewriter);\n+\n+    auto *concreteThis = static_cast<const ConcreteT *>(this);\n+    auto lhss = this->getElementsFromStruct(loc, concreteThis->getLhs(adaptor),\n+                                            elems, rewriter);\n+    auto rhss = this->getElementsFromStruct(loc, concreteThis->getRhs(adaptor),\n+                                            elems, rewriter);\n     SmallVector<Value> resultVals(elems);\n     for (unsigned i = 0; i < elems; ++i) {\n-      resultVals[i] = rewriter.create<DestOp>(loc, elemTy, lhss[i], rhss[i]);\n+      resultVals[i] = concreteThis->createDestOp(op, rewriter, elemTy, lhss[i],\n+                                                 rhss[i], loc);\n     }\n     Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n     rewriter.replaceOp(op, view);\n     return success();\n   }\n };\n \n+template <typename SourceOp, typename DestOp>\n+struct BinaryOpConversion\n+    : public BinaryOpConversionBase<SourceOp, DestOp,\n+                                    BinaryOpConversion<SourceOp, DestOp>> {\n+\n+  explicit BinaryOpConversion(LLVMTypeConverter &typeConverter,\n+                              PatternBenefit benefit = 1)\n+      : BinaryOpConversionBase<SourceOp, DestOp,\n+                               BinaryOpConversion<SourceOp, DestOp>>(\n+            typeConverter, benefit) {}\n+\n+  using OpAdaptor = typename SourceOp::Adaptor;\n+  // An interface to support variant DestOp builder.\n+  DestOp createDestOp(SourceOp op, ConversionPatternRewriter &rewriter,\n+                      Type elemTy, Value lhs, Value rhs, Location loc) const {\n+    return rewriter.create<DestOp>(loc, elemTy, lhs, rhs);\n+  }\n+\n+  // Get the left operand of the op.\n+  Value getLhs(OpAdaptor adaptor) const { return adaptor.getLhs(); }\n+  // Get the right operand of the op.\n+  Value getRhs(OpAdaptor adaptor) const { return adaptor.getRhs(); }\n+};\n+\n+struct CmpIOpConversion\n+    : public BinaryOpConversionBase<triton::gpu::CmpIOp, LLVM::ICmpOp,\n+                                    CmpIOpConversion> {\n+  explicit CmpIOpConversion(LLVMTypeConverter &typeConverter,\n+                            PatternBenefit benefit = 1)\n+      : BinaryOpConversionBase(typeConverter, benefit) {}\n+\n+  // An interface to support variant DestOp builder.\n+  LLVM::ICmpOp createDestOp(triton::gpu::CmpIOp op,\n+                            ConversionPatternRewriter &rewriter, Type elemTy,\n+                            Value lhs, Value rhs, Location loc) const {\n+    return rewriter.create<LLVM::ICmpOp>(\n+        loc, elemTy, ArithCmpIPredicteToLLVM(op.predicate()), lhs, rhs);\n+  }\n+\n+  // Get the left operand of the op.\n+  Value getLhs(OpAdaptor adaptor) const { return adaptor.lhs(); }\n+  // Get the right operand of the op.\n+  Value getRhs(OpAdaptor adaptor) const { return adaptor.rhs(); }\n+\n+  static LLVM::ICmpPredicate\n+  ArithCmpIPredicteToLLVM(arith::CmpIPredicate predicate) {\n+    switch (predicate) {\n+#define __PRED_ENUM(item__)                                                    \\\n+  case arith::CmpIPredicate::item__:                                           \\\n+    return LLVM::ICmpPredicate::item__\n+\n+      __PRED_ENUM(eq);\n+      __PRED_ENUM(ne);\n+      __PRED_ENUM(sgt);\n+      __PRED_ENUM(sge);\n+      __PRED_ENUM(slt);\n+      __PRED_ENUM(sle);\n+      __PRED_ENUM(ugt);\n+      __PRED_ENUM(uge);\n+      __PRED_ENUM(ult);\n+      __PRED_ENUM(ule);\n+\n+#undef __PRED_ENUM\n+    }\n+    return LLVM::ICmpPredicate::eq;\n+  }\n+};\n+\n+struct CmpFOpConversion\n+    : public BinaryOpConversionBase<triton::gpu::CmpFOp, LLVM::FCmpOp,\n+                                    CmpFOpConversion> {\n+  explicit CmpFOpConversion(LLVMTypeConverter &typeConverter,\n+                            PatternBenefit benefit = 1)\n+      : BinaryOpConversionBase(typeConverter, benefit) {}\n+\n+  // An interface to support variant DestOp builder.\n+  LLVM::FCmpOp createDestOp(triton::gpu::CmpFOp op,\n+                            ConversionPatternRewriter &rewriter, Type elemTy,\n+                            Value lhs, Value rhs, Location loc) const {\n+    return rewriter.create<LLVM::FCmpOp>(\n+        loc, elemTy, ArithCmpFPredicteToLLVM(op.predicate()), lhs, rhs);\n+  }\n+\n+  // Get the left operand of the op.\n+  Value getLhs(OpAdaptor adaptor) const { return adaptor.lhs(); }\n+  // Get the right operand of the op.\n+  Value getRhs(OpAdaptor adaptor) const { return adaptor.rhs(); }\n+\n+  static LLVM::FCmpPredicate\n+  ArithCmpFPredicteToLLVM(arith::CmpFPredicate predicate) {\n+    switch (predicate) {\n+#define __PRED_ENUM(item__, item1__)                                           \\\n+  case arith::CmpFPredicate::item__:                                           \\\n+    return LLVM::FCmpPredicate::item1__\n+\n+      __PRED_ENUM(OEQ, oeq);\n+      __PRED_ENUM(ONE, one);\n+      __PRED_ENUM(OGT, ogt);\n+      __PRED_ENUM(OGE, oge);\n+      __PRED_ENUM(OLT, olt);\n+      __PRED_ENUM(OLE, ole);\n+      __PRED_ENUM(ORD, ord);\n+      __PRED_ENUM(UEQ, ueq);\n+      __PRED_ENUM(UGT, ugt);\n+      __PRED_ENUM(ULT, ult);\n+      __PRED_ENUM(ULE, ule);\n+      __PRED_ENUM(UNE, une);\n+      __PRED_ENUM(UNO, uno);\n+      __PRED_ENUM(AlwaysTrue, _true);\n+      __PRED_ENUM(AlwaysFalse, _false);\n+\n+#undef __PRED_ENUM\n+    }\n+    return LLVM::FCmpPredicate::_true;\n+  }\n+};\n+\n struct ConvertLayoutOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::ConvertLayoutOp> {\n public:\n@@ -1581,7 +1704,7 @@ void ConvertLayoutOpConversion::processReplica(\n       auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n       Value ptr = gep(elemPtrTy, smemBase, offset);\n       auto vecTy = vec_ty(llvmElemTy, vec);\n-      ptr = bit_cast(ptr_ty(vecTy, 3), ptr);\n+      ptr = bitcast(ptr_ty(vecTy, 3), ptr);\n       if (stNotRd) {\n         Value valVec = undef(vecTy);\n         for (unsigned v = 0; v < vec; ++v) {\n@@ -1614,7 +1737,7 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n   auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n   Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n   auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n-  smemBase = bit_cast(elemPtrTy, smemBase);\n+  smemBase = bitcast(elemPtrTy, smemBase);\n   auto shape = dstTy.getShape();\n   unsigned rank = dstTy.getRank();\n   SmallVector<unsigned> numReplicates(rank);\n@@ -1732,7 +1855,7 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n   Value minVecVal = idx_val(minVec);\n   Value smemBase = getSharedMemoryBase(loc, rewriter, dst);\n   auto elemPtrTy = ptr_ty(getTypeConverter()->convertType(elemTy), 3);\n-  smemBase = bit_cast(elemPtrTy, smemBase);\n+  smemBase = bitcast(elemPtrTy, smemBase);\n   unsigned numWordsEachRep = product<unsigned>(wordsInEachRep);\n   SmallVector<Value> wordVecs(numWordsEachRep);\n   for (unsigned i = 0; i < numElems; ++i) {\n@@ -1783,7 +1906,7 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n \n         // step 3: store\n         Value smemAddr = gep(elemPtrTy, smemBase, offset);\n-        smemAddr = bit_cast(ptr_ty(wordTy, 3), smemAddr);\n+        smemAddr = bitcast(ptr_ty(wordTy, 3), smemAddr);\n         store(wordVecs[linearWordIdx], smemAddr);\n       }\n     }\n@@ -2126,7 +2249,7 @@ class MMA16816SmemLoader {\n           for (int e = 0; e < 4; ++e)\n             i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n                                           i8Elems[m][e], i32_val(e));\n-          i32Elems[m] = bit_cast(i32_ty, i8v4Elems[m]);\n+          i32Elems[m] = bitcast(i32_ty, i8v4Elems[m]);\n         }\n       } else { // k first\n         Value offset = i32_val(sOffsetElem);\n@@ -2144,7 +2267,7 @@ class MMA16816SmemLoader {\n           for (int e = 0; e < 4; ++e)\n             i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n                                           i8Elems[m][e], i32_val(e));\n-          i32Elems[m] = bit_cast(i32_ty, i8v4Elems[m]);\n+          i32Elems[m] = bitcast(i32_ty, i8v4Elems[m]);\n         }\n       }\n \n@@ -2628,7 +2751,7 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n     Type smemPtrTy = helper.getShemPtrTy();\n     for (int i = 0; i < numPtrs; ++i) {\n       ptrs[i] =\n-          bit_cast(smemPtrTy, gep(smemPtrTy, llTensor, ValueRange({offs[i]})));\n+          bitcast(smemPtrTy, gep(smemPtrTy, llTensor, ValueRange({offs[i]})));\n     }\n \n     bool needTrans = kOrder != order[0];\n@@ -2777,6 +2900,229 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n   }\n };\n \n+struct AsyncWaitOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::AsyncWaitOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::gpu::AsyncWaitOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::gpu::AsyncWaitOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    PTXBuilder ptxBuilder;\n+    auto &asyncWaitOp = *ptxBuilder.create<PTXCpAsyncWaitGroupInstr>();\n+    auto num = op->getAttrOfType<IntegerAttr>(\"num\").getInt();\n+    asyncWaitOp(ptxBuilder.newConstantOperand(num));\n+\n+    auto ctx = op.getContext();\n+    auto loc = op.getLoc();\n+    auto voidTy = LLVM::LLVMVoidType::get(ctx);\n+    auto ret = ptxBuilder.launch(rewriter, loc, voidTy);\n+\n+    // Safe to remove the op since it doesn't have any return value.\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+};\n+\n+struct InsertSliceAsyncOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::InsertSliceAsyncOp>,\n+      public LoadStoreConversionBase {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::gpu::InsertSliceAsyncOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  InsertSliceAsyncOpConversion(LLVMTypeConverter &converter,\n+                               const Allocation *allocation, Value smem,\n+                               AxisInfoAnalysis &axisAnalysisPass,\n+                               PatternBenefit benefit)\n+      : ConvertTritonGPUOpToLLVMPattern<triton::gpu::InsertSliceAsyncOp>(\n+            converter, allocation, smem, benefit),\n+        LoadStoreConversionBase(axisAnalysisPass) {}\n+\n+  LogicalResult\n+  matchAndRewrite(triton::gpu::InsertSliceAsyncOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    // insert_slice_async %src, %dst, %index, %mask, %other\n+    auto loc = op.getLoc();\n+    Value src = op.src();\n+    Value dst = op.dst();\n+    Value res = op.result();\n+    Value mask = op.mask();\n+    Value other = op.other();\n+    assert(allocation->getBufferId(res) == Allocation::InvalidBufferId &&\n+           \"Only support in-place insert_slice_async for now\");\n+\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto resTy = dst.getType().cast<RankedTensorType>();\n+    auto resElemTy = resTy.getElementType();\n+    auto srcBlockedLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+    auto resSharedLayout = resTy.getEncoding().cast<SharedEncodingAttr>();\n+    auto srcShape = srcTy.getShape();\n+    assert(srcShape.size() == 2 &&\n+           \"insert_slice_async: Unexpected rank of %src\");\n+\n+    Value llDst = adaptor.dst();\n+    Value llSrc = adaptor.src();\n+    Value llMask = adaptor.mask();\n+    Value llOther = adaptor.other();\n+    Value llIndex = adaptor.index();\n+\n+    // %src\n+    auto srcElems = getLLVMElems(src, llSrc, srcBlockedLayout, rewriter, loc);\n+\n+    // %dst\n+    auto axis = op->getAttrOfType<IntegerAttr>(\"axis\").getInt();\n+    assert(axis == 0 && \"insert_slice_async: Only axis=0 is supported for now\");\n+    auto dstBase = createIndexAttrConstant(rewriter, loc,\n+                                           getTypeConverter()->getIndexType(),\n+                                           product<int64_t>(resTy.getShape()));\n+    Value offset = mul(llIndex, dstBase);\n+    auto dstPtrTy = LLVM::LLVMPointerType::get(\n+        getTypeConverter()->convertType(resTy.getElementType()), 3);\n+    Value dstPtrBase = gep(dstPtrTy, llDst, offset);\n+\n+    // %mask\n+    SmallVector<Value> maskElems;\n+    if (llMask) {\n+      maskElems = getLLVMElems(mask, llMask, srcBlockedLayout, rewriter, loc);\n+      assert(srcElems.size() == maskElems.size());\n+    }\n+\n+    // %other\n+    SmallVector<Value> otherElems;\n+    if (llOther) {\n+      // TODO(Keren): support \"other\" tensor.\n+      // It's not necessary for now because the pipeline pass will skip\n+      // generating insert_slice_async if the load op has any \"other\" tensor.\n+      assert(false && \"insert_slice_async: Other value not supported yet\");\n+      otherElems =\n+          getLLVMElems(other, llOther, srcBlockedLayout, rewriter, loc);\n+      assert(srcElems.size() == otherElems.size());\n+    }\n+\n+    unsigned inVec = getVectorizeSize(src, srcBlockedLayout);\n+    unsigned outVec = resSharedLayout.getVec();\n+    unsigned minVec = std::min(outVec, inVec);\n+    unsigned numElems = getElemsPerThread(srcBlockedLayout, srcShape);\n+    unsigned perPhase = resSharedLayout.getPerPhase();\n+    unsigned maxPhase = resSharedLayout.getMaxPhase();\n+    auto sizePerThread = srcBlockedLayout.getSizePerThread();\n+    auto threadsPerWarp = srcBlockedLayout.getThreadsPerWarp();\n+    auto warpsPerCTA = srcBlockedLayout.getWarpsPerCTA();\n+    auto threadsPerCTA = getThreadsPerCTA(srcBlockedLayout);\n+\n+    auto inOrder = srcBlockedLayout.getOrder();\n+    auto outOrder = resSharedLayout.getOrder();\n+    // If perPhase * maxPhase > threadsPerCTA, we need to swizzle over elements\n+    // across phases.\n+    // If perPhase * maxPhase == threadsPerCTA, swizzle is not allowd\n+    auto numSwizzleRows = std::max<unsigned>(\n+        (perPhase * maxPhase) / threadsPerCTA[inOrder[1]], 1);\n+    // A sharedLayout encoding has a \"vec\" parameter.\n+    // On the column dimension, if inVec > outVec, it means we have to divide\n+    // single vector read into multiple ones\n+    auto numVecCols = std::max<unsigned>(inVec / outVec, 1);\n+\n+    auto srcIndices = emitIndices(loc, rewriter, srcBlockedLayout, srcShape);\n+    // <<tileVecIdxRow, tileVecIdxCol>, TileOffset>\n+    DenseMap<std::pair<unsigned, unsigned>, Value> tileOffsetMap;\n+    for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n+      // minVec = 2, inVec = 4, outVec = 2\n+      //   baseOffsetCol = 0   baseOffsetCol = 0\n+      //   tileVecIdxCol = 0   tileVecIdxCol = 1\n+      //                -/\\-   -/\\-\n+      //               [|x x| |x x| x x x x x]\n+      //               [|x x| |x x| x x x x x]\n+      // baseOffsetRow [|x x| |x x| x x x x x]\n+      //               [|x x| |x x| x x x x x]\n+      auto vecIdx = elemIdx / minVec;\n+      auto vecIdxCol = vecIdx % (sizePerThread[inOrder[0]] / minVec);\n+      auto vecIdxRow = vecIdx / (sizePerThread[inOrder[0]] / minVec);\n+      auto baseOffsetCol =\n+          vecIdxCol / numVecCols * numVecCols * threadsPerCTA[inOrder[0]];\n+      auto baseOffsetRow = vecIdxRow / numSwizzleRows * numSwizzleRows *\n+                           threadsPerCTA[inOrder[1]];\n+      auto baseOffset = (baseOffsetRow * srcShape[inOrder[0]] + baseOffsetCol);\n+      auto tileVecIdxCol = vecIdxCol % numVecCols;\n+      auto tileVecIdxRow = vecIdxRow % numSwizzleRows;\n+\n+      if (!tileOffsetMap.count({tileVecIdxRow, tileVecIdxCol})) {\n+        // Swizzling\n+        // Since the swizzling index is related to outVec, and we know minVec\n+        // already, inVec doesn't matter\n+        //\n+        // (Numbers represent row indices)\n+        // Example1:\n+        // outVec = 2, inVec = 2, minVec = 2\n+        // outVec = 2, inVec = 4, minVec = 2\n+        //     | [1 2] [3 4]  ... [15 16] |\n+        //     | [3 4] [5 6]  ... [1 2]   |\n+        // Example2:\n+        // outVec = 4, inVec = 2, minVec = 2\n+        //     | [1 2 3 4] [5 6 7 8] ... [13 14 15 16] |\n+        //     | [5 6 7 8] [9 10 11 12] ... [1 2 3 4]  |\n+        auto srcIdx = srcIndices[tileVecIdxRow * sizePerThread[inOrder[0]]];\n+        Value phase = urem(udiv(srcIdx[inOrder[1]], i32_val(perPhase)),\n+                           i32_val(maxPhase));\n+        Value rowOffset =\n+            mul(srcIdx[inOrder[1]], i32_val(srcShape[inOrder[0]]));\n+        Value colOffset =\n+            add(srcIdx[inOrder[0]], i32_val(tileVecIdxCol * minVec));\n+        Value swizzleIdx = udiv(colOffset, i32_val(outVec));\n+        Value swizzleColOffset =\n+            add(mul(xor_(swizzleIdx, phase), i32_val(outVec)),\n+                urem(colOffset, i32_val(outVec)));\n+        Value tileOffset = add(rowOffset, swizzleColOffset);\n+        tileOffsetMap[{tileVecIdxRow, tileVecIdxCol}] =\n+            gep(dstPtrTy, dstPtrBase, tileOffset);\n+      }\n+\n+      // 16 * 8 = 128bits\n+      auto maxBitWidth =\n+          std::max<unsigned>(128, resElemTy.getIntOrFloatBitWidth());\n+      auto vecBitWidth = resElemTy.getIntOrFloatBitWidth() * minVec;\n+      auto bitWidth = std::min<unsigned>(maxBitWidth, vecBitWidth);\n+      auto numWords = vecBitWidth / bitWidth;\n+      auto numWordElems = bitWidth / resElemTy.getIntOrFloatBitWidth();\n+\n+      // XXX(Keren): Tune CG and CA here.\n+      CacheModifier srcCacheModifier =\n+          bitWidth == 128 ? CacheModifier::CG : CacheModifier::CA;\n+      assert(bitWidth == 128 || bitWidth == 64 || bitWidth == 32);\n+\n+      for (int wordIdx = 0; wordIdx < numWords; ++wordIdx) {\n+        PTXBuilder ptxBuilder;\n+        auto &copyAsyncOp = *ptxBuilder.create<PTXCpAsyncLoadInstr>(\n+            srcCacheModifier, op.evict());\n+\n+        auto tileOffset = tileOffsetMap[{tileVecIdxRow, tileVecIdxCol}];\n+        auto *dstOperand =\n+            ptxBuilder.newAddrOperand(tileOffset, \"r\", baseOffset);\n+        auto *srcOperand = ptxBuilder.newAddrOperand(srcElems[vecIdx], \"l\");\n+        auto *copySize = ptxBuilder.newConstantOperand(bitWidth);\n+        auto *srcSize = copySize;\n+        if (op.mask()) {\n+          // We don't use predicate in this case, setting src-size to 0\n+          // if there's any mask. cp.async will automatically fill the\n+          // remaining slots with 0 if cp-size > src-size.\n+          // XXX(Keren): Always assume other = 0 for now.\n+          auto selectOp = select(maskElems[vecIdx + wordIdx * numWordElems],\n+                                 i32_val(bitWidth), i32_val(0));\n+          srcSize = ptxBuilder.newOperand(selectOp, \"r\");\n+        }\n+        copyAsyncOp(dstOperand, srcOperand, copySize, srcSize);\n+        ptxBuilder.launch(rewriter, loc, LLVM::LLVMVoidType::get(getContext()));\n+      }\n+    }\n+\n+    PTXBuilder ptxBuilder;\n+    ptxBuilder.create<PTXCpAsyncCommitGroupInstr>()->operator()();\n+    auto ret =\n+        ptxBuilder.launch(rewriter, loc, LLVM::LLVMVoidType::get(getContext()));\n+    rewriter.replaceOp(op, ret);\n+    return success();\n+  }\n+};\n+\n void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                   RewritePatternSet &patterns, int numWarps,\n                                   AxisInfoAnalysis &axisInfoAnalysis,\n@@ -2786,6 +3132,7 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<AllocTensorOpConversion>(typeConverter, allocation, smem,\n                                         benefit);\n   patterns.add<ArithConstantSplatOpConversion>(typeConverter, benefit);\n+  patterns.add<AsyncWaitOpConversion>(typeConverter, benefit);\n   patterns.add<BinaryOpConversion<arith::AddIOp, LLVM::AddOp>>(typeConverter,\n                                                                benefit);\n   patterns.add<BinaryOpConversion<arith::AddFOp, LLVM::FAddOp>>(typeConverter,\n@@ -2794,12 +3141,22 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                                                benefit);\n   patterns.add<BinaryOpConversion<arith::MulFOp, LLVM::FMulOp>>(typeConverter,\n                                                                 benefit);\n+\n+  patterns.add<BinaryOpConversion<arith::AndIOp, LLVM::AndOp>>(typeConverter,\n+                                                               benefit);\n+  patterns.add<BinaryOpConversion<arith::OrIOp, LLVM::OrOp>>(typeConverter,\n+                                                             benefit);\n+\n+  patterns.add<CmpIOpConversion>(typeConverter, benefit);\n+  patterns.add<CmpFOpConversion>(typeConverter, benefit);\n   patterns.add<BroadcastOpConversion>(typeConverter, benefit);\n   patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n                                           benefit);\n   patterns.add<ExtractSliceOpConversion>(typeConverter, allocation, smem,\n                                          benefit);\n   patterns.add<GetProgramIdOpConversion>(typeConverter, benefit);\n+  patterns.add<InsertSliceAsyncOpConversion>(typeConverter, allocation, smem,\n+                                             axisInfoAnalysis, benefit);\n   patterns.add<LoadOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n   patterns.add<MakeRangeOpConversion>(typeConverter, benefit);\n   patterns.add<ReturnOpConversion>(typeConverter, benefit);\n@@ -2857,6 +3214,7 @@ class ConvertTritonGPUToLLVM\n     mlir::arith::populateArithmeticToLLVMConversionPatterns(typeConverter,\n                                                             patterns);\n     mlir::populateMathToLLVMConversionPatterns(typeConverter, patterns);\n+    mlir::populateStdToLLVMConversionPatterns(typeConverter, patterns);\n \n     mlir::populateGpuToNVVMConversionPatterns(typeConverter, patterns);\n \n@@ -2913,6 +3271,7 @@ TritonLLVMConversionTarget::TritonLLVMConversionTarget(\n   // addIllegalDialect<triton::TritonDialect>();\n   // addIllegalDialect<triton::gpu::TritonGPUDialect>();\n   addIllegalDialect<mlir::gpu::GPUDialect>();\n+  addIllegalDialect<mlir::StandardOpsDialect>();\n   addLegalOp<mlir::UnrealizedConversionCastOp>();\n }\n "}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "@@ -72,6 +72,21 @@ SmallVector<unsigned> getSizePerThread(Attribute layout) {\n   }\n }\n \n+SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout) {\n+  SmallVector<unsigned> threads;\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+    for (int d = 0, n = blockedLayout.getOrder().size(); d < n; ++d)\n+      threads.push_back(blockedLayout.getThreadsPerWarp()[d] *\n+                        blockedLayout.getWarpsPerCTA()[d]);\n+  } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    assert(0 && \"Unimplemented usage of MmaEncodingAttr\");\n+  } else {\n+    assert(0 && \"Unimplemented usage of getShapePerCTA\");\n+  }\n+\n+  return threads;\n+}\n+\n SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n   SmallVector<unsigned> shape;\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -306,6 +306,8 @@ class MoveArgConvertOutOfLoop : public mlir::RewritePattern {\n         auto fwdCvtIt = std::find_if(opIt, fwdEndIt, isCvt);\n         auto bwdCvtIt = std::find_if(bwdBeginIt, opIt, isCvt);\n \n+        if (!iterArg.value().getType().isa<RankedTensorType>())\n+          continue;\n         if (fwdCvtIt != fwdEndIt) {\n           auto newFor = tryConvertIterArg(forOp, rewriter, iterArg.index(),\n                                           (*fwdCvtIt)->getResult(0).getType());"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -1,4 +1,5 @@\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n+#include \"mlir/Conversion/Passes.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/ExecutionEngine/ExecutionEngine.h\"\n #include \"mlir/ExecutionEngine/OptUtils.h\"\n@@ -135,6 +136,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n       /*printAfterOnlyOnChange=*/true,\n       /*printAfterOnlyOnFailure*/ false, llvm::dbgs(), printingFlags);\n \n+  pm.addPass(mlir::createLowerToCFGPass());\n   pm.addPass(createConvertTritonGPUToLLVMPass());\n   // Conanicalize to eliminate the remaining UnrealizedConversionCastOp\n   pm.addPass(mlir::createCanonicalizerPass());"}, {"filename": "python/setup.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -97,8 +97,8 @@ def build_extension(self, ext):\n         python_include_dirs = [distutils.sysconfig.get_python_inc()] + ['/usr/local/cuda/include']\n         cmake_args = [\n             \"-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=\" + extdir,\n-            \"-DBUILD_TUTORIALS=OFF\",\n-            \"-DBUILD_PYTHON_MODULE=ON\",\n+            \"-DTRITON_BUILD_TUTORIALS=OFF\",\n+            \"-DTRITON_BUILD_PYTHON_MODULE=ON\",\n             \"-DLLVM_INCLUDE_DIRS=\" + llvm_include_dir,\n             \"-DLLVM_LIBRARY_DIR=\" + llvm_library_dir,\n             # '-DPYTHON_EXECUTABLE=' + sys.executable,"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 9, "deletions": 2, "changes": 11, "file_content_changes": "@@ -3,6 +3,7 @@\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/Verifier.h\"\n \n+#include \"mlir/Conversion/Passes.h\"\n #include \"mlir/Pass/Pass.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"mlir/Transforms/Passes.h\"\n@@ -1185,8 +1186,12 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::PassManager &self) {\n              self.addPass(mlir::createTritonGPUVerifier());\n            })\n-      .def(\"add_triton_gpu_to_llvm\", [](mlir::PassManager &self) {\n-        self.addPass(mlir::triton::createConvertTritonGPUToLLVMPass());\n+      .def(\"add_triton_gpu_to_llvm\",\n+           [](mlir::PassManager &self) {\n+             self.addPass(mlir::triton::createConvertTritonGPUToLLVMPass());\n+           })\n+      .def(\"add_scf_to_cfg\", [](mlir::PassManager &self) {\n+        self.addPass(mlir::createLowerToCFGPass());\n       });\n }\n \n@@ -1205,6 +1210,8 @@ void init_triton_translation(py::module &m) {\n         llvm::LLVMContext llvmContext;\n         auto llvmModule =\n             ::mlir::triton::translateTritonGPUToLLVMIR(&llvmContext, op);\n+        if (!llvmModule)\n+          llvm::report_fatal_error(\"Failed to translate TritonGPU to LLVM IR.\");\n \n         std::string str;\n         llvm::raw_string_ostream os(str);"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,6 +1,6 @@\n import pytest\n import torch\n-from torch.testing import assert_allclose\n+from torch.testing import assert_close\n \n import triton\n import triton.language as tl\n@@ -49,4 +49,4 @@ def test_gemm_impl(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n                         num_warps=NUM_WARPS)\n     golden = torch.matmul(a, b)\n     torch.set_printoptions(profile=\"full\")\n-    assert_allclose(c, golden, rtol=1e-3, atol=1e-3)\n+    assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)"}, {"filename": "python/tests/test_transpose.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,6 +1,6 @@\n import pytest\n import torch\n-from torch.testing import assert_allclose\n+from torch.testing import assert_close\n \n import triton\n import triton.language as tl\n@@ -44,4 +44,4 @@ def test_convert_layout_impl(NUM_WARPS, SIZE_M, SIZE_N):\n     z = torch.empty((SIZE_N, SIZE_M), device=x.device, dtype=x.dtype)\n     kernel[grid](x_ptr=x, stride_xm=x.stride(0), z_ptr=z, stride_zn=z.stride(0), SIZE_M=SIZE_M, SIZE_N=SIZE_N, num_warps=NUM_WARPS)\n     golden_z = torch.t(x)\n-    assert_allclose(z, golden_z, rtol=1e-7, atol=1e-7)\n+    assert_close(z, golden_z, rtol=1e-7, atol=1e-7, check_dtype=False)"}, {"filename": "python/tests/test_vecadd.py", "status": "added", "additions": 215, "deletions": 0, "changes": 215, "file_content_changes": "@@ -0,0 +1,215 @@\n+import math\n+import random\n+\n+import pytest\n+import torch\n+from torch.testing import assert_close\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@pytest.mark.parametrize('num_warps, block_size, iter_size', [\n+    [4, 256, 1],\n+    [4, 1024, 256],\n+])\n+def test_vecadd_scf_no_mask(num_warps, block_size, iter_size):\n+\n+    @triton.jit\n+    def kernel(x_ptr,\n+               y_ptr,\n+               z_ptr,\n+               block_size,\n+               iter_size: tl.constexpr):\n+        pid = tl.program_id(axis=0)\n+        for i in range(0, block_size, iter_size):\n+            offset = pid * block_size + tl.arange(0, iter_size)\n+            x_ptrs = x_ptr + offset\n+            y_ptrs = y_ptr + offset\n+\n+            x = tl.load(x_ptrs)\n+            y = tl.load(y_ptrs)\n+            z = x + y\n+            z_ptrs = z_ptr + offset\n+            tl.store(z_ptrs, z)\n+\n+            x_ptr += iter_size\n+            y_ptr += iter_size\n+            z_ptr += iter_size\n+\n+    x = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n+    y = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n+    z = torch.empty((block_size,), device=x.device, dtype=x.dtype)\n+\n+    grid = lambda EA: (x.shape.numel() // (block_size),)\n+    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z,\n+                 block_size=x.shape[0], iter_size=iter_size, num_warps=num_warps)\n+\n+    golden_z = x + y\n+    assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n+\n+\n+@pytest.mark.parametrize('shape, num_warps, block_size, iter_size', [\n+    [(127, 3), 2, 128, 1],\n+    [(127, 3), 2, 128, 32],\n+])\n+def test_vecadd_scf_mask(shape, num_warps, block_size, iter_size):\n+    @triton.jit\n+    def kernel(x_ptr,\n+               y_ptr,\n+               z_ptr,\n+               num_elements,\n+               block_size: tl.constexpr,\n+               iter_size: tl.constexpr\n+               ):\n+        '''\n+        @block_size: size of a block\n+        @iter_size: size of the iteration, a block has multiple iterations\n+        @num_elements: number of elements\n+        '''\n+        pid = tl.program_id(axis=0)\n+        for i in range(math.ceil(block_size / iter_size)):\n+            # TODO: a bug here, if put the offset outside the forloop, there will be a GPU mis-aligned error.\n+            offset = pid * block_size + tl.arange(0, iter_size)\n+            x_ptrs = x_ptr + offset\n+            y_ptrs = y_ptr + offset\n+\n+            x = tl.load(x_ptrs, mask=offset < num_elements)\n+            y = tl.load(y_ptrs, mask=offset < num_elements)\n+            z = x + y\n+            z_ptrs = z_ptr + offset\n+            tl.store(z_ptrs, z, mask=offset < num_elements)\n+\n+            x_ptr += iter_size\n+            y_ptr += iter_size\n+            z_ptr += iter_size\n+\n+    x = torch.randn(shape, device='cuda', dtype=torch.float32)\n+    y = torch.randn(shape, device='cuda', dtype=torch.float32)\n+    z = torch.empty(shape, device=x.device, dtype=x.dtype)\n+\n+    grid = lambda EA: (math.ceil(x.numel() / block_size),)\n+    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z,\n+                 block_size=x.shape[0], iter_size=iter_size, num_warps=num_warps,\n+                 num_elements=x.numel())\n+\n+    golden_z = x + y\n+    assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n+\n+\n+def vecadd_no_scf_tester(num_warps, block_size, shape):\n+    @triton.jit\n+    def kernel(x_ptr,\n+               y_ptr,\n+               z_ptr,\n+               n_elements,\n+               block_size_N: tl.constexpr):\n+        pid = tl.program_id(axis=0)\n+\n+        offset = pid * block_size_N + tl.arange(0, block_size_N)\n+        x_ptrs = x_ptr + offset\n+        y_ptrs = y_ptr + offset\n+\n+        mask = offset < n_elements\n+\n+        x = tl.load(x_ptrs, mask=mask)\n+        y = tl.load(y_ptrs, mask=mask)\n+        z = x + y\n+        z_ptrs = z_ptr + offset\n+        tl.store(z_ptrs, z, mask=mask)\n+\n+    x = torch.randn(shape, device='cuda', dtype=torch.float32)\n+    y = torch.randn(shape, device='cuda', dtype=torch.float32)\n+    z = torch.empty(shape, device=x.device, dtype=x.dtype)\n+\n+    grid = lambda EA: (math.ceil(x.shape.numel() / block_size),)\n+    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z, n_elements=x.shape.numel(), block_size_N=block_size, num_warps=num_warps)\n+\n+    golden_z = x + y\n+    assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n+\n+\n+def vecadd_fcmp_no_scf_tester(num_warps, block_size, shape):\n+    '''\n+    vecadd tester with float comparation as load/store mask.\n+    '''\n+    @triton.jit\n+    def kernel(x_ptr,\n+               y_ptr,\n+               z_ptr,\n+               n_elements,\n+               block_size_N: tl.constexpr):\n+        pid = tl.program_id(axis=0)\n+\n+        offset = pid * block_size_N + tl.arange(0, block_size_N)\n+        x_ptrs = x_ptr + offset\n+        y_ptrs = y_ptr + offset\n+\n+        io_mask = offset < n_elements\n+        x = tl.load(x_ptrs, mask=io_mask)\n+        y = tl.load(y_ptrs, mask=io_mask)\n+\n+        z = x + y\n+        val_mask = offset < n_elements and (z < 0. or z > 1.)\n+\n+        z_ptrs = z_ptr + offset\n+        tl.store(z_ptrs, z, mask=val_mask)\n+\n+    x = torch.randn(shape, device='cuda', dtype=torch.float32)\n+    y = torch.randn(shape, device='cuda', dtype=torch.float32)\n+    z = torch.zeros(shape, device=x.device, dtype=x.dtype)\n+\n+    grid = lambda EA: (math.ceil(x.shape.numel() / block_size),)\n+    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z, n_elements=x.shape.numel(), block_size_N=block_size, num_warps=num_warps)\n+\n+    golden_z: torch.Tensor = x + y\n+    gz_data = torch.flatten(golden_z)\n+    for i in range(golden_z.numel()):\n+        gz_data[i] = gz_data[i] if gz_data[i] < 0. or gz_data[i] > 1. else 0.\n+\n+    assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n+\n+\n+@pytest.mark.parametrize('num_warps, block_size, shape', [\n+    [4, 256, (256,)],\n+    [2, 256, (256,)],\n+    [1, 256, (256,)],\n+    [4, 16, (256,)],\n+    [2, 64, (256,)],\n+    [1, 128, (256,)],\n+])\n+def test_vecadd_no_scf(num_warps, block_size, shape):\n+    vecadd_no_scf_tester(num_warps, block_size, shape)\n+\n+\n+@pytest.mark.parametrize('num_warps, block_size, shape', [\n+    [1, 128, (256 + 1,)],\n+    [1, 256, (256 + 1,)],\n+    [2, 256, (3, 256 + 7)],\n+    [4, 256, (3, 256 + 7)],\n+])\n+def test_vecadd__no_scf_masked(num_warps, block_size, shape):\n+    vecadd_no_scf_tester(num_warps, block_size, shape)\n+\n+\n+def test_vecadd_no_scf_masked_randomly():\n+    random.seed(0)  # fix seed to make random test reproducible\n+    for i in range(10):\n+        num_elements = random.randint(128, 2048)\n+        shape = (num_elements,)\n+        max_warps = num_elements // 32  # floor div\n+        for num_warps in range(1, max_warps):\n+            is_power2 = num_warps & (num_warps - 1) == 0 and num_warps != 0\n+            if not is_power2: continue\n+            block_size = min(32, num_warps * 32)\n+            vecadd_no_scf_tester(num_warps, block_size, shape)\n+\n+\n+@pytest.mark.parametrize('num_warps, block_size, shape', [\n+    [1, 128, (256 + 1,)],\n+    [1, 256, (256 + 1,)],\n+    [2, 256, (3, 256 + 7)],\n+    [4, 256, (3, 256 + 7)],\n+])\n+def test_vecadd_fcmp_no_scf_masked(num_warps, block_size, shape):\n+    vecadd_fcmp_no_scf_tester(num_warps, block_size, shape)"}, {"filename": "python/tests/test_vecadd_no_scf.py", "status": "removed", "additions": 0, "deletions": 42, "changes": 42, "file_content_changes": "@@ -1,42 +0,0 @@\n-import torch\n-from torch.testing import assert_allclose\n-\n-import triton\n-import triton.language as tl\n-\n-\n-def vecadd_no_scf_tester(num_warps, block_size):\n-    @triton.jit\n-    def kernel(x_ptr,\n-               y_ptr,\n-               z_ptr,\n-               BLOCK_SIZE_N: tl.constexpr):\n-        pid = tl.program_id(axis=0)\n-        offset = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n-        x_ptrs = x_ptr + offset\n-        y_ptrs = y_ptr + offset\n-        x = tl.load(x_ptrs)\n-        y = tl.load(y_ptrs)\n-        z = x + y\n-        z_ptrs = z_ptr + offset\n-        tl.store(z_ptrs, z)\n-\n-    x = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n-    y = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n-    z = torch.empty((block_size,), device=x.device, dtype=x.dtype)\n-\n-    grid = lambda EA: (x.shape.numel() // block_size,)\n-    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z, BLOCK_SIZE_N=block_size, num_warps=num_warps)\n-\n-    golden_z = x + y\n-    assert_allclose(z, golden_z, rtol=1e-7, atol=1e-7)\n-\n-\n-def test_vecadd_no_scf():\n-    vecadd_no_scf_tester(num_warps=4, block_size=256)\n-    vecadd_no_scf_tester(num_warps=2, block_size=256)\n-    vecadd_no_scf_tester(num_warps=1, block_size=256)\n-\n-\n-if __name__ == '__main__':\n-    test_vecadd_no_scf()"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 22, "deletions": 0, "changes": 22, "file_content_changes": "@@ -699,6 +699,28 @@ def visit_Call(self, node):\n     def visit_Constant(self, node):\n         return triton.language.constexpr(node.value)\n \n+    def visit_BoolOp(self, node: ast.BoolOp):\n+        assert len(node.values) == 2\n+        lhs = self.visit(node.values[0])\n+        rhs = self.visit(node.values[1])\n+        if isinstance(lhs, triton.language.constexpr):\n+            lhs = lhs.value\n+        if isinstance(rhs, triton.language.constexpr):\n+            rhs = rhs.value\n+\n+        fn = {\n+            ast.And: 'logical_and',\n+            ast.Or: 'logical_or',\n+        }[type(node.op)]\n+\n+        if self.is_triton_tensor(lhs):\n+            return getattr(lhs, fn)(rhs, _builder=self.builder)\n+        elif self.is_triton_tensor(rhs):\n+            fn = fn[:2] + 'r' + fn[2:]\n+            return getattr(rhs, fn)(lhs, _builder=self.builder)\n+        else:\n+            return getattr(lhs, fn)(rhs)\n+\n     if sys.version_info < (3, 8):\n         def visit_NameConstant(self, node):\n             return triton.language.constexpr(node.value)"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 10, "deletions": 2, "changes": 12, "file_content_changes": "@@ -361,8 +361,6 @@ def __floordiv__(self, other):\n     def __rfloordiv__(self, other):\n         return other.value // self.value\n \n-    #\n-\n     def __gt__(self, other):\n         return self.value > other.value\n \n@@ -557,6 +555,16 @@ def __ne__(self, other, _builder=None):\n         other = _to_tensor(other, _builder)\n         return semantic.not_equal(self, other, _builder)\n \n+    @builtin\n+    def logical_and(self, other, _builder=None):\n+        other = _to_tensor(other, _builder)\n+        return semantic.logical_and(self, other, _builder)\n+\n+    @builtin\n+    def logical_or(self, other, _builder=None):\n+        other = _to_tensor(other, _builder)\n+        return semantic.logical_or(self, other, _builder)\n+\n     @builtin\n     def __getitem__(self, slices, _builder=None):\n         if isinstance(slices, slice):"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -285,6 +285,22 @@ def xor_(input: tl.tensor,\n     return tl.tensor(builder.create_xor(input.handle, other.handle), input.type)\n \n \n+def logical_and(input: tl.tensor, other: tl.tensor, builder: ir.builder) -> tl.tensor:\n+    if not input.type.is_int1():\n+        input = bitcast(input, tl.dtype(\"int1\"), builder)\n+    if not other.type.is_int1():\n+        other = bitcast(other, tl.dtype(\"int1\"), builder)\n+    return and_(input, other, builder)\n+\n+\n+def logical_or(input: tl.tensor, other: tl.tensor, builder: ir.builder) -> tl.tensor:\n+    if not input.type.is_int1():\n+        input = bitcast(input, tl.dtype(\"int1\"), builder)\n+    if not other.type.is_int1():\n+        other = bitcast(other, tl.dtype(\"int1\"), builder)\n+    return or_(input, other, builder)\n+\n+\n def lshr(input: tl.tensor,\n          other: tl.tensor,\n          builder: ir.builder) -> tl.tensor:"}, {"filename": "test/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -18,7 +18,7 @@ set(TRITON_TEST_DEPENDS\n \n add_lit_testsuite(check-triton \"Running the triton regression tests\"\n   ${CMAKE_CURRENT_BINARY_DIR}\n-  DEPENDS ${STANDALONE_TEST_DEPENDS}\n+  DEPENDS ${TRITON_TEST_DEPENDS}\n   )\n set_target_properties(check-triton PROPERTIES FOLDER \"Tests\")\n "}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 145, "deletions": 2, "changes": 147, "file_content_changes": "@@ -333,6 +333,149 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // -----\n \n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: basic_async_wait\n+  func @basic_async_wait() {\n+    // CHECK: cp.async.wait_group 0x4\n+    triton_gpu.async_wait {num = 4: i32}\n+    return\n+  }\n+}\n+\n+// -----\n+\n+#block0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [4], warpsPerCTA = [4], order = [0]}>\n+#block1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [8], warpsPerCTA = [4], order = [0]}>\n+#block2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 1], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#block3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 8], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#AL = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#A = #triton_gpu.shared<{vec = 4, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: basic_insert_slice_async_v4\n+  func @basic_insert_slice_async_v4(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}) {\n+    %off0_ = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #block0>\n+    %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<64xi32, #block1>\n+    %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<16xi32, #block0>) -> tensor<16x1xi32, #block2>\n+    %off1 = tt.expand_dims %off1_ {axis = 0 : i32} : (tensor<64xi32, #block1>) -> tensor<1x64xi32, #block3>\n+    %broadcast_off0_scalar = tt.broadcast %off0 : (tensor<16x1xi32, #block2>) -> tensor<16x64xi32, #block2>\n+    %cst_scalar = arith.constant 64 : i32\n+    %cst = tt.splat %cst_scalar : (i32) -> tensor<16x64xi32, #block2>\n+    %broadcast_off0_ = arith.muli %broadcast_off0_scalar, %cst : tensor<16x64xi32, #block2>\n+    %broadcast_off1_ = tt.broadcast %off1 : (tensor<1x64xi32, #block3>) -> tensor<16x64xi32, #block3>\n+    %broadcast_off0 = triton_gpu.convert_layout %broadcast_off0_ : (tensor<16x64xi32, #block2>) -> tensor<16x64xi32, #AL>\n+    %broadcast_off1 = triton_gpu.convert_layout %broadcast_off1_ : (tensor<16x64xi32, #block3>) -> tensor<16x64xi32, #AL>\n+    %off = arith.addi %broadcast_off0, %broadcast_off1 : tensor<16x64xi32, #AL>\n+    %a_init = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<16x64x!tt.ptr<f32>, #AL>\n+    %a_ptr = tt.addptr %a_init, %off : tensor<16x64x!tt.ptr<f32>, #AL>\n+    %tensor = triton_gpu.alloc_tensor : tensor<2x16x64xf32, #A>\n+    %index = arith.constant 1 : i32\n+\n+    // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n+    // CHECK-SAME: cp.async.cg.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x80, 0x80\n+    // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n+    // CHECK-SAME: cp.async.cg.shared.global.L2::evict_normal [ ${{.*}} + 8 ], [ ${{.*}} + 0 ], 0x80, 0x80\n+    // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n+    // CHECK-SAME: cp.async.commit_group\n+    %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x64x!tt.ptr<f32>, #AL> -> tensor<2x16x64xf32, #A>\n+    return\n+  }\n+}\n+\n+// -----\n+\n+#block0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [4], warpsPerCTA = [4], order = [0]}>\n+#block1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [8], warpsPerCTA = [4], order = [0]}>\n+#block2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 1], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#block3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 8], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#AL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#A = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: basic_insert_slice_async_v1\n+  func @basic_insert_slice_async_v1(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}) {\n+    %off0_ = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #block0>\n+    %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #block1>\n+    %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<16xi32, #block0>) -> tensor<16x1xi32, #block2>\n+    %off1 = tt.expand_dims %off1_ {axis = 0 : i32} : (tensor<32xi32, #block1>) -> tensor<1x32xi32, #block3>\n+    %broadcast_off0_scalar = tt.broadcast %off0 : (tensor<16x1xi32, #block2>) -> tensor<16x32xi32, #block2>\n+    %cst_scalar = arith.constant 32 : i32\n+    %cst = tt.splat %cst_scalar : (i32) -> tensor<16x32xi32, #block2>\n+    %broadcast_off0_ = arith.muli %broadcast_off0_scalar, %cst : tensor<16x32xi32, #block2>\n+    %broadcast_off1_ = tt.broadcast %off1 : (tensor<1x32xi32, #block3>) -> tensor<16x32xi32, #block3>\n+    %broadcast_off0 = triton_gpu.convert_layout %broadcast_off0_ : (tensor<16x32xi32, #block2>) -> tensor<16x32xi32, #AL>\n+    %broadcast_off1 = triton_gpu.convert_layout %broadcast_off1_ : (tensor<16x32xi32, #block3>) -> tensor<16x32xi32, #AL>\n+    %off = arith.addi %broadcast_off0, %broadcast_off1 : tensor<16x32xi32, #AL>\n+    %a_init = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<16x32x!tt.ptr<f32>, #AL>\n+    %a_ptr = tt.addptr %a_init, %off : tensor<16x32x!tt.ptr<f32>, #AL>\n+    %tensor = triton_gpu.alloc_tensor : tensor<2x16x32xf32, #A>\n+    %index = arith.constant 1 : i32\n+\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.commit_group\n+    %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x32x!tt.ptr<f32>, #AL> -> tensor<2x16x32xf32, #A>\n+    return\n+  }\n+}\n+\n+// -----\n+\n+#block0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [8], warpsPerCTA = [4], order = [0]}>\n+#block2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 1], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#block3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 8], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#AL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#A = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: basic_insert_slice_async_v1_multictas\n+  func @basic_insert_slice_async_v1_multictas(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}) {\n+    %off0_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #block0>\n+    %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #block0>\n+    %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<32xi32, #block0>) -> tensor<32x1xi32, #block2>\n+    %off1 = tt.expand_dims %off1_ {axis = 0 : i32} : (tensor<32xi32, #block0>) -> tensor<1x32xi32, #block3>\n+    %broadcast_off0_scalar = tt.broadcast %off0 : (tensor<32x1xi32, #block2>) -> tensor<32x32xi32, #block2>\n+    %cst_scalar = arith.constant 32 : i32\n+    %cst = tt.splat %cst_scalar : (i32) -> tensor<32x32xi32, #block2>\n+    %broadcast_off0_ = arith.muli %broadcast_off0_scalar, %cst : tensor<32x32xi32, #block2>\n+    %broadcast_off1_ = tt.broadcast %off1 : (tensor<1x32xi32, #block3>) -> tensor<32x32xi32, #block3>\n+    %broadcast_off0 = triton_gpu.convert_layout %broadcast_off0_ : (tensor<32x32xi32, #block2>) -> tensor<32x32xi32, #AL>\n+    %broadcast_off1 = triton_gpu.convert_layout %broadcast_off1_ : (tensor<32x32xi32, #block3>) -> tensor<32x32xi32, #AL>\n+    %off = arith.addi %broadcast_off0, %broadcast_off1 : tensor<32x32xi32, #AL>\n+    %a_init = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n+    %a_ptr = tt.addptr %a_init, %off : tensor<32x32x!tt.ptr<f32>, #AL>\n+    %tensor = triton_gpu.alloc_tensor : tensor<2x32x32xf32, #A>\n+    %index = arith.constant 1 : i32\n+\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 512 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 512 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 512 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 512 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: cp.async.commit_group\n+    %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32x!tt.ptr<f32>, #AL> -> tensor<2x32x32xf32, #A>\n+    return\n+  }\n+}\n+\n+// -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK: basic_splat\n@@ -351,9 +494,9 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_store\n   func @basic_store(%ptrs: tensor<256x!tt.ptr<f32>, #blocked0>, %vals: tensor<256xf32, #blocked0>, %mask: tensor<256xi1, #blocked0>) {\n-    // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n+    // CHECK: llvm.inline_asm\n     // CHECK-SAME: st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n-    // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n+    // CHECK: llvm.inline_asm\n     // CHECK-SAME: st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n     tt.store %ptrs, %vals, %mask : tensor<256xf32, #blocked0>\n     return"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/PtxAsmFormatTest.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -76,7 +76,7 @@ TEST_F(PtxAsmFormatTest, complexInstruction) {\n \n   auto &ld =\n       builder\n-          .create<PtxIOInstr>(\"ld\") //\n+          .create<PTXIOInstr>(\"ld\") //\n           ->o(\"volatile\", isVolatile)\n           .global()\n           .o(\"ca\", cache == CacheModifier::CA)"}]