[{"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 8, "deletions": 33, "changes": 41, "file_content_changes": "@@ -30,28 +30,6 @@ def matmul_no_scf_kernel(\n # TODO: num_warps could only be 4 for now\n \n \n-# @pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS', [\n-#     [128, 256, 32, 4],\n-#     [256, 128, 16, 4],\n-#     [128, 16, 32, 4],\n-#     [32, 128, 64, 4],\n-# ])\n-# def test_gemm_no_scf(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n-#     a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n-#     b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n-#     c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n-#     grid = lambda META: (1, )\n-#     matmul_no_scf_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n-#                                stride_am=a.stride(0), stride_ak=a.stride(1),\n-#                                stride_bk=b.stride(0), stride_bn=b.stride(1),\n-#                                stride_cm=c.stride(0), stride_cn=c.stride(1),\n-#                                M=SIZE_M, N=SIZE_N, K=SIZE_K,\n-#                                num_warps=NUM_WARPS)\n-#     golden = torch.matmul(a, b)\n-#     torch.set_printoptions(profile=\"full\")\n-#     assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)\n-\n-\n @pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS', [\n     [128, 256, 32, 4],\n     [256, 128, 16, 4],\n@@ -61,25 +39,20 @@ def matmul_no_scf_kernel(\n     [64, 128, 128, 4],\n     [64, 128, 128, 2],\n ])\n-def test_gemm_no_scf_int8(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n-    a = torch.randint(-5, 5, (SIZE_M, SIZE_K), device='cuda', dtype=torch.int8)\n-    b = torch.randint(-5, 5, (SIZE_K, SIZE_N), device='cuda', dtype=torch.int8)\n-    c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.int32)\n+def test_gemm_no_scf(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n+    a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n+    b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+    c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n     grid = lambda META: (1, )\n     matmul_no_scf_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n                                stride_am=a.stride(0), stride_ak=a.stride(1),\n                                stride_bk=b.stride(0), stride_bn=b.stride(1),\n                                stride_cm=c.stride(0), stride_cn=c.stride(1),\n                                M=SIZE_M, N=SIZE_N, K=SIZE_K,\n                                num_warps=NUM_WARPS)\n-\n-    aa = a.cpu()\n-    bb = b.cpu()\n-    golden = torch.matmul(aa, bb)\n+    golden = torch.matmul(a, b)\n     torch.set_printoptions(profile=\"full\")\n-    print(\"c\", c.cpu())\n-    print(\"gloden\", golden)\n-    assert c.cpu() == golden\n+    assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)\n \n \n @triton.jit\n@@ -107,6 +80,8 @@ def matmul_kernel(\n     c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n     tl.store(c_ptrs, accumulator)\n \n+# TODO: DotConversion in TritonGPUToLLVM cannot support non-splat C for the moment\n+\n \n def get_variant_golden(a, b):\n     SIZE_M = a.shape[0]"}]