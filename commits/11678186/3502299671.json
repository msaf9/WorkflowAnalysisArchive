[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 75, "deletions": 90, "changes": 165, "file_content_changes": "@@ -356,25 +356,6 @@ Value getStructFromElements(Location loc, ValueRange resultVals,\n   return llvmStruct;\n }\n \n-// TODO[goostavz]: to be deprecated\n-// delinearize supposing order is [n, .. , 2, 1, 0]\n-template <typename T>\n-static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape) {\n-  // shape: {a, b, c, d}  ->  accMul: {b*c*d, c*d, d, 1}\n-  size_t rank = shape.size();\n-  T accMul = product(shape.drop_front());\n-  T linearRemain = linearIndex;\n-  SmallVector<T> multiDimIndex(rank);\n-  for (size_t i = 0; i < rank; ++i) {\n-    multiDimIndex[i] = linearRemain / accMul;\n-    linearRemain = linearRemain % accMul;\n-    if (i != (rank - 1)) {\n-      accMul = accMul / shape[i + 1];\n-    }\n-  }\n-  return multiDimIndex;\n-}\n-\n // delinearize supposing order is [0, 1, .. , n]\n template <typename T>\n static SmallVector<T> getMultiDimIndexImpl(T linearIndex, ArrayRef<T> shape) {\n@@ -407,24 +388,7 @@ static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape,\n   return multiDim;\n }\n \n-// TODO[goostavz]: to be deprecated\n-// linearize supposing order is [n, .. , 2, 1, 0]\n-template <typename T>\n-static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n-  assert(multiDimIndex.size() == shape.size());\n-  // shape: {a, b, c, d}  ->  accMul: {b*c*d, c*d, d, 1}\n-  size_t rank = shape.size();\n-  T accMul = product(shape.drop_front());\n-  T linearIndex = 0;\n-  for (size_t i = 0; i < rank; ++i) {\n-    linearIndex += multiDimIndex[i] * accMul;\n-    if (i != (rank - 1)) {\n-      accMul = accMul / shape[i + 1];\n-    }\n-  }\n-  return linearIndex;\n-}\n-\n+// linearize supposing order is [0, 1, .. , n]\n template <typename T>\n static T getLinearIndexImpl(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n   assert(multiDimIndex.size() == shape.size());\n@@ -621,6 +585,13 @@ class ConvertTritonGPUOpToLLVMPattern\n     return multiDim;\n   }\n \n+  Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n+                  ArrayRef<Value> multiDim, ArrayRef<unsigned> shape,\n+                  ArrayRef<unsigned> order) const {\n+    return linearize(rewriter, loc, reorder<Value>(multiDim, order),\n+                     reorder<unsigned>(shape, order));\n+  }\n+\n   Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n                   ArrayRef<Value> multiDim, ArrayRef<unsigned> shape) const {\n     int rank = multiDim.size();\n@@ -1428,38 +1399,42 @@ struct BroadcastOpConversion\n     Value result = op.result();\n     auto srcTy = op.src().getType().cast<RankedTensorType>();\n     auto resultTy = result.getType().cast<RankedTensorType>();\n-    auto srcLayout = srcTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n-    auto resultLayout = resultTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n-    assert(srcLayout && (srcLayout == resultLayout) &&\n-           \"Unexpected layout of BroadcastOp\");\n+    auto srcLayout = srcTy.getEncoding();\n+    auto resultLayout = resultTy.getEncoding();\n     auto srcShape = srcTy.getShape();\n     auto resultShape = resultTy.getShape();\n     unsigned rank = srcTy.getRank();\n     assert(rank == resultTy.getRank());\n+    auto order = triton::gpu::getOrder(srcLayout);\n \n-    SmallVector<int64_t, 4> srcLogicalShape(2 * rank);\n-    SmallVector<int64_t, 4> resultLogicalShape(2 * rank);\n-    SmallVector<unsigned, 2> broadcastDims;\n+    SmallVector<int64_t> srcLogicalShape(2 * rank);\n+    SmallVector<unsigned> srcLogicalOrder(2 * rank);\n+    SmallVector<int64_t> resultLogicalShape(2 * rank);\n+    SmallVector<unsigned> broadcastDims;\n     for (unsigned d = 0; d < rank; ++d) {\n-      unsigned resultShapePerCTA = resultLayout.getSizePerThread()[d] *\n-                                   resultLayout.getThreadsPerWarp()[d] *\n-                                   resultLayout.getWarpsPerCTA()[d];\n+      unsigned resultShapePerCTA = triton::gpu::getSizePerThread(resultLayout)[d] *\n+                                   triton::gpu::getThreadsPerWarp(resultLayout)[d] *\n+                                   triton::gpu::getWarpsPerCTA(resultLayout)[d];\n       int64_t numCtas = ceil<unsigned>(resultShape[d], resultShapePerCTA);\n       if (srcShape[d] != resultShape[d]) {\n         assert(srcShape[d] == 1);\n         broadcastDims.push_back(d);\n         srcLogicalShape[d] = 1;\n         srcLogicalShape[d + rank] =\n-            std::max<unsigned>(1, srcLayout.getSizePerThread()[d]);\n+            std::max<unsigned>(1, triton::gpu::getSizePerThread(srcLayout)[d]);\n       } else {\n         srcLogicalShape[d] = numCtas;\n-        srcLogicalShape[d + rank] = resultLayout.getSizePerThread()[d];\n+        srcLogicalShape[d + rank] = triton::gpu::getSizePerThread(resultLayout)[d];\n       }\n       resultLogicalShape[d] = numCtas;\n-      resultLogicalShape[d + rank] = resultLayout.getSizePerThread()[d];\n+      resultLogicalShape[d + rank] = triton::gpu::getSizePerThread(resultLayout)[d];\n+\n+      srcLogicalOrder[d] = order[d] + rank;\n+      srcLogicalOrder[d + rank] = order[d];\n     }\n     int64_t duplicates = 1;\n-    SmallVector<int64_t, 2> broadcastSizes(broadcastDims.size() * 2);\n+    SmallVector<int64_t> broadcastSizes(broadcastDims.size() * 2);\n+    SmallVector<unsigned> broadcastOrder(broadcastDims.size() * 2);\n     for (auto it : llvm::enumerate(broadcastDims)) {\n       // Incase there are multiple indices in the src that is actually\n       // calculating the same element, srcLogicalShape may not need to be 1.\n@@ -1468,30 +1443,44 @@ struct BroadcastOpConversion\n       // [1, 2]\n       int64_t d = resultLogicalShape[it.value()] / srcLogicalShape[it.value()];\n       broadcastSizes[it.index()] = d;\n+      broadcastOrder[it.index()] = srcLogicalOrder[it.value()];\n       duplicates *= d;\n       d = resultLogicalShape[it.value() + rank] /\n           srcLogicalShape[it.value() + rank];\n       broadcastSizes[it.index() + broadcastDims.size()] = d;\n+      broadcastOrder[it.index() + broadcastDims.size()] =\n+          srcLogicalOrder[it.value() + rank];\n       duplicates *= d;\n     }\n+    auto argsort = [](SmallVector<unsigned> input) {\n+      SmallVector<unsigned> idx(input.size());\n+      std::iota(idx.begin(), idx.end(), 0);\n+      std::sort(idx.begin(), idx.end(), [&input](unsigned a, unsigned b) {\n+        return input[a] < input[b];\n+      });\n+      return idx;\n+    };\n+    broadcastOrder = argsort(broadcastOrder);\n \n     unsigned srcElems = getElemsPerThread(srcTy);\n     auto srcVals = getElementsFromStruct(loc, src, rewriter);\n     unsigned resultElems = getElemsPerThread(resultTy);\n     SmallVector<Value> resultVals(resultElems);\n     for (unsigned i = 0; i < srcElems; ++i) {\n-      auto srcMultiDim = getMultiDimIndex<int64_t>(i, srcLogicalShape);\n+      auto srcMultiDim =\n+          getMultiDimIndex<int64_t>(i, srcLogicalShape, srcLogicalOrder);\n       for (int64_t j = 0; j < duplicates; ++j) {\n         auto resultMultiDim = srcMultiDim;\n-        auto bcastMultiDim = getMultiDimIndex<int64_t>(j, broadcastSizes);\n+        auto bcastMultiDim =\n+            getMultiDimIndex<int64_t>(j, broadcastSizes, broadcastOrder);\n         for (auto bcastDim : llvm::enumerate(broadcastDims)) {\n           resultMultiDim[bcastDim.value()] += bcastMultiDim[bcastDim.index()];\n           resultMultiDim[bcastDim.value() + rank] +=\n               bcastMultiDim[bcastDim.index() + broadcastDims.size()] *\n               srcLogicalShape[bcastDim.index() + broadcastDims.size()];\n         }\n-        auto resultLinearIndex =\n-            getLinearIndex<int64_t>(resultMultiDim, resultLogicalShape);\n+        auto resultLinearIndex = getLinearIndex<int64_t>(\n+            resultMultiDim, resultLogicalShape, srcLogicalOrder);\n         resultVals[resultLinearIndex] = srcVals[i];\n       }\n     }\n@@ -1665,9 +1654,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n     SmallVector<Value> writeIdx = indices[key];\n \n     writeIdx[axis] = udiv(writeIdx[axis], sizePerThread);\n-    Value writeOffset =\n-        linearize(rewriter, loc, reorder<Value>(writeIdx, srcOrd),\n-                  reorder<unsigned>(smemShape, srcOrd));\n+    Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape, srcOrd);\n     Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n     store(acc, writePtr);\n \n@@ -1676,9 +1663,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n       readIdx[axis] = ints[N];\n       Value readMask = icmp_slt(writeIdx[axis], ints[N]);\n       Value readOffset =\n-          select(readMask,\n-                 linearize(rewriter, loc, reorder<Value>(readIdx, srcOrd),\n-                           reorder<unsigned>(smemShape, srcOrd)),\n+          select(readMask, linearize(rewriter, loc, readIdx, smemShape, srcOrd),\n                  ints[0]);\n       Value readPtr = gep(elemPtrTy, writePtr, readOffset);\n       barrier();\n@@ -1702,9 +1687,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n     for (unsigned i = 0; i < resultElems; ++i) {\n       SmallVector<Value> readIdx = resultIndices[i];\n       readIdx.insert(readIdx.begin() + axis, ints[0]);\n-      Value readOffset =\n-          linearize(rewriter, loc, reorder<Value>(readIdx, srcOrd),\n-                    reorder<unsigned>(smemShape, srcOrd));\n+      Value readOffset = linearize(rewriter, loc, readIdx, smemShape, srcOrd);\n       Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n       resultVals[i] = load(readPtr);\n     }\n@@ -1798,9 +1781,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n \n     SmallVector<Value> writeIdx = indices[key];\n     writeIdx[axis] = (sizeInterWarps == 1) ? zero : warpIdAxis;\n-    Value writeOffset =\n-        linearize(rewriter, loc, reorder<Value>(writeIdx, order),\n-                  reorder<unsigned>(smemShape, order));\n+    Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape, order);\n     Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n     storeShared(rewriter, loc, writePtr, acc, laneZero);\n   }\n@@ -1851,23 +1832,25 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   if (auto resultTy = op.getType().dyn_cast<RankedTensorType>()) {\n     // nd-tensor where n >= 1\n     auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n-    auto resultShape = resultTy.getShape();\n     SmallVector<unsigned> resultOrd;\n     for (auto ord : order) {\n       if (ord != 0)\n         resultOrd.push_back(ord - 1);\n     }\n \n     unsigned resultElems = getElemsPerThread(resultTy);\n-    auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultShape);\n+    auto resultIndices =\n+        emitIndices(loc, rewriter, resultLayout, resultTy.getShape());\n     assert(resultIndices.size() == resultElems);\n \n     SmallVector<Value> resultVals(resultElems);\n+    SmallVector<unsigned> resultShape;\n+    std::copy(resultTy.getShape().begin(), resultTy.getShape().end(),\n+              std::back_inserter(resultShape));\n     for (size_t i = 0; i < resultElems; ++i) {\n       SmallVector<Value> readIdx = resultIndices[i];\n       Value readOffset =\n-          linearize(rewriter, loc, reorder<Value>(readIdx, resultOrd),\n-                    reorder<int64_t, unsigned>(resultShape, resultOrd));\n+          linearize(rewriter, loc, readIdx, resultShape, resultOrd);\n       Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n       resultVals[i] = load(readPtr);\n     }\n@@ -2818,8 +2801,8 @@ struct ConvertLayoutOpConversion\n       auto multiDimOffsetFirstElem =\n           emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n       SmallVector<Value> multiDimOffset(rank);\n-      SmallVector<unsigned> multiDimElemId =\n-          getMultiDimIndex<unsigned>(elemId, blockedLayout.getSizePerThread());\n+      SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n+          elemId, blockedLayout.getSizePerThread(), blockedLayout.getOrder());\n       for (unsigned d = 0; d < rank; ++d) {\n         multiDimOffset[d] = add(multiDimOffsetFirstElem[d],\n                                 idx_val(multiDimCTAInRepId[d] * shapePerCTA[d] +\n@@ -2850,9 +2833,7 @@ struct ConvertLayoutOpConversion\n       Value warpSize = idx_val(32);\n       Value laneId = urem(threadId, warpSize);\n       Value warpId = udiv(threadId, warpSize);\n-      // auto multiDimWarpId =\n-      //     delinearize(rewriter, loc, warpId, mmaLayout.getWarpsPerCTA());\n-      // TODO: double confirm if its document bug or DotConversion's Bug\n+      // TODO: fix the bug in MMAEncodingAttr document\n       SmallVector<Value> multiDimWarpId(2);\n       multiDimWarpId[0] = urem(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n       multiDimWarpId[1] = udiv(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n@@ -2942,6 +2923,7 @@ void ConvertLayoutOpConversion::processReplica(\n   auto accumSizePerThread = product<unsigned>(sizePerThread);\n   SmallVector<unsigned> numCTAs(rank);\n   auto shapePerCTA = getShapePerCTA(layout);\n+  auto order = getOrder(layout);\n   for (unsigned d = 0; d < rank; ++d) {\n     numCTAs[d] = ceil<unsigned>(type.getShape()[d], shapePerCTA[d]);\n   }\n@@ -2957,14 +2939,16 @@ void ConvertLayoutOpConversion::processReplica(\n   auto llvmElemTy = getTypeConverter()->convertType(elemTy);\n \n   for (unsigned ctaId = 0; ctaId < accumNumCTAsEachRep; ++ctaId) {\n-    auto multiDimCTAInRepId = getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep);\n+    auto multiDimCTAInRepId =\n+        getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep, order);\n     SmallVector<unsigned> multiDimCTAId(rank);\n     for (auto it : llvm::enumerate(multiDimCTAInRepId)) {\n       auto d = it.index();\n       multiDimCTAId[d] = multiDimRepId[d] * numCTAsEachRep[d] + it.value();\n     }\n \n-    unsigned linearCTAId = getLinearIndex<unsigned>(multiDimCTAId, numCTAs);\n+    unsigned linearCTAId =\n+        getLinearIndex<unsigned>(multiDimCTAId, numCTAs, order);\n     // TODO: This is actually redundant index calculation, we should\n     //       consider of caching the index calculation result in case\n     //       of performance issue observed.\n@@ -2973,8 +2957,7 @@ void ConvertLayoutOpConversion::processReplica(\n           getMultiDimOffset(layout, loc, rewriter, elemId, type.getShape(),\n                             multiDimCTAInRepId, shapePerCTA);\n       Value offset =\n-          linearize(rewriter, loc, reorder<Value>(multiDimOffset, outOrd),\n-                    reorder<unsigned>(paddedRepShape, outOrd));\n+          linearize(rewriter, loc, multiDimOffset, paddedRepShape, outOrd);\n       auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n       Value ptr = gep(elemPtrTy, smemBase, offset);\n       auto vecTy = vec_ty(llvmElemTy, vec);\n@@ -3055,7 +3038,8 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n   SmallVector<Value> outVals(outElems);\n \n   for (unsigned repId = 0; repId < accumNumReplicates; ++repId) {\n-    auto multiDimRepId = getMultiDimIndex<unsigned>(repId, numReplicates);\n+    auto multiDimRepId =\n+        getMultiDimIndex<unsigned>(repId, numReplicates, outOrd);\n     barrier();\n     if (srcLayout.isa<BlockedEncodingAttr>() ||\n         srcLayout.isa<SliceEncodingAttr>() ||\n@@ -5958,19 +5942,19 @@ struct AtomicRMWOpConversion\n     auto elemsPerThread = getElemsPerThread(val.getType());\n     SmallVector<Value> resultVals(elemsPerThread);\n     for (size_t i = 0; i < elemsPerThread; i += vec) {\n-      Value rmvVal = undef(vecTy);\n+      Value rmwVal = undef(vecTy);\n       for (int ii = 0; ii < vec; ++ii) {\n         Value iiVal = createIndexAttrConstant(\n             rewriter, loc, getTypeConverter()->getIndexType(), ii);\n-        rmvVal = insert_element(vecTy, rmvVal, valElements[i], iiVal);\n+        rmwVal = insert_element(vecTy, rmwVal, valElements[i + ii], iiVal);\n       }\n-      Value rmwPtr = bitcast(ptrElements[i], ptr_ty(valTy.getElementType()));\n+      Value rmwPtr = ptrElements[i];\n       std::string sTy;\n       PTXBuilder ptxBuilder;\n \n       auto *dstOpr = ptxBuilder.newOperand(\"=r\");\n-      auto *ptrOpr = ptxBuilder.newAddrOperand(rmwPtr, \"r\");\n-      auto *valOpr = ptxBuilder.newOperand(rmvVal, \"r\");\n+      auto *ptrOpr = ptxBuilder.newAddrOperand(rmwPtr, \"l\");\n+      auto *valOpr = ptxBuilder.newOperand(rmwVal, \"r\");\n \n       auto &atom = ptxBuilder.create<>(\"atom\")->global().o(\"gpu\");\n       auto rmwOp = stringifyRMWOp(atomicRmwAttr).str();\n@@ -6012,12 +5996,13 @@ struct AtomicRMWOpConversion\n         return failure();\n       }\n       atom.o(rmwOp).o(sTy);\n-\n-      atom(dstOpr, ptrOpr, valOpr);\n-      auto ret = ptxBuilder.launch(rewriter, loc, valueElemTy, false);\n+      //TODO:[dongdongl] actual mask support\n+      Value pred = int_val(1, 1);\n+      atom(dstOpr, ptrOpr, valOpr).predicate(pred);\n+      auto ret = ptxBuilder.launch(rewriter, loc, valueElemTy);\n       for (int ii = 0; ii < vec; ++ii) {\n         resultVals[i * vec + ii] =\n-            vec == 1 ? ret : extract_element(vecTy, ret, idx_val(ii));\n+            vec == 1 ? ret : extract_element(valueElemTy, ret, idx_val(ii));\n       }\n     }\n     Type structTy = getTypeConverter()->convertType(valueTy);"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -378,7 +378,7 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n       TritonGenericPattern<triton::AddPtrOp>, TritonReducePattern,\n       TritonExpandDimsPattern, TritonMakeRangePattern, TritonDotPattern,\n       TritonLoadPattern, TritonStorePattern, TritonExtElemwisePattern,\n-      TritonPrintfPattern>(typeConverter, context);\n+      TritonPrintfPattern, TritonAtomicRMWPattern>(typeConverter, context);\n }\n \n //"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 25, "deletions": 7, "changes": 32, "file_content_changes": "@@ -163,7 +163,19 @@ void init_triton_ir(py::module &&m) {\n \n   py::class_<mlir::Type>(m, \"type\")\n       .def(\"is_integer\", &mlir::Type::isInteger)\n-      .def(\"is_fp16\", &mlir::Type::isF16);\n+      .def(\"is_fp16\", &mlir::Type::isF16)\n+      .def(\"__str__\", [](mlir::Type &self) {\n+        std::string str;\n+        llvm::raw_string_ostream os(str);\n+        self.print(os);\n+        return os.str();\n+      });\n+\n+  py::class_<mlir::FunctionType>(m, \"function_type\")\n+      .def(\"param_types\", [](mlir::FunctionType &self) {\n+        return std::vector<mlir::Type>(self.getInputs().begin(),\n+                                       self.getInputs().end());\n+      });\n \n   py::class_<mlir::Value>(m, \"value\")\n       .def(\"set_attr\",\n@@ -314,7 +326,14 @@ void init_triton_ir(py::module &&m) {\n       .def(\"get_function\",\n            [](mlir::ModuleOp &self, std::string &funcName) -> mlir::FuncOp {\n              return self.lookupSymbol<mlir::FuncOp>(funcName);\n-           });\n+           })\n+      .def(\"get_single_function\", [](mlir::ModuleOp &self) -> mlir::FuncOp {\n+        llvm::SmallVector<mlir::FuncOp> funcs;\n+        self.walk([&](mlir::FuncOp func) { funcs.push_back(func); });\n+        if (funcs.size() != 1)\n+          throw std::runtime_error(\"Expected a single function\");\n+        return funcs[0];\n+      });\n \n   m.def(\n       \"parse_mlir_module\",\n@@ -363,6 +382,7 @@ void init_triton_ir(py::module &&m) {\n             self.setArgAttr(arg_no, name, mlir::IntegerAttr::get(attrTy, val));\n           },\n           ret::reference)\n+      .def_property_readonly(\"type\", &mlir::FuncOp::getType)\n       .def(\"reset_type\", &mlir::FuncOp::setType);\n \n   py::class_<mlir::OpBuilder::InsertPoint>(m, \"InsertPoint\");\n@@ -1086,9 +1106,7 @@ void init_triton_ir(py::module &&m) {\n               mlir::Value &ptr, mlir::Value &val,\n               mlir::Value &mask) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n-             auto ptrType = mlir::getElementTypeOrSelf(ptr)\n-                                .cast<mlir::triton::PointerType>();\n-             mlir::Type dstType = ptrType.getPointeeType();\n+             mlir::Type dstType = val.getType();\n              return self.create<mlir::triton::AtomicRMWOp>(loc, dstType, rmwOp,\n                                                            ptr, val, mask);\n            })\n@@ -1276,8 +1294,8 @@ void init_triton_ir(py::module &&m) {\n void init_triton_translation(py::module &m) {\n   using ret = py::return_value_policy;\n \n-  m.def(\"get_shared_memory_size\", [](mlir::ModuleOp module) {\n-    auto shared = module->getAttrOfType<mlir::IntegerAttr>(\"triton_gpu.shared\");\n+  m.def(\"get_shared_memory_size\", [](mlir::ModuleOp mod) {\n+    auto shared = mod->getAttrOfType<mlir::IntegerAttr>(\"triton_gpu.shared\");\n     return shared.getInt();\n   });\n "}, {"filename": "python/tests/test_backend.py", "status": "added", "additions": 91, "deletions": 0, "changes": 91, "file_content_changes": "@@ -0,0 +1,91 @@\n+import triton\n+import triton.language as tl\n+import torch\n+import pytest\n+from .test_core import  numpy_random, to_triton\n+\n+class MmaLayout:\n+    def __init__(self, version, warps_per_cta):\n+        self.version = version\n+        self.warps_per_cta = str(warps_per_cta)\n+\n+    def __str__(self):\n+        return f\"#triton_gpu.mma<{{version={self.version}, warpsPerCTA={self.warps_per_cta}}}>\"\n+\n+class BlockedLayout:\n+    def __init__(self, size_per_thread, threads_per_warp, warps_per_cta, order):\n+        self.sz_per_thread = str(size_per_thread)\n+        self.threads_per_warp = str(threads_per_warp)\n+        self.warps_per_cta = str(warps_per_cta)\n+        self.order = str(order)\n+\n+    def __str__(self):\n+        return f\"#triton_gpu.blocked<{{sizePerThread={self.sz_per_thread}, threadsPerWarp={self.threads_per_warp}, warpsPerCTA={self.warps_per_cta}, order={self.order}}}>\"\n+\n+layouts = [\n+  # MmaLayout(version=1, warps_per_cta=[1, 4]),\n+  MmaLayout(version=2, warps_per_cta=[1, 4]),\n+  # MmaLayout(version=1, warps_per_cta=[4, 1]),\n+  MmaLayout(version=2, warps_per_cta=[4, 1]),\n+  BlockedLayout([1, 8], [2, 16], [4, 1], [1, 0]),\n+  BlockedLayout([1, 4], [4, 8], [2, 2], [1, 0]),\n+  BlockedLayout([1, 1], [1, 32], [2, 2], [1, 0]),\n+  BlockedLayout([8, 1], [16, 2], [1, 4], [0, 1]),\n+  BlockedLayout([4, 1], [8, 4], [2, 2], [0, 1]),\n+  BlockedLayout([1, 1], [32, 1], [2, 2], [0, 1])\n+]\n+\n+\n+@pytest.mark.parametrize(\"shape\", [(128, 128)])\n+@pytest.mark.parametrize(\"dtype\", ['float16'])\n+@pytest.mark.parametrize(\"src_layout\", layouts)\n+@pytest.mark.parametrize(\"dst_layout\", layouts)\n+def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n+    if str(src_layout) == str(dst_layout):\n+        pytest.skip()\n+    if 'mma' in str(src_layout) and 'mma' in str(dst_layout):\n+        pytest.skip()\n+    \n+    \n+\n+    ir = f\"\"\"\n+#src = {src_layout}\n+#dst = {dst_layout}\n+\"\"\"  + \"\"\"\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  func public @kernel_0d1d(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n+    %cst = arith.constant dense<128> : tensor<128x1xi32, #src>\n+    %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #src}>>\n+    %1 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 0, parent = #src}>>\n+    %2 = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<128x128x!tt.ptr<f16>, #src>\n+    %4 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #src}>>) -> tensor<128x1xi32, #src>\n+    %5 = arith.muli %4, %cst : tensor<128x1xi32, #src>\n+    %6 = tt.expand_dims %1 {axis = 0 : i32} : (tensor<128xi32, #triton_gpu.slice<{dim = 0, parent = #src}>>) -> tensor<1x128xi32, #src>\n+    %7 = tt.broadcast %6 : (tensor<1x128xi32, #src>) -> tensor<128x128xi32, #src>\n+    %8 = tt.broadcast %5 : (tensor<128x1xi32, #src>) -> tensor<128x128xi32, #src>\n+    %9 = arith.addi %8, %7 : tensor<128x128xi32, #src>\n+    %10 = tt.addptr %2, %9 : tensor<128x128x!tt.ptr<f16>, #src>\n+    %11 = tt.load %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf16, #src>\n+    %3 = tt.splat %arg1 : (!tt.ptr<f16>) -> tensor<128x128x!tt.ptr<f16>, #dst>\n+    %12 = triton_gpu.convert_layout %9 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #dst>\n+    %13 = triton_gpu.convert_layout %11 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #dst>\n+    %14 = tt.addptr %3, %12 : tensor<128x128x!tt.ptr<f16>, #dst>\n+    tt.store %14, %13 : tensor<128x128xf16, #dst>\n+    return\n+  }\n+}    \n+\"\"\"\n+\n+    x = to_triton(numpy_random(shape, dtype_str=dtype))\n+    z = torch.empty_like(x)\n+\n+    # write the IR to a temporary file using mkstemp\n+    import tempfile\n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.ttgir') as f:\n+        f.write(ir)\n+        f.flush()\n+        kernel = triton.compile(f.name)\n+    kernel[(1,1,1)](x.data_ptr(), z.data_ptr())\n+\n+    assert torch.equal(z, x)\n+"}, {"filename": "python/tests/test_compiler.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -16,7 +16,7 @@ def test_empty_kernel_cubin_compile():\n \n     device = torch.cuda.current_device()\n     kernel = triton.compile(empty_kernel,\n-                            \"*fp32,i32,i32\",\n+                            signature=\"*fp32,i32,i32\",\n                             device=device,\n                             constants={\"BLOCK\": 256})\n "}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 19, "deletions": 0, "changes": 19, "file_content_changes": "@@ -671,6 +671,25 @@ def without_fn(X, Y, A, B, C):\n #     kernel[(1,)](z_tri, x_tri, axis, shape0, shape1)\n #     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n \n+def test_tensor_atomic_rmw_add_elementwise(device=\"cuda\"):\n+    shape0, shape1 = 16, 16\n+    @triton.jit\n+    def kernel(Z, X, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n+        off0 = tl.arange(0, SHAPE0)\n+        off1 = tl.arange(0, SHAPE1)\n+        x = tl.load(X + off0[:, None] * SHAPE1 + off1[None, :])\n+        tl.atomic_add(Z + off0[:, None] * SHAPE1 + off1[None, :], x)\n+\n+    rs = RandomState(17)\n+    x = numpy_random((shape0, shape1), dtype_str=\"float32\", rs=rs)\n+    z = numpy_random((shape0, shape1), dtype_str=\"float32\", rs=rs)\n+    # reference\n+    z_ref = z + x\n+    # triton result\n+    x_tri = torch.from_numpy(x).to(device=device)\n+    z_tri = torch.from_numpy(z).to(device=device)\n+    kernel[(1,)](z_tri, x_tri, shape0, shape1)\n+    np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n \n # def test_atomic_cas():\n #     # 1. make sure that atomic_cas changes the original value (Lock)"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 16, "deletions": 11, "changes": 27, "file_content_changes": "@@ -154,6 +154,19 @@ def get_variant_golden(a, b):\n     c_padded = torch.matmul(a_padded, b_padded)\n     return c_padded[:SIZE_M, :SIZE_N]\n \n+# It's not easy to get a proper error threshold in different size\n+# Here the gemm calculation is padded to a different size in order to get\n+# a variant version of the golden result. And the error between golden and\n+# golden_variant provide reference on selecting the proper rtol / atol.\n+\n+\n+def get_proper_err(a, b, golden):\n+    golden_variant = get_variant_golden(a, b)\n+    golden_diff = golden - golden_variant\n+    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n+    golden_rel_err = torch.max(torch.abs(golden_diff / golden)).item()\n+    return (golden_abs_err, golden_rel_err)\n+\n \n @pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,TRANS_A,TRANS_B', [\n     # Non-forloop\n@@ -198,16 +211,7 @@ def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLO\n                         BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n                         num_warps=NUM_WARPS)\n     golden = torch.matmul(a, b)\n-\n-    # It's not easy to get a proper error threshold in different size\n-    # Here the gemm calculation is padded to a different size in order to get\n-    # a variant version of the golden result. And the error between golden and\n-    # golden_variant provide reference on selecting the proper rtol / atol.\n-    golden_variant = get_variant_golden(a, b)\n-    golden_diff = golden - golden_variant\n-    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n-    golden_rel_err = torch.max(torch.abs(golden_diff / golden)).item()\n-\n+    golden_abs_err, golden_rel_err = get_proper_err(a, b, golden)\n     torch.set_printoptions(profile=\"full\")\n     assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err), check_dtype=False)\n \n@@ -272,4 +276,5 @@ def matmul_kernel(\n                         BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N, BLOCK_SIZE_K=block_K)\n \n     golden = torch.matmul(a, b)\n-    torch.testing.assert_close(c, golden)\n+    golden_abs_err, golden_rel_err = get_proper_err(a, b, golden)\n+    torch.testing.assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err))"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 117, "deletions": 50, "changes": 167, "file_content_changes": "@@ -1028,7 +1028,7 @@ def binary_name_to_header_name(name):\n     return f\"{name}.h\"\n \n \n-def generate_launcher(identifier, constants, signature):\n+def generate_launcher(constants, signature):\n     arg_decls = ', '.join(f\"{ty_to_cpp(ty)} arg{i}\" for i, ty in signature.items())\n \n     def _extracted_type(ty):\n@@ -1184,6 +1184,9 @@ def has_file(self, filename):\n     def put(self, data, filename, binary=True):\n         if not self.cache_dir:\n             return\n+        binary = isinstance(data, bytes)\n+        if not binary:\n+          data = str(data)\n         assert self.lock_path is not None\n         filepath = self._make_path(filename)\n         with FileLock(self.lock_path):\n@@ -1296,74 +1299,138 @@ def read_or_execute(cache_manager, force_compile, file_name, metadata,\n     cache_manager.put(data, file_name, True if isinstance(data, bytes) else data)\n     return module, md5, True, False\n \n-\n-def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: int = 4, num_stages: int = 3, extern_libs=None, configs=None):\n-    if isinstance(signature, str):\n-        signature = {k: v.strip() for k, v in enumerate(signature.split(\",\"))}\n-    # we get the kernel, i.e. the first function generated in the module\n-    if configs is None:\n-        configs = [instance_descriptor()]\n-    assert len(configs) == 1\n-    # cache manager\n-    name = fn.__name__\n+# \n+def make_stub(name, signature, constants):\n     # name of files that are cached\n     so_cache_key = make_so_cache_key(signature, constants)\n     so_cache_manager = CacheManager(so_cache_key)\n     so_name = f\"{name}.so\"\n     # retrieve stub from cache if it exists\n     if not so_cache_manager.has_file(so_name):\n         with tempfile.TemporaryDirectory() as tmpdir:\n-            src = generate_launcher(name, constants, signature)\n+            src = generate_launcher(constants, signature)\n             src_path = os.path.join(tmpdir, \"main.c\")\n             with open(src_path, \"w\") as f:\n                 f.write(src)\n-            so = _build(fn.__name__, src_path, tmpdir)\n+            so = _build(name, src_path, tmpdir)\n             with open(so, \"rb\") as f:\n                 so_cache_manager.put(f.read(), so_name, binary=True)\n-    so_path = so_cache_manager._make_path(so_name)\n+    return so_cache_manager._make_path(so_name)\n+\n+\n+def convert_type_repr(x):\n+    match = re.search('!tt\\.ptr<(.*)>', x)\n+    if match is not None:\n+      return '*' + convert_type_repr(match.group(1))\n+    return x\n+\n+def make_hash(fn, **kwargs):\n+    if isinstance(fn, triton.runtime.JITFunction):\n+        configs = kwargs[\"configs\"]\n+        signature = kwargs[\"signature\"]\n+        constants = kwargs.get(\"constants\", dict())\n+        num_warps = kwargs.get(\"num_warps\", 4)\n+        num_stages = kwargs.get(\"num_stages\", 3)\n+        # Get unique key for the compiled code\n+        get_conf_key = lambda conf: (sorted(conf.divisible_by_16), sorted(conf.equal_to_1))\n+        configs_key = [get_conf_key(conf) for conf in configs]\n+        key = f\"{fn.cache_key}-{''.join(signature.values())}-{configs_key}-{constants}-{num_warps}-{num_stages}\"\n+        return hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n+    assert isinstance(fn, str)\n+    return hashlib.md5(Path(fn).read_text().encode(\"utf-8\")).hexdigest()\n+\n+\n+\n+# def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: int = 4, num_stages: int = 3, extern_libs=None, configs=None):\n+def compile(fn, **kwargs):\n+    # we get the kernel, i.e. the first function generated in the module\n+    # if fn is not a JITFunction, then it \n+    # has to be a path to a file\n+    context = _triton.ir.context()\n+    asm, md5 = dict(), dict()\n+    constants = kwargs.get(\"constants\", dict())\n+    if isinstance(fn, triton.runtime.JITFunction):\n+        configs = kwargs.get(\"configs\", None)\n+        signature = kwargs[\"signature\"]\n+        if configs is None:\n+            configs = [instance_descriptor()]\n+        assert len(configs) == 1\n+        kwargs[\"configs\"] = configs\n+        name = fn.__name__\n+        first_stage = 0\n+        if isinstance(signature, str):\n+            signature = {k: v.strip() for k, v in enumerate(signature.split(\",\"))}\n+        kwargs[\"signature\"] = signature\n+    else:\n+        assert isinstance(fn, str)\n+        name, ir = os.path.basename(fn).split(\".\")\n+        assert ir == \"ttgir\"\n+        asm[ir] = _triton.ir.parse_mlir_module(fn, context)\n+        function = asm[ir].get_single_function()\n+        param_tys = [convert_type_repr(str(ty)) for ty in function.type.param_types()]\n+        signature = {k: v for k, v in enumerate(param_tys)}\n+        first_stage = 2\n+        \n+    # cache manager\n+    so_path = make_stub(name, signature, constants)\n     # create cache manager\n-    fn_cache_key = make_fn_cache_key(fn.cache_key, signature, configs, constants, num_warps, num_stages)\n-    fn_cache_manager = CacheManager(fn_cache_key)\n+    fn_cache_manager = CacheManager(make_hash(fn, **kwargs))\n+    # determine name and extension type of provided function\n+    if isinstance(fn, triton.runtime.JITFunction):\n+      name, ext = fn.__name__, \"ast\"\n+    else:\n+      name, ext = os.path.basename(fn).split(\".\")\n+    # initialize compilation params\n+    num_warps = kwargs.get(\"num_warps\", 4)\n+    num_stages = kwargs.get(\"num_stages\", 3)\n+    extern_libs = kwargs.get(\"extern_libs\", dict())\n+    device = kwargs.get(\"device\", torch.cuda.current_device())\n     # load metadata if any\n     metadata = None\n     if fn_cache_manager.has_file(f'{name}.json'):\n       with open(fn_cache_manager._make_path(f\"{name}.json\")) as f:\n             metadata = json.load(f)\n-    context = _triton.ir.context()\n-    force_compile = False\n-    # ast -> triton-ir (or read from cache)\n-    ttir, ttir_md5, force_compile, _ = read_or_execute(fn_cache_manager, force_compile, f\"{name}.ttir\", metadata,\n-                           run_if_found = lambda path: _triton.ir.parse_mlir_module(path, context),\n-                           run_if_not_found = lambda: ast_to_ttir(fn, signature, configs[0], constants))\n-    # triton-ir -> triton-gpu-ir (or read from cache)\n-    ttgir, ttgir_md5, force_compile, _ = read_or_execute(fn_cache_manager, force_compile, f\"{name}.ttgir\", metadata,\n-                            run_if_found = lambda path: _triton.ir.parse_mlir_module(path, context),\n-                            run_if_not_found = lambda: ttir_to_ttgir(ttir, num_warps, num_stages))\n-    # triton-gpu-ir -> llvm-ir (or read from cache)\n-    llir, llir_md5, force_compile, llvm_cached = read_or_execute(fn_cache_manager, force_compile, f\"{name}.llir\", metadata,\n-                           run_if_found = lambda path: Path(path).read_bytes(),\n-                           run_if_not_found = lambda: ttgir_to_llir(ttgir, extern_libs))\n-    if llvm_cached:\n-        shmem_size = metadata[\"shared\"]\n     else:\n-        shmem_size = _triton.get_shared_memory_size(ttgir)\n-    # llvm-ir -> ptx (or read from cache)\n-    ptx, ptx_md5, force_compile, _ = read_or_execute(fn_cache_manager, force_compile, f\"{name}.ptx\", metadata,\n-                          run_if_found = lambda path: Path(path).read_text(),\n-                          run_if_not_found = lambda: llir_to_ptx(llir))\n-    # ptx -> cubin (or read from cache)\n-    cubin, cubin_md5, force_compile, _ = read_or_execute(fn_cache_manager, force_compile, f\"{name}.cubin\", metadata,\n-                            run_if_found = lambda path: Path(path).read_bytes(),      \n-                            run_if_not_found= lambda: ptx_to_cubin(ptx, device))\n-    # dump new metadata\n-    kernel_name = ptx_get_kernel_name(ptx)\n-    metadata = {\"name\": kernel_name, \"shared\": shmem_size, \"num_warps\": num_warps, \"num_stages\": num_stages,\n-                \"md5\": {  \"cubin\": cubin_md5,  \"ptx\": ptx_md5,  \n-                          \"llir\": llir_md5,\n-                          \"ttir\": ttir_md5, \"ttgir\": ttgir_md5 }}\n+      metadata = {\"num_warps\": num_warps, \"num_stages\": num_stages, \"ctime\": dict()}\n+    # build compilation stages\n+    stages = {\n+      \"ast\" : (lambda path: fn, None),\n+      \"ttir\": (lambda path: _triton.ir.parse_mlir_module(path, context), \n+               lambda src: ast_to_ttir(src, signature, configs[0], constants)),\n+      \"ttgir\": (lambda path: _triton.ir.parse_mlir_module(path, context), \n+                lambda src: ttir_to_ttgir(src, num_warps, num_stages)),\n+      \"llir\": (lambda path: Path(path).read_bytes(), \n+              lambda src: ttgir_to_llir(src, extern_libs)),\n+      \"ptx\":  (lambda path: Path(path).read_text(), \n+              llir_to_ptx),\n+      \"cubin\": (lambda path: Path(path).read_bytes(), \n+               lambda src: ptx_to_cubin(src, device))\n+    }\n+    first_stage = list(stages.keys()).index(ext)\n+    asm = dict()\n+    module = fn\n+    # run compilation pipeline  and populate metadata\n+    for ir, (parse, compile) in list(stages.items())[first_stage:]:\n+      path = fn_cache_manager._make_path(f\"{name}.{ir}\")\n+      if ir == ext:\n+        next_module = parse(fn)\n+      elif os.path.exists(path) and\\\n+           os.path.getctime(path) == metadata[\"ctime\"][ir]:\n+        next_module = parse(path)\n+      else:\n+        next_module = compile(module)\n+        fn_cache_manager.put(next_module, f\"{name}.{ir}\")\n+      if os.path.exists(path):\n+        metadata[\"ctime\"][ir] = os.path.getctime(path)\n+      asm[ir] = next_module if ir == \"cubin\" else str(next_module)\n+      if ir == \"llir\" and \"shared\" not in metadata:\n+        metadata[\"shared\"] = _triton.get_shared_memory_size(module)\n+      if ir == \"ptx\":\n+        metadata[\"name\"] = ptx_get_kernel_name(next_module)\n+      module = next_module\n+    # write-back metadata\n     fn_cache_manager.put(json.dumps(metadata), f\"{name}.json\", binary=False)\n-\n-    asm = {\"ttir\": ttir, \"ttgir\": ttgir, \"llir\": llir, \"ptx\": ptx, \"cubin\": cubin}\n+    # return handle to compiled kernel\n     return CompiledKernel(so_path, metadata, asm)\n \n \n@@ -1395,7 +1462,7 @@ def runner(*args, stream=None):\n             if stream is None:\n                 stream = torch.cuda.current_stream().cuda_stream\n             self.c_wrapper(grid[0], grid[1], grid[2], self.num_warps, self.shared, stream, self.cu_function, *args)\n-        return\n+        return runner\n \n     def get_sass(self, fun=None):\n         if 'sass' in self.asm:"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -275,7 +275,7 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n           raise TypeError(f\"Callable constexpr at index {i} is not supported\")\n       device = 0\n       if not self._call_hook(key, signature, device, constants, num_warps, num_stages, extern_libs, configs):\n-        bin = triton.compile(self, signature, device, constants, num_warps, num_stages, extern_libs=extern_libs, configs=configs)\n+        bin = triton.compile(self, signature=signature, device=device, constants=constants, num_warps=num_warps, num_stages=num_stages, extern_libs=extern_libs, configs=configs)\n         if not warmup:\n             bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.shared, stream, bin.cu_function, *args)\n         self.cache[key] = bin"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -245,12 +245,12 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %0 = tt.view %arg : (tensor<256xf32, #blocked0>) -> tensor<256x1xf32,#blocked2>\n     // CHECK: llvm.mlir.undef\n     // CHECK: llvm.insertvalue %[[T0]]\n-    // CHECK: llvm.insertvalue %[[T0]]\n-    // CHECK: llvm.insertvalue %[[T0]]\n-    // CHECK: llvm.insertvalue %[[T0]]\n     // CHECK: llvm.insertvalue %[[T1]]\n+    // CHECK: llvm.insertvalue %[[T0]]\n     // CHECK: llvm.insertvalue %[[T1]]\n+    // CHECK: llvm.insertvalue %[[T0]]\n     // CHECK: llvm.insertvalue %[[T1]]\n+    // CHECK: llvm.insertvalue %[[T0]]\n     // CHECK: llvm.insertvalue %[[T1]]\n     %1 = tt.broadcast %0 : (tensor<256x1xf32,#blocked2>) -> tensor<256x4xf32, #blocked2>\n     return"}]