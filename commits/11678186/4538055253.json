[{"filename": "CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n-cmake_minimum_required(VERSION 3.20)\n+cmake_minimum_required(VERSION 3.18)\n \n if(POLICY CMP0116)\n # Introduced in cmake 3.20"}, {"filename": "include/triton/Dialect/Triton/IR/Traits.h", "status": "modified", "additions": 55, "deletions": 5, "changes": 60, "file_content_changes": "@@ -1,9 +1,8 @@\n #ifndef TRITON_IR_TRAITS_H_\n #define TRITON_IR_TRAITS_H_\n \n-#include \"mlir/IR/OpDefinition.h\"\n-\n #include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/OpDefinition.h\"\n #include \"mlir/Support/LogicalResult.h\"\n \n #include <iostream>\n@@ -12,11 +11,9 @@ namespace mlir {\n namespace OpTrait {\n \n // These functions are out-of-line implementations of the methods in the\n-// corresponding trait classes.  This avoids them being template\n+// corresponding trait classes. This avoids them being template\n // instantiated/duplicated.\n namespace impl {\n-LogicalResult verifySameOperandsAndResultEncoding(Operation *op);\n-LogicalResult verifySameOperandsEncoding(Operation *op);\n // The rationale for this trait is to prevent users from creating programs\n // that would have catastrophic register pressure and cause the compiler to\n // hang.\n@@ -25,7 +22,22 @@ LogicalResult verifySameOperandsEncoding(Operation *op);\n // but we probably should limit number of elements (rather than bytes) to\n // keep specs simple\n int constexpr maxTensorNumElements = 1048576;\n+\n LogicalResult verifyTensorSize(Operation *op);\n+\n+LogicalResult verifySameOperandsEncoding(Operation *op,\n+                                         bool allowTensorPointerType = false);\n+\n+LogicalResult\n+verifySameOperandsAndResultEncoding(Operation *op,\n+                                    bool allowTensorPointerType = false);\n+\n+LogicalResult verifySameLoadStoreOperandsShape(Operation *op);\n+\n+LogicalResult verifySameLoadStoreOperandsAndResultShape(Operation *op);\n+\n+bool verifyLoadStorePointerAndValueType(Type valueType, Type ptrType);\n+\n } // namespace impl\n \n template <class ConcreteType>\n@@ -54,6 +66,44 @@ class SameOperandsEncoding\n   }\n };\n \n+template <typename ConcreteType>\n+class SameLoadStoreOperandsShape\n+    : public TraitBase<ConcreteType, SameLoadStoreOperandsShape> {\n+public:\n+  static LogicalResult verifyTrait(Operation *op) {\n+    return impl::verifySameLoadStoreOperandsShape(op);\n+  }\n+};\n+\n+template <typename ConcreteType>\n+class SameLoadStoreOperandsAndResultShape\n+    : public TraitBase<ConcreteType, SameLoadStoreOperandsAndResultShape> {\n+public:\n+  static LogicalResult verifyTrait(Operation *op) {\n+    return impl::verifySameLoadStoreOperandsAndResultShape(op);\n+  }\n+};\n+\n+template <typename ConcreteType>\n+class SameLoadStoreOperandsEncoding\n+    : public TraitBase<ConcreteType, SameLoadStoreOperandsEncoding> {\n+public:\n+  static LogicalResult verifyTrait(Operation *op) {\n+    return impl::verifySameOperandsEncoding(op,\n+                                            /*allowTensorPointerType=*/true);\n+  }\n+};\n+\n+template <typename ConcreteType>\n+class SameLoadStoreOperandsAndResultEncoding\n+    : public TraitBase<ConcreteType, SameLoadStoreOperandsAndResultEncoding> {\n+public:\n+  static LogicalResult verifyTrait(Operation *op) {\n+    return impl::verifySameOperandsAndResultEncoding(\n+        op, /*allowTensorPointerType=*/true);\n+  }\n+};\n+\n } // namespace OpTrait\n } // namespace mlir\n "}, {"filename": "include/triton/Dialect/Triton/IR/TritonAttrDefs.td", "status": "modified", "additions": 12, "deletions": 1, "changes": 13, "file_content_changes": "@@ -3,7 +3,7 @@\n \n include \"mlir/IR/EnumAttr.td\"\n \n-// Attrs for LoadOp\n+// Attributes for LoadOp\n def TT_CacheModifierAttr : I32EnumAttr<\n     \"CacheModifier\", \"\",\n     [\n@@ -13,6 +13,7 @@ def TT_CacheModifierAttr : I32EnumAttr<\n     ]> {\n     let cppNamespace = \"::mlir::triton\";\n }\n+\n def TT_EvictionPolicyAttr : I32EnumAttr<\n     \"EvictionPolicy\", \"\",\n     [\n@@ -23,6 +24,16 @@ def TT_EvictionPolicyAttr : I32EnumAttr<\n     let cppNamespace = \"::mlir::triton\";\n }\n \n+def TT_PaddingOptionAttr : I32EnumAttr<\n+    \"PaddingOption\", \"\",\n+    [\n+        I32EnumAttrCase<\"PAD_ZERO\", 1, \"zero\">,\n+        // We can not set the string value to \"NAN\" because it is a keyword in C++\n+        I32EnumAttrCase<\"PAD_NAN\", 2, \"nan\">\n+    ]> {\n+    let cppNamespace = \"::mlir::triton\";\n+}\n+\n // reduction\n def TT_RedOpAttr : I32EnumAttr<\n     /*name*/\"RedOp\", /*summary*/\"\","}, {"filename": "include/triton/Dialect/Triton/IR/TritonInterfaces.td", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -4,8 +4,11 @@\n include \"mlir/IR/OpBase.td\"\n \n def TensorSizeTrait : NativeOpTrait<\"TensorSizeTrait\">;\n-def SameOperandsAndResultEncoding : NativeOpTrait<\"SameOperandsAndResultEncoding\">;\n def SameOperandsEncoding : NativeOpTrait<\"SameOperandsEncoding\">;\n-\n+def SameOperandsAndResultEncoding : NativeOpTrait<\"SameOperandsAndResultEncoding\">;\n+def SameLoadStoreOperandsShape : NativeOpTrait<\"SameLoadStoreOperandsShape\">;\n+def SameLoadStoreOperandsAndResultShape : NativeOpTrait<\"SameLoadStoreOperandsAndResultShape\">;\n+def SameLoadStoreOperandsEncoding : NativeOpTrait<\"SameLoadStoreOperandsEncoding\">;\n+def SameLoadStoreOperandsAndResultEncoding : NativeOpTrait<\"SameLoadStoreOperandsAndResultEncoding\">;\n \n #endif // TRITON_INTERFACES"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 97, "deletions": 17, "changes": 114, "file_content_changes": "@@ -107,66 +107,104 @@ def TT_AddPtrOp : TT_Op<\"addptr\",\n     let assemblyFormat = \"$ptr `,` $offset attr-dict `:` type($result) `,` type($offset)\";\n }\n \n+def TT_AdvanceOp : TT_Op<\"advance\",\n+                         [Pure,\n+                          TypesMatchWith<\"result type matches ptr type\",\n+                                         \"result\", \"ptr\", \"$_self\">]> {\n+    let summary = \"Advance a tensor pointer by offsets\";\n+\n+    let arguments = (ins TT_TensorPtr:$ptr, Variadic<I32>:$offsets);\n+\n+    let results = (outs TT_TensorPtr:$result);\n+\n+    let assemblyFormat = \"$ptr `,` `[` $offsets `]` attr-dict `:` type($result)\";\n+}\n \n //\n // Load/Store Ops\n //\n def TT_LoadOp : TT_Op<\"load\",\n-                      [SameOperandsAndResultShape,\n-                       SameOperandsAndResultEncoding,\n+                      [SameLoadStoreOperandsAndResultShape,\n+                       SameLoadStoreOperandsAndResultEncoding,\n                        AttrSizedOperandSegments,\n                        MemoryEffects<[MemRead]>,\n                        TypesMatchWith<\"infer ptr type from result type\",\n-                                      \"result\", \"ptr\", \"getPointerTypeSameShape($_self)\">,\n+                                      \"result\", \"ptr\", \"$_self\",\n+                                      \"mlir::OpTrait::impl::verifyLoadStorePointerAndValueType\">,\n                        TypesMatchWith<\"infer mask type from result type or none\",\n                                       \"result\", \"mask\", \"getI1SameShape($_self)\",\n                                       \"($_op.getOperands().size() <= 1) || std::equal_to<>()\">,\n                        TypesMatchWith<\"infer other type from result type or none\",\n                                       \"result\", \"other\", \"$_self\",\n                                       \"($_op.getOperands().size() <= 2) || std::equal_to<>()\">]> {\n-    let summary = \"load\";\n+    let summary = \"Load from a tensor of pointers or from a tensor pointer\";\n \n-    let arguments = (ins TT_PtrLike:$ptr, Optional<TT_BoolLike>:$mask, Optional<TT_Type>:$other,\n-                         TT_CacheModifierAttr:$cache, TT_EvictionPolicyAttr:$evict,\n-                         BoolAttr:$isVolatile);\n+    let arguments = (ins AnyTypeOf<[TT_PtrLike, TT_TensorPtr]>:$ptr, Optional<TT_BoolLike>:$mask,\n+                         Optional<TT_Type>:$other, OptionalAttr<DenseI32ArrayAttr>:$boundaryCheck,\n+                         OptionalAttr<TT_PaddingOptionAttr>:$padding, TT_CacheModifierAttr:$cache,\n+                         TT_EvictionPolicyAttr:$evict, BoolAttr:$isVolatile);\n \n     let results = (outs TT_Type:$result);\n \n     let builders = [\n+        // A tensor of pointers or a pointer to a scalar\n         OpBuilder<(ins \"Value\":$ptr, \"triton::CacheModifier\":$cache,\n                        \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile)>,\n+        // A tensor pointer with boundary check and padding\n+        OpBuilder<(ins \"Value\":$ptr, \"ArrayRef<int32_t>\":$boundaryCheck,\n+                       \"Optional<triton::PaddingOption>\":$padding, \"triton::CacheModifier\":$cache,\n+                       \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile)>,\n+        // A tensor of pointers or a pointer to a scalar with mask\n         OpBuilder<(ins \"Value\":$ptr, \"Value\":$mask, \"triton::CacheModifier\":$cache,\n                        \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile)>,\n+        // A tensor of pointers or a pointer to a scalar with mask and other\n         OpBuilder<(ins \"Value\":$ptr, \"Value\":$mask, \"Value\":$other, \"triton::CacheModifier\":$cache,\n                        \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile)>,\n+        // A utility function to build the operation with all attributes\n+        OpBuilder<(ins \"Value\":$ptr, \"Value\":$mask, \"Value\":$other, \"Optional<ArrayRef<int32_t>>\":$boundaryCheck,\n+                       \"Optional<triton::PaddingOption>\":$padding, \"triton::CacheModifier\":$cache,\n+                       \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile)>\n     ];\n \n+    // Format: `tt.load operands attrs : optional(type(ptr)) -> type(result)`\n+    // We need an extra `optional(type(ptr))` for inferring the tensor pointer type with back compatibility\n     let hasCustomAssemblyFormat = 1;\n+\n     let hasCanonicalizer = 1;\n }\n \n def TT_StoreOp : TT_Op<\"store\",\n-                       [SameOperandsShape,\n-                        SameOperandsEncoding,\n+                       [SameLoadStoreOperandsShape,\n+                        SameLoadStoreOperandsEncoding,\n                         MemoryEffects<[MemWrite]>,\n                         TypesMatchWith<\"infer ptr type from value type\",\n-                                       \"value\", \"ptr\",\n-                                       \"getPointerTypeSameShape($_self)\">,\n+                                       \"value\", \"ptr\", \"$_self\",\n+                                       \"mlir::OpTrait::impl::verifyLoadStorePointerAndValueType\">,\n                         TypesMatchWith<\"infer mask type from value type\",\n                                        \"value\", \"mask\", \"getI1SameShape($_self)\",\n                                        \"($_op.getOperands().size() <= 2) || std::equal_to<>()\">]> {\n-    let summary = \"store\";\n+    let summary = \"Store by a tensor of pointers or by a tensor pointer\";\n \n-    let arguments = (ins TT_PtrLike:$ptr, TT_Type:$value, Optional<TT_BoolLike>:$mask,\n-                     DefaultValuedAttr<TT_CacheModifierAttr, \"triton::CacheModifier::NONE\">:$cache,\n-                     DefaultValuedAttr<TT_EvictionPolicyAttr, \"triton::EvictionPolicy::NORMAL\">:$evict);\n+    let arguments = (ins AnyTypeOf<[TT_PtrLike, TT_TensorPtr]>:$ptr, TT_Type:$value, Optional<TT_BoolLike>:$mask,\n+                         OptionalAttr<DenseI32ArrayAttr>:$boundaryCheck,\n+                         DefaultValuedAttr<TT_CacheModifierAttr, \"triton::CacheModifier::NONE\">:$cache,\n+                         DefaultValuedAttr<TT_EvictionPolicyAttr, \"triton::EvictionPolicy::NORMAL\">:$evict);\n \n     let builders = [\n-        OpBuilder<(ins \"Value\":$ptr, \"Value\":$value, \"triton::CacheModifier\":$cache,\n+        // A tensor of pointers or a pointer to a scalar\n+        OpBuilder<(ins \"Value\":$ptr, \"Value\":$value, \"triton::CacheModifier\":$cache, \"triton::EvictionPolicy\":$evict)>,\n+        // A tensor of pointers or a pointer to a scalar with mask\n+        OpBuilder<(ins \"Value\":$ptr, \"Value\":$value, \"Value\":$mask, \"triton::CacheModifier\":$cache,\n                        \"triton::EvictionPolicy\":$evict)>,\n+        // A tensor pointer with boundary check\n+        OpBuilder<(ins \"Value\":$ptr, \"Value\":$value, \"ArrayRef<int32_t>\":$boundaryCheck, \"triton::CacheModifier\":$cache,\n+                       \"triton::EvictionPolicy\":$evict)>\n     ];\n \n+    // Format: `tt.store operands attrs : optional(type(ptr)), type(val)\n+    // We need an extra `optional(type(ptr))` for inferring the tensor pointer type with back compatibility\n     let hasCustomAssemblyFormat = 1;\n+\n     let hasCanonicalizer = 1;\n }\n \n@@ -213,7 +251,7 @@ def TT_AtomicCASOp : TT_Op<\"atomic_cas\", [MemoryEffects<[MemRead]>,\n         return $old\n     }];\n \n-    let arguments = (ins TT_Ptr:$ptr, TT_Type:$cmp, TT_Type:$val);\n+    let arguments = (ins TT_PtrLike:$ptr, TT_Type:$cmp, TT_Type:$val);\n \n     let results = (outs TT_Type:$result);\n }\n@@ -438,4 +476,46 @@ def TT_AssertOp : TT_Op<\"assert\", [MemoryEffects<[MemWrite]>]> {\n   let assemblyFormat = \"$condition `,` $message `,` $file `,` $func `,` $line attr-dict `:` type($condition)\";\n }\n \n+//\n+// Make a Tensor Pointer\n+//\n+def TT_MakeTensorPtrOp : TT_Op<\"make_tensor_ptr\",\n+                               [Pure,\n+                                SameVariadicOperandSize,\n+                                TypesMatchWith<\"infer pointer type from the result type\",\n+                                               \"result\", \"base\",\n+                                               \"getPointerType(getElementTypeOfTensorPointerType($_self))\">]> {\n+  let summary = \"Make a tensor pointer type with meta information of the parent tensor and the block specified\";\n+\n+  let description = [{\n+      `tt.make_tensor_ptr` takes both meta information of the parent tensor and the block tensor, then it returns a\n+      pointer to the block tensor, e.g. returns a type of `tt.ptr<tensor<8x8xf16>>`.\n+  }];\n+\n+  // TODO(Chenggang): unify the integer types. Currently we cannot do that due to hardware constraints.\n+  let arguments = (ins\n+    TT_Ptr:$base,\n+    Variadic<I64>:$shape,\n+    Variadic<I64>:$strides,\n+    Variadic<I32>:$offsets,\n+    DenseI32ArrayAttr:$order\n+  );\n+\n+  let results = (outs TT_TensorPtr:$result);\n+\n+  // Add additional `[]` to increase readability and split variadic lists\n+  let assemblyFormat = \"$base `,` `[` $shape `]` `,` `[` $strides `]` `,` `[` $offsets `]` attr-dict `:` type($result)\";\n+\n+  let builders = [\n+    OpBuilder<(ins\n+        \"Value\":$base,\n+        \"ValueRange\":$shape,\n+        \"ValueRange\":$strides,\n+        \"ValueRange\":$offsets,\n+        \"ArrayRef<int32_t>\":$tensorShape,\n+        \"ArrayRef<int32_t>\":$order\n+    )>\n+  ];\n+}\n+\n #endif // Triton_OPS"}, {"filename": "include/triton/Dialect/Triton/IR/TritonTypes.td", "status": "modified", "additions": 29, "deletions": 7, "changes": 36, "file_content_changes": "@@ -31,19 +31,28 @@ def TT_IntLike : AnyTypeOf<[TT_Int, TT_IntTensor]>;\n // I32 Type\n // TT_I32 -> I32\n // TT_I32Tensor -> I32Tensor\n-def TT_I32Like: AnyTypeOf<[I32, I32Tensor]>;\n+def TT_I32Like : AnyTypeOf<[I32, I32Tensor]>;\n \n // I64 Type\n // TT_I64 -> I64\n // TT_I64Tensor -> I64Tensor\n-def TT_I64Like: AnyTypeOf<[I64, I64Tensor]>;\n+def TT_I64Like : AnyTypeOf<[I64, I64Tensor]>;\n \n-// Pointer Type\n-def TT_Ptr : TritonTypeDef<\"Pointer\", \"ptr\"> {\n-    let summary = \"pointer type\";\n+// Pointer Type in TableGen\n+class TT_PtrOf<list<Type> pointeeTypes> :\n+    DialectType<Triton_Dialect,\n+                And<[CPred<\"$_self.isa<::mlir::triton::PointerType>()\">,\n+                     Concat<\"[](::mlir::Type pointeeType) { return \",\n+                            SubstLeaves<\"$_self\", \"pointeeType\", AnyTypeOf<pointeeTypes>.predicate>,\n+                                        \"; }($_self.cast<::mlir::triton::PointerType>().getPointeeType())\">]>,\n+                \"ptr\", \"::mlir::triton::PointerType\">;\n+\n+// Pointer Type in C++ (corresponding to `TT_PtrOf`)\n+def TT_PtrType : TritonTypeDef<\"Pointer\", \"ptr\"> {\n+    let summary = \"Pointer type (`::mlir::triton::PointerType`) in Triton IR type system\";\n \n     let description = [{\n-        Triton PointerType\n+        Pointer type in Triton IR type system, which could be pointing to scalars or tensors.\n     }];\n \n     let parameters = (ins \"Type\":$pointeeType, \"int\":$addressSpace);\n@@ -58,14 +67,27 @@ def TT_Ptr : TritonTypeDef<\"Pointer\", \"ptr\"> {\n     ];\n \n     let hasCustomAssemblyFormat = 1;\n+\n     let skipDefaultBuilders = 1;\n }\n+\n+// Scalar Pointer Type: `ptr<>`\n+def TT_Ptr : TT_PtrOf<[AnyType]>;\n+\n+// Tensor of Pointer Type\n def TT_PtrTensor : TensorOf<[TT_Ptr]>;\n+\n+// Tensor of Pointer Type or Pointer type: `tensor<ptr<>>` or `ptr<>`\n def TT_PtrLike : AnyTypeOf<[TT_Ptr, TT_PtrTensor]>;\n \n+// Tensor Type\n def TT_FpIntTensor : AnyTypeOf<[TT_FloatTensor, TT_IntTensor]>;\n def TT_Tensor : AnyTypeOf<[TT_FpIntTensor, TT_PtrTensor]>;\n \n-def TT_Type : AnyTypeOf<[TT_FloatLike, TT_IntLike, TT_PtrLike]>;\n+// Pointer Type to Tensor Type: `ptr<tensor<>>`\n+def TT_TensorPtr : TT_PtrOf<[TT_Tensor]>;\n+\n+// Any Type in Triton IR\n+def TT_Type : AnyTypeOf<[TT_FloatLike, TT_IntLike, TT_PtrLike, TT_TensorPtr]>;\n \n #endif"}, {"filename": "include/triton/Dialect/Triton/IR/Types.h", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -12,6 +12,8 @@ namespace mlir {\n \n unsigned getPointeeBitWidth(RankedTensorType tensorTy);\n \n-}\n+bool isTensorPointerType(Type type);\n+\n+} // namespace mlir\n \n #endif // TRITON_IR_TYPES_H_"}, {"filename": "include/triton/Dialect/Triton/Transforms/Passes.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -8,6 +8,9 @@ namespace triton {\n \n std::unique_ptr<Pass> createCombineOpsPass();\n \n+std::unique_ptr<Pass>\n+createRewriteTensorPointerPass(int computeCapability = 80);\n+\n } // namespace triton\n \n #define GEN_PASS_REGISTRATION"}, {"filename": "include/triton/Dialect/Triton/Transforms/Passes.td", "status": "modified", "additions": 19, "deletions": 0, "changes": 19, "file_content_changes": "@@ -19,4 +19,23 @@ def TritonCombineOps : Pass</*cli-arg*/\"triton-combine\", /*Op*/\"mlir::ModuleOp\">\n   let dependentDialects = [\"mlir::arith::ArithDialect\"];\n }\n \n+def TritonRewriteTensorPointer : Pass</*cli-arg*/\"triton-rewrite-tensor-pointer\", /*Op*/\"mlir::ModuleOp\"> {\n+  let summary = \"Rewrite load/stores with tensor pointers into legacy load/stores\";\n+  let description = [{\n+    This pass rewrites all load/store semantics initiated by a `tt.make_tensor_ptr` and `tt.advance` into legacy\n+    semantics. After this pass, `tt.make_tensor_ptr` and `tt.advance` will disappear, and it generates logics to compute\n+    the pointer/mask/other for each load/store.\n+  }];\n+\n+  let constructor = \"mlir::triton::createRewriteTensorPointerPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::TritonDialect\"];\n+\n+  let options = [\n+    Option<\"computeCapability\", \"compute-capability\",\n+           \"int32_t\", /*default*/\"80\",\n+           \"device compute capability\">\n+  ];\n+}\n+\n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -412,6 +412,8 @@ class ConvertTritonGPUToLLVM\n       auto loadOp = builder.create<triton::LoadOp>(\n           insertSliceAsyncOp.getLoc(), tmpTy, insertSliceAsyncOp.getSrc(),\n           insertSliceAsyncOp.getMask(), insertSliceAsyncOp.getOther(),\n+          // TODO(Chenggang): confirm `boundaryCheck` and `padding`\n+          /*boundaryCheck=*/nullptr, /*padding=*/nullptr,\n           insertSliceAsyncOp.getCache(), insertSliceAsyncOp.getEvict(),\n           insertSliceAsyncOp.getIsVolatile());\n "}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -355,6 +355,7 @@ struct TritonLoadPattern : public OpConversionPattern<triton::LoadOp> {\n     addNamedAttrs(rewriter.replaceOpWithNewOp<triton::LoadOp>(\n                       op, typeConverter->convertType(op.getType()),\n                       adaptor.getPtr(), adaptor.getMask(), adaptor.getOther(),\n+                      adaptor.getBoundaryCheckAttr(), adaptor.getPaddingAttr(),\n                       adaptor.getCache(), adaptor.getEvict(),\n                       adaptor.getIsVolatile()),\n                   adaptor.getAttributes());"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 161, "deletions": 32, "changes": 193, "file_content_changes": "@@ -36,66 +36,116 @@ static Type getPointerTypeSameShape(Type type) {\n   }\n }\n \n+static Type getPointerType(Type type) { return PointerType::get(type, 1); }\n+\n+static Type getElementTypeOfTensorPointerType(Type type) {\n+  if (auto ptrType = type.dyn_cast<PointerType>())\n+    if (auto tensorType = ptrType.getPointeeType().dyn_cast<RankedTensorType>())\n+      return tensorType.getElementType();\n+  return {};\n+}\n+\n // Parser & printer for assembly forms\n ParseResult LoadOp::parse(OpAsmParser &parser, OperationState &result) {\n+  // Parse operands\n   SmallVector<OpAsmParser::UnresolvedOperand, 4> allOperands;\n-  Type resultTypes[1];\n   SMLoc allOperandLoc = parser.getCurrentLocation();\n   if (parser.parseOperandList(allOperands) ||\n-      parser.parseOptionalAttrDict(result.attributes) || parser.parseColon() ||\n-      parser.parseCustomTypeWithFallback(resultTypes[0]))\n+      parser.parseOptionalAttrDict(result.attributes) || parser.parseColon())\n     return failure();\n \n-  result.addTypes(resultTypes);\n-\n+  // Operand types\n   SmallVector<Type> operandTypes;\n-  operandTypes.push_back(getPointerTypeSameShape(resultTypes[0])); // ptr\n+\n+  // Parse `optional(type(ptr)) -> type(result)`\n+  Type ptrType, resultType;\n+  if (parser.parseType(resultType))\n+    return failure();\n+  if (parser.parseOptionalArrow().succeeded()) {\n+    ptrType = resultType;\n+    if (parser.parseType(resultType))\n+      return failure();\n+    operandTypes.push_back(ptrType);\n+    result.addTypes(resultType);\n+  } else {\n+    operandTypes.push_back(getPointerTypeSameShape(resultType));\n+    result.addTypes(resultType);\n+  }\n+\n+  // Determine `mask` and `other`\n   int hasMask = 0, hasOther = 0;\n   if (allOperands.size() >= 2) {\n-    operandTypes.push_back(getI1SameShape(resultTypes[0])); // mask\n+    operandTypes.push_back(getI1SameShape(resultType));\n     hasMask = 1;\n   }\n   if (allOperands.size() >= 3) {\n-    operandTypes.push_back(resultTypes[0]); // other\n+    operandTypes.push_back(resultType);\n     hasOther = 1;\n   }\n \n   if (parser.resolveOperands(allOperands, operandTypes, allOperandLoc,\n                              result.operands))\n     return failure();\n-  // Deduce operand_segment_sizes from the number of the operands.\n-  auto operand_segment_sizesAttrName =\n+\n+  // Deduce `operandSegmentSizes` from the number of the operands\n+  auto operandSegmentSizesAttrName =\n       LoadOp::getOperandSegmentSizesAttrName(result.name);\n   result.addAttribute(\n-      operand_segment_sizesAttrName,\n+      operandSegmentSizesAttrName,\n       parser.getBuilder().getDenseI32ArrayAttr({1, hasMask, hasOther}));\n+\n   return success();\n }\n \n void LoadOp::print(OpAsmPrinter &printer) {\n   printer << \" \";\n   printer << getOperation()->getOperands();\n-  // \"operand_segment_sizes\" can be deduced, so we don't print it.\n+\n+  // `operandSegmentSizes` can be deduced, so we don't print it.\n   printer.printOptionalAttrDict(getOperation()->getAttrs(),\n                                 {getOperandSegmentSizesAttrName()});\n+\n+  // `type(ptr) -> type(result)`\n   printer << \" : \";\n+  // `type(ptr)` is optional during parsing, we only print for tensor pointers\n+  if (isTensorPointerType(getPtr().getType())) {\n+    printer.printStrippedAttrOrType(getPtr().getType());\n+    printer << \" -> \";\n+  }\n   printer.printStrippedAttrOrType(getResult().getType());\n }\n \n ParseResult StoreOp::parse(OpAsmParser &parser, OperationState &result) {\n+  // Parse operands\n   SmallVector<OpAsmParser::UnresolvedOperand, 4> allOperands;\n-  Type valueType;\n   SMLoc allOperandLoc = parser.getCurrentLocation();\n   if (parser.parseOperandList(allOperands) ||\n-      parser.parseOptionalAttrDict(result.attributes) || parser.parseColon() ||\n-      parser.parseCustomTypeWithFallback(valueType))\n+      parser.parseOptionalAttrDict(result.attributes) || parser.parseColon())\n     return failure();\n \n+  // Operand types\n   SmallVector<Type> operandTypes;\n-  operandTypes.push_back(getPointerTypeSameShape(valueType)); // ptr\n-  operandTypes.push_back(valueType);                          // value\n+\n+  // Parse `optional(type(ptr)), type(val)`\n+  // Pointer type\n+  Type ptrType, valType;\n+  if (parser.parseType(valType))\n+    return failure();\n+  if (parser.parseOptionalComma().succeeded()) {\n+    ptrType = valType;\n+    if (parser.parseType(valType))\n+      return failure();\n+    operandTypes.push_back(ptrType);\n+  } else {\n+    operandTypes.push_back(getPointerTypeSameShape(valType));\n+  }\n+\n+  // Value type\n+  operandTypes.push_back(valType);\n+\n+  // Determine `mask`\n   if (allOperands.size() >= 3)\n-    operandTypes.push_back(getI1SameShape(valueType)); // mask\n+    operandTypes.push_back(getI1SameShape(valType));\n \n   if (parser.resolveOperands(allOperands, operandTypes, allOperandLoc,\n                              result.operands))\n@@ -107,7 +157,14 @@ void StoreOp::print(OpAsmPrinter &printer) {\n   printer << \" \";\n   printer << getOperation()->getOperands();\n   printer.printOptionalAttrDict(getOperation()->getAttrs(), /*elidedAttrs=*/{});\n+\n+  // `type(ptr), type(value)`\n   printer << \" : \";\n+  // `type(ptr)` is optional during parsing, we only print for tensor pointers\n+  if (isTensorPointerType(getPtr().getType())) {\n+    printer.printStrippedAttrOrType(getPtr().getType());\n+    printer << \", \";\n+  }\n   printer.printStrippedAttrOrType(getValue().getType());\n }\n \n@@ -123,15 +180,6 @@ void StoreOp::print(OpAsmPrinter &printer) {\n namespace mlir {\n namespace triton {\n \n-//-- StoreOp --\n-void StoreOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n-                    ::mlir::Value ptr, ::mlir::Value value,\n-                    ::mlir::triton::CacheModifier cache,\n-                    ::mlir::triton::EvictionPolicy evict) {\n-  return StoreOp::build(builder, state, ptr, value, mlir::Value(), cache,\n-                        evict);\n-}\n-\n //-- LoadOp --\n static Type getLoadOpResultType(::mlir::OpBuilder &builder, Type ptrType) {\n   auto ptrTensorType = ptrType.dyn_cast<RankedTensorType>();\n@@ -146,34 +194,63 @@ static Type getLoadOpResultType(::mlir::OpBuilder &builder, Type ptrType) {\n void LoadOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n                    ::mlir::Value ptr, ::mlir::triton::CacheModifier cache,\n                    ::mlir::triton::EvictionPolicy evict, bool isVolatile) {\n-  LoadOp::build(builder, state, ptr, mlir::Value(), mlir::Value(), cache, evict,\n-                isVolatile);\n+  LoadOp::build(builder, state, ptr, /*mask=*/{}, /*other=*/{},\n+                /*boundaryCheck=*/{}, /*padding=*/{}, cache, evict, isVolatile);\n+}\n+\n+void LoadOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n+                   ::mlir::Value ptr, ArrayRef<int32_t> boundaryCheck,\n+                   std::optional<::mlir::triton::PaddingOption> padding,\n+                   ::mlir::triton::CacheModifier cache,\n+                   ::mlir::triton::EvictionPolicy evict, bool isVolatile) {\n+  LoadOp::build(builder, state, ptr, /*mask=*/{}, /*other=*/{}, boundaryCheck,\n+                padding, cache, evict, isVolatile);\n }\n \n void LoadOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n                    ::mlir::Value ptr, ::mlir::Value mask,\n                    ::mlir::triton::CacheModifier cache,\n                    ::mlir::triton::EvictionPolicy evict, bool isVolatile) {\n-  LoadOp::build(builder, state, ptr, mask, mlir::Value(), cache, evict,\n-                isVolatile);\n+  LoadOp::build(builder, state, ptr, mask, /*other=*/{}, /*boundaryCheck=*/{},\n+                /*padding=*/{}, cache, evict, isVolatile);\n }\n \n void LoadOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n                    ::mlir::Value ptr, ::mlir::Value mask, ::mlir::Value other,\n                    ::mlir::triton::CacheModifier cache,\n                    ::mlir::triton::EvictionPolicy evict, bool isVolatile) {\n-  Type resultType = getLoadOpResultType(builder, ptr.getType());\n+  LoadOp::build(builder, state, ptr, mask, other, /*boundaryCheck=*/{},\n+                /*padding=*/{}, cache, evict, isVolatile);\n+}\n \n+void LoadOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n+                   ::mlir::Value ptr, ::mlir::Value mask, ::mlir::Value other,\n+                   std::optional<ArrayRef<int32_t>> boundaryCheck,\n+                   std::optional<::mlir::triton::PaddingOption> padding,\n+                   ::mlir::triton::CacheModifier cache,\n+                   ::mlir::triton::EvictionPolicy evict, bool isVolatile) {\n+  // Operands\n   state.addOperands(ptr);\n   if (mask) {\n     state.addOperands(mask);\n     if (other) {\n       state.addOperands(other);\n     }\n   }\n+\n+  // Attributes\n   state.addAttribute(\n       getOperandSegmentSizesAttrName(state.name),\n       builder.getDenseI32ArrayAttr({1, (mask ? 1 : 0), (other ? 1 : 0)}));\n+  if (boundaryCheck.has_value()) {\n+    state.addAttribute(getBoundaryCheckAttrName(state.name),\n+                       builder.getDenseI32ArrayAttr(boundaryCheck.value()));\n+  }\n+  if (padding.has_value()) {\n+    state.addAttribute(getPaddingAttrName(state.name),\n+                       ::mlir::triton::PaddingOptionAttr::get(\n+                           builder.getContext(), padding.value()));\n+  }\n   state.addAttribute(\n       getCacheAttrName(state.name),\n       ::mlir::triton::CacheModifierAttr::get(builder.getContext(), cache));\n@@ -182,9 +259,39 @@ void LoadOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n       ::mlir::triton::EvictionPolicyAttr::get(builder.getContext(), evict));\n   state.addAttribute(getIsVolatileAttrName(state.name),\n                      builder.getBoolAttr(isVolatile));\n+\n+  // Result type\n+  Type resultType = getLoadOpResultType(builder, ptr.getType());\n   state.addTypes({resultType});\n }\n \n+//-- StoreOp --\n+void StoreOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n+                    ::mlir::Value ptr, ::mlir::Value value,\n+                    ::mlir::triton::CacheModifier cache,\n+                    ::mlir::triton::EvictionPolicy evict) {\n+  return StoreOp::build(builder, state, ptr, value, /*mask=*/{},\n+                        /*boundaryCheck=*/{}, cache, evict);\n+}\n+\n+void StoreOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n+                    ::mlir::Value ptr, ::mlir::Value value, ::mlir::Value mask,\n+                    ::mlir::triton::CacheModifier cache,\n+                    ::mlir::triton::EvictionPolicy evict) {\n+  return StoreOp::build(builder, state, ptr, value, mask, /*boundaryCheck=*/{},\n+                        cache, evict);\n+}\n+\n+void StoreOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n+                    ::mlir::Value ptr, ::mlir::Value value,\n+                    ArrayRef<int32_t> boundaryCheck,\n+                    ::mlir::triton::CacheModifier cache,\n+                    ::mlir::triton::EvictionPolicy evict) {\n+  return StoreOp::build(builder, state, ptr, value, /*mask=*/{},\n+                        builder.getDenseI32ArrayAttr(boundaryCheck), cache,\n+                        evict);\n+}\n+\n //-- TransOp --\n mlir::LogicalResult mlir::triton::TransOp::inferReturnTypes(\n     MLIRContext *context, Optional<Location> location, ValueRange operands,\n@@ -424,5 +531,27 @@ OpFoldResult BroadcastOp::fold(FoldAdaptor adaptor) {\n   return {};\n }\n \n+//-- MakeTensorPtrOp --\n+void MakeTensorPtrOp::build(::mlir::OpBuilder &builder,\n+                            ::mlir::OperationState &state, ::mlir::Value base,\n+                            ::mlir::ValueRange shape,\n+                            ::mlir::ValueRange strides,\n+                            ::mlir::ValueRange offsets,\n+                            ArrayRef<int32_t> tensorShape,\n+                            ArrayRef<int32_t> order) {\n+  // Get pointer type from `base`\n+  auto pointerType = base.getType().cast<PointerType>();\n+  assert(pointerType != nullptr);\n+\n+  // Build type `tt.ptr<tensor<tensorShape, base.pointeeType>>`\n+  auto tensorType = RankedTensorType::get(\n+      SmallVector<int64_t>(tensorShape.begin(), tensorShape.end()),\n+      pointerType.getPointeeType());\n+  auto result = PointerType::get(tensorType, 1);\n+\n+  return build(builder, state, result, base, shape, strides, offsets,\n+               builder.getDenseI32ArrayAttr(order));\n+}\n+\n } // namespace triton\n } // namespace mlir"}, {"filename": "lib/Dialect/Triton/IR/Traits.cpp", "status": "modified", "additions": 92, "deletions": 23, "changes": 115, "file_content_changes": "@@ -1,42 +1,59 @@\n #include \"triton/Dialect/Triton/IR/Traits.h\"\n \n-static mlir::LogicalResult verifySameEncoding(mlir::Type tyA, mlir::Type tyB) {\n-  using namespace mlir;\n-  auto encA = tyA.dyn_cast<RankedTensorType>();\n-  auto encB = tyB.dyn_cast<RankedTensorType>();\n-  if (!encA || !encB)\n-    return success();\n-  return encA.getEncoding() == encB.getEncoding() ? success() : failure();\n-}\n+#include \"mlir/IR/TypeUtilities.h\"\n+#include \"triton/Dialect/Triton/IR/Types.h\"\n \n-mlir::LogicalResult\n-mlir::OpTrait::impl::verifySameOperandsAndResultEncoding(Operation *op) {\n-  if (failed(verifyAtLeastNOperands(op, 1)) ||\n-      failed(verifyAtLeastNResults(op, 1)))\n-    return failure();\n+using namespace mlir;\n \n-  auto type = op->getOperand(0).getType();\n-  for (auto resultType : op->getResultTypes())\n-    if (failed(verifySameEncoding(resultType, type)))\n-      return op->emitOpError()\n-             << \"requires the same encoding for all operands and results\";\n-  return verifySameOperandsEncoding(op);\n+static LogicalResult verifySameEncoding(Type typeA, Type typeB,\n+                                        bool allowTensorPointerType) {\n+  auto getEncoding = [=](Type type) -> Attribute {\n+    auto rankedType = type.dyn_cast<RankedTensorType>();\n+    if (allowTensorPointerType) {\n+      if (auto ptrType = type.dyn_cast<triton::PointerType>())\n+        rankedType = ptrType.getPointeeType().dyn_cast<RankedTensorType>();\n+    } else {\n+      assert(!isTensorPointerType(type));\n+    }\n+    return rankedType ? rankedType.getEncoding() : Attribute();\n+  };\n+  auto encodingA = getEncoding(typeA);\n+  auto encodingB = getEncoding(typeB);\n+  if (!encodingA || !encodingB)\n+    return success();\n+  return encodingA == encodingB ? success() : failure();\n }\n \n-mlir::LogicalResult\n-mlir::OpTrait::impl::verifySameOperandsEncoding(Operation *op) {\n+LogicalResult\n+OpTrait::impl::verifySameOperandsEncoding(Operation *op,\n+                                          bool allowTensorPointerType) {\n   if (failed(verifyAtLeastNOperands(op, 1)))\n     return failure();\n \n   auto type = op->getOperand(0).getType();\n   for (auto opType : llvm::drop_begin(op->getOperandTypes(), 1))\n-    if (failed(verifySameEncoding(opType, type)))\n+    if (failed(verifySameEncoding(opType, type, allowTensorPointerType)))\n       return op->emitOpError() << \"requires the same encoding for all operands\";\n \n   return success();\n }\n \n-mlir::LogicalResult mlir::OpTrait::impl::verifyTensorSize(Operation *op) {\n+LogicalResult OpTrait::impl::verifySameOperandsAndResultEncoding(\n+    Operation *op, bool allowTensorPointerType) {\n+  if (failed(verifyAtLeastNOperands(op, 1)) ||\n+      failed(verifyAtLeastNResults(op, 1)))\n+    return failure();\n+\n+  auto type = op->getOperand(0).getType();\n+  for (auto resultType : op->getResultTypes())\n+    if (failed(verifySameEncoding(resultType, type, allowTensorPointerType)))\n+      return op->emitOpError()\n+             << \"requires the same encoding for all operands and results\";\n+\n+  return verifySameOperandsEncoding(op, allowTensorPointerType);\n+}\n+\n+LogicalResult OpTrait::impl::verifyTensorSize(Operation *op) {\n   for (auto opType : op->getOperandTypes()) {\n     if (auto tensorType = opType.dyn_cast<RankedTensorType>()) {\n       int64_t numElements = 1;\n@@ -69,3 +86,55 @@ mlir::LogicalResult mlir::OpTrait::impl::verifyTensorSize(Operation *op) {\n   }\n   return success();\n }\n+\n+static ArrayRef<int64_t> getTypeShape(Type type) {\n+  auto rankedType = type.dyn_cast<RankedTensorType>();\n+  if (auto ptrType = type.dyn_cast<triton::PointerType>())\n+    rankedType = ptrType.getPointeeType().dyn_cast<RankedTensorType>();\n+  return rankedType ? rankedType.getShape() : ArrayRef<int64_t>();\n+}\n+\n+LogicalResult OpTrait::impl::verifySameLoadStoreOperandsShape(Operation *op) {\n+  if (failed(verifyAtLeastNOperands(op, 1)))\n+    return failure();\n+\n+  auto firstOperandShape = getTypeShape(op->getOperand(0).getType());\n+  for (auto type : llvm::drop_begin(op->getOperandTypes(), 1))\n+    if (failed(verifyCompatibleShape(getTypeShape(type), firstOperandShape)))\n+      return op->emitOpError() << \"requires the same shape for all operands\";\n+\n+  return success();\n+}\n+\n+LogicalResult\n+OpTrait::impl::verifySameLoadStoreOperandsAndResultShape(Operation *op) {\n+  if (failed(verifyAtLeastNOperands(op, 1)) ||\n+      failed(verifyAtLeastNResults(op, 1)))\n+    return failure();\n+\n+  auto firstOperandShape = getTypeShape(op->getOperand(0).getType());\n+  for (auto type : op->getResultTypes())\n+    if (failed(verifyCompatibleShape(getTypeShape(type), firstOperandShape)))\n+      return op->emitOpError()\n+             << \"requires the same shape for all operands and results\";\n+\n+  return verifySameLoadStoreOperandsShape(op);\n+}\n+\n+bool OpTrait::impl::verifyLoadStorePointerAndValueType(Type valueType,\n+                                                       Type ptrType) {\n+  if (isTensorPointerType(ptrType)) {\n+    return ptrType.cast<triton::PointerType>().getPointeeType() == valueType;\n+  } else if (auto rankedType = ptrType.dyn_cast<RankedTensorType>()) {\n+    if (auto elementPtrType =\n+            dyn_cast<triton::PointerType>(rankedType.getElementType())) {\n+      auto inferValueType = RankedTensorType::get(\n+          rankedType.getShape(), elementPtrType.getPointeeType(),\n+          rankedType.getEncoding());\n+      return inferValueType == valueType;\n+    }\n+  } else if (auto scalarPtrType = ptrType.dyn_cast<triton::PointerType>()) {\n+    return scalarPtrType.getPointeeType() == valueType;\n+  }\n+  return false;\n+}"}, {"filename": "lib/Dialect/Triton/IR/Types.cpp", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -46,4 +46,10 @@ unsigned getPointeeBitWidth(RankedTensorType tensorTy) {\n   return pointeeType.getIntOrFloatBitWidth();\n }\n \n+bool isTensorPointerType(Type type) {\n+  if (auto ptrType = type.dyn_cast<PointerType>())\n+    return ptrType.getPointeeType().isa<RankedTensorType>();\n+  return false;\n+}\n+\n } // namespace mlir"}, {"filename": "lib/Dialect/Triton/Transforms/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -4,6 +4,7 @@ add_public_tablegen_target(TritonCombineIncGen)\n \n add_mlir_dialect_library(TritonTransforms\n   Combine.cpp\n+  RewriteTensorPointer.cpp\n \n   DEPENDS\n   TritonTransformsIncGen"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.cpp", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -94,7 +94,8 @@ class CombineSelectMaskedLoadPattern : public mlir::RewritePattern {\n       return mlir::failure();\n \n     rewriter.replaceOpWithNewOp<triton::LoadOp>(\n-        op, loadOp.getPtr(), loadOp.getMask(), falseValue, loadOp.getCache(),\n+        op, loadOp.getPtr(), loadOp.getMask(), falseValue,\n+        loadOp.getBoundaryCheck(), loadOp.getPadding(), loadOp.getCache(),\n         loadOp.getEvict(), loadOp.getIsVolatile());\n     return mlir::success();\n   }\n@@ -127,6 +128,7 @@ struct CanonicalizeMaskedLoadPattern\n       // mask = splat(1)\n       rewriter.replaceOpWithNewOp<triton::LoadOp>(\n           loadOp, loadOp.getType(), loadOp.getPtr(), Value(), Value(),\n+          loadOp.getBoundaryCheckAttr(), loadOp.getPaddingAttr(),\n           loadOp.getCache(), loadOp.getEvict(), loadOp.getIsVolatile());\n     } else {\n       // mask = splat(0)"}, {"filename": "lib/Dialect/Triton/Transforms/RewriteTensorPointer.cpp", "status": "added", "additions": 503, "deletions": 0, "changes": 503, "file_content_changes": "@@ -0,0 +1,503 @@\n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/Triton/Transforms/Passes.h\"\n+\n+#include <memory>\n+#include <stack>\n+\n+using namespace mlir;\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/Triton/Transforms/Passes.h.inc\"\n+\n+/// An additional struct to record the meta information of operations\n+/// with tensor pointers\n+struct RewritedInfo {\n+private:\n+  Value base;\n+  SmallVector<Value> shape;\n+  SmallVector<Value> strides;\n+  SmallVector<Value> offsets;\n+  ArrayRef<int64_t> tensorShape;\n+\n+  // A cache to avoid generating the same offset with range\n+  DenseMap<unsigned, Value> cachedOffsetWithRange;\n+\n+public:\n+  RewritedInfo() = default;\n+\n+  RewritedInfo(const RewritedInfo &other) = default;\n+\n+  RewritedInfo(Value base, const SmallVector<Value> &shape,\n+               const SmallVector<Value> &strides,\n+               const SmallVector<Value> &offsets,\n+               const ArrayRef<int64_t> &tensorShape)\n+      : base(base), shape(shape), strides(strides), offsets(offsets),\n+        tensorShape(tensorShape) {\n+    assert(shape.size() == strides.size() && shape.size() == offsets.size() &&\n+           shape.size() == tensorShape.size());\n+  }\n+\n+  unsigned int length() const { return shape.size(); }\n+\n+  Value getOffset(unsigned i) { return offsets[i]; }\n+\n+  SmallVector<Value> getOffsets() { return offsets; }\n+\n+  void setOffset(unsigned i, Value newOffset) {\n+    offsets[i] = newOffset;\n+    cachedOffsetWithRange.clear();\n+  }\n+\n+  void setOffsets(const SmallVector<Value> &newOffsets) {\n+    offsets = newOffsets;\n+    cachedOffsetWithRange.clear();\n+  }\n+\n+  Value getExpandedOffsetWithRange(OpBuilder &builder, const Location &loc,\n+                                   unsigned i) {\n+    if (cachedOffsetWithRange.count(i))\n+      return cachedOffsetWithRange[i];\n+\n+    // Add range\n+    auto indexI32RowType =\n+        RankedTensorType::get({tensorShape[i]}, builder.getI32Type());\n+    auto indexRowType =\n+        RankedTensorType::get({tensorShape[i]}, builder.getI64Type());\n+    Value splatOffset =\n+        builder.create<triton::SplatOp>(loc, indexRowType, offsets[i]);\n+    Value range = builder.create<triton::MakeRangeOp>(loc, indexI32RowType, 0,\n+                                                      tensorShape[i]);\n+    Value i64Range = builder.create<arith::ExtSIOp>(loc, indexRowType, range);\n+\n+    // Expand dimensions\n+    Value expandedResult =\n+        builder.create<arith::AddIOp>(loc, splatOffset, i64Range);\n+    for (int j = 0; j < tensorShape.size(); ++j) {\n+      if (j == i)\n+        continue;\n+      expandedResult =\n+          builder.create<triton::ExpandDimsOp>(loc, expandedResult, j);\n+    }\n+\n+    return cachedOffsetWithRange[i] = expandedResult;\n+  }\n+\n+  Value generatePtr(OpBuilder &builder, const Location &loc) {\n+    assert(tensorShape.size() == offsets.size() &&\n+           tensorShape.size() == strides.size());\n+    auto indexTensorType =\n+        RankedTensorType::get(tensorShape, builder.getI64Type());\n+    auto ptrType = base.getType().cast<triton::PointerType>();\n+    auto ptrTensorType = RankedTensorType::get(tensorShape, ptrType);\n+\n+    // Generate offsets per dimension\n+    Value ptr = builder.create<triton::SplatOp>(loc, ptrTensorType, base);\n+    for (unsigned i = 0; i < tensorShape.size(); ++i) {\n+      auto offsetWithRange = getExpandedOffsetWithRange(builder, loc, i);\n+\n+      // We must splat strides into the expanded shape not a row for retaining\n+      // the divisibility information given by strides\n+      Value splatStride = builder.create<triton::SplatOp>(\n+          loc, offsetWithRange.getType(), strides[i]);\n+      Value offsetWithStride =\n+          builder.create<arith::MulIOp>(loc, offsetWithRange, splatStride);\n+      Value broadcasted = builder.create<triton::BroadcastOp>(\n+          loc, indexTensorType, offsetWithStride);\n+\n+      // Add to the pointer\n+      ptr = builder.create<triton::AddPtrOp>(loc, ptrTensorType, ptr,\n+                                             broadcasted);\n+    }\n+\n+    return ptr;\n+  }\n+\n+  Value generateMask(OpBuilder &builder, const Location &loc,\n+                     const std::optional<ArrayRef<int32_t>> &boundaryCheck) {\n+    if (!boundaryCheck.has_value())\n+      return {};\n+\n+    // Generate mask per dimension\n+    auto maskTensorType =\n+        RankedTensorType::get(tensorShape, builder.getI1Type());\n+    Value mask;\n+    for (auto i : boundaryCheck.value()) {\n+      auto offsetWithRange = getExpandedOffsetWithRange(builder, loc, i);\n+\n+      // Compare with lower bound\n+      Value lowerBound = builder.create<mlir::arith::ConstantIntOp>(\n+          loc, 0, builder.getI64Type());\n+      Value splatLowerBound = builder.create<triton::SplatOp>(\n+          loc, offsetWithRange.getType(), lowerBound);\n+      Value cmpLower = builder.create<arith::CmpIOp>(\n+          loc, arith::CmpIPredicate::sge, offsetWithRange, splatLowerBound);\n+\n+      // Compare with upper bound\n+      Value splatUpperBound = builder.create<triton::SplatOp>(\n+          loc, offsetWithRange.getType(), shape[i]);\n+      Value cmpUpper = builder.create<arith::CmpIOp>(\n+          loc, arith::CmpIPredicate::slt, offsetWithRange, splatUpperBound);\n+\n+      // And and broadcast\n+      Value andResult = builder.create<arith::AndIOp>(loc, cmpLower, cmpUpper);\n+      Value broadcasted =\n+          builder.create<triton::BroadcastOp>(loc, maskTensorType, andResult);\n+\n+      // And up all results\n+      if (!mask) {\n+        mask = broadcasted;\n+      } else {\n+        mask = builder.create<arith::AndIOp>(loc, mask, broadcasted);\n+      }\n+    }\n+\n+    return mask;\n+  }\n+\n+  Value generateOther(OpBuilder &builder, const Location &loc,\n+                      const std::optional<triton::PaddingOption> &padding) {\n+    if (!padding.has_value())\n+      return Value();\n+\n+    // Create element attribute\n+    auto elementType =\n+        base.getType().cast<triton::PointerType>().getPointeeType();\n+    auto otherTensorType = RankedTensorType::get(tensorShape, elementType);\n+\n+    // Set zero padding value\n+    Attribute attr =\n+        elementType.isIntOrIndex()\n+            ? builder.getIntegerAttr(elementType, 0).cast<Attribute>()\n+            : builder.getFloatAttr(elementType, 0).cast<Attribute>();\n+\n+    // Float NaN padding case\n+    if (padding.value() == triton::PaddingOption::PAD_NAN) {\n+      assert(!elementType.isIntOrIndex());\n+      auto apNaN = llvm::APFloat::getNaN(\n+          attr.cast<FloatAttr>().getValue().getSemantics());\n+      attr = builder.getFloatAttr(elementType, apNaN);\n+    }\n+\n+    // Create tensor\n+    Value constant = builder.create<arith::ConstantOp>(loc, attr);\n+    return builder.create<triton::SplatOp>(loc, otherTensorType, constant);\n+  }\n+};\n+\n+class RewriteTensorPointerPass\n+    : public TritonRewriteTensorPointerBase<RewriteTensorPointerPass> {\n+private:\n+  int computeCapability;\n+  DenseMap<Value, RewritedInfo> rewritedInfo;\n+\n+public:\n+  explicit RewriteTensorPointerPass(int computeCapability)\n+      : computeCapability(computeCapability) {}\n+\n+  static bool needRewrite(Operation *op) {\n+    return std::any_of(\n+        op->getOperands().begin(), op->getOperands().end(),\n+        [](Value operand) { return isTensorPointerType(operand.getType()); });\n+  }\n+\n+  static SmallVector<Value>\n+  generateNewOperands(const SmallVector<Value> &oldOperands, unsigned index,\n+                      const SmallVector<Value> &newValues) {\n+    assert(index < oldOperands.size());\n+    SmallVector<Value> newOperands;\n+    for (int i = 0; i < index; ++i)\n+      newOperands.push_back(oldOperands[i]);\n+    for (auto value : newValues)\n+      newOperands.push_back(value);\n+    for (auto i = index + 1; i < oldOperands.size(); ++i)\n+      newOperands.push_back(oldOperands[i]);\n+    return newOperands;\n+  }\n+\n+  Operation *rewriteMakeTensorPtrOp(OpBuilder &builder,\n+                                    triton::MakeTensorPtrOp op,\n+                                    std::stack<Operation *> &eraser) {\n+    // Save info for later use\n+    auto ptrType = op.getResult().getType().cast<triton::PointerType>();\n+    auto tensorType = ptrType.getPointeeType().cast<RankedTensorType>();\n+\n+    // Cast I32 offsets into I64\n+    SmallVector<Value> i64Offsets;\n+    for (auto offset : op.getOffsets()) {\n+      auto i64Offset = builder.create<arith::ExtSIOp>(\n+          op.getLoc(), builder.getI64Type(), offset);\n+      i64Offsets.push_back(i64Offset);\n+    }\n+\n+    // Save information\n+    rewritedInfo[op.getResult()] =\n+        RewritedInfo(op.getBase(), op.getShape(), op.getStrides(), i64Offsets,\n+                     tensorType.getShape());\n+\n+    // Erase the original operation\n+    eraser.push(op);\n+    return nullptr;\n+  }\n+\n+  Operation *rewriteAdvanceOp(OpBuilder &builder, triton::AdvanceOp op,\n+                              std::stack<Operation *> &eraser) {\n+    // Get info from previous results\n+    assert(rewritedInfo.count(op.getPtr()));\n+    auto info = rewritedInfo[op.getPtr()];\n+\n+    // Calculate new offsets\n+    assert(info.length() == op.getOffsets().size());\n+    SmallVector<Value> newOffsets;\n+    for (int i = 0; i < info.length(); ++i) {\n+      Value i64Offset = builder.create<arith::ExtSIOp>(\n+          op.getLoc(), builder.getI64Type(), op.getOffsets()[i]);\n+      Value newOffset = builder.create<arith::AddIOp>(\n+          op.getLoc(), info.getOffset(i), i64Offset);\n+      newOffsets.push_back(newOffset);\n+    }\n+\n+    // Save info for later use\n+    info.setOffsets(newOffsets);\n+    rewritedInfo[op.getResult()] = info;\n+\n+    // Erase the original operation\n+    eraser.push(op);\n+    return nullptr;\n+  }\n+\n+  Operation *rewriteLoadStoreOp(OpBuilder &builder, Operation *op,\n+                                std::stack<Operation *> &eraser) {\n+    assert(isa<triton::LoadOp>(op) || isa<triton::StoreOp>(op));\n+\n+    // We only have to rewrite load/stores with tensor pointers\n+    auto ptr = op->getOperand(0);\n+    if (!isTensorPointerType(ptr.getType()))\n+      return nullptr;\n+\n+    // Get info from previous results\n+    assert(rewritedInfo.count(ptr));\n+    auto info = rewritedInfo[ptr];\n+\n+    // Load/store with tensor pointers implicitly will check the bound while\n+    // accessing memory, so we should set `mask` and `other` (according to the\n+    // padding). Also note that load with tensor pointers do not have `mask` and\n+    // `other` while building IR from Python AST\n+    std::optional<ArrayRef<int>> boundaryCheck;\n+    if (auto loadOp = dyn_cast<triton::LoadOp>(op)) {\n+      assert(!loadOp.getMask() && !loadOp.getOther());\n+      boundaryCheck = loadOp.getBoundaryCheck();\n+    } else if (auto storeOp = dyn_cast<triton::StoreOp>(op)) {\n+      assert(!storeOp.getMask());\n+      boundaryCheck = storeOp.getBoundaryCheck();\n+    }\n+\n+    // Generate new `ptr`, `mask` and `other`\n+    auto newPtr = info.generatePtr(builder, op->getLoc());\n+    auto newMask = info.generateMask(builder, op->getLoc(), boundaryCheck);\n+    Value newOther;\n+    if (auto loadOp = dyn_cast<triton::LoadOp>(op))\n+      newOther = info.generateOther(builder, op->getLoc(), loadOp.getPadding());\n+\n+    // Create a new operation\n+    if (auto loadOp = dyn_cast<triton::LoadOp>(op)) {\n+      auto newResult = builder.create<triton::LoadOp>(\n+          loadOp.getLoc(), newPtr, newMask, newOther, loadOp.getCache(),\n+          loadOp.getEvict(), loadOp.getIsVolatile());\n+      op->getResult(0).replaceAllUsesWith(newResult);\n+    } else if (auto storeOp = dyn_cast<triton::StoreOp>(op)) {\n+      builder.create<triton::StoreOp>(storeOp.getLoc(), newPtr,\n+                                      storeOp.getValue(), newMask,\n+                                      storeOp.getCache(), storeOp.getEvict());\n+    }\n+\n+    // Erase the original operation\n+    eraser.push(op);\n+    return nullptr;\n+  }\n+\n+  Operation *rewriteForOp(OpBuilder &builder, scf::ForOp op,\n+                          std::stack<Operation *> &eraser) {\n+    // Generate new iteration operands and set rewrited information\n+    SmallVector<Value> oldIterOperands = op.getIterOperands();\n+    SmallVector<Value> newIterOperands = op.getIterOperands();\n+    for (unsigned i = 0, oldI = 0, size = op.getNumIterOperands(); i < size;\n+         ++i, ++oldI) {\n+      if (!isTensorPointerType(newIterOperands[i].getType()))\n+        continue;\n+\n+      // Expand the tensor pointer into offsets\n+      assert(rewritedInfo.count(newIterOperands[i]));\n+      auto info = rewritedInfo[newIterOperands[i]];\n+      newIterOperands =\n+          generateNewOperands(newIterOperands, i, info.getOffsets());\n+      i += info.length() - 1;\n+      size += info.length() - 1;\n+    }\n+\n+    // Rebuild the loop type\n+    auto newForOp = builder.create<scf::ForOp>(op.getLoc(), op.getLowerBound(),\n+                                               op.getUpperBound(), op.getStep(),\n+                                               newIterOperands);\n+\n+    // Create value mapping. Note that for tensor pointers, we use identity\n+    // mapping. It may refer to a value in the old loop, but we will rewrite it\n+    // later\n+    IRMapping mapping;\n+    for (unsigned i = 0, oldI = 0; oldI < op.getNumIterOperands();\n+         ++i, ++oldI) {\n+      auto oldRegionIterArg = op.getRegionIterArg(oldI);\n+      if (isTensorPointerType(oldRegionIterArg.getType())) {\n+        // Pass rewrited info inside\n+        assert(rewritedInfo.count(oldIterOperands[oldI]));\n+        auto info = rewritedInfo[oldIterOperands[oldI]];\n+        mapping.map(oldRegionIterArg, oldRegionIterArg);\n+        for (unsigned j = 0; j < info.length(); ++j)\n+          info.setOffset(j, newForOp.getRegionIterArg(i + j));\n+        rewritedInfo[oldRegionIterArg] = info;\n+        i += info.length() - 1;\n+      } else {\n+        mapping.map(oldRegionIterArg, newForOp.getRegionIterArg(i));\n+      }\n+    }\n+    mapping.map(op.getInductionVar(), newForOp.getInductionVar());\n+\n+    // Clone body\n+    builder.setInsertionPointToStart(newForOp.getBody());\n+    for (auto &opInFor : *op.getBody()) {\n+      auto *newOp = builder.clone(opInFor, mapping);\n+      for (unsigned i = 0; i < opInFor.getNumResults(); ++i)\n+        mapping.map(op->getResult(i), newOp->getResult(i));\n+    }\n+\n+    // Replace later usages\n+    assert(op.getNumResults() == op.getNumIterOperands());\n+    for (unsigned i = 0, oldI = 0; oldI < op.getNumResults(); ++i, ++oldI) {\n+      auto oldResult = op.getResult(oldI);\n+      if (isTensorPointerType(oldResult.getType())) {\n+        // Pack new offsets into rewrited info\n+        assert(rewritedInfo.count(oldIterOperands[oldI]));\n+        auto info = rewritedInfo[oldIterOperands[oldI]];\n+        for (unsigned j = 0; j < info.length(); ++j)\n+          info.setOffset(j, newForOp.getResult(i + j));\n+        i += info.length() - 1;\n+        rewritedInfo[oldResult] = info;\n+      } else {\n+        oldResult.replaceAllUsesWith(newForOp.getResult(i));\n+      }\n+    }\n+\n+    // Erase later\n+    eraser.push(op);\n+    return newForOp;\n+  }\n+\n+  Operation *rewriteYieldOp(OpBuilder &builder, scf::YieldOp op,\n+                            std::stack<Operation *> &eraser) {\n+    // Replace tensor pointers with offsets\n+    SmallVector<Value> newOperands = op->getOperands();\n+    for (unsigned i = 0, size = op.getNumOperands(); i < size; ++i) {\n+      if (!isTensorPointerType(newOperands[i].getType()))\n+        continue;\n+\n+      assert(rewritedInfo.count(newOperands[i]));\n+      auto info = rewritedInfo[newOperands[i]];\n+      newOperands = generateNewOperands(newOperands, i, info.getOffsets());\n+      i += info.length() - 1;\n+      size += info.length() - 1;\n+    }\n+    op->setOperands(newOperands);\n+\n+    // No need to erase\n+    return nullptr;\n+  }\n+\n+  Operation *rewriteOp(Operation *op, std::stack<Operation *> &eraser) {\n+    OpBuilder builder(op);\n+\n+    // Rewrite `make_tensor_ptr` and `advance` and make a tensor of pointers\n+    // Rewriting functions return the next operation to visit, if there is no\n+    // next one, simply return `nullptr`\n+    std::pair<Value, RewritedInfo> rewrited;\n+    if (auto makeTensorPtrOp = dyn_cast<triton::MakeTensorPtrOp>(op)) {\n+      return rewriteMakeTensorPtrOp(builder, makeTensorPtrOp, eraser);\n+    } else if (auto advanceOp = dyn_cast<triton::AdvanceOp>(op)) {\n+      return rewriteAdvanceOp(builder, advanceOp, eraser);\n+    } else if (isa<triton::LoadOp>(op) || isa<triton::StoreOp>(op)) {\n+      return rewriteLoadStoreOp(builder, op, eraser);\n+    } else if (auto storeOp = dyn_cast<triton::StoreOp>(op)) {\n+      return rewriteLoadStoreOp(builder, op, eraser);\n+    } else if (op->getDialect()->getNamespace() == \"scf\" ||\n+               op->getDialect()->getNamespace() == \"cf\") {\n+      if (!needRewrite(op))\n+        return op;\n+\n+      if (auto forOp = dyn_cast<scf::ForOp>(op)) {\n+        return rewriteForOp(builder, forOp, eraser);\n+      } else if (auto yieldOp = dyn_cast<scf::YieldOp>(op)) {\n+        return rewriteYieldOp(builder, yieldOp, eraser);\n+      } else {\n+        llvm_unreachable(\"Currently we only support tensor pointer usages \"\n+                         \"inside a `scf::ForOp`, others such as `scf::IfOp`,\"\n+                         \"`scf::WhileOp`, `cf::BranchOp` or `cf::CondBranchOp` \"\n+                         \"are not supported yet\");\n+      }\n+    }\n+\n+    // Otherwise return the original one\n+    return op;\n+  }\n+\n+  void visitOperation(Operation *op, std::stack<Operation *> &eraser) {\n+    for (auto &region : op->getRegions()) {\n+      for (auto &block : region) {\n+        // We need an extra copy because erasing operations may break the\n+        // iterator behavior\n+        SmallVector<Operation *> blockCopy;\n+        for (auto &nestedOp : block)\n+          blockCopy.push_back(&nestedOp);\n+\n+        // Rewrite and recursively visit\n+        for (auto &nestedOp : blockCopy) {\n+          if (auto newOp = rewriteOp(nestedOp, eraser))\n+            visitOperation(newOp, eraser);\n+        }\n+      }\n+    }\n+  }\n+\n+  void runOnOperation() override {\n+    // Only rewrite if the hardware does not support\n+    if (computeCapability >= 90)\n+      return;\n+\n+    // NOTES(Chenggang): we don't use `ConversionPatternRewriter`, because\n+    // MLIR does not support one-multiple value mapping. For example, if we use\n+    // `ConversionPatternRewriter`, we can not make a type converter, which\n+    // converts `ptr<tensor>` into multiple types `ptr<>, int64, int64, ...`\n+    // (containing the base/offsets/strides...). What we can do is to convert\n+    // `ptr<tensor>` into a single type `Tuple<ptr<>, int64, int64, ...>`. But\n+    // in this way, we also have to define `PackTuple` and `UnpackTuple`\n+    // operations and make a canonicalization pass to optimize, which is much\n+    // So here we recursively build the IR, to be specific, we have to rewrite\n+    // `tt.make_tensor_ptr`, `tt.advance`, `tt.load`, `tt.store`,\n+    // `scf.for` (tensor pointer usages may be in a loop fashion)\n+    std::stack<Operation *> eraser;\n+    visitOperation(getOperation(), eraser);\n+\n+    // The operation could not be erased during visit, because they may have\n+    // later usages, so we erase after visit\n+    rewritedInfo.clear();\n+    while (!eraser.empty()) {\n+      auto op = eraser.top();\n+      eraser.pop();\n+      op->erase();\n+    }\n+  }\n+};\n+\n+std::unique_ptr<Pass>\n+triton::createRewriteTensorPointerPass(int computeCapability) {\n+  return std::make_unique<RewriteTensorPointerPass>(computeCapability);\n+}"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -407,8 +407,9 @@ void LoopPipeliner::emitPrologue() {\n           newOp = builder.create<triton::LoadOp>(\n               loadOp.getLoc(), loadOp.getResult().getType(),\n               lookupOrDefault(loadOp.getPtr(), stage), newMask,\n-              lookupOrDefault(loadOp.getOther(), stage), loadOp.getCache(),\n-              loadOp.getEvict(), loadOp.getIsVolatile());\n+              lookupOrDefault(loadOp.getOther(), stage),\n+              loadOp.getBoundaryCheckAttr(), loadOp.getPaddingAttr(),\n+              loadOp.getCache(), loadOp.getEvict(), loadOp.getIsVolatile());\n           addNamedAttrs(newOp, op->getAttrDictionary());\n         } else {\n           newOp = builder.clone(*op);\n@@ -630,8 +631,9 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n         nextOp = builder.create<triton::LoadOp>(\n             loadOp.getLoc(), loadOp.getResult().getType(),\n             nextMapping.lookupOrDefault(loadOp.getPtr()), newMask,\n-            nextMapping.lookupOrDefault(loadOp.getOther()), loadOp.getCache(),\n-            loadOp.getEvict(), loadOp.getIsVolatile());\n+            nextMapping.lookupOrDefault(loadOp.getOther()),\n+            loadOp.getBoundaryCheckAttr(), loadOp.getPaddingAttr(),\n+            loadOp.getCache(), loadOp.getEvict(), loadOp.getIsVolatile());\n         addNamedAttrs(nextOp, op->getAttrDictionary());\n         nextMapping.map(loadOp.getResult(), nextOp->getResult(0));\n       } else {"}, {"filename": "python/setup.py", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "file_content_changes": "@@ -155,8 +155,8 @@ def run(self):\n \n         match = re.search(r\"version\\s*(?P<major>\\d+)\\.(?P<minor>\\d+)([\\d.]+)?\", out.decode())\n         cmake_major, cmake_minor = int(match.group(\"major\")), int(match.group(\"minor\"))\n-        if (cmake_major, cmake_minor) < (3, 20):\n-            raise RuntimeError(\"CMake >= 3.20.0 is required\")\n+        if (cmake_major, cmake_minor) < (3, 18):\n+            raise RuntimeError(\"CMake >= 3.18.0 is required\")\n \n         for ext in self.extensions:\n             self.build_extension(ext)\n@@ -242,7 +242,6 @@ def build_extension(self, ext):\n         \"Intended Audience :: Developers\",\n         \"Topic :: Software Development :: Build Tools\",\n         \"License :: OSI Approved :: MIT License\",\n-        \"Programming Language :: Python :: 3.6\",\n         \"Programming Language :: Python :: 3.7\",\n         \"Programming Language :: Python :: 3.8\",\n         \"Programming Language :: Python :: 3.9\",\n@@ -252,7 +251,7 @@ def build_extension(self, ext):\n     test_suite=\"tests\",\n     extras_require={\n         \"build\": [\n-            \"cmake>=3.20\",\n+            \"cmake>=3.18\",\n             \"lit\",\n         ],\n         \"tests\": ["}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 56, "deletions": 4, "changes": 60, "file_content_changes": "@@ -78,6 +78,11 @@ void init_triton_ir(py::module &&m) {\n   using ret = py::return_value_policy;\n   using namespace pybind11::literals;\n \n+  py::enum_<mlir::triton::PaddingOption>(m, \"PADDING_OPTION\")\n+      .value(\"PAD_ZERO\", mlir::triton::PaddingOption::PAD_ZERO)\n+      .value(\"PAD_NAN\", mlir::triton::PaddingOption::PAD_NAN)\n+      .export_values();\n+\n   py::enum_<mlir::triton::CacheModifier>(m, \"CACHE_MODIFIER\")\n       .value(\"NONE\", mlir::triton::CacheModifier::NONE)\n       .value(\"CA\", mlir::triton::CacheModifier::CA)\n@@ -1124,6 +1129,27 @@ void init_triton_ir(py::module &&m) {\n              self.create<mlir::triton::StoreOp>(loc, ptrs, value, cacheModifier,\n                                                 evictionPolicy);\n            })\n+      .def(\"create_tensor_pointer_load\",\n+           [](mlir::OpBuilder &self, mlir::Value &ptr,\n+              std::vector<int32_t> &boundaryCheck,\n+              std::optional<mlir::triton::PaddingOption> paddingOption,\n+              mlir::triton::CacheModifier cacheModifier,\n+              mlir::triton::EvictionPolicy evictionPolicy,\n+              bool isVolatile) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return self.create<mlir::triton::LoadOp>(\n+                 loc, ptr, boundaryCheck, paddingOption, cacheModifier,\n+                 evictionPolicy, isVolatile);\n+           })\n+      .def(\"create_tensor_pointer_store\",\n+           [](mlir::OpBuilder &self, mlir::Value &ptr, mlir::Value &val,\n+              std::vector<int32_t> &boundaryCheck,\n+              mlir::triton::CacheModifier cacheModifier,\n+              mlir::triton::EvictionPolicy evictionPolicy) -> void {\n+             auto loc = self.getUnknownLoc();\n+             self.create<mlir::triton::StoreOp>(loc, ptr, val, boundaryCheck,\n+                                                cacheModifier, evictionPolicy);\n+           })\n       .def(\"create_masked_load\",\n            [](mlir::OpBuilder &self, mlir::Value &ptrs, mlir::Value &mask,\n               std::optional<mlir::Value> &other,\n@@ -1390,10 +1416,31 @@ void init_triton_ir(py::module &&m) {\n              return self.create<::mlir::LLVM::UndefOp>(loc, type);\n            })\n       // Force GPU barrier\n-      .def(\"create_barrier\", [](mlir::OpBuilder &self) {\n-        auto loc = self.getUnknownLoc();\n-        self.create<mlir::gpu::BarrierOp>(loc);\n-      });\n+      .def(\"create_barrier\",\n+           [](mlir::OpBuilder &self) {\n+             auto loc = self.getUnknownLoc();\n+             self.create<mlir::gpu::BarrierOp>(loc);\n+           })\n+      // Make a block pointer (tensor pointer in Triton IR)\n+      .def(\"create_make_block_ptr\",\n+           [](mlir::OpBuilder &self, mlir::Value &base,\n+              std::vector<mlir::Value> &shape,\n+              std::vector<mlir::Value> &strides,\n+              std::vector<mlir::Value> &offsets,\n+              std::vector<int32_t> &tensorShape,\n+              std::vector<int32_t> &order) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return self.create<mlir::triton::MakeTensorPtrOp>(\n+                 loc, base, shape, strides, offsets, tensorShape, order);\n+           })\n+      // Advance a block pointer\n+      .def(\"create_advance\",\n+           [](mlir::OpBuilder &self, mlir::Value &ptr,\n+              std::vector<mlir::Value> &offsets) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return self.create<mlir::triton::AdvanceOp>(loc, ptr.getType(),\n+                                                         ptr, offsets);\n+           });\n \n   py::class_<mlir::PassManager>(m, \"pass_manager\")\n       .def(py::init<mlir::MLIRContext *>())\n@@ -1448,6 +1495,11 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::PassManager &self) {\n              self.addPass(mlir::triton::createCombineOpsPass());\n            })\n+      .def(\"add_rewrite_tensor_pointer_pass\",\n+           [](mlir::PassManager &self, int computeCapability) {\n+             self.addPass(mlir::triton::createRewriteTensorPointerPass(\n+                 computeCapability));\n+           })\n       .def(\"add_convert_triton_to_tritongpu_pass\",\n            [](mlir::PassManager &self, int numWarps) {\n              self.addPass("}, {"filename": "python/test/unit/language/test_block_pointer.py", "status": "added", "additions": 102, "deletions": 0, "changes": 102, "file_content_changes": "@@ -0,0 +1,102 @@\n+import pytest\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def block_copy_kernel(a_ptr, b_ptr, N, BLOCK_SIZE: tl.constexpr, padding_option: tl.constexpr):\n+    pid = tl.program_id(0)\n+    # We only copy half of the data to see if the padding works\n+    a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(N // 2, ), strides=(1, ), offsets=(pid * BLOCK_SIZE, ),\n+                                    block_shape=(BLOCK_SIZE, ), order=(0, ))\n+    b_block_ptr = tl.make_block_ptr(base=b_ptr, shape=(N, ), strides=(1, ), offsets=(pid * BLOCK_SIZE, ),\n+                                    block_shape=(BLOCK_SIZE, ), order=(0, ))\n+    a = tl.load(a_block_ptr, boundary_check=(0, ), padding_option=padding_option)\n+    tl.store(b_block_ptr, a, boundary_check=(0, ))\n+\n+\n+@pytest.mark.parametrize(\"dtype_str, n, padding_option\",\n+                         [(dtype_str, n, padding) for dtype_str in (\"bool\", \"int16\", \"float16\")\n+                          for n in (64, 128, 256, 512, 1024)\n+                          for padding in (\"zero\", \"nan\")])\n+def test_block_copy(dtype_str, n, padding_option):\n+    capability = torch.cuda.get_device_capability()\n+    if capability[0] >= 9:\n+        pytest.skip(\"Hopper support is working in progress\")\n+\n+    dtype = getattr(torch, dtype_str)\n+    if dtype_str in (\"bool\", \"int16\"):\n+        if padding_option == \"nan\":\n+            pytest.skip(\"Padding with NaN is not supported for integer types\")\n+        a = torch.randint(0, 2, (n, ), device=\"cuda\", dtype=dtype)\n+    else:\n+        a = torch.randn((n, ), device=\"cuda\", dtype=dtype)\n+    b = torch.zeros((n, ), device=\"cuda\", dtype=dtype)\n+\n+    grid = lambda meta: (triton.cdiv(n, meta[\"BLOCK_SIZE\"]),)\n+    block_copy_kernel[grid](a_ptr=a, b_ptr=b, N=n, BLOCK_SIZE=64, padding_option=padding_option)\n+\n+    assert torch.all(a[0: n // 2] == b[0: n // 2])\n+    if padding_option == \"zero\":\n+        assert torch.all(b[n // 2: n] == 0)\n+    else:\n+        assert torch.all(torch.isnan(b[n // 2: n]))\n+\n+\n+@triton.jit\n+def matmul_no_scf_with_advance_kernel(\n+        a_ptr, b_ptr, c_ptr,\n+        M, N, K,\n+        stride_am, stride_ak,\n+        stride_bk, stride_bn,\n+        stride_cm, stride_cn,\n+        BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n+):\n+    offs_m = tl.arange(0, BLOCK_M)\n+    offs_n = tl.arange(0, BLOCK_N)\n+    a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),\n+                                    offsets=(0, 0), block_shape=(BLOCK_M, BLOCK_K), order=(1, 0))\n+    b_block_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(stride_bk, stride_bn),\n+                                    offsets=(0, 0), block_shape=(BLOCK_K, BLOCK_N), order=(1, 0))\n+    # Below two lines are just for testing negative offsets for the `advance` API, which could be removed\n+    a_block_ptr = tl.advance(a_block_ptr, (BLOCK_M, -BLOCK_K))\n+    a_block_ptr = tl.advance(a_block_ptr, (-BLOCK_M, BLOCK_K))\n+    a = tl.load(a_block_ptr, boundary_check=(1, ), padding_option=\"zero\")\n+    b = tl.load(b_block_ptr, boundary_check=(0, ), padding_option=\"zero\")\n+\n+    c = tl.dot(a, b)\n+    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n+    tl.store(c_ptrs, c)\n+\n+\n+@pytest.mark.parametrize(\"shape, num_warps\", [\n+    (shape, num_warps)\n+    for shape in [\n+        [64, 64, 16],\n+        [64, 64, 32],\n+        [64, 64, 64],\n+    ]\n+    for num_warps in [4, 8]\n+])\n+def test_block_ptr_matmul_no_scf(shape, num_warps):\n+    capability = torch.cuda.get_device_capability()\n+    if capability[0] >= 9:\n+        pytest.skip(\"Hopper support is working in progress\")\n+\n+    m, n, k = shape\n+    a = torch.randn((m, k), device=\"cuda\", dtype=torch.float16)\n+    b = torch.randn((k, n), device=\"cuda\", dtype=torch.float16)\n+    c = torch.empty((m, n), device=\"cuda\", dtype=torch.float32)\n+\n+    grid = lambda META: (1, )\n+    matmul_no_scf_with_advance_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n+                                            M=m, N=n, K=k,\n+                                            stride_am=a.stride(0), stride_ak=a.stride(1),\n+                                            stride_bk=b.stride(0), stride_bn=b.stride(1),\n+                                            stride_cm=c.stride(0), stride_cn=c.stride(1),\n+                                            BLOCK_M=m, BLOCK_N=n, BLOCK_K=k,\n+                                            num_warps=num_warps)\n+    golden = torch.matmul(a, b)\n+    torch.testing.assert_allclose(c, golden)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 23, "deletions": 5, "changes": 28, "file_content_changes": "@@ -1087,10 +1087,17 @@ def build_triton_ir(fn, signature, specialization, constants, debug=False):\n     return ret, generator\n \n \n-def optimize_triton_ir(mod):\n+def inline_triton_ir(mod):\n     pm = _triton.ir.pass_manager(mod.context)\n     pm.enable_debug()\n     pm.add_inliner_pass()\n+    pm.run(mod)\n+    return mod\n+\n+\n+def optimize_triton_ir(mod):\n+    pm = _triton.ir.pass_manager(mod.context)\n+    pm.enable_debug()\n     pm.add_triton_combine_pass()\n     pm.add_canonicalizer_pass()\n     pm.add_cse_pass()\n@@ -1100,13 +1107,25 @@ def optimize_triton_ir(mod):\n     return mod\n \n \n-def ast_to_ttir(fn, signature, specialization, constants, debug=False):\n+def ttir_compute_capability_rewrite(mod, compute_capability):\n+    # For hardware without support, we must rewrite all load/store with block (tensor) pointers into legacy load/store\n+    pm = _triton.ir.pass_manager(mod.context)\n+    pm.enable_debug()\n+    pm.add_rewrite_tensor_pointer_pass(compute_capability)\n+    pm.run(mod)\n+    return mod\n+\n+\n+def ast_to_ttir(fn, signature, specialization, constants, compute_capability, debug=False):\n     mod, _ = build_triton_ir(fn, signature, specialization, constants, debug)\n+    mod = inline_triton_ir(mod)\n+    mod = ttir_compute_capability_rewrite(mod, compute_capability)\n     return optimize_triton_ir(mod)\n \n \n def ttir_to_ttgir(mod, num_warps):\n     pm = _triton.ir.pass_manager(mod.context)\n+    pm.enable_debug()\n     pm.add_convert_triton_to_tritongpu_pass(num_warps)\n     pm.run(mod)\n     return mod\n@@ -1902,7 +1921,7 @@ def compile(fn, **kwargs):\n         stages = {\n             \"ast\": (lambda path: fn, None),\n             \"ttir\": (lambda path: parse_mlir_module(path, context),\n-                     lambda src: ast_to_ttir(src, signature, configs[0], constants)),\n+                     lambda src: ast_to_ttir(src, signature, configs[0], constants, capability)),\n             \"ttgir\": (lambda path: parse_mlir_module(path, context),\n                       lambda src: optimize_ttgir(ttir_to_ttgir(src, num_warps), num_stages, capability)),\n             \"llir\": (lambda path: Path(path).read_text(),\n@@ -1916,7 +1935,7 @@ def compile(fn, **kwargs):\n         stages = {\n             \"ast\": (lambda path: fn, None),\n             \"ttir\": (lambda path: parse_mlir_module(path, context),\n-                     lambda src: ast_to_ttir(src, signature, configs[0], constants, debug)),\n+                     lambda src: ast_to_ttir(src, signature, configs[0], constants, capability, debug)),\n             \"ttgir\": (lambda path: parse_mlir_module(path, context),\n                       lambda src: optimize_ttgir(ttir_to_ttgir(src, num_warps), num_stages, capability)),\n             \"llir\": (lambda path: Path(path).read_text(),\n@@ -1926,7 +1945,6 @@ def compile(fn, **kwargs):\n             \"cubin\": (lambda path: Path(path).read_bytes(),\n                       lambda src: ptx_to_cubin(src, capability))\n         }\n-\n     # find out the signature of the function\n     if isinstance(fn, triton.runtime.JITFunction):\n         configs = kwargs.get(\"configs\", None)"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -8,6 +8,7 @@\n from . import math\n from .core import (\n     abs,\n+    advance,\n     arange,\n     argmin,\n     argmax,\n@@ -48,6 +49,7 @@\n     int8,\n     load,\n     log,\n+    make_block_ptr,\n     max,\n     max_contiguous,\n     maximum,\n@@ -101,6 +103,7 @@\n \n __all__ = [\n     \"abs\",\n+    \"advance\",\n     \"arange\",\n     \"argmin\",\n     \"argmax\",\n@@ -144,6 +147,7 @@\n     \"math\",\n     \"load\",\n     \"log\",\n+    \"make_block_ptr\",\n     \"max\",\n     \"max_contiguous\",\n     \"maximum\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 85, "deletions": 26, "changes": 111, "file_content_changes": "@@ -884,55 +884,87 @@ def dot(input, other, allow_tf32=True, out_dtype=float32, _builder=None):\n \n \n @builtin\n-def load(pointer, mask=None, other=None, cache_modifier=\"\", eviction_policy=\"\", volatile=False, _builder=None):\n+def load(pointer, mask=None, other=None, boundary_check=tuple(), padding_option=\"\", cache_modifier=\"\",\n+         eviction_policy=\"\", volatile=False, _builder=None):\n     \"\"\"\n-    Return a tensor of data whose values are, elementwise, loaded from memory at location defined by :code:`pointer`.\n-\n-    :code:`mask` and :code:`other` are implicitly broadcast to :code:`pointer.shape`.\n-\n-    :code:`other` is implicitly typecast to :code:`pointer.dtype.element_ty`.\n-\n-    :param pointer: Pointers to the data to be loaded.\n-    :type pointer: Block of dtype=triton.PointerDType\n-    :param mask: if mask[idx] is false, do not load the data at address :code:`pointer[idx]`.\n-    :type mask: Block of triton.int1, optional\n-    :param other: if mask[idx] is false, return other[idx]\n+    Return a tensor of data whose values are loaded from memory at location defined by `pointer`:\n+        (1) `pointer` could be a single element pointer, then a scalar will be loaded\n+            - `mask` and `other` must be scalar too\n+            - `other` is implicitly typecast to `pointer.dtype.element_ty`\n+            - `boundary_check` and `padding_option` must be empty\n+        (2) `pointer` could be element-wise tensor of pointers, in which case:\n+            - `mask` and `other` are implicitly broadcast to `pointer.shape`\n+            - `other` is implicitly typecast to `pointer.dtype.element_ty`\n+            - `boundary_check` and `padding_option` must be empty\n+        (3) `pointer` could be a block pointer defined by `make_block_ptr`, in which case:\n+            - `mask` and `other` must be None\n+            - `boundary_check` and `padding_option` can be specified to control the behavior of out-of-bound access\n+\n+    :param pointer: Pointer to the data to be loaded\n+    :type pointer: `triton.PointerType`, or block of `dtype=triton.PointerType`\n+    :param mask: if `mask[idx]` is false, do not load the data at address `pointer[idx]`\n+        (must be `None` with block pointers)\n+    :type mask: Block of `triton.int1`, optional\n+    :param other: if `mask[idx]` is false, return `other[idx]`\n     :type other: Block, optional\n-    :param cache_modifier: changes cache option in nvidia ptx\n+    :param boundary_check: tuple of integers, indicating the dimensions which should do the boundary check\n+    :type boundary_check: tuple of ints, optional\n+    :param padding_option: should be one of {\"\", \"zero\", \"nan\"}, do padding while out of bound\n+    :param cache_modifier: changes cache option in NVIDIA PTX\n     :type cache_modifier: str, optional\n+    :param eviction_policy: changes eviction policy in NVIDIA PTX\n+    :type eviction_policy: str, optional\n+    :param volatile: changes volatile option in NVIDIA PTX\n+    :type volatile: bool, optional\n     \"\"\"\n-    # mask, other can be constexpr\n+    # `mask` and `other` can be constexpr\n     if _constexpr_to_value(mask) is not None:\n         mask = _to_tensor(mask, _builder)\n     if _constexpr_to_value(other) is not None:\n         other = _to_tensor(other, _builder)\n+    padding_option = _constexpr_to_value(padding_option)\n     cache_modifier = _constexpr_to_value(cache_modifier)\n     eviction_policy = _constexpr_to_value(eviction_policy)\n     volatile = _constexpr_to_value(volatile)\n-    return semantic.load(pointer, mask, other, cache_modifier, eviction_policy, volatile, _builder)\n+    return semantic.load(pointer, mask, other, boundary_check, padding_option, cache_modifier, eviction_policy,\n+                         volatile, _builder)\n \n \n @builtin\n-def store(pointer, value, mask=None, cache_modifier=\"\", eviction_policy=\"\", _builder=None):\n+def store(pointer, value, mask=None, boundary_check=(), cache_modifier=\"\", eviction_policy=\"\", _builder=None):\n     \"\"\"\n-    Stores :code:`value` tensor of elements in memory, element-wise, at the memory locations specified by :code:`pointer`.\n-\n-    :code:`value` is implicitly broadcast to :code:`pointer.shape` and typecast to :code:`pointer.dtype.element_ty`.\n-\n-    :param pointer: The memory locations where the elements of :code:`value` are stored.\n-    :type pointer: Block of dtype=triton.PointerDType\n-    :param value: The tensor of elements to be stored.\n+    Store a tensor of data into memory locations defined by `pointer`:\n+        (1) `pointer` could be a single element pointer, then a scalar will be stored\n+            - `mask` must be scalar too\n+            - `boundary_check` and `padding_option` must be empty\n+        (2) `pointer` could be element-wise tensor of pointers, in which case:\n+            - `mask` is implicitly broadcast to `pointer.shape`\n+            - `boundary_check` must be empty\n+        (3) or `pointer` could be a block pointer defined by `make_block_ptr`, in which case:\n+            - `mask` must be None\n+            - `boundary_check` can be specified to control the behavior of out-of-bound access\n+    `value` is implicitly broadcast to `pointer.shape` and typecast to `pointer.dtype.element_ty`.\n+\n+    :param pointer: The memory location where the elements of `value` are stored\n+    :type pointer: `triton.PointerType`, or block of `dtype=triton.PointerType`\n+    :param value: The tensor of elements to be stored\n     :type value: Block\n-    :param mask: If mask[idx] is false, do not store :code:`value[idx]` at :code:`pointer[idx]`.\n+    :param mask: If `mask[idx]` is false, do not store `value[idx]` at `pointer[idx]`\n     :type mask: Block of triton.int1, optional\n+    :param boundary_check: tuple of integers, indicating the dimensions which should do the boundary check\n+    :type boundary_check: tuple of ints, optional\n+    :param cache_modifier: changes cache option in NVIDIA PTX\n+    :type cache_modifier: str, optional\n+    :param eviction_policy: changes eviction policy in NVIDIA PTX\n+    :type eviction_policy: str, optional\n     \"\"\"\n-    # value can be constexpr\n+    # `value` can be constexpr\n     value = _to_tensor(value, _builder)\n     if _constexpr_to_value(mask) is not None:\n         mask = _to_tensor(mask, _builder)\n     cache_modifier = _constexpr_to_value(cache_modifier)\n     eviction_policy = _constexpr_to_value(eviction_policy)\n-    return semantic.store(pointer, value, mask, cache_modifier, eviction_policy, _builder)\n+    return semantic.store(pointer, value, mask, boundary_check, cache_modifier, eviction_policy, _builder)\n \n \n # -----------------------\n@@ -1387,6 +1419,33 @@ def device_assert(cond, msg=\"\", _builder=None):\n     lineno = frame.f_back.f_lineno\n     return semantic.device_assert(_to_tensor(cond, _builder), msg, file_name, func_name, lineno, _builder)\n \n+\n+@builtin\n+def make_block_ptr(base: tensor, shape, strides, offsets, block_shape, order, _builder=None):\n+    \"\"\"\n+    Returns a pointer to a block in a parent tensor\n+\n+    :param base: The base pointer to the parent tensor\n+    :param shape: The shape of the parent tensor\n+    :param strides: The strides of the parent tensor\n+    :param offsets: The offsets to the block\n+    :param block_shape: The shape of the block\n+    :param order: The order of the original data format\n+    \"\"\"\n+    return semantic.make_block_ptr(base, shape, strides, offsets, block_shape, order, _builder)\n+\n+\n+@builtin\n+def advance(base: tensor, offsets, _builder=None):\n+    \"\"\"\n+    Advance a block pointer\n+\n+    :param base: the block pointer to advance\n+    :param offsets: the offsets to advance, a tuple by dimension\n+    \"\"\"\n+    return semantic.advance(base, offsets, _builder)\n+\n+\n # -----------------------\n # Iterators\n # -----------------------"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 228, "deletions": 39, "changes": 267, "file_content_changes": "@@ -771,7 +771,7 @@ def cast(input: tl.tensor,\n # ===----------------------------------------------------------------------===//\n \n \n-def str_to_cache_modifier(cache_modifier):\n+def _str_to_cache_modifier(cache_modifier):\n     cache = ir.CACHE_MODIFIER.NONE  # default\n     if cache_modifier:\n         if cache_modifier == \".ca\":\n@@ -783,7 +783,7 @@ def str_to_cache_modifier(cache_modifier):\n     return cache\n \n \n-def str_to_eviction_policy(eviction_policy):\n+def _str_to_eviction_policy(eviction_policy):\n     eviction = ir.EVICTION_POLICY.NORMAL  # default\n     if eviction_policy:\n         if eviction_policy == \"evict_last\":\n@@ -795,97 +795,219 @@ def str_to_eviction_policy(eviction_policy):\n     return eviction\n \n \n-def load(ptr: tl.tensor,\n-         mask: Optional[tl.tensor],\n-         other: Optional[tl.tensor],\n-         cache_modifier: str,\n-         eviction_policy: str,\n-         is_volatile: bool,\n-         builder: ir.builder) -> tl.tensor:\n+def _str_to_padding_option(padding_option):\n+    padding = None  # default\n+    if padding_option:\n+        if padding_option == \"zero\":\n+            padding = ir.PADDING_OPTION.PAD_ZERO\n+        elif padding_option == \"nan\":\n+            padding = ir.PADDING_OPTION.PAD_NAN\n+        else:\n+            raise ValueError(f\"Padding option {padding_option} not supported\")\n+    return padding\n+\n+\n+def _canonicalize_boundary_check(boundary_check, block_shape):\n+    if boundary_check:\n+        if not hasattr(boundary_check, \"__iter__\"):\n+            boundary_check = [boundary_check]\n+        boundary_check = [elem.value if isinstance(elem, tl.constexpr) else elem for elem in boundary_check]\n+        for dim in boundary_check:\n+            assert isinstance(dim, int) and 0 <= dim < len(block_shape)\n+        assert len(boundary_check) > 0\n+        assert len(boundary_check) == len(set(boundary_check)), \"Duplicate dimension in `boundary_check`\"\n+        return sorted(boundary_check)\n+    return tuple()\n+\n+\n+def _load_block_pointer(ptr, mask, other, boundary_check, padding, cache, eviction, is_volatile, builder):\n+    # Load by a block pointer: `pointer_type<block_type<>>`\n+    # Block pointer can not have `mask` and `other` arguments\n+    if mask or other:\n+        raise ValueError(\"`mask` and `other` arguments cannot be specified for loading block pointers\")\n+\n+    elt_ty = ptr.type.element_ty.element_ty\n+    assert elt_ty != tl.int1, \"`tl.int1` should be rewrited in `tl.make_block_ptr`\"\n+    if elt_ty.is_int() and padding == ir.PADDING_OPTION.PAD_NAN:\n+        raise ValueError(\"Padding option `nan` is not supported for integer block pointers\")\n+\n+    # `dst_ty` is de-referenced type of the pointer type\n+    dst_ty = ptr.type.element_ty\n+\n+    # Check `boundary_check` argument\n+    boundary_check = _canonicalize_boundary_check(boundary_check, dst_ty.get_block_shapes())\n+\n+    # Build IR\n+    return tl.tensor(builder.create_tensor_pointer_load(ptr.handle, boundary_check, padding, cache, eviction,\n+                                                        is_volatile), dst_ty)\n+\n+\n+def _load_legacy(ptr, mask, other, boundary_check, padding, cache, eviction, is_volatile, builder):\n+    # Load by a tensor of pointers or a pointer of scalar: `block_type<pointer_type<>>` or `pointer_type<>`\n     if not ptr.type.scalar.is_ptr():\n-        raise ValueError(\"Pointer argument of load instruction is \" + ptr.type.__repr__())\n+        raise ValueError(f\"Unsupported ptr type {ptr.type.__repr__()} in `tl.load`\")\n+\n+    # Check `mask`, `other`, `boundary_check`, and `padding` arguments\n+    if not mask and other:\n+        raise ValueError(\"`other` cannot be provided without `mask`\")\n+    if padding or boundary_check:\n+        raise ValueError(\"`padding_option` or `boundary_check` argument is not supported for loading a tensor of\"\n+                         \"pointers or loading a scalar. Because the compiler does not know the boundary; please \"\n+                         \"use block pointers (defined by `make_block_ptr`) instead\")\n+\n+    # For a pointer of scalar, check the type of `mask` and `other`\n     if not ptr.type.is_block():\n         if mask and mask.type.is_block():\n             raise ValueError(\"Mask argument cannot be block type if pointer argument is not a block\")\n         if other and other.type.is_block():\n             raise ValueError(\"Other argument cannot be block type if pointer argument is not a block\")\n+\n+    # Make `mask` and `other` into the same shape as `ptr`\n     if ptr.type.is_block():\n         if mask:\n             mask = broadcast_impl_shape(mask, ptr.type.get_block_shapes(), builder)\n         if other:\n             other = broadcast_impl_shape(other, ptr.type.get_block_shapes(), builder)\n \n+    # Get `pointer_type<elt_ty>` and `elt_ty`\n     ptr_ty = ptr.type.scalar\n     elt_ty = ptr_ty.element_ty\n \n-    # treat bool* as tl.int8*\n+    # Treat `pointer_type<tl.int1>` as `pointer_type<tl.int8>`\n     if elt_ty == tl.int1:\n         elt_ty = tl.int8\n         ptr_ty = tl.pointer_type(elt_ty, ptr_ty.address_space)\n         ptr = cast(ptr, ptr_ty, builder)\n \n+    # Cast `other` into `ele_ty` type\n     if other:\n         other = cast(other, elt_ty, builder)\n \n-    # cache modifier\n-\n+    # Create loaded result type `dst_ty`\n     if ptr.type.is_block():\n         shape = ptr.type.get_block_shapes()\n         dst_ty = tl.block_type(elt_ty, shape)\n     else:\n+        # Load by de-referencing the pointer of scalar\n         dst_ty = elt_ty\n \n-    cache = str_to_cache_modifier(cache_modifier)\n-    eviction = str_to_eviction_policy(eviction_policy)\n-\n+    # Build IR\n     if not mask:\n-        if other:\n-            raise ValueError(\"`other` cannot be provided without `mask`\")\n-        return tl.tensor(builder.create_load(ptr.handle, cache, eviction, is_volatile),\n-                         dst_ty)\n+        return tl.tensor(builder.create_load(ptr.handle, cache, eviction, is_volatile), dst_ty)\n     else:\n-        return tl.tensor(builder.create_masked_load(ptr.handle,\n-                                                    mask.handle,\n-                                                    other.handle if other else None,\n-                                                    cache, eviction, is_volatile),\n-                         dst_ty)\n+        return tl.tensor(builder.create_masked_load(ptr.handle, mask.handle, other.handle if other else None, cache,\n+                                                    eviction, is_volatile), dst_ty)\n \n \n-def store(ptr: tl.tensor,\n-          val: tl.tensor,\n-          mask: Optional[tl.tensor],\n-          cache_modifier: str,\n-          eviction_policy: str,\n-          builder: ir.builder) -> tl.tensor:\n+def load(ptr: tl.tensor,\n+         mask: Optional[tl.tensor],\n+         other: Optional[tl.tensor],\n+         boundary_check,\n+         padding_option: str,\n+         cache_modifier: str,\n+         eviction_policy: str,\n+         is_volatile: bool,\n+         builder: ir.builder) -> tl.tensor:\n+    # Cache, eviction and padding options\n+    cache = _str_to_cache_modifier(cache_modifier)\n+    eviction = _str_to_eviction_policy(eviction_policy)\n+    padding = _str_to_padding_option(padding_option)\n+\n+    if ptr.type.is_ptr() and ptr.type.element_ty.is_block():\n+        # Load by a block pointer: `pointer_type<block_type<>>`\n+        return _load_block_pointer(ptr, mask, other, boundary_check, padding, cache, eviction, is_volatile, builder)\n+    else:\n+        # Load by a tensor of pointers or a pointer of scalar: `block_type<pointer_type<>>` or `pointer_type<>`\n+        return _load_legacy(ptr, mask, other, boundary_check, padding, cache, eviction, is_volatile, builder)\n+\n+\n+def _store_block_pointer(ptr, val, mask, boundary_check, cache, eviction, builder):\n+    # Store by a block pointer: `pointer_type<block_type<>>`\n+    # Block pointers can not have the `mask` argument\n+    if mask:\n+        raise ValueError(\"`mask` and `other` arguments cannot be specified for loading block pointers\")\n+\n+    # Check same shape\n+    block_shape = ptr.type.element_ty.get_block_shapes()\n+    if not val.type.is_block():\n+        val = broadcast_impl_shape(val, block_shape, builder)\n+    assert val.type.is_block(), \"Value argument must be block type or a scalar\"\n+    assert block_shape == val.type.get_block_shapes(), \"Block shape and value shape mismatch\"\n+\n+    elt_ty = ptr.type.element_ty.element_ty\n+    assert elt_ty != tl.int1, \"`tl.int1` should be rewrited in `tl.make_block_ptr`\"\n+\n+    # Check `boundary_check` argument\n+    boundary_check = _canonicalize_boundary_check(boundary_check, block_shape)\n+\n+    # Build IR\n+    return tl.tensor(builder.create_tensor_pointer_store(ptr.handle, val.handle, boundary_check, cache, eviction),\n+                     tl.void)\n+\n+\n+def _store_legacy(ptr, val, mask, boundary_check, cache, eviction, builder):\n+    # Store by a tensor of pointers or a pointer of scalar: `block_type<pointer_type<>>` or `pointer_type<>`\n     if not ptr.type.scalar.is_ptr():\n-        raise ValueError(\"Pointer argument of store instruction is \" + ptr.type.__repr__())\n+        raise ValueError(f\"Unsupported ptr type {ptr.type.__repr__()} in `tl.store`\")\n+\n+    # Check `boundary_check` argument\n+    if boundary_check:\n+        raise ValueError(\"`boundary_check` argument is not supported for storing a tensor of pointers or storing a \"\n+                         \"scalar. Because the compiler does not know the boundary; please use block pointers \"\n+                         \"(defined by `make_block_ptr`) instead\")\n+\n+    # For a pointer of scalar, check the type of `val` and `mask`\n     if not ptr.type.is_block():\n         if val.type.is_block():\n             raise ValueError(\"Value argument cannot be block type if pointer argument is not a block\")\n         if mask and mask.type.is_block():\n             raise ValueError(\"Mask argument cannot be block type if pointer argument is not a block\")\n+\n+    # Make `mask` and `val` into the same shape as `ptr`\n     if ptr.type.is_block():\n         val = broadcast_impl_shape(val, ptr.type.get_block_shapes(), builder)\n-    if mask and ptr.type.is_block():\n-        mask = broadcast_impl_shape(mask, ptr.type.get_block_shapes(), builder)\n+        if mask:\n+            mask = broadcast_impl_shape(mask, ptr.type.get_block_shapes(), builder)\n+\n     ptr_ty = ptr.type.scalar\n     elt_ty = ptr_ty.element_ty\n-    # treat bool* as tl.int8*\n+\n+    # Treat `pointer_type<tl.int1>` as `pointer_type<tl.int8>`\n     if elt_ty == tl.int1:\n         elt_ty = tl.int8\n         ptr_ty = tl.pointer_type(elt_ty, ptr_ty.address_space)\n         ptr = cast(ptr, ptr_ty, builder)\n-    # attributes\n-    cache = str_to_cache_modifier(cache_modifier)\n-    eviction = str_to_eviction_policy(eviction_policy)\n-    # cast to target data-type\n+\n+    # Cast to target data type\n     val = cast(val, elt_ty, builder)\n+\n+    # Build IR\n     if not mask:\n         return tl.tensor(builder.create_store(ptr.handle, val.handle, cache, eviction), tl.void)\n     if not mask.type.scalar.is_bool():\n         raise ValueError(\"Mask must have boolean scalar type\")\n     return tl.tensor(builder.create_masked_store(ptr.handle, val.handle, mask.handle, cache, eviction), tl.void)\n \n+\n+def store(ptr: tl.tensor,\n+          val: tl.tensor,\n+          mask: Optional[tl.tensor],\n+          boundary_check,\n+          cache_modifier: str,\n+          eviction_policy: str,\n+          builder: ir.builder) -> tl.tensor:\n+    # Cache and eviction options\n+    cache = _str_to_cache_modifier(cache_modifier)\n+    eviction = _str_to_eviction_policy(eviction_policy)\n+\n+    if ptr.type.is_ptr() and ptr.type.element_ty.is_block():\n+        # Store by a block pointer: `pointer_type<block_type<>>`\n+        return _store_block_pointer(ptr, val, mask, boundary_check, cache, eviction, builder)\n+    else:\n+        # Store by a tensor of pointers or a pointer of scalar: `block_type<pointer_type<>>` or `pointer_type<>`\n+        return _store_legacy(ptr, val, mask, boundary_check, cache, eviction, builder)\n+\n+\n #########\n # atomic\n #########\n@@ -1265,3 +1387,70 @@ def device_print(prefix: str, args: List[tl.tensor], builder: ir.builder) -> tl.\n \n def device_assert(cond: tl.tensor, msg: str, file_name: str, func_name, lineno: int, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_assert(cond.handle, msg, file_name, func_name, lineno), tl.void)\n+\n+\n+def _convert_elem_to_ir_value(builder, elem, require_i64):\n+    if isinstance(elem, tl.constexpr):\n+        return builder.get_int64(elem.value) if require_i64 else builder.get_int32(elem.value)\n+    elif isinstance(elem, tl.tensor):\n+        assert elem.numel.value == 1, \"Expected a scalar in shape/strides/offsets\"\n+        assert elem.dtype.is_int(), \"Expected an integer scalar type in shape/strides/offsets\"\n+        if elem.dtype != tl.int64 and require_i64:\n+            return builder.create_int_cast(elem.handle, builder.get_int64_ty(), elem.dtype.is_int_signed())\n+        elif elem.dtype != tl.int32:\n+            return builder.create_int_cast(elem.handle, builder.get_int32_ty(), elem.dtype.is_int_signed())\n+        return elem.handle\n+    assert False, f\"Unsupported element type in shape/strides/offsets: {type(elem)}\"\n+\n+\n+def _convert_to_ir_values(builder, list_like, require_i64=True):\n+    if hasattr(list_like, \"__iter__\"):\n+        return [_convert_elem_to_ir_value(builder, elem, require_i64) for elem in list_like]\n+    return [_convert_elem_to_ir_value(builder, list_like, require_i64)]\n+\n+\n+def make_block_ptr(base: tl.tensor, shape, strides, offsets, block_shape, order, builder: ir.builder) -> tl.tensor:\n+    # Convert dynamic arguments to IR values\n+    # NOTES(Chenggang): current `shape/strides` are `int64_t`, while `offsets/block_shape` are `int32_t`\n+    shape = _convert_to_ir_values(builder, shape)\n+    strides = _convert_to_ir_values(builder, strides)\n+    offsets = _convert_to_ir_values(builder, offsets, require_i64=False)\n+\n+    # Check `base` type\n+    if not base.type.is_ptr() or base.type.element_ty.is_block():\n+        raise ValueError(\"Expected `base` to be a pointer type (but not a block pointer type or others)\")\n+\n+    # Treat `pointer_type<tl.int1>` as `pointer_type<tl.int8>`\n+    if base.type.element_ty == tl.int1:\n+        base = cast(base, tl.pointer_type(tl.int8, base.type.address_space), builder)\n+\n+    # Check whether `block_shape` is static\n+    if not hasattr(block_shape, \"__iter__\"):\n+        block_shape = [block_shape]\n+    block_shape = [elem.value if isinstance(elem, tl.constexpr) else elem for elem in block_shape]\n+    assert all([isinstance(elem, int) and -2**31 <= elem < 2**31 for elem in block_shape]), \\\n+        \"Expected a list of constant integers (`int32_t` range) in `block_shape`\"\n+\n+    # Check `order`\n+    if not hasattr(order, \"__iter__\"):\n+        order = [order]\n+    order = [elem.value if isinstance(elem, tl.constexpr) else elem for elem in order]\n+    assert sorted(order) == list(range(len(order))), \"Expected a permutation of (0, 1, ..., len(order)-1) in order\"\n+\n+    # Must have same length\n+    assert all([len(block_shape) == len(list_like) for list_like in [shape, strides, offsets, order]]), \\\n+        \"Expected shape/strides/offsets/block_shape to have the same length\"\n+\n+    # Build value, the type is:\n+    #   `pointer_type<blocked<shape, element_type>>` in Python\n+    #   `tt.ptr<tensor<shape, element_type>>` in MLIR\n+    handle = builder.create_make_block_ptr(base.handle, shape, strides, offsets, block_shape, order)\n+    return tl.tensor(handle, tl.pointer_type(tl.block_type(base.type.element_ty, block_shape)))\n+\n+\n+def advance(base: tl.tensor, offsets, builder: ir.builder) -> tl.tensor:\n+    # Convert dynamic offsets to IR values\n+    offsets = _convert_to_ir_values(builder, offsets, require_i64=False)\n+\n+    # Advanced block pointer type is the same as before\n+    return tl.tensor(builder.create_advance(base.handle, offsets), base.type)"}, {"filename": "test/Triton/rewrite-tensor-pointer.mlir", "status": "added", "additions": 83, "deletions": 0, "changes": 83, "file_content_changes": "@@ -0,0 +1,83 @@\n+// RUN: triton-opt %s -triton-rewrite-tensor-pointer | FileCheck %s\n+func.func public @matmul_kernel(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}) {\n+  %c31_i32 = arith.constant 31 : i32\n+  %c127_i32 = arith.constant 127 : i32\n+  %c1 = arith.constant 1 : index\n+  %c0 = arith.constant 0 : index\n+  %cst = arith.constant dense<0.000000e+00> : tensor<128x32xf32>\n+  %c0_i32 = arith.constant 0 : i32\n+  %c1_i64 = arith.constant 1 : i64\n+  %c32_i32 = arith.constant 32 : i32\n+  %c128_i32 = arith.constant 128 : i32\n+  %c8_i32 = arith.constant 8 : i32\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = tt.get_program_id {axis = 1 : i32} : i32\n+  %2 = arith.addi %arg3, %c127_i32 : i32\n+  %3 = arith.divsi %2, %c128_i32 : i32\n+  %4 = arith.addi %arg4, %c31_i32 : i32\n+  %5 = arith.divsi %4, %c32_i32 : i32\n+  %6 = arith.muli %5, %c8_i32 : i32\n+  %7 = arith.divsi %0, %6 : i32\n+  %8 = arith.muli %7, %c8_i32 : i32\n+  %9 = arith.subi %3, %8 : i32\n+  %10 = arith.cmpi slt, %9, %c8_i32 : i32\n+  %11 = arith.select %10, %9, %c8_i32 : i32\n+  %12 = arith.remsi %0, %11 : i32\n+  %13 = arith.addi %8, %12 : i32\n+  %14 = arith.remsi %0, %6 : i32\n+  %15 = arith.divsi %14, %11 : i32\n+  %16 = arith.muli %13, %c128_i32 : i32\n+  %17 = arith.muli %1, %c32_i32 : i32\n+  %18 = arith.extsi %arg3 : i32 to i64\n+  %19 = arith.extsi %arg5 : i32 to i64\n+  %20 = arith.extsi %arg6 : i32 to i64\n+  // CHECK-NOT: tt.make_tensor_ptr\n+  %21 = tt.make_tensor_ptr %arg0, [%18, %19], [%20, %c1_i64], [%16, %17] {order = array<i32: 1, 0>} : !tt.ptr<tensor<128x32xf16>>\n+  %22 = arith.muli %15, %c32_i32 : i32\n+  %23 = arith.extsi %arg4 : i32 to i64\n+  %24 = arith.extsi %arg7 : i32 to i64\n+  // CHECK-NOT: tt.make_tensor_ptr\n+  %25 = tt.make_tensor_ptr %arg1, [%19, %23], [%24, %c1_i64], [%17, %22] {order = array<i32: 1, 0>} : !tt.ptr<tensor<32x32xf16>>\n+  %26 = arith.addi %arg5, %c31_i32 : i32\n+  %27 = arith.divsi %26, %c32_i32 : i32\n+  %28 = arith.index_cast %27 : i32 to index\n+  %29:3 = scf.for %arg9 = %c0 to %28 step %c1 iter_args(%arg10 = %cst, %arg11 = %21, %arg12 = %25) -> (tensor<128x32xf32>, !tt.ptr<tensor<128x32xf16>>, !tt.ptr<tensor<32x32xf16>>) {\n+    // CHECK: tt.load %{{.*}}, %{{.*}}, %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16>\n+    %55 = tt.load %arg11 {boundaryCheck = array<i32: 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, padding = 2 : i32} : !tt.ptr<tensor<128x32xf16>> -> tensor<128x32xf16>\n+    // CHECK: tt.load %{{.*}}, %{{.*}}, %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf16>\n+    %56 = tt.load %arg12 {boundaryCheck = array<i32: 0>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, padding = 2 : i32} : !tt.ptr<tensor<32x32xf16>> -> tensor<32x32xf16>\n+    %57 = tt.dot %55, %56, %arg10 {allowTF32 = true} : tensor<128x32xf16> * tensor<32x32xf16> -> tensor<128x32xf32>\n+    // CHECK-NOT: tt.advance\n+    %58 = tt.advance %arg11, [%c0_i32, %c32_i32] : !tt.ptr<tensor<128x32xf16>>\n+    // CHECK-NOT: tt.advance\n+    %59 = tt.advance %arg12, [%c32_i32, %c0_i32] : !tt.ptr<tensor<32x32xf16>>\n+    scf.yield %57, %58, %59 : tensor<128x32xf32>, !tt.ptr<tensor<128x32xf16>>, !tt.ptr<tensor<32x32xf16>>\n+  }\n+  %30 = arith.truncf %29#0 : tensor<128x32xf32> to tensor<128x32xf16>\n+  %31 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  %32 = tt.splat %16 : (i32) -> tensor<128xi32>\n+  %33 = arith.addi %32, %31 : tensor<128xi32>\n+  %34 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>\n+  %35 = tt.splat %22 : (i32) -> tensor<32xi32>\n+  %36 = arith.addi %35, %34 : tensor<32xi32>\n+  %37 = tt.expand_dims %33 {axis = 1 : i32} : (tensor<128xi32>) -> tensor<128x1xi32>\n+  %38 = tt.splat %arg8 : (i32) -> tensor<128x1xi32>\n+  %39 = arith.muli %37, %38 : tensor<128x1xi32>\n+  %40 = tt.expand_dims %36 {axis = 0 : i32} : (tensor<32xi32>) -> tensor<1x32xi32>\n+  %41 = tt.broadcast %39 : (tensor<128x1xi32>) -> tensor<128x32xi32>\n+  %42 = tt.broadcast %40 : (tensor<1x32xi32>) -> tensor<128x32xi32>\n+  %43 = arith.addi %41, %42 : tensor<128x32xi32>\n+  %44 = tt.splat %arg2 : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>>\n+  %45 = tt.addptr %44, %43 : tensor<128x32x!tt.ptr<f16>>, tensor<128x32xi32>\n+  %46 = tt.splat %arg3 : (i32) -> tensor<128xi32>\n+  %47 = arith.cmpi slt, %33, %46 : tensor<128xi32>\n+  %48 = tt.expand_dims %47 {axis = 1 : i32} : (tensor<128xi1>) -> tensor<128x1xi1>\n+  %49 = tt.splat %arg4 : (i32) -> tensor<32xi32>\n+  %50 = arith.cmpi slt, %36, %49 : tensor<32xi32>\n+  %51 = tt.expand_dims %50 {axis = 0 : i32} : (tensor<32xi1>) -> tensor<1x32xi1>\n+  %52 = tt.broadcast %48 : (tensor<128x1xi1>) -> tensor<128x32xi1>\n+  %53 = tt.broadcast %51 : (tensor<1x32xi1>) -> tensor<128x32xi1>\n+  %54 = arith.andi %52, %53 : tensor<128x32xi1>\n+  tt.store %45, %30, %54 {cache = 1 : i32, evict = 1 : i32} : tensor<128x32xf16>\n+  return\n+}"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -1032,9 +1032,9 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   func.func public @reduce_cvt2(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}) {\n     %cst = arith.constant dense<0.000000e+00> : tensor<1x256xf32, #blocked>\n-    %c3136_i32 = arith.constant 3136 : i32\n-    %c256_i32 = arith.constant 256 : i32\n-    %c0_i32 = arith.constant 0 : i32\n+    %c3136_i32 = arith.constant 3136 : index\n+    %c256_i32 = arith.constant 256 : index\n+    %c0_i32 = arith.constant 0 : index\n     %cst_0 = arith.constant dense<3.136000e+03> : tensor<1x1xf32, #blocked>\n     %cst_1 = arith.constant dense<50176> : tensor<1x256xi32, #blocked>\n     %cst_2 = arith.constant dense<196> : tensor<1x1xi32, #blocked>\n@@ -1056,8 +1056,9 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %12 = tt.broadcast %11 : (tensor<1x1xi32, #blocked>) -> tensor<1x256xi32, #blocked>\n     %13 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<1x256x!tt.ptr<f32>, #blocked>\n     %14 = tt.broadcast %7 : (tensor<1x1xi1, #blocked>) -> tensor<1x256xi1, #blocked>\n-    %15 = scf.for %arg5 = %c0_i32 to %c3136_i32 step %c256_i32 iter_args(%arg6 = %cst) -> (tensor<1x256xf32, #blocked>)  : i32 {\n-      %43 = tt.splat %arg5 : (i32) -> tensor<1x256xi32, #blocked>\n+    %15 = scf.for %arg5 = %c0_i32 to %c3136_i32 step %c256_i32 iter_args(%arg6 = %cst) -> (tensor<1x256xf32, #blocked>) {\n+      %42 = arith.index_cast %arg5 : index to i32\n+      %43 = tt.splat %42 : (i32) -> tensor<1x256xi32, #blocked>\n       %44 = arith.addi %43, %10 : tensor<1x256xi32, #blocked>\n       %45 = \"triton_gpu.cmpi\"(%44, %cst_4) {predicate = 2 : i64} : (tensor<1x256xi32, #blocked>, tensor<1x256xi32, #blocked>) -> tensor<1x256xi1, #blocked>\n       %46 = arith.remsi %44, %cst_3 : tensor<1x256xi32, #blocked>"}]