[{"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 3, "deletions": 6, "changes": 9, "file_content_changes": "@@ -103,15 +103,12 @@ def TT_AddPtrOp : TT_Op<\"addptr\",\n                      SameOperandsAndResultShape,\n                      SameOperandsAndResultEncoding,\n                       TypesMatchWith<\"result type matches ptr type\",\n-                                     \"result\", \"ptr\", \"$_self\">,\n-                      TypesMatchWith<\"result shape matches offset shape\",\n-                                     \"result\", \"offset\",\n-                                     \"getI32SameShape($_self)\">]> {\n-    let arguments = (ins TT_PtrLike:$ptr, TT_I32Like:$offset);\n+                                     \"result\", \"ptr\", \"$_self\">]> {\n+    let arguments = (ins TT_PtrLike:$ptr, TT_IntLike:$offset);\n \n     let results = (outs TT_PtrLike:$result);\n \n-    let assemblyFormat = \"$ptr `,` $offset attr-dict `:` type($result)\";\n+    let assemblyFormat = \"$ptr `,` $offset attr-dict `:` type($result) `,` type($offset)\";\n }\n \n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 38, "deletions": 0, "changes": 38, "file_content_changes": "@@ -1707,6 +1707,43 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n \n /// ====================== reduce codegen end ==========================\n \n+/// ====================== cat codegen begin  ==========================\n+\n+struct CatOpConversion : public ConvertTritonGPUOpToLLVMPattern<CatOp> {\n+  using OpAdaptor = typename CatOp::Adaptor;\n+\n+  explicit CatOpConversion(LLVMTypeConverter &typeConverter,\n+                           PatternBenefit benefit = 1)\n+      : ConvertTritonGPUOpToLLVMPattern<CatOp>(typeConverter, benefit) {}\n+\n+  LogicalResult\n+  matchAndRewrite(CatOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Location loc = op->getLoc();\n+    auto resultTy = op.getType().template cast<RankedTensorType>();\n+    unsigned elems = getElemsPerThread(resultTy);\n+    Type elemTy =\n+        this->getTypeConverter()->convertType(resultTy.getElementType());\n+    SmallVector<Type> types(elems, elemTy);\n+    // unpack input values\n+    auto lhsVals = getElementsFromStruct(loc, adaptor.lhs(), rewriter);\n+    auto rhsVals = getElementsFromStruct(loc, adaptor.rhs(), rewriter);\n+    // concatenate (and potentially reorder) values\n+    SmallVector<Value> retVals;\n+    for(Value v: lhsVals)\n+      retVals.push_back(v);\n+    for(Value v: rhsVals)\n+      retVals.push_back(v);\n+    // pack and replace\n+    Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n+    Value ret = getStructFromElements(loc, retVals, rewriter, structTy);\n+    rewriter.replaceOp(op, ret);\n+    return success();\n+  }\n+};\n+\n+/// ====================== cat codegen end    ==========================\n+\n template <typename SourceOp>\n struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n   using OpAdaptor = typename SourceOp::Adaptor;\n@@ -4560,6 +4597,7 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                                            benefit);\n   patterns.add<DotOpConversion>(typeConverter, allocation, smem, benefit);\n   patterns.add<TransOpConversion>(typeConverter, benefit);\n+  patterns.add<CatOpConversion>(typeConverter, benefit);\n   patterns.add<PrintfOpConversion>(typeConverter, benefit);\n }\n "}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 19, "deletions": 1, "changes": 20, "file_content_changes": "@@ -251,6 +251,22 @@ struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n   }\n };\n \n+struct TritonCatPattern : public OpConversionPattern<triton::CatOp> {\n+\n+  using OpConversionPattern<triton::CatOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::CatOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    // For now, this behaves like generic, but this will evolve when\n+    // we add support for `can_reorder=False`\n+    Type retType = this->getTypeConverter()->convertType(op.getType());\n+    rewriter.replaceOpWithNewOp<triton::CatOp>(op, retType, adaptor.getOperands());\n+    return success();\n+  }\n+\n+};\n+\n struct TritonTransPattern : public OpConversionPattern<triton::TransOp> {\n \n   using OpConversionPattern<triton::TransOp>::OpConversionPattern;\n@@ -433,7 +449,9 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n       TritonGenericPattern<triton::IntToPtrOp>,\n       TritonGenericPattern<triton::PtrToIntOp>,\n       TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n-      TritonGenericPattern<triton::AddPtrOp>, TritonReducePattern,\n+      TritonGenericPattern<triton::AddPtrOp>, \n+      TritonCatPattern,\n+      TritonReducePattern,\n       TritonTransPattern, TritonExpandDimsPattern, TritonMakeRangePattern,\n       TritonDotPattern, TritonLoadPattern, TritonStorePattern,\n       TritonExtElemwisePattern, TritonPrintfPattern, TritonAtomicRMWPattern>("}, {"filename": "lib/Dialect/Triton/IR/Traits.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -19,7 +19,7 @@ mlir::OpTrait::impl::verifySameOperandsAndResultEncoding(Operation *op) {\n   for (auto resultType : op->getResultTypes())\n     if (failed(verifySameEncoding(resultType, type)))\n       return op->emitOpError()\n-             << \"requires the same shape for all operands and results\";\n+             << \"requires the same encoding for all operands and results\";\n   return verifySameOperandsEncoding(op);\n }\n "}, {"filename": "lib/Dialect/Triton/Transforms/Combine.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -196,7 +196,7 @@ class CombineOpsPass : public TritonCombineOpsBase<CombineOpsPass> {\n     patterns.add<CombineDotAddFRevPattern>(context);\n     // %}\n     patterns.add<CombineSelectMaskedLoadPattern>(context);\n-    patterns.add<CombineAddPtrPattern>(context);\n+    // patterns.add<CombineAddPtrPattern>(context);\n     patterns.add<CombineBroadcastConstantPattern>(context);\n \n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.td", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -29,13 +29,14 @@ def CombineDotAddFRevPattern : Pat<\n         (TT_DotOp $a, $b, $d, $allowTF32),\n         [(Constraint<CPred<\"isZero($0)\">> $c)]>;\n \n-\n+// TODO: this fails for addptr(addptr(ptr, i32), i64)\n+// Commented out until fixed\n // addptr(addptr(%ptr, %idx0), %idx1) => addptr(%ptr, AddI(%idx0, %idx1))\n //   Note: leave (sub %c0, %c0) canceling to ArithmeticDialect\n //         (ref: ArithmeticCanonicalization.td)\n-def CombineAddPtrPattern : Pat<\n-        (TT_AddPtrOp (TT_AddPtrOp $ptr, $idx0), $idx1),\n-        (TT_AddPtrOp $ptr, (Arith_AddIOp $idx0, $idx1))>;\n+// def CombineAddPtrPattern : Pat<\n+//         (TT_AddPtrOp (TT_AddPtrOp $ptr, $idx0), $idx1),\n+//         (TT_AddPtrOp $ptr, (Arith_AddIOp $idx0, $idx1))>;\n \n // broadcast(cst) => cst\n def getConstantValue : NativeCodeCall<\"getConstantValue($_builder, $0, $1)\">;"}, {"filename": "python/tests/test_backend.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -64,12 +64,12 @@ def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n     %7 = tt.broadcast %6 : (tensor<1x128xi32, #src>) -> tensor<128x128xi32, #src>\n     %8 = tt.broadcast %5 : (tensor<128x1xi32, #src>) -> tensor<128x128xi32, #src>\n     %9 = arith.addi %8, %7 : tensor<128x128xi32, #src>\n-    %10 = tt.addptr %2, %9 : tensor<128x128x!tt.ptr<f16>, #src>\n+    %10 = tt.addptr %2, %9 : tensor<128x128x!tt.ptr<f16>, #src>, tensor<128x128xi32, #src>\n     %11 = tt.load %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf16, #src>\n     %3 = tt.splat %arg1 : (!tt.ptr<f16>) -> tensor<128x128x!tt.ptr<f16>, #dst>\n     %12 = triton_gpu.convert_layout %9 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #dst>\n     %13 = triton_gpu.convert_layout %11 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #dst>\n-    %14 = tt.addptr %3, %12 : tensor<128x128x!tt.ptr<f16>, #dst>\n+    %14 = tt.addptr %3, %12 : tensor<128x128x!tt.ptr<f16>, #dst>, tensor<128x128xi32, #dst>\n     tt.store %14, %13 : tensor<128x128xf16, #dst>\n     return\n   }"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -371,6 +371,7 @@ def visit_If(self, node):\n                 # 1. we have an orelse node\n                 #   or\n                 # 2. the then block defines new variable\n+                else_defs = {}\n                 if then_defs or node.orelse:\n                     if node.orelse:\n                         self.lscope = liveins\n@@ -381,7 +382,6 @@ def visit_If(self, node):\n                         else_defs = self.local_defs.copy()\n                     else:\n                         # collect else_defs\n-                        else_defs = {}\n                         for name in then_defs:\n                             if name in liveins:\n                                 assert self.is_triton_tensor(then_defs[name])"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -55,6 +55,7 @@\n     printf,\n     program_id,\n     ravel,\n+    reshape,\n     sigmoid,\n     sin,\n     softmax,\n@@ -70,6 +71,7 @@\n     uint64,\n     uint8,\n     umulhi,\n+    view,\n     void,\n     where,\n     xor_sum,\n@@ -149,6 +151,7 @@\n     \"randn\",\n     \"randn4x\",\n     \"ravel\",\n+    \"reshape\",\n     \"sigmoid\",\n     \"sin\",\n     \"softmax\",\n@@ -165,6 +168,7 @@\n     \"uint64\",\n     \"uint8\",\n     \"umulhi\",\n+    \"view\",\n     \"void\",\n     \"where\",\n     \"xor_sum\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 8, "deletions": 3, "changes": 11, "file_content_changes": "@@ -731,16 +731,20 @@ def trans(input, _builder=None):\n     return semantic.trans(input, _builder)\n \n @builtin\n-def cat(input, other, _builder=None):\n+def cat(input, other, can_reorder=False, _builder=None):\n     \"\"\"\n     Concatenate the given blocks\n \n     :param input: The first input tensor.\n     :type input:\n     :param other: The second input tensor.\n     :type other:\n+    :param reorder: Compiler hint. If true, the compiler is\n+    allowed to reorder elements while concatenating inputs.\n+    Only use if the order does not matter (e.g., result is\n+    only used in reduction ops)\n     \"\"\"\n-    return semantic.cat(input, other, _builder)\n+    return semantic.cat(input, other, can_reorder, _builder)\n \n \n @builtin\n@@ -761,7 +765,8 @@ def view(input, shape, _builder=None):\n @builtin\n def reshape(input, shape, _builder=None):\n     # TODO: should be more than just a view\n-    return view(input, shape, _builder)\n+    shape = [x.value for x in shape]\n+    return semantic.view(input, shape, _builder)\n \n # -----------------------\n # Linear Algebra"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -498,9 +498,11 @@ def expand_dims(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_expand_dims(input.handle, axis), ret_ty)\n \n \n-def cat(lhs: tl.tensor, rhs: tl.tensor, builder: ir.builder) -> tl.tensor:\n-    # TODO: check types\n-    return tl.tensor(builder.create_cat(lhs.handle, rhs.handle), lhs.type)\n+def cat(lhs: tl.tensor, rhs: tl.tensor, can_reorder: bool, builder: ir.builder) -> tl.tensor:\n+    assert can_reorder, \"current implementation of `cat` always may reorder elements\"\n+    assert len(lhs.shape) == 1\n+    ret_type = tl.block_type(lhs.type.scalar, [lhs.shape[0] + rhs.shape[0]])\n+    return tl.tensor(builder.create_cat(lhs.handle, rhs.handle), ret_type)\n \n def trans(input: tl.tensor, builder: ir.builder) -> tl.tensor:\n     if len(input.shape) != 2:"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -27,8 +27,8 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n     %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B_DOT>\n     %c = tt.dot %a, %b, %prev_c {transA = false, transB = false, allowTF32 = true} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>\n \n-    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n-    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi32, #AL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return"}, {"filename": "test/Analysis/test-alignment.mlir", "status": "modified", "additions": 12, "deletions": 12, "changes": 24, "file_content_changes": "@@ -18,21 +18,21 @@ func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {t\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1]\n   %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>>\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 1]\n-  %6 = tt.addptr %5, %4 : tensor<128x1x!tt.ptr<f32>>\n+  %6 = tt.addptr %5, %4 : tensor<128x1x!tt.ptr<f32>>, tensor<128x1xi32>\n   // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 65536] ; Constancy: [1, 1]\n   %7 = tt.expand_dims %1 {axis = 0 : i32}: (tensor<128xi32>) -> tensor<1x128xi32>\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 128]\n   %8 = tt.broadcast %6 : (tensor<128x1x!tt.ptr<f32>>) -> tensor<128x128x!tt.ptr<f32>>\n   // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 65536] ; Constancy: [128, 1]\n   %9 = tt.broadcast %7 : (tensor<1x128xi32>) -> tensor<128x128xi32>\n   // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 16] ; Constancy: [1, 1]\n-  %10 = tt.addptr %8, %9 : tensor<128x128x!tt.ptr<f32>>\n+  %10 = tt.addptr %8, %9 : tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>\n   // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [65536, 1] ; Constancy: [1, 1]\n   %11 = tt.expand_dims %0 {axis = 1 : i32}: (tensor<128xi32>) -> tensor<128x1xi32>\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1]\n   %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>>\n   // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 1]\n-  %13 = tt.addptr %12, %11 : tensor<128x1x!tt.ptr<f32>>\n+  %13 = tt.addptr %12, %11 : tensor<128x1x!tt.ptr<f32>>, tensor<128x1xi32>\n   // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 65536] ; Constancy: [1, 1]\n   %14 = tt.expand_dims %1 {axis = 0 : i32} : (tensor<128xi32>) -> tensor<1x128xi32>\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 128]\n@@ -44,7 +44,7 @@ func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {t\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 1048576] ; Constancy: [128, 1]\n   %18 = tt.broadcast %16 : (tensor<1x128xi32>) -> tensor<128x128xi32>\n   // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 1]\n-  %19 = tt.addptr %17, %18 : tensor<128x128x!tt.ptr<f32>>\n+  %19 = tt.addptr %17, %18 : tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [1, 1]\n   %20 = tt.load %10, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf32>\n   tt.store %19, %20, %cst : tensor<128x128xf32>\n@@ -72,7 +72,7 @@ func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n:\n   // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16] ; Constancy: [128]\n   %5 = tt.splat %addr : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n   // CHECK-NEXT: Contiguity: [128] ; Divisibility: [16] ; Constancy: [1]\n-  %6 = tt.addptr %5, %4 : tensor<128x!tt.ptr<f32>>\n+  %6 = tt.addptr %5, %4 : tensor<128x!tt.ptr<f32>>, tensor<128xi32>\n   // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16] ; Constancy: [128]\n   %9 = tt.splat %n : (i32) -> tensor<128xi32>\n   // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [16]\n@@ -97,18 +97,18 @@ func @vecadd_mask_align_16(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %ar\n   %3 = tt.splat %1 : (i32) -> tensor<64xi32>\n   %4 = arith.addi %3, %2 : tensor<64xi32>\n   %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n-  %6 = tt.addptr %5, %4 : tensor<64x!tt.ptr<f32>>\n+  %6 = tt.addptr %5, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n-  %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>>\n+  %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   %9 = tt.splat %n_elements : (i32) -> tensor<64xi32>\n   // CHECK: Contiguity: [1] ; Divisibility: [64] ; Constancy: [16] ( %{{.*}} = arith.cmpi slt, %{{.*}}, %{{.*}} : tensor<64xi32> )\n   %mask = arith.cmpi slt, %4, %9 : tensor<64xi32>\n   %11 = tt.load %6, %mask {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n   %12 = tt.load %8, %mask {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n   %13 = arith.addf %11, %12 : tensor<64xf32>\n   %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n-  // CHECK: Contiguity: [64] ; Divisibility: [16] ; Constancy: [1] ( %{{.*}} = tt.addptr %{{.*}}, %{{.*}} : tensor<64x!tt.ptr<f32>> )\n-  %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>>\n+  // CHECK: Contiguity: [64] ; Divisibility: [16] ; Constancy: [1] ( %{{.*}} = tt.addptr %{{.*}}, %{{.*}} : tensor<64x!tt.ptr<f32>>, tensor<64xi32> )\n+  %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   tt.store %15, %13, %mask : tensor<64xf32>\n   return\n }\n@@ -125,17 +125,17 @@ func @vecadd_mask_align_1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg\n   %3 = tt.splat %1 : (i32) -> tensor<64xi32>\n   %4 = arith.addi %3, %2 : tensor<64xi32>\n   %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n-  %6 = tt.addptr %5, %4 : tensor<64x!tt.ptr<f32>>\n+  %6 = tt.addptr %5, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n-  %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>>\n+  %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   %9 = tt.splat %n_elements : (i32) -> tensor<64xi32>\n   // CHECK: Contiguity: [1] ; Divisibility: [64] ; Constancy: [1] ( %{{.*}} = arith.cmpi slt, %{{.*}}, %{{.*}} : tensor<64xi32> )\n   %10 = arith.cmpi slt, %4, %9 : tensor<64xi32>\n   %11 = tt.load %6, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n   %12 = tt.load %8, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n   %13 = arith.addf %11, %12 : tensor<64xf32>\n   %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n-  %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>>\n+  %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   tt.store %15, %13, %10 : tensor<64xf32>\n   return\n }"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -35,8 +35,8 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n \n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>\n \n-    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n-    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi32, #AL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -33,8 +33,8 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n     %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B_DOT>\n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>\n \n-    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n-    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi32, #AL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return"}, {"filename": "test/Conversion/triton_ops.mlir", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -38,19 +38,19 @@ func @cast_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_f32: f32, %scalar_i64: i64) {\n func @addptr_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_i32: i32) {\n   // scalar -> scalar\n   // CHECK: !tt.ptr<f32>\n-  %0 = tt.addptr %scalar_ptr, %scalar_i32 : !tt.ptr<f32>\n+  %0 = tt.addptr %scalar_ptr, %scalar_i32 : !tt.ptr<f32>, i32\n \n   // 0D tensor -> 0D tensor\n   %tensor_ptr_0d = tt.splat %scalar_ptr : (!tt.ptr<f32>) -> tensor<!tt.ptr<f32>>\n   %tensor_i32_0d = tt.splat %scalar_i32 : (i32) -> tensor<i32>\n   // CHECK: tensor<!tt.ptr<f32>>\n-  %1 = tt.addptr %tensor_ptr_0d, %tensor_i32_0d : tensor<!tt.ptr<f32>>\n+  %1 = tt.addptr %tensor_ptr_0d, %tensor_i32_0d : tensor<!tt.ptr<f32>>, tensor<i32>\n \n   // 1D tensor -> 1D tensor\n   %tensor_ptr_1d = tt.splat %scalar_ptr : (!tt.ptr<f32>) -> tensor<16x!tt.ptr<f32>>\n   %tensor_i32_1d = tt.splat %scalar_i32 : (i32) -> tensor<16xi32>\n   // CHECK: tensor<16x!tt.ptr<f32>>\n-  %2 = tt.addptr %tensor_ptr_1d, %tensor_i32_1d : tensor<16x!tt.ptr<f32>>\n+  %2 = tt.addptr %tensor_ptr_1d, %tensor_i32_1d : tensor<16x!tt.ptr<f32>>, tensor<16xi32>\n   return\n }\n "}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 17, "deletions": 17, "changes": 34, "file_content_changes": "@@ -92,9 +92,9 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n     %4 = arith.addi %3, %2 : tensor<256xi32, #blocked0>\n     %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %6 = tt.addptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %6 = tt.addptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n     %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n \n     // Load 4 elements from vector0\n     // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n@@ -111,7 +111,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     %10 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n     %11 = arith.addf %9, %10 : tensor<256xf32, #blocked0>\n     %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n \n     // Store 4 elements to global\n     // CHECK: @${{.*}} st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n@@ -136,9 +136,9 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n     %4 = arith.addi %3, %2 : tensor<256xi32, #blocked0>\n     %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %6 = tt.addptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %6 = tt.addptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n     %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n \n     // Load 4 elements from A with single one vectorized load instruction\n     // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n@@ -150,7 +150,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     %10 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n     %11 = arith.addf %9, %10 : tensor<256xf32, #blocked0>\n     %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n \n     // Store 4 elements to global with single one vectorized store instruction\n     // CHECK: @$5 st.global.v4.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n@@ -173,9 +173,9 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     %3 = tt.splat %1 : (i32) -> tensor<64xi32, #blocked>\n     %4 = arith.addi %3, %2 : tensor<64xi32, #blocked>\n     %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>, #blocked>\n-    %6 = tt.addptr %5, %4 : tensor<64x!tt.ptr<f32>, #blocked>\n+    %6 = tt.addptr %5, %4 : tensor<64x!tt.ptr<f32>, #blocked>, tensor<64xi32, #blocked>\n     %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>, #blocked>\n-    %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>, #blocked>\n+    %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>, #blocked>, tensor<64xi32, #blocked>\n     %9 = tt.splat %n_elements : (i32) -> tensor<64xi32, #blocked>\n     %10 = \"triton_gpu.cmpi\"(%4, %9) {predicate = 2 : i64} : (tensor<64xi32, #blocked>, tensor<64xi32, #blocked>) -> tensor<64xi1, #blocked>\n     // load op has a vector width = 1 due to the %mask's alignment\n@@ -184,7 +184,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     %12 = tt.load %8, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32, #blocked>\n     %13 = arith.addf %11, %12 : tensor<64xf32, #blocked>\n     %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>, #blocked>\n-    %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>, #blocked>\n+    %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>, #blocked>, tensor<64xi32, #blocked>\n     tt.store %15, %13, %10 : tensor<64xf32, #blocked>\n     return\n   }\n@@ -203,9 +203,9 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n     %4 = arith.addi %3, %2 : tensor<256xi32, #blocked0>\n     %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %6 = tt.addptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %6 = tt.addptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n     %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n \n     // Load 8 elements from A with two vectorized load instruction\n     // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n@@ -219,7 +219,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     %10 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n     %11 = arith.addf %9, %10 : tensor<256xf32, #blocked0>\n     %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n \n     // Store 8 elements to global with two vectorized store instruction\n     // CHECK: @$5 st.global.v4.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n@@ -317,7 +317,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   func @basic_addptr(%arg0 : tensor<256x!tt.ptr<f32>,#blocked0>, %arg1 : tensor<256xi32,#blocked0>) {\n     // CHECK: llvm.getelementptr\n     // CHECK: llvm.getelementptr\n-    %0 = tt.addptr %arg0, %arg1 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %0 = tt.addptr %arg0, %arg1 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n     return\n   }\n }\n@@ -411,7 +411,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %broadcast_off1 = triton_gpu.convert_layout %broadcast_off1_ : (tensor<16x64xi32, #block3>) -> tensor<16x64xi32, #AL>\n     %off = arith.addi %broadcast_off0, %broadcast_off1 : tensor<16x64xi32, #AL>\n     %a_init = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<16x64x!tt.ptr<f16>, #AL>\n-    %a_ptr = tt.addptr %a_init, %off : tensor<16x64x!tt.ptr<f16>, #AL>\n+    %a_ptr = tt.addptr %a_init, %off : tensor<16x64x!tt.ptr<f16>, #AL>, tensor<16x64xi32, #AL>\n     %tensor = triton_gpu.alloc_tensor : tensor<2x16x64xf16, #A>\n     %index = arith.constant 1 : i32\n \n@@ -450,7 +450,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %broadcast_off1 = triton_gpu.convert_layout %broadcast_off1_ : (tensor<16x64xi32, #block3>) -> tensor<16x64xi32, #AL>\n     %off = arith.addi %broadcast_off0, %broadcast_off1 : tensor<16x64xi32, #AL>\n     %a_init = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<16x64x!tt.ptr<f32>, #AL>\n-    %a_ptr = tt.addptr %a_init, %off : tensor<16x64x!tt.ptr<f32>, #AL>\n+    %a_ptr = tt.addptr %a_init, %off : tensor<16x64x!tt.ptr<f32>, #AL>, tensor<16x64xi32, #AL>\n     %tensor = triton_gpu.alloc_tensor : tensor<2x16x64xf32, #A>\n     %index = arith.constant 1 : i32\n \n@@ -491,7 +491,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %broadcast_off1 = triton_gpu.convert_layout %broadcast_off1_ : (tensor<16x32xi32, #block3>) -> tensor<16x32xi32, #AL>\n     %off = arith.addi %broadcast_off0, %broadcast_off1 : tensor<16x32xi32, #AL>\n     %a_init = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<16x32x!tt.ptr<f32>, #AL>\n-    %a_ptr = tt.addptr %a_init, %off : tensor<16x32x!tt.ptr<f32>, #AL>\n+    %a_ptr = tt.addptr %a_init, %off : tensor<16x32x!tt.ptr<f32>, #AL>, tensor<16x32xi32, #AL>\n     %tensor = triton_gpu.alloc_tensor : tensor<2x16x32xf32, #A>\n     %index = arith.constant 1 : i32\n \n@@ -535,7 +535,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %broadcast_off1 = triton_gpu.convert_layout %broadcast_off1_ : (tensor<32x32xi32, #block3>) -> tensor<32x32xi32, #AL>\n     %off = arith.addi %broadcast_off0, %broadcast_off1 : tensor<32x32xi32, #AL>\n     %a_init = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n-    %a_ptr = tt.addptr %a_init, %off : tensor<32x32x!tt.ptr<f32>, #AL>\n+    %a_ptr = tt.addptr %a_init, %off : tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32xi32, #AL>\n     %tensor = triton_gpu.alloc_tensor : tensor<2x32x32xf32, #A>\n     %index = arith.constant 1 : i32\n "}, {"filename": "test/Triton/combine.mlir", "status": "modified", "additions": 9, "deletions": 7, "changes": 16, "file_content_changes": "@@ -22,28 +22,30 @@ func @test_combine_dot_add_pattern() -> (tensor<128x128xf32>, tensor<128x128xf32\n     return %res0, %res1 : tensor<128x128xf32>, tensor<128x128xf32>\n }\n \n-// CHECK-LABEL: @test_combine_addptr_pattern\n+\n+// COM: CHECK-LABEL: @test_combine_addptr_pattern\n func @test_combine_addptr_pattern(%base: !tt.ptr<f32>) -> tensor<8x!tt.ptr<f32>> {\n     %off0 = arith.constant 10 : i32\n     %off1 = arith.constant 15 : i32\n \n     // 10 + 15 = 25\n-    // CHECK-NEXT: %[[cst:.*]] = arith.constant dense<25> : tensor<8xi32>\n+    // COM: CHECK-NEXT: %[[cst:.*]] = arith.constant dense<25> : tensor<8xi32>\n \n     %base_ = tt.broadcast %base : (!tt.ptr<f32>) -> tensor<8x!tt.ptr<f32>>\n \n-    // CHECK-NEXT: %[[tmp0:.*]] = tt.broadcast %{{.*}} : (!tt.ptr<f32>) -> tensor<8x!tt.ptr<f32>>\n+    // COM: CHECK-NEXT: %[[tmp0:.*]] = tt.broadcast %{{.*}} : (!tt.ptr<f32>) -> tensor<8x!tt.ptr<f32>>\n \n     %idx0 = tt.broadcast %off0 : (i32) -> tensor<8xi32>\n     %idx1 = tt.broadcast %off1 : (i32) -> tensor<8xi32>\n-\n-    // CHECK-NEXT: %1 = tt.addptr %[[tmp0]], %[[cst]] : tensor<8x!tt.ptr<f32>>\n-    %ptr0 = tt.addptr %base_, %idx0 : tensor<8x!tt.ptr<f32>>\n-    %ptr1 = tt.addptr %ptr0, %idx1 : tensor<8x!tt.ptr<f32>>\n+ \n+    // COM: CHECK-NEXT: %1 = tt.addptr %[[tmp0]], %[[cst]] : tensor<8x!tt.ptr<f32>>, tensor<8xi32>\n+    %ptr0 = tt.addptr %base_, %idx0 : tensor<8x!tt.ptr<f32>>, tensor<8xi32>\n+    %ptr1 = tt.addptr %ptr0, %idx1 : tensor<8x!tt.ptr<f32>>, tensor<8xi32>\n \n     return %ptr1 : tensor<8x!tt.ptr<f32>>\n }\n \n+\n // CHECK-LABEL: @test_combine_select_masked_load_pattern\n func @test_combine_select_masked_load_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %cond: i1) -> (tensor<8xf32>, tensor<8xf32>) {\n     %mask = tt.broadcast %cond : (i1) -> tensor<8xi1>"}, {"filename": "test/Triton/vecadd.mlir", "status": "modified", "additions": 18, "deletions": 18, "changes": 36, "file_content_changes": "@@ -11,9 +11,9 @@ module {\n     %5 = tt.broadcast %arg3 : (i32) -> tensor<256xi32>\n     %6 = arith.cmpi slt, %4, %5 : tensor<256xi32>\n     %7 = tt.broadcast %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n-    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>>\n+    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>>, tensor<256xi32>\n     %9 = tt.broadcast %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n-    %10 = tt.addptr %9, %4 : tensor<256x!tt.ptr<f32>>\n+    %10 = tt.addptr %9, %4 : tensor<256x!tt.ptr<f32>>, tensor<256xi32>\n     %cst = arith.constant 0.000000e+00 : f32\n     %11 = tt.broadcast %cst : (f32) -> tensor<256xf32>\n     %c0_i32 = arith.constant 0 : i32\n@@ -31,13 +31,13 @@ module {\n       %22 = arith.addf %19, %21 : tensor<256xf32>\n       %23 = arith.addf %arg7, %22 : tensor<256xf32>\n       %24 = tt.broadcast %arg5 : (i32) -> tensor<256xi32>\n-      %25 = tt.addptr %arg8, %24 : tensor<256x!tt.ptr<f32>>\n+      %25 = tt.addptr %arg8, %24 : tensor<256x!tt.ptr<f32>>, tensor<256xi32>\n       %26 = tt.broadcast %arg5 : (i32) -> tensor<256xi32>\n-      %27 = tt.addptr %arg9, %26 : tensor<256x!tt.ptr<f32>>\n+      %27 = tt.addptr %arg9, %26 : tensor<256x!tt.ptr<f32>>, tensor<256xi32>\n       scf.yield %23, %25, %27 : tensor<256xf32>, tensor<256x!tt.ptr<f32>>, tensor<256x!tt.ptr<f32>>\n     }\n     %16 = tt.broadcast %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n-    %17 = tt.addptr %16, %4 : tensor<256x!tt.ptr<f32>>\n+    %17 = tt.addptr %16, %4 : tensor<256x!tt.ptr<f32>>, tensor<256xi32>\n     tt.store %17, %15#0, %6 : tensor<256xf32>\n     return\n   }\n@@ -57,9 +57,9 @@ module {\n //     %5 = tt.broadcast %arg3 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %6 = \"triton_gpu.cmpi\"(%4, %5) {predicate = 2 : i64} : (tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>) -> tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %7 = tt.broadcast %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %8 = tt.addptr %7, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %8 = tt.addptr %7, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //     %9 = tt.broadcast %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %10 = tt.addptr %9, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %10 = tt.addptr %9, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //     %11 = tt.broadcast %cst : (f32) -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %12 = arith.index_cast %arg4 : i32 to index\n //     %13 = arith.cmpi slt, %c0, %12 : index\n@@ -72,9 +72,9 @@ module {\n //     %20 = arith.andi %6, %19 : tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %21 = triton_gpu.copy_async %10, %20, %18 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">> -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %22 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %23 = tt.addptr %8, %22, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %23 = tt.addptr %8, %22, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //     %24 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %25 = tt.addptr %10, %24, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %25 = tt.addptr %10, %24, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //     %26 = arith.cmpi slt, %c32, %12 : index\n //     %27 = tt.broadcast %cst : (f32) -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %28 = tt.broadcast %26 : (i1) -> tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n@@ -85,9 +85,9 @@ module {\n //     %33 = arith.andi %6, %32 : tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %34 = triton_gpu.copy_async %25, %33, %31 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">> -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %35 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %36 = tt.addptr %23, %35, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %36 = tt.addptr %23, %35, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //     %37 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %38 = tt.addptr %25, %37, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %38 = tt.addptr %25, %37, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //     %39 = arith.cmpi slt, %c64, %12 : index\n //     %40 = tt.broadcast %cst : (f32) -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %41 = tt.broadcast %39 : (i1) -> tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n@@ -98,16 +98,16 @@ module {\n //     %46 = arith.andi %6, %45 : tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %47 = triton_gpu.copy_async %38, %46, %44 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">> -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %48 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %49 = tt.addptr %36, %48, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %49 = tt.addptr %36, %48, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //     %50 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %51 = tt.addptr %38, %50, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %51 = tt.addptr %38, %50, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //     %52:12 = scf.for %arg6 = %c0 to %12 step %c32 iter_args(%arg7 = %11, %arg8 = %8, %arg9 = %10, %arg10 = %17, %arg11 = %30, %arg12 = %43, %arg13 = %21, %arg14 = %34, %arg15 = %47, %arg16 = %51, %arg17 = %49, %arg18 = %c64) -> (tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, index) {\n //       %55 = arith.addf %arg10, %arg13 : tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       %56 = arith.addf %arg7, %55 : tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       %57 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//       %58 = tt.addptr %arg8, %57, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//       %58 = tt.addptr %arg8, %57, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //       %59 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//       %60 = tt.addptr %arg9, %59, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//       %60 = tt.addptr %arg9, %59, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //       %61 = arith.addi %arg18, %c32 : index\n //       %62 = arith.cmpi slt, %61, %12 : index\n //       %63 = tt.broadcast %cst : (f32) -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n@@ -117,13 +117,13 @@ module {\n //       %67 = tt.broadcast %cst : (f32) -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       %68 = triton_gpu.copy_async %arg16, %65, %67 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">> -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       %69 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//       %70 = tt.addptr %arg17, %69, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//       %70 = tt.addptr %arg17, %69, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //       %71 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//       %72 = tt.addptr %arg16, %71, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//       %72 = tt.addptr %arg16, %71, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //       scf.yield %56, %58, %60, %arg11, %arg12, %66, %arg14, %arg15, %68, %72, %70, %61 : tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, index\n //     }\n //     %53 = tt.broadcast %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %54 = tt.addptr %53, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %54 = tt.addptr %53, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //     tt.store %54, %52#0, %6 : tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     return\n //   }"}, {"filename": "test/TritonGPU/coalesce.mlir", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -31,20 +31,20 @@ func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %2 = tt.splat %arg1 : (i32) -> tensor<64x1xi32, #blocked1>\n   %3 = arith.muli %1, %2 : tensor<64x1xi32, #blocked1>\n   %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-  %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n   %6 = tt.expand_dims %01 {axis = 0 : i32} : (tensor<64xi32, #slice2dim0>) -> tensor<1x64xi32, #blocked2>\n   %7 = tt.broadcast %5 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %8 = tt.broadcast %6 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n   %9 = triton_gpu.convert_layout %8 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-  %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n   %11 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-  %12 = tt.addptr %11, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %12 = tt.addptr %11, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n   %13 = tt.splat %arg3 : (i32) -> tensor<1x64xi32, #blocked2>\n   %14 = arith.muli %6, %13 : tensor<1x64xi32, #blocked2>\n   %15 = tt.broadcast %12 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %16 = tt.broadcast %14 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n   %17 = triton_gpu.convert_layout %16 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-  %18 = tt.addptr %15, %17 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %18 = tt.addptr %15, %17 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n   %19 = tt.load %10, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, #blocked1>\n   tt.store %18, %19, %cst : tensor<64x64xf32, #blocked1>\n   return"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 31, "deletions": 31, "changes": 62, "file_content_changes": "@@ -74,20 +74,20 @@ func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt\n   %2 = tt.splat %arg1 : (i32) -> tensor<64x1xi32, #blocked1>\n   %3 = arith.muli %1, %2 : tensor<64x1xi32, #blocked1>\n   %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-  %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n   %6 = tt.expand_dims %01 {axis = 0 : i32} : (tensor<64xi32, #slice2dim0>) -> tensor<1x64xi32, #blocked2>\n   %7 = tt.broadcast %5 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %8 = tt.broadcast %6 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n   %9 = triton_gpu.convert_layout %8 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-  %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n   %11 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-  %12 = tt.addptr %11, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %12 = tt.addptr %11, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n   %13 = tt.splat %arg3 : (i32) -> tensor<1x64xi32, #blocked2>\n   %14 = arith.muli %6, %13 : tensor<1x64xi32, #blocked2>\n   %15 = tt.broadcast %12 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %16 = tt.broadcast %14 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n   %17 = triton_gpu.convert_layout %16 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-  %18 = tt.addptr %15, %17 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %18 = tt.addptr %15, %17 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n   %19 = triton_gpu.convert_layout %10 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked3>\n   %20 = triton_gpu.convert_layout %cst_0 : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked3>\n   %21 = triton_gpu.convert_layout %cst : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked3>\n@@ -106,7 +106,7 @@ func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %ar\n     // CHECK: [[loop_ret:%.*]]:2 = scf.for {{.*}} -> (tensor<64x64xf32, [[row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[row_layout]]>)\n     // CHECK-NEXT: {{.*}} = tt.load {{.*}} : tensor<64x64xf32, [[row_layout]]>\n     // CHECK-NEXT: {{.*}} = arith.addf {{.*}} : tensor<64x64xf32, [[row_layout]]>\n-    // CHECK-NEXT: {{.*}} = tt.addptr {{.*}} : tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n+    // CHECK-NEXT: {{.*}} = tt.addptr {{.*}} : tensor<64x64x!tt.ptr<f32>, [[row_layout]]>, tensor<64x64xi32, [[row_layout]]>\n     // CHECK-NEXT: scf.yield {{.*}} : tensor<64x64xf32, [[row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n     // CHECK-NEXT: }\n     // CHECK-NEXT: {{.*}} = triton_gpu.convert_layout [[loop_ret]]#0 : (tensor<64x64xf32, [[row_layout]]>) -> tensor<64x64xf32, [[col_layout_novec]]>\n@@ -123,30 +123,30 @@ func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %ar\n     %2 = tt.splat %arg1 : (i32) -> tensor<64x1xi32, #blocked1>\n     %3 = arith.muli %1, %2 : tensor<64x1xi32, #blocked1>\n     %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-    %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+    %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n     %6 = tt.expand_dims %01 {axis = 0 : i32} : (tensor<64xi32, #slice2dim0>) -> tensor<1x64xi32, #blocked2>\n     %7 = tt.broadcast %5 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n     %8 = tt.broadcast %6 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n     %9 = triton_gpu.convert_layout %8 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-    %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+    %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n     %11:2 = scf.for %arg5 = %c0 to %c32 step %c1 iter_args(%arg6 = %cst_1, %arg7 = %10) -> (tensor<64x64xf32, #blocked1>, tensor<64x64x!tt.ptr<f32>, #blocked1>) {\n       %23 = triton_gpu.convert_layout %arg7 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked3>\n       %24 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked3>\n       %25 = triton_gpu.convert_layout %cst_1 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked3>\n       %26 = tt.load %23, %24, %25 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, #blocked3>\n       %27 = triton_gpu.convert_layout %26 : (tensor<64x64xf32, #blocked3>) -> tensor<64x64xf32, #blocked1>\n       %28 = arith.addf %arg6, %27 : tensor<64x64xf32, #blocked1>\n-      %29 = tt.addptr %arg7, %cst_0 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+      %29 = tt.addptr %arg7, %cst_0 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n       scf.yield %28, %29 : tensor<64x64xf32, #blocked1>, tensor<64x64x!tt.ptr<f32>, #blocked1>\n     }\n     %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-    %13 = tt.addptr %12, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+    %13 = tt.addptr %12, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n     %14 = tt.splat %arg3 : (i32) -> tensor<1x64xi32, #blocked2>\n     %15 = arith.muli %6, %14 : tensor<1x64xi32, #blocked2>\n     %16 = tt.broadcast %13 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n     %17 = tt.broadcast %15 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n     %18 = triton_gpu.convert_layout %17 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-    %19 = tt.addptr %16, %18 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+    %19 = tt.addptr %16, %18 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n     %20 = triton_gpu.convert_layout %19 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n     %21 = triton_gpu.convert_layout %11#0 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked1>\n     %22 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked1>\n@@ -160,27 +160,27 @@ func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f3\n   %c256_i32 = arith.constant 256 : i32\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n   %1 = arith.muli %0, %c256_i32 : i32\n-  %2 = tt.splat %1 : (i32) -> tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %3 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %4 = tt.splat %1 : (i32) -> tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %5 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %6 = tt.splat %1 : (i32) -> tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %7 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %8 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %9 = arith.addi %6, %7 : tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %10 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %11 = arith.addi %4, %5 : tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %12 = tt.addptr %8, %9 : tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %13 = tt.load %12 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %14 = triton_gpu.convert_layout %13 : (tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>) -> tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>\n-  %15 = tt.addptr %10, %11 : tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %16 = tt.load %15 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %17 = triton_gpu.convert_layout %16 : (tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>) -> tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>\n+  %2 = tt.splat %1 : (i32) -> tensor<256xi32, #layout1>\n+  %3 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #layout1>\n+  %4 = tt.splat %1 : (i32) -> tensor<256xi32, #layout1>\n+  %5 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #layout1>\n+  %6 = tt.splat %1 : (i32) -> tensor<256xi32, #layout1>\n+  %7 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #layout1>\n+  %8 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #layout1>\n+  %9 = arith.addi %6, %7 : tensor<256xi32, #layout1>\n+  %10 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #layout1>\n+  %11 = arith.addi %4, %5 : tensor<256xi32, #layout1>\n+  %12 = tt.addptr %8, %9 : tensor<256x!tt.ptr<f32>, #layout1>, tensor<256xi32, #layout1>\n+  %13 = tt.load %12 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #layout1>\n+  %14 = triton_gpu.convert_layout %13 : (tensor<256xf32, #layout1>) -> tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>\n+  %15 = tt.addptr %10, %11 : tensor<256x!tt.ptr<f32>, #layout1>, tensor<256xi32, #layout1>\n+  %16 = tt.load %15 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #layout1>\n+  %17 = triton_gpu.convert_layout %16 : (tensor<256xf32, #layout1>) -> tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>\n   %18 = arith.addf %14, %17 : tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>\n-  %19 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %20 = arith.addi %2, %3 : tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %21 = tt.addptr %19, %20 : tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %22 = triton_gpu.convert_layout %18 : (tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>) -> tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  tt.store %21, %22 : tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n+  %19 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #layout1>\n+  %20 = arith.addi %2, %3 : tensor<256xi32, #layout1>\n+  %21 = tt.addptr %19, %20 : tensor<256x!tt.ptr<f32>, #layout1>, tensor<256xi32, #layout1>\n+  %22 = triton_gpu.convert_layout %18 : (tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>) -> tensor<256xf32, #layout1>\n+  tt.store %21, %22 : tensor<256xf32, #layout1>\n   return\n }"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -65,8 +65,8 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n \n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n \n-    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n-    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi32, #AL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return\n@@ -125,8 +125,8 @@ func @matmul_loop_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f\n \n       %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n \n-      %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n-      %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+      %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi32, #AL>\n+      %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n       scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n     }\n   }\n@@ -176,7 +176,7 @@ func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index, %A :\n     %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n     %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n-    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_b_ptr, %c : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return"}, {"filename": "test/TritonGPU/matmul.mlir", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -46,7 +46,7 @@ func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32__12c6\n     %31 = tt.broadcast %29 : (tensor<1x64xi32>) -> tensor<64x64xi32>\n     %32 = arith.addi %30, %31 : tensor<64x64xi32>\n     %33 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x64x!tt.ptr<f32>>\n-    %34 = tt.addptr %33, %32 : tensor<64x64x!tt.ptr<f32>>\n+    %34 = tt.addptr %33, %32 : tensor<64x64x!tt.ptr<f32>>, tensor<64x64xi32>\n     %35 = tt.expand_dims %23 {axis = 1 : i32} : (tensor<64xi32>) -> tensor<64x1xi32>\n     %36 = tt.splat %arg8 : (i32) -> tensor<64x1xi32>\n     %37 = arith.muli %35, %36 : tensor<64x1xi32>\n@@ -57,7 +57,7 @@ func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32__12c6\n     %42 = tt.broadcast %40 : (tensor<1x64xi32>) -> tensor<64x64xi32>\n     %43 = arith.addi %41, %42 : tensor<64x64xi32>\n     %44 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x64x!tt.ptr<f32>>\n-    %45 = tt.addptr %44, %43 : tensor<64x64x!tt.ptr<f32>>\n+    %45 = tt.addptr %44, %43 : tensor<64x64x!tt.ptr<f32>>, tensor<64x64xi32>\n     %46 = arith.index_cast %arg5 : i32 to index\n     %47:3 = scf.for %arg12 = %c0 to %46 step %c64 iter_args(%arg13 = %cst_0, %arg14 = %34, %arg15 = %45) -> (tensor<64x64xf32>, tensor<64x64x!tt.ptr<f32>>, tensor<64x64x!tt.ptr<f32>>) {\n       %76 = tt.load %arg14, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false, transA=false, transB=false} : tensor<64x64xf32>\n@@ -66,10 +66,10 @@ func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32__12c6\n       %79 = arith.addf %arg13, %78 : tensor<64x64xf32>\n       %80 = arith.muli %arg7, %c64_i32 : i32\n       %81 = tt.splat %80 : (i32) -> tensor<64x64xi32>\n-      %82 = tt.addptr %arg14, %81 : tensor<64x64x!tt.ptr<f32>>\n+      %82 = tt.addptr %arg14, %81 : tensor<64x64x!tt.ptr<f32>>, tensor<64x64xi32>\n       %83 = arith.muli %arg8, %c64_i32 : i32\n       %84 = tt.splat %83 : (i32) -> tensor<64x64xi32>\n-      %85 = tt.addptr %arg15, %84 : tensor<64x64x!tt.ptr<f32>>\n+      %85 = tt.addptr %arg15, %84 : tensor<64x64x!tt.ptr<f32>>, tensor<64x64xi32>\n       scf.yield %79, %82, %85 : tensor<64x64xf32>, tensor<64x64x!tt.ptr<f32>>, tensor<64x64x!tt.ptr<f32>>\n     }\n     %48 = arith.muli %12, %c64_i32 : i32\n@@ -90,7 +90,7 @@ func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32__12c6\n     %63 = tt.broadcast %61 : (tensor<1x64xi32>) -> tensor<64x64xi32>\n     %64 = arith.addi %62, %63 : tensor<64x64xi32>\n     %65 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x64x!tt.ptr<f32>>\n-    %66 = tt.addptr %65, %64 : tensor<64x64x!tt.ptr<f32>>\n+    %66 = tt.addptr %65, %64 : tensor<64x64x!tt.ptr<f32>>, tensor<64x64xi32>\n     %67 = tt.expand_dims %51 {axis = 1 : i32} : (tensor<64xi32>) -> tensor<64x1xi32>\n     %68 = tt.splat %arg3 : (i32) -> tensor<64x1xi32>\n     %69 = arith.cmpi slt, %67, %68 : tensor<64x1xi32>"}, {"filename": "test/TritonGPU/prefetch.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -51,8 +51,8 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n     %b_op = triton_gpu.convert_layout %b : (tensor<32x128xf16, #B>) -> tensor<32x128xf16, #B_OP>\n     %c = tt.dot %a_op, %b_op, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A_OP> * tensor<32x128xf16, #B_OP> -> tensor<128x128xf32, #C>\n \n-    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n-    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi32, #AL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     %next_a_ = tt.load %next_a_ptr, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n     %next_a = triton_gpu.convert_layout %next_a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n     %next_b_ = tt.load %next_b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>"}]