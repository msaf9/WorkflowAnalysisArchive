[{"filename": "include/triton/Analysis/AxisInfo.h", "status": "modified", "additions": 152, "deletions": 32, "changes": 184, "file_content_changes": "@@ -3,52 +3,62 @@\n \n #include \"mlir/Analysis/DataFlowAnalysis.h\"\n #include \"llvm/Support/raw_ostream.h\"\n-#include <iostream>\n \n+#include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n \n+#include <optional>\n+#include <type_traits>\n+\n namespace mlir {\n \n //===----------------------------------------------------------------------===//\n // AxisInfo\n //===----------------------------------------------------------------------===//\n \n /// This lattice value represents known information on the axes of a lattice.\n-/// Axis information is represented by a std::map<int, int>\n class AxisInfo {\n public:\n-  typedef SmallVector<int, 4> DimVectorT;\n+  typedef SmallVector<int64_t, 4> DimVectorT;\n \n public:\n-  // Default constructor\n+  /// Default constructor\n   AxisInfo() : AxisInfo({}, {}, {}) {}\n-  // Construct contiguity info with known contiguity\n+  /// Construct contiguity info with known contiguity\n   AxisInfo(DimVectorT knownContiguity, DimVectorT knownDivisibility,\n            DimVectorT knownConstancy)\n+      : AxisInfo(knownContiguity, knownDivisibility, knownConstancy, {}) {}\n+  AxisInfo(DimVectorT knownContiguity, DimVectorT knownDivisibility,\n+           DimVectorT knownConstancy, std::optional<int64_t> knownConstantValue)\n       : contiguity(knownContiguity), divisibility(knownDivisibility),\n-        constancy(knownConstancy), rank(contiguity.size()) {\n-    assert(knownDivisibility.size() == (size_t)rank);\n-    assert(knownConstancy.size() == (size_t)rank);\n+        constancy(knownConstancy), constantValue(knownConstantValue),\n+        rank(contiguity.size()) {\n+    assert(knownContiguity.size() == static_cast<size_t>(rank));\n+    assert(knownDivisibility.size() == static_cast<size_t>(rank));\n+    assert(knownConstancy.size() == static_cast<size_t>(rank));\n   }\n \n-  // Accessors\n-  int getContiguity(size_t d) const { return contiguity[d]; }\n+  /// Accessors\n+  int64_t getContiguity(size_t dim) const { return contiguity[dim]; }\n   const DimVectorT &getContiguity() const { return contiguity; }\n \n-  int getDivisibility(size_t d) const { return divisibility[d]; }\n+  int64_t getDivisibility(size_t dim) const { return divisibility[dim]; }\n   const DimVectorT &getDivisibility() const { return divisibility; }\n \n-  int getConstancy(size_t d) const { return constancy[d]; }\n+  int64_t getConstancy(size_t dim) const { return constancy[dim]; }\n   const DimVectorT &getConstancy() const { return constancy; }\n \n   int getRank() const { return rank; }\n \n-  // Comparison\n+  std::optional<int64_t> getConstantValue() const { return constantValue; }\n+\n+  /// Comparison\n   bool operator==(const AxisInfo &other) const {\n     return (contiguity == other.contiguity) &&\n            (divisibility == other.divisibility) &&\n-           (constancy == other.constancy);\n+           (constancy == other.constancy) &&\n+           (constantValue == other.constantValue) && (rank == other.rank);\n   }\n \n   /// The pessimistic value state of the contiguity is unknown.\n@@ -57,13 +67,18 @@ class AxisInfo {\n   }\n   static AxisInfo getPessimisticValueState(Value value);\n \n-  // The gcd of both arguments for each dimension\n+  /// The gcd of both arguments for each dimension\n   static AxisInfo join(const AxisInfo &lhs, const AxisInfo &rhs);\n \n private:\n   /// The _contiguity_ information maps the `d`-th\n   /// dimension to the length of the shortest\n-  /// sequence of contiguous integers along it\n+  /// sequence of contiguous integers along it.\n+  /// Suppose we have an array of N elements,\n+  /// with a contiguity value C,\n+  /// the array can be divided into a list of\n+  /// N/C sequences of C contiguous elements.\n+  /// Since we have N = 2^k, C must be a power of two.\n   /// For example:\n   /// [10, 11, 12, 13, 18, 19, 20, 21]\n   /// [20, 21, 22, 23, 28, 29, 30, 31]\n@@ -97,42 +112,147 @@ class AxisInfo {\n   /// dimension to the length of the shortest\n   /// sequence of constant integer along it. This is\n   /// particularly useful to infer the contiguity\n-  /// of operations (e.g., add) involving a constant\n+  /// of operations (e.g., add) involving a constant.\n+  /// Suppose we have an array of N elements,\n+  /// with a constancy value C,\n+  /// the array can be divided into a list of\n+  /// N/C sequences of C elements with the same value.\n+  /// Since we have N = 2^k, C must be a power of two.\n   /// For example\n   /// [8, 8, 8, 8, 12, 12, 12, 12]\n   /// [16, 16, 16, 16, 20, 20, 20, 20]\n   /// would have constancy [1, 4]\n   DimVectorT constancy;\n \n+  /// The constant value of the lattice if we can infer it.\n+  std::optional<int64_t> constantValue;\n+\n   // number of dimensions of the lattice\n-  int rank;\n+  int rank{};\n };\n \n-class AxisInfoAnalysis : public ForwardDataFlowAnalysis<AxisInfo> {\n+class AxisInfoVisitor {\n+public:\n+  AxisInfoVisitor() = default;\n+  virtual ~AxisInfoVisitor() = default;\n \n-private:\n-  static const int maxPow2Divisor = 65536;\n+  static bool isContiguousDim(const AxisInfo &info, ArrayRef<int64_t> shape,\n+                              int dim) {\n+    return info.getContiguity(dim) == shape[dim];\n+  }\n+\n+  static bool isConstantDim(const AxisInfo &info, ArrayRef<int64_t> shape,\n+                            int dim) {\n+    return info.getConstancy(dim) == shape[dim];\n+  }\n+\n+  virtual AxisInfo\n+  getAxisInfo(Operation *op, ArrayRef<LatticeElement<AxisInfo> *> operands) = 0;\n \n-  int highestPowOf2Divisor(int n) {\n-    if (n == 0)\n-      return maxPow2Divisor;\n-    return (n & (~(n - 1)));\n+  virtual bool match(Operation *op) = 0;\n+};\n+\n+/// Base class for all operations\n+template <typename OpTy> class AxisInfoVisitorImpl : public AxisInfoVisitor {\n+public:\n+  using AxisInfoVisitor::AxisInfoVisitor;\n+\n+  AxisInfo getAxisInfo(Operation *op,\n+                       ArrayRef<LatticeElement<AxisInfo> *> operands) final {\n+    return getAxisInfo(cast<OpTy>(op), operands);\n   }\n \n-  AxisInfo visitBinaryOp(\n-      Operation *op, AxisInfo lhsInfo, AxisInfo rhsInfo,\n-      const std::function<int(AxisInfo, AxisInfo, int)> &getContiguity,\n-      const std::function<int(AxisInfo, AxisInfo, int)> &getDivisibility,\n-      const std::function<int(AxisInfo, AxisInfo, int)> &getConstancy);\n+  bool match(Operation *op) final { return isa<OpTy>(op); }\n+\n+  virtual AxisInfo getAxisInfo(OpTy op,\n+                               ArrayRef<LatticeElement<AxisInfo> *> operands) {\n+    llvm_unreachable(\"Unimplemented getAxisInfo\");\n+  }\n+};\n+\n+/// Binary operations\n+template <typename OpTy>\n+class BinaryOpVisitorImpl : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n+\n+  AxisInfo getAxisInfo(OpTy op,\n+                       ArrayRef<LatticeElement<AxisInfo> *> operands) override {\n+    auto lhsInfo = operands[0]->getValue();\n+    auto rhsInfo = operands[1]->getValue();\n+    auto rank = lhsInfo.getRank();\n+    assert(operands.size() == 2 && \"Expected two operands\");\n+    AxisInfo::DimVectorT contiguity;\n+    AxisInfo::DimVectorT divisibility;\n+    AxisInfo::DimVectorT constancy;\n+    auto constantValue = getConstantValue(op, lhsInfo, rhsInfo);\n+    for (auto d = 0; d < rank; ++d) {\n+      if (constantValue.has_value()) {\n+        contiguity.push_back(1);\n+        constancy.push_back(\n+            std::max(lhsInfo.getConstancy(d), rhsInfo.getConstancy(d)));\n+        divisibility.push_back(highestPowOf2Divisor(constantValue.value()));\n+      } else {\n+        contiguity.push_back(getContiguity(op, lhsInfo, rhsInfo, d));\n+        constancy.push_back(getConstancy(op, lhsInfo, rhsInfo, d));\n+        divisibility.push_back(getDivisibility(op, lhsInfo, rhsInfo, d));\n+      }\n+    }\n+    return AxisInfo(contiguity, divisibility, constancy, constantValue);\n+  }\n+\n+protected:\n+  virtual int64_t getContiguity(OpTy op, const AxisInfo &lhs,\n+                                const AxisInfo &rhs, int dim) {\n+    return 1;\n+  }\n+\n+  virtual int64_t getDivisibility(OpTy op, const AxisInfo &lhs,\n+                                  const AxisInfo &rhs, int dim) {\n+    return 1;\n+  }\n+\n+  virtual int64_t getConstancy(OpTy op, const AxisInfo &lhs,\n+                               const AxisInfo &rhs, int dim) {\n+    return 1;\n+  }\n+\n+  virtual std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                                  const AxisInfo &rhs) {\n+    return {};\n+  }\n+};\n+\n+class AxisInfoVisitorList {\n+public:\n+  template <typename... Ts, typename = std::enable_if_t<sizeof...(Ts) != 0>>\n+  void append() {\n+    (visitors.emplace_back(std::make_unique<Ts>()), ...);\n+  }\n+\n+  AxisInfo apply(Operation *op, ArrayRef<LatticeElement<AxisInfo> *> operands) {\n+    for (auto &visitor : visitors)\n+      if (visitor->match(op))\n+        return visitor->getAxisInfo(op, operands);\n+    return AxisInfo();\n+  }\n+\n+private:\n+  std::vector<std::unique_ptr<AxisInfoVisitor>> visitors;\n+};\n+\n+class AxisInfoAnalysis : public ForwardDataFlowAnalysis<AxisInfo> {\n+private:\n+  AxisInfoVisitorList visitors;\n \n public:\n-  using ForwardDataFlowAnalysis<AxisInfo>::ForwardDataFlowAnalysis;\n+  AxisInfoAnalysis(MLIRContext *context);\n \n   ChangeResult\n   visitOperation(Operation *op,\n                  ArrayRef<LatticeElement<AxisInfo> *> operands) override;\n \n-  unsigned getPtrVectorSize(Value ptr);\n+  unsigned getPtrContiguity(Value ptr);\n \n   unsigned getPtrAlignment(Value ptr);\n "}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -77,6 +77,15 @@ SmallVector<RES_T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) {\n   return result;\n }\n \n+template <typename T> T highestPowOf2Divisor(T n) {\n+  if (n == 0) {\n+    return (static_cast<T>(1) << (sizeof(T) * 8 - 2));\n+  }\n+  return (n & (~(n - 1)));\n+}\n+\n+bool isSingleValue(Value value);\n+\n bool isMmaToDotShortcut(triton::gpu::MmaEncodingAttr &mmaLayout,\n                         triton::gpu::DotOperandEncodingAttr &dotOperandLayout);\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -31,7 +31,7 @@ SmallVector<unsigned> getWarpsPerCTA(const Attribute &layout);\n \n SmallVector<unsigned> getSizePerThread(const Attribute &layout);\n \n-SmallVector<unsigned> getContigPerThread(Attribute layout);\n+SmallVector<unsigned> getContigPerThread(const Attribute &layout);\n \n SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout);\n "}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 710, "deletions": 162, "changes": 872, "file_content_changes": "@@ -1,45 +1,54 @@\n #include \"mlir/Analysis/DataFlowAnalysis.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"llvm/Support/raw_ostream.h\"\n-#include <iostream>\n \n #include \"triton/Analysis/AxisInfo.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n \n namespace mlir {\n \n-//===----------------------------------------------------------------------===//\n-// AxisInfo\n-//===----------------------------------------------------------------------===//\n-\n // Function for extended Euclidean Algorithm\n-static int gcd_impl(int a, int b, int *x, int *y) {\n+static int64_t gcdImpl(int64_t a, int64_t b, int64_t *x, int64_t *y) {\n   // Base Case\n   if (a == 0) {\n     *x = 0;\n     *y = 1;\n     return b;\n   }\n-  int x1, y1; // To store results of recursive call\n-  int gcd = gcd_impl(b % a, a, &x1, &y1);\n+  int64_t x1, y1; // To store results of recursive call\n+  int64_t gcd = gcdImpl(b % a, a, &x1, &y1);\n   // Update x and y using results of\n   // recursive call\n   *x = y1 - (b / a) * x1;\n   *y = x1;\n   return gcd;\n }\n \n-static int gcd(int a, int b) {\n-  int x, y;\n-  return gcd_impl(a, b, &x, &y);\n+static int64_t gcd(int64_t a, int64_t b) {\n+  if (a == 0)\n+    return b;\n+  if (b == 0)\n+    return a;\n+  int64_t x, y;\n+  return gcdImpl(a, b, &x, &y);\n }\n \n+static constexpr int log2Int(int64_t num) {\n+  return (num > 1) ? 1 + log2Int(num / 2) : 0;\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// AxisInfo\n+//===----------------------------------------------------------------------===//\n+\n AxisInfo AxisInfo::getPessimisticValueState(Value value) {\n-  size_t rank = 1;\n+  auto rank = 1;\n   if (TensorType ty = value.getType().dyn_cast<TensorType>())\n     rank = ty.getRank();\n-  int divHint = 1;\n+  auto contiHint = 1;\n+  auto divHint = 1;\n+  auto constHint = 1;\n   BlockArgument blockArg = value.dyn_cast<BlockArgument>();\n   if (blockArg && blockArg.getOwner()->isEntryBlock()) {\n     Operation *op = blockArg.getOwner()->getParentOp();\n@@ -53,139 +62,342 @@ AxisInfo AxisInfo::getPessimisticValueState(Value value) {\n           fun.getArgAttr(blockArg.getArgNumber(), \"tt.divisibility\");\n       if (attr)\n         divHint = attr.cast<IntegerAttr>().getValue().getZExtValue();\n+    } else {\n+      // Derive the divisibility of the induction variable only when\n+      // the step and the lower bound are both constants\n+      if (auto forOp = dyn_cast<scf::ForOp>(op)) {\n+        if (blockArg == forOp.getInductionVar()) {\n+          if (auto lowerBound =\n+                  forOp.getLowerBound().getDefiningOp<arith::ConstantOp>()) {\n+            if (auto step =\n+                    forOp.getStep().getDefiningOp<arith::ConstantOp>()) {\n+              auto lowerBoundVal = lowerBound.getValue()\n+                                       .cast<IntegerAttr>()\n+                                       .getValue()\n+                                       .getZExtValue();\n+              auto stepVal =\n+                  step.getValue().cast<IntegerAttr>().getValue().getZExtValue();\n+              auto k = gcd(lowerBoundVal, stepVal);\n+              if (k != 0)\n+                divHint = k;\n+            }\n+          }\n+        }\n+      }\n     }\n   }\n-  DimVectorT contiguity(rank, 1);\n-  DimVectorT divisibility(rank, divHint);\n-  DimVectorT constancy(rank, 1);\n-  return AxisInfo(contiguity, divisibility, constancy);\n+\n+  return AxisInfo(/*knownContiguity=*/DimVectorT(rank, contiHint),\n+                  /*knownDivisibility=*/DimVectorT(rank, divHint),\n+                  /*knownConstancy=*/DimVectorT(rank, constHint));\n }\n \n // The gcd of both arguments for each dimension\n AxisInfo AxisInfo::join(const AxisInfo &lhs, const AxisInfo &rhs) {\n-  DimVectorT retContiguity;\n-  DimVectorT retDivisibility;\n-  DimVectorT retConstancy;\n-  for (int d = 0; d < lhs.getRank(); ++d) {\n-    retContiguity.push_back(gcd(lhs.getContiguity(d), rhs.getContiguity(d)));\n-    retDivisibility.push_back(\n-        gcd(lhs.getDivisibility(d), rhs.getDivisibility(d)));\n-    retConstancy.push_back(gcd(lhs.getConstancy(d), rhs.getConstancy(d)));\n-  }\n-  return AxisInfo(retContiguity, retDivisibility, retConstancy);\n+  DimVectorT contiguity;\n+  DimVectorT divisibility;\n+  DimVectorT constancy;\n+  for (auto d = 0; d < lhs.getRank(); ++d) {\n+    contiguity.push_back(gcd(lhs.getContiguity(d), rhs.getContiguity(d)));\n+    divisibility.push_back(gcd(lhs.getDivisibility(d), rhs.getDivisibility(d)));\n+    constancy.push_back(gcd(lhs.getConstancy(d), rhs.getConstancy(d)));\n+  }\n+  std::optional<int64_t> constantValue;\n+  if (lhs.getConstantValue().has_value() &&\n+      rhs.getConstantValue().has_value() &&\n+      lhs.getConstantValue() == rhs.getConstantValue())\n+    constantValue = lhs.getConstantValue();\n+  return AxisInfo(contiguity, divisibility, constancy, constantValue);\n }\n \n //===----------------------------------------------------------------------===//\n-// AxisInfoAnalysis\n+// AxisInfoVisitor\n //===----------------------------------------------------------------------===//\n \n-AxisInfo AxisInfoAnalysis::visitBinaryOp(\n-    Operation *op, AxisInfo lhsInfo, AxisInfo rhsInfo,\n-    const std::function<int(AxisInfo, AxisInfo, int)> &getContiguity,\n-    const std::function<int(AxisInfo, AxisInfo, int)> &getDivisibility,\n-    const std::function<int(AxisInfo, AxisInfo, int)> &getConstancy) {\n-  int rank = lhsInfo.getRank();\n-  AxisInfo::DimVectorT newContiguity;\n-  AxisInfo::DimVectorT newDivisibility;\n-  AxisInfo::DimVectorT newConstancy;\n-  for (int d = 0; d < rank; ++d) {\n-    newContiguity.push_back(getContiguity(lhsInfo, rhsInfo, d));\n-    newDivisibility.push_back(getDivisibility(lhsInfo, rhsInfo, d));\n-    newConstancy.push_back(getConstancy(lhsInfo, rhsInfo, d));\n-  }\n-  return AxisInfo(newContiguity, newDivisibility, newConstancy);\n-}\n+template <typename OpTy>\n+class CastOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n \n-ChangeResult AxisInfoAnalysis::visitOperation(\n-    Operation *op, ArrayRef<LatticeElement<AxisInfo> *> operands) {\n-  AxisInfo curr;\n-  // This preserves the input axes (e.g., cast):\n-  if (llvm::isa<arith::ExtSIOp, arith::ExtUIOp, arith::TruncIOp,\n-                triton::PtrToIntOp, triton::IntToPtrOp,\n-                triton::gpu::ConvertLayoutOp>(op))\n-    curr = operands[0]->getValue();\n-  // Constant ranges\n-  if (triton::MakeRangeOp make_range =\n-          llvm::dyn_cast<triton::MakeRangeOp>(op)) {\n-    int start = make_range.start();\n-    int end = make_range.end();\n-    AxisInfo::DimVectorT contiguity = {end - start};\n-    AxisInfo::DimVectorT divisibility = {highestPowOf2Divisor(start)};\n-    AxisInfo::DimVectorT constancy = {1};\n-    curr = AxisInfo(contiguity, divisibility, constancy);\n-  }\n-  // Constant\n-  if (arith::ConstantOp constant = llvm::dyn_cast<arith::ConstantOp>(op)) {\n-    auto intAttr = constant.getValue().dyn_cast<IntegerAttr>();\n-    if (intAttr) {\n-      size_t val = intAttr.getValue().getZExtValue();\n-      curr = AxisInfo({1}, {highestPowOf2Divisor(val)}, {1});\n+  AxisInfo getAxisInfo(OpTy op,\n+                       ArrayRef<LatticeElement<AxisInfo> *> operands) override {\n+    return operands[0]->getValue();\n+  }\n+};\n+\n+class MakeRangeOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<triton::MakeRangeOp> {\n+public:\n+  using AxisInfoVisitorImpl<triton::MakeRangeOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo getAxisInfo(triton::MakeRangeOp op,\n+                       ArrayRef<LatticeElement<AxisInfo> *> operands) override {\n+    auto start = op.start();\n+    auto end = op.end();\n+    return AxisInfo(/*contiguity=*/{end - start},\n+                    /*divisibility=*/{highestPowOf2Divisor(start)},\n+                    /*constancy=*/{1});\n+  }\n+};\n+\n+class ConstantOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<arith::ConstantOp> {\n+public:\n+  using AxisInfoVisitorImpl<arith::ConstantOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo getAxisInfo(arith::ConstantOp op,\n+                       ArrayRef<LatticeElement<AxisInfo> *> operands) override {\n+    auto intAttr = op.getValue().dyn_cast<IntegerAttr>();\n+    auto boolAttr = op.getValue().dyn_cast<BoolAttr>();\n+    if (intAttr || boolAttr) {\n+      int64_t value{};\n+      if (intAttr)\n+        value = intAttr.getValue().getZExtValue();\n+      else\n+        value = boolAttr.getValue() ? 1 : 0;\n+      return AxisInfo(/*contiguity=*/{1},\n+                      /*divisibility=*/{highestPowOf2Divisor(value)},\n+                      /*constancy=*/{1},\n+                      /*knownConstantValue=*/{value});\n     }\n     // TODO: generalize to dense attr\n-    auto splatAttr = constant.getValue().dyn_cast<SplatElementsAttr>();\n-    if (splatAttr && splatAttr.getElementType().isInteger(32)) {\n-      auto value = splatAttr.getSplatValue<int>();\n+    auto splatAttr = op.getValue().dyn_cast<SplatElementsAttr>();\n+    if (splatAttr && splatAttr.getElementType().isIntOrIndex()) {\n+      int64_t value = splatAttr.getSplatValue<APInt>().getZExtValue();\n       TensorType ty = splatAttr.getType().cast<TensorType>();\n-      curr = AxisInfo(\n-          AxisInfo::DimVectorT(ty.getRank(), 1),\n+      return AxisInfo(\n+          /*contiguity=*/AxisInfo::DimVectorT(ty.getRank(), 1),\n+          /*divisibility=*/\n           AxisInfo::DimVectorT(ty.getRank(), highestPowOf2Divisor(value)),\n-          AxisInfo::DimVectorT(ty.getShape().begin(), ty.getShape().end()));\n+          /*constancy=*/\n+          AxisInfo::DimVectorT(ty.getShape().begin(), ty.getShape().end()),\n+          /*knownConstantValue=*/{value});\n+    }\n+    return AxisInfo();\n+  }\n+};\n+\n+template <typename OpTy>\n+class AddSubOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                        int dim) override {\n+    return std::max(gcd(lhs.getConstancy(dim), rhs.getContiguity(dim)),\n+                    gcd(lhs.getContiguity(dim), rhs.getConstancy(dim)));\n+  }\n+\n+  int64_t getDivisibility(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                          int dim) override {\n+    // lhs = k * d_lhs = k * k' * gcd(d_lhs, d_rhs)\n+    // rhs = p * d_rhs = p * p' * gcd(d_lhs, d_rhs)\n+    // lhs + rhs = k * d_lhs + p * d_rhs = (k * d_lhs + p * d_rhs) *\n+    // gcd(d_lhs, d_rhs)\n+    return gcd(lhs.getDivisibility(dim), rhs.getDivisibility(dim));\n+  }\n+\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value()) {\n+      if constexpr (std::is_same_v<OpTy, arith::AddIOp> ||\n+                    std::is_same_v<OpTy, triton::AddPtrOp>) {\n+        return {lhs.getConstantValue().value() +\n+                rhs.getConstantValue().value()};\n+      } else if constexpr (std::is_same_v<OpTy, arith::SubIOp>) {\n+        return {lhs.getConstantValue().value() -\n+                rhs.getConstantValue().value()};\n+      }\n     }\n+    return {};\n   }\n-  // TODO: refactor & complete binary ops\n-  // Addition\n-  if (llvm::isa<arith::AddIOp, triton::AddPtrOp>(op)) {\n-    auto newContiguity = [&](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return std::max(gcd(lhs.getContiguity(d), rhs.getConstancy(d)),\n-                      gcd(lhs.getConstancy(d), rhs.getContiguity(d)));\n-    };\n-    auto newConstancy = [&](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getConstancy(d), rhs.getConstancy(d));\n-    };\n-    auto newDivisibility = [&](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getDivisibility(d), rhs.getDivisibility(d));\n-    };\n-    curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n-                         newContiguity, newDivisibility, newConstancy);\n-  }\n-  // Multiplication\n-  if (llvm::isa<arith::MulIOp>(op)) {\n-    auto newContiguity = [](AxisInfo lhs, AxisInfo rhs, int d) { return 1; };\n-    auto newConstancy = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getConstancy(d), rhs.getConstancy(d));\n-    };\n-    auto newDivisibility = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return lhs.getDivisibility(d) * rhs.getDivisibility(d);\n-    };\n-    curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n-                         newContiguity, newDivisibility, newConstancy);\n-  }\n-  // Remainder\n-  if (llvm::isa<arith::RemSIOp, arith::RemUIOp>(op)) {\n-    auto newContiguity = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getContiguity(d), rhs.getDivisibility(d));\n-    };\n-    auto newDivisibility = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getDivisibility(d), rhs.getDivisibility(d));\n-    };\n-    auto newConstancy = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getConstancy(d), rhs.getConstancy(d));\n-    };\n-    curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n-                         newContiguity, newDivisibility, newConstancy);\n-  }\n-  // TODO: All other binary ops\n-  if (llvm::isa<arith::AndIOp, arith::OrIOp>(op)) {\n-    auto newContiguity = [](AxisInfo lhs, AxisInfo rhs, int d) { return 1; };\n-    auto newDivisibility = [](AxisInfo lhs, AxisInfo rhs, int d) { return 1; };\n-    auto newConstancy = [](AxisInfo lhs, AxisInfo rhs, int d) {\n-      return gcd(lhs.getConstancy(d), rhs.getConstancy(d));\n-    };\n-    curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n-                         newContiguity, newDivisibility, newConstancy);\n-  }\n-  // Splat\n-  if (llvm::isa<triton::SplatOp>(op)) {\n+};\n+\n+class MulIOpAxisInfoVisitor final : public BinaryOpVisitorImpl<arith::MulIOp> {\n+public:\n+  using BinaryOpVisitorImpl<arith::MulIOp>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(arith::MulIOp op, const AxisInfo &lhs,\n+                        const AxisInfo &rhs, int dim) override {\n+    // lhs * 1 = lhs\n+    auto lhsContiguity =\n+        rhs.getConstantValue().has_value() && rhs.getConstantValue() == 1\n+            ? lhs.getContiguity(dim)\n+            : 1;\n+    // 1 * rhs = rhs\n+    auto rhsContiguity =\n+        lhs.getConstantValue().has_value() && lhs.getConstantValue() == 1\n+            ? rhs.getContiguity(dim)\n+            : 1;\n+    return std::max(lhsContiguity, rhsContiguity);\n+  }\n+\n+  int64_t getConstancy(arith::MulIOp op, const AxisInfo &lhs,\n+                       const AxisInfo &rhs, int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  int64_t getDivisibility(arith::MulIOp op, const AxisInfo &lhs,\n+                          const AxisInfo &rhs, int dim) override {\n+    // lhs = k * d_lhs\n+    // rhs = p * d_rhs\n+    // lhs * rhs = k * d_lhs * p * d_rhs = k * p * d_lhs * d_rhs\n+    return lhs.getDivisibility(dim) * rhs.getDivisibility(dim);\n+  }\n+\n+  std::optional<int64_t> getConstantValue(arith::MulIOp op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() * rhs.getConstantValue().value()};\n+    return {};\n+  }\n+};\n+\n+template <typename OpTy>\n+class DivOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                        int dim) override {\n+    // lhs / 1 = lhs\n+    return rhs.getConstantValue().has_value() &&\n+                   rhs.getConstantValue().value() == 1\n+               ? lhs.getContiguity(dim)\n+               : 1;\n+  }\n+\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return BinaryOpVisitorImpl<OpTy>::getConstancy(op, lhs, rhs, dim);\n+    auto shape = resTy.getShape();\n+    // Case 1: both lhs and rhs are constants.\n+    auto constancy = gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+    // Case 2: lhs contiguous, rhs constant.\n+    // lhs: d_lhs * k, d_lhs * k + 1, ..., d_lhs * k + n\n+    // rhs: d_rhs * p, d_rhs * p, ..., d_rhs * p\n+    // lhs / rhs = d_lhs * k / (d_rhs * p), (d_lhs * k + 1) / (d_rhs * p),\n+    // ..., (d_lhs * k + n) / (d_rhs * p)\n+    // Because d_lhs % d_rhs = 0 || d_rhs % d_lhs = 0,\n+    // the minimal constancy is gcd(d_lhs, d_rhs).\n+    // Since gcd(d_lhs, d_rhs) maybe > len(lhs),\n+    // we need to use another gcd to get the actual constancy.\n+    if (AxisInfoVisitor::isContiguousDim(lhs, shape, dim) &&\n+        AxisInfoVisitor::isConstantDim(rhs, shape, dim)) {\n+      constancy = std::max(constancy, gcd(lhs.getContiguity(dim),\n+                                          gcd(lhs.getDivisibility(dim),\n+                                              rhs.getDivisibility(dim))));\n+    }\n+    return constancy;\n+  }\n+\n+  int64_t getDivisibility(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                          int dim) override {\n+    // lhs = k * d_lhs = k * k' * gcd(d_lhs, d_rhs)\n+    // rhs = p * d_rhs = p * p' * gcd(d_lhs, d_rhs)\n+    // lhs / rhs = k * k' * gcd(d_lhs, d_rhs) / (p * p' * gcd(d_lhs, d_rhs))\n+    //           = k / p * k' / p'\n+    // gcd(k', p') = divisibility(d_lhs / gcd(d_lhs, d_rhs), d_rhs / gcd(d_lhs,\n+    // d_rhs))\n+    auto lhsDivisibility = lhs.getDivisibility(dim);\n+    auto rhsDivisibility = rhs.getDivisibility(dim);\n+    auto initGcd = gcd(lhsDivisibility, rhsDivisibility);\n+    return std::max(lhsDivisibility / initGcd, rhsDivisibility / initGcd);\n+  };\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() / rhs.getConstantValue().value()};\n+    return {};\n+  }\n+};\n+\n+template <typename OpTy>\n+class RemOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                        int dim) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return BinaryOpVisitorImpl<OpTy>::getContiguity(op, lhs, rhs, dim);\n+    auto shape = resTy.getShape();\n+    int64_t contiguity = 1;\n+    // lhs contiguous, rhs constant\n+    // lhs: d_lhs * k, d_lhs * k + 1, ..., d_lhs * k + n\n+    // rhs: d_rhs * p, d_rhs * p, ..., d_rhs * p\n+    // lhs % rhs = d_lhs * k % (d_rhs * p), (d_lhs * k + 1) % (d_rhs * p),\n+    // ..., (d_lhs * k + n) % (d_rhs * p)\n+    // Because d_lhs % d_rhs = 0 || d_rhs % d_lhs = 0,\n+    // The minimal contiguity is gcd(d_lhs, d_rhs).\n+    // Since gcd(d_lhs, d_rhs) maybe > len(lhs),\n+    // we need to use another gcd to get the actual contiguity.\n+    if (AxisInfoVisitor::isContiguousDim(lhs, shape, dim) &&\n+        AxisInfoVisitor::isConstantDim(rhs, shape, dim)) {\n+      contiguity = std::max(contiguity, gcd(lhs.getContiguity(dim),\n+                                            gcd(lhs.getDivisibility(dim),\n+                                                rhs.getDivisibility(dim))));\n+    }\n+    return contiguity;\n+  }\n+\n+  int64_t getDivisibility(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                          int dim) override {\n+    // lhs: d_lhs * k = gcd(d_lhs, d_rhs) * k' * k = gcd(d_lhs, d_rhs) * k''\n+    // rhs: d_rhs * p = gcd(d_lhs, d_rhs) * p' * p = gcd(d_lhs, d_rhs) * p''\n+    // lhs = gcd(d_lhs, d_rhs) * k'' = gcd(d_lhs, d_rhs) * d + r\n+    // r must be divisible by gcd(d_lhs, d_rhs)\n+    return gcd(lhs.getDivisibility(dim), rhs.getDivisibility(dim));\n+  };\n+\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return BinaryOpVisitorImpl<OpTy>::getConstancy(op, lhs, rhs, dim);\n+    auto shape = resTy.getShape();\n+    // lhs % 1 = 0\n+    return rhs.getConstantValue().has_value() &&\n+                   rhs.getConstantValue().value() == 1\n+               ? shape[dim]\n+               : gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() % rhs.getConstantValue().value()};\n+    else if (rhs.getConstantValue().has_value() &&\n+             rhs.getConstantValue().value() == 1)\n+      return {0};\n+    return {};\n+  }\n+};\n+\n+class SplatOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<triton::SplatOp> {\n+public:\n+  using AxisInfoVisitorImpl<triton::SplatOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo getAxisInfo(triton::SplatOp op,\n+                       ArrayRef<LatticeElement<AxisInfo> *> operands) override {\n     Type _retTy = *op->result_type_begin();\n     TensorType retTy = _retTy.cast<TensorType>();\n     AxisInfo opInfo = operands[0]->getValue();\n@@ -197,21 +409,37 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n       divisibility.push_back(opInfo.getDivisibility(0));\n       constancy.push_back(retTy.getShape()[d]);\n     }\n-    curr = AxisInfo(contiguity, divisibility, constancy);\n+    return AxisInfo(contiguity, divisibility, constancy,\n+                    operands[0]->getValue().getConstantValue());\n   }\n-  // expandDims\n-  if (auto expandDims = llvm::dyn_cast<triton::ExpandDimsOp>(op)) {\n+};\n+\n+class ExpandDimsOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<triton::ExpandDimsOp> {\n+public:\n+  using AxisInfoVisitorImpl<triton::ExpandDimsOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo getAxisInfo(triton::ExpandDimsOp op,\n+                       ArrayRef<LatticeElement<AxisInfo> *> operands) override {\n     AxisInfo opInfo = operands[0]->getValue();\n     AxisInfo::DimVectorT contiguity = opInfo.getContiguity();\n     AxisInfo::DimVectorT divisibility = opInfo.getDivisibility();\n     AxisInfo::DimVectorT constancy = opInfo.getConstancy();\n-    contiguity.insert(contiguity.begin() + expandDims.axis(), 1);\n-    divisibility.insert(divisibility.begin() + expandDims.axis(), 1);\n-    constancy.insert(constancy.begin() + expandDims.axis(), 1);\n-    curr = AxisInfo(contiguity, divisibility, constancy);\n+    contiguity.insert(contiguity.begin() + op.axis(), 1);\n+    divisibility.insert(divisibility.begin() + op.axis(), 1);\n+    constancy.insert(constancy.begin() + op.axis(), 1);\n+    return AxisInfo(contiguity, divisibility, constancy,\n+                    operands[0]->getValue().getConstantValue());\n   }\n-  // Broadcast\n-  if (llvm::isa<triton::BroadcastOp>(op)) {\n+};\n+\n+class BroadcastOpAxisInfoVisitor final\n+    : public AxisInfoVisitorImpl<triton::BroadcastOp> {\n+public:\n+  using AxisInfoVisitorImpl<triton::BroadcastOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo getAxisInfo(triton::BroadcastOp op,\n+                       ArrayRef<LatticeElement<AxisInfo> *> operands) override {\n     Type _retTy = *op->result_type_begin();\n     Type _opTy = *op->operand_type_begin();\n     TensorType retTy = _retTy.cast<TensorType>();\n@@ -228,42 +456,362 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n       constancy.push_back(opShape[d] == 1 ? retShape[d]\n                                           : opInfo.getConstancy(d));\n     }\n-    curr = AxisInfo(contiguity, divisibility, constancy);\n+    return AxisInfo(contiguity, divisibility, constancy,\n+                    operands[0]->getValue().getConstantValue());\n   }\n+};\n \n-  // CmpI\n-  if ((llvm::dyn_cast<arith::CmpIOp>(op) ||\n-       llvm::dyn_cast<triton::gpu::CmpIOp>(op)) &&\n-      op->getResult(0).getType().dyn_cast<TensorType>()) {\n-    auto resTy = op->getResult(0).getType().cast<TensorType>();\n+template <typename OpTy>\n+class CmpOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n+\n+  AxisInfo getAxisInfo(OpTy op,\n+                       ArrayRef<LatticeElement<AxisInfo> *> operands) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return AxisInfo();\n+    auto shape = resTy.getShape();\n     short rank = resTy.getRank();\n     auto lhsInfo = operands[0]->getValue();\n     auto rhsInfo = operands[1]->getValue();\n-    auto shape = resTy.getShape();\n \n     AxisInfo::DimVectorT contiguity, divisibility, constancy;\n+    std::optional<int64_t> constantValue;\n     for (short d = 0; d < rank; ++d) {\n-      if (rhsInfo.getConstancy(d) % lhsInfo.getContiguity(d) == 0 ||\n-          rhsInfo.getConstancy(d) % lhsInfo.getConstancy(d))\n-        constancy.push_back(\n-            gcd(lhsInfo.getDivisibility(d), rhsInfo.getDivisibility(d)));\n-      else\n-        constancy.push_back(1);\n+      int64_t constHint = 1;\n+      if (lhsInfo.getConstantValue().has_value() &&\n+          rhsInfo.getConstantValue().has_value()) {\n+        constHint = lhsInfo.getConstancy(d);\n+        constantValue =\n+            compare(getPredicate(op), lhsInfo.getConstantValue().value(),\n+                    rhsInfo.getConstantValue().value())\n+                ? 1\n+                : 0;\n+      } else {\n+        // Case 1: lhs and rhs are both partial constants\n+        constHint = gcd(lhsInfo.getConstancy(d), rhsInfo.getConstancy(d));\n+        // Case 2: lhs all constant, rhs all contiguous\n+        // NOTE:\n+        // lhs: 4 4 4 4\n+        // rhs: 4 5 6 7\n+        // lhs ge rhs: 1, 0, 0, 0\n+        // Case 3: lhs all contiguous, rhs all constant\n+        // NOTE\n+        // lhs: 4 5 6 7\n+        // rhs: 4 4 4 4\n+        // lhs sle rhs: 1, 0, 0, 0\n+        if (/*Case 2=*/(\n+                notGePredicate(getPredicate(op)) &&\n+                (AxisInfoVisitor::isConstantDim(lhsInfo, shape, d) &&\n+                 AxisInfoVisitor::isContiguousDim(rhsInfo, shape, d))) ||\n+            /*Case 3=*/(notLePredicate(getPredicate(op)) &&\n+                        (AxisInfoVisitor::isContiguousDim(lhsInfo, shape, d) &&\n+                         AxisInfoVisitor::isConstantDim(rhsInfo, shape, d)))) {\n+          constHint = std::max(constHint, gcd(lhsInfo.getContiguity(d),\n+                                              gcd(lhsInfo.getDivisibility(d),\n+                                                  rhsInfo.getDivisibility(d))));\n+        }\n+      }\n \n-      divisibility.push_back(shape[d]);\n+      constancy.push_back(constHint);\n+      divisibility.push_back(1);\n       contiguity.push_back(1);\n     }\n \n-    curr = AxisInfo(contiguity, divisibility, constancy);\n+    return AxisInfo(contiguity, divisibility, constancy, constantValue);\n+  }\n+\n+private:\n+  static arith::CmpIPredicate getPredicate(triton::gpu::CmpIOp op) {\n+    return op.predicate();\n+  }\n+\n+  static arith::CmpIPredicate getPredicate(arith::CmpIOp op) {\n+    return op.getPredicate();\n+  }\n+\n+  static bool notGePredicate(arith::CmpIPredicate predicate) {\n+    return predicate != arith::CmpIPredicate::sge &&\n+           predicate != arith::CmpIPredicate::uge;\n+  }\n+\n+  static bool notLePredicate(arith::CmpIPredicate predicate) {\n+    return predicate != arith::CmpIPredicate::sle &&\n+           predicate != arith::CmpIPredicate::ule;\n+  }\n+\n+  static bool compare(arith::CmpIPredicate predicate, int64_t lhs,\n+                      int64_t rhs) {\n+    switch (predicate) {\n+    case arith::CmpIPredicate::eq:\n+      return lhs == rhs;\n+    case arith::CmpIPredicate::ne:\n+      return lhs != rhs;\n+    case arith::CmpIPredicate::slt:\n+      return lhs < rhs;\n+    case arith::CmpIPredicate::sle:\n+      return lhs <= rhs;\n+    case arith::CmpIPredicate::sgt:\n+      return lhs > rhs;\n+    case arith::CmpIPredicate::sge:\n+      return lhs >= rhs;\n+    case arith::CmpIPredicate::ult:\n+      return (uint64_t)lhs < (uint64_t)rhs;\n+    case arith::CmpIPredicate::ule:\n+      return (uint64_t)lhs <= (uint64_t)rhs;\n+    case arith::CmpIPredicate::ugt:\n+      return (uint64_t)lhs > (uint64_t)rhs;\n+    case arith::CmpIPredicate::uge:\n+      return (uint64_t)lhs >= (uint64_t)rhs;\n+    default:\n+      break;\n+    }\n+    llvm_unreachable(\"unknown comparison predicate\");\n+  }\n+};\n+\n+template <typename OpTy>\n+class SelectOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n+\n+  AxisInfo getAxisInfo(OpTy op,\n+                       ArrayRef<LatticeElement<AxisInfo> *> operands) override {\n+    auto resTy = op.getResult().getType().template dyn_cast<RankedTensorType>();\n+    if (!resTy)\n+      return AxisInfo();\n+    auto shape = resTy.getShape();\n+    auto rank = shape.size();\n+    auto condConstancy = operands[0]->getValue().getConstancy();\n+    auto lhsInfo = operands[1]->getValue();\n+    auto rhsInfo = operands[2]->getValue();\n+\n+    AxisInfo::DimVectorT contiguity, divisibility, constancy;\n+    std::optional<int64_t> constantValue;\n+    if (operands[0]->getValue().getConstantValue().has_value()) {\n+      if (operands[0]->getValue().getConstantValue() == 0) {\n+        contiguity = rhsInfo.getContiguity();\n+        divisibility = rhsInfo.getDivisibility();\n+        constancy = rhsInfo.getConstancy();\n+        constantValue = rhsInfo.getConstantValue();\n+      } else {\n+        contiguity = lhsInfo.getContiguity();\n+        divisibility = lhsInfo.getDivisibility();\n+        constancy = lhsInfo.getConstancy();\n+        constantValue = lhsInfo.getConstantValue();\n+      }\n+    } else {\n+      for (auto d = 0; d < rank; ++d) {\n+        constancy.push_back(\n+            std::min(gcd(lhsInfo.getConstancy(d), condConstancy[d]),\n+                     gcd(rhsInfo.getConstancy(d), condConstancy[d])));\n+        divisibility.push_back(\n+            std::min(lhsInfo.getDivisibility(d), rhsInfo.getDivisibility(d)));\n+        contiguity.push_back(\n+            std::min(gcd(lhsInfo.getContiguity(d), condConstancy[d]),\n+                     gcd(rhsInfo.getContiguity(d), condConstancy[d])));\n+      }\n+      if (lhsInfo.getConstantValue().has_value() &&\n+          rhsInfo.getConstantValue().has_value() &&\n+          lhsInfo.getConstantValue() == rhsInfo.getConstantValue())\n+        constantValue = lhsInfo.getConstantValue();\n+    }\n+\n+    return AxisInfo(contiguity, divisibility, constancy, constantValue);\n+  }\n+};\n+\n+template <typename OpTy>\n+class LogicalOpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value()) {\n+      if constexpr (std::is_same<OpTy, arith::AndIOp>::value) {\n+        return {lhs.getConstantValue().value() &\n+                rhs.getConstantValue().value()};\n+      } else if constexpr (std::is_same<OpTy, arith::OrIOp>::value) {\n+        return {lhs.getConstantValue().value() |\n+                rhs.getConstantValue().value()};\n+      } else if constexpr (std::is_same<OpTy, arith::XOrIOp>::value) {\n+        return {lhs.getConstantValue().value() ^\n+                rhs.getConstantValue().value()};\n+      }\n+    }\n+    return {};\n   }\n+};\n \n-  // UnrealizedConversionCast\n+class ShLIOpAxisInfoVisitor final : public BinaryOpVisitorImpl<arith::ShLIOp> {\n+public:\n+  using BinaryOpVisitorImpl<arith::ShLIOp>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(arith::ShLIOp op, const AxisInfo &lhs,\n+                        const AxisInfo &rhs, int dim) override {\n+    if (rhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().value() == 0)\n+      return lhs.getContiguity(dim);\n+    else\n+      return 1;\n+  }\n+\n+  int64_t getDivisibility(arith::ShLIOp op, const AxisInfo &lhs,\n+                          const AxisInfo &rhs, int dim) override {\n+    auto shift = rhs.getConstantValue().has_value()\n+                     ? rhs.getConstantValue().value()\n+                     : rhs.getDivisibility(dim);\n+    auto numBits = log2Int(lhs.getDivisibility(dim));\n+    auto maxBits = log2Int(highestPowOf2Divisor<int64_t>(0));\n+    // Make sure the return value doesn't exceed highestPowOf2Divisor<int64>(0)\n+    if (shift + numBits > maxBits)\n+      return highestPowOf2Divisor<int64_t>(0);\n+    return lhs.getDivisibility(dim) << shift;\n+  }\n+\n+  int64_t getConstancy(arith::ShLIOp op, const AxisInfo &lhs,\n+                       const AxisInfo &rhs, int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(arith::ShLIOp op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() << rhs.getConstantValue().value()};\n+    return {};\n+  }\n+};\n+\n+template <typename OpTy>\n+class ShROpAxisInfoVisitor final : public BinaryOpVisitorImpl<OpTy> {\n+public:\n+  using BinaryOpVisitorImpl<OpTy>::BinaryOpVisitorImpl;\n+\n+private:\n+  int64_t getContiguity(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                        int dim) override {\n+    if (rhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().value() == 0)\n+      return lhs.getContiguity(dim);\n+    else\n+      return 1;\n+  }\n+\n+  int64_t getDivisibility(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                          int dim) override {\n+    if (rhs.getConstantValue().has_value())\n+      return std::max<int64_t>(1, lhs.getDivisibility(dim) /\n+                                      (1 << rhs.getConstantValue().value()));\n+    else\n+      return std::max<int64_t>(1, lhs.getDivisibility(dim) /\n+                                      (1 << rhs.getDivisibility(dim)));\n+  }\n+\n+  int64_t getConstancy(OpTy op, const AxisInfo &lhs, const AxisInfo &rhs,\n+                       int dim) override {\n+    return gcd(lhs.getConstancy(dim), rhs.getConstancy(dim));\n+  }\n+\n+  std::optional<int64_t> getConstantValue(OpTy op, const AxisInfo &lhs,\n+                                          const AxisInfo &rhs) override {\n+    if (lhs.getConstantValue().has_value() &&\n+        rhs.getConstantValue().has_value())\n+      return {lhs.getConstantValue().value() >> rhs.getConstantValue().value()};\n+    return {};\n+  }\n+};\n+\n+template <typename OpTy>\n+class MaxMinOpAxisInfoVisitor final : public AxisInfoVisitorImpl<OpTy> {\n+public:\n+  using AxisInfoVisitorImpl<OpTy>::AxisInfoVisitorImpl;\n+\n+  AxisInfo getAxisInfo(OpTy op,\n+                       ArrayRef<LatticeElement<AxisInfo> *> operands) override {\n+    auto lhsInfo = operands[0]->getValue();\n+    auto rhsInfo = operands[1]->getValue();\n+    std::optional<int64_t> constantValue;\n+    if (lhsInfo.getConstantValue().has_value() &&\n+        rhsInfo.getConstantValue().has_value()) {\n+      if constexpr (std::is_same_v<OpTy, arith::MaxSIOp> ||\n+                    std::is_same_v<OpTy, arith::MaxUIOp>) {\n+        constantValue = {std::max(lhsInfo.getConstantValue().value(),\n+                                  rhsInfo.getConstantValue().value())};\n+      } else if constexpr (std::is_same_v<OpTy, arith::MinSIOp> ||\n+                           std::is_same_v<OpTy, arith::MinUIOp>) {\n+        constantValue = {std::min(lhsInfo.getConstantValue().value(),\n+                                  rhsInfo.getConstantValue().value())};\n+      }\n+    }\n+    auto rank = lhsInfo.getRank();\n+    return AxisInfo(/*knownContiguity=*/AxisInfo::DimVectorT(rank, 1),\n+                    /*knownDivisibility=*/AxisInfo::DimVectorT(rank, 1),\n+                    /*knownConstancy=*/AxisInfo::DimVectorT(rank, 1),\n+                    /*constantValue=*/constantValue);\n+  }\n+};\n+\n+//===----------------------------------------------------------------------===//\n+// AxisInfoAnalysis\n+//===----------------------------------------------------------------------===//\n+\n+AxisInfoAnalysis::AxisInfoAnalysis(MLIRContext *context)\n+    : ForwardDataFlowAnalysis<AxisInfo>(context) {\n+  // UnrealizedConversionCast:\n   // This is needed by TritonGPUToLLVM, to get AxisInfo when the graph is\n   // in the process of a PartialConversion, where UnrealizedConversionCast\n   // may exist\n-  if (llvm::isa<mlir::UnrealizedConversionCastOp>(op)) {\n-    curr = operands[0]->getValue();\n-  }\n+  visitors.append<CastOpAxisInfoVisitor<arith::ExtSIOp>,\n+                  CastOpAxisInfoVisitor<arith::ExtUIOp>,\n+                  CastOpAxisInfoVisitor<arith::TruncIOp>,\n+                  CastOpAxisInfoVisitor<arith::IndexCastOp>,\n+                  CastOpAxisInfoVisitor<triton::PtrToIntOp>,\n+                  CastOpAxisInfoVisitor<triton::IntToPtrOp>,\n+                  CastOpAxisInfoVisitor<triton::gpu::ConvertLayoutOp>,\n+                  CastOpAxisInfoVisitor<mlir::UnrealizedConversionCastOp>,\n+                  CastOpAxisInfoVisitor<triton::BitcastOp>>();\n+  visitors.append<MakeRangeOpAxisInfoVisitor>();\n+  visitors.append<ConstantOpAxisInfoVisitor>();\n+  visitors.append<AddSubOpAxisInfoVisitor<triton::AddPtrOp>,\n+                  AddSubOpAxisInfoVisitor<arith::AddIOp>,\n+                  AddSubOpAxisInfoVisitor<arith::SubIOp>>();\n+  visitors.append<MulIOpAxisInfoVisitor>();\n+  visitors.append<DivOpAxisInfoVisitor<arith::DivSIOp>,\n+                  DivOpAxisInfoVisitor<arith::DivUIOp>>();\n+  visitors.append<RemOpAxisInfoVisitor<arith::RemSIOp>,\n+                  RemOpAxisInfoVisitor<arith::RemUIOp>>();\n+  visitors.append<BroadcastOpAxisInfoVisitor>();\n+  visitors.append<SplatOpAxisInfoVisitor>();\n+  visitors.append<ExpandDimsOpAxisInfoVisitor>();\n+  visitors.append<CmpOpAxisInfoVisitor<arith::CmpIOp>,\n+                  CmpOpAxisInfoVisitor<triton::gpu::CmpIOp>>();\n+  visitors.append<LogicalOpAxisInfoVisitor<arith::AndIOp>,\n+                  LogicalOpAxisInfoVisitor<arith::OrIOp>,\n+                  LogicalOpAxisInfoVisitor<arith::XOrIOp>>();\n+  visitors.append<SelectOpAxisInfoVisitor<mlir::SelectOp>,\n+                  SelectOpAxisInfoVisitor<triton::gpu::SelectOp>>();\n+  visitors.append<ShLIOpAxisInfoVisitor, ShROpAxisInfoVisitor<arith::ShRUIOp>,\n+                  ShROpAxisInfoVisitor<arith::ShRSIOp>>();\n+  visitors.append<MaxMinOpAxisInfoVisitor<arith::MaxSIOp>,\n+                  MaxMinOpAxisInfoVisitor<arith::MaxUIOp>,\n+                  MaxMinOpAxisInfoVisitor<arith::MinSIOp>,\n+                  MaxMinOpAxisInfoVisitor<arith::MinUIOp>>();\n+}\n+\n+ChangeResult AxisInfoAnalysis::visitOperation(\n+    Operation *op, ArrayRef<LatticeElement<AxisInfo> *> operands) {\n+  AxisInfo curr = visitors.apply(op, operands);\n   if (curr.getRank() == 0) {\n     return markAllPessimisticFixpoint(op->getResults());\n   }\n@@ -276,7 +824,7 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n   return result;\n }\n \n-unsigned AxisInfoAnalysis::getPtrVectorSize(Value ptr) {\n+unsigned AxisInfoAnalysis::getPtrContiguity(Value ptr) {\n   auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n   if (!tensorTy)\n     return 1;\n@@ -289,10 +837,10 @@ unsigned AxisInfoAnalysis::getPtrVectorSize(Value ptr) {\n   unsigned align = getPtrAlignment(ptr);\n \n   unsigned contigPerThread = triton::gpu::getSizePerThread(layout)[order[0]];\n-  unsigned vec = std::min(align, contigPerThread);\n-  vec = std::min<unsigned>(shape[order[0]], vec);\n+  contigPerThread = std::min(align, contigPerThread);\n+  contigPerThread = std::min<unsigned>(shape[order[0]], contigPerThread);\n \n-  return vec;\n+  return contigPerThread;\n }\n \n unsigned AxisInfoAnalysis::getPtrAlignment(Value ptr) {\n@@ -302,8 +850,8 @@ unsigned AxisInfoAnalysis::getPtrAlignment(Value ptr) {\n   auto axisInfo = lookupLatticeElement(ptr)->getValue();\n   auto layout = tensorTy.getEncoding();\n   auto order = triton::gpu::getOrder(layout);\n-  unsigned maxMultiple = axisInfo.getDivisibility(order[0]);\n-  unsigned maxContig = axisInfo.getContiguity(order[0]);\n+  auto maxMultiple = axisInfo.getDivisibility(order[0]);\n+  auto maxContig = axisInfo.getContiguity(order[0]);\n   unsigned alignment = std::min(maxMultiple, maxContig);\n   return alignment;\n }"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "file_content_changes": "@@ -165,6 +165,19 @@ bool isMmaToDotShortcut(triton::gpu::MmaEncodingAttr &mmaLayout,\n          dotOperandLayout.getParent() == mmaLayout;\n }\n \n+bool isSingleValue(Value value) {\n+  // Don't consider load as expensive if it is loading a scalar.\n+  if (auto tensorTy = value.getType().dyn_cast<RankedTensorType>())\n+    return tensorTy.getNumElements() == 1;\n+  // TODO: Handle other cases.\n+  // For example, when ptr is a tensor of single value.\n+  // It means that ptr is a resultant of broadcast or generated through\n+  // a chain of broadcast and other operations.\n+  // Rematerialize it without considering contiguous memory access pattern is\n+  // fine.\n+  return true;\n+}\n+\n namespace {\n \n /// A data structure similar to SetVector but maintains"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 23, "deletions": 2, "changes": 25, "file_content_changes": "@@ -31,8 +31,26 @@ struct LoadStoreConversionBase {\n     return valueVals;\n   }\n \n+  unsigned getContiguity(Value ptr) const {\n+    auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorTy)\n+      return 1;\n+    return axisAnalysisPass.getPtrContiguity(ptr);\n+  }\n+\n   unsigned getVectorSize(Value ptr) const {\n-    return axisAnalysisPass.getPtrVectorSize(ptr);\n+    auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorTy)\n+      return 1;\n+    auto contiguity = getContiguity(ptr);\n+    unsigned numElemBits = 0;\n+    auto ptrTy = tensorTy.getElementType().cast<triton::PointerType>();\n+    auto pointeeType = ptrTy.getPointeeType();\n+    numElemBits = pointeeType.isa<triton::Float8Type>()\n+                      ? 8\n+                      : pointeeType.getIntOrFloatBitWidth();\n+    // The maximum vector size is 128 bits on NVIDIA GPUs.\n+    return std::min<unsigned>(128 / numElemBits, contiguity);\n   }\n \n   unsigned getMaskAlignment(Value mask) const {\n@@ -734,7 +752,10 @@ struct InsertSliceAsyncOpConversion\n       assert(srcElems.size() == otherElems.size());\n     }\n \n-    unsigned inVec = getVectorSize(src);\n+    // We don't use getVec() here because we are copying from memory to memory.\n+    // If contiguity > vector size, we can have one pointer maintaining the\n+    // start of the vector and the other pointer moving to the next vector.\n+    unsigned inVec = getContiguity(src);\n     unsigned outVec = resSharedLayout.getVec();\n     unsigned minVec = std::min(outVec, inVec);\n     unsigned numElems = getElemsPerThread(srcTy);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -342,7 +342,7 @@ class ConvertTritonGPUToLLVM\n       auto resSharedLayout =\n           dstTy.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n       auto resElemTy = dstTy.getElementType();\n-      unsigned inVec = axisInfoAnalysis.getPtrVectorSize(src);\n+      unsigned inVec = axisInfoAnalysis.getPtrContiguity(src);\n       unsigned outVec = resSharedLayout.getVec();\n       unsigned minVec = std::min(outVec, inVec);\n       auto maxBitWidth ="}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -143,7 +143,7 @@ SmallVector<unsigned> getSizePerThread(const Attribute &layout) {\n   }\n }\n \n-SmallVector<unsigned> getContigPerThread(Attribute layout) {\n+SmallVector<unsigned> getContigPerThread(const Attribute &layout) {\n   if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     assert(mmaLayout.isVolta() || mmaLayout.isAmpere());\n     return {1, 2};"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 24, "deletions": 14, "changes": 38, "file_content_changes": "@@ -281,26 +281,35 @@ LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n   return success();\n }\n \n-// TODO: Interface\n-LogicalResult getForwardEncoding(Attribute sourceEncoding, Operation *op,\n-                                 Attribute &ret) {\n-  if (op->hasTrait<mlir::OpTrait::Elementwise>()) {\n-    ret = sourceEncoding;\n-    return success();\n+inline bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n+  // Case 1: A size 1 tensor is not expensive since all threads will load the\n+  // same\n+  if (isSingleValue(op->getOperand(0)))\n+    return false;\n+  auto ptr = op->getOperand(0);\n+  if (auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>()) {\n+    auto encoding = tensorTy.getEncoding();\n+    // Case 2: Different type conversion is expensive (e.g., mma <-> block)\n+    if (encoding.getTypeID() != targetEncoding.getTypeID())\n+      return true;\n+    auto sizePerThread = triton::gpu::getSizePerThread(encoding);\n+    auto targetSizePerThread = triton::gpu::getSizePerThread(targetEncoding);\n+    auto order = triton::gpu::getOrder(encoding);\n+    auto targetOrder = triton::gpu::getOrder(targetEncoding);\n+    // Case 3: The targeEncoding may expose more vectorization opportunities\n+    return sizePerThread[order[0]] >= targetSizePerThread[targetOrder[0]];\n   }\n-  if (isa<triton::ReduceOp>(op)) {\n-    ret = Attribute();\n-    return success();\n-  }\n-  return failure();\n+  return false;\n }\n \n-inline bool expensiveToRemat(Operation *op, const Attribute &targetEncoding) {\n+inline bool expensiveToRemat(Operation *op, Attribute &targetEncoding) {\n   if (!op)\n     return true;\n+  if (isa<triton::LoadOp, triton::StoreOp>(op))\n+    return expensiveLoadOrStore(op, targetEncoding);\n   if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n-          triton::gpu::InsertSliceAsyncOp, triton::LoadOp, triton::StoreOp,\n-          triton::AtomicRMWOp, triton::AtomicCASOp, triton::DotOp>(op))\n+          triton::gpu::InsertSliceAsyncOp, triton::AtomicRMWOp,\n+          triton::AtomicCASOp, triton::DotOp>(op))\n     return true;\n   if (isa<scf::YieldOp, scf::ForOp, scf::IfOp, scf::WhileOp, scf::ConditionOp>(\n           op))\n@@ -509,6 +518,7 @@ class FoldConvertAndReduce : public mlir::RewritePattern {\n         cvt.getOperand().getType().cast<RankedTensorType>().getEncoding();\n     auto dstEncoding =\n         cvt.getResult().getType().cast<RankedTensorType>().getEncoding();\n+    // XXX: why is this needed?\n     if (srcEncoding.isa<triton::gpu::SliceEncodingAttr>())\n       return failure();\n     SetVector<Operation *> cvtSlices;"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -168,7 +168,7 @@ LogicalResult LoopPipeliner::initialize() {\n   for (Operation &op : *loop)\n     if (auto loadOp = dyn_cast<triton::LoadOp>(&op)) {\n       auto ptr = loadOp.ptr();\n-      unsigned vec = axisInfoAnalysis.getPtrVectorSize(ptr);\n+      unsigned vec = axisInfoAnalysis.getPtrContiguity(ptr);\n       auto ty = getElementTypeOrSelf(ptr.getType())\n                     .cast<triton::PointerType>()\n                     .getPointeeType();"}, {"filename": "test/Analysis/test-alignment.mlir", "status": "modified", "additions": 325, "deletions": 37, "changes": 362, "file_content_changes": "@@ -1,51 +1,336 @@\n // RUN: triton-opt %s -test-print-alignment -split-input-file 2>&1 | FileCheck %s\n \n+// CHECK-LABEL: cast\n+func @cast() {\n+  // CHECK: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [1]\n+  %cst = arith.constant 1 : i32\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [1]\n+  %0 = arith.extsi %cst : i32 to i64\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [1]\n+  %cst_tensor = arith.constant dense<1> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [1]\n+  %1 = tt.bitcast %cst_tensor : tensor<128xi32> -> tensor<128xi64>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: add\n+func @add() {\n+  // CHECK: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [1]\n+  %1 = arith.constant dense<1> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n+  %2 = arith.addi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [127]\n+  %3 = arith.constant dense<127> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [128] ; ConstantValue: [128]\n+  %4 = arith.addi %1, %3 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: sub\n+func @sub() {\n+  // CHECK: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [1]\n+  %1 = arith.constant dense<1> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n+  %2 = arith.subi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [129]\n+  %3 = arith.constant dense<129> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [128] ; ConstantValue: [128]\n+  %4 = arith.subi %3, %1 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: mul\n+func @mul() {\n+  // CHECK: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [1]\n+  %1 = arith.constant dense<1> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %2 = arith.muli %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [128] ; ConstantValue: [128]\n+  %3 = arith.constant dense<128> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [128] ; ConstantValue: [128]\n+  %4 = arith.muli %3, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [2] ; Constancy: [128] ; ConstantValue: [2]\n+  %5 = arith.constant dense<2> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [256] ; Constancy: [128] ; ConstantValue: [256]\n+  %6 = arith.muli %4, %5 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: div\n+func @div() {\n+  // CHECK: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [1]\n+  %1 = arith.constant dense<1> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %2 = arith.divsi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %3 = arith.divui %1, %0 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [64] ; Constancy: [128] ; ConstantValue: [64]\n+  %4 = arith.constant dense<64> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16777216] ; Constancy: [64] ; ConstantValue: [None]\n+  %5 = arith.divsi %0, %4 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16777216] ; Constancy: [1] ; ConstantValue: [None]\n+  %6 = arith.divsi %4, %0 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [2] ; Constancy: [128] ; ConstantValue: [66]\n+  %7 = arith.constant dense<66> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [536870912] ; Constancy: [2] ; ConstantValue: [None]\n+  %8 = arith.divui %0, %7 : tensor<128xi32>\n+  return \n+}\n+\n+// -----\n+\n+// CHECK-LABEL: rem\n+func @rem() {\n+  // CHECK: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [1]\n+  %1 = arith.constant dense<1> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [4611686018427387904] ; Constancy: [128] ; ConstantValue: [0]\n+  %2 = arith.remsi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n+  %3 = arith.remui %1, %0 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [64] ; Constancy: [128] ; ConstantValue: [64]\n+  %4 = arith.constant dense<64> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [64] ; Divisibility: [64] ; Constancy: [1] ; ConstantValue: [None]\n+  %5 = arith.remsi %0, %4 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [64] ; Constancy: [1] ; ConstantValue: [None]\n+  %6 = arith.remsi %4, %0 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [2] ; Constancy: [128] ; ConstantValue: [66]\n+  %7 = arith.constant dense<66> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [2] ; Divisibility: [2] ; Constancy: [1] ; ConstantValue: [None]\n+  %8 = arith.remui %0, %7 : tensor<128xi32>\n+  return \n+}\n+\n+// -----\n+\n+// CHECK-LABEL: broadcast\n+func @broadcast() {\n+  // CHECK: Contiguity: [1] ; Divisibility: [64] ; Constancy: [128] ; ConstantValue: [64]\n+  %0 = arith.constant dense<64> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [64, 1] ; Constancy: [128, 1] ; ConstantValue: [64]\n+  %1 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<128xi32>) -> tensor<128x1xi32>\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [64, 1] ; Constancy: [128, 128] ; ConstantValue: [64]\n+  %2 = tt.broadcast %1 : (tensor<128x1xi32>) -> tensor<128x128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: splat\n+func @splat(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n+  // CHECK: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 128] ; ConstantValue: [None]\n+  %0 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: cmp\n+func @cmp() {\n+  // CHECK: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [4611686018427387904] ; Constancy: [128] ; ConstantValue: [0]\n+  %1 = arith.constant dense<0> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [None]\n+  %2 = arith.cmpi eq, %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [None]\n+  %3 = arith.cmpi slt, %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n+  %4 = arith.cmpi sle, %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [None]\n+  %5 = arith.cmpi sge, %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [8] ; Constancy: [128] ; ConstantValue: [8]\n+  %6 = arith.constant dense<8> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [8] ; ConstantValue: [None]\n+  %7 = arith.cmpi sgt, %0, %6 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [0]\n+  %8 = arith.cmpi sgt, %1, %6 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: logic\n+func @logic() {\n+  // CHECK: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [64] ; Constancy: [128] ; ConstantValue: [64]\n+  %1 = arith.constant dense<64> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16777216] ; Constancy: [64] ; ConstantValue: [None]\n+  %2 = arith.divsi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [8] ; Constancy: [128] ; ConstantValue: [8]\n+  %3 = arith.constant dense<8> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [134217728] ; Constancy: [8] ; ConstantValue: [None]\n+  %4 = arith.divsi %0, %3 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n+  %5 = arith.andi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n+  %6 = arith.ori %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n+  %7 = arith.xori %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [8] ; ConstantValue: [None]\n+  %8 = arith.andi %2, %4 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [8] ; ConstantValue: [None]\n+  %9 = arith.ori %2, %4 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [8] ; ConstantValue: [None]\n+  %10 = arith.xori %2, %4 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: select\n+func @select() {\n+  // CHECK: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [4611686018427387904] ; Constancy: [128] ; ConstantValue: [0]\n+  %1 = arith.constant dense<0> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [None]\n+  %2 = arith.cmpi eq, %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [None]\n+  %3 = arith.cmpi slt, %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [4611686018427387904] ; Constancy: [1] ; ConstantValue: [0]\n+  %4 = arith.constant 0 : i1\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [4611686018427387904] ; Constancy: [128] ; ConstantValue: [0]\n+  %7 = tt.splat %4 : (i1) -> tensor<128xi1>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [4611686018427387904] ; Constancy: [128] ; ConstantValue: [0]\n+  %5 = select %4, %3, %7 : tensor<128xi1>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [128] ; ConstantValue: [None]\n+  %8 = \"triton_gpu.select\"(%7, %3, %2) : (tensor<128xi1>, tensor<128xi1>, tensor<128xi1>) -> tensor<128xi1>\n+  return\n+}\n+\n+// -----\n+\n+func @shift() {\n+  // CHECK: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [8] ; Constancy: [128] ; ConstantValue: [8]\n+  %1 = arith.constant dense<8> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [4] ; Constancy: [128] ; ConstantValue: [4]\n+  %2 = arith.constant dense<4> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [274877906944] ; Constancy: [1] ; ConstantValue: [None]\n+  %3 = arith.shli %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [67108864] ; Constancy: [1] ; ConstantValue: [None]\n+  %4 = arith.shrsi %0, %2 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [128] ; ConstantValue: [128]\n+  %5 = arith.shli %1, %2 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+func @max_min() {\n+  // CHECK: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [64] ; Constancy: [1] ; ConstantValue: [None]\n+  %1 = tt.make_range {end = 192 : i32, start = 64 : i32} : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n+  %2 = arith.maxsi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n+  %3 = arith.minsi %0, %1 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [8] ; Constancy: [128] ; ConstantValue: [8]\n+  %4 = arith.constant dense<8> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [4] ; Constancy: [128] ; ConstantValue: [4]\n+  %5 = arith.constant dense<4> : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [8]\n+  %6 = arith.maxsi %4, %5 : tensor<128xi32>\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: for\n+func @for() {\n+  // CHECK: Contiguity: [1, 1] ; Divisibility: [4611686018427387904, 4611686018427387904] ; Constancy: [128, 32] ; ConstantValue: [0]\n+  %a_init = arith.constant dense<0> : tensor<128x32xi32>\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [128, 32] ; ConstantValue: [1]\n+  %b_init = arith.constant dense<1> : tensor<128x32xi32>\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [4, 4] ; Constancy: [128, 32] ; ConstantValue: [4]\n+  %c_init = arith.constant dense<4> : tensor<128x32xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [1] ; ConstantValue: [128]\n+  %ub = arith.constant 128 : index\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [4611686018427387904] ; Constancy: [1] ; ConstantValue: [0]\n+  %lb = arith.constant 0 : index\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16] ; Constancy: [1] ; ConstantValue: [16]\n+  %step = arith.constant 16 : index\n+  %a, %b, %c = scf.for %iv = %lb to %ub step %step iter_args(%a = %a_init, %b = %b_init, %c = %c_init) -> (tensor<128x32xi32>, tensor<128x32xi32>, tensor<128x32xi32>) {\n+    // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16] ; Constancy: [1] ; ConstantValue: [None]\n+    %t = arith.index_cast %iv : index to i32\n+    // CHECK: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [128, 32] ; ConstantValue: [None]\n+    // CHECK: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [128, 32] ; ConstantValue: [None]\n+    // CHECK: Contiguity: [1, 1] ; Divisibility: [4, 4] ; Constancy: [128, 32] ; ConstantValue: [4]\n+    scf.yield %b, %a, %c : tensor<128x32xi32>, tensor<128x32xi32>, tensor<128x32xi32>\n+  }\n+  return\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: permute_2d\n func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n-  // CHECK: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [1, 1]\n+  // CHECK: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [128, 128] ; ConstantValue: [1]\n   %cst = arith.constant dense<true> : tensor<128x128xi1>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %cst_0 = arith.constant dense<0.000000e+00> : tensor<128x128xf32>\n-  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [65536] ; Constancy: [1]\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n   %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n-  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [65536] ; Constancy: [1]\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n   %1 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n-  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [65536, 1] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [1073741824, 1] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %2 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<128xi32>) -> tensor<128x1xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1]\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1] ; ConstantValue: [None]\n   %3 = tt.splat %arg1 : (i32) -> tensor<128x1xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [1048576, 16] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [17179869184, 16] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %4 = arith.muli %2, %3 : tensor<128x1xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1]\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1] ; ConstantValue: [None]\n   %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %6 = tt.addptr %5, %4 : tensor<128x1x!tt.ptr<f32>>, tensor<128x1xi32>\n-  // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 65536] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 1073741824] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %7 = tt.expand_dims %1 {axis = 0 : i32}: (tensor<128xi32>) -> tensor<1x128xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 128]\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 128] ; ConstantValue: [None]\n   %8 = tt.broadcast %6 : (tensor<128x1x!tt.ptr<f32>>) -> tensor<128x128x!tt.ptr<f32>>\n-  // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 65536] ; Constancy: [128, 1]\n+  // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 1073741824] ; Constancy: [128, 1] ; ConstantValue: [None]\n   %9 = tt.broadcast %7 : (tensor<1x128xi32>) -> tensor<128x128xi32>\n-  // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 16] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 16] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %10 = tt.addptr %8, %9 : tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>\n-  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [65536, 1] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [1073741824, 1] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %11 = tt.expand_dims %0 {axis = 1 : i32}: (tensor<128xi32>) -> tensor<128x1xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1]\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1] ; ConstantValue: [None]\n   %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>>\n-  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %13 = tt.addptr %12, %11 : tensor<128x1x!tt.ptr<f32>>, tensor<128x1xi32>\n-  // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 65536] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 1073741824] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %14 = tt.expand_dims %1 {axis = 0 : i32} : (tensor<128xi32>) -> tensor<1x128xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 128]\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 128] ; ConstantValue: [None]\n   %15 = tt.splat %arg3 : (i32) -> tensor<1x128xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 1048576] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 17179869184] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %16 = arith.muli %14, %15 : tensor<1x128xi32>\n-  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 128]\n+  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 128] ; ConstantValue: [None]\n   %17 = tt.broadcast %13 : (tensor<128x1x!tt.ptr<f32>>) -> tensor<128x128x!tt.ptr<f32>>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 1048576] ; Constancy: [128, 1]\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 17179869184] ; Constancy: [128, 1] ; ConstantValue: [None]\n   %18 = tt.broadcast %16 : (tensor<1x128xi32>) -> tensor<128x128xi32>\n-  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %19 = tt.addptr %17, %18 : tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>\n-  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [1, 1]\n+  // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [1, 1] ; ConstantValue: [None]\n   %20 = tt.load %10, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf32>\n   tt.store %19, %20, %cst : tensor<128x128xf32>\n   return\n@@ -56,28 +341,29 @@ func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {t\n module {\n \n // This is a tiny test for verifying StoreOp-related alignment, It simply store a constant to a buffer.\n+// CHECK-LABEL: store_constant_align\n func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n: i32 {tt.divisibility = 16 : i32}) {\n-  // CHECK: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1]\n+  // CHECK: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n   %pid = tt.get_program_id {axis = 0 : i32} : i32\n-  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [1]\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [1] ; ConstantValue: [128]\n   %c128_i32 = arith.constant 128 : i32\n-  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [1]\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [1] ; ConstantValue: [None]\n   %1 = arith.muli %pid, %c128_i32 : i32\n-  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [65536] ; Constancy: [1]\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [1073741824] ; Constancy: [1] ; ConstantValue: [None]\n   %2 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n- // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [128]\n+ // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [128] ; ConstantValue: [None]\n   %3 = tt.splat %1 : (i32) -> tensor<128xi32>\n- // CHECK-NEXT: Contiguity: [128] ; Divisibility: [128] ; Constancy: [1]\n+ // CHECK-NEXT: Contiguity: [128] ; Divisibility: [128] ; Constancy: [1] ; ConstantValue: [None]\n   %4 = arith.addi %3, %2 : tensor<128xi32>\n-  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16] ; Constancy: [128]\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16] ; Constancy: [128] ; ConstantValue: [None]\n   %5 = tt.splat %addr : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n-  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [16] ; Constancy: [1]\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [16] ; Constancy: [1] ; ConstantValue: [None]\n   %6 = tt.addptr %5, %4 : tensor<128x!tt.ptr<f32>>, tensor<128xi32>\n-  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16] ; Constancy: [128]\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16] ; Constancy: [128] ; ConstantValue: [None]\n   %9 = tt.splat %n : (i32) -> tensor<128xi32>\n-  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [16]\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [16] ; ConstantValue: [None]\n   %mask = arith.cmpi slt, %4, %9 : tensor<128xi32>\n-  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1]\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None]\n   %cst = arith.constant dense<0.0> : tensor<128xf32>\n   tt.store %5, %cst, %mask : tensor<128xf32>\n   return\n@@ -89,6 +375,7 @@ func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n:\n \n // This IR is dumped from vecadd test.\n // Note, the hint {tt.divisibility = 16 : i32} for %n_elements affects the alignment of mask.\n+// CHECK-LABEL: vecadd_mask_align_16\n func @vecadd_mask_align_16(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32 {tt.divisibility = 16 : i32}) {\n   %c64_i32 = arith.constant 64 : i32\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n@@ -101,13 +388,13 @@ func @vecadd_mask_align_16(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %ar\n   %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n   %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   %9 = tt.splat %n_elements : (i32) -> tensor<64xi32>\n-  // CHECK: Contiguity: [1] ; Divisibility: [64] ; Constancy: [16] ( %{{.*}} = arith.cmpi slt, %{{.*}}, %{{.*}} : tensor<64xi32> )\n+  // CHECK: Contiguity: [1] ; Divisibility: [1] ; Constancy: [16] ; ConstantValue: [None] ( %{{.*}} = arith.cmpi slt, %{{.*}}, %{{.*}} : tensor<64xi32> )\n   %mask = arith.cmpi slt, %4, %9 : tensor<64xi32>\n   %11 = tt.load %6, %mask {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n   %12 = tt.load %8, %mask {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n   %13 = arith.addf %11, %12 : tensor<64xf32>\n   %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n-  // CHECK: Contiguity: [64] ; Divisibility: [16] ; Constancy: [1] ( %{{.*}} = tt.addptr %{{.*}}, %{{.*}} : tensor<64x!tt.ptr<f32>>, tensor<64xi32> )\n+  // CHECK: Contiguity: [64] ; Divisibility: [16] ; Constancy: [1] ; ConstantValue: [None] ( %{{.*}} = tt.addptr %{{.*}}, %{{.*}} : tensor<64x!tt.ptr<f32>>, tensor<64xi32> )\n   %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   tt.store %15, %13, %mask : tensor<64xf32>\n   return\n@@ -117,6 +404,7 @@ func @vecadd_mask_align_16(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %ar\n \n // This IR is dumped from vecadd test.\n // Note, there is no divisibility hint for %n_elements, Triton should assume its divisibility to be 1 by default.\n+// CHECK-LABEL: vecadd_mask_align_1\n func @vecadd_mask_align_1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32) {\n   %c64_i32 = arith.constant 64 : i32\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n@@ -129,7 +417,7 @@ func @vecadd_mask_align_1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg\n   %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n   %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   %9 = tt.splat %n_elements : (i32) -> tensor<64xi32>\n-  // CHECK: Contiguity: [1] ; Divisibility: [64] ; Constancy: [1] ( %{{.*}} = arith.cmpi slt, %{{.*}}, %{{.*}} : tensor<64xi32> )\n+  // CHECK: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1] ; ConstantValue: [None] ( %{{.*}} = arith.cmpi slt, %{{.*}}, %{{.*}} : tensor<64xi32> )\n   %10 = arith.cmpi slt, %4, %9 : tensor<64xi32>\n   %11 = tt.load %6, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n   %12 = tt.load %8, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 56, "deletions": 0, "changes": 56, "file_content_changes": "@@ -2,6 +2,7 @@\n \n #layout0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n #layout1 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n+#layout2 = #triton_gpu.mma<{version = 2, warpsPerCTA = [4, 1]}>\n \n // CHECK: [[target_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n // CHECK: [[row_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n@@ -54,6 +55,61 @@ func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   // CHECK: return %6 : tensor<1024xi32, [[target_layout]]>\n }\n \n+// CHECK-LABEL: remat_load_store\n+func @remat_load_store(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %0 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #layout0>\n+  %1 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<64x!tt.ptr<i32>, #layout0>\n+  %2 = tt.addptr %1, %0 : tensor<64x!tt.ptr<i32>, #layout0>, tensor<64xi32, #layout0>\n+  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xi32, #layout0>\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %4 = triton_gpu.convert_layout %3 : (tensor<64xi32, #layout0>) -> tensor<64xi32, #layout1>\n+  %5 = triton_gpu.convert_layout %2 : (tensor<64x!tt.ptr<i32>, #layout0>) -> tensor<64x!tt.ptr<i32>, #layout1>\n+  tt.store %5, %4 : tensor<64xi32, #layout1>\n+  return\n+}\n+\n+// Don't rematerialize vectorized loads\n+// CHECK-LABEL: remat_expensive\n+func @remat_expensive(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %0 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #layout1>\n+  %1 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<64x!tt.ptr<i32>, #layout1>\n+  %2 = tt.addptr %1, %0 : tensor<64x!tt.ptr<i32>, #layout1>, tensor<64xi32, #layout1>\n+  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xi32, #layout1>\n+  // CHECK: triton_gpu.convert_layout\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %4 = triton_gpu.convert_layout %3 : (tensor<64xi32, #layout1>) -> tensor<64xi32, #layout0>\n+  %5 = triton_gpu.convert_layout %2 : (tensor<64x!tt.ptr<i32>, #layout1>) -> tensor<64x!tt.ptr<i32>, #layout0>\n+  tt.store %5, %4 : tensor<64xi32, #layout0>\n+  return\n+}\n+\n+// Don't rematerialize loads when original and target layouts are different\n+// CHECK-LABEL: remat_multi_layout\n+func @remat_multi_layout(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %0 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #layout0>\n+  %1 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<64x!tt.ptr<i32>, #layout0>\n+  %2 = tt.addptr %1, %0 : tensor<64x!tt.ptr<i32>, #layout0>, tensor<64xi32, #layout0>\n+  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xi32, #layout0>\n+  // CHECK: triton_gpu.convert_layout\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %4 = triton_gpu.convert_layout %3 : (tensor<64xi32, #layout0>) -> tensor<64xi32, #layout2>\n+  %5 = triton_gpu.convert_layout %2 : (tensor<64x!tt.ptr<i32>, #layout0>) -> tensor<64x!tt.ptr<i32>, #layout2>\n+  tt.store %5, %4 : tensor<64xi32, #layout2>\n+  return\n+}\n+\n+// Always rematerialize single value loads\n+// CHECK-LABEL: remat_single_value\n+func @remat_single_value(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+  %0 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<1x!tt.ptr<i32>, #layout1>\n+  %1 = tt.load %0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1xi32, #layout1>\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  %2 = triton_gpu.convert_layout %1 : (tensor<1xi32, #layout1>) -> tensor<1xi32, #layout0>\n+  %3 = triton_gpu.convert_layout %0 : (tensor<1x!tt.ptr<i32>, #layout1>) -> tensor<1x!tt.ptr<i32>, #layout0>\n+  tt.store %3, %2 : tensor<1xi32, #layout0>\n+  return\n+}\n+\n // CHECK-LABEL: if\n func @if(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout"}, {"filename": "test/lib/Analysis/TestAlias.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -36,8 +36,8 @@ struct TestAliasPass\n   void runOnOperation() override {\n     Operation *operation = getOperation();\n     auto &os = llvm::errs();\n-    auto op_name = SymbolTable::getSymbolName(operation).getValue().str();\n-    os << op_name << \"\\n\";\n+    auto opName = SymbolTable::getSymbolName(operation).getValue().str();\n+    os << opName << \"\\n\";\n \n     SharedMemoryAliasAnalysis analysis(&getContext());\n     analysis.run(operation);"}, {"filename": "test/lib/Analysis/TestAllocation.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -19,9 +19,9 @@ struct TestAllocationPass\n   void runOnOperation() override {\n     Operation *operation = getOperation();\n     auto &os = llvm::errs();\n-    // Convert to std::string can remove quotes from op_name\n-    auto op_name = SymbolTable::getSymbolName(operation).getValue().str();\n-    os << op_name << \"\\n\";\n+    // Convert to std::string can remove quotes from opName\n+    auto opName = SymbolTable::getSymbolName(operation).getValue().str();\n+    os << opName << \"\\n\";\n     Allocation allocation(operation);\n     operation->walk([&](Operation *op) {\n       auto scratchBufferId = allocation.getBufferId(op);"}, {"filename": "test/lib/Analysis/TestAxisInfo.cpp", "status": "modified", "additions": 11, "deletions": 3, "changes": 14, "file_content_changes": "@@ -11,7 +11,7 @@ struct TestAxisInfoPass\n   // LLVM15+\n   // MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestAlignmentPass);\n \n-  void print(const std::string &name, raw_ostream &os, ArrayRef<int> vals) {\n+  void print(const std::string &name, raw_ostream &os, ArrayRef<int64_t> vals) {\n     os << name << \": [\";\n     for (size_t d = 0; d < vals.size(); d++) {\n       if (d != 0)\n@@ -29,7 +29,8 @@ struct TestAxisInfoPass\n   void runOnOperation() override {\n     Operation *operation = getOperation();\n     auto &os = llvm::errs();\n-    os << \"Testing: \" << operation->getName() << \"\\n\";\n+    auto opName = SymbolTable::getSymbolName(operation).getValue().str();\n+    os << opName << \"\\n\";\n     AxisInfoAnalysis analysis(&getContext());\n     analysis.run(operation);\n     operation->walk([&](Operation *op) {\n@@ -51,7 +52,14 @@ struct TestAxisInfoPass\n         print(\"Divisibility\", os, info.getDivisibility());\n         os << \" ; \";\n         print(\"Constancy\", os, info.getConstancy());\n-        os << \" ( \";\n+        os << \" ; \";\n+        auto constantValue = info.getConstantValue();\n+        os << \"ConstantValue: [\";\n+        if (constantValue.has_value())\n+          os << constantValue.value();\n+        else\n+          os << \"None\";\n+        os << \"] ( \";\n         result.print(os);\n         os << \" ) \";\n         os << \"\\n\";"}, {"filename": "test/lib/Analysis/TestMembar.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -23,8 +23,8 @@ struct TestMembarPass\n     Operation *operation = getOperation();\n     auto &os = llvm::errs();\n     // Convert to std::string can remove quotes from op_name\n-    auto op_name = SymbolTable::getSymbolName(operation).getValue().str();\n-    os << op_name << \"\\n\";\n+    auto opName = SymbolTable::getSymbolName(operation).getValue().str();\n+    os << opName << \"\\n\";\n     Allocation allocation(operation);\n     MembarAnalysis membarPass(&allocation);\n     membarPass.run();"}]